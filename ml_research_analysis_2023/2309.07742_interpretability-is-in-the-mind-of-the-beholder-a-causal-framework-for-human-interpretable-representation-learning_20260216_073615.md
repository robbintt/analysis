---
ver: rpa2
title: 'Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable
  Representation Learning'
arxiv_id: '2309.07742'
source_url: https://arxiv.org/abs/2309.07742
tags:
- concept
- concepts
- learning
- alignment
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a causal framework for human-interpretable
  representation learning (HRL) that explicitly models a human stakeholder as an external
  observer. The key contribution is a formal notion of alignment between machine and
  human representations, which is defined as the ability to transfer names between
  concepts while preserving semantics.
---

# Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning

## Quick Facts
- arXiv ID: 2309.07742
- Source URL: https://arxiv.org/abs/2309.07742
- Reference count: 20
- Key outcome: Proposes a causal framework for human-interpretable representation learning with formal alignment definition between machine and human representations

## Executive Summary
This paper introduces a causal framework for human-interpretable representation learning (HRL) that explicitly models human stakeholders as external observers. The framework defines alignment as the ability to transfer names between concepts while preserving semantics, which is shown to be strictly stronger than disentanglement and to entail no concept leakage. While no quantitative results are provided, the paper offers a novel perspective on interpretability by connecting it to causal abstractions and provides a stepping stone for new research on human-interpretable representations.

## Method Summary
The method proposes a variational autoencoder architecture that maps observations X to a representation M = (MJ, M-J), where MJ contains only interpretable concepts. The framework defines alignment through two desiderata: D1 (disentanglement) ensuring each machine concept encodes information about at most one human concept, and D2 (monotonicity) ensuring predictable relationships between concepts. Alignment is evaluated through causal interventions and information-theoretic metrics, with the goal of enabling symbolic communication between machine and human representations.

## Key Results
- Alignment is formally defined and shown to be strictly stronger than disentanglement
- Concept leakage is identified as a failure of content-style separation
- The framework connects interpretability to causal abstractions for bidirectional communication
- No quantitative results are provided, but theoretical foundations are established

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alignment is strictly stronger than disentanglement for ensuring interpretability
- Mechanism: Alignment combines disentanglement (D1) with monotonicity (D2), making it strictly stronger for effective communication
- Core assumption: Interpretability requires both independent encoding AND predictable relationships between human and machine concepts
- Evidence anchors: Abstract statement on alignment vs disentanglement, formal proof in section 4.2, weak corpus evidence

### Mechanism 2
- Claim: Concept leakage is fundamentally a failure of content-style separation
- Mechanism: Concept leakage occurs when machine concepts encode information about irrelevant factors (style), violating content-style separation
- Core assumption: For interpretability, concepts must encode only task-relevant information without capturing task-irrelevant information
- Evidence anchors: Abstract connection to concept leakage, formal proof in section 4.3, weak corpus evidence

### Mechanism 3
- Claim: Causal abstractions provide the theoretical foundation for bidirectional interpretability
- Mechanism: When both human and machine SCMs are known, their consistency under interventions ensures interpretability in both directions
- Core assumption: True interpretability requires understanding how interventions propagate through both systems consistently
- Evidence anchors: Abstract identification of link to causal abstraction, formal definition in section 4.6, weak corpus evidence

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and do-calculus
  - Why needed here: The entire framework builds on causal representation learning, using SCMs to formalize how concepts are generated and how interventions affect representations
  - Quick check question: Can you explain how the do-operator transforms a causal graph and why this is crucial for defining disentanglement?

- Concept: Information theory (mutual information, entropy)
  - Why needed here: Quantifying concept leakage and alignment requires information-theoretic measures to capture how much information flows between relevant and irrelevant factors
  - Quick check question: How does the mutual information I(MJ,Y) relate to the upper bound I(G-Ι,Y) in the concept leakage analysis?

- Concept: Variational autoencoders and latent variable models
  - Why needed here: The framework assumes data generation through latent factors, requiring understanding of how observations are generated from unobserved concepts and how these can be recovered
  - Quick check question: Why does the marginalization over inputs X in pθ(M|G) = Ex∼p(X|G)[pθ(M|x)] represent a stochastic mapping from ground-truth to machine concepts?

## Architecture Onboarding

- Component map: SCMs for human and machine concepts -> Alignment specification (D1, D2) -> Evaluation through causal interventions and information-theoretic metrics -> Learning through supervision or architectural constraints
- Critical path: 1) Define human concept vocabulary H and machine representation M, 2) Establish causal relationships in SCMs CH and CM, 3) Measure current alignment using IRS and concept leakage metrics, 4) Optimize model architecture or loss functions to improve alignment, 5) Validate through name transfer experiments and causal abstraction consistency checks
- Design tradeoffs: Choosing between disentangled vs block-aligned vs general alignment based on human concept complexity; balancing alignment strength against model expressiveness; deciding between explicit supervision vs architectural bias for alignment
- Failure signatures: High concept leakage despite low IRS scores indicates content-style separation failure; perfect disentanglement but poor interpretability suggests monotonicity (D2) violations; successful name transfer for some concepts but not others reveals partial alignment issues
- First 3 experiments:
  1. Implement IRS measurement on a VAE trained on dSprites to quantify current disentanglement level and identify which generative factors are most entangled
  2. Create a simple linear concept extractor with monotonicity constraints and measure concept leakage using the Λ metric on a synthetic dataset with known ground-truth concepts
  3. Build a toy causal abstraction checker that verifies consistency between human and machine SCMs under interventions on a small, interpretable dataset like Colored MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alignment between machine and human representations be effectively measured in practice for non-disentangled generative factors or human representations?
- Basis in paper: [explicit] The paper states that "none of the existing metrics are suited for non-disentangled generative factors GI or human representations H, which are central for alignment in the block-wise (section 4.4) and general (section 4.5) cases."
- Why unresolved: Existing metrics like DCI and IRS are designed for disentangled representations, and no suitable alternatives are proposed for the more complex cases of block-wise or general alignment.
- What evidence would resolve it: Development and validation of new metrics that can assess alignment in settings where the generative factors or human concepts are not disentangled, tested on real-world datasets.

### Open Question 2
- Question: How can concept annotations be effectively collected from human observers to ensure alignment in personalized settings?
- Basis in paper: [explicit] The paper discusses the challenge of collecting human annotations, noting that "reducing the annotation effort for personalized supervision is challenging" and suggesting strategies like fine-tuning with generic annotations followed by personalized fine-tuning.
- Why unresolved: While strategies are proposed, the effectiveness and efficiency of these methods in practice are not evaluated, and the optimal approach for minimizing annotation effort while ensuring alignment is unclear.
- What evidence would resolve it: Empirical studies comparing different annotation elicitation strategies, including their annotation efficiency, alignment accuracy, and user satisfaction, across various domains and user expertise levels.

### Open Question 3
- Question: What is the impact of imperfect alignment on the interpretability and usefulness of machine explanations?
- Basis in paper: [explicit] The paper suggests that "perfect alignment is not strictly necessary" and that "slight differences in semantics between machine and human concepts are unlikely to have major effects on communication."
- Why unresolved: The paper does not quantify the threshold of acceptable misalignment or provide empirical evidence on how varying degrees of misalignment affect user understanding and trust in explanations.
- What evidence would resolve it: User studies measuring the impact of different levels of alignment (or misalignment) on explanation comprehension, task performance, and user trust, across various domains and explanation types.

## Limitations
- Lack of empirical validation for theoretical claims
- Assumption that human concepts can be accurately modeled using SCMs
- No quantification of alignment thresholds for practical interpretability

## Confidence
- Confidence in theoretical claims: Medium
- Confidence in framework's applicability to real-world problems: Low

## Next Checks
1. Empirical Validation on Synthetic Data: Implement IRS measurement and concept leakage metrics on synthetic datasets where ground-truth causal relationships are known.
2. Human Study on Name Transfer: Conduct a human study where participants attempt to transfer names between concepts in aligned vs non-aligned representations.
3. Robustness to Concept Complexity: Evaluate the framework's performance as the number and complexity of human concepts increases.