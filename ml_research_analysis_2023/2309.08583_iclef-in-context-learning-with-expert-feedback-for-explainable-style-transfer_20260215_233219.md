---
ver: rpa2
title: 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer'
arxiv_id: '2309.08583'
source_url: https://arxiv.org/abs/2309.08583
tags:
- style
- language
- informal
- transfer
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ICLEF, a novel human-AI collaboration approach
  that uses scarce expert human feedback to improve model-generated data for explainable
  style transfer. The authors use in-context learning and self-critique to prompt
  ChatGPT to generate synthetic feedback based on a small number of expert corrections.
---

# ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer

## Quick Facts
- arXiv ID: 2309.08583
- Source URL: https://arxiv.org/abs/2309.08583
- Reference count: 26
- One-line primary result: This paper proposes ICLEF, a novel human-AI collaboration approach that uses scarce expert human feedback to improve model-generated data for explainable style transfer.

## Executive Summary
This paper introduces ICLEF, a novel approach that leverages in-context learning and self-critique to improve model-generated explanations for style transfer tasks using minimal expert feedback. The method combines large language model outputs with expert corrections to create high-quality training data for smaller, more efficient models. The authors demonstrate their approach on the e-GY AFC dataset, showing that models fine-tuned on this data outperform one-shot instruction-tuned models and ChatGPT on explainable style transfer tasks.

## Method Summary
The ICLEF framework uses model distillation to augment style transfer corpora with natural language explanations. It employs in-context learning with expert feedback corrections to guide a large language model's self-critique and improve its generated explanations. The method involves generating initial explanations using ChatGPT, collecting expert corrections, and then using these corrections in-context to prompt the model to refine its outputs. The resulting dataset is used to fine-tune smaller, more efficient models for the explainable style transfer task.

## Key Results
- Models fine-tuned on e-GY AFC outperform one-shot instruction-tuned models and ChatGPT on explainable style transfer
- Generated explanations can be used as interpretable features for authorship verification
- The formal-to-informal explainable style transfer model can be used as an interpretable adversarial attack on AI-generated text detection methods

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning with Expert Feedback
- Claim: A small number of expert corrections can guide a large language model to self-critique and improve its own outputs.
- Mechanism: The model is provided with a few examples of expert corrections and then prompted to act as an annotator, identifying and fixing errors in its own generated explanations.
- Core assumption: The model's self-critique ability, when conditioned on expert feedback, is sufficient to significantly improve the quality of generated explanations.
- Evidence anchors:
  - [abstract]: "Inspired by the success of self-critiquing approaches... we provide the model with the expert human feedback corrections in-context and prompt it to act as an annotator on the new instances"
  - [section]: "we provide the model with the expert human feedback corrections in-context and prompt it to act as an annotator on the new instances"
  - [corpus]: Weak evidence. The corpus only shows that this approach was applied, but does not directly evaluate the effectiveness of the self-critique mechanism.
- Break condition: If the model cannot effectively learn from the limited expert feedback, or if the expert corrections are not representative of the errors in the model's outputs, the self-critique mechanism will fail.

### Mechanism 2: Distillation from Large to Small Models
- Claim: Fine-tuning smaller models on the outputs of large language models can achieve comparable or better performance on the explainable style transfer task.
- Mechanism: Large language models generate initial explanations, which are then refined with expert feedback and used to train smaller, more efficient models.
- Core assumption: The large language model's outputs, when refined with expert feedback, provide high-quality training data for smaller models.
- Evidence anchors:
  - [abstract]: "We propose a framework to augment and improve a formality style transfer dataset with explanations via model distillation from ChatGPT."
  - [section]: "We propose to use distillation as a means to augment existing style transfer corpora with natural language explanations."
  - [corpus]: Weak evidence. The corpus mentions the use of distillation but does not provide direct evidence of its effectiveness.
- Break condition: If the large language model's initial outputs are of poor quality, or if the expert feedback is insufficient to correct all errors, the distilled smaller models will not perform well.

### Mechanism 3: Semi-Structured Explanations for Style Transfer
- Claim: Using semi-structured explanations with attributes and textual evidence improves the interpretability and evaluability of style transfer models.
- Mechanism: The model generates explanations in a structured format, listing style attributes and providing evidence from the text, rather than free-form text.
- Core assumption: Structured explanations are easier for both humans and automatic metrics to understand and evaluate compared to free-form text.
- Evidence anchors:
  - [abstract]: "We chose the semi-structured format for the explanations... These explanations are more helpful to the user than traditional free-text explanations since they have a consistent format with textual evidence"
  - [section]: "We use a semi-structured format for the explanations. Specifically, we ask the model to generate a list of attributes followed by an excerpt from the sentence as the evidence"
  - [corpus]: No direct evidence. The corpus does not evaluate the effectiveness of semi-structured explanations.
- Break condition: If the structured format is too rigid and prevents the model from capturing important nuances, or if humans find it harder to understand than free-form text, this approach will not be effective.

## Foundational Learning

- Concept: Style Transfer
  - Why needed here: The task involves transforming text along a particular style dimension, such as formality.
  - Quick check question: What are some common style dimensions used in text style transfer tasks?
- Concept: Model Distillation
  - Why needed here: The approach uses large language models to generate initial explanations, which are then used to train smaller, more efficient models.
  - Quick check question: What is the main benefit of using model distillation in this context?
- Concept: In-Context Learning
  - Why needed here: The approach uses a small number of expert corrections provided in-context to guide the model's self-critique.
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

## Architecture Onboarding

- Component map:
  ChatGPT generation -> Expert feedback -> GPT-Critic self-critique -> Fine-tuning smaller models
- Critical path: ChatGPT generation → Expert feedback → GPT-Critic self-critique → Fine-tuning smaller models
- Design tradeoffs:
  - Using a large language model for initial generation allows for high-quality outputs, but is computationally expensive.
  - Incorporating expert feedback improves quality but is costly and time-consuming.
  - Self-critique allows for iterative improvement but may not catch all errors.
- Failure signatures:
  - Poor quality initial generations from ChatGPT.
  - Expert feedback that is not representative of the model's errors.
  - Self-critique that fails to identify or correct errors.
  - Overfitting of smaller models to the training data.
- First 3 experiments:
  1. Evaluate the quality of initial explanations generated by ChatGPT.
  2. Assess the effectiveness of expert feedback in correcting errors.
  3. Measure the performance of smaller models fine-tuned on the refined data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do the explainable style transfer models perform on formality style transfer datasets beyond GYAFC?
- Basis in paper: The paper focuses on evaluating models on the e-GY AFC dataset, which is derived from GYAFC. There is no exploration of performance on other formality style transfer datasets.
- Why unresolved: The authors chose to focus on GYAFC and its explainable variant for their experiments, leaving open the question of generalizability to other datasets.
- What evidence would resolve it: Experiments evaluating the fine-tuned models on other formality style transfer datasets like FASHION, SYNSIN, or PCtrans would provide insights into generalizability.

### Open Question 2
- Question: How does the quality of explanations generated by the fine-tuned models compare to human-generated explanations for the style transfer task?
- Basis in paper: The paper evaluates the quality of explanations using automatic metrics like BLEU and human evaluation, but does not directly compare to human-generated explanations.
- Why unresolved: While the explanations are shown to be useful for downstream tasks, their quality relative to human explanations is not assessed.
- What evidence would resolve it: Collecting human-generated explanations for a subset of the e-GY AFC data and comparing them to the model-generated explanations using metrics like BLEU or human preference would provide a direct comparison.

### Open Question 3
- Question: How does the proposed ICLEF method compare to other approaches for incorporating scarce expert feedback in natural language generation tasks?
- Basis in paper: The paper introduces ICLEF as a novel approach for incorporating expert feedback using in-context learning and self-critique. However, it does not compare ICLEF to other feedback incorporation methods.
- Why unresolved: The effectiveness of ICLEF relative to other methods for incorporating scarce expert feedback is not established.
- What evidence would resolve it: Experiments comparing ICLEF to other feedback incorporation methods like reinforcement learning from human feedback (RLHF) or direct preference optimization (DPO) on a similar task would provide a comparison.

## Limitations
- Quality and representativeness of expert feedback: The study relies on corrections from 10 expert linguists, but provides limited information about their selection criteria or potential biases.
- Scalability constraints: The manual nature of expert feedback collection presents a significant bottleneck for broader application.
- Limited evaluation scope: The evaluation focuses primarily on formality style transfer and authorship verification tasks, with unproven generalizability to other style dimensions.

## Confidence

- **High confidence**: The effectiveness of model distillation for training smaller, task-specific models on large language model outputs is well-established in the literature and supported by the paper's experimental results.

- **Medium confidence**: The claim that ICLEF can generate high-quality explanations with minimal expert input is supported by the results, but the limited scale of expert feedback collection (10 experts) and the specific task domain introduce uncertainty about broader applicability.

- **Low confidence**: The assertion that the generated explanations can serve as interpretable features for authorship verification lacks extensive validation. The paper provides preliminary evidence but does not thoroughly explore the limitations or potential failure modes of this application.

## Next Checks
1. Cross-domain validation: Test ICLEF on style transfer tasks outside of formality, such as sentiment modification or domain adaptation (e.g., academic to lay language), to assess generalizability.

2. Scalability analysis: Conduct experiments varying the number of expert corrections (e.g., 5, 20, 50) to determine the minimum effective sample size and identify potential saturation points in the self-critique mechanism.

3. Ablation study on structured format: Compare the performance and human interpretability of semi-structured explanations against free-form explanations generated by the same model, controlling for the amount of expert feedback, to isolate the impact of the structured format on task performance.