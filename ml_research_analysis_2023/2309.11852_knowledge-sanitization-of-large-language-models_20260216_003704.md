---
ver: rpa2
title: Knowledge Sanitization of Large Language Models
arxiv_id: '2309.11852'
source_url: https://arxiv.org/abs/2309.11852
tags:
- sanitization
- knowledge
- information
- llama
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces knowledge sanitization, a method to mitigate\
  \ privacy risks in large language models (LLMs) by fine-tuning them to respond with\
  \ harmless phrases like \u201CI don\u2019t know\u201D when queried about specific\
  \ sensitive information. Using Low-Rank Adaptation (LoRA) to fine-tune only MLP\
  \ layers, the approach reduces information leakage without harming overall model\
  \ performance."
---

# Knowledge Sanitization of Large Language Models

## Quick Facts
- arXiv ID: 2309.11852
- Source URL: https://arxiv.org/abs/2309.11852
- Reference count: 31
- One-line primary result: Sanitized models achieved 0% accuracy on forgetting targets while preserving accuracy on retention targets using LoRA fine-tuning on MLP layers

## Executive Summary
This paper introduces knowledge sanitization, a method to mitigate privacy risks in large language models by fine-tuning them to respond with harmless phrases like "I don't know" when queried about specific sensitive information. The approach uses Low-Rank Adaptation (LoRA) to fine-tune only MLP layers, reducing information leakage without harming overall model performance. Experiments with LLaMA and GPT-J on TriviaQA demonstrated that sanitized models achieved 0% accuracy on forgetting targets while preserving accuracy on retention targets, compared to 74% and 49.9% respectively in the original models.

## Method Summary
The method uses LoRA to fine-tune MLP layers of pre-trained LLMs on a mixed dataset containing both sanitization targets (K_S) and retention targets (K_R). For forgetting targets, the model is trained to generate harmless phrases like "I don't know" instead of the correct answer. The approach selectively modifies knowledge storage while preserving general language abilities, achieving effective sanitization without catastrophic forgetting of non-targeted knowledge.

## Key Results
- Sanitized models achieved 0% accuracy on forgetting targets while preserving 49.8% accuracy on retention targets
- Performance maintained on common-sense reasoning and reading comprehension tasks
- Leakage rate reduced to 0-4.3% for forgetting targets in generated text
- Robustness demonstrated against extraction attacks using adversarial prompts

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning MLP layers via LoRA selectively modifies knowledge storage while preserving general language abilities. MLP layers are primary knowledge storage in transformers (Geva et al., 2021; Dai et al., 2022), so applying LoRA to these layers changes specific factual associations without disrupting other learned patterns. Break condition: If knowledge is distributed across attention layers, this selective fine-tuning would fail.

### Mechanism 2
Combining sanitization data (K_S) with retention data (K_R) prevents catastrophic forgetting. Training on mixed dataset K_S ∪ K_R balances the model's tendency to generate sanitization phrases only when appropriate, maintaining performance on non-targeted knowledge domains. Break condition: If the model cannot distinguish between sanitization and retention prompts, it would either over-sanitize or under-sanitize.

### Mechanism 3
Generating predefined harmless phrases is more effective than unlearning approaches. By explicitly training the model to generate "I don't know" or similar phrases, the method avoids the problem of unlearning approaches that may generate plausible but false information. Break condition: If the model learns to bypass sanitization phrases or if the harmless phrases themselves become problematic.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables efficient fine-tuning by modifying only specific weight matrices rather than full model parameters
  - Quick check question: How does LoRA decompose weight matrices to enable parameter-efficient fine-tuning?

- Concept: Knowledge storage in transformer architectures
  - Why needed here: Understanding where knowledge is stored (MLP vs attention layers) is critical for effective selective fine-tuning
  - Quick check question: What evidence suggests MLP layers are primary knowledge storage locations in transformers?

- Concept: Differential privacy vs. knowledge sanitization
  - Why needed here: The paper contrasts its approach with differential privacy, highlighting that sanitization directly controls outputs rather than adding noise
  - Quick check question: How does knowledge sanitization differ fundamentally from differential privacy approaches?

## Architecture Onboarding

- Component map: Pre-trained LLM → LoRA-modified MLP layers → Knowledge sanitization phrases for targeted prompts → Preserved responses for non-targeted prompts
- Critical path: Input prompt → MLP layer processing (with LoRA modifications) → Output generation → Classification as sanitization target or retention target
- Design tradeoffs: Selective fine-tuning of MLP layers vs. full model fine-tuning (computational efficiency vs. completeness), explicit sanitization phrases vs. unlearning approaches (safety vs. naturalness)
- Failure signatures: Over-sanitization (model responds "I don't know" too frequently), under-sanitization (model leaks sensitive information), performance degradation on non-targeted knowledge
- First 3 experiments:
  1. Verify LoRA fine-tuning on MLP layers changes targeted knowledge while preserving other knowledge using TriviaQA
  2. Test different sanitization phrases ("I don't know" vs. "I lack knowledge" vs. other variants) for effectiveness
  3. Evaluate robustness against extraction attacks using prompts designed to elicit sensitive information about specific entities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does knowledge sanitization impact the performance of large language models on tasks that require generation of specific factual information?
- Basis in paper: Explicit
- Why unresolved: The paper only provides evidence that knowledge sanitization reduces accuracy on forgetting targets while maintaining accuracy on retention targets. It does not explore the impact on tasks that require generation of specific factual information.
- What evidence would resolve it: Experiments that evaluate the performance of knowledge-sanitized models on tasks that require generation of specific factual information, such as generating biographical details about a person or summarizing a news article.

### Open Question 2
- Question: Can knowledge sanitization be used to mitigate privacy risks in large language models without significantly impacting their overall performance?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that knowledge sanitization can effectively reduce privacy risks while maintaining overall performance. However, it does not explore the potential trade-offs between privacy and performance.
- What evidence would resolve it: Experiments that systematically vary the level of knowledge sanitization and measure its impact on both privacy and performance. This would help identify the optimal balance between privacy and performance for different applications.

### Open Question 3
- Question: How does the choice of sanitization phrase affect the effectiveness of knowledge sanitization in large language models?
- Basis in paper: Inferred
- Why unresolved: The paper uses "I don't know" as the default sanitization phrase. It does not explore the impact of using different sanitization phrases on the effectiveness of knowledge sanitization.
- What evidence would resolve it: Experiments that compare the effectiveness of different sanitization phrases in reducing privacy risks while maintaining overall performance. This would help identify the most effective sanitization phrases for different applications.

## Limitations

- The study focuses on fine-tuning rather than retrieval-augmented generation (RAG) approaches, which could be more effective for handling factual knowledge without storing it directly in model weights
- Evaluation primarily uses TriviaQA as the knowledge source, which may not generalize to other domains or types of sensitive information
- The method's effectiveness against more sophisticated extraction attacks beyond the simple adversarial prompts tested is unclear

## Confidence

**High Confidence:** The empirical results showing successful knowledge sanitization on TriviaQA are well-supported by the experimental data with clear accuracy metrics.

**Medium Confidence:** The claim that MLP layers are the primary location for knowledge storage in transformers is supported by cited literature but not directly validated in this specific paper.

**Low Confidence:** The assertion that this approach is superior to unlearning methods in terms of hallucination reduction is based on indirect comparisons rather than direct head-to-head evaluations.

## Next Checks

1. Cross-domain validation: Test the knowledge sanitization approach on datasets beyond TriviaQA, including different types of factual knowledge (medical, legal, financial) and longer-form text to assess generalizability.

2. Robustness against sophisticated attacks: Evaluate the sanitized models against state-of-the-art extraction attacks, including membership inference, model inversion, and adversarial prompting techniques that go beyond the simple adversarial prompts used in the current study.

3. LoRA hyperparameter sensitivity analysis: Systematically vary LoRA rank, learning rate, and training duration to understand how these parameters affect sanitization effectiveness and knowledge preservation, identifying optimal configurations for different use cases.