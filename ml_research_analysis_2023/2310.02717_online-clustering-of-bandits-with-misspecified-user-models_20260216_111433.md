---
ver: rpa2
title: Online Clustering of Bandits with Misspecified User Models
arxiv_id: '2310.02717'
source_url: https://arxiv.org/abs/2310.02717
tags:
- user
- users
- uni00000013
- bound
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of clustering of bandits with
  misspecified user models (CBMUM), where the expected rewards are not perfectly linear
  in user preferences. The authors design two robust algorithms, RCLUMB and RSCLUMB,
  that adaptively learn the clustering structure while handling model misspecifications.
---

# Online Clustering of Bandits with Misspecified User Models

## Quick Facts
- **arXiv ID:** 2310.02717
- **Source URL:** https://arxiv.org/abs/2310.02717
- **Reference count:** 40
- **Key outcome:** Introduces CBMUM problem and designs RCLUMB/RSCLUMB algorithms with regret bounds O(ε* T√md log T + d√mT log T) under milder assumptions than previous works.

## Executive Summary
This paper addresses the problem of clustering bandits with misspecified user models (CBMUM), where expected rewards are not perfectly linear in user preferences. The authors introduce two robust algorithms, RCLUMB and RSCLUMB, that adaptively learn the clustering structure while handling model misspecifications through more tolerant edge deletion rules and larger confidence radii. Theoretical analysis proves regret upper bounds that match state-of-the-art results in several degenerate cases, while experiments on synthetic and real-world data demonstrate superiority over existing methods.

## Method Summary
The paper proposes two algorithms: RCLUMB (graph-based) and RSCLUMB (set-based) that maintain dynamic clustering structures while handling model misspecifications. Both algorithms use enlarged confidence radii to account for estimation uncertainty from misspecifications, and employ tolerant edge deletion rules to avoid misclustering users with large preference gaps. RCLUMB maintains a complete graph over users and filters 1-hop neighbors for recommendations, while RSCLUMB maintains set-based clusters with both split and merge operations. The algorithms balance tolerance for misspecifications with selective clustering to achieve regret bounds of O(ε* T√md log T + d√mT log T).

## Key Results
- Proves regret upper bounds of O(ε* T√md log T + d√mT log T) under milder assumptions than previous contextual bandit works
- Introduces RCLUMB algorithm with graph-based clustering structure that filters users directly linked to current user
- Introduces RSCLUMB algorithm with set-based clustering structure allowing both split and merge operations
- Experiments show superiority over existing methods on both synthetic and real-world datasets (Yelp, Movielens)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RCLUMB achieves regret bound O(ε* T√md log T + d√mT log T) by balancing tolerance for misspecifications with selective clustering.
- **Mechanism:** Uses dynamic graph where users connect if preference vectors within ζ distance, with tolerant edge deletion and filtering of 1-hop neighbors to avoid using distant users with large preference gaps.
- **Core assumption:** Users in same cluster share same preference vector; users in different clusters separated by at least γ.
- **Evidence anchors:** Abstract shows regret bounds match state-of-the-art; Section 4 describes tolerant edge deletion and filtering approach.
- **Break condition:** If ε* is too large relative to γ, algorithm cannot distinguish between clusters, leading to high regret.

### Mechanism 2
- **Claim:** Enlarged confidence radius Ca,t accounts for both exploration bonus and uncertainty from model deviations.
- **Mechanism:** Confidence radius includes standard exploration term, misspecification uncertainty term ε*√(2d/λ3/2x), and term summing products of feature vectors and inverse Gram matrix elements.
- **Core assumption:** Deviation vectors for users in same cluster can differ, with known upper bound ε* on maximum misspecification level.
- **Evidence anchors:** Section 4 describes enlarged confidence radius design; Lemma 5.6 provides concentration inequality with probability 1 - 5δ.
- **Break condition:** If λ is too small, confidence radius becomes too large, leading to overly conservative recommendations.

### Mechanism 3
- **Claim:** RSCLUMB achieves similar bounds through flexible set-based clustering with split and merge operations.
- **Mechanism:** Maintains multiple clusters, splits inconsistent users, and merges clusters with close estimated preference vectors to leverage collaborative information.
- **Core assumption:** Users can be inconsistent due to misspecifications; clusters with close estimated preferences should be merged.
- **Evidence anchors:** Appendix K describes set-based structure with split/merge operations; Appendix L provides regret theorem with specific bound.
- **Break condition:** If thresholds not set appropriately, algorithm may split too aggressively or merge dissimilar clusters.

## Foundational Learning

- **Concept:** Linear Bandits and UCB Strategy
  - Why needed here: Paper builds on linear bandit framework where expected rewards are linear in features; UCB balances exploration and exploitation.
  - Quick check question: What is the key difference between the UCB strategy used here and standard UCB for linear bandits?

- **Concept:** Clustering of Bandits (CB)
  - Why needed here: Extends CB framework to handle model misspecifications where rewards aren't perfectly linear; clustering leverages collaborative information among similar users.
  - Quick check question: How does assumption of model misspecifications affect clustering process compared to previous CB works?

- **Concept:** Sub-Gaussian Random Variables and Concentration Inequalities
  - Why needed here: Uses concentration inequalities to bound estimation errors of preference vectors and deviations from linearity; sub-Gaussian noise models reward observations.
  - Quick check question: What role does sub-Gaussian assumption play in concentration inequalities used in regret analysis?

## Architecture Onboarding

- **Component map:** User Graph/Cluster Sets -> Preference Estimation -> Arm Selection -> Graph/Cluster Update
- **Critical path:**
  1. Receive current user and arm set
  2. Determine cluster for current user based on clustering structure
  3. Estimate preference vector of cluster
  4. Select arm using UCB with enlarged confidence radius
  5. Observe reward and update user statistics
  6. Update clustering structure based on rewards and misspecifications
- **Design tradeoffs:** Tolerance vs. selectivity in clustering; exploration vs. exploitation in arm selection; graph vs. set structure flexibility vs. computational complexity
- **Failure signatures:** High regret (poor collaborative information leverage or misclustering); slow convergence (ineffective exploration or inaccurate preference estimation); unstable clustering (poor misspecification handling or incorrect thresholds)
- **First 3 experiments:**
  1. Synthetic data with known misspecification level to verify regret bounds and compare RCLUMB vs RSCLUMB
  2. Real-world data (Yelp, Movielens) with known misspecification levels to validate practical effectiveness
  3. Ablation study on misspecification levels (known/unknown) to understand impact on performance and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the exact regret lower bound for CBMUM, and does it depend on the number of clusters m?
- **Basis in paper:** [explicit] Authors conjecture gap for m factor is due to strong assumption that cluster structures are known to prove current lower bound
- **Why unresolved:** Authors leave proving tighter lower bound for CBMUM as open problem
- **What evidence would resolve it:** Rigorous proof of regret lower bound that either matches upper bound asymptotically or shows gap with respect to m

### Open Question 2
- **Question:** How would proposed algorithms perform if exact maximum model misspecification level is unknown?
- **Basis in paper:** [inferred] Authors mention incorporating recent model selection methods as interesting future work
- **Why unresolved:** Current algorithms assume knowledge of upper bound on maximum misspecification level
- **What evidence would resolve it:** Experimental results or theoretical analysis when exact maximum misspecification level is unknown

### Open Question 3
- **Question:** How would proposed algorithms perform if underlying user clustering structure is misspecified?
- **Basis in paper:** [explicit] Authors mention considering setting with misspecifications in underlying clustering structure as interesting future work
- **Why unresolved:** Current algorithms assume known underlying clustering structure, focus on model misspecifications
- **What evidence would resolve it:** Experimental results or theoretical analysis when underlying clustering structure is misspecified

## Limitations

- Theoretical analysis assumes known upper bounds on misspecification levels and clustering gaps, which may not hold in practice
- Regret bounds depend on ratio between misspecification level ε* and clustering gap γ, making performance sensitive to this relationship
- Sample size for real-world datasets (Yelp, Movielens) is relatively small (around 100 users per cluster), limiting generalizability

## Confidence

- **High confidence:** Core algorithmic framework (RCLUMB and RSCLUMB) and regret analysis are mathematically rigorous and well-supported by proofs
- **Medium confidence:** Practical effectiveness demonstrated by experiments, though limited sample size in real-world datasets warrants caution
- **Medium confidence:** Assumption of known misspecification bounds, which may not always be realistic in practice

## Next Checks

1. Conduct experiments on larger real-world datasets with more users per cluster to validate scalability and robustness
2. Test algorithms under varying levels of misspecification (both known and unknown) to assess adaptability and performance degradation
3. Perform ablation studies to understand impact of different hyperparameters (edge deletion thresholds, confidence radius scaling) on regret bounds and clustering accuracy