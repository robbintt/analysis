---
ver: rpa2
title: Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning
  and Prompt-tuning with Rules
arxiv_id: '2308.16770'
source_url: https://arxiv.org/abs/2308.16770
tags:
- tasks
- labour
- market
- performance
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of prompt-tuning with rules
  (PTR) and instruction-based finetuning to enhance the performance of pre-trained
  language models (PLMs) on labour market-specific tasks. The authors generate self-supervised
  benchmark datasets from the ESCO taxonomy and evaluate the performance of different
  PLM configurations on entity classification, relation classification, entity linking,
  and question answering tasks.
---

# Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning and Prompt-tuning with Rules

## Quick Facts
- arXiv ID: 2308.16770
- Source URL: https://arxiv.org/abs/2308.16770
- Reference count: 37
- Primary result: FLAN-T5 instruction-tuned variant outperforms T5 on two of three labor market-specific tasks

## Executive Summary
This paper explores how instruction-based finetuning and prompt-tuning with rules (PTR) can enhance pre-trained language models (PLMs) on labor market-specific tasks. The authors generate self-supervised benchmark datasets from the ESCO taxonomy and evaluate different PLM configurations on entity classification, relation classification, entity linking, and question answering tasks. The study demonstrates that instruction-based finetuning (FLAN-T5) substantially outperforms T5 on two of three tasks, and that PTR-based finetuning with labor market-specific examples yields improvements beyond general finetuning. The research also highlights the potential of transfer learning across labor market-specific tasks.

## Method Summary
The authors generate self-supervised benchmark datasets from the ESCO taxonomy containing 3,008 occupations and 13,980 skills. They evaluate FLAN-T5 and T5 models using both PTR and instruction-based finetuning approaches with K-shot learning (K=64, 128, 256) and multitask learning. The models are assessed on entity classification, relation classification, entity linking, and question answering tasks using F1 scores averaged over 9 runs with standard deviations. The finetuning pipeline leverages HuggingFace, PyTorch, and OpenPrompt frameworks to implement PTR and instruction-based finetuning methods.

## Key Results
- FLAN-T5 outperforms T5 on QA (83.44 vs. 33.75) and EL tasks (57.38 vs. 33.89)
- PTR-based finetuning with 128 examples yields 7.06% performance increase over zero-shot FLAN-T5
- Transfer learning across labor market tasks shows performance improvements on unseen tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-based finetuning improves PLM performance on unseen domain-specific tasks
- Mechanism: Instruction tuning exposes PLMs to a variety of task formulations, improving their ability to generalize to new tasks with minimal additional training
- Core assumption: The PLM can learn to map instruction patterns to task-solving strategies that transfer across domains
- Evidence anchors:
  - [abstract]: "instruction-based finetuning improves PLMs on evaluation benchmarks by up to 9.4%, requiring only 0.2% of the pre-training compute"
  - [section]: "FLAN-T5 substantially outperforms T5 on the QA (83.44 vs. 33.75 respectively) and EL tasks (57.38 vs. 33.89 respectively)"

### Mechanism 2
- Claim: Prompt-tuning with rules (PTR) enables effective few-shot learning for labor market tasks
- Mechanism: PTR combines template-based prompting with label word mapping, allowing PLMs to learn task-specific patterns with minimal examples
- Core assumption: The ESCO taxonomy provides sufficient structured knowledge to create effective prompts for labor market tasks
- Evidence anchors:
  - [abstract]: "PTR-based finetuning with labour market-specific examples yields improvements above and beyond the general finetuning"
  - [section]: "PTR-based finetuning with 128 examples leveraged the best performance. Overall, this yielded an 7.06% performance increase over the zero-shot performance of FLAN-T5"

### Mechanism 3
- Claim: Transfer learning across labor market tasks improves performance on unseen tasks
- Mechanism: Learning multiple related tasks simultaneously allows PLMs to develop shared representations that benefit performance on new tasks
- Core assumption: Labor market tasks share sufficient underlying structure to enable transfer
- Evidence anchors:
  - [abstract]: "the tuned PLM able to transfer the learned behavior across labour market specific tasks"
  - [section]: "models tuned on (a) task(s) that do not include the test task used for evaluation... perform substantially worse... versus between 95.17 and 98.48 for models that have seen EL prompts"

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Establishes baseline performance of PLMs on labor market tasks without any domain-specific training
  - Quick check question: What is the performance difference between T5 and FLAN-T5 on labor market tasks in a zero-shot setting?

- Concept: Few-shot learning
  - Why needed here: Enables efficient adaptation of PLMs to domain-specific tasks with minimal labeled data
  - Quick check question: How does performance change as the number of training examples (K) increases for PTR and instruction-based finetuning?

- Concept: Transfer learning
  - Why needed here: Leverages knowledge gained from one task to improve performance on related tasks
  - Quick check question: Does training on multiple labor market tasks improve performance on unseen tasks compared to training on single tasks?

## Architecture Onboarding

- Component map: ESCO taxonomy -> T5/FLAN-T5 models -> PTR templates -> Instruction templates -> Datasets -> Finetuning pipeline
- Critical path: Extract triples from ESCO taxonomy -> Populate templates with ESCO data to create datasets -> Perform zero-shot evaluation of base PLMs -> Apply PTR and instruction-based finetuning -> Evaluate few-shot and transfer learning performance
- Design tradeoffs: Template design vs. flexibility; Number of examples vs. computational cost; Task selection for transfer learning
- Failure signatures: Poor template design leading to suboptimal prompt generation; Inadequate label word mapping causing classification errors; Task interference where learning one task negatively impacts performance on another
- First 3 experiments: 1) Zero-shot evaluation of T5 and FLAN-T5 on EC+RC, QA, and EL tasks; 2) Few-shot learning with varying K values for PTR and instruction-based finetuning; 3) Multi-task learning evaluation across different task combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does instruction-based fine-tuning improve performance on labor market-specific tasks compared to general tasks?
- Basis in paper: [explicit] The authors demonstrate that FLAN-T5, an instruction-tuned variant of T5, outperforms the original T5 model on two of the three labor market-specific tasks.
- Why unresolved: While the authors show that instruction-based fine-tuning improves performance on labor market-specific tasks, they do not directly compare the performance of models fine-tuned on general tasks versus those fine-tuned on labor market-specific tasks.
- What evidence would resolve it: A direct comparison of the performance of models fine-tuned on general tasks versus those fine-tuned on labor market-specific tasks on the same labor market-specific tasks.

### Open Question 2
- Question: What is the optimal number of examples (K) for few-shot learning in labor market-specific tasks?
- Basis in paper: [explicit] The authors propose using K = {64, 128, 256} for few-shot learning and find that K = 128 yields the best performance.
- Why unresolved: While the authors find that K = 128 yields the best performance, they do not explore a wider range of K values or investigate the relationship between K and performance.
- What evidence would resolve it: A systematic exploration of a wider range of K values and an analysis of the relationship between K and performance.

### Open Question 3
- Question: How does transfer learning across labor market-specific tasks affect performance?
- Basis in paper: [explicit] The authors demonstrate that transfer learning across labor market-specific tasks can improve performance on unseen tasks.
- Why unresolved: While the authors show that transfer learning can improve performance, they do not investigate the optimal way to combine tasks or the impact of the order in which tasks are learned.
- What evidence would resolve it: An investigation of the optimal way to combine tasks and the impact of the order in which tasks are learned on performance.

## Limitations
- Reliance on single taxonomy (ESCO) may introduce European labor market bias
- Evaluation confined to English despite ESCO's multilingual nature
- Absolute performance on some tasks (particularly entity linking at 57.38% F1) suggests significant room for improvement

## Confidence
- High Confidence: Core finding that instruction-based finetuning improves performance on labor market tasks
- Medium Confidence: Transfer learning claims across labor market tasks
- Low Confidence: Generalizability to real-world labor market applications

## Next Checks
1. Cross-lingual evaluation: Test the finetuned models on non-English labor market tasks to validate whether the improvements transfer across languages present in the ESCO taxonomy.
2. Real-world document testing: Evaluate model performance on actual job postings, CVs, and labor market reports rather than synthetically generated data to assess practical applicability.
3. Ablation studies on task combinations: Systematically test which specific task combinations in multitask learning yield the best transfer effects, and whether certain tasks serve as better "teachers" for others in the labor market domain.