---
ver: rpa2
title: Generative Retrieval via Term Set Generation
arxiv_id: '2305.13859'
source_url: https://arxiv.org/abs/2305.13859
tags:
- document
- identifier
- generation
- retrieval
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generative retrieval
  models, which generate document identifiers (DocIDs) to retrieve relevant documents
  for a given query. The key problem with existing approaches is that they require
  exact generation of the DocID, which can lead to false pruning and poor retrieval
  quality.
---

# Generative Retrieval via Term Set Generation

## Quick Facts
- arXiv ID: 2305.13859
- Source URL: https://arxiv.org/abs/2305.13859
- Authors: 
- Reference count: 13
- Primary result: Term-Set Generation (TSGen) framework outperforms existing generative retrieval methods on NQ320k and MS300k benchmarks

## Executive Summary
This paper addresses the challenge of improving generative retrieval models by proposing Term-Set Generation (TSGen), a novel framework that uses unordered term-sets as document identifiers instead of sequences. The key innovation relaxes the exact generation requirement that plagues existing approaches, eliminating false pruning while maintaining retrieval accuracy. Extensive experiments on Natural Questions and MS MARCO demonstrate that TSGen achieves superior performance in retrieval quality, generalizability, scalability, and efficiency compared to baseline methods.

## Method Summary
The Term-Set Generation framework represents each document as a set of unordered terms that uniquely identify it while summarizing its semantic content. A matching-oriented term selection process chooses terms that distinguish each document from others in the corpus. During inference, a constrained greedy search algorithm generates valid term permutations in any order using a T5 backbone model. The training employs likelihood-adapted Seq2Seq learning with iterative optimization, sampling favorable term permutations based on the model's current likelihood estimates to improve generation quality over time.

## Key Results
- TSGen achieves higher MRR@10 and MRR@100 scores compared to baseline generative retrieval methods
- Significant improvements in Recall@1, Recall@10, and Recall@100 metrics across both NQ320k and MS300k benchmarks
- Demonstrates superior performance in terms of retrieval quality, generalizability, scalability, and efficiency
- Outperforms methods like GENRE, DSI, SEAL, Ultron, and NCI on standard retrieval metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using unordered term-sets as DocIDs relaxes the requirement for exact generation sequence
- Mechanism: By representing documents as sets of terms rather than fixed sequences, any permutation retrieves the same document, eliminating false pruning from incorrect token ordering
- Core assumption: Term selection produces unique identifiers across all documents in the corpus
- Evidence anchors: [abstract] "Instead of sequences, we use a set of terms as DocID, which are automatically selected to concisely summarize the document's semantics and distinguish it from others"

### Mechanism 2
- Claim: Permutation-invariant decoding allows model to generate terms in any order while retrieving correct document
- Mechanism: Constrained greedy search explores term space by selecting terms that maximize generation likelihood while ensuring validity, since any permutation of the term-set retrieves the same document
- Core assumption: Generation model can reliably estimate term likelihoods conditioned on query and previously generated terms
- Evidence anchors: [abstract] "we propose a permutation-invariant decoding algorithm, with which the term set can be generated in any permutation yet will always lead to the corresponding document"

### Mechanism 3
- Claim: Likelihood-adapted Seq2Seq learning enables model to learn optimal term permutations for each query
- Mechanism: Iterative sampling of favorable term-set permutations based on current model likelihood estimates, allowing learning of sequences that are both easy to generate and query-relevant
- Core assumption: Iterative sampling process converges to favorable permutations that improve retrieval quality
- Evidence anchors: [abstract] "we design an iterative optimization procedure to incentivize the model to generate the relevant term set in its favorable permutation"

## Foundational Learning

- Concept: Term importance estimation for document identification
  - Why needed here: To select concise sets of terms that uniquely identify each document while capturing semantic content
  - Quick check question: How does the model estimate which terms are most important for distinguishing a document from others in the corpus?

- Concept: Constrained search algorithms
  - Why needed here: To efficiently explore valid term permutation space while ensuring generated sequences correspond to actual documents
  - Quick check question: What mechanism ensures generated term sequences are valid document identifiers rather than arbitrary combinations?

- Concept: Iterative optimization and convergence analysis
  - Why needed here: To ensure likelihood-adapted learning process actually improves model performance over time
  - Quick check question: How do we verify iterative permutation sampling process is converging to better solutions?

## Architecture Onboarding

- Component map: Term selection module -> Document identifier schema -> Constrained greedy search -> Likelihood-adapted learning -> Seq2Seq backbone
- Critical path: Query → Seq2Seq generation → Constrained search → Document retrieval
- Design tradeoffs:
  - Term granularity (single words vs n-grams) affects identifier uniqueness and model capacity
  - Number of terms per document balances discrimination vs generation complexity
  - Beam size in constrained search trades off efficiency vs retrieval quality
  - Iterative learning steps balance convergence speed vs final performance
- Failure signatures:
  - Low recall indicates insufficient term selection or overly restrictive constraints
  - Poor ranking suggests suboptimal term ordering or inadequate learning
  - High memory usage may indicate inefficient data structures for term storage
- First 3 experiments:
  1. Compare term selection methods (random, title-based, matching-oriented) on retrieval quality
  2. Evaluate different numbers of terms per document for balancing uniqueness vs generation difficulty
  3. Test various beam sizes in constrained search to find efficiency-quality tradeoff point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AutoTSG compare to traditional retrieval methods like BM25 and DPR when scaled to very large corpora?
- Basis in paper: [explicit] The paper mentions DPR leads to highest recall@100 on MS300k but doesn't provide direct comparison on very large corpora
- Why unresolved: Paper only evaluates on NQ320k and MS300k datasets; performance on larger datasets is unknown
- What evidence would resolve it: Evaluating AutoTSG on datasets significantly larger than MS300k and comparing performance to BM25 and DPR

### Open Question 2
- Question: What is the impact of different term selection strategies (e.g., semantic embeddings, TF-IDF) on AutoTSG performance?
- Basis in paper: [explicit] Paper compares random, title, and matching-oriented term selection but doesn't explore other strategies like semantic embeddings or TF-IDF
- Why unresolved: Paper only evaluates three term selection strategies; performance impact of other strategies is unknown
- What evidence would resolve it: Evaluating AutoTSG with different term selection strategies and comparing their performance

### Open Question 3
- Question: How does choice of backbone model (e.g., T5, GPT) affect AutoTSG performance?
- Basis in paper: [explicit] Paper uses T5 as backbone but mentions other models like GPT could be used; impact of different backbone models is not explored
- Why unresolved: Paper only evaluates AutoTSG with T5; performance impact of other models is unknown
- What evidence would resolve it: Evaluating AutoTSG with different backbone models (e.g., GPT, BERT) and comparing their performance to T5

## Limitations

- Term selection process specifics remain underspecified, making it difficult to assess robustness across diverse document types
- Iterative likelihood-adapted learning lacks comprehensive convergence analysis and may get stuck in local optima
- Assumption about reliable term likelihood estimation is not empirically validated, with potential error propagation concerns

## Confidence

- High Confidence: Fundamental mechanism of using unordered term-sets instead of sequences (Mechanism 1) is well-justified theoretically
- Medium Confidence: Permutation-invariant decoding algorithm's effectiveness depends on constrained greedy search quality, which lacks thorough validation
- Medium Confidence: Likelihood-adapted learning approach shows promise but lacks comprehensive convergence behavior analysis

## Next Checks

1. **Term Collision Analysis**: Conduct systematic experiments measuring term-set collision rates across different document similarity levels and corpus sizes to verify uniqueness guarantee
2. **Permutation Robustness Testing**: Evaluate retrieval quality when model generates term-sets in intentionally suboptimal orders to quantify true permutation invariance
3. **Learning Convergence Study**: Track MRR and Recall metrics across iterative learning iterations with multiple random seeds to assess convergence stability