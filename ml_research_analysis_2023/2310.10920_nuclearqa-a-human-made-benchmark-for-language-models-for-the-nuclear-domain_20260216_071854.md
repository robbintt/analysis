---
ver: rpa2
title: 'NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain'
arxiv_id: '2310.10920'
source_url: https://arxiv.org/abs/2310.10920
tags:
- questions
- benchmark
- arxiv
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NuclearQA, a human-made benchmark for evaluating
  language models in the nuclear domain. The authors created a balanced benchmark
  with 100 questions across multiple dimensions like difficulty, question format,
  answer format, and answer type.
---

# NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain

## Quick Facts
- arXiv ID: 2310.10920
- Source URL: https://arxiv.org/abs/2310.10920
- Reference count: 9
- Primary result: Llama 2 achieved 27% correct answers on NuclearQA, demonstrating significant knowledge gaps in current LLMs for nuclear domain questions

## Executive Summary
This paper introduces NuclearQA, a human-made benchmark of 100 questions designed to evaluate language models in the nuclear domain. Unlike existing benchmarks that adapt human tests for models, NuclearQA was specifically crafted by subject matter experts to test deep scientific understanding across physics, chemistry, and material science. The benchmark is balanced across difficulty levels, question formats, answer formats, and answer types. When evaluated on four state-of-the-art language models, even the best model (Llama 2) achieved only 27% correct answers, highlighting significant knowledge gaps in current LLMs for specialized scientific domains.

## Method Summary
The authors created NuclearQA by having subject matter experts design 100 questions specifically to test language models' understanding of nuclear science concepts. Questions were categorized across four dimensions: difficulty (Easy, Medium, Hard), question format (multiple-choice, fill-in-the-blank, open-ended), answer format (single-answer, multi-answer), and answer type (Numerical, Scientific, Numerical+Scientific, General). Four language models (Llama 2, Galactica, FlanT5, UnifiedQA) were evaluated using standard prompting with increased response length for open questions. Responses were assessed through a human-in-the-loop evaluation system using a 5-point scale, with SMEs evaluating answers without prior knowledge of model performance.

## Key Results
- Llama 2 achieved the highest score with 27% correct answers, while other models scored significantly lower
- Even the best-performing models showed substantial knowledge gaps in nuclear domain understanding
- Performance varied significantly across question dimensions, with some formats and difficulty levels proving more challenging than others
- The benchmark successfully demonstrated that current evaluation metrics like exact match and F1 are inadequate for scientific question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's expert-crafted nature ensures higher-quality, domain-specific questions that truly test deep scientific understanding.
- Mechanism: Domain experts design questions specifically to probe understanding rather than surface-level pattern matching, avoiding reliance on automatically extracted questions from existing texts.
- Core assumption: Expert-crafted questions are more effective at testing true comprehension than questions derived from existing human tests or automated extraction.
- Evidence anchors: The paper states the benchmark was "crafted by subject matter experts specifically to assure that these questions are well suited to judge a language model's ability to solve nuclear-related questions."

### Mechanism 2
- Claim: The multi-dimensional categorization creates a balanced benchmark that can comprehensively assess model capabilities across different cognitive demands.
- Mechanism: By structuring questions across multiple axes, the benchmark avoids overfitting to a single type of question and ensures a more holistic evaluation of model understanding.
- Core assumption: A balanced distribution across these dimensions is necessary to properly test a model's scientific understanding in the nuclear domain.
- Evidence anchors: The paper describes how questions were categorized by difficulty, format, answer format, and answer type to create a comprehensive evaluation framework.

### Mechanism 3
- Claim: The human-in-the-loop evaluation system is necessary because traditional metrics fail to accurately capture the nuanced correctness of answers in the scientific domain.
- Mechanism: Expert evaluators use a multi-point scale (5 = Correct, 4 = Partially Correct, 3 = Incorrect but related, 2 = Unrelated but in-domain, 1 = Out-domain/nonsensical) to assess responses, addressing limitations of exact match and F1 metrics.
- Core assumption: Traditional evaluation metrics like exact match and F1 are inadequate for scientific question answering because they cannot distinguish between subtly correct and incorrect answers.
- Evidence anchors: The paper presents a 5-point scoring system and states that "traditional methods of evaluation are not suited to judge the success of models on our questions."

## Foundational Learning

- Concept: Scientific domain knowledge requirements
  - Why needed here: The benchmark tests understanding across physics, chemistry, and material science within the nuclear domain, requiring evaluators to have deep subject matter expertise
  - Quick check question: Can you explain the difference between a neutron and a proton, and why this distinction matters in nuclear reactions?

- Concept: Question design principles for benchmarking
  - Why needed here: Creating effective benchmark questions requires understanding how to test different cognitive abilities (recall, reasoning, application) through question structure
  - Quick check question: What makes a good multiple-choice question different from an open-ended question in terms of what cognitive abilities they test?

- Concept: Evaluation metric design
  - Why needed here: The human-in-the-loop evaluation system requires understanding how to create meaningful scales that capture partial correctness and domain relevance
  - Quick check question: How would you design a scoring system that can distinguish between a completely correct answer, a partially correct answer, and an answer that's related but incorrect?

## Architecture Onboarding

- Component map:
  - SME question design → Review and validation → Categorization by dimensions
  - Benchmark dataset: 100 questions with metadata (difficulty, format, answer format, answer type)
  - Model evaluation pipeline: Model inference → Human-in-the-loop evaluation using 5-point scale → Score aggregation
  - Analysis component: Error analysis → Performance reporting → Failure pattern identification

- Critical path:
  1. SME creates and validates questions
  2. Questions are categorized across all dimensions
  3. Selected models generate responses
  4. SME evaluates responses using the 5-point scale
  5. Scores are aggregated and analyzed

- Design tradeoffs:
  - Quality vs. Quantity: Expert-crafted questions ensure quality but limit the total number (100 vs. thousands in other benchmarks)
  - Automation vs. Accuracy: Human-in-the-loop evaluation is accurate but time-consuming compared to automated metrics
  - Domain Specificity vs. Generalizability: Nuclear-specific benchmark is highly relevant but less applicable to other domains

- Failure signatures:
  - If models score uniformly high across all dimensions, the benchmark may be too easy
  - If models score uniformly low, the benchmark may be too difficult or the questions may be poorly designed
  - If one dimension shows consistently poor performance, it may indicate a specific weakness in model capabilities
  - If human evaluators show low inter-rater reliability, the evaluation scale may need refinement

- First 3 experiments:
  1. Run a small pilot with 10 questions across all dimensions and 2 models to test evaluation scale reliability and identify any immediate issues
  2. Evaluate model performance on single-answer vs. multi-answer questions separately to understand which types are more challenging
  3. Test the correlation between difficulty ratings and model performance to validate the difficulty categorization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific factors that contribute to the performance gap between models on NuclearQA, and how can these be addressed?
- Basis in paper: The paper discusses the performance of different models on NuclearQA and notes that even the best models perform poorly, indicating a knowledge gap.
- Why unresolved: The paper does not delve into the specific factors contributing to the performance gap, such as model architecture, training data, or evaluation methods.
- What evidence would resolve it: Further analysis of model architectures, training data, and evaluation methods to identify the specific factors contributing to the performance gap.

### Open Question 2
- Question: How can the human-in-the-loop evaluation system be improved to reduce subjectivity and increase efficiency?
- Basis in paper: The paper discusses the limitations of traditional evaluation metrics and proposes a human-in-the-loop evaluation system, but acknowledges its limitations.
- Why unresolved: The paper does not provide a detailed discussion on how to improve the human-in-the-loop evaluation system to reduce subjectivity and increase efficiency.
- What evidence would resolve it: Development and testing of new evaluation methods that incorporate human judgment while reducing subjectivity and increasing efficiency.

### Open Question 3
- Question: What are the key challenges in scaling up the creation of human-made benchmarks for other scientific domains, and how can these be overcome?
- Basis in paper: The paper discusses the limitations of the current approach, which requires an extensive time commitment from subject matter experts, and suggests the need for automated steps to speed up the process.
- Why unresolved: The paper does not provide a detailed discussion on the specific challenges in scaling up the creation of human-made benchmarks and potential solutions.
- What evidence would resolve it: Development and testing of automated methods for creating high-quality questions, and evaluation of their effectiveness in reducing the time and effort required by subject matter experts.

## Limitations

- Small benchmark size (100 questions) may not provide comprehensive coverage despite expert curation
- Human-in-the-loop evaluation introduces potential subjectivity and limits reproducibility
- Nuclear-specific focus reduces applicability to other scientific domains
- Only tested four models, results may not generalize to newer architectures
- Resource-intensive evaluation process may not scale to larger benchmarks

## Confidence

**High Confidence**: The benchmark creation methodology and expert-crafted question design are well-documented and methodologically sound. The finding that current LLMs perform poorly on nuclear domain questions is supported by consistent results across four different models.

**Medium Confidence**: The 5-point human evaluation scale provides reasonable granularity for assessing answers, though inter-rater reliability is not reported. The categorization of questions across difficulty, format, answer format, and answer type appears balanced based on the authors' analysis.

**Low Confidence**: The claim that NuclearQA is "the first-of-its-kind benchmark" specifically designed for nuclear domain evaluation, as there may be other domain-specific benchmarks not referenced in the paper.

## Next Checks

1. **Inter-rater reliability assessment**: Have multiple SMEs independently evaluate the same set of model responses to calculate Cohen's kappa or similar metrics, ensuring the human evaluation system is consistent and reliable.

2. **Benchmark expansion and difficulty calibration**: Add 50 additional questions to test whether the current difficulty ratings hold up and whether the expanded benchmark maintains balance across all dimensions while providing more robust statistical power.

3. **Automated metric correlation study**: Develop and test automated evaluation metrics (e.g., semantic similarity, domain-specific embeddings) against the human-in-the-loop results to identify which automated approaches can approximate expert judgment, enabling faster scaling of evaluation.