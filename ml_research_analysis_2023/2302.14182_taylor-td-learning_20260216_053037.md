---
ver: rpa2
title: Taylor TD-learning
arxiv_id: '2302.14182'
source_url: https://arxiv.org/abs/2302.14182
tags:
- taylor
- updates
- policy
- learning
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high variance problem in standard TD-learning
  updates by introducing Taylor TD, a model-based RL framework that uses first-order
  Taylor expansions to analytically integrate over stochasticity in action choices
  and some state distributions. The key innovation is combining Monte Carlo estimates
  with analytic integration through Taylor expansions, resulting in lower variance
  updates compared to standard TD-learning.
---

# Taylor TD-learning

## Quick Facts
- arXiv ID: 2302.14182
- Source URL: https://arxiv.org/abs/2302.14182
- Reference count: 20
- This paper introduces Taylor TD, a model-based RL framework that reduces variance in TD updates by analytically integrating over action and state stochasticity using Taylor expansions.

## Executive Summary
This paper addresses the high variance problem in standard TD-learning updates by introducing Taylor TD, a model-based RL framework that uses first-order Taylor expansions to analytically integrate over stochasticity in action choices and some state distributions. The key innovation is combining Monte Carlo estimates with analytic integration through Taylor expansions, resulting in lower variance updates compared to standard TD-learning. The authors prove that Taylor TD maintains the same stable learning guarantees as standard TD-learning under linear function approximation. When combined with TD3 into TaTD3, the method performs as well as or better than state-of-the-art model-free and model-based baselines on benchmark continuous control tasks. The benefits are particularly pronounced in high-dimensional state-action spaces, where Taylor TD shows significant advantages over Monte Carlo sampling approaches.

## Method Summary
Taylor TD reduces variance in TD updates by using first-order Taylor expansions to analytically integrate over action and state distributions rather than relying solely on Monte Carlo sampling. The method expands the TD error around the mean action and state, then integrates out the stochasticity analytically, yielding additional gradient-matching terms that align the gradients of the critic with the gradients of the TD target. This is implemented in TaTD3 by adding cosine similarity terms to the standard TD3 critic loss. The framework uses an ensemble of environment models to generate imaginary transitions, with the critic updates incorporating both the standard TD loss and Taylor expansion terms weighted by hyperparameters λa and λs.

## Key Results
- Taylor TD reduces variance in TD updates compared to standard TD-learning by analytically integrating over action and state distributions
- Maintains the same stable learning guarantees as standard TD-learning under linear function approximation
- When combined with TD3 into TaTD3, performs as well as or better than state-of-the-art model-free and model-based baselines on benchmark continuous control tasks
- Shows significant advantages over Monte Carlo sampling approaches particularly in high-dimensional state-action spaces like Humanoid-v2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taylor TD reduces variance in TD updates by analytically integrating over action stochasticity using first-order Taylor expansions
- Mechanism: Instead of sampling actions from a policy and computing high-variance Monte Carlo estimates, Taylor TD computes an expected TD update by expanding around the mean action and integrating out the stochasticity analytically. This is achieved through the Taylor expansion δθ(st, at) + ξT a ∇aδθ(st, at) multiplied by ∇θQθ(st, at) + ξT a ∇2 a,θQθ(st, at), where the expectation over ξa eliminates the linear terms and leaves only the constant term plus a gradient-matching term involving the action gradient of the TD error.
- Core assumption: The first-order Taylor expansion provides a sufficiently accurate approximation of the expected TD update when the action distribution has small covariance (λa is small enough)
- Evidence anchors:
  - [abstract] "Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice"
  - [section 4.1] Detailed derivation showing how the Taylor expansion eliminates the linear terms in ξa and provides a lower-variance update
  - [corpus] Weak - the related papers mention Taylor expansions but not in the specific context of variance reduction in TD learning
- Break condition: If the action covariance is too large relative to the curvature of the TD error function, the first-order approximation becomes poor and the variance reduction benefits disappear

### Mechanism 2
- Claim: Taylor TD maintains the same stable learning guarantees as standard TD-learning under linear function approximation
- Mechanism: The additional gradient terms introduced by Taylor TD can be shown to preserve the positive definiteness of the matrix A that governs convergence stability. Specifically, the Taylor term contributes λa times a matrix Ã that is also positive definite, ensuring that the combined matrix (A + λaÃ) remains positive definite for appropriate λa and η values
- Core assumption: The environment dynamics and reward function are sufficiently smooth to allow meaningful gradients, and λa is chosen appropriately
- Evidence anchors:
  - [abstract] "we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption"
  - [section A.1] Formal proof showing that the additional Taylor term does not affect the positive definiteness of the convergence matrix
  - [corpus] Weak - no related papers discuss stability guarantees for Taylor-based RL methods
- Break condition: If λa is chosen too large, the additional gradient terms can destabilize learning by overwhelming the standard TD update

### Mechanism 3
- Claim: Taylor TD can simultaneously reduce variance from both action and state stochasticity through state expansion
- Mechanism: In addition to the action expansion, Taylor TD can expand the expected TD update over a Gaussian distribution of states. This involves a similar first-order Taylor expansion around the mean state, yielding an additional term λs∇2 θ,sQθ(st, at)∇sδθ(st, at) that tries to align the state gradient of the critic with the state gradient of the TD target
- Core assumption: The state distribution is approximately Gaussian with small covariance, and the gradients are well-behaved near the mean state
- Evidence anchors:
  - [abstract] "This expansion allows to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution"
  - [section 4.2] Derivation of the state expansion update showing how it reduces variance from state stochasticity
  - [corpus] Weak - related papers don't discuss state-based Taylor expansions in RL contexts
- Break condition: If λs is too large, the state expansion terms can introduce noise rather than reduce variance, particularly when the state distribution is non-Gaussian or has large covariance

## Foundational Learning

- Concept: Temporal Difference (TD) learning and the bias-variance tradeoff
  - Why needed here: Understanding that TD learning trades off bias (from bootstrapping) against variance (from sampling) is crucial to appreciating why Taylor TD's variance reduction is valuable while maintaining the same bias properties as standard TD
  - Quick check question: Why does standard TD learning have high variance, and how does this affect sample efficiency?

- Concept: First-order Taylor series expansions and their approximation error
  - Why needed here: The entire Taylor TD approach relies on using first-order Taylor expansions to approximate expected updates. Understanding when these approximations are accurate (small perturbations, smooth functions) is essential for proper implementation
  - Quick check question: Under what conditions does a first-order Taylor expansion provide a good approximation of a function?

- Concept: Function approximation in reinforcement learning (especially linear vs nonlinear)
  - Why needed here: Taylor TD's theoretical guarantees rely on linear function approximation, while practical implementations use nonlinear function approximators. Understanding this distinction is crucial for interpreting the results and limitations
  - Quick check question: What is the key difference between the theoretical analysis (linear) and practical implementation (nonlinear) of Taylor TD?

## Architecture Onboarding

- Component map:
  - Environment model ensemble (8 Gaussian models) -> Reward model (NN) -> Critic (2 Q-networks with Taylor TD updates) -> Actor (policy network) -> Memory buffer -> Update scheduler

- Critical path:
  1. Collect real transition (s, a, s', r) from environment
  2. Store in replay buffer
  3. Sample batch of real states from buffer
  4. Generate imaginary transitions using learned model
  5. Compute Taylor TD updates for both critics
  6. Update actor on real states
  7. Update environment and reward models

- Design tradeoffs:
  - Model ensemble size (8) vs computational cost: Larger ensembles reduce model bias but increase computation
  - λa and λs values: Balance between variance reduction and approximation accuracy
  - Number of imaginary transitions per step: More transitions provide better variance reduction but increase computation
  - Taylor expansion order: First-order is computationally tractable; higher orders might be more accurate but expensive

- Failure signatures:
  - If learning is unstable: λa or λs may be too large, or the model is overfitting
  - If performance doesn't improve over standard TD: The Taylor expansion approximation error may be too large (likely due to large λa/λs or highly nonlinear value functions)
  - If model learning fails: Insufficient real data or poor model architecture
  - If variance isn't reduced: Action/state distributions have too much covariance, or the function is too nonlinear

- First 3 experiments:
  1. Implement the basic Taylor TD update for a simple critic (no actor) on a low-dimensional problem like Pendulum-v1, comparing variance and performance against standard TD
  2. Add the model-based component with a single transition model (not ensemble) to verify the full TaTD3 pipeline works before adding complexity
  3. Gradually increase problem complexity (HalfCheetah-v2) while monitoring the cosine similarity terms to ensure they're providing meaningful gradients rather than noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Taylor TD's performance scale with increasing dimensionality of state-action spaces in continuous control tasks?
- Basis in paper: [explicit] The authors note that the largest benefits of Taylor TD over MC sampling approaches are seen in high-dimensional state-action spaces, particularly in Humanoid-v2, which has the highest dimensional state-action space by a large margin.
- Why unresolved: The paper only provides empirical evidence on a limited set of benchmark tasks. A systematic study across a wider range of environments with varying dimensionalities of state-action spaces would be needed to fully understand the scaling behavior.
- What evidence would resolve it: Experimental results showing Taylor TD's performance relative to MC sampling across a diverse set of environments with controlled dimensionalities of state-action spaces.

### Open Question 2
- Question: What is the theoretical relationship between the variance reduction achieved by Taylor TD and the choice of action and state covariance parameters (λa and λs)?
- Basis in paper: [inferred] The paper introduces Taylor TD using first-order Taylor expansions and claims variance reduction, but does not provide a rigorous theoretical analysis of how the choice of λa and λs affects the variance reduction.
- Why unresolved: While the paper shows empirical evidence of variance reduction, a theoretical analysis of the relationship between the covariance parameters and variance reduction is missing.
- What evidence would resolve it: A theoretical analysis or empirical study showing how different values of λa and λs affect the variance reduction achieved by Taylor TD.

### Open Question 3
- Question: How does Taylor TD compare to other methods for reducing TD-learning variance, such as gradient-based methods or distributional approaches?
- Basis in paper: [inferred] The paper focuses on comparing Taylor TD to standard TD-learning and MC sampling approaches, but does not directly compare it to other variance reduction techniques.
- Why unresolved: The paper does not provide a comprehensive comparison with other state-of-the-art methods for reducing TD-learning variance.
- What evidence would resolve it: Empirical results comparing Taylor TD's performance to other variance reduction methods on the same benchmark tasks.

## Limitations

- The Taylor expansion approximation fundamentally relies on small action and state covariances, which may not hold in practice for high-entropy policies or highly stochastic environments
- The theoretical guarantees are proven only for linear function approximation, while the empirical results use nonlinear neural networks where stability is not guaranteed
- The method adds computational overhead through model learning and Taylor expansion computations that may not be justified in all settings

## Confidence

- **High confidence** in the variance reduction mechanism when action covariances are small and the value function is smooth
- **Medium confidence** in the theoretical stability guarantees due to the gap between linear and nonlinear function approximation settings
- **Medium confidence** in empirical performance claims based on benchmark results, though ablation studies show sensitivity to hyperparameter choices

## Next Checks

1. **Approximation error analysis**: Systematically vary λa and λs across orders of magnitude to identify the regime where Taylor expansions provide net benefit versus when they introduce harmful bias

2. **Non-Gaussian stochasticity test**: Evaluate performance when action distributions deviate from Gaussian assumptions (e.g., using stochastic policies with entropy regularization) to stress-test the Taylor expansion's validity

3. **Transfer stability test**: Train Taylor TD on one task (e.g., HalfCheetah) and evaluate transfer to a similar task (e.g., Walker2d) to assess whether the variance reduction benefits generalize beyond the training distribution