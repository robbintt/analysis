---
ver: rpa2
title: 'Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned
  Generative Transformers'
arxiv_id: '2306.04504'
source_url: https://arxiv.org/abs/2306.04504
tags:
- chatgpt
- biomedical
- question
- cancer
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates ChatGPT on biomedical tasks such as relation
  extraction, document classification, question answering, and summarization. It compares
  zero-shot ChatGPT with fine-tuned generative transformers like BioGPT and BioBART.
---

# Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers

## Quick Facts
- arXiv ID: 2306.04504
- Source URL: https://arxiv.org/abs/2306.04504
- Reference count: 24
- Primary result: ChatGPT outperforms fine-tuned BioGPT on small biomedical datasets in zero-shot settings, but performance degrades on larger datasets

## Executive Summary
This paper evaluates ChatGPT's performance on biomedical tasks including relation extraction, document classification, question answering, and summarization, comparing zero-shot ChatGPT against fine-tuned models like BioGPT and BioBART. The study finds that ChatGPT excels on smaller datasets (500-664 instances) where it even surpasses state-of-the-art fine-tuned models, demonstrating strong generalization from large-scale pre-training. However, on larger datasets (12K instances), ChatGPT's performance drops below fine-tuned baselines, highlighting the limits of zero-shot learning. The research also reveals that ChatGPT's performance is highly sensitive to prompt variations, with more descriptive prompts yielding better precision scores.

## Method Summary
The study constructs task-specific prompts for various biomedical datasets including BC5CDR, KD-DTI, DDI, HoC, PubMedQA, iCliniq, HealthCareMagic, MeQSum, MEDIQA-QS, MEDIQA-MAS, and MEDIQA-ANS. ChatGPT (gpt-3.5-turbo) generates responses based on these prompts, which are then evaluated against gold labels using metrics like Precision, Recall, F1, accuracy, ROUGE scores, and BERTScore. The evaluation compares zero-shot ChatGPT performance with fine-tuned BioGPT and BioBART models across relation extraction, document classification, question answering, and summarization tasks.

## Key Results
- ChatGPT outperforms fine-tuned BioGPT on small datasets (BC5CDR: F1 0.63 vs 0.57; KD-DTI: F1 0.77 vs 0.72)
- Performance degrades on larger datasets (DD corpus: F1 0.29 vs BioGPT's 0.56)
- ChatGPT shows significant sensitivity to prompt variations, with descriptive prompts improving precision
- In summarization tasks, ChatGPT achieves ROUGE-1 scores of 0.51 on iCliniq dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot ChatGPT outperforms fine-tuned BioGPT on datasets with small training sizes.
- Mechanism: ChatGPT's pre-training on large text corpora gives it generalized knowledge that allows it to perform well even without task-specific fine-tuning, especially when task-specific training data is scarce.
- Core assumption: The generalization power from large-scale pre-training compensates for the lack of task-specific data.
- Evidence anchors:
  - [abstract]: "Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART."
  - [section]: "This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain."
- Break condition: When training data size is large enough that task-specific fine-tuning provides a clear advantage.

### Mechanism 2
- Claim: ChatGPT's performance degrades significantly on larger datasets compared to fine-tuned models.
- Mechanism: While ChatGPT has strong generalization, it lacks the task-specific optimization that fine-tuning provides, which becomes more critical as the dataset size increases.
- Core assumption: Task-specific fine-tuning on large datasets provides performance gains that zero-shot models cannot match.
- Evidence anchors:
  - [abstract]: "However, in larger datasets, ChatGPT's performance is lower."
  - [section]: "While in this paper, we mostly evaluate ChatGPT on tasks that require it to generate responses by only analyzing the input text..."
- Break condition: When the dataset is small enough that zero-shot generalization is sufficient.

### Mechanism 3
- Claim: ChatGPT is sensitive to prompt variations, with more descriptive prompts yielding better performance.
- Mechanism: The model's performance depends heavily on how instructions are phrased, with detailed prompts providing clearer task guidance.
- Core assumption: ChatGPT's instruction-following capability is prompt-dependent.
- Evidence anchors:
  - [section]: "We also observe that ChatGPT is sensitive to prompts, as variations in prompts led to a noticeable difference in results."
  - [section]: "We observe that more descriptive prompts may help ChatGPT to obtain better Precision scores."
- Break condition: When prompts are sufficiently clear and detailed to eliminate ambiguity.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates ChatGPT's ability to perform tasks without any task-specific fine-tuning, which is the definition of zero-shot learning.
  - Quick check question: What is the key difference between zero-shot and few-shot learning?

- Concept: Fine-tuning
  - Why needed here: The paper compares zero-shot ChatGPT with models that have been fine-tuned on biomedical datasets, making understanding fine-tuning essential.
  - Quick check question: What is the primary purpose of fine-tuning a pre-trained language model?

- Concept: Prompt engineering
  - Why needed here: The paper shows that ChatGPT's performance varies significantly with different prompts, highlighting the importance of prompt design.
  - Quick check question: How can prompt variations affect the performance of large language models?

## Architecture Onboarding

- Component map: ChatGPT (zero-shot evaluation) → BioGPT/BioBART (fine-tuned baselines) → Benchmark datasets → Performance metrics
- Critical path: Task instruction → Prompt construction → ChatGPT generation → Manual/automatic evaluation → Performance comparison
- Design tradeoffs: Zero-shot convenience vs. fine-tuned accuracy; prompt sensitivity vs. robustness
- Failure signatures: Low precision in relation extraction; sensitivity to prompt variations; degradation on large datasets
- First 3 experiments:
  1. Evaluate ChatGPT on a small biomedical dataset (e.g., BC5CDR) and compare with BioGPT
  2. Test ChatGPT with varied prompts on the same dataset to measure prompt sensitivity
  3. Evaluate ChatGPT on a large biomedical dataset and compare performance drop with fine-tuned models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT compare to fine-tuned models on biomedical tasks with medium-sized training datasets (between 1K and 10K instances)?
- Basis in paper: [inferred] The paper shows that ChatGPT performs better than fine-tuned models on datasets with smaller training sizes (e.g., 500-664 instances) but worse on larger datasets (e.g., 12K instances). However, the performance on medium-sized datasets is not explicitly evaluated.
- Why unresolved: The paper only provides results for very small and very large training datasets, leaving a gap in understanding ChatGPT's performance on medium-sized datasets.
- What evidence would resolve it: Additional experiments evaluating ChatGPT's performance on biomedical tasks with medium-sized training datasets (e.g., 1K-10K instances) would provide insights into its effectiveness in these scenarios.

### Open Question 2
- Question: What is the impact of prompt engineering on ChatGPT's performance in biomedical tasks, and are there optimal prompt strategies for different types of tasks?
- Basis in paper: [explicit] The paper mentions that ChatGPT's performance is sensitive to prompt variations, as demonstrated in the relation extraction and document classification tasks.
- Why unresolved: The paper only provides a few examples of prompt variations and their effects. A comprehensive study of different prompt strategies and their impact on performance across various biomedical tasks is needed.
- What evidence would resolve it: Systematic experimentation with different prompt engineering techniques (e.g., prompt length, specificity, task framing) across multiple biomedical tasks would reveal optimal prompt strategies for each task type.

### Open Question 3
- Question: How does ChatGPT's performance on biomedical tasks change over time as the model is updated and retrained by OpenAI?
- Basis in paper: [explicit] The paper mentions that ChatGPT's results may not be reproducible due to the model generating different responses for the same input prompt, and that a new version (GPT-4) has been released.
- Why unresolved: The paper only evaluates ChatGPT's performance at a single point in time, without considering potential changes in performance due to model updates.
- What evidence would resolve it: Repeated evaluations of ChatGPT's performance on the same biomedical tasks over time, as the model is updated by OpenAI, would reveal any changes in its effectiveness and potential improvements or degradations in performance.

## Limitations
- Evaluation relies heavily on manual assessment for certain tasks, introducing potential subjectivity and reproducibility challenges
- Comparison lacks detailed information about fine-tuned models' training procedures and hyperparameters
- Study only evaluates ChatGPT at a single point in time without considering model updates

## Confidence
- Performance on small datasets: Medium - supported by direct experimental results but mechanisms remain speculative
- Performance degradation on large datasets: Medium - results show degradation but don't establish linear relationship or threshold
- Prompt sensitivity: Medium - well-documented through controlled experiments but lacks systematic exploration

## Next Checks
1. Conduct a systematic ablation study varying dataset sizes to identify the exact threshold where fine-tuning overtakes zero-shot performance, testing intermediate dataset sizes between the smallest and largest used in this study.

2. Implement a blind multi-rater evaluation protocol for the manual assessment tasks to quantify inter-annotator agreement and establish confidence intervals for the reported metrics.

3. Test a prompt optimization framework (such as automated prompt tuning or few-shot exemplars) to determine if prompt engineering can close the performance gap on larger datasets, providing evidence for whether prompt sensitivity is a fundamental limitation or a solvable engineering challenge.