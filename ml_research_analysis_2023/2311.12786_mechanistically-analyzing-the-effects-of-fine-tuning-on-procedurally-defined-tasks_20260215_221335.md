---
ver: rpa2
title: Mechanistically analyzing the effects of fine-tuning on procedurally defined
  tasks
arxiv_id: '2311.12786'
source_url: https://arxiv.org/abs/2311.12786
tags:
- fine-tuning
- task
- capability
- learning
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning large pre-trained models minimally alters their underlying
  capabilities, instead learning localized "wrappers" that transform existing abilities.
  This was shown using synthetic setups with Tracr and PCFGs, where mechanistic interpretability
  tools (pruning, probing, attention visualization) revealed that fine-tuned models
  continue to produce outputs aligned with pretraining capabilities despite behavioral
  changes.
---

# Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks

## Quick Facts
- **arXiv ID:** 2311.12786
- **Source URL:** https://arxiv.org/abs/2311.12786
- **Reference count:** 40
- **Primary result:** Fine-tuning large pre-trained models minimally alters their underlying capabilities, instead learning localized "wrappers" that transform existing abilities.

## Executive Summary
This paper investigates how fine-tuning affects the underlying capabilities of large pre-trained models by using synthetic tasks and mechanistic interpretability tools. The authors demonstrate that fine-tuning typically adds minimal "wrapper" transformations on top of existing capabilities rather than deleting or fundamentally altering them. Through controlled experiments with Tracr and PCFG synthetic setups, they show that capabilities can be "revived" through reverse fine-tuning, and that the relevance of pretraining capabilities to the downstream task determines whether wrappers are learned or capabilities are amplified.

## Method Summary
The authors use synthetic tasks (Tracr and PCFG) to precisely control and measure model capabilities during fine-tuning. They pretrain models on specific capabilities, then fine-tune them on downstream tasks while varying learning rates and spurious correlation levels. Mechanistic interpretability tools including pruning, probing, and attention visualization are applied to analyze whether capabilities persist beneath behavioral changes. Reverse fine-tuning experiments test whether pretraining capabilities can be efficiently recovered by further training on the original pretraining data.

## Key Results
- Fine-tuning adds minimal "wrapper" transformations rather than deleting underlying capabilities
- Reverse fine-tuning efficiently recovers pretraining capabilities, demonstrating they persist beneath wrappers
- The relevance of pretraining capabilities to downstream tasks determines whether wrappers are learned or capabilities are amplified

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning does not delete or replace underlying model capabilities, but instead adds localized "wrapper" transformations that modulate their use.
- **Mechanism:** When a pretrained model is fine-tuned on a downstream task, it learns to apply a minimal transformation (the wrapper) on top of existing capabilities rather than fundamentally altering or removing them. This wrapper acts as a gating mechanism that can inhibit or redirect the use of pretraining capabilities based on the new task's requirements.
- **Core assumption:** The pretraining dataset covers a superset of capabilities that may be relevant to downstream tasks, and fine-tuning primarily identifies and repurposes existing capabilities rather than creating entirely new ones.
- **Evidence anchors:**
  - [abstract] "fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities"
  - [section 3] "Let Readl(M (.)) denote the action where a linear layer is trained on intermediate outputs at layer l of model M using DPT"
  - [corpus] Weak evidence - only one related paper discusses capability preservation but not mechanistically.
- **Break condition:** If fine-tuning substantially alters model weights in non-localized ways that prevent revival of pretraining capabilities through reverse fine-tuning.

### Mechanism 2
- **Claim:** Pretraining capabilities can be "revived" through reverse fine-tuning, demonstrating they persist beneath the wrapper layer.
- **Mechanism:** After fine-tuning, if the model is further fine-tuned on the original pretraining distribution, the pretraining capabilities rapidly re-emerge with minimal training steps. This shows the capabilities were never deleted but merely suppressed by the wrapper.
- **Core assumption:** The pretraining data distribution contains sufficient signal to re-trigger the original capability patterns in the model's weights.
- **Evidence anchors:**
  - [abstract] "further fine-tuning on a task where such hidden capabilities are relevant leads to sample-efficient 'revival' of the capability"
  - [section 4.1] "Reverse Fine-Tuning: In scenarios where upon fine-tuning a model behaviorally seems to not possess a capability, we find that further fine-tuning the model on a subset of pretraining data leads to a sample-efficient 'revival' of the capability"
  - [corpus] Weak evidence - no corpus papers directly discuss capability revival through reverse fine-tuning.
- **Break condition:** If reverse fine-tuning requires as many or more steps than original pretraining to recover capabilities.

### Mechanism 3
- **Claim:** The relevance of pretraining capabilities to the downstream task determines whether wrappers are learned or capabilities are amplified.
- **Mechanism:** When pretraining capabilities are strongly relevant to the downstream task (high sampling probability during pretraining), the model learns to amplify their use rather than learning wrappers. When capabilities are weakly relevant, wrappers are learned to exploit spurious correlations.
- **Core assumption:** The sampling probability of task operands during pretraining determines the strength of capability relevance to downstream tasks.
- **Evidence anchors:**
  - [section 4] "By making the sampling probability of OFT tokens high during pretraining, we can make the model preemptively performant on the downstream task"
  - [section 5] "When the pretraining prior has low probability of sampling the token OFT, we see the fine-tuned model performs well only when the spurious correlation is present"
  - [corpus] No direct evidence - corpus papers don't discuss capability relevance gradients.
- **Break condition:** If models learn wrappers regardless of pretraining sampling probability distributions.

## Foundational Learning

- **Concept:** Mechanistic interpretability tools (pruning, probing, attention visualization)
  - Why needed here: These tools provide different perspectives on whether capabilities persist beneath wrappers by examining model behavior at intermediate layers, removing specific components, and visualizing attention patterns.
  - Quick check question: Can you explain how pruning might reveal a wrapper if removing a small number of neurons suddenly restores pretraining capability?

- **Concept:** PCFG and Tracr synthetic setups
  - Why needed here: These controlled environments allow precise definition and measurement of capabilities, making it possible to track whether fine-tuning alters them or merely adds wrappers.
  - Quick check question: Why are synthetic setups like PCFG and Tracr better than natural language for studying capability preservation than natural language datasets?

- **Concept:** Reverse fine-tuning methodology
  - Why needed here: This technique tests whether capabilities were truly deleted or just suppressed by measuring how quickly they can be recovered through additional training on pretraining data.
  - Quick check question: If reverse fine-tuning takes as many steps as original pretraining to recover a capability, what does that suggest about the fine-tuning mechanism?

## Architecture Onboarding

- **Component map:** Tracr/PCFG pretraining -> Fine-tuning with varying learning rates -> Mechanistic interpretability (pruning, probing, attention) -> Behavioral evaluation -> Reverse fine-tuning -> Capability revival measurement
- **Critical path:** Pretraining → Fine-tuning with different learning rates → Evaluate behavioral changes → Apply interpretability tools → Reverse fine-tuning → Compare sample efficiency.
- **Design tradeoffs:** Synthetic setups offer precise control but may not capture all aspects of natural language model behavior; different learning rates reveal different wrapper behaviors but may affect convergence stability.
- **Failure signatures:** If behavioral changes don't align with mechanistic interpretability results, or if reverse fine-tuning doesn't efficiently recover capabilities, the wrapper hypothesis may be incomplete.
- **First 3 experiments:**
  1. Run fine-tuning with low learning rate on a model with weakly relevant pretraining capability and measure behavioral changes.
  2. Apply pruning to identify neurons that, when removed, restore pretraining accuracy while reducing fine-tuning accuracy.
  3. Perform reverse fine-tuning on the fine-tuned model and measure how quickly pretraining capability is recovered.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning affect model capabilities in larger models and more complex tasks beyond the synthetic PCFG and Tracr setups?
- Basis in paper: [explicit] The authors note that their analysis focuses on synthetic, controlled settings and suggest future work understanding fine-tuning in more realistic settings with larger models.
- Why unresolved: The paper's findings are based on small models (3M-91M parameters) trained on simple procedural tasks. Scaling to real-world models like LLMs with billions of parameters could reveal different phenomena.
- What evidence would resolve it: Experiments applying the same mechanistic interpretability tools (pruning, probing, reverse fine-tuning) to larger models like LLaMA, GPT-2, or BERT on real-world datasets.

### Open Question 2
- Question: What specific architectural features or training conditions lead to more substantial capability changes during fine-tuning rather than wrapper learning?
- Basis in paper: [inferred] The authors show wrapper learning is the typical outcome but suggest this might not always be the case, particularly when weakly relevant capabilities are involved.
- Why unresolved: The paper demonstrates wrapper learning but doesn't fully characterize when and why this pattern might break down or when more substantial capability changes occur.
- What evidence would resolve it: Systematic experiments varying model architecture (attention mechanisms, depth, width), pretraining duration, and fine-tuning protocols to identify conditions that produce non-wrapper capability changes.

### Open Question 3
- Question: Can the wrapper phenomenon be leveraged for more efficient fine-tuning methods that explicitly target capability modulation rather than full parameter updates?
- Basis in paper: [explicit] The authors identify wrappers as localized transformations and suggest this points toward future work developing methods beyond standard fine-tuning.
- Why unresolved: While the paper characterizes wrappers mechanistically, it doesn't explore practical applications of this knowledge for designing new adaptation methods.
- What evidence would resolve it: Development and evaluation of fine-tuning algorithms that explicitly identify and modify wrapper components rather than performing gradient updates across all parameters.

## Limitations

- The paper relies heavily on synthetic datasets which may not fully capture natural language complexity
- Some capabilities may be lost during fine-tuning, though the focus remains on preserved capabilities
- Sample sizes for certain experiments (particularly TinyStories validation) are limited

## Confidence

- **High confidence**: The wrapper hypothesis is supported by multiple experimental approaches (mechanistic interpretability, reverse fine-tuning, varying learning rates). The synthetic setups provide clean, reproducible evidence for capability preservation beneath wrappers.
- **Medium confidence**: The claim that fine-tuning rarely deletes capabilities extends to more complex natural language tasks based on TinyStories results, though this validation is less extensive than the synthetic experiments.
- **Medium confidence**: The relationship between pretraining sampling probability and wrapper learning (Mechanism 3) is theoretically sound but has limited empirical validation across diverse task types.

## Next Checks

1. **Natural language generalization test**: Apply the wrapper detection methodology to a real-world NLP task (e.g., sentiment analysis or question answering) where the pretraining capabilities are known. Use the same suite of mechanistic interpretability tools to determine if wrappers are present and whether reverse fine-tuning can revive the original capabilities.

2. **Cross-architecture validation**: Test whether the wrapper phenomenon holds across different model architectures (RNNs, CNNs, and various transformer variants) beyond the specific Tracr-compiled and standard transformer models used in the paper. This would establish whether the mechanism is architecture-dependent or general.

3. **Capability deletion edge cases**: Design experiments specifically targeting scenarios where capabilities might be expected to be deleted rather than wrapped - such as fine-tuning on contradictory tasks or with extreme learning rates. Measure whether any true capability deletion occurs and identify conditions under which this might happen.