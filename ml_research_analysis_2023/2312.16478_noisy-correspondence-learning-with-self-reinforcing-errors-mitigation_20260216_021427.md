---
ver: rpa2
title: Noisy Correspondence Learning with Self-Reinforcing Errors Mitigation
arxiv_id: '2312.16478'
source_url: https://arxiv.org/abs/2312.16478
tags:
- noisy
- learning
- srem
- samples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel noisy correspondence learning framework
  called SREM to address the challenges of mismatched pairs in cross-modal retrieval.
  The core idea is to use energy uncertainty derived from classification logits to
  filter clean samples, swapped gradient weighting to estimate model sensitivity,
  and cross-modal biased complementary learning to leverage overlooked negative matches.
---

# Noisy Correspondence Learning with Self-Reinforcing Errors Mitigation

## Quick Facts
- arXiv ID: 2312.16478
- Source URL: https://arxiv.org/abs/2312.16478
- Reference count: 14
- Primary result: Achieves over 1% improvement in average recall on Flickr30K and MS-COCO benchmarks while being 40% more efficient than existing methods

## Executive Summary
This paper addresses the critical challenge of noisy correspondence learning in cross-modal retrieval, where mismatched image-text pairs can cause self-reinforcing errors that degrade model performance. The authors propose SREM (Self-Reinforcing Errors Mitigation), a novel framework that tackles this issue through three complementary mechanisms: energy-guided sample filtration using uncertainty derived from classification logits, swapped gradient weighting to estimate model sensitivity, and cross-modal biased complementary learning to leverage overlooked negative matches. The method demonstrates significant performance improvements across challenging benchmarks while maintaining computational efficiency.

## Method Summary
SREM operates by first using energy uncertainty calculated from classification logits to filter clean samples from noisy ones, moving beyond simple similarity scores. It then applies swapped gradient weighting (SGW) to adjust optimization sensitivity based on classification entropy, with weights swapped between modalities to promote consistency. Finally, it incorporates cross-modal biased complementary learning (CMBCL) that treats overlooked negative matches as complementary labels, providing additional supervision signals. The framework is trained using a modified hinge-based ranking loss with the new gradient weighting scheme, implemented on top of the SGRAF backbone with specified hyperparameters including batch size of 128 and Adam optimizer with learning rate 2e-4.

## Key Results
- Achieves over 1% improvement in average recall@1,5,10 on Flickr30K and MS-COCO benchmarks
- Demonstrates 40% efficiency improvement compared to existing methods
- Shows consistent performance gains across different noise ratios (20%, 40%, 60%) and real-world noisy datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy uncertainty derived from classification logits enables more precise clean/noisy sample separation than similarity alone.
- Mechanism: Treats sample matching as a classification task within a batch, generating classification logits for each sample. Energy uncertainty is computed as the negative log of the sum of exponentiated logits. Samples with low energy uncertainty and high similarity are selected as clean.
- Core assumption: Clean samples will have a unimodal distribution concentrated on the correct class, while noisy correspondences will have a more uniform distribution across classes.
- Evidence anchors: [abstract] "Instead of a single similarity score, we refine sample filtration through energy uncertainty and estimate model's sensitivity of selected clean samples using swapped classification entropy, in view of the overall prediction distribution." [section] "Specifically, the energy uncertainty corresponding to the visual input Ii can be calculated by: Energy (Ii) = − log PB b=1eFib . Intuively, more uniformly distributed prediction (i.e., noisy correspondence) leads to higher estimated energy uncertainty (Zhang et al. 2023)."
- Break condition: If the classification logits do not exhibit the expected unimodal vs uniform distribution pattern, or if energy uncertainty fails to correlate with sample quality.

### Mechanism 2
- Claim: Swapped Gradient Weighting (SGW) adjusts model sensitivity to different samples during optimization based on classification entropy.
- Mechanism: Classification entropy is computed from the softmax distribution of logits. Samples with lower entropy (higher confidence) receive higher gradient weights. The method also swaps the weights derived from one modality to the other to promote cross-modal consistency.
- Core assumption: Samples with lower classification entropy represent more confident predictions and should contribute more to optimization.
- Evidence anchors: [abstract] "estimate model's sensitivity of selected clean samples using swapped classification entropy, in view of the overall prediction distribution." [section] "In light of above, let wIi denote the entropy-based model's sensitivity to visual input Ii in i2t retrieval, formulated by: wIi = 1 − e(Pi)1(α − Sii + σ(h(Ii, Tφ(i)))), where α > 0 is the expected margin between positive and negative match."
- Break condition: If classification entropy does not correlate with sample quality or if swapped weighting does not improve cross-modal consistency.

### Mechanism 3
- Claim: Cross-Modal Biased Complementary Learning (CMBCL) leverages overlooked negative matches as complementary labels to improve optimization stability.
- Mechanism: Identifies negative samples that are not considered in the ranking loss and treats them as complementary labels indicating non-matching samples. These are used to train an auxiliary classifier that learns from the negative information.
- Core assumption: Overlooked negative matches contain useful information that can guide the model to distance positive samples from all negatives.
- Evidence anchors: [abstract] "Additionally, we propose cross-modal biased complementary learning to leverage negative matches overlooked in hard-negative training, further improving model optimization stability and curbing self-reinforcing errors." [section] "Evidently, Equation (7) highlights that Li2tw overlooks numerous negative similarities defined as: {Sij | j ≠ i; and if i ∈ Dclean, j ≠ φ(i)}. These overlooked negative similarities maintain zero gradients and are ignored in model optimization."
- Break condition: If the negative samples used as complementary labels do not provide useful information or if the additional complexity does not improve performance.

## Foundational Learning

- Concept: Energy-based uncertainty estimation in classification tasks
  - Why needed here: To quantify the model's uncertainty about sample matching beyond simple similarity scores
  - Quick check question: How does energy uncertainty differ from entropy as a measure of uncertainty?
- Concept: Gradient weighting based on sample confidence
  - Why needed here: To ensure the model is more sensitive to confident samples during optimization
  - Quick check question: What is the relationship between classification entropy and sample confidence?
- Concept: Complementary label learning
  - Why needed here: To leverage negative information that is overlooked in standard ranking losses
  - Quick check question: How do complementary labels differ from traditional negative samples in contrastive learning?

## Architecture Onboarding

- Component map: Feature Encoder -> Energy-Guided Sample Filtration -> Swapped Gradient Weighting -> Cross-Modal Biased Complementary Learning -> Optimization
- Critical path: Feature encoding → Energy filtration → SGW weighting → CMBCL objective → Model update
- Design tradeoffs: Energy uncertainty vs similarity for sample filtration; gradient weighting complexity vs simplicity; CMBCL benefits vs computational overhead
- Failure signatures:
  - Energy uncertainty not distinguishing clean/noisy samples
  - SGW causing instability in optimization
  - CMBCL not improving performance or causing overfitting
- First 3 experiments:
  1. Test energy uncertainty on synthetic noisy data to verify it distinguishes clean/noisy samples
  2. Implement SGW with simple gradient weighting to verify it improves over standard ranking loss
  3. Add CMBCL to a baseline model to verify it leverages negative information effectively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the energy uncertainty threshold (τ) impact the performance of SREM across different noise ratios and datasets?
- Basis in paper: [explicit] The paper mentions using a threshold τ to select clean samples based on energy uncertainty, but does not provide a detailed analysis of its sensitivity to different noise ratios or datasets.
- Why unresolved: The paper does not explore the impact of varying the energy uncertainty threshold (τ) on the performance of SREM across different noise ratios and datasets.
- What evidence would resolve it: An ablation study varying the energy uncertainty threshold (τ) and its impact on performance across different noise ratios and datasets would provide insights into its sensitivity and optimal selection.

### Open Question 2
- Question: Can SREM be effectively applied to other cross-modal retrieval tasks beyond image-text retrieval, such as video-text or audio-text retrieval?
- Basis in paper: [inferred] The paper focuses on image-text retrieval, but the methodology of SREM, including energy-guided sample filtration and cross-modal biased complementary learning, could potentially be applied to other cross-modal retrieval tasks.
- Why unresolved: The paper does not explore the applicability of SREM to other cross-modal retrieval tasks beyond image-text retrieval.
- What evidence would resolve it: Applying SREM to other cross-modal retrieval tasks and evaluating its performance would demonstrate its generalizability and effectiveness in different domains.

### Open Question 3
- Question: How does the swapped gradient weighting (SGW) strategy compare to other sample weighting or re-weighting methods in terms of performance and computational efficiency?
- Basis in paper: [explicit] The paper introduces the SGW strategy as a way to estimate model sensitivity using classification entropy, but does not compare it to other sample weighting or re-weighting methods.
- Why unresolved: The paper does not provide a comparative analysis of the SGW strategy against other sample weighting or re-weighting methods in terms of performance and computational efficiency.
- What evidence would resolve it: A comparative study evaluating the performance and computational efficiency of the SGW strategy against other sample weighting or re-weighting methods would provide insights into its effectiveness and efficiency.

## Limitations

- Energy uncertainty calculation relies on classification logits having expected unimodal vs uniform distribution patterns, which may not hold for all noise scenarios
- Swapped gradient weighting approach is novel with limited corpus evidence for its effectiveness in cross-modal retrieval tasks
- Computational overhead of additional SGW and CMBCL modules could impact scalability to larger datasets

## Confidence

- High Confidence: Overall framework architecture and motivation for addressing self-reinforcing errors; experimental results showing performance improvements
- Medium Confidence: Specific mechanisms of energy uncertainty calculation and complementary label learning; novelty limits corpus validation
- Low Confidence: Effectiveness of swapped gradient weighting strategy; appears novel with limited supporting evidence

## Next Checks

1. **Energy Uncertainty Validation**: Conduct controlled experiments with synthetic noisy data to verify that energy uncertainty calculations reliably distinguish clean from noisy samples across different noise ratios and distributions.

2. **SGW Ablation Study**: Perform an ablation study comparing standard ranking loss with SGW to isolate the contribution of swapped gradient weighting and verify it improves cross-modal consistency without causing optimization instability.

3. **CMBCL Complementary Label Quality**: Analyze the distribution and quality of complementary labels generated by CMBCL to ensure they provide useful negative information and do not introduce noise or bias into the training process.