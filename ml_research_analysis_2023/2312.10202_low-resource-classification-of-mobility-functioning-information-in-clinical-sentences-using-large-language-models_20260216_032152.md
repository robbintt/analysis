---
ver: rpa2
title: Low-resource classification of mobility functioning information in clinical
  sentences using large language models
arxiv_id: '2312.10202'
source_url: https://arxiv.org/abs/2312.10202
tags:
- fine-tuning
- performance
- mobility
- dataset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that publicly available large language
  models (LLMs) can effectively identify mobility functioning information in clinical
  notes through in-context learning and parameter-efficient fine-tuning. Using a balanced
  dataset of 1000 clinical sentences, the researchers evaluated multiple instruction-tuned
  LLMs in zero-shot and few-shot settings, achieving F1 scores up to 0.865.
---

# Low-resource classification of mobility functioning information in clinical sentences using large language models

## Quick Facts
- arXiv ID: 2312.10202
- Source URL: https://arxiv.org/abs/2312.10202
- Reference count: 40
- Primary result: F1 scores up to 0.865 using LLMs with kNN sampling, 0.922 with PEFT fine-tuning

## Executive Summary
This study investigates the use of large language models (LLMs) for low-resource clinical text classification, specifically identifying mobility functioning information in clinical notes. The researchers evaluate multiple publicly available instruction-tuned LLMs using in-context learning with both zero-shot and few-shot approaches, demonstrating that k-nearest neighbor sampling significantly improves performance over random sampling. They also implement parameter-efficient fine-tuning using LoRA, achieving results comparable to full fine-tuning with substantially fewer parameters. The findings show that LLMs can effectively perform clinical text classification tasks even with limited training data, offering a promising approach for healthcare applications where annotated datasets are scarce.

## Method Summary
The researchers used a balanced dataset of 1000 clinical sentences from the Mobility NER dataset, containing 500 positive sentences with mobility entities and 500 negative sentences. They evaluated multiple instruction-tuned LLMs (Flan-T5-xxl, Flan-T5-xl, Flan-UL2, T0++, and Llama 2-Chat) using in-context learning with zero-shot and few-shot settings, comparing random sampling to k-nearest neighbor (kNN) sampling for demonstration examples. For fine-tuning, they employed parameter-efficient prompt-based fine-tuning using LoRA with rank decomposition on self-attention weights, training on subsets of 50, 100, and 500 examples. Four-fold cross-validation was used for evaluation, with F1 score as the primary metric.

## Key Results
- kNN-based sampling for demonstrations significantly improved LLM classification performance compared to random sampling, with F1 scores up to 0.865
- Parameter-efficient prompt-based fine-tuning achieved F1 scores up to 0.922 using the full training dataset, with smaller models requiring only 2.3M additional parameters
- Flan-T5-xxl outperformed all other models in both zero-shot and few-shot settings
- The optimal number of demonstration examples was found to be just one positive example using kNN sampling

## Why This Works (Mechanism)

### Mechanism 1: kNN-based demonstration selection
- Claim: kNN-based sampling of demonstration examples significantly improves LLM classification performance by selecting semantically similar examples to the test sentence.
- Mechanism: The LLM uses the kNN-selected examples as in-context demonstrations that are more relevant to the test case, reducing noise from unrelated examples and improving prediction accuracy.
- Core assumption: Semantic similarity between demonstration examples and test sentences is a good proxy for relevance to the classification task.
- Evidence anchors: k-nearest neighbor sampling for demonstrations significantly improved performance compared to random sampling; we utilize the all-mpnet-base-v2 model to generate embeddings for all training sentences; for each test sentence, we first generate its corresponding embedding and calculate its cosine similarity with each pre-computed training sentence embedding to find the kNN examples.
- Break condition: If the embedding model used for kNN sampling doesn't capture task-relevant semantics, or if the dataset has very uniform semantic distribution making random sampling equally effective.

### Mechanism 2: Parameter-efficient fine-tuning with LoRA
- Claim: Parameter-efficient fine-tuning (PEFT) with LoRA achieves comparable performance to full-model fine-tuning while requiring significantly fewer parameters.
- Mechanism: LoRA introduces low-rank decomposition matrices to the existing model weights, allowing efficient adaptation of the model to the task-specific data without updating all parameters.
- Core assumption: Low-rank adaptation can capture the essential task-specific knowledge while preserving the pre-trained model's general capabilities.
- Evidence anchors: smaller models requiring only 2.3M additional parameters to match the performance of fully fine-tuned models; We employ a PEFT method called Low Rank Adaptation (LoRA). This method introduces rank-decomposition weight matrices to the existing model weights and trains only these newly added weights.
- Break condition: If the rank r is set too low to capture task-specific patterns, or if the task requires significant architectural changes that LoRA cannot accommodate.

### Mechanism 3: Instruction-tuned LLM generalization
- Claim: Instruction-tuned LLMs demonstrate superior in-context learning capabilities for clinical text classification tasks compared to standard LLMs.
- Mechanism: The instruction tuning process exposes the model to a diverse set of task instructions during pre-training, enabling better generalization to new classification tasks with minimal demonstrations.
- Core assumption: Exposure to diverse instruction formats during pre-training creates a flexible understanding of task instructions that transfers to new domains.
- Evidence anchors: Flan-T5-xxl outperforms all other models in both zero-shot and few-shot settings, achieving a F1 score of 0.865; Instruction tuning has demonstrated its effectiveness in improving the overall performance of LLMs across a wide range of unseen tasks.
- Break condition: If the clinical domain is too specialized or uses terminology that differs significantly from the instruction tuning corpus, reducing the transfer benefit.

## Foundational Learning

- Concept: In-context learning and few-shot prompting
  - Why needed here: The study relies heavily on demonstrating how LLMs can learn from examples provided in the prompt without parameter updates, which is central to understanding the experimental setup and results
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of LLMs?

- Concept: Parameter-efficient fine-tuning (PEFT) and LoRA
  - Why needed here: The study employs PEFT with LoRA to fine-tune models efficiently, and understanding this is crucial for interpreting the resource requirements and performance tradeoffs discussed
  - Quick check question: How does LoRA reduce the number of parameters that need to be fine-tuned compared to traditional fine-tuning?

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: The kNN sampling method relies on calculating semantic similarity between sentences using embeddings, which is fundamental to understanding how demonstration examples are selected
  - Quick check question: What is the purpose of using cosine similarity between sentence embeddings in the kNN sampling approach?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt construction -> Model inference -> Fine-tuning pipeline -> Evaluation
- Critical path:
  1. Load and preprocess the mobility classification dataset
  2. Construct prompts for in-context learning experiments
  3. Evaluate LLM performance with zero-shot, random sampling, and kNN sampling
  4. Implement PEFT with LoRA for fine-tuning experiments
  5. Train and evaluate models with varying training set sizes
  6. Analyze results and identify best-performing configurations
- Design tradeoffs:
  - Model size vs. computational resources: Larger models (e.g., Flan-T5-xxl) perform better but require more GPU memory
  - Shot count vs. performance: More demonstration examples don't always improve performance, especially with random sampling
  - Training data size vs. fine-tuning effectiveness: Smaller datasets can still yield good results with PEFT, but performance improves with more data
  - kNN vs. random sampling: kNN requires additional computation for embeddings but often yields better results
- Failure signatures:
  - Poor performance with random sampling but improvement with kNN sampling: Indicates the importance of demonstration example selection
  - No improvement or degradation with more demonstration examples: Suggests potential overfitting to demonstrations or model saturation
  - Significant performance gap between PEFT and full fine-tuning: May indicate the task requires more extensive parameter updates than LoRA can provide
  - Inconsistent results across cross-validation folds: Could indicate data quality issues or insufficient model capacity
- First 3 experiments:
  1. Evaluate zero-shot performance of Flan-T5-xxl on the mobility classification task to establish baseline capability
  2. Compare random sampling vs. kNN sampling for 1-shot and 2-shot settings to validate the importance of demonstration selection
  3. Implement PEFT with LoRA on Flan-T5-xl using 50 training examples to test low-resource fine-tuning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of kNN-based sampling for demonstration selection scale with dataset size and class distribution?
- Basis in paper: The paper observes that kNN-based sampling improves performance over random sampling, with the improvement decreasing as the number of demonstrations increases.
- Why unresolved: The paper only evaluates kNN sampling on a balanced dataset with a fixed size. Performance may vary with different dataset characteristics.
- What evidence would resolve it: Experiments evaluating kNN sampling performance across datasets of varying sizes and class distributions.

### Open Question 2
- Question: What is the optimal number of demonstration examples for in-context learning on mobility functioning classification tasks?
- Basis in paper: The paper notes that increasing the number of demonstration examples does not guarantee improvement, and optimal performance is achieved with just one positive example using kNN sampling.
- Why unresolved: The paper only tests up to 8-shot demonstrations. The optimal number may differ for larger models or different tasks.
- What evidence would resolve it: Systematic evaluation of in-context learning performance with varying numbers of demonstrations (e.g., 1-32 examples) across different model sizes.

### Open Question 3
- Question: How do contextual calibration techniques affect the performance of LLMs on mobility functioning classification?
- Basis in paper: The paper mentions that generative LLMs may exhibit biases toward "yes" or "no" answers and suggests that contextual calibration could mitigate these biases.
- Why unresolved: The paper does not implement or evaluate any calibration techniques.
- What evidence would resolve it: Experiments comparing LLM performance with and without contextual calibration techniques applied to the mobility functioning classification task.

## Limitations
- Dataset size of 1000 sentences may be insufficient for robust clinical NLP evaluation
- kNN-based sampling requires pre-computed embeddings, limiting scalability for larger datasets
- Evaluation focuses primarily on F1 score without exploring clinical utility metrics

## Confidence
- High confidence: kNN-based sampling improves few-shot learning performance; PEFT with LoRA achieves comparable results to full fine-tuning with fewer parameters
- Medium confidence: Instruction-tuned LLMs generalize well to clinical text classification; smaller models can achieve performance parity with larger models using PEFT
- Low confidence: kNN sampling approach generalizes to other clinical classification tasks; optimal number of demonstrations is universally applicable

## Next Checks
1. Test kNN-based sampling approach on a larger, more diverse clinical dataset to evaluate scalability and robustness across different clinical domains
2. Conduct ablation studies to determine optimal number of demonstration examples for different LLM sizes and task complexities
3. Compare performance of instruction-tuned LLMs against domain-specific pre-trained clinical language models on the same classification task