---
ver: rpa2
title: A Framework for Neurosymbolic Robot Action Planning using Large Language Models
arxiv_id: '2303.00438'
source_url: https://arxiv.org/abs/2303.00438
tags:
- planning
- training
- accuracy
- plan
- teriyaki
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Teriyaki, a framework for training large
  language models (LLMs) such as GPT-3 to solve PDDL planning problems in robotics.
  The method involves generating training datasets of problem-plan pairs, training
  GPT-3 to predict plans from PDDL problem descriptions, and evaluating performance
  in two domains modeling articulated object manipulation.
---

# A Framework for Neurosymbolic Robot Action Planning using Large Language Models

## Quick Facts
- arXiv ID: 2303.00438
- Source URL: https://arxiv.org/abs/2303.00438
- Reference count: 27
- One-line primary result: Trained GPT-3 models achieve 94-95.5% planning accuracy for articulated object manipulation tasks, with plans up to 13.5% shorter than traditional planners

## Executive Summary
This paper introduces Teriyaki, a framework for training large language models (LLMs) like GPT-3 to solve PDDL planning problems in robotics. The method involves generating training datasets of problem-plan pairs, training GPT-3 to predict plans from PDDL problem descriptions, and evaluating performance in two domains modeling articulated object manipulation. The trained models achieve planning accuracy comparable to traditional symbolic planners while generating shorter plans, demonstrating the viability of neurosymbolic planning using LLMs.

## Method Summary
The Teriyaki framework trains GPT-3 on problem-plan pairs generated from PDDL domains, allowing the model to implicitly learn planning rules without explicit domain specification. The approach uses structured prompts containing PDDL problem descriptions and trains the model to output valid plans. The method was evaluated on two articulated object manipulation domains (MACRO and NO-MACRO), comparing planning accuracy, plan length, and planning times against a traditional symbolic planner baseline.

## Key Results
- GPT-3 achieves 94-95.5% planning accuracy on test problems, comparable to traditional planners
- Generated plans are up to 13.5% shorter than those produced by traditional planners
- Planning times scale linearly with input/output length rather than domain complexity
- Transfer learning between similar domains improves performance with fewer training samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 can implicitly learn PDDL domain rules through training on problem-plan pairs, eliminating the need for explicit domain specification.
- Mechanism: The model learns probabilistic relationships between tokens representing actions, preconditions, and effects through exposure to many problem-plan pairs. This allows it to generate valid plans for new problems by predicting the most likely sequence of actions given the problem description.
- Core assumption: The statistical patterns in valid plans capture the underlying logical constraints of the PDDL domain sufficiently for the model to generalize.
- Evidence anchors:
  - [abstract] "The rationale is training Large Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner compatible with the Planning Domain Definition Language (PDDL)"
  - [section] "We assume that the GPT-3 model can implicitly learn by example the rules usually defined in a PDDL domain file"
- Break condition: If the domain contains complex conditional effects or rare action sequences that don't appear frequently enough in the training data, the model may fail to generate valid plans.

### Mechanism 2
- Claim: Training GPT-3 as a PDDL solver enables scalable planning for complex domains by leveraging linear scaling of LLM inference time with input/output length.
- Mechanism: Unlike traditional planners where planning time explodes combinatorially with domain complexity, LLM-based planners scale linearly with the combined length of the problem description and generated plan. This makes them potentially more efficient for large, complex planning domains.
- Core assumption: The planning domain complexity correlates with the length of the generated plan, not the number of possible actions.
- Evidence anchors:
  - [abstract] "Unlike symbolic approaches, LLMs require a training process. However, their response time scales with the combined length of the prompt and the completion"
  - [section] "The computational complexity of LLMs is known to scale linearly with the combined length of the prompt and the completion"
- Break condition: If plan length doesn't correlate well with domain complexity, or if the LLM's attention mechanism can't maintain coherence for very long sequences, the linear scaling advantage may disappear.

### Mechanism 3
- Claim: Transfer learning between similar planning domains improves planning accuracy with fewer training samples.
- Mechanism: Training a model on one domain (e.g., MACRO) and then fine-tuning it on a related domain (e.g., NO-MACRO) leverages shared concepts and reduces the amount of new training data needed to achieve high accuracy.
- Core assumption: Planning domains with similar action structures and effects share enough underlying concepts that knowledge transfers effectively between them.
- Evidence anchors:
  - [section] "Regarding the training data set, the only difference with the example provided in Listing 1 is that the prompt part is preceded by the \n--NO-MACRO tag. This tag was introduced to test whether the model could be used to solve problems for both the MACRO and the NO-MACRO domains"
  - [section] "Results shown in Section IV-B confirmed this hypothesis, so we interrupted training when the model trained from the MACRO model reached comparable results to its parent model"
- Break condition: If domains are too dissimilar, or if the shared concepts are not the most important ones for planning accuracy, transfer learning may not provide significant benefits.

## Foundational Learning

- Concept: Planning Domain Definition Language (PDDL)
  - Why needed here: PDDL is the standard language for high-level planning in robotics, and this work aims to create GPT-3 models that can solve PDDL problems directly.
  - Quick check question: What are the key components of a PDDL problem file, and how do they differ from the domain file?

- Concept: Large Language Models (LLMs) and Transformers
  - Why needed here: GPT-3 is an LLM based on the Transformer architecture, and understanding how it processes and generates text is crucial for understanding this work's approach to planning.
  - Quick check question: How does the self-attention mechanism in Transformers enable them to weigh different parts of the input differently?

- Concept: Transfer Learning
  - Why needed here: This work uses transfer learning to improve planning accuracy for a new domain by leveraging knowledge from a previously trained model.
  - Quick check question: What are the potential benefits and drawbacks of using transfer learning for planning tasks?

## Architecture Onboarding

- Component map:
  - Data generation -> Training data preparation -> GPT-3 training -> Evaluation -> Validation
  - Generate PDDL problem-plan pairs -> Format problem descriptions as prompts -> Fine-tune GPT-3 on prepared data -> Test trained model on unseen problems -> Use VAL to verify plan validity

- Critical path:
  1. Generate training data (problems and plans)
  2. Prepare training data for GPT-3
  3. Train GPT-3 model
  4. Evaluate trained model on test set
  5. Analyze results and iterate if necessary

- Design tradeoffs:
  - Using GPT-3 vs. a smaller, more customizable LLM: GPT-3 is powerful but expensive and less flexible, while smaller models might be more efficient but require more training data
  - Including static predicates in the prompt: Including them might improve accuracy but increase prompt length and planning times
  - Training on plans from a single planner vs. multiple planners: Using a single planner ensures consistency but might bias the model towards that planner's style

- Failure signatures:
  - Low planning accuracy: The model may not have learned the domain rules well enough, or the training data may not be diverse enough
  - Long planning times: The model may be generating overly long plans, or the prompt may be too long
  - Invalid plans: The model may not be properly handling conditional effects or other complex domain features

- First 3 experiments:
  1. Train a GPT-3 model on a simple PDDL domain (e.g., blocks world) and test it on a few problems to verify that it can learn basic planning concepts
  2. Train a model on a more complex domain (e.g., the articulated object manipulation domains used in this work) and compare its performance to a traditional planner on a small test set
  3. Experiment with different training data sizes and hyperparameters to find the optimal configuration for a given domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does planning accuracy change with different LLM architectures beyond GPT-3 (e.g., Transformer variants, BERT, T5)?
- Basis in paper: [inferred] The paper focuses exclusively on GPT-3, though it notes that results "should not be limited to the specific features of GPT-3" and mentions other models like LaMDA, PALM, and BLOOM as potential alternatives.
- Why unresolved: The paper only tests one LLM architecture (GPT-3), leaving open whether other architectures would perform similarly or better for neurosymbolic planning.
- What evidence would resolve it: Systematic testing of multiple LLM architectures on the same PDDL domains with identical training protocols, comparing planning accuracy, plan quality, and computational efficiency.

### Open Question 2
- Question: What is the optimal balance between prompt length and plan quality, and how does this trade-off affect real-time planning performance?
- Basis in paper: [explicit] The paper notes that "only part of the PDDL problem is used as a prompt" to reduce length and avoid truncation, but doesn't systematically investigate the relationship between prompt size and planning outcomes.
- Why unresolved: The paper makes pragmatic choices about prompt truncation but doesn't explore how different prompt lengths affect accuracy, planning time, or the model's ability to capture domain knowledge.
- What evidence would resolve it: Controlled experiments varying prompt lengths while measuring planning accuracy, plan length, and response times across different domain complexities.

### Open Question 3
- Question: How well does transfer learning generalize to entirely new but structurally similar planning domains beyond the MACRO/NO-MACRO comparison?
- Basis in paper: [explicit] The paper demonstrates transfer learning between MACRO and NO-MACRO domains but notes this "remains one of the points to further investigate" and only tested two closely related domains.
- Why unresolved: The successful transfer between MACRO and NO-MACRO domains doesn't establish whether the approach scales to more diverse planning problems or truly novel domains.
- What evidence would resolve it: Training on multiple diverse domains and testing transfer to completely new domains, measuring planning accuracy decay and identifying characteristics that facilitate or hinder transfer.

## Limitations

- The evaluation is confined to two highly controlled domains with a total of 8,000-9,000 training samples each
- GPT-3 inference is slower than traditional planners and API costs are substantial
- The method's performance on domains with larger state spaces or more complex conditional effects remains untested

## Confidence

**High Confidence Claims:**
- GPT-3 can learn to generate valid plans for PDDL problems when trained on problem-plan pairs
- The proposed framework successfully produces plans for the tested domains
- Planning accuracy of 94-95.5% is achievable with the current approach
- Plan lengths are comparable to or shorter than traditional planners in the tested domains

**Medium Confidence Claims:**
- The linear scaling advantage of LLM inference time over traditional planners will hold for larger, more complex domains
- Transfer learning between similar domains provides meaningful performance improvements
- The approach is viable for real-world robotics applications beyond the tested domains

**Low Confidence Claims:**
- The method is cost-effective compared to traditional planners when considering API costs and training requirements
- The approach generalizes well to domains with significantly different characteristics from MACRO and NO-MACRO
- Planning times are acceptable for real-time robotic applications

## Next Checks

1. **Domain Generalization Test**: Evaluate the trained models on a diverse set of planning domains with varying complexity, state space sizes, and action structures. Test on at least 5-10 additional domains from existing PDDL benchmarks to assess generalizability.

2. **Real-World Integration**: Implement the framework in a physical robot system and test on actual articulated object manipulation tasks. Measure end-to-end performance including perception-to-planning pipeline and execution success rates.

3. **Scalability Analysis**: Systematically vary problem complexity (number of objects, predicates, and actions) and measure how planning accuracy and time scale. Compare these scaling properties against multiple traditional planners across a range of domain complexities.