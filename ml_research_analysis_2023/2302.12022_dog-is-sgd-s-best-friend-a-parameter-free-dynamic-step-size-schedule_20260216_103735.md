---
ver: rpa2
title: 'DoG is SGD''s Best Friend: A Parameter-Free Dynamic Step Size Schedule'
arxiv_id: '2302.12022'
source_url: https://arxiv.org/abs/2302.12022
tags:
- learning
- rate
- step
- adam
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes DoG (Distance over Gradients), a parameter-free\
  \ dynamic learning rate schedule for stochastic gradient descent (SGD). The key\
  \ idea is to set the step size at iteration t as \u03B7t = max{i\u2264t} ||xi -\
  \ x0|| / sqrt(sum{i\u2264t} ||gi||^2), where xi are iterates and gi are stochastic\
  \ gradients."
---

# DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule

## Quick Facts
- arXiv ID: 2302.12022
- Source URL: https://arxiv.org/abs/2302.12022
- Reference count: 40
- Primary result: Proposes DoG, a parameter-free dynamic learning rate schedule that achieves competitive performance with tuned SGD and approaches tuned Adam on fine-tuning tasks

## Executive Summary
This paper introduces DoG (Distance over Gradients), a parameter-free dynamic learning rate schedule for stochastic gradient descent that eliminates the need for learning rate tuning. The method sets the step size at iteration t as the ratio of the maximum distance traveled to the initial point over the root sum of squared gradient norms. Theoretically, a tamed variant T-DoG achieves optimal convergence rates for stochastic convex optimization under locally bounded gradients. Empirically, DoG performs close to well-tuned SGD across 23 vision and language fine-tuning tasks, while a per-layer variant L-DoG closes much of the gap to tuned Adam.

## Method Summary
DoG implements a parameter-free learning rate schedule where η_t = max_{i≤t} ||x_i - x_0|| / sqrt(sum_{i≤t} ||g_i||^2), requiring no learning rate tuning. The method includes a tamed variant T-DoG with logarithmic safety margins for theoretical guarantees, and a per-layer variant L-DoG that applies the formula separately to each layer. The approach is tested on fine-tuning 8 model architectures across 23 vision and language tasks, comparing against tuned SGD and Adam with cosine annealing.

## Key Results
- DoG achieves performance close to well-tuned SGD on 23 vision and language fine-tuning tasks
- L-DoG generally outperforms tuned SGD and approaches the performance of tuned Adam
- The method is robust to the choice of initial movement size r_epsilon as long as it's small enough

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DoG achieves parameter-free convergence by implicitly satisfying the step-size condition that guarantees optimal rates
- Mechanism: The step size ηt = max_{i≤t} ||x_i - x_0|| / sqrt(sum_{i≤t} ||g_i||^2) is constructed so that at each iteration t, it satisfies the implicit equation from Carmon and Hinder [10] that ensures near-optimal convergence
- Core assumption: The iterates remain within a bounded region around the initial point x0, specifically ||x_t - x0|| ≤ 3d0 where d0 is the distance to the optimum
- Evidence anchors:
  - [abstract]: "Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only locally bounded stochastic gradients"
  - [section]: "Crucially, DoG has no multiplicative 'learning rate' parameter: if one considers step sizes of the form ηt = c· maxi≤t||xi−x0||/√∑_{i≤t}||gi||^2 then c = 1 is a universally good setting"
  - [corpus]: Weak evidence - no direct mention of the implicit equation mechanism in corpus papers
- Break condition: The iterates move far from x0 (specifically ||x_t - x0|| > 3d0), causing gradient norms to exceed L* and breaking the boundedness assumption

### Mechanism 2
- Claim: T-DoG maintains iterate stability by increasing the denominator with logarithmic factors
- Mechanism: T-DoG modifies the step size to ηt = rt/√G'_t where G'_t = 84θT,δ log^2(t+1)(Gt-1 + 16θT,δ||gt||^2∨L^2), adding a polylogarithmic safety margin that prevents the iterates from escaping the bounded region
- Core assumption: The stochastic gradients are locally bounded in a ball of radius 3d0 around x0
- Evidence anchors:
  - [section]: "we consider a theoretical, tamed variant of DoG, which we call T-DoG, whose step sizes are smaller by a logarithmic factor. We prove that, with high probability, the T-DoG iterates never leave B"
  - [section]: "Proposition 2. Suppose that Assumptions 1 and 2 hold and rϵ≤ 3d0. For any δ∈ (0, 1), T∈ N and L≥L⋆, the iterations of T-DoG satisfy P(¯rT > 3d0)≤δ"
  - [corpus]: Weak evidence - no direct mention of the logarithmic taming mechanism in corpus papers
- Break condition: The bound L on L* is set too low (L < L*), or the failure probability δ is set too high, causing the iterate stability guarantee to fail

### Mechanism 3
- Claim: Layer-wise DoG (L-DoG) closes the gap to Adam by exploiting per-layer adaptivity
- Mechanism: L-DoG applies the DoG formula separately for each layer l with η^l_t = max_{i≤t}||x^l_i - x^l_0||/√∑_{i≤t}||g^l_i||^2+ε, allowing each layer to have its own adaptive step size based on its gradient history
- Core assumption: Different layers benefit from different step sizes, similar to how Adam's per-parameter adaptivity helps
- Evidence anchors:
  - [abstract]: "we also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam"
  - [section]: "We also propose a per-layer variant of DoG (which we call L-DoG) that generally outperforms tuned SGD, approaching the performance of tuned Adam"
  - [corpus]: Weak evidence - the corpus mentions layer-wise adaptive step-sizes (paper 215464) but doesn't specifically validate L-DoG
- Break condition: The per-layer normalization fails when some layers have very small gradient norms, causing numerical instability or inappropriate step sizes

## Foundational Learning

- Concept: Convex optimization and subgradient methods
  - Why needed here: The theoretical analysis assumes convexity and uses subgradient descent techniques to prove convergence
  - Quick check question: Can you explain why the weighted regret bound in Lemma 1 holds for convex functions but would fail for non-convex ones?

- Concept: Martingale concentration inequalities
  - Why needed here: The noise bound in Lemma 2 relies on concentration results for martingale difference sequences to handle the stochastic gradient noise
  - Quick check question: What's the difference between the Azuma-Hoeffding inequality and the Bernstein-type bound used in the paper's Corollary 3?

- Concept: Parameter-free optimization and coin betting
  - Why needed here: The paper builds on the parameter-free optimization literature, particularly the implicit equation technique from Carmon and Hinder [10]
  - Quick check question: How does the DoG step size formula relate to the "implicit equation" technique used in parameter-free optimization?

## Architecture Onboarding

- Component map:
  - PyTorch optimizer subclass implementing DoG and L-DoG step size calculations
  - Mathematical proofs for convergence and iterate stability
  - Testing infrastructure for vision and language fine-tuning tasks
  - Hyperparameter tuning mechanisms for r_epsilon

- Critical path:
  1. Implement DoG step size calculation in PyTorch optimizer
  2. Add polynomial decay averaging following Shamir and Zhang [78]
  3. Implement T-DoG variant with logarithmic safety margin
  4. Add layer-wise extension for L-DoG
  5. Set up experimental testbed with vision and language tasks
  6. Add sensitivity analysis for r_epsilon parameter

- Design tradeoffs:
  - Simplicity vs. adaptivity: DoG is simpler than Adam but lacks momentum and per-parameter adaptivity
  - Theoretical guarantees vs. practical performance: T-DoG has stronger theoretical guarantees but may be slower in practice
  - Per-layer vs. global: L-DoG has better performance but higher memory and computation overhead

- Failure signatures:
  - Divergence: η_t grows exponentially early in training, indicating r_epsilon is too large
  - Poor final performance: Step sizes don't stabilize to optimal values, suggesting issues with gradient normalization
  - Sensitivity to r_epsilon: Large performance variations across r_epsilon values, possibly due to batch normalization effects

- First 3 experiments:
  1. Verify basic convergence on simple convex problems (logistic regression) compared to tuned SGD
  2. Test sensitivity to r_epsilon on a vision task (e.g., CIFAR-100) to identify the stability threshold
  3. Compare L-DoG vs. DoG vs. Adam on a language task (e.g., SST-2) to measure the performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which batch normalization affects the stability of DoG, and can we develop a modification to DoG that is robust to batch normalization?
- Basis in paper: [explicit] The paper notes that DoG is sensitive to the choice of r_ϵ when batch normalization is present, and provides preliminary evidence by showing that disabling batch normalization stabilizes DoG's step size sequence.
- Why unresolved: The paper only provides initial evidence and hypotheses, but does not offer a definitive explanation or solution for the interaction between DoG and batch normalization.
- What evidence would resolve it: Further experiments analyzing the gradient norms and step size sequences in the presence and absence of batch normalization, along with theoretical analysis of the effect of normalization on the implicit equation underlying DoG, would help understand and potentially mitigate this issue.

### Open Question 2
- Question: Can DoG be effectively combined with momentum and other techniques like per-parameter learning rates and learning rate annealing, and what are the theoretical and empirical implications of such combinations?
- Basis in paper: [explicit] The paper acknowledges the importance of combining DoG with proven techniques like momentum and per-parameter learning rates, but does not explore these combinations in the current work.
- Why unresolved: The paper focuses on the basic DoG algorithm and does not investigate how it interacts with other common optimization techniques.
- What evidence would resolve it: Experiments comparing DoG with and without momentum, per-parameter learning rates, and learning rate annealing, both theoretically and empirically, would shed light on the effectiveness of these combinations.

### Open Question 3
- Question: How does DoG perform in settings where training from scratch is required, particularly for complex models like transformers, and what modifications might be necessary for such scenarios?
- Basis in paper: [explicit] The paper includes a preliminary experiment with training a Wide ResNet 28-10 from scratch on CIFAR-10, but acknowledges that further exploration of DoG in training-from-scratch settings is needed.
- Why unresolved: The paper's main experiments focus on fine-tuning pre-trained models, and the training-from-scratch experiment is limited in scope.
- What evidence would resolve it: Extensive experiments comparing DoG to other optimizers in training complex models like transformers from scratch on diverse datasets would provide insights into its effectiveness in this setting.

## Limitations

- The empirical evaluation shows mixed results, with DoG outperforming tuned SGD on 16 of 23 tasks but underperforming on 7 tasks
- The theoretical analysis is limited to the T-DoG variant under strong convexity assumptions, while empirical evaluation uses the simpler DoG on non-convex deep learning tasks
- The paper doesn't address potential numerical stability issues when gradient norms become very small or the computational overhead of the per-layer variant

## Confidence

- Theoretical claims about T-DoG convergence: High (rigorous proofs provided)
- Empirical claims about DoG matching tuned SGD: Medium (mixed results across tasks)
- Claims about L-DoG approaching Adam: Medium-Low (limited ablation studies)

## Next Checks

1. **Convergence robustness test**: Run DoG with systematically varied r_epsilon values on a subset of tasks to quantify the "close to well-tuned" claim and identify failure modes.

2. **Non-convex generalization**: Design experiments to test whether the iterate stability guarantees from T-DoG's convex analysis extend to the non-convex deep learning setting.

3. **Resource efficiency analysis**: Measure the memory and computation overhead of L-DoG compared to DoG and Adam across different model sizes to assess practical viability.