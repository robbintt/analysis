---
ver: rpa2
title: 'Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM'
arxiv_id: '2310.04836'
source_url: https://arxiv.org/abs/2310.04836
tags:
- quantization
- arxiv
- a8w4
- activation
- fp16
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Dual Grained Quantization (DGQ), an efficient
  A8W4 quantization scheme for Large Language Models (LLMs). The core idea is to dequantize
  fine-grained INT4 weights back to INT8 representation to enable efficient INT8 matrix
  multiplication, addressing the inefficiency of group-wise quantization on existing
  hardware.
---

# Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM

## Quick Facts
- arXiv ID: 2310.04836
- Source URL: https://arxiv.org/abs/2310.04836
- Reference count: 14
- Primary result: Proposes DGQ achieving up to 3.24× speed gains and 1.12× memory reduction compared to A16W4 implementation for LLM quantization

## Executive Summary
This paper introduces Dual Grained Quantization (DGQ), an efficient post-training quantization scheme for Large Language Models that combines 8-bit activation with 4-bit weight quantization. The core innovation is dequantizing INT4 weights to INT8 representation, enabling efficient INT8 matrix multiplication while preserving the memory benefits of fine-grained quantization. DGQ employs a two-phase grid search algorithm for quantization scale determination and a percentile clipping strategy for activation outlier smoothing. The method demonstrates consistent improvements over prior approaches across various LLM architectures and tasks, achieving significant speed-ups and memory reductions without substantial accuracy loss.

## Method Summary
DGQ implements A8W4 quantization by first quantizing weights to group-wise INT4, then dequantizing them to INT8 representation using group-wise scales. A two-phase grid search algorithm determines both fine-grained and coarse-grained quantization scales, while a percentile clipping strategy smooths activation outliers. The method performs INT8 GEMM operations on the dequantized weights, achieving efficiency gains over direct INT4 GEMM implementations. The approach is validated on LLaMA and OPT model families using WikiText-2, C4, PTB, and MMLU datasets, measuring perplexity and accuracy across language modeling and downstream tasks.

## Key Results
- Achieves up to 3.24× speed gains compared to A16W4 implementation
- Provides 1.12× memory reduction through INT4 weight quantization
- Consistently outperforms prior quantization methods across LLaMA and OPT models
- Maintains perplexity within 2.0 of full-precision baselines on language modeling tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-grained quantization enables INT8 GEMM on INT4 weights without segmenting GEMM
- Mechanism: DGQ dequantizes INT4 weights to INT8 representation using group-wise scales, then applies channel-wise FP16 scaling to maintain accuracy
- Core assumption: Dequantizing INT4 weights into INT8 format allows direct INT8 GEMM operations
- Evidence anchors:
  - [abstract]: "DSQ dequantizes the fine-grained INT4 weight into coarse-grained INT8 representation and preform matrix multiplication using INT8 kernels."
  - [section]: "DGQ dequantizes INT4-weight back to INT8, instead of directly casting to INT8. This results in INT8 weights with a coarser-grained scale."
- Break condition: If hardware does not support INT8 GEMM with the dequantized INT8 weights

### Mechanism 2
- Claim: Two-phase grid search prevents accuracy loss from direct quantization of group-wise scales
- Mechanism: First phase quantizes weights to group-wise INT4; second phase decouples group-wise FP16 scale into channel-wise FP16 and group-wise INT8 scales
- Core assumption: Separating quantization scale search into two phases preserves accuracy better than one-step quantization
- Evidence anchors:
  - [abstract]: "We develop a two-phase grid search algorithm to simplify the determination of fine-grained and coarse-grained quantization scales."
  - [section]: "We propose a novel two-phase search approach... First phase... Second phase..."
- Break condition: If search space reduction leads to suboptimal quantization parameters

### Mechanism 3
- Claim: Percentile clipping smoothing mitigates activation outlier impact without gradient-based optimization
- Mechanism: Identify top 0.5% largest activation values, clamp them to threshold, use as amplification for sensitive channels
- Core assumption: Activation outliers concentrate in fixed channels, so percentile clipping can smooth without harming generalization
- Evidence anchors:
  - [abstract]: "We also devise a percentile clipping schema for smoothing the activation outliers without the need for complex optimization techniques."
  - [section]: "We propose a novel percentile clipping smoothing strategy... One of the notable advantages... its gradient-free nature..."
- Break condition: If outliers are not confined to fixed channels, leading to performance degradation

## Foundational Learning

- Concept: Quantization (symmetric vs asymmetric, scale computation)
  - Why needed here: Understanding quantization is essential to grasp how DGQ transforms INT4 weights and scales activations
  - Quick check question: What is the formula for symmetric quantization scale and how does it differ from asymmetric quantization?

- Concept: Group-wise vs channel-wise quantization
  - Why needed here: DGQ combines these two granularities; understanding their differences explains why group-wise alone is inefficient
  - Quick check question: Why does group-wise quantization disrupt continuous integer matrix multiplication compared to channel-wise quantization?

- Concept: General Matrix Multiplication (GEMM) and hardware kernels
  - Why needed here: DGQ relies on INT8 GEMM kernels; knowing how GEMM works clarifies why dequantizing to INT8 is beneficial
  - Quick check question: How does INT8 GEMM differ from FP16 GEMM in terms of hardware efficiency?

## Architecture Onboarding

- Component map: Activation tensor (INT8) -> Weight quantization: Group-wise INT4 → Dequantized INT8 -> Scale computation: Channel-wise FP16 (s(1)), Group-wise INT8 (S(2)) -> Scaled INT8 matrix product

- Critical path:
  1. Quantize weights to INT4 (group-wise)
  2. Dequantize to INT8 using S(2) and ZP
  3. Apply channel-wise scaling s(1)
  4. Perform INT8 GEMM
  5. Apply percentile clipping smoothing to activations

- Design tradeoffs:
  - Accuracy vs speed: INT4 quantization reduces memory but needs dequantization for efficient GEMM
  - Complexity vs performance: Two-phase search adds complexity but preserves accuracy better than one-step
  - Memory vs computation: Percentile clipping smoothing avoids gradient optimization, saving compute

- Failure signatures:
  - Accuracy drop: Likely from suboptimal scale search or insufficient clipping threshold
  - Slow inference: Possible if dequantization or GEMM kernels are not optimized
  - Memory overflow: If group-wise scales or activations are not properly bounded

- First 3 experiments:
  1. Validate INT8 GEMM with dequantized INT4 weights on small matrices
  2. Test two-phase grid search accuracy vs one-step quantization on a single layer
  3. Evaluate percentile clipping smoothing effect on activation outlier mitigation in a controlled setup

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the work:
- How does the DGQ approach scale to larger model sizes beyond those tested?
- What is the impact of DGQ on model fine-tuning performance compared to full-precision fine-tuning?
- How does DGQ perform on tasks outside of language modeling, such as computer vision or speech recognition?

## Limitations
- Technical uncertainty around whether dequantizing INT4 weights to INT8 truly enables efficient INT8 GEMM operations
- Hardware dependency on specific INT8 GEMM kernel implementations, particularly NVIDIA's CUTLASS
- Lack of analysis on scale search sensitivity and computational overhead of the two-phase grid search algorithm

## Confidence
- High Confidence: The overall approach of combining fine-grained and coarse-grained quantization is technically sound
- Medium Confidence: The specific implementation details of the two-phase grid search algorithm and its effectiveness
- Low Confidence: The universal applicability claim across all LLM architectures and tasks

## Next Checks
1. Perform ablation study removing the dequantization step to quantify its direct contribution to accuracy and speed
2. Test the proposed method on different GPU architectures (AMD, Intel) and CPU implementations to verify hardware generalization
3. Systematically vary the grid search parameters to determine sensitivity to hyperparameters and quantify computational overhead