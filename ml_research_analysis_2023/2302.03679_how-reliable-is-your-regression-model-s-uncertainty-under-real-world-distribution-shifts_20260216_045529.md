---
ver: rpa2
title: How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution
  Shifts?
arxiv_id: '2302.03679'
source_url: https://arxiv.org/abs/2302.03679
tags:
- gaussian
- selective
- test
- prediction
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of regression uncertainty estimation
  reliability under real-world distribution shifts, motivated by safety-critical applications
  like medical imaging. The authors propose a benchmark of 8 image-based regression
  datasets with various challenging distribution shifts, including synthetic and real-world
  datasets from medical domains.
---

# How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?

## Quick Facts
- arXiv ID: 2302.03679
- Source URL: https://arxiv.org/abs/2302.03679
- Reference count: 40
- Key outcome: All evaluated regression uncertainty estimation methods become highly overconfident under distribution shifts, despite being well-calibrated without shifts

## Executive Summary
This paper addresses the critical problem of regression uncertainty estimation reliability under real-world distribution shifts, motivated by safety-critical applications like medical imaging. The authors propose a benchmark of 8 image-based regression datasets with various challenging distribution shifts, evaluating 10 different uncertainty estimation methods. The primary finding is that while methods perform well under no distribution shift, all become highly overconfident when test data distributions diverge from training data. This systematic failure reveals important limitations in current regression uncertainty estimation approaches and provides a challenge to the research community.

## Method Summary
The paper evaluates regression uncertainty estimation methods using a ResNet34 backbone trained on 8 image-based regression datasets with distribution shifts. The evaluation includes 10 methods: conformal prediction, quantile regression, Gaussian models, ensembles, and selective prediction variants using out-of-distribution detection. Models are trained for 75 epochs using ADAM optimizer, with calibration performed on validation sets using the Romano et al. (2019) procedure. The primary metric is prediction interval coverage (calibration), measuring empirical coverage relative to the 90% target (1-α where α=0.1), with secondary metrics including MAE and average interval length.

## Key Results
- All 10 evaluated methods become highly overconfident on many benchmark datasets under distribution shift, with test coverage dropping significantly below the 90% target
- Conformal prediction and quantile regression fail to provide calibrated intervals despite theoretical guarantees due to exchangeability assumption violations
- Methods based on out-of-distribution uncertainty scores perform relatively well but are still overconfident in many cases
- Selective prediction methods show better performance on synthetic datasets compared to real-world datasets, suggesting limitations in feature-space density modeling for real data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction and quantile regression fail to provide calibrated intervals under distribution shift despite theoretical guarantees.
- Mechanism: These methods assume exchangeability of training and test data, which breaks when input distributions shift.
- Core assumption: Train and test data are exchangeable (i.i.d.).
- Evidence anchors:
  - [abstract] "While methods are well calibrated when there is no distribution shift, they all become highly overconfident on many of the benchmark datasets."
  - [section] "Since the guarantees depend on the assumption that all data points are exchangeable (true for i.i.d. data, for instance), which generally does not hold under distribution shifts..."
  - [corpus] "Weak evidence - related papers focus on uncertainty estimation but do not directly confirm conformal failure modes."

### Mechanism 2
- Claim: Selective prediction based on feature-space density can improve calibration by rejecting out-of-distribution inputs.
- Mechanism: By fitting a model to training feature vectors and rejecting inputs with low likelihood, the method filters out inputs where uncertainty estimation would be unreliable.
- Core assumption: Feature space representations are meaningful for detecting distribution shift.
- Evidence anchors:
  - [abstract] "Methods based on the state-of-the-art OOD uncertainty scores perform well relative to other methods, but are also overconfident in many cases."
  - [section] "To compute an uncertainty score κf(x) for a given input x, we extract g(x) and evaluate its likelihood according to the fitted GMM, κf(x) = −GMM(g(x))."
  - [corpus] "Moderate evidence - related work on OOD detection supports feature-space methods but does not confirm their effectiveness for regression calibration."

### Mechanism 3
- Claim: Synthetic datasets show better performance with selective prediction methods compared to real-world datasets.
- Mechanism: Synthetic data likely has clearer boundaries in feature space, making density-based OOD detection more effective.
- Core assumption: Synthetic datasets have more distinct distributional boundaries than real-world data.
- Evidence anchors:
  - [abstract] "Comparing the selective prediction methods, we observe that Gaussian + Selective GMM consistently outperforms Gaussian + Selective Variance..."
  - [section] "We find it interesting that selective prediction based on feature-space density...works almost perfectly in terms of test coverage across the synthetic datasets...but fails to give significant improvements on the real-world datasets."
  - [corpus] "Limited evidence - no corpus papers directly address synthetic vs real-world performance differences."

## Foundational Learning

- Concept: Exchangeability assumption in conformal prediction
  - Why needed here: Understanding why conformal prediction fails under distribution shift requires knowing its theoretical foundation.
  - Quick check question: What assumption must hold for conformal prediction to guarantee marginal coverage?

- Concept: Feature-space density modeling for OOD detection
  - Why needed here: Selective prediction methods rely on fitting density models to feature vectors to detect OOD samples.
  - Quick check question: How does fitting a GMM to training features help identify OOD inputs during prediction?

- Concept: Prediction interval calibration metrics
  - Why needed here: Evaluating uncertainty estimation requires understanding coverage metrics and their interpretation.
  - Quick check question: If a method outputs 90% prediction intervals, what coverage should it achieve on test data to be considered well-calibrated?

## Architecture Onboarding

- Component map: ResNet34 backbone → feature extraction (g(x)) → uncertainty estimation head → selective prediction filter (optional) → prediction interval output
- Critical path: Data preprocessing → model training → calibration on validation set → evaluation on test set
- Design tradeoffs: Ensemble methods provide better uncertainty but require more computation; selective prediction reduces coverage but improves calibration
- Failure signatures: Test coverage significantly below target (e.g., 59% instead of 90%) indicates overconfidence under distribution shift
- First 3 experiments:
  1. Run conformal prediction on Cells dataset to establish baseline performance with no distribution shift
  2. Apply Gaussian + Selective GMM to ChairAngle-Tails to test selective prediction effectiveness
  3. Compare test coverage between synthetic and real-world datasets using the same uncertainty method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do synthetic datasets show much better calibration performance than real-world datasets for feature-space density-based selective prediction methods?
- Basis in paper: [explicit] The authors observe that Gaussian + Selective GMM performs almost perfectly on synthetic datasets but fails to significantly improve test coverage on real-world datasets, particularly on VentricularVolume where prediction rate drops significantly without corresponding coverage improvement.
- Why unresolved: The paper notes this performance difference but does not provide a definitive explanation for why synthetic and real-world data behave differently with the same methodology.
- What evidence would resolve it: Comparative analysis of feature-space distributions between synthetic and real-world datasets, investigation of whether p(x) modeling requires more sophisticated approaches for real-world data, or experiments with different backbone architectures to see if this affects the synthetic/real-world performance gap.

### Open Question 2
- Question: Can more sophisticated models for p(x) in selective prediction close the performance gap between synthetic and real-world datasets?
- Basis in paper: [inferred] The authors suggest exploring more sophisticated models for p(x) within selective prediction as a future direction, noting that the relatively simple feature-space density approaches may be insufficient for real-world data.
- Why unresolved: The paper evaluates only basic GMM and kNN approaches for modeling feature-space density, leaving open whether more advanced density estimation techniques could improve performance on real-world datasets.
- What evidence would resolve it: Experiments comparing GMM/kNN approaches with more advanced density estimation methods (normalizing flows, diffusion models, etc.) across both synthetic and real-world datasets, measuring whether the synthetic/real-world performance gap narrows.

### Open Question 3
- Question: Why does Conformal Prediction consistently outperform other methods specifically on HistologyNucleiPixels despite all methods struggling with calibration on this dataset?
- Basis in paper: [explicit] The authors note that HistologyNucleiPixels is the only dataset where Conformal Prediction clearly obtains the best test coverage, while other methods that output input-dependent length intervals struggle.
- Why unresolved: The paper acknowledges this observation but does not investigate the specific characteristics of this dataset that make fixed-length intervals more effective.
- What evidence would resolve it: Detailed analysis of the relationship between input-dependent interval lengths and prediction errors on HistologyNucleiPixels compared to other datasets, investigation of whether this dataset has particular properties that make fixed-length intervals advantageous.

## Limitations
- The paper does not fully explain the mechanisms behind selective prediction's superior performance on synthetic versus real-world datasets
- Limited investigation into why Conformal Prediction performs exceptionally well on HistologyNucleiPixels compared to other methods
- The benchmark focuses primarily on image-based regression, potentially limiting generalizability to other data types

## Confidence
**High Confidence**: The empirical finding that all evaluated methods become overconfident under distribution shifts is well-supported by the extensive benchmark results across 12 datasets.

**Medium Confidence**: The mechanism explanations for why conformal prediction fails (exchangeability assumption violation) and why selective prediction based on feature density can help are theoretically sound but not empirically validated for each method-dataset combination.

**Low Confidence**: The explanation for why synthetic datasets show better selective prediction performance than real-world datasets relies on limited evidence and lacks direct feature space analysis verification.

## Next Checks
1. **Feature Space Analysis**: Perform t-SNE or UMAP visualization of feature vectors from synthetic vs real-world datasets to empirically verify whether synthetic data exhibits clearer separation in feature space.

2. **Method-Specific Failure Modes**: For each uncertainty estimation method, conduct ablation studies to identify which components contribute most to overconfidence on specific dataset types.

3. **Calibration Stability**: Test whether the observed overconfidence patterns persist across different target coverage levels (e.g., 80%, 95%) to determine if the calibration failures are systematic or coverage-dependent.