---
ver: rpa2
title: The Troubling Emergence of Hallucination in Large Language Models -- An Extensive
  Definition, Quantification, and Prescriptive Remediations
arxiv_id: '2310.04988'
source_url: https://arxiv.org/abs/2310.04988
tags:
- hallucination
- llms
- text
- have
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive study of hallucination in\
  \ large language models (LLMs), introducing detailed categorizations based on orientation,\
  \ category, and degree of severity. The authors define two main orientations\u2014\
  factual mirage and silver lining\u2014each further subdivided into intrinsic and\
  \ extrinsic types with three severity levels."
---

# The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations

## Quick Facts
- arXiv ID: 2310.04988
- Source URL: https://arxiv.org/abs/2310.04988
- Reference count: 22
- Primary result: Introduces HILT dataset and HVI metric to quantify and rank LLM hallucination vulnerability, with proposed mitigation strategies

## Executive Summary
This paper presents a comprehensive study of hallucination in large language models (LLMs), introducing detailed categorizations based on orientation, category, and degree of severity. The authors define two main orientations—factual mirage and silver lining—each further subdivided into intrinsic and extrinsic types with three severity levels. They also identify six hallucination categories including acronym ambiguity, numeric nuisance, and geographic erratum. The study introduces HallucInation eLiciTation (HILT), a dataset of 75,000 samples from 15 contemporary LLMs with human annotations, and proposes Hallucination Vulnerability Index (HVI) as a quantitative measure to rank LLMs based on their hallucination susceptibility. Two mitigation strategies are presented: high entropy word spotting and replacement (ENTROPYBB) and factuality checking (FACTUALITYGB), demonstrating that while complete elimination is challenging, significant reduction is achievable.

## Method Summary
The authors collected 75,000 text samples from 15 contemporary LLMs using prompts from NYTimes tweets (factually correct) and Politifact dataset (factually incorrect). These samples were manually annotated using a three-layer classification system capturing orientation (factual mirage vs silver lining), category (acronym ambiguity, numeric nuisance, generated golem, virtual voice, geographic erratum, time wrap), and degree of severity (mild, moderate, alarming). The Hallucination Vulnerability Index (HVI) was calculated using a formula that incorporates the ratio of hallucinated sentences, types of hallucinations, and damping factors based on mean and standard deviation across LLMs. Two mitigation strategies were developed: ENTROPYBB which replaces high-entropy words from high-HVI LLMs with predictions from lower-HVI LLMs, and FACTUALITYGB which uses textual entailment to flag sentences requiring human review.

## Key Results
- Introduced HILT dataset with 75,000 samples and three-layer human annotations across 15 LLMs
- Proposed HVI metric to quantitatively rank LLM hallucination vulnerability on a 0-100 scale
- Demonstrated that larger LLMs with RLHF training tend to have lower HVI scores
- Showed ENTROPYBB and FACTUALITYGB mitigation strategies can significantly reduce hallucination rates
- Found that factual mirage and silver lining orientations have different vulnerability patterns across LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorizing hallucination into two orientations (Factual Mirage and Silver Lining) with three severity levels allows systematic detection and comparison of LLM vulnerabilities.
- Mechanism: By splitting hallucination into orientations based on whether the prompt is factually correct or incorrect, the model can be evaluated for both consistency and creativity. Severity levels (mild, moderate, alarming) enable granularity in measuring impact.
- Core assumption: Hallucination behavior is orientation-dependent and severity correlates with model vulnerability.
- Evidence anchors:
  - [abstract]: "We define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL)... with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming."
  - [section]: "We introduce two primary orientations of hallucination: (i) Factual Mirage (FM) and (ii) Silver Lining (SL)... further sub-categorized into intrinsic and extrinsic, with three degrees of severity."
- Break condition: If severity levels are not consistently applied across annotators or orientations overlap in a way that prevents clear classification.

### Mechanism 2
- Claim: The Hallucination Vulnerability Index (HVI) provides a comparative metric for ranking LLMs by their hallucination susceptibility.
- Mechanism: HVI aggregates sentence-level hallucination counts, orientation bias, and damping factors to create a normalized 0-100 scale for model comparison.
- Core assumption: Hallucination frequency and orientation are reliable indicators of model vulnerability.
- Evidence anchors:
  - [abstract]: "to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI)."
  - [section]: "HVI is calculated as in Eq. (1): HV Ix = 100... allowing us to recalculate the HVIs for all the LLMs. The resulting HVIs are then ranked and scaled..."
- Break condition: If the damping factors (δ1, δ2) do not adequately differentiate between models or the normalization introduces bias.

### Mechanism 3
- Claim: Entropy-based word replacement (ENTROPYBB) and entailment-based fact-checking (FACTUALITYGB) can mitigate hallucination when used together.
- Mechanism: ENTROPYBB replaces high-entropy words from high-HVI LLMs with predictions from lower-HVI LLMs, while FACTUALITYGB flags sentences needing human review based on textual entailment.
- Core assumption: High-entropy words are more likely to be hallucinated, and entailment scores can reliably detect factual errors.
- Evidence anchors:
  - [section]: "We propose to identify high-entropy points in text generated by an LLM with a high HVI and replace them using an LLM with a lower HVI, yielding desired results."
  - [section]: "We use Google Search API... to search for a given prompt... and retrieve the top 20 documents. Then each sentence of AI-generated text has been validated either into support, refute, or not enough information using RoBERTa Large..."
- Break condition: If high-entropy words are not actually more prone to hallucination or if entailment models fail to distinguish similar entities.

## Foundational Learning

- Concept: Hallucination in LLMs
  - Why needed here: Understanding hallucination is essential to apply the categorization and mitigation strategies in this paper.
  - Quick check question: What is the difference between intrinsic and extrinsic hallucination?

- Concept: Entropy in language models
  - Why needed here: Entropy is used to identify words likely to be hallucinated in the ENTROPYBB mitigation technique.
  - Quick check question: How does entropy relate to the predictability of a word in a given context?

- Concept: Textual entailment
  - Why needed here: FACTUALITYGB uses textual entailment to check the factual accuracy of generated sentences.
  - Quick check question: What are the three possible outcomes of a textual entailment model?

## Architecture Onboarding

- Component map: Prompt generation -> LLM text generation -> Human annotation -> HVI calculation -> Model ranking -> Mitigation application
- Critical path: Prompt generation → LLM text generation → Human annotation → HVI calculation → Model ranking → Mitigation application
- Design tradeoffs: Using multiple LLMs increases coverage but introduces variability; manual annotation is accurate but slow and expensive
- Failure signatures: Inconsistent HVI scores across similar models, high inter-annotator disagreement, or mitigation techniques not reducing hallucination in complex cases
- First 3 experiments:
  1. Generate 1,000 samples using 5 LLMs and manually annotate for orientation, category, and degree.
  2. Calculate HVI for these 5 LLMs and verify ranking matches expectations.
  3. Apply ENTROPYBB to high-HVI samples and measure hallucination reduction via manual review.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different large language models (LLMs) with varying parameter sizes and architectures compare in terms of their hallucination tendencies across different categories and orientations?
- Basis in paper: [explicit] The paper introduces Hallucination Vulnerability Index (HVI) as a comparative spectrum to evaluate and rank LLMs based on their vulnerability to producing hallucinations. It presents HVI scores for 15 contemporary LLMs and discusses patterns observed in the relationship between LLM size and hallucination.
- Why unresolved: While the paper provides HVI scores and discusses general trends, it does not conduct a comprehensive comparative analysis of how specific architectural differences (e.g., flash attention, multi-query attention, SwiGLU activation) impact hallucination tendencies across different categories and orientations.
- What evidence would resolve it: A detailed comparative study analyzing the hallucination patterns of different LLMs with varying architectures, parameter sizes, and training approaches (e.g., with and without RLHF) across all six hallucination categories and two orientations.

### Open Question 2
- Question: What are the most effective combinations of black-box and gray-box mitigation techniques for reducing hallucinations in LLMs?
- Basis in paper: [explicit] The paper proposes two mitigation strategies - high entropy word spotting and replacement (ENTROPYBB) as a black-box approach, and factuality checking using textual entailment (FACTUALITYGB) as a gray-box approach. It presents comparative results showing that each technique is better suited for different types of hallucinations.
- Why unresolved: The paper demonstrates that each mitigation technique has strengths in different hallucination categories, but does not explore optimal combinations or hybrid approaches that could leverage the strengths of both techniques.
- What evidence would resolve it: Systematic evaluation of various combinations of black-box and gray-box mitigation techniques, including sequential application, parallel processing, and weighted approaches, to determine the most effective strategies for different hallucination categories.

### Open Question 3
- Question: How can the Hallucination Vulnerability Index (HVI) be adapted and improved to better capture the severity and impact of different types of hallucinations?
- Basis in paper: [explicit] The paper introduces HVI as a quantifiable measure to rank LLMs based on their hallucination vulnerability, using a formula that considers the ratio of hallucinated sentences, the types of hallucinations (EFM and ESL), and damping factors for comparison.
- Why unresolved: While HVI provides a useful comparative measure, the paper acknowledges limitations in its current formulation, such as not considering variations of intrinsic hallucinations and the subjective interpretation of hallucination degrees. The paper suggests that HVI could be improved and adapted for better assessment.
- What evidence would resolve it: Development and validation of an improved HVI formula that incorporates additional factors such as the severity and impact of different hallucination types, the context and potential consequences of hallucinations, and feedback from domain experts. Testing the improved HVI against real-world scenarios and user studies to assess its effectiveness in capturing the true risk of LLM hallucinations.

## Limitations

- Human annotation introduces subjectivity in classifying hallucination categories and severity degrees
- The HILT dataset may not fully capture rare hallucination types or domain-specific use cases
- Mitigation strategies effectiveness has not been validated at scale or across diverse domains
- The damping factors in HVI calculation may introduce bias or fail to differentiate between similar models

## Confidence

- High confidence: The categorization framework (orientation, category, degree) is logically sound and the HILT dataset construction methodology is clearly specified.
- Medium confidence: The HVI calculation methodology is well-defined, though the damping factors' sensitivity to parameter choices requires further validation.
- Medium confidence: The mitigation strategies show promise in initial experiments, but their effectiveness across different domains and longer text sequences remains to be proven.

## Next Checks

1. Conduct inter-annotator agreement analysis on a subset of HILT samples to quantify annotation consistency and identify potential bias sources.
2. Test the HVI framework on an independent dataset with different prompt sources to verify generalizability and robustness.
3. Evaluate ENTROPYBB and FACTUALITYGB on extended text sequences (1000+ words) and across multiple domains (scientific, legal, technical) to assess scalability and domain-specific performance.