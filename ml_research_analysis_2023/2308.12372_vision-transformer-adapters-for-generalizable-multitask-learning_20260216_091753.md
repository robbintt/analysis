---
ver: rpa2
title: Vision Transformer Adapters for Generalizable Multitask Learning
arxiv_id: '2308.12372'
source_url: https://arxiv.org/abs/2308.12372
tags:
- task
- vision
- transformer
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first vision transformer adapters for
  generalizable multitask learning. The key idea is to learn transferable task affinities
  that can be applied to novel tasks and domains, enabling a single model to solve
  multiple dense vision tasks efficiently.
---

# Vision Transformer Adapters for Generalizable Multitask Learning

## Quick Facts
- arXiv ID: 2308.12372
- Source URL: https://arxiv.org/abs/2308.12372
- Reference count: 40
- Primary result: Introduces vision transformer adapters for generalizable multitask learning that achieve state-of-the-art performance on dense vision tasks while enabling zero-shot task transfer

## Executive Summary
This paper presents the first vision transformer adapters for generalizable multitask learning, introducing a framework that learns transferable task affinities applicable to novel tasks and domains. The approach combines gradient-based task similarities with attention-based mechanisms and task-scaled normalization to enable a single model to efficiently solve multiple dense vision tasks. The adapters can be integrated into any pre-trained vision transformer backbone, achieving state-of-the-art results on semantic segmentation, depth estimation, surface normal prediction, and edge detection.

## Method Summary
The method introduces vision transformer adapters that integrate three key components: Task Representation Optimization Algorithm (TROA) for computing transferable task affinities through gradient similarity, Task-Adapted Attention (TAA) that conditions standard self-attention on these affinities, and Task-Scaled Normalization (TSN) for balancing different task scales. The adapters are integrated into a pre-trained Swin-B V2 backbone and trained using Adam optimizer with a warm-up cosine learning rate schedule. The framework is evaluated on multiple datasets including Taskonomy, NYUDv2, Cityscapes, and Synthia across four dense prediction tasks.

## Key Results
- Achieves state-of-the-art performance on semantic segmentation (mIoU%), depth estimation (RMSE↓), surface normal prediction (mErr.↓), and edge detection (F1%↑)
- Enables zero-shot task transfer and unsupervised domain adaptation without fine-tuning
- Outperforms existing CNN and vision transformer-based multitasking approaches on standard benchmarks
- Maintains parameter efficiency through adapter-based architecture

## Why This Works (Mechanism)

### Mechanism 1: Gradient-based Task Similarity
The Task Representation Optimization Algorithm (TROA) computes transferable task affinities by measuring cosine similarity between task gradients. During each iteration, TROA minimizes a weighted multitask objective where weights are updated based on gradient similarity between tasks. The affinity matrix represents how much influence one task has on another, assuming task affinity can be meaningfully captured by gradient similarity.

### Mechanism 2: Task-adapted Attention
Task-adapted attention (TAA) combines gradient-based affinities with attention-based mechanisms to learn task-specific feature representations. TAA conditions the standard self-attention mechanism on the gradient-based affinity matrix using Feature-wise Linear Modulation (FiLM) to match dimensions, allowing query and key matrices to compute attention from the most similar tasks.

### Mechanism 3: Task-Scaled Normalization
Task-Scaled Normalization (TSN) balances different task scales to prevent learning interference in the multitask framework. TSN modulates normalization output based on task weights, unlike Layer Norm which keeps weights fixed. It scales the task-specific activation by the mean and variance of all inputs within each layer, assuming different tasks have inherently different scales that need to be balanced.

## Foundational Learning

- Concept: Gradient-based task similarity measurement
  - Why needed here: Forms the basis for computing transferable task affinities that can generalize to novel tasks and domains
  - Quick check question: How would you compute the cosine similarity between two task gradients to measure their affinity?

- Concept: Multi-head attention mechanism in transformers
  - Why needed here: Enables the model to learn task-specific feature representations by combining gradient-based affinities with attention-based ones
  - Quick check question: What is the difference between self-attention and task-adapted attention in this context?

- Concept: Parameter-efficient adapter design
  - Why needed here: Allows the model to learn multiple dense vision tasks without adding excessive parameters to the pre-trained backbone
  - Quick check question: How does the bottleneck network in the adapter reduce computational requirements while maintaining performance?

## Architecture Onboarding

- Component map: Pre-trained Swin-B V2 encoder → Vision Transformer Adapters (TROA + TAA + TSN) → Shared transformer decoders with task-specific heads
- Critical path: Input → Encoder → Adapter (TROA computes affinities → TAA conditions attention → TSN balances scales) → Decoder → Task head
- Design tradeoffs: Parameter efficiency vs. performance - using adapters instead of full fine-tuning reduces parameters but may limit task-specific adaptation
- Failure signatures: Poor task affinity learning (gradient similarity doesn't capture task relationships), attention mechanism not conditioning properly on affinities, normalization causing instability
- First 3 experiments:
  1. Verify TROA computes reasonable task affinity matrix on a simple 2-task setup
  2. Test TAA conditioning by comparing with standard self-attention on a single task
  3. Validate TSN balances task scales by training with and without TSN on tasks with known scale differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of vision transformer adapters vary when applied to extremely disparate domains (e.g., training on indoor scenes and testing on "faces" or "animals")?
- Basis in paper: The authors mention that their model does not generalize to extremely disparate domains, as networks trained on indoor scenes from Taskonomy cannot generalize to datasets with 'faces' or 'animals'.
- Why unresolved: While the paper shows generalization to novel domains, it does not provide empirical results on extremely disparate domains.
- What evidence would resolve it: Experiments comparing the performance of vision transformer adapters on extremely disparate domains (e.g., training on indoor scenes and testing on "faces" or "animals") compared to existing methods.

### Open Question 2
- Question: How does the choice of pre-trained initialization (e.g., ImageNet-1K vs. ImageNet-22K) affect the performance of vision transformer adapters on dense prediction tasks?
- Basis in paper: The authors mention using ImageNet-22K-trained weights for their experiments, but do not explore the impact of different pre-trained initializations.
- Why unresolved: The paper does not provide a comparison of the performance of vision transformer adapters using different pre-trained initializations.
- What evidence would resolve it: Experiments comparing the performance of vision transformer adapters using different pre-trained initializations (e.g., ImageNet-1K vs. ImageNet-22K) on dense prediction tasks.

### Open Question 3
- Question: How does the performance of vision transformer adapters compare to existing methods when applied to unsupervised multitask learning scenarios?
- Basis in paper: The authors mention that their current model is trained in a supervised manner and requires paired data, but extending the methodology to an unsupervised paradigm is feasible.
- Why unresolved: The paper does not provide empirical results on unsupervised multitask learning scenarios.
- What evidence would resolve it: Experiments comparing the performance of vision transformer adapters on unsupervised multitask learning scenarios to existing methods.

## Limitations

- The core mechanisms rely heavily on gradient similarity for task affinity computation, which may not capture complex task relationships beyond linear correlation
- The performance claims depend on specific backbone choices (Swin-B V2) that may not generalize to other architectures
- The zero-shot generalization results lack detailed ablation studies to isolate the contribution of each adapter component

## Confidence

- High confidence: The adapter architecture design and parameter efficiency claims (based on well-established ViT adapter principles)
- Medium confidence: The TROA gradient similarity mechanism (supported by theory but limited empirical validation of assumptions)
- Low confidence: The zero-shot task transfer generalization results (based on few experimental conditions, limited ablation studies)

## Next Checks

1. Perform controlled ablation studies removing TAA and TSN components separately to quantify their individual contributions to performance gains
2. Test gradient similarity correlation with actual task performance when trained jointly on synthetic task pairs with known relationships
3. Evaluate zero-shot transfer performance across different backbone architectures (e.g., ConvNeXt, ConvMixer) to assess architectural generalization of the learned affinities