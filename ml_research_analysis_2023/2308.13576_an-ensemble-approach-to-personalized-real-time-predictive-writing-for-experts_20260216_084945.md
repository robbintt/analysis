---
ver: rpa2
title: An Ensemble Approach to Personalized Real Time Predictive Writing for Experts
arxiv_id: '2308.13576'
source_url: https://arxiv.org/abs/2308.13576
tags:
- language
- text
- gpt2
- word
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an ensemble approach combining a global GPT-2
  language model with local personalized Markov models for real-time predictive writing
  assistance for Intuit experts. The system provides word and sentence auto-completion
  suggestions based on the user's writing style and domain-specific data.
---

# An Ensemble Approach to Personalized Real Time Predictive Writing for Experts

## Quick Facts
- arXiv ID: 2308.13576
- Source URL: https://arxiv.org/abs/2308.13576
- Reference count: 32
- One-line primary result: 39.1% exact match rate and 14.01% effort saved for word-level suggestions

## Executive Summary
This paper presents a personalized real-time predictive writing system for Intuit experts that combines a fine-tuned GPT-2 language model with user-specific Markov models. The system provides word and sentence auto-completion suggestions by interpolating probabilities from both global and local models. Character-level completion is enabled through caching of word-level suggestions. The approach has been deployed in production, saving over a million keystrokes for Intuit experts.

## Method Summary
The system uses an ensemble approach combining a global GPT-2 model (fine-tuned on domain-specific financial data) with personalized k-th order Markov models trained on individual user data. Suggestions are generated by interpolating probabilities from both models using a weighted parameter (alpha=0.6). A character-level LSTM model provides additional suggestions by matching typed prefixes against cached word-level predictions. The system is deployed on AWS SageMaker with GPU instances for inference.

## Key Results
- 39.1% exact match rate for word-level suggestions
- 14.01% effort saved for word-level suggestions
- Character-level completion improves effort saved by 50% compared to word-level only

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining a global pre-trained GPT-2 model with personalized k-th order Markov models improves prediction accuracy for domain-specific writing.
- Mechanism: The global model captures general language patterns, while local Markov models capture user-specific style and frequently used phrases. The ensemble interpolates their probabilities to balance general and personalized suggestions.
- Core assumption: Both global and local models can contribute useful predictive information, and their combination improves overall performance compared to either alone.
- Evidence anchors:
  - [abstract] "Our proposed system is not only efficient and personalized but also robust as it leverages multiple machine learning techniques along with transfer learning approach to fine tune large language model with Intuit specific data."
  - [section] "From the global model we will get a list of suggestions with assigned probabilities and from the local model also we will get a list of suggestions with assigned probabilities. We then combine these two lists with a particular weight to get the final probabilities of each word in the vocabulary."
  - [corpus] Weak evidence - no direct corpus mention of Markov models or ensemble approaches.
- Break condition: If user-specific data is insufficient to train reliable Markov models, or if global model predictions are consistently better than local ones, the ensemble may degrade performance.

### Mechanism 2
- Claim: Caching suggestions from the word-level ensemble model enables low-latency character-level auto-completion.
- Mechanism: The system performs expensive model inference only after each word, caching top suggestions. When users type characters, the system performs a fast prefix match against cached suggestions, presenting matching ones immediately.
- Core assumption: Users typically type characters that match previously suggested words, making cached suggestions useful for character-level completion without additional model calls.
- Evidence anchors:
  - [section] "Performing model inference after every character can be very costly, especially in a real time latency sensitive task like text auto-completion. To handle such scenario, we cache suggestions from word level ensemble model at the UI client side."
  - [corpus] Weak evidence - no direct corpus mention of caching strategies for autocomplete systems.
- Break condition: If users type characters that don't match any cached suggestions, or if cache size becomes too large for efficient prefix matching, this approach becomes ineffective.

### Mechanism 3
- Claim: Fine-tuning a pre-trained GPT-2 model on domain-specific financial data improves performance while maintaining efficiency.
- Mechanism: Transfer learning allows the model to leverage pre-existing language understanding while adapting to domain-specific terminology and patterns, reducing training time and data requirements compared to training from scratch.
- Core assumption: The pre-trained GPT-2 model has learned general language patterns that can be effectively adapted to the financial domain with relatively small amounts of domain-specific data.
- Evidence anchors:
  - [abstract] "Our proposed system is not only efficient and personalized but also robust as it leverages multiple machine learning techniques along with transfer learning approach to fine tune large language model with Intuit specific data."
  - [section] "We started with pre-trained GPT2 [20] tokenizer and model, and then, fine-tuned the model on our financial data using transfer learning."
  - [corpus] Weak evidence - no direct corpus mention of transfer learning effectiveness for language models.
- Break condition: If domain-specific patterns are too different from general language patterns, or if insufficient domain data is available for effective fine-tuning, this approach may not improve performance.

## Foundational Learning

- Concept: Language modeling and probability estimation
  - Why needed here: The entire system relies on predicting the probability of the next word given context, which requires understanding language modeling fundamentals
  - Quick check question: How does a k-th order Markov model estimate the probability of the next word given k previous words?

- Concept: Transfer learning in neural networks
  - Why needed here: The system uses a pre-trained GPT-2 model and fine-tunes it on domain-specific data, requiring understanding of transfer learning principles
  - Quick check question: What are the key differences between training a language model from scratch versus fine-tuning a pre-trained model?

- Concept: Ensemble methods and interpolation
  - Why needed here: The system combines predictions from multiple models using weighted interpolation, requiring understanding of ensemble techniques
  - Quick check question: How does the choice of alpha parameter affect the balance between global and local model predictions in an ensemble system?

## Architecture Onboarding

- Component map: Text input -> Pre-processing (decontraction, date removal, normalization) -> Word-level ensemble prediction (GPT-2 + Markov models) -> Cache suggestions -> Character-level prediction (LSTM + prefix matching) -> Present suggestions to user

- Critical path: Text input → Pre-processing → Word-level ensemble prediction → Cache suggestions → Character-level prediction (if needed) → Present suggestions to user

- Design tradeoffs:
  - Global vs. local model balance: Higher alpha favors general patterns but may miss user-specific terms
  - Cache size vs. latency: Larger caches improve character-level suggestions but increase memory usage and prefix matching time
  - Model complexity vs. inference speed: More sophisticated models may improve accuracy but exceed latency constraints

- Failure signatures:
  - High cache miss rate at character level indicates poor word-level suggestion coverage
  - Consistent mismatch between model suggestions and actual user input suggests alpha parameter needs adjustment
  - Degraded performance with specific user groups may indicate insufficient personalization data

- First 3 experiments:
  1. Test ensemble approach with different alpha values (0.2, 0.4, 0.6, 0.8) to find optimal balance between global and local models
  2. Measure cache hit rate for character-level predictions to validate the caching strategy effectiveness
  3. Compare performance of fine-tuned GPT-2 vs. pre-trained GPT-2 to quantify the benefit of domain adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ensemble approach compare to a more complex neural architecture like GPT-3 or GPT-4 in terms of exact match rate and effort saved, while maintaining real-time latency?
- Basis in paper: [explicit] The paper discusses the trade-off between model complexity and latency, choosing GPT-2 due to its 102ms P99 inference latency. It also mentions that the proposed system is modular and can be easily swapped with other global language models.
- Why unresolved: The paper does not experiment with more complex models like GPT-3 or GPT-4, leaving the potential performance gain from these models unknown.
- What evidence would resolve it: Experimenting with GPT-3 or GPT-4 and comparing their exact match rate, effort saved, and latency against the proposed ensemble approach.

### Open Question 2
- Question: How does the proposed ensemble approach perform on data from different domains or industries, such as healthcare or legal, where the writing style and vocabulary may be significantly different from financial experts?
- Basis in paper: [inferred] The paper focuses on the financial domain and uses domain-specific data for fine-tuning. However, it does not explore the generalizability of the approach to other domains.
- Why unresolved: The paper does not provide any evidence of the model's performance on data from different domains or industries.
- What evidence would resolve it: Evaluating the model's performance on data from different domains or industries and comparing the results to the financial domain.

### Open Question 3
- Question: How does the proposed ensemble approach handle rare or unusual phrases that are not present in the training data, and what is the impact on the exact match rate and effort saved?
- Basis in paper: [explicit] The paper mentions that the proposed system is robust and can provide relevant auto-complete suggestions even in cases of rare or unusual phrases, leveraging multiple machine learning techniques and transfer learning.
- Why unresolved: The paper does not provide any specific examples or quantitative results on how the model handles rare or unusual phrases.
- What evidence would resolve it: Analyzing the model's performance on a dataset containing rare or unusual phrases and measuring the exact match rate and effort saved for these cases.

## Limitations
- Limited comparative analysis against established baselines like standard GPT-2 or commercial autocomplete systems
- No analysis of personalization limits with insufficient user data or new users
- Cache effectiveness not quantified through cache hit rate measurements

## Confidence

**Confidence: Low** - The paper reports promising real-world performance metrics (39.1% exact match rate, 14.01% effort saved) but lacks rigorous comparative analysis. The system's performance is evaluated only against its own variants (word-level vs. character-level, different alpha values) rather than against established baselines like standard GPT-2 or other autocomplete systems. Without such comparisons, it's unclear whether the ensemble approach provides meaningful improvements over existing solutions.

**Confidence: Medium** - The personalization aspect relies heavily on Markov models trained on individual user data. While this captures user-specific writing patterns, the paper doesn't address scenarios with insufficient user data (new users or infrequent writers). The system's robustness in these edge cases remains unknown. Additionally, the computational overhead of maintaining personalized models for each user at scale is not discussed.

**Confidence: Medium** - The caching strategy for character-level completion assumes users will type characters matching cached word-level suggestions. However, the paper doesn't quantify cache hit rates or analyze scenarios where this assumption breaks down. If users frequently type unexpected character sequences, the character-level model's effectiveness may be significantly reduced, though this impact isn't measured.

## Next Checks
1. **Benchmark against established baselines**: Compare the ensemble system's performance against standard GPT-2, n-gram models, and commercial autocomplete systems using the same evaluation metrics (exact match rate, effort saved, coverage) on the Intuit dataset.

2. **Analyze personalization limits**: Conduct experiments with varying amounts of user-specific training data to determine the minimum data required for effective personalization, and measure performance degradation when user data is sparse or unavailable.

3. **Measure caching effectiveness**: Track cache hit rates and character-level prediction accuracy across different user segments and writing patterns to quantify how often the caching strategy provides value versus when it fails.