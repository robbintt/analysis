---
ver: rpa2
title: LIPEx-Locally Interpretable Probabilistic Explanations-To Look Beyond The True
  Class
arxiv_id: '2310.04856'
source_url: https://arxiv.org/abs/2310.04856
tags:
- lipex
- data
- lime
- features
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LIPEx, a perturbation-based multi-class explanation
  framework that outputs a matrix explaining how each feature impacts the probability
  distribution over all possible classes. Unlike existing methods that focus only
  on the predicted class, LIPEx provides feature importance insights for every class.
---

# LIPEx-Locally Interpretable Probabilistic Explanations-To Look Beyond The True Class

## Quick Facts
- arXiv ID: 2310.04856
- Source URL: https://arxiv.org/abs/2310.04856
- Reference count: 40
- Primary result: LIPEx is a multi-class explanation framework that outputs a matrix explaining how each feature impacts probability distribution over all classes, achieving near-zero TV distance from complex model outputs while being more data-efficient than LIME

## Executive Summary
LIPEx introduces a novel perturbation-based multi-class explanation framework that extends beyond traditional XAI methods focused solely on the predicted class. By minimizing Hellinger distance between the complex model's probability distribution and a linear explainer's output, LIPEx generates explanation matrices showing feature importance for all classes. The method demonstrates superior performance in matching complex model distributions, requires fewer perturbations for stable explanations, and guides more effective feature removal compared to existing methods.

## Method Summary
LIPEx is a perturbation-based multi-class explanation framework that generates explanation matrices through regression minimizing Hellinger distance. The method perturbs input data, selects important features via forward selection, computes embeddings, and optimizes an explanation matrix that best approximates the complex model's probability distribution across all classes. This approach captures feature importance for every class rather than just the predicted class, enabling more comprehensive explanations.

## Key Results
- LIPEx matches complex model's output distribution with TV distance near 0 across hundreds of test instances
- LIPEx is more data-efficient than LIME, requiring fewer perturbations for stable feature importance rankings
- Ablation tests show LIPEx-guided feature removal causes more significant prediction changes than other XAI methods
- LIPEx computes explanations approximately 53% faster than comparable methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LIPEx accurately replicates the probability distribution over classes predicted by a complex model
- Mechanism: LIPEx defines explanations as matrices obtained via regression with respect to Hellinger distance in the space of probability distributions
- Core assumption: Hellinger distance is an effective metric for comparing probability distributions in classification tasks
- Evidence anchors: TV distance between LIPEx and complex model outputs is near 0; weak related corpus evidence

### Mechanism 2
- Claim: LIPEx is more data-efficient than LIME
- Mechanism: LIPEx's matrix formulation provides stable feature importance rankings even with few perturbations
- Core assumption: Matrix structure captures more stable feature-class relationships than LIME's vector approach
- Evidence anchors: Figure 4 shows LIPEx feature stability with fewer perturbations; weak related corpus evidence

### Mechanism 3
- Claim: LIPEx-guided feature removal causes more significant prediction changes
- Mechanism: LIPEx matrix models feature importance for every class, enabling precise identification of critical features
- Core assumption: Features important for multiple classes are more critical to model decisions
- Evidence anchors: Ablation studies show LIPEx-guided removal causes more prediction changes; weak related corpus evidence

## Foundational Learning

- Concept: Hellinger distance as a metric for probability distributions
  - Why needed here: LIPEx uses Hellinger distance to measure difference between complex model and explainer outputs
  - Quick check question: What properties make Hellinger distance suitable for comparing probability distributions in classification tasks?

- Concept: Local explanation frameworks and perturbation-based methods
  - Why needed here: Understanding LIPEx's approach to perturbations and feature selection compared to LIME
  - Quick check question: How does LIPEx's perturbation methodology differ from LIME's approach?

- Concept: Multi-class classification and probability distributions over classes
  - Why needed here: LIPEx provides explanations for feature importance across all classes
  - Quick check question: Why is multi-class feature importance important for comprehensive explanations?

## Architecture Onboarding

- Component map: Complex model -> Perturbation generator -> Feature selector -> Explanation matrix (LIPEx) -> Hellinger distance calculator -> Linear regression optimizer
- Critical path: 1) Generate perturbations around input data, 2) Apply feature selection, 3) Compute embeddings, 4) Calculate Hellinger distance, 5) Optimize explanation matrix, 6) Extract feature importance
- Design tradeoffs: Hellinger distance vs other metrics, perturbation count vs efficiency, forward selection vs other methods, matrix size vs interpretability
- Failure signatures: High TV distance from complex model, unstable feature rankings, poor ablation study performance, long computation times
- First 3 experiments: 1) Replicate Figure 2 to verify TV distance near 0, 2) Test data efficiency comparing LIPEx and LIME perturbation stability, 3) Validate ablation study performance on new dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of Hellinger distance as loss function affect explanation quality compared to other probability metrics?
- Basis in paper: Explicit mention of Hellinger distance choice and comparison with TV distance underperformance
- Why unresolved: Paper shows Hellinger works well but lacks comprehensive comparison with other metrics
- What evidence would resolve it: Experiments comparing LIPEx with different probability metrics and analyzing explanation quality

### Open Question 2
- Question: How does the number of features selected by forward feature selection impact explanation quality and stability?
- Basis in paper: Explicit mention of using top-3 features per class with fixed feature count
- Why unresolved: Paper doesn't investigate varying the number of selected features
- What evidence would resolve it: Experiments with different feature counts analyzing explanation quality and stability

### Open Question 3
- Question: How does the choice of embedding function affect LIPEx explanation quality for different data types?
- Basis in paper: Explicit mention of using pre-chosen embedding functions like BERT for text
- Why unresolved: Paper doesn't investigate different embedding functions for various data types
- What evidence would resolve it: Experiments with different embedding functions analyzing explanation quality across data types

## Limitations

- Perturbation-based approach may struggle with very high-dimensional data or complex decision boundaries
- Forward feature selection might miss important feature interactions
- Computational overhead of Hellinger distance optimization could limit scalability

## Confidence

- Distribution matching capability: High - supported by consistent TV distance near 0 results
- Data efficiency advantage: Medium - promising results but limited comparison scope
- Ablation study performance: Medium - strong results but potentially dataset-dependent

## Next Checks

1. Test LIPEx's performance on additional complex datasets (medical imaging, genomics) to assess generalizability
2. Compare LIPEx's computational efficiency against LIME on large-scale datasets
3. Conduct ablation studies with different feature removal strategies to validate robustness of LIPEx's feature importance rankings