---
ver: rpa2
title: 'State2Explanation: Concept-Based Explanations to Benefit Agent Learning and
  User Understanding'
arxiv_id: '2309.12482'
source_url: https://arxiv.org/abs/2309.12482
tags:
- agent
- lander
- explanations
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unified framework, State2Explanation (S2E),
  that learns a joint embedding model between agent state-action pairs and concept-based
  explanations, and leverages such learned model to both (1) inform reward shaping
  during an agent's training, and (2) provide explanations to end-users at deployment
  for improved task performance. The S2E framework involves learning a joint embedding
  model to align agent state-action pairs to concept-based explanations, and then
  leveraging the learned model to inform reward shaping during agent training and
  provide explanations to end-users at deployment.
---

# State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding

## Quick Facts
- arXiv ID: 2309.12482
- Source URL: https://arxiv.org/abs/2309.12482
- Authors: 
- Reference count: 40
- The S2E framework successfully improves both agent learning rates through automatic reward shaping and end-user task performance through concept-based explanations in Connect 4 and Lunar Lander domains.

## Executive Summary
This paper introduces State2Explanation (S2E), a unified framework that learns a joint embedding model between agent state-action pairs and concept-based explanations. The framework leverages this learned model to inform reward shaping during agent training and provide explanations to end-users at deployment. The authors demonstrate that S2E achieves dual benefits: improving agent learning rates through automatic reward shaping (Recall@1 of 86.1% for Connect 4 and 100% for Lunar Lander) and significantly improving end-user task performance through concept-based explanations. The framework includes abstraction methods (Information Filtering and Temporal Grouping) for high-frequency domains like Lunar Lander.

## Method Summary
The S2E framework learns a joint embedding model that aligns agent state-action pairs with concept-based explanations in a shared embedding space. During training, this alignment enables automatic reward shaping by retrieving relevant concept explanations that indicate goal-relevant transitions. During deployment, the same model generates concept-based explanations for end-users, with additional abstraction methods applied for high-frequency domains. The framework uses contrastive loss to train the joint embedding models (MC4 for Connect 4, MLL for Lunar Lander) on datasets of state-action pairs paired with expert-defined concepts. The framework is integrated with the MuZero RL algorithm and evaluated through both model performance metrics and user studies.

## Key Results
- The joint embedding model achieves Recall@1 of 86.1% for Connect 4 and 100% for Lunar Lander in explanation retrieval
- Concept-based explanations significantly improve user task performance in Connect 4
- Abstraction methods (Information Filtering and Temporal Grouping) are important for producing effective explanations in high-frequency domains like Lunar Lander
- S2E informs reward shaping comparable to expert-defined dense reward functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning a joint embedding model between state-action pairs and concept-based explanations can improve RL agent learning rate through automatic reward shaping.
- Mechanism: The joint embedding model aligns state-action pairs with their associated concept-based explanations in a shared embedding space. During training, this alignment allows the agent to automatically identify when intermediate rewards should be provided based on meaningful concept explanations, reducing the need for manual reward engineering.
- Core assumption: Concept-based explanations capture high-level, goal-relevant knowledge that correlates with positive or negative influence on the agent's objective, making them suitable for reward shaping.
- Evidence anchors:
  - [abstract] "learns a joint embedding model between agent state-action pairs and concept-based explanations, and leveraging such learned model to both (1) inform reward shaping during agent training"
  - [section 5.3] "demonstrate that both MC4 and MLL inform reward shaping comparable to expert-defined dense, reward functions"
  - [corpus] Weak evidence - related work focuses on reward shaping via language but doesn't validate joint embedding approaches specifically
- Break condition: If the learned embedding space fails to capture meaningful relationships between states and concepts, or if concept-based explanations don't reliably indicate goal-relevant transitions, the reward shaping would become ineffective or misleading.

### Mechanism 2
- Claim: Providing concept-based explanations to end-users improves their task performance through better understanding of agent decision-making.
- Mechanism: By presenting natural language explanations that capture high-level concepts relevant to the task goal, users gain insight into why the agent takes specific actions. This improved understanding enables users to learn from the agent's behavior and improve their own performance.
- Core assumption: Users can map natural language concept explanations to actionable insights that improve their task performance, and that these explanations are comprehensible to non-AI experts.
- Evidence anchors:
  - [abstract] "provide explanations to end-users at deployment for improved task performance"
  - [section 6.1] "concept-based explanations can significantly improve user task performance (Connect 4)"
  - [corpus] Weak evidence - related work validates concept-based explanations for classification but limited validation for sequential decision making
- Break condition: If explanations are too complex, too frequent, or not aligned with users' mental models of the task, they may confuse rather than help users, potentially degrading performance.

### Mechanism 3
- Claim: The dual benefit of concept-based explanations (to both agent and user) is achieved through a unified framework that learns a single joint embedding model.
- Mechanism: The same joint embedding model serves two purposes - informing reward shaping during training and generating explanations during deployment. This creates a unified representation that captures both the agent's perspective and the user's need for understanding.
- Core assumption: The embedding space learned during training captures information that remains relevant and interpretable during deployment, and that the same concepts useful for reward shaping are also meaningful to users.
- Evidence anchors:
  - [abstract] "unified framework, State2Explanation (S2E), that involves learning a joint embedding model between agent state-action pairs and concept-based explanations"
  - [section 4] "learns a single joint embedding model between state-action pairs and concept-based explanations and leverages such learned model to both (1) inform reward shaping during agent training, and (2) provide explanations to end-users at deployment"
  - [corpus] Weak evidence - related work typically separates agent learning and explanation generation rather than unifying them
- Break condition: If the joint embedding model overfits to training conditions or if the concepts that drive agent learning don't translate to user understanding, the dual benefit would not materialize.

## Foundational Learning

- Concept: Joint embedding models for multimodal alignment
  - Why needed here: The framework requires mapping between agent state representations and human-understandable explanations, which is fundamentally a multimodal alignment problem
  - Quick check question: What type of loss function is typically used to train joint embedding models that align two different modalities?

- Concept: Concept-based explanations in sequential decision making
  - Why needed here: The framework defines specific desiderata for what constitutes a "concept" in sequential settings, distinguishing it from classification-based approaches
  - Quick check question: According to the paper's desiderata, what key characteristic must a concept have to be valid in sequential decision making?

- Concept: Reward shaping in reinforcement learning
  - Why needed here: The framework leverages learned concepts to inform when and how to provide intermediate rewards during agent training
  - Quick check question: What is the primary benefit of reward shaping compared to sparse reward functions in reinforcement learning?

## Architecture Onboarding

- Component map:
  - Joint Embedding Model (M) -> Reward Shaping Module -> RL Agent
  - Joint Embedding Model (M) -> Explanation Abstraction Module -> User Interface
  - Concept Collection -> Joint Embedding Model (M)

- Critical path:
  1. Collect aligned dataset of state-action pairs with expert-defined concepts
  2. Train joint embedding model using contrastive loss
  3. Validate explanation retrieval accuracy (Recall@1)
  4. Integrate with RL agent for reward shaping
  5. Deploy with explanation generation for users

- Design tradeoffs:
  - Fixed templates vs. dynamic generation: The framework uses fixed templates for explanation generation, trading flexibility for reliability and simplicity
  - Manual vs. automated concept definition: Concepts are manually defined by experts, ensuring quality but limiting scalability
  - Separate vs. unified models: The framework learns a single joint embedding model for both reward shaping and explanation generation, reducing complexity but potentially limiting specialization

- Failure signatures:
  - Low Recall@1 in explanation retrieval indicates poor embedding alignment
  - Agent performance plateaus despite reward shaping suggests concepts aren't driving meaningful behavior
  - Users show no improvement in task performance indicates explanations aren't comprehensible or actionable

- First 3 experiments:
  1. Train joint embedding model on Connect 4 dataset and measure Recall@1 on held-out test set
  2. Integrate with MuZero agent and compare learning curves with/without reward shaping
  3. Conduct user study comparing concept-based explanations vs. action-only explanations for task performance improvement

## Open Questions the Paper Calls Out
- The paper suggests exploring whether the joint embedding model M can also inform the shaping values for each concept, rather than relying on expert-defined values or hyperparameter sweeps
- The paper raises the question of whether the S2E framework can be generalized to work with a single joint embedding model applicable across multiple domains, rather than requiring separate models for each domain
- The paper suggests that the information filtering (InF) method can be automated in future work, rather than relying on qualitative analysis and expert-defined thresholds

## Limitations
- The framework relies on manually defined domain concepts, limiting scalability to new tasks without expert knowledge engineering
- User studies conducted with small sample sizes (8 participants for Connect 4, 11 for Lunar Lander), reducing generalizability
- Limited ablation studies on abstraction methods, making it unclear which components drive performance improvements

## Confidence
- High confidence in the joint embedding model's retrieval performance (validated with strong Recall@1 metrics)
- Medium confidence in reward shaping effectiveness (shown through learning curves but not directly compared to alternative reward shaping methods)
- Low confidence in user performance improvements (based on small user studies with limited controls)

## Next Checks
1. Conduct larger-scale user studies (n > 30) with proper control conditions to validate the impact of concept-based explanations on task performance
2. Perform ablation studies comparing the unified joint embedding model against separate models for reward shaping and explanation generation
3. Test the framework's transferability by applying it to a third domain without manual concept engineering, evaluating both retrieval performance and downstream task outcomes