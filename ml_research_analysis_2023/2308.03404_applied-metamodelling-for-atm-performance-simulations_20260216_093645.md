---
ver: rpa2
title: Applied metamodelling for ATM performance simulations
arxiv_id: '2308.03404'
source_url: https://arxiv.org/abs/2308.03404
tags:
- learning
- delay
- time
- active
- simulations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XALM (eXplainable Active Learning Metamodel),
  a three-step framework integrating active learning and SHAP values into simulation
  metamodels to support ATM decision-making. The framework efficiently uncovers hidden
  relationships among input and output variables in ATM simulators, which are of interest
  in policy analysis.
---

# Applied metamodelling for ATM performance simulations

## Quick Facts
- arXiv ID: 2308.03404
- Source URL: https://arxiv.org/abs/2308.03404
- Reference count: 40
- Primary result: XALM achieves XGBoost-level predictive performance using fewer simulations while providing superior explainability

## Executive Summary
This paper introduces XALM (eXplainable Active Learning Metamodel), a three-step framework that integrates active learning and SHAP values into simulation metamodels for ATM decision-making. The framework efficiently uncovers hidden relationships among input and output variables in ATM simulators, which are critical for policy analysis. Experiments demonstrate that XALM's predictive performance matches XGBoost metamodels while using fewer simulations, and exhibits superior explanatory capabilities compared to non-active learning approaches. The method was applied to a real-world scenario at Paris Charles de Gaulle airport, analyzing six variables to extend an arrival manager's range and scope.

## Method Summary
XALM combines Gaussian Process metamodels with active learning query strategies and SHAP-based explainability. The framework starts with an initial random training set of 10 simulations, then iteratively selects the most informative points based on GP epistemic uncertainty until a stopping criterion is met. The GP uses an ARD RBF kernel with automatic relevance determination to capture input importance. SHAP values are computed using a KernelExplainer to provide consistent feature attributions. The method also introduces simulation reuse across multiple KPIs to reduce computational burden.

## Key Results
- XALM achieves predictive performance comparable to XGBoost metamodels using fewer simulations
- The framework demonstrates superior explanatory capabilities through SHAP values compared to non-active learning metamodels
- Simulation reuse across KPIs reduces overall computational cost by eliminating redundant simulator runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning with GPs outperforms random sampling by selecting the most informative simulation points
- Mechanism: The GP model computes epistemic uncertainty for each unlabelled input; the query function selects points with highest uncertainty, thus reducing prediction error faster than random selection
- Core assumption: Uncertainty estimates from the GP reliably reflect prediction error reduction potential
- Evidence anchors:
  - [abstract] "Our experiments show XALM's predictive performance comparable to the XGBoost metamodel with fewer simulations"
  - [section] "Active learning aims to select the most informative data points sequentially to improve model prediction performance and control costs"
  - [corpus] Weak: no direct corpus evidence, but the active learning literature cited (Settles, 2010) supports the claim
- Break condition: If GP epistemic uncertainty is poorly calibrated, the active learning selection may become inefficient

### Mechanism 2
- Claim: SHAP values provide consistent and interpretable explanations of variable importance
- Mechanism: SHAP values are computed using a KernelExplainer that assigns feature attributions consistent with the model's predictions, enabling comparison across models and iterations
- Core assumption: The explanation model's additivity and consistency properties hold for the GP predictions
- Evidence anchors:
  - [abstract] "XALM exhibits superior explanatory capabilities compared to non-active learning metamodels"
  - [section] "SHAP values originate from game theory and have lately been used in machine learning to enhance model explainability"
  - [corpus] Weak: corpus shows SHAP usage in similar contexts but no ATM-specific SHAP studies
- Break condition: If the model is highly nonlinear or contains strong interactions, SHAP attributions may be misleading

### Mechanism 3
- Claim: Reusing simulations across multiple KPIs reduces total computational cost
- Mechanism: The same input space is queried once for a primary KPI, and the resulting labelled data is reused to fit metamodels for other KPIs without additional simulations
- Core assumption: The input variables affect all KPIs similarly enough that the same simulations are informative for each
- Evidence anchors:
  - [abstract] "we show how the simulations used for the metamodel can be reused across key performance indicators, thus decreasing the overall number of simulations needed"
  - [section] "Often, a simulator outputs multiple KPIs at once, and thus for each simulation input variable vector x, there are multiple outputs y"
  - [corpus] No corpus evidence directly supporting reuse across KPIs
- Break condition: If KPIs have disjoint or very different input sensitivities, reuse may degrade performance

## Foundational Learning

- Gaussian Processes
  - Why needed here: GPs provide both predictions and uncertainty estimates needed for active learning and confidence intervals
  - Quick check question: Can you write the GP posterior mean and covariance formulas given training data?

- Active Learning Query Strategies
  - Why needed here: Efficient selection of simulation points reduces the number of costly simulator runs
  - Quick check question: How does uncertainty sampling choose the next point in active learning?

- SHAP Explainability
  - Why needed here: Provides interpretable feature importance consistent across models
  - Quick check question: What does the additivity property of SHAP values guarantee?

## Architecture Onboarding

- Component map: Mercury simulator -> Active learning loop -> GP metamodel -> SHAP explainer -> Performance evaluation
- Critical path: Simulate -> Train GP -> Compute epistemic uncertainty -> Select next point -> Repeat until stopping criterion
- Design tradeoffs:
  - GP vs XGBoost: GP offers uncertainty estimates but is slower to train; XGBoost is faster but lacks epistemic uncertainty
  - Number of simulations: Fewer simulations reduce cost but may hurt accuracy; active learning mitigates this
  - SHAP explainer choice: KernelExplainer works for GP but is slower than TreeExplainer
- Failure signatures:
  - Stagnant RMSE despite more simulations -> poor uncertainty estimates or bad query strategy
  - Wide confidence intervals across all predictions -> model uncertainty or noisy simulator
  - SHAP values inconsistent across iterations -> explanation model instability
- First 3 experiments:
  1. Compare GP with 30, 100, 1000 simulations to XGBoost baseline on RMSE
  2. Compare active vs passive learning curves for RMSE and SHAP RMSE
  3. Test stopping criterion based on epistemic uncertainty and measure savings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does XALM's performance compare to other state-of-the-art metamodeling approaches beyond XGBoost, such as neural network-based methods or other tree ensemble techniques?
- Basis in paper: [inferred] The paper only compares XALM to XGBoost and passive learning approaches, but does not explore other potential metamodeling methods
- Why unresolved: The authors did not conduct experiments comparing XALM to a broader range of metamodeling techniques, limiting the understanding of XALM's relative performance
- What evidence would resolve it: Experiments comparing XALM's predictive and explanatory performance to other metamodeling approaches like neural networks, random forests, or other gradient boosting methods on the same ATM simulation datasets

### Open Question 2
- Question: What is the impact of different active learning query strategies (e.g., query-by-committee, variance reduction) on XALM's performance compared to the uncertainty sampling strategy used in the paper?
- Basis in paper: [explicit] The paper only uses uncertainty sampling as the active learning strategy, but mentions other strategies like query-by-committee and variance reduction in the background section
- Why unresolved: The authors did not explore the effects of different active learning strategies on XALM's performance, leaving the optimal strategy for ATM simulation metamodeling unclear
- What evidence would resolve it: Experiments comparing XALM's performance using different active learning query strategies on the same ATM simulation datasets, measuring predictive accuracy, computational efficiency, and SHAP value quality

### Open Question 3
- Question: How does XALM's performance scale with increasing input dimensionality and complexity of the ATM simulation model?
- Basis in paper: [inferred] The paper demonstrates XALM on a specific ATM scenario with 6 input variables, but does not explore how its performance changes with more complex models or higher-dimensional input spaces
- Why unresolved: The authors did not conduct experiments varying the complexity of the simulation model or the number of input variables, leaving the scalability of XALM unclear
- What evidence would resolve it: Experiments applying XALM to ATM simulation models with varying numbers of input variables (e.g., 10, 20, 50) and increasing complexity, measuring predictive performance, computational efficiency, and SHAP value quality

## Limitations
- Performance depends on quality of GP uncertainty estimates, which may degrade for highly nonlinear or discontinuous simulator responses
- Simulation reuse across KPIs assumes similar input sensitivities, which may not hold for all performance indicators
- Stopping criterion based on epistemic uncertainty requires careful threshold selection and may not generalize well across different simulator domains

## Confidence

- Predictive performance claims: Medium
- Explainability claims: Medium
- Computational efficiency claims: Medium-High

## Next Checks

1. **Uncertainty Calibration Test**: Evaluate whether GP epistemic uncertainty estimates accurately predict actual prediction errors across different simulator regimes to verify the active learning selection mechanism

2. **KPI Sensitivity Analysis**: Systematically assess the impact of simulation reuse across KPIs by measuring performance degradation when KPIs have different input sensitivities

3. **Cross-Validation Stability**: Test the framework's robustness by conducting multiple runs with different random seeds and measuring variance in both predictive performance and SHAP explanations