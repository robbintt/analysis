---
ver: rpa2
title: Rigorously Assessing Natural Language Explanations of Neurons
arxiv_id: '2309.10312'
source_url: https://arxiv.org/abs/2309.10312
tags:
- neurons
- explanation
- explanations
- language
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for rigorously evaluating natural
  language explanations of neurons in language models, addressing the problem of verifying
  whether such explanations accurately describe neuron behavior. The authors propose
  two modes of evaluation: an observational mode that tests whether neuron activations
  align with the claimed concepts, and an intervention mode that assesses causal efficacy
  by intervening on neuron activations.'
---

# Rigorously Assessing Natural Language Explanations of Neurons

## Quick Facts
- arXiv ID: 2309.10312
- Source URL: https://arxiv.org/abs/2309.10312
- Reference count: 11
- Primary result: Natural language explanations of individual neurons have high error rates and show little causal effect on model behavior

## Executive Summary
This paper introduces a rigorous framework for evaluating natural language explanations of neurons in language models, addressing the challenge of verifying whether such explanations accurately describe neuron behavior. The authors propose two complementary evaluation modes: observational (testing whether neuron activations align with claimed concepts) and intervention (assessing causal efficacy through neuron activation manipulation). Applied to GPT-4-generated explanations of GPT-2 XL neurons, the framework reveals that even high-confidence explanations have low precision (0.64) and recall (0.50), with minimal causal effects on model behavior. The results suggest that individual neurons are unreliable units for explaining model behavior, supporting the hypothesis that concepts are distributed across multiple neurons rather than encoded in single neurons.

## Method Summary
The framework evaluates natural language explanations through two modes. The observational mode tests whether neuron activations match the concepts claimed in explanations by constructing test sentences and measuring precision, recall, and F1-score. The intervention mode assesses causal mediation by intervening on neuron activations and computing interchange intervention accuracy (IIA) to determine if neurons causally influence the concepts they're claimed to represent. The authors apply this framework to 300 GPT-4-generated explanations of GPT-2 XL neurons, constructing test datasets with sentences containing strings from the sets denoted by explanations and strings not in those sets. For intervention evaluation, they use templates and sample strings to create input pairs for testing causal mediation across different tasks.

## Key Results
- GPT-4 explanations of GPT-2 XL neurons achieve precision of 0.64 and recall of 0.50, indicating high error rates
- Intervention analysis shows IIA of 0.33-0.48 for the most confident explanations, close to random chance (0.33-0.35)
- Causal effects increase as more neurons are intervened upon, with IIA reaching 0.79-0.82 at K=100 neurons
- Numerical expression explanations show higher IIA (0.53) than random baseline, while other concepts show little causal effect

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Natural language explanations fail due to inherent vagueness and ambiguity
- **Mechanism:** Natural language terms can refer to multiple concepts, preventing accurate predictions of neuron behavior
- **Core assumption:** Ambiguity in natural language directly causes misalignment with neuron behavior
- **Evidence anchors:** Abstract mentions "natural language is an appealing medium" but evaluating faithfulness is challenging; section 5.1 discusses vagueness, ambiguity, and context dependence
- **Break condition:** Using formal language with precise semantics instead of natural language

### Mechanism 2
- **Claim:** Individual neurons are poor analysis units because concepts are distributed across multiple neurons
- **Mechanism:** Concepts in language models are represented through distributed representations across many neurons, not isolated in single neurons
- **Core assumption:** Distributed representations mean single neurons cannot reliably represent or control specific concepts
- **Evidence anchors:** Abstract notes explanations "can be useful in exploring hypotheses"; section 4.4 discusses high IIA from token-activation baseline; section 5.2 mentions analyzing neurons in MLP layers vs. attention heads
- **Break condition:** Demonstrating individual neurons can reliably represent single concepts through new analysis methods

### Mechanism 3
- **Claim:** Correlation-based evaluation methods are insufficient for assessing explanation quality
- **Mechanism:** GPT-4 score measures correlation on small datasets, missing rare but important Type I errors
- **Core assumption:** GPT-4 score's insensitivity to rare errors makes it unreliable as a quality metric
- **Evidence anchors:** Abstract states "even the most confident explanations have high error rates"; section 3.4 shows unfaithful explanations can have perfect GPT-4 scores
- **Break condition:** Constructing evaluation datasets with sufficient examples of all relevant concepts

## Foundational Learning

- **Concept: Type I and Type II errors in classification**
  - Why needed here: The framework explicitly measures precision (Type I errors) and recall (Type II errors)
  - Quick check question: If an explanation predicts a neuron activates on strings in JEK, what type of error occurs when the neuron doesn't activate on a string that should be in JEK?

- **Concept: Causal mediation analysis**
  - Why needed here: The intervention mode uses causal mediation to determine if neurons are causal mediators of claimed concepts
  - Quick check question: What does it mean for a neuron to be a "causal mediator" of a concept in this context?

- **Concept: Distributed representations in neural networks**
  - Why needed here: The paper argues concepts are distributed across multiple neurons rather than encoded in individual neurons
  - Quick check question: Why might a single neuron activate on inputs that seem unrelated to its natural language explanation?

## Architecture Onboarding

- **Component map:** Data generation -> Observational evaluation -> Intervention evaluation -> Metric calculation -> Baseline comparison -> Results analysis
- **Critical path:** Generate test sentences → evaluate neuron activations → calculate precision/recall/IIA → compare against baselines → draw conclusions about explanation quality
- **Design tradeoffs:** Observational vs. intervention modes trade off computational cost (intervention is more expensive) against insight type (encoding vs. causal use)
- **Failure signatures:** High GPT-4 scores but low precision/recall indicate evaluation method missing important errors; high observational scores but low IIA indicate explanations capture encoding but not causal use
- **First 3 experiments:**
  1. Replicate observational evaluation on different layer of GPT-2 XL to verify pattern holds
  2. Test intervention evaluation on simple synthetic model with known ground truth
  3. Apply framework to explanations from different method (e.g., Network Dissection) to compare results

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but identifies several areas for future work: exploring explanations at different levels of abstraction (neurons vs. attention heads vs. residual streams), developing better methods for evaluating explanations, and investigating whether distributed representations across multiple neurons are the appropriate unit of analysis for understanding model behavior.

## Limitations
- Focus on GPT-2 XL and GPT-4-generated explanations limits generalizability to other model architectures and explanation methods
- Manual verification process introduces potential subjectivity that isn't fully characterized
- Doesn't explore whether higher-capacity models might produce more reliable explanations

## Confidence
- **High:** Framework's utility for rigorous evaluation and demonstrated failure of current explanations to meet high standards
- **Medium:** Claims about distributed representations being fundamental reason for neuron explanation failures
- **Low:** Whether intervention mode fully captures causal relationships without extensive validation

## Next Checks
1. Apply framework to explanations generated by different methods (Network Dissection, automated concept discovery tools) to determine if failure patterns are specific to GPT-4-generated explanations
2. Test framework on simple synthetic model where ground truth neuron-concept relationships are known to validate evaluation metrics
3. Replicate analysis on attention heads and residual streams rather than MLP neurons to determine if individual neuron failure extends to other architectural components