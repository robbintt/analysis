---
ver: rpa2
title: Feature propagation as self-supervision signals on graphs
arxiv_id: '2303.08644'
source_url: https://arxiv.org/abs/2303.08644
tags:
- graph
- learning
- views
- node
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RGI is a self-supervised learning framework for graphs that trains
  a graph neural network encoder by maximizing the mutual information between output
  node embeddings and their propagation through the graph. It is augmentation-free,
  non-contrastive, and does not depend on a two-branch architecture.
---

# Feature propagation as self-supervision signals on graphs

## Quick Facts
- arXiv ID: 2303.08644
- Source URL: https://arxiv.org/abs/2303.08644
- Authors: 
- Reference count: 30
- Key outcome: RGI achieves state-of-the-art performance on node classification tasks across multiple graph benchmarks without using graph augmentations or contrastive learning

## Executive Summary
RGI introduces a self-supervised learning framework for graphs that leverages feature propagation as a source of self-supervision signals. Unlike existing methods that rely on contrastive learning with negative sampling or graph data augmentations, RGI trains a graph neural network encoder by maximizing mutual information between node embeddings and their propagated versions through the graph. The framework is augmentation-free, non-contrastive, and uses a single-branch architecture with covariance regularization to prevent representation collapse.

## Method Summary
RGI trains a graph neural network encoder to maximize mutual information between local node embeddings and their propagated versions through the graph. The method uses feature propagation with a shift operator to create global views for each node, then employs reconstruction networks to predict global views from local views and vice versa. The loss function combines reconstruction error with covariance regularization terms that maximize variance and minimize covariance of the embeddings. This approach avoids the need for negative sampling or complex two-branch architectures while maintaining strong performance.

## Key Results
- Achieves 90.45%, 92.94%, 93.37%, and 95.91% accuracy on Amazon Computers, Amazon Photos, Coauthor CS, and Coauthor Physics datasets respectively
- Achieves 72.16% micro-average F1-score on inductive PPI dataset
- Outperforms state-of-the-art methods on both transductive and inductive settings
- Demonstrates effectiveness of augmentation-free, non-contrastive self-supervised learning on graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RGI avoids collapse by regularizing the covariance matrices of local and global views
- Mechanism: The loss function includes terms that maximize diagonal elements (variances) and minimize off-diagonal elements (covariances) of the covariance matrices, which encourages spread-out representations in the embedding space
- Core assumption: Under Gaussianity assumption, maximizing variance and minimizing covariance regularizes entropy to avoid collapse
- Evidence anchors:
  - [abstract] "The collapse is avoided with variance-covariance regularization loss terms that attempt to maximize the entropy of the representation space"
  - [section] "Covariance regularization consists of extending the loss function by including regularization terms on the covariance matrix of the representations, forcing high variances for every feature and low co-variances"
  - [corpus] Weak evidence - most corpus papers mention contrastive approaches rather than covariance regularization specifically
- Break condition: If the representation space becomes highly non-Gaussian, the entropy regularization through covariance may not effectively prevent collapse

### Mechanism 2
- Claim: RGI achieves strong performance without requiring negative sampling or complex architectures
- Mechanism: By maximizing mutual information between local and global views through reconstruction objectives, RGI creates a single-branch architecture that is more computationally efficient while maintaining effectiveness
- Core assumption: The mutual information between propagated node embeddings and original embeddings contains sufficient signal for self-supervision
- Evidence anchors:
  - [abstract] "RGI do not use graph data augmentations but instead generates self-supervision signals with feature propagation, is non-contrastive and does not depend on a two branch architecture"
  - [section] "RGI trains an encoder maximizing the mutual information between nodes' representations output by the model and their propagation through the graph"
  - [corpus] Weak evidence - corpus focuses on augmentation-based methods rather than propagation-based approaches
- Break condition: If the graph structure does not support meaningful propagation (e.g., disconnected graphs or graphs with very low homophily)

### Mechanism 3
- Claim: Node-level global views provide better one-to-one reconstruction capability than graph-level global views
- Mechanism: By propagating each node's representation K steps through the graph, RGI creates a unique global view for each node that captures both local and global context, enabling direct reconstruction comparison
- Core assumption: The (L+K)-hop neighborhood captures sufficient global information about a node's position and role in the graph
- Evidence anchors:
  - [abstract] "We define a node-level global view, particular to every node, as opposed to the graph-level view proposed in other methods"
  - [section] "The node views V are referenced as global views. This definition is different from the global pooling proposed in DGI [7], since the views are node level rather than graph level"
  - [corpus] Moderate evidence - some corpus papers mention node-level approaches but focus more on contrastive methods
- Break condition: If the graph diameter is too small relative to L+K, the global views may not provide additional information beyond local views

## Foundational Learning

- Concept: Mutual Information Maximization
  - Why needed here: RGI's core objective is to maximize mutual information between local and global node representations, which forms the theoretical foundation for the self-supervised learning framework
  - Quick check question: What is the relationship between mutual information and the InfoMax principle, and how does RGI extend this principle to graph domains?

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: RGI uses a GCN encoder to obtain local views, and understanding how message passing aggregates neighborhood information is crucial for grasping how global views are created through propagation
  - Quick check question: How does the choice of K (propagation steps) affect the information captured in the global views, and what graph properties determine the optimal K value?

- Concept: Covariance Matrix Regularization
  - Why needed here: The variance-covariance regularization terms in RGI's loss function are key to preventing representation collapse, and understanding this regularization technique is essential for modifying or extending the method
  - Quick check question: Under what conditions does covariance regularization effectively prevent collapse, and how does this differ from other collapse prevention techniques like negative sampling?

## Architecture Onboarding

- Component map:
  - Encoder (GCN/GAT) -> Local Views (U) -> Propagation -> Global Views (V) -> Reconstruction Networks (hφ, hψ) -> Loss Calculation -> Parameter Updates

- Critical path: Input features and adjacency matrix → GCN/GAT encoder → Local node embeddings (U) → K-step propagation via shift operator → Global views (V) → Reconstruction networks predict V from U and U from V → Combined loss with reconstruction and covariance regularization → Encoder parameter updates

- Design tradeoffs: Single-branch architecture vs. contrastive two-branch approaches (efficiency vs. potential performance); augmentation-free vs. augmentation-based methods (simplicity vs. potential robustness)

- Failure signatures: Representation collapse indicated by low variance in embeddings; poor reconstruction quality suggesting inadequate propagation steps or insufficient mutual information

- First experiments: 1) Verify reconstruction accuracy between local and global views; 2) Test covariance matrix properties (diagonal dominance); 3) Evaluate downstream classification performance on validation set

## Open Questions the Paper Calls Out

None explicitly stated in the paper.

## Limitations

- Theoretical justification for covariance regularization under Gaussianity assumption is limited
- No principled method for determining optimal propagation steps K across different graph structures
- Computational efficiency gains compared to contrastive methods are not quantified

## Confidence

**High Confidence:** The empirical results showing RGI outperforming state-of-the-art methods on standard benchmarks (Amazon, Coauthor, and PPI datasets). The methodology for calculating mutual information through reconstruction is well-defined and reproducible.

**Medium Confidence:** The claim that RGI achieves strong performance without negative sampling or complex architectures. While results support this, direct ablation studies comparing computational efficiency are lacking.

**Low Confidence:** The theoretical mechanism of collapse prevention through covariance regularization under Gaussianity assumptions. The paper asserts this works but provides limited theoretical justification or empirical verification of the Gaussian assumption on real data.

## Next Checks

1. **Gaussianity Verification:** Test whether node embeddings produced by RGI approximately follow Gaussian distributions on real graph datasets, using statistical tests (e.g., Shapiro-Wilk test) and visualization of embedding distributions.

2. **Propagation Step Sensitivity:** Systematically vary the propagation parameter K across different graph types (small diameter vs. large diameter graphs) and analyze the impact on both reconstruction accuracy and downstream classification performance.

3. **Efficiency Benchmarking:** Implement a contrastive baseline with similar architecture but including negative sampling, then measure and compare training time, memory usage, and convergence speed between RGI and the contrastive approach on identical hardware.