---
ver: rpa2
title: Zero-shot Learning with Minimum Instruction to Extract Social Determinants
  and Family History from Clinical Notes using GPT Model
arxiv_id: '2309.05475'
source_url: https://arxiv.org/abs/2309.05475
tags:
- history
- social
- clinical
- family
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of zero-shot learning using
  GPT-3.5 for extracting social determinants and family history from clinical notes.
  The approach involves providing minimum instruction to the model and exploring both
  traditional NER evaluation metrics and semantic similarity metrics to assess performance.
---

# Zero-shot Learning with Minimum Instruction to Extract Social Determinants and Family History from Clinical Notes using GPT Model

## Quick Facts
- arXiv ID: 2309.05475
- Source URL: https://arxiv.org/abs/2309.05475
- Reference count: 21
- Key outcome: GPT-3.5 achieves 0.975 F1 for demographics, 0.615 F1 for social determinants, and 0.722 F1 for family history extraction using zero-shot learning with minimal instruction

## Executive Summary
This study investigates zero-shot learning using GPT-3.5 for extracting social determinants and family history from clinical notes. The approach uses minimal instruction prompts and evaluates performance using both traditional NER metrics and semantic similarity measures. Results show strong performance on demographics extraction (F1 0.975) and moderate performance on social determinants (F1 0.615) and family history (F1 0.722). The findings demonstrate GPT-3.5's capability to extract relevant clinical information with minimal guidance, highlighting its potential for clinical natural language processing applications.

## Method Summary
The study employs GPT-3.5 API with simple prompts to extract social determinants and family history from 1,000 de-identified clinical notes. The prompts provide minimal instruction through system and user role definitions, asking the model to extract specified categories from clinical text. Post-processing breaks down multi-entity outputs into subtypes using keyword matching. Performance is evaluated using both NER metrics (relaxed match F1 score) and semantic similarity metrics (cosine similarity with thresholds of 0.8 and 0.9) with sentence embeddings from all-MiniLM-L6-v2 model.

## Key Results
- GPT-3.5 achieves average F1 score of 0.975 for demographics extraction
- Social determinants extraction achieves F1 score of 0.615
- Family history extraction achieves F1 score of 0.722
- Semantic similarity evaluation captures extraction quality better than exact string matching for certain entity types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot learning with minimal instruction can achieve competitive F1 scores on clinical text extraction tasks.
- Mechanism: GPT-3.5 leverages its pre-trained knowledge to infer entity boundaries and semantics from context without explicit training data, mapping unstructured text to structured outputs through pattern matching and semantic reasoning.
- Core assumption: The model's pre-training on diverse text includes sufficient clinical and social terminology coverage to generalize to zero-shot tasks.
- Evidence anchors:
  - [abstract] "GPT-3.5 method achieved an average of 0.975 F1 on demographics extraction, 0.615 F1 on social determinants extraction, and 0.722 F1 on family history extraction"
  - [section IV] "we employ both NER evaluation metrics and a semantic evaluation approach"
- Break condition: Performance degrades sharply if the clinical text contains rare or highly domain-specific terms not represented in pre-training data.

### Mechanism 2
- Claim: Semantic similarity evaluation better captures GPT model's extraction quality than exact string matching.
- Mechanism: Sentence embeddings (e.g., all-MiniLM-L6-v2) allow comparison of extracted concepts against ground truth based on meaning rather than token-level overlap, accommodating paraphrasing and rephrasing by GPT.
- Core assumption: The embedding model captures clinically relevant semantic equivalence between phrases like "unemployed" and "has no job".
- Evidence anchors:
  - [section IV] "if the cosine similarity ( s) is above a threshold θ (e.g., θ = 0.8), it is a match"
  - [section IV] "the performance on education status is the lowest, with accuracy of 0.378, and tobacco use is the highest, with an accuracy of 0.649"
- Break condition: If the embedding space poorly represents clinical concepts, semantic evaluation may incorrectly accept semantically dissimilar outputs.

### Mechanism 3
- Claim: Simple, intuitive prompts with minimal instruction suffice for GPT-3.5 to extract multiple entity types simultaneously.
- Mechanism: The model's in-context learning ability allows it to infer task requirements from concise system and user role definitions, reducing prompt engineering complexity.
- Evidence anchors:
  - [section III.A] "The prompt is simple with minimum information to guide the GPT model to process the input text and output the results in a given format"
  - [section V.A] "with minimum instruction, the GPT model can achieve high F1 on demographic information extraction (0.975)"
- Break condition: Complex or nested entity relationships may require more detailed prompts to guide accurate extraction.

## Foundational Learning

- Concept: Zero-shot learning in NLP
  - Why needed here: Understanding how models generalize without task-specific training data is critical to interpreting GPT-3.5's performance on clinical entity extraction.
  - Quick check question: What distinguishes zero-shot from few-shot or fine-tuned learning in transformer models?

- Concept: Named Entity Recognition (NER) evaluation metrics
  - Why needed here: Traditional NER metrics (precision, recall, F1) are used alongside semantic similarity to assess extraction quality.
  - Quick check question: How does relaxed match differ from exact match in NER evaluation?

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: Semantic evaluation relies on comparing embeddings to judge extraction correctness beyond token overlap.
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance for comparing sentence embeddings?

## Architecture Onboarding

- Component map:
  Input clinical notes -> GPT-3.5 API with prompt -> Raw GPT output -> Post-processing into subtypes -> Evaluation metrics calculation -> Results aggregation

- Critical path:
  1. Clinical note ingestion -> 2. GPT-3.5 API call with prompt -> 3. Raw GPT output -> 4. Post-processing into subtypes -> 5. Evaluation metrics calculation -> 6. Results aggregation

- Design tradeoffs:
  - Simple prompt reduces engineering effort but may miss complex entity relationships
  - Semantic evaluation tolerates rephrasing but requires embedding model selection and threshold tuning
  - Post-processing via keyword matching is lightweight but may misclassify ambiguous phrases

- Failure signatures:
  - Low F1 scores on specific subtypes suggest prompt or keyword list inadequacy
  - High semantic similarity but low NER F1 indicates GPT rephrases rather than extracts verbatim
  - Inconsistent outputs across repeated GPT calls signal low self-consistency

- First 3 experiments:
  1. Vary semantic similarity threshold (θ) from 0.7 to 0.95 and observe impact on F1 vs accuracy
  2. Replace keyword-based post-processing with a lightweight classifier trained on a small labeled set
  3. Test GPT-4 vs GPT-3.5 to quantify impact of model capacity on zero-shot extraction quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT models on clinical text extraction tasks change with different prompting strategies, particularly when comparing simple vs. detailed instructions?
- Basis in paper: [explicit] The paper compares zero-shot learning with minimum instruction to existing approaches using more detailed prompts, noting that GPT models can still achieve competitive results with simpler instructions.
- Why unresolved: The study only tested one simple prompting strategy, and there is no systematic comparison of how different levels of detail in prompts affect performance.
- What evidence would resolve it: A controlled experiment comparing GPT model performance across a range of prompt complexities (simple to detailed) on the same clinical text extraction tasks.

### Open Question 2
- Question: Can the inconsistency in GPT model outputs (low self-consistency) be effectively mitigated through techniques like multiple runs with top-k ranked outputs or reinforcement learning?
- Basis in paper: [explicit] The paper identifies low self-consistency as a limitation and mentions potential solutions such as applying GPT multiple times and integrating top-k ranked outputs or using reinforcement learning.
- Why unresolved: While potential solutions are proposed, the paper does not test these methods or provide empirical evidence of their effectiveness in improving consistency.
- What evidence would resolve it: Experimental results demonstrating improved consistency in GPT outputs when using techniques like multiple runs with top-k integration or reinforcement learning-based fine-tuning.

### Open Question 3
- Question: How can post-processing techniques be developed to validate and normalize GPT-generated clinical information while maintaining traceability to the original source text?
- Basis in paper: [explicit] The paper notes that GPT models may generate content that differs from the original text and do not provide source information, making validation challenging.
- Why unresolved: The study identifies this as a challenge but does not propose or test specific post-processing methods for validation and normalization of GPT outputs.
- What evidence would resolve it: Development and testing of post-processing algorithms that can validate GPT outputs against original text and provide source traceability, with demonstrated improvements in accuracy and reliability.

### Open Question 4
- Question: What is the optimal approach for incorporating domain-specific knowledge into GPT models for clinical NLP tasks, and how does it compare to general fine-tuning or few-shot learning methods?
- Basis in paper: [inferred] The paper mentions that domain knowledge is often needed to guide GPT learning for clinical tasks, but does not explore specific methods for incorporating this knowledge.
- Why unresolved: While the importance of domain knowledge is acknowledged, there is no comparison of different approaches for integrating this knowledge into GPT models.
- What evidence would resolve it: Comparative study of various methods for incorporating clinical domain knowledge (e.g., knowledge graphs, domain-specific pretraining, few-shot learning with clinical examples) and their impact on GPT model performance for clinical NLP tasks.

## Limitations

- The study lacks comparison with traditional supervised or few-shot learning approaches for the same extraction tasks, making it unclear if the observed F1 scores represent competitive performance.
- The study does not report on model self-consistency - whether GPT-3.5 produces identical outputs for identical inputs across multiple runs, which is critical for clinical applications.
- The post-processing approach relies on keyword matching with undisclosed keywords, making it difficult to assess potential sources of error or bias in this step.

## Confidence

**High confidence** in the demographics extraction results (F1 0.975), as this represents a straightforward task with well-defined categories and likely reflects GPT-3.5's strong performance on common entity types.

**Medium confidence** in the overall extraction framework, particularly the combination of NER metrics and semantic similarity evaluation, though the semantic similarity thresholds were not validated through sensitivity analysis.

**Low confidence** in the generalizability of results beyond the specific clinical note corpus from one university hospital, as the study does not report on the diversity of clinical settings or patient populations represented.

## Next Checks

1. **Self-consistency testing**: Run each clinical note through GPT-3.5 ten times with identical prompts and measure the variance in outputs. Calculate the percentage of cases where the model produces identical entity sets, and analyze whether high variance correlates with specific entity types or note characteristics.

2. **Semantic similarity threshold validation**: Systematically vary the cosine similarity threshold (θ) from 0.7 to 0.95 in 0.05 increments and plot F1 scores versus accuracy for each entity type. Identify the optimal threshold that maximizes the F1 score while maintaining acceptable accuracy, and report how sensitive performance is to threshold selection.

3. **Cross-institutional validation**: Apply the same zero-shot extraction pipeline to clinical notes from at least two different healthcare systems with distinct documentation practices and patient populations. Compare performance metrics across institutions to assess whether the approach generalizes beyond the original corpus or requires institution-specific prompt engineering.