---
ver: rpa2
title: 'Beyond Document Page Classification: Design, Datasets, and Challenges'
arxiv_id: '2308.12896'
source_url: https://arxiv.org/abs/2308.12896
tags:
- document
- page
- classification
- documents
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of multi-page document classification
  datasets for benchmarking real-world document understanding applications. The authors
  propose a taxonomy of document classification tasks (single-page, document, document
  bundle, page stream, and page splitting) and create two novel multi-page datasets
  (RVL-CDIP MP and RVL-CDIP-N MP) by reconstructing the original documents from the
  RVL-CDIP benchmark.
---

# Beyond Document Page Classification: Design, Datasets, and Challenges

## Quick Facts
- arXiv ID: 2308.12896
- Source URL: https://arxiv.org/abs/2308.12896
- Reference count: 40
- Key outcome: Multi-page document classification improves accuracy by 2-6% compared to single-page classification, highlighting the need for updated benchmarks

## Executive Summary
This paper addresses the inadequacy of current single-page document classification benchmarks for real-world applications by proposing a comprehensive taxonomy of document classification tasks and constructing two novel multi-page datasets derived from RVL-CDIP. The authors demonstrate that while first-page classification remains a strong baseline, combining information across multiple pages yields measurable accuracy improvements of 2-6%. The study also reveals that soft voting and maximum confidence aggregation strategies can effectively leverage multi-page information. The paper concludes with recommendations for improving document classification benchmarking, including more diverse document types, better label quality, and expanded evaluation metrics beyond accuracy.

## Method Summary
The authors created two multi-page datasets (RVL-CDIP MP with ~400K documents averaging 5 pages, and RVL-CDIP-N MP with 1,002 documents averaging 10 pages) by reconstructing original documents from the RVL-CDIP benchmark. They employed DiT-Base as a visual-only document foundation model, fine-tuned on RVL-CDIP, and evaluated it across five document classification tasks using various inference strategies including first page only, last page, sequence max confidence, soft voting, hard voting, and grid-based tiling. The evaluation included standard accuracy metrics as well as calibration error and area under risk-coverage curves.

## Key Results
- Multi-page document classification accuracy improves by 2-6% compared to single-page classification
- Soft voting and maximum confidence aggregation strategies outperform single-page classification while requiring only 2-3x more processing
- Current benchmarks are inadequate for real-world applications due to their focus on single-page images and simplified label sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-page document classification accuracy improves when leveraging information across pages compared to using only the first page.
- Mechanism: Classification models benefit from context spread across multiple pages because key information may be distributed non-uniformly (e.g., summary on first page, details on later pages).
- Core assumption: The document's label depends on content distributed across pages, not just a single page.
- Evidence anchors:
  - [abstract]: "combining information across multiple pages can improve accuracy by 2-6%"
  - [section]: "Table 6 reports a best-case error analysis, where a page prediction is counted as correct if the model would have had access to the other pages"
  - [corpus]: Weak - corpus doesn't directly address multi-page classification improvement
- Break condition: If document content is entirely self-contained on the first page, or if later pages contain contradictory/noisy information.

### Mechanism 2
- Claim: Soft voting and maximum confidence strategies outperform single-page classification by aggregating predictions across pages.
- Mechanism: Probabilistic aggregation methods (soft voting, max confidence) can better capture the most confident predictions across pages rather than relying on a single page's prediction.
- Core assumption: Different pages provide complementary information that, when aggregated probabilistically, yields better overall predictions.
- Evidence anchors:
  - [section]: "Max-imum confidence and soft voting require L (pages) times more processing, yet attain similar performance as the best single-page prediction"
  - [abstract]: "combining information across multiple pages can improve accuracy by 2-6%"
  - [corpus]: Weak - corpus doesn't directly compare aggregation strategies
- Break condition: If aggregation methods are computationally expensive relative to marginal accuracy gains, or if page predictions are highly correlated.

### Mechanism 3
- Claim: Current document classification benchmarks are inadequate for real-world applications because they only support single-page images and simplified label sets.
- Mechanism: Real-world documents are multi-page and require more complex label structures, while current benchmarks (like RVL-CDIP) use simplified, single-page, and often noisy labels.
- Core assumption: Industrial document classification involves multi-page documents with richer, more complex label sets than current benchmarks provide.
- Evidence anchors:
  - [abstract]: "current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice"
  - [section]: "Real-world DC use cases typically support a richer number of classes (K ~50-400)"
  - [corpus]: Weak - corpus doesn't directly address benchmark inadequacy
- Break condition: If new multi-page datasets are created but fail to capture the complexity and diversity of real-world documents.

## Foundational Learning

- Concept: Document classification task taxonomy
  - Why needed here: Understanding the different classification tasks (single-page, document, bundle, stream, splitting) is crucial for designing appropriate evaluation methods and datasets
  - Quick check question: Can you explain the difference between document classification and document bundle classification in terms of input and output spaces?

- Concept: Multi-modal document representation
  - Why needed here: Documents contain both visual and textual information that must be processed together for accurate classification
  - Quick check question: What are the challenges in representing multi-page documents as inputs to deep learning models?

- Concept: Evaluation metrics beyond accuracy
  - Why needed here: Real-world applications require metrics like calibration, robustness, and inference efficiency, not just accuracy
  - Quick check question: Why might calibration error be as important as accuracy in production document classification systems?

## Architecture Onboarding

- Component map: Document loading -> Page extraction -> Feature extraction -> Aggregation module -> Classification head -> Evaluation module
- Critical path: Document loading → Page processing → Feature aggregation → Classification → Evaluation
- Design tradeoffs:
  - Single-page vs. multi-page processing: Tradeoff between computational efficiency and accuracy
  - Fixed vs. variable page count: Fixed allows simpler architectures but may lose information
  - Visual-only vs. multimodal: Visual-only is more efficient but may miss textual cues
- Failure signatures:
  - Low accuracy improvement from multi-page processing suggests content is concentrated on first page
  - Poor calibration indicates overconfidence in predictions
  - High computational cost with marginal accuracy gains suggests inefficient aggregation strategy
- First 3 experiments:
  1. Compare single-page classification (first page only) vs. multi-page classification using soft voting
  2. Test different aggregation strategies (max confidence, soft voting, hard voting) on multi-page documents
  3. Evaluate calibration error and AURC to assess confidence quality beyond accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do document classification models perform when evaluated on multi-page documents versus single-page images?
- Basis in paper: [explicit] The paper shows that classifying based on the first page alone is a strong baseline, but combining information across multiple pages can improve accuracy by 2-6%.
- Why unresolved: While the paper demonstrates the potential of multi-page document classification, it does not provide a comprehensive evaluation of different document classification models on multi-page documents.
- What evidence would resolve it: Conducting a large-scale evaluation of various document classification models on multi-page documents would provide a clearer understanding of their performance and limitations.

### Open Question 2
- Question: What are the most effective strategies for representing multi-page documents for classification purposes?
- Basis in paper: [inferred] The paper mentions different inference strategies such as sequence max confidence, soft voting, and hard voting, but it does not explore other potential strategies or representations for multi-page documents.
- Why unresolved: The paper focuses on a specific set of inference strategies and does not explore alternative approaches for representing and classifying multi-page documents.
- What evidence would resolve it: Exploring and comparing different representation strategies, such as using document embeddings or hierarchical models, would provide insights into the most effective approaches for multi-page document classification.

### Open Question 3
- Question: How can document classification benchmarks be improved to better reflect real-world applications and challenges?
- Basis in paper: [explicit] The paper highlights the need to bring document classification benchmarking closer to real-world applications by considering multi-page documents, bundles, page streams, and splits.
- Why unresolved: While the paper identifies the limitations of current benchmarks and proposes improvements, it does not provide a comprehensive framework for designing and evaluating benchmarks that align with real-world document classification tasks.
- What evidence would resolve it: Developing a standardized framework for designing and evaluating document classification benchmarks, including guidelines for dataset construction, evaluation metrics, and task formulations, would address this open question.

## Limitations

- The constructed datasets are derived from a single source benchmark (RVL-CDIP), limiting diversity compared to real-world documents
- The study focuses exclusively on visual features using DiT-Base, potentially missing complementary information from text or layout analysis
- The 16-class taxonomy is limited in scope compared to industrial applications that typically handle 50-400 classes

## Confidence

- **High Confidence**: The mechanism that multi-page documents can contain information distributed across pages (Mechanism 1) is well-supported by the 2-6% accuracy improvement observed
- **Medium Confidence**: The claim that current benchmarks are inadequate for real-world applications (Mechanism 3) is supported by the paper's analysis but would benefit from broader empirical validation
- **Low Confidence**: The comparative effectiveness of different aggregation strategies (Mechanism 2) shows similar performance between MaxConfidence and SoftVoting, but the computational cost implications are not fully explored

## Next Checks

1. Validate the 2-6% accuracy improvement on a truly diverse, multi-source document collection beyond RVL-CDIP to ensure the multi-page benefit generalizes across document domains
2. Test the aggregation strategies on documents with known content distribution patterns (e.g., summary on first page, details on later pages) to quantify when multi-page processing provides maximum benefit
3. Evaluate the proposed datasets and methods on industrial-scale document classification tasks with 50-400 classes to assess scalability and practical utility beyond the 16-class taxonomy