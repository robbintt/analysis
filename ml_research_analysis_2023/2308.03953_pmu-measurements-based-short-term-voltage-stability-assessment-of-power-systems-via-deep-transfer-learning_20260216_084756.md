---
ver: rpa2
title: PMU measurements based short-term voltage stability assessment of power systems
  via deep transfer learning
arxiv_id: '2308.03953'
source_url: https://arxiv.org/abs/2308.03953
tags:
- learning
- power
- data
- dataset
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses short-term voltage stability assessment (STVSA)
  in power systems using deep transfer learning with PMU measurements. The authors
  propose a novel method that leverages real-time PMU data, temporal ensembling for
  sample labeling, and least squares generative adversarial networks (LSGAN) for data
  augmentation to effectively handle small datasets.
---

# PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning

## Quick Facts
- arXiv ID: 2308.03953
- Source URL: https://arxiv.org/abs/2308.03953
- Reference count: 40
- Primary result: Deep transfer learning improves STVSA accuracy by ~20% on IEEE 39-bus test system

## Executive Summary
This paper presents a novel approach for short-term voltage stability assessment (STVSA) using PMU measurements and deep transfer learning. The method addresses the challenge of limited labeled data by combining temporal ensembling for semi-supervised labeling, LSGAN for data augmentation, and transfer learning between different fault scenarios. The Transformer model is employed to capture temporal dependencies in voltage measurements. Experimental results on the IEEE 39-bus test system demonstrate significant improvements in assessment accuracy compared to shallow learning methods and other deep learning approaches, particularly in handling topological changes and small datasets.

## Method Summary
The proposed method integrates PMU measurements with deep transfer learning for STVSA. It uses temporal ensembling to label unlabeled samples by aggregating predictions over training epochs via EMA updates. LSGAN is employed for data augmentation to address small dataset challenges. The Transformer model captures temporal dependencies in voltage measurements. Transfer learning enables knowledge transfer between source and target fault datasets with different network topologies. The approach involves generating time-domain simulation datasets, applying temporal ensembling and LSGAN augmentation, pre-training on source domain, fine-tuning on target domain, and evaluating stability classification performance using metrics like accuracy, AUC, and MCC.

## Key Results
- Transfer learning improves model evaluation accuracy by approximately 20% compared to baseline methods
- Temporal ensembling effectively generates labels for unlabeled samples, enabling semi-supervised learning on small datasets
- LSGAN data augmentation enhances model generalization and handles dataset size limitations
- The method demonstrates superior performance in handling sequential data and learning long-distance dependencies compared to shallow learning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning bridges knowledge from source to target fault datasets, enabling stable performance under topological changes
- Mechanism: Pre-train Transformer on a diverse fault source dataset, then fine-tune on target domain with different line outages
- Core assumption: Fault scenarios share common voltage stability patterns that are transferable across topologies
- Evidence anchors:
  - [abstract]: "the method enhances adaptability to topological changes by exploring connections between different faults"
  - [section]: "pre-trained model is adapted for a different but related problem... This is particularly beneficial for situations where we have limited data or similar tasks with slight differences"
- Break condition: If fault characteristics are too dissimilar (e.g., load dynamics or system inertia differ greatly), the shared patterns break down

### Mechanism 2
- Claim: Temporal ensembling generates reliable labels for unlabeled samples by aggregating model predictions over training epochs
- Mechanism: EMA updates of prediction targets stabilize the semi-supervised clustering process, enabling effective deep learning with small datasets
- Core assumption: The temporal consistency of model predictions correlates with correct class assignment
- Evidence anchors:
  - [section]: "Temporal ensembling... incorporates temporal elements while adhering to the fundamental principles of consistency regularization"
  - [section]: "The value of z ̂ is continuously updated by each output z using the Exponentially Moving Average (EMA), and the output of the previous epochs model is averaged through EMA"
- Break condition: If initial predictions are too noisy or the model is too unstable early in training, EMA may converge to wrong labels

### Mechanism 3
- Claim: LSGAN augments the dataset with realistic synthetic samples, improving model generalization on small datasets
- Mechanism: Generator learns to produce voltage stability samples matching real data distribution; discriminator learns to distinguish real vs synthetic
- Core assumption: The synthetic samples preserve the statistical properties of real fault data, and the adversarial loss helps capture realistic dynamics
- Evidence anchors:
  - [section]: "least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets"
  - [section]: "In contrast, LSGAN offers a more effective solution by leveraging the least squares function with binary encoding as the loss function"
- Break condition: If GAN training collapses (generator produces unrealistic samples), augmented data degrades model performance

## Foundational Learning

- Concept: Voltage stability in power systems
  - Why needed here: Defines the target problem and stability thresholds (0.9 p.u. stable, 0.7 p.u. unstable)
  - Quick check question: What voltage thresholds distinguish stable vs unstable systems in this paper?

- Concept: Transformer self-attention mechanism
  - Why needed here: Captures temporal dependencies in PMU time-series data for accurate stability assessment
  - Quick check question: How does the multi-head attention in Transformer differ from a standard recurrent model for sequential data?

- Concept: Semi-supervised learning
  - Why needed here: Enables labeling of unlabeled samples using temporal ensembling, addressing lack of explicit STVS criteria
  - Quick check question: Why does temporal ensembling improve upon the π-model for sample labeling?

## Architecture Onboarding

- Component map: PMU data ingestion → Temporal ensembling labeling → LSGAN augmentation → Transfer Learning (source→target) → Transformer encoder → FC + Softmax → Stability classification
- Critical path: Data processing (PMU → label → augment) → Model training (source train → target fine-tune) → Online assessment
- Design tradeoffs: Small dataset handling vs model complexity; temporal ensembling accuracy vs training time; GAN realism vs training stability
- Failure signatures: Low AUC/MCC after fine-tuning indicates poor transfer; label inconsistency suggests temporal ensembling instability; synthetic samples too different from real data suggests GAN mode collapse
- First 3 experiments:
  1. Train Transformer on source dataset only; evaluate accuracy on source test set
  2. Apply temporal ensembling to label unlabeled samples; compute silhouette coefficient to assess cluster quality
  3. Fine-tune pre-trained Transformer on target dataset; measure improvement in accuracy and AUC over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Transfer Learning-Transformer method perform on real-world power system datasets, particularly those with complex topological structures and dynamic conditions?
- Basis in paper: [inferred] The authors mention that real-world operational data is challenging to obtain and resort to simulation software for dataset generation. They also suggest that future research could explore the performance of the method using real-world data
- Why unresolved: The paper only evaluates the method on simulated datasets, which may not fully capture the complexity and variability of real-world power systems
- What evidence would resolve it: Testing the method on real-world datasets from power systems with diverse topologies and operating conditions, and comparing its performance to existing methods

### Open Question 2
- Question: How does the performance of the proposed method scale with increasing system size and complexity?
- Basis in paper: [inferred] The paper evaluates the method on the IEEE 39-bus test system, but does not explore its performance on larger or more complex power systems
- Why unresolved: The scalability of the method to larger power systems with more buses and dynamic components is not addressed in the paper
- What evidence would resolve it: Evaluating the method on power systems with varying sizes and complexities, and analyzing its performance in terms of accuracy, computational time, and resource requirements

### Open Question 3
- Question: How does the proposed method handle missing or corrupted PMU measurements, which are common in real-world scenarios?
- Basis in paper: [explicit] The authors mention that the method assumes all buses are equipped with PMUs, but acknowledge that real-world scenarios may involve partial PMU information missing
- Why unresolved: The paper does not address how the method performs when faced with missing or corrupted PMU measurements
- What evidence would resolve it: Testing the method on datasets with simulated missing or corrupted PMU measurements, and comparing its performance to existing methods that are designed to handle such scenarios

## Limitations
- Transfer learning performance heavily dependent on similarity between source and target fault patterns, which may not hold for drastically different topologies
- Temporal ensembling accuracy relies on stable EMA convergence, but sensitivity to initial predictions and training stability is not quantified
- LSGAN augmentation quality not validated against real data distributions, raising concerns about synthetic sample realism
- Method assumes complete PMU coverage, but real-world systems often have missing or corrupted measurements

## Confidence

- Transfer learning mechanism: **Medium** - Supported by architectural description and IEEE 39-bus results, but lacks ablation studies isolating transfer benefits from other components
- Temporal ensembling effectiveness: **Low-Medium** - Mechanism described but validation limited to silhouette coefficients without comparison to alternative semi-supervised methods
- LSGAN augmentation contribution: **Low-Medium** - Theoretical justification provided but no experiments showing degradation when augmentation is removed

## Next Checks

1. Conduct ablation study: Train Transformer with and without transfer learning, with and without temporal ensembling, and with and without LSGAN to isolate each component's contribution to the reported 20% accuracy improvement
2. Perform topology sensitivity analysis: Systematically vary network topology complexity between source and target domains to quantify transfer learning breakdown points
3. Validate synthetic data quality: Compare voltage trajectory distributions and stability margin statistics between real and LSGAN-generated samples using statistical hypothesis testing