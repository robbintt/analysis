---
ver: rpa2
title: Improving Large Language Model Fine-tuning for Solving Math Problems
arxiv_id: '2310.10047'
source_url: https://arxiv.org/abs/2310.10047
tags:
- solution
- solutions
- performance
- math
- re-ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving large language
  models (LLMs) for solving math problems, particularly on the MATH dataset. The authors
  investigate three fine-tuning strategies: (1) supervised solution fine-tuning (SSFT),
  where LLMs are fine-tuned to generate detailed step-by-step solutions; (2) solution-cluster
  re-ranking (SCR), where LLMs are fine-tuned as solution evaluators to choose among
  generated candidate solution clusters; and (3) multi-task sequential fine-tuning,
  which integrates both solution generation and evaluation tasks efficiently.'
---

# Improving Large Language Model Fine-tuning for Solving Math Problems

## Quick Facts
- arXiv ID: 2310.10047
- Source URL: https://arxiv.org/abs/2310.10047
- Reference count: 22
- 58.8% accuracy on MATH dataset with fine-tuned PaLM 2-L models, an 11.2% improvement over few-shot performance

## Executive Summary
This paper investigates three fine-tuning strategies to improve large language models (LLMs) for solving math problems on the MATH dataset. The authors explore supervised solution fine-tuning (SSFT), solution-cluster re-ranking (SCR), and multi-task sequential fine-tuning (MTSF) approaches using PaLM 2 models. They find that solution quality significantly impacts performance, that combining solution re-ranking with majority voting yields substantial gains, and that sequential multi-task fine-tuning outperforms single-task approaches. Their final fine-tuning recipe achieves 58.8% accuracy on MATH, representing a notable improvement over baseline methods.

## Method Summary
The paper proposes three fine-tuning strategies: SSFT fine-tunes LLMs to generate detailed step-by-step solutions using MLE objectives; SCR fine-tunes LLMs as solution evaluators to re-rank candidate solutions using margin loss or cross-entropy; and MTSF sequentially applies SSFT, SCR, and SSFT again to leverage both generation and evaluation objectives. The authors use mathematical equivalence checking to cluster candidate solutions and apply majority voting on top-K clusters. They compare models fine-tuned on original MATH solutions versus GPT-4-generated PRM800K solutions, finding the latter significantly improves performance.

## Key Results
- Models fine-tuned on PRM800K solutions achieve significantly better performance than those fine-tuned on original MATH solutions
- Solution re-ranking and majority voting are both effective when used separately, with even greater performance when combined
- Multi-task sequential fine-tuning that separates solution generation and evaluation tasks offers improved performance over solution fine-tuning alone
- The fine-tuning recipe achieves approximately 58.8% accuracy on MATH dataset with PaLM 2-L models

## Why This Works (Mechanism)

### Mechanism 1
Solution-cluster re-ranking (SCR) improves performance by focusing evaluation on the most-frequent solution clusters rather than all candidates. The LLM is fine-tuned as a solution evaluator to score candidate solutions, re-ranking only solutions in the top-K majority-vote clusters. This reduces computational cost while maintaining or improving accuracy, based on the assumption that correct solutions tend to cluster together by mathematical equivalence.

### Mechanism 2
Multi-task sequential fine-tuning (MTSF) improves solution generation by incorporating solution evaluation training objectives. The model is fine-tuned sequentially: first as a generator (SSFT), then as an evaluator (SCR), then as a generator again. The evaluator training objective (margin loss or cross-entropy) is formulated as a natural language generation task, providing useful supervision to the generator and better leveraging the LLM's pre-training ability.

### Mechanism 3
Quality and style of step-by-step solutions used for fine-tuning significantly impacts model performance. Models fine-tuned on finer-grained, well-formatted solutions (like PRM800K generated by GPT-4) achieve better performance than those fine-tuned on more abstract original MATH solutions. Detailed step-by-step solutions provide better learning signals for the model to understand the reasoning process.

## Foundational Learning

- Concept: Mathematical equivalence checking
  - Why needed here: The evaluation function g(A, ˜A) checks mathematical equivalence rather than textual equivalence, which is crucial for clustering solutions and grading
  - Quick check question: How does the grading function handle 2+2 vs 4 vs "four"?

- Concept: Solution clustering and majority voting
  - Why needed here: Clustering candidate solutions by mathematical equivalence and selecting from the most frequent cluster (majority voting) is a key inference-time technique used in the baseline and combined with re-ranking
  - Quick check question: Why does majority voting work better than picking the first sampled solution?

- Concept: Contrastive learning vs. MLE objectives
  - Why needed here: The paper compares MLE-based fine-tuning with contrastive learning objectives, finding that pure contrastive objectives may not be suitable for binary math problem solving
  - Quick check question: What's the key difference between MLE and contrastive objectives in the context of math problem solving?

## Architecture Onboarding

- Component map: Pre-trained LLM → Fine-tuning stage 1 (SSFT as generator) → Fine-tuning stage 2 (SCR as evaluator) → Fine-tuning stage 3 (SSFT again) → Inference with sampling + majority voting + re-ranking
- Critical path: Solution generation → Solution clustering → Solution evaluation → Final answer selection
- Design tradeoffs: Computational efficiency vs. accuracy (re-ranking top-K clusters vs. all solutions), fine-tuning time vs. performance gain (sequential multi-task vs. single task)
- Failure signatures: Overfitting to training solutions (poor generalization), evaluator failing to discriminate correct from incorrect solutions, computational cost becoming prohibitive with larger N
- First 3 experiments:
  1. Implement and evaluate SSFT baseline with different solution styles (MATH vs. PRM800K)
  2. Implement SCR with re-ranking all solutions vs. top-K clusters
  3. Implement MTSF sequential fine-tuning and compare with SSFT baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and style of step-by-step solutions used for fine-tuning impact model performance?
- Basis in paper: [explicit] The paper states that "the quality and style of the step-by-step solutions can make a large impact on the model performance" and that models fine-tuned on PRM800K solutions significantly outperform those fine-tuned on original MATH solutions.
- Why unresolved: While the paper demonstrates that finer-grained, well-formatted solutions lead to better performance, it doesn't fully explain the mechanisms behind this improvement or explore whether models fine-tuned on their own generated solutions could achieve similar results.
- What evidence would resolve it: Comparative studies showing performance differences between models fine-tuned on various solution styles (abstract vs. detailed), and experiments where models are fine-tuned on their own generated solutions with different formatting approaches.

### Open Question 2
- Question: What is the optimal balance between solution generation and evaluation tasks in multi-task sequential fine-tuning?
- Basis in paper: [explicit] The paper proposes sequential multi-task fine-tuning but notes that "it is difficult to balance the two loss terms" and that models "start to overfit the MLE training objective very soon."
- Why unresolved: The paper uses a sequential approach rather than true multi-task training, but doesn't explore optimal ways to balance these objectives or investigate whether different task sequences might yield better results.
- What evidence would resolve it: Experiments comparing different task sequences, weightings, and training strategies for balancing generation and evaluation objectives, along with analysis of how these choices affect convergence and generalization.

### Open Question 3
- Question: How well do solution evaluators generalize across different model architectures and training regimes?
- Basis in paper: [explicit] The paper finds that "PaLM 2-S* is ineffective at re-ranking both the PaLM 2-L and PaLM 2-S* solutions" while the PaLM 2-L evaluator shows strong generalization ability.
- Why unresolved: The paper only tests generalization between two model variants (PaLM 2-S* and PaLM 2-L) and doesn't explore how evaluator performance varies across different model sizes, architectures, or training approaches.
- What evidence would resolve it: Comprehensive studies testing evaluator generalization across a wider range of model sizes, architectures (including models from different organizations), and training regimes, with analysis of factors affecting cross-model evaluation performance.

## Limitations

- The evaluation relies entirely on the MATH dataset and PRM800K solutions, with no external validation on other math benchmarks or real-world problem sets
- The paper lacks ablation studies on critical hyperparameters including learning rates, batch sizes, and training epochs for each fine-tuning stage
- The computational cost analysis is incomplete - while claiming solution-cluster re-ranking is more efficient, it doesn't provide quantitative comparisons of wall-clock time or GPU memory usage

## Confidence

- **High confidence**: The observation that solution quality and style significantly impact performance (PRM800K vs. MATH solutions), the effectiveness of majority voting, and the basic utility of supervised fine-tuning for math problems
- **Medium confidence**: The specific claim that multi-task sequential fine-tuning provides superior performance over solution fine-tuning alone, and that solution-cluster re-ranking combined with majority voting yields better results than either technique separately
- **Low confidence**: The assertion that solution-cluster re-ranking is computationally more efficient without quantitative evidence, and the claim that contrastive learning objectives are unsuitable for binary math problem solving without thorough exploration of alternative formulations

## Next Checks

1. **External validation**: Evaluate the best fine-tuned model on alternative math benchmarks (e.g., GSM8K, AMC problems, or competition mathematics) to verify generalization beyond the MATH dataset and PRM800K solution style

2. **Computational efficiency analysis**: Measure and compare wall-clock training time, inference latency, and GPU memory usage for SSFT, SCR, and MTSF approaches across different cluster sizes (K values) and candidate counts (N values) to quantify the claimed efficiency benefits

3. **Ablation on solution quality**: Conduct controlled experiments varying solution granularity, formatting consistency, and solution length within the PRM800K dataset to determine which aspects of solution quality most strongly influence model performance, and whether the improvements are robust to solution noise or incompleteness