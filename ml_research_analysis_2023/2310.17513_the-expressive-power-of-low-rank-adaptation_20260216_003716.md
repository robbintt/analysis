---
ver: rpa2
title: The Expressive Power of Low-Rank Adaptation
arxiv_id: '2310.17513'
source_url: https://arxiv.org/abs/2310.17513
tags:
- rank
- target
- lora
- low-rank
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical analysis of the expressive
  power of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method. For
  fully connected neural networks, it proves that LoRA can exactly represent any smaller
  target model when the LoRA-rank is at least (width of frozen model) x (depth of
  target model / depth of frozen model).
---

# The Expressive Power of Low-Rank Adaptation

## Quick Facts
- **arXiv ID**: 2310.17513
- **Source URL**: https://arxiv.org/abs/2310.17513
- **Reference count**: 40
- **Key outcome**: This paper provides the first theoretical analysis of LoRA's expressive power, proving exact representation capabilities under specific rank conditions for both fully connected and Transformer networks.

## Executive Summary
This paper provides the first theoretical analysis of the expressive power of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method. The authors prove that LoRA can exactly represent any smaller target model for fully connected networks when the LoRA-rank is at least (width of frozen model) x (depth of target model / depth of frozen model). For Transformer networks, they show that rank-(embedding size/2) adapters are sufficient for exact representation. When the rank is lower than the threshold, the paper quantifies the approximation error using singular value decomposition. These theoretical results are validated through numerical experiments demonstrating the relationship between LoRA-rank and approximation quality.

## Method Summary
The paper analyzes LoRA's expressive power through mathematical proofs that characterize when exact representation of target models is possible and how to bound approximation errors otherwise. For fully connected networks, the analysis involves partitioning the frozen model into blocks and constructing low-rank adapters to decompose the difference between the frozen model's weight product and the target model's weights. For Transformers, the authors analyze how LoRA adapters on self-attention and feedforward layers can exactly represent target models. The theoretical framework uses singular value decomposition to find optimal low-rank approximations and establishes bounds on approximation error based on the (R+1)-th singular value, where R is the LoRA-rank.

## Key Results
- LoRA can exactly represent any smaller target model for FNNs when rank ≥ (width of frozen model) x (depth of target model / depth of frozen model)
- For Transformers, rank-(embedding size/2) LoRA adapters are sufficient to exactly represent any target model of the same size
- When LoRA-rank is below threshold, approximation error is bounded by the (R+1)-th singular value of the error matrix
- Numerical experiments validate theoretical bounds and show practical rank requirements are often lower than theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA can exactly represent any smaller target model if the LoRA-rank is at least (width of frozen model) x (depth of target model / depth of frozen model).
- Mechanism: The paper shows that low-rank adapters can decompose the difference between the frozen model's weight product and the target model's weights into rank-bounded terms. By matching these decomposed terms with the best rank approximation of the error matrix, the adapters reconstruct the target model exactly.
- Core assumption: The weight matrices of the frozen model and the sum of the frozen weight product with the best rank approximation of the error are non-singular.
- Evidence anchors:
  - [abstract] "for fully connected neural networks, LoRA can adapt any model f to accurately represent any smaller target model f if LoRA-rank ≥ (width of f) x depth of f / depth of f"
  - [section] "Under mild conditions on ranks and network architectures, there exist low-rank adapters such that a low-rank adapted version of f0 is exactly equal to f"
  - [corpus] Weak - no direct corpus evidence cited in the paper's mechanisms.
- Break condition: If the non-singularity assumption fails, the low-rank decomposition cannot guarantee exact representation.

### Mechanism 2
- Claim: When LoRA-rank is lower than the threshold, the approximation error can be quantified and bounded.
- Mechanism: The paper uses the Eckart-Young-Mirsky theorem to bound the approximation error by the (RL+1)-th singular value of the error matrix, where R is the LoRA-rank and L is the depth of the frozen model.
- Core assumption: The rank of the error matrix is bounded and the low-rank approximation of the error matrix converges as R increases.
- Evidence anchors:
  - [abstract] "We also quantify the approximation error when LoRA-rank is lower than the threshold."
  - [section] "We characterize the approximation error between the finetuned model and the target model as a function of the LoRA-rank"
  - [corpus] Weak - no direct corpus evidence cited for this specific error quantification.
- Break condition: If the error matrix has a flat singular value spectrum, increasing R may not reduce the approximation error significantly.

### Mechanism 3
- Claim: For Transformer networks, a rank of (embedding size / 2) LoRA adapters are sufficient to exactly represent any target model of the same size.
- Mechanism: The paper shows that by adding LoRA adapters to the self-attention layers and the first feedforward layer in each transformer block, the adapted model can exactly match the target model. This is achieved by aligning the intermediate outputs of the frozen model with those of the target model.
- Core assumption: The weight matrices of both the target and frozen models and certain combinations of these matrices with low-rank approximations are non-singular.
- Evidence anchors:
  - [abstract] "For Transformer networks, we show any model can be adapted to a target model of the same size with rank-(embedding size / 2) LoRA adapters."
  - [section] "We identify the necessary LoRA-rank for adapting a frozen model to exactly match a target model."
  - [corpus] Weak - no direct corpus evidence cited for this specific Transformer result.
- Break condition: If the non-singularity assumption fails, the low-rank decomposition cannot guarantee exact representation in the Transformer case.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its application to low-rank matrix approximation.
  - Why needed here: SVD is used to decompose the error matrix and find its best rank approximation, which is crucial for the LoRA adapter construction.
  - Quick check question: What is the relationship between the rank of a matrix and its SVD? How is the best rank-r approximation of a matrix computed using SVD?

- Concept: Matrix norms (Frobenius norm and spectral norm) and their properties.
  - Why needed here: Matrix norms are used to measure the approximation error between the adapted model and the target model.
  - Quick check question: What is the difference between the Frobenius norm and the spectral norm of a matrix? How do these norms relate to the rank of a matrix?

- Concept: ReLU activation function and its properties.
  - Why needed here: The ReLU function introduces non-linearity in the neural networks, and the paper needs to handle this non-linearity when constructing LoRA adapters.
  - Quick check question: What is the effect of the ReLU activation function on the output of a neural network layer? How can the non-linearity introduced by ReLU be handled in the analysis of LoRA adapters?

## Architecture Onboarding

- Component map:
  - Frozen model (f0): The pre-trained model that remains fixed during adaptation
  - Target model (f): The model that the adapted model aims to represent
  - LoRA adapters: Low-rank matrices added to the weight matrices of the frozen model to adapt it to the target model
  - Bias vectors: Parameters that can be updated in addition to the LoRA adapters

- Critical path:
  1. Compute the error matrix between the target model's weights and the frozen model's weight product
  2. Find the best rank approximation of the error matrix using SVD
  3. Decompose the best rank approximation into rank-bounded terms
  4. Construct LoRA adapters to match these decomposed terms
  5. Update the bias vectors to align the intermediate outputs of the frozen and target models

- Design tradeoffs:
  - Rank vs. Expressiveness: Higher LoRA-rank allows for more expressive adaptation but increases the number of tunable parameters
  - Depth vs. Width: The required LoRA-rank depends on the width of the frozen model and the ratio of the target model's depth to the frozen model's depth
  - Exact vs. Approximate: Exact representation of the target model requires meeting the rank threshold, while approximation is possible with lower rank at the cost of increased error

- Failure signatures:
  - Non-singularity assumption failure: If the weight matrices of the frozen model or certain combinations with low-rank approximations are singular, the LoRA adapter construction may fail
  - Rank underestimation: If the LoRA-rank is set too low, the approximation error may be significant, and the adapted model may not perform well on the target task
  - Overfitting: If the LoRA-rank is set too high, the adapted model may overfit to the target task and not generalize well to new data

- First 3 experiments:
  1. Linear model approximation: Test the LoRA adapter construction on a simple linear model with known weights to verify exact representation when the rank threshold is met
  2. FNN approximation with varying ranks: Experiment with different LoRA-ranks on a fully connected neural network and measure the approximation error as a function of rank
  3. TFN approximation with multi-head attention: Apply the LoRA adapter construction to a Transformer network with multi-head attention and evaluate its ability to exactly represent the target model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the approximation error and the LoRA-rank when the rank is below the critical threshold for FNN and TFN cases?
- Basis in paper: [explicit] The paper provides an upper bound on the approximation error for FNNs when the LoRA-rank is lower than the critical threshold, but it does not provide a precise relationship between the approximation error and the LoRA-rank.
- Why unresolved: The paper only provides an upper bound on the approximation error, which may not be tight. A more precise relationship between the approximation error and the LoRA-rank would provide a better understanding of the expressive power of LoRA.
- What evidence would resolve it: Conducting numerical experiments to empirically determine the relationship between the approximation error and the LoRA-rank for various FNN and TFN architectures and comparing the results with the theoretical upper bound.

### Open Question 2
- Question: How does the performance of LoRA compare to other parameter-efficient fine-tuning methods, such as prompt tuning or prefix tuning, in terms of expressive power and computational efficiency?
- Basis in paper: [explicit] The paper focuses solely on the expressive power of LoRA and does not compare it to other parameter-efficient fine-tuning methods.
- Why unresolved: The paper does not provide a comprehensive comparison of LoRA with other parameter-efficient fine-tuning methods, making it difficult to determine its relative advantages and disadvantages.
- What evidence would resolve it: Conducting numerical experiments to compare the performance of LoRA with other parameter-efficient fine-tuning methods on various tasks and architectures, and analyzing the trade-offs between expressive power and computational efficiency.

### Open Question 3
- Question: How does the inclusion of skip connections and layer normalization in TFNs affect the expressive power of LoRA?
- Basis in paper: [explicit] The paper simplifies the TFN architecture by omitting skip connections and layer normalization for analytical feasibility.
- Why unresolved: The paper does not investigate the impact of skip connections and layer normalization on the expressive power of LoRA in TFNs.
- What evidence would resolve it: Conducting theoretical analysis and numerical experiments to study the effect of skip connections and layer normalization on the expressive power of LoRA in TFNs, and comparing the results with the simplified TFN case presented in the paper.

### Open Question 4
- Question: What is the optimal strategy for partitioning the layers of the frozen model when applying LoRA to FNNs?
- Basis in paper: [inferred] The paper proposes a uniform partitioning strategy for FNNs, but acknowledges that this may not always yield optimal results.
- Why unresolved: The paper does not provide a general solution for determining the optimal partitioning strategy for FNNs when applying LoRA.
- What evidence would resolve it: Developing a theoretical framework for determining the optimal partitioning strategy based on the characteristics of the target and frozen models, and validating the results through numerical experiments on various FNN architectures.

## Limitations
- The theoretical rank bounds appear to be loose upper bounds that may overestimate practical requirements
- The analysis relies on non-singularity assumptions that may not hold for all model pairs
- The results are specific to fully connected networks and standard Transformers, with unclear generalization to other architectures
- The paper does not compare LoRA's expressive power to other parameter-efficient fine-tuning methods

## Confidence
- **High Confidence**: The basic framework for analyzing LoRA expressive power through matrix decomposition and singular value analysis is sound
- **Medium Confidence**: The specific rank bounds for exact representation are mathematically correct given stated assumptions, but practical relevance is questionable
- **Low Confidence**: The extension of results to arbitrary neural network architectures beyond the specific cases studied

## Next Checks
1. **Singular Value Spectrum Analysis**: Systematically analyze the singular value distributions of error matrices across different model pairs and tasks to validate whether the assumption of rapidly decaying singular values holds in practice.

2. **Robustness to Non-Singularity Violations**: Construct experiments where non-singularity assumptions are deliberately violated and measure how approximation quality degrades to establish the practical importance of these theoretical conditions.

3. **Cross-Architecture Generalization Study**: Apply the theoretical framework to convolutional neural networks and other architectures not covered in the paper to assess the generality of the analysis approach.