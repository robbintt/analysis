---
ver: rpa2
title: Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators
  in Neural Networks
arxiv_id: '2303.01538'
source_url: https://arxiv.org/abs/2303.01538
tags:
- importance
- perturbation
- scores
- estimators
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating post-hoc interpretability
  methods for deep neural networks (DNNs), which assign importance scores to input
  features. The main issue is that the change in prediction accuracy after perturbing
  features deemed important may stem from perturbation artifacts rather than actual
  information removal, since perturbed samples are out of distribution compared to
  the training data.
---

# Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators in Neural Networks

## Quick Facts
- arXiv ID: 2303.01538
- Source URL: https://arxiv.org/abs/2303.01538
- Reference count: 37
- Primary result: Feature Perturbation Augmentation (FPA) improves reliability of evaluating importance estimators by making models robust to perturbed inputs

## Executive Summary
This paper addresses the challenge of evaluating post-hoc interpretability methods for deep neural networks by proposing Feature Perturbation Augmentation (FPA). The core insight is that perturbed samples during evaluation are out-of-distribution compared to training data, causing unreliable measurements. FPA solves this by training models with perturbed inputs, making them robust to feature masking during evaluation. Through extensive experiments on CIFAR-10, Food101, and ImageNet, the authors demonstrate that FPA mitigates perturbation artifacts and reveals that signed importance scores better capture model behavior than unsigned ones.

## Method Summary
FPA is a data augmentation technique that creates and adds perturbed images during model training. During training, with probability p, the method iterates through input features and with probability p1 sets them to zero, and with probability p2 creates a square of zeros of random size. This exposes the model to masked features during training, making it robust to feature masking during evaluation. The method uses ResNet-18 for CIFAR-10 and ResNet-50 for Food101 and ImageNet. Four importance estimators (VG, IG, SG, SQ-SG) are evaluated in both signed and unsigned variants, with fidelity measured as the area between minimal and maximal information fraction curves.

## Key Results
- FPA training results in delayed decrease in logits for MIF curves and initial increase for LIF curves
- Signed importance estimators consistently outperform unsigned ones when FPA is used
- Sign fluctuations in importance scores reflect meaningful model behavior, indicating alternating evidence and counter-evidence within regions of interest
- The fidelity metric (area between LIF and MIF curves) provides a reliable comparison between importance estimators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training with perturbed data prevents perturbation artifacts during evaluation
- Mechanism: FPA exposes the model to masked features during training, making it robust to feature masking during evaluation
- Core assumption: Out-of-distribution perturbations cause unexpected model behavior
- Evidence anchors:
  - [abstract]: "perturbed samples in the test dataset are out of distribution (OOD) compared to the training dataset"
  - [section]: "The major challenge with evaluating importance estimators is the lack of a ground truth against which a given estimator can be compared"
  - [corpus]: No direct corpus evidence; claim relies on paper's internal reasoning

### Mechanism 2
- Claim: Signed importance scores better capture model behavior than unsigned ones
- Mechanism: Signed scores preserve direction of feature influence (evidence vs counter-evidence)
- Core assumption: Model uses both positive and negative feature contributions for predictions
- Evidence anchors:
  - [section]: "negative importance scores from the signed estimators may imply counter-evidence for the predicted class"
  - [section]: "we use the area A between the MIF and LIF curves as a metric to measure the relative fidelity of importance estimators"
  - [corpus]: Weak corpus evidence; claim is primarily supported by experimental results

### Mechanism 3
- Claim: Sign fluctuations in importance scores reflect meaningful model behavior
- Mechanism: Alternating positive/negative scores within ROI indicate complex feature interactions
- Core assumption: Deep networks use intricate feature combinations for predictions
- Evidence anchors:
  - [section]: "the sign fluctuations may actually be meaningful" and "the model often focuses on the ROI, but within, it may find rapidly alternating evidence and counter-evidence"
  - [section]: "masking pixels with a negative importance score coincides with an increase in the logit values"
  - [corpus]: No direct corpus evidence; claim is novel interpretation of results

## Foundational Learning

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: Understanding why perturbed test samples cause model misbehavior
  - Quick check question: What happens when a model encounters data that differs from training distribution?

- Concept: Adversarial robustness
  - Why needed here: Perturbations during evaluation act like adversarial attacks
  - Quick check question: How do small input changes affect model predictions?

- Concept: Attribution methods
  - Why needed here: Understanding how importance scores are computed and interpreted
  - Quick check question: What's the difference between signed and unsigned attribution methods?

## Architecture Onboarding

- Component map: FPA -> Data augmentation -> Training loop -> Evaluation -> Fidelity metric (A)
- Critical path: FPA application -> Model training -> Importance score computation -> Perturbation curves -> Fidelity calculation
- Design tradeoffs: Accuracy vs interpretability (slight accuracy drop for better evaluation)
- Failure signatures: No improvement in perturbation curves, decreased model accuracy, inconsistent sign behavior
- First 3 experiments:
  1. Implement basic FPA with p1_max=0.25, p2=0.1, smax=3 on CIFAR-10 and verify perturbation curves
  2. Compare signed vs unsigned importance scores on a small dataset
  3. Test different FPA parameters (p1_max, p2, smax) and measure impact on fidelity metric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sign of importance scores in saliency maps reliably indicate evidence vs counter-evidence for model predictions, or is it still an unresolved question?
- Basis in paper: [explicit] The paper concludes that sign fluctuations may not be a bug but a feature, reflecting alternating evidence/counter-evidence within the ROI. However, they only tested on 3 datasets and 4 importance estimators.
- Why unresolved: The authors acknowledge more datasets and importance estimators should be tested to generalize their findings. The behavior may not hold universally.
- What evidence would resolve it: Testing on a wider range of datasets and importance estimators, including non-gradient-based methods, would clarify if sign fluctuations are consistently meaningful.

### Open Question 2
- Question: What is the optimal balance between model performance and robustness to perturbations when using Feature Perturbation Augmentation (FPA)?
- Basis in paper: [explicit] The authors note there is a trade-off between interpretability and accuracy when using FPA. They did a partial grid search for FPA parameters but did not optimize for both performance and robustness.
- Why unresolved: The paper does not explore the full parameter space or provide guidance on choosing FPA parameters to balance accuracy and robustness.
- What evidence would resolve it: Comprehensive grid searches and ablation studies to determine FPA parameters that maximize both model performance and robustness to perturbations.

### Open Question 3
- Question: Can FPA be applied to other evaluation methods beyond perturbation-based ones to improve their reliability?
- Basis in paper: [explicit] The authors mention FPA could potentially be applied to methods like Performance Information Curves that use bokeh filters, but do not test this.
- Why unresolved: The paper only demonstrates FPA's benefits for perturbation-based evaluation methods. Its utility for other evaluation methods is speculative.
- What evidence would resolve it: Applying FPA to other evaluation methods and empirically demonstrating improvements in reliability and robustness.

## Limitations
- FPA introduces a slight accuracy drop that is not quantified
- The method assumes perturbation artifacts are the primary evaluation challenge
- Sign interpretation of importance scores lacks strong corpus support

## Confidence
- FPA improves perturbation-based evaluation: **High**
- Signed estimators outperform unsigned ones: **Medium**
- Sign fluctuations reflect meaningful model behavior: **Low**

## Next Checks
1. Quantify the accuracy trade-off introduced by FPA across all datasets
2. Test FPA's effectiveness on non-image domains where OOD perturbation behavior may differ
3. Validate the sign interpretation hypothesis using ablation studies that isolate positive vs negative feature contributions