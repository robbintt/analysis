---
ver: rpa2
title: 'DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4'
arxiv_id: '2303.11032'
source_url: https://arxiv.org/abs/2303.11032
tags:
- chatgpt
- gpt-4
- data
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first application of ChatGPT and GPT-4 to
  de-identify medical text data, using a prompt-based zero-shot learning approach.
  The DeID-GPT framework integrates HIPAA identifiers into prompts and processes clinical
  notes through large language models to automatically redact protected health information.
---

# DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4

## Quick Facts
- arXiv ID: 2303.11032
- Source URL: https://arxiv.org/abs/2303.11032
- Reference count: 40
- Primary result: GPT-4 with explicit prompts achieves over 99% de-identification accuracy on i2b2/UTHealth dataset

## Executive Summary
This work presents the first application of ChatGPT and GPT-4 to de-identify medical text data using a prompt-based zero-shot learning approach. The DeID-GPT framework integrates HIPAA identifiers into prompts and processes clinical notes through large language models to automatically redact protected health information. When evaluated on the i2b2/UTHealth dataset, GPT-4 with an explicit prompt achieved over 99% de-identification accuracy, significantly outperforming fine-tuned baseline models such as ClinicalBERT (97.4%) and RoBERTa (94.7%). The approach eliminates the need for manual annotation and model fine-tuning, demonstrating high accuracy, generalizability, and adaptability for privacy protection in medical text.

## Method Summary
The method uses GPT-4 in a zero-shot learning paradigm, where clinical text is processed through carefully crafted prompts that explicitly define the de-identification task and HIPAA identifier categories. The approach involves parsing XML clinical notes from the i2b2/UTHealth dataset, constructing prompts that incorporate task statements, commands, and rules for handling PHI, and submitting these through the GPT-4 API. Entity-wise accuracy is calculated by comparing the model's redacted output against ground truth annotations, measuring the proportion of correctly identified and masked PHI entities.

## Key Results
- GPT-4 with explicit prompts achieved over 99% de-identification accuracy on the i2b2/UTHealth dataset
- GPT-4 outperformed fine-tuned baseline models (ClinicalBERT at 97.4% and RoBERTa at 94.7%)
- Prompt engineering significantly improved performance, with optimal prompts achieving 0.929 accuracy compared to 0.686 with suboptimal prompts
- The zero-shot approach eliminated the need for manual annotation and model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves high de-identification accuracy through zero-shot in-context learning when provided with well-crafted prompts.
- Mechanism: The model leverages its pre-trained knowledge and language understanding to recognize and redact protected health information (PHI) categories without requiring task-specific fine-tuning.
- Core assumption: GPT-4's training data and architecture allow it to generalize to medical text de-identification tasks when given clear task instructions.
- Evidence anchors:
  - [abstract] "When evaluated on the i2b2/UTHealth dataset, GPT-4 with an explicit prompt achieved over 99% de-identification accuracy"
  - [section] "Experimental results show that GPT-4 achieves the highest de-identification accuracy (over 0.99) in a zero-shot scenario when provided with an optimal, explicitly specified prompt"
  - [corpus] Weak evidence - corpus shows related work on LLMs for de-identification but doesn't directly validate this specific mechanism
- Break condition: The mechanism breaks when prompts are poorly designed or when PHI categories fall outside GPT-4's learned patterns.

### Mechanism 2
- Claim: Prompt engineering significantly improves LLM performance on de-identification tasks.
- Mechanism: Well-designed prompts that explicitly define the task, specify output format, and provide concrete examples help the model understand and execute the de-identification task more effectively.
- Core assumption: LLMs are sensitive to prompt quality and can be guided to better performance through careful prompt construction.
- Evidence anchors:
  - [section] "We have presented the optimal prompt template in the previous section. In this section, we present ineffective prompts that lead to sub-optimal results"
  - [section] "an optimally designed prompt improves ChatGPT performance from 0.686 to 0.929"
  - [corpus] Weak evidence - corpus mentions prompt engineering but doesn't validate its impact on de-identification accuracy
- Break condition: The mechanism breaks when prompts lack specificity or contain contradictory instructions.

### Mechanism 3
- Claim: GPT-4's performance on de-identification is less sensitive to prompt quality than ChatGPT.
- Mechanism: GPT-4's larger scale and more advanced architecture allow it to achieve good results even with simpler prompts, though optimal prompts still yield superior performance.
- Core assumption: GPT-4's enhanced capabilities provide more robust performance across varying prompt qualities.
- Evidence anchors:
  - [section] "GPT-4 is less susceptible to defects in the prompt, since it nonetheless performs well even when fed with a simple, implicit prompt"
  - [section] "an optimally designed prompt propels the accuracy of GPT-4 to a new level"
  - [corpus] Weak evidence - corpus doesn't directly compare GPT-4 and ChatGPT prompt sensitivity
- Break condition: The mechanism breaks when prompts are completely inadequate or contradictory.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The approach relies on GPT-4's ability to perform de-identification without task-specific training data
  - Quick check question: What distinguishes zero-shot learning from few-shot learning in the context of LLMs?

- Concept: Named Entity Recognition (NER)
  - Why needed here: De-identification fundamentally requires identifying and classifying PHI entities in text
  - Quick check question: How does NER in medical text differ from NER in general text?

- Concept: Prompt engineering
  - Why needed here: The success of the approach depends heavily on designing effective prompts that guide the model
  - Quick check question: What are the key components of an effective prompt for medical text de-identification?

## Architecture Onboarding

- Component map: XML clinical notes -> Text extraction -> Prompt construction -> GPT-4 API call -> Post-processing -> Evaluation

- Critical path: XML → Text extraction → Prompt construction → GPT-4 API call → Post-processing → Evaluation

- Design tradeoffs:
  - Zero-shot learning eliminates fine-tuning needs but may be less precise than fine-tuned models
  - API-based approach requires internet connectivity and raises privacy concerns
  - Prompt-based approach is interpretable but sensitive to prompt quality

- Failure signatures:
  - Missing PHI entities in output (under-deidentification)
  - Over-deidentification (masking non-PHI content)
  - Inconsistent formatting or structure changes
  - API errors or timeouts

- First 3 experiments:
  1. Test basic prompt with simple clinical note to verify API connectivity and basic functionality
  2. Test with known PHI examples to validate detection accuracy
  3. Compare performance with and without explicit examples in prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DeID-GPT compare to fine-tuned domain-specific LLMs trained on medical text data?
- Basis in paper: [explicit] The paper compares DeID-GPT to fine-tuned models like ClinicalBERT but does not explore fine-tuned LLMs like BioBERT or domain-specific models.
- Why unresolved: The paper only evaluates DeID-GPT against fine-tuned transformer models, not against LLMs fine-tuned on medical text data.
- What evidence would resolve it: Benchmarking DeID-GPT against fine-tuned domain-specific LLMs (e.g., BioBERT, ClinicalBERT) on medical text de-identification tasks.

### Open Question 2
- Question: Can DeID-GPT be effectively deployed locally in hospitals with limited computational resources?
- Basis in paper: [inferred] The paper mentions the need for local deployment but does not provide empirical evidence or implementation details.
- Why unresolved: The paper only discusses the potential for local deployment without testing or providing technical solutions.
- What evidence would resolve it: Successful local deployment of DeID-GPT on hospital servers with limited resources, including performance and resource usage metrics.

### Open Question 3
- Question: How well does DeID-GPT generalize to other languages and non-English medical text data?
- Basis in paper: [explicit] The paper evaluates DeID-GPT on English clinical notes but does not test its performance on multilingual or non-English medical text.
- Why unresolved: The paper only tests DeID-GPT on English data and does not explore its cross-lingual capabilities.
- What evidence would resolve it: Testing DeID-GPT on medical text data in multiple languages and evaluating its de-identification accuracy across different linguistic contexts.

## Limitations

- Evaluation was conducted on a single dataset (i2b2/UTHealth), limiting generalizability claims
- The approach requires API access to GPT-4, raising privacy and cost concerns for healthcare organizations
- Lacks detailed ablation studies on prompt components, making it difficult to understand which prompt elements are most critical

## Confidence

**High confidence** in GPT-4's superior performance (over 99% accuracy) compared to fine-tuned baselines, as this claim is directly supported by experimental results on the i2b2 dataset.

**Medium confidence** in the zero-shot learning approach's generalizability, as the paper demonstrates strong performance on one dataset but doesn't validate across multiple medical text sources or different medical specialties.

**Medium confidence** in the claim that prompt engineering significantly impacts performance, though the specific mechanisms by which different prompt components contribute to success remain incompletely understood.

## Next Checks

1. **Cross-dataset validation**: Test the same prompt-based approach on additional de-identification datasets (e.g., nursing notes, pathology reports, or multilingual medical texts) to assess generalizability beyond the i2b2 corpus.

2. **Critical PHI analysis**: Conduct a stratified evaluation focusing on the most sensitive PHI categories (patient names, medical record numbers) to verify that the 99% accuracy claim holds for the most privacy-critical entities.

3. **Cost-benefit analysis**: Measure the per-document processing time and cost of the GPT-4 API approach versus fine-tuned models, including the trade-offs between accuracy, speed, and expense for real-world deployment scenarios.