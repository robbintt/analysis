---
ver: rpa2
title: Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces
arxiv_id: '2309.16597'
source_url: https://arxiv.org/abs/2309.16597
tags:
- mphd
- prior
- function
- training
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MPHD, a transfer learning method for Bayesian
  optimization (BO) on heterogeneous search spaces. The key idea is to pre-train a
  hierarchical Gaussian process (GP) with domain-specific priors using a neural network
  that maps from domain contexts to GP hyperparameters.
---

# Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces

## Quick Facts
- arXiv ID: 2309.16597
- Source URL: https://arxiv.org/abs/2309.16597
- Authors: 
- Reference count: 40
- One-line primary result: MPHD is a transfer learning method for Bayesian optimization that learns domain-specific GP priors, achieving superior sample efficiency on heterogeneous search spaces.

## Executive Summary
This paper introduces MPHD, a transfer learning method for Bayesian optimization (BO) on heterogeneous search spaces. The key idea is to pre-train a hierarchical Gaussian process (GP) with domain-specific priors using a neural network that maps from domain contexts to GP hyperparameters. MPHD consists of two steps: first, estimating GP parameters for each training domain; second, training the neural network to map domain contexts to GP hyperparameter priors. The method is theoretically proven to be asymptotically consistent, meaning it converges to the ground truth solution as the number of training functions increases. Empirical results demonstrate that MPHD significantly improves sample efficiency for BO on functions with unseen search spaces compared to existing methods.

## Method Summary
MPHD is a transfer learning method for Bayesian optimization that learns domain-specific Gaussian process priors. It uses a two-step process: first, estimating GP parameters for each training domain; second, training a neural network to map domain contexts to GP hyperparameter priors. The pre-trained model can then generate domain-specific hierarchical GPs as priors for new test functions, which can be seamlessly integrated with BO. MPHD is theoretically proven to be asymptotically consistent and empirically demonstrates superior sample efficiency compared to existing methods on challenging BO transfer learning problems involving 17 search spaces across multiple benchmark datasets.

## Key Results
- MPHD significantly improves sample efficiency for BO on functions with unseen search spaces compared to existing methods
- MPHD achieves superior or comparable performance on challenging BO transfer learning problems involving 17 search spaces across multiple benchmark datasets
- MPHD is asymptotically consistent, meaning it converges to the ground truth solution as the number of training functions increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPHD learns a transferable prior for Gaussian processes by mapping domain contexts to GP hyperparameter distributions.
- Mechanism: MPHD trains a neural network to map domain-specific contexts (e.g., number of continuous/discrete dimensions) to hyperparameters of Gamma or Normal priors for GP parameters. This enables generating domain-specific hierarchical GPs for new, unseen search spaces.
- Core assumption: The relationship between domain characteristics and GP hyperparameter distributions is learnable and generalizable.
- Evidence anchors:
  - [abstract] "MPH D can generate a customized hierarchical GP as the prior for the test function"
  - [section] "MPH D learns a model with a mapping from domain-specific contexts to specifications of hierarchical GPs"
  - [corpus] No direct evidence found in corpus; MPHD is novel per paper.
- Break condition: If the context-to-hyperparameter mapping is not sufficiently expressive or the training data is too heterogeneous, the learned priors may not generalize.

### Mechanism 2
- Claim: Pre-training on heterogeneous domains improves BO sample efficiency by providing better initial priors.
- Mechanism: By pre-training on a super-dataset containing functions from diverse domains, MPHD learns a prior that captures common structure across domains. This prior is then used to initialize BO on new tasks, reducing the number of evaluations needed to find good solutions.
- Core assumption: Functions from different domains share some underlying structure that can be captured in the prior.
- Evidence anchors:
  - [abstract] "MPHD significantly improves sample efficiency for BO on functions with unseen search spaces compared to existing methods"
  - [section] "MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces"
  - [corpus] No direct evidence found in corpus; MPHD is novel per paper.
- Break condition: If the functions in different domains are too dissimilar, the shared prior may not be helpful and could even hurt performance.

### Mechanism 3
- Claim: Asymptotic consistency ensures MPHD learns the true prior as the number of training functions increases.
- Mechanism: The paper proves that as the number of training functions (and their observations) increases, the estimated GP parameters converge to the true parameters, and the learned hyperparameters of the prior distributions also converge to their true values.
- Core assumption: The increasing domain setting applies, where observations from multiple functions can be viewed as samples from a single function with increasing domain.
- Evidence anchors:
  - [abstract] "Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance"
  - [section] "MPHD is asymptotically consistent, meaning that it converges to the ground truth solution as the number of training functions increases"
  - [corpus] No direct evidence found in corpus; MPHD is novel per paper.
- Break condition: If the increasing domain assumption is violated (e.g., observations are not sufficiently spaced), the asymptotic consistency may not hold.

## Foundational Learning

- Concept: Gaussian Processes (GPs)
  - Why needed here: MPHD uses GPs as the surrogate model for BO and learns priors over GP parameters.
  - Quick check question: What are the key components of a GP (mean function, kernel, hyperparameters) and how are they typically set in BO?

- Concept: Bayesian Optimization (BO)
  - Why needed here: MPHD is a transfer learning method for BO, so understanding the BO framework is crucial.
  - Quick check question: How does BO use a surrogate model (typically a GP) to select the next point to evaluate, and what is the role of the prior in this process?

- Concept: Transfer Learning
  - Why needed here: MPHD is a transfer learning method that aims to transfer knowledge from training functions to new tasks.
  - Quick check question: What are the key challenges in transfer learning for BO, and how does MPHD address them?

## Architecture Onboarding

- Component map: Super-dataset -> Dataset -> Sub-dataset -> Neural network (ϕ) -> Hierarchical GP -> BO
- Critical path:
  1. Pre-train the model ϕ on the super-dataset by estimating GP parameters for each training domain and training ϕ to map contexts to GP hyperparameter priors.
  2. Use the pre-trained model ϕ to generate domain-specific hierarchical GPs as priors for new test functions.
  3. Integrate the hierarchical GPs with an acquisition function (e.g., Probability of Improvement) to perform BO on the new functions.
- Design tradeoffs:
  - Expressiveness vs. generalizability of the neural network ϕ.
  - Complexity of the GP model (e.g., kernel choice, number of hyperparameters) vs. ease of learning.
  - Size and diversity of the training super-dataset vs. computational cost of pre-training.
- Failure signatures:
  - Poor BO performance on new tasks, indicating the learned priors are not helpful.
  - High variance in the estimated GP parameters, suggesting the training data is not sufficient or representative.
  - Difficulty in training the neural network ϕ, possibly due to complex or conflicting patterns in the data.
- First 3 experiments:
  1. Test MPHD on a simple synthetic super-dataset with a fixed GP prior across all domains to verify the basic mechanism.
  2. Test MPHD on a synthetic super-dataset with domain-specific GP priors to assess the ability to learn complex, context-dependent priors.
  3. Test MPHD on a real-world super-dataset (e.g., HPO-B) to evaluate performance on practical BO tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MPHD perform when the ground truth GP prior is significantly different from the assumed GP prior (e.g., different kernel type or mean function)?
- Basis in paper: [inferred] The paper assumes a constant mean and anisotropic Matérn kernel, but in practice the true function could have a different structure.
- Why unresolved: The experiments only test on functions generated from the assumed GP prior.
- What evidence would resolve it: Testing MPHD on functions with different GP structures (e.g., linear mean, different kernel types) and comparing performance to baselines.

### Open Question 2
- Question: How sensitive is MPHD to the choice of context features used to parameterize the hierarchical GP prior?
- Basis in paper: [inferred] The paper uses a fixed context feature set, but the impact of different context choices is not explored.
- Why unresolved: The paper does not provide ablation studies on the context features.
- What evidence would resolve it: Running experiments with different context feature sets and analyzing the impact on MPHD's performance.

### Open Question 3
- Question: Can MPHD be extended to handle non-stationary kernels or learn the kernel structure?
- Basis in paper: [inferred] The paper assumes a fixed kernel type, but in practice the kernel structure could be unknown or vary across functions.
- Why unresolved: The paper does not explore methods for kernel structure learning.
- What evidence would resolve it: Developing and testing methods for learning the kernel structure within the MPHD framework.

### Open Question 4
- Question: How does the performance of MPHD scale with the number of training datasets and the complexity of the function space?
- Basis in paper: [explicit] The paper provides theoretical and empirical results on the asymptotic behavior of MPHD as the number of training datasets increases.
- Why unresolved: The paper does not provide a comprehensive analysis of the scaling behavior with varying numbers of training datasets and function complexities.
- What evidence would resolve it: Conducting experiments with different numbers of training datasets and varying function complexities, and analyzing the impact on MPHD's performance.

## Limitations
- The effectiveness of MPHD hinges on the ability of the neural network to accurately map domain contexts to GP hyperparameter distributions, which is not thoroughly explored in the paper.
- MPHD's performance on truly novel domains that are significantly different from the training data is not thoroughly explored.
- The asymptotic consistency proof relies on the increasing domain setting, which may not always hold in practice.

## Confidence
- High Confidence: The core mechanism of MPHD (pre-training a hierarchical GP with domain-specific priors using a neural network) is well-defined and the asymptotic consistency proof is provided.
- Medium Confidence: The empirical results demonstrating improved sample efficiency are promising, but the evaluation is limited to a specific set of benchmark datasets.
- Low Confidence: The generalizability of MPHD to highly diverse or novel domains is not thoroughly explored, and the assumptions underlying the theoretical analysis may not always hold in practice.

## Next Checks
1. Verify the synthetic data generation process by creating a controlled super-dataset with known GP priors across domains. Test MPHD's ability to recover these priors and assess its sensitivity to the choice of kernel and hyperparameters.
2. Experiment with different neural network architectures for the context-to-hyperparameter mapping. Evaluate the impact of network depth, width, and activation functions on the quality of the learned priors and BO performance.
3. Design an experiment where MPHD is trained on a diverse set of domains but tested on domains that are intentionally dissimilar to the training data. Assess the method's ability to generalize to truly novel search spaces and identify potential failure modes.