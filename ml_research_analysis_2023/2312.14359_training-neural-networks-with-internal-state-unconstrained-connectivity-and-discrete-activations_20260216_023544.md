---
ver: rpa2
title: Training Neural Networks with Internal State, Unconstrained Connectivity, and
  Discrete Activations
arxiv_id: '2312.14359'
source_url: https://arxiv.org/abs/2312.14359
tags:
- training
- state
- input
- such
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a training algorithm for a simple two-layer
  neural network with binary activations, internal state, and unconstrained connectivity.
  The model learns to represent temporal sequences by minimizing reconstruction error
  of the current input and previous state from the next state.
---

# Training Neural Networks with Internal State, Unconstrained Connectivity, and Discrete Activations

## Quick Facts
- arXiv ID: 2312.14359
- Source URL: https://arxiv.org/abs/2312.14359
- Reference count: 14
- One-line primary result: Achieves 82.2% accuracy on 4-class news classification using linear regression on features from binary stateful model

## Executive Summary
This paper presents a training algorithm for a simple two-layer neural network with binary activations, internal state, and unconstrained connectivity. The model learns to represent temporal sequences by minimizing reconstruction error of the current input and previous state from the next state. Applied to text classification, the model learns useful representations from a small subset of training data, achieving 82.2% accuracy on classifying news articles into four topics using linear regression. While this is below state-of-the-art transformer-based approaches, the model demonstrates the feasibility of training stateful architectures with discrete activations and few initial constraints.

## Method Summary
The paper proposes an unsupervised training approach where a two-layer network with binary Heaviside activations learns to minimize reconstruction error. The model computes the next state using a weighted combination of current input and previous state, then attempts to reconstruct both from the next state. Weight updates are based on outer products of reconstruction errors, scaled by learning rates. After training, average hidden states across time steps serve as feature vectors for supervised classification with linear regression.

## Key Results
- Achieves 82.2% accuracy on 4-class news classification (World, Sports, Business, Sci/Tech)
- Demonstrates effective learning with binary activations and sparse representations (density parameter d=0.1)
- Shows unsupervised pre-training can generate useful features for supervised classification
- Proves stateful models with discrete activations can learn temporal representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns useful representations by minimizing reconstruction error of both current input and previous state from the next state.
- Mechanism: At each time step, the model computes the next state ht+1 using a weighted combination of current input xt and previous state ht, then attempts to reconstruct both xt and ht from ht+1. The difference between actual and reconstructed values serves as a training signal to adjust weights W.
- Core assumption: Minimizing reconstruction error of both input and state leads to learning meaningful temporal representations that capture relevant information for classification.
- Evidence anchors:
  - [abstract] "The model learns to represent temporal sequences by minimizing reconstruction error of the current input and previous state from the next state."
  - [section] "The reconstruction error is the difference [xt, ht] − [x′t, h′t] between the true and the reconstructed input and state. The model learns by attempting to reduce the reconstruction error, by adjusting the weights"
- Break condition: If reconstruction error becomes trivially minimized (e.g., by always outputting zero vectors), the model would fail to learn useful representations.

### Mechanism 2
- Claim: Binary activations and sparse representations help the model focus on essential information while maintaining computational efficiency.
- Mechanism: The Heaviside step function creates binary activations (0 or 1), and the density parameter d encourages sparse representations by adjusting hidden neuron biases to maintain an average activation rate of d. This forces the model to be selective about what information to store in its state.
- Core assumption: Binary activations combined with sparsity constraints lead to more interpretable and robust representations compared to continuous activations.
- Evidence anchors:
  - [section] "Finally, we adjust the hidden biases as follows: b ← b + r(h) ⊙ (d − ht+1) (5) Here, d is the density parameter, provided as a vector of dimension n. Whenever some hidden neuron is inactive (i.e., the corresponding element of ht+1 is 0), the above rule increases the neuron's bias by r(h)d; if it is active, then it decreases the bias by r(h)(1 − d); this attempts to achieve an average activation rate of d for a given neuron (encouraging sparse representations if d is low)."
- Break condition: If d is set too low, the model may become overly sparse and lose important information; if too high, it may store irrelevant details.

### Mechanism 3
- Claim: The unsupervised training approach allows the model to learn general-purpose representations without requiring labeled data during training.
- Mechanism: The model is trained solely to minimize reconstruction error without using topic labels. After training, the average hidden states across time steps are used as feature vectors for supervised classification with linear regression.
- Core assumption: Good reconstruction of temporal sequences implies the model has learned to capture semantically meaningful features that are useful for downstream classification tasks.
- Evidence anchors:
  - [section] "Once the model has been thus trained, we use it to convert an additional 5000 samples... into fixed-sized feature vectors... We then use ridge regression to train a classifier on these feature vectors, with topic labels provided as ground truth."
  - [abstract] "Applied to text classification, the model learns useful representations from a small subset of training data, achieving 82.2% accuracy"
- Break condition: If the learned representations don't capture task-relevant information, supervised classification performance will be poor despite low reconstruction error.

## Foundational Learning

- Concept: Temporal sequence modeling with internal state
  - Why needed here: The paper's core contribution is demonstrating that stateful architectures with discrete activations can learn useful representations of sequential data like text
  - Quick check question: What distinguishes a stateful model from a stateless one in terms of how they process sequential inputs?

- Concept: Reconstruction-based learning without backpropagation
  - Why needed here: The paper proposes an alternative to gradient-based training that works with binary activations, using reconstruction error as the learning signal
  - Quick check question: How does the weight update rule in equation (3) differ from standard backpropagation through time?

- Concept: Feature extraction from hidden states
  - Why needed here: The paper converts temporal hidden states into fixed-size feature vectors for classification, which is a key step in applying the model to text classification
  - Quick check question: Why does the paper use the average state over the entire sentence rather than just the final state for classification?

## Architecture Onboarding

- Component map: xt (input) → W[x_t, h_t] + b → H(·) → ht+1 → W^T ht+1 + a → H(·) → [x′t, h′t] → error calculation → weight updates
- Critical path: xt and ht → W[x_t, h_t] + b → H(·) → ht+1 → W^T ht+1 + a → H(·) → [x′t, h′t] → error calculation → weight updates
- Design tradeoffs:
  - Binary vs continuous activations: binary provides interpretability and potential robustness but limits expressiveness
  - Sparse vs dense representations: sparse representations are more interpretable but may lose information
  - Unsupervised pre-training vs supervised training: unsupervised allows learning from unlabeled data but may not optimize directly for the task
- Failure signatures:
  - Model converges too quickly (reconstruction error plateaus early) → learning rate may be too high or architecture too simple
  - Classification accuracy close to baseline (character frequency) → model may be storing only surface-level patterns
  - Very low activation density → sparsity constraint may be too aggressive
  - All hidden neurons activate equally → model may not be learning selective features
- First 3 experiments:
  1. Train on synthetic sequential data where ground truth patterns are known to verify the model can learn simple temporal dependencies
  2. Vary the density parameter d to find optimal sparsity level for text classification performance
  3. Compare classification performance using different state aggregation methods (final state vs average state vs max pooling)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the training algorithm be modified to better leverage large training datasets while maintaining its effectiveness?
- Basis in paper: [explicit] The paper notes that the current training algorithm converges quickly and does not benefit from large quantities of training data, which is a significant limitation compared to state-of-the-art approaches.
- Why unresolved: The authors suggest exploring various ideas, such as introducing randomness, adjusting learning rates, or changing initialization schemes, but these remain untested hypotheses.
- What evidence would resolve it: Experimental results demonstrating improved performance on larger datasets using one or more of the proposed modifications.

### Open Question 2
- Question: Can purely local learning rules be discovered that effectively train stateful architectures with discrete activations?
- Basis in paper: [explicit] The authors propose exploring a space of 43 million possible local rules for weight updates based on binary neuron activations at different time steps.
- Why unresolved: This represents a massive search space that has not yet been systematically explored or evaluated.
- What evidence would resolve it: Discovery of one or more local rules that achieve competitive performance on benchmark tasks compared to existing training methods.

### Open Question 3
- Question: Do stateful models with binary activations demonstrate improved robustness to adversarial perturbations compared to traditional models?
- Basis in paper: [explicit] The authors suggest that binary activations and state management could potentially make the model more robust to adversarial attacks.
- Why unresolved: This hypothesis has not been tested with gradient-free attacks or other adversarial evaluation methods.
- What evidence would resolve it: Empirical comparison showing lower performance degradation under adversarial attacks for the proposed model versus baseline models.

### Open Question 4
- Question: Can formal verification techniques be effectively applied to models with binary inputs and states to prove properties about their behavior?
- Basis in paper: [explicit] The authors note that converting RNNs to finite state machines requires discretization, which would be unnecessary for their binary model.
- Why unresolved: The feasibility and effectiveness of applying formal methods to such models remains unexplored.
- What evidence would resolve it: Successful application of formal verification techniques to prove specific properties (e.g., safety, liveness) about trained models.

## Limitations
- The model achieves significantly lower accuracy (82.2%) compared to modern transformer-based approaches
- Training algorithm converges quickly and doesn't benefit from large training datasets
- Claims about robustness, explainability, and low power consumption are speculative and not empirically validated
- Performance evaluation is limited to a small subset (5000 samples) of a single text classification dataset

## Confidence

**Major Uncertainties:**
The paper presents a novel training algorithm for stateful neural networks with binary activations, but several limitations affect confidence in the results. The model achieves 82.2% accuracy on a 4-class news classification task, which is significantly below modern transformer-based approaches but represents a proof-of-concept for this architecture. The training algorithm's convergence properties and sensitivity to hyperparameters (particularly the density parameter d and learning rates) are not extensively explored. The evaluation uses a relatively small dataset subset (5000 training samples) compared to typical modern benchmarks.

**Confidence Labels:**
- High confidence: The core algorithm mechanics and training procedure are clearly described and reproducible
- Medium confidence: The classification results are reproducible but may not generalize well to larger datasets or more complex tasks
- Low confidence: Claims about robustness to adversarial perturbations, improved explainability, and low power consumption are speculative and not empirically validated

## Next Checks

1. Test model sensitivity to density parameter d across a wider range (0.05-0.5) to identify optimal sparsity levels and understand robustness to hyperparameter choice
2. Evaluate performance on a larger text classification dataset (e.g., full AG's News or IMDb reviews) to assess scalability beyond the small 5000-sample subset
3. Implement controlled adversarial perturbations to test the claimed robustness properties and measure actual performance degradation compared to baseline models