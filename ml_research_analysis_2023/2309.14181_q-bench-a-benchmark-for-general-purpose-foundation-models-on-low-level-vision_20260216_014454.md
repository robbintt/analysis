---
ver: rpa2
title: 'Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision'
arxiv_id: '2309.14181'
source_url: https://arxiv.org/abs/2309.14181
tags:
- image
- mllms
- low-level
- quality
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Q-Bench is a benchmark designed to assess general-purpose foundation
  models, particularly Multi-modality Large Language Models (MLLMs), on low-level
  vision tasks. It evaluates MLLMs across three abilities: low-level visual perception,
  description, and overall visual quality assessment.'
---

# Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision

## Quick Facts
- **arXiv ID:** 2309.14181
- **Source URL:** https://arxiv.org/abs/2309.14181
- **Reference count:** 35
- **Primary result:** MLLMs show preliminary low-level visual skills but are unstable and imprecise on low-level vision tasks

## Executive Summary
Q-Bench is a comprehensive benchmark designed to evaluate general-purpose foundation models, particularly Multi-modality Large Language Models (MLLMs), on low-level vision tasks. The benchmark assesses three distinct abilities: low-level visual perception, description, and overall visual quality assessment. Using domain-specific datasets and evaluation protocols, Q-Bench reveals that while MLLMs demonstrate some capability in low-level visual tasks, they remain unstable and imprecise, highlighting the need for further development in this area.

## Method Summary
Q-Bench evaluates MLLMs on low-level vision through three tasks using specialized datasets: LLVisionQA (2,990 images with perception questions), LLDescribe (499 images with expert-labeled descriptions), and multiple IQA datasets. The evaluation uses zero-shot settings without fine-tuning. Perception is measured through accuracy on multi-choice questions, description through GPT evaluation of completeness, preciseness, and relevance, and quality assessment through SRCC/PLCC scores. A softmax-based strategy converts token probabilities into quantifiable quality scores for IQA tasks.

## Key Results
- MLLMs significantly outperform random guess on low-level perception tasks (up to 31.25% improvement) but show unstable performance across datasets
- Description generation shows MLLMs can produce low-level attribute descriptions but with limited precision and completeness
- Softmax-based scoring strategy improves correlation with human perception compared to direct token outputs
- Overall, MLLMs demonstrate preliminary low-level visual skills but require substantial enhancement for reliable performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Q-Bench isolates low-level visual abilities from general-purpose MLLM capabilities by constructing domain-specific datasets and evaluation protocols.
- **Mechanism:** The benchmark separates low-level perception, description, and assessment into distinct tasks, each targeting unique abilities. Domain-specific questions and expert-labeled descriptions force MLLMs to engage with low-level visual features rather than relying on high-level reasoning.
- **Core assumption:** Low-level visual abilities are distinct from high-level understanding and can be evaluated independently.
- **Evidence anchors:** Abstract statement about systematic evaluation; section definitions of emerging language abilities; corpus shows neighboring papers focus on multimodal evaluation generally.
- **Break condition:** If perception tasks are too entangled with high-level reasoning, the benchmark would no longer isolate pure low-level abilities.

### Mechanism 2
- **Claim:** The softmax-based scoring strategy enables MLLMs to produce quantifiable outputs for IQA, overcoming weak measurability of direct token generation.
- **Mechanism:** Softmax pooling on logits of the two most frequent tokens produces continuous scores that better correlate with human opinion scores than hard selection.
- **Core assumption:** Probability distribution over tokens encodes meaningful perceptual information extractable via softmax.
- **Evidence anchors:** Section describing softmax pooling and its correlation benefits; corpus shows no direct evidence in neighboring work.
- **Break condition:** If two-token softmax fails to capture sufficient granularity or MLLMs consistently favor one token.

### Mechanism 3
- **Claim:** Expert-labeled golden descriptions provide high-quality reference for evaluating MLLM-generated descriptions.
- **Mechanism:** Experts write detailed descriptions (~58 words) covering low-level attributes, which GPT then uses to evaluate MLLM outputs across completeness, preciseness, and relevance.
- **Core assumption:** Expert descriptions are comprehensive and consistent enough to serve as reliable ground truth.
- **Evidence anchors:** Section on LLDescribe dataset and expert description writing; corpus shows weak evidence for annotation quality.
- **Break condition:** If expert descriptions are inconsistent or incomplete, GPT evaluation would yield unreliable scores.

## Foundational Learning

- **Concept:** Multi-modal grounding in visual attributes
  - **Why needed here:** MLLMs must correctly map visual distortions and low-level features to natural language responses.
  - **Quick check question:** Can the model distinguish between "overexposed" and "underexposed" without context cues?

- **Concept:** Zero-shot instruction following
  - **Why needed here:** The benchmark evaluates MLLMs without fine-tuning, requiring them to follow novel prompts.
  - **Quick check question:** Does the model produce coherent answers when prompted with "Rate the quality of the image" versus detailed instructions?

- **Concept:** Logit-to-probability conversion (softmax)
  - **Why needed here:** Enables conversion of token logits into continuous quality scores for IQA tasks.
  - **Quick check question:** How does softmax probability vary when input image shifts from "good" to "poor" quality?

## Architecture Onboarding

- **Component map:** LLVisionQA/LLDescribe/IQA datasets -> MLLM inference under zero-shot prompts -> GPT evaluation (description) or softmax scoring (assessment)
- **Critical path:** GPT-assisted evaluation requiring 5 rounds per example, which should be optimized first if scaling experiments
- **Design tradeoffs:** GPT evaluation avoids manual labeling but introduces variability; softmax pooling reduces bias but assumes only two tokens are sufficient
- **Failure signatures:** High variance in GPT scores indicates prompt instability; low SRCC/PLCC in IQA suggests poor perceptual alignment; high "yes" bias in perception indicates prompt design issues
- **First 3 experiments:**
  1. Run LLVisionQA dataset on baseline MLLM and measure bias toward "yes" vs "no" answers
  2. Compare softmax vs argmax scoring on small IQA subset to confirm improved correlation
  3. Generate MLLM descriptions for LLDescribe images and run GPT evaluation to confirm three-dimensional scoring works

## Open Questions the Paper Calls Out

- **Open Question 1:** Can MLLMs be fine-tuned to improve their low-level visual perception abilities without losing general-purpose capabilities? (Basis: MLLMs outperform random guess without explicit training; unresolved because paper only evaluates zero-shot settings)

- **Open Question 2:** How do MLLMs perform on low-level visual perception tasks when provided with higher-level contextual information as prompts? (Basis: MLLMs perform better on in-context questions; unresolved because paper doesn't systematically investigate contextual prompts)

- **Open Question 3:** What is the optimal strategy for MLLMs to provide quantifiable quality scores that align with human opinion scores? (Basis: Paper proposes softmax-based strategy shown to be effective; unresolved because alternative strategies not explored)

## Limitations

- Isolation of low-level abilities from high-level reasoning is uncertain, as perception tasks may still require some degree of contextual understanding
- Generalizability of softmax-based scoring strategy across different MLLM models and IQA datasets requires further validation
- Quality and consistency of expert annotations for description evaluation have not been thoroughly validated

## Confidence

- **High Confidence:** Benchmark design and dataset construction are clearly specified with reproducible evaluation pipeline
- **Medium Confidence:** Softmax-based scoring strategy shows improved correlation but needs validation across diverse scenarios
- **Low Confidence:** Claim that low-level abilities can be fully isolated from high-level reasoning lacks robust support

## Next Checks

1. **Bias Analysis in Perception Tasks:** Conduct detailed analysis of answer biases in LLVisionQA dataset to quantify MLLM response patterns and determine if perception task truly isolates low-level abilities

2. **Cross-Dataset Generalization of Softmax Scoring:** Test softmax-based scoring strategy on additional IQA datasets not included in original evaluation and compare against alternative methods to assess robustness

3. **Inter-Annotator Agreement for Descriptions:** Measure agreement among experts who created golden descriptions in LLDescribe to validate consistency of ground truth references