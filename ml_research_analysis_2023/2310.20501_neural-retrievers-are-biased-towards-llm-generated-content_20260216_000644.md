---
ver: rpa2
title: Neural Retrievers are Biased Towards LLM-Generated Content
arxiv_id: '2310.20501'
source_url: https://arxiv.org/abs/2310.20501
tags:
- llm-generated
- human-written
- corpus
- retrieval
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a new source bias in neural information retrieval
  (IR) models that favor large language model (LLM)-generated texts over human-written
  texts with similar semantic content. To evaluate this bias, the authors construct
  two new benchmarks (SciFact+AIGC and NQ320K+AIGC) by using LLMs to rewrite human-written
  corpus while preserving semantic meaning.
---

# Neural Retrievers are Biased Towards LLM-Generated Content

## Quick Facts
- **arXiv ID**: 2310.20501
- **Source URL**: https://arxiv.org/abs/2310.20501
- **Reference count**: 40
- **Primary result**: Neural information retrieval models consistently rank LLM-generated texts higher than human-written texts with similar semantic content.

## Executive Summary
This study reveals a new source bias in neural information retrieval models that systematically favors large language model (LLM)-generated content over human-written text with equivalent semantic meaning. The authors construct two new benchmarks by using LLMs to rewrite human-written corpora while preserving semantic content, then systematically evaluate multiple neural retrievers and re-rankers. Results show consistent preference for LLM-generated content across both first-stage retrieval and second-stage re-ranking tasks, with bias severity varying based on LLM generation temperature settings. The bias is explained through text compression analysis showing that LLM-generated texts have more focused semantics with less noise, making them easier for neural models to understand.

## Method Summary
The authors construct two new benchmarks (SciFact+AIGC and NQ320K+AIGC) by using LLMs to rewrite human-written corpora while preserving semantic meaning. They systematically test various neural retrieval models (TF-IDF, BM25, ANCE, BERM, TAS-B, Contriever) and re-rankers (MiniLM, monoT5) on these mixed corpora, comparing performance metrics for human-written versus LLM-generated content. The evaluation uses standard IR metrics (NDCG@K, MAP@K) calculated separately for each content type. Analysis includes SVD decomposition to examine semantic concentration and perplexity calculations to measure text predictability, providing theoretical insights into why neural models prefer LLM-generated content.

## Key Results
- Neural retrieval models consistently rank LLM-generated documents higher than human-written documents with similar semantic content (up to 19.1% NDCG@1 difference for ANCE).
- The bias extends to both first-stage neural retrievers and second-stage neural re-rankers, with re-rankers showing more severe bias (up to 67.3% relative NDCG@1 difference for monoT5).
- Lower temperature settings in LLM generation produce more conservative text with lower perplexity, leading to more substantial compression, greater topic concentration, and more pronounced source bias.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural retrieval models exhibit source bias, preferentially ranking LLM-generated texts higher than human-written texts with similar semantic content.
- Mechanism: LLM-generated texts have more focused semantics with less noise, making them easier for neural retrieval models to understand. This is supported by SVD analysis showing higher singular values for LLM-generated texts and lower perplexity scores.
- Core assumption: Both neural retrieval models (based on PLMs) and LLMs share similar Transformer-based architectures and pretraining paradigms, leading to similar biases in text prediction.
- Evidence anchors:
  - [abstract]: "neural retrieval models tend to rank LLM-generated documents higher"
  - [section]: "LLM-generated text maintains more focused semantics with minimal noise"
  - [corpus]: Weak - corpus analysis shows Jaccard similarity peaks around 0.4-0.8, but no direct SVD/perplexity data provided in corpus section

### Mechanism 2
- Claim: The source bias extends to both first-stage neural retrievers and second-stage neural re-rankers.
- Mechanism: Re-rankers, which fine-tune document ordering from initial retrieval results, show even more severe bias than retrievers, as evidenced by relative percentage differences up to 67.3% in NDCG@1 for monoT5 re-ranker.
- Core assumption: Re-rankers use similar PLM-based architectures as retrievers, inheriting the same bias tendencies.
- Evidence anchors:
  - [abstract]: "bias not confined to first-stage neural retrievers, but extends to second-stage neural re-rankers"
  - [section]: "re-ranking models once again demonstrate a bias in favor of LLM-generated content"
  - [corpus]: Weak - corpus section doesn't provide re-ranking specific analysis

### Mechanism 3
- Claim: Temperature settings in LLM generation affect the severity of source bias.
- Mechanism: Lower temperature coefficients (e.g., 0.2) produce more conservative and predictable text with lower perplexity, leading to more pronounced source bias. Higher temperatures (e.g., 1.0) create more diverse text that is less compressible and results in reduced bias.
- Core assumption: Temperature directly controls the randomness of word selection, affecting text compressibility and predictability.
- Evidence anchors:
  - [abstract]: "bias is more pronounced in models using higher temperature settings"
  - [section]: "lower temperature coefficients lead to more substantial compression, greater topic concentration, and a more pronounced source bias"
  - [corpus]: Weak - corpus section shows term similarity differences but doesn't directly link to temperature effects

## Foundational Learning

- Concept: Perplexity as a measure of language model confidence
  - Why needed here: The paper uses perplexity to demonstrate that LLM-generated texts are more comprehensible to PLMs, supporting the source bias mechanism
  - Quick check question: How does perplexity relate to a language model's understanding of text patterns?

- Concept: Singular Value Decomposition (SVD) for topic analysis
  - Why needed here: SVD is used to show that LLM-generated texts have more focused semantics with less noise, which explains why they're easier for neural models to understand
  - Quick check question: What do high singular values in SVD analysis indicate about a text corpus?

- Concept: Transformer architecture and pretraining paradigms
  - Why needed here: Understanding that both neural retrievers and LLMs use similar architectures helps explain why they share biases in text prediction
  - Quick check question: How do the training objectives of BERT-like models and LLMs differ, and how might this affect their text processing?

## Architecture Onboarding

- Component map: Data construction pipeline (human-written → LLM-generated corpus) -> Retrieval models (TF-IDF, BM25, ANCE, BERM, TAS-B, Contriever) -> Re-ranking models (MiniLM, monoT5) -> Evaluation metrics (NDCG@K, MAP@K, Relative Δ) -> Analysis tools (SVD, perplexity calculation, semantic embedding)

- Critical path: 1. Construct benchmark datasets (SciFact+AIGC, NQ320K+AIGC) 2. Generate LLM-rewritten corpus using prompts 3. Run retrieval models on mixed corpus 4. Calculate ranking metrics separately for human vs LLM content 5. Analyze results for source bias patterns 6. Conduct compression/perplexity analysis to explain bias

- Design tradeoffs: Using LLM rewriting vs human annotation for relevance labels (cost vs accuracy), choosing temperature settings for LLM generation (bias strength vs text quality), selecting evaluation metrics (individual corpus vs mixed corpus performance)

- Failure signatures: No significant performance difference between human and LLM corpora across all models, lexical models showing preference for LLM content instead of human content, SVD analysis showing no difference in semantic concentration between corpora

- First 3 experiments: 1. Reproduce the source bias experiment using BM25 as baseline retriever, measuring NDCG@1 difference between human and LLM corpora 2. Run SVD analysis on human vs LLM corpora to verify semantic concentration differences 3. Calculate perplexity scores for both corpora using BERT to confirm LLM texts are more predictable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively mitigate the source bias in neural retrieval models to ensure fair representation of both human-written and LLM-generated content?
- Basis in paper: [explicit] The authors discuss the potential concerns and risks posed by source bias in neural retrieval models and suggest that mitigating this bias is a crucial research direction.
- Why unresolved: While the paper identifies the source bias and provides theoretical insights, it does not offer concrete solutions or methods to address this bias. Developing effective mitigation strategies requires further research and experimentation.
- What evidence would resolve it: Empirical studies demonstrating successful mitigation techniques that reduce the preference for LLM-generated content in neural retrieval models, along with quantitative analysis of the impact on retrieval performance and fairness.

### Open Question 2
- Question: Does the source bias observed in neural retrieval models extend to other information systems beyond IR, such as recommender and advertising systems?
- Basis in paper: [inferred] The authors mention that the findings and conclusions are not IR-specific and suggest exploring whether this bias manifests in other information systems.
- Why unresolved: The paper primarily focuses on the impact of source bias in IR systems and does not investigate its presence in other domains. Further research is needed to determine if similar biases exist in different information systems.
- What evidence would resolve it: Comparative studies analyzing the behavior of retrieval, recommendation, and advertising models when exposed to both human-written and LLM-generated content, along with quantitative measures of bias in each system.

### Open Question 3
- Question: How does the source bias affect the understanding and ranking of multimodal content, such as images and text, in retrieval tasks?
- Basis in paper: [inferred] The authors suggest exploring the source bias on tasks beyond text, particularly in light of emerging generative models capable of generating high-quality images.
- Why unresolved: The paper primarily focuses on text-based retrieval tasks and does not investigate the impact of source bias on multimodal content. Understanding the behavior of retrieval models in multimodal scenarios requires further research.
- What evidence would resolve it: Empirical studies comparing the performance of multimodal retrieval models when exposed to human-generated and LLM-generated images and text, along with analysis of the impact on ranking and relevance judgments.

## Limitations
- The generated LLM content may not fully capture the diversity and complexity of naturally occurring LLM-generated content across different domains and applications.
- The study focuses on English-language corpora, limiting generalizability to other languages.
- The temperature setting analysis may not capture all nuances of LLM generation parameters that could affect retrieval performance.

## Confidence

- **High Confidence**: The core finding that neural retrievers exhibit source bias favoring LLM-generated content is well-supported by consistent results across multiple retrieval models and datasets (NDCG@1 differences up to 19.1% for ANCE).
- **Medium Confidence**: The explanation that LLM-generated texts have more focused semantics with less noise is supported by SVD analysis and perplexity calculations, though direct empirical validation of the text compression hypothesis would strengthen this claim.
- **Medium Confidence**: The observation that re-rankers show more severe bias than retrievers is based on relative percentage differences, but the absolute performance differences are relatively small (NDCG@1 differences around 0.5-1.0).

## Next Checks

1. **Cross-domain validation**: Test the source bias hypothesis on additional datasets from different domains (e.g., news articles, product reviews, technical documentation) to assess generalizability beyond scientific and question-answering corpora.

2. **Human evaluation study**: Conduct blinded human assessments of retrieval quality to determine whether the model preference for LLM content aligns with human judgment of document quality and relevance.

3. **Alternative LLM generation parameters**: Systematically vary generation parameters beyond temperature (e.g., top-k sampling, nucleus sampling) to determine whether the observed bias patterns hold across different LLM generation strategies.