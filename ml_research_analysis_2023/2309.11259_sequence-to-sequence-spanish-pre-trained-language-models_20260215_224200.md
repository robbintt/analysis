---
ver: rpa2
title: Sequence-to-Sequence Spanish Pre-trained Language Models
arxiv_id: '2309.11259'
source_url: https://arxiv.org/abs/2309.11259
tags:
- language
- spanish
- tasks
- linguistics
- barto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces sequence-to-sequence pre-trained language
  models for Spanish, addressing the scarcity of such models in this language. The
  authors present Spanish versions of BART (BARTO) and T5 (T5S), as well as BERT2BERT-style
  models, pre-trained exclusively on Spanish corpora.
---

# Sequence-to-Sequence Spanish Pre-trained Language Models

## Quick Facts
- arXiv ID: 2309.11259
- Source URL: https://arxiv.org/abs/2309.11259
- Reference count: 18
- The paper introduces Spanish versions of BART, T5, and BERT2BERT-style models, demonstrating competitive performance on summarization, split-and-rephrase, and generative question answering tasks.

## Executive Summary
This paper addresses the scarcity of sequence-to-sequence pre-trained language models for Spanish by introducing Spanish versions of BART (BARTO), T5 (T5S), and BERT2BERT-style models. The authors pre-train these models exclusively on Spanish corpora and evaluate their performance across multiple sequence-to-sequence tasks including summarization, split-and-rephrase, and generative question answering. The results show that BARTO and T5S achieve competitive performance, with BARTO excelling in long-form summarization tasks and T5S showing strong results in split-and-rephrase tasks. All models are made publicly available to support further research in Spanish natural language processing.

## Method Summary
The authors pre-train sequence-to-sequence models on Spanish corpora including OSCAR 21.09 (~160GB), mC4-es (~500GB), and SUC (~14GB), with additional Wikipedia data. BARTO is pre-trained with 6 encoder/decoder layers for 100,000 steps on 8 NVIDIA A100 GPUs, while T5S uses 12 encoder/decoder layers for 80,000 steps on 4 NVIDIA A100 GPUs. All models are fine-tuned on downstream tasks using the transformers library with task-specific hyperparameters, and evaluated using ROUGE scores for summarization, SARI and BLEU for split-and-rephrase, and ROUGE for generative question answering.

## Key Results
- BARTO and T5S demonstrate competitive performance across all evaluated sequence-to-sequence tasks
- BARTO excels in long-form summarization tasks, outperforming T5S in this domain
- T5S shows strong results in split-and-rephrase tasks compared to other model variants
- BERT2BERT-style models initialized from BETO and RoBERTa checkpoints show solid but slightly lower performance than BARTO and T5S

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training exclusively on Spanish corpora enables the model to learn language-specific patterns and nuances, which improves downstream performance on Spanish tasks.
- Mechanism: By training only on Spanish text, the model develops a deeper understanding of Spanish syntax, grammar, and vocabulary without being influenced by other languages. This focused learning allows the model to generate more accurate and fluent Spanish text.
- Core assumption: Spanish corpora used for pre-training is diverse and representative of the language's usage across different domains.
- Evidence anchors:
  - [abstract]: "Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across various sequence-to-sequence tasks, including summarization, question answering, split-and-rephrase, dialogue, and translation."
  - [section]: "We employ the OSCAR 21.09 corpus, which includes a deduplicated Spanish dataset of approximately 160GB of text. Furthermore, we utilize the mC4-es corpus (Xue et al., 2021), specifically adopting the Gaussian perplexity sampling subset proposed by De la Rosa et al. (2022), which boasts an extensive 500GB text dataset and has demonstrated superior model consistency. Additionally, we incorporate SUC, the corpus utilized for pre-training BETO, comprising around 14GB of raw text from diverse sources."
- Break condition: If the Spanish corpora is not diverse or representative, the model may not learn the full range of Spanish language patterns, leading to suboptimal performance on certain tasks or domains.

### Mechanism 2
- Claim: Using sequence-to-sequence architectures like BART and T5 allows the model to effectively handle tasks that involve generating output sequences based on input sequences.
- Mechanism: These architectures consist of an encoder that processes the input sequence and a decoder that generates the output sequence autoregressively. This design enables the model to learn the mapping between input and output sequences, making it suitable for tasks like summarization, translation, and question answering.
- Core assumption: The encoder-decoder architecture is appropriate for the specific sequence-to-sequence tasks being evaluated.
- Evidence anchors:
  - [abstract]: "Encoder-decoder models primarily serve for addressing sequence-to-sequence tasks, and over recent years, numerous architectures have emerged."
  - [section]: "A sequence-to-sequence model aims to map a fixed-length input with a fixed-length output where the length of the input and output may differ (Sutskever et al., 2014). It comprises an encoder, which concurrently processes the entire input sequence, and a decoder, which receives the representations computed by the encoder and generates the output sequence in an autoregressive manner."
- Break condition: If the sequence-to-sequence task requires a different architectural approach, such as a transformer with a different attention mechanism, the encoder-decoder architecture may not be optimal.

### Mechanism 3
- Claim: Fine-tuning the pre-trained models on specific downstream tasks allows them to adapt to the nuances and requirements of those tasks, leading to improved performance.
- Mechanism: By further training the pre-trained models on task-specific datasets, the models learn to generate outputs that are tailored to the particular task, such as generating concise summaries or rephrasing sentences.
- Core assumption: The fine-tuning datasets are representative of the task and contain sufficient examples for the model to learn the required patterns.
- Evidence anchors:
  - [abstract]: "Our findings underscore the competitive performance of all models, with the BART- and T5-based models emerging as top performers across all tasks."
  - [section]: "We fine-tune all the models on an RTX 3090 GPU for each task using the transformers library implemented in PyTorch. For a fair comparison, we use the same hyperparameters with the exception of the batch size, learning rate, and the number of training epochs."
- Break condition: If the fine-tuning datasets are too small or not representative of the task, the model may not learn the necessary patterns, leading to suboptimal performance on the specific task.

## Foundational Learning

- Concept: Pre-training language models
  - Why needed here: Pre-training on large corpora allows the model to learn general language patterns and representations before being fine-tuned on specific tasks.
  - Quick check question: What is the purpose of pre-training language models before fine-tuning them on downstream tasks?

- Concept: Sequence-to-sequence tasks
  - Why needed here: Understanding the nature of sequence-to-sequence tasks helps in selecting the appropriate model architecture and training approach.
  - Quick check question: What is the key characteristic of sequence-to-sequence tasks that distinguishes them from other NLP tasks?

- Concept: Fine-tuning pre-trained models
  - Why needed here: Fine-tuning allows the pre-trained models to adapt to specific downstream tasks, improving their performance on those tasks.
  - Quick check question: Why is fine-tuning necessary after pre-training language models, and how does it help improve performance on specific tasks?

## Architecture Onboarding

- Component map:
  - Pre-training: Large Spanish corpora (OSCAR, mC4-es, SUC) -> Model architectures: BART, T5, BERT2BERT-style models -> Fine-tuning: Task-specific datasets (MLSUM, WikiLingua, XL-Sum, etc.) -> Evaluation: ROUGE, SARI, BLEU metrics

- Critical path:
  1. Pre-train models on Spanish corpora
  2. Fine-tune models on downstream tasks
  3. Evaluate model performance using appropriate metrics

- Design tradeoffs:
  - Pre-training exclusively on Spanish corpora vs. multilingual corpora
  - Using encoder-decoder architectures vs. other architectures
  - Fine-tuning on small vs. large task-specific datasets

- Failure signatures:
  - Poor performance on downstream tasks despite pre-training
  - Overfitting to pre-training corpora or fine-tuning datasets
  - Inefficient use of computational resources during pre-training or fine-tuning

- First 3 experiments:
  1. Pre-train a small-scale version of the model on a subset of the Spanish corpora and evaluate its performance on a simple downstream task.
  2. Fine-tune the pre-trained model on a larger task-specific dataset and compare its performance to a baseline model.
  3. Experiment with different fine-tuning strategies (e.g., learning rates, batch sizes) to optimize the model's performance on the downstream task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the Spanish sequence-to-sequence models perform on other generative tasks not evaluated in this study, such as dialogue generation or machine translation?
- Basis in paper: [explicit] The paper mentions that the models are evaluated on summarization, split-and-rephrase, and generative question answering tasks, but also states "There has been limited advancement in addressing tasks revolving around generating new sentences depending on a given input, such as summarization, generative question answering, dialogue, or translation."
- Why unresolved: The study focuses on a specific set of tasks and does not explore the full range of potential applications for sequence-to-sequence models in Spanish.
- What evidence would resolve it: Evaluating the models on additional tasks like dialogue generation or machine translation, and comparing their performance to other specialized models for these tasks.

### Open Question 2
- Question: How do the Spanish sequence-to-sequence models compare to multilingual models when applied to Spanish-specific tasks?
- Basis in paper: [explicit] The paper states that "Numerous language-specific BERT-based and GPT-based models have emerged in recent times" and that "These models have consistently outperformed their multilingual counterparts," but does not compare the Spanish sequence-to-sequence models to multilingual ones.
- Why unresolved: The study focuses on Spanish-specific models but does not provide a direct comparison with multilingual models that could potentially handle Spanish tasks.
- What evidence would resolve it: Conducting experiments that compare the performance of the Spanish models to multilingual models like mBART or multilingual T5 on the same set of Spanish tasks.

### Open Question 3
- Question: What is the impact of using different pre-training corpora sizes and qualities on the performance of the Spanish sequence-to-sequence models?
- Basis in paper: [explicit] The paper mentions that "the corpus quality significantly impacts the outcomes of pre-training models" and describes the use of different corpora (OSCAR, mC4-es, and SUC) for pre-training, but does not explore the impact of varying corpus sizes or qualities.
- Why unresolved: While the paper acknowledges the importance of corpus quality, it does not systematically investigate how different corpus characteristics affect model performance.
- What evidence would resolve it: Conducting experiments that pre-train models on corpora of varying sizes and qualities, and comparing their performance on the same set of downstream tasks to determine the relationship between corpus characteristics and model effectiveness.

## Limitations
- The evaluation is restricted to specific benchmarks (MLSUM, WikiLingua, XL-Sum, TweetQA) that may not fully represent the breadth of Spanish NLP applications
- The exact composition and quality control measures of the Spanish corpora are not fully specified, making it difficult to assess whether the models truly capture diverse Spanish language patterns
- The lack of ablation studies makes it challenging to isolate the impact of pre-training exclusively on Spanish versus multilingual approaches

## Confidence
**High Confidence**: The paper's core methodology for pre-training sequence-to-sequence models on Spanish corpora is technically sound. The use of established architectures (BART, T5) with documented training procedures provides a solid foundation. The claim that pre-training on Spanish-only corpora improves downstream performance is supported by the literature on monolingual versus multilingual models.

**Medium Confidence**: The comparative performance claims between BARTO and T5S across different task types appear reasonable given the architectural differences and evaluation results presented. However, the confidence level is tempered by the limited scope of evaluation tasks and the absence of statistical significance testing for performance differences.

**Low Confidence**: The claim that these are the first sequence-to-sequence models pre-trained exclusively on Spanish corpora cannot be verified without a comprehensive literature review of all Spanish NLP efforts. Additionally, the assertion that BARTO "excels" in long-form summarization and T5S shows "strong results" in split-and-rephrase tasks lacks the quantitative backing needed for such definitive statements.

## Next Checks
1. **Statistical Significance Testing**: Conduct t-tests or other appropriate statistical tests to determine whether the performance differences between BARTO, T5S, and existing models are statistically significant across all evaluated tasks. This will provide quantitative backing for the competitive performance claims.

2. **Human Evaluation Study**: Implement a human evaluation protocol to assess the quality of generated outputs beyond automated metrics. This should include fluency, coherence, and relevance assessments for both short and long-form summarization tasks to validate the automated metrics and provide qualitative insights into model strengths and weaknesses.

3. **Cross-Domain Generalization Test**: Evaluate the models on out-of-domain Spanish texts (e.g., legal documents, scientific papers, social media) not represented in the pre-training corpora to assess their ability to generalize beyond the specific domains covered by the current evaluation benchmarks. This will help determine whether the models have learned truly general Spanish language patterns or are overfitting to specific domains.