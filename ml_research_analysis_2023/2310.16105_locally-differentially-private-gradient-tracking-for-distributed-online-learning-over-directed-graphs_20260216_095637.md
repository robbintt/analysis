---
ver: rpa2
title: Locally Differentially Private Gradient Tracking for Distributed Online Learning
  over Directed Graphs
arxiv_id: '2310.16105'
source_url: https://arxiv.org/abs/2310.16105
tags:
- algorithm
- distributed
- learning
- where
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a locally differentially private gradient tracking
  algorithm for distributed online learning over directed graphs, addressing the challenge
  of preserving individual learners' privacy while maintaining learning accuracy.
  The key innovation lies in modifying the conventional gradient tracking architecture
  to effectively handle persistent differential privacy (DP) noises without accumulating
  them in gradient estimation, thus ensuring accurate convergence.
---

# Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs

## Quick Facts
- arXiv ID: 2310.16105
- Source URL: https://arxiv.org/abs/2310.16105
- Reference count: 40
- This paper presents a locally differentially private gradient tracking algorithm for distributed online learning over directed graphs, achieving accurate convergence while guaranteeing rigorous LDP with finite cumulative privacy budget.

## Executive Summary
This paper addresses the challenge of preserving individual learners' privacy in distributed online learning over directed graphs while maintaining learning accuracy. The authors propose a modified gradient tracking algorithm that effectively handles persistent differential privacy noises without accumulating them in gradient estimation. By incorporating the difference between tracking variables into parameter updates and carefully designing noise injection mechanisms, the algorithm achieves exact convergence to the optimal solution while providing rigorous local differential privacy guarantees. Experimental results demonstrate superior performance compared to existing DP solutions across multiple benchmark datasets.

## Method Summary
The method introduces a locally differentially private gradient tracking algorithm for distributed online learning over directed graphs. The key innovation is modifying the conventional gradient tracking architecture to prevent DP noise accumulation by feeding the difference between tracking variables into parameter updates. The algorithm adds Laplace noise to exchanged messages, with noise variances decaying as 1/(t+1)^ς to ensure finite cumulative privacy budget. Each learner maintains local model parameters, tracking variables, and eigenvector estimates, which are updated using information from neighboring learners. The method proves convergence in mean square to the exact optimal solution while guaranteeing rigorous local differential privacy with a finite cumulative privacy budget.

## Key Results
- Algorithm converges in mean square to exact optimal solution while guaranteeing rigorous local differential privacy
- Cumulative privacy budget remains finite even with infinite iterations
- Outperforms existing DP solutions in both training and testing accuracies on logistic regression and CNN tasks
- Achieves faster convergence speed without compromising privacy protection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm prevents DP noise accumulation in gradient estimation by incorporating the difference st+1 - st into the parameter update.
- Mechanism: Unlike conventional gradient tracking, where the global gradient estimate (subject to accumulating DP noises) is directly used in parameter updates, this design feeds the difference st+1 - st into the update equation. This difference equals ζC,t + λt∇f t(θt), which does not accumulate noise over time.
- Core assumption: The DP noise variances decay sufficiently fast (σi,t,ζ = σi,0,ζ/(t+1)^ςi,ζ with ςi,ζ > 1/2) so that the noise in st+1 - st remains bounded.
- Evidence anchors:
  - [abstract]: "modifying the conventional gradient tracking architecture to effectively handle persistent differential privacy (DP) noises without accumulating them in gradient estimation"
  - [section III.B]: Derivation showing 1T(st+1 - st) = 1T(ζC,t + λt∇f t(θt)), proving no accumulation
  - [corpus]: Weak - no direct corpus evidence of this specific mechanism
- Break condition: If DP noise variances do not decay fast enough (ςi,ζ ≤ 1/2), noise accumulation will occur and break this mechanism.

### Mechanism 2
- Claim: The algorithm achieves rigorous ϵi-LDP with finite cumulative privacy budget even in infinite time horizon.
- Mechanism: DP noise is added to the messages exchanged between learners (ζi,t and ϑi,t). The sensitivities of these messages are analyzed and shown to grow sublinearly. The noise variances decay as 1/(t+1)^ς, ensuring that the cumulative privacy budget ∑(sensitivity/noise_variance) converges.
- Core assumption: The data points are independently and identically distributed, and the gradient sensitivity is bounded (Assumption 5).
- Evidence anchors:
  - [abstract]: "ensures rigorous local differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iterations tends to infinity"
  - [section V]: Theorem 4 proves the finite cumulative privacy budget using sensitivity analysis and decaying noise variances
  - [corpus]: Weak - no direct corpus evidence of this specific privacy analysis
- Break condition: If the data distribution changes over time or the gradient sensitivity grows unbounded, the sensitivity analysis breaks down.

### Mechanism 3
- Claim: The algorithm enables each learner to locally estimate the left normalized Perron eigenvector of the interaction graph.
- Mechanism: A local variable zi,t is introduced and updated according to zi,t+1 = zi,t + ΣRij(zj,t - zi,t). This local estimate converges to the left eigenvector u of R with geometric rate.
- Core assumption: The interaction graph GR is strongly connected (Assumption 1).
- Evidence anchors:
  - [abstract]: "enables each learner to locally estimate the left normalized Perron eigenvector of the interaction graph"
  - [section III.B]: Lemma 3 proves the convergence of zi,t to u with rate czγ^z
  - [corpus]: Weak - no direct corpus evidence of this eigenvector estimation
- Break condition: If the interaction graph becomes disconnected, the eigenvector estimation will fail.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP is the privacy framework used to protect individual learners' sensitive data. Understanding DP is crucial for analyzing the privacy guarantees of the algorithm.
  - Quick check question: What is the difference between central DP and local DP?

- Concept: Gradient Tracking
  - Why needed here: Gradient tracking is the distributed optimization technique used to handle data heterogeneity among learners. Understanding gradient tracking is essential for understanding how the algorithm achieves accurate convergence.
  - Quick check question: How does gradient tracking differ from distributed stochastic gradient descent (DSGD)?

- Concept: Perron-Frobenius Theorem
  - Why needed here: The Perron-Frobenius theorem guarantees the existence and uniqueness of the left eigenvector of the interaction matrix R. This eigenvector is crucial for the local estimation performed by each learner.
  - Quick check question: What are the conditions for a matrix to have a unique positive left eigenvector?

## Architecture Onboarding

- Component map:
  - Local model parameter θi,t
  - Local tracking variable si,t
  - Local eigenvector estimate zi,t
  - DP noise ζi,t (added to si,t)
  - DP noise ϑi,t (added to θi,t)
  - Interaction matrices R and C

- Critical path:
  1. Compute local gradient ∇fi,t(θi,t)
  2. Update tracking variable si,t+1 using neighbors' si,t + ζi,t
  3. Update model parameter θi,t+1 using neighbors' θi,t + ϑi,t and the difference si,t+1 - si,t
  4. Update eigenvector estimate zi,t+1 using neighbors' zi,t

- Design tradeoffs:
  - Privacy vs. accuracy: Increasing privacy budget ϵi improves privacy but reduces accuracy due to larger DP noises.
  - Convergence speed vs. accuracy: Using larger stepsize λ0 improves convergence speed but may reduce accuracy due to larger gradient estimation errors.

- Failure signatures:
  - Divergence: If the stepsize λt is too large or the noise variances do not decay fast enough, the algorithm may diverge.
  - Poor privacy: If the privacy budget ϵi is too large, the algorithm may leak sensitive information.
  - Slow convergence: If the interaction graph is poorly connected or the data is highly heterogeneous, the algorithm may converge slowly.

- First 3 experiments:
  1. Implement the algorithm on a small synthetic dataset with known optimal solution to verify convergence accuracy.
  2. Vary the privacy budget ϵi and measure the tradeoff between privacy and accuracy.
  3. Test the algorithm on a real-world dataset (e.g., MNIST) to evaluate performance on practical machine learning tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm perform under time-varying network topologies where the communication graph changes dynamically?
- Basis in paper: [inferred] The paper assumes static communication graphs (directed graphs GR and GC) and analyzes convergence under these assumptions. However, real-world distributed systems often experience dynamic network topologies.
- Why unresolved: The analysis in the paper is based on fixed interaction matrices R and C, and does not account for scenarios where the graph structure changes over time.
- What evidence would resolve it: Experimental results and theoretical analysis showing the algorithm's performance and convergence properties under time-varying directed graphs, including varying degrees of connectivity and network partitions.

### Open Question 2
- Question: What is the impact of heterogeneous data distributions across learners on the convergence rate and privacy guarantees of the algorithm?
- Basis in paper: [inferred] The paper assumes data points are independently and identically distributed (i.i.d.) across iterations and learners. However, in practice, data distributions can be highly non-uniform across different learners.
- Why unresolved: The current analysis does not explicitly address how data heterogeneity affects the algorithm's performance or privacy guarantees.
- What evidence would resolve it: Empirical studies and theoretical bounds quantifying the effect of data heterogeneity on convergence speed, accuracy, and privacy budget consumption.

### Open Question 3
- Question: How does the algorithm scale with the number of learners and the dimensionality of the problem?
- Basis in paper: [inferred] The paper provides convergence guarantees for a fixed number of learners and problem dimension, but does not discuss scalability aspects.
- Why unresolved: The computational and communication complexity of the algorithm with respect to the number of learners (m) and problem dimension (n) is not explicitly analyzed.
- What evidence would resolve it: Complexity analysis showing how the algorithm's computational time, memory requirements, and communication overhead scale with m and n, along with experimental results on large-scale problems.

## Limitations
- Theoretical analysis assumes strongly connected interaction graphs, which may not hold in real-world networks
- Laplace noise assumption may not hold in practice, as real-world implementations often use Gaussian noise or other mechanisms
- Convergence rate analysis assumes bounded gradient sensitivity, which may not hold for all problem instances

## Confidence
- **High Confidence**: The core mechanism of preventing noise accumulation through difference-based updates (Mechanism 1) - supported by direct derivation in section III.B
- **Medium Confidence**: The privacy analysis and finite cumulative privacy budget guarantee (Mechanism 2) - relies on sublinear growth assumptions that may not hold in practice
- **Low Confidence**: The eigenvector estimation convergence rate (Mechanism 3) - proof depends on strongly connected graph assumption which may be violated

## Next Checks
1. **Empirical Sensitivity Validation**: Test the algorithm on datasets with varying gradient sensitivity to verify the bounded sensitivity assumption holds in practice
2. **Graph Topology Robustness**: Evaluate performance on time-varying or disconnected graphs to assess the strongly connected assumption
3. **Noise Distribution Verification**: Implement the algorithm with Gaussian noise instead of Laplace noise to test the robustness of the privacy guarantees