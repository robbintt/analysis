---
ver: rpa2
title: 'SpArX: Sparse Argumentative Explanations for Neural Networks [Technical Report]'
arxiv_id: '2301.09559'
source_url: https://arxiv.org/abs/2301.09559
tags:
- sparx
- neurons
- explanations
- lime
- mlps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpArX provides faithful and interpretable explanations for neural
  networks by leveraging the formal relationship between multi-layer perceptrons (MLPs)
  and quantitative argumentation frameworks (QAFs). The method first sparsifies the
  MLP through clustering of similar neurons, then translates the sparse MLP into an
  equivalent QAF to explain the network's reasoning.
---

# SpArX: Sparse Argumentative Explanations for Neural Networks [Technical Report]

## Quick Facts
- arXiv ID: 2301.09559
- Source URL: https://arxiv.org/abs/2301.09559
- Reference count: 40
- Primary result: SpArX achieves up to 100% improvement in structural faithfulness and 99% in input-output faithfulness compared to LIME

## Executive Summary
SpArX provides faithful and interpretable explanations for neural networks by leveraging the formal relationship between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs). The method first sparsifies the MLP through clustering of similar neurons, then translates the sparse MLP into an equivalent QAF to explain the network's reasoning. Experimental results demonstrate that SpArX explanations are more input-output and structurally faithful to the original model than state-of-the-art methods like LIME, with up to 100% improvement in structural faithfulness and 99% in input-output faithfulness. The method successfully scales to large datasets and varying network architectures while maintaining interpretability through reduced cognitive complexity.

## Method Summary
SpArX translates neural network predictions into argumentative explanations by first clustering similar neurons in MLPs to create a sparse representation, then converting this clustered MLP into a quantitative argumentation framework (QAF). The method uses average aggregation functions to compute cluster parameters, minimizing least-squares error between cluster and original neurons. The resulting QAF captures the reasoning process of the MLP, providing both global and local explanations. The approach balances faithfulness to the original model with cognitive complexity by adjusting compression ratios during clustering.

## Key Results
- SpArX achieves up to 100% improvement in structural faithfulness compared to HAP
- Input-output faithfulness is up to 99% higher than LIME
- The method maintains explanation quality while reducing cognitive complexity through sparsification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SpArX creates more faithful explanations than LIME by preserving the structural similarity between the original MLP and the clustered QAF.
- Mechanism: SpArX first sparsifies the MLP through clustering of neurons based on their output-similarity, then translates the sparse MLP into an equivalent QAF, maintaining the mechanics of the original model.
- Core assumption: Clustering neurons based on their output-similarity and aggregating their parameters preserves the core reasoning structure of the MLP.
- Evidence anchors:
  - [abstract]: "SpArX provides faithful and interpretable explanations for neural networks by leveraging the formal relationship between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs)."
  - [section 5]: "We define global and local explanations for MLPs by translating their corresponding clustered MLPs into QAFs."
  - [corpus]: Weak evidence - related papers focus on compression and argumentation frameworks but don't directly validate the structural faithfulness claim.
- Break condition: If the clustering fails to preserve critical neuron interactions or the aggregation functions poorly approximate neuron behavior.

### Mechanism 2
- Claim: The average aggregation functions used in SpArX minimize deviation between cluster neurons and original neurons in terms of least-squares error.
- Mechanism: For biases, the average bias aggregation function minimizes the sum of squared differences between cluster bias and original neuron biases. For weights, the edge aggregation function minimizes the sum of squared differences between original edge weights and aggregated weights.
- Core assumption: Minimizing least-squares error between cluster parameters and original neuron parameters leads to faithful explanations.
- Evidence anchors:
  - [section 5.1]: "As we explain in the supplementary material (SM), they minimize the deviation (with respect to the least-squares error) of bias and weights of the cluster neuron and the neurons contained in the cluster."
  - [section 5.1]: "For biases, given a cluster neuron vC with corresponding bias b, we want to minimize..."
  - [corpus]: Weak evidence - related papers discuss compression and aggregation but don't validate the specific least-squares error minimization approach.
- Break condition: If the least-squares error minimization doesn't effectively preserve the MLP's reasoning or leads to significant information loss.

### Mechanism 3
- Claim: SpArX explanations are more input-output faithful than LIME by focusing on maintaining the mechanics of the MLP rather than just input-output behavior.
- Mechanism: SpArX generates explanations by translating the sparse MLP into a QAF, which captures the reasoning process of the MLP. This approach preserves the structural faithfulness of the explanation.
- Core assumption: Explanations based on maintaining the mechanics of the MLP are more faithful than those based solely on input-output behavior.
- Evidence anchors:
  - [section 7.2]: "The results show that the local explanations produced by our approach are more input-output faithful to the original model."
  - [section 7.1]: "Our method has a much lower structural unfaithfulness than HAP by preserving activation values close to the original model."
  - [corpus]: Weak evidence - related papers discuss input-output faithfulness but don't directly compare to SpArX's approach.
- Break condition: If maintaining the mechanics of the MLP doesn't lead to more faithful explanations or if the QAF translation process introduces significant errors.

## Foundational Learning

- Concept: Multi-layer perceptrons (MLPs)
  - Why needed here: SpArX is designed to explain MLPs, so understanding their structure and function is crucial.
  - Quick check question: What are the key components of an MLP and how do they interact during forward propagation?

- Concept: Quantitative argumentation frameworks (QAFs)
  - Why needed here: SpArX translates sparse MLPs into QAFs to create interpretable explanations, so understanding QAFs is essential.
  - Quick check question: How do QAFs represent arguments and relations, and how are they used to model the reasoning process of MLPs?

- Concept: Clustering algorithms
  - Why needed here: SpArX uses clustering to sparsify MLPs, so understanding clustering techniques and their applications is important.
  - Quick check question: What are the key considerations when choosing a clustering algorithm for sparsifying MLPs, and how does the choice affect the faithfulness of the resulting explanations?

## Architecture Onboarding

- Component map:
  MLP input layer -> MLP hidden layers -> MLP output layer -> Clustering algorithm -> Aggregation functions (bias and edge) -> QAF translation module -> Explanation generation module

- Critical path:
  1. Input MLP and dataset
  2. Apply clustering algorithm to sparsify MLP
  3. Use aggregation functions to compute parameters for clustered MLP
  4. Translate clustered MLP into QAF
  5. Generate explanations based on QAF

- Design tradeoffs:
  - Number of clusters vs. explanation faithfulness
  - Global vs. local explanations
  - Compression ratio vs. cognitive complexity
  - Computational efficiency vs. explanation quality

- Failure signatures:
  - High input-output unfaithfulness between clustered MLP and original MLP
  - High structural unfaithfulness between clustered MLP and original MLP
  - Low classification performance of clustered MLP compared to original MLP
  - Excessive cognitive complexity of generated explanations

- First 3 experiments:
  1. Test SpArX on a simple XOR MLP to verify the basic functionality and explanation quality.
  2. Compare the input-output faithfulness of SpArX and LIME on a larger MLP using the iris dataset.
  3. Evaluate the scalability of SpArX by testing it on deeper MLPs with varying numbers of hidden layers and neurons using the forest covertype dataset.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the following issues remain unresolved based on the content:

## Limitations
- Clustering algorithms introduce uncertainty about optimal parameters and potential information loss
- QAF translation process may introduce errors that aren't fully characterized
- Computational complexity for large networks is not thoroughly analyzed
- Limited testing on complex, real-world datasets with noisy data

## Confidence
- **High Confidence**: The core mechanism of translating MLPs to QAFs and the basic faithfulness improvements over LIME
- **Medium Confidence**: The specific aggregation functions and their optimization through least-squares error minimization
- **Low Confidence**: The scalability claims and performance on larger, more complex datasets

## Next Checks
1. Sensitivity Analysis: Test how variations in the number of clusters affect explanation faithfulness and cognitive complexity to determine optimal clustering parameters.
2. Error Propagation Study: Analyze how errors introduced during the QAF translation process affect overall explanation quality and identify potential failure points.
3. Real-World Application Test: Apply SpArX to a real-world dataset with noisy data and complex decision boundaries to validate scalability and robustness claims.