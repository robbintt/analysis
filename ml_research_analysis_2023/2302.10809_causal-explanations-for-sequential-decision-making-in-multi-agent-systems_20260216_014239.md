---
ver: rpa2
title: Causal Explanations for Sequential Decision-Making in Multi-Agent Systems
arxiv_id: '2302.10809'
source_url: https://arxiv.org/abs/2302.10809
tags:
- causes
- explanations
- actions
- causal
- exit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a conversational framework called CEMA for
  generating causal natural language explanations for agent decisions in stochastic
  multi-agent systems. The method relies on a generative model to simulate counterfactual
  worlds and identify the causes behind an agent's actions.
---

# Causal Explanations for Sequential Decision-Making in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2302.10809
- Source URL: https://arxiv.org/abs/2302.10809
- Authors: 
- Reference count: 11
- One-line primary result: CEMA generates causal natural language explanations for agent decisions in stochastic multi-agent systems using counterfactual reasoning

## Executive Summary
This paper introduces CEMA, a conversational framework for generating causal explanations of agent decisions in stochastic multi-agent systems. The method uses counterfactual reasoning to identify salient causes behind agent actions by simulating alternative worlds and correlating features with action presence/absence. The framework is applied to autonomous driving scenarios, demonstrating correct identification and ranking of relevant causes. User studies indicate positive effects on trust in autonomous vehicles, though the methodology and results are not fully detailed.

## Method Summary
CEMA generates causal explanations through a pipeline of natural language understanding, counterfactual analysis, and natural language generation. The framework samples counterfactual trajectories from a generative model of agent interactions, then uses causal attribution methods to identify which features most strongly influence observed actions. Explanations are provided at multiple levels of causal reasoning (associative, mechanistic, teleological) to match user query types. The method is demonstrated on autonomous driving scenarios using hand-crafted features and a motion planning system.

## Key Results
- Correctly and robustly identifies causes behind agent interactions in autonomous driving scenarios
- Generates concise explanations for a wide range of user queries about agent behavior
- Shows positive effects on trust in autonomous vehicles according to user studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CEMA identifies salient causes by simulating counterfactual worlds using a generative model of agent interactions.
- Mechanism: The framework samples alternative joint state sequences from a generative model given prior observations, then correlates features of these sequences with the presence or absence of the queried action to determine causal effect sizes.
- Core assumption: A generative model p(Ŝ_{τ+1:n}|s_1:τ) exists that can accurately predict future agent interactions given past states.
- Evidence anchors:
  - [abstract] "CEMA simulates counterfactual worlds that identify the salient causes behind the agent's decisions."
  - [section 3.3] "Algorithm 1 starts by rolling back the joint state sequence s_1:t to a time step τ ≤ u < t... It then samples K joint state sequences according to the generative model p(Ŝ_{τ+1:n}|s_1:τ)"
  - [corpus] Weak - neighbors discuss related multi-agent causal analysis but don't directly validate this specific counterfactual simulation mechanism.
- Break condition: The generative model fails to accurately predict agent interactions, leading to incorrect counterfactual samples and spurious causal attributions.

### Mechanism 2
- Claim: CEMA generates explanations at multiple levels of causal reasoning (associative, mechanistic, teleological) matching human cognitive expectations.
- Mechanism: The framework implements Pearl's Ladder of Causation by providing different explanation modes - associative explanations describe observed correlations, mechanistic explanations use intervention analysis, and teleological explanations use average treatment effect on reward components.
- Core assumption: Users' queries can be parsed into specific causal reasoning levels that map to these explanation modes.
- Evidence anchors:
  - [abstract] "explanations are given via natural language conversations answering a wide range of user queries and requiring associative, interventionist, or counterfactual causal reasoning"
  - [section 3.2] "explanatory function e : (O_ε)* × (A_ε)* → E that maps a sequence of local observations and actions to an explanation"
  - [section 2] "Table 1 shows Pearl and Mackenzie [2018]'s 'Ladder of Causation' which gives three categories of inquiry requiring different levels of reasoning"
  - [corpus] Weak - neighbors discuss causal analysis in multi-agent systems but don't validate the specific implementation of multiple causal reasoning levels.
- Break condition: The query parsing fails to correctly identify the required causal reasoning level, resulting in explanations that don't match user expectations.

### Mechanism 3
- Claim: CEMA's explanations improve trust in autonomous vehicles through contrastive, causally-grounded reasoning.
- Mechanism: By providing counterfactual explanations that show what would happen under alternative conditions, the system helps users understand the rationale behind decisions and builds confidence in the agent's reasoning process.
- Core assumption: Users' trust is positively influenced by understanding the causal factors behind autonomous vehicle decisions.
- Evidence anchors:
  - [abstract] "User studies indicate positive effects on trust in autonomous vehicles"
  - [section 1] "what is to be done then, if we want to build powerful yet trustworthy AI? ... Emphasis is now put on creating methods for generating contrastive, causal, and intelligible explanations for AI and its many stakeholders"
  - [corpus] Weak - neighbors discuss multi-agent explanations but don't provide evidence about trust improvement specifically.
- Break condition: User studies show no improvement in trust or even decreased trust due to explanations being confusing or misleading.

## Foundational Learning

- Concept: Generative models for multi-agent systems
  - Why needed here: CEMA relies on sampling counterfactual worlds from a generative model to identify causal factors
  - Quick check question: What is the form of the generative model p(Ŝ_{τ+1:n}|s_1:τ) and how is it trained?

- Concept: Counterfactual reasoning and causal effect estimation
  - Why needed here: The framework uses counterfactual sampling to determine which features have causal influence on observed actions
  - Quick check question: How does Algorithm 2 calculate feature importance attributions from the counterfactual dataset D?

- Concept: Multi-level causal explanations (Pearl's Ladder of Causation)
  - Why needed here: Different user queries require different levels of causal reasoning, implemented as associative, mechanistic, and teleological explanations
  - Quick check question: What distinguishes associative, mechanistic, and teleological explanations in the context of autonomous driving?

## Architecture Onboarding

- Component map: User query → NLU parser → Causal analysis (sampling + attribution) → Featurizer → NLG generator → Natural language explanation
- Critical path: Query parsing → Counterfactual sampling → Causal attribution → Natural language generation
- Design tradeoffs: Hand-crafted features vs. learned features; fixed rollback time τ vs. dynamic determination; choice of classifier for feature importance
- Failure signatures: Incorrect query parsing leading to wrong causal analysis; poor generative model predictions leading to spurious causal attributions; feature selection missing key explanatory factors
- First 3 experiments:
  1. Implement the NLU parser to convert natural language queries into structured query objects with agent ID, tense, actions, and time specifications
  2. Implement the counterfactual sampling algorithm to generate K alternative trajectories and compute presence of queried sequences and rewards
  3. Implement the causal attribution algorithm to calculate feature importance for mechanistic explanations and reward differences for teleological explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of explanatory features (F) impact the quality and usefulness of the generated explanations for different user groups?
- Basis in paper: [inferred] The paper mentions that the usefulness of the framework depends on the choice of features F, and that hand-crafted features were used for interpretability. It also suggests that features could be generated with a black-box neural model, but these may not be readily intelligible.
- Why unresolved: The paper does not provide empirical evidence on how different feature choices affect explanation quality for various user groups (e.g., domain experts vs. non-experts).
- What evidence would resolve it: User studies comparing the effectiveness of explanations generated with different feature sets (e.g., hand-crafted vs. neural-generated) across different user groups.

### Open Question 2
- Question: What is the optimal rollback time (τ) for determining causal attributions in different types of multi-agent decision-making scenarios?
- Basis in paper: [explicit] The paper discusses the challenge of selecting τ, noting that setting it too far back in time may lead to identifying less relevant causes. It suggests using methods like changepoint detection or clustering to find τ in general settings.
- Why unresolved: The paper does not provide a systematic method for determining τ or empirical results on how different τ values affect explanation quality.
- What evidence would resolve it: Experimental results comparing explanation quality across different τ values in various scenarios, and/or a method for automatically determining optimal τ based on scenario characteristics.

### Open Question 3
- Question: How does the framework perform when applied to real-world autonomous driving data compared to simulated scenarios?
- Basis in paper: [explicit] The paper states that the framework is demonstrated on motion planning for autonomous driving using simulated scenarios, but does not evaluate it on real-world data.
- Why unresolved: The paper does not provide evidence of the framework's performance in real-world conditions, which may differ significantly from simulations.
- What evidence would resolve it: Application of the framework to real-world autonomous driving datasets and comparison of explanation quality and causal attribution accuracy with simulated results.

## Limitations

- The framework's reliance on generative models introduces significant uncertainty - if the model fails to capture key interaction patterns, causal attributions will be unreliable.
- User study results are limited in scope and sample size, with insufficient methodology and statistical analysis to assess robustness.
- The approach is demonstrated primarily on autonomous driving scenarios, raising questions about generalizability to other multi-agent domains.

## Confidence

**High Confidence:** The theoretical framework for causal explanation generation using counterfactual sampling is well-grounded in established causal inference literature.

**Medium Confidence:** The implementation details for the autonomous driving scenario appear reasonable, but the paper lacks sufficient technical depth to fully assess practical effectiveness.

**Low Confidence:** The user study methodology and results are not adequately described, making it difficult to assess claimed benefits for trust in autonomous vehicles.

## Next Checks

1. **Generative Model Validation:** Test the framework's performance when the generative model is deliberately misspecified or trained on limited data to understand sensitivity of causal attributions to model accuracy.

2. **Cross-Domain Applicability:** Apply CEMA to at least two different multi-agent domains (e.g., robotics, game playing) to evaluate how well the approach generalizes beyond autonomous driving scenarios.

3. **User Study Replication:** Conduct a larger-scale user study with randomized controlled trials to verify claimed trust improvements, including measures of explanation quality, understanding, and potential negative effects.