---
ver: rpa2
title: Towards Activated Muscle Group Estimation in the Wild
arxiv_id: '2303.00952'
source_url: https://arxiv.org/abs/2303.00952
tags:
- region
- back
- thigh
- upper
- abdominis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the new task of video-based Activated Muscle
  Group Estimation (AMGE), which aims to identify active muscle regions during physical
  activities in the wild. To support this task, the authors present MuscleMap, a large-scale
  dataset featuring over 15K video clips with 136 different activities and 20 labeled
  muscle groups.
---

# Towards Activated Muscle Group Estimation in the Wild

## Quick Facts
- arXiv ID: 2303.00952
- Source URL: https://arxiv.org/abs/2303.00952
- Reference count: 40
- Primary result: TransM3E achieves state-of-the-art performance on video-based Activated Muscle Group Estimation for both seen and unseen physical activities.

## Executive Summary
This paper introduces the new task of video-based Activated Muscle Group Estimation (AMGE), which aims to identify active muscle regions during physical activities in the wild. The authors present MuscleMap, a large-scale dataset featuring over 15K video clips with 136 different activities and 20 labeled muscle groups. To address the challenge of generalizing to unseen activities, they propose TransM3E, a novel approach using multi-modality feature fusion between video transformers and skeleton-based GCNs with cross-modal knowledge distillation. The approach achieves state-of-the-art performance on both seen and new types of physical activities.

## Method Summary
The paper proposes TransM3E for AMGE, which uses a multi-modality feature fusion mechanism between video transformer models and skeleton-based graph convolution models. The approach incorporates cross-modal knowledge distillation executed on multi-classification tokens (MCTs), where 20 separate tokens are created (one per muscle group) to enable independent learning of activation patterns. The model is trained using MCTs alone for 40 epochs, then with all components for 10 additional epochs using AdamW optimizer with specific hyperparameters.

## Key Results
- TransM3E surpasses current activity recognition models for AMGE, especially for dealing with previously unseen activities
- The model achieves 79.8% mAP on val_a and 81.2% mAP on test_a of MuscleMap136 dataset
- MCTF (Multi-Classification Tokens Fusion) shows the best performance among attention-based approaches
- DenseMCTKD (token-wise knowledge distillation) achieves the best performance in ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Multi-Classification Tokens (MCTs)
The MCT approach improves generalizability by allowing each of the 20 tokens to learn activation cues for a specific muscle group independently. This enables fine-grained learning of activation patterns without forcing alignment with specific class labels during training.

### Mechanism 2: Multi-Classification Tokens Knowledge Distillation (MCTKD)
MCTKD improves cross-modal knowledge transfer by distilling knowledge from auxiliary modalities (RGB difference) to the main modality (RGB) at multiple intermediate layers using KL-Divergence loss. This captures temporal and spatial patterns in muscle activation more effectively than final-layer distillation.

### Mechanism 3: Multi-Classification Tokens Fusion (MCTF)
MCTF integrates knowledge from receiver MCTs (distilled from auxiliary modality) with original MCTs through a mixed attention mechanism. Cross-attention between original and receiver MCTs effectively combines complementary information without causing interference.

## Foundational Learning

- **Concept:** Multi-label classification
  - Why needed here: AMGE requires predicting multiple muscle groups as active or inactive for each video clip
  - Quick check question: If a video shows both bicep and tricep activation, what should the model output for those classes?

- **Concept:** Knowledge distillation
  - Why needed here: Transfers information from auxiliary modality to main modality, improving generalization without requiring auxiliary data at inference
  - Quick check question: What is the main benefit of using knowledge distillation instead of simply concatenating features from multiple modalities?

- **Concept:** Transformer attention mechanisms
  - Why needed here: Processes spatiotemporal features and integrates information from multiple modalities for capturing complex muscle activation patterns
  - Quick check question: How does self-attention differ from cross-attention in the context of multimodal fusion?

## Architecture Onboarding

- **Component map:** RGB video frames → MViTv2 backbone → MCTs (20 tokens) → MCTKD (with RGB difference) → MCTF fusion → binary predictions

- **Critical path:** RGB video → MViTv2 backbone → MCTs → MCTKD (with RGB difference) → MCTF fusion → binary predictions

- **Design tradeoffs:** Using 20 separate MCTs increases model complexity but enables fine-grained learning; MCTKD adds training complexity but enables inference with only RGB data; MCTF increases computational cost but potentially improves accuracy

- **Failure signatures:** Poor performance on unseen activities may indicate overfitting; degraded performance with only RGB data may indicate over-reliance on auxiliary modality; inconsistent predictions across similar activities may indicate MCTF instability

- **First 3 experiments:**
  1. Train baseline MViTv2-S with only RGB data and evaluate on MuscleMap136
  2. Add MCTs to baseline and evaluate performance improvement on seen vs. unseen activities
  3. Implement MCTKD with RGB difference as auxiliary modality and compare performance to baseline and MCT-only models

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of Multi-Classification Tokens (MCTs) for maximizing generalization to unseen activities? The paper found 20 tokens performed best, but the optimal number for different datasets or activity distributions remains unknown.

### Open Question 2
How does the choice of auxiliary modality affect the performance and generalizability of the TransM3E model? The paper tested limited modality combinations, but the impact of other potential modalities or combinations on generalization is unexplored.

### Open Question 3
Can the TransM3E model be further improved by incorporating temporal information beyond the current multi-scale vision transformer approach? The paper does not investigate additional temporal modeling techniques on generalization performance.

## Limitations

- Dataset construction relies on crowd-sourced annotations which may introduce noise or inconsistencies
- MCTKD effectiveness depends on RGB difference providing meaningful complementary information, which may not hold for all activity types
- Model performance on full 20 muscle group classification is not explicitly validated, leaving scalability uncertainty

## Confidence

- **High Confidence:** Experimental results showing superior performance on both seen and unseen activity splits
- **Medium Confidence:** Claims about MCTKD and MCTF improving generalization through cross-modal transfer and attention-based fusion
- **Low Confidence:** Assertion that model can effectively scale to full 20 muscle group classification without performance degradation

## Next Checks

1. Test the full TransM3E model on the complete 20 muscle group classification task to verify scalability performance
2. Conduct detailed ablation study removing MCTKD component to quantify its exact contribution to performance on unseen activities
3. Measure inference time and memory usage of complete TransM3E model with MCTF compared to simpler baselines