---
ver: rpa2
title: 'SDGym: Low-Code Reinforcement Learning Environments using System Dynamics
  Models'
arxiv_id: '2310.12494'
source_url: https://arxiv.org/abs/2310.12494
tags:
- learning
- environment
- agent
- system
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SDGym, a low-code library that enables the
  generation of custom reinforcement learning (RL) environments from System Dynamics
  (SD) simulation models. The key contribution is demonstrating that well-specified,
  rich RL environments can be generated from pre-existing SD models using only a few
  lines of configuration code.
---

# SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models

## Quick Facts
- arXiv ID: 2310.12494
- Source URL: https://arxiv.org/abs/2310.12494
- Reference count: 19
- One-line primary result: SDGym enables RL environment generation from SD models with minimal code, validated on electric vehicle adoption in Norway.

## Executive Summary
This paper introduces SDGym, a low-code library that enables the generation of custom reinforcement learning (RL) environments from System Dynamics (SD) simulation models. The key contribution is demonstrating that well-specified, rich RL environments can be generated from pre-existing SD models using only a few lines of configuration code. The feasibility of this approach is validated through a proof-of-concept using an SD model of the electric vehicle adoption problem in Norway. Two SD simulators, PySD and BPTK-Py, are compared for parity, and a D4PG agent is trained using the Acme framework to showcase learning and environment interaction. The results highlight the dual potential of SD to improve RL environment design and for RL to enhance dynamic policy discovery within SD models. By open-sourcing SDGym, the authors aim to promote further research and adoption across the SD and RL communities, catalyzing collaboration in this emerging interdisciplinary space.

## Method Summary
The method involves wrapping SD simulation models in a Gym-compatible environment interface, allowing RL agents to interact with SD models as if they were standard RL environments. The core class, `SDEnv`, handles simulation integration, state extraction, and action injection, exposing standard Gym interfaces (`reset`, `step`, `observation_space`, `action_space`). Users provide only the SD model file and optional parameter overrides; SDGym handles the rest. The method is validated through a proof-of-concept using an SD model of the electric vehicle adoption problem in Norway, comparing two SD simulators (PySD and BPTK-Py) for parity, and training a D4PG agent using the Acme framework to showcase learning and environment interaction.

## Key Results
- SDGym successfully generates RL environments from SD models with minimal code.
- Parity tests show consistent behavior between PySD and BPTK-Py simulators.
- A D4PG agent learns to increase electric vehicle adoption through dynamic interventions in the SD model.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SDGym enables RL environment generation from SD models with minimal code by leveraging Gym's Env interface and a thin abstraction layer over SD simulators.
- Mechanism: The `SDEnv` class wraps an SD simulation model and exposes standard Gym interfaces (`reset`, `step`, `observation_space`, `action_space`). Users provide only the SD model file and optional parameter overrides; SDGym handles simulation integration, state extraction, and action injection.
- Core assumption: The SD model is fully specified and uses supported XMILE/MDL formats, and actionable variables are limited to constant converters.
- Evidence anchors:
  - [abstract] "enables the generation of custom RL environments based on SD simulation models" and "few lines of configuration code."
  - [section 3.2] "SDEnv class, which is derived from the Env class provided by the OpenAI Gym"
  - [corpus] No direct neighbor papers, but the abstract and section clearly describe the minimal code requirement.
- Break condition: If the SD model uses unsupported constructs (e.g., flows, functions, or random elements not handled by PySD/BPTK-Py), the abstraction fails.

### Mechanism 2
- Claim: Dynamic interventions in SD models can be learned by RL agents because SDGym exposes continuous action spaces that map to SD model variables.
- Mechanism: Actionable variables (constant converters) are exposed as inputs to the simulation. SDGym flattens composite action spaces (continuous/discrete/categorical) into a single continuous Box space, which RL agents can manipulate. The agent learns to adjust these variables over time to maximize the reward.
- Core assumption: The SD model's dynamics remain stable under input injection and the action space adequately represents the intervention space.
- Evidence anchors:
  - [section 3.5] "Action Space" defines the composite action space and flattening strategy.
  - [section 4.2] Demonstrates a D4PG agent learning to adjust variables like `electricity_price`, `vat`, and `gov_policy_on_taxes` to increase electric vehicle adoption.
  - [corpus] No direct neighbor support; claim is based on paper's experimental section.
- Break condition: If interventions cause non-physical or unstable model states (e.g., negative stocks), the agent's policy may fail to converge.

### Mechanism 3
- Claim: SDGym accelerates policy discovery in SD models by replacing manual intervention sweeps with automated RL exploration.
- Mechanism: Traditional SD practice requires manually changing initial conditions and re-running simulations. SDGym enables agents to interact at each time step, exploring dynamic intervention strategies without restarting simulations, thereby discovering policies faster.
- Core assumption: The reward function effectively encodes the policy objective and the agent can learn from the resulting state transitions.
- Evidence anchors:
  - [section 4.2.2] "RL is able to learn a dynamic intervention strategy in SD models given a goal" and "improvement over intervention discovery performed by humans in SD models."
  - [section 2.4] Discusses the contrast between SD's single-shot interventions and RL's step-by-step control.
  - [corpus] No direct neighbor papers; the claim is supported by the paper's own feasibility study.
- Break condition: If the reward signal is sparse or misleading, the agent may not learn meaningful policies.

## Foundational Learning

- Concept: Reinforcement Learning basics (states, actions, rewards, policies)
  - Why needed here: SDGym translates SD model dynamics into an RL environment; understanding RL concepts is essential to use or extend the library.
  - Quick check question: What is the difference between on-policy and off-policy RL methods, and why might one choose off-policy for SDGym environments?

- Concept: System Dynamics modeling (stocks, flows, converters, feedback loops)
  - Why needed here: SDGym relies on SD model structure; knowing how stocks accumulate and flows change is critical to designing actionable variables and interpreting agent behavior.
  - Quick check question: In an SD model, which variable types can be safely exposed as actionable without breaking model stability?

- Concept: OpenAI Gym interface (Env, spaces, step/reset semantics)
  - Why needed here: SDGym subclasses Gym Env; users must understand Gym's expectations to integrate agents or customize environments.
  - Quick check question: How does Gym's `observation_space` differ from `action_space`, and what Gym classes are used for continuous vs discrete variables?

## Architecture Onboarding

- Component map:
  - `SDEnv`: Main Gym environment class wrapping an SD model.
  - `SDSimulator`: Abstract base with PySD and BPTK-Py implementations.
  - `State`: Wrapper exposing model variables and observation history.
  - `Params`: Configuration object for simulation settings.
  - `Reward` functions: ScalarDeltaReward, custom functions, or user-defined.
  - Action space utilities: Flattening and rescaling for composite spaces.

- Critical path:
  1. Load SD model file â†’ instantiate appropriate `SDSimulator`.
  2. Build `SDEnv` with Params (actionables, observables, limits).
  3. Define reward function (or use default).
  4. Initialize agent with Gym environment.
  5. Train agent via standard RL loop.

- Design tradeoffs:
  - Supports only constant converters as actions to preserve model stability, limiting expressiveness.
  - Flattens composite action spaces for compatibility but may lose precision for discrete/categorical actions.
  - Relies on external SD libraries (PySD/BPTK-Py) whose limitations propagate to SDGym.

- Failure signatures:
  - Unsupported XMILE/MDL constructs cause simulator errors.
  - Invalid action values (outside variable limits) trigger model instability or NaNs.
  - Sparse or poorly designed reward functions lead to non-convergent training.

- First 3 experiments:
  1. **Parity test**: Load a simple XMILE model with both PySD and BPTK-Py simulators; verify identical state trajectories without interventions.
  2. **Static intervention**: Define a model with a single actionable variable; run a fixed-action episode and confirm state changes match expectations.
  3. **RL training**: Use the Electric Vehicle Norway model; train a D4PG agent to maximize electric vehicle adoption; compare agent trajectory to a naive random policy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SDGym compare to other causality-based approaches in improving robustness and fairness in ML models, particularly in high-stakes domains like healthcare?
- Basis in paper: [inferred] from the authors' discussion of the potential role of SDGym in ML fairness, safety, and robustness, and the need for studies to compare its effectiveness with other causality-based approaches.
- Why unresolved: The paper does not provide any empirical comparisons or studies to validate the effectiveness of SDGym in improving robustness and fairness in ML models, especially in high-stakes domains.
- What evidence would resolve it: Empirical studies comparing the performance of SDGym with other causality-based approaches in improving robustness and fairness in ML models, particularly in high-stakes domains like healthcare, would provide evidence to resolve this question.

### Open Question 2
- Question: What are the limitations of using PySD and BPTK-Py simulators in SDGym, and how can these limitations be addressed to improve the coverage of SD models?
- Basis in paper: [explicit] from the authors' discussion of the limitations of PySD and BPTK-Py simulators, such as PySD's lack of support for the RANDOM function and BPTK-Py's lack of support for non-negative stocks.
- Why unresolved: The paper does not provide a comprehensive analysis of the limitations of using PySD and BPTK-Py simulators in SDGym, nor does it propose solutions to address these limitations.
- What evidence would resolve it: A detailed analysis of the limitations of PySD and BPTK-Py simulators in SDGym, along with proposed solutions to address these limitations, would provide evidence to resolve this question.

### Open Question 3
- Question: How can SDGym be extended to support generative actions that alter the structure of the environment, and what are the potential benefits and challenges of this extension?
- Basis in paper: [inferred] from the authors' discussion of the current limitations of SDGym in supporting generative actions and the potential benefits of such an extension.
- Why unresolved: The paper does not provide any concrete proposals or empirical studies to explore the extension of SDGym to support generative actions that alter the structure of the environment.
- What evidence would resolve it: Empirical studies exploring the extension of SDGym to support generative actions that alter the structure of the environment, along with an analysis of the potential benefits and challenges of this extension, would provide evidence to resolve this question.

## Limitations
- The experimental validation is limited to a single SD model and does not demonstrate performance on models with more complex dynamics or multiple interacting stocks.
- The claim about RL improving policy discovery over traditional SD methods is not quantitatively compared against standard SD intervention analysis techniques.
- The core claims rely on two untested assumptions: that SDGym's abstraction layer fully preserves model semantics across all SD constructs, and that RL agents can reliably learn dynamic interventions in real-world SD models.

## Confidence

- **High confidence**: The basic feasibility of wrapping SD models in Gym environments (Mechanism 1) is well-supported by the code examples and implementation details.
- **Medium confidence**: The claim that RL can learn dynamic interventions (Mechanism 2) is plausible but only demonstrated on a single model with a relatively simple reward structure.
- **Low confidence**: The assertion that SDGym accelerates policy discovery compared to manual SD analysis (Mechanism 3) lacks quantitative comparison to traditional methods and is based on anecdotal evidence.

## Next Checks

1. Test SDGym on multiple SD models with varying complexity, including models with feedback loops and multiple stocks, to assess the abstraction's generality.
2. Compare RL-discovered policies against traditional SD sensitivity analysis and intervention sweeps on the same models to quantify potential acceleration benefits.
3. Evaluate agent performance when exposed to edge cases like invalid actions or model instability to assess robustness and failure modes.