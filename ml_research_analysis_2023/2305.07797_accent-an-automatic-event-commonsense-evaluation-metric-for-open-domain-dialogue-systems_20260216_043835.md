---
ver: rpa2
title: 'ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue
  Systems'
arxiv_id: '2305.07797'
source_url: https://arxiv.org/abs/2305.07797
tags:
- personx
- commonsense
- accent
- event
- tuples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACCENT introduces an automatic evaluation metric for event commonsense
  in open-domain dialogues by leveraging commonsense knowledge bases. The method extracts
  event-relation tuples from dialogue responses using a prompt-based generative model,
  then scores these tuples by measuring their compatibility with a dynamic commonsense
  knowledge base via similarity-based matching.
---

# ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems

## Quick Facts
- **arXiv ID**: 2305.07797
- **Source URL**: https://arxiv.org/abs/2305.07797
- **Reference count**: 40
- **Primary result**: ACCENT achieves higher correlation with human judgments than existing baselines for event commonsense evaluation in open-domain dialogues.

## Executive Summary
ACCENT introduces an automatic evaluation metric for event commonsense in open-domain dialogue systems by leveraging commonsense knowledge bases. The method extracts event-relation tuples from dialogue responses using a prompt-based generative model, then scores these tuples by measuring their compatibility with a dynamic commonsense knowledge base via similarity-based matching. Experiments on a newly constructed evaluation dataset show ACCENT achieves higher correlations with human judgments than existing baselines, demonstrating its effectiveness in detecting event commonsense errors and enabling interpretable evaluation of dialogue system outputs.

## Method Summary
ACCENT evaluates event commonsense in dialogue responses by first extracting event-relation tuples using a fine-tuned T5 model, then scoring these tuples based on their compatibility with a dynamic commonsense knowledge base (COMET). The extracted tuples contain head events, relations, and tail events, which are compared against commonsense-compliant tail events generated by COMET. The final score is computed by averaging the compatibility scores of all extracted tuples, providing an interpretable measure of event commonsense quality in dialogue responses.

## Key Results
- ACCENT achieves 57.44 Pearson correlation with human judgments, outperforming baselines like BLEU (39.47) and BERTScore (40.12)
- The metric successfully identifies event commonsense errors through interpretable compatibility scores
- Dynamic CSKB approach shows better performance than static CSKB search methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ACCENT's symbolic intermediate representation bridges the gap between free-form dialogue and compact CSKB knowledge.
- **Mechanism**: By extracting event-relation tuples, ACCENT translates conversational data into a structured form that aligns with the format of ATOMIC2020 tuples, enabling direct compatibility testing.
- **Core assumption**: Event-relation tuples extracted from dialogue responses accurately capture the relevant commonsense aspects of the conversation.
- **Evidence anchors**:
  - [abstract]: "ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB."
  - [section]: "ACCENT uses event-relation tuples as the symbolic intermediate representation. Each tuple contains a head event and a tail event which are connected through an event relation."
  - [corpus]: Weak. While the paper demonstrates improved correlation with human judgments, there is no explicit evidence that the extracted tuples perfectly capture all relevant commonsense aspects.
- **Break condition**: If the event-relation extraction fails to capture the key events and relations in the dialogue, the compatibility test will not accurately reflect the event commonsense quality.

### Mechanism 2
- **Claim**: The dynamic CSKB (COMET) provides more generalizable compatibility scores than static CSKB search methods.
- **Mechanism**: By fine-tuning COMET on ATOMIC2020, ACCENT can generate commonsense-compliant tail events for any given head event and relation, allowing for flexible and context-aware compatibility scoring.
- **Core assumption**: The dynamic CSKB can generate sensible tail events that are compatible with the given head event and relation.
- **Evidence anchors**:
  - [abstract]: "ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB."
  - [section]: "ACCENT uses COMET as the dynamic CSKB. COMET adapts the pre-trained language model by fine-tuning it on C through a conditional generation task where 'head relation [GEN]' is the source and a tail event is the target."
  - [corpus]: Weak. While the paper shows improved performance compared to static CSKB search baselines, there is no explicit evaluation of the quality of the generated tail events.
- **Break condition**: If the dynamic CSKB fails to generate sensible tail events for certain head events and relations, the compatibility scores will not accurately reflect the commonsense compliance.

### Mechanism 3
- **Claim**: The similarity-based compatibility scoring method provides interpretable results.
- **Mechanism**: By computing the cosine similarity between the tail event in the extracted tuple and the generated commonsense tail events, ACCENT provides a clear and interpretable measure of compatibility.
- **Core assumption**: The cosine similarity between event embeddings is a meaningful measure of their compatibility in terms of commonsense knowledge.
- **Evidence anchors**:
  - [abstract]: "The compatibility score for (h,r,t) is then computed by checking the similarity between t and the most similar tgen among {tig}k i=1."
  - [section]: "The compatibility score for (h,r,t) is then computed by checking the similarity between t and the most similar tgen among {tig}k i=1."
  - [corpus]: Weak. While the paper demonstrates improved correlation with human judgments, there is no explicit evaluation of the interpretability of the compatibility scores.
- **Break condition**: If the event embeddings do not capture the relevant semantic aspects for compatibility scoring, the similarity-based scores will not accurately reflect the commonsense compliance.

## Foundational Learning

- **Concept**: Event commonsense and its importance in open-domain dialogue systems
  - **Why needed here**: ACCENT focuses on evaluating event commonsense in dialogue responses, so understanding this concept is crucial for grasping the paper's motivation and approach.
  - **Quick check question**: What is event commonsense, and why is it important for open-domain dialogue systems?

- **Concept**: Knowledge bases (CSKBs) and their role in commonsense reasoning
  - **Why needed here**: ACCENT leverages ATOMIC2020 as its knowledge source, so understanding CSKBs and their structure is essential for comprehending the paper's methodology.
  - **Quick check question**: What is ATOMIC2020, and how does it differ from other commonsense knowledge bases like ConceptNet?

- **Concept**: Event-relation extraction and its challenges
  - **Why needed here**: ACCENT relies on extracting event-relation tuples from dialogue responses, so understanding the challenges and approaches in this task is important for grasping the paper's implementation details.
  - **Quick check question**: What are the main challenges in joint event-relation extraction, and how does ACCENT address them?

## Architecture Onboarding

- **Component map**: Event-relation extraction -> Dynamic CSKB query -> Compatibility scoring -> Event commonsense score

- **Critical path**: Event-relation extraction → Dynamic CSKB query → Compatibility scoring → Event commonsense score

- **Design tradeoffs**:
  - Using a fixed set of event relations vs. allowing for more flexible relation extraction
  - Fine-tuning the event-relation extraction model in a low-resource setting vs. using a larger, more diverse training set
  - Generating commonsense tail events vs. directly searching for compatible tuples in the CSKB

- **Failure signatures**:
  - Poor correlation with human judgments: Indicates issues with event-relation extraction, dynamic CSKB, or compatibility scoring
  - Inconsistent scores across similar dialogue responses: Suggests problems with the event-relation extraction or dynamic CSKB
  - High computational cost: May indicate inefficiencies in the event-relation extraction or dynamic CSKB components

- **First 3 experiments**:
  1. Evaluate the correlation between ACCENT scores and human judgments on a held-out test set
  2. Analyze the quality of the extracted event-relation tuples by comparing them to human annotations
  3. Investigate the impact of different event-relation extraction models and dynamic CSKB configurations on the final ACCENT scores

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of commonsense knowledge base (CSKB) impact ACCENT's performance, and could other CSKBs improve results?
- **Basis in paper**: [explicit] The authors mention that ACCENT uses ATOMIC2020 as the CSKB and discuss its comprehensiveness, but they also note that augmenting the framework with more commonsense resources could improve performance.
- **Why unresolved**: The paper does not explore the impact of using alternative or additional CSKBs beyond ATOMIC2020, leaving open the question of whether other knowledge bases might yield better results.
- **What evidence would resolve it**: Comparative experiments using different CSKBs (e.g., GLUCOSE, ConceptNet) to evaluate their impact on ACCENT's performance in terms of correlation with human judgments.

### Open Question 2
- **Question**: What are the limitations of the current event-relation extraction model, and how can it be improved for better accuracy?
- **Basis in paper**: [explicit] The authors acknowledge that the T5 model used for event-relation extraction is fine-tuned in a low-resource setting and that there is room for improvement, particularly in handling multiple participants and relation identification.
- **Why unresolved**: While the paper discusses the current model's limitations, it does not provide specific solutions or experiments to address these issues, leaving the potential for improvement unclear.
- **What evidence would resolve it**: Experiments with alternative models or training strategies, such as using high-quality synthetic data or transfer learning, to improve the accuracy of event-relation extraction.

### Open Question 3
- **Question**: How does ACCENT's performance vary across different types of dialogue contexts, such as human-human versus human-machine interactions?
- **Basis in paper**: [explicit] The authors mention testing ACCENT on both human-human dialogues (DECO) and human-machine dialogues (ConTurE Subset), but they do not provide a detailed analysis of performance differences across these contexts.
- **Why unresolved**: The paper does not explore how ACCENT's performance might vary depending on the type of dialogue context, which could impact its generalizability and effectiveness.
- **What evidence would resolve it**: Detailed performance comparisons of ACCENT across different dialogue contexts, including error analysis and insights into how context affects its accuracy.

### Open Question 4
- **Question**: How does the choice of sentence embedding method affect ACCENT's performance, and are there more effective alternatives?
- **Basis in paper**: [explicit] The authors experiment with different sentence embedding methods (Sentence-BERT, DiffCSE, ESimCSE, Sup-SimCSE) and find that Sup-SimCSE improves performance, but they do not explore other potential alternatives.
- **Why unresolved**: The paper does not exhaustively test all possible sentence embedding methods, leaving open the question of whether even more effective alternatives exist.
- **What evidence would resolve it**: Comparative experiments using a wider range of sentence embedding methods to determine their impact on ACCENT's performance and identify the most effective approach.

## Limitations
- Event-relation extraction component shows moderate accuracy (BLEU 27.68, BERTScore 81.54), potentially limiting overall metric reliability
- Dynamic CSKB generation quality is not explicitly validated, creating uncertainty about compatibility score accuracy
- Evaluation dataset construction may introduce selection bias through filtering criteria

## Confidence
- **High**: Correlation improvements over baselines are well-supported by experimental results
- **Medium**: Interpretability claim lacks direct evidence about alignment with human understanding
- **Low**: Claim that symbolic representation fully captures conversational commonsense nuance

## Next Checks
1. **Event-Relation Extraction Quality**: Conduct human evaluation study comparing extracted tuples to ground truth, focusing on systematically missed or incorrectly extracted events and relations

2. **Dynamic CSKB Generation Validation**: Have human raters assess quality and commonsense compliance of COMET-generated tail events compared to static CSKB search results

3. **Error Analysis on Diverse Dialogue Domains**: Test ACCENT on task-oriented dialogues and debates to identify systematic failure patterns across different conversational contexts