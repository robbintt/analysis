---
ver: rpa2
title: Scaling Session-Based Transformer Recommendations using Optimized Negative
  Sampling and Loss Functions
arxiv_id: '2307.14906'
source_url: https://arxiv.org/abs/2307.14906
tags:
- sasrec
- sampling
- tron
- negative
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRON, a scalable session-based transformer
  recommendation system that addresses limitations of existing models like SASRec
  and GRU4Rec+. TRON integrates top-k negative sampling and listwise loss functions
  (sampled softmax) to enhance recommendation accuracy and training efficiency.
---

# Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions

## Quick Facts
- **arXiv ID:** 2307.14906
- **Source URL:** https://arxiv.org/abs/2307.14906
- **Reference count:** 23
- **Primary result:** TRON achieves 6.5%+ improvement in Recall@20 and MRR@20 on OTTO dataset with 1090% training speedup over GRU4Rec+

## Executive Summary
This paper introduces TRON, a scalable session-based transformer recommendation system that addresses limitations of existing models like SASRec and GRU4Rec+. TRON integrates top-k negative sampling and listwise loss functions (sampled softmax) to enhance recommendation accuracy and training efficiency. The key innovation is combining batchwise uniform and in-batch sessionwise negative sampling with a top-k strategy that focuses on the most misranked negatives. Experiments on three large-scale e-commerce datasets show TRON improves recommendation quality over benchmark models while maintaining training speeds similar to SASRec.

## Method Summary
TRON builds upon the SASRec architecture, incorporating a dual negative sampling strategy (batchwise uniform and in-batch sessionwise) combined with top-k filtering and sampled softmax loss. The model samples 8192 batchwise uniform negatives and 127 in-batch sessionwise negatives per training step, then selects the top 100 negatives ranked by model score for gradient updates. This approach focuses computational resources on the most challenging negatives while maintaining semantic relevance through in-batch sampling. The system is trained using a transformer encoder with 8 attention heads and embedding size of 100, optimized with the Adam optimizer.

## Key Results
- 6.5%+ increase in both Recall@20 and MRR@20 metrics on the OTTO dataset
- 1090% training speedup compared to GRU4Rec+ while maintaining similar accuracy to SASRec
- 18.14% increase in click-through rate in live A/B test against SASRec
- Achieves 30.61% Recall@20 and 24.32% MRR@20 on OTTO dataset, significantly outperforming baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-k negative sampling improves both accuracy and training speed by focusing gradient updates on the hardest negatives.
- Mechanism: Instead of updating gradients for all sampled negatives, TRON selects only the top-k negatives ranked by model score (i.e., those most likely to be misranked as positives) and updates only those. This reduces computational load while improving discriminative learning.
- Core assumption: The model's current predictions already give a meaningful ranking of negatives, so updating the top misranked ones yields more useful gradient signals than updating all negatives uniformly.
- Evidence anchors:
  - [abstract] "This strategy focuses on updating the top-k negatives during training instead of updating the whole set of negative ratings."
  - [section] "These top-k items are then used for updates in the backpropagation step, while the rest are discarded."
- Break condition: If the scoring distribution is too flat (many negatives have similar low scores), the top-k selection may become arbitrary and provide little benefit.

### Mechanism 2
- Claim: Combining batchwise uniform and in-batch sessionwise negative sampling balances scalability and representation quality.
- Mechanism: Batchwise uniform sampling draws negatives from the global item distribution, ensuring broad coverage and computational efficiency. In-batch sessionwise sampling reuses actual items from the current batch as negatives, improving semantic relevance while avoiding the need for additional CPU-GPU transfers.
- Core assumption: Uniform sampling is efficient but may miss hard negatives; in-batch sampling adds semantic relevance but must be filtered to avoid including positives from the same session.
- Evidence anchors:
  - [section] "TRON uses a combination of uniform batchwise and in-batch sessionwise negative sampling to maintain training speed while improving accuracy."
  - [section] "This is possible in GRU4Rec because, due to the way a batch is constructed, at each time step ùë°, no other item from the session ùë† except ùëñùë° exists in the batch."
- Break condition: If the batch contains too few diverse sessions, in-batch negatives may become too similar to positives, reducing their effectiveness.

### Mechanism 3
- Claim: Listwise loss (sampled softmax) outperforms pointwise and pairwise losses in handling large item sets and reducing popularity bias.
- Mechanism: Sampled softmax approximates the full softmax over the entire item set by computing scores only for a small, sampled set of negatives, then normalizing over this set. This reduces computation while focusing on ranking within a realistic candidate set, alleviating the dominance of popular items.
- Core assumption: The sampled set of negatives, when combined with the positive, provides a representative subset for accurate ranking.
- Evidence anchors:
  - [abstract] "TRON integrates top-k negative sampling and listwise loss functions (sampled softmax) to enhance recommendation accuracy and training efficiency."
  - [section] "TRON uses sampled softmax (SSM) [2, 3], a listwise loss function with several beneficial properties, such as alleviating popularity bias and maximizing ranking metrics [16]."
- Break condition: If the negative sample size is too small, the approximation error in the softmax becomes too large, hurting ranking performance.

## Foundational Learning

- Concept: Negative sampling in large-scale recommendation
  - Why needed here: With millions of items, computing full softmax is infeasible; negative sampling reduces the problem to distinguishing positives from a small set of negatives.
  - Quick check question: Why can't we just use random negatives from the full item set for every training step?

- Concept: Session-based recommendation and sequence modeling
  - Why needed here: The model predicts the next item based on a user's interaction sequence within a session, requiring careful handling of temporal order and session boundaries.
  - Quick check question: What happens if items from the same session leak into the negative sample set?

- Concept: Listwise vs pointwise vs pairwise ranking losses
  - Why needed here: Different loss functions optimize different aspects of ranking; sampled softmax (listwise) directly optimizes ranking metrics and reduces popularity bias, unlike binary cross-entropy (pointwise) or BPR-MAX (pairwise).
  - Quick check question: How does sampled softmax approximate the full softmax, and why is this approximation useful?

## Architecture Onboarding

- Component map:
  Input sequences ‚Üí Embedding layer ‚Üí Transformer encoder ‚Üí Score computation ‚Üí Negative sampling ‚Üí Sampled softmax ‚Üí Backprop (top-k only)

- Critical path:
  CPU ‚Üí Embedding lookup ‚Üí Transformer blocks ‚Üí Score computation ‚Üí Negative sampling ‚Üí Sampled softmax ‚Üí Backprop (top-k only)

- Design tradeoffs:
  - More negatives ‚Üí better coverage but higher compute; mitigated by batchwise sampling and top-k filtering.
  - Top-k selection ‚Üí focuses gradients but requires reliable scoring; if scoring is poor, top-k may be arbitrary.
  - In-batch negatives ‚Üí efficient and semantically relevant but require careful filtering to avoid positives.

- Failure signatures:
  - Accuracy stalls despite more negatives ‚Üí likely top-k selection is arbitrary due to flat score distribution.
  - GPU memory spikes ‚Üí too many negatives passed through the network before top-k filtering.
  - Slow convergence ‚Üí negative sampling distribution too narrow (e.g., only in-batch negatives).

- First 3 experiments:
  1. Baseline: SASRec with standard uniform negatives and BCE loss; verify training speed and recall.
  2. Add batchwise uniform negatives and in-batch sessionwise negatives with sampled softmax; measure accuracy and speed changes.
  3. Introduce top-k filtering on negatives; compare training speed and accuracy against experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TRON's top-k negative sampling strategy perform with different k values across various dataset sizes and characteristics?
- Basis in paper: [explicit] The paper mentions using top-100 negatives but doesn't explore how different k values affect performance
- Why unresolved: The authors only tested with a fixed k=100 value and didn't conduct sensitivity analysis on the top-k parameter
- What evidence would resolve it: Systematic experiments varying k from small (e.g., 10) to large (e.g., 500) values across different datasets showing accuracy and training time trade-offs

### Open Question 2
- Question: Can TRON's negative sampling approach be effectively combined with other advanced recommendation techniques like multi-interest modeling or graph neural networks?
- Basis in paper: [inferred] The paper focuses on transformer architecture but doesn't explore integration with other modern recommendation approaches
- Why unresolved: The experiments only test TRON with the basic SASRec architecture without exploring hybrid approaches
- What evidence would resolve it: Comparative studies of TRON integrated with multi-interest extractors or graph-based methods versus standalone implementations

### Open Question 3
- Question: How does TRON's performance scale with extremely large item sets (100M+ items) common in major e-commerce platforms?
- Basis in paper: [explicit] The OTTO dataset has 12.9M items, but authors note TRON shows improved scalability without testing at larger scales
- Why unresolved: The largest dataset tested (OTTO) is still far smaller than what major platforms handle, and scaling behavior beyond tested range is unknown
- What evidence would resolve it: Performance evaluation on datasets with 100M+ items measuring accuracy, training time, and memory usage compared to baseline models

## Limitations
- Evaluation relies on three offline benchmarks and one online A/B test, limiting generalizability
- The OTTO dataset is proprietary and not publicly available for independent verification
- Top-k selection assumes reliable model scoring, which may be problematic during early training stages
- In-batch sessionwise negatives depend on batch composition and may have limited diversity

## Confidence
- High confidence in the conceptual benefits of top-k filtering and listwise loss for efficiency and accuracy
- Medium confidence in the combined sampling strategy due to lack of ablation studies
- Low confidence in absolute performance gains without independent replication

## Next Checks
1. Re-implement TRON on a publicly available dataset (e.g., Diginetica) and verify the claimed 6.5% improvement in Recall@20/MRR@20
2. Conduct an ablation study isolating the effects of top-k filtering, in-batch negatives, and sampled softmax loss
3. Test the sensitivity of top-k selection to early training instability by varying k and measuring convergence behavior