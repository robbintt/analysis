---
ver: rpa2
title: Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision
arxiv_id: '2311.15108'
source_url: https://arxiv.org/abs/2311.15108
tags:
- image
- fairness
- images
- dataset
- occupation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a diffusion-based approach to generate datasets
  balanced along demographic traits for evaluating fairness in computer vision models.
  The key idea is to use diffusion models to generate a large set of base images,
  then perturb them using inpainting to create variants representing different perceived
  races.
---

# Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision

## Quick Facts
- arXiv ID: 2311.15108
- Source URL: https://arxiv.org/abs/2311.15108
- Reference count: 6
- Key outcome: Diffusion-based approach generates balanced datasets to measure fairness in vision models, finding racial bias in occupation classification

## Executive Summary
This paper introduces a novel method for measuring fairness in computer vision models by using diffusion models to generate and perturb images along demographic traits. The approach creates datasets where the only variation between images is the perceived race of the subject, while maintaining identical contexts. By evaluating vision-language models on these perturbed datasets, the authors demonstrate significant disparities in classification accuracy across demographic groups, suggesting racial biases in the models. The paper also proposes a new fairness metric based on the variance of prediction probabilities across demographic groups.

## Method Summary
The method uses Stable Diffusion XL to generate base images for specific occupations, then applies inpainting to create variants representing different perceived races while preserving context. Images are automatically filtered using VQA models for quality assessment and FairFace models for demographic validation. The resulting datasets are used to evaluate vision-language models on occupation classification tasks, with fairness measured by the standard deviation of true label probabilities across demographic groups.

## Key Results
- Images with non-Caucasian labels show higher misclassification rates than Caucasian-labeled images
- The proposed fairness metric effectively captures disparities between models
- CLIP-OpenAI achieves the highest fairness score among evaluated models
- FLA V A shows significant performance differences across demographic groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion perturbations isolate demographic bias by generating image sets with identical contexts but different perceived race
- Mechanism: Base images are generated with a diffusion model, then masked and inpainted with race-specific prompts, producing image sets where only the demographic trait varies
- Core assumption: The inpainting process preserves background and context while only altering the demographic trait
- Evidence anchors: [abstract] "Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race." [section] "Images, in a given set of perturbations, share the same backgrounds and contexts, with the only difference being the perturbed demographic trait."

### Mechanism 2
- Claim: Fairness metric captures model bias by measuring probability variance across demographic groups
- Mechanism: For each image set, compute the standard deviation of the true label probability across race variants, then take the median across all sets
- Core assumption: A fair model should predict the true label with similar probability across all demographic groups
- Evidence anchors: [abstract] "We measure a model's downstream fairness by computing the standard deviation in the probability of predicting the true occupation label across the different perceived identity groups."

### Mechanism 3
- Claim: Automated filtering ensures high realism and prompt fidelity in generated images
- Mechanism: Images are filtered using VQA model questions about occupation presence, limb realism, and overall realism, followed by color checks and FairFace model validation
- Core assumption: VQA and FairFace models can reliably assess image quality and demographic representation
- Evidence anchors: [section] "We use a ViLT-B/32 VQA model... to evaluate the base images... We ask the VQA model 'Is there a <occupation> in this image?' [Q1] where <occupation> is the occupation in the original prompt used to generate the image."

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: Core technology for generating base images and performing inpainting for demographic perturbations
  - Quick check question: What is the key difference between diffusion models and GANs in terms of image realism?

- Concept: Fairness metrics in ML
  - Why needed here: Method for quantifying and comparing model bias across demographic groups
  - Quick check question: How does measuring standard deviation in prediction probabilities across groups capture fairness?

- Concept: VQA (Visual Question Answering) models
  - Why needed here: Tool for automated assessment of image quality and prompt fidelity
  - Quick check question: What types of questions are asked of the VQA model to filter images?

## Architecture Onboarding

- Component map: Stable Diffusion XL (base image generation) → VQA model (base image filtering) → Segmentation models (mask generation) → Stable Diffusion inpainting (demographic perturbation) → FairFace model (perturbed image filtering) → Evaluation models (fairness assessment)
- Critical path: Image generation → Base filtering → Mask generation → Inpainting → Perturbed filtering → Evaluation
- Design tradeoffs: Using diffusion models provides high realism but may introduce demographic biases from training data. Automated filtering reduces human labor but relies on model accuracy
- Failure signatures: Poor realism in generated images, misclassification of demographic groups, high variance in fairness metric indicating bias
- First 3 experiments:
  1. Generate base images for one occupation and manually inspect realism
  2. Apply inpainting with one race prompt and check if background/context is preserved
  3. Run VQA and FairFace filtering on a small sample and verify filtering criteria are met

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the perceived race of the base image affect the quality of perturbations for other identity groups?
- Basis in paper: [explicit] The authors note that for occupations with a white skew in the US, the base image tends to be Caucasian-presenting, possibly due to bias in Stable Diffusion. They question whether this affects the quality of perturbations for other identity groups.
- Why unresolved: The authors believe that since diffusion inpainting is used, the perceived race of the base image is less likely to affect perturbation quality. However, they do not provide empirical evidence to confirm this.
- What evidence would resolve it: Conducting an experiment where base images of different perceived races are perturbed and then evaluating the quality of these perturbations would provide evidence.

### Open Question 2
- Question: How can the diversity of people belonging to different identity groups be ensured in image generations?
- Basis in paper: [inferred] The authors acknowledge that diffusion models may not generate images that reflect the diversity of people from different identity groups, as image generations build on the representations found in the training dataset, which may be narrowly defined for people of color.
- Why unresolved: The authors suggest evaluating methods like FairDiffusion which can instruct generative models to produce fairer and more diverse images, but do not provide a concrete solution or evaluate such methods.
- What evidence would resolve it: Implementing and evaluating methods that aim to produce fairer and more diverse images, such as FairDiffusion, would provide evidence.

### Open Question 3
- Question: How does the size of the dataset used to develop richer embeddings affect the fairness of the model?
- Basis in paper: [explicit] The authors find that CLIP-OpenAI, which was likely trained on a better curated dataset, scores the highest fairness metric, while CLIP-LAION2B, which has a larger dataset, has a lower fairness metric. This suggests that there might be diminishing returns to fairness improvements with dataset size, especially if the underlying data contains significant biases.
- Why unresolved: While the authors provide evidence that a larger dataset does not necessarily lead to a fairer model, they do not explore the relationship between dataset size and fairness in depth or provide a definitive conclusion.
- What evidence would resolve it: Conducting a comprehensive study that evaluates the fairness of models trained on datasets of varying sizes and biases would provide evidence.

## Limitations

- The approach relies heavily on potentially biased filtering models (VQA and FairFace) without systematic validation of their fairness
- Limited evaluation scope to only 5 occupations restricts generalizability of findings
- No quantitative analysis of perturbation fidelity or success rate in preserving context during inpainting
- Potential biases in the base diffusion model itself may influence the starting point for perturbations

## Confidence

**High Confidence**: Technical methodology of using diffusion models for image generation and inpainting is well-established. Fairness metric definition (standard deviation of prediction probabilities across groups) is mathematically sound and directly interpretable.

**Medium Confidence**: Core empirical finding that images with non-Caucasian labels have higher misclassification rates appears supported by the data, but limited occupational scope and lack of statistical significance testing reduce confidence in generalizing this finding.

**Low Confidence**: Claim that the proposed approach provides a comprehensive solution for fairness evaluation in computer vision. Reliance on potentially biased filtering models and limited scope of evaluation occupations create significant uncertainty about real-world applicability.

## Next Checks

1. **Filter Model Bias Analysis**: Run the VQA and FairFace filtering models on a balanced dataset with known ground truth labels to quantify their accuracy and potential demographic biases. This would validate whether the automated filtering step is introducing its own biases.

2. **Perturbation Fidelity Study**: Select a random sample of perturbed image sets and conduct human evaluation to measure how well the inpainting preserves background/context while only altering demographic traits. Quantify the success rate and identify failure modes.

3. **Extended Occupational Evaluation**: Generate and evaluate at least 10 additional occupations spanning different gender and racial associations to test whether the observed bias patterns hold across a broader occupational spectrum. This would validate the generalizability of the findings beyond the initial 5 occupations.