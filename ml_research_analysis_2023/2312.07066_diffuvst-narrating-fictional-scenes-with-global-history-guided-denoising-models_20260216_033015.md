---
ver: rpa2
title: 'DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising
  Models'
arxiv_id: '2312.07066'
source_url: https://arxiv.org/abs/2312.07066
tags:
- diffu
- visual
- guidance
- story
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DIFFU VST, a diffusion-based system for visual
  storytelling that generates a series of visual descriptions as a single conditional
  denoising process. DIFFU VST leverages a transformer encoder to restore word embeddings
  of story texts from Gaussian noise sequences, conditioned on image features and
  guided by global textual history.
---

# DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models

## Quick Facts
- arXiv ID: 2312.07066
- Source URL: https://arxiv.org/abs/2312.07066
- Reference count: 12
- Key outcome: Diffusion-based system generating visual descriptions in parallel with improved text quality and faster inference

## Executive Summary
DIFFU VST introduces a novel diffusion-based approach for visual storytelling that generates coherent narratives from image sequences through a single non-autoregressive denoising process. The system leverages a transformer encoder to simultaneously reconstruct word embeddings from noisy inputs, conditioned on image features and guided by global textual history across all panels. By incorporating multimodal adapter modules and classifier-free guidance, DIFFU VST achieves state-of-the-art performance on text quality metrics while significantly reducing inference time compared to autoregressive baselines.

## Method Summary
DIFFU VST treats visual storytelling as a conditional denoising problem where a transformer encoder jointly reconstructs word embeddings for all captions in parallel from Gaussian noise. The model uses frozen BLIP encoders with adapter modules to extract multimodal features from image-text pairs, then fuses these with corrupted word embeddings through a feature fusion module. A global visual history condition provides comprehensive context across the entire image stream, while classifier-free guidance enables bidirectional text history guidance. The denoising process generates all captions simultaneously, drastically reducing inference time compared to sequential autoregressive approaches.

## Key Results
- Outperforms autoregressive baselines on BLEU, ROUGE-L, METEOR, and CIDEr metrics across four fictional visual-story datasets
- Achieves significantly faster inference speeds through parallel non-autoregressive generation
- Demonstrates improved inter-sentence coherence through global textual history guidance
- Shows effective domain adaptation through multimodal adapter modules for fictional imagery

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-autoregressive denoising enables parallel generation of all captions, drastically reducing inference time.
- **Mechanism:** DIFFU VST reconstructs all word embeddings simultaneously from noisy inputs rather than generating tokens sequentially.
- **Core assumption:** Transformer can denoise multiple embeddings in parallel without losing coherence.
- **Evidence anchors:** Abstract mentions "highly diverse narratives more efficiently"; section describes simultaneous denoising of multiple random vectors.
- **Break condition:** If parallel generation fails to maintain coherence or produces low-quality embeddings.

### Mechanism 2
- **Claim:** Global textual history guidance improves inter-sentence coherence through bidirectional context.
- **Mechanism:** Fuses textual features of all panels into denoising process, conditioning generation on both preceding and following text.
- **Core assumption:** Model can utilize textual features from all panels without overfitting to unavailable ground-truth context.
- **Evidence anchors:** Abstract mentions "bi-directional text history guidance"; section describes global visual history across image stream.
- **Break condition:** Improper tuning of guidance strength or unguided probability degrades coherence.

### Mechanism 3
- **Claim:** Multimodal adapter modules transfer BLIP features to fictional domain, improving image-to-text fidelity.
- **Mechanism:** Adapter layers fine-tune BLIP features for fictional imagery, bridging domain gap between real-world pretraining and synthetic datasets.
- **Core assumption:** Adapters can effectively adapt general knowledge to fictional scenes without full fine-tuning.
- **Evidence anchors:** Abstract mentions adapter modules improving image-to-text fidelity; section discusses BLIP pretrained on real-world data.
- **Break condition:** If adapters cannot transfer knowledge effectively or frozen backbone limits adaptation.

## Foundational Learning

- **Concept:** Diffusion models in continuous domain
  - **Why needed here:** Fundamental to understanding forward noise injection and reverse denoising processes
  - **Quick check question:** What distinguishes the forward process (adding noise) from the reverse process (denoising) in diffusion models?

- **Concept:** Classifier-free guidance mechanism
  - **Why needed here:** Crucial for implementing global textual history guidance and understanding trade-offs
  - **Quick check question:** How does classifier-free guidance combine conditional and unconditional score estimates?

- **Concept:** Multimodal feature fusion
  - **Why needed here:** Central to combining visual and textual information from all panels for global history context
  - **Quick check question:** What purpose does concatenating corrupted word embeddings with multimodal features serve?

## Architecture Onboarding

- **Component map:** BLIP multimodal encoders (frozen) → Adapter modules → Feature fusion → Transformer denoiser → Word embedding reconstruction
- **Critical path:** 1) Extract image/text features from all panels using BLIP 2) Apply adapter modules for fictional domain 3) Fuse features with noisy embeddings 4) Transformer denoises to reconstruct embeddings 5) Project embeddings to generate all captions in parallel
- **Design tradeoffs:** Frozen BLIP saves parameters but may limit adaptation; non-autoregressive provides speed but needs coherence design; global guidance improves context but needs tuning; single process simplifies but may struggle with long stories
- **Failure signatures:** Low BLEU/ROUGE with high CIDEr suggests over-filtering noise; high BLEU/ROUGE with low CIDEr suggests common phrases but lack specificity; slow inference despite non-autoregressive design indicates inefficiency; poor coherence suggests inadequate global guidance
- **First 3 experiments:** 1) Test basic denoising with synthetic noise and clean embeddings to verify transformer functionality 2) Evaluate adapter modules by comparing BLIP vs adapted features on small fictional dataset 3) Measure inference speedup by comparing DIFFU VST vs autoregressive baseline on single story

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does optimal configuration of guidance strength w and unguided probability punguide vary across different visual storytelling datasets?
- **Basis in paper:** [explicit] Paper shows best performance with w = 1.0 and punguide = 0.7 on DiDeMoSV but states optimal values may differ for other datasets
- **Why unresolved:** Only detailed results for one dataset, mentions variation but doesn't explore across all four datasets
- **What evidence would resolve it:** Experiments with varying w and punguide values on all four datasets (AESOP, PororoSV, FlintstonesSV, DiDeMoSV) reporting optimal configurations for each

### Open Question 2
- **Question:** What is impact of using different pretrained multimodal backbones instead of BLIP on DIFFU VST performance?
- **Basis in paper:** [inferred] Uses BLIP with adapter modules but doesn't explore alternative backbones or compare their impact
- **Why unresolved:** Focuses on BLIP effectiveness without investigating benefits/drawbacks of alternative multimodal backbones
- **What evidence would resolve it:** Replacing BLIP with other pretrained models (CLIP, ALIGN) and comparing performance on visual storytelling task

### Open Question 3
- **Question:** How does number of denoising steps affect coherence and diversity of generated visual stories?
- **Basis in paper:** [explicit] Mentions inference time positively correlated with denoising steps and more steps may lead to refined output, but lacks detailed analysis of quality impact
- **Why unresolved:** Briefly discusses relationship between steps and inference time but doesn't explore trade-off with coherence/diversity
- **What evidence would resolve it:** Experiments with varying denoising steps analyzing impact on coherence and diversity metrics (BLEU, ROUGE-L, CIDEr)

## Limitations
- Lack of detailed hyperparameter specifications, particularly for guidance mechanism and dynamic loss weight scheduling
- No ablation studies on adapter modules to quantify their contribution to domain adaptation
- Absence of qualitative analysis or user studies to validate narrative coherence and engagement beyond automated metrics

## Confidence
**High Confidence:** Non-autoregressive nature enabling parallel generation and faster inference - well-supported by diffusion model properties and explicit abstract claims
**Medium Confidence:** Global textual history guidance improving inter-sentence coherence - plausible based on bidirectional context mechanism but lacks direct comparative evidence
**Medium Confidence:** Multimodal adapter modules improving image-to-text fidelity - reasonable assumption given domain gap but without ablation studies remains weakly supported

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Vary guidance strength w (0.5 to 2.0) and unguided probability punguide (0.1 to 0.5) to determine impact on BLEU/ROUGE scores and inference speed
2. **Adapter Module Ablation Study:** Train and evaluate DIFFU VST with and without adapter modules on all four datasets, measuring change in CIDEr scores and inference time
3. **Qualitative Coherence Assessment:** Generate stories from held-out test set and have human raters evaluate inter-sentence coherence and narrative flow on 1-5 scale, comparing against autoregressive baselines