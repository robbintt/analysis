---
ver: rpa2
title: Contributions to the Improvement of Question Answering Systems in the Biomedical
  Domain
arxiv_id: '2307.13631'
source_url: https://arxiv.org/abs/2307.13631
tags:
- questions
- question
- biomedical
- answer
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis proposes methods to improve biomedical question answering
  (QA) systems, which provide precise answers to natural language questions from biomedical
  literature. The work addresses challenges like complex terminology and large document
  collections.
---

# Contributions to the Improvement of Question Answering Systems in the Biomedical Domain

## Quick Facts
- arXiv ID: 2307.13631
- Source URL: https://arxiv.org/abs/2307.13631
- Reference count: 0
- Key outcome: Proposed SemBioNLQA system achieves 89.40% question classification accuracy and ranks among top winners in BioASQ 2017 challenge

## Executive Summary
This thesis presents methods to improve biomedical question answering systems that provide precise answers to natural language questions from biomedical literature. The work addresses challenges like complex terminology and large document collections by developing SemBioNLQA, an integrated system with four key components: question type classification, document retrieval, passage retrieval, and answer extraction. The system leverages semantic knowledge resources like UMLS Metathesaurus and achieves significant improvements over state-of-the-art methods, demonstrating effectiveness across various question types including yes/no, factoid, list, and summary questions.

## Method Summary
The SemBioNLQA system implements a four-component pipeline for biomedical QA. It begins with machine learning-based question type classification using handcrafted lexico-syntactic patterns and SVM classifiers to categorize questions into yes/no, factoid, list, or summary types. Document retrieval uses MetaMap to map question terms to UMLS concepts and retrieves relevant MEDLINE documents, which are then reranked using semantic similarity. Passage retrieval employs BM25 with stemmed words and UMLS concepts to extract relevant sentences from top documents. Answer extraction methods vary by question type: sentiment analysis for yes/no answers, UMLS mapping for factoid/list questions, and BM25-based summarization for ideal answers.

## Key Results
- Achieved 89.40% accuracy in question type classification
- Ranked among top winners in the 2017 BioASQ challenge
- Demonstrated effectiveness across all question types (yes/no, factoid, list, summary)
- Outperformed state-of-the-art methods in biomedical QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The system integrates four key components—question type classification, document retrieval, passage retrieval, and answer extraction—to handle biomedical QA comprehensively.
- **Mechanism:** Each component processes a specific stage of the QA pipeline, ensuring that the question is correctly classified before retrieving relevant documents, extracting relevant passages, and finally generating precise answers.
- **Core assumption:** The pipeline architecture is necessary and sufficient for effective biomedical QA.
- **Evidence anchors:** [abstract] The thesis proposes methods to improve biomedical QA systems, which provide precise answers to natural language questions from biomedical literature.
- **Break condition:** If any component fails to perform adequately, the overall QA system performance will be compromised.

### Mechanism 2
- **Claim:** The system uses machine learning-based methods for question type classification to determine the expected answer format.
- **Mechanism:** The system first classifies questions into categories (yes/no, factoid, list, summary) using handcrafted lexico-syntactic patterns and machine learning algorithms. This classification enables the system to use the appropriate answer extraction method.
- **Core assumption:** Accurate question type classification is crucial for effective answer extraction.
- **Evidence anchors:** [abstract] We propose a machine learning-based method for question type classification to determine the types of given questions which enable to a biomedical QA system to use the appropriate answer extraction method.
- **Break condition:** If the question type classification is inaccurate, the system will use an inappropriate answer extraction method, leading to incorrect answers.

### Mechanism 3
- **Claim:** The system uses semantic knowledge resources like UMLS Metathesaurus for document and passage retrieval.
- **Mechanism:** The system uses MetaMap to map terms in questions to UMLS Metathesaurus concepts, then uses these concepts to formulate queries for document retrieval. It also uses UMLS similarity for document reranking and BM25 model with UMLS concepts for passage retrieval.
- **Core assumption:** Semantic knowledge resources improve the relevance of retrieved documents and passages.
- **Evidence anchors:** [abstract] We propose a document retrieval method to retrieve a set of relevant documents that are likely to contain the answers to biomedical questions from the MEDLINE database.
- **Break condition:** If the semantic knowledge resources are not comprehensive or accurate, the relevance of retrieved documents and passages will be compromised.

## Foundational Learning

- **Concept:** Question type classification
  - Why needed here: To determine the expected answer format and use the appropriate answer extraction method.
  - Quick check question: Can the system accurately classify a given biomedical question into one of the four categories (yes/no, factoid, list, summary)?

- **Concept:** Document retrieval
  - Why needed here: To retrieve relevant documents that are likely to contain the answers to biomedical questions from the MEDLINE database.
  - Quick check question: Can the system retrieve relevant documents using a specialized IR system that gives access to the MEDLINE database?

- **Concept:** Passage retrieval
  - Why needed here: To retrieve relevant passages from the retrieved documents that are likely to contain the answers to biomedical questions.
  - Quick check question: Can the system retrieve relevant passages using the BM25 model with UMLS concepts as features?

- **Concept:** Answer extraction
  - Why needed here: To generate both exact and ideal answers from the retrieved passages based on the question type and expected answer format.
  - Quick check question: Can the system generate appropriate answers for each question type (yes/no, factoid, list, summary) using the appropriate answer extraction method?

## Architecture Onboarding

- **Component map:** Question → Question classification → Document retrieval → Passage retrieval → Answer extraction → Answer
- **Critical path:** Question → Question classification → Document retrieval → Passage retrieval → Answer extraction → Answer
- **Design tradeoffs:**
  - Using handcrafted lexico-syntactic patterns vs. purely machine learning-based methods for question type classification.
  - Using UMLS Metathesaurus for semantic knowledge vs. other semantic resources.
  - Using BM25 model with UMLS concepts vs. other retrieval models for passage retrieval.
- **Failure signatures:**
  - Incorrect question type classification → Inappropriate answer extraction method used.
  - Irrelevant documents retrieved → Irrelevant passages extracted.
  - Irrelevant passages extracted → Incorrect answers generated.
- **First 3 experiments:**
  1. Test the accuracy of the question type classification method on a set of biomedical questions.
  2. Test the relevance of the retrieved documents using the document retrieval method.
  3. Test the relevance of the retrieved passages using the passage retrieval method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can deep learning models be incorporated into biomedical QA systems to improve performance, particularly for factoid and list questions?
- Basis in paper: [inferred] The authors mention deep learning models as a potential future direction for improving question classification and answer extraction, specifically noting their potential for factoid and list questions.
- Why unresolved: Deep learning models require large amounts of training data, which may be challenging to obtain in the biomedical domain. Additionally, the authors note that most deep learning approaches are currently applied to factoid-type QA.
- What evidence would resolve it: Experiments comparing the performance of deep learning models to traditional machine learning methods on biomedical QA tasks, particularly for factoid and list questions, using large-scale biomedical datasets.

### Open Question 2
- Question: How can structured data sources, such as databases, ontologies, RDF triples, and the LOD cloud, be effectively integrated into biomedical QA systems to improve answer extraction performance?
- Basis in paper: [explicit] The authors suggest exploring structured data sources as a future direction for improving the proposed QA system, SemBioNLQA.
- Why unresolved: Integrating structured data sources into QA systems presents challenges in terms of data harmonization, semantic alignment, and efficient querying.
- What evidence would resolve it: Comparative studies evaluating the performance of biomedical QA systems using structured data sources versus traditional text-based approaches, demonstrating improvements in answer accuracy and comprehensiveness.

### Open Question 3
- Question: How can the use of full-text articles, such as those available in PubMed Central, improve the performance of biomedical QA systems compared to using abstracts only?
- Basis in paper: [explicit] The authors propose exploring full-text articles as a future direction, noting that MEDLINE contains links to full-text articles in PubMed Central.
- Why unresolved: Full-text articles may contain more detailed information and context than abstracts, but they also introduce challenges in terms of processing and information extraction from unstructured text.
- What evidence would resolve it: Experiments comparing the performance of biomedical QA systems using full-text articles versus abstracts, demonstrating improvements in answer accuracy and completeness for different question types.

## Limitations

- The evaluation relies heavily on BioASQ challenge datasets, which may not fully represent the broader biomedical literature landscape.
- The handcrafted lexico-syntactic patterns for question classification require manual effort and may not generalize to new question types.
- The system's dependence on UMLS Metathesaurus assumes comprehensive coverage of biomedical terminology, which may not hold for emerging concepts or specialized domains.

## Confidence

- **High Confidence:** The pipeline architecture (question classification → document retrieval → passage retrieval → answer extraction) is well-established in QA systems, and the integration of semantic resources (UMLS) for retrieval is supported by prior work in biomedical IR.
- **Medium Confidence:** The specific machine learning features and patterns for question classification, while plausible, are not fully detailed in the available information. The effectiveness of SentiWordNet for yes/no answer extraction in biomedical context is reasonable but unverified.
- **Low Confidence:** The exact performance metrics (89.40% accuracy, top rankings in BioASQ 2017) cannot be independently verified without access to the original evaluation scripts and datasets.

## Next Checks

1. **Question Classification Validation:** Test the SVM classifier on an independent biomedical question dataset to verify accuracy and robustness across diverse question types.
2. **Document Retrieval Relevance:** Conduct a manual relevance assessment of the top 50 retrieved documents for a sample of questions to measure precision and semantic coverage.
3. **Answer Extraction Quality:** Perform a human evaluation of generated yes/no and factoid answers against gold standards to assess factual correctness and clinical appropriateness.