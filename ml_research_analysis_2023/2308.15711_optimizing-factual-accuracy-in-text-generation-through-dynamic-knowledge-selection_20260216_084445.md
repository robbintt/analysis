---
ver: rpa2
title: Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection
arxiv_id: '2308.15711'
source_url: https://arxiv.org/abs/2308.15711
tags:
- text
- reference
- generation
- dkgen
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DKGen, a model designed to enhance the factual
  accuracy of text generation by employing dynamic knowledge selection. Instead of
  incorporating all available reference passages into a single-pass generator, DKGen
  adopts an iterative process that selects reference passages to support the generation
  of individual sentences.
---

# Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection

## Quick Facts
- arXiv ID: 2308.15711
- Source URL: https://arxiv.org/abs/2308.15711
- Reference count: 40
- DKGen outperforms all baseline models on factuality metrics

## Executive Summary
This paper introduces DKGen, a novel approach to improve the factual accuracy of text generation by employing dynamic knowledge selection. Unlike traditional retrieval-augmented generation methods that incorporate all retrieved passages, DKGen iteratively selects relevant reference passages for each sentence, reducing knowledge mix-up and improving factuality. The model also distills the relevance of passages into the decoder's attention distribution to further enhance the use of relevant knowledge. Experiments on the WebBrain dataset demonstrate that DKGen achieves superior performance on factuality metrics while maintaining comparable text quality.

## Method Summary
DKGen is an encoder-decoder model (BART or T5) that generates text iteratively at the sentence level. For each sentence, it re-ranks retrieved reference passages based on their relevance to the query and previously generated text, selecting only the top-m passages. The model uses a ranking loss to learn passage relevance and a knowledge distillation loss to align the decoder's attention with passage relevance. The generated sentences are then combined to produce the final output. DKGen is trained on the WebBrain dataset, with target texts split into sentences, and evaluated using BLEU, ROUGE, BARTScore, FactScore, and TripleScore metrics.

## Key Results
- DKGen outperforms all baseline models on FactScore and TripleScore metrics for factual accuracy
- Sentence-level iterative generation reduces sampling randomness and improves factual accuracy
- Relevance distillation effectively aligns decoder attention with passage importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic selection of reference passages per sentence reduces knowledge mix-up.
- Mechanism: DKGen re-ranks passages based on relevance to query and previously generated text, selecting only top-m passages for each sentence.
- Core assumption: Relevance scoring can accurately filter out irrelevant knowledge.
- Evidence anchors:
  - [abstract]: "the subset is dynamically selected from the full passage set based on their relevance to the previously generated text and the query, largely eliminating the irrelevant references from input"
  - [section 3.5]: "DKGen selects the subset Pi from the full set P by considering the the relevance Prob(p|T1:i−1, q) of each passage p ∈ P"
- Break condition: If relevance scoring is inaccurate or top-m passages still contain conflicting information, knowledge mix-up may persist.

### Mechanism 2
- Claim: Sentence-level generation reduces sampling randomness and improves factual accuracy.
- Mechanism: By generating short texts (sentences) iteratively rather than the full output in one pass, DKGen limits the length of each decoding step, reducing the accumulation of sampling randomness that can lead to factual errors.
- Core assumption: Shorter generation sequences are less prone to random errors than longer sequences, and the context from previously generated sentences is sufficient to guide the next sentence.
- Evidence anchors:
  - [abstract]: "as the length of the output text grows, the randomness of sampling can escalate, detrimentally impacting the factual accuracy of the generated text"
  - [section 3.2]: "shortening the target text during generation (e.g., sentence-level) to increase the determinacy of the sampling process"
- Break condition: If sentence-level coherence is not maintained, or if the context window is too short to capture necessary dependencies, factual accuracy may not improve.

### Mechanism 3
- Claim: Relevance distillation aligns decoder attention with the importance of reference passages.
- Mechanism: DKGen distills the relevance scores of reference passages into the cross-attention distribution of the decoder, so that passages with higher relevance scores receive more attention during generation.
- Core assumption: The attention distribution of the decoder can be effectively supervised by external relevance scores, and this supervision improves the model's ability to use relevant knowledge.
- Evidence anchors:
  - [abstract]: "DKGen distills the relevance order of reference passages to the cross-attention distribution of decoder"
  - [section 3.6]: "we design a relevance distillation mechanism to supervise the decoder’s attention distributions with the relevance scores of input passages"
- Break condition: If the relevance scores are noisy or if the distillation loss does not effectively guide attention, the model may still attend to irrelevant passages.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: DKGen builds on RAG by integrating retrieved passages into the generation process, but improves it with dynamic selection and distillation.
  - Quick check question: What is the difference between standard RAG and DKGen's approach to incorporating retrieved knowledge?

- Concept: Attention mechanisms in transformers
  - Why needed here: DKGen's relevance distillation relies on manipulating the cross-attention distribution of the decoder.
  - Quick check question: How does the cross-attention matrix in a transformer decoder relate to the relevance of input passages?

- Concept: Iterative text generation
  - Why needed here: DKGen breaks text generation into sentence-level iterations to reduce randomness and improve factuality.
  - Quick check question: Why might generating text sentence by sentence be more effective than generating it all at once?

## Architecture Onboarding

- Component map: Encoder-decoder model (BART/T5) -> Retriever -> Relevance scorer -> Selector -> Decoder with relevance distillation
- Critical path: Query -> Retriever -> Passage re-ranking -> Sentence generation (with selected passages) -> Repeat until full text generated
- Design tradeoffs: DKGen trades off some coherence (by generating sentence by sentence) for higher factual accuracy; it also trades computational efficiency in decoding for the overhead of iterative re-ranking.
- Failure signatures: Factual errors due to irrelevant passages, incoherence between sentences, or degraded semantic quality compared to single-pass models.
- First 3 experiments:
  1. Verify that dynamic selection improves over using all passages by comparing factuality metrics (FactScore, TripleScore).
  2. Test the impact of relevance distillation by ablating the Lkd loss and measuring changes in factuality and attention alignment.
  3. Measure decoding efficiency and compare latency with baseline models to confirm the claimed efficiency gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DKGen's iterative sentence generation approach compare to single-pass methods in terms of semantic coherence and overall text quality?
- Basis in paper: [inferred] The paper mentions that human evaluation indicates DKGen may lead to lower semantic coherence compared to producing the entire output in a single pass.
- Why unresolved: The paper does not provide a detailed comparison of semantic coherence between DKGen and single-pass methods, nor does it offer specific metrics or evaluation results for this aspect.
- What evidence would resolve it: A comprehensive evaluation comparing DKGen's output with single-pass methods in terms of semantic coherence, using both automated metrics and human evaluation, would provide insights into the trade-offs between factuality and coherence.

### Open Question 2
- Question: What is the optimal number of reference passages to select in each iteration of DKGen's dynamic knowledge selection process?
- Basis in paper: [explicit] The paper states that DKGen relies on heuristic strategies to select the number of reference passages, which may result in insufficient knowledge synthesis from diverse sources.
- Why unresolved: The paper does not provide a systematic analysis of the impact of different numbers of selected reference passages on the model's performance or offer a principled method for determining the optimal number.
- What evidence would resolve it: An empirical study comparing DKGen's performance with different numbers of selected reference passages in each iteration, along with an analysis of the trade-offs between knowledge coverage and computational efficiency, would help determine the optimal strategy.

### Open Question 3
- Question: How does the choice of retriever model affect DKGen's performance in terms of factuality and overall text quality?
- Basis in paper: [explicit] The paper mentions that the performance of retrievers has a direct impact on the final generation outcome and compares the impact of different retrievers, but the focus is not on developing improved retrievers.
- Why unresolved: While the paper compares the impact of different retrievers, it does not provide a detailed analysis of how the choice of retriever model affects DKGen's factuality and overall text quality, nor does it explore the potential benefits of using a retriever that takes into account both the query and the previously generated text.
- What evidence would resolve it: A comprehensive evaluation of DKGen's performance using various retriever models, including those that consider the previously generated text, would provide insights into the importance of the retriever choice and guide the development of more effective retrieval strategies for knowledge-enhanced text generation.

## Limitations

- Evaluation relies on automatic metrics which may not fully capture human judgments of factuality and coherence.
- The WebBrain dataset may have inherent biases affecting generalizability to other domains.
- Detailed implementation specifics for relevance scoring and dynamic selection mechanisms are not provided, impacting reproducibility.

## Confidence

**High Confidence:** The core claim that sentence-level iterative generation with dynamic reference selection improves factual accuracy is well-supported by the experimental results showing DKGen's superior performance on FactScore and TripleScore metrics compared to baselines.

**Medium Confidence:** The effectiveness of relevance distillation in aligning decoder attention with passage importance is demonstrated through ablation studies, but the exact mechanism and its impact on factuality could benefit from more detailed analysis and visualization.

**Low Confidence:** The claim that DKGen achieves higher efficiency than standard RAG models is not directly validated with empirical runtime or memory usage comparisons, and the trade-off between efficiency and accuracy is not thoroughly explored.

## Next Checks

1. Replicate the dynamic selection and relevance distillation mechanisms using the described training procedure and evaluate the resulting model on both factuality (FactScore, TripleScore) and text quality (BLEU, ROUGE, BARTScore) metrics to verify the claimed improvements.

2. Conduct a human evaluation to assess the factual accuracy and coherence of DKGen's outputs compared to baselines, supplementing the automatic metric-based evaluation and addressing potential limitations of metric-based evaluation.

3. Perform an efficiency analysis by measuring the runtime and memory usage of DKGen during both training and inference, comparing it to standard RAG models to validate the claimed efficiency gains and understand the trade-offs involved.