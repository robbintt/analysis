---
ver: rpa2
title: 'Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations'
arxiv_id: '2306.08658'
source_url: https://arxiv.org/abs/2306.08658
tags:
- uni00000013
- languages
- uni00000003
- uni00000008
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating vision-and-language
  (VL) models across many languages, as existing multilingual benchmarks are limited
  and costly to create. The authors introduce Babel-ImageNet, a massively multilingual
  dataset that provides translations of ImageNet labels into 92 languages using BabelNet,
  a multilingual semantic network, avoiding manual annotation and machine translation
  issues.
---

# Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations

## Quick Facts
- arXiv ID: 2306.08658
- Source URL: https://arxiv.org/abs/2306.08658
- Reference count: 40
- Primary result: Babel-ImageNet ZS-IC accuracy correlates at 0.82-0.89 with multilingual image-text retrieval R@1 across three datasets

## Executive Summary
This paper addresses the critical gap in evaluating vision-and-language models across many languages, as existing multilingual benchmarks are limited and costly to create. The authors introduce Babel-ImageNet, a massively multilingual dataset that provides translations of ImageNet labels into 92 languages using BabelNet, a multilingual semantic network, avoiding manual annotation and machine translation issues. They evaluate 11 public multilingual CLIP models on zero-shot image classification across these languages, showing a significant performance gap between English and other languages, especially low-resource ones. Crucially, they demonstrate that ZS-IC performance on Babel-ImageNet correlates highly with multilingual image-text retrieval performance, validating its use as a proxy benchmark. They also show that parameter-efficient language-specific fine-tuning can drastically improve performance for low-resource languages.

## Method Summary
The authors create Babel-ImageNet by linking ImageNet synsets to BabelNet synsets via WordNet, automatically obtaining high-quality translations for 92 languages without manual annotation. They evaluate 11 multilingual CLIP models on zero-shot image classification using these translated labels. To validate Babel-ImageNet as a proxy benchmark, they correlate ZS-IC performance with image-text retrieval performance across three multilingual datasets. For improving low-resource language performance, they implement parameter-efficient adapter-based fine-tuning on top of pre-trained models using synthetic multilingual captions.

## Key Results
- Multilingual CLIP models show dramatic performance gaps, with accuracy dropping from 79.8% in English to 60.6% in German and 41.4% in Swahili
- ZS-IC accuracy on Babel-ImageNet correlates highly with image-text retrieval performance (0.82-0.89 R values) across three datasets
- Adapter-based fine-tuning improves low-resource language performance by up to 10-15 percentage points
- The "curse of multilinguality" affects low-resource languages most severely in fixed-capacity models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Babel-ImageNet provides a robust multilingual evaluation benchmark without requiring manual translation or machine translation post-editing.
- Mechanism: The dataset uses WordNet synsets as a bridge between ImageNet classes and BabelNet synsets, enabling automatic extraction of high-quality translations across 92 languages.
- Core assumption: BabelNet reliably contains accurate translations for ImageNet concepts in the target languages, and WordNet synset mappings are accurate.
- Evidence anchors: [abstract] "We instead automatically obtain reliable translations by linking them -- via shared WordNet synsets -- to BabelNet, a massively multilingual lexico-semantic network." [section] "WordNet as a matchmaker for ImageNet and BabelNet. Unlike in most image classification datasets... ImageNet [10] links images to concepts, represented with sets of synonyms (synsets) from English WordNet [38]."

### Mechanism 2
- Claim: Babel-ImageNet ZS-IC performance correlates highly with multilingual image-text retrieval performance, validating it as a proxy benchmark.
- Mechanism: Models that perform well on Babel-ImageNet for a given language also perform well on image-text retrieval tasks for that language, indicating shared underlying representation quality.
- Core assumption: ZS-IC and image-text retrieval tasks measure similar aspects of multilingual VL model quality.
- Evidence anchors: [abstract] "crucially, we show that the models' ZS-IC performance highly correlates with their performance in image-text retrieval, validating the use of Babel-ImageNet to evaluate multilingual models." [section] "Figure 3 displays the retrieval results on xFlickrCo, XM3600, and XTD, respectively, against the ZS-IC accuracy on Babel-ImageNet: each dot represents one model-language combination. The plots reveal high correlation between the Babel-ImageNet and text-to-image retrieval scores across model-language pairs: 0.82 for xFlickrCo, 0.87 for XM3600, and 0.85 for XTD."

### Mechanism 3
- Claim: Parameter-efficient language-specific fine-tuning with adapters drastically improves performance for low-resource languages.
- Mechanism: Adapter layers inserted into the text encoder allow for language-specific adaptation while keeping the base CLIP model frozen, providing computational efficiency.
- Core assumption: Language-specific adaptation is more effective than general multilingual training for low-resource languages, and adapter-based training is computationally efficient.
- Evidence anchors: [abstract] "Finally, we show that the performance of multilingual CLIP for low-resource languages can be drastically improved via cheap, parameter-efficient language-specific training." [section] "Setup. We train language-specific adapters on top of (a) OpenClip B-32 model... and (b) M-CLIP XLMR-L B-32... We experiment with three training objectives... We perform adapter-based specialization for 16 languages."

## Foundational Learning

- Concept: Contrastive learning and representation learning
  - Why needed here: CLIP models are trained using contrastive learning to align image and text representations in a shared space, which is fundamental to understanding their zero-shot capabilities.
  - Quick check question: What is the difference between contrastive learning and supervised learning, and why is contrastive learning particularly suited for vision-language tasks?

- Concept: Multilingual text representation and cross-lingual transfer
  - Why needed here: Understanding how multilingual text encoders like XLM-R handle different languages and how knowledge transfers across languages is crucial for interpreting the performance gaps observed in the paper.
  - Quick check question: What are the main challenges in training multilingual text representations, and how do they affect performance on low-resource languages?

- Concept: WordNet and BabelNet structure and organization
  - Why needed here: Understanding how WordNet synsets and BabelNet synsets are structured and linked is essential for grasping how Babel-ImageNet translations are obtained.
  - Quick check question: How do WordNet and BabelNet organize lexical information, and what are the key differences between their approaches to multilingual representation?

## Architecture Onboarding

- Component map: Babel-ImageNet -> CLIP models -> Adapters -> Evaluation metrics
- Critical path: Obtain ImageNet class translations from BabelNet using WordNet synsets -> Evaluate CLIP models on ZS-IC using Babel-ImageNet translations -> Validate Babel-ImageNet as a proxy benchmark by correlating with image-text retrieval performance -> Improve low-resource language performance using adapter-based fine-tuning
- Design tradeoffs:
  - Manual vs. automatic translation: Manual translation ensures high quality but is expensive and time-consuming; automatic translation is faster but may contain errors
  - Full vs. partial translation: Full translation provides more comprehensive evaluation but may be more challenging to obtain; partial translation is easier but may miss important concepts
  - Parameter-efficient vs. full fine-tuning: Parameter-efficient fine-tuning is computationally cheaper but may not achieve the same performance gains as full fine-tuning
- Failure signatures:
  - Poor correlation between ZS-IC and image-text retrieval performance: Indicates that Babel-ImageNet may not be a reliable proxy benchmark
  - Limited improvement from adapter-based fine-tuning: Suggests that language-specific adaptation may not be effective for low-resource languages
  - Significant performance gaps between languages: Indicates that multilingual CLIP models may not generalize well across languages
- First 3 experiments:
  1. Evaluate a publicly available CLIP model on Babel-ImageNet for a high-resource language (e.g., German) and compare performance to English
  2. Validate the correlation between Babel-ImageNet ZS-IC performance and image-text retrieval performance for a subset of languages
  3. Implement adapter-based fine-tuning for a low-resource language (e.g., Sinhala) and measure performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the strong correlation between Babel-ImageNet ZS-IC performance and multilingual image-text retrieval R@1 scores generalize to other VL tasks beyond retrieval, such as visual question answering or visual reasoning?
- Basis in paper: [explicit] The paper validates Babel-ImageNet by showing high correlation (0.82-0.89) between ZS-IC performance and text-to-image retrieval R@1 across three datasets (xFlickrCo, XM3600, XTD), concluding it is a "sensible benchmark for comparing proficiency of VL models for a multitude of languages not covered in task-specific benchmarks."
- Why unresolved: The correlation analysis is limited to image-text retrieval tasks. While the authors argue this validates Babel-ImageNet as a proxy for multilingual VL quality, they do not empirically test whether ZS-IC performance on Babel-ImageNet correlates with performance on other VL tasks like visual QA or reasoning, which require more complex cross-modal understanding.
- What evidence would resolve it: Empirical results showing correlation (or lack thereof) between Babel-ImageNet ZS-IC scores and VL model performance on visual QA, visual reasoning, or other complex multilingual VL tasks across the same language set.

### Open Question 2
- Question: Is the "curse of multilinguality" primarily driven by the XLM-R text encoder's capacity constraints, or are there other architectural or training factors in multilingual CLIP models that contribute to degraded performance for low-resource languages?
- Basis in paper: [explicit] The paper observes that "multilingual CLIP models perform dramatically worse for high-resource languages than for English, and for low- and mid-resource languages than for high-resource languages" and notes that "under a fixed model capacity, an improvement of representation quality for some language(s) comes at the expense of representational deterioration for others," citing prior work on XLM-R [8, 46].
- Why unresolved: While the paper attributes the performance degradation to the "curse of multilinguality" and references XLM-R's limitations, it does not conduct controlled experiments to isolate whether this is solely due to text encoder capacity, or if factors like training objectives, data distribution, or vision encoder interactions also play significant roles.
- What evidence would resolve it: Controlled experiments varying text encoder capacity, training objectives, and data distributions while keeping other factors constant to measure their individual impact on low-resource language performance.

### Open Question 3
- Question: How robust are Babel-ImageNet translations to polysemy and contextual meaning shifts, and what is the actual error rate in practice beyond the authors' estimated "less than 1%"?
- Basis in paper: [explicit] The paper acknowledges that "automatic translation of concepts (i.e., words and short phrases) out of context, on the other hand, is problematic due to polysemy" and estimates "less than 1% of labels are incorrect" based on "manual inspection of class labels for a handful of languages that the authors speak."
- Why unresolved: The authors' error rate estimate is based on a limited manual inspection of a few languages they personally speak, which may not be representative of the 92 languages in the benchmark. The actual impact of polysemy errors on downstream model evaluation is not quantified.
- What evidence would resolve it: Large-scale human evaluation of Babel-ImageNet translations across multiple languages by native speakers, measuring precision and recall against ground-truth translations, and analysis of how translation errors affect model ranking and performance trends.

## Limitations

- Babel-ImageNet relies on the accuracy and coverage of BabelNet's multilingual synset mappings, which are not directly validated in the paper
- The high correlation between ZS-IC and image-text retrieval performance could be inflated if the retrieval datasets themselves use similar translation sources
- Adapter-based fine-tuning experiments are evaluated on synthetic captions from BLIP rather than human-annotated multilingual data, raising questions about real-world generalization

## Confidence

- **High confidence**: The existence of significant performance gaps between English and other languages in multilingual CLIP models
- **Medium confidence**: Babel-ImageNet as a reliable proxy benchmark for multilingual VL model evaluation
- **Medium confidence**: Parameter-efficient adapter-based fine-tuning as an effective solution for low-resource languages

## Next Checks

1. **BabelNet accuracy validation**: Manually validate a random sample (e.g., 100 ImageNet classes) of Babel-ImageNet translations across 10-15 languages, particularly for low-resource languages, to quantify translation quality and identify systematic errors or cultural/semantic mismatches.

2. **Correlation robustness check**: Test the correlation between Babel-ImageNet ZS-IC performance and image-text retrieval across additional multilingual VL benchmarks (e.g., M3P, UC2) and with models trained on different datasets to verify that the correlation holds beyond the specific models and datasets used in the paper.

3. **Real-world generalization of adapter fine-tuning**: Evaluate adapter-based fine-tuned models on human-annotated multilingual image captioning datasets (e.g., Multi30K, MSCOCO in other languages) to assess whether improvements on synthetic data transfer to real-world multilingual image-text understanding tasks.