---
ver: rpa2
title: An open-source deep learning algorithm for efficient and fully-automatic analysis
  of the choroid in optical coherence tomography
arxiv_id: '2307.00904'
source_url: https://arxiv.org/abs/2307.00904
tags:
- gpet
- deepgpet
- segmentation
- choroid
- choroidal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DeepGPET is a fully-automatic, open-source deep learning algorithm\
  \ for choroidal segmentation in OCT data, achieving excellent agreement with the\
  \ clinically validated semi-automatic GPET method (AUC=0.994, Dice=0.966; Pearson\
  \ correlation of 0.891 for choroidal thickness and 0.908 for choroidal area). It\
  \ reduces processing time per image from 34.49s (\xB115.09) using GPET to 1.25s\
  \ (\xB10.10) on a standard laptop CPU, eliminating the need for manual interventions."
---

# An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography

## Quick Facts
- arXiv ID: 2307.00904
- Source URL: https://arxiv.org/abs/2307.00904
- Reference count: 32
- DeepGPET achieves AUC=0.994, Dice=0.966 for choroidal segmentation, reducing processing time from 34.49s to 1.25s per image

## Executive Summary
This paper presents DeepGPET, a fully-automatic deep learning algorithm for choroidal segmentation in optical coherence tomography (OCT) images. DeepGPET distills the semi-automatic Gaussian Process Edge Tracing (GPET) method into an efficient, open-source solution that eliminates the need for manual interventions while maintaining high accuracy. The algorithm achieves excellent agreement with GPET (AUC=0.994, Dice=0.966) and significantly reduces processing time from 34.49 seconds to 1.25 seconds per image on a standard laptop CPU.

## Method Summary
DeepGPET uses a UNet architecture with a MobileNetV3 backbone pre-trained on ImageNet. The model was fine-tuned on 603 OCT B-scans from 82 subjects using GPET ground truth segmentations, with data augmentation including brightness/contrast changes, flipping, speckle noise, blur, and affine transforms. Images were cropped to remove black space and resized to 544×768 pixels, then standardized. The model was trained for 60 epochs using AdamW optimizer with exponential moving average of weights applied after epoch 30.

## Key Results
- DeepGPET achieves excellent segmentation agreement with GPET (AUC=0.994, Dice=0.966)
- Choroidal thickness and area measurements show strong correlation (Pearson correlation 0.891 and 0.908 respectively)
- Processing time reduced from 34.49s (±15.09) to 1.25s (±0.10) per image
- Qualitative assessment by clinical ophthalmologist shows comparable boundary smoothness and accuracy to GPET

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning distillation effectively replaces manual intervention in choroid segmentation
- Mechanism: By finetuning a pre-trained UNet with MobileNetV3 backbone on GPET ground truth data, DeepGPET learns the same boundary detection patterns but operates automatically
- Core assumption: GPET ground truth segmentations contain sufficient information to train a generalizable deep learning model
- Evidence anchors: [abstract]: "We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet"; [section]: "We aim to distil GPET into a fully-automatic method, DeepGPET, for choroidal segmentation"
- Break condition: If GPET segmentations contain systematic errors or bias, the deep learning model will inherit and amplify these flaws

### Mechanism 2
- Claim: Pre-trained models accelerate convergence and improve performance
- Mechanism: Using MobileNetV3 pre-trained on ImageNet provides feature extraction capabilities that transfer to OCT image analysis
- Core assumption: Visual features learned from natural images transfer to medical imaging tasks
- Evidence anchors: [abstract]: "We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet"; [section]: "We fine-tune a UNet [23] with MobileNetV3 [13] backbone pre-trained on ImageNet"
- Break condition: If OCT image features are too domain-specific, pre-training on natural images provides minimal benefit

### Mechanism 3
- Claim: Data augmentation improves model robustness to OCT image variability
- Mechanism: Applying brightness/contrast changes, flipping, speckle noise, blur, and affine transforms creates diverse training samples
- Core assumption: The augmented variations reflect real-world OCT image variations
- Evidence anchors: [abstract]: "We use the following data augmentations: brightness and contrast changes, horizontal flipping, and simulated OCT speckle noise"; [section]: "To reduce memory-load, we crop the black space above and below the OCT B-scan and process images at a resolution of 544 × 768 pixels"
- Break condition: If augmentations create unrealistic variations that don't match test data, performance may degrade

## Foundational Learning

- Concept: Gaussian Process Edge Tracing (GPET)
  - Why needed here: Understanding GPET is crucial as DeepGPET distills its methodology
  - Quick check question: What makes GPET semi-automatic and how does it trace choroidal boundaries?

- Concept: OCT image characteristics
  - Why needed here: OCT images have specific properties like hyperreflective layers and noise patterns
  - Quick check question: Why is choroidal segmentation more challenging than retinal layer segmentation in OCT?

- Concept: Deep learning for medical image segmentation
  - Why needed here: The core methodology relies on UNet architecture and transfer learning
  - Quick check question: How does UNet architecture handle multi-scale feature extraction for segmentation tasks?

## Architecture Onboarding

- Component map: Input B-scans → Cropping and resizing (544×768) → Standardization → Augmentation pipeline → UNet with MobileNetV3 → Binary segmentation mask

- Critical path: Input → Augmentation → UNet forward pass → Dice loss computation → Weight update → EMA weight update

- Design tradeoffs:
  - Resolution reduction (544×768) vs. computational efficiency
  - Using pre-trained backbone vs. training from scratch
  - Automatic processing vs. potential loss of expert fine-tuning capabilities

- Failure signatures:
  - Poor boundary detection in regions with stroma fluid obscuration
  - Systematic under-segmentation in low contrast regions
  - Over-segmentation of large suprachoroidal spaces

- First 3 experiments:
  1. Baseline test: Run DeepGPET on test set and compare Dice coefficient with GPET
  2. Ablation test: Remove data augmentation and measure performance degradation
  3. Transfer test: Replace MobileNetV3 with random initialization and measure convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is DeepGPET to OCT images from patients with ocular pathologies?
- Basis in paper: [inferred] The paper states "we only used data from subjects that were either healthy or had systemic but not eye disease, to which DeepGPET might not be robust to."
- Why unresolved: The current model was not trained or tested on images containing ocular pathologies, which could present different segmentation challenges.
- What evidence would resolve it: External validation on datasets containing OCT images from patients with various ocular pathologies would demonstrate the model's robustness to pathological cases.

### Open Question 2
- Question: What is the impact of using "gold standard" choroidal segmentations for training DeepGPET?
- Basis in paper: [explicit] The paper mentions "GPET does not always segment the whole width of the choroid" and suggests "revisiting some of the existing segmentations and manually improving them to a 'gold standard' for purposes of training the model could improve DeepGPET."
- Why unresolved: The current model was trained on segmentations produced by GPET, which the paper acknowledges are not perfect. Improving these segmentations to a gold standard could potentially enhance model performance.
- What evidence would resolve it: Training and evaluating DeepGPET on a dataset with manually curated gold standard segmentations would show if this improves performance compared to the current GPET-based training.

### Open Question 3
- Question: Can DeepGPET be extended to automatically identify the fovea location and segment choroidal vessels?
- Basis in paper: [explicit] The paper states "we plan to extend DeepGPET to output the fovea location" and "we aim to explore whether DeepGPET can automatically segment the vasculature within the choroid as well."
- Why unresolved: While DeepGPET currently performs choroid segmentation, additional functionality for fovea identification and vessel segmentation would make it a more comprehensive analysis tool.
- What evidence would resolve it: Developing and evaluating extended versions of DeepGPET that include fovea detection and choroidal vessel segmentation would demonstrate the feasibility and performance of these additional capabilities.

## Limitations

- Dependency on GPET ground truth data means DeepGPET may inherit and amplify systematic errors from the semi-automatic method
- Model generalizability to different OCT devices and acquisition protocols remains untested beyond two Heidelberg SPECTRALIS systems
- Qualitative assessment by a single clinical ophthalmologist lacks inter-rater reliability measures

## Confidence

- High confidence: Processing speed improvement claims (concrete measurements provided)
- Medium confidence: Segmentation performance metrics (AUC=0.994, Dice=0.966) based on GPET ground truth
- Medium confidence: Transferability of results to other OCT devices and protocols

## Next Checks

1. Cross-device validation: Test DeepGPET on OCT images from different manufacturers to assess generalizability
2. Expert consensus validation: Obtain qualitative assessments from multiple clinical experts to establish inter-rater reliability
3. Error analysis: Conduct systematic analysis of segmentation failures, particularly in regions with stroma fluid obscuration and large suprachoroidal spaces