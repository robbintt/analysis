---
ver: rpa2
title: Explaining black box text modules in natural language with language models
arxiv_id: '2305.09863'
source_url: https://arxiv.org/abs/2305.09863
tags:
- explanation
- explanations
- language
- sasc
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SASC, a method that automatically generates
  natural language explanations for black box text modules by summarizing ngrams and
  scoring candidates using synthetic data. It successfully recovers ground truth explanations
  for synthetic modules and produces explanations for BERT transformer factors and
  fMRI voxel modules, with fMRI explanations highlighting social concepts.
---

# Explaining black box text modules in natural language with language models

## Quick Facts
- arXiv ID: 2305.09863
- Source URL: https://arxiv.org/abs/2305.09863
- Reference count: 40
- This paper introduces SASC, a method that automatically generates natural language explanations for black box text modules by summarizing ngrams and scoring candidates using synthetic data.

## Executive Summary
This paper presents SASC (Summarize and Score), a novel method for automatically generating natural language explanations for black box text modules without requiring access to their internal structure. SASC works by summarizing high-response ngrams using a helper LLM and then evaluating candidate explanations through synthetic data scoring. The method is evaluated on synthetic modules (where it successfully recovers ground truth explanations), BERT transformer factors, and fMRI voxel modules, demonstrating its potential for interpretability in both AI models and neuroscience.

## Method Summary
SASC is a two-step pipeline that generates natural language explanations for text modules (functions mapping text to scalar continuous values). First, it extracts ngrams from a reference corpus, feeds them through the module, and uses a helper LLM to summarize the top ngrams into candidate explanations. Second, it generates synthetic text based on each candidate explanation and computes an explanation score by comparing the module's response to related versus unrelated synthetic text. The highest-scoring explanation is selected as the final output. The method requires only black-box access to the module and no human intervention.

## Key Results
- SASC successfully recovers ground truth explanations for synthetic modules in many cases
- Explanations for BERT transformer factors are often comparable in quality to human-given explanations
- fMRI voxel explanations highlight social concepts and align with downstream task relevance
- Explanation performance increases with the capabilities of the helper LLM used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SASC works because it leverages a pre-trained LLM to bridge the gap between module outputs and natural language explanations through two complementary steps: summarization of high-response ngrams and synthetic data scoring.
- Mechanism: First, the module's responses to ngrams are sorted, and the top ngrams are summarized by the helper LLM into candidate explanations. Then, synthetic text is generated based on each candidate explanation, and the module's responses to this synthetic text are scored. The explanation with the highest score is selected as the final output.
- Core assumption: The helper LLM can accurately summarize ngrams into concise natural language explanations and generate synthetic text that is related to the candidate explanation.
- Evidence anchors:
  - [abstract]: "SASC produces natural language explanations for text modules... SASC requires only black-box access to the module (it does not require access to the module internals) and no human intervention."
  - [section]: "The first step generates candidate explanations by summarizing ngrams... This step is similar to prior works which summarize ngrams using manual inspection/parse trees [10, 11], but the use of the helper LLM enables flexible, automated summarization."
- Break condition: If the helper LLM cannot accurately summarize ngrams or generate synthetic text, the SASC method will fail to produce reliable explanations.

### Mechanism 2
- Claim: SASC works because it uses the synthetic data scoring step to evaluate the reliability of candidate explanations.
- Mechanism: Synthetic text is generated based on each candidate explanation, and the module's responses to this synthetic text are scored. The explanation with the highest score is selected as the final output.
- Core assumption: The module's response to synthetic text generated from an accurate explanation will be higher than its response to unrelated synthetic text.
- Evidence anchors:
  - [abstract]: "We compute the explanation score as follows: Explanation score = E[f (Text+)−f (Text−)] with unitsσf"
  - [section]: "SASC evaluates each candidate explanation by generating synthetic text based on the explanation (again with a helper LLM) and testing the response of f to the text."
- Break condition: If the module's response to synthetic text is not correlated with the accuracy of the candidate explanation, the synthetic data scoring step will not be able to select the most reliable explanation.

### Mechanism 3
- Claim: SASC works because it can recover ground truth explanations for synthetic modules and produce explanations for BERT transformer factors and fMRI voxel modules.
- Mechanism: SASC is evaluated on synthetic modules, BERT transformer factors, and fMRI voxel modules. The recovered explanations are compared to ground truth explanations or human-given explanations.
- Core assumption: The explanations generated by SASC are accurate and reliable.
- Evidence anchors:
  - [abstract]: "We evaluate SASC in 3 contexts... First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations."
  - [section]: "We use SASC to explain modules found within a pre-trained BERT model after applying dictionary learning... we find that SASC explanations are often of comparable quality to human-given explanations."
- Break condition: If the explanations generated by SASC are not accurate or reliable, the method will not be useful for interpreting black box text modules.

## Foundational Learning

- Concept: Language models and their applications
  - Why needed here: SASC relies on pre-trained LLMs for summarization and synthetic data generation.
  - Quick check question: What are some common applications of language models, and how do they differ from traditional rule-based systems?

- Concept: Module interpretation and explainability
  - Why needed here: SASC is a method for interpreting and explaining black box text modules.
  - Quick check question: What are some challenges in interpreting and explaining complex models, and how can SASC address these challenges?

- Concept: Synthetic data generation and evaluation
  - Why needed here: SASC uses synthetic data to evaluate the reliability of candidate explanations.
  - Quick check question: What are some benefits and limitations of using synthetic data for model evaluation, and how can synthetic data be generated effectively?

## Architecture Onboarding

- Component map: Text module -> Ngram extraction and summarization -> Candidate explanations -> Synthetic data generation and scoring -> Selected explanation and score

- Critical path: Input text module → Summarization of ngrams → Candidate explanations → Synthetic data generation and scoring → Selected explanation and score

- Design tradeoffs:
  - Using a larger corpus for ngram extraction can make SASC more accurate, but it also increases computational cost.
  - Using a more capable helper LLM for summarization and synthetic data generation can improve the quality of explanations, but it also increases computational cost.

- Failure signatures:
  - If the helper LLM cannot accurately summarize ngrams or generate synthetic text, SASC will fail to produce reliable explanations.
  - If the module's response to synthetic text is not correlated with the accuracy of the candidate explanation, SASC will not be able to select the most reliable explanation.

- First 3 experiments:
  1. Evaluate SASC on a synthetic module with a known ground truth explanation.
  2. Compare SASC explanations to human-given explanations for BERT transformer factors.
  3. Generate explanations for fMRI voxel modules and evaluate their accuracy and reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reliability of SASC explanations vary across different types of text modules (e.g., neurons, experts in MOE, attention heads) within large language models?
- Basis in paper: [inferred] The paper mentions that SASC could potentially enable better mechanistic interpretability for LLMs by analyzing submodules, but does not provide empirical evidence for different module types.
- Why unresolved: The paper primarily focuses on transformer factors in BERT and does not explore other module types within LLMs.
- What evidence would resolve it: Empirical results comparing SASC explanations across different module types in various LLMs, showing variation in reliability and effectiveness.

### Open Question 2
- Question: Can SASC be extended to explain the full behavior of a module, rather than just its top responses?
- Basis in paper: [explicit] The paper states that "SASC only describes the inputs that elicit the largest responses from f, rather than its full behavior" and suggests this as a limitation.
- Why unresolved: The current implementation of SASC is designed to summarize top-activating ngrams and select the best explanation based on synthetic data scoring.
- What evidence would resolve it: Modified SASC algorithm that incorporates more comprehensive sampling of ngrams or uses additional techniques to capture the full range of module behavior, validated against known module behaviors.

### Open Question 3
- Question: How does the choice of helper LLM affect the quality and reliability of SASC explanations?
- Basis in paper: [explicit] The paper mentions that "explanation performance increases with the capabilities of the helper LLM used for summarization/generation" and shows this in Fig. A1.
- Why unresolved: While the paper demonstrates that more capable LLMs lead to better explanations, it does not explore the specific impact of different helper LLM architectures, sizes, or training objectives.
- What evidence would resolve it: Systematic comparison of SASC explanations generated using different helper LLMs, varying in architecture, size, and training data, with quantitative metrics for explanation quality and reliability.

### Open Question 4
- Question: Can SASC explanations be used to identify and correct biases or unwanted behaviors in large language models?
- Basis in paper: [explicit] The paper mentions that "Trustworthy explanations could help audit increasingly powerful LLMs for undesired behavior or improve the distillation of smaller task-specific modules."
- Why unresolved: While the paper suggests this potential application, it does not provide concrete examples or experiments demonstrating how SASC explanations could be used for bias detection or correction.
- What evidence would resolve it: Case studies where SASC explanations are used to identify specific biases or unwanted behaviors in LLMs, followed by successful attempts to mitigate these issues through model adjustments or fine-tuning.

### Open Question 5
- Question: How do SASC explanations for fMRI voxel modules compare to those generated by other neuroimaging analysis techniques?
- Basis in paper: [explicit] The paper states that "SASC could also be used to generate explanations in a variety of domains, such as analysis of text models in computational social science or in medicine."
- Why unresolved: The paper focuses on comparing SASC explanations to human-given explanations for BERT modules and does not provide a comparison with other neuroimaging analysis methods for fMRI data.
- What evidence would resolve it: Comparative study of SASC explanations for fMRI voxel modules against explanations generated by other established neuroimaging techniques, such as representational similarity analysis or decoding methods, using standardized datasets and evaluation metrics.

## Limitations
- The method relies heavily on helper LLM performance, which introduces variability based on the specific LLM used and prompt engineering quality
- The explanation scores are computed relative to the module's own response scale, making cross-module comparisons potentially problematic
- The approach assumes that high-response ngrams from a reference corpus are meaningful, but this may not hold for all module types or domains

## Confidence
- **High confidence**: The core SASC pipeline (summarization + scoring) is clearly specified and reproducible
- **Medium confidence**: The method's effectiveness on BERT factors and fMRI modules, based on limited qualitative examples
- **Medium confidence**: The synthetic module recovery results, given the relatively small number of test cases

## Next Checks
1. Conduct ablation studies removing the synthetic data scoring step to quantify its contribution to explanation quality
2. Test SASC on modules from different architectures (e.g., GPT-style models) to assess generalizability
3. Perform human evaluation studies comparing SASC explanations to expert explanations across multiple domains and module types