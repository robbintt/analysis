---
ver: rpa2
title: On-Manifold Projected Gradient Descent
arxiv_id: '2308.12279'
source_url: https://arxiv.org/abs/2308.12279
tags:
- data
- manifold
- which
- vector
- nystr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for projecting adversarial examples
  onto the manifold of the correct class in input space, allowing for more realistic
  and semantically meaningful adversarial examples. The approach uses Conformally
  Invariant Diffusion Maps (CIDM) to approximate class manifolds, and the Spectral
  Exterior Calculus (SEC) to determine tangent vectors on the manifold.
---

# On-Manifold Projected Gradient Descent

## Quick Facts
- **arXiv ID**: 2308.12279
- **Source URL**: https://arxiv.org/abs/2308.12279
- **Reference count**: 40
- **Primary result**: Method projects adversarial examples onto class manifolds using CIDM and SEC, generating realistic, semantically meaningful attacks on VGG11 classifier.

## Executive Summary
This paper introduces a method for generating adversarial examples that lie on the manifold of the correct class in input space. The approach uses Conformally Invariant Diffusion Maps (CIDM) to approximate class manifolds, Spectral Exterior Calculus (SEC) to determine tangent vectors, and a novel Nyström projection to map points onto the manifold. The method is demonstrated on a synthetic vehicle dataset with VGG11 classifier, showing adversarial examples that are both effective and explainable in terms of human-understandable manipulations.

## Method Summary
The method projects adversarial examples onto the manifold of the correct class using three key components: CIDM approximates class manifolds in diffusion coordinates by computing a kernel matrix based on local density estimates; SEC identifies vector fields spanning tangent spaces by minimizing Dirichlet energy relative to the Riemannian metric; and Nyström projection maps off-manifold points to the nearest point on the manifold by extending coordinate functions via Nyström extension. The on-manifold PGD algorithm computes gradients, projects them onto tangent spaces, and uses Nyström projection to maintain manifold constraints during optimization.

## Key Results
- Successfully generates adversarial examples that fool VGG11 classifier while remaining on correct class manifolds
- Examples are semantically meaningful and explainable in terms of azimuth and down look angle manipulations
- Nyström projection effectively maps off-manifold points to nearest manifold locations, even with noisy data

## Why This Works (Mechanism)

### Mechanism 1
- CIDM approximates Laplace-Beltrami operator on manifold using variable bandwidth kernel based on local density estimates
- Core assumption: Data lies on smooth manifold with smooth sampling density
- Evidence: CIDM Laplacian converges to Laplace-Beltrami operator for infinite data and vanishing bandwidth
- Break condition: Fails with highly non-smooth manifolds or irregular sampling density

### Mechanism 2
- Nyström projection maps points to manifold by extending coordinate functions via Nyström extension
- Core assumption: Nyström extension behaves like nearest-neighbor interpolation for points far from training data
- Evidence: Extension returns coordinates of nearest point on manifold
- Break condition: Fails with highly curved manifolds or sparse data

### Mechanism 3
- SEC identifies tangent vector fields by minimizing Dirichlet energy relative to Riemannian metric
- Core assumption: Manifold smoothness allows Laplace-Beltrami operator to capture geometry
- Evidence: Minimizing energy yields smoothest possible vector fields respecting manifold structure
- Break condition: Fails with manifold singularities or insufficient data density

## Foundational Learning

- **Manifold hypothesis**: High-dimensional data lies on low-dimensional manifolds; crucial for CIDM, Nyström projection, and SEC
  - Why needed: Entire approach relies on manifold structure assumption
  - Quick check: Why are linear methods like PCA problematic for nonlinear manifolds?

- **Spectral graph theory**: Graph Laplacians capture manifold geometry through eigenfunctions
  - Why needed: CIDM constructs Laplacian from kernel matrix
  - Quick check: How does Gaussian kernel choice affect graph Laplacian properties?

- **Differential geometry**: Riemannian metrics and Laplace-Beltrami operator encode manifold geometry
  - Why needed: SEC uses Riemannian metric to define Dirichlet energy
  - Quick check: What geometric information is recoverable from Laplace-Beltrami operator?

## Architecture Onboarding

- **Component map**: Data preprocessing -> CIDM module -> Nyström projection module -> SEC module -> Adversarial attack module
- **Critical path**: 1) Compute CIDM Laplacian and eigenfunctions, 2) Build Nyström projection, 3) Use SEC to identify tangent vectors, 4) Apply on-manifold PGD with gradient projection and Nyström mapping
- **Design tradeoffs**: CIDM bandwidth (resolution vs connectivity), number of eigenfunctions (accuracy vs noise robustness), tangent vector count (completeness vs simplicity)
- **Failure signatures**: Classifier correctly classifies all adversarial examples (check eigenfunctions/tangent vectors), Nyström projection fails (check kernel/density), tangent vectors not orthogonal (check SEC energy minimization)
- **First 3 experiments**: 1) Apply CIDM to synthetic circular data, verify eigenfunctions capture geometry, 2) Implement Nyström projection on synthetic data, verify nearest-point mapping, 3) Use SEC to identify tangent vectors on synthetic data, verify tangent space spanning

## Open Questions the Paper Calls Out

### Open Question 1
- How does on-manifold PGD perform on real-world datasets with non-continuous intrinsic parameters?
- Basis: Paper mentions need for additional work to transition tools to real-world datasets
- Why unresolved: Only demonstrated on synthetic dataset with continuous parameters
- Evidence needed: Testing on ImageNet/CIFAR with comparison to standard PGD methods

### Open Question 2
- What is optimal choice of intrinsic parameters for modeling dataset manifold?
- Basis: Uses azimuth/down look angle for synthetic vehicles, mentions impact on CIDM/SEC performance
- Why unresolved: No general approach for selecting intrinsic parameters for arbitrary datasets
- Evidence needed: Experiments with different parameters (pose, lighting, context) across datasets

### Open Question 3
- How does number of eigenfunctions affect manifold approximation quality and PGD performance?
- Basis: Mentions L controls resolution, smaller L can project through noise
- Why unresolved: No systematic analysis of L impact on approximation quality or attack performance
- Evidence needed: Experiments varying L, evaluating manifold quality and attack success/semantic meaningfulness

## Limitations
- Only validated on synthetic dataset, not real-world image data
- Computational complexity for high-dimensional data not discussed
- No comparison to other on-manifold adversarial attack methods
- Robustness to noise and outliers not evaluated

## Confidence
- **High confidence**: Theoretical foundations of CIDM and SEC are well-established
- **Medium confidence**: Implementation details and results plausible for synthetic dataset
- **Low confidence**: Generalizability to real-world data and comparison to other methods

## Next Checks
1. Replicate results on standard image dataset like CIFAR-10 or MNIST to verify performance and computational feasibility
2. Evaluate robustness by adding Gaussian noise to synthetic dataset and assessing impact on CIDM eigenfunctions and Nyström projections
3. Compare proposed method to other on-manifold adversarial attack approaches to evaluate relative performance