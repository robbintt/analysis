---
ver: rpa2
title: 'ShiftKD: Benchmarking Knowledge Distillation under Distribution Shift'
arxiv_id: '2312.16242'
source_url: https://arxiv.org/abs/2312.16242
tags:
- shift
- data
- knowledge
- distribution
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a systematic framework to benchmark Knowledge
  Distillation (KD) under distribution shifts, which is an important problem since
  real-world data often differs from training data. The framework covers over 30 KD
  methods from algorithmic, data-driven, and optimization perspectives on five benchmark
  datasets.
---

# ShiftKD: Benchmarking Knowledge Distillation under Distribution Shift

## Quick Facts
- arXiv ID: 2312.16242
- Source URL: https://arxiv.org/abs/2312.16242
- Reference count: 40
- Key outcome: KD improves worst-group accuracy but few methods consistently outperform vanilla KD in average accuracy under distribution shifts

## Executive Summary
This paper introduces ShiftKD, a systematic framework for benchmarking Knowledge Distillation (KD) methods under distribution shifts. The framework evaluates over 30 KD algorithms across five benchmark datasets, focusing on two types of distributional shifts: diversity (P(X) changes) and correlation (P(Y|X) changes). Extensive experiments reveal that while KD helps improve worst-group accuracy, it struggles to consistently outperform vanilla KD in average accuracy under distribution shifts. The study provides insights into the effectiveness of data augmentation, pruning, and optimization techniques for improving KD robustness.

## Method Summary
ShiftKD evaluates KD methods under distribution shift by implementing a unified framework covering algorithmic approaches (logits, feature, relation-based), data-driven techniques (augmentation, pruning), and optimization choices (SGD, Adam). The framework uses pre-trained teacher models (ResNet-50) and various student architectures (ResNet-18, ResNet-34, WRN, MobileNet) across five benchmark datasets. Experiments measure both average and worst-group accuracy to assess performance under shift conditions. The evaluation includes systematic ablation studies on data manipulation and optimization strategies to identify factors affecting KD robustness.

## Key Results
- KD methods improve worst-group accuracy but few consistently outperform vanilla KD in average accuracy under distribution shifts
- Data augmentation helps marginally, with random-based augmentation typically improving performance while generation-based augmentation performs better on correlation shift
- Data pruning makes sense for students under distribution shift and can outperform data augmentation techniques in correlation shift-dominated datasets

## Why This Works (Mechanism)

### Mechanism 1: Systematic benchmarking under distribution shift
- Claim: The framework reveals how KD methods perform under distribution shift by covering multiple shift types and algorithmic approaches.
- Mechanism: Evaluates over 30 KD methods across diversity and correlation shifts using consistent data, training, and evaluation settings.
- Core assumption: Distribution shift can be meaningfully quantified and separated into diversity (P(X) changes) and correlation (P(Y|X) changes) types.
- Evidence anchors:
  - [abstract] "We propose a unified and systematic framework to benchmark KD against two general distributional shifts: diversity and correlation shift."
  - [section 2.2] "We propose to characterize how features change in the downstream domain... diversity shift and correlation shift."
  - [corpus] Weak - no corpus entries directly address KD benchmarking under shift.
- Break condition: If the quantification of shift types (Fdiv, Fcor) fails to capture real-world domain differences, the benchmarking framework loses validity.

### Mechanism 2: Importance of teacher-student knowledge alignment under shift
- Claim: KD improves worst-group accuracy but struggles with average accuracy under distribution shift due to misalignment between teacher and student knowledge.
- Mechanism: Student learns from teacher representations, but distribution shift causes teacher's learned features to be suboptimal or biased, limiting student performance.
- Core assumption: The teacher model's knowledge, though imperfect, still provides better guidance than no KD under shift.
- Evidence anchors:
  - [abstract] "While KD helps improve worst-group accuracy, few methods consistently outperform vanilla KD in average accuracy under distribution shifts."
  - [section 4.1] "All KD algorithms can be effective for improving worst-group accuracy... but none of the KD methods consistently outperforms concerning the average accuracy."
  - [corpus] Weak - no corpus entries directly compare KD under shift.
- Break condition: If the teacher model is highly biased under shift, KD may harm student performance more than help.

### Mechanism 3: Data augmentation and pruning effectiveness varies by shift type
- Claim: Data manipulation techniques like augmentation and pruning can help KD under shift, but their effectiveness depends on shift type and data characteristics.
- Mechanism: Augmentation increases data diversity for diversity shifts; pruning removes low-quality samples to improve data quality for correlation shifts.
- Core assumption: Data quality and diversity are key bottlenecks for KD robustness under distribution shift.
- Evidence anchors:
  - [section 4.2] "Data augmentation helps marginally... random-based augmentation typically improves performance, while generation-based augmentation performs better on correlation shift."
  - [section 4.2] "Data pruning makes sense for student under distribution shift... pruning even can outperform data augmentation techniques in correlation shift-dominated datasets."
  - [corpus] Weak - no corpus entries discuss data manipulation for KD under shift.
- Break condition: If the manipulated data distribution diverges too far from ground truth, student learning degrades.

## Foundational Learning

- Concept: Distribution shift quantification
  - Why needed here: Understanding how to measure and categorize shifts is essential to design appropriate KD methods and benchmarks.
  - Quick check question: Can you compute Fdiv and Fcor for a given domain shift scenario?

- Concept: Knowledge distillation algorithms and knowledge types
  - Why needed here: Different KD methods transfer different types of knowledge (logits, features, relations), which behave differently under shift.
  - Quick check question: What is the difference between feature-based and relation-based KD?

- Concept: Data augmentation and pruning strategies
  - Why needed here: Choosing appropriate data manipulation techniques can mitigate the impact of distribution shift on KD.
  - Quick check question: When would you prefer pruning over augmentation for KD under shift?

## Architecture Onboarding

- Component map:
  Teacher model (ResNet-50) -> KD algorithm (logits/feature/relation-based) -> Data manipulation (augmentation/pruning/identity) -> Student model (ResNet-18/34, WRN, MobileNet) -> Optimizer (SGD/Adam) -> Evaluation (average/worst-group accuracy)

- Critical path:
  1. Load teacher and student models
  2. Apply data manipulation to training data
  3. Train student using KD algorithm
  4. Evaluate on test domains
  5. Record average and worst-group accuracy

- Design tradeoffs:
  - Teacher model size vs. robustness under shift
  - Complexity of KD algorithm vs. effectiveness under shift
  - Data augmentation vs. pruning based on shift type
  - Optimizer choice (SGD vs. Adam) under shift

- Failure signatures:
  - Student performance drops significantly under shift despite KD
  - Augmentation or pruning degrades student performance
  - Complex KD algorithms underperform vanilla KD under shift
  - Pre-training on ImageNet does not help student task

- First 3 experiments:
  1. Run vanilla KD with identity data manipulation on PACS dataset to establish baseline.
  2. Compare feature-based KD (AT) vs. relation-based KD (SP) on OfficeHome to see which works better under diversity shift.
  3. Apply random pruning vs. AutoAugment on CMNIST to test which data manipulation helps more under correlation shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do data augmentation techniques fail to improve knowledge distillation performance under distribution shift?
- Basis in paper: [explicit] The paper states "Data augmentation is effective for distribution shift and knowledge distillation separately, but not as effective when combined" and notes that generation-based augmentation like Mixup and CutMix performs poorly on datasets with diversity and correlation shift.
- Why unresolved: The paper provides some examples but does not establish clear conditions or metrics for predicting when augmentation will fail. The effectiveness seems dataset-dependent and not systematically characterized.
- What evidence would resolve it: Systematic ablation studies varying augmentation types, intensity, and dataset characteristics to identify failure patterns. Clear metrics correlating dataset shift properties with augmentation effectiveness.

### Open Question 2
- Question: How can knowledge distillation methods be designed to specifically address spurious correlation shifts in real-world data?
- Basis in paper: [explicit] The paper identifies that "vanilla KD still achieves good results, but the problem is that students receive bias in the teacher model while being taught" and notes that correlation shifts create "spurious correlations in the data."
- Why unresolved: While the paper identifies the problem of correlation shifts, it does not propose specific architectural or algorithmic solutions beyond suggesting environmental labels and high-quality data.
- What evidence would resolve it: Development and evaluation of distillation methods that explicitly detect and mitigate spurious correlations, such as methods incorporating causal inference or invariant feature learning during distillation.

### Open Question 3
- Question: What is the optimal balance between student model capacity and robustness to distribution shifts in knowledge distillation?
- Basis in paper: [explicit] The paper mentions "a trade-off between handling shifts and model capacity limits the student model" and shows that different student architectures (ResNet variants, MobileNet) have varying performance under distribution shifts.
- Why unresolved: The paper evaluates different architectures but does not provide a systematic framework for determining the optimal student size given specific shift characteristics or task requirements.
- What evidence would resolve it: Empirical studies mapping student model capacity (parameter count, architecture depth) against performance metrics under controlled distribution shift scenarios to identify capacity thresholds for robustness.

### Open Question 4
- Question: How can environmental labels be effectively incorporated into knowledge distillation to improve robustness to distribution shifts?
- Basis in paper: [explicit] The paper suggests "it is recommended to use environment labels to guide the student model's predictions" and notes this as a potential area for improvement.
- Why unresolved: The paper mentions this as a recommendation but does not explore how to implement it or evaluate its effectiveness compared to other approaches.
- What evidence would resolve it: Development and comparison of distillation methods that incorporate environment labels (domain adaptation techniques, multi-task learning, etc.) against state-of-the-art methods on benchmark datasets with known environmental structure.

## Limitations
- The framework's validity depends on the assumption that diversity and correlation shifts can be cleanly separated and quantified through Fdiv and Fcor metrics
- The analysis focuses primarily on computer vision datasets, raising questions about generalizability to other domains like NLP or tabular data
- Limited exploration of hyperparameter tuning and ensemble approaches that might change conclusions about KD performance

## Confidence

- **High**: The observation that KD improves worst-group accuracy while struggling with average accuracy is well-supported by extensive experimental results across multiple datasets.
- **Medium**: The claim about data augmentation and pruning effectiveness varying by shift type is supported by experiments but lacks deeper theoretical grounding for why specific techniques work better for certain shifts.
- **Low**: The assertion that few methods consistently outperform vanilla KD requires more nuanced analysis, as the paper doesn't fully explore hyperparameter tuning or ensemble approaches that might change this conclusion.

## Next Checks

1. Validate the Fdiv and Fcor metrics on a held-out dataset with known shift characteristics to test their reliability and distinguishability.
2. Conduct ablation studies on the impact of teacher model architecture and pre-training strategy on KD robustness under distribution shift.
3. Test the framework's applicability on non-vision datasets (e.g., text classification) to assess cross-domain generalizability of the findings.