---
ver: rpa2
title: 'Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source
  dataset and dataset creation methodology using instruction-following models'
arxiv_id: '2311.16338'
source_url: https://arxiv.org/abs/2311.16338
tags:
- sentence
- question
- dataset
- answer
- required
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRaQAn, a novel open-source dataset for coreference
  resolution in question-answering tasks. The dataset was created using an automated
  methodology leveraging GPT-4 and a Recursive Criticism and Improvement Loop.
---

# Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source dataset and dataset creation methodology using instruction-following models

## Quick Facts
- arXiv ID: 2311.16338
- Source URL: https://arxiv.org/abs/2311.16338
- Authors: 
- Reference count: 40
- Created a novel open-source dataset for coreference resolution in question-answering tasks using GPT-4 and Recursive Criticism and Improvement Loop

## Executive Summary
This paper introduces CRaQAn, a novel open-source dataset for coreference resolution in question-answering tasks. The dataset was created using an automated methodology leveraging GPT-4 and a Recursive Criticism and Improvement Loop. The approach addresses the lack of publicly available datasets specifically designed to test chunking strategies for long documents in information retrieval systems. The resulting dataset contains over 250 high-quality question-answer pairs requiring coreference resolution across sentences. The automated generation achieved approximately 60.2% yield, with final dataset quality ensured through human review.

## Method Summary
The dataset creation methodology employs a Recursive Criticism and Improvement Loop (RCI) where a GENERATOR creates candidate question-answer pairs from split Wikipedia text sections, and four specialized REVIEWER personas (Content Cohesion, Information Accuracy, Linguistic Quality, Required Sentence) provide iterative feedback. The GENERATOR iterates up to five times based on reviewer feedback before human review serves as a final quality filter. The approach leverages GPT-4's instruction-following capabilities to automate the creation of coreference-dependent QA pairs while maintaining quality through structured feedback and human oversight.

## Key Results
- Created over 250 high-quality question-answer pairs requiring coreference resolution across sentences
- Achieved approximately 60.2% yield rate in automated generation process
- Demonstrated scalable approach for creating specialized NLP datasets using instruction-following models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive Criticism and Improvement Loop (RCI) enables automated dataset creation by iteratively refining question-answer pairs
- Mechanism: The GENERATOR creates candidates, REVIEWERS provide feedback based on predefined guidelines, and the GENERATOR iterates to improve quality until acceptance or rejection
- Core assumption: GPT-4 can interpret feedback and modify outputs to meet specific quality criteria
- Evidence anchors:
  - [abstract] "To develop this dataset, we developed a novel approach for creating high-quality datasets using an instruction-following model (GPT-4) and a Recursive Criticism and Improvement Loop."
  - [section] "The feedback from each REVIEWER is sent back to the GENERATOR for iterative improvement. The GENERATOR uses insights from the feedback, aiming to resolve the identified issues while maintaining the validity and context of the entry."
  - [corpus] Weak evidence - the corpus only shows related work on coreference resolution, not RCI methodology
- Break condition: If the RCI loop exceeds five iterations without consensus, the candidate is discarded

### Mechanism 2
- Claim: Human review serves as a final quality filter, catching errors that automated reviewers miss
- Mechanism: Human reviewers apply the same guidelines as automated reviewers but add contextual judgment to identify subtle issues like irrelevant sentences or ambiguous questions
- Core assumption: Human judgment remains superior for nuanced quality assessment despite automation
- Evidence anchors:
  - [abstract] "The resulting dataset contains over 250 high-quality question-answer pairs requiring coreference resolution across sentences. The automated generation achieved approximately 60.2% yield, with final dataset quality ensured through human review."
  - [section] "Human review remains an essential step in dataset generation. Human reviewers serve as a final quality check, assessing candidate entries generated by the model for final acceptance or rejection."
  - [corpus] Weak evidence - corpus shows related datasets but not human review processes
- Break condition: If human reviewers consistently reject candidates, it indicates guideline misalignment

### Mechanism 3
- Claim: Memetic proxy technique improves output quality by establishing reviewer and generator personas
- Mechanism: Reviewers are assigned specific roles (Content Cohesion, Information Accuracy, Linguistic Quality, Required Sentence) that align with dataset guidelines, creating specialized feedback
- Core assumption: Role-specific personas produce more consistent and targeted feedback than generic reviewers
- Evidence anchors:
  - [section] "Panel Formation To create a robust feedback system, we recommend distributing the rules among multiple REVIEWER personas, with each one specializing in a subset of rules."
  - [section] "The REVIEWER prompts are each specialized in different aspects of our dataset, with some overlap in the prompts themselves, including a: 1) Content Cohesion Reviewer, 2) Information Accuracy Reviewer, 3) Linguistic Quality Reviewer, and 4) Required Sentence Reviewer."
  - [corpus] No direct evidence in corpus about memetic proxy effectiveness
- Break condition: If personas overlap too much, feedback becomes redundant and inefficient

## Foundational Learning

- Concept: Coreference resolution in NLP
  - Why needed here: The dataset specifically tests chunking strategies that may split coreference sequences, requiring understanding of how pronouns and references link across sentences
  - Quick check question: What's the difference between pronominal, nominal, and anaphoric coreference?

- Concept: Recursive Criticism and Improvement (RCI) methodology
  - Why needed here: The automated dataset creation relies on iterative feedback loops between generator and reviewers
  - Quick check question: How does RCI differ from traditional supervised learning approaches?

- Concept: Instruction-following language models
  - Why needed here: GPT-4 serves as both the generator and the basis for reviewer prompts, requiring understanding of its capabilities and limitations
  - Quick check question: What are the key characteristics that make GPT-4 suitable for instruction-following tasks?

## Architecture Onboarding

- Component map: Generator → Reviewer Panel (4 specialized reviewers) → Human Review → Dataset
- Critical path: Generator creates candidate → Each reviewer evaluates → Feedback loop (max 5 iterations) → Human review → Acceptance/rejection
- Design tradeoffs: Automated generation provides scalability but requires human review for quality; specialized reviewers improve consistency but increase complexity
- Failure signatures: Low yield rate (<50%) indicates guideline misalignment; high human rejection rate suggests reviewer feedback is insufficient
- First 3 experiments:
  1. Test single reviewer vs. panel performance on same candidates to measure specialization benefits
  2. Vary temperature settings for generator and reviewers to find optimal balance between creativity and consistency
  3. Compare human review time vs. automated review time to quantify efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal chunk size for balancing coreference preservation and contextual meaning in information retrieval systems?
- Basis in paper: [explicit] The paper discusses the need to balance preservation of long-range coreferences and chunk size, where chunk size might be limited by embedding model architecture or diluted contextual meaning
- Why unresolved: The paper states the need for balance but does not provide specific optimal chunk sizes or methodologies to determine them
- What evidence would resolve it: Experimental results comparing different chunk sizes and their impact on coreference resolution accuracy and information retrieval performance

### Open Question 2
- Question: How does the CRaQAn dataset's performance compare to human performance in coreference resolution tasks?
- Basis in paper: [inferred] The paper mentions that GPT-4-generated questions and answers are only reflective of types of questions GPT-4 comprehends, which may not serve as an unbiased benchmark for comparison against human performance
- Why unresolved: The paper does not include direct comparisons between human and model performance on the CRaQAn dataset
- What evidence would resolve it: Benchmark results showing performance metrics (e.g., accuracy, F1 score) for both humans and models on the CRaQAn dataset

### Open Question 3
- Question: How does the CRaQAn dataset generalize to other languages beyond English?
- Basis in paper: [explicit] The dataset is created using English Wikipedia articles and GPT-4, which may limit its applicability to other languages
- Why unresolved: The paper does not discuss the dataset's applicability or performance in non-English contexts
- What evidence would resolve it: Results from applying the dataset creation methodology and evaluating the dataset in multiple languages, along with performance comparisons across languages

### Open Question 4
- Question: What is the long-term scalability and cost-effectiveness of the automated dataset generation approach presented in the paper?
- Basis in paper: [explicit] The paper mentions that the requirement for human review, crafting effective prompts, and generation costs are among the challenges that need to be addressed for scaling the method
- Why unresolved: The paper does not provide detailed cost analyses or scalability projections for the automated generation approach
- What evidence would resolve it: Detailed cost-benefit analyses comparing manual and automated generation approaches over large-scale implementations, including time and resource requirements

## Limitations
- The 60.2% yield rate suggests approximately 40% of generated candidates fail to meet quality standards, indicating limitations in the GENERATOR's ability to create valid question-answer pairs from the first attempt
- The methodology's effectiveness depends heavily on the quality and specificity of the prompt templates for both GENERATOR and REVIEWER components
- The reliance on GPT-4 for both generation and review introduces potential biases and inconsistencies that may not be fully captured by human review

## Confidence
**High Confidence**: The dataset creation methodology using Recursive Criticism and Improvement Loop is technically sound and well-documented. The separation of concerns between generation and specialized review roles follows established best practices in automated dataset creation.

**Medium Confidence**: The claim of achieving "high-quality" question-answer pairs is supported by the human review process, but the specific quality metrics and evaluation criteria are not fully detailed. The 250+ pair count is verifiable but the distribution of difficulty and coreference types is unclear.

**Low Confidence**: The scalability and generalizability of this approach to other NLP tasks beyond coreference resolution remains untested. The memetic proxy technique's effectiveness over simpler reviewer approaches is theoretical without comparative experiments.

## Next Checks
1. **Yield Rate Analysis**: Analyze the distribution of rejection reasons across the 40% of failed candidates to identify systematic issues in the GENERATOR prompt or guidelines

2. **Reviewer Specialization Impact**: Conduct controlled experiments comparing the current 4-reviewer panel approach against simpler configurations (single reviewer or fewer specialized roles) to quantify the benefits of the memetic proxy technique

3. **Cross-Domain Transferability**: Test the automated dataset creation methodology on a different NLP task (e.g., named entity recognition or sentiment analysis) using the same RCI framework to assess generalizability