---
ver: rpa2
title: Compositional Program Generation for Few-Shot Systematic Generalization
arxiv_id: '2309.16467'
source_url: https://arxiv.org/abs/2309.16467
tags:
- which
- cogs
- grammar
- types
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neuro-symbolic architecture called Compositional
  Program Generator (CPG) that achieves perfect compositional generalization on SCAN
  and COGS benchmarks with minimal training data. CPG generates rule-specific probability
  distribution parameters for probabilistic copy or substitution programs, which are
  composed recursively in a structure that mirrors the parse structure of the input
  sentence.
---

# Compositional Program Generation for Few-Shot Systematic Generalization

## Quick Facts
- arXiv ID: 2309.16467
- Source URL: https://arxiv.org/abs/2309.16467
- Reference count: 7
- Primary result: Perfect compositional generalization on SCAN and COGS benchmarks using minimal training data

## Executive Summary
This paper presents Compositional Program Generator (CPG), a neuro-symbolic architecture that achieves perfect compositional generalization on SCAN and COGS benchmarks with minimal training data. CPG generates rule-specific probability distribution parameters for probabilistic copy or substitution programs, which are composed recursively according to the parse structure of input sentences. The approach achieves 100% accuracy on both benchmarks using just 14 examples for SCAN and 22 examples for COGS, representing a 1000x improvement in sample efficiency compared to previous state-of-the-art models.

## Method Summary
CPG is a neuro-symbolic architecture that leverages a context-free grammar and parser to generate abstract parse structures, with each grammar rule assigned a unique semantic module (copy or substitution program). These modules are composed recursively according to the parse structure, ensuring that expressions with the same abstract parse use the same composed program. CPG uses curricular training by sentence length, incrementally learning new types and freezing parameters once a stage's accuracy reaches 1.0. The model learns parameters for rule-specific probability distributions that determine how the semantic modules operate, enabling systematic generalization to new concepts from minimal examples.

## Key Results
- Achieved 100% accuracy on SCAN benchmark using only 14 examples (1000x sample efficiency improvement)
- Achieved 100% accuracy on COGS benchmark using only 22 examples
- Perfect generalization to both primitive and composed concepts not seen during training
- Demonstrated incremental learning capability without forgetting previously learned rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model achieves compositional generalization by using rule-specific semantic modules that mirror the parse structure of the input.
- Mechanism: Each grammar rule is assigned a unique semantic module (either copy or substitution program), and these modules are composed recursively according to the parse structure. This ensures that expressions with the same abstract parse use the same composed program.
- Core assumption: Grammar rules can be mapped to meaningful semantic operations that generalize across different inputs sharing the same parse structure.

### Mechanism 2
- Claim: The incremental learning approach with curricular training enables CPG to learn new types without forgetting previously learned ones.
- Mechanism: Training proceeds in stages by sentence length, freezing parameters for learned rules once stage accuracy reaches 1.0 before moving to the next stage with longer sentences and new types.
- Core assumption: Types needed for parsing grow with sentence length until a fixpoint is reached, allowing for systematic incremental learning.

### Mechanism 3
- Claim: The separation of syntax (grammar parsing) from semantics (semantic modules) enables systematic generalization.
- Mechanism: The parser generates an abstract parse structure, and the semantic modules operate only on this abstract structure rather than the raw input, ensuring consistent handling of expressions with identical parses.
- Core assumption: Abstract parse structures capture the essential compositional relationships needed for generalization.

## Foundational Learning

- Concept: Context-Free Grammars (CFGs)
  - Why needed here: CPG requires a CFG of the input language to generate parses that determine how semantic modules are composed.
  - Quick check question: What is the key property of a CFG that makes it suitable for CPG's approach to compositional generalization?

- Concept: Probabilistic Parameter Learning
  - Why needed here: CPG learns parameters for rule-specific probability distributions that determine how copy and substitution modules operate.
  - Quick check question: How does CPG ensure that learning parameters for one rule doesn't interfere with learning parameters for another rule?

- Concept: Curriculum Learning
  - Why needed here: CPG uses curricular training by sentence length to incrementally learn types without forgetting previously learned ones.
  - Quick check question: Why is it important that types needed for parsing grow with sentence length until a fixpoint is reached?

## Architecture Onboarding

- Component map: Input sentence → Parser → Parse tree with type annotations → Parameter Generator → Distribution parameters → Semantic Modules composed recursively → Output sequence

- Critical path:
  1. Input sentence → Parser → Parse tree with type annotations
  2. Parse tree types → Parameter Generator → Distribution parameters
  3. Parameters + Parse structure → Semantic Modules composed recursively
  4. Output: Generated sequence or logical form

- Design tradeoffs:
  - Flexibility vs. complexity: Using explicit grammars provides interpretability but requires grammar engineering
  - Modularity vs. efficiency: Separate modules for each rule enable systematic generalization but increase model size
  - Incrementality vs. convergence: Curricular training avoids forgetting but may require careful stage design

- Failure signatures:
  - Local minima in parameter learning (detected through stage accuracy plateauing below 1.0)
  - Poor grammar factorization leading to redundant learning of similar concepts
  - Type misalignment causing semantic modules to learn the wrong concepts

- First 3 experiments:
  1. Test CPG on SCAN length split with classic training data to verify perfect generalization
  2. Test CPG on COGS with reduced training data (few-shot setting) to verify sample efficiency
  3. Test CPG with modified grammar (introducing type redundancy) to observe impact on learning efficiency

## Open Questions the Paper Calls Out

- Question: How can the CPG architecture be extended to learn the grammar/parser and dictionary automatically, rather than requiring them to be supplied?
  - Basis in paper: The paper mentions that "Although CPG is directly applicable to problems for which the grammatical structure and dictionary is known... our aim is to scale it to a broader set of real-world problems. For this, factoring the problem into the sub-tasks of learning the grammar/parser, dictionary, and CPG is a helpful decomposition..."
  - Why unresolved: The paper only evaluates CPG with hand-supplied grammars and dictionaries, and discusses the challenges of learning these components jointly but does not present a solution.
  - What evidence would resolve it: A successful extension of CPG that learns the grammar, parser, and dictionary jointly with the compositional program generator, achieving comparable performance on SCAN and COGS benchmarks.

- Question: How does the performance of CPG scale with increasing grammar complexity and vocabulary size?
  - Basis in paper: The paper demonstrates perfect generalization on SCAN and COGS with relatively simple grammars (60 types for COGS), but does not explore performance on more complex grammars or larger vocabularies.
  - Why unresolved: The experiments are limited to the SCAN and COGS benchmarks, which have fixed, relatively simple grammars. The paper does not investigate how CPG would perform on more complex natural language tasks.
  - What evidence would resolve it: Evaluation of CPG on natural language tasks with larger vocabularies and more complex grammars, such as machine translation or semantic parsing of real-world sentences, showing that CPG maintains high compositional generalization accuracy.

- Question: Can the type refactoring and merging techniques used to improve CPG performance be automated?
  - Basis in paper: The paper discusses manual grammar refactoring to improve CPG performance, such as replacing repeated sequences of types with a single type or merging closely linked types. It mentions that "Automating this process may also provide an interesting strategy for learning the grammar in future work by searching through the space of grammar refactorings."
  - Why unresolved: The paper only demonstrates manual grammar refactoring and does not present an automated method for identifying and applying these optimizations.
  - What evidence would resolve it: An automated algorithm that analyzes CPG performance during training, identifies opportunities for type refactoring and merging, and applies these optimizations to improve compositional generalization accuracy.

## Limitations

- The approach requires carefully engineered context-free grammars that must align well with the semantic task, with limited exploration of performance when this alignment is imperfect.
- The 1000x sample efficiency improvement is impressive but lacks comparison against other few-shot approaches using similar training splits.
- The curricular training approach may not scale well to more complex domains where type fixpoints are harder to identify or where the curriculum ordering is less obvious.

## Confidence

**High Confidence:** The core mechanism of using parse-structure-mirrored composition with rule-specific modules is well-supported by the experimental results on both SCAN and COGS. The perfect generalization scores (100% accuracy) provide strong empirical validation.

**Medium Confidence:** The claims about incremental learning without forgetting are supported by the training methodology, but the evaluation focuses on final performance rather than detailed forgetting analysis. The curricular training approach shows promise but lacks robustness testing across different curriculum orderings.

**Low Confidence:** The generalizability of the approach to domains beyond the specific benchmarks tested remains uncertain. The paper doesn't provide evidence about performance on tasks requiring more complex semantic relationships or those with less regular grammatical structures.

## Next Checks

1. **Grammar Alignment Stress Test:** Systematically evaluate CPG performance when the provided grammar is deliberately misaligned with the semantic task, measuring how much performance degrades and identifying break points in the approach.

2. **Curriculum Sensitivity Analysis:** Test CPG with randomized versus optimized curriculum orderings across multiple runs to quantify the sensitivity of the incremental learning approach to curriculum design choices.

3. **Cross-Domain Transfer:** Apply CPG to a different compositional generalization task (such as mathematical word problems or a new semantic parsing dataset) to assess whether the approach generalizes beyond the specific SCAN/COGS paradigm.