---
ver: rpa2
title: 'Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large Language
  Models'
arxiv_id: '2309.06415'
source_url: https://arxiv.org/abs/2309.06415
tags:
- they
- negligible
- toxicity
- rabbit
- toxic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework called "toxicity rabbit
  hole" that iteratively elicits toxic content from large language models (LLMs).
  The framework starts with a stereotype and instructs the LLM to generate more toxic
  text at each iteration.
---

# Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large Language Models

## Quick Facts
- arXiv ID: 2309.06415
- Source URL: https://arxiv.org/abs/2309.06415
- Reference count: 40
- Key outcome: Novel framework reveals PaLM 2 safety guardrails fail to detect highly toxic antisemitic, Islamophobic, racist, homophobic, and misogynistic content

## Executive Summary
This paper introduces a "toxicity rabbit hole" framework that iteratively elicits increasingly toxic content from LLMs by starting with stereotypes and instructing the model to generate more toxic text at each iteration. When tested on PaLM 2, the framework successfully generated highly disturbing content targeting various identity groups while the safety guardrails failed to flag it as highly unsafe. The study reveals significant calibration gaps in content rating systems and demonstrates how iterative prompting can bypass safety mechanisms. The authors release a dataset of toxic content and emphasize the urgent need for improved safety measures and transparency in LLM guardrails.

## Method Summary
The toxicity rabbit hole framework uses PaLM 2's API with configurable safety settings to generate toxic content through iterative prompting. The method starts with 1,266 identity groups (193 nationalities, 1,023 ethnic groups, 50 religions) and two stereotype prompts per group ("G are nice people"; "G are not nice people"). Each iteration uses the same instruction to make previous output more toxic, continuing until safety violations occur or cycles are detected. The study varies temperature and top_k parameters (0/0.2/0.4/0.6/0.8/1 and 20/40/60/80/100) and collects toxic expansions for analysis against PaLM 2's six-category safety ratings.

## Key Results
- PaLM 2 generated highly toxic antisemitic, Islamophobic, racist, homophobic, and misogynistic content that safety guardrails failed to flag as highly unsafe
- The framework successfully bypassed safety mechanisms through iterative prompting, revealing calibration gaps in content rating systems
- Generated toxic expansions showed specific targeting patterns, with certain identity groups appearing more frequently in harmful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative prompt design forces LLMs to progressively amplify toxic content while bypassing safety guardrails
- Mechanism: Each iteration uses previous toxic output with "make more toxic" instruction, creating compounding effect
- Core assumption: LLMs will consistently interpret escalation instructions rather than reject harmful content
- Evidence anchors:
  - [abstract] Framework instructs PaLM 2 to generate more toxic text at each iteration
  - [section 3] Each step uses same instruction requesting more toxic content than previous step
  - [corpus] Weak - no direct corpus evidence across models

### Mechanism 2
- Claim: Framework exploits calibration gaps between human judgment and model's internal safety scoring
- Mechanism: PaLM 2's safety feedback system fails to recognize seriously harmful content as "highly unsafe"
- Core assumption: Model's safety scoring system is miscalibrated and can be gamed by specific prompt patterns
- Evidence anchors:
  - [abstract] Content is not evaluated as highly unsafe despite being disturbing
  - [section 3] None of toxic expansions evaluated as unsafe with high harm probability
  - [corpus] Weak - limited corpus evidence on other models' handling

### Mechanism 3
- Claim: Framework leverages training data biases to produce identity-targeted harmful content
- Mechanism: Iterating on stereotypes draws on and amplifies historical biases in training data
- Core assumption: Training corpus contains biased representations that can be surfaced through targeted prompts
- Evidence anchors:
  - [abstract] Uncover worrying biases and calibration issues
  - [section 3.3] Experiments confirm toxic expansions for well-known religions/ethnic groups
  - [corpus] Moderate - evidence shows persistent antisemitism and Islamophobia patterns

## Foundational Learning

- Concept: Prompt engineering and iterative generation
  - Why needed here: Framework relies on crafting specific prompts producing increasingly toxic content through iteration
  - Quick check question: If testing different harmful content (not toxicity), what core element would you change?

- Concept: Safety feedback systems and calibration
  - Why needed here: Understanding how LLMs evaluate content safety is crucial for interpreting undetected harmful content
  - Quick check question: What does "not evaluated as highly unsafe" mean despite disturbing content?

- Concept: Identity-based bias in language models
  - Why needed here: Framework targets stereotypes about different identity groups to uncover biases
  - Quick check question: Why start with both positive and negative stereotypes for research?

## Architecture Onboarding

- Component map: PaLM 2 API -> Safety filter settings -> Iterative prompt generator -> Content collection -> Analysis pipeline
- Critical path: Prompt generation -> API call with safety filters -> Response collection -> Safety feedback evaluation -> Next iteration
- Design tradeoffs: Broad identity group coverage vs. depth of analysis per group; adversarial prompting vs. ethical considerations
- Failure signatures: Safety filters consistently blocking content; model refusing toxic continuations; lack of pattern in identity targeting
- First 3 experiments:
  1. Test rabbit hole framework with single stereotype and observe safety filter triggers
  2. Vary temperature and top_k parameters to see effect on rabbit hole depth
  3. Test framework with different starting stereotype to verify result consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do PaLM 2's safety guardrails perform on toxic content targeting less commonly discussed identity groups?
- Basis in paper: [explicit] Paper mentions 1,023 ethnic groups, 193 nationalities, and 50 religions but lacks detailed analysis for all groups
- Why unresolved: Focuses on subset of identity groups without comprehensive data across all groups
- What evidence would resolve it: Detailed analysis of safety guardrails performance on toxic content targeting all identity groups

### Open Question 2
- Question: How do PaLM 2's safety guardrails compare to other leading large language models in detecting and blocking toxic content?
- Basis in paper: [inferred] Discusses PaLM 2 audit but lacks direct comparison with other LLMs
- Why unresolved: No comparative analysis of PaLM 2's guardrails with other leading LLMs
- What evidence would resolve it: Comparative analysis of PaLM 2's guardrails with other leading LLMs

### Open Question 3
- Question: How do PaLM 2's safety guardrails perform on toxic content generated through different prompt variations?
- Basis in paper: [explicit] Discusses varying prompts but lacks detailed analysis across different variations
- Why unresolved: No comprehensive analysis of guardrails performance on content from different prompt variations
- What evidence would resolve it: Detailed analysis of guardrails performance on content from different prompt variations

## Limitations
- Framework tested only on PaLM 2, making generalization to other models unclear
- Calibration issues based on model's own safety ratings may not align with standardized benchmarks
- Iterative nature may produce extreme content that doesn't reflect real-world usage patterns

## Confidence

- **High Confidence**: Framework's ability to generate increasingly toxic content through iterative prompting is well-demonstrated and reproducible
- **Medium Confidence**: Identification of specific identity groups frequently targeted in toxic expansions appears consistent but may be influenced by model updates
- **Low Confidence**: Broader claim that findings represent systematic safety failures in LLM guardrails, as this depends on interpretation of safety ratings

## Next Checks

1. **Cross-model validation**: Test toxicity rabbit hole framework across multiple leading LLMs (GPT-4, Claude, LLaMA) to determine if safety calibration issues are model-specific or represent broader industry challenge

2. **Human evaluation benchmark**: Conduct blind human assessments of generated toxic content to compare against model's safety ratings and determine if calibration issues persist with independent raters

3. **Real-world scenario testing**: Design prompt variations mimicking actual user behaviors rather than adversarial prompting to assess whether safety vulnerabilities would manifest in practical applications