---
ver: rpa2
title: 'Large Language Models and Explainable Law: a Hybrid Methodology'
arxiv_id: '2311.11811'
source_url: https://arxiv.org/abs/2311.11811
tags:
- legal
- right
- translation
- language
- mario
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid methodology that combines rule-based
  legal systems with Large Language Models (LLMs) to enhance accessibility and explainability
  of legal reasoning. The authors develop a methodology to use LLMs to translate rule-based
  system outputs from high-level programming languages to natural language, enabling
  laypeople to understand complex legal inferences.
---

# Large Language Models and Explainable Law: a Hybrid Methodology

## Quick Facts
- arXiv ID: 2311.11811
- Source URL: https://arxiv.org/abs/2311.11811
- Reference count: 20
- One-line primary result: A hybrid methodology combining rule-based legal systems with LLMs to translate complex legal inferences into accessible natural language explanations

## Executive Summary
This paper proposes a hybrid methodology that combines rule-based legal systems with Large Language Models (LLMs) to enhance accessibility and explainability of legal reasoning. The authors develop a methodology to use LLMs to translate rule-based system outputs from high-level programming languages to natural language, enabling laypeople to understand complex legal inferences. They apply this approach to the CrossJustice platform, using GPT-4 to explain legal reasoning in a case involving translation rights under EU and Polish law.

## Method Summary
The methodology involves translating Prolog rule-based system outputs into natural language explanations using LLMs, with a focus on maintaining legal validity while enhancing accessibility. The approach employs chain-of-thought prompting to break down complex legal comparison tasks into manageable intermediate reasoning steps. Temperature is set to minimum to reduce LLM stochasticity and ensure stable, consistent outputs. The methodology is applied to a case study involving translation rights under EU Directive 2010/64 and Polish law, demonstrating the potential for creating chains of prompts to enable autonomous legal comparison tasks.

## Key Results
- Successfully generates accessible natural language explanations while preserving legal validity
- Demonstrates effective use of chain-of-prompts methodology for autonomous legal comparison tasks
- Shows promise for democratizing access to legal reasoning and enabling complex legal operations for non-experts
- Temperature control at minimum settings ensures repeatable, stable outputs for legal explanations

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought prompting enables LLMs to break down complex legal comparison tasks into manageable intermediate reasoning steps by structuring prompts into sequential subtasks (first extract information, then compare), maintaining focus and reducing hallucination risk during multi-step legal analysis.

### Mechanism 2
Rule-based systems provide legally valid foundations that LLMs can safely translate without legal reasoning errors, acting as translators rather than legal reasoners, converting Prolog inference traces into natural language while preserving the validity of the original rule-based conclusions.

### Mechanism 3
Temperature control at minimum settings ensures repeatable, stable outputs from LLMs for legal explanations by reducing creativity and hallucination while forcing extraction of precise legal inferences from rule-based systems.

## Foundational Learning

- Concept: Prolog programming and rule-based inference systems
  - Why needed here: The methodology relies on translating Prolog outputs, requiring understanding of how rule-based systems represent legal reasoning
  - Quick check question: What is the difference between a Prolog fact and a rule, and how does each appear in the CrossJustice output?

- Concept: Chain-of-Thought prompting methodology
  - Why needed here: The comparison task specifically uses this technique to break down complex legal analysis into manageable steps
  - Quick check question: How does dividing a legal comparison task into two prompts improve accuracy compared to a single comprehensive prompt?

- Concept: Legal terminology and structure (rights, auxiliary rights, properties)
  - Why needed here: Understanding these concepts is essential for evaluating whether LLM translations accurately capture legal meaning
  - Quick check question: In the CrossJustice system, what distinguishes an auxiliary right from a property, and why does this distinction matter for translation?

## Architecture Onboarding

- Component map: Prolog rule-based system (CrossJustice) -> LLM (GPT-4) -> Natural language interface -> User
- Critical path: User case input -> Prolog inference generation -> LLM prompt construction -> LLM output generation -> User explanation delivery
- Design tradeoffs: Expert system accuracy vs. LLM accessibility; structured precision vs. natural language flexibility; single-shot vs. chain-of-prompts approaches
- Failure signatures: LLM hallucination of legal terms not in Prolog; missing inference steps; inconsistent output structure; temperature settings causing either rigidity or creativity
- First 3 experiments:
  1. Test single-shot prompt on simple CrossJustice inference (right to translation) to verify basic translation capability
  2. Test temperature variation (0.0, 0.3, 0.7) on same inference to measure consistency impact
  3. Test chain-of-prompts approach by first translating, then comparing two simple legal sources on same factual scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the methodology be extended to handle multilingual legal documents and comparisons across different languages?
- Basis in paper: The paper mentions that the methodology could embrace the multilingual nature of European Law and overcome language barriers in legal technology.
- Why unresolved: The current case study only involves legal texts in English and Polish, and the paper does not provide details on how the approach would work with multiple languages.
- What evidence would resolve it: Testing the methodology with legal documents in multiple languages and demonstrating successful translation and comparison across them.

### Open Question 2
- Question: What is the impact of the limited context provided to the model on the accuracy and completeness of the natural language explanations?
- Basis in paper: The paper mentions that providing the full text of relevant legal norms did not substantially improve the performance, suggesting that context might be a factor.
- Why unresolved: The paper does not explore the effects of varying the amount of context provided to the model on the quality of the output.
- What evidence would resolve it: Experiments comparing the quality of explanations generated with different amounts of context provided to the model.

### Open Question 3
- Question: How can the methodology be improved to reliably identify and present all sub-rules and conditions in the legal reasoning process?
- Basis in paper: The paper states that GPT-4 struggles in giving the exact meaning to Prolog terms and cannot reliably identify all sub-rules and conditions.
- Why unresolved: The paper acknowledges this limitation but does not propose a solution or further exploration of the issue.
- What evidence would resolve it: Developing and testing modifications to the prompts or methodology that successfully address this limitation and demonstrate improved identification of sub-rules and conditions.

## Limitations
- Methodology's scalability remains uncertain when applied to more complex legal domains beyond translation rights
- Assumes rule-based systems will always provide clean, unambiguous Prolog outputs - an assumption that may break down in more complex legal scenarios
- Limited corpus evidence for several key mechanisms, particularly regarding temperature control effects and chain-of-thought prompting for legal comparison tasks

## Confidence

- **High confidence**: The core translation methodology from Prolog to natural language is well-grounded and reproducible based on the detailed specification provided.
- **Medium confidence**: The claim that temperature minimization improves output stability is supported by the experimental design but lacks corpus validation.
- **Medium confidence**: The chain-of-prompts approach for legal comparison shows promise but requires further validation on more complex legal tasks.
- **Low confidence**: The scalability of this approach to domains with more ambiguous legal reasoning or less structured rule-based systems.

## Next Checks

1. **Temperature robustness test**: Run the translation task across a range of temperature settings (0.0, 0.3, 0.7, 1.0) on multiple Prolog traces to empirically validate the minimum temperature recommendation and identify any optimal balance between stability and expressiveness.

2. **Complexity scaling test**: Apply the methodology to a more complex legal domain (e.g., contract law or intellectual property) with nested conditional rules and cross-references to evaluate whether the Prolog-to-natural-language translation maintains accuracy and completeness.

3. **Alternative LLM comparison**: Test the methodology using different LLM models (Claude, Llama, or open-source alternatives) to determine whether the approach is specific to GPT-4 or generalizes across language models, particularly for the chain-of-prompts comparison task.