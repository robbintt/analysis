---
ver: rpa2
title: 'LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention
  Bias'
arxiv_id: '2307.02912'
source_url: https://arxiv.org/abs/2307.02912
tags:
- typos
- similarity
- lexical
- noise
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of sentence similarity robustness
  to typos and textual noise, which degrades the performance of transformer models.
  The core method, Lexical-aware Attention (LEA), modifies the self-attention mechanism
  in cross-encoders by adding a lexical bias that leverages raw character-level similarities
  between words across sentences.
---

# LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias

## Quick Facts
- arXiv ID: 2307.02912
- Source URL: https://arxiv.org/abs/2307.02912
- Reference count: 40
- Primary result: LEA improves F1-score by 5-7 points on noisy e-commerce datasets

## Executive Summary
This paper introduces Lexical-aware Attention (LEA), a method that enhances transformer-based sentence similarity models' robustness to typos and textual noise. LEA modifies the self-attention mechanism by adding a lexical bias term that computes character-level similarities between words across sentences, independent of tokenization. The approach addresses the tokenization shift problem where corrupted words are split into subwords, disrupting attention matching. Experiments on five e-commerce product matching datasets show LEA consistently improves performance in noisy settings while maintaining competitive results on clean data.

## Method Summary
LEA adds a lexical attention bias to the self-attention mechanism in cross-encoders. For each transformer layer, it computes pairwise lexical similarities between words across the concatenated sentence pair using metrics like Jaccard coefficient. These similarities are transformed via sinusoidal encoding and learnable projection matrices (W_L) to create lexical attention embeddings, which are added to the standard self-attention scores. LEA is applied to the second half of transformer layers using independent W_L matrices per attention head, with 128-dimensional lexical embeddings and Jaccard similarity as default.

## Key Results
- LEA improves F1-score by 5-7 absolute points on noisy e-commerce datasets compared to vanilla cross-encoders
- LEA remains competitive with baseline on clean data (sometimes slightly underperforming)
- Applying LEA to last half of layers outperforms full-layer application (5.7 vs 4.9 F1 points on average)
- LEA generalizes to textual entailment and paraphrasing tasks, maintaining similar improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical similarity bias compensates for tokenization shift caused by typos
- Mechanism: When typos cause words to be split into subwords, LEA's character-level similarity (e.g., Jaccard) maintains semantic links independent of tokenization
- Core assumption: Character-level lexical similarity is robust even when tokenization fails
- Evidence anchors: Abstract states LEA "avoids the tokenization shift problem"; section explains noise causes token distribution shifts
- Break condition: If lexical similarity metric is too coarse (e.g., only exact matches), it fails on corrupted words

### Mechanism 2
- Claim: Adding lexical bias to late layers yields better performance than early layers
- Mechanism: Late layers capture high-level semantics; LEA integrates lexical similarity into contextualized representations without disrupting early syntactic processing
- Core assumption: High-level semantic representations benefit more from lexical cues than low-level token features
- Evidence anchors: Section shows applying LEA to second half outperforms full application; argues character-level similarity is "high-level interaction information"
- Break condition: If applied too early, LEA might interfere with essential syntactic feature extraction

### Mechanism 3
- Claim: Independent lexical projection matrices per head improve flexibility and performance
- Mechanism: Different attention heads can specialize in different lexical similarity patterns; independent W_L matrices allow this specialization
- Core assumption: Different heads have distinct roles and lexical bias should be modulated accordingly
- Evidence anchors: Section uses non-shared W_L matrices as default; states independent matrices provide "higher flexibility"
- Break condition: If lexical similarity is uniform across all heads, sharing parameters may suffice

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: LEA modifies the self-attention computation by adding lexical bias term
  - Quick check question: In Eq. 3, what is the role of the scaling factor 1/âˆšd_h?

- Concept: Subword tokenization and its failure modes
  - Why needed here: LEA addresses robustness issues caused by tokenization shifts when typos create OOV words
  - Quick check question: Why does the tokenizer split "blk" into multiple subwords, and how does this affect attention matching?

- Concept: String similarity metrics (Jaccard, Levenshtein, Smith-Waterman)
  - Why needed here: LEA relies on lexical similarity metric to compute pairwise lexical embeddings
  - Quick check question: How does Jaccard coefficient differ from Levenshtein distance in handling character swaps?

## Architecture Onboarding

- Component map:
  Input sentences -> Tokenizer -> Standard transformer layers with modified self-attention -> LEA module (lexical similarity + sinusoidal encoding + W_L projection) -> Mean-pooled token embeddings -> MLP classifier

- Critical path:
  1. Tokenize input sentences
  2. Compute pairwise lexical similarity between words across sentences
  3. Generate sinusoidal embeddings from similarities
  4. Apply LEA bias to self-attention in selected layers
  5. Forward pass through transformer
  6. Mean pooling and classification

- Design tradeoffs:
  - Applying LEA to all layers vs. last half: Wider application risks disrupting early feature extraction; narrower application may miss reinforcement opportunities
  - Shared vs. independent W_L: Shared parameters reduce model size but limit head specialization; independent parameters increase flexibility at cost of memory

- Failure signatures:
  - Performance drop on clean data: Lexical bias may overfit to noise patterns
  - No improvement on noisy data: Similarity metric may be too coarse or LEA applied to wrong layers
  - High variance across runs: Insufficient lexical bias strength or unstable similarity computation

- First 3 experiments:
  1. Baseline: Vanilla cross-encoder without LEA, evaluate on clean and noisy test sets
  2. LEA with Jaccard similarity, applied to last 4 layers, independent W_L per head; compare against baseline
  3. Vary lexical similarity metric (Smith-Waterman, Levenshtein, LCS) with LEA; measure impact on noisy performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal configuration for LEA's lexical attention bias in terms of the layers where it should be applied?
- Basis in paper: [explicit] Experiments with different layer subsets (all, excluding first two, second half, last two) show second half yields best performance
- Why unresolved: Paper doesn't explain why second half is optimal or whether this consistency across model sizes and tasks
- What evidence would resolve it: Further experiments comparing LEA across various model sizes and tasks with attention pattern analysis

### Open Question 2
- Question: How does LEA compare to other methods for handling textual noise like character-level models or denoising autoencoders?
- Basis in paper: [inferred] Mentions character-level information is important and LEA avoids tokenization shift, but doesn't directly compare to alternatives
- Why unresolved: Direct comparison with other noise-handling methods needed to establish relative strengths
- What evidence would resolve it: Experiments comparing LEA to character-level models or denoising autoencoders on same datasets and tasks

### Open Question 3
- Question: Can LEA be effectively applied to other Transformer-based architectures for text generation or summarization?
- Basis in paper: [explicit] Mentions LEA designed for architectures using multiple sentences as input and discusses potential for bi-encoders with late-interaction
- Why unresolved: Paper focuses on cross-encoders for sentence similarity; effectiveness in other architectures unexplored
- What evidence would resolve it: Experiments applying LEA to text generation or summarization transformers with textual noise evaluation

## Limitations
- Tokenization dependency: LEA still relies on word-level boundaries from tokenizer, potentially degrading on heavily corrupted text
- Dataset domain specificity: All primary experiments on e-commerce datasets; only two additional domains tested
- Computational overhead: Introduces additional parameters and computations without runtime or memory usage metrics

## Confidence
**High Confidence Claims**:
- LEA improves F1-score by 5-7 points on noisy e-commerce datasets
- LEA remains competitive on clean data
- Applying LEA to last half of layers outperforms full-layer application

**Medium Confidence Claims**:
- LEA generalizes to textual entailment and paraphrasing tasks
- Independent W_L matrices per head provide meaningful gains
- Lexical similarity metrics have comparable performance

**Low Confidence Claims**:
- LEA scales effectively to much larger models
- Sinusoidal encoding scheme is optimal for lexical embeddings
- Specific layer allocation (last 4 layers) is universally optimal

## Next Checks
- Check 1: Evaluate LEA on dataset with extreme noise levels (40-50% character corruption) to determine breaking point where lexical similarity becomes unreliable
- Check 2: Implement LEA on larger model (BERT-Large or RoBERTa) and measure performance gains vs computational overhead (GPU memory, inference latency)
- Check 3: Conduct ablation studies varying lexical similarity metric granularity and test on domain-specific noise patterns (OCR errors, phonetic substitutions, code-switching)