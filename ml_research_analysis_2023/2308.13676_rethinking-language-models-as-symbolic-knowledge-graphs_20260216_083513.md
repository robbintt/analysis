---
ver: rpa2
title: Rethinking Language Models as Symbolic Knowledge Graphs
arxiv_id: '2308.13676'
source_url: https://arxiv.org/abs/2308.13676
tags:
- knowledge
- these
- triples
- language
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in evaluating whether large language
  models (LMs) can capture the intricate topological and semantic attributes of symbolic
  knowledge graphs (KGs), which are crucial for reasoning processes. The authors construct
  nine novel benchmarks based on the T-REx dataset, covering attributes like symmetry,
  hierarchy, compositionality, and bias.
---

# Rethinking Language Models as Symbolic Knowledge Graphs

## Quick Facts
- arXiv ID: 2308.13676
- Source URL: https://arxiv.org/abs/2308.13676
- Reference count: 40
- Primary result: LMs achieve only 23.7% hit@1 on novel KG benchmarks vs up to 50% on LAMA, revealing gaps in capturing relational structure.

## Executive Summary
This paper investigates whether large language models can capture the topological and semantic attributes of symbolic knowledge graphs that are crucial for reasoning. The authors construct nine novel benchmarks based on the T-REx dataset covering attributes like symmetry, hierarchy, and compositionality, along with custom evaluation metrics tailored for each attribute. Experiments with various LMs including BERT, RoBERTa, T5, GPT-3, and GPT-4 reveal that even the largest models achieve only 23.7% hit@1 on the proposed benchmarks, compared to up to 50% on existing LAMA benchmarks. The results show that LMs may find it simpler to retrieve independent KG triples compared to capturing intricate topological and semantic traits.

## Method Summary
The authors construct nine novel benchmarks from the T-REx dataset, each targeting specific KG attributes (symmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths, entity-centricity, bias, ambiguity). They transform triples into cloze-style prompts and generate multiple paraphrased variants to reduce prompt sensitivity. Various language models (BERT, RoBERTa, T5 for masked prediction; GPT-3/4 for text completion) are evaluated on these benchmarks using custom metrics that measure relational consistency rather than isolated triple accuracy. The evaluation compares LM performance against traditional LAMA-style metrics and examines correlations between partial scores and the proposed metrics.

## Key Results
- LMs achieve only 23.7% average hit@1 on novel KG benchmarks versus up to 50% on LAMA benchmarks
- Larger models like GPT-4 are not universally better; they are outperformed by BERT on bidirectional, compositional, and ambiguous benchmarks
- Proposed evaluation metrics show only weak correlation with traditional precision@1 metrics
- LMs struggle more with relational consistency than with retrieving independent triples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmarks expose a fundamental mismatch between how LMs learn facts and how KGs represent relational structure.
- Mechanism: By requiring simultaneous prediction of multiple related triples (e.g., both (A, r, B) and (B, r, A) for symmetry), the evaluation forces LMs to internalize relational semantics rather than just memorizing isolated facts.
- Core assumption: Standard LAMA-style metrics overestimate LM capability because they treat each triple independently and don't require relational consistency across multiple linked triples.
- Evidence anchors:
  - [abstract] "Existing metrics (i.e., precision@1) focus primarily on individual triples and are inadequate in providing a dependable assessment for the proposed benchmarks."
  - [section 3.1] The symmetric metric definition explicitly discounts models that retrieve only one of the two symmetric triples.
- Break condition: If prompts fail to reliably elicit relational structure, the metric may over-penalize models due to prompt sensitivity rather than lack of understanding.

### Mechanism 2
- Claim: Larger LMs are not universally better at capturing KG attributes; scale benefits are attribute-specific.
- Mechanism: The evaluation reveals that while large LMs like GPT-4 excel at factual recall, they underperform smaller models like BERT on bidirectional and compositional reasoning, suggesting scaling improves breadth but not depth of relational understanding.
- Core assumption: Model size correlates with memorization capacity, not with the ability to learn complex relational patterns requiring compositional reasoning.
- Evidence anchors:
  - [abstract] "larger LMs are not universally better than smaller ones. For example, GPT-4 is outperformed by BERT on bidirectional, compositional and ambiguity benchmarks."
  - [section 4.2] Detailed comparison showing GPT-4's relative weaknesses in bidirectional, paths, and ambiguous benchmarks.
- Break condition: If future models incorporate explicit relational reasoning modules, the size vs. performance relationship may invert.

### Mechanism 3
- Claim: The proposed metrics are more reliable because they measure relational consistency, not just isolated triple recall.
- Mechanism: By requiring models to correctly predict all triples in a relational example, the metrics ensure LMs have internalized semantic constraints of the relation, validated by weak correlation between partial scores and proposed metrics.
- Core assumption: Relational consistency is a better proxy for understanding than isolated fact recall.
- Evidence anchors:
  - [abstract] "our proposed evaluation metrics are more reliable in evaluating these abilities than the existing metrics."
  - [section 4.3] "We find that there is only a very weak correlation between these partial scores and our proposed metrics."
- Break condition: If relational examples are too small or artificial, the metrics may not generalize to real-world reasoning tasks.

## Foundational Learning

- Concept: Relational reasoning in knowledge graphs
  - Why needed here: The paper's core contribution is evaluating whether LMs can capture relational semantics (symmetry, hierarchy, compositionality) that are central to KG reasoning.
  - Quick check question: What is the difference between a symmetric and an asymmetric relation in a knowledge graph?

- Concept: Prompt engineering and paraphrasing
  - Why needed here: The evaluation relies on transforming triples into cloze prompts and generating paraphrased variants to reduce prompt sensitivity.
  - Quick check question: Why does the paper use multiple paraphrased prompts for each triple?

- Concept: Evaluation metric design
  - Why needed here: The novel metrics (e.g., symmetric_metric) are designed to measure relational consistency, not just isolated triple accuracy.
  - Quick check question: How does the symmetric metric differ from standard precision@1?

## Architecture Onboarding

- Component map: T-REx dataset -> sampled triples -> benchmark construction -> cloze prompt generation -> paraphrasing -> LM querying -> metric computation -> analysis
- Critical path: Benchmark creation -> prompt generation -> LM querying -> metric computation -> analysis
- Design tradeoffs:
  - Using cloze prompts vs. natural language questions: Cloze is simpler but may be less robust
  - Sampling strategy: Balancing coverage of relations vs. depth of examples
  - Metric design: Penalizing partial correctness ensures relational consistency but may be too strict
- Failure signatures:
  - Low scores across all models: Benchmarks may be too difficult or prompts ineffective
  - High variance between prompt paraphrases: Prompt sensitivity issue
  - Correlation between entity popularity and accuracy: Model may be memorizing rather than reasoning
- First 3 experiments:
  1. Verify that the symmetric metric correctly identifies symmetric relations by manually checking a subset of examples
  2. Test prompt paraphrasing by comparing LM outputs across paraphrases for a fixed triple
  3. Evaluate a simple baseline (e.g., random guessing) to establish a lower bound for each benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop evaluation metrics that accurately capture the nuanced topological and semantic attributes of KGs in language models?
- Basis in paper: [explicit] The authors argue that existing metrics like precision@1 are inadequate for assessing these nuanced attributes and propose new evaluation metrics tailored for each attribute.
- Why unresolved: The proposed metrics may not fully capture all aspects of KG attributes, and there might be other attributes that are not covered in this study.
- What evidence would resolve it: Developing and validating new metrics that accurately measure various KG attributes in LMs, and comparing their effectiveness against existing metrics.

### Open Question 2
- Question: Are there specific topological and semantic patterns in KGs that language models consistently struggle to understand, regardless of their size?
- Basis in paper: [explicit] The authors find that larger models like GPT-4 are not universally better than smaller ones, and they face more challenges in certain benchmarks like bidirectional relationships, complex paths, and scenarios involving ambiguity.
- Why unresolved: The reasons behind the models' difficulties in understanding specific patterns are not fully explored, and it is unclear whether these challenges can be addressed through model improvements or additional training data.
- What evidence would resolve it: Analyzing the performance of various LMs on different KG patterns and identifying the specific challenges they face, along with potential solutions to address these issues.

### Open Question 3
- Question: How does the size of language models affect their ability to capture and understand topological and semantic attributes of KGs?
- Basis in paper: [explicit] The authors observe that larger models like GPT-4 outperform smaller ones in most benchmarks, but there are exceptions where smaller models perform better, especially in bidirectional relationships, complex paths, and ambiguous scenarios.
- Why unresolved: The relationship between model size and performance on KG attributes is not fully understood, and it is unclear whether there is an optimal model size for capturing these attributes.
- What evidence would resolve it: Conducting a systematic study of the performance of various LMs with different sizes on a wide range of KG attributes, and identifying the factors that contribute to their success or failure in capturing these attributes.

## Limitations

- Benchmark construction bias: The T-REx-based benchmarks may over-represent Wikipedia-centric knowledge, limiting generalizability to other domains.
- Prompt sensitivity: Despite using paraphrased variants, the evaluation still relies on cloze-style prompts which may not uniformly elicit relational knowledge across different LMs.
- Metric design assumptions: The relational consistency metrics assume that partial correctness indicates lack of understanding rather than prompt or model limitations.

## Confidence

- **High confidence**: The empirical finding that LMs struggle with relational consistency beyond isolated triple recall is well-supported by the experimental results across multiple model families and benchmarks.
- **Medium confidence**: The claim that larger LMs aren't universally better at relational reasoning is supported, but the specific attribution to scale vs. architectural differences remains uncertain without ablation studies.
- **Medium confidence**: The assertion that proposed metrics are more reliable than standard ones is plausible given the weak correlation results, but the alternative metrics themselves haven't been validated against external reasoning tasks.

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary prompt templates and paraphrasing strategies for a subset of benchmarks, measuring variance in LM predictions to quantify prompt sensitivity effects on the proposed metrics.

2. **Domain generalization test**: Apply the same benchmarks to a non-Wikipedia knowledge source (e.g., biomedical or scientific triples) to assess whether the observed LM limitations generalize beyond the T-REx domain.

3. **Partial credit validation**: Design an experiment comparing the current all-or-nothing relational metrics against a partial credit version that rewards models for predicting at least one triple correctly, measuring correlation with downstream reasoning task performance.