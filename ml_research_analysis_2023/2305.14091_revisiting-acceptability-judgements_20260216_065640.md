---
ver: rpa2
title: Revisiting Acceptability Judgements
arxiv_id: '2305.14091'
source_url: https://arxiv.org/abs/2305.14091
tags:
- colac
- acceptability
- language
- linguistic
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoLAC, the first large-scale acceptability
  dataset for Chinese. It includes 7,495 sentences verified by native speakers and
  comes with two sets of labels.
---

# Revisiting Acceptability Judgements

## Quick Facts
- arXiv ID: 2305.14091
- Source URL: https://arxiv.org/abs/2305.14091
- Reference count: 27
- Primary result: Even the largest InstructGPT model performs at chance level on Chinese acceptability judgments

## Executive Summary
This paper introduces CoLAC, the first large-scale acceptability dataset for Chinese, containing 7,495 sentences verified by native speakers. The authors evaluate various models on this dataset and find that supervised models significantly outperform few-shot approaches, with even the largest InstructGPT models performing at chance level. Through cross-lingual transfer experiments and fine-grained linguistic analysis, they demonstrate that knowledge of linguistic acceptability can be transferred across typologically distinct languages and may originate from pre-training rather than fine-tuning.

## Method Summary
The paper introduces CoLAC, a Chinese acceptability dataset of 7,495 sentences collected from linguistic sources and verified by native speakers. The authors fine-tune multilingual (XLM-R) and monolingual (Chinese RoBERTa) models on CoLAC, evaluate using Matthews Correlation Coefficient (MCC), and conduct cross-lingual transfer experiments from English (CoLA) to Chinese. They also analyze intermediate layer representations to trace the emergence of acceptability-specific features and examine model performance across 18 syntactic categories.

## Key Results
- Supervised models significantly outperform few-shot approaches on acceptability judgments
- Cross-lingual transfer is possible between English and Chinese despite typological differences
- Acceptability knowledge appears to be encoded in intermediate layers of pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Acceptability judgments can be learned during pre-training, not only via fine-tuning.
- Mechanism: Large language models encode linguistic acceptability features in intermediate layers, transforming generic syntax into acceptability-specific representations as the input propagates deeper.
- Core assumption: Pre-training data contains sufficient implicit acceptability signals across typologically distinct languages.
- Evidence anchors:
  - [abstract]: "demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training."
  - [section 6.1]: "We observe that in the first two layers, the sentences are grouped into four distinct clusters... As the sentences are propagated further up the network, these generic syntactical phenomena are gradually transformed into acceptability-specific features."
  - [corpus]: No explicit acceptability annotations in pre-training data, so this is inferred from cross-lingual performance and layer-wise representations.
- Break condition: If cross-lingual transfer disappears when models are fine-tuned only on one language, the claim of pre-training origin weakens.

### Mechanism 2
- Claim: Few-shot learning is ineffective for acceptability judgments even with large models.
- Mechanism: Acceptability judgments rely on fine-grained syntactic knowledge that does not appear frequently enough in few-shot examples or instruction-tuning datasets.
- Core assumption: Acceptability is a specialized linguistic phenomenon not captured by general-purpose instruction tuning.
- Evidence anchors:
  - [abstract]: "even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance... is also way below supervised models."
  - [section 5.2]: "InstructGPT and its predecessor GPT-3 have demonstrated strong in-context learning performance... but they fail to exhibit any informed judgement on linguistic acceptability."
  - [corpus]: CoLAC is handcrafted by linguists and verified by native speakers, indicating specialized knowledge not commonly represented in general text.
- Break condition: If future models with more specialized pre-training data show improved few-shot performance on acceptability tasks.

### Mechanism 3
- Claim: Cross-lingual transfer is possible for acceptability judgments despite typological differences.
- Mechanism: Multilingual models trained on one language's acceptability dataset can generalize to another language's acceptability judgments, suggesting shared underlying representations.
- Core assumption: Multilingual pre-training creates shared representations for linguistic acceptability across languages.
- Evidence anchors:
  - [abstract]: "demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages."
  - [section 4.2]: "we are able to study the effects of cross-lingual transfer in linguistic acceptability."
  - [corpus]: CoLAC and CoLA cover typologically distinct languages (Chinese and English) with different syntactic phenomena.
- Break condition: If transfer performance degrades significantly when moving to languages with even more distant typological relationships.

## Foundational Learning

- Concept: Cross-lingual transfer in NLP
  - Why needed here: The paper demonstrates that acceptability knowledge transfers between English and Chinese, which is central to understanding model capabilities.
  - Quick check question: What is the key difference between zero-shot and few-shot cross-lingual transfer?

- Concept: Fine-grained syntactic analysis
  - Why needed here: The paper categorizes sentences into 18 syntactic phenomena to analyze model performance on specific linguistic structures.
  - Quick check question: How does categorizing by syntactic phenomena help diagnose model weaknesses?

- Concept: Pre-training vs. fine-tuning distinction
  - Why needed here: The paper argues that acceptability knowledge may originate from pre-training, not just fine-tuning, which is a key theoretical contribution.
  - Quick check question: What experimental evidence would distinguish between pre-training vs. fine-tuning origins of a capability?

## Architecture Onboarding

- Component map: CoLAC dataset collection -> fine-tuning (XLM-R/Chinese RoBERTa) -> MCC evaluation -> layer-wise analysis -> cross-lingual transfer experiments

- Critical path:
  1. Collect and verify acceptability sentences from linguistic sources
  2. Fine-tune multilingual model on CoLAC training set
  3. Evaluate on development set with MCC metric
  4. Analyze layer-wise representations for acceptability features
  5. Conduct cross-lingual transfer experiments with downsampled datasets

- Design tradeoffs:
  - Handcrafted vs. automatically generated data: CoLAC uses linguist-verified examples for quality vs. BLiMP/CLiMP using template generation for scale
  - Crowd labels vs. linguist labels: Crowd labels used as ground truth for broader representation vs. linguist labels for theoretical correctness
  - Monolingual vs. multilingual models: Monolingual performs slightly better on Chinese but multilingual enables cross-lingual transfer

- Failure signatures:
  - Poor MCC scores indicate model fails to capture acceptability distinctions
  - High inter-annotator disagreement correlates with model uncertainty (weakly)
  - Certain syntactic categories (like QUESTION) show systematic model errors suggesting specific weaknesses

- First 3 experiments:
  1. Fine-tune XLM-R on CoLAC training set and evaluate on development set
  2. Fine-tune XLM-R on CoLA and test zero-shot transfer to CoLAC
  3. Visualize intermediate layer representations of CoLAC sentences in XLM-R

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key syntactic features that distinguish Chinese grammatical acceptability from English, and how do these differences impact cross-lingual transfer of acceptability judgments?
- Basis in paper: [explicit] The paper discusses cross-lingual transfer experiments between English and Chinese, noting that the two languages are typologically different and that certain syntactic phenomena are language-specific.
- Why unresolved: The paper demonstrates that cross-lingual transfer is possible but does not fully characterize which specific syntactic features enable or hinder this transfer. The analysis of syntactic categories shows some differences in model performance across languages, but a deeper investigation into the linguistic mechanisms underlying successful transfer is needed.
- What evidence would resolve it: Detailed linguistic analysis comparing the specific syntactic structures in CoLA and CoLAC, coupled with targeted experiments varying these structures in transfer tasks, would help identify which features are most critical for cross-lingual acceptability judgment.

### Open Question 2
- Question: How does the performance of large language models on acceptability judgments change with model size and training data composition, particularly for typologically distant languages?
- Basis in paper: [explicit] The paper shows that even the largest InstructGPT model performs near chance level on acceptability judgments, while supervised models outperform few-shot approaches. It also explores how training on combined datasets affects performance.
- Why unresolved: While the paper provides initial findings, it doesn't systematically investigate how scaling model size or varying training data composition affects performance on acceptability judgments, especially for non-English languages. The relationship between model capacity, pretraining data, and acceptability judgment ability remains unclear.
- What evidence would resolve it: Systematic scaling experiments varying model size and pretraining data composition, combined with analysis of how these factors affect performance on different syntactic phenomena, would clarify the relationship between model architecture and acceptability judgment capabilities.

### Open Question 3
- Question: What are the limitations of in-context learning for specialized linguistic tasks like acceptability judgment, and how can these limitations be addressed through instruction tuning or other approaches?
- Basis in paper: [explicit] The paper demonstrates that in-context learning fails for acceptability judgments even with ChatGPT, while supervised fine-tuning performs significantly better. This contrasts with in-context learning's success on other NLP tasks.
- Why unresolved: The paper identifies the failure of in-context learning for acceptability judgments but doesn't explore why this occurs or what modifications might improve performance. The relationship between task specificity, instruction quality, and in-context learning effectiveness remains unexplored.
- What evidence would resolve it: Experiments systematically varying instruction templates, in-context example selection, and model architecture for acceptability judgment tasks would help identify the specific limitations of in-context learning for this task and potential solutions.

## Limitations
- The claim that acceptability knowledge originates from pre-training relies heavily on indirect evidence from cross-lingual transfer rather than direct experimental validation
- The paper doesn't explore whether instruction-tuning data contains acceptability examples or whether more specialized few-shot approaches might succeed
- Alternative explanations for cross-lingual transfer (e.g., shared pre-training data, common syntactic features) are not fully ruled out

## Confidence
**High Confidence**: The dataset collection methodology and verification process for CoLAC are well-documented and transparent. The MCC evaluation methodology is standard and clearly specified.

**Medium Confidence**: The claim that multilingual models can transfer acceptability knowledge across typologically distinct languages is supported by experimental results, though alternative explanations are not fully ruled out.

**Low Confidence**: The assertion that acceptability knowledge originates from pre-training rather than fine-tuning relies heavily on indirect evidence from cross-lingual transfer, without direct experimental validation.

## Next Checks
1. **Fine-tuning ablation study**: Fine-tune XLM-R on unrelated tasks (e.g., sentiment analysis) before evaluating on CoLAC to test whether acceptability knowledge is preserved or degraded, providing direct evidence for pre-training origins.

2. **Instruction-tuning data analysis**: Analyze the datasets used for instruction-tuning to determine if they contain acceptability examples, and test whether models trained on instruction-tuning data containing acceptability examples show improved few-shot performance.

3. **Cross-lingual transfer expansion**: Evaluate transfer performance from Chinese to additional languages beyond English (e.g., Japanese, Korean) to test whether the observed transfer generalizes across broader typological distances.