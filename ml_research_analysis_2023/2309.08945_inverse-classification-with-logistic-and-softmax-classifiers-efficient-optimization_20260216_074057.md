---
ver: rpa2
title: 'Inverse classification with logistic and softmax classifiers: efficient optimization'
arxiv_id: '2309.08945'
source_url: https://arxiv.org/abs/2309.08945
tags:
- newton
- psfrag
- replacements
- log10
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of efficiently finding the closest
  input instance to a given one that changes the classifier''s predicted label, a
  problem relevant to counterfactual explanations, adversarial examples, and model
  inversion. The authors focus on two widely used classifiers: logistic regression
  and softmax classifiers.'
---

# Inverse classification with logistic and softmax classifiers: efficient optimization

## Quick Facts
- arXiv ID: 2309.08945
- Source URL: https://arxiv.org/abs/2309.08945
- Reference count: 40
- This paper develops extremely fast iterative methods for inverse classification problems with logistic and softmax classifiers, achieving near-machine precision solutions in milliseconds to around a second.

## Executive Summary
This paper addresses the problem of efficiently finding the closest input instance to a given one that changes a classifier's predicted label, which is relevant to counterfactual explanations, adversarial examples, and model inversion. The authors focus on two widely used classifiers: logistic regression and softmax classifiers. For logistic regression, they derive a closed-form solution, while for softmax classifiers, they develop an extremely fast iterative method based on special properties of the Hessian matrix. Their approach leverages the strong convexity of the objective function and the special structure of the Hessian to achieve near-machine precision solutions in milliseconds to around a second, even for very high-dimensional instances and many classes.

## Method Summary
The paper addresses inverse classification by minimizing an objective function E(x; λ, k) = λ/2 ||x - x||^2 + g_k(x), where the first term is a strongly convex quadratic and g_k(x) is the negative log probability of class k. For logistic regression, a closed-form solution is derived. For softmax classifiers, Newton's method is used with a specially structured Hessian that enables efficient computation through the Sherman-Morrison-Woodbury formula. The algorithm includes a line search for robustness and warm-start initialization across different λ values to accelerate convergence.

## Key Results
- Derived closed-form solution for logistic regression inverse classification
- Developed Newton's method for softmax classifiers exploiting Hessian structure for O(DK) complexity
- Achieved near-machine precision solutions in milliseconds to seconds for high-dimensional problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The softmax Hessian has special structure enabling efficient Newton optimization.
- Mechanism: The Hessian ∇²E(x) = λI + Aᵀ(diag(p) - ppᵀ)A where the second term has rank ≤ K-1, making it low-rank plus diagonal.
- Core assumption: The softmax probabilities are bounded away from 0 and 1 (interior simplex).
- Evidence anchors:
  - [section] Theorem 3.3 proves the Hessian is positive definite with at least D-K+1 eigenvalues equal to λ.
  - [section] Theorem 3.6 shows the inverse can be computed via Sherman-Morrison-Woodbury using only K×K matrix inversion.
  - [corpus] Related work on inverse classification typically uses gradient descent, suggesting this structural insight is novel.
- Break condition: If softmax probabilities approach 0 or 1 (edge of simplex), the low-rank structure weakens and conditioning degrades.

### Mechanism 2
- Claim: Strong convexity guarantees unique global minimizer regardless of λ value.
- Mechanism: The objective E(x; λ, k) = λ∥x-x∥²/2 + gₖ(x) combines a strongly convex quadratic with a convex log-loss.
- Core assumption: λ > 0 and gₖ is convex (true for softmax).
- Evidence anchors:
  - [section] Theorem 3.2 proves E is strongly convex from the sum of convex gₖ and strongly convex quadratic.
  - [section] Theorem 3.8 shows Newton's method converges globally from any starting point.
  - [abstract] Claims solutions reach "nearly machine precision" suggesting strong convexity holds throughout optimization.
- Break condition: If λ = 0, only gₖ remains which may have flat regions; if gₖ is not convex, uniqueness fails.

### Mechanism 3
- Claim: Warm-start across λ values enables efficient solution paths.
- Mechanism: Solution x*(λ) changes slowly with λ, so initializing from previous solution accelerates convergence.
- Core assumption: The optimization landscape varies smoothly with λ parameter.
- Evidence anchors:
  - [section] Figure 4 shows warm-start is 5× faster than cold-start across 100 λ values.
  - [section] Theorem 3.5 indicates Hessian approaches λI for λ ≫ 1 or near minimizer when λ ≪ 1, suggesting smooth behavior.
  - [corpus] No direct corpus evidence, but this is standard practice in parametric optimization.
- Break condition: If λ changes abruptly or problem instances are very different, warm-start provides little benefit.

## Foundational Learning

- Concept: Convex optimization and strong convexity
  - Why needed here: The entire theoretical analysis relies on convexity properties of the objective function.
  - Quick check question: What guarantees does strong convexity provide for optimization algorithms?

- Concept: Newton's method and its convergence properties
  - Why needed here: The paper's main algorithm is Newton's method with specific Hessian structure exploitation.
  - Quick check question: Under what conditions does Newton's method achieve quadratic convergence?

- Concept: Sherman-Morrison-Woodbury formula
  - Why needed here: Used to efficiently compute Hessian inverse without forming D×D matrices.
  - Quick check question: How does this formula reduce computational complexity when one matrix is low-rank?

## Architecture Onboarding

- Component map: Input preprocessing → Softmax classifier parameters → Objective function E(x;λ,k) → Newton optimization with structured Hessian → Solution output
- Critical path: Hessian computation → Inverse Hessian computation via Theorem 3.6 → Newton direction calculation → Line search → Iterate update
- Design tradeoffs: Using Newton's method vs gradient descent trades computational complexity per iteration for faster convergence; warm-start trades memory for speed.
- Failure signatures: Non-convergence indicates λ too small or Hessian ill-conditioning; slow convergence suggests poor initialization or inappropriate λ range.
- First 3 experiments:
  1. Verify Theorem 3.3 by computing eigenvalues of ∇²E(x) for random instances and checking D-K+1 equal λ.
  2. Test warm-start efficiency by solving for increasing λ values with/without initialization and measuring iteration counts.
  3. Compare runtime of closed-form logistic solution vs Newton's method on binary classification variants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Newton's method compare to other optimization methods when solving inverse classification problems for other types of classifiers beyond logistic regression and softmax classifiers?
- Basis in paper: [explicit] The paper focuses on logistic regression and softmax classifiers, and compares Newton's method to other optimization methods (gradient descent, L-BFGS, BFGS, conjugate gradient) on these classifiers. However, it does not explore other types of classifiers.
- Why unresolved: The paper only provides experimental results for logistic regression and softmax classifiers, and does not discuss how the proposed method might perform on other classifiers.
- What evidence would resolve it: Experimental results comparing the performance of Newton's method to other optimization methods on inverse classification problems for a variety of other classifiers, such as decision trees, random forests, or neural networks.

### Open Question 2
- Question: How does the performance of Newton's method scale with the number of classes (K) in the softmax classifier?
- Basis in paper: [explicit] The paper mentions that K is typically much smaller than D (the number of features) in practice, but does not provide detailed analysis of how the performance of Newton's method scales with K.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of how the runtime or convergence properties of Newton's method change as K increases.
- What evidence would resolve it: Experimental results showing the runtime and convergence properties of Newton's method for softmax classifiers with varying numbers of classes, as well as theoretical analysis of the computational complexity and convergence behavior in terms of K.

### Open Question 3
- Question: How does the choice of distance function (e.g., Euclidean distance, Manhattan distance) affect the performance of Newton's method in solving inverse classification problems?
- Basis in paper: [explicit] The paper uses the Euclidean distance as the cost function in the inverse classification problem, but does not explore the impact of using other distance functions.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of how the choice of distance function affects the performance of Newton's method.
- What evidence would resolve it: Experimental results comparing the performance of Newton's method with different distance functions on inverse classification problems, as well as theoretical analysis of how the choice of distance function affects the convexity, smoothness, and conditioning of the objective function.

## Limitations

- Theoretical guarantees rely heavily on strong convexity assumptions that may be violated in edge cases where softmax probabilities approach the simplex boundary.
- The claimed millisecond-to-second runtime guarantees assume standard computational hardware and may not hold for extremely high-dimensional problems.
- Numerical stability for very large-scale problems and behavior with poorly conditioned classifier parameters remain uncertain.

## Confidence

High: Core optimization algorithms for well-conditioned problems
Medium: Performance with extreme parameter values or ill-conditioned Hessian matrices
Low: Numerical stability for very large-scale problems and behavior with poorly conditioned classifier parameters

## Next Checks

1. Verify Hessian eigenvalue structure empirically across diverse problem instances and parameter settings, particularly near decision boundaries.
2. Test algorithm robustness to classifier parameter conditioning and input feature scaling, measuring degradation in convergence rates.
3. Benchmark warm-start efficiency across problem families with varying smoothness properties to identify regimes where the approach breaks down.