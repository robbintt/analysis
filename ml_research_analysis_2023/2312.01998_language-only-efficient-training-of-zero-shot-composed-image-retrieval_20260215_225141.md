---
ver: rpa2
title: Language-only Efficient Training of Zero-shot Composed Image Retrieval
arxiv_id: '2312.01998'
source_url: https://arxiv.org/abs/2312.01998
tags:
- lincir
- image
- training
- cond
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language-only Efficient Training of Zero-shot Composed Image Retrieval
  This paper introduces LinCIR, a novel zero-shot composed image retrieval (ZS-CIR)
  framework that uses language-only training. Unlike existing ZS-CIR methods, which
  require expensive triplet datasets, LinCIR employs a self-supervision technique
  called Self-Masking Projection (SMP) that enables training solely on text inputs.
---

# Language-only Efficient Training of Zero-shot Composed Image Retrieval

## Quick Facts
- arXiv ID: 2312.01998
- Source URL: https://arxiv.org/abs/2312.01998
- Authors: 
- Reference count: 40
- Primary result: Language-only training with Self-Masking Projection (SMP) enables efficient zero-shot composed image retrieval without triplet datasets

## Executive Summary
This paper introduces LinCIR, a zero-shot composed image retrieval (ZS-CIR) framework that uses language-only training via Self-Masking Projection (SMP). Unlike existing ZS-CIR methods requiring expensive triplet datasets, LinCIR achieves remarkable efficiency and performance by training solely on text inputs. The method projects text embeddings to token embedding space, replaces keyword tokens with projected embeddings, and minimizes reconstruction error. LinCIR achieves the best ZS-CIR performances on multiple benchmarks including CIRCO, GeneCIS, FashionIQ, and CIRR, with 48-minute training time using CLIP ViT-G backbone.

## Method Summary
LinCIR trains a projection module using Self-Masking Projection (SMP) that operates entirely on text inputs without requiring image-text pairs or triplet datasets. The method extracts keywords (consecutive adjectives and nouns) from text captions, projects the text embedding to token embedding space, replaces keyword tokens with projected embeddings, and minimizes MSE between original and modified embeddings. Random noise (Unif(0,1) × N(0,1)) is added to textual embeddings to bridge the modality gap. The projection module is trained using AdamW optimizer with learning rate 1e-5 for 50 epochs. The approach uses frozen CLIP or BLIP encoders and demonstrates significant efficiency gains by eliminating visual encoder computations and reducing storage requirements.

## Key Results
- LinCIR with CLIP ViT-G backbone trained in 48 minutes and shows best ZS-CIR performances on CIRCO, GeneCIS, FashionIQ, and CIRR benchmarks
- Achieves state-of-the-art results even outperforming supervised methods on FashionIQ dataset
- Caption storage size of CC3M dataset is only 125 MB compared to 430GB for images (3,400x reduction)
- Textual encoder throughput is ×1.4 times faster than visual encoder throughput

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Masking Projection (SMP) improves zero-shot CIR by forcing the projection module to focus on semantic keywords rather than raw visual tokens
- Mechanism: SMP replaces keyword tokens in the original text with the projected embedding, then minimizes MSE between the original and modified embeddings
- Core assumption: Keywords (consecutive adjectives and nouns) carry most of the semantic weight in a sentence
- Evidence anchors: [abstract] "We project the text latent embedding zc to the token embedding space ec and construct a new text by replacing the keyword tokens of the original text"
- Break condition: If keyword extraction fails in languages without clear POS boundaries

### Mechanism 2
- Claim: Language-only training via SMP is more efficient than image-based training
- Mechanism: By training only on text inputs, LinCIR avoids loading and processing images
- Core assumption: Textual encoder throughput is significantly higher than visual encoder throughput
- Evidence anchors: [abstract] "the caption storage size of CC3M dataset [39] is only 125 MB, while its image storage size is about 430GB"
- Break condition: If visual information is critical for certain CIR tasks

### Mechanism 3
- Claim: Adding random noise to textual embeddings bridges the modality gap between text and image spaces
- Mechanism: Random noise with diverse norm distribution is added to textual embeddings before projection
- Core assumption: The modality gap exists because text and image embeddings occupy different regions of the joint embedding space
- Evidence anchors: [abstract] "We employ a random noise addition strategy [33], carefully choosing a probability distribution that ensures the diversity of the noise-augmented textual embeddings"
- Break condition: If the noise distribution is poorly chosen

## Foundational Learning

- Concept: Vision-language models (VLMs) map images and text to a shared embedding space using separate encoders
  - Why needed here: LinCIR builds on pre-trained VLMs like CLIP and modifies their training approach
  - Quick check question: What are the two main components of a VLM that LinCIR uses without modification?

- Concept: Cross-modal retrieval requires embeddings that are semantically aligned across different modalities
  - Why needed here: The core challenge LinCIR addresses is the modality gap between text and image embeddings
  - Quick check question: Why does adding noise to textual embeddings help when projecting to the image space?

- Concept: Self-supervision techniques can train models without labeled data by creating artificial training signals
  - Why needed here: SMP is a self-supervision method that trains the projection module using only text inputs
  - Quick check question: How does SMP create a training signal without using image-text pairs?

## Architecture Onboarding

- Component map: Textual encoder (frozen CLIP/BLIP) → Projection module (trainable MLP) → CIR inference via text-to-image retrieval
- Critical path: Text input → Textual encoder → Projection module → Token embedding space → Image retrieval
- Design tradeoffs: Language-only training trades visual information for efficiency and scalability; keyword masking trades comprehensive semantic understanding for focused keyword preservation
- Failure signatures: Degraded performance on object-centric queries, poor generalization to unseen domains, modality gap issues
- First 3 experiments:
  1. Compare keyword masking vs. no masking on CIRR dev split to verify SMP effectiveness
  2. Test different noise distributions (Gaussian vs. uniform × Gaussian) on FashionIQ to validate modality gap bridging
  3. Measure training time and storage requirements between language-only and image-based approaches using CC3M dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LinCIR compare when using different types of training corpora?
- Basis in paper: [explicit] The paper explores the impact of different training corpora including CC3M, StableDiffusion Prompts, COYO-700M, and OpenWebText
- Why unresolved: The paper provides comparison but doesn't delve into reasons behind observed differences
- What evidence would resolve it: Detailed analysis of corpus characteristics and their impact on LinCIR's performance

### Open Question 2
- Question: What is the impact of the choice of noise distribution on LinCIR's performance?
- Basis in paper: [explicit] The paper investigates different noise distributions including Gaussian, uniform, exponential, student-t, and chi-square
- Why unresolved: The paper shows performance differences but lacks comprehensive analysis of why certain distributions perform better
- What evidence would resolve it: Detailed study of noise distribution properties and their effect on modality gap bridging

### Open Question 3
- Question: How does LinCIR perform with different types of prompts for zero-shot CIR?
- Basis in paper: [explicit] The paper explores different prompts including "a photo of [$]that [cond]" and "Observe[$]that [cond]"
- Why unresolved: The paper provides comparison but doesn't investigate reasons behind performance differences
- What evidence would resolve it: Detailed analysis of prompt characteristics and their impact on LinCIR's performance

## Limitations

- Keyword masking effectiveness depends on linguistic structure and may not generalize across languages or domains
- Noise injection mechanism lacks theoretical justification beyond empirical observation
- Efficiency claims may not scale to massive datasets or specialized hardware configurations
- Limited analysis of failure cases and edge conditions across different domains and image types

## Confidence

**High Confidence**: Core architectural contribution (SMP with keyword masking) is well-defined and reproducible; efficiency claims regarding storage reduction are mathematically sound; training methodology is clearly specified

**Medium Confidence**: Performance improvements over existing ZS-CIR methods are reported with standard metrics but lack confidence intervals or statistical significance tests; noise injection mechanism shows empirical benefits but lacks rigorous theoretical grounding

**Low Confidence**: Generalizability of keyword extraction across languages and domains is asserted but not empirically validated; claim that language-only training is universally more efficient lacks comprehensive benchmarking across different hardware and dataset scales

## Next Checks

1. **Cross-lingual keyword masking validation**: Test SMP performance on non-English text corpora (e.g., Japanese, Chinese) where POS tagging and consecutive noun/adjective patterns differ significantly from English. Compare against English-only results to quantify language dependency.

2. **Noise distribution ablation study**: Systematically test alternative noise distributions (uniform, Gaussian, Laplacian) across different magnitude scales on multiple benchmarks. Measure both performance impact and sensitivity to distribution parameters to determine robustness.

3. **Efficiency benchmarking at scale**: Compare language-only vs. image-based training across datasets of increasing size (10K, 100K, 1M images) on multiple hardware configurations (CPU, GPU, TPU). Include total cost analysis considering storage, computation time, and energy consumption.