---
ver: rpa2
title: 'TheoremQA: A Theorem-driven Question Answering dataset'
arxiv_id: '2305.12524'
source_url: https://arxiv.org/abs/2305.12524
tags:
- arxiv
- language
- math
- these
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TheoremQA, a new dataset for evaluating AI
  models' ability to apply domain-specific knowledge to solve challenging science
  problems. TheoremQA contains 800 high-quality questions covering 350 theorems from
  Math, Physics, EE&CS, and Finance.
---

# TheoremQA: A Theorem-driven Question Answering dataset

## Quick Facts
- arXiv ID: 2305.12524
- Source URL: https://arxiv.org/abs/2305.12524
- Authors: Wenchuan Wu, Victor Zhong, Li Zhang, Sheng Shen, Peter Schaldenbrand, Victor Zhong, Joshua B. Tenenbaum, Karthik Narasimhan, Shivani Agarwal, Chieko Asakawa, Karthik Narasimhan, Peter Schaldenbrand, Sheng Shen, Li Zhang, Victor Zhong, Wenchuan Wu
- Reference count: 13
- Primary result: GPT-4 achieves 51% accuracy on theorem-driven science problems using Program-of-Thoughts prompting, while all other models score below 15%

## Executive Summary
TheoremQA is a new dataset designed to evaluate AI models' ability to apply domain-specific knowledge to solve challenging science problems. The dataset contains 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance. The authors evaluate 16 large language and code models using Chain-of-Thoughts and Program-of-Thoughts prompting strategies. GPT-4 demonstrates significantly superior performance, achieving 51% accuracy with Program-of-Thoughts prompting, while all other models score below 15%, barely surpassing random guess baseline. The results highlight the need for further enhancement strategies to close the performance gap between GPT-4 and open-source models.

## Method Summary
The authors created TheoremQA by collecting 800 question-theorem-answer triples from university-level science problems across four domains. They standardized answers to five formats (integer, float, list, boolean, multiple choice) to enable automatic evaluation through external tools like WolframAlpha. Sixteen models were evaluated using Chain-of-Thoughts and Program-of-Thoughts prompting, with and without theorem augmentation. The Program-of-Thoughts strategy involved generating executable Python programs to apply theorem knowledge, which were then executed to produce answers.

## Key Results
- GPT-4 achieves 51% accuracy on TheoremQA using Program-of-Thoughts prompting
- All open-source models score below 15% accuracy, barely exceeding random guess baseline
- GPT-4 generates runnable programs 92% of the time with PoT prompting
- Theorem augmentation through simple concatenation shows limited improvement
- Multimodal models do not significantly outperform text-only models on multimodal questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TheoremQA achieves strong evaluation by using domain-expert-curated theorem-problem pairs that enforce answer format standardization
- Mechanism: The dataset design requires all answers to be in one of five formats (integer, float, list, boolean, multiple choice), which enables reliable automatic evaluation via string extraction and normalization through external tools like WolframAlpha
- Core assumption: Standardizing answers to simple formats does not fundamentally alter the reasoning difficulty of theorem application problems
- Evidence anchors:
  - [abstract] "we collected 800 high-quality question-theorem-answer triples as our release version"
  - [section 3] "we instructed domain experts to alter the question so the answer would be limited to the following forms: (1) integer, (2) float, (3) list of integers/ floats, (4) boolean, and (5) multiple-choice options"
  - [corpus] Weak evidence - corpus lacks direct discussion of evaluation methodology
- Break condition: If theorem problems inherently require complex symbolic or diagrammatic answers that cannot be meaningfully reduced to simple formats, the evaluation becomes less representative of true theorem-solving capability

### Mechanism 2
- Claim: GPT-4's superior performance on TheoremQA stems from its ability to generate executable Python programs that correctly apply theorem knowledge
- Mechanism: The Program-of-Thoughts prompting strategy allows GPT-4 to produce Python code that encodes the theorem application logic, which is then executed to produce answers, reducing error propagation from text-based reasoning
- Core assumption: Symbolic execution through code generation provides more reliable reasoning than pure text chain-of-thought for mathematical theorem application
- Evidence anchors:
  - [abstract] "GPT-4’s capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting"
  - [section 5.2] "By delegating computational tasks to an external executor, the problem-solving process is considerably enhanced in its reliability"
  - [section 5.2] "GPT-4 is extremely accurate in generating programs, where 92% of the generated programs are runnable"
- Break condition: If the theorem application requires reasoning that cannot be easily encoded in Python code, or if the execution environment introduces errors, the advantage of PoT may diminish

### Mechanism 3
- Claim: TheoremQA reveals a performance gap between GPT-4 and open-source models that reflects differences in science knowledge encoding rather than just model scale
- Mechanism: The dataset's focus on university-level theorem application exposes limitations in how open-source models encode scientific knowledge, as evidenced by their sub-15% accuracy despite being scaled to 13-16B parameters
- Core assumption: The performance gap is primarily due to differences in science knowledge representation rather than other factors like instruction tuning quality or inference efficiency
- Evidence anchors:
  - [abstract] "All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline"
  - [section 5.2] "All open-source, instruction-tuned language and code models scored below 15% in accuracy, barely exceeding the random guess baseline of 10%"
  - [corpus] Weak evidence - corpus lacks comparative analysis of model architectures or training approaches
- Break condition: If open-source models can achieve similar performance through targeted fine-tuning on theorem-specific data, the gap may be attributed to data rather than inherent knowledge encoding differences

## Foundational Learning

- Concept: Domain-specific theorem knowledge integration
  - Why needed here: TheoremQA requires models to apply specific mathematical and scientific theorems rather than perform general arithmetic, demanding knowledge integration beyond standard language modeling
  - Quick check question: What distinguishes theorem application from general math problem solving in terms of required knowledge representation?

- Concept: Program-of-Thoughts reasoning framework
  - Why needed here: The dataset's complexity benefits from breaking problems into executable code segments that can symbolically verify theorem applications, reducing reasoning errors
  - Quick check question: How does generating Python code as intermediate reasoning differ from text-based chain-of-thought in terms of error propagation?

- Concept: Multimodal diagram interpretation limitations
  - Why needed here: Some theorem problems include visual inputs that current captioning approaches cannot adequately represent, limiting multimodal model performance
  - Quick check question: Why might standard visual encoding methods fail to capture the information needed for theorem-based diagram problems?

## Architecture Onboarding

- Component map: Data pipeline → Prompt template (CoT/PoT) → Model inference → Answer extraction (ChatGPT + WolframAlpha) → Evaluation; optional theorem augmentation → Model; optional multimodal captioning → Model
- Critical path: Theorem question → Prompt generation → Model output → Answer extraction → Score comparison
- Design tradeoffs: Answer format standardization simplifies evaluation but may reduce problem authenticity; code generation improves reliability but requires execution environment; theorem augmentation has limited benefit with simple concatenation
- Failure signatures: High program generation failure rates indicate reasoning breakdown; multimodal captioning loss causes accuracy drops; answer extraction failures suggest format mismatch
- First 3 experiments:
  1. Test CoT vs PoT prompting on a subset of problems to measure program executability and accuracy gains
  2. Evaluate answer extraction pipeline on model outputs with varied formatting to ensure robustness
  3. Compare theorem-augmented generation with and without structured theorem representation to test augmentation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific pre-training or fine-tuning strategies could effectively close the performance gap between GPT-4 and open-source models on theorem-driven problems?
- Basis in paper: [explicit] The paper notes that all open-source models score below 15% accuracy, while GPT-4 achieves 51%, suggesting a need for science-focused pre-training or fine-tuning.
- Why unresolved: The paper identifies the gap but does not explore specific strategies for science-focused pre-training or fine-tuning that could improve open-source models.
- What evidence would resolve it: Experiments comparing different pre-training or fine-tuning approaches focused on science and theorem-based reasoning would provide evidence.

### Open Question 2
- Question: How can multimodal inputs, such as diagrams, be better integrated into large language models to improve performance on science questions?
- Basis in paper: [explicit] The paper finds that multimodal models do not significantly outperform text-only models on multimodal questions, suggesting current visual encoding modules are inadequate.
- Why unresolved: The paper does not investigate alternative visual encoding methods or integration strategies that could better handle diagrammatic inputs.
- What evidence would resolve it: Testing alternative visual encoding techniques or multimodal integration methods on the same dataset would provide evidence.

### Open Question 3
- Question: What advanced theorem-augmented generation strategies could improve the performance of large language models on theorem-driven problems?
- Basis in paper: [explicit] The paper attempts simple concatenation of theorem descriptions but finds limited improvement, suggesting more complex integration strategies may be needed.
- Why unresolved: The paper does not explore more sophisticated theorem integration methods beyond simple concatenation.
- What evidence would resolve it: Evaluating advanced theorem integration strategies, such as structured encoding or reasoning-aware augmentation, would provide evidence.

## Limitations

- The five-answer-format constraint may artificially simplify theorem problems and exclude complex reasoning tasks
- The attribution of performance gaps to knowledge encoding differences lacks controlled experiments with fine-tuned open-source models
- Multimodal diagram interpretation limitations are not thoroughly explored with alternative visual encoding methods

## Confidence

- **High confidence**: Experimental setup is clearly specified with concrete numbers (800 questions, 350 theorems, 16 models tested, specific accuracy percentages)
- **Medium confidence**: GPT-4's superior performance with Program-of-Thoughts prompting is supported by data, but lacks analysis across problem types
- **Low confidence**: Attribution of open-source model performance gap primarily to science knowledge encoding differences, lacking comparative architecture analysis

## Next Checks

1. **Format Constraint Analysis**: Systematically test whether problems requiring complex symbolic or diagrammatic answers are being excluded or artificially simplified through the five-answer-format constraint

2. **Error Pattern Investigation**: Analyze the 8% of GPT-4's program generation failures and error patterns in open-source models' sub-15% accuracy to determine whether failures stem from reasoning breakdowns, execution errors, or prompt misinterpretation

3. **Knowledge Encoding Isolation**: Conduct controlled experiments where open-source models are fine-tuned on theorem-specific data to determine whether the performance gap with GPT-4 can be closed through targeted training