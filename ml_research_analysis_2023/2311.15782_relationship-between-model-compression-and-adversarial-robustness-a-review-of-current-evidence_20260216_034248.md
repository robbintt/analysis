---
ver: rpa2
title: 'Relationship between Model Compression and Adversarial Robustness: A Review
  of Current Evidence'
arxiv_id: '2311.15782'
source_url: https://arxiv.org/abs/2311.15782
tags:
- robustness
- pruning
- networks
- adversarial
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review analyzes the relationship between model compression
  techniques (pruning and quantization) and adversarial robustness in deep neural
  networks. The authors examined 13 relevant publications, finding that while naive
  compression can reduce robustness, careful application can preserve or even improve
  it.
---

# Relationship between Model Compression and Adversarial Robustness: A Review of Current Evidence

## Quick Facts
- **arXiv ID**: 2311.15782
- **Source URL**: https://arxiv.org/abs/2311.15782
- **Reference count**: 40
- **Key outcome**: Model compression can preserve or enhance adversarial robustness when combined with adversarial training, but excessive compression reduces robustness earlier than accuracy

## Executive Summary
This review analyzes the relationship between model compression techniques (pruning and quantization) and adversarial robustness in deep neural networks. The authors examined 13 relevant publications and found that while naive compression can reduce robustness, careful application can preserve or even improve it. Quantization combined with adversarial training generally showed positive effects on robustness, while pure quantization without training had mixed results. For pruning, most studies found that moderate compression levels can maintain or enhance robustness, especially when comparing compressed models to compact models of similar size. The review concludes that combining model compression with adversarial training is possible but requires balancing three competing objectives: compression ratio, accuracy, and robustness.

## Method Summary
The review analyzes existing experimental results from 13 published works examining the effects of model compression on adversarial robustness. The studies are categorized based on whether quantization or pruning was applied, whether adversarial training was used, and the observed effects on robustness. The analysis covers various architectures (LeNet, ResNet, VGG, Wide ResNet, MobileNet) and datasets (MNIST, CIFAR-10, ImageNet, Cityscapes, SynPeDS), with evaluation metrics including adversarial robustness under various attacks (PGD, FGSM, C&W, etc.), accuracy, and compression ratio.

## Key Results
- Quantization combined with adversarial training generally showed positive effects on robustness
- Moderate pruning levels can maintain or enhance robustness, especially when comparing compressed models to compact models of similar size
- Excessive compression reduces robustness earlier than it affects accuracy
- Retraining after compression is crucial for maintaining or improving robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model compression can preserve or even enhance adversarial robustness when combined with adversarial training
- Mechanism: Adversarial training adapts the model to resist adversarial perturbations, and when combined with compression, the resulting smaller model maintains this resistance. The compressed model retains the robust decision boundaries learned during adversarial training while being more efficient
- Core assumption: Adversarial training effectively transfers to compressed models, and compression doesn't destroy the learned robust features
- Evidence anchors: "Quantization combined with adversarial training generally showed positive effects on robustness" (abstract)

### Mechanism 2
- Claim: Moderate pruning levels can maintain or enhance robustness, especially when comparing compressed models to compact models of similar size
- Mechanism: Pruning removes redundant or less important parameters, potentially eliminating weak points in the network that adversarial examples exploit. When comparing models of similar size (one compressed, one originally compact), the compressed model may retain more useful features
- Core assumption: Not all parameters are equally important for robustness, and pruning can remove vulnerable components while preserving essential robust features
- Evidence anchors: "For pruning, most studies found that moderate compression levels can maintain or enhance robustness, especially when comparing compressed models to compact models of similar size" (abstract)

### Mechanism 3
- Claim: Retraining after compression is crucial for maintaining or improving robustness
- Mechanism: After compression (pruning or quantization), retraining allows the model to adapt to the new parameter space and rediscover robust decision boundaries. Without retraining, the compressed model may not fully utilize its remaining capacity for robustness
- Core assumption: Retraining enables the model to compensate for information loss from compression and rediscover robust features in the compressed parameter space
- Evidence anchors: "Both papers observing positive effects [of pruning on robustness] have used retraining – this confirms again that omitted retraining strongly weakens robustness" (section)

## Foundational Learning

- **Concept**: Adversarial training (AT)
  - Why needed here: Understanding how adversarial training works is essential to grasp why combining it with compression can preserve or enhance robustness
  - Quick check question: What is the core principle of adversarial training, and how does it differ from standard training?

- **Concept**: Model compression techniques (pruning and quantization)
  - Why needed here: The review analyzes how these specific compression methods affect adversarial robustness, so understanding their mechanisms is crucial
  - Quick check question: What are the key differences between pruning and quantization in terms of how they reduce model size?

- **Concept**: Trade-offs between compression ratio, accuracy, and robustness
  - Why needed here: The review concludes that combining compression with adversarial training requires balancing these three competing objectives
  - Quick check question: Why might a model lose robustness before accuracy when subjected to excessive compression?

## Architecture Onboarding

- **Component map**: Model compression techniques (pruning, quantization) → Adversarial training methods → Evaluation metrics for robustness → Experimental frameworks comparing different combinations
- **Critical path**: Compression → Evaluation of robustness → Comparison with baseline models → Analysis of trade-offs between compression, accuracy, and robustness
- **Design tradeoffs**: Balancing compression ratio against robustness preservation, choosing between different compression methods, deciding when to apply adversarial training (before, after, or during compression)
- **Failure signatures**: Reduced robustness without accuracy improvement, inability to maintain robustness beyond certain compression thresholds, inconsistent results across different model architectures
- **First 3 experiments**:
  1. Apply quantization to a trained model and evaluate robustness against FGSM and PGD attacks, comparing to the full-precision baseline
  2. Apply moderate pruning to an adversarially trained model, then retrain and evaluate robustness, comparing to both the original and a compact model of similar size
  3. Combine quantization with adversarial training from scratch, evaluating robustness at different quantization levels (1-, 2-, 3-bit activations) against multiple attack methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does network pruning combined with adversarial training consistently improve robustness across different architectures and datasets, or are the observed benefits highly architecture-specific?
- Basis in paper: Multiple papers report contradictory findings on pruning's effect on robustness, with some showing improvements and others showing degradation
- Why unresolved: Studies use different architectures, datasets, pruning methods, and attack types, making it difficult to isolate the effect of pruning on robustness
- What evidence would resolve it: Systematic experiments comparing the same pruning method across multiple architectures and datasets with standardized attack evaluation

### Open Question 2
- Question: What is the fundamental relationship between model compression (particularly quantization) and adversarial robustness - does compression inherently reduce robustness or can it be beneficial when properly implemented?
- Basis in paper: "Throughout all experiments, it was shown that naive pruning and quantization can reduce robustness" but "as long as networks are compressed within certain limits, pruning may preserve or even improve robustness"
- Why unresolved: Evidence shows both positive and negative effects depending on implementation details and whether adversarial training is used
- What evidence would resolve it: Comprehensive study comparing different compression methods (quantization, pruning) with and without adversarial training across multiple threat models

### Open Question 3
- Question: Is there a universal compression-robustness trade-off curve, or does the relationship vary based on factors like network architecture, dataset characteristics, or attack methodology?
- Basis in paper: "They observed that with the training procedures applied, a model cannot be both highly robust and pruned simultaneously" and "a trade-off exists between compression ratio, accuracy, and robustness"
- Why unresolved: Different studies observe different points of diminishing returns and trade-off shapes
- What evidence would resolve it: Large-scale empirical study mapping robustness vs. compression across diverse conditions to identify generalizable patterns or architecture-specific relationships

## Limitations

- Analysis based on a relatively small sample of 13 publications, limiting generalizability
- Relies on reported results without access to raw experimental data, potentially missing implementation details
- Focuses primarily on two compression methods (pruning and quantization) while other techniques remain unexplored
- Doesn't account for differences in attack configurations, evaluation protocols, or model architectures across studies

## Confidence

- **High**: The finding that excessive compression reduces robustness earlier than accuracy is well-supported across multiple studies
- **Medium**: Claims about quantization combined with adversarial training showing positive effects are supported but based on limited experimental variations
- **Medium**: The importance of retraining after compression for maintaining robustness is supported but requires more systematic investigation

## Next Checks

1. Conduct controlled experiments varying compression ratios systematically while keeping all other factors constant to establish precise robustness-accuracy trade-off curves
2. Test the transferability of adversarial training benefits to compressed models across different architectures (CNNs, Transformers) and datasets
3. Evaluate the impact of compression order (compression before vs. after adversarial training) on final robustness levels using identical experimental conditions