---
ver: rpa2
title: 'Document Layout Annotation: Database and Benchmark in the Domain of Public
  Affairs'
arxiv_id: '2306.10046'
source_url: https://arxiv.org/abs/2306.10046
tags:
- text
- documents
- document
- layout
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a semi-automatic procedure to annotate digital
  documents with layout information. The method combines web scraping, PDF miner tools,
  and human-assisted data curation to detect and classify basic document components,
  including text blocks with 4 semantic categories (identifier, title, summary, body).
---

# Document Layout Annotation: Database and Layout Analysis in the Domain of Public Affairs

## Quick Facts
- arXiv ID: 2306.10046
- Source URL: https://arxiv.org/abs/2306.10046
- Reference count: 23
- Key outcome: 37.9K documents with 441K pages annotated with layout information, achieving up to 99% text classification accuracy using font and positional features

## Executive Summary
This work presents a semi-automatic procedure to annotate digital documents with layout information, focusing on Spanish official gazettes. The method combines web scraping, PDF miner tools, and human-assisted data curation to detect and classify basic document components including text blocks with four semantic categories. The resulting Public Affairs Layout (PAL) database contains over 441K pages with 8M layout labels, providing a benchmark for document layout analysis in the legal domain and a large corpus of Spanish public affairs text for NLP applications.

## Method Summary
The method uses a Human-in-the-loop AI-based Data Curation process to create layout annotations. It starts with heuristic rules based on font features to create initial noisy labels, then uses human validation to correct errors. These validated documents train Random Forest classifiers, which label new documents in an iterative feedback loop. The approach uses PyMuPDF and Camelot to extract layout information including font characteristics and positional features, which form feature vectors for each text block. Documents are split at document level to prevent intra-document biases.

## Key Results
- PAL database contains 37.9K documents with 441K pages and 8M layout labels
- Text labeling approach achieves up to 99% accuracy using Random Forest classifiers
- 24 diverse Spanish administrative sources represented with varying performance (61.08% to 99.97% accuracy)
- Font and positional features successfully discriminate between semantic text categories

## Why This Works (Mechanism)

### Mechanism 1
Font and positional features extracted from PDF miner tools can accurately discriminate between semantic text categories (identifier, title, summary, body). The method uses PyMuPDF to extract layout information including font characteristics (size, bold, italic, type) and positional features (bounding box coordinates, page position). These features form a feature vector for each text block that captures visual and spatial cues indicating semantic role. The core assumption is that text blocks with different semantic roles have distinct visual characteristics and spatial positions preserved in the PDF structure.

### Mechanism 2
A Human-in-the-loop AI-based Data Curation process can efficiently create a large annotated dataset from initially noisy rule-based annotations. The method starts with heuristic rules based on font features to create initial noisy labels, then uses human validation to correct errors. These validated documents train a Random Forest classifier, which is used to label new documents, creating a feedback loop where more validated data leads to better models. The core assumption is that initial heuristic rules provide sufficient signal to bootstrap the human validation process.

### Mechanism 3
Document-level train/test splitting prevents intra-document biases from inflating performance metrics. Instead of splitting at page level, the method splits at document level, ensuring that all pages from the same document are in either train or test set, preventing the model from learning document-specific layout patterns that wouldn't generalize. The core assumption is that documents from the same source may have consistent intra-document layout patterns that would cause overfitting if pages were split independently.

## Foundational Learning

- **PDF structure and coordinate systems**: Understanding how PDF miners extract layout information requires knowledge of PDF internal structure and coordinate systems (origin at top-left, bounding box representation). Quick check: What is the origin point of the coordinate system used by PyMuPDF for bounding boxes?

- **Text block semantic categorization**: The method defines 4 text categories (identifier, title, summary, body) based on their semantic role in official gazettes, requiring understanding of document structure and layout conventions. Quick check: How does the semantic role of a text block (e.g., title vs body) typically manifest in its visual presentation?

- **Random Forest classification principles**: The text labeling classifier uses Random Forest, which requires understanding of ensemble methods, feature importance, and how hierarchical decision logic applies to font-based classification. Quick check: Why might Random Forest be particularly suitable for classifying text blocks based on font and positional features?

## Architecture Onboarding

- **Component map**: Web scraping layer (Scrapy) → PDF miner layer (PyMuPDF, Camelot) → Feature extraction → Initial heuristic labeling → Human validation interface → Random Forest classifier training → Iterative refinement loop → Database layer storing documents, extracted features, labels, and metadata

- **Critical path**: 1) Web scraping collects PDF documents from 24 Spanish administrative sources, 2) PDF miners extract layout components with features, 3) Heuristic rules create initial text block labels, 4) Human validator corrects errors through interactive interface, 5) Random Forest classifier trained on validated data, 6) Classifier labels remaining documents, 7) Models validated and results reported

- **Design tradeoffs**: Using font features vs raw text (font features are faster and generalize across languages but may miss semantic content), document-level vs page-level splitting (document-level prevents overfitting but reduces training data), single vs multiple human validators (single ensures consistency but may introduce individual bias)

- **Failure signatures**: Low accuracy on certain sources (e.g., Source16 with 61.08% on summaries) indicates source-specific layout patterns not captured by features, high variance in performance (e.g., Source6 and Source22 with std > 3) suggests inconsistent labeling or feature extraction issues, documents with missing font information will have incomplete feature vectors

- **First 3 experiments**: 1) Run feature extraction on a small sample from each source to verify PDF miner compatibility and identify any sources with missing font information, 2) Apply heuristic rules to create initial labels and manually inspect a few documents to assess rule quality and identify edge cases, 3) Train a baseline Random Forest on 10% of validated data from one source and evaluate per-class accuracy to identify which categories are most challenging to classify

## Open Questions the Paper Calls Out

### Open Question 1
How does the PAL database compare to existing document layout analysis datasets in terms of size, diversity, and annotation quality? The authors claim PAL is the largest dataset for document layout analysis, with 37.9K documents and 441K pages, and that it includes diverse layouts from 24 Spanish administrative sources. However, they do not directly compare it to other datasets like PubLayNet or ICDAR datasets. A detailed comparison with other major DLA datasets in terms of scale, annotation granularity, and performance of state-of-the-art models would resolve this.

### Open Question 2
How effective are the proposed layout features (font size, bold/italic proportions, positional features) for text block classification compared to more advanced features like semantic embeddings or visual features? The authors use font and positional features and report high accuracy (up to 99%) but do not explore potential benefits of incorporating semantic or visual features. Experiments comparing the proposed feature set with models that incorporate semantic embeddings or visual features would resolve this.

### Open Question 3
How well does the PAL database generalize to documents from other domains (e.g., scientific articles, invoices, or receipts) or languages beyond Spanish and its co-official languages? The authors focus on official gazettes from Spanish administrations but do not test the generalizability of the database or models to other document types or languages. Experiments applying models trained on PAL to documents from other domains or languages would resolve this.

## Limitations

- The method relies heavily on PDF font features being consistently available and meaningful across sources, yet several sources lack this information entirely
- The heuristic rules for initial labeling are minimally specified beyond the 0.5 bold token threshold for titles
- The human validation process is described but not quantified in terms of inter-annotator agreement or validation time

## Confidence

- **High**: The overall framework of semi-automatic annotation with human-in-the-loop validation is sound and well-documented
- **Medium**: The 99% accuracy claim is supported by experimental results but may be inflated by document-level splitting and careful source selection
- **Low**: The generalizability of font-based features across different PDF generation methods and languages remains unproven

## Next Checks

1. **Feature Extraction Robustness**: Test the PDF miner tools on documents from Source4 and other sources missing font information to quantify the impact on feature completeness

2. **Heuristic Rule Coverage**: Manually evaluate the initial heuristic labeling on 50 randomly selected documents to identify edge cases and rule failures before human validation

3. **Cross-Source Generalization**: Train a single Random Forest model on pooled data from 3-4 sources and evaluate on held-out documents from each source to test layout pattern generalization