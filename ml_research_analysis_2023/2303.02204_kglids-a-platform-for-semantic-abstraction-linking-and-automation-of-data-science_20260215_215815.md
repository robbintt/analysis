---
ver: rpa2
title: 'KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data
  Science'
arxiv_id: '2303.02204'
source_url: https://arxiv.org/abs/2303.02204
tags:
- data
- kglids
- graph
- pipeline
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KGLiDS is a platform that employs machine learning and knowledge
  graph technologies to capture and share the semantics of data science artifacts.
  KGLiDS abstracts data science pipelines by combining static code analysis with documentation
  and dataset usage analyses to have a rich semantic abstraction that captures the
  essential concepts in data science pipelines.
---

# KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science

## Quick Facts
- arXiv ID: 2303.02204
- Source URL: https://arxiv.org/abs/2303.02204
- Reference count: 40
- KGLiDS outperforms state-of-the-art in pipeline abstraction and data discovery with superior precision, recall, and scalability

## Executive Summary
KGLiDS is a knowledge graph platform that captures and shares semantics of data science artifacts through machine learning and knowledge graph technologies. The system abstracts data science pipelines by combining static code analysis with documentation and dataset usage analyses, creating rich semantic representations. For datasets, KGLiDS employs novel methods to generate column embeddings and discover relationships between columns and tables. The platform demonstrates superior performance compared to existing approaches in pipeline abstraction, data discovery, and system scalability.

## Method Summary
KGLiDS employs a three-component architecture: the KG Governor performs pipeline abstraction using static code analysis combined with library documentation and dataset usage analysis; Data Profiling generates column embeddings via deep learning models and infers fine-grained types; and Knowledge Graph Construction builds an RDF-based LiDS graph with SPARQL query support. The system processes pipeline scripts to extract code flow, data flow, and control flow, enriches this with documentation analysis, and links pipelines to datasets through usage patterns. Column embeddings capture content similarities more accurately than hand-crafted metadata features, while the RDF knowledge graph enables interoperability and powerful querying capabilities.

## Key Results
- KGLiDS outperforms state-of-the-art systems in pipeline abstraction by capturing dataset and library semantics previously unrepresented
- Achieves significantly better precision and recall than existing approaches for data discovery tasks including table relatedness and join path prediction
- Demonstrates superior performance and scalability compared to related systems, with efficient multi-machine deployment capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining static code analysis with documentation and dataset usage analysis enables rich semantic abstraction of data science pipelines.
- Mechanism: Static code analysis extracts code flow, data flow, and control flow from pipeline scripts. Documentation analysis enriches this by inferring parameter names, values, and return types from library documentation. Dataset usage analysis predicts which tables and columns are read by statements, enabling linking pipeline graphs to dataset graphs.
- Core assumption: Static analysis alone is insufficient for dynamic languages like Python, but combining it with documentation and usage analysis compensates for this limitation.
- Evidence anchors:
  - [abstract] "KGLiDS abstracts data science pipelines by combining static code analysis with documentation and dataset usage analyses to have a rich semantic abstraction that captures the essential concepts in data science pipelines."
  - [section] "Static analysis is not sufficient to capture all semantics of a pipeline. For instance, it cannot detect that pd.read_csv() in line 6 in Figure 3 returns a Pandas DataFrame object. In Algorithm 1, we enrich the static program analysis using the documentation of data science libraries (lines 8 to 13)."
  - [corpus] No direct corpus evidence for this specific mechanism. Weak evidence from related papers on static analysis limitations in dynamic languages.
- Break condition: If library documentation is incomplete or unavailable, or if pipeline scripts use highly dynamic constructs that evade static analysis, the semantic abstraction will be incomplete or inaccurate.

### Mechanism 2
- Claim: Column embeddings generated using deep learning models capture content similarities more accurately than hand-crafted metadata features.
- Mechanism: KGLiDS uses a deep learning model to generate column learned representations (CoLR) based on fine-grained column types and actual values. These embeddings capture similarities between column values, distributions, and measurements even with different distributions. Column embeddings are compared pairwise for similarity prediction.
- Core assumption: Deep learning models trained on column value pairs can learn meaningful representations that capture column similarities better than traditional metadata features.
- Evidence anchors:
  - [section] "KGLiDS utilizes an advanced data profiler empowered by machine learning to analyze data items, including datasets, tables, and columns."
  - [section] "Higher accuracy of predicted column content similarities in contrast to hand-crafted meta-features, which have been shown to fail when a column distribution does not match the designed features [20, 28]."
  - [corpus] No direct corpus evidence for this specific mechanism. Weak evidence from related papers on deep learning for column type detection.
- Break condition: If the deep learning model is not well-trained on diverse datasets, or if the column values are too sparse or noisy, the embeddings may not capture meaningful similarities.

### Mechanism 3
- Claim: RDF-based knowledge graph technology enables interoperability, schema-agnosticism, and powerful querying capabilities for the LiDS graph.
- Mechanism: KGLiDS uses the RDF model to store the LiDS graph, enabling interoperability through standardized vocabularies, modularity through named graphs, schema-agnosticism for evolving platforms, and SPARQL for federated query processing. RDF-star supports annotating edges with metadata like similarity scores.
- Core assumption: RDF-based knowledge graph technology provides the necessary features for a scalable and interoperable data science platform.
- Evidence anchors:
  - [section] "KGLiDS uses RDF-based knowledge graph technology because (i) it already includes the formalization of rules and metadata using a controlled vocabulary for the labels in the graphs ensuring interoperability [13, 37], (ii) it has built-in notions of modularity in the form of named graphs, for instance, each pipeline is abstracted in its own named graph [10], and (iii) it is schema-agnostic, allowing the platform to support reasoning and semantic manipulation, e.g., adding new labeled edges between equivalent artifacts, as the platform evolves [9, 14, 39], and (iv) it has a powerful query language (SPARQL) to support federated query processing [4, 27, 35]."
  - [corpus] No direct corpus evidence for this specific mechanism. Weak evidence from related papers on RDF and knowledge graphs for data integration.
- Break condition: If the RDF storage engine cannot scale to handle the size of the LiDS graph, or if SPARQL queries become too complex or slow, the platform's performance will degrade.

## Foundational Learning

- Concept: RDF and SPARQL
  - Why needed here: KGLiDS uses RDF to store the LiDS graph and SPARQL to query it. Understanding these technologies is essential for working with the platform's core data structures and interfaces.
  - Quick check question: What is the difference between an RDF resource and an RDF property?

- Concept: Static code analysis and dynamic languages
  - Why needed here: KGLiDS uses static code analysis to extract semantics from pipeline scripts, but static analysis is challenging for dynamic languages like Python. Understanding the limitations and techniques for static analysis in dynamic languages is crucial for interpreting the platform's results.
  - Quick check question: What are the main challenges of performing static code analysis on Python scripts compared to Java?

- Concept: Deep learning for column embeddings
  - Why needed here: KGLiDS uses deep learning models to generate column embeddings that capture content similarities. Understanding the principles and applications of deep learning for column embeddings is necessary for working with the platform's data profiling component.
  - Quick check question: How does a deep learning model learn to generate embeddings that capture column similarities?

## Architecture Onboarding

- Component map: KG Governor -> KGLiDS Storage -> KGLiDS Interfaces
- Critical path: Pipeline Abstraction -> Data Profiling -> Knowledge Graph Construction -> KGLiDS Storage -> KGLiDS Interfaces
- Design tradeoffs: KGLiDS trades off accuracy for scalability by using static code analysis combined with documentation and usage analysis instead of dynamic analysis. It also trades off precision for coverage by using deep learning models for column embeddings instead of hand-crafted metadata features.
- Failure signatures: Incomplete or inaccurate semantic abstraction of pipeline scripts, poor column similarity predictions, slow or unresponsive queries against the LiDS graph.
- First 3 experiments:
  1. Run the Pipeline Abstraction component on a small set of sample pipeline scripts and inspect the generated pipeline graphs.
  2. Use the Data Profiling component to profile a sample dataset and inspect the generated column embeddings and metadata.
  3. Query the LiDS graph using SPARQL to find pipelines that use a specific library or dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the CoLR models for column embeddings compare when trained on datasets from different domains (e.g., healthcare vs. economics)?
- Basis in paper: [explicit] The paper mentions training CoLR models on 5,500 tables from Kaggle and OpenML, covering various domains.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of CoLR models varies across different data domains or the impact of domain-specific characteristics on the quality of column embeddings.
- What evidence would resolve it: A comparative evaluation of CoLR model accuracy on datasets from diverse domains, along with an analysis of how domain characteristics affect embedding quality.

### Open Question 2
- Question: What is the impact of the RDF-star extension on query performance and storage requirements compared to standard RDF?
- Basis in paper: [explicit] The paper mentions using RDF-star to annotate edges with metadata like similarity scores.
- Why unresolved: The paper does not provide quantitative data on how RDF-star affects system performance or storage overhead compared to traditional RDF.
- What evidence would resolve it: Benchmark results comparing query performance and storage usage between RDF-star and standard RDF implementations in the context of the LiDS graph.

### Open Question 3
- Question: How does the system handle schema evolution when new datasets with different structures are added to the knowledge graph?
- Basis in paper: [inferred] The paper mentions incremental maintenance of the LiDS graph but doesn't detail schema evolution handling.
- Why unresolved: The paper doesn't explain how the system adapts to changes in dataset schemas over time or how it manages inconsistencies when integrating new data sources.
- What evidence would resolve it: A description of the schema evolution mechanism, including how the system detects schema changes and updates the knowledge graph accordingly.

### Open Question 4
- Question: What is the computational complexity of the pairwise column similarity calculations, and how does it scale with the number of columns?
- Basis in paper: [explicit] The paper mentions pairwise comparisons between columns of the same type in Algorithm 3.
- Why unresolved: The paper doesn't provide a detailed complexity analysis or empirical data on how the pairwise comparison step scales with large numbers of columns.
- What evidence would resolve it: A formal analysis of the time and space complexity of the pairwise comparison algorithm, along with empirical benchmarks showing scaling behavior.

## Limitations
- Claims about outperforming state-of-the-art systems lack independent validation and detailed comparisons on standardized datasets
- The scalability of the RDF-based approach for production-scale deployments has not been thoroughly tested
- Performance on edge cases and failure modes is not well-documented

## Confidence
- **High Confidence**: The architectural design combining static analysis, documentation mining, and dataset usage analysis is well-justified and technically sound. The use of RDF for knowledge graph storage follows established best practices.
- **Medium Confidence**: Performance claims on precision/recall benchmarks are supported by experimental results, but the small sample sizes and specific benchmark characteristics limit generalizability.
- **Low Confidence**: Claims about outperforming state-of-the-art systems lack independent verification and detailed comparisons on standardized datasets.

## Next Checks
1. **Scalability Test**: Deploy KGLiDS on a multi-machine cluster with datasets exceeding 100,000 tables to verify performance claims under realistic production loads and identify bottlenecks in RDF storage or SPARQL query processing.

2. **Cross-Ecosystem Validation**: Apply KGLiDS to pipeline repositories from GitHub and other platforms beyond Kaggle to assess generalizability across different data science workflows, coding styles, and library usage patterns.

3. **Robustness Analysis**: Systematically test KGLiDS' pipeline abstraction and data profiling components on edge cases including dynamic code constructs (eval, exec), highly unstructured data, and scripts with incomplete documentation to identify failure modes and error handling capabilities.