---
ver: rpa2
title: Adversarial Attacks to Reward Machine-based Reinforcement Learning
arxiv_id: '2311.09014'
source_url: https://arxiv.org/abs/2311.09014
tags:
- agent
- reward
- state
- agents
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis presents the first analysis of security implications
  for Reward Machine (RM)-based reinforcement learning (RL). RMs, an automata-based
  formalism, enhance RL agents by exposing task structure, improving sample efficiency,
  especially in partially observable environments.
---

# Adversarial Attacks to Reward Machine-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.09014
- Source URL: https://arxiv.org/abs/2311.09014
- Reference count: 0
- One-line primary result: First analysis of security implications for Reward Machine-based reinforcement learning agents, demonstrating that blinding attacks can significantly reduce agent performance in partially observable environments.

## Executive Summary
This thesis presents the first analysis of security implications for Reward Machine (RM)-based reinforcement learning (RL). RMs, an automata-based formalism, enhance RL agents by exposing task structure, improving sample efficiency, especially in partially observable environments. The work introduces blinding attacks, a novel class of attacks that manipulate the agent's labeling function to alter RM state dynamics, thereby degrading agent performance. The proposed attacks target the agent's perception of high-level events, leading to desynchronization between the RM state and the actual environment state.

Experimental evaluation across three partially observable benchmark domains (Cookie World, Keys World, and Symbol World) demonstrates the effectiveness of blinding attacks. Results show that blinding attacks can significantly reduce agent performance, with some achieving high failure rates. However, the attacks often require substantial tampering, limiting their real-world applicability. The thesis also explores event-blinding and edge-blinding variations, with edge-blinding attacks showing promising results with lower tampering rates in some scenarios. Overall, the work highlights the security vulnerabilities of RM-based RL and suggests directions for future research, including more sophisticated attack strategies and defenses.

## Method Summary
The research introduces blinding attacks that manipulate the labeling function outputs to disrupt Reward Machine state dynamics. The methodology involves training DQRM agents on three benchmark domains (Cookie World, Keys World, Symbol World) with and without Automated Reward Shaping, then applying various blinding attack strategies (random, event-based, edge-based) with different timing strategies. The attacks intercept labeling function outputs and remove specific events to cause RM desynchronization. Performance is evaluated using metrics including average success rate, failure rate, time-to-success, tampering rate, and impact score.

## Key Results
- Blinding attacks can cause RM desynchronization by removing labeling function outputs, degrading agent performance
- Random blinding noise increases failure rate proportionally to tampering probability, with effectiveness varying across domains
- Edge-blinding attacks targeting specific RM transitions are more efficient than event-blinding attacks in some scenarios
- Attacks require substantial tampering rates (often 50%+) to achieve significant impact, limiting real-world applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blinding attacks can cause RM desynchronization by removing labeling function outputs
- Mechanism: The attacker intercepts labeling function outputs and removes specific events, preventing the RM from transitioning to the correct state
- Core assumption: The RM relies on the labeling function outputs to maintain state synchronization with the environment
- Evidence anchors:
  - [abstract]: "blinding attacks, a novel class of attacks that manipulate the agent's labeling function to alter RM state dynamics"
  - [section]: "an adversarial actor could carry out a DoS attack to hinder the agent's ability to make good decisions"
  - [corpus]: Found 25 related papers, average neighbor FMR=0.439, indicating moderate field overlap
- Break condition: If the labeling function is not observable or tamperable, or if the agent has redundant state tracking mechanisms

### Mechanism 2
- Claim: Random blinding noise increases failure rate proportionally to tampering probability
- Mechanism: The attacker randomly removes events from labeling outputs with probability ρ, introducing noise that disrupts policy execution
- Core assumption: The agent's policy depends on accurate high-level event detection for optimal decision-making
- Evidence anchors:
  - [abstract]: "Experimental evaluation across three partially observable benchmark domains (Cookie World, Keys World, and Symbol World) demonstrates the effectiveness of blinding attacks"
  - [section]: "The Cookie World agents showed an increase in their average failure rate that grew slightly faster than the amount of noise they were subjected to"
  - [corpus]: Limited direct evidence for random noise effectiveness, weak anchor from corpus papers
- Break condition: If the agent can learn to ignore noisy inputs or if the environment provides redundant state information

### Mechanism 3
- Claim: Edge-blinding attacks are more efficient than event-blinding attacks
- Mechanism: The attacker targets specific RM transitions by removing only the events that trigger them, rather than removing events from all outputs
- Core assumption: Each RM transition requires a specific combination of events, and removing any one event prevents that transition
- Evidence anchors:
  - [abstract]: "Experimental evaluation...demonstrates the effectiveness of blinding attacks. Results show that blinding attacks can significantly reduce agent performance"
  - [section]: "In the case of edge-based attacks, the adversary only tampers with the labeling function outputs when he knows that doing so would cause the victim's RM to desynchronize"
  - [corpus]: Weak evidence for edge-specific attack efficiency, no direct corpus support
- Break condition: If the attacker cannot observe the current RM state or if transitions have multiple triggering event combinations

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Understanding the baseline environment structure before RM augmentation
  - Quick check question: What are the five components of an MDP tuple?

- Concept: Reward Machines (RMs)
  - Why needed here: The attack targets the interaction between RMs and labeling functions
  - Quick check question: How does a labeling function interact with an RM's state transition function?

- Concept: Partially Observable MDPs (POMDPs)
  - Why needed here: The attack exploits memory limitations in POMDPs where RMs provide additional state information
  - Quick check question: What additional component does a POMDP have compared to an MDP?

## Architecture Onboarding

- Component map:
  - Environment → Agent → Policy Network → Action → Environment (standard RL loop)
  - Environment → Labeling Function → RM State → Policy Decision (RM-specific path)
  - Attacker positioned between Labeling Function and RM State

- Critical path: Labeling Function Output → RM State Transition → Policy Decision
  - This is the attack vector where blinding occurs

- Design tradeoffs:
  - Black-box vs gray-box assumptions: Black-box provides wider applicability but limits attack sophistication
  - Tampering rate vs detection risk: Higher tampering rates increase effectiveness but also increase detectability
  - Event targeting vs transition targeting: Event-based attacks are simpler but less efficient than edge-based attacks

- Failure signatures:
  - Agent getting stuck in rooms without progress
  - Agent repeatedly attempting impossible actions
  - Agent failing to complete tasks despite having correct information at some point
  - Unexpected behavior when specific event combinations occur

- First 3 experiments:
  1. Baseline test: Run trained agents without attacks to establish performance metrics
  2. Random blinding test: Apply 10% random event removal and measure failure rate increase
  3. Targeted transition test: Identify critical RM transitions and remove only the triggering events to measure efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can blinding attacks be made more stealthy while maintaining effectiveness?
- Basis in paper: [explicit] The paper highlights that blinding attacks often require substantial tampering, which limits their real-world applicability and detectability.
- Why unresolved: The current blinding strategies focus on effectiveness but do not adequately address the trade-off between attack success and stealth.
- What evidence would resolve it: Developing and testing blinding strategies that minimize tampering while achieving high failure rates in agents, along with empirical evaluation of detectability in simulated environments.

### Open Question 2
- Question: What are the theoretical conditions under which blinding attacks guarantee agent failure?
- Basis in paper: [inferred] The paper discusses the effectiveness of blinding attacks but does not provide theoretical guarantees or conditions for failure.
- Why unresolved: The paper focuses on experimental results and heuristic strategies rather than formal proofs of attack success.
- What evidence would resolve it: Formal analysis of the relationship between labeling function tampering, reward machine state dynamics, and agent policy failure, possibly through mathematical modeling or formal verification.

### Open Question 3
- Question: How do blinding attacks perform against agents with advanced memory mechanisms (e.g., LSTM, GRU)?
- Basis in paper: [explicit] The paper evaluates blinding attacks on agents using reward machines but does not compare their effectiveness against agents with alternative memory architectures.
- Why unresolved: The experimental setup focuses solely on reward machine-based agents, leaving the generalizability of the attacks unexplored.
- What evidence would resolve it: Comparative experiments testing blinding attacks on agents with LSTM, GRU, or other memory mechanisms in partially observable environments.

## Limitations

- Attacks require substantial tampering rates (often 50%+) to achieve significant impact, limiting practical applicability
- Experimental domains are relatively simple compared to real-world RL applications
- No defense mechanisms are explored or evaluated against these attacks

## Confidence

- **High confidence**: The fundamental mechanism of blinding attacks causing RM desynchronization is well-established through multiple experimental validations
- **Medium confidence**: The relative efficiency of edge-blinding versus event-blinding attacks, as results vary significantly across domains
- **Low confidence**: Real-world applicability given the high tampering rates required and lack of defense evaluation

## Next Checks

1. Test attack effectiveness with progressively lower tampering rates to determine practical detectability thresholds
2. Evaluate the same attack strategies against RM-based agents using different RL algorithms (e.g., PPO, SAC) to assess generalizability
3. Implement and test basic detection mechanisms that could identify abnormal labeling function behavior patterns