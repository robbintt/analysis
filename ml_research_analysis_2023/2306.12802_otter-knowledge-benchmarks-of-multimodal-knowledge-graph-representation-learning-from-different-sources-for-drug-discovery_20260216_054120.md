---
ver: rpa2
title: 'Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning
  from different sources for drug discovery'
arxiv_id: '2306.12802'
source_url: https://arxiv.org/abs/2306.12802
tags:
- learning
- otter
- data
- graph
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that incorporating multimodal knowledge graphs\u2014\
  combining protein sequences, drug SMILES, text, and numerical data\u2014into drug-target\
  \ binding affinity prediction yields state-of-the-art results on TDC benchmarks.\
  \ The authors preprocess and release datasets from seven sources totaling over 30\
  \ million triples, and train GNN models to enhance embeddings for downstream affinity\
  \ prediction."
---

# Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery

## Quick Facts
- **arXiv ID:** 2306.12802
- **Source URL:** https://arxiv.org/abs/2306.12802
- **Reference count:** 40
- **Primary result:** Incorporating multimodal knowledge graphs into drug-target binding affinity prediction yields state-of-the-art results on TDC benchmarks

## Executive Summary
This paper introduces a novel approach for drug-target binding affinity prediction by pretraining graph neural networks on multimodal knowledge graphs that combine protein sequences, drug SMILES, text, and numerical data from seven public databases. The authors construct knowledge graphs with over 30 million triples and use GNN message passing to enrich initial embeddings (ESM-1b for proteins, Morgan fingerprints or MolFormer for drugs) with relational context. Their approach achieves superior performance on TDC benchmarks compared to traditional deep learning methods, with ensemble models showing the best results.

## Method Summary
The authors preprocess and release multimodal knowledge graphs from seven sources (Uniprot, BindingDB, ChEMBL, Drugbank, DUDe, PrimeKG, STITCH) totaling over 30 million triples. They use graph neural networks (GNNs) with three different scoring functions (DistMult, TransE, Classifier) to pretrain representations on these graphs. Initial embeddings (ESM-1b for proteins, Morgan fingerprints or MolFormer for drugs) are enhanced through GNN message passing over graph triples. The pretrained embeddings are then used as input to downstream binding affinity prediction models adapted from DeepDTA architecture. Ensemble models combining predictions from multiple pretrained GNNs achieve the best performance.

## Key Results
- Incorporating multimodal knowledge graphs improves drug-target binding affinity prediction performance on TDC benchmarks
- Ensemble models combining multiple pretrained GNN embeddings achieve best results
- Controlling information flow during pretraining or removing noisy links does not significantly affect downstream performance
- GNN pretraining with link prediction objectives consistently outperforms baseline models that use only initial embeddings

## Why This Works (Mechanism)

### Mechanism 1
Pretraining on multimodal knowledge graphs enriches protein and drug embeddings beyond sequence or SMILES alone, improving downstream binding affinity prediction. Initial embeddings are enhanced via GNN message passing over triples, allowing nodes to absorb relational context from neighbors in the graph, propagating factual information such as protein function, drug interactions, and disease associations.

### Mechanism 2
Ensemble learning across multiple pretrained GNN models trained on different datasets and objectives yields better generalization than any single model. Each GNN is trained on a different knowledge graph with a different scoring function, and downstream models trained on these distinct embeddings are linearly combined with equal weights, averaging out model-specific biases.

### Mechanism 3
Restricting information flow during GNN pretraining (so that Drug/Protein entities only receive information from sequence/SMILES neighbors) does not significantly degrade downstream performance. This enforces a domain-relevant inductive bias that aligns pretraining with downstream task constraints.

## Foundational Learning

- **Graph Neural Networks (GNN) for link prediction on knowledge graphs**: The core representation learning method uses GNNs to propagate embeddings through the multimodal knowledge graph. Quick check: How does a multi-relational GCN layer differ from a standard GCN layer?

- **Pretrained protein and molecule embeddings (ESM-1b, Morgan fingerprints, MolFormer)**: Initial node embeddings are the starting point before GNN enhancement. Quick check: What are the input modalities and dimensionalities for ESM-1b vs Morgan fingerprints?

- **Downstream binding affinity prediction architecture**: Final evaluation uses concatenated or summed protein and drug embeddings fed through separate MLP branches. Quick check: Why does the TDC baseline architecture underperform when using pretrained embeddings compared to a direct concatenation approach?

## Architecture Onboarding

- **Component map:** Data ingestion → JSON schema → Multimodal KG construction → modality handlers → initial embeddings (ESM-1b, Morgan, MolFormer, text, number) → GNN (R-GCN encoder + DistMult/TransE/Classifier decoder) → GNN outputs → downstream MLP (separate for drug and protein, then sum) → optionally ensemble of downstream predictions

- **Critical path:** Build multimodal KG from raw data sources using schema → Compute initial embeddings per modality → Train GNN with link prediction objective → Use pretrained GNN embeddings as input to downstream binding affinity model → Evaluate on TDC benchmarks

- **Design tradeoffs:** Graph scaling (partitioning with Metis vs training on full graph), information flow control (all attribute signals vs sequence/SMILES neighbors), scoring function choice (DistMult vs TransE vs Classifier)

- **Failure signatures:** Training collapse (check loss curves, embedding variance), overfitting (gap between training and validation accuracy), downstream degradation (pretrained embeddings worse than raw initial embeddings)

- **First 3 experiments:** 1) Train GNN on small synthetic KG with one scoring function; verify link prediction improves over initial embeddings. 2) Train two GNNs on same KG with different scoring functions; run ensemble and compare to single model. 3) Vary information flow control flag; measure downstream binding affinity correlation difference.

## Open Questions the Paper Calls Out

### Open Question 1
How does incorporating 3D structural information of proteins and drugs into the multimodal knowledge graphs affect the performance of drug-target binding affinity prediction? The authors mention license restrictions prevented including 3D structural data, which could be an interesting modality for future work.

### Open Question 2
What are the challenges and potential solutions for aligning and integrating multiple heterogeneous databases in a scalable and efficient manner? The authors discuss the difficulty of handling large graphs and need for efficient methods to manage diverse data sources.

### Open Question 3
How can the learned graph representations be evaluated for their generativity and robustness across different predictive or generative downstream tasks? The authors mention the need for more robust learning methods for generalizing the learned representation to multiple tasks under data distribution shift.

## Limitations
- Performance gains are primarily demonstrated on a specific set of benchmark datasets, limiting generalizability
- Contribution of individual knowledge sources remains unclear since ensemble methods are used in best results
- Preprocessing of numerical data properties during pretraining with regression objectives is not fully specified

## Confidence
- **High Confidence**: Core methodology of using GNNs for link prediction on multimodal KGs to enrich embeddings is technically sound and well-supported by experimental results
- **Medium Confidence**: Ensemble models achieve best performance claim is supported by ablation studies but lacks theoretical justification for equal weighting
- **Medium Confidence**: Controlling information flow does not significantly affect performance is based on internal experiments but would benefit from external validation

## Next Checks
1. Systematically vary the degree of information flow restriction in GNN pretraining and measure downstream performance across multiple affinity prediction tasks to determine hidden thresholds where performance degrades
2. Train individual models using each knowledge source separately and perform statistical analysis to quantify marginal contribution of each source to overall performance
3. Apply pretrained embeddings to additional drug discovery tasks beyond binding affinity prediction (e.g., drug repurposing, side effect prediction) to assess broader utility of multimodal KG representations