---
ver: rpa2
title: Demystifying Instruction Mixing for Fine-tuning Large Language Models
arxiv_id: '2312.10793'
source_url: https://arxiv.org/abs/2312.10793
tags:
- instruction
- instructions
- performance
- alignment
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how mixing different types of instruction
  data affects the performance of fine-tuned large language models. The authors categorize
  instructions into three types: NLP downstream tasks, coding, and general chat.'
---

# Demystifying Instruction Mixing for Fine-tuning Large Language Models

## Quick Facts
- arXiv ID: 2312.10793
- Source URL: https://arxiv.org/abs/2312.10793
- Reference count: 6
- Key outcome: Mixing different types of instruction data during fine-tuning affects LLM performance across NLP, coding, and alignment tasks, with optimal mixtures depending on model size and target usage.

## Executive Summary
This paper investigates how mixing different types of instruction data affects the performance of fine-tuned large language models. The authors categorize instructions into three types: NLP downstream tasks, coding, and general chat. They fine-tune LLaMA-2 models with various combinations of datasets from these categories and evaluate the models on NLP benchmarks, code generation, and alignment skills. Key findings include: 1) Each type of instruction improves performance on corresponding tasks. 2) Incorporating instructions from NLP tasks negatively impacts alignment skills. 3) Code instructions improve both coding ability and alignment. 4) The optimal instruction mixture depends on the model size and target usage. The study highlights the importance of carefully designing instruction mixtures to maximize model performance while considering potential trade-offs.

## Method Summary
The authors fine-tune LLaMA-2 (7B and 13B) models for two epochs using a linear scheduler with 3% warmup and batch size 64. Maximum learning rate is 5 × 10−5. They use three types of instruction data: NLP downstream tasks (P3), coding (CodeAlpaca), and general chat (Alpaca). Models are evaluated in a zero-shot setting on NLP benchmarks (ARC, Winogrande, PIQA, MMLU, RACE, HellaSwag), code generation (HumanEval), and alignment skills (FLASK framework).

## Key Results
- Each type of instruction data consistently improves performance on the corresponding task type
- Mixing P3 (NLP task-formatted instructions) with general instructions degrades alignment skills by 2.8-3.6 points
- Code instructions improve both coding ability and alignment skills, with combined datasets showing +0.6 to +1.6 alignment improvement
- Optimal instruction mixture ratios vary by model size and target application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction mixing improves task-specific performance when the target task aligns with the fine-tuning data
- Mechanism: Fine-tuning on specialized instruction datasets conditions the model to generalize better within that task domain, improving zero-shot performance on related benchmarks
- Core assumption: The model's capacity allows it to absorb domain-specific patterns without catastrophic forgetting of general capabilities
- Evidence anchors:
  - [abstract] "Using a single type of STF data consistently improves the performance of the model on the corresponding task"
  - [section] "In the no-mixture setting (comparing A, C, and P), models fine-tuned on P3 achieve the highest average score for NLP tasks, while models fine-tuned on CodeAlpaca excel in code generation benchmarks."

### Mechanism 2
- Claim: Mixing general-purpose instructions (Alpaca) with specialized instructions (CodeAlpaca) enhances both coding ability and alignment skills
- Mechanism: General instructions provide alignment signals while specialized code instructions supply domain-specific reasoning patterns, leading to synergistic improvements in both coding benchmarks and alignment metrics
- Core assumption: Alignment and domain-specific reasoning are complementary skills that reinforce each other during fine-tuning
- Evidence anchors:
  - [abstract] "Incorporating code instructions can improve the model’s coding ability and boost the alignment skills"
  - [section] "While CodeAlpaca alone doesn’t significantly enhance alignment abilities, combining it with general instructions results in a substantial improvement of +0.6 (7B) and +1.6 (13B) points."

### Mechanism 3
- Claim: Mixing NLP task-formatted instructions (P3) with general instructions harms alignment skills due to task-format mismatch
- Mechanism: Reformatted NLP tasks emphasize task completion over conversational alignment, causing the model to prioritize task-oriented responses over human-aligned communication styles
- Core assumption: Alignment skills are sensitive to the style and format of fine-tuning data; conversational prompts are necessary for maintaining alignment
- Evidence anchors:
  - [abstract] "Incorporating instructions that are simply reformatted from NLP downstream tasks (e.g., P3) downgrades the model’s alignment skills, resulting in a worse chat experience"
  - [section] "Mixing P3 data causes a drop of -2.8 (7B) and -3.6 (13B) in average alignment skills."

## Foundational Learning

- Concept: Instruction fine-tuning
  - Why needed here: The paper focuses on how mixing different types of instruction data during fine-tuning affects downstream performance
  - Quick check question: What is the difference between supervised fine-tuning and instruction fine-tuning in the context of LLMs?

- Concept: Zero-shot evaluation
  - Why needed here: The experiments measure model performance without additional adaptation, relying solely on the knowledge gained during fine-tuning
  - Quick check question: Why is zero-shot evaluation particularly relevant for assessing the generalization of instruction-tuned models?

- Concept: Alignment skills
  - Why needed here: The study evaluates not just task performance but also how well models align with human expectations in conversational contexts
  - Quick check question: What are the key dimensions of alignment skills as evaluated in the FLASK framework?

## Architecture Onboarding

- Component map: Data preparation -> Fine-tuning engine (LLaMA-2) -> Evaluation suite (NLP benchmarks, code benchmarks, FLASK alignment)
- Critical path: Data preparation -> Fine-tuning (2 epochs, linear scheduler) -> Zero-shot evaluation
- Design tradeoffs: Balancing specialized vs. general instructions to optimize for target tasks while maintaining alignment; model size considerations
- Failure signatures: Performance degradation on alignment metrics when mixing P3; overfitting when instruction ratios are too skewed
- First 3 experiments:
  1. Fine-tune LLaMA-2-7B on only Alpaca and evaluate on all three benchmark categories to establish baseline alignment
  2. Fine-tune on only CodeAlpaca and evaluate coding and alignment to test domain-specific + alignment synergy
  3. Mix Alpaca and P3 at 1:1 ratio and compare alignment scores to detect potential negative transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs change when using a different instruction dataset size than the 20K used in this study?
- Basis in paper: [inferred] The paper mentions using a 20K subset from each type of dataset for a balanced comparison, but does not explore the impact of different dataset sizes
- Why unresolved: The paper only focuses on a specific dataset size and does not provide information on how varying the dataset size might affect the model's performance
- What evidence would resolve it: Conducting experiments with different instruction dataset sizes and comparing the results would provide insights into the optimal dataset size for fine-tuning LLMs

### Open Question 2
- Question: How does the performance of LLMs vary when using different ratios of instruction types in the fine-tuning process?
- Basis in paper: [explicit] The paper mentions mixing different ratios of specialized instructions and finding that the performance of both NLP and code benchmarks first decreases and then increases as the ratio of specialized instructions increases, reaching a maximum at a 1.5 ratio
- Why unresolved: The paper only explores a limited range of ratios and does not provide a comprehensive understanding of how different ratios impact model performance
- What evidence would resolve it: Conducting experiments with a wider range of ratios and analyzing the results would provide a more comprehensive understanding of the optimal ratio for fine-tuning LLMs

### Open Question 3
- Question: How does the performance of LLMs change when using instruction datasets from different domains or task types?
- Basis in paper: [inferred] The paper focuses on three types of instructions (NLP downstream tasks, coding, and general chat) but does not explore the impact of using instruction datasets from other domains or task types
- Why unresolved: The paper only investigates the effects of three specific types of instructions and does not provide information on how using instruction datasets from other domains or task types might affect model performance
- What evidence would resolve it: Conducting experiments with instruction datasets from various domains or task types and comparing the results would provide insights into the impact of using different types of instructions on LLM performance

## Limitations
- Limited to LLaMA-2 models (7B and 13B parameters) - effects may not generalize to other architectures
- Evaluation focuses on zero-shot performance, not capturing few-shot or fine-tuned scenarios
- Only three specific instruction categories investigated, not exploring broader instruction diversity

## Confidence
- High: General pattern that instruction mixing affects performance differently across task categories
- Medium: Specific magnitude of effects reported
- Low: Claims about optimal mixture ratios being highly dependent on specific use cases

## Next Checks
1. Test whether observed instruction mixing effects persist when applying the same methodology to other model families (e.g., Mistral, GPT-Neo) to verify generalizability
2. Implement curriculum-based instruction mixing that varies the mixture ratio during training rather than using fixed ratios, to test if this mitigates the alignment degradation observed with P3 mixing
3. Conduct human assessments of alignment and code quality to validate the automated metrics, particularly for the claim that CodeAlpaca improves both coding ability and alignment