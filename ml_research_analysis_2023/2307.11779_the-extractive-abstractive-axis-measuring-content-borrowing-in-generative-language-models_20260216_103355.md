---
ver: rpa2
title: 'The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative
  Language Models'
arxiv_id: '2307.11779'
source_url: https://arxiv.org/abs/2307.11779
tags:
- content
- text
- arxiv
- axis
- extractive-abstractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Extractive-Abstractive Axis as a framework
  to measure content borrowing in generative language models, addressing the gap between
  traditional extractive search engine outputs and abstractive LLM responses. The
  proposed axis aims to quantify how much generated text is derived from source content,
  which is critical for evaluating licensing and attribution risks.
---

# The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models

## Quick Facts
- arXiv ID: 2307.11779
- Source URL: https://arxiv.org/abs/2307.11779
- Reference count: 7
- This paper introduces a framework to measure content borrowing in LLMs along an Extractive-Abstractive Axis using adapted NLP metrics and expert annotations.

## Executive Summary
This paper proposes the Extractive-Abstractive Axis as a framework for measuring how much content in generative language model outputs is derived from source material. The framework addresses the critical gap between traditional extractive search engine outputs and abstractive LLM responses, providing a systematic way to quantify content borrowing for licensing and attribution risk assessment. By adapting existing NLP evaluation metrics and repurposing summarization datasets with expert annotations, the framework aims to help content owners, developers, and legal practitioners evaluate generative models' compliance with copyright and attribution requirements.

## Method Summary
The method involves creating a measurement axis ranging from extractive (verbatim copying) to abstractive (highly synthesized) to quantify content borrowing in LLM outputs. Existing NLP metrics like ROUGE, BERTScore, and QAEval are adapted to measure similarity between generated text and source content. Summarization datasets with expert annotations are repurposed to assess perceived similarity and copyright concerns, with human evaluators rating outputs along dimensions including relevance, fluency, coherence, and content ownership perspectives. The framework provides evaluation tools for different stakeholders to audit generative models for content borrowing patterns.

## Key Results
- The framework establishes a quantifiable spectrum for measuring content borrowing from extractive to abstractive generation
- Existing NLP metrics can be repurposed to measure position on the Extractive-Abstractive Axis
- Summarization datasets with expert annotations provide a foundation for benchmarking content borrowing concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The extractive-abstractive axis provides a quantifiable framework to measure content borrowing risk in LLMs.
- Mechanism: By positioning LLM outputs along a spectrum from extractive (verbatim copying) to abstractive (highly synthesized), the framework enables systematic measurement of how much source content is borrowed versus transformed.
- Core assumption: Content borrowing can be meaningfully measured along a continuous spectrum rather than as a binary property.
- Evidence anchors:
  - [abstract] "we propose the the so-called Extractive-Abstractive axis for benchmarking generative models"
  - [section] "Being able to quantify a generative language model's extractiveness/abstractiveness level â€“ in other words where the model lies on what we call the Extractive-Abstractive axis"
  - [corpus] Weak - corpus neighbors discuss extractive-abstractive spectrum but don't provide empirical evidence for this specific framework
- Break condition: The framework breaks if content borrowing cannot be meaningfully quantified along a continuous spectrum, or if attribution requirements are context-dependent in ways that cannot be captured by this axis.

### Mechanism 2
- Claim: Existing NLP evaluation metrics can be repurposed to measure content borrowing along the extractive-abstractive axis.
- Mechanism: Metrics like ROUGE, BERTScore, and QAEval that measure similarity between generated and reference text can be adapted to measure similarity between LLM outputs and source content.
- Core assumption: Text similarity metrics correlate with perceived content borrowing and copyright concerns.
- Evidence anchors:
  - [abstract] "We suggest adapting existing NLP metrics like ROUGE, BERTScore, and QAEval to measure content similarity"
  - [section] "In principle, while some of the above mentioned NLP metrics can be repurposed to measure LLMs along the Extractive-Abstractive axis"
  - [corpus] Moderate - corpus contains papers on unsupervised summarization that use similar metrics, suggesting feasibility
- Break condition: The mechanism breaks if similarity metrics do not correlate with human perceptions of content borrowing, or if adversarial generation techniques can obscure borrowing patterns.

### Mechanism 3
- Claim: Summarization datasets with expert annotations can serve as benchmarks for evaluating content borrowing.
- Mechanism: Summarization tasks already involve human evaluation of summary quality, which can be extended to include assessments of content borrowing and copyright concerns.
- Core assumption: Summarization evaluation frameworks can be extended to capture content ownership perspectives without requiring entirely new datasets.
- Evidence anchors:
  - [abstract] "repurposing summarization datasets with expert annotations to assess perceived similarity and copyright concerns"
  - [section] "Currently, summarization tasks are benchmarked using human annotators who rate the generated summaries along dimensions such as summary relevance, fluency, coherence"
  - [corpus] Strong - multiple corpus neighbors discuss summarization evaluation and datasets, validating the approach
- Break condition: The mechanism breaks if summarization evaluation does not capture the nuanced concerns of content owners, or if the extended annotation requirements are prohibitively expensive.

## Foundational Learning

- Concept: Natural Language Processing evaluation metrics (ROUGE, BERTScore, QAEval)
  - Why needed here: These metrics form the technical foundation for measuring content similarity between LLM outputs and source material
  - Quick check question: What is the key difference between ROUGE (token overlap) and BERTScore (semantic similarity) in evaluating text similarity?

- Concept: Copyright law and fair use doctrine
  - Why needed here: Understanding legal frameworks is essential for interpreting what levels of content borrowing are problematic
  - Quick check question: How does the concept of "substantial similarity" in copyright law relate to the extractive-abstractive axis?

- Concept: Human evaluation in NLP
  - Why needed here: Expert annotation is critical for capturing content owner perspectives on borrowing
  - Quick check question: What are the key challenges in scaling human evaluation of content borrowing across different domains and languages?

## Architecture Onboarding

- Component map: The framework consists of (1) a measurement axis ranging from extractive to abstractive, (2) adapted NLP metrics for quantifying position on this axis, (3) benchmark datasets with extended annotations, and (4) evaluation tools for different stakeholders.
- Critical path: The core workflow involves taking LLM outputs, comparing them to source content using adapted metrics, and classifying the results along the extractive-abstractive spectrum with expert validation.
- Design tradeoffs: Balancing automated metric efficiency against the need for human expert evaluation; choosing between precision (focusing on specific content sources) versus recall (catching all potential borrowing).
- Failure signatures: High false negatives (missing significant borrowing), poor correlation between metric scores and human perceptions, inability to handle multilingual content, or adversarial obfuscation of borrowing patterns.
- First 3 experiments:
  1. Apply ROUGE, BERTScore, and QAEval to a set of LLM outputs paired with their source documents, measuring correlation between metrics
  2. Conduct expert annotation on a subset of outputs to establish ground truth for content borrowing perceptions
  3. Test the framework's sensitivity by generating outputs at known levels of abstraction and measuring how well the framework captures these differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific metrics (e.g., ROUGE, BERTScore) most effectively measure content borrowing along the Extractive-Abstractive Axis for licensing and attribution purposes?
- Basis in paper: Explicit
- Why unresolved: The paper identifies several NLP metrics (ROUGE, BERTScore, QAEval) but states no empirical studies exist on their effectiveness for licensing and attribution contexts. The paper calls for evaluation benchmarks to guide analysis.
- What evidence would resolve it: Empirical studies comparing these metrics' correlation with content owners' perception of licensing and attribution quality, using annotated datasets where human experts rate similarity and copyright concerns.

### Open Question 2
- Question: How can summarization benchmarks be effectively augmented with legal expert annotations to assess content borrowing while maintaining factual accuracy and coherence?
- Basis in paper: Explicit
- Why unresolved: The paper proposes repurposing existing summarization datasets with legal expert annotations but does not provide specific guidelines or methodologies for balancing factual accuracy, abstraction levels, and attribution concerns.
- What evidence would resolve it: Development and validation of annotation guidelines that capture both technical summarization quality (relevance, fluency, coherence) and legal dimensions (copyright concerns, proper attribution), along with inter-annotator agreement studies.

### Open Question 3
- Question: What methods can be developed to audit black box LLMs for content borrowing without inadvertently providing additional training signals to potentially copyright-infringing systems?
- Basis in paper: Explicit
- Why unresolved: The paper identifies this as a practical challenge but does not propose specific solutions, noting that interactions with LLMs can be used as training signals while auditing is needed.
- What evidence would resolve it: Novel auditing methodologies that either use synthetic or carefully curated queries that don't contribute meaningful training data, or approaches that can extract necessary information about content borrowing patterns without iterative interaction.

## Limitations
- The framework assumes content borrowing can be meaningfully quantified along a continuous spectrum, which may not hold across different domains or cultural contexts
- Reliance on expert human annotation introduces scalability challenges and potential subjectivity in defining problematic content borrowing
- The framework does not address potential adversarial techniques that could obscure borrowing patterns

## Confidence
- **High confidence**: The foundational concepts of using existing NLP metrics (ROUGE, BERTScore, QAEval) for text similarity measurement are well-established and validated in the NLP literature.
- **Medium confidence**: The adaptation of summarization evaluation frameworks to capture content ownership perspectives is theoretically sound but requires empirical validation across diverse domains and content types.
- **Low confidence**: The assumption that a single Extractive-Abstractive axis can capture the full complexity of content borrowing concerns, including legal, ethical, and cultural dimensions, remains unproven.

## Next Checks
1. **Correlation validation**: Conduct systematic studies measuring the correlation between automated metric scores (ROUGE, BERTScore, QAEval) and human expert annotations across diverse content types, languages, and domains to establish the framework's reliability and generalizability.

2. **Adversarial robustness testing**: Design and implement adversarial generation techniques that intentionally obscure borrowed content, then evaluate whether the framework can still detect problematic borrowing patterns, identifying potential vulnerabilities.

3. **Cross-jurisdictional legal validation**: Collaborate with legal experts across different jurisdictions to validate whether the framework's quantification of content borrowing aligns with varying legal standards for copyright infringement, fair use, and attribution requirements.