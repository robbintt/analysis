---
ver: rpa2
title: Is RLHF More Difficult than Standard RL?
arxiv_id: '2306.14111'
source_url: https://arxiv.org/abs/2306.14111
tags:
- algorithm
- reward
- policy
- learning
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reinforcement learning from human
  feedback (RLHF) is more difficult than standard RL. The key finding is that RLHF
  is not inherently more difficult than standard RL, and existing RL techniques can
  be directly applied to preference-based RL with minimal modifications.
---

# Is RLHF More Difficult than Standard RL?

## Quick Facts
- arXiv ID: 2306.14111
- Source URL: https://arxiv.org/abs/2306.14111
- Reference count: 40
- One-line primary result: RLHF is not inherently more difficult than standard RL, with existing techniques applying directly to preference-based RL

## Executive Summary
This paper investigates whether reinforcement learning from human feedback (RLHF) is more difficult than standard reinforcement learning (RL). The authors show that RLHF can be solved efficiently using existing RL techniques with minimal overhead. They provide two main approaches: reducing utility-based preferences to robust reward-based RL, and reducing general arbitrary preferences to multiagent reward-based RL. The theoretical analysis demonstrates that RLHF can achieve similar sample complexity to standard RL while maintaining low query complexity to the comparison oracle.

## Method Summary
The paper presents two approaches to solve RLHF problems. For utility-based preferences drawn from reward-based probabilistic models, the authors reduce the problem to robust reward-based RL using a Preference-to-Reward (P2R) interface that maintains a confidence set of reward functions and only queries the comparison oracle when uncertainty is large. For general arbitrary preferences where the objective is to find the von Neumann winner, the problem is reduced to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The authors instantiate these reductions with concrete provable algorithms and provide sample and query complexity bounds for various settings including tabular MDPs, MDPs with generic function approximation, and K-wise comparisons.

## Key Results
- RLHF can be reduced to standard RL problems with no sample complexity overhead for utility-based preferences
- The number of human queries does not scale with the RL algorithm's sample complexity
- General arbitrary preferences can be solved by reducing to multiagent reward-based RL
- Concrete sample and query complexity bounds are provided for tabular MDPs and MDPs with function approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Utility-based preferences can be reduced to robust reward-based RL with no sample complexity overhead
- Mechanism: The Preference-to-Reward (P2R) interface maintains a confidence set of reward functions and only queries the comparison oracle when uncertainty is large, converting preference feedback to reward labels
- Core assumption: The link function σ is known and has a gradient lower bound α > 0
- Evidence anchors: [abstract] "for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards"
- Break condition: If σ is unknown or has no gradient lower bound, the optimal policy cannot be identified (Lemma 2,3)

### Mechanism 2
- Claim: General arbitrary preferences can be reduced to multiagent reward-based RL to find Nash equilibria
- Mechanism: By constructing a factored Markov game where each player controls one copy of the original MDP, finding the von Neumann winner reduces to finding restricted Nash equilibria
- Core assumption: The preference function can be expressed as rewards in a two-player zero-sum game
- Evidence anchors: [abstract] "for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games"
- Break condition: If the preference function depends on the entire trajectory in complex ways, the reduction may not be computationally tractable

### Mechanism 3
- Claim: The P2R interface achieves better query complexity than naive approaches through black-box reduction
- Mechanism: Unlike exhaustive querying (which increases oracle complexity proportionally to sample complexity) or pre-learning rewards (which incurs exploration overhead), P2R queries only when uncertainty is large and explores through the RL algorithm itself
- Core assumption: The RL algorithm using P2R is g(ϵ)-robust
- Evidence anchors: [section] "Compared to the two alternative approaches, our reduction achieves the best of both worlds by avoiding sample complexity overhead with a query complexity that does not scale with the sample complexity"
- Break condition: If the reward function class R has very high eluder dimension, query complexity may become prohibitive

## Foundational Learning

- Concept: Eluder dimension and its role in function approximation
  - Why needed here: Eluder dimension measures the complexity of the reward function class and determines both the confidence set size and query complexity in P2R
  - Quick check question: How does eluder dimension relate to the number of queries needed to identify an unknown reward function?

- Concept: Robustness to reward perturbations
  - Why needed here: The theoretical guarantees require the underlying RL algorithm to tolerate small errors in reward estimates, which is essential for the P2R interface to work
  - Quick check question: What does it mean for an RL algorithm to be g(ϵ)-robust, and why is this property important for P2R?

- Concept: Nash equilibria in Markov games
  - Why needed here: For general preferences, finding the von Neumann winner requires finding Nash equilibria in factored Markov games, which connects preference learning to multiagent RL
  - Quick check question: How does the von Neumann winner concept extend the idea of optimal policy to settings without underlying utility functions?

## Architecture Onboarding

- Component map: RL algorithm -> P2R interface -> Comparison oracle (for utility-based preferences); or Two adversarial MDP algorithms -> Comparison oracle -> Nash equilibrium solver (for general preferences)
- Critical path: For utility-based preferences: RL algorithm requests reward → P2R checks confidence set → if uncertain, queries oracle → updates confidence set → returns reward to RL algorithm. For general preferences: run two adversarial MDP algorithms in parallel → collect trajectories → query oracle → update game strategies.
- Design tradeoffs: P2R trades computational simplicity (black-box reduction) for potentially higher query complexity compared to white-box modifications. The Markov game approach trades generality for computational efficiency, as it may not be tractable for complex trajectory-based preferences.
- Failure signatures: If the RL algorithm fails to converge, check whether the reward confidence set in P2R is too narrow or too wide. If the von Neumann winner cannot be found, verify whether the preference function satisfies the factorization assumptions required for the Markov game reduction.
- First 3 experiments:
  1. Implement P2R interface with UCBVI-BF on a tabular MDP with Bradley-Terry preferences, measuring both sample and query complexity
  2. Test the Markov game reduction on a simple preference function that depends only on final states, using two copies of an adversarial MDP algorithm
  3. Compare the query complexity of P2R against naive exhaustive querying on a linear MDP with synthetic preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RLHF compare to standard RL when the link function is unknown or lacks a gradient lower bound?
- Basis in paper: The paper proves that if the link function σ is unknown or has no gradient lower bound, the optimal policy can be impossible to identify (Lemmas 2 and 3).
- Why unresolved: The paper provides impossibility results for utility-based preference models with unknown or unbounded gradient link functions, but does not explore potential workarounds or alternative approaches.
- What evidence would resolve it: Experiments or theoretical results demonstrating the performance of RLHF algorithms when the link function is unknown or has no gradient lower bound, compared to standard RL.

### Open Question 2
- Question: Can the P2R interface be extended to handle non-stationary preferences or preferences that change over time?
- Basis in paper: The P2R interface is designed for stationary preferences, but the paper does not discuss how it would perform with non-stationary preferences.
- Why unresolved: The paper focuses on stationary preference models and does not explore the impact of non-stationary preferences on the interface's performance.
- What evidence would resolve it: Experiments or theoretical analysis showing the performance of the P2R interface with non-stationary preferences, compared to standard RL algorithms.

### Open Question 3
- Question: How does the query complexity of the P2R interface scale with the horizon length H of the MDP?
- Basis in paper: The paper provides query complexity bounds for the P2R interface, but the dependence on the horizon length H is not explicitly discussed.
- Why unresolved: The paper does not analyze the impact of the MDP horizon length H on the query complexity of the P2R interface.
- What evidence would resolve it: Theoretical analysis or experiments showing how the query complexity of the P2R interface scales with the horizon length H of the MDP.

## Limitations
- Theoretical framework assumes perfect comparison oracle and known link functions, which may not hold in practical RLHF settings
- Reduction to factored Markov games relies on restrictive assumptions about preference structure that may not generalize to complex real-world feedback
- Extension to deep RL with neural networks remains unproven, with current analysis limited to tabular MDPs and specific function approximation settings

## Confidence
- High confidence: The reduction of utility-based preferences to robust reward-based RL is theoretically sound and the sample complexity bounds are well-established
- Medium confidence: The Markov game reduction for general preferences is valid under stated assumptions, but computational tractability for complex preferences is uncertain
- Low confidence: Practical implementation details for deep RL settings and the performance of P2R with unknown link functions

## Next Checks
1. Implement P2R interface with UCBVI-BF on a tabular MDP with Bradley-Terry preferences, measuring both sample and query complexity
2. Test the Markov game reduction on a simple preference function that depends only on final states, using two copies of an adversarial MDP algorithm
3. Compare the query complexity of P2R against naive exhaustive querying on a linear MDP with synthetic preferences