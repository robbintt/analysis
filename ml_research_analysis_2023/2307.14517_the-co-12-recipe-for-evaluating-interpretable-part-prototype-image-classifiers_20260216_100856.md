---
ver: rpa2
title: The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers
arxiv_id: '2307.14517'
source_url: https://arxiv.org/abs/2307.14517
tags:
- part-prototype
- image
- explanation
- prototypes
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews existing work on evaluating interpretable part-prototype
  models, which are inherently explainable by design. The authors organize the evaluation
  approaches according to the Co-12 properties for explanation quality, revealing
  research gaps and outlining future directions.
---

# The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers

## Quick Facts
- arXiv ID: 2307.14517
- Source URL: https://arxiv.org/abs/2307.14517
- Reference count: 40
- Part-prototype models are interpretable by design but need comprehensive evaluation across 12 quality properties

## Executive Summary
This paper reviews evaluation approaches for interpretable part-prototype image classifiers, organizing methods according to the Co-12 properties framework. Part-prototype models learn prototypical image parts and base their reasoning on recognizing these components, making them inherently interpretable. However, the authors identify gaps in current evaluation practices and propose a comprehensive framework for assessing explanation quality across 12 dimensions including correctness, completeness, consistency, continuity, contrastivity, covariate complexity, compactness, composition, confidence, context, coherence, and controllability.

## Method Summary
The authors systematically review existing work on part-prototype model evaluation, organizing findings according to the Co-12 properties framework. They analyze how current evaluation methods address each property, identify research gaps, and propose future directions. The methodology involves examining prototype visualization correctness, completeness for human users, consistency across runs, continuity for robustness, contrastivity for discriminative power, covariate complexity for understandability, compactness, composition, confidence, context, coherence, and controllability. The paper provides a "Co-12 cheat sheet" summarizing findings and recommendations for thorough evaluation of part-prototype models.

## Key Results
- Part-prototype models are interpretable by design but require evaluation of prototype visualization correctness
- Existing evaluation methods need adaptation to account for prototype-based explanations rather than pixel-level heatmaps
- User studies are needed to evaluate completeness, context, and coherence of explanations
- Implementation invariance and adversarial robustness checks can assess consistency and continuity properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Part-prototype models achieve interpretability by design because their explanation method equals their predictive model (e = f).
- Mechanism: The model learns prototypical parts and bases classification on recognizing these components, so the decision process is transparent and identical to the explanation.
- Core assumption: Prototypes are semantically meaningful and their visualization accurately reflects the latent representations.
- Evidence anchors:
  - [abstract] "Part-prototype models are interpretable by design. The models learn prototypical parts and recognise these components in an image, thereby combining classification and explanation."
  - [section] "Part-prototype models are interpretable by design: explanation method e is incorporated in predictive model f. Since a part-prototype model is simultaneously making predictions and providing explanations, the correctness of the reasoning process is fulfilled by design."
- Break condition: If prototype visualization fails to accurately represent latent representations or prototypes are not semantically meaningful, the model loses its interpretability advantage.

### Mechanism 2
- Claim: The Co-12 properties framework provides a comprehensive evaluation approach for part-prototype models.
- Mechanism: By organizing evaluation methods according to the 12 properties (Correctness, Completeness, Consistency, etc.), researchers can systematically assess different aspects of explanation quality.
- Core assumption: The Co-12 properties comprehensively capture the dimensions of explanation quality needed for part-prototype models.
- Evidence anchors:
  - [abstract] "Based on the Co-12 properties for explanation quality as introduced in [42] (e.g., correctness, completeness, compactness), we review existing work that evaluates part-prototype models, reveal research gaps and outline future approaches for evaluation of the explanation quality of part-prototype models."
  - [section] "By addressing the Co-12 properties individually, we identify research gaps and we conceptualise how the Co-12 properties can be put in practice for a thorough evaluation of interpretable image classifiers based on prototypical parts."
- Break condition: If important evaluation dimensions are missing from the Co-12 framework, the evaluation will be incomplete.

### Mechanism 3
- Claim: Part-prototype models can be evaluated using existing XAI evaluation methods adapted to their specific nature.
- Mechanism: Methods like deletion/addition tests, implementation invariance checks, and user studies can be modified to account for prototype-based explanations rather than pixel-level heatmaps.
- Core assumption: The specific characteristics of part-prototype models (e.g., prototypes as object parts, multiple occurrences in images) can be incorporated into existing evaluation frameworks.
- Evidence anchors:
  - [section] "We also recommend to apply existing deletion and addition methodology, usually applied to heatmaps, to part-prototype models. Important here is to take the specific nature of part-prototype models into account."
  - [section] "We see multiple ways to automatically quantify the target discriminativeness of prototypes visualised as image patches."
- Break condition: If adaptations are insufficient or the specific nature of part-prototype models is not properly accounted for, evaluation results may be misleading.

## Foundational Learning

- Concept: Interpretability vs Explainability
  - Why needed here: Understanding the distinction is crucial for evaluating part-prototype models which are interpretable by design rather than using post-hoc explanations
  - Quick check question: What is the key difference between interpretable models (e = f) and explainable models (e ≠ f)?

- Concept: Prototype-based classification
  - Why needed here: Part-prototype models classify images based on similarity to learned prototypical parts, which is fundamental to understanding their operation
  - Quick check question: How do part-prototype models use similarity scores between image patches and prototypes to make classification decisions?

- Concept: XAI evaluation frameworks
  - Why needed here: The paper uses the Co-12 properties framework, so understanding evaluation frameworks is essential for applying the methodology
  - Quick check question: What are the key components of a comprehensive XAI evaluation framework beyond just predictive accuracy?

## Architecture Onboarding

- Component map:
  Backbone neural network -> Feature extraction -> Prototype layer (stores prototypical parts) -> Similarity computation module (compares image patches to prototypes) -> Decision layer (maps prototypes to classes) -> Visualization module (converts latent prototypes to image patches)

- Critical path:
  1. Input image → Backbone network → Feature extraction
  2. Image patches compared to prototypes using similarity metrics
  3. Similarity scores aggregated for each class
  4. Decision layer produces final classification
  5. Prototypes and decision path visualized for explanation

- Design tradeoffs:
  - Number of prototypes vs interpretability (more prototypes = more detail but potentially less comprehensible)
  - Prototype sharing between classes vs class-specific prototypes (affects contrastivity and compactness)
  - Backbone architecture choice (affects feature quality and computation cost)
  - Visualization method (affects correctness and covariate complexity)

- Failure signatures:
  - Poor predictive performance despite interpretable structure
  - Prototypes that don't correspond to semantically meaningful concepts
  - Inconsistent explanations across similar inputs (continuity issues)
  - Overfitting to training data (lack of generalization)

- First 3 experiments:
  1. Train a simple part-prototype model (ProtoPNet) on a standard dataset (e.g., CUB-200) and visualize the learned prototypes to verify they correspond to object parts
  2. Apply deletion/addition tests to evaluate the correctness of prototype-based explanations
  3. Implement implementation invariance check by training the same model with different random seeds and comparing prototype consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can part-prototype models be made more continuous and robust to slight variations in input data?
- Basis in paper: [explicit] The paper discusses the continuity property and mentions that part-prototype models can be improved with adversarial training and data augmentation to enhance their robustness and generalizability.
- Why unresolved: While the paper suggests that adversarial training and data augmentation can improve continuity, it does not provide specific methods or evidence for how this can be achieved in practice for part-prototype models.
- What evidence would resolve it: Experimental results showing the effectiveness of different adversarial training and data augmentation techniques in improving the continuity of part-prototype models.

### Open Question 2
- Question: How can the reliability of confidence estimates in part-prototype models be improved, especially for out-of-distribution data?
- Basis in paper: [explicit] The paper discusses the confidence property and mentions that softmax probabilities can be over-confident and may fail for out-of-distribution data.
- Why unresolved: The paper suggests that further research is needed to investigate the reliability of confidence estimates in part-prototype models and how they handle out-of-distribution data.
- What evidence would resolve it: Experimental results comparing the reliability of confidence estimates in part-prototype models with other methods, especially for out-of-distribution data.

### Open Question 3
- Question: How can part-prototype models be made more interactive and controllable for users?
- Basis in paper: [explicit] The paper discusses the controllability property and mentions that existing part-prototype models are static and user-independent, but suggests that a graphical user interface could enable personalized explanations and explanatory debugging.
- Why unresolved: The paper suggests that further research is needed to enable users to manipulate the explanation and model's reasoning in part-prototype models.
- What evidence would resolve it: Development and evaluation of a graphical user interface for part-prototype models that allows users to interact with the model and manipulate its reasoning, along with experimental results showing the effectiveness of such an interface.

## Limitations
- Heavy reliance on theoretical analysis rather than empirical validation of the Co-12 framework applied to part-prototype models
- Many proposed evaluation methods are conceptual rather than demonstrated through experiments
- Lack of empirical evidence for the effectiveness of proposed evaluation approaches, particularly for user studies

## Confidence
- High confidence in part-prototype models being interpretable by design (Mechanism 1)
- Medium confidence in completeness of Co-12 framework for part-prototype evaluation (Mechanism 2)
- Medium confidence in proposed adaptations of existing XAI evaluation methods (Mechanism 3)

## Next Checks
1. Implement and validate deletion/addition methodology adapted for part-prototype models on CUB-200 benchmark to empirically test correctness claims
2. Conduct user study comparing human understanding of part-prototype explanations versus traditional heatmap-based explanations to evaluate completeness and context properties
3. Perform systematic experiments on prototype visualization correctness using controlled synthetic data where ground truth important features are known, testing multiple visualization methods across different part-prototype architectures