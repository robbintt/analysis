---
ver: rpa2
title: 'Deficiency of Large Language Models in Finance: An Empirical Examination of
  Hallucination'
arxiv_id: '2311.15548'
source_url: https://arxiv.org/abs/2311.15548
tags:
- stock
- financial
- finance
- price
- symbol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper empirically examines hallucination in large language
  models (LLMs) when applied to financial tasks. The authors evaluate LLM performance
  on three tasks: recognizing financial abbreviations and stock symbols, explaining
  financial terminology, and querying historical stock prices.'
---

# Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination

## Quick Facts
- arXiv ID: 2311.15548
- Source URL: https://arxiv.org/abs/2311.15548
- Reference count: 40
- Key outcome: Off-the-shelf LLMs frequently generate factually incorrect financial content, with accuracy ranging from 0-93% depending on task and model

## Executive Summary
This paper empirically examines hallucination in large language models when applied to financial tasks. The authors evaluate LLM performance on three tasks: recognizing financial abbreviations and stock symbols, explaining financial terminology, and querying historical stock prices. Testing general-purpose models like Llama2 and GPT-3.5/4, as well as domain-specific FinMA, results show that off-the-shelf LLMs frequently generate factually incorrect content in finance. To mitigate hallucinations, the authors evaluate four methods: few-shot learning, Decoding by Contrasting Layers (DoLa), Retrieval Augmentation Generation (RAG), and prompt-based tool learning for function calls. RAG and tool learning significantly improve performance, while few-shot learning and DoLa have limited impact. The study highlights the need for further research to reduce hallucinations and improve LLM reliability for practical financial applications.

## Method Summary
The paper evaluates large language models on three financial tasks: recognizing financial abbreviations and stock symbols, explaining financial terminology, and querying historical stock prices. Models tested include Llama2 (7B, 13B), Llama2-chat variants, GPT-3.5-turbo, GPT-4, and FinMA-7B-NLP. Four hallucination mitigation methods are implemented and evaluated: few-shot learning, DoLa, RAG, and prompt-based tool learning. The evaluation uses accuracy, Mean Absolute Error (MAE), and FactScore metrics across datasets for acronyms, stock symbols, terminology, and historical prices. Experiments compare zero-shot, few-shot, and fine-tuned models with and without mitigation methods.

## Key Results
- Off-the-shelf LLMs show high hallucination rates in financial tasks, with accuracy ranging from 0-93% depending on model and task
- RAG significantly improves factuality in acronym recognition and terminology explanation tasks
- Prompt-based tool learning achieves perfect accuracy (100%) for stock price queries with minimal training
- DoLa and few-shot learning show limited effectiveness, particularly when models lack comprehensive financial knowledge in pretraining data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval Augmented Generation (RAG) significantly improves factual accuracy in financial tasks by grounding model outputs in current, verifiable external knowledge.
- Mechanism: RAG retrieves relevant financial information from external sources (e.g., Wikipedia) during generation, reducing reliance on potentially outdated or incomplete training data.
- Core assumption: The external knowledge source contains accurate, up-to-date information that addresses the gaps in the model's pretraining data.
- Evidence anchors:
  - [abstract] "To alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including... the Retrieval Augmentation Generation (RAG) method..."
  - [section 4.2] "RAG significantly improves the factuality in finance... integrating RAG consistently elevates the performance of both Llama-2 and Llama-2-chat models."
  - [corpus] Weak evidence; no direct citations of RAG's effectiveness in finance from neighbors.

### Mechanism 2
- Claim: Prompt-based tool learning enables models to generate accurate function calls for real-time data retrieval, significantly improving performance on time-sensitive tasks like stock price queries.
- Mechanism: The model learns to generate correct Python function calls with appropriate parameters based on natural language instructions, allowing it to access current financial data via APIs.
- Core assumption: The model can accurately parse natural language instructions and translate them into syntactically correct function calls with appropriate parameters.
- Evidence anchors:
  - [abstract] "...the prompt-based tool learning method that generates correct function calls."
  - [section 4.2] "Prompt-based tool learning helps significantly on the time-sensitive task... Llama2-7B+tool and Llama2-7B-chat+tool achieving remarkable accuracies of 100.00% with only one training example."
  - [corpus] Weak evidence; no direct citations of prompt-based tool learning effectiveness from neighbors.

### Mechanism 3
- Claim: DoLa (Decoding by Contrasting Layers) improves factual accuracy by contrasting outputs from different model layers, assuming higher layers contain more factual knowledge.
- Mechanism: During decoding, the method contrasts outputs from different layers to select the most factually accurate response, reducing hallucinations.
- Core assumption: Higher layers of the model contain more factual knowledge that can be leveraged to reduce hallucinations and improve accuracy.
- Evidence anchors:
  - [abstract] "...Decoding by Contrasting Layers (DoLa) [3]..."
  - [section 4.2] "DoLa has limitations in enhancing models with knowledge gaps in training data... its effectiveness is limited when the underlying model lacks comprehensive knowledge in its pretraining dataset."
  - [corpus] Weak evidence; no direct citations of DoLa's effectiveness or limitations from neighbors.

## Foundational Learning

- Concept: Financial domain knowledge and terminology
  - Why needed here: Understanding financial concepts, acronyms, and stock symbols is crucial for evaluating model performance and identifying hallucinations in financial tasks.
  - Quick check question: What does the acronym "TIF" commonly stand for in finance? (Answer: Tax Increment Financing)

- Concept: Hallucination detection and mitigation techniques
  - Why needed here: Recognizing and addressing hallucinations is the core focus of the paper, requiring understanding of various detection methods and mitigation strategies.
  - Quick check question: What is the primary difference between RAG and prompt-based tool learning as hallucination mitigation techniques? (Answer: RAG retrieves external knowledge during generation, while tool learning enables models to generate function calls for real-time data access)

- Concept: Evaluation metrics for language model performance
  - Why needed here: Accurately assessing model performance across different financial tasks requires understanding of appropriate evaluation metrics like accuracy, FactScore, and MAE.
  - Quick check question: What metric is used to measure factuality in long-form financial term explanations? (Answer: FactScore)

## Architecture Onboarding

- Component map:
  1. Large Language Models (Llama2, GPT-3.5/4, FinMA)
  2. Financial task datasets (acronyms, stock symbols, terminology, historical prices)
  3. Hallucination mitigation methods (few-shot learning, DoLa, RAG, prompt-based tool learning)
  4. Evaluation metrics (accuracy, FactScore, MAE)
  5. External knowledge sources (Wikipedia, Alpha Vantage API)

- Critical path:
  1. Define financial tasks and evaluation metrics
  2. Prepare datasets and prompts
  3. Implement hallucination mitigation methods
  4. Run experiments with different models and methods
  5. Analyze results and draw conclusions

- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but require more resources
  - External knowledge vs. model knowledge: RAG and tool learning improve accuracy but add complexity and latency
  - Task-specific vs. general approaches: Specialized models may perform better on specific tasks but lack versatility

- Failure signatures:
  - Low accuracy on financial tasks indicates hallucinations or insufficient domain knowledge
  - High MAE in stock price queries suggests model reliance on outdated information or inability to access current data
  - Minimal improvement with mitigation methods indicates potential issues with method implementation or task suitability

- First 3 experiments:
  1. Evaluate base models (Llama2, GPT-3.5-turbo, GPT-4) on all three financial tasks without any mitigation methods
  2. Apply RAG to acronym recognition and terminology explanation tasks, measure improvement in accuracy and FactScore
  3. Implement prompt-based tool learning for stock price queries, compare accuracy with and without the method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hallucinations in LLMs vary across different financial subdomains (e.g., stock trading vs. real estate vs. insurance)?
- Basis in paper: [inferred] The paper focuses on general finance tasks but doesn't explore subdomain-specific differences in hallucination patterns.
- Why unresolved: The study uses a limited set of general finance tasks without distinguishing between different financial subdomains that may have unique terminology and knowledge structures.
- What evidence would resolve it: Comparative analysis of LLM performance across multiple financial subdomains using domain-specific datasets and hallucination metrics.

### Open Question 2
- Question: What is the long-term impact of domain-specific fine-tuning on LLM hallucination rates across diverse financial tasks?
- Basis in paper: [explicit] The paper notes that FinMA-7B underperforms its base model after multi-task domain-specific fine-tuning.
- Why unresolved: The study only examines one domain-specific model and doesn't investigate the long-term effects of fine-tuning on hallucination patterns.
- What evidence would resolve it: Longitudinal study tracking hallucination rates across multiple domain-specific models and diverse financial tasks over extended periods.

### Open Question 3
- Question: How do different hallucination mitigation strategies perform when combined versus used individually?
- Basis in paper: [explicit] The paper evaluates four mitigation methods separately but doesn't explore their combined effects.
- Why unresolved: Each method (RAG, tool learning, few-shot learning, DoLa) is tested in isolation without investigating potential synergies.
- What evidence would resolve it: Systematic evaluation of all possible combinations of mitigation methods across various financial tasks and model architectures.

## Limitations

- Evaluation focuses primarily on English-language financial data, potentially limiting applicability to other languages or regional financial markets
- The effectiveness of DoLa appears constrained by the underlying model's pretraining data quality, with limited exploration of alternative mitigation strategies
- The study examines a limited set of general finance tasks without distinguishing between different financial subdomains that may have unique terminology and knowledge structures

## Confidence

**High Confidence:**
- The observation that off-the-shelf LLMs frequently generate factually incorrect financial content is well-supported by the empirical results across multiple models and tasks
- The effectiveness of RAG in improving factuality for acronym recognition and terminology explanation is consistently demonstrated across different model variants
- The success of prompt-based tool learning for stock price queries is clearly evidenced by the perfect accuracy scores achieved

**Medium Confidence:**
- The relative ineffectiveness of few-shot learning and DoLa methods, while observed, may depend on specific implementation details not fully explored in the paper
- The FinMA-7B-NLP model's performance improvements could be influenced by factors beyond just financial domain fine-tuning, such as general pretraining quality

**Low Confidence:**
- The claim that hallucinations stem primarily from knowledge gaps in pretraining data, rather than potential issues with model architecture or training objectives
- The assertion that current mitigation methods are sufficient for practical financial applications, given the limited scope of tested scenarios

## Next Checks

1. **Cross-domain validation**: Test the effectiveness of RAG and tool learning on non-English financial datasets and regional stock markets to assess generalizability

2. **Ablation study**: Conduct controlled experiments removing Wikipedia integration from RAG to quantify the specific contribution of external knowledge vs. model capabilities

3. **Long-term stability analysis**: Evaluate model performance over extended time periods to assess whether improvements from mitigation methods persist as financial information evolves