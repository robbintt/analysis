---
ver: rpa2
title: Hypergraph-Guided Disentangled Spectrum Transformer Networks for Near-Infrared
  Facial Expression Recognition
arxiv_id: '2312.05907'
source_url: https://arxiv.org/abs/2312.05907
tags:
- expression
- facial
- recognition
- proposed
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of near-infrared (NIR) facial
  expression recognition (FER) by proposing a novel transformer-based method called
  NFER-Former. The key idea is to leverage abundant visible (VIS) expression data
  to improve NIR FER performance through a self-attention orthogonal decomposition
  (SAOD) module and a hypergraph-guided feature embedding (HGFE) module.
---

# Hypergraph-Guided Disentangled Spectrum Transformer Networks for Near-Infrared Facial Expression Recognition

## Quick Facts
- arXiv ID: 2312.05907
- Source URL: https://arxiv.org/abs/2312.05907
- Reference count: 7
- Primary result: NFER-Former achieves 84.03% accuracy and 83.82% F1 score on Oulu-CASIA, outperforming state-of-the-art NIR FER methods.

## Executive Summary
This paper addresses the challenge of near-infrared (NIR) facial expression recognition (FER) by proposing NFER-Former, a transformer-based method that leverages abundant visible (VIS) expression data to improve NIR FER performance. The key innovation lies in the Self-Attention Orthogonal Decomposition (SAOD) module, which disentangles expression and spectrum information from input images, and the Hypergraph-Guided Feature Embedding (HGFE) module, which models complex inter-class correlations among expressions using an AU-based knowledge hypergraph. The authors also construct a new Large-HFE dataset with 360 subjects to validate their approach, demonstrating significant performance improvements over state-of-the-art methods.

## Method Summary
NFER-Former combines a Vision Transformer backbone with two novel modules: SAOD and HGFE. SAOD uses dual-head self-attention with endogenously orthogonal projection to decompose input features into modality-specific (spectrum) and modality-invariant (expression) subspaces. HGFE constructs an AU-based knowledge hypergraph representing co-occurrence relationships between action units and expressions, then uses hypergraph neural networks to learn knowledge-guided expression features. The method is trained end-to-end using joint loss functions and achieves state-of-the-art results on Oulu-CASIA and the newly constructed Large-HFE dataset.

## Key Results
- Achieves 84.03% accuracy and 83.82% F1 score on Oulu-CASIA dataset
- Achieves 82.17% accuracy and 81.99% F1 score on Large-HFE dataset
- Outperforms state-of-the-art methods by significant margins in both NIR FER benchmarks

## Why This Works (Mechanism)

### Mechanism 1
The self-attention orthogonal decomposition (SAOD) module disentangles modality-specific (spectrum) and modality-invariant (expression) features from NIR-VIS facial images. Using dual-head self-attention with endogenously orthogonal projection (Householder transformation), it decomposes input features into two orthogonal subspaces without external constraints. This works because spectrum information dominates intra-class variation between NIR and VIS modalities and can be separated from expression information. If spectrum information is not the dominant source of modality-specific variation, the decomposition will fail to improve expression recognition accuracy.

### Mechanism 2
The hypergraph-guided feature embedding (HGFE) module models complex inter-class correlations among facial expressions to alleviate inter-class similarity. It constructs an AU-based knowledge hypergraph representing co-occurrence relationships between action units and expressions, then uses hypergraph neural networks to learn knowledge-guided expression features. This works because different facial expressions share key facial behaviors (AUs) that create complex correlations, and modeling these correlations improves discriminative feature learning. If the AU-expression relationships are not consistent or well-represented in the dataset, the hypergraph model will not effectively reduce inter-class similarity.

### Mechanism 3
The combination of SAOD and HGFE modules in NFER-Former significantly improves NIR facial expression recognition by leveraging abundant VIS expression data. SAOD extracts modality-invariant expression features by removing spectrum variation, while HGFE refines these features by modeling inter-class correlations, creating a two-stage feature enhancement process. This works because VIS expression data contains valuable information that can improve NIR expression recognition when properly aligned and processed. If VIS and NIR modalities have fundamentally different expression patterns that cannot be aligned, the fusion approach will degrade performance.

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: The SAOD module adapts transformer encoder layers to perform orthogonal decomposition of facial expression features
  - Quick check question: How does self-attention weight the importance of different tokens in a sequence, and why is this property useful for feature decomposition?

- Concept: Orthogonal matrix decomposition
  - Why needed here: The SAOD module uses Householder transformation to create endogenously orthogonal projection matrices for feature decomposition
  - Quick check question: What properties make a matrix orthogonal, and how does Householder transformation guarantee these properties?

- Concept: Hypergraph neural networks
  - Why needed here: The HGFE module uses hypergraph neural networks to model complex AU-expression relationships that cannot be captured by simple pairwise graphs
  - Quick check question: How do hypergraph neural networks differ from standard graph neural networks in handling higher-order relationships?

## Architecture Onboarding

- Component map: Image → Patch embedding → [spectrum] token → SAOD → HGFE → Expression classification
- Critical path: Image → Patch embedding → SAOD → HGFE → Expression classification
- Design tradeoffs:
  - Orthogonal decomposition vs. domain adaptation: SAOD provides explicit separation of spectrum and expression features
  - Hypergraph vs. graph: HGFE captures complex AU-expression relationships better than pairwise approaches
  - Complexity vs. performance: The two-module approach increases computational cost but achieves significant accuracy improvements

- Failure signatures:
  - Poor performance on both modalities: Indicates backbone or training issues
  - Good VIS performance but poor NIR performance: Suggests SAOD decomposition is not working effectively
  - Good modality-invariant features but poor expression classification: Indicates HGFE module is not properly modeling inter-class correlations

- First 3 experiments:
  1. Baseline ViT performance on Oulu-CASIA to establish performance without SAOD or HGFE
  2. SAOD module alone to verify spectrum information disentanglement
  3. HGFE module alone to verify inter-class correlation modeling effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of NFER-Former compare to state-of-the-art methods when trained only on NIR data without VIS augmentation? The paper extensively compares NFER-Former against baselines when trained on NIR+VIS and NIR+VIS+Extra VIS data, but does not present results for training solely on NIR data.

### Open Question 2
How does NFER-Former perform on in-the-wild NIR facial expression datasets that are not captured in controlled laboratory conditions? The paper constructs Large-HFE with controlled laboratory conditions and evaluates on Oulu-CASIA. Real-world applications would likely involve more challenging, uncontrolled environments.

### Open Question 3
What is the impact of different AU (Action Unit) subsets on the performance of the HGFE module? Are all 27 AUs equally important for expression recognition? The HGFE module uses an AU-based knowledge hypergraph constructed according to (Du, Tao, and Martinez 2014), but the paper does not analyze the contribution of individual AUs or explore different AU subsets.

## Limitations
- Performance validation limited to laboratory-controlled datasets may not reflect real-world challenges
- Computational complexity and memory requirements not thoroughly analyzed for practical deployment
- No analysis of individual AU contributions to HGFE module performance

## Confidence

High confidence: NFER-Former achieves state-of-the-art performance on Oulu-CASIA and Large-HFE datasets, with reported accuracy and F1 scores significantly higher than baseline methods.

Medium confidence: The SAOD and HGFE modules effectively improve NIR FER by disentangling spectrum and expression information and modeling inter-class correlations, respectively. However, the generalizability of these improvements to other NIR-VIS FER datasets is uncertain.

Low confidence: The proposed method's computational efficiency and scalability to larger datasets are not thoroughly discussed or validated.

## Next Checks

1. Test NFER-Former on additional NIR-VIS FER datasets to assess generalizability and robustness to dataset variations.
2. Conduct ablation studies to quantify the individual contributions of the SAOD and HGFE modules to overall performance improvements.
3. Evaluate the computational efficiency and memory requirements of NFER-Former compared to baseline methods, especially for real-time applications.