---
ver: rpa2
title: Assessing Large Language Models on Climate Information
arxiv_id: '2310.02932'
source_url: https://arxiv.org/abs/2310.02932
tags:
- climate
- answer
- change
- question
- assistance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a comprehensive evaluation framework for
  Large Language Models (LLMs) on climate information, grounded in science communication
  research. The framework assesses both presentational (style, clarity, correctness,
  tone) and epistemological (accuracy, specificity, completeness, uncertainty) dimensions
  across 30 distinct issues.
---

# Assessing Large Language Models on Climate Information

## Quick Facts
- arXiv ID: 2310.02932
- Source URL: https://arxiv.org/abs/2310.02932
- Reference count: 40
- Primary result: LLMs show high presentational quality but significant epistemological gaps in climate communication

## Executive Summary
This study introduces a comprehensive evaluation framework for Large Language Models (LLMs) on climate information, grounded in science communication research. The framework assesses both presentational (style, clarity, correctness, tone) and epistemological (accuracy, specificity, completeness, uncertainty) dimensions across 30 distinct issues. A novel protocol using AI Assistance and raters with relevant educational backgrounds improves rating performance. Experiments on recent LLMs reveal that while surface-level quality is high, epistemological scores lag significantly, with room for improvement in completeness and uncertainty communication. The evaluation demonstrates sufficient resolution to distinguish model performance and identifies attribution-based methods as complementary but not fully correlated with epistemological quality.

## Method Summary
The study collected 300 climate-related questions from Wikipedia, Skeptical Science, and Google Trends, then generated LLM answers (3-4 sentences each). Raters with relevant educational backgrounds evaluated answers across four presentational and four epistemological dimensions, using AI Assistance (GPT-4-generated critiques) for half the evaluations. Keypoints and supporting evidence were extracted from answers to aid evaluation. The framework compared multiple LLMs including GPT-4, ChatGPT, InstructGPT, PaLM-2, and Falcon-180B-Chat.

## Key Results
- ChatGPT-3.5 excels in presentational quality but ranks fifth in epistemological scores
- Epistemological scores lag behind presentational scores across all evaluated models
- AI Assistance improves rater detection of real issues without replacing human judgment
- Attribution-based methods (AIS) are complementary but not fully correlated with epistemological quality
- The framework successfully distinguishes model performance with sufficient resolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI Assistance improves rater detection of real issues without replacing human judgment
- Mechanism: The assistance provides targeted, concise critiques grounded in supporting evidence, which raises rater awareness of issues they might otherwise miss
- Core assumption: Human raters can effectively use AI-generated critiques when they are specific and evidence-backed
- Evidence anchors:
  - Abstract: "To improve rating performance, we introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational background"
  - Section: "We find supporting evidence in two separate experiments... The data we collected on the helpfulness of assistance suggests that when raters do not find assistance helpful, they give higher ratings"
  - Corpus: Found 25 related papers, but none directly testing AI Assistance for human raters in scientific communication contexts

### Mechanism 2
- Claim: Separating presentational and epistemological dimensions provides clearer diagnostic insights than single-score evaluations
- Mechanism: By evaluating both surface quality and content depth, the framework reveals where models excel and where they fall short
- Core assumption: Presentational quality does not strongly correlate with epistemological quality
- Evidence anchors:
  - Abstract: "Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication"
  - Section: "ChatGPT-3.5 is the best overall in presentation, but... places fifth on epistemological scores"
  - Corpus: Related work (e.g., Liu et al. 2023) shows inverse correlation between fluency/utility and evidential support

### Mechanism 3
- Claim: Domain-specific expertise in raters improves evaluation quality compared to general crowdsourcing
- Mechanism: Raters with climate-related educational backgrounds bring contextual knowledge that helps them detect subtle inaccuracies
- Core assumption: Relevant educational background is sufficient for detecting domain-specific issues
- Evidence anchors:
  - Abstract: "We introduce a novel protocol... that relies on raters with relevant educational background"
  - Section: "Our rating task involves evaluating an answer to a climate-related question... We select candidate raters with relevant educational background"
  - Corpus: Med-PaLM study (Singhal et al. 2023) shows expert-level evaluation is needed for medical domains

## Foundational Learning

- Concept: Scalable oversight
  - Why needed here: Human evaluation of LLM outputs becomes infeasible at scale; AI Assistance extends human capacity without replacing judgment
  - Quick check question: How does AI Assistance differ from full automation in evaluation protocols?

- Concept: Attribution vs. epistemological quality
  - Why needed here: Understanding that grounding in sources (AIS) is necessary but not sufficient for high-quality scientific communication
  - Quick check question: Can an answer be fully attributed but still epistemologically weak? (Yes, if completeness or uncertainty are poor.)

- Concept: Science communication dimensions
  - Why needed here: The framework relies on established principles from communication science to define what makes climate information adequate
  - Quick check question: Why distinguish between presentational and epistemological adequacy rather than a single "quality" score?

## Architecture Onboarding

- Component map: Question generation -> Answer generation -> Keypoint extraction -> Evidence retrieval -> AI Assistance generation -> Human rating (4 dimensions Ã— 2 adequacy types) -> Aggregation and analysis
- Critical path: Question -> Answer -> Keypoint -> Evidence -> Rating (without Assistance) -> Rating (with Assistance) -> Validation
- Design tradeoffs: 
  - Assistance adds quality but requires generation overhead
  - Domain-specific raters improve accuracy but reduce pool size
  - Wikipedia-only evidence ensures consistency but limits coverage
- Failure signatures: 
  - Low agreement among raters -> unclear issue definitions
  - High ratings despite assistance -> assistance too generic
  - Long rating times -> interface or task complexity issues
- First 3 experiments:
  1. Run same questions with and without AI Assistance on a small subset to measure detection improvement
  2. Compare ratings from climate-educated vs. general educated raters on identical examples
  3. Test whether attribution-based metrics (AIS) predict epistemological ratings on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-offs between presentational and epistemological qualities be better balanced in LLM-generated climate information?
- Basis in paper: Explicit
- Why unresolved: The paper identifies a performance gap between surface-level presentation and deeper epistemological qualities, with models like ChatGPT-3.5 excelling in presentation but ranking fifth in epistemological scores
- What evidence would resolve it: Experiments varying model architectures, training objectives, and evaluation metrics that directly measure the relationship between presentational and epistemological qualities

### Open Question 2
- Question: How do different audience characteristics affect the effectiveness of LLM-generated climate communication?
- Basis in paper: Inferred
- Why unresolved: The paper notes that ideal answers should be tailored to audience attributes but doesn't explore how different audiences respond differently to LLM outputs
- What evidence would resolve it: User studies with diverse audience segments evaluating the same LLM responses, measuring comprehension, trust, and behavioral intentions across different demographic groups

### Open Question 3
- Question: What is the relationship between attribution-based evaluation methods and epistemological quality assessment in LLM outputs?
- Basis in paper: Explicit
- Why unresolved: The paper finds attribution methods (AIS) to be mostly orthogonal and complementary to epistemological assessments
- What evidence would resolve it: Comparative studies measuring correlation coefficients between attribution scores and epistemological ratings across multiple domains

## Limitations
- The use of Wikipedia as the sole evidence source may underestimate model performance in accessing broader knowledge bases
- The relatively small pool of 17 qualified raters raises concerns about reproducibility and potential bias
- The effectiveness of AI Assistance depends heavily on the quality of generated critiques

## Confidence
- **High confidence**: The framework's ability to distinguish model performance across presentational and epistemological dimensions
- **Medium confidence**: The finding that AI Assistance improves rater performance is supported by experimental data
- **Low confidence**: The claim that domain-specific expertise is sufficient for reliable evaluation

## Next Checks
1. Replicate the rating experiment with a larger, more diverse pool of raters across different educational backgrounds
2. Test the framework's sensitivity by introducing controlled variations in answer quality and measuring whether ratings correctly identify these differences
3. Compare the presentational-epistemological gap across multiple knowledge domains to determine if the pattern is domain-specific or generalizable