---
ver: rpa2
title: The Impact of Background Removal on Performance of Neural Networks for Fashion
  Image Classification and Segmentation
arxiv_id: '2308.09764'
source_url: https://arxiv.org/abs/2308.09764
tags:
- background
- removal
- fashion
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether background removal improves model
  performance on fashion data, addressing the challenge of diverse backgrounds in
  fashion images. The core method involves using Salient Object Detection to remove
  backgrounds from fashion images, creating "rembg" images for comparison with original
  images.
---

# The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation

## Quick Facts
- arXiv ID: 2308.09764
- Source URL: https://arxiv.org/abs/2308.09764
- Reference count: 36
- Background removal improves classification accuracy by up to 5% in shallow networks trained from scratch on FashionStyle14 dataset

## Executive Summary
This paper investigates whether background removal improves model performance on fashion data, addressing the challenge of diverse backgrounds in fashion images. The core method involves using Salient Object Detection to remove backgrounds from fashion images, creating "rembg" images for comparison with original images. Extensive experiments across various model architectures, initialization methods, data augmentations, and task types show that background removal is beneficial only for shallow networks trained from scratch in classification tasks, improving accuracy by up to 5%. However, it is incompatible with deep networks, pre-trained initialization, and most data augmentation techniques due to increased overfitting risk and loss of background information. Background removal does not benefit segmentation tasks, as annotations are unaffected by background changes.

## Method Summary
The paper uses Salient Object Detection (SOD) to remove backgrounds from fashion images, creating "rembg" images for comparison with original images. Extensive experiments are conducted across various model architectures (VGG, ResNet, ResNeSt, Swin Transformer), initialization methods (random, pre-trained), data augmentations, and task types (classification, instance segmentation, semantic segmentation) using FashionStyle14 and Fashionpedia datasets. The method compares model performance on original vs. rembg images to determine the impact of background removal.

## Key Results
- Background removal improves classification accuracy by up to 5% in shallow networks trained from scratch on FashionStyle14 dataset
- Background removal is incompatible with deep networks, pre-trained initialization, and most data augmentation techniques
- Background removal does not benefit segmentation tasks, as annotations are unaffected by background changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Background removal improves classification accuracy in shallow networks by reducing noise and forcing focus on foreground objects
- Mechanism: Salient Object Detection removes background pixels, reducing visual noise and allowing simpler models to concentrate on discriminative foreground features without complex regularization
- Core assumption: Shallow networks lack strong regularization and benefit from explicit noise reduction
- Evidence anchors:
  - [abstract]: "background removal can effectively work for fashion data in simple and shallow networks that are not susceptible to overfitting. It can improve model accuracy by up to 5% in the classification on the FashionStyle14 dataset when training models from scratch."
  - [section]: "background-removed (rembg) images significantly improve performance in the VGG series, especially for VGG16 without batch normalization, which can improve over 5% accuracy."
  - [corpus]: Weak evidence; related work on background removal focuses on interpretability rather than accuracy gains
- Break condition: When models are deep or include batch normalization, background removal conflicts with regularization and harms performance

### Mechanism 2
- Claim: Background removal is incompatible with pre-trained initialization and deep architectures due to loss of domain consistency and overfitting risk
- Mechanism: Pre-trained models learn features from ImageNet's diverse backgrounds; removing backgrounds in fashion images creates domain mismatch that degrades transfer learning. Deep networks rely on regularization (batch norm, dropout) that background removal undermines
- Core assumption: ImageNet-pretrained models and deep architectures depend on background context for feature generalization
- Evidence anchors:
  - [abstract]: "background removal does not perform well in deep neural networks due to incompatibility with other regularization techniques like batch normalization, pre-trained initialization, and data augmentations introducing randomness."
  - [section]: "All the models training from pre-trained parameters on ImageNet perform worse on rembg images than on original images."
  - [corpus]: No direct corpus support; related papers focus on bias removal rather than initialization compatibility
- Break condition: When using transfer learning or deep models, the loss of background context outweighs any foreground focus benefit

### Mechanism 3
- Claim: Background removal does not benefit segmentation tasks because annotations are spatially independent of background changes
- Mechanism: Segmentation models rely on precise location annotations (bounding boxes, masks) that remain valid regardless of background removal; the loss of background pixels does not improve mask prediction
- Core assumption: Segmentation annotation coordinates are defined relative to the object, not the background
- Evidence anchors:
  - [abstract]: "Background removal does not benefit segmentation tasks, as annotations are unaffected by background changes."
  - [section]: "background removal can not positively influence fashion image segmentation since the bbox and mask annotations can not be affected by background."
  - [corpus]: No corpus evidence; related work focuses on background bias rather than annotation invariance
- Break condition: When segmentation requires contextual background cues for object separation, background removal may degrade performance

## Foundational Learning

- Concept: Salient Object Detection (SOD)
  - Why needed here: SOD is the core technique for background removal; understanding its limitations is essential for interpreting results
  - Quick check question: What happens if the fashion image has a person in dark clothing against a dark background—will SOD still work effectively?

- Concept: Transfer learning and domain adaptation
  - Why needed here: Explains why pre-trained models perform worse on rembg images; understanding feature generalization across domains is critical
  - Quick check question: If you fine-tune a model pre-trained on ImageNet on fashion images with backgrounds removed, will the learned features still transfer effectively?

- Concept: Regularization techniques (batch normalization, dropout, data augmentation)
  - Why needed here: Background removal conflicts with these techniques; understanding their role clarifies why deep networks suffer
  - Quick check question: How does batch normalization's reliance on mini-batch statistics conflict with background removal's elimination of background pixels?

## Architecture Onboarding

- Component map:
  Data preprocessing: Salient Object Detection (Rembg) → original vs. rembg images
  Model zoo: VGG (shallow), ResNet (deep), ResNeSt (attention), Swin (transformer)
  Tasks: Classification (FashionStyle14), Instance Segmentation (Fashionpedia), Semantic Segmentation (Fashionpedia)
  Training: Random init vs. pre-trained init, with/without batch norm, with/without data augmentation

- Critical path:
  1. Preprocess images with Rembg to create rembg dataset
  2. Train baseline models on original images
  3. Train same models on rembg images under controlled conditions
  4. Compare accuracy, focusing on shallow vs. deep, random vs. pre-trained

- Design tradeoffs:
  - Background removal vs. data augmentation: mutually exclusive due to loss of background pixels
  - Shallow vs. deep: shallow benefit from noise reduction; deep suffer from overfitting
  - Random init vs. pre-trained: only random init benefits from background removal

- Failure signatures:
  - Accuracy drop on deep models with rembg input
  - Incompatibility errors when applying CutMix/MixUp to rembg images
  - No improvement or degradation in segmentation mAP/IoU

- First 3 experiments:
  1. Train VGG16 with and without batch norm on original vs. rembg FashionStyle14 images; measure Top-1 accuracy
  2. Fine-tune ResNet-50 pre-trained on ImageNet on original vs. rembg images; compare Top-1 accuracy
  3. Train Mask R-CNN with ResNeXt-101 backbone on original vs. rembg Fashionpedia images; measure bbox-mAP and segm-mAP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of background removal vary across different fashion datasets with varying background complexities and fashion object salience?
- Basis in paper: [explicit] The paper states that SOD-based background removal works well for datasets like FashionStyle14 and Fashionpedia where fashion objects are clearly visible and centered, but acknowledges that some fashion images (e.g., product details or partial models) are not suitable for background removal
- Why unresolved: The study only tested on two specific datasets (FashionStyle14 and Fashionpedia) with relatively consistent image characteristics. It does not explore performance across a broader range of fashion datasets with varying background complexities or fashion object salience levels
- What evidence would resolve it: Systematic experiments comparing background removal performance across multiple fashion datasets with diverse background types (studio, street, event, online shopping), varying levels of fashion object visibility, and different image compositions

### Open Question 2
- Question: Can alternative background removal techniques (beyond SOD) improve model performance for fashion classification and segmentation tasks?
- Basis in paper: [explicit] The paper specifically uses SOD-based methods (Rembg tool based on U2-Net) for background removal and notes that this approach has limitations, particularly for images that don't have salient persons in clothes positioned in the center
- Why unresolved: The study only evaluated one background removal technique and did not explore other potential methods such as depth estimation, instance segmentation-based background removal, or hybrid approaches that might better handle edge cases
- What evidence would resolve it: Comparative experiments testing multiple background removal techniques (SOD, depth-based, instance segmentation-based, hybrid approaches) across various fashion datasets and task types to determine which methods provide optimal performance improvements

### Open Question 3
- Question: What is the optimal balance between background removal and data augmentation for different model architectures and training scenarios?
- Basis in paper: [explicit] The paper demonstrates that background removal is incompatible with data augmentation techniques because they serve opposite purposes - background removal reduces information while data augmentation introduces randomness for regularization
- Why unresolved: The study identifies this incompatibility but does not explore potential hybrid approaches or adaptive strategies that could dynamically balance background removal and data augmentation based on model architecture depth, initialization method, or training stage
- What evidence would resolve it: Experiments developing and testing adaptive background removal/data augmentation strategies, potentially including conditional application, progressive background removal during training, or architecture-specific optimization of the background removal vs. augmentation trade-off

## Limitations
- The effectiveness of background removal may vary with different image characteristics, object poses, or clothing styles across fashion datasets
- Specific hyperparameters for each model architecture and training configuration are not fully detailed, affecting reproducibility
- The study focuses on specific datasets (FashionStyle14 and Fashionpedia), limiting generalizability to other domains

## Confidence
- High: Background removal improves shallow network performance on classification tasks
- Medium: Incompatibility with pre-trained initialization and deep architectures
- Low: Background removal does not benefit segmentation tasks

## Next Checks
1. Test background removal effectiveness on a different fashion dataset (e.g., DeepFashion) to assess generalizability across domains
2. Investigate the impact of background removal on semi-supervised learning scenarios where unlabeled data with diverse backgrounds is used for pre-training
3. Explore hybrid approaches that combine background removal with selective data augmentation techniques that preserve background information for deep networks