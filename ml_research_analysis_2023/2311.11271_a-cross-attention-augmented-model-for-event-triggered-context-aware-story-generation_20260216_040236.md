---
ver: rpa2
title: A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation
arxiv_id: '2311.11271'
source_url: https://arxiv.org/abs/2311.11271
tags:
- story
- generation
- event
- stories
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EtriCA, a novel neural generation model for
  event-triggered context-aware story generation. The key idea is to use a cross-attention
  mechanism to map context features onto event sequences through residual mapping,
  enabling the model to exploit logical relationships between events more effectively.
---

# A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation

## Quick Facts
- arXiv ID: 2311.11271
- Source URL: https://arxiv.org/abs/2311.11271
- Reference count: 6
- Primary result: 15-25% perplexity reduction and 10%+ human evaluation improvement over state-of-the-art baselines

## Executive Summary
This paper introduces EtriCA, a neural generation model for event-triggered context-aware story generation that employs a cross-attention mechanism to map context features onto event sequences through residual mapping. The model also incorporates a post-training framework on BookCorpus for knowledge enhancement, resulting in approximately 5% improvement in automatic metrics and over 10% improvement in human evaluation. Extensive experiments demonstrate EtriCA's superiority over state-of-the-art baselines in terms of fluency, coherence, and relevance metrics.

## Method Summary
EtriCA uses a dual-encoder architecture with separate BART encoders for context and event sequences, followed by a cross-attention mechanism that maps context feature matrices onto event feature matrices via weighted summation. The resulting context-aware event features are combined with original event features through residual mapping. The decoder incorporates a sentence similarity prediction auxiliary task to enhance coherence between sentences. KeEtriCA extends this by post-training on BookCorpus for broader genre adaptation. The model is trained with token-level cross-entropy loss and sentence similarity loss, using sampling strategies for inference.

## Key Results
- 15% perplexity reduction on ROC dataset compared to state-of-the-art baselines
- 25% perplexity reduction on Writing Prompts dataset
- Over 10% improvement in human evaluation metrics for fluency, coherence, and relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention enables the model to fuse context features into event representations, improving coherence and relevance.
- Mechanism: A dual-encoder structure encodes context and events separately. A cross-attention layer maps context feature matrices onto event feature matrices via weighted summation. The resulting context-aware event features are added back to the original event features through residual mapping, producing a richer representation that the decoder can exploit for generating coherent stories.
- Core assumption: Context and event features are initially orthogonal and complementary, and their fusion requires explicit attention rather than simple concatenation.
- Evidence anchors:
  - [abstract]: "enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping"
  - [section]: "We employ a cross attention mechanism...to implicitly map contextual features to event features by employing information fusion techniques within their respective numeric vector spaces"
  - [corpus]: Found 25 related papers; average neighbor FMR=0.382, but no specific citations on this cross-attention mechanism.
- Break condition: If context and events share overlapping semantics or if the cross-attention weighting collapses to near-uniform, the fusion may become redundant or harmful.

### Mechanism 2
- Claim: Post-training on BookCorpus increases the model's adaptability to diverse story genres and structures.
- Mechanism: After fine-tuning on the target datasets (ROC and WP), the model undergoes further training on BookCorpus, which contains over 11,000 books spanning multiple genres. This extended training exposes the model to varied narrative styles, plot dynamics, and character archetypes, improving generalization and creative aptitude.
- Core assumption: BookCorpus narratives are sufficiently diverse and high-quality to transfer beneficial stylistic and structural knowledge to the model's story generation capability.
- Evidence anchors:
  - [abstract]: "we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples"
  - [section]: "By leveraging the extensive textual data provided by BookCorpus, the language model of KeEtriCA is empowered to generate captivating and coherent stories across multiple genres"
  - [corpus]: Found 25 related papers; no specific citations on BookCorpus post-training for story generation.
- Break condition: If the BookCorpus contains low-quality or noisy data, or if the distribution mismatch with target datasets is too large, post-training could degrade performance.

### Mechanism 3
- Claim: Sentence similarity prediction auxiliary task improves sentence-level coherence by aligning generated sentence representations with reference story sentence embeddings.
- Mechanism: During training, the decoder predicts special sentence-separator tokens. The hidden state at each separator is used to compute similarity scores with all other sentences' representations (via dot product and sigmoid). These scores are compared to ground-truth cosine similarities computed from Sentence-BERT embeddings of reference stories, and the loss encourages the model to generate sentences that maintain consistent semantic similarity patterns with reference stories.
- Core assumption: Sentence-level coherence in stories can be captured by pairwise similarity distributions, and the model can learn to replicate these patterns through supervision.
- Evidence anchors:
  - [abstract]: "we introduce an auxiliary task of Sentence Similarity Prediction...to enhance the coherence between sentences driven by events"
  - [section]: "We employ Sentence-BERT to obtain a numerical vector...encapsulating the features of individual sentences...we enforce the similarity score between generated sentences to align with the similarities observed in reference stories"
  - [corpus]: Found 25 related papers; no specific citations on sentence similarity prediction for story coherence.
- Break condition: If the sentence similarity metric does not correlate well with human judgments of coherence, or if the auxiliary task overwhelms the main generation objective, the approach may fail.

## Foundational Learning

- Concept: Dependency parsing for event extraction
  - Why needed here: Events are represented as verb phrases with arguments (subject, object, modifiers). Dependency parsing provides the syntactic relations needed to extract these components accurately, going beyond simple verb extraction.
  - Quick check question: What is the role of the `dep` attribute in spaCy's dependency parsing, and how does it help identify event triggers and arguments?

- Concept: Residual mapping in neural networks
  - Why needed here: Residual connections help the model learn the context gap between event sequences and full stories by adding context features to event features, preserving both representations and facilitating gradient flow.
  - Quick check question: In the residual mapping equation `F_he = F_e + β ⊙ F_ca`, what would happen if `β` is set to zero?

- Concept: Cross-attention mechanism
  - Why needed here: Cross-attention allows the model to attend to context features while processing events, dynamically weighting context relevance for each event. This is crucial for generating coherent stories where events depend on contextual details.
  - Quick check question: How does the cross-attention score computation differ from standard self-attention in terms of the query, key, and value matrices?

## Architecture Onboarding

- Component map:
  Context → Context encoder → Context features → Cross-attention → Context-aware event features → Residual fusion → Decoder → Generated story tokens

- Critical path:
  Context → Context encoder → Context features → Cross-attention → Context-aware event features → Residual fusion → Decoder → Generated story tokens

- Design tradeoffs:
  - Separate encoders vs. single encoder: Separate encoders allow specialized processing but increase parameters; single encoder is more parameter-efficient but may struggle with heterogeneous feature fusion.
  - Post-training vs. no post-training: Post-training on BookCorpus improves generalization but requires more compute and risks negative transfer if distributions mismatch.
  - Sentence similarity auxiliary task: Improves coherence but adds complexity and training time.

- Failure signatures:
  - High perplexity but low BLEU: Model generates fluent but off-topic text.
  - Low coherence scores: Context-event fusion fails or sentence similarity task ineffective.
  - High repetition: Decoder overfits to training data or lacks diversity mechanisms.

- First 3 experiments:
  1. Ablation: Remove cross-attention module and compare coherence/relevance metrics.
  2. Ablation: Remove sentence similarity prediction and evaluate impact on sentence-level coherence.
  3. Ablation: Remove post-training on BookCorpus and measure performance drop on ROC and WP datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the event extraction framework be improved to capture more diverse and nuanced semantic roles in narratives?
- Basis in paper: [explicit] The paper mentions that the current event extraction relies on a limited set of dependencies and roles, and acknowledges that some essential roles may be missed.
- Why unresolved: The paper's event extraction is based on a fixed set of dependencies, which may not be comprehensive enough to capture all important semantic roles in narratives.
- What evidence would resolve it: A comparison of the proposed event extraction framework with alternative methods (e.g., using more advanced semantic role labeling techniques) on a benchmark dataset of narratives, demonstrating improvements in capturing diverse and nuanced semantic roles.

### Open Question 2
- Question: Can the cross-attention mechanism be extended to incorporate other types of features beyond context and events, such as character relationships or plot structures?
- Basis in paper: [inferred] The paper focuses on incorporating context and event features, but the cross-attention mechanism could potentially be extended to other types of features.
- Why unresolved: The paper does not explore the use of other types of features in the cross-attention mechanism, leaving open the question of whether incorporating additional features could further improve story generation.
- What evidence would resolve it: Experiments comparing the performance of the proposed model with and without incorporating additional features (e.g., character relationships or plot structures) in the cross-attention mechanism, demonstrating improvements in story quality metrics.

### Open Question 3
- Question: How does the performance of EtriCA and KeEtriCA compare to other state-of-the-art models on larger and more diverse datasets?
- Basis in paper: [explicit] The paper mentions that KeEtriCA is trained on a large-scale book corpus, but does not compare its performance to other models on such datasets.
- Why unresolved: The paper only evaluates the models on two specific datasets, and it is unclear how they would perform on larger and more diverse datasets that better represent the real-world distribution of narratives.
- What evidence would resolve it: Experiments comparing the performance of EtriCA and KeEtriCA to other state-of-the-art models on larger and more diverse datasets, demonstrating their relative strengths and weaknesses.

## Limitations

- Limited empirical validation of the cross-attention mechanism's necessity, with no ablation studies isolating its contribution
- Post-training framework assumes uniform benefit across all genres without examining genre-specific improvements or degradations
- Sentence similarity prediction auxiliary task complexity not justified by validation of its effectiveness in improving perceived coherence

## Confidence

**High Confidence** in experimental setup and dataset preparation - clear specification of ROC Stories and Writing Prompts datasets, event extraction methodology, and evaluation metrics.

**Medium Confidence** in overall model architecture and superiority claims - significant improvements reported but lack of ablation studies makes it difficult to attribute gains specifically to cross-attention mechanism.

**Low Confidence** in generalizability across different story domains - evaluation limited to two specific datasets with no analysis of out-of-domain performance or transfer to other narrative styles.

## Next Checks

1. **Ablation study of the cross-attention mechanism**: Remove the cross-attention and residual mapping components while keeping the rest of the architecture identical. Compare the performance on coherence, relevance, and fluency metrics to determine if the claimed improvements are specifically due to the context-event fusion or general architectural enhancements.

2. **Genre-specific performance analysis**: Conduct detailed analysis of how the post-training framework affects different story genres within the BookCorpus. Use genre classification to measure whether improvements are uniform across romance, sci-fi, mystery, etc., or whether certain genres benefit more than others.

3. **Human evaluation validation**: Design a controlled human evaluation study that specifically tests whether sentence similarity prediction actually improves perceived coherence. Have human raters compare stories generated with and without the auxiliary task, focusing on narrative flow and logical connections between sentences.