---
ver: rpa2
title: Adversarial training for tabular data with attack propagation
arxiv_id: '2307.15677'
source_url: https://arxiv.org/abs/2307.15677
tags:
- adversarial
- features
- training
- attacks
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adversarial training framework to improve
  the robustness of tabular ML models against adversarial attacks. The key idea is
  to propagate perturbations between raw and engineered feature spaces in the training
  loop.
---

# Adversarial training for tabular data with attack propagation

## Quick Facts
- arXiv ID: 2307.15677
- Source URL: https://arxiv.org/abs/2307.15677
- Reference count: 18
- Primary result: Prevents up to 30% performance drops under moderate attacks on credit card fraud detection

## Executive Summary
This paper introduces a novel adversarial training framework for tabular machine learning models that addresses a critical gap: the propagation of perturbations between raw and engineered feature spaces. The key insight is that feature engineering creates non-bijective mappings that must be considered during adversarial training to achieve meaningful robustness. The authors demonstrate that their approach can prevent significant performance degradation under moderate attacks while maintaining acceptable performance on clean data.

The framework is evaluated on a large-scale credit card fraud detection dataset, showing that the proposed method achieves robust performance under various attack scenarios. The paper also benchmarks different attack search strategies, identifying greedy search as the most effective approach for tabular data. The results highlight the importance of considering the entire feature engineering pipeline when developing adversarial defenses for tabular data.

## Method Summary
The proposed method involves an adversarial training loop where perturbations are first applied to raw features, then propagated to engineered features using profile estimation models. These profile models are trained to estimate how perturbations in raw features affect engineered features. During training, attacks are generated on the raw space, propagated to the engineered space using these profiles, and the model is updated using these attacked examples. The process iterates until model performance on attacked validation data stabilizes. The framework includes both exact and approximate methods for propagating perturbations efficiently.

## Key Results
- Prevents up to 30% performance drops under moderate adversarial attacks
- Essential for maintaining performance under very aggressive attacks
- Maintains less than 7% performance trade-off under no attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Propagating perturbations between raw and engineered feature spaces is necessary for effective adversarial training on tabular data.
- Mechanism: Feature engineering creates non-bijective mappings from raw to engineered features. When perturbations are applied only to raw space, they must be propagated to engineered features for the model to learn robustness against realistic attacks.
- Core assumption: The perturbations applied to raw features can be accurately mapped to engineered features using the proposed propagation methods.
- Evidence anchors:
  - [abstract] "Thus, we propose a new form of adversarial training where attacks are propagated between the two spaces in the training loop."
  - [section] "Thus, we propose a new form of adversarial training where attacks are propagated between the two spaces in the training loop."
  - [corpus] The related work section shows this is a novel approach not addressed in other papers focusing only on attacks.
- Break condition: If the propagation methods fail to accurately map perturbations, the adversarial training would not reflect realistic attack scenarios.

### Mechanism 2
- Claim: Greedy search is the most effective attack search strategy for finding successful adversarial examples in tabular data.
- Mechanism: The paper benchmarks several attack search strategies including random search, stochastic coordinate descent, and greedy search. Greedy search consistently outperforms other methods across different norm constraints, finding successful attacks with higher success rates and better cost-efficiency.
- Core assumption: The greedy search algorithm can effectively navigate the discrete perturbation space of tabular data to find successful adversarial examples.
- Evidence anchors:
  - [abstract] "Regarding the attack search method, we found greedy search to be the most effective."
  - [section] "The greedy approach, on the other hand, is the best since it beats the benchmark for nearly all norm constraints."
  - [corpus] The benchmarking results show greedy search outperforming other methods, but the corpus lacks detailed comparisons with white-box approaches.
- Break condition: If the greedy search gets stuck in local minima or fails to explore the perturbation space effectively, it would not find the most successful attacks.

### Mechanism 3
- Claim: Adversarial training with propagated perturbations prevents up to 30% performance drops under moderate attacks and is essential under very aggressive attacks.
- Mechanism: The adversarial training framework iteratively generates attacks on training data, propagates these perturbations to engineered features, and updates the model using these attacked examples. This process continues until the model performance stabilizes on attacked validation data.
- Core assumption: The model can learn to be robust to the specific types of attacks used during training, and this robustness generalizes to similar attacks.
- Evidence anchors:
  - [abstract] "Results show that the proposed approach can prevent up to 30% performance drops under moderate attacks and is essential under very aggressive attacks."
  - [section] "We show that our method can prevent about 30% performance drops under moderate attacks and is essential under very aggressive attacks."
  - [corpus] The corpus lacks direct evidence comparing performance drops with and without adversarial training on similar datasets.
- Break condition: If the attacks used during training do not represent realistic attack scenarios, the model may not gain meaningful robustness.

## Foundational Learning

- Concept: Feature engineering and its impact on model inputs
  - Why needed here: Understanding how raw features are transformed into engineered features is crucial for comprehending how perturbations propagate through the system.
  - Quick check question: What is the difference between row-wise map operations and aggregations of groups of rows in feature engineering?

- Concept: Adversarial attacks and their goal
  - Why needed here: Knowing how adversarial attacks work and their objective to mislead models is essential for understanding the adversarial training process.
  - Quick check question: What is the primary goal of an adversarial attack in the context of fraud detection?

- Concept: Gradient boosting decision trees
  - Why needed here: The paper uses LightGBM, a gradient boosting framework, for both the baseline classifier and profile estimation models. Understanding how these models work is important for grasping the training process.
  - Quick check question: How does gradient boosting improve model performance compared to a single decision tree?

## Architecture Onboarding

- Component map: Raw features → Feature engineering → Profile estimation → Baseline classifier training → Attack generation → Perturbation propagation → Model update → Evaluation
- Critical path: Raw features → Feature engineering → Profile estimation → Baseline classifier training → Attack generation → Perturbation propagation → Model update → Evaluation
- Design tradeoffs: The choice between exact and approximate methods for updating engineered features under perturbations balances computational efficiency with accuracy. Exact methods are more accurate but computationally expensive, while approximate methods are faster but may introduce errors.
- Failure signatures: If the model performance on clean data drops significantly during adversarial training, it may indicate that the attacks are too strong or the propagation methods are introducing too much noise. If the adversarial performance does not improve, it may suggest that the attack search method is not finding effective attacks.
- First 3 experiments:
  1. Implement the feature engineering pipeline and verify that perturbations can be accurately propagated to engineered features.
  2. Benchmark the attack search strategies on a small subset of data to confirm that greedy search outperforms other methods.
  3. Train a model with adversarial examples using a small norm constraint and evaluate its performance on both clean and attacked validation data to observe the trade-off between clean and adversarial performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance trade-offs between robustness and accuracy vary across different datasets and use cases beyond credit card fraud detection?
- Basis in paper: [explicit] The paper mentions the performance trade-off under no attacks is less than 7%, but this is based on a specific credit card fraud detection dataset. It would be valuable to understand if similar trade-offs exist in other tabular data domains.
- Why unresolved: The paper only presents results on a single proprietary dataset. Testing the framework on diverse datasets from different domains would provide a more comprehensive understanding of the trade-offs.
- What evidence would resolve it: Experiments on multiple tabular datasets from various domains (e.g., healthcare, finance, cybersecurity) showing the performance-robustness trade-off under no attacks. Comparing the trade-off magnitude and patterns across datasets would provide insights.

### Open Question 2
- Question: How does the choice of perturbation norm impact the effectiveness of adversarial training and the resulting model's robustness?
- Basis in paper: [explicit] The paper defines a custom perturbation norm tailored to credit card fraud detection, but notes that the choice is use-case specific. The effectiveness of different norm choices is not explored.
- Why unresolved: The paper uses a single norm definition based on domain expert input. Exploring alternative norm definitions and their impact on adversarial training outcomes is an open question.
- What evidence would resolve it: Experiments varying the perturbation norm definition (e.g., different cost functions, feature weightings) and measuring the resulting model's robustness and performance on clean data. Comparing the effectiveness of different norms would inform best practices.

### Open Question 3
- Question: How does the proposed adversarial training framework perform against adaptive attacks that specifically target its defense mechanisms?
- Basis in paper: [inferred] The paper evaluates the model's robustness against various attack strategies, but does not consider adaptive attacks that exploit the specific defense mechanisms employed (e.g., attack propagation, profile estimation models).
- Why unresolved: Adaptive attacks that are aware of the defense mechanisms could potentially bypass them more effectively. The framework's resilience against such attacks is not explored.
- What evidence would resolve it: Experiments with adaptive attack strategies that are designed to exploit the specific defense mechanisms of the framework (e.g., attacking the profile estimation models, finding ways to bypass attack propagation). Comparing the model's performance against adaptive vs non-adaptive attacks would quantify the framework's robustness.

## Limitations

- The evaluation focuses primarily on a single credit card fraud detection dataset, limiting generalizability.
- The proposed propagation methods for engineered features are not thoroughly validated against ground-truth attacks on the engineered space.
- The computational overhead of the adversarial training process is not discussed in detail, which is critical for real-world deployment.

## Confidence

- **High**: The mechanism of propagating perturbations between raw and engineered feature spaces is novel and technically sound, supported by the authors' implementation.
- **Medium**: The claim that greedy search is the most effective attack strategy is supported by internal benchmarking but lacks broader validation.
- **Low**: The assertion that the method prevents up to 30% performance drops under moderate attacks, while promising, is based on a single dataset and attack methodology.

## Next Checks

1. **Cross-dataset validation**: Evaluate the adversarial training framework on multiple tabular datasets (e.g., Kaggle competitions, UCI repository) to assess generalizability of the 30% performance drop prevention claim.
2. **White-box attack comparison**: Implement and test white-box attacks on both raw and engineered feature spaces to validate that the propagated perturbations accurately represent realistic attack scenarios.
3. **Computational efficiency analysis**: Measure and report the training time overhead introduced by the adversarial training loop, including the cost of profile feature estimators and periodic attack generation.