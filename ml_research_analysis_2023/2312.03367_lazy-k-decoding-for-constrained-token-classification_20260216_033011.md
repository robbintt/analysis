---
ver: rpa2
title: 'Lazy-k: Decoding for Constrained Token Classification'
arxiv_id: '2312.03367'
source_url: https://arxiv.org/abs/2312.03367
tags:
- constraints
- decoding
- total
- search
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores improving probabilistic models in structured
  prediction tasks by using constrained decoding methods. The authors propose a novel
  algorithm called Lazy-k that efficiently iterates over high-likelihood predictions
  to find a constraint-satisfying solution.
---

# Lazy-k: Decoding for Constrained Token Classification

## Quick Facts
- arXiv ID: 2312.03367
- Source URL: https://arxiv.org/abs/2312.03367
- Authors: [Not specified in source]
- Reference count: 11
- Primary result: Lazy-k decoding achieves similar or better performance than existing methods while being significantly faster, especially for smaller models in invoice information extraction tasks.

## Executive Summary
This paper introduces Lazy-k, a novel constrained decoding algorithm for token classification that efficiently explores high-likelihood predictions to find constraint-satisfying solutions. The method maintains a heap of the k best states and iteratively explores sequences within edit distance 1 until constraints are met. Evaluated on three invoice datasets, Lazy-k demonstrates performance comparable to or better than beam search, integer linear programming, and best-first search baselines while achieving faster decoding times. The approach is particularly effective for smaller models, where constrained decoding can substantially improve performance by correcting high-probability but incorrect BIO sequences.

## Method Summary
Lazy-k decoding works by maintaining a heap of the k most probable label sequences, prioritizing exploration of states with the highest score of their next best unexplored neighbor. The algorithm iteratively expands the most promising state within edit distance 1 until a constraint-satisfying solution is found or the maximum number of iterations is reached. During inference, token-level probabilities from a fine-tuned token classification model (such as LayoutLM) are fed into the Lazy-k decoder along with task-specific constraints like BIO labeling schemes and arithmetic relationships between extracted fields. The method is evaluated on CORD, WildReceipt, and DocILE invoice datasets, comparing against Argmax, Beam Search, ILP, and Best-First search baselines using F1 score weighted by constraint satisfaction ratio.

## Key Results
- Lazy-k achieves similar or better F1 scores compared to ILP and beam search while being 2-10x faster across all three invoice datasets
- For smaller models, Lazy-k shows significant performance improvements where ILP gains advantage, demonstrating effectiveness in correcting high-probability incorrect BIO sequences
- The method consistently finds constraint-satisfying solutions within a reasonable number of iterations (typically 100-1000), maintaining decoding efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lazy-k decoding explores high-likelihood predictions to find constraint-satisfying solutions.
- Mechanism: The algorithm maintains a heap of the k best states, prioritized by the score of the next best unexplored state within edit distance 1. It iteratively explores states in decreasing order of probability until a constraint-satisfying solution is found or a maximum number of iterations is reached.
- Core assumption: The constraint-satisfying sequence is among the high-probability sequences.
- Evidence anchors:
  - [abstract]: "The decoding methods search for constraint-satisfying label-assignments while maximizing the total probability."
  - [section]: "To do this efficiently, we exploit the fact that the k-th most probable sequence is always within 'edit-distance' 1 from one of the k âˆ’ 1 more probable sequences."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.376. Top related titles include "Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars" and "LMask: Learn to Solve Constrained Routing Problems with Lazy Masking." The corpus evidence is relevant but not directly supporting this specific mechanism.
- Break condition: If the constraint-satisfying sequence is not among the high-probability sequences explored by the algorithm.

### Mechanism 2
- Claim: Lazy-k decoding is more efficient than existing methods like beam search and integer linear programming.
- Mechanism: Lazy-k only iterates over the necessary number of sequences and stops once a satisfying solution is found. It also reduces the heap size by only adding the next best state yk instead of all children, leading to better runtime complexity.
- Core assumption: The constraint-satisfying sequence can be found within a reasonable number of iterations.
- Evidence anchors:
  - [abstract]: "The Lazy- k approach allows for more flexibility between decoding time and accuracy."
  - [section]: "The advantage of these informed search methods is that they will always find the optimal answer within a reasonable amount of time, should it exist. However, they also have a non-negligible minimum running time and impose aforementioned requirements on the constraints."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.376. Top related titles include "Flexible and Efficient Grammar-Constrained Decoding" and "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction." The corpus evidence is relevant but not directly supporting this specific mechanism.
- Break condition: If the constraint-satisfying sequence requires exploring a large number of sequences, leading to increased runtime.

### Mechanism 3
- Claim: Lazy-k decoding is particularly effective for smaller models.
- Mechanism: As the models get smaller, ILP gains in advantage with respect to Lazy-k when keeping the number of iterations constant. This means that in many cases the top-8 linear (BIO) constraint-satisfying solutions are outside of the 2^14 highest probability label-assignments. Lazy-k can still find these solutions efficiently.
- Core assumption: Smaller models have a higher probability of incorrect BIO sequences, which can be corrected by exploring alternative high-likelihood predictions.
- Evidence anchors:
  - [abstract]: "Our findings demonstrate that constrained decoding approaches can significantly improve the models' performances, especially when using smaller models."
  - [section]: "A stated advantage is the possibility of using smaller models in combination with the constrained decoding methods to improve their performance."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.376. Top related titles include "Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding" and "Constrained Decoding of Diffusion LLMs with Context-Free Grammars." The corpus evidence is relevant but not directly supporting this specific mechanism.
- Break condition: If the constraint-satisfying sequence is not found within the explored high-likelihood predictions for smaller models.

## Foundational Learning

- Concept: Probabilistic models in structured prediction
  - Why needed here: The paper explores improving probabilistic models in structured prediction tasks by using constrained decoding methods.
  - Quick check question: What is the goal of a probabilistic model in structured prediction?

- Concept: Token classification and BIO labeling scheme
  - Why needed here: The paper focuses on token classification for information extraction, and the BIO labeling scheme is used to classify spans of multiple tokens.
  - Quick check question: What is the BIO labeling scheme and how is it used in token classification?

- Concept: Constrained decoding and global constraints
  - Why needed here: The paper proposes a novel decoding method called Lazy-k that efficiently iterates over high-likelihood predictions to find a constraint-satisfying solution. The decoding methods search for constraint-satisfying label-assignments while maximizing the total probability.
  - Quick check question: What is constrained decoding and how does it differ from standard decoding in probabilistic models?

## Architecture Onboarding

- Component map:
  Token Classification Model -> Lazy-k Decoder -> Constraint Satisfaction Checker

- Critical path:
  1. Train a token-classification model using BIO labels.
  2. Generate predictions for the test set using the trained model.
  3. Feed the predictions into the Lazy-k decoder along with the constraints.
  4. The Lazy-k decoder iterates over high-likelihood predictions until a constraint-satisfying solution is found.

- Design tradeoffs:
  - Flexibility vs. runtime: Lazy-k allows for more flexibility between decoding time and accuracy compared to ILP, but it may require exploring a larger number of sequences.
  - Model size vs. performance: Smaller models can benefit more from constrained decoding approaches, but they may also have a higher probability of incorrect BIO sequences.

- Failure signatures:
  - The constraint-satisfying sequence is not found within the explored high-likelihood predictions.
  - The Lazy-k decoder takes a long time to find a constraint-satisfying solution, indicating that the sequence is far from the top-k most probable sequences.
  - The F1 score and constraint satisfaction ratio are low, suggesting that the model's predictions are not accurate enough.

- First 3 experiments:
  1. Evaluate the Lazy-k decoder on a small dataset with simple constraints (e.g., BIO labeling scheme) to verify its basic functionality.
  2. Compare the performance of Lazy-k with existing methods like beam search and ILP on a larger dataset with more complex constraints.
  3. Test the effectiveness of Lazy-k on smaller models and measure the improvement in F1 score and constraint satisfaction ratio.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Lazy-k decoding be effectively combined with confidence calibration methods like temperature scaling to further improve performance?
- Basis in paper: [explicit] The authors mention exploring this as future work, stating "Another interesting direction to explore would be the combination of Lazy-k decoding with confidence calibration methods such as temperature scaling."
- Why unresolved: The paper does not investigate this combination experimentally, leaving the potential benefits and implementation details unexplored.
- What evidence would resolve it: Experiments comparing Lazy-k decoding with and without temperature scaling on various datasets, measuring improvements in F1 score and constraint satisfaction ratio.

### Open Question 2
- Question: How would training the network to better predict correct BIO sequences impact the overall performance of Lazy-k decoding, especially for smaller models?
- Basis in paper: [inferred] The authors note that as models get smaller, ILP gains an advantage over Lazy-k, suggesting that correct BIO sequence prediction is crucial for Lazy-k's effectiveness.
- Why unresolved: The paper does not explore this direction experimentally, leaving the potential impact of improved BIO sequence prediction on Lazy-k's performance unknown.
- What evidence would resolve it: Training experiments where the model is explicitly optimized for correct BIO sequence prediction, followed by comparisons of Lazy-k decoding performance on various datasets with different model sizes.

### Open Question 3
- Question: Can Lazy-k decoding be extended to handle soft constraints, where constraints have degrees of satisfaction rather than binary values?
- Basis in paper: [explicit] The authors mention this as a limitation, stating "We did not explore the integration of soft constraints, which are constraints that can have a degree of satisfaction instead of the binary values considered in this paper."
- Why unresolved: The paper only considers hard constraints with binary values, leaving the potential benefits and implementation challenges of soft constraints unexplored.
- What evidence would resolve it: Development and experimental evaluation of Lazy-k decoding variants that can handle soft constraints, measuring improvements in F1 score and constraint satisfaction ratio on datasets with complex, non-binary constraints.

## Limitations

- The evaluation is limited to invoice-specific datasets with relatively simple arithmetic constraints, potentially limiting generalizability to other structured prediction domains.
- The computational advantage of Lazy-k over ILP is demonstrated but not extensively characterized across different constraint complexity levels or model architectures.
- The heap management strategy and NextBest function implementation details are summarized rather than fully specified, potentially affecting reproducibility.

## Confidence

**High Confidence**: The core contribution of Lazy-k as a more efficient constrained decoding algorithm compared to ILP and beam search is well-supported by empirical results across multiple datasets. The mechanism of exploring high-likelihood predictions through edit-distance-1 neighbors is clearly articulated and demonstrated.

**Medium Confidence**: The claim that Lazy-k is particularly effective for smaller models is supported by the presented results but lacks a thorough mechanistic explanation. The relationship between model size, probability distributions of BIO sequences, and constraint satisfaction is observed but not fully theorized.

**Low Confidence**: The paper's assertion that constrained decoding approaches can significantly improve model performances across diverse structured prediction tasks is based on a narrow set of invoice-related datasets. Generalization to other domains or constraint types would require additional validation.

## Next Checks

1. **Constraint Complexity Scaling**: Test Lazy-k on datasets with progressively more complex constraints (e.g., nested arithmetic relationships, cross-field dependencies) to evaluate how heap management and iteration count scale with constraint complexity.

2. **Cross-Domain Generalization**: Apply Lazy-k to non-invoice structured prediction tasks such as named entity recognition in biomedical texts or semantic parsing, where constraints differ substantially from arithmetic relationships, to assess domain transferability.

3. **Model Architecture Ablation**: Compare Lazy-k's performance across different transformer architectures (BERT, RoBERTa, DeBERTa, etc.) with varying parameter counts to systematically map the relationship between model capacity, prediction probability distributions, and Lazy-k's effectiveness in finding constraint-satisfying solutions.