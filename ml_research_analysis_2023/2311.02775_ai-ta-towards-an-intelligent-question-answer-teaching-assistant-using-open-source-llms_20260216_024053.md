---
ver: rpa2
title: 'AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source
  LLMs'
arxiv_id: '2311.02775'
source_url: https://arxiv.org/abs/2311.02775
tags:
- answer
- statement
- evaluation
- human
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHATA, an intelligent QA teaching assistant
  that leverages open-source Large Language Models (LLMs) from the LLaMA-2 family
  to address the challenge of scalable question-answering in computing courses. The
  approach combines retrieval augmented generation (RAG), supervised fine-tuning (SFT),
  and Direct Preference Optimization (DPO) to improve answer quality.
---

# AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs

## Quick Facts
- **arXiv ID**: 2311.02775
- **Source URL**: https://arxiv.org/abs/2311.02775
- **Reference count**: 40
- **Primary result**: Open-source LLM teaching assistant achieves 0.53 usefulness and 0.61 accuracy scores, approaching GPT-4 performance

## Executive Summary
This paper introduces CHATA, an intelligent QA teaching assistant that leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to address the challenge of scalable question-answering in computing courses. The approach combines retrieval augmented generation (RAG), supervised fine-tuning (SFT), and Direct Preference Optimization (DPO) to improve answer quality. Extensive experiments on a Piazza dataset of 10,000 QA pairs and 1,500 preference pairs show that the proposed techniques collectively enhance answer quality by 33%, with RAG being particularly impactful. The best-performing model achieves a usefulness score of 0.53 and accuracy score of 0.61 in human evaluation, approaching the performance of GPT-4 while ensuring data privacy.

## Method Summary
The CHATA system employs a multi-stage approach combining open-source LLaMA-2 models with three augmentation techniques: RAG for grounding responses in course materials, SFT for teaching task format, and DPO for aligning with human preferences. The system uses a union of embedding-based and keyword-based retrievers for context retrieval, processes student questions through prompt engineering, and generates answers using the fine-tuned LLaMA-2 models. The pipeline is evaluated on a Piazza dataset using both human evaluation and GPT-4 based metrics measuring usefulness and accuracy.

## Key Results
- RAG technique improves answer quality by 30% by reducing hallucination and increasing factual accuracy
- Combining all three techniques (RAG + SFT + DPO) achieves a 33% improvement in overall answer quality
- The best model reaches 0.53 usefulness and 0.61 accuracy scores, approaching GPT-4 performance while maintaining data privacy
- DPO shows 23% improvement in accuracy scores compared to SFT alone by reducing hallucination

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Generation
RAG significantly improves answer quality by grounding responses in course-specific materials. The mechanism retrieves relevant context from instructional documents and injects it into the prompt, reducing hallucination and increasing factual accuracy. Core assumption: course materials contain sufficient information to answer most student questions. Evidence: 30% improvement in answer quality and reduced hallucination observed in experiments. Break condition: incomplete course materials or questions requiring reasoning beyond provided context.

### Mechanism 2: Direct Preference Optimization
DPO stabilizes model outputs and reduces hallucination compared to SFT alone. The mechanism fine-tunes the model using human preference pairs, aligning outputs with human-judged quality without the instability of RLHF. Core assumption: human preference data captures desired answer characteristics better than instruction data alone. Evidence: 23% improvement in accuracy scores and reduced hallucination observed in experiments. Break condition: noisy preference data or preferences not representing desired answer qualities.

### Mechanism 3: Combined Technique Approach
Combining multiple techniques (RAG + SFT + DPO) produces better results than any single technique. Each technique addresses different weaknesses - RAG provides grounding, SFT teaches task format, DPO aligns with human preferences. Core assumption: techniques are complementary rather than redundant. Evidence: 29% improvement when combining all techniques compared to DPO alone. Break condition: techniques interfere with each other or computational constraints prevent effective implementation.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation**
  - Why needed here: RAG grounds LLM responses in course materials, reducing hallucination and improving accuracy
  - Quick check question: How does RAG differ from fine-tuning when incorporating new knowledge?

- **Concept: Direct Preference Optimization**
  - Why needed here: DPO aligns model outputs with human preferences without the complexity of RLHF
  - Quick check question: What's the key mathematical difference between DPO and traditional RLHF approaches?

- **Concept: Parameter-Efficient Fine-Tuning**
  - Why needed here: Enables adaptation of large LLaMA-2 models on limited computational resources
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Student question → Embedding retrieval → Context retrieval (RAG) → Prompt engineering → LLaMA-2 model → Human preference fine-tuning (DPO) → Answer generation
- **Critical path**: Question processing → Context retrieval → Answer generation → Human evaluation
- **Design tradeoffs**: Open-source models (privacy) vs. commercial models (performance); computational efficiency vs. answer quality
- **Failure signatures**: Hallucination in answers (insufficient RAG context), irrelevant responses (poor retrieval), unstable outputs (DPO issues)
- **First 3 experiments**:
  1. Baseline LLaMA-2-13B performance on evaluation set without any augmentation
  2. RAG-only implementation with instructional materials to measure grounding impact
  3. DPO fine-tuning on preference pairs to measure alignment improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of the CHATA system change if multi-turn and conversational QA support were implemented?
- Basis in paper: [explicit] The authors explicitly mention that their work is limited by excluding follow-ups and multi-turn conversations in their dataset preparation process.
- Why unresolved: The authors note that handling multi-turn conversations is a future direction but do not provide any experimental data on how this would impact performance.
- What evidence would resolve it: Experimental results comparing the current single-turn system with a multi-turn version on the same dataset, showing changes in usefulness and accuracy scores.

### Open Question 2
- Question: Would training a retriever end-to-end improve the quality of retrieved context compared to the current approach of using a union of embedding-based and keyword-based retrievers?
- Basis in paper: [explicit] The authors explicitly state that they did not train the retriever end-to-end and suggest this as future work.
- Why unresolved: The authors hypothesize that end-to-end training might improve retrieval quality but provide no experimental validation.
- What evidence would resolve it: A comparison of RAG performance using end-to-end trained retrievers versus the current method, showing improvements in answer quality metrics.

### Open Question 3
- Question: How would the performance of CHATA change when deployed across different institutions and programming languages beyond the single CS1 course tested?
- Basis in paper: [inferred] The authors mention they only tested on one particular course and note that pre-trained models may not perform as well on Matlab and C compared to Python and Java.
- Why unresolved: The authors discuss plans to deploy CHATA in various university CS courses but have not conducted such experiments yet.
- What evidence would resolve it: Comparative performance data across multiple institutions and programming languages, showing generalizability and any necessary adaptations.

### Open Question 4
- Question: How well does GPT-4's automatic evaluation correlate with human TA evaluations across different types of questions and answer qualities?
- Basis in paper: [explicit] The authors note that GPT-4 assigned much higher scores to baseline models compared to human evaluation and that the correlation is not strong enough.
- Why unresolved: The authors acknowledge the misalignment between GPT-4 and human evaluation but don't provide a detailed analysis of when and why these divergences occur.
- What evidence would resolve it: A detailed confusion matrix analysis showing agreement/disagreement patterns between GPT-4 and human evaluators across different question categories and answer quality levels.

## Limitations
- Limited to one course context and dataset, constraining generalizability claims
- Reliance on human evaluation introduces subjectivity and potential bias
- Open questions about long-term deployment and maintenance remain unanswered
- Computational requirements for the full pipeline remain unclear

## Confidence
- **High confidence**: RAG's impact on reducing hallucination and improving factual accuracy (30% improvement observed)
- **Medium confidence**: Combined technique approach effectiveness (limited ablation studies, 29% improvement claim)
- **Medium confidence**: Approach to GPT-4 performance claim (based on single course evaluation, requires independent verification)

## Next Checks
1. Replicate the evaluation framework across multiple courses with different subject matter to test generalizability
2. Conduct A/B testing comparing CHATA against human TAs in actual classroom deployment
3. Perform ablation studies isolating the contribution of each technique (RAG, SFT, DPO) with statistical significance testing