---
ver: rpa2
title: 'Out-of-distribution forgetting: vulnerability of continual learning to intra-class
  distribution shift'
arxiv_id: '2306.00427'
source_url: https://arxiv.org/abs/2306.00427
tags:
- learning
- task
- shift
- oodf
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a novel form of catastrophic forgetting,
  termed out-of-distribution forgetting (OODF), that affects continual learning (CL)
  systems. OODF occurs when intra-class distribution shifts in training data lead
  to severe performance degradation during subsequent learning, even though the initial
  learning phase appears unaffected.
---

# Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift

## Quick Facts
- arXiv ID: 2306.00427
- Source URL: https://arxiv.org/abs/2306.00427
- Authors: 
- Reference count: 40
- This study identifies out-of-distribution forgetting (OODF), a novel form of catastrophic forgetting in continual learning systems caused by intra-class distribution shifts

## Executive Summary
This paper identifies a novel vulnerability in continual learning (CL) systems called out-of-distribution forgetting (OODF). OODF occurs when intra-class distribution shifts in training data cause severe performance degradation during subsequent learning, even though the initial learning phase appears unaffected. Through experiments across multiple CL strategies, datasets, and network architectures, the authors demonstrate that OODF impacts both regularization-based and memory-based CL methods, but not parameter-isolation-based methods. The effect is triggered by various distribution shift conditions and manifests only after subsequent learning tasks, making it difficult to detect. This work highlights a critical vulnerability in current CL approaches and underscores the need for more robust methods to handle intra-class distribution shifts in dynamic environments.

## Method Summary
The study investigates OODF by comparing control groups (standard CL) with shift groups (CL with distribution-shifted data) across three CL strategies: regularization-based (OWM, AOP), memory-based (iCaRL, ER, DGR, DER++, GDumb), and parameter-isolation-based (CN-DPM). Distribution shifts are introduced by adding pixel blocks or adversarial samples to a subset of training data for one task. The performance is evaluated at intermediate (immediately after learning the shifted task) and final time points (after all tasks) on original test sets. The OODF phenomenon is identified when the shift group shows comparable performance to the control group at the intermediate time point but experiences severe degradation by the final time point.

## Key Results
- OODF causes severe performance degradation in regularization-based and memory-based CL methods, but not in parameter-isolation-based methods
- The effect is triggered by various distribution shift conditions including pixel occlusions (1x1 to 4x4 blocks) and adversarial examples (FGSM)
- OODF severity depends on occlusion strength and percentage of shifted samples, with performance plateauing then dropping as occlusion strength increases
- OODF is a delayed phenomenon that only manifests after subsequent learning tasks, making it difficult to detect during initial learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OODF occurs because the CL system protects features that are frequently seen in both original and shifted data, while neglecting features that are unique to clean samples.
- Mechanism: During learning, the model updates weights to preserve accuracy on both clean and shifted samples. However, this protection focuses on the overlapping feature space between clean and shifted data. Features unique to clean samples become less protected, making the model vulnerable when only clean samples are presented in later tasks.
- Core assumption: The feature space of shifted data overlaps substantially with clean data, allowing the model to maintain performance during initial learning despite the shift.
- Evidence anchors:
  - [abstract] "introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning"
  - [section 3.6.3] "frequently occurring shifts can serve as informative features for classification. This compromises the mechanism designed to protect the intrinsic features for the learned class"
  - [corpus] Weak evidence - no direct mention of this specific mechanism in related papers
- Break condition: If the shifted data introduces completely disjoint features from clean data, the model would fail immediately rather than showing delayed forgetting.

### Mechanism 2
- Claim: OODF is delayed because the catastrophic forgetting manifests only after subsequent learning tasks introduce interference with the less-protected clean features.
- Mechanism: Initially, the model can classify both clean and shifted samples well because it has learned a representation that works for both. However, when learning new tasks, the shared representation space is modified to accommodate new classes, which interferes with the unprotected clean features, causing performance to drop.
- Core assumption: The representation space is shared across tasks, allowing interference between new learning and old clean features.
- Evidence anchors:
  - [abstract] "this phenomenon is special for CL as the same level of distribution shift had only negligible effects in the joint learning scenario"
  - [section 3.5.1] "performance in the shift group at t = S was comparable to the control group but dropped dramatically at the end of learning t = K"
  - [corpus] Weak evidence - related papers focus on general catastrophic forgetting but not this specific delayed mechanism
- Break condition: If each task had completely isolated parameter spaces (like CN-DPM), there would be no interference between tasks.

### Mechanism 3
- Claim: Parameter-isolation-based methods are immune to OODF because they maintain independent feature spaces for each task.
- Mechanism: In parameter-isolation methods, each task has its own dedicated subnetwork or parameter space. Distribution shifts in one task's data only affect that task's parameters, leaving other tasks' representations untouched and preventing interference.
- Core assumption: The feature representations for different tasks are completely independent in parameter-isolation methods.
- Evidence anchors:
  - [abstract] "CL methods without dedicating subnetworks for individual tasks are all vulnerable to OODF"
  - [section 3.6.3] "The feature space representations in these methods vary from task to task, and more importantly, they are independent from each other"
  - [corpus] Weak evidence - no direct mention of OODF immunity in related papers
- Break condition: If parameter-isolation methods had some shared components (like a shared input layer), OODF could still occur in those shared components.

## Foundational Learning

- Concept: Continual Learning (CL) - learning tasks sequentially without catastrophic forgetting
  - Why needed here: The entire paper is about a novel form of forgetting specific to CL systems
  - Quick check question: What are the three main strategies for CL mentioned in the paper?

- Concept: Out-of-Distribution (OOD) Robustness - ability to handle data that differs from training distribution
  - Why needed here: OODF is a specific form of forgetting caused by intra-class distribution shifts
  - Quick check question: How does OODF differ from standard OOD problems in joint learning?

- Concept: Catastrophic Forgetting - severe performance degradation on old tasks after learning new ones
  - Why needed here: OODF is described as a "special form of catastrophic forgetting"
  - Quick check question: What makes OODF different from standard catastrophic forgetting?

## Architecture Onboarding

- Component map: Input -> Feature extraction -> Classification -> Memory update (if memory-based) -> Parameter protection (if regularization-based)
- Critical path: Input → Feature extraction → Classification → Memory update (if applicable) → Parameter protection (if regularization-based)
- Design tradeoffs:
  - Shared vs. isolated representations: Shared improves efficiency but is vulnerable to OODF; isolated prevents OODF but increases complexity
  - Memory buffer size vs. performance: Larger buffers better prevent forgetting but increase computational cost
  - Regularization strength vs. plasticity: Stronger regularization prevents forgetting but slows learning of new tasks
- Failure signatures:
  - Initial good performance on shifted data followed by severe degradation after subsequent learning
  - Performance drop only on the task with distribution-shifted data, not on other tasks
  - No immediate performance drop after training on shifted data (delayed effect)
- First 3 experiments:
  1. Reproduce the SplitMNIST-10 experiment with OWM to verify the basic OODF phenomenon
  2. Test the delayed effect by measuring accuracy at t=S (immediately after learning shifted task) vs t=K (after all tasks)
  3. Compare regularization-based vs parameter-isolation methods to verify the immunity of CN-DPM to OODF

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms through which intra-class distribution shifts cause catastrophic forgetting in regularization-based and memory-based CL methods, but not in parameter-isolation-based methods?
- Basis in paper: [explicit] The paper states that parameter-isolation-based methods are robust to OODF, while regularization-based and memory-based methods are vulnerable.
- Why unresolved: The paper hypothesizes that shared representation spaces in the former methods cause interference, but does not provide a detailed analysis of the underlying mechanisms.
- What evidence would resolve it: Detailed experiments comparing feature representations and weight updates in different CL methods under distribution shift conditions, possibly using techniques like representation similarity analysis or gradient flow visualization.

### Open Question 2
- Question: How can CL methods be designed to maintain performance under varying levels of intra-class distribution shift, particularly when the shift affects overlapping features with subsequent tasks?
- Basis in paper: [explicit] The paper shows that OODF severity depends on occlusion strength and shift percentage, and provides a hypothesis about overlapping features causing interference.
- Why unresolved: While the paper identifies factors influencing OODF, it does not propose or test methods to mitigate the effect under different shift conditions.
- What evidence would resolve it: Development and evaluation of CL methods incorporating robust feature learning or adaptive memory management that can handle varying distribution shifts, tested across multiple shift scenarios and datasets.

### Open Question 3
- Question: To what extent does OODF generalize beyond image classification tasks to other domains such as natural language processing or reinforcement learning?
- Basis in paper: [inferred] The paper mentions that OODF can be extended to other computer vision or NLP tasks, but focuses on image classification for empirical validation.
- Why unresolved: The study is limited to image classification tasks, leaving uncertainty about OODF's impact in other domains with different data structures and learning dynamics.
- What evidence would resolve it: Empirical studies applying CL methods to NLP tasks (e.g., text classification) or RL tasks under intra-class distribution shifts, measuring OODF effects and comparing with image classification results.

## Limitations
- Experiments primarily use synthetic distribution shifts (pixel occlusions and adversarial examples) rather than natural distribution variations
- Memory buffer sizes and specific hyperparameters for each CL method are not fully specified, affecting reproducibility
- Study focuses on classification tasks and may not capture OODF effects in other domains like regression or reinforcement learning

## Confidence
- **High confidence**: The basic OODF phenomenon (delayed forgetting after distribution shift) is well-supported by consistent results across MNIST, CIFAR-10, and CIFAR-100 experiments
- **Medium confidence**: The mechanism explanations (feature overlap protection, delayed interference) are logically sound but not directly measured
- **Low confidence**: Claims about OODF being "unobservable" without subsequent learning are based on limited experimental scenarios

## Next Checks
1. Test OODF with naturally occurring distribution shifts (domain adaptation datasets like DomainNet) to verify the phenomenon extends beyond synthetic occlusions and adversarial examples
2. Conduct ablation studies measuring feature space overlap between clean and shifted data during learning to directly validate the proposed mechanism of feature protection
3. Evaluate OODF in non-classification settings (e.g., continual regression or reinforcement learning) to determine if the phenomenon generalizes across task types