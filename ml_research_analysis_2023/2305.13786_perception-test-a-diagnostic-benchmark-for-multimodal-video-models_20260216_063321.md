---
ver: rpa2
title: 'Perception Test: A Diagnostic Benchmark for Multimodal Video Models'
arxiv_id: '2305.13786'
source_url: https://arxiv.org/abs/2305.13786
tags:
- object
- video
- videos
- perception
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Perception Test is a novel benchmark for evaluating multimodal
  video models, focusing on perception and reasoning skills rather than computational
  tasks. It includes 11.6k real-world videos with dense annotations (190K object tracks,
  73.5K action segments, 137K sound segments, 38K mc-vQA pairs, 6K g-vQA pairs) across
  four skill areas (Memory, Abstraction, Physics, Semantics) and four types of reasoning
  (descriptive, explanatory, predictive, counterfactual).
---

# Perception Test: A Diagnostic Benchmark for Multimodal Video Models

## Quick Facts
- **arXiv ID**: 2305.13786
- **Source URL**: https://arxiv.org/abs/2305.13786
- **Reference count**: 40
- **Key outcome**: Perception Test benchmark reveals significant performance gap between humans (91.4%) and state-of-the-art models (46.2%) on multimodal video understanding tasks.

## Executive Summary
The Perception Test is a novel benchmark designed to evaluate multimodal video models' perception and reasoning skills in real-world scenarios. Unlike existing benchmarks that focus on computational tasks, this benchmark emphasizes understanding of real-world scenes through 11.6k videos with dense annotations across four skill areas: Memory, Abstraction, Physics, and Semantics. The benchmark assesses transfer capabilities in zero/few-shot or limited fine-tuning regimes, making it particularly relevant for evaluating pre-trained models' generalization abilities. With human baseline performance significantly outperforming current models, the benchmark highlights substantial room for improvement in multimodal video understanding.

## Method Summary
The Perception Test benchmark consists of 11.6k real-world videos (23s average length) densely annotated with six types of labels: object tracks, point tracks, temporal action and sound segments, multiple-choice video question-answers, and grounded video question-answers. The videos are divided into training (2184 videos), validation, and held-out test splits. The benchmark evaluates models across four skill areas (Memory, Abstraction, Physics, Semantics) and four reasoning types (descriptive, explanatory, predictive, counterfactual). Models are assessed in transfer learning settings with limited fine-tuning data, using metrics such as average IoU for tracking, mAP for temporal localization, and accuracy for video QA. Baseline models include SiamFC for object tracking, TAP-Net for point tracking, ActionFormer for temporal localization, and Flamingo for video QA.

## Key Results
- Human baseline performance (91.4%) significantly exceeds state-of-the-art models (46.2%) on the Perception Test
- Flamingo model performs below random on counterfactual questions and struggles with memory-related skills and physics understanding
- Dense multi-type annotations per video enable efficient transfer evaluation by sharing latent representations across tasks
- Scripted video creation with controlled variations successfully probes specific reasoning skills while avoiding language biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense, multi-type annotations per video enable efficient transfer evaluation by sharing latent representations across tasks.
- Mechanism: Multiple annotation types (object tracks, point tracks, action/sound segments, vQA) are grounded in the same object tracks, allowing models to leverage shared visual features for different tasks without needing separate video samples.
- Core assumption: Latent visual features learned for one task (e.g., object tracking) are useful for other tasks (e.g., action localization).
- Evidence anchors:
  - [abstract] "The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments), enabling both language and non-language evaluations."
  - [section] "Having multiple types of annotations per video is useful also for analysis purposes, as the correlations between successes and failures across tasks may uncover biases that prevent generalisation."
- Break condition: If tasks require fundamentally different visual representations (e.g., fine-grained texture vs. global motion), shared features may be insufficient, degrading transfer performance.

### Mechanism 2
- Claim: Scripted video creation with controlled variations probes specific reasoning skills while avoiding language biases.
- Mechanism: Scripts are designed to test memory, abstraction, physics, and semantics through controlled variations; questions are framed to avoid cues that would let models guess based on language alone.
- Core assumption: Models cannot rely on superficial cues (e.g., frequent word patterns) to answer questions correctly.
- Evidence anchors:
  - [abstract] "We designed video scripts and tasks to diagnose the perception skills of our models."
  - [section] "Having multiple variations per script allows us to ask the exact same question with the same set of options, and the correct answer depends on the specific script variation – in this way, we can avoid language biases in questions that give away the answer."
- Break condition: If model exploits subtle linguistic patterns or dataset-wide correlations, scripted variations may not fully prevent bias exploitation.

### Mechanism 3
- Claim: Small training split + held-out test split regime robustly evaluates transfer capabilities of pre-trained models.
- Mechanism: Models are evaluated in a few-shot or limited fine-tuning setting, ensuring that performance reflects generalisation from pre-training rather than overfitting to the benchmark.
- Core assumption: Pre-trained models retain useful representations for the Perception Test tasks despite limited fine-tuning data.
- Evidence anchors:
  - [abstract] "The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime."
  - [section] "We aim to benchmark any representation or model, pre-trained with any external dataset or task, of any scale available."
- Break condition: If pre-trained representations are too domain-specific or tasks require extensive task-specific adaptation, few-shot evaluation may not reflect true capabilities.

## Foundational Learning

- Concept: Multimodal video understanding
  - Why needed here: The benchmark evaluates models that integrate visual, audio, and textual information from videos, requiring understanding of both spatial and temporal dynamics.
  - Quick check question: Can a model localize an action in time and space while also answering a question about the video content using both visual and audio cues?

- Concept: Transfer learning and few-shot evaluation
  - Why needed here: The benchmark is designed to assess how well pre-trained models can adapt to new tasks with minimal data, reflecting real-world deployment scenarios.
  - Quick check question: Does a model perform well on the Perception Test after being fine-tuned on only the small training split, or does it rely heavily on the training data?

- Concept: Diagnostic benchmarking
  - Why needed here: The benchmark aims to diagnose specific perception and reasoning skills (memory, abstraction, physics, semantics) rather than just overall performance, requiring careful task design.
  - Quick check question: Can you identify which specific reasoning skill a model struggles with by analyzing its performance across different task types?

## Architecture Onboarding

- Component map: Video processing pipeline → Multimodal feature extraction (visual, audio, text) → Task-specific heads (tracking, localization, QA) → Evaluation metrics
- Critical path: Video input → Dense annotation grounding → Task specification → Model prediction → Metric computation
- Design tradeoffs: Dense annotations per video vs. larger dataset with sparse annotations; scripted vs. naturalistic videos; multiple modalities vs. single modality focus
- Failure signatures: Poor performance on tasks requiring temporal reasoning despite good spatial understanding; inability to generalize across script variations; over-reliance on language cues
- First 3 experiments:
  1. Evaluate a baseline object tracker on the Perception Test to establish a reference for transfer performance
  2. Test a multimodal QA model in zero-shot and few-shot settings to measure adaptation capability
  3. Analyze performance across skill areas to identify specific reasoning gaps in current models

## Open Questions the Paper Calls Out

- **Open Question 1**: How would multimodal video models perform on the Perception Test if trained with a larger training set size?
  - Basis in paper: The paper states the Perception Test is designed to assess transfer capabilities with a limited training set, and the human baseline outperforms state-of-the-art models by a large margin.
  - Why unresolved: The paper only reports results with the small training set provided, and does not explore the impact of increasing training data on model performance.
  - What evidence would resolve it: Evaluating multimodal video models on the Perception Test with varying training set sizes would show how performance scales with more data, and whether the human-model performance gap can be reduced.

- **Open Question 2**: What specific types of reasoning (descriptive, explanatory, predictive, counterfactual) do current multimodal video models struggle with the most on the Perception Test?
  - Basis in paper: The paper reports Flamingo performs below random on counterfactual questions, and struggles with memory-related skills and physics understanding.
  - Why unresolved: While the paper identifies some weaknesses, it does not provide a detailed breakdown of model performance across all four reasoning types for each skill area.
  - What evidence would resolve it: Analyzing model performance on the Perception Test in detail, broken down by skill area and reasoning type, would reveal specific weaknesses and areas for improvement.

- **Open Question 3**: How well do multimodal video models generalize to real-world scenarios beyond the Perception Test?
  - Basis in paper: The paper emphasizes the Perception Test is designed to assess transfer capabilities and real-world scene understanding, but does not evaluate generalization to external datasets.
  - Why unresolved: The paper only reports performance on the Perception Test itself, without testing how well models trained on it can handle other video understanding tasks or datasets.
  - What evidence would resolve it: Evaluating multimodal video models trained on the Perception Test on other video understanding benchmarks or real-world applications would show how well the skills and reasoning learned transfer.

## Limitations

- The evaluation regime relies heavily on the quality and consistency of scripted video creation to avoid language biases, though subtle linguistic patterns may still be exploitable.
- Transfer learning evaluation assumes pre-trained representations retain sufficient generality, but specific pre-training domains and scales are not detailed.
- The benchmark's focus on real-world videos with dense annotations provides rich data but may not fully capture the diversity of unconstrained video content.

## Confidence

- **High confidence**: The benchmark design (11.6k videos with dense annotations across four skill areas and four reasoning types) is clearly specified and methodologically sound.
- **Medium confidence**: The claim that scripted variations effectively prevent language biases, as this depends on implementation details not fully disclosed.
- **Medium confidence**: The transfer learning evaluation's effectiveness, as it assumes pre-trained models retain useful representations without specifying pre-training conditions.

## Next Checks

1. Analyze model performance across script variations to verify that the correct answer truly depends on the specific variation rather than linguistic patterns.
2. Evaluate models pre-trained on different domains (e.g., action recognition vs. general video understanding) to assess the generality of transfer capabilities.
3. Test the benchmark with models that have access to the held-out test split to establish upper bounds and identify potential overfitting concerns.