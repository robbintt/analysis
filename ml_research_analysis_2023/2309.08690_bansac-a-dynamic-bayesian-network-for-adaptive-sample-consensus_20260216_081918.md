---
ver: rpa2
title: 'BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus'
arxiv_id: '2309.08690'
source_url: https://arxiv.org/abs/2309.08690
tags:
- inlier
- outlier
- bansac
- data
- ransac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BANSAC is a RANSAC sampling improvement using dynamic Bayesian
  networks to adaptively update data point inlier scores during iterations. It applies
  weighted sampling based on these updated probabilities and derives a new stopping
  criterion.
---

# BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus

## Quick Facts
- arXiv ID: 2309.08690
- Source URL: https://arxiv.org/abs/2309.08690
- Reference count: 40
- Key outcome: BANSAC achieves state-of-the-art accuracy while reducing computational time compared to baselines by using dynamic Bayesian networks to adaptively update data point inlier scores during RANSAC iterations.

## Executive Summary
BANSAC is a RANSAC sampling improvement that uses dynamic Bayesian networks (DBNs) to adaptively update data point inlier scores during iterations. It applies weighted sampling based on these updated probabilities and derives a new stopping criterion. Tested on calibrated/uncalibrated relative pose and homography estimation, BANSAC achieves state-of-the-art accuracy while reducing computational time compared to baselines. In essential matrix estimation, it improves mAA(10°) from 0.653 to 0.680; in homography, from 0.291 to 0.446.

## Method Summary
BANSAC improves RANSAC by incorporating a dynamic Bayesian network that updates individual data points' inlier scores during iterations. The method models data points' inlier/outlier probabilities and uses these as weights for sampling minimal sets. After each iteration, the DBN updates probabilities based on the current model hypothesis's inlier/outlier classifications. The algorithm also introduces a new stopping criterion based on these updated probabilities, allowing earlier termination without sacrificing accuracy. The method works with or without prior data point scorings and shows robust performance across varying inlier ratios.

## Key Results
- BANSAC improves essential matrix estimation mAA(10°) from 0.653 to 0.680 compared to RANSAC
- BANSAC improves homography estimation mAA(10px) from 0.291 to 0.446 compared to RANSAC
- BANSAC achieves competitive accuracy with reduced execution time, as shown in Table 3 and Figure 3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Updating inlier probabilities using a dynamic Bayesian network improves sampling efficiency by focusing iterations on more likely inliers.
- **Mechanism:** At each iteration, inlier/outlier classifications from previous hypotheses are used as evidence to update each data point's inlier probability via exact inference in a DBN. Points with higher updated probabilities are more likely to be sampled in the next iteration.
- **Core assumption:** The quality of the current model hypothesis correlates with the correctness of inlier/outlier classifications used to update probabilities.
- **Evidence anchors:**
  - [abstract] "We derive a dynamic Bayesian network that updates individual data points’ inlier scores while iterating RANSAC."
  - [section 4.2] "We describe our method by modeling data points’ inlier/outlier probability."
  - [corpus] Weak evidence: No related papers specifically use DBNs for iterative inlier scoring.
- **Break condition:** If model hypotheses are consistently poor due to degenerate configurations, the probability updates may reinforce incorrect classifications, leading to biased sampling.

### Mechanism 2
- **Claim:** Using the updated inlier probabilities as sampling weights reduces the number of iterations needed to find a good model.
- **Mechanism:** Instead of uniform sampling, data points are sampled according to their updated inlier probabilities, increasing the chance of selecting truly inlier points for minimal sets.
- **Core assumption:** Higher probability data points are more likely to be true inliers, so sampling them yields better hypotheses.
- **Evidence anchors:**
  - [abstract] "At each iteration, we apply weighted sampling using the updated scores."
  - [section 4.3] "To increase the chances of only selecting inliers, we take the estimated probabilities P k−1 and create a weighted discrete distribution."
  - [section 5.2] Figure 3 shows improved accuracy with fewer iterations compared to baselines.
- **Break condition:** If the probability updates are inaccurate or converge too slowly, the sampling advantage may not materialize.

### Mechanism 3
- **Claim:** The new stopping criterion based on probability thresholds allows earlier termination without sacrificing accuracy.
- **Mechanism:** The algorithm tracks the number of points with inlier probability below a threshold τ. When this count exceeds the best model's outlier count, it assumes sampling only inliers is unlikely to improve results and terminates.
- **Core assumption:** Once the probability threshold identifies all remaining points as unlikely inliers, further sampling cannot yield better models.
- **Evidence anchors:**
  - [abstract] "we derive a new stopping criterion using the updated scores."
  - [section 4.4] "Our stopping criterion is triggered when eOk >= O∗."
  - [section 5.4] Tab. 3 shows competitive accuracy with reduced execution time.
- **Break condition:** If the probability threshold τ is set too high, the criterion may trigger prematurely, missing better models; if too low, the benefit in execution time diminishes.

## Foundational Learning

- **Concept:** Dynamic Bayesian Networks (DBNs)
  - Why needed here: DBNs provide a principled framework for modeling how inlier probabilities evolve over iterations based on evidence from model hypotheses.
  - Quick check question: What is the key difference between a DBN and a static Bayesian network in the context of RANSAC?

- **Concept:** Markov assumptions in temporal models
  - Why needed here: To limit computational complexity, the method assumes the current inlier probability depends only on the previous state (or two/three previous states), not the entire history.
  - Quick check question: How does the first-order Markov assumption simplify the computation of P(xk | C1:k) compared to considering all previous iterations?

- **Concept:** Weighted sampling and its impact on estimator efficiency
  - Why needed here: Understanding how biased sampling toward higher-scoring points affects the probability of finding a good model is crucial for analyzing BANSAC's performance.
  - Quick check question: If data points are sampled with probabilities proportional to their estimated inlier scores, how does this affect the probability of selecting a minimal set of all inliers compared to uniform sampling?

## Architecture Onboarding

- **Component map:** DBN module -> Sampling module -> Hypothesis evaluation module -> Stopping criterion module -> Model refinement module
- **Critical path:**
  1. Initialize probabilities (optional from prior scores)
  2. Sample minimal set based on current probabilities
  3. Compute hypothesis and classify all points
  4. Update best model if necessary
  5. Update probabilities using new evidence
  6. Check stopping criterion
  7. Repeat until stopping criterion met or max iterations reached
- **Design tradeoffs:**
  - Accuracy vs. speed: Higher-order Markov assumptions may improve accuracy but increase computation per iteration
  - Stopping criterion sensitivity: Threshold τ must balance early termination with missing better models
  - Sampling bias: Aggressive weighting may cause premature convergence to local optima if probability updates are noisy
- **Failure signatures:**
  - If BANSAC consistently underperforms RANSAC, check if probability updates are too conservative or if the stopping criterion triggers too early
  - If execution time is unexpectedly high, verify that the probability update loop is not the bottleneck and that sampling is actually more efficient
  - Degenerate configurations may cause probability updates to reinforce incorrect classifications
- **First 3 experiments:**
  1. Implement BANSAC for homography estimation on a small HPatches sequence; compare accuracy and runtime against RANSAC with 1000 fixed iterations
  2. Vary the stopping criterion threshold τ; observe its effect on accuracy and execution time for calibrated relative pose
  3. Test different Markov assumption orders (1st, 2nd, 3rd) on the same problem; measure accuracy and runtime to determine the best tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed BANSAC algorithm compare in terms of robustness to other state-of-the-art RANSAC-based methods when applied to datasets with extremely high outlier ratios (e.g., >80%)?
- Basis in paper: [inferred] The paper shows that BANSAC improves accuracy for varying inlier ratios, but the experiments only go down to 30% inliers.
- Why unresolved: The paper does not test BANSAC on datasets with extremely high outlier ratios, so its performance in such scenarios remains unknown.
- What evidence would resolve it: Testing BANSAC on datasets with outlier ratios higher than 80% and comparing its accuracy and robustness to other methods would provide this evidence.

### Open Question 2
- Question: What is the impact of the proposed BANSAC algorithm on the computational complexity when applied to high-dimensional data, and how does it scale with increasing dimensionality?
- Basis in paper: [inferred] The paper mentions that BANSAC requires an extra loop over all data points per iteration for updating scores, but does not discuss its impact on computational complexity in high-dimensional scenarios.
- Why unresolved: The paper does not provide any analysis or experiments on the computational complexity of BANSAC in high-dimensional data, leaving this aspect unexplored.
- What evidence would resolve it: Analyzing the computational complexity of BANSAC in high-dimensional data and comparing it to other methods would provide this evidence.

### Open Question 3
- Question: How does the performance of the proposed BANSAC algorithm vary with different initial probability distributions for the inlier/outlier classification, and what is the optimal way to set these initial probabilities?
- Basis in paper: [explicit] The paper mentions that BANSAC can work with or without prior data point scorings, and when not using prior scorings, it uses a predefined value (0.5) for all data points.
- Why unresolved: The paper does not explore the impact of different initial probability distributions on the performance of BANSAC, nor does it provide guidance on how to optimally set these initial probabilities.
- What evidence would resolve it: Experimenting with different initial probability distributions and analyzing their impact on the performance of BANSAC would provide this evidence.

## Limitations

- Performance depends heavily on quality of initial inlier probability estimates, particularly for P-BANSAC variant
- Conditional probability tables were empirically tuned, raising questions about generalizability across different problem domains
- Computational overhead from probability updates could become a bottleneck for very large datasets

## Confidence

- **High confidence:** The core DBN framework and weighted sampling mechanism are well-founded theoretically and produce measurable improvements in accuracy
- **Medium confidence:** The stopping criterion effectiveness varies with threshold settings, and while the paper demonstrates competitive results, optimal parameter tuning may be dataset-dependent
- **Low confidence:** The computational complexity analysis assumes efficient implementation of the probability update step, but this could become a bottleneck for very large datasets

## Next Checks

1. Test BANSAC performance across diverse feature descriptors beyond RootSIFT to assess robustness to matching quality variations
2. Conduct ablation studies comparing first, second, and third-order Markov assumptions on real-world datasets to quantify accuracy-speed tradeoffs
3. Implement cross-dataset validation where models trained/parameterized on one dataset are evaluated on unseen datasets to test generalizability of the CPTs