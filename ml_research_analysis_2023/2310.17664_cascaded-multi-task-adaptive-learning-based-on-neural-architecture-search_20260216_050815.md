---
ver: rpa2
title: Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search
arxiv_id: '2310.17664'
source_url: https://arxiv.org/abs/2310.17664
tags:
- adapter
- fine-tuning
- architecture
- parameters
- cascaded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for automatically searching and optimizing
  the tuning strategies for cascaded multi-task models using neural architecture search
  (NAS). The key idea is to extend the candidate operations for each module in the
  cascaded model to include frozen, inserting an adapter, and fine-tuning.
---

# Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search

## Quick Facts
- **arXiv ID**: 2310.17664
- **Source URL**: https://arxiv.org/abs/2310.17664
- **Reference count**: 0
- **Primary result**: NAS-based adapter insertion reduces trainable parameters to 8.7% of full fine-tuning while achieving better NLU CER (12.81% vs 12.42%)

## Executive Summary
This paper introduces a neural architecture search (NAS) framework for automatically optimizing cascaded multi-task models by determining optimal adapter insertion strategies. The approach extends candidate operations for each module to include frozen, adapter insertion, and fine-tuning, with a parameter penalty guiding the search toward efficient architectures. Evaluated on a three-task cascade (speech enhancement, ASR, NLU) using the SLURP corpus, the method successfully compresses trainable parameters to 8.7% of full fine-tuning while achieving better performance (NLU CER of 12.81% vs 12.42%).

## Method Summary
The method extends NAS to cascaded multi-task models by treating adapter insertion as a path selection problem. For each module, NAS learns architecture parameters that determine whether to freeze, insert an adapter, or fine-tune. A parameter penalty term in the loss function encourages efficient architectures by weighting operations by their parameter count. The framework uses Gumbel-Softmax sampling for differentiable architecture selection and employs a two-stage training process: first alternating architecture and network parameter updates, then only network parameter updates.

## Key Results
- Successfully searched adapter insertion strategies similar to hand-crafted designs
- Compressed trainable parameters to 8.7% of full fine-tuning
- Achieved better NLU performance (12.81% CER) than full fine-tuning (12.42% CER)
- Validated effectiveness on SLURP corpus with three cascaded tasks

## Why This Works (Mechanism)

### Mechanism 1
NAS can automatically search for effective adapter insertion strategies in cascaded models by treating adapter insertion as a path selection problem. For each module, NAS learns architecture parameters that determine whether to freeze, insert an adapter, or fine-tune, using Gumbel-Softmax sampling and validation data while network parameters are updated using training data.

### Mechanism 2
Parameter penalty in the loss function guides NAS toward more efficient architectures by including a penalty term that weights each operation by its parameter count. This encourages the search to prefer adapter insertion over full fine-tuning when possible, reducing total trainable parameters while maintaining performance.

### Mechanism 3
Two-stage training improves final performance by first updating both architecture and network parameters alternately, then only updating network parameters while keeping the architecture fixed. This allows the model to first find a good architecture, then fine-tune the parameters within that architecture.

## Foundational Learning

- **Neural Architecture Search (NAS) fundamentals**: The entire method relies on NAS to automatically determine adapter placement and fine-tuning strategies. *Quick check: What are the two types of parameters learned in NAS, and how are they updated differently?*

- **Adapter modules and their parameter efficiency**: Adapters are the core mechanism for efficient transfer learning. *Quick check: How does a bottleneck adapter (BA) differ structurally from a gated adapter (GA)?*

- **Gumbel-Softmax for differentiable architecture sampling**: This technique allows gradient-based optimization of discrete architecture decisions during NAS. *Quick check: What is the purpose of the temperature parameter in Gumbel-Softmax, and how does it affect the sampling?*

## Architecture Onboarding

- **Component map**: NAS controller -> Cascaded model (SE -> ASR -> NLU) -> NFA modules (frozen/adapter/fine-tuning) -> Parameter penalty module -> Two-stage trainer

- **Critical path**:
  1. Split training data into training and validation sets
  2. For each iteration: update architecture parameters using validation data, then update network parameters using training data
  3. After NAS convergence: perform two-stage training (alternating updates, then network-only updates)

- **Design tradeoffs**:
  - Adapter strength vs. parameter count: Stronger adapters improve performance but increase parameters
  - Search space granularity: More candidate locations increase search time but may find better solutions
  - Parameter penalty weight: Higher penalty reduces parameters but may hurt performance

- **Failure signatures**:
  - NAS not converging: Check if validation signal is too noisy or search space is too constrained
  - Performance worse than full fine-tuning: Parameter penalty may be too strong or adapter locations suboptimal
  - Excessive parameters: Reduce adapter strength or increase penalty weight

- **First 3 experiments**:
  1. Compare NA (adapter-only NAS) vs. full plugging baseline to verify NAS effectiveness
  2. Compare NFA vs. full fine-tuning to validate parameter efficiency
  3. Test different parameter penalty weights to find optimal balance between efficiency and performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the cascaded multi-task model change when using different adapter types (e.g., bottleneck adapter vs. gated adapter) in the NFA module? While the paper compares bottleneck adapter (BA) and gated adapter (GA), it does not explore the performance of other adapter types or combinations of adapters within the NFA module.

### Open Question 2
How does the performance of the cascaded multi-task model change with varying amounts of training data? The paper includes experiments with reduced training data (5hr, 10hr, 20hr), showing that the searched architecture remains consistent, but does not explore performance with other amounts of training data.

### Open Question 3
How does the performance of the cascaded multi-task model change when using different numbers of tasks in the cascade? The paper includes experiments with different numbers of tasks, showing that performance degrades with the addition of a fourth task (wav2vec2), but does not explore performance with more than four tasks or different combinations of tasks.

## Limitations
- Experimental validation restricted to specific three-task cascade (SE-ASR-NLU) on SLURP corpus
- Architecture search space may not capture all potentially effective adaptation strategies
- Computational overhead of NAS itself is not fully characterized

## Confidence

- **High confidence**: The core NAS framework for searching adapter insertion locations is technically sound and the parameter penalty mechanism is correctly implemented
- **Medium confidence**: The two-stage training procedure's effectiveness is demonstrated empirically but lacks theoretical explanation
- **Low confidence**: Generalization to other cascaded multi-task scenarios is not established

## Next Checks

1. Apply the same NAS-based adapter insertion method to a different cascaded task scenario (e.g., speech recognition → machine translation → sentiment analysis) to test generalizability beyond the SLURP corpus and speech domain.

2. Measure the total compute time and energy consumption for the NAS search phase compared to the parameter savings achieved, to determine if the efficiency gains justify the search cost in practical deployment scenarios.

3. Compare the final performance of NFA with single-stage NAS (no separation between architecture search and parameter optimization) to determine if the two-stage procedure provides statistically significant benefits that justify its added complexity.