---
ver: rpa2
title: Model Extraction Attacks Revisited
arxiv_id: '2312.05386'
source_url: https://arxiv.org/abs/2312.05386
tags:
- attacks
- attack
- fidelity
- mlaas
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines the vulnerability of real-world
  MLaaS APIs to model extraction (ME) attacks. Using the MeBench platform, the authors
  evaluate 4 leading APIs (Amazon, Microsoft, Face++, Google) on FER and NLU tasks.
---

# Model Extraction Attacks Revisited

## Quick Facts
- arXiv ID: 2312.05386
- Source URL: https://arxiv.org/abs/2312.05386
- Reference count: 40
- Primary result: Model extraction attacks remain highly effective against real-world MLaaS APIs, with attack fidelity ranging from 63-85% across platforms

## Executive Summary
This study systematically evaluates the vulnerability of real-world MLaaS APIs to model extraction attacks using the MeBench platform. The authors test four leading APIs (Amazon, Microsoft, Face++, Google) on facial expression recognition and natural language understanding tasks. Results demonstrate that model extraction attacks remain highly effective, with attack fidelity ranging from 63-85% across platforms. The study identifies several factors that influence attack success, including optimizer choice, pre-training, and attack strategy, while also revealing that current defense mechanisms like confidence score quantization provide only marginal protection.

## Method Summary
The researchers developed a general model extraction framework using MeBench to systematically evaluate API vulnerability. They implemented various attack strategies including basic, semi-supervised learning, active learning, and adversarial learning approaches. The framework queries MLaaS APIs with crafted inputs and receives confidence scores, which are then used to train piracy models that mimic the victim models. The study evaluates multiple piracy model architectures across six benchmark datasets and compares different optimizers (SGD, Adam, AdamW, Lion) and pre-training approaches. A retrospective analysis spanning 2020-2022 was also conducted using historical API data from the HAPI dataset.

## Key Results
- Advanced optimizers (Lion, AdamW) significantly improve attack effectiveness compared to traditional SGD
- Model architecture has limited impact on attack success across different APIs
- Pre-training improves attack performance more substantially for language models than vision models
- Semi-supervised learning boosts attack fidelity but reduces adversarial fidelity
- Quantization of confidence scores only marginally mitigates attacks
- A retrospective analysis reveals evolving vulnerability patterns, with significant changes in Microsoft's FER API likely due to backend model updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model extraction attacks remain highly effective against real-world MLaaS APIs due to the availability of confidence scores and the lack of effective defenses.
- Mechanism: The attacker queries the API with crafted inputs and receives confidence scores for each class. These scores provide detailed information about the model's decision boundaries, allowing the attacker to train a piracy model that mimics the victim model's behavior.
- Core assumption: MLaaS platforms return full confidence information for all classes, enabling detailed model extraction.
- Evidence anchors:
  - [abstract]: "Key findings include: advanced optimizers (Lion, AdamW) improve attack effectiveness; model architecture has limited impact on attack success; pre-training improves attacks more for language models than vision models; semi-supervised learning boosts attack fidelity but reduces adversarial fidelity; and quantization of confidence scores only marginally mitigates attacks."
  - [section]: "Key findings include: advanced optimizers (Lion, AdamW) improve attack effectiveness; model architecture has limited impact on attack success; pre-training improves attacks more for language models than vision models; semi-supervised learning boosts attack fidelity but reduces adversarial fidelity; and quantization of confidence scores only marginally mitigates attacks."
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.413, average citations=0.0."
- Break condition: If MLaaS platforms start returning quantized or obfuscated confidence scores, or implement other defenses that limit the information available to attackers.

### Mechanism 2
- Claim: Advanced optimizers like Lion, AdamW, and Adam significantly improve the effectiveness of model extraction attacks.
- Mechanism: These optimizers converge faster and achieve higher attack fidelity compared to traditional optimizers like SGD. This allows the attacker to extract a more accurate piracy model with fewer queries.
- Core assumption: The choice of optimizer has a significant impact on the training dynamics and final performance of the piracy model.
- Evidence anchors:
  - [abstract]: "Key findings include: advanced optimizers (Lion, AdamW) improve attack effectiveness; model architecture has limited impact on attack success; pre-training improves attacks more for language models than vision models; semi-supervised learning boosts attack fidelity but reduces adversarial fidelity; and quantization of confidence scores only marginally mitigates attacks."
  - [section]: "Table 3 shows the attack fidelity at the end of 200 training epochs. Under the same query budget, Lion, Adam, and AdamW attain comparable attack fidelity. There is no distinct advantage between Lion and AdamW; however, it is evident that SGD lags behind the performance of other optimizers significantly."
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.413, average citations=0.0."
- Break condition: If the victim model uses a different optimizer or training regime that is not well-suited to the attacker's choice of optimizer.

### Mechanism 3
- Claim: Pre-training the piracy model on public datasets significantly improves the effectiveness of model extraction attacks, especially for language models.
- Mechanism: Pre-training provides the piracy model with a good initialization and general knowledge about the task domain. This allows the model to learn the victim model's behavior more effectively during the extraction process.
- Core assumption: The victim model has been pre-trained on a similar dataset or task, and the pre-training helps the piracy model learn relevant features.
- Evidence anchors:
  - [abstract]: "Key findings include: advanced optimizers (Lion, AdamW) improve attack effectiveness; model architecture has limited impact on attack success; pre-training improves attacks more for language models than vision models; semi-supervised learning boosts attack fidelity but reduces adversarial fidelity; and quantization of confidence scores only marginally mitigates attacks."
  - [section]: "In NLU, we compare randomly initialized XLNet and RoBERTa and that pre-trained on the BERT dataset [16], with results shown in Table 6. Surprisingly, unlike the PER task, pre-training language models significantly improves the attack performance. For instance, the pre-training of RoBERTa boosts fidelity by over 25%."
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.413, average citations=0.0."
- Break condition: If the victim model is not pre-trained, or if the pre-training dataset is very different from the victim model's training data.

## Foundational Learning

- Concept: Model extraction attacks
  - Why needed here: Understanding the threat model and attack strategies is crucial for evaluating the vulnerability of MLaaS platforms.
  - Quick check question: What is the difference between exact and approximate model extraction attacks?

- Concept: Knowledge distillation
  - Why needed here: Many model extraction attacks are based on knowledge distillation, where the piracy model is trained to mimic the victim model's behavior.
  - Quick check question: How does knowledge distillation differ from traditional supervised learning?

- Concept: Adversarial examples
  - Why needed here: Adversarial examples can be used to enhance model extraction attacks by providing more informative queries to the victim model.
  - Quick check question: What is the difference between adversarial examples and regular examples in the context of model extraction?

## Architecture Onboarding

- Component map: MeBench platform -> Attack strategies (basic, semi-supervised learning, active learning, adversarial learning) -> Performance metrics (Accuracy, fidelity, adversarial fidelity) -> Piracy models (VGG, ResNet, DenseNet, ViT, RoBERTa, XLNet) -> Benchmark datasets (RAFDB, EXPW, KDEF, FER+, IMDB, YELP)

- Critical path: 1. Query the MLaaS API with crafted inputs 2. Receive confidence scores for each class 3. Train the piracy model using the query-response pairs 4. Evaluate the attack fidelity and accuracy of the piracy model

- Design tradeoffs:
  - Query budget vs. attack fidelity: More queries generally lead to higher attack fidelity, but increase the cost and risk of detection
  - Model complexity vs. attack success: More complex piracy models may achieve higher attack fidelity, but are harder to train and optimize
  - Optimizer choice vs. training efficiency: Advanced optimizers like Lion and AdamW converge faster and achieve higher attack fidelity, but may require more computational resources

- Failure signatures:
  - Low attack fidelity: The piracy model fails to accurately mimic the victim model's behavior
  - High query cost: The attack requires an excessive number of queries to achieve satisfactory attack fidelity
  - Detection by MLaaS provider: The attacker's queries are flagged as suspicious and blocked by the platform

- First 3 experiments:
  1. Evaluate the basic model extraction attack on a simple MLaaS API using a small query budget and a basic piracy model architecture.
  2. Compare the attack fidelity and query cost of different optimizers (SGD, Adam, AdamW, Lion) on the same API and piracy model.
  3. Assess the impact of pre-training the piracy model on a public dataset for the NLU task, using the best-performing optimizer from experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific defense mechanisms are currently employed by MLaaS platforms against model extraction attacks?
- Basis in paper: [explicit] The paper mentions that Amazon, Microsoft, and Face++ provide exact confidence values for each class, while Google uses likelihood descriptors instead of confidence scores. However, the effectiveness of these defenses is unclear.
- Why unresolved: The paper only briefly mentions these observations without a comprehensive analysis of the effectiveness of current defense mechanisms.
- What evidence would resolve it: A detailed study of various defense mechanisms implemented by different MLaaS providers, including their effectiveness against different types of ME attacks and their impact on legitimate users.

### Open Question 2
- Question: How do model extraction attacks evolve when the adversary has partial knowledge about the victim model's architecture or training data?
- Basis in paper: [inferred] The paper assumes a black-box setting where the adversary has little knowledge about the victim model, but many prior studies (e.g., [32]) assume the adversary possesses knowledge about the victim model's architecture.
- Why unresolved: The paper focuses on the black-box setting but doesn't explore scenarios with varying levels of adversary knowledge.
- What evidence would resolve it: Comparative studies of ME attacks under different knowledge assumptions, measuring attack success rates and query efficiency as the adversary's knowledge increases.

### Open Question 3
- Question: What is the relationship between the query budget and attack fidelity in real-world MLaaS APIs, and how does this relationship vary across different tasks and datasets?
- Basis in paper: [explicit] The paper mentions that prior work (e.g., [19]) shows a positive correlation between query budget and attack fidelity, but the study doesn't provide a comprehensive analysis of this relationship in the context of real-world MLaaS APIs.
- Why unresolved: While the paper evaluates different query budgets, it doesn't systematically analyze how the relationship between query budget and attack fidelity varies across tasks and datasets.
- What evidence would resolve it: Detailed analysis of attack fidelity as a function of query budget across multiple MLaaS APIs, tasks, and datasets, identifying patterns and potential plateaus in the relationship.

## Limitations

- The study's findings are based on a limited set of APIs and datasets, which may not fully represent the diversity of real-world MLaaS platforms.
- The historical data analysis is constrained by the availability of API endpoints in the HAPI dataset, potentially missing important changes in platform behavior over time.
- The study assumes that MLaaS providers return full confidence scores, which may not always be the case in practice.

## Confidence

- **High Confidence**: The effectiveness of advanced optimizers (Lion, AdamW) in improving attack fidelity is well-supported by empirical results across multiple APIs and tasks.
- **Medium Confidence**: The impact of pre-training on attack success for language models is demonstrated, but the generalizability to other model types and tasks requires further validation.
- **Low Confidence**: The retrospective analysis of vulnerability trends over 2020-2022 is limited by the availability of historical data and may not capture all relevant changes in platform behavior.

## Next Checks

1. Replicate Results on Additional APIs: Test the attack effectiveness on a broader range of MLaaS platforms, including those not covered in the study, to assess the generalizability of the findings.

2. Evaluate Defense Mechanisms: Investigate the effectiveness of proposed defenses (e.g., quantized confidence scores) against the attack strategies presented in the study.

3. Analyze Attack Transferability: Assess whether the attack fidelity achieved on one API translates to other APIs or models with similar architectures, to understand the broader implications of the vulnerability.