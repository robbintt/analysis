---
ver: rpa2
title: Closed-Form Diffusion Models
arxiv_id: '2310.12395'
source_url: https://arxiv.org/abs/2310.12395
tags:
- samples
- training
- score
- sampling
- cfdm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to training-free diffusion
  models that leverage closed-form score functions of perturbed empirical distributions.
  The key idea is to explicitly smooth the exact closed-form score function to promote
  generalization, avoiding the need for neural network approximations and associated
  training costs.
---

# Closed-Form Diffusion Models

## Quick Facts
- arXiv ID: 2310.12395
- Source URL: https://arxiv.org/abs/2310.12395
- Reference count: 40
- Key outcome: Proposes training-free diffusion models using closed-form score functions of perturbed empirical distributions, achieving competitive sample quality on consumer-grade CPUs without neural network training.

## Executive Summary
This paper introduces a novel approach to diffusion models that eliminates the need for neural network training by leveraging closed-form score functions of perturbed empirical distributions. The key innovation is explicitly smoothing the exact closed-form score function to promote generalization, avoiding the need for neural network approximations and associated training costs. The method achieves competitive sample quality and generation times on a consumer-grade CPU, outperforming trained diffusion models in some cases. Notably, it generates novel samples without any training by outputting barycenters of training data tuples, with the smoothing parameter controlling the trade-off between fidelity and generalization.

## Method Summary
The method involves smoothing the exact closed-form score function of a perturbed empirical distribution to promote generalization. This smoothed score is efficiently approximated using a nearest-neighbor-based estimator, enabling scalable sampling in high-dimensional tasks like image generation. The approach works by computing the score function as the gradient of the log-density of a Gaussian kernel density estimator, then applying explicit smoothing through additive Gaussian noise. Sampling is performed using a forward Euler scheme with the smoothed score, starting from random noise and iteratively updating sample positions. The smoothing parameter σ controls the trade-off between fidelity to training data and generation of novel samples.

## Key Results
- Achieves competitive sample quality on CIFAR-10 and CelebA without any neural network training
- Generates samples on consumer-grade CPU in reasonable time (e.g., 13.4s for 128x128 CelebA)
- Outperforms trained diffusion models in some cases, particularly for smaller datasets
- Provides theoretical bounds on approximation error and practical implementation guidelines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smoothing the exact closed-form score function promotes generalization by reducing memorization.
- Mechanism: The smoothing operation adds noise to the distance weights in the softmax, causing the score function to point toward barycenters of training points rather than directly toward individual training samples.
- Core assumption: Neural networks exhibit spectral bias, fitting low-frequency components first, which means explicit smoothing can approximate this effect without a neural network.
- Evidence anchors: [abstract]: "we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training"; [section 4.1]: "Deep neural networks fit the low-frequency components of their target functions first during training...we induce error in the score function by smoothing it"
- Break condition: If smoothing parameter σ is too large, the model spreads out to fill the convex hull of training data and loses fidelity to the true distribution.

### Mechanism 2
- Claim: The smoothed score can be efficiently approximated using nearest-neighbor search, enabling scalability to high-dimensional tasks.
- Mechanism: In the t → 1 regime, the softmax temperature is low and the score sum is dominated by nearest neighbors. By using approximate nearest-neighbor search, we can subsample the O(N) terms while ensuring nearest neighbors are included with high probability.
- Core assumption: The contribution of distant points becomes negligible when the temperature is low, making nearest-neighbor approximation valid.
- Evidence anchors: [section 5.3]: "the temperature of the softmax in (4) is low, and the large sum is dominated by the handful of terms corresponding to the smallest values of ∥z − txi∥2"; [section 5.3]: "we employ Karppa et al. (2022)'s unbiased nearest-neighbor estimator for KDEs to estimate the denominator"
- Break condition: If smoothing parameter σ is too small, the approximation error from nearest-neighbor search becomes significant.

### Mechanism 3
- Claim: Starting sampling at T > 0 with samples from an unsmoothed CFDM reduces computational cost while maintaining accuracy for practical values of σ.
- Mechanism: By initializing at a later time T, we can take fewer sampling steps. The approximation error between starting at T > 0 versus T = 0 is bounded and depends on both T and σ.
- Core assumption: The error from starting later is acceptable for the range of σ values that work well in practice.
- Evidence anchors: [section 5.2]: "we can reduce the cost of sampling a σ-CFDM by taking fewer steps" and provides the bound W2(ρ0σ,1−ϵ, ρTσ,1−ϵ) = O(exp(1/(1−T)2)·σ/(ϵ2(1−T)2)); [section 6.2]: "empirical evidence that this bound is pessimistic, and that for practical values of σ one can begin sampling at T close to 1 with little accuracy loss"
- Break condition: If T is too close to 1 or σ is too large, the approximation error becomes unacceptable.

## Foundational Learning

- Concept: Score-based generative models and the connection between score functions and probability densities
  - Why needed here: The entire method relies on understanding that the score function ∇ log ρ is the gradient of the log-density, and that diffusion models use this to transform noise into samples
  - Quick check question: If ρ is a probability density, what is the relationship between ∇ log ρ and the gradient of ρ?

- Concept: Kernel density estimation and its connection to Gaussian mixtures
  - Why needed here: The closed-form score is the score of a Gaussian kernel density estimator, and understanding this connection is crucial for the nearest-neighbor approximation
  - Quick check question: If you have N data points and use a Gaussian kernel with bandwidth h, what is the form of the resulting density estimate?

- Concept: Wasserstein distance and its role in measuring distributional similarity
  - Why needed here: The paper uses 2-Wasserstein distance W2 to measure how close the generated samples are to true samples, both for evaluation and in the theoretical bounds
  - Quick check question: What does W2(ρ, ν) measure between two probability distributions ρ and ν?

## Architecture Onboarding

- Component map: Training data loader -> Smoothing parameter σ -> Nearest-neighbor estimator (K, L) -> Sampling loop with forward Euler -> Generated samples

- Critical path:
  1. Load training data
  2. Set smoothing parameter σ
  3. For each sampling step:
     - Compute smoothed score using nearest-neighbor estimator
     - Update sample position using Euler integration
  4. Output generated samples

- Design tradeoffs:
  - Larger σ → more generalization but less fidelity to training data
  - Smaller K in nearest-neighbor estimator → faster but noisier
  - Starting at T > 0 → fewer steps but approximation error
  - Pixel space vs. latent space → direct generation vs. better quality

- Failure signatures:
  - If samples are just training data copies → σ is too small
  - If samples are too blurry/smoothed → σ is too large
  - If sampling is too slow → K or L are too large, or not using T > 0
  - If samples don't resemble data manifold → working in wrong space or σ inappropriate

- First 3 experiments:
  1. Run with σ = 0 on a simple 2D dataset (e.g., two moons) to verify the model just outputs training data
  2. Run with small σ (e.g., 0.1-0.3) on the same dataset to see generalization behavior
  3. Vary K and L in the nearest-neighbor estimator to find the accuracy-speed trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise form of the effective regularization that enables neural score functions to generalize in practice?
- Basis in paper: The paper notes that the error in the approximation of the score function by a neural network promotes generalization, but the effective regularization this error provides is not well-understood theoretically.
- Why unresolved: Existing work studies the support of an SGM's model distribution and provides conditions under which an SGM memorizes its training data or learns to sample from the true data manifold, but does not fully characterize the effective regularization.
- What evidence would resolve it: A theoretical analysis of the effective regularization provided by the approximation error in neural score functions, or empirical evidence demonstrating the impact of this regularization on generalization.

### Open Question 2
- Question: How does the choice of smoothing parameter σ impact the quality of samples generated by a smoothed CFDM in high-dimensional tasks such as image generation?
- Basis in paper: The paper introduces the smoothing parameter σ to promote generalization in the closed-form score function, and shows that the quality of samples approaches the true distribution for appropriate values of σ.
- Why unresolved: The paper does not provide a systematic study of the impact of σ on sample quality in high-dimensional tasks, or guidelines for choosing σ in practice.
- What evidence would resolve it: A comprehensive empirical study of the impact of σ on sample quality in high-dimensional tasks, or theoretical bounds on the distance between the model distribution and the true distribution as a function of σ.

### Open Question 3
- Question: Can smoothed CFDMs achieve competitive sample quality in image generation while operating directly in pixel space, without the need for a pretrained autoencoder?
- Basis in paper: The paper notes that to generate high-quality images, smoothed CFDMs sample in the latent space of a pretrained autoencoder, and suggests investigating whether large training sets and appropriate latent encodings can enable competitive sample quality in image generation.
- Why unresolved: The paper does not provide a systematic study of the impact of sampling in pixel space versus latent space, or guidelines for choosing an appropriate latent encoding.
- What evidence would resolve it: A comparative study of sample quality when sampling in pixel space versus latent space, or theoretical bounds on the distance between the model distribution and the true distribution when sampling in different spaces.

## Limitations

- Empirical validation is limited to relatively small-scale image datasets (CIFAR-10 and CelebA) without comparison against the latest diffusion model architectures
- Claims about outperforming trained diffusion models are based on limited metrics (W2 distance and LPIPS) without standard FID scores or comparisons against modern guidance techniques
- Computational complexity analysis for the nearest-neighbor estimator is theoretical, with practical performance potentially degrading on larger datasets or higher-dimensional data

## Confidence

- **High confidence**: The core mathematical framework connecting closed-form scores to Gaussian kernel density estimation is well-established. The smoothing mechanism and its theoretical justification are rigorous.
- **Medium confidence**: The nearest-neighbor approximation strategy and its computational benefits are theoretically sound, but practical performance depends heavily on implementation details not fully specified in the paper.
- **Low confidence**: Claims about competitive performance relative to trained models, particularly the assertion of outperforming them in some cases, require more extensive empirical validation across diverse datasets and model architectures.

## Next Checks

1. Replicate the fundamental behavior: Test the σ = 0 case on a simple 2D dataset (e.g., two moons or Swiss roll) to verify the model simply outputs training data, then gradually increase σ to observe the transition to novel samples.

2. Benchmark against modern diffusion: Compare the proposed method against current state-of-the-art diffusion models (DDIM, EDM, etc.) on standard benchmarks using multiple metrics (FID, IS, precision-recall) beyond just W2 distance and LPIPS.

3. Stress test the approximation: Systematically vary K and L in the nearest-neighbor estimator across different dataset sizes and dimensions to empirically verify the claimed computational advantages and identify break points where the approximation becomes unreliable.