---
ver: rpa2
title: A novel post-hoc explanation comparison metric and applications
arxiv_id: '2311.10811'
source_url: https://arxiv.org/abs/2311.10811
tags:
- learning
- distance
- machine
- shreyan
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Shreyan Distance, a new metric for quantifying
  differences between post-hoc explanatory systems by computing weighted differences
  between ranked feature importance lists. It applies this metric to compare SHAP
  and LIME explanations across 64 regression and classification models.
---

# A novel post-hoc explanation comparison metric and applications

## Quick Facts
- arXiv ID: 2311.10811
- Source URL: https://arxiv.org/abs/2311.10811
- Reference count: 34
- Key outcome: The Shreyan Distance metric shows that explainer consistency depends on both explainer properties and learning task type

## Executive Summary
This paper introduces the Shreyan Distance, a novel metric for quantifying differences between post-hoc explanatory systems by computing weighted differences between ranked feature importance lists. The authors apply this metric to compare SHAP and LIME explanations across 64 regression and classification models, finding statistically significant differences in explainer consistency between task types. They also present XAISuite, a Python library that integrates the Shreyan Distance into machine learning pipelines for seamless explainer comparison.

## Method Summary
The authors developed the Shreyan Distance metric to measure differences between explainer rankings by computing weighted sums of absolute differences between feature importance ranks. They tested this metric by comparing SHAP and LIME explanations across 32 regression and 32 classification models from scikit-learn, using synthetic datasets generated by make_regression and make_classification. For each model, they computed 100-instance datasets, generated explanations, calculated Shreyan Distances, repeated this process 3 times, and performed statistical analysis using a two-sample t-test to compare average distances between regression and classification tasks.

## Key Results
- The average Shreyan Distance differs significantly between regression and classification tasks (p < 0.05)
- The metric captures relative importance of feature ranking differences with higher weights for top-ranked features
- XAISuite library successfully integrates Shreyan Distance into ML pipelines for automated explainer comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Shreyan Distance metric captures the relative importance of feature ranking differences by weighting discrepancies based on rank position, with higher weights for top-ranked features.
- Mechanism: The metric computes a weighted sum of absolute differences between feature importance ranks, where the weight for each position decreases linearly with rank. This ensures that differences in the most important features contribute more to the distance than differences in less important features.
- Core assumption: Users care more about consistency in the most important features than in less important ones when comparing explanations.
- Evidence anchors:
  - [abstract]: "The Shreyan Distance, a novel metric based on the weighted difference between ranked feature importance lists produced by such systems."
  - [section]: "The Shreyan Distance rectifies this by assigning a linear weightage to each vector dimension."
  - [corpus]: Weak - no direct citations to support this mechanism, but the metric definition itself provides the basis.
- Break condition: If users care equally about all features or if the feature importance ranking does not reflect true feature importance, the weighted approach may misrepresent similarity.

### Mechanism 2
- Claim: The Shreyan Distance is sensitive to the absolute difference in feature importance rankings but not to the sign of correlation between explainers.
- Mechanism: Unlike correlation-based metrics, the Shreyan Distance focuses on the cumulative weighted difference between ranked lists. Two explainers that rank features in opposite orders but share some common top features will have a non-zero distance, reflecting partial similarity.
- Core assumption: Users interpret similarity as shared identification of important features, regardless of overall correlation direction.
- Evidence anchors:
  - [abstract]: "consistency between explainers not only depends on inherent properties of the explainers themselves, but also the type of learning task."
  - [section]: "Example 3: Negative Correlation Does Not Result in Minimum Shreyan Distance" demonstrates this property explicitly.
  - [corpus]: Weak - no external validation, but the example in the paper provides internal evidence.
- Break condition: If users expect negative correlation to imply complete dissimilarity, the Shreyan Distance may not align with their expectations.

### Mechanism 3
- Claim: The XAISuite library enables seamless integration of the Shreyan Distance into machine learning pipelines, allowing users to compare explainers across different models and tasks.
- Mechanism: The library provides a unified framework with components for data loading, model training, explanation generation, and explainer comparison. It abstracts away the complexity of implementing the Shreyan Distance and integrates it with common ML utilities.
- Core assumption: Users need a simple, flexible tool to incorporate explainer comparison into their existing workflows without extensive coding.
- Evidence anchors:
  - [abstract]: "XAISuite library, which integrates the Shreyan distance algorithm into machine learning pipelines."
  - [section]: "The XAISuite library’s data visualization and explanation comparison abilities build on the core functionality offered by OmniXAI."
  - [corpus]: Weak - no citations to the library itself, but the description in the paper outlines its intended functionality.
- Break condition: If the library is not compatible with the user's preferred ML framework or if the abstraction hides important implementation details, it may limit usability.

## Foundational Learning

- Concept: Ranked vector comparison metrics (e.g., Spearman's footrule, Kendall's tau)
  - Why needed here: Understanding how the Shreyan Distance differs from existing metrics requires knowledge of how other metrics handle rank differences.
  - Quick check question: How does Spearman's footrule treat differences in top-ranked features compared to lower-ranked features?

- Concept: Post-hoc explanation methods (e.g., SHAP, LIME)
  - Why needed here: The paper compares SHAP and LIME using the Shreyan Distance, so understanding how these methods generate feature importance rankings is crucial.
  - Quick check question: What is the key difference between SHAP and LIME in how they approximate model predictions locally?

- Concept: Statistical hypothesis testing (e.g., two-sample t-test)
  - Why needed here: The paper uses a t-test to determine if the average Shreyan Distance differs significantly between regression and classification tasks.
  - Quick check question: What assumptions must hold for a two-sample t-test to be valid?

## Architecture Onboarding

- Component map:
  Data Loader -> Data Processor -> Model Trainer -> Explainers -> Insight Generator -> XAISuite Library

- Critical path:
  1. Load dataset → 2. Process data → 3. Train model → 4. Generate explanations → 5. Calculate Shreyan Distance → 6. Visualize results.

- Design tradeoffs:
  - Simplicity vs. flexibility: The XAISuite framework prioritizes simplicity by abstracting away implementation details, but this may limit flexibility for advanced users who want more control.
  - Generality vs. specificity: The framework is designed to work with any model or explainer, but this generality may come at the cost of optimized performance for specific use cases.

- Failure signatures:
  - Incorrect Shreyan Distance values: Could indicate a bug in the metric calculation or an issue with the input data.
  - Incompatible data formats: May occur if the Data Processor is not configured correctly for the input dataset.
  - Missing explainer results: Could be due to an issue with the Explainers component or the model not supporting the requested explainer.

- First 3 experiments:
  1. Verify the Shreyan Distance calculation: Compare the output of the Insight Generator on a simple, hand-crafted dataset with expected values.
  2. Test the XAISuite pipeline: Run the full pipeline on a small dataset with a simple model (e.g., Linear Regression) and verify that all components work together.
  3. Evaluate explainer consistency: Use the XAISuite library to compare SHAP and LIME on a synthetic dataset where the ground truth feature importance is known, and verify that the Shreyan Distance reflects the expected similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Shreyan Distance metric outperform other existing metrics like Spearman's Footrule or Kendall's Tau in capturing explainer consistency across diverse learning tasks?
- Basis in paper: [explicit] The authors compare Shreyan Distance to Spearman's Distance and weighted Kendall's Tau, noting advantages in weighting differences in feature importance rankings, but acknowledge the need for further evaluation.
- Why unresolved: The paper only provides limited comparisons and does not conduct comprehensive experiments to validate Shreyan Distance superiority across multiple datasets and explainers.
- What evidence would resolve it: Systematic experiments comparing Shreyan Distance to alternative metrics across diverse datasets, learning tasks, and explainer types, with statistical validation of performance differences.

### Open Question 2
- Question: Are the observed differences in explainer consistency between regression and classification tasks due to fundamental differences in these tasks or artifacts of the specific models and datasets used?
- Basis in paper: [explicit] The authors find statistically significant differences in average Shreyan Distance between regression and classification tasks but acknowledge this could depend on the specific models and datasets used.
- Why unresolved: The study only uses 64 models and autogenerated datasets, which may not be representative of all possible regression and classification scenarios.
- What evidence would resolve it: Replicating the analysis with diverse real-world datasets, additional learning tasks (e.g., time series, anomaly detection), and a broader range of models to determine if the pattern holds.

### Open Question 3
- Question: How can the discrepancies between SHAP and LIME explanations be systematically resolved to improve explainer consistency?
- Basis in paper: [inferred] The authors identify inconsistencies between SHAP and LIME but do not propose specific methods to resolve them, suggesting this as an area for future work.
- Why unresolved: The paper focuses on quantifying differences rather than developing methods to harmonize explanations.
- What evidence would resolve it: Development and validation of techniques to reconcile SHAP and LIME explanations, such as ensemble methods or post-processing algorithms, with empirical evaluation of improved consistency.

## Limitations

- The study relies on synthetic datasets rather than real-world data, limiting generalizability
- The Shreyan Distance metric lacks comprehensive comparison to established rank correlation metrics
- The XAISuite library is described but not independently evaluated for usability or performance

## Confidence

- Shreyan Distance formulation and calculation: High
- Statistical significance of regression vs classification difference: Medium
- Practical utility of XAISuite library: Low
- General superiority over existing metrics: Low

## Next Checks

1. Compare Shreyan Distance results with established rank correlation metrics on the same datasets to validate its behavior.
2. Test the metric on real-world datasets with known feature importance patterns to assess practical utility.
3. Conduct a user study evaluating whether the weighted rank differences captured by Shreyan Distance align with human judgments of explainer similarity.