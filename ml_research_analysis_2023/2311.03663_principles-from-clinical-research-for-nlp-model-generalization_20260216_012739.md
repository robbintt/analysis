---
ver: rpa2
title: Principles from Clinical Research for NLP Model Generalization
arxiv_id: '2311.03663'
source_url: https://arxiv.org/abs/2311.03663
tags:
- test
- data
- generalization
- validity
- surface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that poor generalization in NLP models can stem
  from internal validity issues rather than just out-of-distribution (OOD) data. The
  authors demonstrate that a relation extraction model may rely on spurious correlations
  (e.g., entity distance) in training data, leading to inflated test performance but
  poor real-world accuracy.
---

# Principles from Clinical Research for NLP Model Generalization

## Quick Facts
- arXiv ID: 2311.03663
- Source URL: https://arxiv.org/abs/2311.03663
- Reference count: 25
- Key outcome: Poor generalization in NLP models can stem from internal validity issues like spurious correlations rather than just out-of-distribution data

## Executive Summary
This paper argues that poor generalization in NLP models can stem from internal validity issues rather than just out-of-distribution (OOD) data. The authors demonstrate that a relation extraction model may rely on spurious correlations (e.g., entity distance) in training data, leading to inflated test performance but poor real-world accuracy. To analyze such failures, they propose adapting clinical research practices: ensuring internal validity by detecting spurious correlations and using methods like randomized controlled trials (RCTs) for cause-effect analysis. Their case study shows strong correlations between model predictions and handcrafted surface patterns, indicating reliance on spurious features. They recommend rigorous data analysis, model explainability, and robust test sets to ensure reliable generalization.

## Method Summary
The authors fine-tune BioBERT on phosphorylation relation extraction (PTM-PPI) and protein-chemical relation extraction (ChemProt) datasets. They create three additional datasets: test set with original labels, test set with BioBERT predictions as labels, and generalization set with high-confidence BioBERT predictions. They then train surrogate models (Naive Bayes and Naive Bayes + Decision Tree) on these datasets to measure correlation between surrogate predictions and target labels. They also propose using perturbed counterpart test sets to verify model predictions, similar to RCTs in clinical research.

## Key Results
- BioBERT shows high performance on PTM-PPI test set but fails to generalize to ChemProt dataset
- Strong correlations between BioBERT predictions and surrogate models indicate reliance on surface patterns (entity distance)
- Proposed perturbed counterpart evaluation method can help verify if models learn genuine linguistic patterns or spurious correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spurious correlations in training data can lead to inflated test performance but poor real-world accuracy
- Mechanism: The model learns to rely on surface patterns (e.g., entity distance) that are present in training and test data but not generalizable
- Core assumption: Surface patterns are sufficiently correlated with labels in the training set to allow the model to achieve high accuracy without deeper linguistic understanding
- Evidence anchors:
  - [abstract] "We demonstrate how learning spurious correlations, such as the distance between entities in relation extraction tasks, can affect a model's internal validity and in turn adversely impact generalization."
  - [section] "In the case of PTM dataset BioBERT appears to heavily rely on surface patterns reflected in our hand crafted distance-based patterns."
  - [corpus] Found 25 related papers, average neighbor FMR=0.335, suggesting moderate relevance to the topic
- Break condition: If surface patterns are not correlated with labels in the training data, or if the test set does not contain the same spurious patterns, the mechanism breaks

### Mechanism 2
- Claim: Ensuring internal validity is a prerequisite for external generalization
- Mechanism: Internal validity ensures that the cause (linguistic capability) and effect (model performance) are properly measured within the test set before considering broader generalization
- Core assumption: If a model cannot demonstrate reliable performance within its intended test set, it cannot be expected to generalize to other datasets or tasks
- Evidence anchors:
  - [abstract] "We argue that the external validity of a model should only be examined if the internal validity of the model is established."
  - [section] "Internal validity is crucial to ensure that the cause and effect is not affected by unintended consequence as a result of spurious correlations or bias in the data."
  - [corpus] Related papers on clinical NLP and generalizability suggest this is a recognized concern in the field
- Break condition: If the test set itself is flawed (e.g., contains the same spurious patterns as training), internal validity may be established but not meaningful for real-world performance

### Mechanism 3
- Claim: Randomized controlled trials (RCTs) principles can be adapted to NLP evaluation to measure causation
- Mechanism: By creating perturbed counterpart test sets where ground-truth labels are altered, we can verify if the model's predictions change accordingly, indicating reliance on learned linguistic patterns
- Core assumption: If a model has learned the key linguistic aspects required for the task, its predictions should change when the input is perturbed in a way that alters the ground-truth label
- Evidence anchors:
  - [abstract] "We also propose adapting the idea of matching in randomized controlled trials and observational studies to NLP evaluation to measure causation."
  - [section] "We propose that a model's prediction should be marked as correct if and only if it's prediction on the perturbed counterpart is also correct."
  - [corpus] No direct evidence in corpus, but the concept is novel and not widely adopted in NLP literature
- Break condition: If the perturbations do not meaningfully alter the ground-truth label, or if the model is not sensitive to the perturbations, the mechanism breaks

## Foundational Learning

- Concept: Internal vs. External Validity
  - Why needed here: Understanding the distinction is crucial for diagnosing generalization failures and designing appropriate experiments
  - Quick check question: Can a model with high internal validity still fail to generalize? Why or why not?
- Concept: Spurious Correlations
  - Why needed here: Identifying and mitigating spurious correlations is key to ensuring that model performance is based on genuine linguistic understanding
  - Quick check question: How can you detect if a model is relying on spurious correlations rather than meaningful features?
- Concept: Randomized Controlled Trials (RCTs)
  - Why needed here: Adapting RCT principles to NLP evaluation can help establish causation and improve the reliability of generalization claims
  - Quick check question: How would you design a perturbed test set to verify a model's reliance on learned linguistic patterns?

## Architecture Onboarding

- Component map: Data preprocessing (surface pattern extraction) -> Model training (BioBERT) -> Surrogate model (NB/NB-T) -> Evaluation (correlation analysis)
- Critical path: Data preprocessing → Model training → Surrogate model fitting → Correlation analysis → Interpretation
- Design tradeoffs: Simple surrogate models (NB) are interpretable but may miss complex patterns; more complex models (NB-T) can capture more patterns but are less interpretable
- Failure signatures: High correlation between surrogate model and main model predictions indicates reliance on surface patterns; low correlation suggests the model may be learning more robust features
- First 3 experiments:
  1. Train BioBERT on PTM-PPI dataset and evaluate performance on official test set
  2. Fit surrogate models (NB and NB-T) on training and test sets with ground-truth labels; analyze correlation
  3. Fit surrogate models on test sets with BioBERT predictions; analyze correlation to detect reliance on surface patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective methods to detect and mitigate spurious correlations learned by NLP models beyond simple surrogate models?
- Basis in paper: [explicit] The paper discusses the limitations of simple surrogate models in detecting spurious correlations, stating that they can only detect dominant spurious features while deep learning models can learn thousands of such features
- Why unresolved: Current methods rely on apriori knowledge of what to look for and may miss less dominant but still impactful spurious correlations. There is a need for more comprehensive approaches to analyze large volumes of data for surface patterns
- What evidence would resolve it: Development and validation of novel techniques that can automatically detect a wider range of spurious correlations without requiring apriori feature engineering, along with experimental results demonstrating their effectiveness in improving model generalization

### Open Question 2
- Question: What are the most effective strategies to design test sets that can reliably assess a model's linguistic capabilities rather than its reliance on spurious correlations?
- Basis in paper: [explicit] The paper discusses the need for effective test sets that can measure a model's linguistic skills within the context of the test set to ensure internal validity. It proposes using contrast sets with minor perturbations to create counterpart test sets
- Why unresolved: Designing test sets that are both comprehensive and can effectively differentiate between genuine linguistic understanding and spurious correlation reliance is challenging. There is a need for more systematic approaches to test set design
- What evidence would resolve it: Empirical studies comparing the effectiveness of different test set design strategies in identifying models that rely on spurious correlations versus those with genuine linguistic capabilities, along with analysis of the characteristics of successful test sets

### Open Question 3
- Question: How can we adapt the concept of randomized controlled trials (RCTs) from clinical studies to NLP model evaluation to establish cause and effect in model performance?
- Basis in paper: [explicit] The paper proposes adapting the idea of RCTs from clinical studies to NLP evaluation, suggesting the use of counterpart test sets created by making minor perturbations to original test samples to verify if the model changes its prediction
- Why unresolved: While the concept is proposed, there is no clear methodology for implementing RCTs in NLP model evaluation. Questions remain about how to create effective perturbations, how to define control groups, and how to measure the impact of these perturbations on model performance
- What evidence would resolve it: Development of a concrete framework for implementing RCTs in NLP model evaluation, along with experimental results demonstrating its effectiveness in identifying the true causes of model performance compared to traditional evaluation methods

## Limitations

- The framework's applicability beyond relation extraction tasks remains unproven
- The perturbed counterpart evaluation method may not capture all forms of spurious correlation, particularly those involving complex contextual patterns or multi-hop reasoning
- The case study focuses on a single dataset (PTM-PPI), limiting generalizability of findings

## Confidence

- High confidence: The core argument that internal validity issues can masquerade as generalization failures, and the utility of correlation analysis with surrogate models to detect spurious correlations
- Medium confidence: The specific claim that surface patterns (entity distance) were the primary source of spurious correlation in the PTM-PPI dataset, as other unmeasured factors may contribute
- Medium confidence: The proposed perturbed counterpart evaluation method, as it requires further validation across different tasks and model architectures

## Next Checks

1. Apply the framework to a different NLP task (e.g., sentiment analysis or named entity recognition) to verify generalizability beyond relation extraction
2. Implement and evaluate the perturbed counterpart method on a task where the expected linguistic patterns are well-understood to verify its effectiveness
3. Test whether incorporating explicit distance features into BioBERT training eliminates the spurious correlation, providing stronger evidence that distance was indeed the problematic feature