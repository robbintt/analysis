---
ver: rpa2
title: 'Bridging the Language Gap: Dynamic Learning Strategies for Improving Multilingual
  Performance in LLMs'
arxiv_id: '2305.17740'
source_url: https://arxiv.org/abs/2305.17740
tags:
- languages
- multilingual
- performance
- language
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a dynamic learning approach for improving multilingual
  performance in large language models (LLMs) by optimizing prompt strategies, embedding
  models, and LLM selection per query at runtime. The method combines polyglot prompt
  optimization with a hybrid retrieval-augmented generation approach using multilingual
  embeddings.
---

# Bridging the Language Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs

## Quick Facts
- arXiv ID: 2305.17740
- Source URL: https://arxiv.org/abs/2305.17740
- Reference count: 40
- Key outcome: Dynamic learning approach achieves 10-15% improvements in multilingual QA performance over pre-trained models and 4x gains compared to fine-tuned language-specific models

## Executive Summary
This work presents a dynamic learning approach for improving multilingual performance in large language models by optimizing prompt strategies, embedding models, and LLM selection per query at runtime. The method combines polyglot prompt optimization with a hybrid retrieval-augmented generation approach using multilingual embeddings. A contextual bandit learning algorithm dynamically selects the optimal strategy for each query based on real-time feedback. Evaluated across 18 languages using QA datasets, the approach demonstrates substantial advancements in multilingual understanding and generation, particularly for non-Latin and low-resource languages.

## Method Summary
The approach uses a contextual bandit algorithm to dynamically select from 10 possible combinations (5 prompt strategies x 2 embedding models) based on contextual features like query, language, and dataset. The system employs hybrid RAG with multilingual embeddings (MuRIL for Indic languages, XLM-R for global languages) to improve comprehension of non-Latin scripts and low-resource languages. A second LLM (GPTAnnotator) evaluates generated answers, providing more reliable semantic-based scoring than traditional word-level F1 metrics. The method was evaluated on IndicQA and TyDiQA datasets covering 18 languages.

## Key Results
- Achieves 10-15% improvements in multilingual performance over pre-trained models
- Demonstrates 4x gains compared to fine-tuned language-specific models
- GPTAnnotator reduces evaluation error by 30% compared to MLQA-F1 scores

## Why This Works (Mechanism)

### Mechanism 1
Dynamic strategy selection improves multilingual LLM performance by matching prompt strategy and embedding model to each query. A contextual bandit algorithm selects from 10 possible combinations based on contextual features like query, language, and dataset, maximizing performance metrics. Performance on multilingual QA tasks is not uniform across languages and strategies; optimal choices vary by query context.

### Mechanism 2
Hybrid RAG with multilingual embeddings improves comprehension of non-Latin and low-resource languages. RAG retrieves relevant passages using multilingual embeddings before passing them to the LLM, enhancing context understanding beyond the LLM's intrinsic knowledge. Default GPT embeddings inadequately represent non-Latin and low-resource languages, leading to poorer retrieval and generation quality.

### Mechanism 3
GPTAnnotator provides a more reliable evaluation metric for generative multilingual QA than traditional word-level F1 scores. A second LLM evaluates the correctness of answers generated by the first LLM, assigning scores based on semantic meaning rather than exact token overlap. Human annotators judge answer correctness based on semantic equivalence, not strict token matching, and this aligns better with real-world usage.

## Foundational Learning

- **Multi-armed bandits (MAB) for strategy selection**: Needed to dynamically choose among prompt strategies and embedding models without exhaustive manual tuning. Quick check: How does the exploration-exploitation tradeoff work in the context of selecting multilingual LLM strategies?
- **Contextual bandits for incorporating query features**: Needed to leverage information about the query, language, and dataset to make more informed strategy choices. Quick check: What features would you extract from a multilingual QA query to improve contextual bandit decisions?
- **Retrieval-augmented generation (RAG) with multilingual embeddings**: Needed to provide context beyond the LLM's training data for non-Latin and low-resource languages. Quick check: How do multilingual embeddings differ from GPT embeddings in representing low-resource languages?

## Architecture Onboarding

- **Component map**: Query input → Context extraction → Multilingual embedding model (MuRIL/XLM-R) → RAG retrieval → LLM (GPT-3.5 Turbo) with prompt strategy → Output generation → GPTAnnotator evaluation → Learning agent (contextual bandit) selects strategy and embedding per query based on feedback
- **Critical path**: Input query → Feature extraction → Bandit strategy selection → Embedding model selection → RAG retrieval → LLM call with chosen prompt → Output
- **Design tradeoffs**: More LLM calls for aggregate strategies vs. potentially better performance; Multilingual embeddings vs. GPT embeddings: improved retrieval but higher latency; Exploration in bandit learning vs. immediate exploitation of known good strategies
- **Failure signatures**: Performance plateaus or degrades if bandit exploration is insufficient; Retrieval fails if embedding space poorly represents the language; Evaluation misalignment if GPTAnnotator prompt is ambiguous or biased
- **First 3 experiments**: 1) Compare MLQA-F1 and GPTAnnotator scores on a small multilingual QA sample to validate the new metric; 2) Test RAG with default GPT embeddings vs. multilingual embeddings on a few languages to measure retrieval improvement; 3) Run contextual bandit with only MLQA-F1 feedback vs. only GPTAnnotator feedback to see which drives better strategy selection

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the contextual bandit approach compare when using human feedback (HA) versus GPTAnnotator scores as rewards? The paper presents results for both CB-1 (MLQA-F1 as reward) and CB-2 (GPTAnnotator score as reward) but does not directly compare these approaches using HA scores.

### Open Question 2
How do the proposed prompt strategies and learning algorithms generalize to other NLP tasks beyond question answering? The paper focuses on QA tasks but mentions that the learning algorithms are generic and can be extended to accommodate additional dimensions.

### Open Question 3
What is the impact of varying the temperature setting in the GPT models on the performance of the proposed approach? The paper mentions that the learning algorithms can be extended to accommodate additional dimensions, such as temperature setting, but does not explore this effect.

### Open Question 4
How does the proposed hybrid approach with multilingual embeddings perform on languages outside the Indic and global language families covered in the evaluation? The paper evaluates on IndicQA and TyDiQA but does not discuss performance on languages from other language families.

## Limitations

- The contextual bandit's effectiveness depends heavily on the quality of extracted features, which may not capture all nuances affecting strategy performance
- The RAG approach assumes the retrieval corpus adequately covers target languages, which may not hold for extremely low-resource languages
- GPTAnnotator evaluation introduces potential biases through the evaluation LLM's inherent limitations and may not fully capture human judgment across all domains

## Confidence

- **High Confidence**: The general framework of combining dynamic strategy selection with multilingual RAG shows measurable improvements on tested datasets
- **Medium Confidence**: The 4x improvement over fine-tuned language-specific models is impressive but may not generalize beyond the specific QA task
- **Low Confidence**: The GPTAnnotator metric's robustness across different question types and domains hasn't been thoroughly established

## Next Checks

1. **Ablation study on feature importance**: Systematically remove or modify context features in the bandit to determine which features actually drive performance improvements versus adding noise

2. **Cross-task generalization test**: Apply the same dynamic strategy selection approach to a different multilingual task (e.g., summarization or translation) to verify that improvements generalize beyond QA

3. **GPTAnnotator bias analysis**: Test the GPTAnnotator evaluation on a dataset with known challenging cases (e.g., questions requiring reasoning versus fact recall) to identify potential systematic biases in the evaluation metric