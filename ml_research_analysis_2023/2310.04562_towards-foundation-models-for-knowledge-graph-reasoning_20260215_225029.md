---
ver: rpa2
title: Towards Foundation Models for Knowledge Graph Reasoning
arxiv_id: '2310.04562'
source_url: https://arxiv.org/abs/2310.04562
tags:
- graph
- graphs
- relation
- inductive
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ULTRA, a foundation model for knowledge graph
  (KG) reasoning that enables zero-shot transfer to unseen KGs with different entity
  and relation vocabularies. The key idea is to leverage the invariance of relational
  structure and learn conditional relation representations conditioned on query relations.
---

# Towards Foundation Models for Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2310.04562
- Source URL: https://arxiv.org/abs/2310.04562
- Reference count: 23
- A single pre-trained ULTRA model outperforms state-of-the-art baselines trained specifically on each of 57 target KGs (sizes 1k-120k nodes), achieving an average MRR of 0.395 in zero-shot inference compared to 0.344 for supervised models.

## Executive Summary
This paper introduces ULTRA, a foundation model for knowledge graph reasoning that enables zero-shot transfer to unseen KGs with different entity and relation vocabularies. The key innovation is leveraging the invariance of relational structure by learning conditional relation representations conditioned on query relations. ULTRA builds a graph of relations capturing four fundamental interaction types (tail-to-head, head-to-head, head-to-tail, tail-to-tail), applies a graph neural network to obtain unique relative representations of each relation, and uses these as input features for any inductive link predictor. Experimental results demonstrate that ULTRA achieves superior zero-shot performance compared to models trained specifically on each target KG, with further improvements possible through fine-tuning.

## Method Summary
ULTRA is a foundation model for knowledge graph reasoning that learns transferable representations through conditional relation modeling. The method constructs a graph of relations capturing four fundamental interaction types, applies a graph neural network with labeling tricks to obtain conditional relation representations, and uses these representations as input features for inductive link predictors. The model is pre-trained on a mixture of standard KGs and evaluated on 57 different target KGs through zero-shot transfer, achieving better performance than supervised baselines without fine-tuning. The architecture consists of 6-layer GNNs with 64-dimensional hidden states and DistMult message functions, totaling 177k parameters.

## Key Results
- Zero-shot transfer: ULTRA achieves 0.395 average MRR across 57 target KGs, outperforming supervised baselines (0.344 MRR)
- Fine-tuning effectiveness: 10% average improvement on top of zero-shot performance with 1000-2000 fine-tuning batches
- Model efficiency: 177k parameters sufficient for strong performance across diverse KG sizes (1k-120k nodes)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relational structure invariance enables zero-shot transfer across graphs with different vocabularies.
- Mechanism: ULTRA builds a graph of relations capturing four fundamental interaction types (tail-to-head, head-to-head, head-to-tail, tail-to-tail). These interactions are independent of specific relation vocabularies, allowing learned representations to generalize.
- Core assumption: The four fundamental relation interactions are sufficient to capture all necessary structural information for transfer learning.
- Evidence anchors:
  - [abstract] "The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies."
  - [section 4.1] "We distinguish four such core, fundamental relation-to-relation interactions: tail-to-head (t2h), head-to-head (h2h), head-to-tail (h2t), and tail-to-tail (t2t)."
  - [corpus] Weak - no direct corpus evidence for sufficiency of four interactions.
- Break condition: If graphs contain interaction types beyond the four fundamental ones, transfer performance degrades.

### Mechanism 2
- Claim: Conditional relation representations conditioned on query relations enable inductive generalization.
- Mechanism: Using labeling tricks with INDICATOR functions, ULTRA initializes nodes with query relation vectors, making relation representations conditional on the query. This allows the model to handle unseen relations.
- Core assumption: Conditioning on query relations is sufficient to break symmetries and enable generalization.
- Evidence anchors:
  - [section 4.2] "Following Huang et al. (2023), we found that all-ones labeling with 1d generalizes better to unseen graphs of various sizes than a learnable vector."
  - [section 4.3] "Given a query (h, q, ?) over a graph G and conditional relation representations Rq from the previous step, it is now possible to adapt any off-the-shelf inductive link predictor."
  - [corpus] Weak - limited evidence on labeling tricks for KG reasoning specifically.
- Break condition: If query relations cannot adequately condition representations, transfer fails.

### Mechanism 3
- Claim: Graph neural networks over relation graphs capture transferable structural patterns.
- Mechanism: A GNN is applied to the relation graph to obtain unique relative representations of each relation, which can then be used by any inductive link predictor.
- Core assumption: Message passing on relation graphs preserves transferable structural information.
- Evidence anchors:
  - [section 4.2] "Applying a graph neural network (GNN) with a labeling trick (Zhang et al., 2021) over the graph of relations, ULTRA obtains a unique relative representation of each relation."
  - [section 5.3] "We therefore posit that conditional representations (both on relation and entity levels) are crucial for transferable representations for link prediction tasks that often require pairwise representations to break neighborhood symmetries."
  - [corpus] Weak - limited evidence for GNNs specifically for relation graphs.
- Break condition: If GNN architecture cannot capture necessary structural patterns, transfer fails.

## Foundational Learning

- Concept: Inductive learning
  - Why needed here: The model must generalize to graphs with new entities and relations not seen during training
  - Quick check question: What distinguishes inductive from transductive learning in KG reasoning?

- Concept: Transfer learning
  - Why needed here: Enables zero-shot transfer to unseen KGs with different vocabularies
  - Quick check question: How does transfer learning differ from traditional supervised learning on each target dataset?

- Concept: Graph neural networks
  - Why needed here: Needed to process graph-structured data and capture relational patterns
  - Quick check question: What role do GNNs play in processing the graph of relations?

## Architecture Onboarding

- Component map: Graph construction → Relation representation → Entity prediction → Loss computation
- Critical path: Graph construction → Relation representation → Entity prediction → Loss computation
- Design tradeoffs:
  - Small model size (177k parameters) vs. expressiveness
  - Four fundamental interactions vs. richer relation modeling
  - Conditional vs. unconditional representations
- Failure signatures:
  - Poor performance on larger transductive graphs
  - Degradation when new relation types have complex interactions
  - Limited improvement from fine-tuning on some datasets
- First 3 experiments:
  1. Verify graph of relations construction with simple test graph
  2. Test conditional relation representation learning on pre-training mix
  3. Evaluate zero-shot transfer to one small inductive dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ULTRA's performance scale with model capacity and size of the pre-training dataset?
- Basis in paper: [inferred] The authors note that increasing the diversity of training data may require increasing model size and that their preliminary experiments did not show significant improvements beyond 200k parameters. They also suggest that getting higher inference performance may be tied to model capacity, scale, and optimization.
- Why unresolved: The authors did not conduct experiments to systematically evaluate the impact of model capacity and pre-training dataset size on ULTRA's performance.
- What evidence would resolve it: Experiments varying the number of parameters in ULTRA and the size and diversity of the pre-training dataset, measuring zero-shot and fine-tuned performance on a range of target datasets.

### Open Question 2
- Question: How does ULTRA compare to other foundation models for KG reasoning, such as those based on language models?
- Basis in paper: [explicit] The authors note that text-based methods like BLP, KEPLER, StATIK, and RAILD are orthogonal to ULTRA and that their zero-shot inductive transfer to an arbitrary KG implies running inference on graphs from different domains that might need different language encoders.
- Why unresolved: The authors did not directly compare ULTRA to these text-based foundation models.
- What evidence would resolve it: Experiments comparing ULTRA's performance to text-based foundation models on the same set of target datasets, including those with and without text descriptions of entities and relations.

### Open Question 3
- Question: What is the impact of the four fundamental relation interactions (t2h, h2h, h2t, t2t) on ULTRA's performance?
- Basis in paper: [explicit] The authors note that these four interaction types are captured by the graph of relations and that their representations can be learned. They also mention that other strategies for capturing relation-to-relation interactions might exist and leave their exploration for future work.
- Why unresolved: The authors did not conduct experiments to ablate the four fundamental relation interactions and measure the impact on ULTRA's performance.
- What evidence would resolve it: Experiments training ULTRA with and without each of the four fundamental relation interactions, measuring zero-shot and fine-tuned performance on a range of target datasets.

## Limitations
- Reliance on four fundamental interaction types may not capture all structural patterns in complex KGs
- Limited validation on diverse KG types beyond the 57 test datasets
- No evaluation of performance on temporal or noisy KGs
- Scalability to significantly larger graphs than tested (120k nodes maximum) remains unknown

## Confidence

**High Confidence**: Zero-shot transfer performance claims (0.395 vs 0.344 MRR average), pre-training methodology (200k steps on 3 standard KGs), and the core architectural components (graph of relations with 4 interaction types, GNN-based conditional representations).

**Medium Confidence**: The sufficiency of four fundamental interaction types for all KG structures, the generalizability of results to KGs with different characteristics than the 57 test datasets, and the specific implementation details of the labeling trick mechanism.

**Low Confidence**: Performance on temporal KGs, handling of noisy or incomplete KGs, and scalability to graphs significantly larger than those tested.

## Next Checks

1. **Architecture validation**: Construct the graph of relations for a simple test KG (e.g., a 10-node graph with 3 relation types) and verify that all four interaction types are correctly computed using sparse matrix multiplications.

2. **Zero-shot transfer baseline**: Evaluate ULTRA on one additional KG not in the 57-test set (e.g., WN18) to verify that the zero-shot performance pattern holds for truly unseen datasets.

3. **Interaction type ablation**: Test ULTRA with only 2-3 of the 4 fundamental interaction types to empirically validate whether all four are necessary for optimal transfer performance.