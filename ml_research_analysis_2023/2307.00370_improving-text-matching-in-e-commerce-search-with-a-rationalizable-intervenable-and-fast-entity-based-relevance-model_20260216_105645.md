---
ver: rpa2
title: Improving Text Matching in E-Commerce Search with A Rationalizable, Intervenable
  and Fast Entity-Based Relevance Model
arxiv_id: '2307.00370'
source_url: https://arxiv.org/abs/2307.00370
tags:
- relevance
- query
- item
- search
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently and accurately
  predicting relevance between user queries and items in e-commerce search systems.
  It proposes the Entity-Based Relevance Model (EBRM), which decomposes the query-item
  (QI) relevance problem into multiple query-entity (QE) relevance problems using
  entity extraction from item titles.
---

# Improving Text Matching in E-Commerce Search with A Rationalizable, Intervenable and Fast Entity-Based Relevance Model

## Quick Facts
- arXiv ID: 2307.00370
- Source URL: https://arxiv.org/abs/2307.00370
- Authors: 
- Reference count: 23
- Key outcome: EBRM improves accuracy and F1 score over Bi-encoder and Cross-encoder baselines while offering faster inference and interpretability through entity-based decomposition and soft logic aggregation.

## Executive Summary
This paper addresses the challenge of efficiently predicting relevance between user queries and items in e-commerce search systems. The proposed Entity-Based Relevance Model (EBRM) decomposes the query-item relevance problem into multiple query-entity problems using product type entities extracted from item titles. By combining a Cross-encoder for accurate entity scoring with soft logic aggregation and caching, EBRM achieves both high accuracy and fast online inference while enabling interpretability and intervention capabilities.

## Method Summary
EBRM extracts product type entities from item titles and decomposes the query-item relevance prediction into multiple query-entity relevance problems. It uses a Cross-encoder to score each query-entity pair, aggregates results using soft logic (max operation), and caches QE relevance scores for fast online inference. The model is pretrained on pseudo-labeled QE data derived from user click logs, providing cleaner training signals than direct query-item labels. This approach balances the accuracy-speed tradeoff between traditional Bi-encoders and Cross-encoders while maintaining interpretability through entity-level explanations.

## Key Results
- EBRM achieves higher accuracy and F1 scores compared to both Bi-encoder and Cross-encoder baselines on e-commerce datasets
- Inference speed is significantly faster than Cross-encoders due to caching of QE relevance scores
- The model demonstrates memory efficiency through its caching mechanism while maintaining competitive performance
- Pretraining with QE pseudo-labels from search logs improves model discriminative ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-based decomposition reduces computational complexity while maintaining accuracy
- Mechanism: The EBRM decomposes the QI relevance problem into multiple QE problems, allowing pre-computation and caching of QE relevance scores
- Core assumption: Product type entities are the primary drivers of relevance between queries and items in e-commerce
- Evidence anchors:
  - [abstract] "We identify the entities contained in an item and decompose the QI (query-item) relevance problem into multiple QE (query-entity) relevance problems"
  - [section 3.1] "We follow the insight that a query and an item are relevant if and only if there exist some product type entities of the item that are relevant to the query"
  - [corpus] Weak - no direct citation evidence for this specific decomposition claim
- Break condition: If product type entities are not sufficient to determine relevance (e.g., if attribute mismatch becomes important)

### Mechanism 2
- Claim: Soft logic aggregation enables interpretability and intervention capability
- Mechanism: The model uses max operation from Zadeh soft logic to aggregate QE scores into QI predictions, allowing entity-level explanations
- Core assumption: Users and system operators can understand and act on entity-level relevance explanations
- Evidence anchors:
  - [abstract] "Utilizing soft logic makes the prediction procedure interpretable and intervenable"
  - [section 3.1] "If the predicted result of a query and an item is relevant, the cached entities for the query and item should have at least one overlapping entity"
  - [corpus] Weak - no direct citation evidence for this specific soft logic implementation
- Break condition: If users cannot understand entity-level explanations or if intervention overhead outweighs benefits

### Mechanism 3
- Claim: Pretraining with pseudo-labeled QE data from search logs improves model performance
- Mechanism: The model is pretrained on large-scale pseudo-labeled QE pairs derived from user click data, providing cleaner training signals than direct QI labels
- Core assumption: Click data on entities provides better relevance signals than click data on entire items
- Evidence anchors:
  - [section 3.3] "Compared with QI relevance data, the QE training data is more useful to our QE module because the QE data can be seen as a distillation of the large-scale QI data"
  - [section 4.2.5] "We also enhance it with QE pretraining and initialization with Cross-encoder"
  - [corpus] Weak - no direct citation evidence for this specific pretraining approach
- Break condition: If click data is too sparse or noisy for certain entity types

## Foundational Learning

- Concept: Cross-encoder vs Bi-encoder architectures
  - Why needed here: Understanding the tradeoff between accuracy (Cross-encoder) and inference speed (Bi-encoder) that EBRM aims to balance
  - Quick check question: What is the key architectural difference between Cross-encoders and Bi-encoders in how they process query-item pairs?

- Concept: Entity extraction and named entity recognition (NER)
  - Why needed here: EBRM relies on extracting product type entities from item titles to decompose the relevance problem
  - Quick check question: How does EBRM use the results of entity extraction in its relevance prediction process?

- Concept: Soft logic and fuzzy set theory
  - Why needed here: The aggregation layer uses Zadeh soft logic (max operation) to combine QE scores into QI predictions
  - Quick check question: Why does EBRM use max operation from soft logic instead of traditional logical OR for aggregating QE relevance scores?

## Architecture Onboarding

- Component map:
  Entity Extraction Module -> QE Relevance Module -> Soft Logic Aggregation Layer -> Caching System -> Pretraining Module

- Critical path:
  1. Extract entities from item title
  2. Retrieve cached QE relevance scores for query-entity pairs
  3. Apply soft logic aggregation (max operation)
  4. Return QI relevance prediction

- Design tradeoffs:
  - Accuracy vs. Speed: Cross-encoder for QE scoring vs. caching for fast inference
  - Coverage vs. Precision: Using product type entities only vs. including other entity types
  - Pretraining data quality vs. quantity: QE pretraining from logs vs. limited labeled QI data

- Failure signatures:
  - Poor accuracy: Check entity extraction quality, QE module training, and soft logic implementation
  - Slow inference: Verify caching system is working, check for cache misses
  - Low intervention effectiveness: Validate entity relevance labels and intervention interface

- First 3 experiments:
  1. Ablation study: Remove entity extraction and test QI relevance prediction performance
  2. Caching efficiency: Measure cache hit rate and inference speed improvement
  3. Pretraining impact: Compare EBRM performance with and without QE pretraining from search logs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Entity-Based Relevance Model (EBRM) perform in scenarios with extremely high-dimensional sparse data compared to other models?
- Basis in paper: [inferred] The paper mentions that EBRM is effective in high-dimensional sparse data scenarios, as indicated by the statement, "To enable reproducible research, we also utilize a recently released open-source e-commerce dataset WANDS (Chen et al., 2022)."
- Why unresolved: The paper does not provide explicit performance metrics for EBRM in high-dimensional sparse data scenarios.
- What evidence would resolve it: Empirical results comparing EBRM's performance in high-dimensional sparse data scenarios against other models, such as Bi-encoders and Cross-encoders, would provide insights into its effectiveness.

### Open Question 2
- Question: What is the impact of the size of the pseudo-labeled QE dataset on the performance of EBRM?
- Basis in paper: [explicit] The paper discusses the use of pseudo-labeled QE data for pretraining, stating, "We also propose to collect large-scale pseudo-labeled QI and QE data for pretraining."
- Why unresolved: The paper does not explore the effects of varying the size of the pseudo-labeled QE dataset on EBRM's performance.
- What evidence would resolve it: Experiments varying the size of the pseudo-labeled QE dataset and measuring the corresponding performance of EBRM would clarify the impact of dataset size on model performance.

### Open Question 3
- Question: How does the EBRM handle product type relevance prediction in scenarios where product types are not clearly defined or are ambiguous?
- Basis in paper: [inferred] The paper focuses on product type relevance prediction, but it does not address scenarios with ambiguous product types, as indicated by the statement, "For e-commerce search, the product type is the most important entity type as all queries must have intended products to search for and all items must have intended products to sell."
- Why unresolved: The paper does not discuss the handling of ambiguous product types or provide strategies for addressing this issue.
- What evidence would resolve it: Case studies or experiments demonstrating EBRM's performance in scenarios with ambiguous product types would provide insights into its handling of such cases.

## Limitations
- Entity extraction dependency: The entire approach relies on accurate product type entity extraction from item titles, which is not specified in the paper
- Domain-specific assumptions: Assumes product type entities are primary relevance drivers, which may not generalize across all e-commerce categories
- Soft logic interpretability: Claims interpretability and intervention capability without empirical validation of user understanding or effectiveness

## Confidence
**High Confidence:**
- Accuracy and F1 score improvements over Bi-encoder baselines
- Speed advantage over Cross-encoder baselines
- Memory efficiency benefits from caching

**Medium Confidence:**
- QE pretraining effectiveness
- Intervention capability claims
- Generalizability to different e-commerce domains

**Low Confidence:**
- Specific choice of product type entities as sole entity category
- Long-term stability of cached QE relevance rules
- Practical interpretability of soft logic explanations for end users

## Next Checks
1. **Entity Extraction Robustness Test**: Evaluate the NER tool's precision and recall on diverse item titles across product categories. Measure how entity extraction errors propagate through the EBRM pipeline and affect final relevance predictions.

2. **Cross-Domain Generalization**: Implement EBRM on a second e-commerce dataset with different product characteristics. Compare performance degradation and identify which components are most sensitive to domain shifts.

3. **User Intervention Effectiveness**: Design a user study where search operators use entity-level explanations to improve search relevance. Measure whether interventions based on EBRM explanations actually improve search metrics compared to baseline methods.