---
ver: rpa2
title: 'A Paradigm Shift in Machine Translation: Boosting Translation Performance
  of Large Language Models'
arxiv_id: '2309.11674'
source_url: https://arxiv.org/abs/2309.11674
tags:
- data
- translation
- bleu
- comet
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle with translation tasks, especially
  for moderate-sized models (7B or 13B parameters), which lag behind traditional supervised
  translation models. Previous efforts to improve translation in these models have
  seen limited gains.
---

# A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models

## Quick Facts
- **arXiv ID**: 2309.11674
- **Source URL**: https://arxiv.org/abs/2309.11674
- **Reference count**: 36
- **Primary result**: Two-stage fine-tuning achieves >12 BLEU and >12 COMET improvements across 10 translation directions using only 7B or 13B parameter models

## Executive Summary
This paper addresses the persistent challenge of machine translation performance in moderate-sized large language models (7B-13B parameters), which typically underperform compared to traditional supervised translation models. The authors propose a novel two-stage fine-tuning approach that eliminates the need for large parallel datasets while achieving state-of-the-art results. By first enhancing multilingual proficiency through monolingual data fine-tuning followed by high-quality parallel data fine-tuning, the method called ALMA achieves over 12 BLEU and 12 COMET improvements across 10 translation directions. Notably, the approach requires only 18 hours of computation when fine-tuning with 1B monolingual tokens, establishing a new paradigm for machine translation training.

## Method Summary
The approach uses LLaMA-2 as the base model and employs a two-stage fine-tuning process. Stage 1 involves fine-tuning on monolingual data from target languages (German, Czech, Icelandic, Chinese, Russian, and English) sourced from the OSCAR corpus with specific sampling ratios to balance learning emphasis. Stage 2 applies fine-tuning on a small set of high-quality human-written parallel data including WMT'17-WMT'20 test sets and Flores-200 dev/test sets. The method uses full-weight fine-tuning for both stages with batch size 256, sequence length 512, learning rate 2e-5, and inverse square decay for 2 epochs in stage 2. This approach effectively leverages the model's existing multilingual knowledge while avoiding catastrophic forgetting through controlled parallel data exposure.

## Key Results
- ALMA achieves over 12 BLEU and 12 COMET improvements across 10 translation directions compared to zero-shot baselines
- Outperforms NLLB-54B and GPT-3.5 models despite using only 7B or 13B parameters
- Fine-tuning with just 1B monolingual tokens achieves competitive performance with only 18 hours of computation
- Demonstrates that excessive parallel data causes catastrophic forgetting, with performance peaking at smaller dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs possess inherent multilingual knowledge but underperform due to English-dominant pre-training
- **Mechanism**: Monolingual fine-tuning redistributes language knowledge across target languages without requiring parallel data
- **Core assumption**: English-dominant pre-training creates language imbalance that monolingual fine-tuning can correct
- **Evidence anchors**: Abstract, section 4, corpus (weak)
- **Break condition**: Low-quality monolingual data or insufficient model capacity may limit effectiveness

### Mechanism 2
- **Claim**: Excessive parallel data causes catastrophic forgetting of pre-existing knowledge
- **Mechanism**: Limited high-quality parallel data preserves and builds upon existing knowledge rather than replacing it
- **Core assumption**: LLMs retain valuable pre-training knowledge that gets overwritten with too much new data
- **Evidence anchors**: Section 3.2, abstract, corpus (moderate)
- **Break condition**: Extremely high-quality parallel data or underutilized model capacity may benefit from larger datasets

### Mechanism 3
- **Claim**: Combined monolingual and parallel data fine-tuning synergistically improves translation
- **Mechanism**: Monolingual data enhances comprehension while parallel data provides explicit translation guidance
- **Core assumption**: Language comprehension and translation generation are complementary skills
- **Evidence anchors**: Abstract, section 6.2, corpus (strong)
- **Break condition**: Insufficient quality or capacity to integrate both data types effectively

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: Explains why excessive parallel data fine-tuning degrades performance
  - Quick check question: What happens to a neural network's performance on previously learned tasks when fine-tuned extensively on new data?

- **Concept**: Cross-lingual representation learning
  - Why needed here: Underpins effectiveness of monolingual fine-tuning for multilingual enhancement
  - Quick check question: How do models trained on one language perform on tasks in another language, and what techniques improve cross-lingual transfer?

- **Concept**: Data quality vs. quantity trade-off
  - Why needed here: Highlights importance of high-quality parallel data over large volumes of lower-quality data
  - Quick check question: When might a smaller dataset of higher quality outperform a larger dataset of lower quality in machine learning?

## Architecture Onboarding

- **Component map**: LLaMA-2-7B/13B → Monolingual fine-tuning stage → High-quality parallel fine-tuning stage → BLEU/COMET evaluation

- **Critical path**: 
  1. Preprocess monolingual data with balanced sampling across languages
  2. Fine-tune backbone model on monolingual data
  3. Fine-tune on high-quality parallel data
  4. Evaluate translation performance on test datasets

- **Design tradeoffs**: 
  - Full-weight vs. LoRA fine-tuning: Better performance vs. computational efficiency
  - Monolingual data volume: More data improves performance but increases computational cost

- **Failure signatures**: 
  - Performance degradation with excessive parallel data
  - Off-target translations indicating prompt-model misalignment
  - Plateau in performance despite increased monolingual data volume

- **First 3 experiments**:
  1. Fine-tune LLaMA-2-7B on monolingual data only; evaluate cross-lingual proficiency
  2. Fine-tune LLaMA-2-7B on high-quality parallel data only; compare to zero-shot baseline
  3. Combine monolingual and parallel data fine-tuning; assess performance improvements

## Open Questions the Paper Calls Out

- **Question 1**: What is the precise relationship between monolingual data fine-tuning amount and performance gains across different language families?
  - Basis: Paper tests 6 languages without family grouping analysis
  - Evidence needed: Diverse language set grouped by family with quantitative improvement analysis

- **Question 2**: How does two-stage fine-tuning compare to interleaved single-stage training with mixed data?
  - Basis: Paper contrasts sequential approach with prior large parallel dataset methods
  - Evidence needed: Experiments comparing two-stage vs. interleaved training with same total data and budget

- **Question 3**: What is the impact of varying monolingual data sampling ratios on translation quality and efficiency?
  - Basis: Paper uses 1/6 English ratio without exploring alternatives
  - Evidence needed: Controlled experiments varying English sampling ratio while keeping total data constant

## Limitations

- Evaluation limited to LLaMA-2 base model with minimal testing on alternative architectures
- Monolingual data selection process lacks transparency in quality filtering criteria
- "High-quality parallel data" definition is vague beyond human-written designation
- Total computational cost analysis incomplete, focusing only on monolingual fine-tuning stage

## Confidence

- **High Confidence**: Two-stage fine-tuning outperforms zero-shot translation; ablation studies are methodologically sound
- **Medium Confidence**: Catastrophic forgetting mechanisms and superiority of smaller parallel datasets supported by ablation but rely on post-hoc interpretation
- **Low Confidence**: Claims about computational efficiency and generalizability to other LLMs beyond LLaMA-2 lack empirical validation

## Next Checks

1. **Cross-model validation**: Replicate two-stage fine-tuning using multiple base models (Mistral, Llama-3, XGLM variants) to test generalizability beyond LLaMA-2 and identify architectural features influencing success.

2. **Data quality sensitivity analysis**: Systematically vary quality of monolingual and parallel data used in each stage, testing performance using filtered vs. unfiltered OSCAR data and examining how different parallel dataset sizes affect the trade-off between performance and catastrophic forgetting.

3. **Catastrophic forgetting mechanism validation**: Design experiments to directly test the catastrophic forgetting hypothesis by measuring performance degradation on held-out parallel data during monolingual fine-tuning and examining activation patterns in attention layers before and after each fine-tuning stage using probing techniques.