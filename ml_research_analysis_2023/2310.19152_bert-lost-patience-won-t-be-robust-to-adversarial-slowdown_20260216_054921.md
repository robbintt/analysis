---
ver: rpa2
title: BERT Lost Patience Won't Be Robust to Adversarial Slowdown
arxiv_id: '2310.19152'
source_url: https://arxiv.org/abs/2310.19152
tags:
- adversarial
- slowdown
- attacks
- language
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WAFFLE, an adversarial attack that generates
  natural text to bypass early-exit points in multi-exit language models, thereby
  increasing computational cost. WAFFLE integrates a slowdown objective into existing
  adversarial text attack algorithms, pushing internal classifier outputs toward uniform
  distributions.
---

# BERT Lost Patience Won't Be Robust to Adversarial Slowdown

## Quick Facts
- arXiv ID: 2310.19152
- Source URL: https://arxiv.org/abs/2310.19152
- Reference count: 40
- Key outcome: WAFFLE adversarial attack significantly reduces computational savings of multi-exit language models by bypassing early-exit points through uniform distribution manipulation

## Executive Summary
This paper introduces WAFFLE, an adversarial attack that generates natural text to bypass early-exit points in multi-exit language models, thereby increasing computational cost. WAFFLE integrates a slowdown objective into existing adversarial text attack algorithms, pushing internal classifier outputs toward uniform distributions. Evaluations on GLUE benchmark tasks show that WAFFLE significantly reduces computational savings across three multi-exit mechanisms (DeeBERT, PABEE, PastFuture) in both white-box and black-box settings. Linguistic analysis reveals that subject-predicate disagreement and named entity changes are key characteristics of successful attacks. Adversarial training is ineffective as a defense, but input sanitization using conversational models like ChatGPT can mitigate the slowdown.

## Method Summary
The paper proposes WAFFLE, an adversarial attack that integrates a slowdown objective into existing adversarial text attack algorithms (TextFooler and A2T). The slowdown objective pushes multi-exit model predictions at early-exits toward uniform distributions by maximizing a score function based on L1 distance from uniform distribution. The attack is evaluated on three multi-exit mechanisms (DeeBERT, PABEE, PastFuture) across GLUE benchmark tasks. The authors measure both classification accuracy and efficacy (computational savings), test transferability of attacks, analyze linguistic patterns in successful attacks, and evaluate two defense strategies (adversarial training and input sanitization).

## Key Results
- WAFFLE significantly reduces computational savings (efficacy) across all three multi-exit mechanisms, with PastFuture being most vulnerable
- Linguistic analysis shows subject-predicate disagreement and named entity changes are key characteristics of successful attacks
- Adversarial training is ineffective as a defense, while input sanitization using conversational models like ChatGPT shows promise but is computationally expensive
- The more complex the multi-exit mechanism, the more vulnerable it is to adversarial slowdown

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WAFFLE generates adversarial text that bypasses early-exit points by pushing internal classifier outputs toward uniform distributions.
- Mechanism: The attack integrates a slowdown objective into existing adversarial text algorithms, replacing the standard misclassification goal with uniform probability distribution across classes.
- Core assumption: Early-exit mechanisms rely on prediction confidence exceeding thresholds; uniform distributions minimize confidence at all exits.
- Evidence anchors: [abstract] "We design a novel objective function that pushes a multi-exit model's predictions at its early-exits toward the uniform distribution"; [section 3.2] "We formulate our score function s(x′, fθ) as follows: s(x′, fθ) = Σ₀<ᵢ≤ᴷ (1 − 1/(N−1) · ℓ₁(Fᵢ(x′), ŷ))"

### Mechanism 2
- Claim: The more complex a multi-exit mechanism is, the more vulnerable it becomes to adversarial slowdown.
- Mechanism: Complex mechanisms like PastFuture that consider past and future predictions create more decision points that can be exploited through input perturbations.
- Core assumption: Additional complexity in decision-making introduces more opportunities for adversarial manipulation.
- Evidence anchors: [abstract] "The more complex a mechanism is, the more vulnerable it is to adversarial slowdown"; [section 4.1] "WAFFLE causes the most significant slowdown on PastFuture, followed by PABEE and DeeBERT"

### Mechanism 3
- Claim: Subject-predicate disagreement and named entity changes are key characteristics that make adversarial texts effective at bypassing early exits.
- Mechanism: These linguistic patterns push inputs outside the distribution the model was trained on, reducing prediction confidence across all internal classifiers.
- Core assumption: BERT and similar models have strong subject-predicate agreement understanding and named entity awareness that can be exploited.
- Evidence anchors: [section 6] "We find two critical characteristics present in a vast majority of successful samples: (1) subject-predicate disagreement... and (2) the changing of named entities"; [abstract] "Linguistic analysis reveals that subject-predicate disagreement and named entity changes are key characteristics of successful attacks"

## Foundational Learning

- Concept: Adversarial examples in NLP
  - Why needed here: Understanding how small perturbations in discrete text spaces can manipulate model predictions is fundamental to grasping why WAFFLE works
  - Quick check question: Why are adversarial text attacks more challenging than image attacks?

- Concept: Multi-exit neural network architectures
  - Why needed here: The attack specifically targets early-exit mechanisms, so understanding how these architectures save computation through confidence-based early stopping is crucial
  - Quick check question: How do early-exit mechanisms determine when to stop forward propagation?

- Concept: Gradient-based vs gradient-free attack methods
  - Why needed here: WAFFLE adapts both types of existing attacks, so understanding their differences and when each is applicable matters for implementation
  - Quick check question: What are the main challenges of applying gradient-based attacks to discrete text data?

## Architecture Onboarding

- Component map: Text input → Early-exit mechanism → Internal classifier predictions → WAFFLE perturbation → Bypassed exits → Final layer classification
- Critical path: Text input → Early-exit mechanism → Internal classifier predictions → WAFFLE perturbation → Bypassed exits → Final layer classification
- Design tradeoffs: Computational efficiency vs robustness - more aggressive early exits provide better efficiency but create more vulnerability points for adversarial slowdown attacks
- Failure signatures: 
  - Significant reduction in efficacy metric (0.34→0.11 in DeeBERT example)
  - Minimal change in accuracy despite large efficacy reduction
  - Subject-predicate disagreement and named entity changes in perturbed text
- First 3 experiments:
  1. Run WAFFLE attack on clean test set using DeeBERT mechanism to verify basic slowdown functionality
  2. Test transferability by crafting attacks on one mechanism and evaluating on different mechanisms
  3. Implement input sanitization defense and measure recovery of both accuracy and efficacy metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are adversarial slowdown attacks when using character-level or sentence-level perturbations instead of word-level perturbations?
- Basis in paper: [inferred] The authors mention that their attack is based on word-level perturbations and note that it could be extended to other perturbation types, but do not evaluate these alternatives.
- Why unresolved: The paper only evaluates word-level perturbations, leaving the effectiveness of other perturbation types unknown.
- What evidence would resolve it: Experimental results comparing the effectiveness of word-level, character-level, and sentence-level perturbations in causing slowdown across different multi-exit mechanisms.

### Open Question 2
- Question: Can input sanitization defenses that leverage conversational models like ChatGPT be made more computationally efficient while maintaining effectiveness?
- Basis in paper: [explicit] The authors find ChatGPT-based sanitization effective but computationally expensive, and suggest future work is needed to develop faster alternatives.
- Why unresolved: The paper only tests one sanitization approach (ChatGPT) and notes its computational cost without exploring optimizations or alternatives.
- What evidence would resolve it: Development and evaluation of faster input sanitization methods that maintain or improve upon ChatGPT's effectiveness in removing adversarial perturbations.

### Open Question 3
- Question: What specific attributes of conversational models make them effective at removing adversarial slowdown perturbations?
- Basis in paper: [explicit] The authors observe that conversational models like ChatGPT are effective at sanitization but note uncertainty about what specific capabilities enable this effectiveness.
- Why unresolved: The paper demonstrates effectiveness but does not analyze what aspects of conversational models (grammar correction, semantic understanding, etc.) drive their success.
- What evidence would resolve it: Comparative analysis of different sanitization approaches (grammar correction, semantic rewriting, paraphrasing) to identify which capabilities are most critical for removing slowdown-inducing perturbations.

## Limitations

- The evaluation scope is limited to BERT and ALBERT architectures, potentially not generalizing to other transformer-based models
- The defense evaluation is sparse, testing only two approaches (adversarial training and input sanitization)
- The linguistic analysis is correlational rather than causal, not establishing whether patterns are necessary or merely sufficient conditions

## Confidence

**High Confidence**: The core mechanism of WAFFLE - integrating a slowdown objective that pushes classifier outputs toward uniform distributions - is well-specified and technically sound. The claim that more complex multi-exit mechanisms are more vulnerable is supported by the empirical results showing PastFuture being most affected.

**Medium Confidence**: The transferability of attacks across different mechanisms is demonstrated but the underlying reasons for why certain attacks transfer better than others remain unclear. The linguistic analysis provides interesting insights but lacks statistical rigor in establishing causation.

**Low Confidence**: The effectiveness of adversarial training as a defense is dismissed based on minimal accuracy drops, but the evaluation doesn't account for potential adaptive attacks that could exploit the defended models' learned representations.

## Next Checks

1. **Cross-Architecture Validation**: Test WAFFLE against other transformer architectures (RoBERTa, DistilBERT, DeBERTa) to determine if the slowdown vulnerability is specific to BERT/ALBERT or represents a broader architectural weakness in multi-exit models.

2. **Defense Mechanism Expansion**: Implement and evaluate additional defense strategies including dynamic threshold adjustment, uncertainty-based input filtering, and ensemble methods to compare their effectiveness against WAFFLE across different attack intensities.

3. **Causal Linguistic Analysis**: Conduct controlled experiments where subject-predicate agreement and named entity changes are systematically introduced or removed to determine their causal relationship with attack success, using ablation studies and statistical significance testing.