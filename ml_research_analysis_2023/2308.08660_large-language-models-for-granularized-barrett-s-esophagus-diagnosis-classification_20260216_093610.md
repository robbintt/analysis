---
ver: rpa2
title: Large Language Models for Granularized Barrett's Esophagus Diagnosis Classification
arxiv_id: '2308.08660'
source_url: https://arxiv.org/abs/2308.08660
tags:
- reports
- report
- development
- dysplasia
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applied transformer-based large language models to\
  \ automate Barrett\u2019s esophagus (BE) diagnosis classification using pathology\
  \ reports. Models were trained on 619 reports annotated by gastroenterologists for\
  \ binary dysplasia and multi-class BE diagnosis tasks."
---

# Large Language Models for Granularized Barrett's Esophagus Diagnosis Classification

## Quick Facts
- arXiv ID: 2308.08660
- Source URL: https://arxiv.org/abs/2308.08660
- Reference count: 22
- Primary result: Transformer models achieved 0.964 F1-score for binary dysplasia and 0.911 F1-score for multi-class BE diagnosis classification

## Executive Summary
This study applied transformer-based large language models to automate Barrett's esophagus (BE) diagnosis classification using pathology reports. The models were trained on 619 reports annotated by gastroenterologists for binary dysplasia and multi-class BE diagnosis tasks. The best-performing models achieved high F1-scores comparable to rule-based methods while offering faster implementation and generalizability. Clinical-BigBird outperformed ClinicalBERT in three of four tasks, and models trained on full reports performed similarly to those trained on sub-sectioned reports, suggesting pre-processing may not be necessary.

## Method Summary
The study fine-tuned ClinicalBERT and Clinical-BigBird models on pathology reports from Columbia University Irving Medical Center (CUIMC) to classify BE-related diagnoses. Reports were annotated by gastroenterologists into six diagnosis classes. The models were trained using grid search over hyperparameters including learning rate, random seed, model type, and maximum input tokens. Performance was evaluated on held-out validation sets using metrics such as F1-score, accuracy, and AU-ROC for both binary dysplasia and multi-class classification tasks.

## Key Results
- Clinical-BigBird achieved 0.964 F1-score for binary dysplasia classification
- Clinical-BigBird achieved 0.911 F1-score for multi-class BE diagnosis classification
- Models trained on full reports performed similarly to those trained on sub-sectioned reports
- Clinical-BigBird outperformed ClinicalBERT in three of four tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clinical-BigBird outperforms ClinicalBERT in clinical NLP tasks due to its longer input capacity (4,096 tokens vs 512 tokens).
- Mechanism: Clinical-BigBird's longer input capacity allows it to process entire pathology reports without truncation, capturing more contextual information from the full text. This is particularly beneficial for clinical documents where relevant diagnostic information may be distributed throughout the report.
- Core assumption: The additional contextual information captured by Clinical-BigBird's longer input capacity translates to improved model performance in clinical NLP tasks.
- Evidence anchors:
  - [abstract] "Clinical-BigBird outperformed ClinicalBERT in three of four tasks, indicating potential advantages for clinical NLP applications."
  - [section] "Clinical-BigBird allowed the input of all text in this dataset for training, especially important as >20% of full-text reports had >512 tokens per report."
- Break condition: If the additional contextual information captured by Clinical-BigBird's longer input capacity does not translate to improved model performance in clinical NLP tasks.

### Mechanism 2
- Claim: The transformer-based approach achieves comparable performance to rule-based methods but with faster implementation.
- Mechanism: The transformer-based approach leverages pre-trained language models that have learned general language patterns, requiring less domain-specific rule engineering compared to rule-based methods. This allows for faster development and implementation while maintaining comparable performance.
- Core assumption: The pre-trained language models have learned sufficient general language patterns that can be fine-tuned for specific clinical tasks without extensive domain-specific rule engineering.
- Evidence anchors:
  - [abstract] "The method offers a generalizable, faster alternative to rule-based approaches for automating BE diagnosis classification."
  - [section] "Our method is generalizable and faster to implement as compared to a tailored rule-based approach."
- Break condition: If the pre-trained language models do not learn sufficient general language patterns that can be fine-tuned for specific clinical tasks without extensive domain-specific rule engineering.

### Mechanism 3
- Claim: The multi-class BE diagnosis classification task is more challenging than the binary dysplasia classification task due to class imbalance.
- Mechanism: The multi-class BE diagnosis classification task involves six diagnosis classes, four of which have less than 10% representation in the development set. This class imbalance makes it more difficult for the model to learn distinguishing features for the underrepresented classes, resulting in lower performance compared to the binary dysplasia classification task.
- Core assumption: The class imbalance in the multi-class BE diagnosis classification task negatively impacts the model's ability to learn distinguishing features for the underrepresented classes.
- Evidence anchors:
  - [section] "For this task, the dataset exhibited greater imbalance amongst classes as compared to the binary task, with 4 of 6 diagnosis categories having <10% representation in the development set."
  - [section] "Best-performing BERT-based models did not perform as well as the customized rule-based method, for both sub-section-trained and full-report-trained models."
- Break condition: If the class imbalance in the multi-class BE diagnosis classification task does not negatively impact the model's ability to learn distinguishing features for the underrepresented classes.

## Foundational Learning

- Concept: Natural Language Processing (NLP)
  - Why needed here: NLP is the foundation for processing and understanding unstructured text data, such as pathology reports, which is essential for the BE diagnosis classification task.
  - Quick check question: What is the primary goal of NLP in the context of BE diagnosis classification?

- Concept: Transformer-based models
  - Why needed here: Transformer-based models, such as BERT and Clinical-BigBird, are used for fine-tuning on the pathology reports to perform the BE diagnosis classification task.
  - Quick check question: How do transformer-based models differ from traditional machine learning models in terms of processing text data?

- Concept: Binary and multi-class classification
  - Why needed here: The study involves both binary dysplasia classification (dysplasia vs. no dysplasia) and multi-class BE diagnosis classification (six diagnosis classes), which are fundamental concepts in supervised learning.
  - Quick check question: What is the difference between binary and multi-class classification tasks?

## Architecture Onboarding

- Component map: Data pre-processing -> Annotation -> Model fine-tuning -> Hyperparameter optimization -> Model evaluation
- Critical path: 1. Data pre-processing and annotation 2. Model fine-tuning and hyperparameter optimization 3. Model evaluation on validation set
- Design tradeoffs:
  - Sub-sectioned vs. full-report text inputs: Sub-sectioned reports may be more focused on diagnostic information, while full reports capture more context but may include irrelevant information.
  - ClinicalBERT vs. Clinical-BigBird: Clinical-BigBird has longer input capacity but may consume more memory at runtime.
- Failure signatures:
  - Poor performance on the validation set: Indicates overfitting to the training data or insufficient model capacity.
  - Class imbalance in the multi-class task: May lead to poor performance on underrepresented classes.
- First 3 experiments:
  1. Fine-tune ClinicalBERT on sub-sectioned reports for binary dysplasia classification.
  2. Fine-tune Clinical-BigBird on full reports for multi-class BE diagnosis classification.
  3. Compare the performance of ClinicalBERT and Clinical-BigBird on the validation set for both tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the models perform on pathology reports from different institutions or time periods not covered in this study?
- Basis in paper: [explicit] "In future work, we may further validate our model by evaluating on CUIMC pathology reports from time periods not covered in this study and assess external validity using pathology reports from outside medical systems."
- Why unresolved: The study only tested the models on reports from a single institution (CUIMC) and a specific time period (2016-2020).
- What evidence would resolve it: Testing the models on pathology reports from other institutions and different time periods to assess generalizability and temporal stability.

### Open Question 2
- Question: How would the models compare to few-shot or zero-shot classification approaches using state-of-the-art LLMs like PaLM 2 or GPT-4?
- Basis in paper: [explicit] "We may also compare our method to few-shot or zero-shot classification approaches, using state-of-the-art LLMs like PaLM 2 or GPT-4, to further reduce the development time of automated clinical NLP applications."
- Why unresolved: The study only tested fine-tuned versions of ClinicalBERT and Clinical-BigBird, not few-shot or zero-shot approaches with other LLMs.
- What evidence would resolve it: Comparing the performance of the current models to few-shot or zero-shot classification approaches using other state-of-the-art LLMs.

### Open Question 3
- Question: What are the potential privacy implications and safeguards needed when using protected health information (PHI) for training these models?
- Basis in paper: [explicit] "A limitation of our method is that protected health information (PHI) was used for training. To publicly release our trained model for external use, we would be required to implement privacy-focused safeguards to prevent any possible de-identification."
- Why unresolved: The study mentions the use of PHI and the need for privacy safeguards but does not provide details on the specific safeguards implemented or their effectiveness.
- What evidence would resolve it: Detailed information on the privacy safeguards implemented and their effectiveness in preventing de-identification of PHI when using these models.

## Limitations

- Dataset size and generalizability: The study used only 619 pathology reports from a single institution, raising concerns about model generalizability to other healthcare systems with different documentation practices and population demographics.
- Annotation process transparency: The specific annotation criteria, inter-rater agreement statistics, and adjudication processes used by gastroenterologists remain unspecified, making it difficult to assess label quality and consistency.
- Rule-based baseline comparison: Limited details about the rule-based method's implementation make meaningful performance comparisons between transformer-based and rule-based approaches challenging.

## Confidence

**High Confidence**: The core finding that transformer-based models can achieve high performance (F1-scores of 0.964 for binary dysplasia and 0.911 for multi-class diagnosis) on BE diagnosis classification tasks is well-supported by the experimental results presented.

**Medium Confidence**: The claim that Clinical-BigBird outperforms ClinicalBERT in three of four tasks has some supporting evidence but requires additional validation with larger datasets and additional clinical institutions.

**Low Confidence**: The assertion that the method is "faster to implement" than rule-based approaches lacks quantitative support and comparative data on development time and complexity.

## Next Checks

1. **External validation study**: Deploy the trained models on pathology reports from at least two additional healthcare institutions with different documentation practices and population demographics. Compare performance metrics (F1-scores, accuracy) across institutions to assess true generalizability and identify any systematic performance drops that might indicate overfitting to CUIMC's specific documentation patterns.

2. **Rule-based method implementation**: Develop and implement a comparable rule-based classification system using the same input data and annotation scheme. Measure and compare: (a) development time and complexity, (b) inference speed on identical hardware, (c) maintenance requirements for handling edge cases, and (d) performance metrics. This would provide empirical evidence for the claimed implementation advantages of the transformer-based approach.

3. **Class-imbalanced scenario testing**: Create synthetic datasets with varying levels of class imbalance by artificially undersampling majority classes. Systematically evaluate model performance across these imbalance scenarios to determine the robustness threshold where performance degrades significantly. This would help establish practical limits for deploying these models in real-world settings where certain BE diagnoses may be rarer.