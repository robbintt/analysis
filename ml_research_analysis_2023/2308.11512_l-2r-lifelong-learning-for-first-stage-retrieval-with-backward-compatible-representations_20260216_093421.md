---
ver: rpa2
title: 'L^2R: Lifelong Learning for First-stage Retrieval with Backward-Compatible
  Representations'
arxiv_id: '2308.11512'
source_url: https://arxiv.org/abs/2308.11512
tags:
- learning
- retrieval
- documents
- data
- lifelong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies lifelong learning for first-stage retrieval,
  where a retrieval model must continuously adapt to new-coming documents without
  relevance labels while maintaining performance on historical data. The key challenge
  is to adapt effectively to evolving data distribution without recomputing all document
  embeddings for efficiency.
---

# L^2R: Lifelong Learning for First-stage Retrieval with Backward-Compatible Representations

## Quick Facts
- arXiv ID: 2308.11512
- Source URL: https://arxiv.org/abs/2308.11512
- Reference count: 40
- Primary result: L^2R achieves better performance on both learning new data and addressing forgetting compared to competitive lifelong learning baselines, with ranking alignment sometimes outperforming non-compatible learning.

## Executive Summary
This paper addresses the challenge of lifelong learning for first-stage retrieval, where retrieval models must continuously adapt to new documents without relevance labels while maintaining performance on historical data. The key insight is that effective lifelong learning requires both adaptation to new distributions and backward-compatible representations to avoid recomputing all document embeddings. The proposed L^2R method uses a memory buffer with diverse support negative selection and a ranking alignment objective to achieve both goals simultaneously. Experiments on constructed benchmarks show significant improvements over existing approaches.

## Method Summary
L^2R is a memory-based method that enables backward-compatible learning in first-stage retrieval. It selects diverse support negatives from new data using confidence (PSS) and diversity (ISD) criteria, and incorporates a ranking alignment objective to ensure backward-compatible representations. The method maintains a buffer of historical support negatives per query, selects new negatives based on PSS/ISD, and updates the model with a contrastive learning objective plus ranking alignment. This approach avoids rebuilding the entire index while maintaining performance on both historical and new data.

## Key Results
- L^2R significantly outperforms competitive lifelong learning baselines on constructed benchmarks LL-LoTTE and LL-MultiCPR
- The ranking alignment objective achieves not only representation compatibility but also even better performance than non-compatible learning in some cases
- With representation backward-compatibility, L^2R can save 79% of computation costs for inferring document representations
- Without the PSS component to filter out unlabeled relevant documents, retrieval performance significantly decreases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ranking alignment objective (ùêøalignr) provides more effective backward-compatible learning than embedding alignment.
- Mechanism: Instead of enforcing strict ‚Ñì2 distance between old and new embeddings, ranking alignment minimizes KL divergence between predicted distributions from old embeddings and current model outputs. This allows more flexible encoder updates while preserving relative rankings from previous sessions.
- Core assumption: Relative document rankings contain sufficient information to guide compatible learning without exact embedding preservation.
- Evidence anchors:
  - [abstract]: "the ranking alignment objective achieves not only representation compatibility but also remarkably even better performance"
  - [section 4.3]: "This ranking-based alignment not only allows more flexible exploration in the representation space but also facilitates bidirectional supervision for model learning"
  - [corpus]: No direct evidence; assumption based on internal consistency of ranking-based approaches in retrieval literature.
- Break condition: If relative rankings become unreliable due to severe distribution drift, or if embedding-level compatibility is required for downstream systems.

### Mechanism 2
- Claim: Diverse support negative selection using PSS and ISD criteria effectively adapts to new distributions while avoiding unlabeled positives.
- Mechanism: Documents are selected based on their positive sample superiority (PSS) - measuring relevance to queries - and inter-sample diversity (ISD) - ensuring varied negatives. This balances adaptation with avoiding false negatives.
- Core assumption: PSS and ISD can effectively identify hard negatives while filtering out unlabeled positives without explicit labels.
- Evidence anchors:
  - [section 4.2]: "Documents that are likely to be unlabeled positives should be avoided during selection...The selected documents should have minimum redundancy"
  - [section 6.2]: "Without the PSS component to filter out unlabeled relevant documents in the new data, the retrieval performance on the two datasets significantly decreases"
  - [corpus]: No direct evidence; mechanism relies on retrieval-specific negative sampling principles.
- Break condition: If PSS/ISD criteria fail to distinguish hard negatives from unlabeled positives, or if computational cost of diversity calculation becomes prohibitive.

### Mechanism 3
- Claim: Memory buffer with limited slots and strategic replacement enables efficient lifelong learning without full index rebuilds.
- Mechanism: Historical support negatives are stored per query, with new samples selected based on PSS/ISD and memory samples chosen for diversity and difference from new samples. Only new document embeddings are computed each session.
- Core assumption: Storing a subset of diverse negatives per query is sufficient to prevent catastrophic forgetting without full dataset storage.
- Evidence anchors:
  - [section 4.1]: "maintains a buffer to store the historical support negatives (i.e., negative samples that are important for learning the decision boundary of the model) for each training query"
  - [section 6.1]: "With representation backward-compatibility, it can save 79%...of computation costs for inferring document representations"
  - [corpus]: No direct evidence; standard memory-based continual learning assumption.
- Break condition: If memory buffer size is insufficient to capture necessary negative diversity, or if frequent memory updates disrupt learning stability.

## Foundational Learning

- Concept: Lifelong learning with catastrophic forgetting prevention
  - Why needed here: Retrieval models must adapt to new document distributions without losing performance on historical data
  - Quick check question: What happens to model performance on old data if only trained on new data without any forgetting prevention mechanism?

- Concept: Negative sampling in contrastive learning
  - Why needed here: Effective retrieval requires learning from hard negatives, but unlabeled new data may contain positives
  - Quick check question: Why is it problematic to treat all unlabeled new documents as negatives during training?

- Concept: Representation compatibility and backward compatibility
  - Why needed here: Avoid recomputing embeddings for entire document collection each time model updates
  - Quick check question: What computational advantage does backward compatibility provide in large-scale retrieval systems?

## Architecture Onboarding

- Component map: Dual-encoder retrieval model with query and document encoders ‚Üí contrastive learning loss with ranking alignment objective ‚Üí memory buffer for historical negatives ‚Üí PSS/ISD-based data selection ‚Üí embedding computation only for new documents
- Critical path: New document arrival ‚Üí PSS/ISD filtering ‚Üí memory selection ‚Üí model update with ranking alignment ‚Üí index update with new embeddings
- Design tradeoffs: Memory buffer size vs. negative diversity vs. storage cost; strict embedding alignment vs. flexible ranking alignment vs. adaptation capability
- Failure signatures: Performance degradation on historical queries (forgetting); poor performance on new data (insufficient adaptation); computational inefficiency (lack of compatibility)
- First 3 experiments:
  1. Implement basic L2R without compatibility to establish baseline performance on new vs. historical data
  2. Add embedding alignment to verify backward compatibility while measuring adaptation capability
  3. Implement ranking alignment and compare performance gains over embedding alignment approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the methods proposed for domain adaptation be adapted or evaluated under lifelong learning settings, particularly when addressing distribution changes in first-stage retrieval?
- Basis in paper: [explicit] The paper discusses that lifelong learning for first-stage retrieval involves adapting to evolving data distributions, and mentions that domain adaptation methods address similar distribution changes. However, it notes that this remains unexplored.
- Why unresolved: The paper identifies the potential overlap between domain adaptation and lifelong learning but does not provide empirical results or theoretical analysis on how domain adaptation techniques would perform in the lifelong learning context for first-stage retrieval.
- What evidence would resolve it: Experiments comparing domain adaptation methods directly with lifelong learning approaches on the constructed benchmarks (LL-LoTTE and LL-MultiCPR), analyzing performance differences and identifying which techniques transfer effectively.

### Open Question 2
- Question: What specialized techniques are needed to handle queries related to booming topics in lifelong learning for first-stage retrieval, beyond the general approaches proposed in L2R?
- Basis in paper: [explicit] The paper mentions that handling queries related to booming topics is an avenue for future research, as the current method does not have specialized techniques for this scenario.
- Why unresolved: While the paper constructs benchmarks with booming domains (e.g., lifestyle, recreation, science in LL-LoTTE), it does not investigate whether these domain shifts require fundamentally different handling compared to gradual distribution drift.
- What evidence would resolve it: Comparative analysis of model performance on queries within booming topics versus non-booming topics, identifying specific failure modes and developing targeted techniques to address them.

### Open Question 3
- Question: How does the selection of diverse support negatives scale with extremely large document collections, and what are the computational trade-offs between diversity and efficiency in practical deployments?
- Basis in paper: [inferred] The paper proposes selecting diverse support negatives based on confidence and diversity criteria, but acknowledges efficiency concerns when dealing with large collections and mentions using subsampling to reduce computation costs.
- Why unresolved: The paper provides preliminary experiments with subsampling but does not thoroughly investigate how the selection strategy performs as collection size scales to millions or billions of documents, nor does it analyze the fundamental trade-offs between diversity and computational efficiency.
- What evidence would resolve it: Systematic scaling experiments varying collection sizes from thousands to billions of documents, measuring both retrieval performance and computational costs, identifying optimal parameter settings for different scale regimes.

## Limitations
- Performance claims rely on constructed benchmarks (LL-LoTTE, LL-MultiCPR) without validation on real-world lifelong retrieval scenarios
- The memory buffer size (1000 samples per query) and its impact on different dataset scales is not thoroughly explored
- Paper lacks detailed implementation specifications for critical components like PSS/ISD calculation and ranking alignment objective
- Confidence: Medium - The proposed mechanisms are theoretically sound but practical implementation details are underspecified

## Confidence
- Ranking alignment superiority: Medium confidence
- PSS/ISD selection effectiveness: Medium confidence  
- Computational efficiency gains: High confidence

## Next Checks
1. Implement ablation study comparing ranking alignment vs. embedding alignment vs. no alignment to isolate contribution of compatibility mechanism
2. Test memory buffer sensitivity by varying buffer sizes (100, 500, 1000, 2000) to find optimal tradeoff between storage cost and performance
3. Evaluate on a real-world document stream with known temporal distribution drift to validate benchmark relevance and generalization