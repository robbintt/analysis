---
ver: rpa2
title: 'Sim-GPT: Text Similarity via GPT Annotated Data'
arxiv_id: '2312.05603'
source_url: https://arxiv.org/abs/2312.05603
tags:
- input
- output
- sentence
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Sim-GPT, a method to measure semantic textual
  similarity (STS) using GPT-annotated data. The approach addresses the lack of large,
  high-quality labeled STS datasets by using GPT-4 to generate (similar, dissimilar)
  sentence pairs from diverse sources like captions, questions, and multi-genre sentences.
---

# Sim-GPT: Text Similarity via GPT Annotated Data

## Quick Facts
- **arXiv ID**: 2312.05603
- **Source URL**: https://arxiv.org/abs/2312.05603
- **Reference count**: 40
- **Key outcome**: Proposes Sim-GPT, a method using GPT-4 to generate STS-labeled data for training contrastive models, achieving state-of-the-art performance on 7 STS benchmarks

## Executive Summary
Sim-GPT addresses the lack of large, high-quality labeled STS datasets by using GPT-4 to generate (similar, dissimilar) sentence pairs from diverse sources. A contrastive learning model (SimCSE or PromCSE) is then trained on this synthetic data. The method achieves state-of-the-art performance on seven STS benchmarks, outperforming supervised-SimCSE by +0.99 and PromCSE by +0.42. The approach also demonstrates stability across different few-shot settings and prompt variations, with the authors releasing 371K GPT-4-annotated examples and trained models.

## Method Summary
Sim-GPT generates (similar, dissimilar) sentence pairs using GPT-4 from diverse sources like captions, questions, and multi-genre sentences. A contrastive learning model (SimCSE or PromCSE) is trained on this synthetic data to learn sentence embeddings. The trained model computes similarity scores for new sentence pairs using cosine similarity. This approach provides a one-time data generation cost advantage over repeatedly invoking LLMs for each sentence pair.

## Key Results
- Achieves state-of-the-art performance on seven STS benchmarks
- Outperforms supervised-SimCSE by +0.99 and PromCSE by +0.42
- Demonstrates stability across different few-shot settings and prompt variations
- Shows effectiveness across multiple backbone models (BERT, RoBERTa)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sim-GPT achieves state-of-the-art performance by leveraging GPT-4 to generate high-quality training data with STS labels.
- **Mechanism**: GPT-4 generates (similar, dissimilar) sentence pairs from diverse sources, providing direct STS supervision that traditional NLI datasets lack. This data trains a contrastive learning model which learns to align similar sentences and separate dissimilar ones.
- **Core assumption**: GPT-4 can reliably generate semantically accurate similar and dissimilar sentence pairs that reflect true textual similarity.
- **Evidence anchors**: [abstract] "The core idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which an STS model is trained." [section] "Sim-GPT does not directly ask LLMs to provide STS scores for a newly-encounter sentence pair..."
- **Break condition**: If GPT-4 fails to generate semantically accurate pairs or introduces significant noise, the model's performance would degrade.

### Mechanism 2
- **Claim**: Training on GPT-4 annotated data provides long-term cost and speed advantages over repeated LLM invocations.
- **Mechanism**: By generating a one-time dataset using GPT-4 and training a smaller model (BERT/RoBERTa), Sim-GPT avoids calling LLMs for each new sentence pair, reducing inference costs and latency.
- **Core assumption**: The cost of generating a large training dataset once is offset by the savings from not invoking LLMs repeatedly during inference.
- **Evidence anchors**: [abstract] "Sim-GPT is trained on a one-time generated dataset using BERT or RoBERTa as the backbone, which offers long-term savings in cost and speed compared to repeatedly invoking LLMs for each sentence pair."
- **Break condition**: If the cost of generating the initial dataset is too high or the smaller model's performance is insufficient, the cost/speed advantage is negated.

### Mechanism 3
- **Claim**: Sim-GPT demonstrates stability across different few-shot settings and prompt variations.
- **Mechanism**: The model's performance is robust to changes in the number of few-shot examples and prompt wording, unlike in-context learning methods which are highly sensitive to prompt design.
- **Core assumption**: The large amount of training data generated by GPT-4 makes the model less sensitive to variations in prompt design during inference.
- **Evidence anchors**: [abstract] "Sim-GPT achieves state-of-the-art performance on seven STS benchmarks... The method also demonstrates stability across different few-shot settings and prompt variations."
- **Break condition**: If the model's performance degrades significantly with different few-shot settings or prompts, the claimed stability is not valid.

## Foundational Learning

- **Concept**: Contrastive learning
  - **Why needed here**: Sim-GPT uses contrastive learning (SimCSE/PromCSE) to learn sentence embeddings that pull similar sentences together and push dissimilar sentences apart in the embedding space.
  - **Quick check question**: How does contrastive learning differ from traditional supervised learning in the context of STS?

- **Concept**: Semantic Textual Similarity (STS)
  - **Why needed here**: Understanding the STS task is crucial for interpreting the results and evaluating the model's performance on benchmarks.
  - **Quick check question**: What are the typical score ranges for STS tasks, and how are they interpreted?

- **Concept**: Large Language Models (LLMs) for data generation
  - **Why needed here**: Sim-GPT leverages GPT-4 to generate training data, so understanding LLM capabilities and limitations is important.
  - **Quick check question**: What are the potential biases and errors that could arise when using LLMs to generate STS training data?

## Architecture Onboarding

- **Component map**: GPT-4 -> Data processing -> SimCSE/PromCSE -> Inference
- **Critical path**: Data generation (GPT-4) -> Data processing -> Model training (SimCSE/PromCSE) -> Inference
- **Design tradeoffs**: Cost vs. performance: Generating a large dataset with GPT-4 is expensive, but it enables training a smaller, faster model. Data diversity vs. quality: Using diverse sources for input sentences may introduce noise, but it also makes the model more robust.
- **Failure signatures**: Poor performance on STS benchmarks: Could indicate issues with data generation, model training, or both. High inference latency: Could indicate the smaller model is not optimized for speed.
- **First 3 experiments**:
  1. Evaluate Sim-GPT's performance on a held-out STS benchmark to verify the claimed state-of-the-art results.
  2. Compare the cost and speed of Sim-GPT inference vs. direct GPT-4 invocation for a set of sentence pairs.
  3. Test Sim-GPT's stability across different few-shot settings and prompt variations to verify the claimed robustness.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the content, potential open questions include:
- How does Sim-GPT's performance compare to traditional supervised STS models when trained on limited annotated data?
- Can Sim-GPT generalize well to languages other than English?
- How does Sim-GPT's performance compare to in-context learning approaches as the number of shots increases?

## Limitations

- The quality of GPT-4-generated training data is not directly validated through human evaluation
- Performance on out-of-distribution datasets and languages beyond English is not reported
- Cost-benefit analysis comparing the upfront cost of data generation to alternatives is lacking

## Confidence

- **High confidence**: The paper demonstrates that GPT-4 can generate STS training data and that training contrastive models on this data improves STS performance compared to NLI-only approaches.
- **Medium confidence**: The state-of-the-art results on the reported benchmarks are likely valid, but the magnitude of improvement (+0.99 over supervised-SimCSE) should be interpreted cautiously without ablation studies isolating the contribution of the synthetic data.
- **Low confidence**: Claims about stability across few-shot settings and prompt variations lack quantitative validation with error bars or statistical significance testing.

## Next Checks

1. **Human evaluation of synthetic data**: Conduct pairwise human judgments comparing GPT-4 generated similar/dissimilar pairs against human-annotated pairs from STS-B to measure annotation quality and potential biases.

2. **Cost-benefit analysis**: Calculate the exact compute cost of generating the 371K examples with GPT-4 versus the total cost of using GPT-4 for inference on a representative set of sentence pairs, including both API costs and latency measurements.

3. **Prompt sensitivity analysis**: Systematically vary the few-shot examples and prompt wording in the GPT-4 data generation phase, then measure the variance in downstream STS performance across multiple random seeds to quantify the claimed stability.