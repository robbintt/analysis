---
ver: rpa2
title: Towards objective and systematic evaluation of bias in artificial intelligence
  for medical imaging
arxiv_id: '2311.02115'
source_url: https://arxiv.org/abs/2311.02115
tags:
- bias
- group
- disease
- medical
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for systematically and objectively
  studying the impact of bias on medical imaging AI. The authors developed a tool
  for generating synthetic datasets with known bias effects, enabling controlled in
  silico trials.
---

# Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging

## Quick Facts
- arXiv ID: 2311.02115
- Source URL: https://arxiv.org/abs/2311.02115
- Reference count: 19
- Key outcome: Framework for controlled bias evaluation in medical imaging AI using synthetic data and explainable methods

## Executive Summary
This work introduces a systematic framework for evaluating bias in medical imaging AI using synthetic data generation with known bias effects. The authors developed SimBA, a tool for creating controlled counterfactual datasets where the same subjects are represented with or without specific bias effects. The framework was validated by measuring performance disparities in CNN classifiers trained on three bias scenarios and testing three bias mitigation strategies, with reweighing emerging as the most effective approach.

## Method Summary
The study employed SimBA to generate synthetic T1-weighted brain MR images with controlled bias effects. Three bias scenarios were created: no bias, near bias (left putamen), and far bias (right postcentral gyrus). A CNN classifier with five convolutional blocks was trained on these datasets using identical training protocols. Three bias mitigation strategies were evaluated: reweighing, adversarial unlearning, and group models. Performance was measured using accuracy, TPR/FPR disparities between bias groups, and SmoothGrad saliency maps for interpretability.

## Key Results
- Synthetic bias scenarios successfully induced expected subgroup performance disparities in CNN classifiers
- Reweighing achieved near-perfect mitigation of TPR/FPR disparities (relative ΔTPR: 0.44±1.80%, relative ΔFPR: -0.69±1.45%)
- Saliency maps revealed model reliance on bias regions for classification in biased scenarios
- The framework demonstrated feasibility for controlled in silico trials of bias effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic datasets with known bias allow controlled isolation of bias effects from confounding factors.
- Mechanism: By generating counterfactual datasets where the same subjects are represented with or without bias, the model training pipeline can be held constant while bias is varied, enabling attribution of performance differences purely to the bias effect.
- Core assumption: The synthetic bias is the sole systematic difference between the counterfactual datasets; all other distributions (subject effects, disease effects) are balanced.
- Evidence anchors:
  - [abstract]: "The feasibility is showcased by using three counterfactual bias scenarios to measure the impact of simulated bias effects on a convolutional neural network (CNN) classifier..."
  - [section]: "Thus, with identical model training schemata, we can conduct in silico trials exploring bias in AI, in which we systematically study the impact of different bias manifestations on the same deep learning pipeline."
  - [corpus]: Weak evidence; the corpus papers discuss bias evaluation but not controlled counterfactual generation methods.
- Break condition: If the synthetic data generation introduces unintended correlations or fails to balance subject/disease effect distributions, observed disparities may reflect these confounds rather than the intended bias.

### Mechanism 2
- Claim: Reweighing mitigates bias by balancing subgroup representation in the loss function.
- Mechanism: Assigning higher weights to underrepresented bias groups during training reduces the model's tendency to favor majority groups, thereby reducing subgroup performance disparities.
- Core assumption: The source of bias is due to unequal representation of bias groups in the training data, and the weighting scheme correctly identifies and compensates for this imbalance.
- Evidence anchors:
  - [abstract]: "Moreover, reweighing was identified as the most successful bias mitigation strategy for this setup..."
  - [section]: "The reweighing strategy almost perfectly mitigated any measured performance disparity between the bias groups, to respective values of relative ∆TPR and relative ∆FPR of 0·44±1·80% and –0·69±1·45%..."
  - [corpus]: Weak evidence; the corpus mentions reweighing in the context of demographic bias but does not validate its effectiveness on synthetic data with controlled bias.
- Break condition: If the bias is not primarily due to representation imbalance (e.g., if it stems from model architecture or dataset contamination), reweighing may not address the true source.

### Mechanism 3
- Claim: Explainable AI methods reveal how models use bias regions in decision-making.
- Mechanism: Saliency maps highlight regions that the model attends to for classification; higher saliency in bias regions indicates reliance on these regions, suggesting shortcut learning.
- Core assumption: The saliency method accurately reflects the model's decision process and is not itself biased or misleading in highlighting spurious correlations.
- Evidence anchors:
  - [abstract]: "...and we demonstrated how explainable AI methods can aid in investigating the manifestation of bias in the model using this framework."
  - [section]: "Saliency maps highlighted both the disease and the corresponding bias region in each of the bias scenarios..."
  - [corpus]: Weak evidence; the corpus discusses fairness and bias in medical imaging but does not provide evidence on the interpretability of saliency maps in controlled synthetic bias experiments.
- Break condition: If the saliency method highlights irrelevant regions or fails to capture the true decision process, it may mislead bias diagnosis.

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: To attribute performance differences to specific bias effects by comparing identical models trained on datasets differing only in bias.
  - Quick check question: If two models are trained on the same data except for a known bias variable, what performance differences can be attributed to that bias?

- Concept: Stratification in dataset generation
  - Why needed here: Ensures that subject and disease effect distributions are balanced across bias groups, isolating the effect of bias itself.
  - Quick check question: Why is it important to control for subject and disease effect distributions when generating synthetic biased datasets?

- Concept: Bias mitigation strategies (reweighing, adversarial unlearning, group models)
  - Why needed here: To test whether and how different methods can reduce subgroup performance disparities in the presence of known bias.
  - Quick check question: How does reweighing attempt to correct for representation imbalance in biased datasets?

## Architecture Onboarding

- Component map: SimBA synthetic data generator -> CNN classifier -> Bias mitigation module -> Evaluation metrics (accuracy, ΔTPR, ΔFPR) -> Explainable AI (SmoothGrad saliency) -> Statistical analysis
- Critical path: Synthetic dataset generation -> Model training (with/without mitigation) -> Performance evaluation -> Interpretability analysis
- Design tradeoffs:
  - Synthetic vs real data: Controlled bias vs ecological validity
  - Simple CNN vs complex architectures: Interpretability vs representational capacity
  - Single bias source vs multiple interacting biases: Clarity of attribution vs real-world complexity
- Failure signatures:
  - High accuracy but persistent ΔTPR/ΔFPR -> model using bias shortcuts
  - Mitigation strategy increases variance -> instability in balancing subgroups
  - Saliency maps highlight non-bias regions -> saliency method not aligned with model decisions
- First 3 experiments:
  1. Generate a no-bias dataset and confirm baseline performance without disparities.
  2. Introduce a near-bias effect and measure increase in ΔTPR/ΔFPR to confirm bias manifestation.
  3. Apply reweighing to the near-bias dataset and verify reduction in performance disparities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance disparities and bias mitigation efficacy vary when using vision transformer architectures instead of CNNs?
- Basis in paper: [inferred] The paper suggests investigating this relationship between bias proximity and model architecture.
- Why unresolved: The study only evaluated CNNs, so the impact on transformers is unknown.
- What evidence would resolve it: Training and evaluating the same bias scenarios using vision transformer architectures and comparing performance disparities and mitigation efficacy to the CNN results.

### Open Question 2
- Question: What is the relationship between the magnitude of bias effects and the resulting performance disparities?
- Basis in paper: [inferred] The paper uses fixed effect magnitudes, so the impact of varying magnitudes is unclear.
- Why unresolved: The study used fixed effect magnitudes, so the relationship between magnitude and disparities is unknown.
- What evidence would resolve it: Generating datasets with varying magnitudes of bias effects and measuring the resulting performance disparities.

### Open Question 3
- Question: How do different explainability methods compare in identifying salient regions associated with bias effects?
- Basis in paper: [explicit] The paper suggests using this framework to study different explainability methods.
- Why unresolved: Only SmoothGrad was used, so the performance of other methods is unknown.
- What evidence would resolve it: Applying various explainability methods to the same datasets and comparing their ability to highlight bias-associated regions.

### Open Question 4
- Question: How does mislabeling of bias attributes affect the efficacy of bias mitigation strategies?
- Basis in paper: [explicit] The paper suggests investigating the impact of mislabeling bias attributes.
- Why unresolved: The study used correctly labeled attributes, so the impact of mislabeling is unknown.
- What evidence would resolve it: Deliberately mislabeling bias attributes in the datasets and evaluating the performance of bias mitigation strategies.

## Limitations

- Synthetic data approach may not capture full complexity of real clinical datasets
- Three bias scenarios represent simplified, localized effects rather than multifactorial real-world bias
- Limited evaluation to single CNN architecture and three bias mitigation strategies

## Confidence

- Synthetic data generation methodology: High
- Bias mitigation evaluation: Medium
- Interpretability analysis using saliency maps: Low to Medium

## Next Checks

1. Apply the framework to a real clinical dataset with known demographic or technical bias sources to assess ecological validity of the synthetic approach.

2. Evaluate the framework's effectiveness across different model architectures (e.g., transformers, ensemble methods) to determine if findings generalize beyond the tested CNN.

3. Design experiments with multiple concurrent bias sources (e.g., demographic + technical) to assess how the framework handles more complex, realistic bias scenarios.