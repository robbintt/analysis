---
ver: rpa2
title: 'Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an
  Alternative to Attention Layers in Transformers'
arxiv_id: '2311.10642'
source_url: https://arxiv.org/abs/2311.10642
tags:
- networks
- replacement
- attention
- transformer
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether shallow feed-forward networks can
  replace attention mechanisms in Transformers for sequence-to-sequence tasks. The
  authors propose substituting attention layers with shallow feed-forward networks
  trained via knowledge distillation on the IWSLT2017 dataset.
---

# Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers

## Quick Facts
- arXiv ID: 2311.10642
- Source URL: https://arxiv.org/abs/2311.10642
- Authors: 
- Reference count: 2
- Key outcome: Shallow feed-forward networks can replace self-attention in Transformers for sequence-to-sequence tasks, achieving comparable BLEU scores on IWSLT2017, but struggle with cross-attention components.

## Executive Summary
This study investigates whether shallow feed-forward networks can effectively replace attention mechanisms in Transformers for sequence-to-sequence tasks. The authors propose substituting attention layers with shallow feed-forward networks trained via knowledge distillation on the IWSLT2017 dataset. They conduct extensive ablation studies using four replacement methods tested across five network sizes. The results demonstrate that feed-forward networks can successfully emulate self-attention mechanisms in both encoder and decoder, achieving BLEU scores comparable to the original Transformer. However, replacing cross-attention in the decoder yields significantly worse performance, suggesting limitations in modeling complex inter-sequence interactions. While these "attentionless Transformers" match baseline performance for self-attention, they require more parameters and lack flexibility for variable sequence lengths.

## Method Summary
The authors replace attention mechanisms in Transformers with shallow feed-forward networks trained via knowledge distillation. They train baseline Transformers on IWSLT2017 (French-English, English-French, German-English, English-German translation pairs) with maximum sentence length capped at 50 tokens, then extract intermediate activations from attention layers. Four replacement approaches are tested: Attention Layer Replacement (ALR), Attention Layer with Residual Connection Replacement (ALRR), Attention Separate Heads Layer Replacement (ASLR), and Encoder Layer Replacement (ELR), across five network sizes (XS to L). The shallow feed-forward networks (one hidden layer) are trained to mimic attention behavior using extracted activations as supervision, then integrated into Transformer architectures and evaluated using BLEU scores.

## Key Results
- Feed-forward networks successfully replace self-attention in both encoder and decoder, achieving BLEU scores comparable to original Transformers
- Cross-attention replacement in the decoder significantly degrades performance, indicating limitations in modeling inter-sequence interactions
- Attentionless Transformers require more parameters than standard attention-based models while achieving similar performance
- Knowledge distillation is necessary for training - direct training from scratch is not successful with current methods

## Why This Works (Mechanism)

### Mechanism 1
Feed-forward networks can emulate attention behavior when trained via knowledge distillation from attention layers. These FF networks take concatenated word representations as input and produce updated representations as output, effectively mimicking the linear combination of values performed by attention. The core assumption is that attention's primary function can be captured by a fixed-size feed-forward network when provided with teacher supervision through knowledge distillation. The approach fails when modeling cross-attention interactions between sequences, suggesting the limitation is in the expressiveness of shallow FF networks for complex inter-sequence mappings.

### Mechanism 2
The residual connection in attention layers contributes independently to model performance and can be separated from attention computation. The ALRR method replaces both the multi-head attention and the residual connection with a single feed-forward network, allowing isolation of whether the residual connection's benefits come from the attention mechanism itself or from the skip connection structure. The core assumption is that the residual connection provides value independent of the attention computation it follows. Results show ALRR performs worse than ALR, indicating the residual connection has value beyond just being paired with attention.

### Mechanism 3
Cross-attention requires more complex function approximation than self-attention due to modeling interactions between different sequences. While FF networks successfully replace self-attention in both encoder and decoder, they fail to match performance when replacing cross-attention. This suggests cross-attention performs a more complex mapping that requires greater expressiveness than shallow feed-forward networks provide. The core assumption is that cross-attention involves more intricate interactions between sequences than self-attention, requiring higher model capacity.

## Foundational Learning

- **Concept: Knowledge Distillation**
  - Why needed here: The feed-forward networks are trained to replicate attention layer behavior using intermediate activations from a trained Transformer as supervision, rather than learning from raw data directly.
  - Quick check question: What is the teacher-student relationship in knowledge distillation, and why is it necessary for training attentionless Transformers?

- **Concept: Attention Mechanism Mathematics**
  - Why needed here: Understanding how attention computes weighted combinations of value vectors based on query-key compatibility is essential for designing appropriate feed-forward replacements.
  - Quick check question: How does multi-head attention transform input representations, and what operations must a replacement network replicate?

- **Concept: BLEU Score Calculation**
  - Why needed here: BLEU is the evaluation metric used to compare attentionless Transformers against baseline performance in translation tasks.
  - Quick check question: What components make up the BLEU score, and what range of values would indicate successful attention replacement?

## Architecture Onboarding

- **Component map**: Teacher model -> Data extraction layer -> Replacement networks -> Integration layer -> Evaluation layer
- **Critical path**: 
  1. Train baseline Transformer (teacher)
  2. Extract intermediate activations for all attention layers
  3. Train FF replacement networks using extracted data
  4. Replace attention layers in Transformer with trained FF networks
  5. Evaluate performance on IWSLT2017 test sets

- **Design tradeoffs**:
  - Parameter efficiency vs performance: FF replacements require more parameters than attention layers
  - Fixed sequence length: FF networks need padding/masking due to fixed input size
  - Expressiveness vs simplicity: Shallow networks work for self-attention but struggle with cross-attention

- **Failure signatures**:
  - Significant BLEU score drop when replacing cross-attention indicates insufficient model capacity
  - Poor training convergence without knowledge distillation suggests need for teacher supervision
  - Performance degradation with sequence lengths beyond training maximum indicates architectural limitation

- **First 3 experiments**:
  1. Implement ALR replacement for encoder self-attention only, compare BLEU scores against baseline
  2. Train deeper feed-forward networks to test if increased capacity improves cross-attention emulation
  3. Apply ALR to decoder self-attention while keeping cross-attention intact, measure individual component contributions

## Open Questions the Paper Calls Out

### Open Question 1
Can attention mechanisms in Transformers be completely replaced by feed-forward networks for all sequence-to-sequence tasks? The authors conclude that their findings suggest Transformers do not necessarily need attention mechanisms, as feed-forward networks can achieve comparable performance to the original Transformer in many cases. This remains unresolved because the study found that replacing cross-attention in the decoder significantly reduced performance, indicating limitations in modeling complex inter-sequence interactions. Developing more sophisticated feed-forward architectures or optimization techniques that can successfully replace cross-attention mechanisms while maintaining or improving translation quality would provide a definitive answer.

### Open Question 2
What optimization techniques beyond knowledge distillation could enable training attentionless Transformers from scratch? The authors note that current optimization methods require knowledge distillation rather than training from scratch, and emphasize that with advancements in optimization techniques, less specialized architectures like feed-forward networks could be used for advanced tasks. This remains unresolved because the paper demonstrates that knowledge distillation is currently necessary for training these models, but does not explore alternative optimization strategies that might allow for direct training. Successfully training attentionless Transformers from scratch using novel optimization methods without relying on knowledge distillation would address this question.

### Open Question 3
How does the performance of attentionless Transformers scale with increasing model size and sequence length? The authors note that their replacement networks require more parameters and lack flexibility for variable sequence lengths, limiting their practical deployment. This remains unresolved because the study used a maximum sequence length of 50 words and tested only up to large model sizes, leaving questions about scalability unanswered. Systematic experiments testing attentionless Transformers on longer sequences and with larger model sizes, comparing their performance, parameter efficiency, and computational requirements to traditional attention-based models, would provide clarity on scalability limitations.

## Limitations
- Feed-forward networks successfully replace self-attention but fail to match cross-attention performance, indicating fundamental architectural constraints
- Knowledge distillation approach requires extensive training data from teacher model, making it impractical for training from scratch on new tasks
- Fixed input size requirement limits flexibility for variable-length sequences, necessitating padding or masking strategies

## Confidence
- **High confidence**: The empirical results demonstrating that feed-forward networks can successfully replace self-attention mechanisms in both encoder and decoder components
- **Medium confidence**: The claim that attention mechanisms can be "potentially replaced" by simpler architectures, though this requires qualification due to cross-attention limitations
- **Low confidence**: The broader implications about Transformer architecture flexibility and potential for simpler sequence modeling due to limited scope and practical constraints

## Next Checks
1. **Cross-attention capacity test**: Train deeper feed-forward networks (multiple hidden layers with increasing width) to systematically test whether the cross-attention performance gap can be closed with greater model capacity
2. **Scratch training experiment**: Attempt to train attentionless Transformers directly on IWSLT2017 without knowledge distillation using various initialization strategies and training schedules
3. **Cross-domain generalization**: Evaluate attentionless Transformers on different sequence-to-sequence tasks beyond translation, such as summarization, dialogue generation, or code generation