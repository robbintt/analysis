---
ver: rpa2
title: Mitigating Adversarial Attacks in Federated Learning with Trusted Execution
  Environments
arxiv_id: '2309.07197'
source_url: https://arxiv.org/abs/2309.07197
tags:
- adversarial
- attacks
- learning
- attack
- against
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Pelta, a novel defense mechanism against adversarial
  attacks in federated learning using trusted execution environments (TEEs). Pelta
  masks inside the TEE the first part of the backpropagation chain rule, which is
  typically exploited by attackers to craft adversarial samples.
---

# Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments

## Quick Facts
- arXiv ID: 2309.07197
- Source URL: https://arxiv.org/abs/2309.07197
- Reference count: 40
- One-line primary result: Pelta defends federated learning against white-box adversarial attacks by masking gradients inside TEEs

## Executive Summary
This paper introduces Pelta, a novel defense mechanism that leverages Trusted Execution Environments (TEEs) to protect federated learning systems from white-box adversarial attacks. The approach masks the first part of the backpropagation chain rule inside a TEE enclave, preventing attackers from computing gradients needed to craft adversarial samples. The authors evaluate Pelta on state-of-the-art models including Vision Transformers and CNNs using CIFAR-10, CIFAR-100, and ImageNet datasets, demonstrating significant improvements in robust accuracy against multiple attack types.

## Method Summary
Pelta operates by shielding the shallowest layers of neural networks inside a TEE enclave, specifically masking the local Jacobian between input and the first trainable layer. This prevents attackers from completing the chain rule calculation required for gradient-based adversarial attacks. The method is designed to work with both single models and ensemble models, with special consideration for Vision Transformer architectures where position embeddings are shielded. The approach balances security with practical constraints, requiring less than 16MB of TEE memory even in the worst case, making it feasible for deployment on TrustZone-enabled devices.

## Key Results
- Pelta achieves robust accuracy improvements of 15-25% on CIFAR-10 and CIFAR-100 against white-box attacks including PGD, MIM, and C&W
- The method successfully defends ensemble models against Self-Attention Gradient Attacks (SAGA), with both models shielded providing maximum protection
- Memory overhead remains below 16MB even for ensemble models, consistent with typical TrustZone-enabled device constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking the local Jacobian between input and the first trainable layer prevents the attacker from completing the chain rule for gradient-based adversarial attacks.
- Mechanism: By storing ∂f₁ⱼ/∂x inside a TEE enclave and shielding intermediate gradients J₀→₁, the attacker cannot reconstruct the full gradient ∇ₓL needed to compute adversarial perturbations.
- Core assumption: The attacker has no prior knowledge of the shielded parameters and cannot reconstruct them through inference or input-output comparison.
- Evidence anchors:
  - [abstract] "Pelta masks inside the TEE the first part of the backpropagation chain rule, typically exploited by attackers to craft the malicious samples."
  - [section] "Because it is the hardest to defend against, we consider the white-box setting... an attacker leverages the model's gradients to craft images designed to fool a classifier."
  - [corpus] Weak - the corpus neighbors focus on Byzantine threats and federated learning robustness but don't directly address gradient masking or TEE-based defenses.
- Break condition: If the attacker can infer the shielded parameters through repeated queries or has access to prior knowledge of the model's initial layers.

### Mechanism 2
- Claim: Shielding both ViT and CNN models in an ensemble prevents SAGA from exploiting the vulnerability of the non-shielded model.
- Mechanism: When both models are shielded, SAGA cannot use the non-shielded model's gradients to craft adversarial examples, forcing it to rely on the under-factored gradient from the shielded model.
- Core assumption: SAGA relies on computing gradients for both models to blend them for attack, and without access to one model's gradients, the attack becomes significantly less effective.
- Evidence anchors:
  - [section] "For the ensemble model is evaluated against the SAGA in four settings: no model is shielded, only the BiT model is shielded, only the ViT model is shielded, both models are shielded (maximum protection for the ensemble)."
  - [section] "Interestingly, for the ensemble, the attack success rate of the upsampling against the Pelta defense scheme sometimes surpasses that of the random uniform attack against BiT when the shield is applied only to it."
  - [corpus] Weak - the corpus doesn't specifically discuss ensemble defenses against gradient-based attacks in federated learning.
- Break condition: If the attacker can successfully upsample the under-factored gradient from the shielded model to create adversarial examples, or if the ensemble policy is not random selection.

### Mechanism 3
- Claim: Limiting the TEE memory usage to less than 16 MB makes Pelta feasible for deployment on TrustZone-enabled devices while still providing significant protection.
- Mechanism: By shielding only the first few layers of the model (e.g., up to position embedding for ViT, first convolution for BiT), Pelta achieves a balance between security and memory constraints.
- Core assumption: The first few layers of the model contain enough sensitive information that masking them significantly impedes the attacker's ability to craft adversarial examples.
- Evidence anchors:
  - [section] "Assuming the most resource-intensive case where gradients are produced, the shielding of the ensemble requires less than 16 MB of TEE memory at the very worst, consistent with what typical TrustZone-enabled devices allow [21]."
  - [section] "Notice that, because it only ever obfuscates the shallowest parts of a model, Pelta is barely ever affected by the scale of larger and more complex variants, which makes it suitable for a wide variety of state-of-the-art models."
  - [corpus] Weak - the corpus neighbors don't discuss memory constraints or TEE memory usage in the context of federated learning defenses.
- Break condition: If the memory constraints of the TEE are tighter than assumed, or if the first few layers are not sufficient to impede the attacker.

## Foundational Learning

- Concept: Backpropagation and the chain rule in neural networks
  - Why needed here: Understanding how gradients are computed through the chain rule is crucial for grasping how Pelta masks the gradients to prevent adversarial attacks.
  - Quick check question: How does the chain rule allow the computation of gradients through multiple layers in a neural network?

- Concept: Trusted Execution Environments (TEEs) and their role in security
  - Why needed here: TEEs are the core technology enabling Pelta to physically hide sensitive parameters and gradients from attackers.
  - Quick check question: What security guarantees do TEEs like TrustZone provide, and how do they differ from software-based obfuscation?

- Concept: Adversarial attacks and their types (white-box vs. black-box)
  - Why needed here: Understanding the threat model and the types of attacks Pelta is designed to defend against is essential for evaluating its effectiveness.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks, and why is Pelta specifically designed to defend against white-box attacks?

## Architecture Onboarding

- Component map:
  Input layer → Pelta shielding (TEE) → Masked gradients → Subsequent layers → Output
  Ensemble models: Each model has its own Pelta shielding, with a random selection policy for output

- Critical path:
  Input data → Secure world (TEE) → Masked gradient computation → Clear world → Subsequent layers → Model output
  For ensemble: Each model's path is independent until the random selection policy combines outputs

- Design tradeoffs:
  Security vs. performance: Shielding more layers increases security but also increases TEE memory usage and computation time
  Complexity vs. effectiveness: Ensemble models provide better security but add complexity in implementation and inference

- Failure signatures:
  High attack success rate despite shielding: Indicates the attacker can reconstruct shielded parameters or the TEE is compromised
  Increased inference time or memory usage: Suggests the shielding is too aggressive or the TEE is not optimized for the workload

- First 3 experiments:
  1. Test Pelta's effectiveness on a single ViT model against FGSM and PGD attacks, measuring robust accuracy with and without shielding.
  2. Evaluate the memory usage and inference time overhead of Pelta on different model sizes (ViT-B/16, ViT-L/16) to determine the practical limits of shielding.
  3. Test the ensemble defense with Pelta against SAGA, comparing the attack success rate when only one model is shielded vs. both models are shielded.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Pelta's effectiveness scale with the size and complexity of models beyond those tested in the evaluation?
  - Basis in paper: [inferred] The paper mentions that Pelta's memory overhead is minimal and independent of model size, but does not provide empirical evidence for larger or more complex architectures.
  - Why unresolved: The evaluation focused on specific models (Vision Transformers, CNNs, and Big Transfer models) on three datasets. The paper suggests Pelta could be applied to a wide variety of state-of-the-art models, but does not demonstrate this.
  - What evidence would resolve it: Empirical results showing Pelta's effectiveness on larger models (e.g., GPT-3, ViT-Huge) and more complex architectures, along with a detailed analysis of any scaling challenges or limitations.

- **Open Question 2**: How does Pelta perform against black-box attacks, where the attacker does not have access to the model's gradients or architecture?
  - Basis in paper: [explicit] The paper explicitly states that Pelta provides no defense capabilities against black-box attacks.
  - Why unresolved: The paper focuses solely on white-box attacks and does not explore Pelta's behavior in a black-box scenario, which is a common attack vector in real-world applications.
  - What evidence would resolve it: Experimental results comparing Pelta's performance against both white-box and black-box attacks, including attacks that use transferability or model approximation techniques.

- **Open Question 3**: What is the optimal balance between the number of layers shielded and the overall system performance (e.g., inference latency, memory usage) in a real-world federated learning deployment?
  - Basis in paper: [inferred] The paper discusses Pelta's memory overhead and mentions potential performance implications, but does not provide a detailed analysis of the trade-offs between security and system performance.
  - Why unresolved: The paper focuses on the theoretical aspects of Pelta's design and its effectiveness against attacks, but does not explore the practical considerations of implementing Pelta in a real-world federated learning system.
  - What evidence would resolve it: A comprehensive study evaluating Pelta's performance in a real-world federated learning scenario, including metrics such as inference latency, memory usage, and communication overhead, along with a sensitivity analysis of the number of shielded layers.

## Limitations

- Pelta provides no defense capabilities against black-box attacks, limiting its effectiveness in scenarios where attackers don't have access to model gradients.
- The security guarantees of the TEE shielding mechanism depend on the specific TEE implementation and its resistance to side-channel attacks, which are not thoroughly evaluated.
- The evaluation focuses on benchmark datasets and models, with limited exploration of Pelta's performance in real-world federated learning systems with heterogeneous devices and communication constraints.

## Confidence

- High confidence: Pelta's ability to improve robust accuracy against white-box attacks on benchmark datasets.
- Medium confidence: The scalability and memory efficiency of Pelta on different TEE-enabled devices.
- Low confidence: The security guarantees of the TEE shielding mechanism and its resilience against advanced attack strategies.

## Next Checks

1. Implement and evaluate Pelta using an actual TEE framework (e.g., Intel SGX, ARM TrustZone) to assess the real-world feasibility and security of the shielding mechanism.
2. Test Pelta's effectiveness against a broader range of adversarial attacks, including black-box attacks, transfer attacks, and data poisoning, to evaluate its robustness in more realistic threat scenarios.
3. Conduct a thorough analysis of the memory and performance overhead of Pelta on various TEE-enabled devices, and compare it with other defense mechanisms to determine its practicality for deployment in federated learning systems.