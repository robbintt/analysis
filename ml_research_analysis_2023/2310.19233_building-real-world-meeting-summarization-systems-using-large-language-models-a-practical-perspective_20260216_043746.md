---
ver: rpa2
title: 'Building Real-World Meeting Summarization Systems using Large Language Models:
  A Practical Perspective'
arxiv_id: '2310.19233'
source_url: https://arxiv.org/abs/2310.19233
tags:
- summarization
- llms
- datasets
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using LLMs for meeting summarization, focusing
  on balancing performance, cost, and privacy. It compares closed-source (GPT-3.5,
  GPT-4, PaLM-2) and open-source (LLaMA-2) models.
---

# Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective

## Quick Facts
- arXiv ID: 2310.19233
- Source URL: https://arxiv.org/abs/2310.19233
- Reference count: 21
- Large language models like GPT-4 achieve strong performance on meeting summarization, but smaller open-source models like LLaMA-2-7B can offer competitive results with lower cost and privacy advantages.

## Executive Summary
This paper investigates the practical deployment of large language models for meeting summarization, balancing performance, cost, and privacy considerations. The study compares closed-source models (GPT-3.5, GPT-4, PaLM-2) with open-source LLaMA-2 models across multiple datasets. While GPT-4 achieves the highest summarization quality, the analysis shows that LLaMA-2-7B provides a compelling alternative for real-world deployment due to its competitive performance, lower cost, and privacy benefits. The paper introduces practical strategies for handling long meeting transcripts, including truncation and chapterization approaches.

## Method Summary
The study conducts zero-shot evaluation of multiple LLMs (GPT-3.5, GPT-4, PaLM-2, LLaMA-2-7B) on meeting summarization tasks. Two main approaches are tested: truncation (limiting input to 2500 words) and chapterization (splitting transcripts into chapters, summarizing each, then combining). Performance is measured using ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore metrics across four datasets: AMI, ICSI, MeetingBank, and QMSUM. The study also evaluates cost, latency, and privacy implications of different model choices.

## Key Results
- GPT-4 generally achieves the best summarization performance but is 25x more expensive than GPT-3.5
- LLaMA-2-7B achieves performance comparable to larger closed-source models while being significantly smaller (7B vs 175B parameters)
- Truncation often performs as well as chapterization, suggesting gold summaries are biased toward early transcript content
- LLaMA-2-7B shows promise for industrial deployment due to cost, privacy, and hardware flexibility advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLaMA-2-7B achieves competitive performance with GPT-4 on meeting summarization despite being 7B vs 175B parameters.
- Mechanism: The truncation approach (limiting input to first 2500 words) plus chapterization (summarizing chunks then recombining) allows smaller models to approximate full-context understanding, trading some coherence for cost/privacy.
- Core assumption: Summarization quality is sufficiently preserved when only the first ~2500 words are provided, or when summaries are stitched from chapter-level outputs.
- Evidence anchors:
  - [abstract] "much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios."
  - [section] "our experimental results show that while most closed-source models generally achieve better performance in meeting summarization datasets, the open-source LLaMA-2 models still achieve comparable performance while being significantly smaller."
- Break condition: If chapterization introduces incoherence or if key information consistently appears after the 2500-word cutoff, performance will degrade sharply.

### Mechanism 2
- Claim: GPT-4's superior performance is offset by higher cost and slower inference, making LLaMA-2-7B preferable for real-world deployment.
- Mechanism: Cost-per-token and latency measurements show GPT-4 is ~25x more expensive and ~3-4x slower than GPT-3.5, and ~3x slower than LLaMA-2-7B on comparable hardware.
- Core assumption: Production constraints (privacy, API cost, speed) outweigh marginal performance gains.
- Evidence anchors:
  - [section] "the LLaMA-2-7B model looks more promising for industrial usage... balancing performance with associated costs and privacy concerns"
  - [section] "GPT-4 is the fastest, as it takes 2.5 seconds on average per transcript for inference, PaLM-2 which takes about 3.2 seconds... GPT-4... takes about 11 seconds on average per transcript... LLaMA-2-7B... takes 15 seconds on average."
  - [section] "the pricing... GPT-4... costs 0.03$ per 1K input tokens and 0.06$ per 1K output tokens... for the 4K context version of GPT-3.5... costs 0.0015$ per 1K input tokens... making GPT-4 25 times more costly than GPT-3.5."
- Break condition: If inference hardware costs for LLaMA-2-7B rise above cost savings, or if GPT-4's performance edge becomes mission-critical.

### Mechanism 3
- Claim: Summarization via truncation works as well as chapterization in some datasets because reference summaries are biased toward early transcript content.
- Mechanism: Analysis shows truncation often matches or beats chapterization in MeetingBank/QMSUM, suggesting gold summaries focus on beginnings of transcripts.
- Core assumption: Dataset construction or annotation process weights early content more heavily.
- Evidence anchors:
  - [section] "we surprisingly find that the summarization via chapterization approaches fail to outperform the summarization via truncation approach in many scenarios... the model does not have access to the whole context when the summarization via truncation approach is used, indicating that the gold reference summaries in these datasets could possibly be more biased towards the beginning of the transcript."
  - [section] "In the MeetingBank dataset, we find that the chapterization via resummarization approach using GPT-4 is the best-performing approach... while LLaMA-2-7B based on Truncation is the best-performing approach in terms of ROUGE-2."
- Break condition: If future datasets are constructed to require full-context understanding, truncation will underperform.

## Foundational Learning

- Concept: Zero-shot summarization with LLMs
  - Why needed here: Models must generate summaries without fine-tuning, relying on prompt engineering and inherent capabilities.
  - Quick check question: What prompt format is used to trigger summarization without in-context examples?

- Concept: Context window and truncation strategies
  - Why needed here: Meeting transcripts exceed model input limits; engineers must decide how to handle overflow.
  - Quick check question: How does the truncation threshold (2500 words) relate to average transcript length in the datasets?

- Concept: Evaluation metrics for summarization
  - Why needed here: ROUGE and BERTScore measure overlap and semantic similarity; understanding their strengths/weaknesses is critical.
  - Quick check question: Why might BERTScore capture aspects ROUGE misses in meeting summarization?

## Architecture Onboarding

- Component map:
  Preprocess -> LLM inference -> Postprocess -> Store/return summary

- Critical path:
  Preprocess → LLM inference → Postprocess → Store/return summary.

- Design tradeoffs:
  - Model size vs cost vs performance: LLaMA-2-7B is cheaper/faster but may lag GPT-4.
  - Truncation vs chapterization: Truncation simpler but may miss late content; chapterization more complex but can preserve global context.
  - On-prem vs API: On-prem (LLaMA-2) better for privacy, API (GPT) better for speed.

- Failure signatures:
  - Summaries missing critical information from later transcript sections → truncation too aggressive.
  - Incoherent stitched summaries → chapterization logic flawed.
  - High latency → model too large or GPU underprovisioned.
  - API failures or cost overruns → closed-source model unsuitable.

- First 3 experiments:
  1. Run same 10 transcripts through LLaMA-2-7B truncation vs GPT-3.5 truncation; compare ROUGE/BERTScore and latency.
  2. Compare truncation vs chapterization for LLaMA-2-7B on AMI dataset; measure coherence and performance.
  3. Test LLaMA-2-7B on a real meeting transcript; manually check for key topic coverage vs reference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of meeting summaries in the AMI and ICSI datasets make concatenation or rewriting of chapter summaries more effective than re-summarization or truncation?
- Basis in paper: [explicit] The authors observe that in AMI and ICSI datasets, concatenation or rewriting of chapter summaries is more effective than other approaches, but do not explain why.
- Why unresolved: The paper does not provide a detailed analysis of the characteristics of these datasets that lead to this observation.
- What evidence would resolve it: A detailed qualitative analysis of the summaries in these datasets, identifying common patterns or structures that favor concatenation or rewriting approaches.

### Open Question 2
- Question: How do different prompt variations impact the quality and style of generated meeting summaries, and can these variations be optimized for specific use cases?
- Basis in paper: [explicit] The authors conduct a case study on prompt variations, showing that different prompts yield different results, but do not explore this further.
- Why unresolved: The paper does not investigate the full range of prompt variations or their impact on summary quality and style.
- What evidence would resolve it: A comprehensive study of various prompt variations, their impact on summary quality, and their suitability for different use cases.

### Open Question 3
- Question: What are the specific trade-offs between model size, performance, and cost when using open-source LLMs like LLaMA-2 for meeting summarization, and how can these be optimized for different deployment scenarios?
- Basis in paper: [explicit] The authors discuss the trade-offs between model size, performance, and cost, but do not provide a detailed analysis of how these factors interact or how they can be optimized.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-offs or guidelines for optimizing these factors for different deployment scenarios.
- What evidence would resolve it: A detailed study of the performance, cost, and deployment requirements of different LLaMA-2 model sizes, along with guidelines for selecting the optimal model based on specific use cases and constraints.

## Limitations
- Zero-shot prompting without fine-tuning may not represent optimal performance for each model.
- Truncation approach (2500-word cutoff) may miss critical information in longer meetings.
- Analysis suggests dataset biases toward early transcript content, requiring further investigation.
- Privacy benefits of LLaMA-2 are assumed but not empirically verified against specific organizational requirements.

## Confidence
**High confidence** in comparative performance rankings (GPT-4 > PaLM-2 > GPT-3.5 > LLaMA-2-7B on most metrics) and cost/latency measurements. **Medium confidence** in the practical recommendation favoring LLaMA-2-7B, as this depends on organizational priorities around privacy, cost, and acceptable performance tradeoffs. **Low confidence** in the generalizability of truncation effectiveness across different meeting types and industries, as the analysis is dataset-dependent.

## Next Checks
1. Test truncation vs chapterization on real-world organizational meeting transcripts with known critical information appearing throughout, not just at the beginning.
2. Conduct privacy risk assessment comparing on-premise LLaMA-2 deployment against API-based models under specific regulatory frameworks (HIPAA, GDPR).
3. Implement a cost model incorporating inference hardware amortization, API fees, and human review time to validate the economic recommendation.