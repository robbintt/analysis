---
ver: rpa2
title: Kindness in Multi-Agent Reinforcement Learning
arxiv_id: '2311.04239'
source_url: https://arxiv.org/abs/2311.04239
tags:
- agent
- kindmarl
- each
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KindMARL, a method to incorporate human-like
  fairness and reciprocity into Multi-Agent Reinforcement Learning (MARL). KindMARL
  evaluates the intentions of other agents using counterfactual reasoning, then uses
  these intentions as weights in reward comparisons to encourage cooperation.
---

# Kindness in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.04239
- Source URL: https://arxiv.org/abs/2311.04239
- Reference count: 5
- Primary result: KindMARL achieved 89% and 44% more total rewards than Inequity Aversion in Cleanup and Harvest environments, and 8.8% and 5.9% lower travel times in traffic light control compared to Colight and Inequity Aversion methods.

## Executive Summary
This paper introduces KindMARL, a method that incorporates human-like fairness and reciprocity into Multi-Agent Reinforcement Learning (MARL) by using counterfactual reasoning to evaluate other agents' intentions. The method calculates "kindness" as the difference between an agent's reward and its fellow's reward, weighted by the fellow's intention score derived from counterfactual reasoning. Experiments on Cleanup, Harvest, and traffic light control environments demonstrate that KindMARL agents earned significantly more total rewards and achieved lower travel times compared to baseline methods like Inequity Aversion and Social Influence.

## Method Summary
KindMARL uses an Extended Intrinsic Curiosity Module (EICM) with forward and inverse models to perform counterfactual reasoning about other agents' actions and their environmental impacts. Agents estimate intentions by predicting state changes for counterfactual actions, then use these intentions as weights in reward comparisons to encourage cooperation. The method adjusts intrinsic rewards based on whether agents perceive themselves as superior or inferior to their fellows, implementing reciprocity through reward reduction when kindness is perceived. The approach was tested using DQN, A3C, and PPO algorithms across various cooperative environments.

## Key Results
- KindMARL agents earned 89% more total rewards than Inequity Aversion in Cleanup environment
- KindMARL achieved 44% more total rewards than Inequity Aversion in Harvest environment
- KindMARL achieved 8.8% and 5.9% lower travel times compared to Colight and Inequity Aversion methods in traffic light control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents perceive kindness when their fellows' actions lead to higher rewards, weighted by the fellows' intentions.
- Mechanism: Kindness is calculated as the difference between an agent's reward and its fellow's reward, multiplied by the fellow's intention score (derived from counterfactual reasoning).
- Core assumption: Intention can be accurately estimated by comparing the environmental impact of an agent's actual action to the impact of its counterfactual actions.
- Evidence anchors:
  - [abstract] "The difference between each agent's reward, as the outcome of its action, with that of its fellow, multiplied by the intention of the fellow is then taken as the fellow's 'kindness'."
  - [section] "dk,j t = l / max(Lj t )" where dk,j t represents the intention of agent j as measured by agent k.
- Break condition: If counterfactual reasoning fails to predict environmental impact accurately, intention scores will be unreliable, leading to incorrect kindness assessments.

### Mechanism 2
- Claim: Agents internalize kindness assessments by adjusting their rewards based on whether they perceive themselves as superior or inferior to their fellows.
- Mechanism: When an agent perceives kindness from a fellow (via the weighted reward comparison), it reduces its own reward as a form of reciprocity.
- Core assumption: Agents can distinguish between advantageous and disadvantageous inequity through reward comparisons.
- Evidence anchors:
  - [abstract] "If the result of each reward-comparison confirms the agent's superiority, it perceives the fellow's kindness and reduces its own reward."
  - [section] "ik t = -αk/(N-1) Σj≠k dk,j t max(wj t - wk t, 0) - βk/(N-1) Σj≠k dk,j t max(wk t - wj t, 0)" shows the reward adjustment formula.
- Break condition: If agents cannot reliably compare rewards (e.g., due to noise or stochastic environments), the reciprocity mechanism breaks down.

### Mechanism 3
- Claim: EICM enables agents to estimate counterfactual environmental impacts by predicting state changes for each possible action.
- Mechanism: Agents use a forward model to predict the current state encoding if a fellow had taken a different action, then compute the difference from the actual state.
- Core assumption: The forward model can accurately predict state transitions for counterfactual actions.
- Evidence anchors:
  - [section] "agent k estimates the environment impact of each of its fellows considering counterfactual reasoning... agent k predicts its current local state sk t by supposing each agent j applies one of its other available actions."
  - [section] "Lt = 1/2 ||ϕ(sk t-1) - ˆϕ(ϕ(sk t-1), a˜j t-1)||^2" shows the MSE-based counterfactual prediction.
- Break condition: If the forward model is poorly trained or the state space is too complex, counterfactual predictions become inaccurate.

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: To estimate an agent's intention by comparing what actually happened to what would have happened if the agent had chosen a different action.
  - Quick check question: How does the model compute the "difference" between actual and counterfactual environmental states?

- Concept: Inequity aversion and reciprocity
  - Why needed here: To create intrinsic rewards that encourage cooperation by making agents sensitive to fairness and the intentions behind others' actions.
  - Quick check question: What is the mathematical relationship between reward comparison and the intention weight in the kindness formula?

- Concept: Graph attention networks for multi-agent coordination
  - Why needed here: To aggregate and process information from neighboring agents in environments like traffic light control.
  - Quick check question: How does the GAT layer summarize neighborhood information for each agent's decision-making?

## Architecture Onboarding

- Component map:
  - EICM (Extended Intrinsic Curiosity Module) -> IA (Inequity Aversion) module -> GAT (Graph Attention Network) -> DQN/A3C/PPO

- Critical path:
  1. Observe current state and joint action history
  2. Use EICM forward model to generate counterfactual state predictions
  3. Compute intention scores based on counterfactual differences
  4. Calculate kindness-weighted reward comparisons
  5. Adjust intrinsic rewards and train policy/value networks

- Design tradeoffs:
  - Computational cost of counterfactual reasoning vs. accuracy of intention estimation
  - Granularity of counterfactual action sets (all actions vs. sampled actions)
  - Balance between extrinsic and intrinsic rewards (α and β hyperparameters)

- Failure signatures:
  - High variance in intention scores across training episodes
  - Policy collapse to selfish behavior despite kindness incentives
  - Slow convergence or divergence in reward comparison calculations

- First 3 experiments:
  1. Implement EICM forward model to predict counterfactual state encodings for a single agent
  2. Add intention computation and verify it correlates with expected environmental impact
  3. Integrate kindness-weighted reward comparisons into IA module and test on simple social dilemma

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- Performance depends heavily on the accuracy of counterfactual reasoning through the forward model
- Computational complexity scales with the number of agents and actions due to counterfactual predictions
- Limited evaluation to specific environments (Cleanup, Harvest, traffic light control) without broader generalization testing

## Confidence

- High confidence in the core mechanism of kindness-weighted reward comparisons
- Medium confidence in the generalizability of EICM-based counterfactual reasoning
- Low confidence in performance without the specific forward model architecture described

## Next Checks

1. Test KindMARL on a synthetic environment with known ground-truth intentions to verify counterfactual reasoning accuracy
2. Conduct ablation studies removing the EICM component to measure its contribution to performance gains
3. Evaluate robustness by introducing varying levels of environmental stochasticity and measuring intention estimation drift