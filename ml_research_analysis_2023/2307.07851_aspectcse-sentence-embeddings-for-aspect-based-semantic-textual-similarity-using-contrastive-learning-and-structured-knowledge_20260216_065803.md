---
ver: rpa2
title: 'AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity
  Using Contrastive Learning and Structured Knowledge'
arxiv_id: '2307.07851'
source_url: https://arxiv.org/abs/2307.07851
tags:
- sentence
- embeddings
- aspect
- industry
- aspects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AspectCSE, an approach for aspect-based contrastive
  learning of sentence embeddings, which achieves an average improvement of 3.97%
  on information retrieval tasks across multiple aspects compared to previous best
  results. The authors use Wikidata knowledge graph properties to train models of
  multi-aspect sentence embeddings, where multiple specific aspects are simultaneously
  considered during similarity predictions.
---

# AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity Using Contrastive Learning and Structured Knowledge

## Quick Facts
- arXiv ID: 2307.07851
- Source URL: https://arxiv.org/abs/2307.07851
- Reference count: 20
- Primary result: 3.97% average improvement on information retrieval tasks across multiple aspects

## Executive Summary
AspectCSE introduces a novel approach for training aspect-based sentence embeddings using contrastive learning and structured knowledge from Wikidata. The method trains multi-aspect embeddings simultaneously, achieving superior performance on aspect-specific information retrieval tasks compared to single-aspect approaches. The approach leverages Wikidata properties to generate aspect labels and uses a contrastive learning framework to create semantically meaningful sentence embeddings.

## Method Summary
AspectCSE uses Wikidata KG properties to generate aspect labels for training multi-aspect sentence embeddings. The approach employs contrastive learning with anchor-positive-negative triplets to train BERT-based models, using a temperature-scaled softmax loss function. The method fine-tunes models with specific configurations (batch size 14, learning rate 5e-5, max sequence length 320) and evaluates performance using precision@k, recall@k, and MRR metrics.

## Key Results
- Achieves 3.97% average improvement on information retrieval tasks across multiple aspects
- Multi-aspect embeddings outperform single-aspect embeddings on aspect-specific tasks
- Embeddings of semantically similar aspect labels cluster together in embedding space, even without explicit similarity training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning with multiple negatives improves aspect-specific sentence embedding quality
- Mechanism: Pulls together sentences sharing the same aspect label while pushing apart those with different aspect labels using temperature-scaled softmax loss
- Core assumption: Semantic similarity within an aspect is consistent enough to define reliable positive/negative pairs
- Evidence: Abstract reports 3.97% average improvement; section describes supervised contrastive learning framework

### Mechanism 2
- Claim: Multi-aspect embeddings trained jointly outperform separate single-aspect embeddings
- Mechanism: Training on multiple aspects simultaneously provides richer supervision signals and captures correlations between aspects
- Core assumption: Aspects in data are not independent and share useful correlations
- Evidence: Abstract states multi-aspect embeddings outperform single-aspect; section shows best MRR scores for country/industry using multi-aspect model

### Mechanism 3
- Claim: KG properties implicitly encode semantic similarity between aspect labels
- Mechanism: Labels sharing similar semantic contexts end up close in embedding space without explicit cross-label similarity training
- Core assumption: Data distribution and KG property usage naturally group semantically similar labels
- Evidence: Abstract mentions embeddings of semantically similar aspect labels are often close; section observes clustering of related industry labels

## Foundational Learning

- **Concept: Contrastive learning with supervised labels**
  - Why needed: Approach relies on having labeled positive and negative pairs to pull and push embeddings in vector space
  - Quick check: What happens to loss function if you accidentally label a negative as a positive?

- **Concept: Knowledge graph (KG) property extraction**
  - Why needed: Method uses Wikidata properties to generate aspect labels for training
  - Quick check: How would you handle missing KG properties for some entities during dataset construction?

- **Concept: Information retrieval evaluation metrics (P@k, R@k, MRR)**
  - Why needed: Paper frames aspect-based similarity as retrieval task and reports performance using these metrics
  - Quick check: Why might MRR be more informative than precision@k alone for evaluating embedding quality?

## Architecture Onboarding

- **Component map**: Wikidata KG queries -> Wikipedia text extraction -> Aspect-labeled dataset construction -> BERT-based encoder + AspectCSE contrastive loss -> Nearest neighbor retrieval + P@k, R@k, MRR metrics -> t-SNE visualization

- **Critical path**: 1) Construct labeled dataset from KG and Wikipedia, 2) Train aspect-specific sentence embeddings using AspectCSE, 3) Evaluate retrieval performance on held-out test set, 4) Visualize and analyze embedding space structure

- **Design tradeoffs**: Single-aspect vs. multi-aspect training (simpler vs. richer supervision); Union vs. Intersection multi-aspect (higher recall vs. stricter)

- **Failure signatures**: Poor retrieval scores despite high training accuracy (overfitting/noise), random-looking t-SNE plots (embedding collapse), slow training convergence (inappropriate learning rate/batch size)

- **First 3 experiments**: 1) Train single-aspect embeddings on country labels and evaluate retrieval, 2) Train multi-aspect (Union) embeddings and compare retrieval performance, 3) Visualize t-SNE plots for single- and multi-aspect embeddings to inspect label clustering

## Open Questions the Paper Calls Out

1. **How would AspectCSE perform on sentence-level text rather than paragraph-level text?**
   - Basis: Authors evaluated on entire paragraphs and acknowledge need for future investigation on sentence-level
   - Unresolved: No experiments conducted on sentence-level text
   - Resolution: Experiments comparing performance on sentence-level vs paragraph-level text using same metrics

2. **How would AspectCSE perform on underrepresented languages and domains where structured knowledge is sparse?**
   - Basis: Authors note that sparse Wikidata information in underrepresented areas negatively impacts AspectCSE
   - Unresolved: No experiments conducted on underrepresented languages/domains
   - Resolution: Experiments comparing performance across language/domain representation levels and studies on addressing knowledge sparsity

3. **How does choice of pooling method affect performance of AspectCSE?**
   - Basis: Authors follow Gao et al.'s claim that different pooling methods don't matter much, using CLS without empirical evidence
   - Unresolved: No comparison of different pooling methods (CLS, MEAN, MAX)
   - Resolution: Experiments comparing AspectCSE performance using different pooling methods with same evaluation metrics

## Limitations
- Weak external corpus validation of approach mechanisms
- Limited experimental evidence for handling missing KG properties
- No comprehensive comparison of different negative sampling strategies

## Confidence
- Mechanism 1: Medium confidence (abstract results but lacks corpus validation)
- Mechanism 2: Medium confidence (demonstrated on specific metrics but not externally validated)
- Mechanism 3: Medium confidence (based on specific observations but lacks broader corpus support)

## Next Checks
1. Implement and evaluate data pipeline for handling missing KG properties, measuring impact on final embedding quality
2. Conduct ablation studies comparing different negative sampling strategies in contrastive loss function
3. Replicate t-SNE analysis on held-out dataset to verify semantic similarity patterns between aspect labels