---
ver: rpa2
title: Contrastive Training of Complex-Valued Autoencoders for Object Discovery
arxiv_id: '2305.15001'
source_url: https://arxiv.org/abs/2305.15001
tags:
- ctcae
- phase
- objects
- learning
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles unsupervised object discovery in multi-object
  color datasets, where prior synchrony-based models struggled with color images and
  more than three objects. The authors propose two key innovations: architectural
  modifications (removing sigmoid activation and 1x1 convolutions, using convolution+upsample
  layers) and a novel contrastive learning objective.'
---

# Contrastive Training of Complex-Valued Autoencoders for Object Discovery

## Quick Facts
- arXiv ID: 2305.15001
- Source URL: https://arxiv.org/abs/2305.15001
- Authors: 
- Reference count: 40
- Key outcome: Contrastive training of complex-valued autoencoders enables object discovery in color images with up to six objects, achieving ARI-FG scores up to 0.84-0.85 on Tetrominoes, dSprites, and CLEVR datasets.

## Executive Summary
This work addresses unsupervised object discovery in multi-object color datasets, where prior synchrony-based models struggled with color images and scenes containing more than three objects. The authors propose two key innovations: architectural modifications (removing sigmoid activation and 1x1 convolutions, using convolution+upsample layers) and a novel contrastive learning objective. Their improved model, CtCAE, significantly outperforms both the baseline CAE and CAE++ on Tetrominoes, dSprites, and CLEVR datasets, achieving ARI-FG scores up to 0.84-0.85. CtCAE successfully handles color images and discovers up to six objects, whereas prior synchrony models were limited to grayscale images and three objects.

## Method Summary
The method combines complex-valued autoencoders with contrastive learning to enable unsupervised object discovery. The architectural modifications include removing the restrictive sigmoid output layer and 1x1 convolutions, and replacing transposed convolutions with convolution+upsample layers to better handle color images. The contrastive learning objective operates on complex-valued activations, using magnitude components to mine positive/negative pairs and phase components as features to contrast. This encourages better separability in phase space, where objects with similar features have similar phases while different objects have separated phases. The model is trained end-to-end with both reconstruction and contrastive objectives.

## Key Results
- CtCAE achieves ARI-FG scores of 0.84-0.85 on Tetrominoes, dSprites, and CLEVR datasets, significantly outperforming baseline CAE and CAE++ models
- The model successfully handles color images and discovers up to six objects per scene, advancing beyond the three-object limitation of prior synchrony models
- Contrastive learning provides consistent gains across all datasets, with phase map separability improvements measured by inter-cluster and intra-cluster distances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complex-valued activations store binding information in phase components, allowing flexible representation of varying numbers of objects without fixed slot counts
- Mechanism: Constructive and destructive interference between complex-valued activations causes objects with similar features to have similar phases while different objects have separated phases, enabling natural grouping through phase clustering
- Core assumption: Phase components can effectively encode object identity while magnitude components encode feature strength, and these representations can be learned through gradient descent
- Evidence anchors:
  - [abstract] "Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components"
  - [section] "CAE performs binding through complex-valued activations which transmit two types of messages: magnitudes of complex activations to represent the strength of a feature and phases to represent which features must be processed together"
  - [corpus] Weak - corpus contains related work on complex-valued models but lacks direct evidence for this specific binding mechanism
- Break condition: When phase interference patterns become too complex for gradient-based learning to disentangle, or when objects share too many features making phase separation impossible

### Mechanism 2
- Claim: Contrastive learning on phase-magnitude pairs improves separability in phase space by encouraging phase distance between objects with dissimilar visual properties
- Mechanism: Uses magnitude components as "addresses" to mine positive/negative pairs and phase components as "features" to contrast, where dissimilar magnitudes lead to increased phase separation
- Core assumption: Magnitude components capture sufficient visual similarity information to guide phase separation, and contrastive loss can effectively modulate phase relationships
- Evidence anchors:
  - [section] "We use the term addresses to refer to the representations used to measure similarity, and consequently extract positive and negative pairs w.r.t. to the anchor. We use the term features to refer to the representations that are contrasted... Since the magnitude components of complex-valued activations are used to reconstruct the image, they capture requisite visual properties of objects"
  - [section] "Our contrastive learning method increases or decreases the angular distance of phase components of points (pixels/image regions) in relation to how (dis)similar their visual properties are"
  - [corpus] Weak - corpus lacks direct evidence for this specific contrastive learning approach with complex-valued activations
- Break condition: When magnitude-based similarity measures fail to capture relevant visual differences, or when contrastive loss overwhelms reconstruction objectives

### Mechanism 3
- Claim: Architectural modifications (removing sigmoid activation and 1x1 convolutions, using convolution+upsample) enable color image processing and better feature extraction
- Mechanism: Removing restrictive output layer components allows full-range color values to be reconstructed, while convolution+upsample preserves spatial relationships better than transposed convolutions
- Core assumption: Standard autoencoder architectural choices that work well for real-valued networks need modification for complex-valued representations to handle color images effectively
- Evidence anchors:
  - [section] "We first propose some simple but crucial architectural modifications that enable the vanilla CAE [33] to achieve good grouping performance on multi-object datasets such as Tetrominoes with color images. These architectural modifications include — i) Remove the 1x1 convolution kernel and associated sigmoid activation in the output layer... ii) Use convolution and upsample layers in place of transposed convolution layers in the decoder"
  - [section] "We observe that the sigmoid activation on the output layer of the decoder significantly impedes learning on color datasets. A significant performance jump is also observed when replacing transposed convolution layers [33] with convolution and upsample layers"
  - [corpus] Moderate - corpus contains related architectural discussions but lacks direct evidence for these specific modifications
- Break condition: When modifications lead to unstable training or poor reconstruction quality that cannot be compensated by contrastive learning

## Foundational Learning

- Concept: Complex-valued neural networks and their mathematical properties
  - Why needed here: The entire binding mechanism relies on complex-valued activations and their interference properties
  - Quick check question: What is the difference between real and complex-valued neural networks in terms of parameter representation and activation computation?

- Concept: Contrastive learning principles and InfoNCE loss
  - Why needed here: The proposed method uses contrastive learning to improve phase separability, requiring understanding of how positive/negative pairs are mined and contrasted
  - Quick check question: How does the InfoNCE loss encourage representations to be similar for positive pairs and dissimilar for negative pairs?

- Concept: Object-centric learning and binding problem
  - Why needed here: The work addresses unsupervised object discovery, requiring understanding of why binding is challenging and how different approaches (slots vs synchrony) solve it
  - Quick check question: What are the key limitations of slot-based approaches that synchrony-based methods aim to address?

## Architecture Onboarding

- Component map: Input RGB image → Encoder (complex-valued conv layers) → Contrastive mining (magnitude-based) → Contrastive loss (phase-based) → Decoder (complex-valued conv+upsample) → Output reconstructed image + Phase map
- Critical path: Input → Encoder → Contrastive mining (magnitude-based) → Contrastive loss (phase-based) → Decoder → Reconstruction + Phase clustering
- Design tradeoffs:
  - Complex-valued vs real-valued: More expressive for binding but requires careful handling of interference patterns
  - Contrastive vs reconstruction-only: Better separability but additional computational overhead and hyperparameter tuning
  - Magnitude vs phase as features: Magnitude preserves visual information but phase enables binding; both needed for optimal performance
- Failure signatures:
  - Phase maps show poor separability despite contrastive training: Check magnitude-phase correlation, adjust contrastive loss temperature
  - Training instability with complex-valued layers: Verify weight initialization, consider gradient clipping
  - Poor reconstruction quality: Check architectural modifications, verify complex-valued operations are implemented correctly
- First 3 experiments:
  1. Train baseline CAE++ (no contrastive loss) on Tetrominoes with 32x32 resolution to verify architectural improvements
  2. Add contrastive loss to CAE++ on same dataset, tune temperature parameter to observe phase separation improvement
  3. Test generalization by training on 4-object subset of CLEVR and evaluating on 5-6 object scenes to verify capacity claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the contrastive learning objective's performance scale with different image resolutions and dataset complexities beyond the tested Tetrominoes, dSprites, and CLEVR?
- Basis in paper: [explicit] The authors note that their method consistently improves performance across datasets but acknowledge that their models still lag behind slot-based models, suggesting potential limitations in scaling to more complex datasets.
- Why unresolved: The paper only tests on three relatively simple datasets with varying resolutions (32x32, 64x64, 96x96), and does not explore how the contrastive objective performs on more complex datasets with higher resolution images or more objects per scene.
- What evidence would resolve it: Training and evaluating CtCAE on more complex datasets like CLEVR-CoGenT or real-world datasets with higher resolution images and more objects per scene would provide insights into the method's scalability and potential limitations.

### Open Question 2
- Question: What is the impact of different architectural choices for the encoder and decoder on the final grouping performance, beyond the modifications tested in the paper?
- Basis in paper: [explicit] The authors propose specific architectural modifications (removing sigmoid activation and 1x1 convolutions, using convolution+upsample layers) that improve CAE performance, but do not exhaustively explore all possible architectural variations.
- Why unresolved: The paper focuses on a specific set of modifications and their impact, but does not explore the full space of potential architectural variations for the encoder and decoder that could further improve performance.
- What evidence would resolve it: Systematically testing different architectural choices for the encoder and decoder, such as varying the number of layers, kernel sizes, activation functions, and normalization techniques, would provide a more comprehensive understanding of their impact on grouping performance.

### Open Question 3
- Question: How does the contrastive learning objective interact with the phase separation process in complex-valued autoencoders, and what are the underlying mechanisms driving its effectiveness?
- Basis in paper: [explicit] The authors introduce a novel contrastive learning objective that operates on complex-valued activations, using magnitude components to mine positive/negative pairs and phase components as features to contrast. They observe improved separability in phase space, but do not provide a detailed analysis of the underlying mechanisms.
- Why unresolved: The paper demonstrates the effectiveness of the contrastive objective but does not provide a thorough theoretical or empirical analysis of how it interacts with the phase separation process and why it leads to improved separability.
- What evidence would resolve it: Conducting a detailed analysis of the phase separation process, such as visualizing the phase space before and after contrastive training, measuring the angular distances between phase values of different objects, and exploring the relationship between the contrastive objective and the phase separation dynamics, would provide insights into the underlying mechanisms.

## Limitations
- The method still underperforms compared to slot-based approaches, suggesting fundamental limitations in the synchrony-based binding mechanism
- Complex-valued representations may not generalize well beyond synthetic datasets to more complex real-world scenarios
- The contrastive objective requires careful hyperparameter tuning and may not scale well to scenes with many objects

## Confidence
- **High Confidence**: Architectural modifications (removing sigmoid, using convolution+upsample) demonstrably improve color image processing
- **Medium Confidence**: Contrastive learning consistently improves phase separability across datasets, though the magnitude-phase relationship's robustness needs further validation
- **Low Confidence**: Claims about scaling to arbitrary numbers of objects without architectural changes, as testing was limited to six objects maximum

## Next Checks
1. Test CtCAE on real-world multi-object datasets (e.g., COCO, KITTI) to assess generalization beyond synthetic data
2. Systematically ablate the magnitude-phase relationship by training with random magnitude inputs to verify the contrastive learning mechanism's dependence on visual similarity
3. Scale the model to scenes with 10+ objects to identify the practical limits of phase-based binding and determine if architectural modifications are needed for larger object counts