---
ver: rpa2
title: "\xDAFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference\
  \ Resolution"
arxiv_id: '2311.14391'
source_url: https://arxiv.org/abs/2311.14391
tags:
- coreference
- context
- mention
- multilingual
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CorPipe achieves state-of-the-art results in the CRAC 2023 Shared
  Task on Multilingual Coreference Resolution, surpassing other participants by a
  large margin of 4.5 percent points. The system employs a multilingual coreference
  pipeline with mention detection followed by coreference linking using an antecedent-maximization
  approach.
---

# ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference Resolution

## Quick Facts
- arXiv ID: 2311.14391
- Source URL: https://arxiv.org/abs/2311.14391
- Reference count: 6
- Primary result: State-of-the-art performance in CRAC 2023 Shared Task with 4.5 percentage points improvement

## Executive Summary
ÚFAL CorPipe achieves state-of-the-art results in the CRAC 2023 Shared Task on Multilingual Coreference Resolution, surpassing other participants by a large margin of 4.5 percentage points. The system employs a multilingual coreference pipeline with mention detection followed by coreference linking using an antecedent-maximization approach. Key improvements include using larger input sizes (up to 2560 subwords) and implementing ensembling for better performance. Trained on 17 corpora in 12 languages, CorPipe achieves an overall score of 74.90% and outperforms all other submissions on individual corpora.

## Method Summary
CorPipe uses a multilingual coreference pipeline with mention detection and coreference linking as separate stages. The system employs mT5-xl as the base model with context sizes up to 2560 subwords, uses constrained decoding with depth-independent tags, and creates ensembles from three checkpoints per corpus. The model is trained jointly on 17 corpora across 12 languages using label smoothing, Adafactor optimizer, and a slanted triangular learning schedule.

## Key Results
- Achieves overall CoNLL score of 74.90% on 17 corpora in 12 languages
- Outperforms all other CRAC 2023 submissions by 4.5 percentage points
- Larger context sizes (2560 vs 512 subwords) consistently improve performance
- Ensembling three checkpoints provides additional performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing context size from 512 to 2560 subwords improves coreference resolution performance.
- Mechanism: Larger context allows the model to capture longer-range dependencies and disambiguate mentions that span beyond the original 512 subword limit.
- Core assumption: Coreference resolution benefits from seeing more surrounding text to correctly identify and link mentions across longer distances.
- Evidence anchors: [abstract] "Our main improvements comprise inputs larger than 512 subwords"; [section 5.1] "The performance improves consistently with increasing context size up to 2560"
- Break condition: If the model's attention mechanism cannot effectively process the increased context, or if longer contexts introduce noise that outweighs the benefits.

### Mechanism 2
- Claim: Using larger pretrained models (mT5-xl vs mT5-large) provides performance gains.
- Mechanism: Larger models have more parameters and capacity to learn complex patterns in multilingual coreference resolution.
- Core assumption: Model capacity directly correlates with performance on complex multilingual tasks.
- Evidence anchors: [abstract] "using larger input sizes (up to 2560 subwords) and implementing ensembling"; [section 5.1] "As expected, the increasingly bigger mT5 models improve the performance"
- Break condition: If the larger model overfits to the training data or if additional parameters do not contribute meaningfully to the coreference task.

### Mechanism 3
- Claim: Constrained decoding with depth-independent tags improves mention detection accuracy compared to greedy decoding.
- Mechanism: Constrained decoding ensures that predicted tag sequences are well-formed (balanced), avoiding invalid mention structures.
- Core assumption: Well-formed tag sequences lead to better mention detection and, consequently, better coreference resolution.
- Evidence anchors: [section 3.2] "we propose to replace the CRF with per-token classification during training and perform a constrained dynamic programming decoding during inference"; [section 5.2] "Using conditional random fields for mention decoding provides marginally worse performance compared to using constrained decoding with depth-independent tags"
- Break condition: If constraints imposed by decoding algorithm are too restrictive and prevent finding optimal mention structures.

## Foundational Learning

- Concept: Multilingual pretraining and its impact on downstream tasks
  - Why needed here: Understanding how multilingual models like mT5 are pretrained and how this affects their ability to handle coreference across languages
  - Quick check question: What are the key differences between monolingual and multilingual pretraining, and how might these differences affect coreference resolution performance?

- Concept: Coreference resolution as a two-stage process (mention detection + linking)
  - Why needed here: The paper's approach explicitly separates these stages, and understanding this separation is crucial for understanding the system's architecture
  - Quick check question: What are the advantages and disadvantages of separating mention detection from coreference linking compared to end-to-end approaches?

- Concept: Ensemble methods and their impact on model performance
  - Why needed here: The paper employs ensembling to improve performance, and understanding how ensembling works is important for understanding the system's design
  - Quick check question: How does ensembling multiple models typically affect performance, and what are the potential drawbacks of this approach?

## Architecture Onboarding

- Component map: Pretrained mT5 model → Mention detection head → Coreference linking head → Constrained decoding → Ensemble of models
- Critical path: Input text → mT5 encoder → Mention detection → Coreference linking → Output clusters
- Design tradeoffs: Larger context sizes improve performance but increase computational cost; larger models improve performance but require more resources; constrained decoding ensures well-formed outputs but may be slower than greedy decoding
- Failure signatures: Poor performance on languages with less training data; failure to resolve coreference across sentence boundaries; invalid mention structures in output
- First 3 experiments:
  1. Test the effect of different context sizes (512, 1024, 2560) on a single language corpus to validate the importance of context
  2. Compare the performance of different pretrained models (mT5-small, mT5-base, mT5-large, mT5-xl) on a multilingual benchmark
  3. Evaluate the impact of constrained decoding versus greedy decoding on mention detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CorPipe compare to end-to-end coreference resolution models like Bohnet et al. (2023)?
- Basis in paper: [explicit] The paper mentions that a direct comparison with Bohnet et al. (2023) would be interesting but does not provide one.
- Why unresolved: The paper does not include a direct comparison with end-to-end coreference resolution models.
- What evidence would resolve it: Conducting experiments to compare CorPipe's performance with end-to-end models like Bohnet et al. (2023) on the same datasets.

### Open Question 2
- Question: What is the impact of using larger pretrained language models on the performance of CorPipe?
- Basis in paper: [inferred] The paper mentions that larger models consistently deliver better performance in various applications and that CorPipe uses the largest possible pretrained multilingual model, mT5-xl.
- Why unresolved: The paper does not provide a detailed analysis of the impact of using larger models on CorPipe's performance.
- What evidence would resolve it: Conducting experiments to compare the performance of CorPipe using different sizes of pretrained models, such as mT5-large and mT5-xl.

### Open Question 3
- Question: How does the performance of CorPipe change when using different mix ratios for multilingual training data?
- Basis in paper: [explicit] The paper discusses the effect of various mix ratios during all-corpora training but does not provide a comprehensive analysis of their impact on performance.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different mix ratios on CorPipe's performance.
- What evidence would resolve it: Conducting experiments to compare the performance of CorPipe using different mix ratios for multilingual training data.

## Limitations

- The system was tested only on the CorefUD 1.1 dataset, which may not fully represent real-world diversity across the 12 languages
- Constrained decoding adds computational overhead during inference, with no quantification of trade-offs
- The ensemble approach using three checkpoints per corpus adds complexity and storage requirements that may not be practical for all deployment scenarios

## Confidence

**High Confidence:**
- The overall system achieves state-of-the-art performance on the CRAC 2023 Shared Task
- Larger context sizes (up to 2560 subwords) consistently improve performance across multiple corpora
- mT5-xl provides better performance than mT5-large for this task
- Constrained decoding with depth-independent tags provides marginally better performance than greedy decoding

**Medium Confidence:**
- The combination of all improvements is optimal for this task
- The performance gains will generalize to other multilingual coreference datasets beyond CorefUD
- The computational overhead of constrained decoding and ensembling is justified by performance gains

**Low Confidence:**
- The system will maintain the same performance advantage on non-UD based datasets
- The model's performance on low-resource languages within the 12-language set is equally strong as on high-resource languages

## Next Checks

1. **Ablation Study Validation**: Conduct controlled experiments isolating each improvement (context size, model size, decoding method, ensembling) to quantify their individual contributions to overall performance gain.

2. **Cross-Dataset Generalization Test**: Evaluate the pretrained CorPipe model on an independent multilingual coreference dataset not derived from Universal Dependencies to assess real-world generalization.

3. **Computational Efficiency Analysis**: Measure inference time and memory usage for different configurations (greedy vs constrained decoding, single vs ensemble models) to quantify practical trade-offs between accuracy and efficiency.