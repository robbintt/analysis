---
ver: rpa2
title: 'LLMs and Finetuning: Benchmarking cross-domain performance for hate speech
  detection'
arxiv_id: '2310.18964'
source_url: https://arxiv.org/abs/2310.18964
tags:
- hate
- speech
- detection
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates Large Language Models (LLMs) for hate speech
  detection across multiple domains, addressing the challenge of cross-domain generalization.
  By fine-tuning LLaMA-7B and Vicuna-7B models using QLoRA on hate speech datasets
  and employing binary classifiers on the models' embeddings, the research benchmarks
  in-domain and cross-domain performance.
---

# LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection

## Quick Facts
- arXiv ID: 2310.18964
- Source URL: https://arxiv.org/abs/2310.18964
- Authors: 
- Reference count: 9
- Primary result: Fine-tuned LLMs achieve up to 19% higher F1 scores than previous state-of-the-art models for hate speech detection across multiple domains

## Executive Summary
This study evaluates Large Language Models (LLMs) for hate speech detection across multiple domains, addressing the challenge of cross-domain generalization. By fine-tuning LLaMA-7B and Vicuna-7B models using QLoRA on hate speech datasets and employing binary classifiers on the models' embeddings, the research benchmarks in-domain and cross-domain performance. Results show that LLMs significantly outperform previous state-of-the-art models, with F1 scores improving by up to 19% in some cases. Fine-tuning enhances cross-domain adaptability, particularly for datasets with diverse label heterogeneity.

## Method Summary
The study fine-tunes LLaMA-7B and Vicuna-7B models using QLoRA on nine publicly available hate speech datasets. The datasets are converted to binary classification format (hateful vs. non-hateful) and instruction-formatted using Alpaca format. Binary classifiers are trained on the last hidden layer embeddings from each LLM, and performance is evaluated both in-domain and cross-domain using F1 scores.

## Key Results
- LLMs significantly outperform previous state-of-the-art models, with F1 scores improving by up to 19% in some cases
- Fine-tuning enhances cross-domain adaptability, particularly for datasets with diverse label heterogeneity
- Models fine-tuned on Gab, the dataset with the most positive cases of hate speech, offered the best generalizability to other datasets

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLMs on diverse hate speech datasets improves cross-domain generalizability more than in-domain training alone. Instruction fine-tuning injects domain-specific label heterogeneity into the model, allowing it to recognize nuanced variations of hate speech across different platforms and cultural contexts. Core assumption: Label heterogeneity in training data directly correlates with improved model performance on unseen datasets. Break condition: If training datasets lack sufficient label diversity, fine-tuning does not improve cross-domain performance.

### Mechanism 2
Using LLMs as feature extractors with a binary classifier layer significantly improves hate speech detection accuracy compared to traditional models. LLMs capture complex semantic relationships and contextual embeddings that traditional models miss, leading to higher precision and recall. Core assumption: The last hidden layer embeddings from LLMs contain sufficient discriminative information for hate speech classification. Break condition: If embeddings are too generic or not task-specific, classification accuracy does not improve.

### Mechanism 3
Cross-domain performance is maximized when models are fine-tuned on datasets with high proportions of positive hate speech samples. Exposure to diverse hate speech patterns during training increases model robustness when applied to different platforms with varying hate speech manifestations. Core assumption: Datasets with higher positive sample ratios provide better generalization capabilities. Break condition: If positive samples are not representative of diverse hate speech types, generalization fails.

## Foundational Learning

- Concept: Cross-domain classification
  - Why needed here: Understanding how models trained on one dataset perform on different datasets is central to evaluating hate speech detection robustness
  - Quick check question: What is the difference between in-domain and cross-domain performance?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The study relies on instruction-based fine-tuning to adapt LLMs to hate speech detection, requiring knowledge of how transfer learning works
  - Quick check question: How does fine-tuning differ from training a model from scratch?

- Concept: Embedding extraction and binary classification
  - Why needed here: The pipeline uses LLM embeddings as input to a binary classifier, necessitating understanding of feature extraction and downstream classification
  - Quick check question: What role do embeddings play in the classification pipeline?

## Architecture Onboarding

- Component map: Input text -> Instruction formatting -> Backbone LLM -> Embedding extraction -> Binary classifier -> Output label
- Critical path: Backbone LLM fine-tuning -> Binary classifier training -> Evaluation on test datasets
- Design tradeoffs: Fine-tuning vs. using base models (accuracy vs. computational cost), dataset size vs. label diversity
- Failure signatures: Overfitting on training data, poor cross-domain performance, high variance in F1 scores
- First 3 experiments:
  1. Fine-tune LLaMA-7B on Gab dataset and evaluate on same dataset (in-domain)
  2. Use Vicuna-7B-finetuned on Reddit to classify Gab dataset (cross-domain)
  3. Compare F1 scores of base vs. fine-tuned models across all datasets

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning on datasets with diverse label heterogeneity consistently improve cross-domain generalization performance across all LLM architectures? The paper provides evidence of improvement in cross-domain performance through fine-tuning, but does not explicitly test whether this improvement is consistent across all LLM architectures or if it varies based on the diversity of label heterogeneity in the training datasets. Comparative studies testing different LLM architectures (e.g., LLaMA vs. Vicuna) on datasets with varying levels of label heterogeneity, measuring cross-domain performance before and after fine-tuning, would resolve this question.

### Open Question 2
How do sociocultural contexts influence the effectiveness of fine-tuned LLMs in detecting hate speech across different platforms? While the paper acknowledges the potential impact of sociocultural contexts on model performance, it does not provide empirical evidence or detailed analysis on how these contexts specifically affect the effectiveness of fine-tuned LLMs in different platforms or regions. Experimental studies comparing the performance of fine-tuned LLMs across different sociocultural contexts and platforms, with detailed analysis of model predictions in relation to cultural norms and conventions, would resolve this question.

### Open Question 3
Can semi-supervised methods using AI-inferred labels achieve similar performance to supervised fine-tuning for hate speech detection? The paper suggests an interest in exploring semi-supervised methods but does not provide any experimental results or analysis on their effectiveness compared to supervised fine-tuning. Comparative studies evaluating the performance of models fine-tuned using semi-supervised methods versus those fine-tuned using traditional supervised methods, with metrics such as F1 score and cross-domain generalization ability, would resolve this question.

## Limitations
- The analysis does not adequately address the impact of dataset-specific preprocessing decisions, particularly how hierarchical annotation schemes were collapsed into binary classifications
- The computational efficiency trade-offs between fine-tuning approaches and feature extraction methods are not quantified
- The study focuses on English-language datasets, limiting generalizability to multilingual contexts where hate speech manifests differently

## Confidence
- High confidence: Core finding that fine-tuned LLMs outperform traditional models for hate speech detection
- Medium confidence: Cross-domain generalization claims, as results show variability across dataset pairs
- Low confidence: Practical deployment recommendations due to lack of computational cost analysis

## Next Checks
1. Reconstruct the exact preprocessing pipeline used to convert hierarchical hate speech annotations into binary format across all nine datasets, then measure how sensitive model performance is to different binarization thresholds
2. Measure and compare the inference time and memory requirements for three approaches: base LLM feature extraction, fine-tuned LLM classification, and traditional transformer models across identical hardware configurations
3. Fine-tune the best-performing LLM on an English hate speech dataset, then evaluate its ability to detect hate speech in non-English datasets from the same domains to test language-agnostic generalization