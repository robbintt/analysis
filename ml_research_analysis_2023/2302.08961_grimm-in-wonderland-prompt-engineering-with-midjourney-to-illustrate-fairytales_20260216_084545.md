---
ver: rpa2
title: 'Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales'
arxiv_id: '2302.08961'
source_url: https://arxiv.org/abs/2302.08961
tags:
- prompt
- image
- original
- generation
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates a successful methodology for generating
  believable illustrations of fairytales using text-to-image generation and prompt
  engineering. Through iterative action research with Midjourney v4, the authors develop
  a 4-stage process for converting pre-existing text into effective prompts.
---

# Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales

## Quick Facts
- arXiv ID: 2302.08961
- Source URL: https://arxiv.org/abs/2302.08961
- Authors: 
- Reference count: 18
- Key outcome: Successful methodology for generating believable fairytale illustrations using text-to-image generation and prompt engineering

## Executive Summary
This study presents a successful methodology for generating believable illustrations of fairytales using text-to-image generation and prompt engineering. Through iterative action research with Midjourney v4, the authors develop a 4-stage process for converting pre-existing text into effective prompts. The process involves initial prompt creation from source text, composition adjustment to refine ambiguous elements, style refinement to suppress hallucinations, and variation selection to optimize outputs. The study successfully generates at least 5 believable illustrations for each of 5 popular fairytales.

## Method Summary
The research employs iterative action research with Midjourney v4 to develop a 4-stage prompt engineering process: initial prompt creation from source text, composition adjustment to refine ambiguous elements, style refinement to suppress hallucinations, and variation selection to optimize outputs. The methodology was tested on 5 popular fairytales (Little Red Riding Hood, Cinderella, Hansel and Gretel, Faithful Johannes, Little Snow White), generating at least 5 believable illustrations for each story through systematic prompt refinement.

## Key Results
- Successfully generated at least 5 believable illustrations for each of 5 popular fairytales
- Identified three key challenges for text-to-image models: difficulties with precise counts, bias toward stereotypical configurations, and inability to depict non-realistic scenarios
- Developed a 4-stage prompt engineering process that is generalizable to future models beyond Midjourney

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 4-stage prompt engineering process reliably converts pre-existing text into believable illustrations by iterative refinement.
- Mechanism: Starting from source text, prompts are simplified into sentence-like structures, then adjusted compositionally to fix ambiguities, stylistically refined to suppress hallucinations, and finally optimized through variation selection. Each stage addresses a distinct failure mode of the model.
- Core assumption: Iterative small adjustments in prompt wording yield more predictable model outputs than large rewrites, and diffusion models like Midjourney can benefit from style constraints to limit unwanted detail.
- Evidence anchors:
  - [abstract] "We arrive at a tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii) style refinement, and iv) variation selection."
  - [section 4] "1. Initial prompt Start with a prompt closely representing the original text... 2. Composition adjustment Refine prompt step-wise... 3. Style refinement Whenever superfluous hallucination... 4. Variation selection Once the desired composition is reached..."
  - [corpus] Weak - the corpus papers do not explicitly discuss this specific 4-stage process, though some mention prompt refinement or style adjustments.

### Mechanism 2
- Claim: The iterative action research approach contextualizes prompt engineering within specific texts, producing generalizable methodology rather than model-specific tricks.
- Mechanism: Researchers engage directly with the task, reflecting on each iteration to develop theory about why certain prompts fail or succeed. This builds a process-based knowledge base that is transferable to other models.
- Core assumption: Process knowledge (how to iterate) is more transferable than phenomenology (specific prompt tricks) when dealing with black-box models.
- Evidence anchors:
  - [abstract] "Using Midjourney v4, we engage in action research with a dual aim... to define a prompt engineering process that starts from a pre-existing text and arrives at an illustration of it."
  - [section 3] "Due to inherently complex processes, working with a black-box phenomena is very common... Thus, we propose that an iterative action research approach could produce knowledge that is more directly transferable across models..."
  - [corpus] Weak - the corpus focuses on automated prompt generation or model-specific studies, not on action research methodology.

### Mechanism 3
- Claim: The model struggles with certain prompts due to three identified failure modes: difficulties with counts, bias from stereotypical configurations, and inability to depict non-realistic scenarios.
- Mechanism: These failure modes are inherent to training on real-world image datasets and prior biases. The authors observe these as recurring patterns across different fairytales and test them systematically.
- Core assumption: Text-to-image models inherit statistical biases from their training data, leading to predictable errors like wrong object counts or stereotypical scene elements.
- Evidence anchors:
  - [abstract] "We also discuss three reasons why the generation model struggles with certain illustrations: difficulties with counts, bias from stereotypical configurations and inability to depict overly fantastic situations."
  - [section 4] "There first reason we identify is the difficulty to cause the model to generate a specific number of similar objects... The second hypothesis is a presumed difficulty to generate scenes, different from a dominant stereotypical view..."
  - [corpus] Weak - the corpus papers mention prompt sensitivity and bias, but do not explicitly list these three specific failure modes.

## Foundational Learning

- Concept: Action research methodology
  - Why needed here: The black-box nature of text-to-image models means that systematic reflection on iterative trials yields transferable insights, rather than one-off fixes.
  - Quick check question: What is the main difference between action research and traditional controlled experiments in this context?

- Concept: Prompt engineering basics
  - Why needed here: Understanding how subject terms, style modifiers, and image prompts interact is essential for building effective prompts that the model can interpret.
  - Quick check question: How do style modifiers like "simple" or "minimal" influence the level of detail in the output?

- Concept: Diffusion model behavior
  - Why needed here: Recognizing that diffusion models generate images through denoising processes helps explain why small prompt changes can have large effects and why variation selection is valuable.
  - Quick check question: Why might a diffusion model produce different outputs for semantically similar prompts?

## Architecture Onboarding

- Component map: Text snippet → Initial prompt → Compositional adjustments → Style refinement → Variation selection → Believable illustration
- Critical path: Initial text → Simplified sentence prompt → Iterative compositional adjustments → Style refinement → Variation selection → Believable illustration
- Design tradeoffs:
  - Simplicity vs. richness: Simplifying text for precision may lose narrative nuance
  - Style constraints vs. creativity: Enforcing "simple" style may suppress interesting details
  - Iterations vs. efficiency: Multiple prompt adjustments increase reliability but also time cost
- Failure signatures:
  - Wrong object counts → Indicates model bias toward stereotypical configurations
  - Persistent irrelevant elements (e.g., trees on graves) → Shows prior bias in training data
  - Non-realistic scenarios fail → Reveals dataset limitations for impossible scenes
- First 3 experiments:
  1. Generate a prompt for a simple fairytale scene (e.g., "Little Red Riding Hood walks through the woods") and iterate with small compositional adjustments.
  2. Test style refinement by adding "minimal" or "flatcolor" modifiers and observe changes in hallucination levels.
  3. Use the variation selection step on a near-successful prompt to see if it improves object counts or composition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed 4-stage prompt engineering process be successfully applied to other text-to-image generation models beyond Midjourney v4?
- Basis in paper: [explicit] The authors state "our tentative four-staged process was developed and tested with Midjourney v4, we have kept it generic enough to be applicable also to other current generation models and – most importantly – future ones to come."
- Why unresolved: The methodology has only been tested on Midjourney v4, and the authors acknowledge that different models have different architectures and capabilities. The generalizability of the process to other models remains theoretical.
- What evidence would resolve it: Successful application of the 4-stage process to at least 3 different text-to-image models (e.g., DALL-E 2, Stable Diffusion, Parti) generating believable illustrations for the same fairytales.

### Open Question 2
- Question: What specific limitations do text-to-image generation models have in representing precise counts of objects, and how can these be overcome?
- Basis in paper: [explicit] The authors identify "difficulties with counts" as one of three key challenges, noting that "a well-known issue among practitioners is the difficulty to draw e.g. hands, often getting a wrong number of fingers."
- Why unresolved: While the paper identifies this as a limitation, it does not investigate the underlying causes or propose solutions beyond iterative prompt refinement.
- What evidence would resolve it: A systematic study comparing model performance on prompts with varying counts (1, 2, 3, 5, 7, 10 objects) and analysis of architectural features that contribute to count representation errors.

### Open Question 3
- Question: How can text-to-image models be improved to better handle non-realistic or "impossible scenes" without relying on fine-tuning?
- Basis in paper: [explicit] The authors note "inability to depict overly fantastic situations" as a key challenge, observing that "even when the references of 'Cinderella' and 'mother's' are removed, the model continues to produce a tree" where it shouldn't.
- Why unresolved: The paper identifies this as a limitation but does not explore whether it stems from training data biases, model architecture, or prompt engineering limitations.
- What evidence would resolve it: Development and testing of novel prompt engineering techniques or model architectures that successfully generate non-realistic scenes while maintaining believability and avoiding stereotypical configurations.

## Limitations
- The specific configurations and exact prompts used with Midjourney v4 are not provided, making exact replication impossible.
- The criteria for "believable" illustrations are subjective and not objectively defined, introducing potential bias in assessment.
- The corpus analysis reveals no prior studies using this exact methodology, suggesting limited validation against existing approaches.

## Confidence

- **High**: The identification of three specific failure modes (count difficulties, stereotypical bias, non-realistic scenario limitations) is well-supported by the text and represents a meaningful contribution to understanding model limitations.
- **Medium**: The 4-stage prompt engineering process is clearly articulated and appears logically sound, but lacks direct empirical validation beyond the case study.
- **Low**: Claims about the generalizability of the methodology to other models are speculative without comparative testing across different architectures.

## Next Checks

1. **Replicate with alternative models**: Test the 4-stage process with different text-to-image models (e.g., DALL-E, Stable Diffusion) to assess generalizability beyond Midjourney v4.
2. **Objective believability metrics**: Develop and apply quantitative metrics for illustration believability (e.g., human evaluation studies, feature similarity measures) rather than relying on subjective assessment.
3. **Comparative prompt engineering**: Compare the action research approach against automated prompt generation methods to evaluate whether the iterative human-in-the-loop process provides measurable advantages.