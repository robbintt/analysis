---
ver: rpa2
title: An investigation of phrase break prediction in an End-to-End TTS system
arxiv_id: '2304.04157'
source_url: https://arxiv.org/abs/2304.04157
tags:
- phrase
- phrasing
- speech
- break
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates whether external phrase break prediction
  models can improve listener comprehension in End-to-End TTS systems. Two approaches
  are explored: (1) a bidirectional LSTM model with task-specific embeddings trained
  from scratch, and (2) a pre-trained BERT model fine-tuned on phrase break prediction.'
---

# An investigation of phrase break prediction in an End-to-End TTS system

## Quick Facts
- arXiv ID: 2304.04157
- Source URL: https://arxiv.org/abs/2304.04157
- Authors: 
- Reference count: 40
- One-line primary result: Listener comprehension improves when external phrase break prediction models are used in End-to-End TTS systems.

## Executive Summary
This paper investigates whether external phrase break prediction models can improve listener comprehension in End-to-End TTS systems. Two approaches are explored: a bidirectional LSTM model with task-specific embeddings trained from scratch, and a pre-trained BERT model fine-tuned on phrase break prediction. Both models are trained on a multi-speaker English corpus to predict phrase break locations in text. The End-to-End TTS system comprises a Tacotron2 model with Dynamic Convolutional Attention for mel spectrogram prediction and a WaveRNN vocoder for waveform generation. Subjective listening tests show a clear preference for text synthesized with predicted phrase breaks over text synthesized without them, confirming the value of incorporating external phrasing models within End-to-End TTS to enhance listener comprehension.

## Method Summary
The method involves training two phrase break prediction models on a multi-speaker English corpus (LibriTTS dataset). The first is a bidirectional LSTM model with task-specific static word embeddings trained from scratch, and the second is a pre-trained BERT model fine-tuned on phrase break prediction. These models are used to punctuate text, which is then synthesized using a Tacotron2 model with Dynamic Convolutional Attention and a WaveRNN vocoder. Listener comprehension is evaluated through subjective listening tests (pairwise ABX task) comparing text synthesized with and without predicted phrase breaks.

## Key Results
- BERT fine-tuning on phrase break prediction outperforms BLSTM models using task-specific embeddings
- Listener comprehension improves when TTS systems use external phrasing models for unpunctuated text
- Multi-speaker training produces generic, speaker-independent phrase break models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT fine-tuning on phrase break prediction outperforms BLSTM models.
- Mechanism: Pre-trained BERT embeddings capture rich contextual linguistic features that are fine-tuned for phrase break labeling, improving accuracy over task-specific embeddings.
- Core assumption: BERT's general linguistic knowledge transfers effectively to phrase break prediction.
- Evidence anchors:
  - [abstract] The listening tests show a clear preference for text synthesized with predicted phrase breaks over text synthesized without them. Conclusion: These results confirm the value of incorporating external phrasing models within End-to-End TTS to enhance listener comprehension.
  - [section] An examination of the results shows that fine-tuning a pretrained BERT model with an additional token classification layer outperforms a BLSTM token classification model using task-specific static word embeddings trained from scratch.
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.158, average citations=0.0.
- Break condition: When linguistic cues like POS tags, syntactic boundaries, and context are not well-represented in static embeddings.

### Mechanism 2
- Claim: Listener comprehension improves when TTS systems use external phrasing models.
- Mechanism: Pauses predicted by phrase break models lead to more intelligible and natural-sounding speech, especially for unpunctuated text like children's stories.
- Core assumption: Pause placement in TTS affects perceived clarity and comprehension.
- Evidence anchors:
  - [abstract] The listening tests show a clear preference for text synthesized with predicted phrase breaks over text synthesized without them.
  - [section] In order to evaluate children's stories synthesized using the three scenarios, we performed subjective listening tests setup as pairwise ABX tasks and showed that there is clear preference for text punctuated using a trained phrasing model before being synthesized, over text directly synthesized without predicting the location of commas.
  - [corpus] Top related titles: Duration-aware pause insertion using pre-trained language model for multi-speaker text-to-speech.
- Break condition: When input text lacks natural punctuation cues.

### Mechanism 3
- Claim: Multi-speaker training produces generic, speaker-independent phrase break models.
- Mechanism: Using LibriTTS, which contains diverse speakers, enables phrase break models to generalize across speakers rather than overfitting to a single voice.
- Core assumption: Phrasing patterns vary by speaker but have enough common structure to learn from multi-speaker data.
- Evidence anchors:
  - [section] These alignments provide the locations of pauses introduced by the speaker while recording the utterances, which were used to train the phrasing models. Since the LibriTTS dataset is a multi-speaker dataset, our phrasing models trained using this dataset are generic speaker independent phrasing models for English.
  - [abstract] Both models are trained on a multi-speaker English corpus to predict phrase break locations in text.
  - [corpus] Weak evidence—corpus neighbors focus on phrase break prediction but not specifically on multi-speaker generalization.
- Break condition: When a TTS system must adapt to different speakers without retraining phrasing models.

## Foundational Learning

- Concept: Phrase break prediction
  - Why needed here: Essential for inserting natural pauses that improve speech intelligibility in TTS.
  - Quick check question: What is the main linguistic cue used in this work to indicate phrase breaks?

- Concept: Pre-trained language models (BERT)
  - Why needed here: Provide rich contextual embeddings that improve prediction accuracy when fine-tuned.
  - Quick check question: Why might BERT embeddings outperform static word embeddings for this task?

- Concept: Listener comprehension testing
  - Why needed here: Objective measure of whether phrasing models actually improve perceived speech quality.
  - Quick check question: What evaluation method was used to compare phrasing model effectiveness?

## Architecture Onboarding

- Component map:
  Text → Phrasing Model → Tacotron2 → WaveRNN → Audio → Listener Evaluation

- Critical path:
  Text → Phrasing Model → Tacotron2 → WaveRNN → Audio → Listener Evaluation

- Design tradeoffs:
  - Pre-trained vs. task-specific embeddings: BERT provides better accuracy but requires fine-tuning; task-specific embeddings are lighter but less effective.
  - Speaker-independent vs. speaker-dependent phrasing: Multi-speaker models generalize but may miss speaker-specific nuances.
  - Greedy inference vs. beam search: Greedy is faster but may not always choose the optimal break sequence.

- Failure signatures:
  - Over-punctuation leading to unnatural pauses
  - Under-punctuation causing speech run-on
  - Phrasing model bias toward common patterns missing rare cases
  - Listener confusion when pauses conflict with natural rhythm

- First 3 experiments:
  1. Compare BLSTM and BERT phrasing models on held-out test data using F-measure.
  2. Evaluate listener comprehension with ABX tests between no-phrasing and BERT-phrasing synthesis.
  3. Test generalization by applying models to unpunctuated children's stories from a different corpus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating external phrase break prediction models affect listener comprehension in end-to-end TTS systems for different languages?
- Basis in paper: [explicit] The paper investigates the use of external phrase break prediction models in an English end-to-end TTS system, showing improved listener comprehension for unpunctuated text.
- Why unresolved: The study only evaluates the effectiveness of phrase break prediction models in English. It does not explore how these models perform in other languages or how language-specific characteristics might influence the results.
- What evidence would resolve it: Conduct a similar study using end-to-end TTS systems for different languages, incorporating external phrase break prediction models, and evaluate listener comprehension through subjective listening tests.

### Open Question 2
- Question: Can the effectiveness of phrase break prediction models in end-to-end TTS systems be further improved by incorporating additional linguistic features?
- Basis in paper: [inferred] The paper uses a multi-speaker English corpus and focuses on pause prediction for phrase break modeling. However, it does not explore the potential benefits of incorporating additional linguistic features, such as part-of-speech tags or syntactic structures, into the phrase break prediction models.
- Why unresolved: The study does not investigate the impact of incorporating additional linguistic features on the performance of phrase break prediction models in end-to-end TTS systems.
- What evidence would resolve it: Develop and evaluate phrase break prediction models that incorporate additional linguistic features, and compare their performance with the models used in the paper.

### Open Question 3
- Question: How do different end-to-end TTS architectures perform with external phrase break prediction models?
- Basis in paper: [inferred] The paper uses a Tacotron2 model with Dynamic Convolutional Attention and a WaveRNN vocoder for the end-to-end TTS system. However, it does not compare the performance of different end-to-end TTS architectures with external phrase break prediction models.
- Why unresolved: The study only evaluates the effectiveness of phrase break prediction models in one specific end-to-end TTS architecture.
- What evidence would resolve it: Compare the performance of external phrase break prediction models in different end-to-end TTS architectures, such as Tacotron, Transformer TTS, or other neural network-based models, and evaluate their impact on listener comprehension.

## Limitations
- The study uses a single multi-speaker dataset without testing whether phrasing models generalize to different speaking styles or domains
- No objective metrics are reported for phrase break prediction accuracy
- The listening tests focus only on unpunctuated text, not comparing different phrasing model outputs
- Limited ablation studies showing the impact of phrasing models on comprehension across different text types

## Confidence
- Listener preference for predicted phrase breaks: High
- BERT superiority over BLSTM: Medium
- Speaker-independent generalization: Low

## Next Checks
1. Measure and report phrase break prediction F1 scores for both BLSTM and BERT models on the same test set to quantify their relative accuracy before conducting listening tests.

2. Apply the trained phrasing models to a different TTS system (e.g., FastSpeech2 or Glow-TTS) to verify that the improvements in listener comprehension are not specific to the Tacotron2 + WaveRNN architecture.

3. Test listener comprehension on multiple text categories (news, conversational, technical) with varying punctuation patterns to determine whether phrasing model benefits generalize across domains.