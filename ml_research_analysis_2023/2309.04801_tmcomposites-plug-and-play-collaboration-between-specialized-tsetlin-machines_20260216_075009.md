---
ver: rpa2
title: 'TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines'
arxiv_id: '2309.04801'
source_url: https://arxiv.org/abs/2309.04801
tags:
- accuracy
- class
- tsetlin
- color
- clauses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TM Composites, a plug-and-play architecture
  for collaboration between specialized Tsetlin Machines (TMs). The method enables
  independent TMs to combine their outputs via normalized class sums, allowing specialists
  to contribute where they excel.
---

# TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines

## Quick Facts
- arXiv ID: 2309.04801
- Source URL: https://arxiv.org/abs/2309.04801
- Reference count: 34
- Primary result: TM Composites improve accuracy on Fashion-MNIST by 2 percentage points, CIFAR-10 by 12 points, and CIFAR-100 by 9 points, achieving new state-of-the-art TM performance.

## Executive Summary
This paper introduces TM Composites, a plug-and-play architecture that enables collaboration between specialized Tsetlin Machines (TMs). The method allows independent TMs with different booleanization strategies to combine their outputs via normalized class sums, enabling specialists to contribute where they excel. Three TM specializations were evaluated: Histogram of Gradients, Adaptive Gaussian Thresholding, and Color Thermometers. Results show that TMs specialize rather than generalize, excelling on subsets of data aligned with their booleanization strategy. When combined, TM Composites achieve significant accuracy improvements across multiple image classification benchmarks, demonstrating the effectiveness of this collaborative approach.

## Method Summary
TM Composites combine multiple independently trained Tsetlin Machines, each using different booleanization strategies (Histogram of Gradients, Adaptive Gaussian Thresholding, and Color Thermometers). Each TM outputs normalized class sums, which are aggregated to produce the final classification decision. The architecture is plug-and-play, requiring no fine-tuning when adding or removing TMs. Class sums are normalized by the range (αt) between largest and smallest class sums in the data, ensuring balanced contributions from specialists with different output scales.

## Key Results
- TM Composites improve accuracy on Fashion-MNIST by 2 percentage points
- TM Composites improve accuracy on CIFAR-10 by 12 percentage points
- TM Composites improve accuracy on CIFAR-100 by 9 percentage points, achieving new state-of-the-art TM performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TM Composites enable each specialist to focus on subsets of data where it excels, improving overall accuracy.
- Mechanism: By combining multiple TMs that have been trained on different booleanization strategies, each TM contributes its highest-confidence predictions. The plug-and-play architecture allows them to be combined without retraining, with normalized class sums ensuring balanced contributions.
- Core assumption: Different booleanization strategies lead to specialization, with each TM excelling on particular subsets of the data.
- Evidence anchors:
  - [abstract] "Results show that TMs specialize rather than generalize, excelling on subsets of data aligned with their booleanization strategy."
  - [section] "Figure 3 demonstrates that the Thresholding TM has specialized in recognizing larger pixel structures... for images characterized by color texture, accuracy/confidence is low."
- Break condition: If multiple TMs perform well on the same subsets of data, they may duplicate effort rather than complement each other, reducing the effectiveness of the composite.

### Mechanism 2
- Claim: The classification confidence metric accurately reflects a TM's ability to classify images correctly.
- Mechanism: The max class sum (cmax) serves as a reliable indicator of confidence, with higher values corresponding to higher accuracy. This allows the composite to rely on the most confident TMs for each decision.
- Core assumption: The TM's internal confidence measure correlates strongly with actual classification performance.
- Evidence anchors:
  - [abstract] "When teaming up, the most confident TMs make the decisions, relieving the uncertain ones."
  - [section] "Figure 2 relates the accuracy of a Thresholding TM to its classification confidence on CIFAR-100... When confidence is low, the TM operates at its lowest accuracy. As confidence increases, accuracy grows, eventually reaching 100%."
- Break condition: If the confidence metric becomes unreliable (e.g., due to overfitting or data distribution shifts), the composite may make poor decisions by over-relying on confident but incorrect TMs.

### Mechanism 3
- Claim: The plug-and-play architecture enables flexible combination of TMs without fine-tuning.
- Mechanism: Each TM outputs normalized class sums, which are simply added together to form the composite decision. This eliminates the need for retraining when adding or removing TMs.
- Core assumption: Normalization by the range of class sums (αt) ensures that TMs with different scales contribute appropriately to the final decision.
- Evidence anchors:
  - [abstract] "The collaboration is plug-and-play in that members can be combined in any way, at any time, without fine-tuning."
  - [section] "Equation 7 normalizes the class sums by dividing by the difference αt between the largest and smallest class sums in the data."
- Break condition: If normalization doesn't properly account for the varying scales of different TMs' outputs, some specialists may dominate the composite regardless of their actual competence.

## Foundational Learning

- Concept: Booleanization of input data
  - Why needed here: Tsetlin Machines operate on boolean features, so understanding how different booleanization strategies affect learning is crucial for creating effective composites.
  - Quick check question: What are three different booleanization strategies used in the paper, and how might they lead to different specializations?

- Concept: Propositional logic and Horn clauses
  - Why needed here: TMs use propositional clauses in Horn form to represent patterns, which is fundamental to understanding how they learn and make decisions.
  - Quick check question: How does a TM represent a pattern using AND rules, and how does this differ from arithmetic-based approaches?

- Concept: Classification confidence metrics
  - Why needed here: Understanding how TMs measure and use classification confidence is essential for implementing the plug-and-play collaboration mechanism.
  - Quick check question: What is the max class sum, and how does it relate to the TM's classification confidence?

## Architecture Onboarding

- Component map: Booleanization strategy -> TM specialist (HOG/Thresholding/Color Thermometers) -> Normalized class sums -> Aggregation -> Final classification
- Critical path: 1) Booleanize input using each TM's strategy, 2) Run inference through each TM, 3) Calculate class sums for each TM, 4) Normalize class sums by αt, 5) Sum normalized class sums across TMs, 6) Take argmax to determine final class.
- Design tradeoffs: The plug-and-play nature allows flexibility but may not optimize the combination of specialists. A more complex weighting scheme could potentially improve performance but would sacrifice the simplicity and independence of the current approach.
- Failure signatures: If accuracy doesn't improve with additional TMs, it may indicate that the new TM is specializing in the same data subsets as existing ones. If accuracy drops, it could suggest that the new TM's normalization factor αt is not properly calibrated.
- First 3 experiments:
  1. Implement a basic TM Composite with two specialists (e.g., Thresholding and Color Thermometers) on a simple dataset like Fashion-MNIST.
  2. Add a third specialist (e.g., Histogram of Gradients) and measure the change in accuracy.
  3. Experiment with different normalization strategies (e.g., z-score normalization instead of range-based) and observe the impact on performance.

## Open Questions the Paper Calls Out

- What other image processing techniques beyond Histogram of Gradients, Adaptive Gaussian Thresholding, and Color Thermometers can be used to create complementary TM specialists?
- Can a lightweight optimization layer be designed to dynamically weight TM specialists based on their performance for different inputs?
- What is the optimal strategy for decomposing complex feature spaces among independent TMs in a composite?

## Limitations
- Effectiveness depends heavily on assumption that different booleanization strategies lead to meaningful specialization
- Normalized class sum aggregation is simple and may not optimally weight specialists based on their domain expertise
- Relies on empirical evidence rather than theoretical guarantees for the plug-and-play architecture

## Confidence

- **High confidence**: TM specialization mechanism and accuracy improvements on benchmark datasets (Fashion-MNIST +2%, CIFAR-10 +12%, CIFAR-100 +9%)
- **Medium confidence**: Plug-and-play architecture claims - while the implementation is straightforward, the optimality of simple summation versus learned weighting remains unproven
- **Medium confidence**: Interpretability claims - the paper establishes that TMs are inherently interpretable, but doesn't quantify or compare interpretability benefits of the composite approach

## Next Checks

1. **Generalization testing**: Evaluate TM Composites on additional datasets beyond Fashion-MNIST, CIFAR-10, and CIFAR-100 to assess robustness across diverse image domains
2. **Specialist overlap analysis**: Quantify the degree of overlap between TMs' areas of expertise using confusion matrices to verify that specialists truly complement rather than duplicate each other
3. **Alternative aggregation methods**: Compare simple summation against weighted voting or attention-based aggregation to determine if the plug-and-play simplicity sacrifices optimal performance