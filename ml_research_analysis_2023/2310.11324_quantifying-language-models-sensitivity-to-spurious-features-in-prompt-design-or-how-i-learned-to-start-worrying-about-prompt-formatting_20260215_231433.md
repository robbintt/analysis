---
ver: rpa2
title: 'Quantifying Language Models'' Sensitivity to Spurious Features in Prompt Design
  or: How I learned to start worrying about prompt formatting'
arxiv_id: '2310.11324'
source_url: https://arxiv.org/abs/2310.11324
tags:
- answer
- sentence
- prompt
- accuracy
- spread
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are highly sensitive to prompt formatting,
  with accuracy varying by up to 76 percentage points due to subtle formatting changes
  in few-shot settings. This sensitivity persists across model sizes, number of examples,
  and instruction tuning.
---

# Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting

## Quick Facts
- arXiv ID: 2310.11324
- Source URL: https://arxiv.org/abs/2310.11324
- Reference count: 40
- Large language models show accuracy variations up to 76 percentage points due to subtle formatting changes in few-shot settings

## Executive Summary
This paper demonstrates that large language models are highly sensitive to prompt formatting, with performance differences reaching up to 76 accuracy points when evaluated using LLaMA-2-13B. The authors introduce FormatSpread, a novel algorithm that efficiently estimates the range of performance across plausible prompt formats without accessing model weights. Through experiments on 53 tasks using LLaMA-2 and Falcon models, they show that formatting choices create spurious features that significantly alter LLM output distributions, challenging the validity of comparing models using a single prompt format.

## Method Summary
The authors define a grammar-based space of semantically equivalent prompt formats and use Bayesian optimization (Thompson sampling) to efficiently explore this space without requiring access to model weights. FormatSpread samples prompt formats within a computational budget and evaluates their performance to estimate the spread between best and worst formats. The approach is validated through systematic experiments on 53 classification and multiple-choice tasks from SuperNaturalInstructions, using LLaMA-2-{7B,13B,70B} and Falcon-7B models, with additional validation on GPT-3.5-Turbo to demonstrate API-gated model applicability.

## Key Results
- Performance spreads up to 76 percentage points observed across semantically equivalent prompt formats
- Median spread of 7.5 accuracy points across 53 tasks using FormatSpread algorithm
- Format sensitivity persists across model sizes, number of examples, and instruction tuning
- FormatSpread successfully identifies performance spreads for API-gated models like GPT-3.5-Turbo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formatting choices create spurious features that significantly alter LLM output distributions
- Mechanism: Minor changes in prompt formatting (like separator choice or casing) shift the internal representations of prompts, leading to large performance variations across otherwise semantically equivalent formats
- Core assumption: LLMs are sensitive to formatting features that should be semantically meaningless
- Evidence anchors:
  - [abstract] "with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B"
  - [section 4.2] "we find that choices in formatting few-shot examples during in-context learning introduce spurious biases that may lead to significantly different conclusions in model performance"
  - [corpus] Weak - corpus neighbors focus on robustness/robustness evaluation rather than spurious formatting features specifically
- Break condition: When formatting changes do not affect internal prompt representations in ways that influence output distributions

### Mechanism 2
- Claim: Performance spread across formats is non-monotonic and unpredictable
- Mechanism: The space of prompt formats is highly non-monotonic, meaning local search algorithms cannot reliably find optimal formats; performance changes from atomic formatting changes are large and irregular
- Core assumption: Performance differences between formats do not follow smooth patterns
- Evidence anchors:
  - [section 4.3] "we should often see a triples' accuracy to be strictly monotonic over i. We choose 24 tasks... 32.4 and 33.6% of triples were monotonic"
  - [section 4.4] "we find that classifier accuracy given just the top two components correlates moderately with the spread of performance"
  - [corpus] Weak - corpus focuses on robustness/robustness evaluation rather than non-monotonicity of performance spreads
- Break condition: When formatting changes produce predictable, monotonic performance improvements

### Mechanism 3
- Claim: Format sensitivity persists across model scales and instruction tuning
- Mechanism: Increasing model size, number of few-shot examples, or instruction tuning does not eliminate sensitivity to formatting choices
- Core assumption: Larger or instruction-tuned models remain vulnerable to formatting-induced spurious features
- Evidence anchors:
  - [abstract] "Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning"
  - [section 4.2] "we observe several tasks with performance spread over 70 accuracy points... regardless of increased model size"
  - [corpus] Weak - corpus neighbors don't specifically address persistence of formatting sensitivity across model scales
- Break condition: When model architecture or training method eliminates sensitivity to formatting variations

## Foundational Learning

- Concept: Bayesian optimization for bandit problems
  - Why needed here: FormatSpread uses Thompson sampling (a Bayesian approach) to efficiently explore the space of prompt formats without requiring access to model weights
  - Quick check question: What distribution does Thompson sampling assume for the performance of each arm (format) in this context?

- Concept: Semantic equivalence in prompt formats
  - Why needed here: The grammar defines which prompt formats are considered semantically equivalent, ensuring that performance differences are due to formatting rather than meaning changes
  - Quick check question: How does the grammar ensure that all formats in the equivalence class preserve the same meaning?

- Concept: Principal component analysis for representation analysis
  - Why needed here: FormatSpread uses PCA on prompt embeddings to show that format choices create identifiable transformations of the input representation
  - Quick check question: What correlation was found between the separability of format embeddings and observed performance spread?

## Architecture Onboarding

- Component map: Grammar definition -> Thompson sampling exploration -> Performance evaluation -> PCA analysis of embeddings
- Critical path: Grammar definition → Thompson sampling exploration → Performance evaluation → PCA analysis of embeddings
- Design tradeoffs: Trade accuracy of spread estimation against computational cost; trade completeness of format space exploration against runtime efficiency
- Failure signatures: High variance in spread estimates, failure to find worst-performing formats, poor correlation between embedding separability and performance spread
- First 3 experiments:
  1. Implement grammar parser and test generation of equivalent formats for a simple task
  2. Run Thompson sampling with simulated rewards to verify convergence to optimal and worst formats
  3. Test PCA-based format classification on a small set of prompt embeddings to verify identifiability

## Open Questions the Paper Calls Out

- Can the spread observed in performance due to formatting changes be mitigated through regularization techniques applied during model training or fine-tuning?
- Does the sensitivity to prompt formatting extend beyond classification tasks to more complex tasks requiring generation or reasoning?
- What is the relationship between the model's internal representation of formats and its sensitivity to formatting changes at different layers of the network?

## Limitations

- The grammar-defined format space may not exhaustively capture all real-world prompt variations used in practice
- Results are based on classification and multiple-choice tasks; generative tasks may exhibit different sensitivity patterns
- The computational budget (40,000 evaluations) may limit exploration of very large format spaces for some tasks

## Confidence

- **High confidence**: Performance spreads up to 76 percentage points exist across semantically equivalent formats (supported by systematic evaluation across 53 tasks and multiple model families)
- **Medium confidence**: Format sensitivity persists across model scales and instruction tuning (based on evaluation of LLaMA-2-7B/13B/70B and Falcon-7B with/without instruction tuning)
- **Medium confidence**: FormatSpread algorithm efficiently estimates performance spread (validated through Bayesian optimization comparison and PCA analysis, though algorithm performance depends on grammar quality)

## Next Checks

1. Test FormatSpread on a diverse set of generative tasks (e.g., summarization, translation) to verify sensitivity patterns extend beyond classification tasks
2. Evaluate correlation between format sensitivity and model architecture (e.g., transformer depth, attention patterns) to identify structural predictors of vulnerability
3. Implement a controlled ablation study where individual formatting elements are systematically varied to isolate their specific contribution to performance spread