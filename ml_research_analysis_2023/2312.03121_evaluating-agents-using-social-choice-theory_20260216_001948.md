---
ver: rpa2
title: Evaluating Agents using Social Choice Theory
arxiv_id: '2312.03121'
source_url: https://arxiv.org/abs/2312.03121
tags:
- agents
- agent
- score
- rank
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Voting-as-Evaluation (VasE), a framework
  for evaluating general agents using social choice theory. Each task is interpreted
  as a voter, with agents ranked within tasks based on performance metrics.
---

# Evaluating Agents using Social Choice Theory

## Quick Facts
- arXiv ID: 2312.03121
- Source URL: https://arxiv.org/abs/2312.03121
- Reference count: 40
- Primary result: Voting-as-Evaluation (VasE) framework uses social choice theory to aggregate agent rankings across tasks without requiring score normalization

## Executive Summary
This paper introduces Voting-as-Evaluation (VasE), a framework that treats each evaluation task as a voter to aggregate agent rankings across multiple domains. By applying voting rules from social choice theory to ordinal rankings derived from task-specific metrics, VasE provides robust cross-task evaluation without requiring score normalization. The framework demonstrates advantages over existing methods like Elo and Nash averaging, including clone resistance, task weighting flexibility, and the ability to identify game-theoretic cycles through an iterative maximal lotteries variant.

## Method Summary
VasE converts agent performance scores across multiple tasks into preference profiles where each task acts as a voter ranking agents. These rankings are aggregated using various voting methods (approval, Borda, plurality, STV, Copeland, Kemeny-Young, Schulze, ranked pairs, maximal lotteries) to produce overall agent rankings. The framework handles incomparable metrics by operating on ordinal rankings rather than raw scores, and includes an iterative maximal lotteries variant that identifies game-theoretic cycles by repeatedly solving margin subgames and removing winners.

## Key Results
- VasE identifies unique properties in evaluation data not evident from raw scores alone
- More robust to changes in agent subsets compared to Nash averaging
- Iterative maximal lotteries successfully identifies game-theoretic cycles among agent groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Voting-as-Evaluation (VasE) provides principled cross-task agent evaluation without requiring score normalization
- Mechanism: Each task votes as a separate "voter" by ranking agents within that task's metric, and voting rules aggregate these rankings into an overall evaluation
- Core assumption: Tasks can be meaningfully treated as voters with preferences over agents
- Evidence anchors:
  - [abstract] "Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons"
  - [section] "VasE does not require score normalization across tasks, only ordinal rankings"
  - [corpus] Weak - corpus mentions related work on social choice theory but doesn't specifically validate VasE's normalization-free approach
- Break condition: If tasks have fundamentally incomparable evaluation criteria that cannot be reduced to ordinal rankings

### Mechanism 2
- Claim: VasE provides robustness to clones and task subsets through consistency properties
- Mechanism: Voting methods like maximal lotteries satisfy clone consistency and population consistency, preventing manipulation by similar agents or subset evaluations
- Core assumption: Consistency properties from social choice theory transfer effectively to evaluation contexts
- Evidence anchors:
  - [section] "These axioms also provide guarantees, such as robustness to clones"
  - [section] "Maximal lotteries satisfies all of the consistency properties discussed in this paper"
  - [corpus] Moderate - corpus includes related work on clone-proof voting methods but doesn't validate this specific application
- Break condition: If evaluation contexts have properties that invalidate social choice consistency assumptions

### Mechanism 3
- Claim: Iterative maximal lotteries identifies game-theoretic cycles among agents
- Mechanism: Repeatedly solving margin subgames and removing winners reveals non-transitive relationships between agent groups
- Core assumption: Margin subgames capture meaningful strategic relationships between agent subsets
- Evidence anchors:
  - [abstract] "an iterative variant of the maximal lotteries voting scheme... identifies game-theoretic cycles"
  - [section] "iterative maximal lotteries... produces a full ranking of agents: a game-theoretic method that repeatedly solves two-player zero-sum margin games"
  - [corpus] Moderate - corpus includes related work on maximal lotteries but doesn't specifically validate the iterative variant for cycle detection
- Break condition: If margin subgames fail to capture meaningful strategic relationships between agents

## Foundational Learning

- Social choice theory basics
  - Why needed here: Provides the mathematical framework for treating tasks as voters and aggregating preferences
  - Quick check question: What's the difference between a social choice function and a social welfare function?

- Game theory fundamentals
  - Why needed here: Understanding how margin games and Nash equilibria relate to agent evaluation
  - Quick check question: How does a zero-sum margin game differ from a general-sum game in this context?

- Voting method properties
  - Why needed here: Understanding what properties (Condorcet consistency, clone consistency, etc.) each voting method provides
  - Quick check question: Why can't a voting method be both Condorcet consistent and population consistent?

## Architecture Onboarding

- Component map:
  - Data input layer: Task scores for each agent
  - Vote generation: Converts scores to rankings/comparisons
  - Voting method selection: Chooses appropriate aggregation method
  - Result computation: Applies voting rule to generate rankings
  - Visualization: Displays results and game-theoretic cycles

- Critical path:
  1. Load evaluation data (agent scores across tasks)
  2. Generate preference profiles (votes)
  3. Select voting method based on desired properties
  4. Compute aggregate ranking using chosen method
  5. Visualize results and identify patterns

- Design tradeoffs:
  - Deterministic vs probabilistic methods: Determinism provides clear winners but may miss cycles; probability captures uncertainty but complicates interpretation
  - Computational complexity: Some methods (Kemeny-Young) are factorial while others (approval) are linear
  - Properties vs performance: Condorcet methods guarantee certain winners but may be less robust to clones

- Failure signatures:
  - All methods producing identical rankings: May indicate lack of task diversity
  - Extreme sensitivity to single tasks: Could indicate over-reliance on specific metrics
  - Cycles dominating results: May suggest fundamental non-comparability of tasks

- First 3 experiments:
  1. Run all voting methods on ALE data to observe consistency across methods
  2. Compare VasE results with Elo ratings on same data
  3. Test clone robustness by adding similar agents and observing rank stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VasE be extended to handle non-transitive preferences beyond pairwise comparisons, such as those arising from multi-agent games with complex strategic interactions?
- Basis in paper: [inferred] The paper discusses how VasE can handle pairwise comparisons and non-transitive relationships, but it does not explicitly address more complex preference structures beyond pairwise comparisons.
- Why unresolved: Extending VasE to handle more complex preference structures would require developing new aggregation methods and potentially modifying the existing voting rules to accommodate non-pairwise comparisons.
- What evidence would resolve it: Developing and testing VasE extensions that can handle non-transitive preferences beyond pairwise comparisons, such as those arising from multi-agent games with complex strategic interactions.

### Open Question 2
- Question: How does VasE perform in scenarios where agents have varying levels of expertise or skill across different tasks, and how can this be accounted for in the evaluation process?
- Basis in paper: [inferred] The paper discusses the evaluation of agents across multiple tasks, but it does not explicitly address the issue of varying levels of expertise or skill across different tasks.
- Why unresolved: Accounting for varying levels of expertise or skill across different tasks would require developing new methods for normalizing or weighting the evaluation data, and potentially modifying the existing voting rules to accommodate these variations.
- What evidence would resolve it: Conducting experiments with agents that have varying levels of expertise or skill across different tasks, and developing new methods for normalizing or weighting the evaluation data to account for these variations.

### Open Question 3
- Question: How can VasE be adapted to handle evaluation scenarios where the tasks themselves are dynamic or evolving over time, such as in online learning or reinforcement learning settings?
- Basis in paper: [inferred] The paper discusses the evaluation of agents across multiple tasks, but it does not explicitly address the issue of dynamic or evolving tasks.
- Why unresolved: Adapting VasE to handle dynamic or evolving tasks would require developing new methods for updating the evaluation data and potentially modifying the existing voting rules to accommodate these changes.
- What evidence would resolve it: Conducting experiments with evaluation scenarios where the tasks are dynamic or evolving over time, and developing new methods for updating the evaluation data and modifying the voting rules to accommodate these changes.

## Limitations
- Cross-task comparison without score normalization assumes ordinal rankings capture all relevant information, which may not hold for fundamentally different evaluation metrics
- Transfer of social choice theory properties to evaluation contexts is largely theoretical with limited empirical validation
- Iterative maximal lotteries variant for detecting cycles is introduced but not thoroughly validated across diverse agent sets

## Confidence
- **High confidence**: The core VasE framework of treating tasks as voters and using voting rules for aggregation is mathematically sound
- **Medium confidence**: Claims about robustness to clones and subset evaluations, based on social choice theory properties that need empirical validation
- **Medium confidence**: The cycle detection mechanism through iterative maximal lotteries, which requires more extensive testing

## Next Checks
1. Test clone robustness empirically by systematically adding similar agents to evaluation datasets and measuring rank stability
2. Validate cross-task comparison without normalization across fundamentally different evaluation metrics (e.g., accuracy vs latency)
3. Benchmark cycle detection performance on known non-transitive agent relationships across multiple domains