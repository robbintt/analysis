---
ver: rpa2
title: 'LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer
  with Visual Input'
arxiv_id: '2304.14765'
source_url: https://arxiv.org/abs/2304.14765
tags:
- contrastive
- pets
- images
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents LostPaw, a contrastive learning-based model
  for finding lost pets using visual input. The model utilizes a Vision Transformer
  (ViT) backbone with three additional fully-connected layers, trained on a large
  dataset of dog images with data augmentation.
---

# LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input

## Quick Facts
- arXiv ID: 2304.14765
- Source URL: https://arxiv.org/abs/2304.14765
- Reference count: 8
- Key outcome: 90% test accuracy for finding lost pets using contrastive learning with ViT backbone

## Executive Summary
LostPaw presents a novel approach for finding lost pets using a contrastive learning-based Vision Transformer model. The system processes pet images through a DETR-based detector, applies data augmentation, and extracts features using a ViT backbone with three additional fully-connected layers. The model achieves 90% accuracy on a dataset of 78,702 dog images without overfitting, demonstrating the effectiveness of contrastive learning for visual similarity matching in pet identification.

## Method Summary
The approach uses a ViT backbone with three additional fully-connected layers, trained using contrastive loss on a large dataset of dog images. Images are preprocessed to 384x384 pixels, cropped using DETR, and augmented twice with AutoAugment policies. The model is trained for 350 epochs with a batch size of 8, using a contrastive margin of 1.66. Only the last three layers are fine-tuned while keeping the backbone frozen. 3-fold cross-validation is used for evaluation, achieving an average F1 score of 88.8%.

## Key Results
- 90% test accuracy on held-out test set
- 91.1% F1 score on held-out test set
- 9.7% type I error (false positives) and 0.06% type II error (false negatives)
- No overfitting observed during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive learning allows the model to learn a compact latent representation that can effectively distinguish between similar and dissimilar images.
- **Mechanism**: The contrastive loss function pulls feature vectors of the same pet closer in the latent space while pushing different pets apart, with a margin to control sensitivity.
- **Core assumption**: Euclidean distance in the learned latent space correlates with visual similarity.
- **Evidence anchors**:
  - [abstract] "The loss function in this approach minimizes the distance between feature vectors of the same class (i.e., the same pet) while simultaneously maximizing the distance between feature vectors of different classes."
  - [section] "The loss function ensures that the distance between the feature vectors of dissimilar examples exceeds the margin given by the hyperparameter m."
  - [corpus] Weak; no direct comparison to similar contrastive methods.
- **Break condition**: If the margin is set too small or the dataset lacks sufficient variation, the model may not learn a meaningful separation, leading to high false positive rates.

### Mechanism 2
- **Claim**: Freezing the ViT backbone and fine-tuning only the final three layers stabilizes training and prevents overfitting.
- **Mechanism**: By keeping the backbone fixed, the model avoids re-learning low-level features and instead focuses on adapting the classification head to the specific contrastive task.
- **Core assumption**: The pretrained ViT backbone provides a robust feature extractor that generalizes well to new pet images.
- **Evidence anchors**:
  - [abstract] "Throughout 350 epochs, an average F1 score of 88.8% on the cross-validation set was achieved... the model was trained on a large dataset and did not appear to be overfitting."
  - [section] "During training, only the last three layers of the model were fine-tuned, while the backbone parameters were kept frozen."
  - [corpus] Weak; no ablation results from similar papers.
- **Break condition**: If the pretrained backbone is not well-suited to the domain (e.g., too generic or trained on unrelated data), fine-tuning it may be necessary to improve performance.

### Mechanism 3
- **Claim**: Augmenting images with AutoAugment introduces robustness to color and lighting variations, improving generalization.
- **Mechanism**: Random transformations (shear, rotation, brightness, color inversion) expose the model to diverse versions of the same pet, encouraging invariant feature learning.
- **Core assumption**: Visual similarity should be preserved under natural variations in appearance.
- **Evidence anchors**:
  - [abstract] "Each image was then augmented twice using a pre-trained AutoAugment model, which followed the policies CIFAR10, ImageNet, and SVHN."
  - [section] "AutoAugment is an effective way to augment data, which can improve the performance of machine learning models, and is particularly effective for image classification tasks."
  - [corpus] Weak; no quantitative evidence from similar studies.
- **Break condition**: Excessive augmentation (e.g., extreme color inversion) could distort key pet features, harming the model's ability to distinguish between similar-looking pets.

## Foundational Learning

- **Concept**: Vision Transformer (ViT) architecture and self-attention
  - **Why needed here**: ViT processes image patches as tokens and uses self-attention to capture global context, which is crucial for distinguishing pets with subtle differences.
  - **Quick check question**: How does self-attention in ViT differ from convolutional layers in terms of spatial dependency modeling?

- **Concept**: Contrastive loss and Siamese network training
  - **Why needed here**: The contrastive loss explicitly encourages separation between different pets and clustering of the same pet, enabling accurate pairwise matching.
  - **Quick check question**: What is the role of the margin hyperparameter in contrastive loss, and how does it affect false positives?

- **Concept**: k-fold cross-validation for model evaluation
  - **Why needed here**: Ensures the model's performance is robust and not dependent on a specific train/test split, which is important given the stochastic nature of pair sampling.
  - **Quick check question**: Why might 3-fold cross-validation be chosen over 5-fold in this context?

## Architecture Onboarding

- **Component map**: Raw image -> DETR detector -> Crop to 384x384 -> AutoAugment -> ViT backbone -> FC layers (3) -> Latent vector (512) -> Contrastive loss -> Web app interface

- **Critical path**:
  1. User uploads pet image
  2. Image preprocessed (resize, crop, augment)
  3. ViT backbone extracts features
  4. FC layers produce latent vector
  5. Vector compared against database using contrastive similarity
  6. Top matches returned to user

- **Design tradeoffs**:
  - Fixed backbone vs. full fine-tuning: Stability vs. adaptability
  - Pairwise sampling: Balanced same/different pairs reduce bias but increase dataset size
  - Latent dimension (512): Compact representation vs. expressive power

- **Failure signatures**:
  - High type I error: Model over-generalizes similarity, possibly due to low margin or insufficient variation in training
  - Low accuracy despite low loss: Model may be memorizing augmentations rather than learning true visual similarity
  - Poor held-out test performance: Possible domain shift or insufficient diversity in training data

- **First 3 experiments**:
  1. Vary the contrastive margin (m) from 0.5 to 2.0 and measure impact on type I vs type II error trade-off.
  2. Test with and without AutoAugment to quantify robustness gains from data augmentation.
  3. Compare fixed backbone vs full fine-tuning to confirm overfitting prevention and performance trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the contrastive learning model compare when applied to other types of pets, such as cats, beyond dogs?
- Basis in paper: [inferred] The authors mention that a potential direction for future research would be to expand the network to include other types of pets.
- Why unresolved: The study only focused on images of dogs and did not test the model's performance on other types of pets.
- What evidence would resolve it: Testing the model on a dataset containing images of other pets, such as cats, and comparing its performance to the results obtained with the dog dataset.

### Open Question 2
- Question: How does the performance of the model change when using different backbone architectures, such as ResNet or EfficientNet, instead of the Vision Transformer?
- Basis in paper: [inferred] The authors used the Vision Transformer as the backbone of the model but did not explore the use of other backbone architectures.
- Why unresolved: The study only tested the model with the Vision Transformer backbone and did not investigate the effects of using different architectures.
- What evidence would resolve it: Training and evaluating the model with different backbone architectures and comparing their performance to the results obtained with the Vision Transformer backbone.

### Open Question 3
- Question: What is the impact of varying the number of epochs and learning rate on the model's performance and generalization ability?
- Basis in paper: [explicit] The authors mention that the model was trained for 350 epochs with a fixed learning rate, but they do not discuss the effects of varying these hyperparameters.
- Why unresolved: The study used a fixed number of epochs and learning rate, and did not explore the impact of varying these hyperparameters on the model's performance.
- What evidence would resolve it: Conducting experiments with different numbers of epochs and learning rates, and analyzing the effects on the model's performance and generalization ability.

## Limitations
- Limited to dog images only, not tested on other pet types
- Dataset composition and collection methodology not fully detailed
- No baseline comparisons to alternative approaches
- Real-world application effectiveness not empirically validated

## Confidence
**High Confidence**: The technical implementation of contrastive learning with ViT backbone is sound and well-established. The reported training methodology and evaluation metrics are clearly specified and reproducible.

**Medium Confidence**: The 90% test accuracy claim is supported by 3-fold cross-validation and held-out test set results. However, the lack of baseline comparisons and detailed dataset information limits confidence in the absolute performance claims.

**Low Confidence**: The practical effectiveness of the web application in real-world scenarios is not empirically validated. The model's performance with low-quality user-submitted images, varying lighting conditions, and partial pet visibility remains unverified.

## Next Checks
1. **Real-world testing**: Deploy the web application in a controlled pilot program with actual lost pet reports and measure precision/recall in practical use cases, including analysis of false positive rates when users submit images of similar-looking pets.

2. **Cross-species validation**: Test the model architecture on other common pets (cats, rabbits, birds) to evaluate whether the contrastive learning approach generalizes beyond dogs, and if not, identify what modifications would be needed.

3. **Robustness analysis**: Systematically evaluate model performance under varying image conditions including low resolution, partial occlusions, different lighting, and background clutter to quantify the impact on accuracy and identify failure modes.