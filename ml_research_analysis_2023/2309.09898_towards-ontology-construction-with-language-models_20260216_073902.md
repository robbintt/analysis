---
ver: rpa2
title: Towards Ontology Construction with Language Models
arxiv_id: '2309.09898'
source_url: https://arxiv.org/abs/2309.09898
tags:
- concept
- concepts
- also
- domain
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for automatically constructing a concept
  hierarchy for a given domain by querying a large language model (LLM). The method
  takes a seed concept as input and then uses GPT 3.5 to iteratively discover and
  insert relevant subconcepts into the hierarchy.
---

# Towards Ontology Construction with Language Models

## Quick Facts
- arXiv ID: 2309.09898
- Source URL: https://arxiv.org/abs/2309.09898
- Reference count: 33
- Key outcome: Proposed method automatically constructs concept hierarchies from seed concepts using GPT 3.5 with manual inspection showing reasonable but incomplete results

## Executive Summary
This paper introduces an automated approach for constructing domain-specific concept hierarchies using large language models. The method starts with a seed concept and iteratively queries GPT 3.5 to discover subconcepts, verify relationships, and build a structured ontology. Applied to domains like Animals, Drinks, Music, and Plants, the approach demonstrates that LLMs can generate meaningful hierarchies while acknowledging remaining challenges with errors and incompleteness.

## Method Summary
The method takes a seed concept as input and uses GPT 3.5 to iteratively discover and insert relevant subconcepts into a growing hierarchy. The algorithm employs four main query types: existence checking, listing subconcepts, obtaining descriptions, and verification. It uses frequency-based sampling to identify candidate subconcepts and the KRIS algorithm for efficient insertion while preserving partial order. The exploration is limited by a depth parameter to prevent excessive growth and domain drift.

## Key Results
- Generated concept hierarchies for multiple domains (Animals, Drinks, Music, Plants) using GPT 3.5
- Manual inspection shows reasonable structure but with some errors and incompleteness remaining
- Frequency thresholds varied by domain (10 for Animals, 20 for Drinks/Plants, 5 for Music) to balance completeness and precision
- Temperature and frequency parameters significantly impact ontology quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative subconcept discovery via LLM produces a concept hierarchy that expands from a seed concept
- Mechanism: The algorithm queries the LLM for existence, then listing, then verification of subconcepts, and inserts them into a growing hierarchy
- Core assumption: LLM responses are sufficiently accurate to generate a meaningful hierarchy without human intervention
- Evidence anchors:
  - [abstract] "Our experiments indicate that LLMs can be of considerable help for constructing concept hierarchies"
  - [section] "We then ‚Äòcrawl‚Äô the hierarchy by repeatedly asking the LLM to provide relevant subconcepts of concepts that are already in the hierarchy..."
- Break condition: If the LLM returns irrelevant or hallucinated subconcepts faster than they can be filtered out, the hierarchy quality degrades

### Mechanism 2
- Claim: Frequency-based sampling improves completeness of subconcept discovery
- Mechanism: Multiple queries with max_tokens=1 collect token frequencies; high-frequency tokens trigger full subconcept listing prompts
- Core assumption: The LLM's token frequency distribution correlates with subconcept relevance
- Evidence anchors:
  - [section] "We set the max_tokens parameter to 1, meaning that we only ask for the first token of an answer..."
  - [section] "We then pose the prompt many times (we choose 100) and take all tokens that are returned with a certain minimum frequency..."
- Break condition: If token frequency does not correlate with relevance, low-quality subconcepts get included

### Mechanism 3
- Claim: KRIS algorithm-based insertion preserves partial order while minimizing LLM queries
- Mechanism: Top-down and bottom-up traversal phases exploit transitivity to reduce number of subsumption tests
- Core assumption: LLM subsumption judgments are consistent enough for transitive inference to be valid
- Evidence anchors:
  - [section] "A fundamental algorithm for this task that aims to minimize the number of subsumption tests has been proposed in [21]..."
  - [section] "The basic idea of the KRIS algorithm is to use, for inserting a new concept ùê∂, a top search phase to identify all superconcepts of ùê∂..."
- Break condition: If LLM subsumption judgments violate transitivity, the hierarchy may contain cycles or missing links

## Foundational Learning

- Concept: Prompt engineering with temperature and top_p
  - Why needed here: Controls randomness and probability mass for LLM responses, balancing diversity and determinism
  - Quick check question: What happens to subconcept diversity if temperature=0 versus temperature=2?

- Concept: Frequency analysis for sampling
  - Why needed here: Compensates for lack of direct probability access in GPT API by inferring relevance from repeated sampling
  - Quick check question: How many times must a token appear to be considered a candidate subconcept?

- Concept: Transitive closure in hierarchies
  - Why needed here: Enables efficient insertion by assuming inferred relations without explicit LLM queries
  - Quick check question: Why is transitivity assumed, and what risk does it pose if LLM outputs are inconsistent?

## Architecture Onboarding

- Component map: Seed concept ‚Üí Existence prompt ‚Üí Listing prompt ‚Üí Verification prompts ‚Üí Insertion (KRIS) ‚Üí Hierarchy storage ‚Üí Web interface for browsing
- Critical path: 1) Seed concept input 2) Iterative existence ‚Üí listing ‚Üí verification loop 3) KRIS insertion to maintain DAG structure 4) Output hierarchy with descriptions
- Design tradeoffs: Completeness vs. soundness (higher thresholds reduce noise but miss rare subconcepts), Speed vs. accuracy (parallel queries speed up but increase cost), Automation vs. control (fully automatic vs. human-in-the-loop)
- Failure signatures: Excessive hallucinations (high verification failure rate), Slow expansion (low frequency threshold or overly strict prompts), Cyclic dependencies (transitivity assumption violated)
- First 3 experiments: 1) Run with temperature=1, top_p=0.99, frequency threshold=10 on "Drinks" and inspect output quality, 2) Vary frequency threshold (5, 10, 20) and measure completeness vs. precision trade-off, 3) Replace KRIS insertion with brute-force subsumption tests on small hierarchy to compare query counts and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of frequency threshold affect the precision and recall of the constructed ontologies?
- Basis in paper: [explicit] The paper discusses the impact of the frequency threshold on the quality of the ontologies, noting that lower values tend to work better for strongly structured domains while higher values are more appropriate for weakly structured domains
- Why unresolved: The paper does not provide a detailed analysis of how the frequency threshold affects precision and recall
- What evidence would resolve it: Conducting experiments with different frequency thresholds and measuring the resulting precision and recall would provide insights into the optimal threshold settings for various domains

### Open Question 2
- Question: Can fine-tuning the LLM specifically for ontology construction improve the quality of the generated ontologies?
- Basis in paper: [inferred] The paper mentions that fine-tuning BERT models for subsumption prediction has been used for ontology completion, suggesting that fine-tuning could potentially improve the quality of ontologies generated by LLMs
- Why unresolved: The paper does not explore the impact of fine-tuning on ontology construction
- What evidence would resolve it: Training an LLM specifically for ontology construction and comparing the quality of the generated ontologies with those produced by a general-purpose LLM would provide insights into the benefits of fine-tuning

### Open Question 3
- Question: How does the exploration depth parameter influence the completeness and relevance of the constructed ontologies?
- Basis in paper: [explicit] The paper mentions that the exploration depth parameter is used to limit the depth of concept exploration, which can help avoid excessive size and esoteric concepts in the ontologies
- Why unresolved: The paper does not provide a detailed analysis of how the exploration depth affects the completeness and relevance of the ontologies
- What evidence would resolve it: Conducting experiments with different exploration depths and evaluating the resulting ontologies for completeness and relevance would provide insights into the optimal depth settings for various domains

## Limitations
- Manual inspection-based evaluation without quantitative precision/recall metrics
- Reliance on frequency sampling as proxy for relevance due to lack of direct probability access
- Assumption of transitivity in LLM subsumption judgments may not always hold
- Optimal parameter settings (temperature, frequency thresholds) are domain-dependent and not fully characterized

## Confidence
- **High confidence**: The iterative query framework and KRIS-based insertion algorithm are clearly specified and implementable
- **Medium confidence**: The general approach of using frequency sampling to identify relevant subconcepts is sound, though optimal thresholds are uncertain
- **Low confidence**: The assumption of transitivity in LLM subsumption judgments and the lack of quantitative evaluation metrics

## Next Checks
1. **Quantitative evaluation**: Create ground truth ontologies for a small domain (e.g., "Fruits") and measure precision, recall, and F1-score of the LLM-generated hierarchy against this reference
2. **Parameter sensitivity analysis**: Systematically vary temperature (0.0, 0.5, 1.0, 1.5) and frequency thresholds (5, 10, 15, 20) on the same domain to quantify the trade-off between completeness and accuracy
3. **Transitivity validation**: Run controlled experiments where the LLM is asked to verify transitive relationships (A ‚äë B, B ‚äë C ‚Üí A ‚äë C) to measure consistency rates and identify failure patterns