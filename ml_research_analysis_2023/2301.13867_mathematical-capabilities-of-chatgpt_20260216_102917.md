---
ver: rpa2
title: Mathematical Capabilities of ChatGPT
arxiv_id: '2301.13867'
source_url: https://arxiv.org/abs/2301.13867
tags:
- chatgpt
- uni00000003
- uni00000048
- uni00000052
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive assessment of ChatGPT's
  mathematical abilities, introducing the GHOSTS dataset (728 expert-curated prompts)
  spanning graduate-level mathematics. The authors evaluate ChatGPT against specialized
  models like Minerva and find that while ChatGPT can act as a mathematical search
  engine and knowledge base, its performance falls significantly below that of a graduate
  student.
---

# Mathematical Capabilities of ChatGPT

## Quick Facts
- **arXiv ID**: 2301.13867
- **Source URL**: https://arxiv.org/abs/2301.13867
- **Reference count**: 40
- **Primary result**: ChatGPT performs significantly below graduate-level mathematical reasoning, scoring well below the 3.5 threshold needed to pass a university math class

## Executive Summary
This paper presents the first comprehensive assessment of ChatGPT's mathematical abilities using the GHOSTS dataset - a collection of 728 expert-curated prompts spanning graduate-level mathematics. The authors evaluate ChatGPT across multiple dimensions including proof-based questions, computational tasks, and mathematical Olympiad-style problems, finding that while ChatGPT can function as a mathematical search engine and knowledge base, its performance falls significantly below that of a graduate student. The study provides detailed error analysis showing that ChatGPT struggles with logical reasoning, basic arithmetic operations, and handling unusual constraints, despite often understanding the questions posed.

## Method Summary
The researchers created the GHOSTS dataset containing 728 prompts across six subdatasets covering various mathematical domains and difficulty levels. ChatGPT (9-January-2023 version) was prompted via web interface with new sessions per prompt to avoid bias. Expert mathematicians manually rated ChatGPT's outputs on a 1-5 scale with detailed error codes categorizing different failure modes. The performance was compared against specialized models like Minerva, and various prompt engineering strategies were tested to assess their impact on performance.

## Key Results
- ChatGPT's average rating across all mathematical tasks is well below the 3.5 threshold needed to pass a university math class
- ChatGPT performs poorly on proof-based questions and computational tasks, scoring significantly below specialized models like Minerva
- While ChatGPT can act as a mathematical search engine and knowledge base, it struggles with graduate-level reasoning and Olympiad-style problems
- Prompt engineering reduces certain error types but does not significantly improve overall performance on complex mathematical tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GHOSTS dataset provides a holistic assessment of mathematical reasoning by testing multiple dimensions (proof-based, computational, search engine aspects) that previous datasets do not cover comprehensively.
- Mechanism: By curating prompts across diverse mathematical domains and difficulty levels (elementary arithmetic through graduate-level), the dataset captures different failure modes and capabilities of LLMs.
- Core assumption: Graduate-level mathematical reasoning requires distinct skills from elementary problem-solving, and these differences manifest in LLM performance.
- Evidence anchors: [abstract] states the dataset "distinguish multiple dimensions of mathematical reasoning" and "aim to cover graduate-level mathematics"

### Mechanism 2
- Claim: Manual expert rating of LLM outputs provides more nuanced evaluation than automated scoring methods.
- Mechanism: Expert annotators can assess partial correctness, logical flow, and mathematical sophistication beyond simple right/wrong classification.
- Core assumption: Mathematical reasoning involves qualitative aspects (proof structure, insight, approach) that cannot be reduced to token-level accuracy.
- Evidence anchors: [section 3.2] describes rating scale from 1-5 with detailed error codes for different failure modes

### Mechanism 3
- Claim: Prompt engineering (e.g., step-by-step instructions) reduces certain error types but doesn't improve overall performance on graduate-level problems.
- Mechanism: Structured prompting guides LLM reasoning process, reducing superficial errors but not addressing fundamental limitations in mathematical understanding.
- Core assumption: LLM errors at graduate level stem from inability to understand complex mathematical concepts, not just poor reasoning organization.
- Evidence anchors: [section 4.1] shows prompt engineering reduces e2-e4 errors but not e5 logical errors

## Foundational Learning

- Concept: Mathematical maturity and proof comprehension
  - Why needed here: The dataset tests ability to understand and construct proofs, not just compute answers
  - Quick check question: Can you explain why the error code e5_5 (circular logical argument) represents a fundamental misunderstanding rather than a computational mistake?

- Concept: Mathematical notation and formal language
  - Why needed here: The dataset uses LaTeX and expects proper mathematical expression
  - Quick check question: Why does the dataset's ability to handle LaTeX-encoded mathematics matter for evaluating LLM mathematical capabilities?

- Concept: Research methodology and dataset curation
  - Why needed here: Understanding how to create meaningful benchmarks that test specific capabilities
  - Quick check question: What makes the GHOSTS dataset more comprehensive than existing mathematical LLM benchmarks?

## Architecture Onboarding

- Component map: Dataset creation (prompt curation by experts) → Manual rating (expert assessment with error codes) → Statistical analysis (performance metrics, error type distribution) → Comparative evaluation (against specialized models like Minerva)
- Critical path: Expert creation of prompts → Manual rating of outputs → Analysis of error patterns → Benchmarking against specialized models
- Design tradeoffs: Manual rating provides nuanced assessment but limits dataset size; automated evaluation would scale better but miss qualitative aspects
- Failure signatures: High confidence ratings on incorrect answers (ChatGPT often presents wrong solutions confidently), inability to handle edge cases in proofs, consistent computational errors despite correct approach
- First 3 experiments:
  1. Replicate the rating process on a small subset to understand error code application
  2. Test prompt engineering variations on the Olympiad-Problem-Solving dataset to verify error reduction patterns
  3. Compare manual vs automated evaluation on a mixed dataset to quantify the value of expert assessment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ChatGPT be reliably fine-tuned on the GHOSTS dataset to achieve performance competitive with specialized mathematical models like Minerva?
- Basis in paper: [inferred] The paper demonstrates ChatGPT's limitations on graduate-level mathematics but releases the dataset for community use, suggesting potential for future improvement through fine-tuning.
- Why unresolved: The paper focuses on evaluation rather than training, and the dataset size may be insufficient for effective fine-tuning of large language models.
- What evidence would resolve it: A study showing the results of fine-tuning a language model on GHOSTS and comparing its performance to ChatGPT and specialized models like Minerva on the same benchmarks.

### Open Question 2
- Question: Does prompt engineering with step-by-step instructions significantly improve ChatGPT's performance on mathematical Olympiad problems, or are the failures primarily due to fundamental logical reasoning limitations?
- Basis in paper: [explicit] The paper notes that prompt engineering reduced some error types but did not affect average ratings, and ChatGPT struggled with unusual constraints and mathematical insights required for Olympiad problems.
- Why unresolved: The study only tested one type of prompt engineering (step-by-step instructions) and did not systematically explore the space of possible prompt engineering techniques for mathematical reasoning.
- What evidence would resolve it: A comprehensive study testing various prompt engineering strategies on the Olympiad-Problem-Solving dataset to determine which techniques most effectively improve performance and whether they address fundamental logical limitations.

### Open Question 3
- Question: What specific mathematical reasoning capabilities must be added to large language models to bridge the gap between their current performance and that of graduate-level mathematicians?
- Basis in paper: [explicit] The error analysis reveals that ChatGPT struggles with basic arithmetic operations, logical reasoning, and handling unusual constraints, despite often understanding the questions.
- Why unresolved: The paper identifies failure modes but does not propose specific architectural or training modifications to address these limitations in future language models.
- What evidence would resolve it: A technical study demonstrating that adding specific reasoning modules, training on specialized mathematical datasets, or incorporating symbolic computation capabilities to language models significantly improves performance on graduate-level mathematical tasks.

## Limitations
- Evaluation relies heavily on expert manual rating, introducing potential subjectivity and limiting scalability
- Copyright restrictions on some prompts limit full reproducibility of the study
- Performance may vary with newer ChatGPT versions (study used 9-January-2023)
- Manual rating process cannot capture all aspects of mathematical reasoning, particularly creative problem-solving nuances

## Confidence
- **High confidence**: ChatGPT's poor performance on proof-based and Olympiad problems, and its inability to pass a university math class threshold
- **Medium confidence**: The effectiveness of prompt engineering in reducing certain error types but not overall performance
- **Medium confidence**: The comparative advantage of GHOSTS dataset over existing benchmarks for holistic mathematical evaluation

## Next Checks
1. Replicate the rating process using automated evaluation methods on a subset of the dataset to quantify the value added by expert manual assessment versus potential scalability gains from automation
2. Test the latest ChatGPT version on the GHOSTS dataset to measure performance improvements or regressions since the January 2023 version
3. Apply the GHOSTS dataset to evaluate other general-purpose LLMs (not just ChatGPT) to establish whether these findings represent broader limitations of current language models in mathematical reasoning