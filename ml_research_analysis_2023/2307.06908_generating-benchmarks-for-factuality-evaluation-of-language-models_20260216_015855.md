---
ver: rpa2
title: Generating Benchmarks for Factuality Evaluation of Language Models
arxiv_id: '2307.06908'
source_url: https://arxiv.org/abs/2307.06908
tags:
- factor
- completion
- factual
- accuracy
- contradiction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of evaluating language model (LM)
  factuality in generation, particularly the tendency to produce factually incorrect
  information in a given domain. Existing methods either focus on isolated facts or
  sample facts from the LM itself, which under-represents rare facts and lacks control
  over evaluated facts.
---

# Generating Benchmarks for Factuality Evaluation of Language Models

## Quick Facts
- **arXiv ID**: 2307.06908
- **Source URL**: https://arxiv.org/abs/2307.06908
- **Reference count**: 40
- **Key outcome**: Presents FACTOR, a method to automatically transform factual corpora into benchmarks that evaluate language models' propensity to generate true facts versus incorrect statements.

## Executive Summary
The paper addresses the challenge of evaluating language model factuality in generation by proposing FACTOR (Factual Assessment via Corpus TransfORmation), a scalable approach that automatically transforms factual corpora into benchmarks. The method creates controlled test cases by generating non-factual variations of true statements, then measuring whether models can distinguish between them. Applied to Wikipedia and news articles, the approach shows that FACTOR scores correlate with model size and retrieval augmentation, and that when FACTOR and perplexity disagree, FACTOR better reflects factuality in open-ended generation as validated by human annotators.

## Method Summary
FACTOR transforms factual corpora into benchmarks by extracting multi-sentence prefixes and their next-sentence completions, then using InstructGPT to generate three non-factual variations for each true completion through five error types (predicate, entity, circumstance, coreference, link). These variations are filtered using NLI models to ensure contradiction and LM scores to ensure fluency. The resulting benchmark evaluates models by measuring whether they assign higher likelihood to the factual completion versus the non-factual alternatives, providing a controlled assessment of factuality that can be tailored to specific domains.

## Key Results
- FACTOR scores increase with model size and improve when models are augmented with retrieval
- FACTOR accuracy and perplexity are correlated but can disagree on model ranking
- When perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation as validated by human annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FACTOR evaluates factuality by comparing LM likelihood of factual vs non-factual completions
- Mechanism: The LM is tested on its ability to assign higher likelihood to the original completion over three generated non-factual alternatives, measuring factual accuracy as the percentage of correct choices
- Core assumption: Higher likelihood assigned to factual completions indicates better factuality in open-ended generation
- Evidence anchors:
  - [abstract] "FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements"
  - [section] "The model is correct on a given example if c+ = argmax log p(c|t)/|c|, where |c| is the length of completion c in tokens"
- Break condition: If non-factual completions are too easy/difficult to distinguish from factual ones, the benchmark loses discriminative power

### Mechanism 2
- Claim: Retrieval augmentation improves FACTOR accuracy by grounding the LM in external factual knowledge
- Mechanism: IC-RALM framework prepends retrieved documents to the LM's input, providing context that helps the LM identify correct completions
- Core assumption: External factual knowledge from retrieval improves the LM's ability to distinguish true from false completions
- Evidence anchors:
  - [section] "We observed consistent gains from augmenting the models with retrieval. These results highlight that grounding the model in an external corpus can improve its factual predictions"
  - [section] "We used the lexical BM25 (Robertson and Zaragoza, 2009) over the Wikipedia corpus, excluding the evaluated docs"
- Break condition: If retrieved documents are irrelevant or contain misinformation, retrieval augmentation could decrease accuracy

### Mechanism 3
- Claim: FACTOR accuracy correlates with but provides different information than perplexity
- Mechanism: While both metrics measure LM performance on Wikipedia text, perplexity captures general linguistic quality while FACTOR specifically measures factuality
- Core assumption: Perplexity improvements don't necessarily translate to factuality improvements
- Evidence anchors:
  - [section] "We observe a high correlation between the two metrics. However, there are cases where they disagree (i.e., a pair of models where one is better when measured by perplexity but worse in terms of FACTOR accuracy)"
  - [section] "GPT-J-6B has lower perplexity compared to OPT-66B (7.4 vs 7.6), while OPT-66B is significantly better in terms of FACTOR accuracy (57.7% vs 53.5%)"
- Break condition: If the benchmark corpus and perplexity evaluation corpus are too different, correlation may break down

## Foundational Learning

- Concept: Contrastive evaluation
  - Why needed here: FACTOR uses multiple-choice format with one factual and three non-factual completions to test the LM's ability to discern between similar but contradictory statements
  - Quick check question: What distinguishes contrastive evaluation from traditional fact verification approaches?

- Concept: NLI-based contradiction detection
  - Why needed here: The pipeline uses natural language inference models to verify that generated completions actually contradict the original statements
  - Quick check question: How does NLI help ensure non-factual completions are valid test cases rather than just different statements?

- Concept: Edit-distance minimization
  - Why needed here: Non-factual completions are generated by making minimal edits to the original completion to ensure they're similar in style and fluency
  - Quick check question: Why is edit-distance important for creating challenging test cases?

## Architecture Onboarding

- Component map:
  Corpus selection → Prefix/completion selection → Non-factual generation → Filtering (NLI + LM) → Benchmark creation → LM evaluation
  External dependencies: InstructGPT for generation, DeBERTa for NLI, GPT2 for fluency scoring, BM25 retriever

- Critical path:
  1. Corpus selection and preprocessing
  2. Prefix/completion extraction from corpus
  3. Non-factual completion generation via InstructGPT
  4. Filtering through NLI and fluency checks
  5. Benchmark assembly and LM evaluation

- Design tradeoffs:
  - Using InstructGPT vs rule-based perturbations: Higher quality but more expensive
  - Three non-factual alternatives: Balances difficulty with computational efficiency
  - Fixed edit-distance threshold: Ensures consistency but may exclude valid cases

- Failure signatures:
  - Low contrast between factual and non-factual completions (edit-distance too small)
  - High rejection rate during filtering (thresholds too strict)
  - Poor correlation with human factuality judgments

- First 3 experiments:
  1. Run pipeline on small Wikipedia subset with manual verification of each step
  2. Compare FACTOR scores with manual annotation of generated text from different LMs
  3. Test retrieval augmentation on a single model type to validate improvement claims

## Open Questions the Paper Calls Out

The paper identifies several unresolved questions about FACTOR's applicability and limitations:

- How does FACTOR performance vary across different error types (entity, predicate, circumstance, coreference, link)? The paper mentions that error type distribution varies across datasets and that different model families perform differently on Wiki-FACTOR vs News-FACTOR, suggesting error types may impact performance differently.

- What is the relationship between FACTOR accuracy and model factuality on out-of-domain text? While the paper shows FACTOR correlates with factuality on Wikipedia and news, it doesn't test cross-domain generalization.

- How does FACTOR performance scale with model size beyond the tested range? The paper tests models up to 66B parameters but notes even largest models only achieve 58-68% accuracy, suggesting potential for improvement.

- How does the quality of generated non-factual completions affect FACTOR reliability? The paper uses InstructGPT to generate perturbations but doesn't analyze how generation quality impacts benchmark validity.

## Limitations

- The approach relies heavily on the quality of non-factual completions generated by InstructGPT, with limited manual validation on a subset of examples
- The method's reliance on multi-sentence prefixes and next-sentence completions limits its applicability to other text generation tasks
- Benchmarks are static snapshots of factual knowledge at a specific time, potentially limiting their utility for evaluating models on evolving information

## Confidence

**High Confidence** - The core mechanism of transforming factual corpora into contrastive benchmarks is well-validated through multiple experiments and shows consistent results across different model sizes and architectures.

**Medium Confidence** - The filtering process effectiveness and the quality of generated non-factual completions are reported with high confidence, but the manual validation was limited to a subset of examples.

**Low Confidence** - The specific causes of FACTUAL-accuracy/perplexity disagreements and the relative importance of different error types in the generated non-factual completions need more systematic investigation.

## Next Checks

1. **Cross-domain validation**: Apply FACTOR to a corpus from a different domain (e.g., scientific literature or legal documents) to test whether the filtering thresholds and edit-distance parameters generalize or require adjustment.

2. **Ablation study on error types**: Systematically remove each of the five error types from the InstructGPT prompts and measure their individual contribution to benchmark difficulty and discriminative power, identifying which types are most effective at distinguishing model factuality.

3. **Long-form generation evaluation**: Test whether FACTOR scores correlate with factuality in longer generation tasks by comparing model rankings on Wiki-FACTOR with human evaluations of model-generated Wikipedia articles or news stories of 500+ words.