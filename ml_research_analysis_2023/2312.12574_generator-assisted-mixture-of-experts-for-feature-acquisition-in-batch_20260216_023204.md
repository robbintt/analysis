---
ver: rpa2
title: Generator Assisted Mixture of Experts For Feature Acquisition in Batch
arxiv_id: '2312.12574'
source_url: https://arxiv.org/abs/2312.12574
tags:
- features
- feature
- accuracy
- batch
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of batch feature acquisition in
  classification tasks, where the goal is to select the optimal subset of unobserved
  features to query in batch, given a subset of observed features. The authors propose
  GENEX, a novel approach that combines a feature generator to reduce oracle queries,
  a mixture of experts model on heterogeneous feature spaces, and a tractable lower
  bound of the original objective.
---

# Generator Assisted Mixture of Experts For Feature Acquisition in Batch

## Quick Facts
- arXiv ID: 2312.12574
- Source URL: https://arxiv.org/abs/2312.12574
- Authors: 
- Reference count: 40
- Key outcome: GENEX achieves better trade-off between accuracy and feature acquisition cost than state-of-the-art baselines on four real datasets using a feature generator, mixture of experts, and greedy algorithm with approximation guarantees.

## Executive Summary
This paper addresses the problem of batch feature acquisition in classification tasks, where the goal is to select the optimal subset of unobserved features to query in batch, given a subset of observed features. The authors propose GENEX, a novel approach that combines a feature generator to reduce oracle queries, a mixture of experts model on heterogeneous feature spaces, and a tractable lower bound of the original objective. GENEX outperforms several state-of-the-art baselines in terms of the trade-off between accuracy and feature acquisition cost on four real datasets. The key innovation is the use of a feature generator to produce synthetic features, reducing the number of oracle queries required while maintaining high accuracy. The paper also introduces the notions of (m, m)-partial monotonicity and (γ, γ)-weak submodularity, and provides a theoretical foundation for the method.

## Method Summary
GENEX addresses batch feature acquisition by partitioning data into buckets using random hyperplane-based clustering, then training a mixture of experts models on each bucket. A feature generator creates synthetic features to reduce oracle queries, while a greedy algorithm selects feature subsets using approximation guarantees from partial monotonicity and weak submodularity properties. The method trains separate classifiers and generators for each bucket, using observed features to generate synthetic features that can replace oracle queries. At inference, test instances are assigned to buckets and the precomputed feature subsets are used for classification.

## Key Results
- GENEX outperforms state-of-the-art baselines on DP, MNIST, CIFAR100, and TinyImagenet datasets in accuracy vs. oracle query trade-off
- The feature generator reduces oracle queries by up to 60% while maintaining comparable accuracy
- The greedy algorithm provides near-optimal feature subsets with provable approximation guarantees under defined monotonicity and submodularity properties

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The feature generator reduces oracle queries while maintaining accuracy by generating high-quality synthetic features.
- **Mechanism**: The feature generator draws a subset of synthetic features conditioned on observed features and uses them directly for classification, avoiding costly oracle queries.
- **Core assumption**: The generator can produce synthetic features close enough in value to the oracle features to not significantly impact classification accuracy.
- **Evidence anchors**:
  - [abstract]: "First, we use a feature generator to draw a subset of the synthetic features for some examples, which reduces the cost of oracle queries."
  - [section]: "In our work, instead of querying all features from an oracle, we draw a feature subset from the generator and directly employ them for classification, reducing the number of oracle queries with only a marginal loss in accuracy."
  - [corpus]: Weak. No direct citations to generator performance studies in corpus.
- **Break condition**: If the generator cannot produce features sufficiently close to oracle values, classification accuracy drops below acceptable levels, negating cost savings.

### Mechanism 2
- **Claim**: Random hyperplane-based clustering partitions data into homogeneous buckets, enabling effective mixture of experts models.
- **Mechanism**: The method uses random hyperplanes to partition observations into buckets, with each bucket containing instances likely to share similar optimal features for acquisition. A mixture of experts model is then trained on these clusters.
- **Core assumption**: Instances with similar observed features will have similar optimal feature acquisition strategies.
- **Evidence anchors**:
  - [abstract]: "Second, to make the feature acquisition problem tractable for the large heterogeneous observed features, we partition the data into buckets, by borrowing tools from locality sensitive hashing and then train a mixture of experts model."
  - [section]: "Random hyperplane based clustering... mitigates bucket imbalance and achieves more equitable cluster assignments."
  - [corpus]: Weak. No direct citations to random hyperplane clustering performance studies in corpus.
- **Break condition**: If clustering fails to create homogeneous buckets, the mixture of experts model cannot effectively specialize, reducing overall accuracy.

### Mechanism 3
- **Claim**: The greedy algorithm with partial monotonicity and weak submodularity guarantees provides near-optimal feature subsets.
- **Mechanism**: The algorithm uses a greedy approach to select feature subsets, leveraging theoretical properties (m,m)-partial monotonicity and (γ,γ)-weak submodularity to provide approximation guarantees.
- **Core assumption**: The set functions GF and Gloss satisfy the defined monotonicity and submodularity properties under the stated assumptions.
- **Evidence anchors**:
  - [abstract]: "These properties allow us to design a greedy algorithm G ENEX, to compute near-optimal feature subsets, with new approximation guarantee."
  - [section]: "Theorem 3.6... provides an approximation guarantee for our greedy algorithm for solving the optimization problem."
  - [corpus]: Weak. No direct citations to the specific theoretical properties or their application in corpus.
- **Break condition**: If the set functions do not satisfy the required properties, the approximation guarantee fails, potentially leading to suboptimal feature selection.

## Foundational Learning

- **Concept**: Submodularity and greedy algorithms
  - Why needed here: The feature selection problem is NP-hard, requiring approximation algorithms. Submodular optimization provides theoretical guarantees for greedy approaches.
  - Quick check question: Can you explain why a greedy algorithm with submodularity guarantees provides a constant-factor approximation for maximizing a submodular function under a cardinality constraint?

- **Concept**: Mixture of experts models
  - Why needed here: The heterogeneous feature space requires different models to specialize on different data subsets. Mixture of experts allows this specialization while maintaining a unified framework.
  - Quick check question: How does a mixture of experts model differ from a single monolithic model in handling heterogeneous data?

- **Concept**: Variational autoencoders and feature generation
  - Why needed here: The feature generator must create synthetic features conditioned on observed features. VAEs provide a probabilistic framework for this generation.
  - Quick check question: What is the key difference between a standard autoencoder and a variational autoencoder in terms of the latent space representation?

## Architecture Onboarding

- **Component map**:
  Feature Generator -> Random Hyperplane Clustering -> Mixture of Experts -> Greedy Algorithm

- **Critical path**:
  1. Partition training data into buckets using random hyperplanes
  2. Train feature generator on observed features
  3. For each bucket, use greedy algorithm to select feature subsets
  4. Train mixture of experts models on selected features
  5. At inference, find bucket for test instance and use precomputed feature subsets

- **Design tradeoffs**:
  - Bucket size vs. model specialization: More buckets increase specialization but reduce training data per bucket
  - Generator accuracy vs. cost savings: Better generators reduce oracle queries but may require more complex models
  - Approximation guarantees vs. computational efficiency: Tighter guarantees may require more expensive computations

- **Failure signatures**:
  - Low accuracy despite low oracle queries: Generator producing poor quality features
  - High variance across buckets: Clustering not creating homogeneous partitions
  - Slow training/inference: Inefficient implementation of greedy algorithm or model training

- **First 3 experiments**:
  1. Verify clustering creates balanced buckets by checking bucket sizes and intra-bucket similarity
  2. Test generator quality by comparing generated vs. oracle features on a validation set
  3. Validate approximation guarantees by comparing greedy solution to optimal (exhaustive) search on small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design an exploration-exploitation trade-off strategy for the greedy algorithm to improve feature selection?
- Basis in paper: [inferred] The authors mention that the greedy algorithm does not provide sufficient exploration in the space of features and suggest that an ε-greedy algorithm could potentially remedy this limitation.
- Why unresolved: The current greedy algorithm selects features based solely on immediate marginal gain, which may lead to suboptimal global feature sets. An exploration-exploitation strategy would balance between exploiting known good features and exploring potentially better ones.
- What evidence would resolve it: Experiments comparing the proposed greedy algorithm with an ε-greedy variant, showing improvements in accuracy or efficiency for the feature acquisition task.

### Open Question 2
- Question: Can we develop a censored feature acquisition method that respects regulations on sensitive features while maintaining high accuracy?
- Basis in paper: [explicit] The authors discuss the potential for querying sensitive features and suggest designing methods that can identify and prevent the acquisition of features correlated with sensitive ones.
- Why unresolved: Current methods do not explicitly account for regulatory constraints or correlations with sensitive features, which could lead to ethical or legal issues.
- What evidence would resolve it: A method that can successfully identify and avoid sensitive features while maintaining comparable accuracy to uncensored methods on the same datasets.

### Open Question 3
- Question: How can we incorporate instance-specific feature acquisition costs into the feature selection process?
- Basis in paper: [inferred] The authors mention that some features may have adverse instance-specific costs (e.g., CT scans being more harmful for some people) but do not model cost in an instance-specific manner.
- Why unresolved: The current method treats all features as having equal cost, which may not reflect real-world scenarios where feature costs vary per instance.
- What evidence would resolve it: An extension of the method that can handle instance-specific feature costs and demonstrate improved performance or cost-effectiveness compared to the baseline method.

## Limitations
- The theoretical guarantees rely on specific assumptions about partial monotonicity and weak submodularity that may not hold in all practical scenarios
- The experimental evaluation lacks ablation studies to isolate the individual contributions of each mechanism
- Optimal hyperparameter settings (bucket counts, β-threshold values) across different datasets lack sensitivity analysis

## Confidence
- **High Confidence**: The general framework of combining feature generation with mixture of experts is sound and the experimental methodology is rigorous
- **Medium Confidence**: The approximation guarantees for the greedy algorithm, as they depend on specific theoretical properties that require further validation
- **Low Confidence**: The optimal hyperparameter settings (bucket counts, β-threshold values) across different datasets due to lack of sensitivity analysis

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of the feature generator, clustering mechanism, and greedy algorithm to overall performance
2. Test the method on datasets with different characteristics (e.g., more features, different feature types) to assess generalizability
3. Perform runtime analysis to verify the claimed efficiency improvements and identify potential bottlenecks in the pipeline