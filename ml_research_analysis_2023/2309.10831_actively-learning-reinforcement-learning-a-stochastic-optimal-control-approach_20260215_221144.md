---
ver: rpa2
title: 'Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach'
arxiv_id: '2309.10831'
source_url: https://arxiv.org/abs/2309.10831
tags:
- control
- state
- stochastic
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a reinforcement learning approach to stochastic
  optimal control that aims to address two key challenges: (i) the fragility of reinforcement
  learning under modeling uncertainties and (ii) the computational intractability
  of stochastic optimal control. The core idea is to use reinforcement learning to
  solve the stochastic dynamic programming equation, resulting in a controller that
  can actively explore and exploit in an optimal manner.'
---

# Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach

## Quick Facts
- arXiv ID: 2309.10831
- Source URL: https://arxiv.org/abs/2309.10831
- Reference count: 31
- Primary result: A reinforcement learning approach that combines EKF uncertainty tracking with DDPG to achieve stability and performance in stochastic optimal control under modeling uncertainties.

## Executive Summary
This paper proposes a novel reinforcement learning framework for stochastic optimal control that addresses the fragility of RL under modeling uncertainties and the computational intractability of stochastic optimal control. The approach combines an extended Kalman filter to track system uncertainties with deep deterministic policy gradient reinforcement learning applied to the resulting uncertainty propagation dynamics. This allows the controller to actively balance exploration and exploitation in real-time, achieving superior stability and performance compared to traditional methods like LQR. The framework equips reinforcement learning with caution and probing capabilities that are automatically employed based on the current state of uncertainty.

## Method Summary
The method implements a reinforcement learning approach to stochastic optimal control by using an extended Kalman filter (EKF) to track system uncertainties and then applying deep deterministic policy gradient (DDPG) reinforcement learning on the resulting uncertainty propagation dynamics. The EKF provides finite-dimensional approximations of the information state (mean and covariance), which are used as inputs to the DDPG algorithm. The reward signal is designed to incorporate both performance objectives and uncertainty penalties, enabling the learned policy to balance safety (caution) and active learning (probing). The approach is demonstrated on a numerical example comparing its performance against a Linear Quadratic Regulator baseline.

## Key Results
- The proposed approach achieves stability and performance in the presence of uncertainties where LQR fails due to filter divergence
- The controller automatically balances caution and probing in real-time based on current uncertainty estimates
- The framework provides a general solution that overcomes computational intractability of traditional stochastic optimal control methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach achieves stability and performance in the presence of uncertainties by combining caution and probing in real-time.
- Mechanism: By using reinforcement learning to solve the stochastic dynamic programming equation, the controller can actively explore and exploit in an optimal manner. The extended Kalman filter tracks system uncertainties, and the deep deterministic policy gradient reinforcement learning is applied on the resulting uncertainty propagation dynamics. This allows the controller to automatically employ caution (avoiding undesirable outcomes) and probing (gathering information about uncertain parameters) based on the current state of uncertainty.
- Core assumption: The extended Kalman filter provides a sufficient approximation to the information state for the reinforcement learning algorithm to make optimal decisions.
- Evidence anchors:
  - [abstract]: "Specifically, an extended Kalman filter is used to track system uncertainties, and deep deterministic policy gradient reinforcement learning is applied on the resulting uncertainty propagation dynamics."
  - [section IV.A]: "We replace the infinite dimensional information state πk|k by a finite dimensional approximate one, namely, the state conditional mean vector ˆxk|k and covariance matrix Σk|k."
- Break condition: If the extended Kalman filter fails to accurately track the system uncertainties, the reinforcement learning algorithm may make suboptimal decisions, leading to instability or poor performance.

### Mechanism 2
- Claim: The approach overcomes the computational intractability of stochastic optimal control by using reinforcement learning to approximate the solution to the stochastic dynamic programming equation.
- Mechanism: Reinforcement learning algorithms, such as deep deterministic policy gradient, can learn to approximate the optimal control policy without explicitly solving the stochastic dynamic programming equation. This allows the approach to handle high-dimensional problems that would be computationally prohibitive using traditional methods.
- Core assumption: The reinforcement learning algorithm can learn a good approximation of the optimal control policy given sufficient training data and a well-designed reward signal.
- Evidence anchors:
  - [abstract]: "On the other hand, the resulting stochastic optimal control reinforcement learning agent admits caution and probing, that is, optimal online exploration and exploitation."
  - [section I]: "In this work however, we focus on establishing a more general, less suboptimal framework."
- Break condition: If the reinforcement learning algorithm fails to converge to a good approximation of the optimal control policy, the approach may suffer from poor performance or instability.

### Mechanism 3
- Claim: The approach addresses the fragility of reinforcement learning under modeling uncertainties by incorporating caution and probing into the control policy.
- Mechanism: By using the extended Kalman filter to track system uncertainties and incorporating this information into the reinforcement learning algorithm, the approach can adapt the control policy based on the current level of uncertainty. This allows the controller to be more cautious when uncertainties are high and more exploratory when uncertainties are low, leading to improved stability and performance.
- Core assumption: The level of uncertainty tracked by the extended Kalman filter is a good indicator of the controller's need for caution or probing.
- Evidence anchors:
  - [abstract]: "Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning."
  - [section III.A]: "The vector xk in (1a) retains its Markovian property due to the whiteness of {wk}k. Moreover, the observation yk is conditionally independent when conditioned on xk; {vk}k in (1) is also white."
- Break condition: If the extended Kalman filter fails to accurately track the level of uncertainty, the controller may not appropriately balance caution and probing, leading to suboptimal performance.

## Foundational Learning

- Concept: Stochastic optimal control
  - Why needed here: The approach aims to solve a stochastic optimal control problem using reinforcement learning.
  - Quick check question: What is the key difference between stochastic optimal control and deterministic optimal control?
- Concept: Reinforcement learning
  - Why needed here: The approach uses reinforcement learning algorithms, such as deep deterministic policy gradient, to approximate the solution to the stochastic dynamic programming equation.
  - Quick check question: What is the main advantage of using reinforcement learning for stochastic optimal control problems?
- Concept: Extended Kalman filter
  - Why needed here: The approach uses an extended Kalman filter to track system uncertainties and provide the reinforcement learning algorithm with information about the current level of uncertainty.
  - Quick check question: What is the key assumption of the extended Kalman filter that allows it to approximate the information state?

## Architecture Onboarding

- Component map:
  - System dynamics model (nonlinear state-space equations) -> Extended Kalman filter (tracks state mean and covariance) -> Reinforcement learning algorithm (deep deterministic policy gradient) -> Reward signal design (quadratic stage costs with uncertainty penalties) -> Constraint handling (probabilistic state and input constraints)
- Critical path:
  1. Initialize system state and uncertainty estimates
  2. Extended Kalman filter propagates state mean and covariance
  3. Reinforcement learning algorithm selects control action based on current state and uncertainty
  4. System dynamics evolve according to control action and exogenous disturbances
  5. Extended Kalman filter updates state mean and covariance using new measurements
  6. Reinforcement learning algorithm updates policy based on new state, action, and reward
- Design tradeoffs:
  - Accuracy of extended Kalman filter vs. computational complexity
  - Exploration vs. exploitation in reinforcement learning algorithm
  - Tightness of probabilistic constraints vs. conservatism
- Failure signatures:
  - Filter divergence (state covariance becomes too large)
  - Reinforcement learning algorithm fails to converge
  - System instability or poor performance
- First 3 experiments:
  1. Implement the extended Kalman filter and verify its accuracy on a simple linear system with known parameters.
  2. Implement the deep deterministic policy gradient algorithm and verify its ability to learn a good policy on a simple control problem with known dynamics.
  3. Combine the extended Kalman filter and reinforcement learning algorithm on a simple stochastic control problem and evaluate the system's stability and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What qualifies as a "sufficient" approximation to the information state in the proposed framework?
- Basis in paper: [inferred] The paper mentions that the choice of the Bayesian filter approximation is crucial and suggests alternatives like Gaussian sum filters or incorporating higher order moments into the EKF, but does not provide a definitive answer.
- Why unresolved: The question of what constitutes a "sufficient" approximation is complex and depends on the specific application and system dynamics. Different approximations may be more suitable for different scenarios.
- What evidence would resolve it: Experimental results comparing the performance of the proposed framework with different Bayesian filter approximations on various benchmark problems could provide insights into what constitutes a "sufficient" approximation.

### Open Question 2
- Question: How does the design of the reward signal impact the balance between safety and exploration/exploitation in the learned policy?
- Basis in paper: [explicit] The paper discusses the importance of the reward signal in achieving a balance between caution (safety) and probing (active learning) and suggests several ways to design the reward signal, but does not provide a comprehensive analysis of the impact of different reward signal designs.
- Why unresolved: The relationship between the reward signal design and the learned policy's behavior is complex and depends on the specific problem and system dynamics. Different reward signal designs may lead to different trade-offs between safety and exploration/exploitation.
- What evidence would resolve it: A systematic study of the impact of different reward signal designs on the learned policy's behavior, using various benchmark problems and metrics to quantify safety and exploration/exploitation, could provide insights into the relationship between reward signal design and policy behavior.

### Open Question 3
- Question: How does the choice of the discount factor γ affect the performance of the proposed framework?
- Basis in paper: [inferred] The paper mentions the use of a discount factor γ in the cost function and reward signal, but does not provide a detailed analysis of its impact on the performance of the framework.
- Why unresolved: The discount factor γ controls the trade-off between immediate and long-term rewards, and its choice can significantly impact the learned policy's behavior. However, the optimal choice of γ depends on the specific problem and system dynamics, and there is no one-size-fits-all answer.
- What evidence would resolve it: Experimental results comparing the performance of the proposed framework with different values of the discount factor γ on various benchmark problems could provide insights into the impact of γ on the framework's performance.

## Limitations

- The approach relies on EKF approximations that may degrade with severe nonlinearities or non-Gaussian noise distributions
- Sample efficiency concerns exist for high-dimensional systems with limited training opportunities
- Probabilistic constraint satisfaction guarantees may not hold during the learning phase

## Confidence

- High Confidence: The fundamental mechanism of combining EKF with reinforcement learning for uncertainty-aware control is sound and theoretically justified
- Medium Confidence: The approach's effectiveness on the demonstrated numerical example and its ability to outperform LQR in the presence of uncertainties
- Low Confidence: Scalability to high-dimensional systems and performance under non-ideal conditions (e.g., non-Gaussian noise, severe nonlinearities)

## Next Checks

1. **EKF Robustness Test**: Evaluate the EKF's performance under varying levels of system nonlinearity and non-Gaussian noise distributions to quantify the impact on the overall approach's effectiveness.

2. **Sample Complexity Analysis**: Conduct experiments to determine the number of training episodes required for the DDPG algorithm to converge to a satisfactory policy, and assess how this scales with system dimensionality.

3. **Constraint Violation Analysis**: Implement the approach on a system with known probabilistic constraints and empirically measure the actual constraint violation rate across multiple runs to validate the theoretical guarantees.