---
ver: rpa2
title: 'Tree-Planner: Efficient Close-loop Task Planning with Large Language Models'
arxiv_id: '2310.08582'
source_url: https://arxiv.org/abs/2310.08582
tags:
- layer
- find
- walk
- plan
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a method for improving the efficiency of closed-loop
  task planning with large language models (LLMs). The approach reframes planning
  into three phases: sampling potential plans, constructing an action tree from them,
  and grounded decision-making using real-time observations.'
---

# Tree-Planner: Efficient Close-loop Task Planning with Large Language Models

## Quick Facts
- arXiv ID: 2310.08582
- Source URL: https://arxiv.org/abs/2310.08582
- Reference count: 40
- Key outcome: Achieves up to 3.65% higher success rate while reducing token costs by up to 92.24% compared to prior methods

## Executive Summary
This paper introduces Tree-Planner, a method for improving closed-loop task planning with large language models (LLMs) in embodied environments. The approach addresses the inefficiency of traditional iterative planning by restructuring the process into three phases: plan sampling, action tree construction, and grounded deciding. By separating global context from per-step execution and enabling flexible backtracking through an action tree, the method significantly reduces token consumption and error correction frequency while maintaining or improving task success rates.

## Method Summary
Tree-Planner reframes closed-loop task planning as a three-phase process. First, it samples multiple candidate plans using a single LLM call with global context (task instructions, environment background, and in-context examples). Second, it constructs an action tree by aggregating these plans, merging common prefixes to create a compact representation. Third, it executes the plan through sequential grounded deciding calls that only include local information (current observation and history), allowing backtracking to the last valid fork node when errors occur. This structure reduces repetitive token consumption and enables more flexible error correction compared to iterative or local/global replanning baselines.

## Key Results
- Achieves up to 3.65% higher success rate compared to iterative planning methods
- Reduces token costs by up to 92.24% through efficient context reuse
- Decreases error corrections by 40.5% using action tree backtracking mechanism
- Demonstrates 81.2% maximum goal conditions recall (GCR max) in plan sampling phase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reframing planning into three phases reduces redundant token consumption by separating global context from per-step execution.
- Mechanism: In plan sampling, global info (instructions, environment background, in-context examples) is used once. In grounded deciding, only local info (current observation, history) changes per step, avoiding repeated token charges for static context.
- Core assumption: LLM token pricing is based on total prompt tokens, so static context reuse reduces overall cost.
- Evidence anchors:
  - [abstract]: "By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed."
  - [section 3.1]: "The prompt for plan sampling typically includes global information and in-context examples, while the prompt for grounded deciding includes observation and history."
  - [corpus]: No direct corpus evidence for this exact mechanism.

### Mechanism 2
- Claim: Action tree structure enables more flexible backtracking and reduces error correction frequency.
- Mechanism: When an action fails, invalid nodes are marked, and the model backtracks to the last valid fork node instead of restarting from scratch or retrying locally. This avoids repeated decisions at the same step and limits correction scope.
- Core assumption: Marking subtrees as invalid and backtracking to a fork node reduces redundant error correction compared to LOCAL REPLAN or GLOBAL REPLAN.
- Evidence anchors:
  - [abstract]: "by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections."
  - [section 3.3]: "When a chosen action fails to execute in the environment, TREE-PLANNER (i) marks the nodes on the subtree rooted at the failed node as invalid nodes; (ii) traces back on the action tree to find the previous valid fork node with available valid child nodes."
  - [corpus]: No direct corpus evidence for this exact mechanism.

### Mechanism 3
- Claim: Plan sampling with diverse temperature settings yields a set of candidate plans, increasing the chance that at least one is correct.
- Mechanism: Sampling multiple plans (N plans) before execution increases the upper bound of achievable GCR, as grounded deciding can pick the best available plan.
- Core assumption: LLM can generate multiple diverse plans with some containing correct subsequences, and grounded deciding can select the best among them.
- Evidence anchors:
  - [section 3.1]: "We take 4 representative tasks from the dataset as in-context learning exemplars and the rest as validation set. The examples are fixed to be: 'Watch TV', 'Turn on light', 'Go to sleep', and 'Brush teeth'."
  - [section 5.2]: "The maximum value of GCR max being 81.2% indicates that plan sampling is effective."
  - [corpus]: No direct corpus evidence for this exact mechanism.

## Foundational Learning

- Concept: POMDPs (Partially Observable Markov Decision Processes)
  - Why needed here: The task planning problem is modeled as a POMDP where the agent must plan under partial observability and adapt based on observations.
  - Quick check question: In a POMDP, what does the agent use to decide actions at each step besides the current observation?

- Concept: Token efficiency in LLM API calls
  - Why needed here: The method's efficiency gain relies on understanding how token consumption is charged and how to minimize repeated charges.
  - Quick check question: If a prompt contains 1000 static tokens and 10 dynamic tokens per step, how many tokens are charged for 5 steps using iterative planning vs plan sampling + grounded deciding?

- Concept: Tree-based search and backtracking
  - Why needed here: The action tree enables efficient backtracking by marking invalid subtrees and returning to the last valid fork.
  - Quick check question: In a binary tree of depth 3, if a leaf node fails, how many ancestor nodes must be checked to find the last valid fork?

## Architecture Onboarding

- Component map: Plan Sampling -> Action Tree Construction -> Grounded Deciding -> Error Handler
- Critical path:
  1. Generate N plans via plan sampling.
  2. Build action tree from sampled plans.
  3. Execute plan via grounded deciding, backtracking on errors.
  4. Terminate when task goal is met or max steps reached.
- Design tradeoffs:
  - Larger N increases chance of correct plan but also token cost and tree size.
  - Tree depth affects backtracking efficiency; too deep may slow correction.
  - Temperature in sampling balances diversity vs coherence of plans.
- Failure signatures:
  - High error correction count: Tree may be too sparse or plans too diverse.
  - Low GCR max: Plan sampling not capturing correct strategies.
  - Token cost not reduced: Static context not large enough to offset per-step costs.
- First 3 experiments:
  1. Compare token cost and success rate for N=1 vs N=10 to find sweet spot.
  2. Test error correction count with and without action tree to confirm benefit.
  3. Vary temperature in plan sampling to measure diversity vs plan quality tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of N (number of sampled plans) that balances token efficiency and performance?
- Basis in paper: [explicit] The paper discusses the trade-off between increasing N and its impact on token consumption and performance, but does not provide an optimal value.
- Why unresolved: The optimal value likely depends on the specific task and environment, and may vary across different scenarios.
- What evidence would resolve it: Experiments comparing performance and token efficiency across a range of N values for different tasks and environments.

### Open Question 2
- Question: How can the upper limit of plan sampling (GCR max) be further improved to increase the overall performance of TREE-PLANNER?
- Basis in paper: [explicit] The paper mentions that plan sampling serves as a bottleneck for the model's performance, with GCR max reaching only 81.2%.
- Why unresolved: The paper does not explore methods to break this upper limit or improve plan sampling.
- What evidence would resolve it: Results from experiments incorporating techniques like plan re-sampling during grounded deciding or other methods to improve plan sampling quality.

### Open Question 3
- Question: How can the deciding error rate be reduced in TREE-PLANNER?
- Basis in paper: [explicit] The error analysis shows that 31.8% of errors are due to incorrect decisions during grounded deciding.
- Why unresolved: The paper does not explore specific techniques to reduce these errors beyond the current majority vote approach.
- What evidence would resolve it: Results from experiments incorporating LLM-related techniques like chain-of-thought or self-reflection during grounded deciding.

## Limitations
- The method's efficiency gains rely on assumptions about LLM token pricing that may vary across API providers or change over time.
- The paper does not provide ablation studies on the optimal number of sampled plans (N), leaving uncertainty about whether reported performance is near-optimal.
- Evaluation is limited to a single household task dataset (VirtualHome), raising questions about generalizability to other domains or more complex environments.

## Confidence
- **High confidence**: The mechanism of reducing token costs by separating static global context from dynamic per-step observations is well-supported by the described architecture and token pricing models.
- **Medium confidence**: The claim of 40.5% decrease in error corrections through action tree backtracking is supported by results but lacks ablation studies to isolate the tree structure's specific contribution.
- **Medium confidence**: The 92.24% token cost reduction claim is supported by reported metrics but depends on assumptions about token pricing that may vary across implementations.

## Next Checks
1. **Token cost sensitivity analysis**: Systematically vary N (number of sampled plans) and measure the tradeoff between GCR improvement and token cost to identify the optimal operating point.
2. **Generalization test**: Apply Tree-Planner to a different embodied task planning dataset (e.g., ALFRED) to validate cross-domain performance and identify potential limitations.
3. **Ablation of action tree components**: Compare Tree-Planner performance with and without the action tree structure to isolate its specific contribution to error correction reduction.