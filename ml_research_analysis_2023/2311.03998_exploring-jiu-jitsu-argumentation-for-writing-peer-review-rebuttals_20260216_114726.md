---
ver: rpa2
title: Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals
arxiv_id: '2311.03998'
source_url: https://arxiv.org/abs/2311.03998
tags:
- rebuttal
- review
- attitude
- canonical
- peer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task for peer review rebuttal generation
  guided by attitude roots and themes, inspired by Jiu-Jitsu argumentation. The authors
  enrich an existing peer review dataset with attitude roots, themes, and canonical
  rebuttals, and propose strong baseline strategies for end-to-end rebuttal generation
  and two related subtasks.
---

# Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals

## Quick Facts
- arXiv ID: 2311.03998
- Source URL: https://arxiv.org/abs/2311.03998
- Reference count: 33
- Primary result: Novel framework for peer review rebuttal generation guided by attitude roots and themes, achieving Pearson correlation of 0.491 on canonical rebuttal scoring task

## Executive Summary
This paper introduces a novel task for peer review rebuttal generation guided by attitude roots and themes, inspired by Jiu-Jitsu argumentation. The authors enrich an existing peer review dataset with attitude roots, themes, and canonical rebuttals, and propose strong baseline strategies for end-to-end rebuttal generation and related subtasks. Their approach leverages the idea that arguments are driven by underlying beliefs and worldviews, enabling more congenial counterarguments that align with reviewers' deeper perspectives rather than directly contradicting surface-level arguments.

## Method Summary
The paper proposes attitude root and theme-guided peer review rebuttal generation using the JITSU PEER dataset, created by enriching DISAPERE and PEER_REVIEW_ANALYZE datasets with attitude roots, themes, and canonical rebuttals. The method employs transformer-based models (BERT, RoBERTa, SciBERT) with domain specialization options, training them on three main tasks: canonical rebuttal scoring, review description generation, and end-to-end rebuttal generation. The approach uses Jiu-Jitsu argumentation principles to generate rebuttals that address reviewers' underlying attitudes rather than just surface-level concerns.

## Key Results
- Best model achieves Pearson correlation of 0.491 and MAP of 0.488 on canonical rebuttal scoring task
- SciBERTds_all achieves highest Pearson correlation across all tasks
- All models exhibit steep learning curves, roughly doubling performance with just one example
- Domain specialization improves performance, with SciBERTds_neg outperforming non-specialized variants by 1 percentage point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attitude roots and themes provide effective anchors for generating rebuttals that align with reviewers' underlying beliefs
- Mechanism: By identifying reviewers' underlying beliefs (attitude roots) and thematic concerns (themes), the system can select or generate rebuttals that are more congenial to the reviewer's perspective rather than directly contradicting their surface-level arguments
- Core assumption: Reviewers' arguments are primarily driven by deeper attitudes rather than just surface-level reasoning
- Evidence anchors: Theoretical grounding in Jiu-Jitsu argumentation, but weak empirical validation

### Mechanism 2
- Claim: Domain specialization improves model performance for peer review tasks
- Mechanism: Models pre-trained on scientific text (SciBERT) and further fine-tuned on peer review data show better performance than general-purpose models
- Core assumption: Peer review text has domain-specific characteristics that general language models don't capture
- Evidence anchors: SciBERTds_neg outperforms non-specialized counterparts by 1 percentage point

### Mechanism 3
- Claim: Few-shot learning is effective for rebuttal generation tasks
- Mechanism: The system can quickly adapt to rebuttal generation with minimal training examples
- Core assumption: Rebuttal generation tasks have a limited space of possible outputs (templates) that can be learned quickly
- Evidence anchors: Models double performance with just one example and plateau after two shots

## Foundational Learning

- Concept: Peer review discourse structure and annotation schemes
  - Why needed here: The system builds on existing peer review datasets and their annotation schemes to create the new JITSU PEER dataset
  - Quick check: What are the main reviewing aspects and paper sections used as attitude roots and themes?

- Concept: Computational argumentation and argument quality assessment
  - Why needed here: The system needs to understand how to evaluate argument quality and generate appropriate counterarguments
  - Quick check: How does the system measure the suitability of rebuttals as canonical responses?

- Concept: Transformer-based language models and domain adaptation
  - Why needed here: The system uses various transformer models and domain specialization techniques
  - Quick check: What is the difference between the general-purpose and domain-specialized model variants used?

## Architecture Onboarding

- Component map: DISAPERE → theme prediction → cluster description generation → canonical rebuttal identification → model training and evaluation

- Critical path: 1) Predict themes using PEER_REVIEW_ANALYZE-trained models, 2) Generate cluster descriptions, 3) Identify canonical rebuttals through pairwise annotation and ranking, 4) Train baseline models for each proposed task

- Design tradeoffs: Manual vs automatic cluster descriptions (higher quality vs scalability), domain specialization (better performance vs increased complexity), few-shot vs full fine-tuning (data efficiency vs potential for overfitting)

- Failure signatures: Low inter-annotator agreement on cluster descriptions or canonical rebuttals, models performing similarly to random baselines, significant performance gap between domain-specialized and non-specialized models

- First 3 experiments: 1) Reproduce theme prediction results on PEER_REVIEW_ANALYZE test set, 2) Run canonical rebuttal scoring with different model variants, 3) Test few-shot performance on review description generation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key limitations of the attitude root and theme-guided approach for peer review rebuttal generation compared to more general rebuttal generation methods?
- Basis in paper: The authors discuss limitations including the dataset's focus on a specific domain and the potential need for adaptation in other fields
- Why unresolved: The paper doesn't provide a direct comparison of their approach to more general rebuttal generation methods
- What evidence would resolve it: A comparative study of the attitude root and theme-guided approach against other rebuttal generation methods

### Open Question 2
- Question: How can the performance of the models on the end-to-end canonical rebuttal generation task be improved, especially in few-shot and zero-shot settings?
- Basis in paper: The authors note that models' performance improves significantly with few examples
- Why unresolved: The paper doesn't explore specific strategies for improving performance in few-shot and zero-shot settings
- What evidence would resolve it: Experiments evaluating impact of different data augmentation techniques and transfer learning approaches

### Open Question 3
- Question: How can the concept of attitude roots and themes be further refined and validated in the context of peer review?
- Basis in paper: The authors rely on existing concepts like review aspects and paper sections to define attitude roots and themes
- Why unresolved: The paper doesn't provide a detailed analysis of the validity and reliability of the derived concepts
- What evidence would resolve it: A comprehensive study evaluating the validity and reliability of the attitude roots and themes

## Limitations
- Weak empirical evidence supporting the core mechanism of attitude-root-based rebuttals
- Dataset enrichment relies heavily on human annotation, introducing potential subjectivity
- Limited variety of canonical rebuttals due to original dataset's incompleteness

## Confidence
- **High**: Technical implementation of baseline models and their domain specialization benefits
- **Medium**: Effectiveness of few-shot learning for these tasks
- **Low**: Core Jiu-Jitsu argumentation mechanism's superiority over conventional approaches

## Next Checks
1. Conduct a controlled study comparing Jiu-Jitsu-based rebuttals against traditional surface-level rebuttals, measuring reviewer satisfaction and acceptance rates
2. Evaluate the models on peer reviews from conferences outside the ICLR domain to assess generalization across different research communities
3. Implement a blind comparison study where human reviewers rate the quality and appropriateness of generated rebuttals from the proposed system versus strong non-attitude-based baselines