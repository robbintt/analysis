---
ver: rpa2
title: 'RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained
  Correctional Human Feedback'
arxiv_id: '2312.00849'
source_url: https://arxiv.org/abs/2312.00849
tags:
- image
- data
- arxiv
- hallucination
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLHF-V introduces fine-grained correctional human feedback to address
  hallucination problems in multimodal large language models (MLLMs). Instead of using
  coarse ranking labels, the method collects segment-level corrections where annotators
  directly fix hallucinated segments in model responses.
---

# RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback

## Quick Facts
- **arXiv ID**: 2312.00849
- **Source URL**: https://arxiv.org/abs/2312.00849
- **Reference count**: 40
- **Primary result**: Reduces hallucination rate by 34.8% while maintaining helpfulness using 1.4k fine-grained correctional feedback samples

## Executive Summary
RLHF-V introduces a novel approach to reducing hallucinations in multimodal large language models (MLLMs) by leveraging fine-grained correctional human feedback. Instead of using coarse ranking labels, annotators directly correct hallucinated segments in model responses, creating dense preference pairs that provide more accurate human preference signals. The method employs Dense Direct Preference Optimization (DDPO), a variant of DPO that emphasizes corrected segments during training. Comprehensive experiments show that RLHF-V achieves state-of-the-art trustworthiness among open-source MLLMs while maintaining strong performance on helpfulness metrics.

## Method Summary
RLHF-V collects human preference data through segment-level corrections on hallucinated segments rather than coarse response rankings. Annotators directly fix hallucinated segments in model outputs, creating dense preference pairs (yw, yl). The method employs Dense Direct Preference Optimization (DDPO), which weights corrected segments more heavily during training to ensure hallucinated segments receive stronger feedback. Additionally, RLHF-V mitigates hallucination sources by fine-tuning on high-quality visual question-answering datasets and avoiding untrustworthy image augmentations. The approach is evaluated on a 13B-parameter MLLM using 1.4k preference samples collected from instruction tuning datasets and GPT-4 generated prompts.

## Key Results
- Reduces object hallucination rate by 34.8% on Object HalBench and MHumanEval benchmarks
- Maintains strong performance on helpfulness metrics (LLaVA Bench, VQA-v2)
- Achieves state-of-the-art performance in trustworthiness among open-source MLLMs
- Demonstrates superior robustness against domain-specific hallucinations compared to GPT-4V

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained segment-level corrections provide more direct and accurate human preference signals than coarse ranking labels.
- Mechanism: Instead of ranking entire responses, annotators directly fix hallucinated segments, creating dense preference pairs that eliminate linguistic variance and non-robust bias.
- Core assumption: Segment-level corrections capture the true human preference more accurately than overall response rankings.
- Evidence anchors: [abstract] "We propose to collect human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback." [section 2] "The annotation simultaneously yields a segment-level incremental preference pair (yw, yl). The simple procedure effectively addresses the challenges..."

### Mechanism 2
- Claim: Dense Direct Preference Optimization (DDPO) better exploits fine-grained human feedback by emphasizing corrected segments during training.
- Mechanism: DDPO weights corrected segments more heavily in the likelihood calculation, ensuring hallucinated segments receive stronger feedback to become factually grounded.
- Core assumption: Corrected segments contain more critical information about human preferences than unchanged segments.
- Evidence anchors: [section 3.1] "We propose to score the response as a weighted aggregation of the fine-grained segments... where γ > 1 is a weighting hyperparameter, and larger γ means more contribution from the corrected segments." [section 4.3] "Learning human feedback with vanilla DPO leads to performance degrades, showing the advantage of DDPO in exploiting the fine-grained human preference."

### Mechanism 3
- Claim: Mitigating vision-language mismatch sources reduces hallucination without requiring human feedback.
- Mechanism: High-quality VQA datasets provide accurate learning signals to counter low-quality text influence, while avoiding untrustworthy image augmentations prevents semantic inconsistencies.
- Core assumption: Some hallucinations arise from training data mismatches rather than model capability limitations.
- Evidence anchors: [section 3.2] "We find that the influence can be countered by simply posttraining MLLMs on high-quality visual question-answering datasets." [section 4.3] "Fine-tuning on VQAv2 leads to a significant reduction in hallucination rates compared with the base model."

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF) fundamentals
  - Why needed here: Understanding traditional RLHF helps appreciate why segment-level corrections and DDPO are innovations
  - Quick check question: What are the two main challenges with traditional RLHF mentioned in the paper?

- **Concept**: Direct Preference Optimization (DPO) mechanics
  - Why needed here: DDPO is a variant of DPO, so understanding the base algorithm is crucial
  - Quick check question: How does DPO simplify the traditional RLHF objective compared to using a reward model?

- **Concept**: Vision-language model training paradigms
  - Why needed here: The paper addresses hallucination sources in MLLM training, requiring understanding of standard approaches
  - Quick check question: What are the two main sources of hallucination identified in the paper's analysis of MLLM training?

## Architecture Onboarding

- **Component map**: Input image and prompt → visual encoder (BEiT-3) → LLM (13B Vicuna) generates response → compare with corrections → DDPO applies weighted loss focusing on corrected segments → VQA fine-tuning provides additional calibration

- **Critical path**: 1. Input image and prompt → visual encoder; 2. LLM generates response → compare with corrections; 3. DDPO applies weighted loss focusing on corrected segments; 4. VQA fine-tuning provides additional calibration

- **Design tradeoffs**: Segment-level corrections provide better signals but require more complex annotation; DDPO weighting needs hyperparameter tuning (γ); excluding image cropping may reduce data diversity but improves trustworthiness

- **Failure signatures**: If DDPO weighting is too aggressive, model may produce overly conservative responses; if VQA fine-tuning is insufficient, hallucinations from pre-training data persist; if segment corrections are ambiguous, model learns incorrect patterns

- **First 3 experiments**: 1. Compare DDPO vs vanilla DPO on the same correction data to validate weighting importance; 2. Test different γ values to find optimal segment emphasis; 3. Evaluate hallucination reduction when excluding different image augmentations (cropping, rotation, etc.)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does RLHF-V performance scale with increasing amounts of preference data beyond 1.4k samples?
  - Basis in paper: [explicit] The paper states "Based on this tendency, we expect better performance can be achieved with an increasing amount of feedback data. We leave this for future work."
  - Why unresolved: The current experiments only tested up to 1.4k preference data samples, and the authors explicitly note this as future work.
  - What evidence would resolve it: Systematic experiments with increasing amounts of preference data (e.g., 2k, 5k, 10k samples) showing the relationship between data quantity and hallucination reduction rates.

- **Open Question 2**: Can RLHF-V's fine-grained correctional feedback approach be effectively applied to reduce hallucinations in pure language models (LLMs) without visual components?
  - Basis in paper: [explicit] The paper states "we note that the framework of RLHF-V can potentially also help reduce the hallucinations in LLMs, which we will explore in future."
  - Why unresolved: The current work focuses exclusively on multimodal models, and the authors acknowledge this as a future direction.
  - What evidence would resolve it: Implementation of RLHF-V's feedback collection and DDPO methods on a pure LLM benchmark like summarization or QA tasks, showing hallucination reduction compared to standard RLHF.

- **Open Question 3**: What is the optimal weighting parameter γ in the dense direct preference optimization (DDPO) method, and how sensitive is performance to its value?
  - Basis in paper: [inferred] The paper mentions "γ > 1 is a weighting hyperparameter" but only reports using γ=5 without exploring sensitivity.
  - Why unresolved: The paper only uses a single value (γ=5) and doesn't report ablation studies or sensitivity analysis for this crucial hyperparameter.
  - What evidence would resolve it: Systematic ablation studies varying γ (e.g., 1, 3, 5, 7, 10) to identify optimal values and show performance sensitivity to this parameter.

## Limitations
- Data Scaling Constraint: The study uses only 1.4k preference samples, and it remains unclear whether the approach would scale proportionally with larger datasets.
- Architectural Specificity: The evaluation focuses on the Muffin model architecture, and generalizability to other MLLM architectures remains untested.
- Evaluation Scope: Trustworthiness assessment is primarily limited to object hallucination detection, not broader factual consistency or temporal consistency.

## Confidence
- **High Confidence**: The mechanism of segment-level corrections providing more accurate preference signals than coarse ranking is well-supported by the evidence.
- **Medium Confidence**: The claim of achieving "state-of-the-art performance in trustworthiness among open-source MLLMs" is supported within the tested benchmarks.
- **Low Confidence**: The scalability of the approach beyond 1.4k samples and its effectiveness across diverse MLLM architectures remains uncertain.

## Next Checks
1. **Architecture Transfer Test**: Apply RLHF-V to at least two additional MLLM architectures to validate generalizability across model families.
2. **Data Scaling Experiment**: Systematically increase the preference data volume from 1.4k to 10k+ samples to identify performance scaling behavior.
3. **Cross-Domain Generalization**: Evaluate the trained model on out-of-distribution visual domains (medical imaging, satellite imagery, abstract art) to assess hallucination reduction transfer.