---
ver: rpa2
title: A general learning scheme for classical and quantum Ising machines
arxiv_id: '2310.18411'
source_url: https://arxiv.org/abs/2310.18411
tags:
- ising
- quantum
- machine
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new parametric learning model based on the
  Ising model structure, which can be efficiently trained using gradient descent.
  The key idea is to use the Ising machine itself to estimate the partial derivatives
  of the loss function, instead of explicitly calculating them.
---

# A general learning scheme for classical and quantum Ising machines

## Quick Facts
- arXiv ID: 2310.18411
- Source URL: https://arxiv.org/abs/2310.18411
- Authors: 
- Reference count: 40
- Key outcome: A new parametric learning model based on the Ising model structure, trained using gradient descent with the Ising machine itself estimating partial derivatives of the loss function.

## Executive Summary
This paper introduces a novel parametric learning model that leverages the structure of Ising machines for general learning tasks. The key innovation is using the Ising machine itself to estimate gradients of the loss function, eliminating the need for explicit gradient computation. The model encodes input information into biases of an Ising model while using couplings as adjustable parameters, with the ground state energy serving as the output. The authors demonstrate successful training on simple function approximation and binary classification tasks using both simulated and quantum annealing, highlighting the potential of Ising machines for general learning applications.

## Method Summary
The proposed model is defined by the ground state energy of an Ising model, where input information is encoded into the biases and couplings represent adjustable parameters. Training involves optimizing these parameters to minimize mean squared error loss between predicted and actual values. The key mechanism is that partial derivatives of the loss function are estimated by measuring spin configurations (z_i z_j) returned by the Ising machine, rather than being explicitly calculated. The model can be enhanced with hidden spins to increase expressive power, though this requires careful initialization when input dimensions are low.

## Key Results
- Successfully trained the model on polynomial function approximation tasks using simulated and quantum annealing
- Demonstrated binary classification capability on bars and stripes dataset
- Showed that the model can generalize from training data to unseen inputs
- Verified that hidden spins can improve model expressiveness for simple functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can be trained without explicitly computing gradients by using the Ising machine itself to estimate them.
- Mechanism: Partial derivatives of the loss function with respect to couplings are estimated by measuring spin configuration products (z_i z_j) from the Ising machine. The machine runs to get ground state spin configuration for each training sample, then uses that configuration to compute gradient estimates.
- Core assumption: Ground state spin configuration returned is close enough to true ground state for accurate gradient estimates.
- Evidence anchors: [abstract] "use the Ising machine itself to estimate the partial derivatives of the loss function, instead of explicitly calculating them"; [section 3.2] "approximated ground state does not match the exact solution...is not a severe drawback for the learning process."

### Mechanism 2
- Claim: The proposed model can generalize from training data to unseen inputs.
- Mechanism: Optimizing parameters to minimize MSE on training set learns a mapping from input biases to output energies that approximates the target function. Ising energy landscape complexity enables non-linear mappings despite linear dependence on input biases.
- Core assumption: Training set is representative and Ising model has sufficient expressive power.
- Evidence anchors: [section 3.2] "expectation that, if the model is trained on an extensive dataset, it can assimilate and generalize"; [section 4.3] experimental results showing trained model approximating polynomial functions.

### Mechanism 3
- Claim: Adding hidden spins increases the model's expressive power.
- Mechanism: Additional hidden spins increase tunable couplings in Ising model, allowing more complex energy landscapes and thus more complex functions. Hidden spins initialized with offset values to break symmetry and make them distinguishable.
- Core assumption: Hidden spins are usefully initialized and increased parameters sufficiently improve approximation capability.
- Evidence anchors: [section 3.3] "number of tunable parameters Î“ ij scales quadratically with respect to the input dimension"; [section 4.3] experimental results showing improved performance with more hidden spins for linear function.

## Foundational Learning

- Concept: Ising model and ground state
  - Why needed here: The entire learning scheme is based on Ising model structure and using ground state energy as model output.
  - Quick check question: What is the energy function of the Ising model and how is the ground state defined?

- Concept: Gradient descent and backpropagation
  - Why needed here: Training uses gradient descent to optimize parameters, and understanding backpropagation helps understand novel approach of estimating gradients via Ising machine.
  - Quick check question: How does backpropagation compute gradients in a neural network, and how does this differ from the proposed approach?

- Concept: Quantum annealing and simulated annealing
  - Why needed here: These are the Ising machines used to find ground state of Ising model, both for model execution and training.
  - Quick check question: What is the difference between quantum annealing and simulated annealing, and how do they find ground state of an Ising model?

## Architecture Onboarding

- Component map:
  Input preprocessing -> Ising machine (ground state search) -> Model output (scaled ground state energy) -> Training loop (gradient estimates and parameter updates)

- Critical path:
  1. Input preprocessing
  2. Ising machine execution (ground state search)
  3. Model output computation
  4. Gradient estimation and parameter update
  5. Repeat until convergence

- Design tradeoffs:
  - Expressiveness vs. parameter count: More hidden spins increase expressive power but also increase computational cost and risk of overfitting
  - Accuracy vs. speed: More precise Ising machine solutions improve gradient estimates but take longer to compute
  - Quantum vs. classical: Quantum annealing may provide better solutions but is limited by hardware availability and noise

- Failure signatures:
  - Training loss not decreasing: Poor gradient estimates due to inaccurate Ising machine solutions
  - Overfitting: Model performs well on training data but poorly on test data, possibly due to too many hidden spins or insufficient regularization
  - Poor generalization: Model fails to approximate target function well on unseen inputs, possibly due to insufficient expressive power or unrepresentative training data

- First 3 experiments:
  1. Train on a small synthetic dataset (e.g., 10 samples) to verify training loop works and parameters update correctly
  2. Train on a simple function approximation task (e.g., linear function) with small number of hidden spins to verify model can learn basic mappings
  3. Train on a binary classification task (e.g., bars and stripes) to verify model can handle different output types and encodings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the expressibility of the proposed Ising-machine-based parametric model in terms of the classes of functions it can approximate?
- Basis in paper: [explicit] The paper mentions that determining the class of functions that can be approximated by the model is an open theoretical question due to the complex interplay between the input and the model parameters.
- Why unresolved: The non-linear minimization step involved in the model makes it challenging to characterize the function classes that the model can represent.
- What evidence would resolve it: Theoretical analysis or empirical studies that characterize the function classes that the model can approximate, considering the model's structure and the properties of the Ising model.

### Open Question 2
- Question: How does the choice of the preprocessing function for hidden spins affect the model's performance?
- Basis in paper: [explicit] The paper discusses the impact of the preprocessing function on the model's performance and suggests that different initialization methods for hidden spins can influence the model's behavior.
- Why unresolved: The paper provides limited empirical evidence on the impact of different preprocessing functions and does not offer a systematic approach to choosing appropriate values.
- What evidence would resolve it: Empirical studies comparing the performance of the model with different preprocessing functions for hidden spins, considering various initialization methods and analyzing their impact on the model's ability to approximate functions.

### Open Question 3
- Question: How does the number of hidden spins affect the model's performance, especially when the input dimension is low?
- Basis in paper: [explicit] The paper mentions that when the input dimension is low, a large number of hidden spins may be necessary to have enough trainable model parameters, but particular care must be taken in choosing the corresponding new bias terms.
- Why unresolved: The paper provides limited empirical evidence on the impact of the number of hidden spins and does not offer a systematic approach to determining the optimal number of hidden spins for different tasks.
- What evidence would resolve it: Empirical studies comparing the performance of the model with different numbers of hidden spins, considering various tasks and input dimensions, and analyzing the impact on the model's ability to approximate functions and generalize to unseen examples.

## Limitations

- Limited direct corpus support for the specific mechanisms proposed, relying heavily on authors' own experiments and theoretical arguments
- Success depends on Ising machine's ability to provide sufficiently accurate ground state estimates for gradient computation, which may be challenging for larger problem instances
- Model's generalization capability requires further validation on more diverse and complex tasks beyond small proof-of-concept datasets

## Confidence

- High: The model can be trained without explicitly computing gradients by using the Ising machine itself to estimate them
- Medium: The proposed model can generalize from training data to unseen inputs
- Low: Adding hidden spins increases the model's expressive power in a predictable and controllable manner

## Next Checks

1. **Scalability test:** Evaluate the model's performance on larger datasets and higher-dimensional input spaces to assess its scalability and the impact of Ising machine accuracy on gradient estimates

2. **Comparative study:** Compare the proposed Ising machine-based learning scheme with standard gradient-based optimization methods and other parametric learning models on a range of benchmark tasks

3. **Hardware validation:** Test the model on real quantum annealing hardware to quantify the impact of hardware noise and limitations on the model's performance and assess its practical viability