---
ver: rpa2
title: 'Calibration in Machine Learning Uncertainty Quantification: beyond consistency
  to target adaptivity'
arxiv_id: '2309.06240'
source_url: https://arxiv.org/abs/2309.06240
tags:
- calibration
- uncertainty
- validation
- adaptivity
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the complementary roles of consistency and
  adaptivity in validating uncertainty quantification (UQ) for machine learning regression
  tasks. While consistency (calibration with respect to uncertainty) is commonly assessed,
  adaptivity (calibration with respect to input features) is often overlooked.
---

# Calibration in Machine Learning Uncertainty Quantification: beyond consistency to target adaptivity

## Quick Facts
- arXiv ID: 2309.06240
- Source URL: https://arxiv.org/abs/2309.06240
- Reference count: 40
- Primary result: Distinguishes between consistency and adaptivity in UQ validation using z-scores and local statistics, applied to QM9 molecular property prediction dataset

## Executive Summary
This paper addresses the critical need for comprehensive validation of uncertainty quantification in machine learning regression tasks. While consistency (calibration with respect to uncertainty) is commonly assessed, the study highlights that adaptivity (calibration with respect to input features) is often overlooked. The research proposes validation methods using z-scores and local statistics to evaluate both aspects, distinguishing between these two complementary calibration targets. Applying these methods to a molecular property prediction dataset (QM9), the work reveals good average calibration and consistency but identifies adaptivity issues, particularly for smaller molecules and those with low heteroatom fractions.

## Method Summary
The paper proposes a validation framework that distinguishes between consistency (conditional calibration with respect to uncertainty) and adaptivity (conditional calibration with respect to input features). The method employs z-scores (normalized errors) and Local Z Mean Squares (LZMS) analysis with bootstrapping to test both calibration targets. For validation, the framework calculates the fraction of valid intervals (fv) based on whether confidence intervals around local statistics contain the target value. The approach supports both equal-sized and stratified binning strategies, with stratified binning preserving natural strata in the data when appropriate.

## Key Results
- Good average calibration and consistency observed in QM9 dataset
- Adaptivity issues identified for smaller molecules and those with low heteroatom fractions
- Stratified binning shows reduced variance compared to equal-sized binning for structured datasets
- Validation framework successfully distinguishes between consistency and adaptivity calibration targets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Z-scores provide a unified metric for assessing both consistency and adaptivity in UQ validation.
- Mechanism: By normalizing errors with respect to their predicted uncertainties (Z = E/uE), the method enables direct comparison of error magnitudes across different prediction confidence levels and input conditions.
- Core assumption: Errors and uncertainties are paired one-to-one, allowing meaningful normalization.
- Evidence anchors:
  - [abstract] "The study proposes validation methods using z-scores and local statistics to evaluate both aspects."
  - [section] "Building on the works of Levi et al.40, Pernot18 and Angelopoulos et al.30 about conditional calibration, I propose here to distinguish two calibration targets... namely consistency... and adaptivity..."
  - [corpus] Weak - corpus doesn't directly address z-score methodology.
- Break condition: If errors and uncertainties are not properly paired or if the error distribution is unknown, z-scores become unreliable.

### Mechanism 2
- Claim: Local Z-Mean Squares (LZMS) analysis with bootstrapping provides robust validation of conditional calibration.
- Mechanism: By binning data and computing local statistics (mean and mean squares of z-scores) within each bin, the method tests whether calibration holds locally. Bootstrapping generates confidence intervals to account for finite sample sizes.
- Core assumption: Local statistics can approximate individual calibration when individual testing is impossible.
- Evidence anchors:
  - [section] "To achieve a 95% coverage with a Student's distribution of z-scores, 1000 points per bin are required..."
  - [section] "The availability of confidence intervals on the local statistics of the LZM and LZMS methods provides the basis for a validation metric."
  - [corpus] Weak - corpus doesn't specifically address LZMS methodology.
- Break condition: If bin sizes are too small or the error distribution is highly non-normal, confidence intervals may be unreliable.

### Mechanism 3
- Claim: Stratified binning improves validation power for datasets with structured conditioning variables.
- Mechanism: By preserving natural strata (groups of data with identical or similar conditioning values) rather than using arbitrary equal-sized bins, the method ensures that statistical tests are performed on meaningful groups and reduces variance due to data ordering.
- Core assumption: Stratification reflects meaningful structure in the data that should be preserved during analysis.
- Evidence anchors:
  - [section] "For notably stratified conditioning variables, a binning scheme preserving the strata might be more appropriate than equal-size binning..."
  - [section] "However, many strata might have sizes too small to enable reliable statistics. Instead of rejecting these low-counts strata, one can merge them with their neighbors."
  - [corpus] Weak - corpus doesn't specifically address stratified binning methodology.
- Break condition: If stratification is artificial or if merging strata obscures important calibration patterns.

## Foundational Learning

- Concept: Conditional calibration and its distinction between consistency and adaptivity
  - Why needed here: The paper's central contribution is distinguishing between calibration with respect to uncertainty (consistency) and calibration with respect to input features (adaptivity), which requires understanding conditional probability concepts.
  - Quick check question: Can you explain in one sentence how consistency differs from adaptivity in UQ validation?

- Concept: Bootstrap confidence interval estimation
  - Why needed here: The validation method relies on bootstrapping to generate confidence intervals for local statistics, which is essential for the fv,ZMS validation metric.
  - Quick check question: Why is bootstrapping preferred over analytical formulas for confidence intervals of mean squares of z-scores?

- Concept: Correlation and its impact on validation
  - Why needed here: The paper discusses how correlation between conditioning variables and errors/uncertainties can create spurious features in the analysis, requiring careful variable selection.
  - Quick check question: Why might using predicted values V as a conditioning variable lead to misleading validation results?

## Architecture Onboarding

- Component map: Data preprocessing → Z-score calculation → Binning/grouping → Local statistics computation → Confidence interval estimation (bootstrapping) → Validation metric calculation (fv,ZMS) → Diagnostic visualization
- Critical path: The validation pipeline must compute z-scores correctly, apply appropriate binning strategy, and generate reliable confidence intervals to produce meaningful fv,ZMS values.
- Design tradeoffs: Equal-sized binning offers simplicity and control over bin count but may split natural strata; stratified binning preserves structure but reduces control over bin sizes and total bin count.
- Failure signatures: High variance in fv,ZMS across different binning schemes, confidence intervals that consistently miss the target value, or ACF showing strong serial correlation in LZMS values.
- First 3 experiments:
  1. Apply the LZMS analysis with equal-sized binning to a synthetic dataset with known calibration properties to verify the method works as expected.
  2. Test the impact of different bin sizes on fv,ZMS values using the QM9 dataset to identify optimal bin size for this specific application.
  3. Compare equal-sized vs. stratified binning results on the QM9 dataset to evaluate the impact of stratification on validation outcomes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective method for testing adaptivity in uncertainty quantification (UQ) when input features are complex (e.g., graphs, images, or strings)?
- Basis in paper: [inferred] The paper discusses the challenges of testing adaptivity when input features are not easily represented as numerical variables and suggests potential solutions like dimension reduction algorithms or using proxy variables.
- Why unresolved: The paper mentions potential approaches but does not provide a definitive answer or evaluate the effectiveness of these methods in practice.
- What evidence would resolve it: Comparative studies evaluating the performance of different methods (e.g., dimension reduction, proxy variables, adversarial groups) in testing adaptivity for various complex input feature types would provide evidence to support the most effective approach.

### Open Question 2
- Question: How does the interplay between data ordering and equal-size binning affect the reliability of calibration metrics in stratified datasets?
- Basis in paper: [explicit] The paper discusses the impact of data ordering on calibration metrics in stratified datasets and suggests using stratified binning as an alternative to equal-size binning.
- Why unresolved: The paper provides some evidence on the impact of data ordering but does not fully explore the implications or provide a definitive answer on the best approach to handle stratified datasets.
- What evidence would resolve it: Comparative studies evaluating the performance of equal-size binning and stratified binning in various stratified datasets, considering different levels of stratification and sample sizes, would provide evidence to support the best approach.

### Open Question 3
- Question: What are the limitations of using relative calibration error (RCE) as a metric for local calibration, and how can these limitations be addressed?
- Basis in paper: [inferred] The paper mentions the potential use of RCE as an alternative to the LZMS analysis but does not explore its limitations or provide solutions to address them.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of RCE or propose solutions to overcome them.
- What evidence would resolve it: Studies evaluating the performance of RCE in different scenarios, identifying its limitations, and proposing modifications or alternative approaches to address these limitations would provide evidence to support the use of RCE in local calibration.

## Limitations

- Framework requires large sample sizes per bin (approximately 1000 points) for reliable confidence intervals
- Assumes z-scores follow Student's distribution which may not hold for all ML models
- Sensitive to choice of conditioning variables; correlated variables can create spurious features

## Confidence

- Claims about distinguishing consistency vs. adaptivity: High confidence
- Claims about stratified binning effectiveness: Medium confidence
- Claims about fv,ZMS as reliable validation metric: Low confidence

## Next Checks

1. Apply the LZMS analysis framework to synthetic datasets with known calibration properties (perfectly calibrated, underconfident, overconfident) to verify the method correctly identifies calibration status across different scenarios.

2. Test the framework across multiple ML models (e.g., neural networks, random forests, Gaussian processes) trained on the same dataset to assess model-agnostic properties and identify model-specific calibration patterns.

3. Compare fv,ZMS values with alternative validation metrics (e.g., proper scoring rules, reliability diagrams) on the same datasets to establish relative performance and identify potential complementarities between different validation approaches.