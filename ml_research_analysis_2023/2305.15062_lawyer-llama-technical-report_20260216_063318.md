---
ver: rpa2
title: Lawyer LLaMA Technical Report
arxiv_id: '2305.15062'
source_url: https://arxiv.org/abs/2305.15062
tags:
- legal
- marriage
- llama
- chinese
- lawyer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to adapt LLMs to specific domains,
  focusing on the legal domain. The framework involves injecting domain knowledge
  during continual training, designing supervised fine-tuning tasks to teach domain-specific
  skills, and adding a retrieval module to alleviate the hallucination problem.
---

# Lawyer LLaMA Technical Report

## Quick Facts
- arXiv ID: 2305.15062
- Source URL: https://arxiv.org/abs/2305.15062
- Reference count: 28
- One-line primary result: Lawyer LLaMA outperforms LLaMA on Chinese legal tasks and generates more reliable responses with retrieval augmentation.

## Executive Summary
This paper proposes a framework for adapting large language models (LLMs) to specific domains, with a focus on the legal domain. The framework involves injecting domain knowledge through continual pre-training on legal texts, teaching domain-specific skills via supervised fine-tuning, and incorporating a retrieval module to improve response reliability. The proposed Lawyer LLaMA model is trained on a large amount of Chinese legal text and fine-tuned on real legal consultation data, demonstrating improved performance on various Chinese legal tasks compared to the original LLaMA.

## Method Summary
The method involves three key steps: 1) Continue training an LLM (LLaMA-7B) on a large corpus of Chinese legal texts to inject domain knowledge, 2) Fine-tune the model on real legal consultation data and National Judicial Examination questions to teach domain-specific skills, and 3) Incorporate a law article retrieval module based on RoBERTa to alleviate hallucination and improve response reliability. The model is evaluated on various Chinese legal tasks, including concept discrimination and scenario planning.

## Key Results
- Lawyer LLaMA outperforms the original LLaMA on various Chinese legal tasks, including concept discrimination and scenario planning.
- The model generates more reliable responses when augmented with relevant legal articles from the retrieval module.
- The proposed framework demonstrates the potential for adapting LLMs to specific domains through a combination of continual pre-training, supervised fine-tuning, and retrieval-augmented generation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting legal knowledge during continual training improves the model's understanding of domain-specific concepts.
- Mechanism: Pre-training the model on a large corpus of legal texts exposes it to domain-specific terminology and relationships between legal concepts.
- Core assumption: The model can learn the meaning and usage of legal terms from their context within the legal texts.
- Evidence anchors:
  - [abstract]: "We collect a large amount of raw text in the legal domain, such as law articles, judicial interpretations, and judicial documents of the People's Court of China. We then apply continue-training to help the model learn legal knowledge."
  - [section 2.1]: "To augment our model with Chinese legal knowledge, we collect articles from the websites of China Courts, including judgment documents, law articles, judicial interpretations, court news, and various articles for law popularization."
  - [corpus]: Weak evidence. The paper does not provide quantitative data on the size or quality of the legal corpus used for pre-training.

### Mechanism 2
- Claim: Designing supervised fine-tuning tasks helps the model learn domain-specific skills for solving practical legal problems.
- Mechanism: Creating question-answer pairs and dialogues related to legal tasks and using them to fine-tune the model teaches it how to apply legal knowledge to solve domain-specific problems.
- Core assumption: The model can learn the reasoning skills needed to solve legal problems by observing and mimicking the patterns in the fine-tuning data.
- Evidence anchors:
  - [abstract]: "We teach the model to learn professional skills using properly designed supervised fine-tuning tasks."
  - [section 3.3]: "Solving practical problems requires the model to have reasoning skills in the legal domain. To this end, we select supervised data from downstream tasks and train our model following the instruction-tuning method."
  - [corpus]: Moderate evidence. The paper mentions using real legal consultation data and questions from the National Judicial Examination, but does not provide details on the quantity or quality of the fine-tuning data.

### Mechanism 3
- Claim: Adding a retrieval module alleviates the hallucination problem and improves the reliability of the model's responses.
- Mechanism: Before generating a response, the model retrieves relevant law articles based on the user's query and incorporates them into the input. This provides the model with additional context and helps it generate more accurate and faithful responses.
- Core assumption: The retrieval model can accurately identify and retrieve the most relevant law articles for a given query, and the LLM can effectively incorporate this information into its response.
- Evidence anchors:
  - [abstract]: "Moreover, to alleviate the hallucination problem during the model's generation, we add a retrieval module and extract relevant legal articles before the model answers any queries."
  - [section 3.4]: "To make the model produce more reliable responses, we incorporate it with a law article retrieval module."
  - [corpus]: Weak evidence. The paper does not provide details on the performance of the retrieval model or how the retrieved articles are incorporated into the LLM's input.

## Foundational Learning

- Concept: Continual pre-training
  - Why needed here: To adapt a general-purpose LLM to a specific domain (legal) by exposing it to domain-specific knowledge.
  - Quick check question: What is the difference between pre-training and fine-tuning, and why is continual pre-training necessary for domain adaptation?

- Concept: Supervised fine-tuning
  - Why needed here: To teach the model domain-specific skills and reasoning patterns by providing it with examples of how to solve legal problems.
  - Quick check question: How does supervised fine-tuning differ from pre-training, and what are the key considerations when designing fine-tuning tasks for a specific domain?

- Concept: Retrieval-augmented generation
  - Why needed here: To improve the reliability of the model's responses by incorporating external knowledge (relevant law articles) into the generation process.
  - Quick check question: What are the benefits and challenges of using retrieval-augmented generation, and how can the retrieved information be effectively incorporated into the LLM's input?

## Architecture Onboarding

- Component map: Pre-trained LLM (LLaMA) -> Legal corpus for continual pre-training -> Fine-tuning data (legal consultation, exam questions) -> Law article retrieval module (RoBERTa) -> Prompt engineering for incorporating retrieved articles

- Critical path: 1) Pre-train the LLM on the legal corpus, 2) Fine-tune the model on the domain-specific tasks, 3) Incorporate the retrieval module, 4) Optimize the prompt engineering for incorporating retrieved articles

- Design tradeoffs:
  - Pre-training vs. fine-tuning: Pre-training on a large legal corpus can provide a strong foundation, but fine-tuning on specific tasks is necessary for learning domain-specific skills.
  - Retrieval vs. generation: Retrieval can improve reliability but may also introduce noise if the retrieved articles are not relevant or if the LLM fails to effectively incorporate them.

- Failure signatures:
  - Model generates irrelevant or incorrect responses
  - Model fails to incorporate retrieved articles effectively
  - Model hallucinates or makes up information

- First 3 experiments:
  1. Evaluate the model's performance on a legal concept discrimination task before and after pre-training on the legal corpus.
  2. Compare the model's performance on a legal consultation task using different fine-tuning datasets (e.g., real consultations vs. ChatGPT-generated responses).
  3. Assess the impact of the retrieval module on the model's response reliability by comparing outputs with and without retrieved articles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Lawyer LLaMA compare to other specialized legal LLMs, such as LegalBERT or LexGLUE models, on the same Chinese legal tasks?
- Basis in paper: [inferred] The paper only compares Lawyer LLaMA to the original LLaMA and ChatGLM, but does not compare it to other specialized legal LLMs.
- Why unresolved: The authors do not provide any comparison to other specialized legal LLMs, which would give a better understanding of Lawyer LLaMA's relative performance in the legal domain.
- What evidence would resolve it: Benchmark results of Lawyer LLaMA against other specialized legal LLMs on the same Chinese legal tasks used in the paper.

### Open Question 2
- Question: What is the impact of using a larger or smaller version of LLaMA (e.g., LLaMA-13B or LLaMA-65B) on the performance of Lawyer LLaMA on Chinese legal tasks?
- Basis in paper: [explicit] The paper states that experiments are based on LLaMA-7B, but does not explore the impact of using different LLaMA model sizes.
- Why unresolved: The authors only use LLaMA-7B for their experiments, so the effect of model size on performance is unknown.
- What evidence would resolve it: Results of training and evaluating Lawyer LLaMA using different LLaMA model sizes (e.g., LLaMA-13B, LLaMA-65B) on the same Chinese legal tasks.

### Open Question 3
- Question: How does the performance of Lawyer LLaMA on Chinese legal tasks change when using a more diverse or larger dataset for continual pre-training and supervised fine-tuning?
- Basis in paper: [explicit] The paper mentions the use of specific datasets for continual pre-training and supervised fine-tuning, but does not explore the impact of using different or larger datasets.
- Why unresolved: The authors only use the datasets described in the paper, so the effect of dataset size and diversity on performance is unknown.
- What evidence would resolve it: Results of training and evaluating Lawyer LLaMA using different datasets (e.g., larger or more diverse Chinese legal corpora) for continual pre-training and supervised fine-tuning on the same Chinese legal tasks.

### Open Question 4
- Question: How does the performance of Lawyer LLaMA on Chinese legal tasks change when using different retrieval strategies or retrieval models for the law article retrieval module?
- Basis in paper: [explicit] The paper mentions the use of a law article retrieval module based on RoBERTa, but does not explore the impact of using different retrieval strategies or models.
- Why unresolved: The authors only use the described retrieval module, so the effect of different retrieval strategies or models on performance is unknown.
- What evidence would resolve it: Results of training and evaluating Lawyer LLaMA using different retrieval strategies (e.g., different recall metrics, dynamic retrieval) or models (e.g., other pre-trained language models) for the law article retrieval module on the same Chinese legal tasks.

## Limitations

- The quality and representativeness of the legal corpus used for pre-training is not clearly established, which could impact the model's ability to learn domain-specific knowledge effectively.
- The details of the retrieval module's integration and performance are not fully specified, leaving questions about how well it actually alleviates the hallucination problem.
- The paper lacks quantitative data on the size and quality of the fine-tuning datasets, making it difficult to assess the robustness of the model's domain-specific skills.

## Confidence

**High Confidence:**
- The general framework of using continual pre-training, supervised fine-tuning, and retrieval-augmented generation for domain adaptation is sound and well-supported by existing literature.
- The model outperforms the original LLaMA on various Chinese legal tasks, indicating successful domain adaptation.

**Medium Confidence:**
- The claim that the retrieval module improves response reliability is supported by the paper's results, but the lack of detailed information on the retrieval model's performance and integration makes it difficult to fully assess this claim.
- The model's ability to learn domain-specific skills through supervised fine-tuning is supported by the results, but the quality and representativeness of the fine-tuning data are not clearly established.

**Low Confidence:**
- The specific impact of the legal corpus size and quality on the model's performance is not well-understood due to the lack of quantitative data.
- The generalizability of the proposed framework to other domains beyond legal is not explored in the paper.

## Next Checks

1. **Evaluate Corpus Quality and Representativeness:** Conduct an in-depth analysis of the legal corpus used for pre-training, including its size, diversity, and relevance to the target domain. This will help assess the potential impact of corpus quality on the model's performance.

2. **Assess Retrieval Module Performance:** Implement and evaluate the retrieval module separately to measure its accuracy in identifying relevant law articles for a given query. Additionally, analyze how effectively the LLM incorporates the retrieved information into its responses.

3. **Test Framework Generalizability:** Apply the proposed framework (continual pre-training, supervised fine-tuning, and retrieval-augmented generation) to a different domain (e.g., medical or financial) and compare the results to those obtained in the legal domain. This will help determine the generalizability and robustness of the approach.