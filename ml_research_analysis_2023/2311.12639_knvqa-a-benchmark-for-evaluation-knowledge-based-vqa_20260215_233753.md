---
ver: rpa2
title: 'KNVQA: A Benchmark for evaluation knowledge-based VQA'
arxiv_id: '2311.12639'
source_url: https://arxiv.org/abs/2311.12639
tags:
- evaluation
- lvlms
- answer
- visual
- knvqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KNVQA-Eval, a benchmark designed to evaluate
  the factual accuracy of large vision-language models (LVLMs) on knowledge-based
  visual question answering (VQA) tasks. Existing evaluation methods struggle with
  assessing the nuanced multimodal interactions and factual correctness of LVLMs.
---

# KNVQA: A Benchmark for evaluation knowledge-based VQA

## Quick Facts
- arXiv ID: 2311.12639
- Source URL: https://arxiv.org/abs/2311.12639
- Reference count: 31
- Key outcome: Introduces KNVQA-Eval, a benchmark for evaluating factual accuracy of large vision-language models on knowledge-based VQA tasks using human judgment and multiple evaluation methods

## Executive Summary
This paper introduces KNVQA-Eval, a benchmark designed to evaluate the factual accuracy of large vision-language models (LVLMs) on knowledge-based visual question answering (VQA) tasks. The authors identify limitations in existing evaluation methods that struggle with assessing nuanced multimodal interactions and factual correctness. To address this, they construct the KNVQA dataset incorporating human judgment and perception to evaluate the accuracy of standard answers relative to AI-generated answers. The evaluation framework employs multiple methods including lexical matching, large vision language models, and neural evaluation methods like BERT-Score.

## Method Summary
The KNVQA-Eval framework constructs a new KNVQA dataset by manually annotating 1000 cases from F-VQA and 500 cases from OK-VQA datasets with human judgments. The evaluation employs three methods: lexical matching using exact match rules, large vision language models as judges (GPT-3.5 and GPT-4), and neural evaluation using BERT-Score with a threshold of 0.5. The framework evaluates AI-generated answers from LVLMs (llava1.57b, BLIP7b, InstructBLIP7b, MiniGPT47b) against golden standard answers, providing binary classification (Yes/No) along with precision, recall, and Macro-F1 scores.

## Key Results
- KNVQA-Eval demonstrates improved evaluation capability for LVLMs compared to existing methods
- Human-annotated data provides more reliable evaluation of factual correctness than automated metrics alone
- End-to-end LVLMs can perform evaluation tasks without requiring expensive external APIs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-annotated data provides more reliable evaluation of factual correctness in KNVQA tasks than automated metrics alone.
- Mechanism: Human annotators evaluate AI-generated answers against ground truth answers, capturing semantic equivalence that exact matching cannot detect.
- Core assumption: Human judgment can reliably identify when AI-generated answers are semantically equivalent to ground truth, even when wording differs.
- Evidence anchors: [abstract] "To ensure the robustness and scalability of the evaluation, we develop a new KNVQA dataset by incorporating human judgment and perception"; [section 3.4] "The KNVQA dataset is constructed by manually annotating 1000 and 500 cases in F-VQA [17] and OK-VQA [18], respectively."

### Mechanism 2
- Claim: Using end-to-end LVLMs for evaluation avoids API costs and security risks while maintaining evaluation capability.
- Mechanism: Direct model evaluation eliminates need for expensive external APIs while providing comparable assessment of factual accuracy.
- Core assumption: End-to-end models can perform evaluation tasks comparably to external APIs without requiring internet access.
- Evidence anchors: [abstract] "This work not only comprehensively evaluates the contextual information of LVLMs using reliable human annotations"; [section 1] "Hence, we refrain from using transformer agents (e.g., GPT-3.5, ChatGPT-3.5, and ChatGPT-4) to minimize cost expenditures and risks of information leakage."

### Mechanism 3
- Claim: BERT-Score with appropriate threshold provides robust semantic similarity measurement for evaluating generated answers.
- Mechanism: BERT embeddings capture semantic meaning, allowing comparison of answers with different wording but equivalent meaning.
- Core assumption: BERT-Score with threshold T=0.5 effectively distinguishes semantically correct from incorrect answers.
- Evidence anchors: [section 3.3] "We use the BERT-Score to evaluate the similarity between the generated and the golden standard answer sequences"; [section 3.3] "If BERT âˆ’ score > T , the prediction Y is classified as positive, otherwise as negative. In our work, we set T = 0.5."

## Foundational Learning

- Concept: Exact Match vs Semantic Similarity
  - Why needed here: Understanding limitations of exact matching for VQA evaluation
  - Quick check question: Why might "Yes" and "I think it's right" be semantically equivalent but fail exact match?

- Concept: Human Annotation Reliability
  - Why needed here: Ensuring human evaluation provides consistent and unbiased assessment
  - Quick check question: What measures indicate strong inter-annotator agreement?

- Concept: Evaluation Threshold Selection
  - Why needed here: Choosing appropriate threshold for BERT-Score classification
  - Quick check question: How does changing threshold T affect precision vs recall?

## Architecture Onboarding

- Component map: Dataset construction -> Human annotation -> Model evaluation -> Error analysis -> Benchmark creation
- Critical path: Human annotation -> Model evaluation -> Error analysis -> Benchmark validation
- Design tradeoffs: Human annotation quality vs automated scalability; exact match simplicity vs semantic understanding; threshold sensitivity vs specificity
- Failure signatures: Low inter-annotator agreement; threshold-induced false positives/negatives; model evaluation inconsistencies
- First 3 experiments:
  1. Evaluate same model with different annotation groups to assess inter-rater reliability
  2. Test multiple BERT-Score thresholds on sample dataset to optimize classification
  3. Compare human vs automated evaluation on subset to validate human judgment advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation benchmarks be designed to account for the dynamic nature of factual knowledge in visual question answering, especially when answers may change over time?
- Basis in paper: [explicit] The paper mentions that answers in knowledge-based VQA tasks may change over time, and existing evaluation methods may not fully capture this aspect.
- Why unresolved: Existing evaluation methods focus on static answers and may not account for the temporal variability of factual knowledge. This limitation can lead to inaccurate assessments of model performance, especially for questions where the correct answer evolves over time.
- What evidence would resolve it: Developing evaluation benchmarks that incorporate temporal dynamics, such as time-sensitive datasets or methods that track changes in factual knowledge over time, would help resolve this issue.

### Open Question 2
- Question: What are the limitations of current evaluation methods in handling paraphrasing and synonyms in the context of visual question answering?
- Basis in paper: [explicit] The paper highlights that lexical matching methods struggle with paraphrasing and synonyms, which can lead to incorrect evaluations of model performance.
- Why unresolved: Current evaluation methods often rely on exact matches or rigid semantic similarity metrics, which fail to capture the nuances of language variation. This limitation can result in underestimating the true capabilities of models that generate correct but differently phrased answers.
- What evidence would resolve it: Developing evaluation methods that can effectively handle paraphrasing and synonyms, such as incorporating advanced semantic similarity metrics or leveraging contextual understanding, would help resolve this issue.

### Open Question 3
- Question: How can evaluation benchmarks be improved to assess the factual accuracy of large vision-language models while minimizing the impact of object hallucination?
- Basis in paper: [explicit] The paper discusses the challenge of object hallucination in large vision-language models and its impact on factual accuracy.
- Why unresolved: Current evaluation methods may not adequately account for the potential for object hallucination, which can lead to the generation of contextually irrelevant information. This limitation can result in overestimating the factual accuracy of models.
- What evidence would resolve it: Developing evaluation benchmarks that explicitly test for object hallucination and its impact on factual accuracy, such as incorporating controlled experiments or leveraging human annotations, would help resolve this issue.

## Limitations
- Human annotation quality and inter-annotator agreement are not reported, creating uncertainty about evaluation reliability
- BERT-Score threshold of 0.5 is chosen without systematic analysis of optimal threshold selection
- Paper does not address potential cultural or language biases in the OK-VQA and FVQA datasets

## Confidence

- **High Confidence**: The core claim that KNVQA-Eval improves evaluation capability over existing methods is supported by the experimental results showing better performance across multiple evaluators.
- **Medium Confidence**: The claim that human annotation provides more reliable evaluation than automated metrics is plausible but not fully validated due to missing inter-rater agreement data.
- **Low Confidence**: The assertion that end-to-end LVLMs avoid API costs and security risks while maintaining evaluation capability lacks quantitative comparison data between API-based and direct model evaluation approaches.

## Next Checks

1. **Inter-annotator Agreement Analysis**: Calculate and report Cohen's kappa or similar agreement metrics for human annotations to validate the reliability of human judgment in the evaluation framework.

2. **Threshold Sensitivity Analysis**: Systematically test BERT-Score classification performance across different threshold values (0.4, 0.5, 0.6, 0.7) to identify optimal threshold for KNVQA task and report precision-recall tradeoffs.

3. **Cross-cultural Validation**: Evaluate KNVQA-Eval on a multilingual or culturally diverse subset of visual questions to assess whether the framework maintains effectiveness across different cultural contexts and knowledge bases.