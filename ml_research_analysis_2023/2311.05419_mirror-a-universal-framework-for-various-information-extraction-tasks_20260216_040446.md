---
ver: rpa2
title: 'Mirror: A Universal Framework for Various Information Extraction Tasks'
arxiv_id: '2311.05419'
source_url: https://arxiv.org/abs/2311.05419
tags:
- mirror
- tasks
- extraction
- datasets
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Mirror, a universal framework for various information
  extraction tasks. The key idea is to reformulate IE tasks into a unified multi-slot
  tuple extraction problem and transform the tuples into a multi-span cyclic graph.
---

# Mirror: A Universal Framework for Various Information Extraction Tasks

## Quick Facts
- **arXiv ID**: 2311.05419
- **Source URL**: https://arxiv.org/abs/2311.05419
- **Reference count**: 27
- **Primary result**: Mirror achieves competitive performance with SOTA systems under few-shot and zero-shot settings across 8 information extraction tasks

## Executive Summary
Mirror is a universal framework that reformulates diverse information extraction tasks as a unified multi-span cyclic graph extraction problem. The framework uses a non-autoregressive graph decoding algorithm to extract all spans in a single step, enabling it to handle complex tasks including nested NER, discontinuous entities, n-ary relations, and classification. By pretraining on 57 datasets across 5 tasks and incorporating schema-guided instructions, Mirror demonstrates strong performance in both few-shot and zero-shot settings while being significantly faster than autoregressive baselines.

## Method Summary
Mirror reformulates IE tasks as multi-slot tuple extraction problems and transforms tuples into multi-span cyclic graphs with three edge types (consecutive, jump, tail-to-head). The framework uses a DeBERTa-v3-large encoder with biaffine attention to predict an adjacency matrix for all edge types in parallel, then decodes legal cyclic paths through thresholding and graph traversal. The model takes schema-guided inputs (instruction, schema labels, text) concatenated with special tokens, enabling zero-shot adaptation. Pretraining on 57 datasets followed by task-specific fine-tuning yields competitive performance across 8 downstream tasks.

## Key Results
- Achieves competitive F1 scores with SOTA systems in few-shot and zero-shot settings
- Outperforms other methods on multi-span discontinuous NER and n-ary hyper relation extraction
- Achieves SOTA results on multi-span discontinuous NER and n-ary hyper relation extraction tasks
- Demonstrates significant speed advantages over autoregressive models like UIE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mirror unifies diverse IE tasks by reformulating them as multi-span cyclic graph extraction
- Mechanism: Converts multi-slot tuples into graphs with consecutive, jump, and tail-to-head edge types, decoded non-autoregressively
- Core assumption: Unified graph structure captures relational and positional dependencies for all IE tasks
- Evidence anchors: [abstract] "multi-span cyclic graph extraction problem"; [section 3.2] "transform those tuples into multi-span cyclic graphs"
- Break condition: If biaffine attention fails on long-range dependencies or edge distinctions collapse in dense spans

### Mechanism 2
- Claim: Schema-guided inputs enable generalization across few-shot and zero-shot scenarios
- Mechanism: Concatenates instruction, schema labels, and text with special tokens ([I], [LM], [LR], [LC], [TL], [TP], [B])
- Core assumption: Explicit schema tokens provide sufficient signal for adaptation to unseen tasks
- Evidence anchors: [section 3.1] "Mirror takes schemas as part of the model inputs"; [section 4.3] "benefits from pretraining when utilizing instructions"
- Break condition: If instruction-schema-text ordering is inconsistent or model overfits to training schema formats

### Mechanism 3
- Claim: Non-autoregressive biaffine decoding yields faster inference while preserving exact position indexing
- Mechanism: Predicts adjacency matrix for all edge types in parallel, then decodes legal cyclic paths
- Core assumption: Parallel adjacency prediction doesn't sacrifice accuracy where span boundaries are critical
- Evidence anchors: [abstract] "non-autoregressive graph decoding algorithm to extract all spans in a single step"
- Break condition: If threshold tuning causes missing edges in high-density graphs or decoding heuristics fail on complex n-ary tuples

## Foundational Learning

- Concept: Biaffine attention for graph edge prediction
  - Why needed here: Provides pairwise scoring between tokens for all edge types in single forward pass
  - Quick check question: What is the shape of the biaffine parameter U and how does it map to edge types?

- Concept: Multi-slot tuple formulation
  - Why needed here: Allows arbitrary arity (n-ary) extraction by treating each tuple element as a slot
  - Quick check question: How does the jump connection differ from the consecutive connection in the graph?

- Concept: Schema-guided instruction tuning
  - Why needed here: Enables zero-shot adaptation by conditioning on explicit task descriptions
  - Quick check question: What role does the [TP] token play compared to [TL]?

## Architecture Onboarding

- Component map: Tokenizer -> Encoder (DeBERTa-v3-large) -> Biaffine attention (adjacency matrix) -> Graph decoding (Algorithm 1) -> Postprocessed spans
- Critical path:
  1. Tokenize input (instruction + schema + text)
  2. Forward through PLM → contextualized embeddings
  3. Apply biaffine attention → adjacency matrix
  4. Decode graph → multi-span cyclic tuples
- Design tradeoffs:
  - Non-autoregressive decoding vs autoregressive: speed vs global coherence
  - Fixed graph schema vs generative flexibility: exact positions vs ambiguous text spans
  - Parallel adjacency matrix vs sequential decoding: GPU utilization vs memory
- Failure signatures:
  - Low recall on long discontinuous spans → check adjacency thresholding
  - High false positives in dense entity clusters → examine biaffine bias
  - Zero-shot collapse → verify schema tokens not dropped during pretraining
- First 3 experiments:
  1. Ablation: Remove schema tokens ([LM], [LR], [LC]) and measure zero-shot performance drop
  2. Threshold sweep: Vary 0.5 adjacency threshold and plot precision-recall curves
  3. Speed test: Compare inference throughput with batch size scaling vs T5-large UIE baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of label span types (tags vs. content strings) impact Mirror's performance on different IE tasks?
- Basis in paper: [explicit] Experiment comparing tag-based versus content-based label spans on CoNLL03 dataset
- Why unresolved: Results provide insights for one dataset but impact across diverse tasks remains unexplored
- What evidence would resolve it: Comprehensive study across various IE tasks and datasets comparing tag-based and content-based label spans

### Open Question 2
- Question: What is the effect of different pretraining dataset compositions on Mirror's performance across various tasks?
- Basis in paper: [explicit] Ablation study on pretraining data showing varying impacts of NER, RE, MRC, and classification data
- Why unresolved: Study focuses on limited tasks and datasets; optimal composition for maximizing performance across diverse tasks is unknown
- What evidence would resolve it: Extensive experiments with varying pretraining dataset compositions evaluated across wide range of tasks

### Open Question 3
- Question: How does Mirror's performance scale with model size and pretraining data volume?
- Basis in paper: [inferred] Mirror outperforms larger generative models like DeepStruct (10B) on some datasets
- Why unresolved: Paper doesn't explore impact of scaling model size or pretraining data volume on performance
- What evidence would resolve it: Training Mirror with varying model sizes and pretraining data volumes evaluated on diverse set of tasks

## Limitations

- Graph decoding scalability concerns for very long documents and dense entity clusters due to quadratic memory growth
- Schema generalization robustness uncertain without systematic ablation studies on schema token importance
- Pretraining corpus composition and potential biases not thoroughly analyzed for true out-of-domain generalization

## Confidence

- **High confidence**: Core mechanism of reformulating IE tasks as multi-span cyclic graph extraction is technically sound and well-explained
- **Medium confidence**: Empirical results showing competitive performance across 8 tasks are promising but limited to specific baselines
- **Low confidence**: Universal pretraining strategy's generalization beyond 57-dataset corpus is asserted but not empirically validated on truly out-of-domain tasks

## Next Checks

1. **Schema ablation study**: Systematically remove each schema token type ([LM], [LR], [LC], [TP]) and measure zero-shot performance degradation to identify which instruction components are most critical for generalization

2. **Dense graph precision test**: Construct synthetic documents with varying entity density and measure precision-recall trade-offs at different adjacency thresholds to establish operational limits

3. **Cross-domain transfer validation**: Evaluate Mirror on IE tasks from domains completely absent from pretraining corpus (biomedical, legal, financial) to assess true zero-shot generalization capabilities beyond reported tasks