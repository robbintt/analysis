---
ver: rpa2
title: 'CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction,
  and its Application to Two-player Mahjong'
arxiv_id: '2307.12087'
source_url: https://arxiv.org/abs/2307.12087
tags:
- game
- tiles
- player
- information
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Counterfactual Regret Minimization (CFR) to
  the game of two-player Mahjong, which is more complex than Texas Hold'em poker due
  to the large amount of private hidden information that changes throughout the game.
  The authors conduct game-theoretic analysis and propose a hierarchical policy abstraction
  to CFR based on winning patterns.
---

# CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong

## Quick Facts
- arXiv ID: 2307.12087
- Source URL: https://arxiv.org/abs/2307.12087
- Authors: 
- Reference count: 6
- Primary result: CFR-p reduces exploitability in two-player Mahjong by separating winning pattern selection from action optimization, achieving stable performance after ~10000 iterations

## Executive Summary
This paper introduces CFR-p, a Counterfactual Regret Minimization algorithm enhanced with hierarchical policy abstraction, applied to the complex game of two-player Mahjong. The approach addresses Mahjong's high complexity due to large amounts of changing private information by decomposing the decision process into selecting winning patterns and optimizing actions for those patterns. The framework achieves significant complexity reduction from O(T1 * T2) to O(T1 + T2) while maintaining convergence to Nash equilibrium strategies.

## Method Summary
The method implements a hierarchical CFR algorithm for two-player Mahjong that first decides which winning pattern to achieve (PongPongHu, QiDui, or Normal), then finds the most efficient way to achieve it using heuristic search. The algorithm uses Monte Carlo sampling with random tile shuffling across 10000 iterations to train the policy. Exploitability is measured by playing against heuristic search-based opponents on 3000 random tile sets, with the highest opponent score representing the agent's exploitability. The game logic implements an extensive-form representation with three phases per turn, and hand states are encoded into compact feature labels.

## Key Results
- Exploitability gradually decreases during training, reaching stable levels after approximately 10000 iterations
- The hierarchical abstraction successfully reduces complexity from O(T1 * T2) to O(T1 + T2)
- Trained CFR-p agent demonstrates strong performance against heuristic search-based opponents across diverse tile arrangements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical policy abstraction reduces complexity from O(T1 * T2) to O(T1 + T2) by separating winning pattern selection from action optimization.
- Mechanism: The framework decomposes the decision process into two independent sub-tasks: (1) selecting which winning pattern to achieve, and (2) finding the most efficient way to achieve it. This separation avoids the combinatorial explosion of considering all pattern-action pairs simultaneously.
- Core assumption: Different winning patterns have distinct utilities that can be evaluated independently of the specific actions needed to achieve them.
- Evidence anchors:
  - [abstract] "The game tree complexity is reduced from O(T1 * T2) to O(T1 + T2) by separating the task into two parts: deciding which winning pattern to achieve and finding the most efficient way to achieve it."
  - [section] "Based on the fact that there are several distinct patterns, and that winning with the same pattern usually receives the same points, we separate the task of the game into two parts..."
- Break condition: If winning patterns' utilities depend heavily on specific action sequences, or if pattern selection requires considering interactions between patterns that cannot be decoupled.

### Mechanism 2
- Claim: CFR algorithm converges to Nash equilibrium by iteratively computing counterfactual regrets and updating strategies proportionally to positive cumulative regrets.
- Mechanism: At each information set, the algorithm computes counterfactual values for all actions, calculates regrets as the difference between action values and expected value, and updates strategies based on cumulative positive regrets. The average strategy converges to equilibrium as iterations increase.
- Core assumption: The game has perfect recall and the information sets can be properly defined for counterfactual reasoning.
- Evidence anchors:
  - [section] "CFR computes the utility of each action recursively. Regrets are computed from returned values, and the values of the current node is finally computed and returned."
  - [section] "The average strategy profile at information set I approaches an equilibrium as T → ∞."
- Break condition: If the game tree is too large to traverse completely, or if information sets cannot be properly defined due to imperfect recall or other game-specific issues.

### Mechanism 3
- Claim: Monte Carlo sampling with random tile shuffling provides sufficient coverage of the game space for effective training.
- Mechanism: Instead of exhaustively exploring all possible tile permutations, the training process samples random tile arrangements and runs CFR on each specific arrangement. This stochastic approach allows training to proceed efficiently while still exploring diverse game states.
- Core assumption: Random sampling of tile arrangements provides representative coverage of the game space, and the CFR algorithm can generalize from these samples.
- Evidence anchors:
  - [section] "The training process adopts CFR with Monte Carlo sampling. At every iteration, a suit of tiles is shuffled randomly and the CFR algorithm runs on this specific suit."
  - [section] "Such random training samples can hit the benchmark which shares the same feature encoding in Table 3."
- Break condition: If certain tile arrangements are underrepresented in the random sampling, leading to poor generalization, or if the number of iterations is insufficient to converge to stable strategies.

## Foundational Learning

- Concept: Counterfactual Regret Minimization (CFR)
  - Why needed here: CFR is the core algorithm for solving imperfect information games like Mahjong where players have private information that changes throughout the game.
  - Quick check question: What is the key difference between standard regret matching and counterfactual regret minimization?

- Concept: Extensive-form game representation
  - Why needed here: Mahjong is modeled as an extensive-form game where players take sequential actions, and the game tree captures all possible sequences of moves and chance events.
  - Quick check question: How does the extensive-form representation handle the complexity of Mahjong's turn structure with phases and multiple possible actions?

- Concept: Information sets and imperfect information
  - Why needed here: Players in Mahjong have private hands, creating imperfect information. The algorithm must reason about probabilities over possible hidden states.
  - Quick check question: Why can't standard game tree search algorithms be directly applied to Mahjong without modifications for imperfect information?

## Architecture Onboarding

- Component map: Game Logic -> AI Engine -> GUI Tester -> Training System
- Critical path: Game Logic → AI Decision → Action Execution → Utility Feedback → Strategy Update
- Design tradeoffs:
  - Memory vs. Speed: The recursive game logic trades memory efficiency for the ability to explore deep game trees
  - Abstraction vs. Accuracy: Hierarchical abstraction reduces complexity but may miss some strategic nuances
  - Random Sampling vs. Exhaustive Search: Monte Carlo sampling enables training but may miss rare but important game states
- Failure signatures:
  - Exploitability stops decreasing during training: May indicate convergence or getting stuck in local optima
  - Inconsistent GUI display vs. actual game state: Suggests bugs in the game logic or state tracking
  - Training takes excessively long without improvement: Could indicate issues with the CFR implementation or feature encoding
- First 3 experiments:
  1. Verify the game logic correctly handles all Mahjong rules by playing through sample games manually
  2. Test the hierarchical abstraction by comparing the reduced action space against the full action space on simple game states
  3. Run CFR on a small, tractable game tree to verify convergence to known equilibrium strategies

## Open Questions the Paper Calls Out

- Question: How can the CFR-p algorithm be extended to handle more complex variants of Mahjong with additional winning patterns beyond the three currently considered (PongPongHu, 7Pairs, and Normal)?
  - Basis in paper: [explicit] The paper mentions that the current algorithm relies heavily on abstractions and that other policies like Defense, Same Color, etc. could be considered in other variants of Mahjong.
  - Why unresolved: The paper does not provide details on how to incorporate additional winning patterns or policies into the hierarchical policy abstraction framework.
  - What evidence would resolve it: A demonstration of the CFR-p algorithm being successfully applied to a more complex variant of Mahjong with additional winning patterns, along with an analysis of the performance gains and any modifications required to the algorithm.

- Question: Can the CFR-p algorithm be adapted to handle games with more than two players, such as the traditional four-player Mahjong game?
  - Basis in paper: [inferred] The paper focuses on two-player Mahjong, but mentions that the framework can be generalized to other imperfect information games. However, it does not discuss the potential challenges or modifications required for extending the algorithm to multi-player settings.
  - Why unresolved: The paper does not provide insights into how the CFR-p algorithm would perform or need to be modified for games with more than two players, which is a common scenario in Mahjong.
  - What evidence would resolve it: An implementation and evaluation of the CFR-p algorithm on a four-player Mahjong game, along with a comparison of its performance to the two-player case and an analysis of any necessary modifications to the algorithm.

- Question: How does the performance of the CFR-p algorithm compare to other state-of-the-art algorithms for imperfect information games, such as Deep CFR or subgame solving, when applied to Mahjong?
  - Basis in paper: [explicit] The paper mentions that existing algorithms like Deep CFR work without a problem in games with private actions, but subgame solving is only practical with a small amount of hidden information.
  - Why unresolved: The paper does not provide a direct comparison of the CFR-p algorithm's performance to other algorithms for imperfect information games when applied to Mahjong.
  - What evidence would resolve it: A comprehensive comparison of the CFR-p algorithm's performance to other state-of-the-art algorithms for imperfect information games, such as Deep CFR or subgame solving, when applied to Mahjong, including an analysis of their strengths, weaknesses, and suitability for the game.

## Limitations
- The hierarchical abstraction assumes winning patterns can be evaluated independently, which may not hold in complex game states
- Limited empirical validation of the claimed complexity reduction from O(T1 * T2) to O(T1 + T2)
- Underspecified heuristic search component makes it difficult to assess the complete solution's effectiveness

## Confidence
- High confidence in the CFR algorithm's theoretical foundation and basic implementation
- Medium confidence in the hierarchical abstraction's practical benefits, as exploitability reduction is demonstrated but not compared to baseline approaches
- Low confidence in the completeness of the heuristic search implementation due to lack of specification

## Next Checks
1. Implement a simplified version of the algorithm on a smaller game (e.g., simplified poker variant) to verify the hierarchical abstraction's theoretical complexity reduction holds in practice
2. Conduct ablation studies comparing performance with and without the hierarchical abstraction to quantify its contribution to exploitability reduction
3. Test the algorithm's sensitivity to different numbers of winning patterns to understand the abstraction's scalability and limitations