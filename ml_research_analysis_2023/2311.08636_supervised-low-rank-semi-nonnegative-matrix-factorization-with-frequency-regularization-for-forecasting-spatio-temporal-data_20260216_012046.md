---
ver: rpa2
title: Supervised low-rank semi-nonnegative matrix factorization with frequency regularization
  for forecasting spatio-temporal data
arxiv_id: '2311.08636'
source_url: https://arxiv.org/abs/2311.08636
tags:
- frequency
- data
- regularization
- domain
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes supervised semi-nonnegative matrix factorization
  with frequency regularization for forecasting spatio-temporal data. The key idea
  is to decompose spatio-temporal data into spatial and temporal components, then
  regularize the temporal patterns in the frequency domain to improve interpretability
  and forecasting accuracy.
---

# Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data

## Quick Facts
- arXiv ID: 2311.08636
- Source URL: https://arxiv.org/abs/2311.08636
- Reference count: 22
- Primary result: Soft frequency regularization and Lasso/Ridge regularization methods demonstrate the best predictive performance for spatio-temporal forecasting, while hard frequency regularization is most effective for removing noise frequencies.

## Executive Summary
This paper proposes supervised semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data. The method decomposes spatio-temporal data into spatial and temporal components, then regularizes temporal patterns in the frequency domain to improve interpretability and forecasting accuracy. Two regularization methods are introduced: soft regularization using Minkowski 1-norm, and hard regularization using three operator splitting to remove specific frequencies. The approach is validated on GRACE satellite data for predicting water storage anomalies, showing comparable performance to existing methods while providing clearer interpretability of temporal patterns.

## Method Summary
The method uses supervised semi-nonnegative matrix factorization to decompose spatio-temporal data into spatial (W, W') and temporal (H) components. The temporal patterns are regularized in the frequency domain using either soft regularization (Minkowski 1-norm) or hard regularization (three operator splitting). Block coordinate descent with projected subgradient descent is employed to optimize the non-convex objective function. The supervision parameter ξ controls the influence of auxiliary data Y on predicting the main data X. The algorithm alternates between updating H using projected subgradient descent and updating W, W' using closed-form solutions until convergence.

## Key Results
- Soft frequency regularization and Lasso/Ridge regularization methods demonstrate the best predictive performance for spatio-temporal forecasting
- Hard frequency regularization is most effective for removing noise frequencies from temporal patterns
- The method achieves comparable performance to existing methods while providing clearer interpretability of temporal patterns
- GRACE satellite data experiments show the proposed approach effectively predicts water storage anomalies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularizing in the frequency domain better captures periodic spatial patterns in spatio-temporal data.
- Mechanism: The low-rank spatio-temporal model assumes that both X and Y can be decomposed into spatial and temporal components. By applying regularization directly in the frequency domain using Minkowski 1-norm, the method encourages sparsity in the frequency representation, which aligns with the expectation that geophysical data exhibits dominant periodic patterns (e.g., annual cycles) with fewer significant frequencies.
- Core assumption: The observed spatio-temporal data admits a low-rank time-series representation that uses a small number of dominating frequencies.
- Evidence anchors:
  - [abstract]: "To improve clarity in the temporal patterns, we introduce a nonnegativity constraint on the time domain along with regularization in the frequency domain."
  - [section]: "Options (a-b) above use standard Ridge and Lasso penalties commonly used in the machine learning and statistics literature... (c-d) regularize the Fourier transform bH of H so the regularization is done directly in the frequency domain."
  - [corpus]: No direct evidence for this mechanism in the corpus; the related papers focus on general matrix factorization without specific frequency regularization.
- Break condition: If the data does not exhibit clear periodic patterns, frequency domain regularization may not provide advantages over time domain regularization.

### Mechanism 2
- Claim: Supervised semi-nonnegative matrix factorization leverages auxiliary data to improve prediction of missing spatio-temporal data.
- Mechanism: The method uses an auxiliary dataset Y to help infer the missing data X[:, :, T:] by first encoding the temporal patterns from Y during the training period, then using these patterns to predict the missing period. The supervision parameter ξ controls the influence of the auxiliary data.
- Core assumption: The auxiliary data Y contains complementary information about the same spatial region that can help predict the main data X.
- Evidence anchors:
  - [abstract]: "Matrix factorization is employed to decompose spatio-temporal data into spatial and temporal components... We propose two methods in the frequency domain: soft and hard regularizations."
  - [section]: "The main task of this method is to learn the joint latent spatial patterns (W, W') and the common nonnegative low-rank time-series H from the joint observed time-series data (X, Y)."
  - [corpus]: No direct evidence for this specific supervision mechanism in the corpus; related papers focus on general matrix factorization without explicit supervision.
- Break condition: If the auxiliary data Y is not correlated with X or contains significant noise, the supervision may degrade prediction performance.

### Mechanism 3
- Claim: Block coordinate descent with projected subgradient descent converges to stationary points for the proposed optimization problems.
- Mechanism: The algorithm alternates between updating H (using projected subgradient descent due to non-differentiability of the frequency regularization) and updating W, W' (using closed-form solutions). The convergence is guaranteed because each subproblem is convex and the overall objective satisfies conditions for block coordinate descent convergence.
- Core assumption: The objective function satisfies the conditions for block coordinate descent convergence (quasi-convexity, lower semi-continuity, etc.).
- Evidence anchors:
  - [abstract]: "We propose two methods in the frequency domain: soft and hard regularizations, and provide convergence guarantees to first-order stationary points of the corresponding constrained optimization problem."
  - [section]: "Therefore Wk, W'k and Hk converge to stationary points in Algorithm 1." and "Therefore Wk, W'k and Hk converge to stationary points in Algorithm 3."
  - [corpus]: No direct evidence for this specific convergence mechanism in the corpus; related papers focus on general matrix factorization without specific convergence analysis.
- Break condition: If the objective function does not satisfy the required conditions (e.g., if the regularization term is not convex), convergence guarantees may not hold.

## Foundational Learning

- Concept: Fourier transform and frequency domain analysis
  - Why needed here: The method relies on transforming temporal patterns into the frequency domain to apply regularization that encourages sparsity in frequency representation, which is crucial for capturing periodic patterns in geophysical data.
  - Quick check question: What property of real-valued time series ensures that the Fourier coefficients satisfy the conjugate symmetry condition bHsk = bHs,T-k?

- Concept: Nonnegative matrix factorization and its semi-nonnegative relaxation
  - Why needed here: The method uses semi-nonnegative matrix factorization to decompose spatio-temporal data while allowing negative values in the spatial components but enforcing nonnegativity in the temporal coefficients, which improves interpretability.
  - Quick check question: Why does enforcing nonnegativity on the temporal coefficients H improve the interpretability of the spatial patterns?

- Concept: Block coordinate descent and projected subgradient methods
  - Why needed here: The optimization problem involves non-differentiable regularization terms, requiring the use of projected subgradient descent within a block coordinate descent framework to find stationary points.
  - Quick check question: Under what conditions does block coordinate descent converge for non-convex problems?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Matricization of tensors X and Y into matrices X and Y
  - Core algorithm: Block coordinate descent with three blocks (H, W, W')
  - Regularization: Frequency domain regularization using Minkowski 1-norm
  - Prediction: Encoding step to infer H_new from auxiliary data, then prediction step to reconstruct missing X
  - Post-processing: Atom removal to eliminate noisy spatial patterns

- Critical path:
  1. Initialize W, W', H
  2. Update H using projected subgradient descent with frequency regularization
  3. Update W and W' using closed-form solutions
  4. Iterate until convergence
  5. Encode H_new from auxiliary data
  6. Predict missing X using W and H_new

- Design tradeoffs:
  - Time vs. frequency domain regularization: Time domain regularization (Ridge/Lasso) is computationally simpler but may not capture periodic patterns as effectively as frequency domain regularization
  - Soft vs. hard frequency regularization: Soft regularization (Minkowski 1-norm) is computationally easier and doesn't require prior knowledge of frequencies to remove, but hard regularization can completely eliminate specific frequencies at the cost of requiring prior knowledge
  - Supervised vs. unsupervised: Including auxiliary data Y improves prediction but requires additional data and hyperparameter tuning

- Failure signatures:
  - Poor convergence: Indicates issues with the optimization algorithm or inappropriate hyperparameters
  - Overfitting: High performance on training data but poor performance on test data, suggesting insufficient regularization
  - Loss of interpretability: Spatial patterns that don't align with expected physical phenomena, suggesting issues with the nonnegativity constraints or regularization

- First 3 experiments:
  1. Synthetic data with known periodic patterns: Generate synthetic spatio-temporal data with clear annual cycles and test whether the method correctly identifies and regularizes these frequencies
  2. GRACE data without auxiliary data: Apply the method to GRACE data using only the main dataset to evaluate the impact of removing the supervision component
  3. Comparison of soft vs. hard frequency regularization: Apply both regularization methods to the same dataset and compare interpretability (via µ(H) metric) and prediction accuracy (via NSE)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of remaining frequencies (R) for hard frequency regularization in different datasets?
- Basis in paper: [explicit] The paper states "We chose additional parameters for ξ ∈ {10^-3, 10^-2, 10^-1, 1, 10, 10^2, 10^3, 10^4, 10^5}, λ ∈ {10^-1, 1, 10, 10^2, 10^3, 10^4}, and R ∈ {10, 20, 30, 40}."
- Why unresolved: The paper tested multiple values of R but did not determine an optimal value for different datasets or provide a method to select R.
- What evidence would resolve it: A systematic study varying R across multiple datasets and developing a principled method to select R based on dataset characteristics.

### Open Question 2
- Question: How does the proposed method perform on non-periodic time-series data?
- Basis in paper: [inferred] The paper focuses on time-periodic geophysical data and states "However, there are instances when non-periodic events, such as anomalous occurrences, become significant in time-series data. Our current method might overlook such frequency information."
- Why unresolved: The experiments were conducted on time-periodic GRACE data, leaving the performance on non-periodic data untested.
- What evidence would resolve it: Testing the method on datasets with non-periodic events and comparing its performance to methods designed for non-periodic data.

### Open Question 3
- Question: What is the computational complexity of the proposed algorithms compared to existing methods?
- Basis in paper: [explicit] The paper describes the algorithms but does not provide a complexity analysis or runtime comparison.
- Why unresolved: The paper focuses on convergence guarantees and experimental results but does not analyze the computational efficiency.
- What evidence would resolve it: A detailed computational complexity analysis and runtime comparison between the proposed methods and existing methods like DNN, MLR, and SARIMAX.

## Limitations

- The frequency domain regularization assumes clear periodic patterns in geophysical data, but lacks empirical validation across diverse datasets
- The supervision mechanism's effectiveness depends heavily on the quality and relevance of auxiliary data, which is not thoroughly explored
- Convergence guarantees for the proposed optimization problems are stated but not rigorously proven for the specific non-convex objective with frequency regularization

## Confidence

**High confidence**: The supervised semi-nonnegative matrix factorization framework is clearly defined with explicit mathematical formulation and algorithmic steps. The use of block coordinate descent with projected subgradient methods for non-differentiable regularization is a well-established approach.

**Medium confidence**: The claim that frequency domain regularization better captures periodic spatial patterns is theoretically sound but lacks empirical validation across multiple datasets. The supervision mechanism's effectiveness depends heavily on the quality and relevance of auxiliary data, which is not thoroughly explored.

**Low confidence**: The convergence guarantees for the proposed optimization problems are stated but not rigorously proven. The paper claims first-order stationary point convergence without demonstrating that all required conditions (quasi-convexity, lower semi-continuity) are satisfied for the specific objective function with frequency regularization.

## Next Checks

1. **Synthetic data validation**: Generate synthetic spatio-temporal data with known periodic patterns and evaluate whether the method correctly identifies and regularizes these frequencies compared to time domain regularization approaches.

2. **Correlation sensitivity analysis**: Systematically evaluate the method's performance across varying levels of correlation between auxiliary data Y and target data X to determine the minimum correlation threshold required for supervision to be beneficial.

3. **Convergence condition verification**: Rigorously verify that the objective function with frequency regularization satisfies all required conditions (quasi-convexity, lower semi-continuity, etc.) for block coordinate descent convergence, or provide counterexamples where convergence fails.