---
ver: rpa2
title: Evaluating ChatGPT and GPT-4 for Visual Programming
arxiv_id: '2308.02522'
source_url: https://arxiv.org/abs/2308.02522
tags:
- task
- code
- programming
- solution
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates ChatGPT and GPT-4 for visual programming\
  \ tasks, a domain important for K-8 education but unexplored by prior LLM studies\
  \ focused on text-based programming. The authors design three scenarios\u2014execution\
  \ trace, solution synthesis, and task synthesis\u2014using reference tasks from\
  \ Hour of Code Maze and Karel."
---

# Evaluating ChatGPT and GPT-4 for Visual Programming

## Quick Facts
- arXiv ID: 2308.02522
- Source URL: https://arxiv.org/abs/2308.02522
- Reference count: 36
- GPT-4 improves over ChatGPT but both struggle with visual programming tasks, scoring 10-40% depending on task type

## Executive Summary
This paper evaluates ChatGPT and GPT-4 on visual programming tasks using three scenarios: execution trace, solution synthesis, and task synthesis. The study uses 10 reference tasks from Hour of Code Maze and Karel domains, measuring performance through expert-based annotations. Results show GPT-4 improves over ChatGPT but both models struggle significantly, with overall scores ranging from 10-40% depending on task type. The findings highlight a gap between LLM capabilities in text-based versus visual programming domains.

## Method Summary
The evaluation uses expert annotation of LLM-generated outputs across three scenarios for 10 reference tasks. For each task, prompts are constructed to elicit execution traces, solution codes, or task generation. Each prompt is executed once per task, with the best output selected for evaluation. An expert evaluator annotates outputs using binary metrics (correct/incorrect) for each scenario, and results are aggregated into performance percentages. The method relies on manual annotation without automated validation, and prompt templates are not fully specified in the paper.

## Key Results
- GPT-4 improves over ChatGPT in execution trace tasks through better spatial transitions and sensing
- GPT-4 generates solution codes closer to minimal size and depth compared to ChatGPT
- Both models struggle significantly with task synthesis, unable to generate valid visual programming tasks even for simple codes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 improves over ChatGPT on execution trace tasks by leveraging stronger spatial and logical reasoning.
- Mechanism: GPT-4 integrates visual grid information with programming constructs more coherently, reducing incoherent transitions and improving sensing of Boolean conditions.
- Core assumption: The LLM's architecture enables better alignment of grid parsing with conditional execution logic.
- Evidence anchors:
  - [abstract]: "GPT-4 drastically improves up on ChatGPT... this is because of improvements in spatial transitions and sensing as captured by metrics TransitionsCorrect and SensingCorrect."
  - [section]: "GPT-4 has substantially improved w.r.t. ChatGPT; in particular, this is because of improvements in spatial transitions and sensing."
  - [corpus]: Weak. Related work focuses on Python benchmarks, not visual grid reasoning.
- Break condition: If the visual grid parsing step fails, the coherence of transitions and sensing collapses regardless of logical reasoning ability.

### Mechanism 2
- Claim: GPT-4 generates solution codes closer to minimal size and depth due to improved structural understanding.
- Mechanism: GPT-4 better recognizes nesting patterns and loop depth constraints from the prompt, producing more concise code.
- Core assumption: Prompt cues about minimal block usage are effectively interpreted by GPT-4.
- Evidence anchors:
  - [abstract]: "GPT-4 has improved w.r.t. ChatGPT on the metrics of CodeSimilar, CodeSize, and CodeDepth."
  - [section]: "even though ChatGPT performs better on the metric CodeSolvesTask, it tends to produce a generic, complex code that can solve many tasks in the HoCMaze domain but ignores the specific task provided as input."
  - [corpus]: Weak. No direct corpus evidence of structural minimization in visual programming.
- Break condition: If the prompt fails to emphasize minimal block constraints, GPT-4 may default to generic over-complicated code.

### Mechanism 3
- Claim: Both models struggle with task synthesis because generating valid visual grids requires coordinated spatial reasoning beyond their text-generation strengths.
- Mechanism: The task generation step demands simultaneous satisfaction of layout, solvability, and code-solution alignment, which current models cannot reliably orchestrate.
- Core assumption: Task synthesis involves a multi-step reasoning process not well supported by autoregressive generation.
- Evidence anchors:
  - [abstract]: "GPT-4 still struggles in generating visual programming tasks even for elementary-level codes with low complexity."
  - [section]: "these results highlight that GPT-4 struggles in generating visual programming tasks even for elementary-level codes with low complexity."
  - [corpus]: Weak. Most corpus papers benchmark on text-based programming, not visual grid synthesis.
- Break condition: If the model cannot maintain internal consistency of the grid while aligning it to the code, the task becomes unsolvable or invalid.

## Foundational Learning

- Concept: Visual programming domain specification
  - Why needed here: Understanding the DSL (domain-specific language) structure is essential to interpret prompts and evaluate outputs.
  - Quick check question: What are the four types of coding blocks available in HoCMaze?

- Concept: Abstract Syntax Tree (AST) depth and structure
  - Why needed here: Metrics like CodeDepth and CodeSimilar depend on AST properties of the generated code.
  - Quick check question: How is the depth of a solution code defined in this paper?

- Concept: Grid representation and avatar movement rules
  - Why needed here: Execution trace correctness depends on accurate simulation of avatar movement on the grid.
  - Quick check question: What happens if the avatar attempts to move into a WALL cell?

## Architecture Onboarding

- Component map:
  - Prompt generator -> LLM interface -> Output extractor -> Annotation engine -> Aggregator

- Critical path:
  1. Generate prompt → 2. LLM call → 3. Extract output → 4. Expert annotation → 5. Aggregate results.

- Design tradeoffs:
  - Manual annotation vs. automated evaluation: Expert-based ensures nuanced quality checks but limits scalability.
  - Single output vs. multiple queries: Single output simplifies pipeline but ignores stochastic variance.

- Failure signatures:
  - Coarsely incoherent execution traces (avatar jumps into walls).
  - Overly complex solution codes that ignore minimal block constraint.
  - Invalid or unsolvable generated tasks.

- First 3 experiments:
  1. Vary number of LLM queries per instance (nqueries > 5) to assess stochastic effects.
  2. Swap prompts to include explicit minimal block reminders and measure impact on CodeSize.
  3. Introduce a validation script to automatically check basic solvability of generated tasks before expert annotation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do state-of-the-art generative models perform in visual programming domains compared to text-based programming domains?
- Basis in paper: [explicit] The main research question studied in the paper is: "Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming?"
- Why unresolved: The paper provides results showing that GPT-4 and ChatGPT perform poorly in visual programming tasks compared to their performance in text-based Python programming.
- What evidence would resolve it: Further research with larger and more complex visual programming tasks, and evaluation of additional generative models, could provide more insights into the performance gap between visual and text-based programming.

### Open Question 2
- Question: How can the performance of generative models in visual programming be improved?
- Basis in paper: [explicit] The paper discusses limitations and suggests future work directions, including curating novel benchmarks, evaluating alternate generative models, and developing techniques to improve performance.
- Why unresolved: The paper identifies the poor performance of generative models in visual programming but does not provide specific solutions or techniques to improve their performance.
- What evidence would resolve it: Conducting research to develop and evaluate techniques such as leveraging symbolic methods, automated prompting, or fine-tuning could provide evidence of improved performance in visual programming.

### Open Question 3
- Question: What are the challenges faced by generative models in combining spatial, logical, and programming skills for visual programming?
- Basis in paper: [explicit] The paper mentions that the models struggle to combine spatial, logical, and programming skills crucial for visual programming.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges faced by generative models in integrating these skills.
- What evidence would resolve it: Further research to analyze the specific difficulties encountered by generative models in handling spatial reasoning, logical operations, and programming constructs in visual programming tasks could provide insights into the challenges and potential solutions.

## Limitations

- The evaluation framework relies entirely on expert annotation without automated validation, introducing potential subjectivity and limiting reproducibility
- The paper does not specify the exact prompt templates used, making it difficult to isolate model capabilities from prompt engineering effects
- The study only uses single-output generation without exploring stochastic variance through multiple queries, potentially underrepresenting true model capabilities

## Confidence

- **High confidence**: GPT-4 demonstrates clear improvement over ChatGPT across all three scenarios, particularly in spatial transitions and sensing accuracy for execution traces.
- **Medium confidence**: GPT-4 produces more structurally sound and minimal solution codes compared to ChatGPT.
- **Low confidence**: Both models struggle significantly with task synthesis, with GPT-4 unable to generate valid visual programming tasks even for elementary-level codes.

## Next Checks

1. **Prompt Engineering Analysis**: Systematically vary prompt templates for each scenario while keeping the model fixed, measuring how different prompt formulations affect performance metrics.

2. **Multi-query Performance Evaluation**: Generate 10 outputs per task/scenario combination using both models, then analyze the distribution of metric scores to determine whether single-output selection introduces bias.

3. **Automated Validation Pipeline Development**: Create automated checks for basic output validity (syntax checking, basic solvability verification) to complement expert annotation, reducing subjectivity while maintaining quality assessment through expert review of edge cases.