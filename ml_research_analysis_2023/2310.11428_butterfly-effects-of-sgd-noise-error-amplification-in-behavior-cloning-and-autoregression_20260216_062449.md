---
ver: rpa2
title: 'Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and
  Autoregression'
arxiv_id: '2310.11428'
source_url: https://arxiv.org/abs/2310.11428
tags:
- learning
- training
- loss
- reward
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the training instability in behavior cloning
  (BC) and autoregressive (AR) models. The authors identify a phenomenon they term
  gradient variance amplification (GVA), where minibatch SGD noise propagates through
  unstable closed-loop dynamics, leading to catastrophic error accumulation over long
  horizons.
---

# Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression

## Quick Facts
- arXiv ID: 2310.11428
- Source URL: https://arxiv.org/abs/2310.11428
- Reference count: 40
- Key outcome: This paper studies training instability in behavior cloning (BC) and autoregressive (AR) models, identifying gradient variance amplification (GVA) where minibatch SGD noise propagates through unstable closed-loop dynamics, leading to catastrophic error accumulation over long horizons.

## Executive Summary
This paper investigates a fundamental instability in behavior cloning and autoregressive models, termed gradient variance amplification (GVA). The authors demonstrate that minibatch SGD noise, while negligible for the one-step surrogate loss, propagates through unstable closed-loop dynamics and accumulates catastrophically over long horizons. This phenomenon manifests as sharp oscillations in long-horizon rewards despite smooth behavior cloning loss. The paper introduces EMA as a simple yet effective mitigation technique, and provides theoretical vignettes explaining its benefits. The work bridges theoretical understanding with practical deep learning, showing that EMA can stabilize training without aggressive learning rate decay or increased batch sizes.

## Method Summary
The authors study GVA in behavior cloning and autoregressive models using standard deep learning pipelines. They train expert policies via SAC on MuJoCo environments, collect expert trajectories, and train imitator policies via BC loss minimization. The key intervention is applying EMA to the optimization trajectory with parameters (γ=1e-4, burn-in period, polynomial decay). They evaluate both BC loss and rollout reward over training, observing oscillations in rollout rewards despite smooth BC loss when GVA occurs. The method is tested across multiple environments (Walker2d-v4, Hopper-v4, HalfCheetah-v4, Ant-v4, Humanoid-v4) and datasets (TinyStories, English Wikipedia).

## Key Results
- Minibatch SGD noise propagates through unstable closed-loop dynamics, causing catastrophic error accumulation over long horizons
- Standard mitigation techniques (learning rate decay, gradient accumulation) do not alleviate GVA
- EMA of iterates is surprisingly effective at mitigating GVA while maintaining low BC loss
- The phenomenon is generic across behavior cloning and autoregressive language generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient variance amplification (GVA) occurs when minibatch SGD noise is propagated through unstable closed-loop dynamics, causing catastrophic error accumulation over long horizons.
- Mechanism: Small fluctuations in network parameters induced by SGD noise are exponentially amplified through repeated application in a feedback loop (e.g., autoregressive generation or closed-loop control), while the same fluctuations have negligible effect on the one-step surrogate loss.
- Core assumption: The closed-loop system is marginally unstable or unstable, such that small parameter perturbations lead to exponentially growing deviations in the rollout.
- Evidence anchors:
  - [abstract]: "minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss"
  - [section]: "gradient variance is amplified through the sensitivity of the rollout rewards to fluctuations in network parameters"
  - [corpus]: Found 25 related papers; average neighbor FMR=0.438, indicating moderate relevance but no direct citations yet.

### Mechanism 2
- Claim: Exponential Moving Average (EMA) of iterates stabilizes training by reducing the variance of the optimization trajectory in parameter space.
- Mechanism: EMA acts as a low-pass filter on the parameter trajectory, suppressing high-frequency noise components that would otherwise be amplified through the unstable closed-loop dynamics.
- Core assumption: The parameter trajectory contains high-frequency noise components that are detrimental when propagated through the feedback loop, but beneficial noise components (e.g., those aiding exploration or escaping saddle points) are preserved in the lower-frequency components.
- Evidence anchors:
  - [abstract]: "find an exponential moving average (EMA) of iterates to be surprisingly effective at [alleviating GVA]"
  - [section]: "iterate averaging by taking an Exponential Moving Average (EMA) of the optimization trajectory... stabilizes training and mitigates GVA"
  - [corpus]: 'Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression' and 'Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits' both explore EMA mechanisms.

### Mechanism 3
- Claim: Learning rate decay and EMA serve similar variance-reduction functions, but EMA is more computationally efficient and can enable training without full learning rate decay.
- Mechanism: Both techniques reduce the effective variance in the parameter trajectory over time. EMA achieves this through weighted averaging, while learning rate decay reduces the step size, thus reducing the magnitude of noise injected per update.
- Core assumption: The variance in the parameter trajectory is the primary driver of GVA, and reducing this variance will mitigate the phenomenon regardless of the specific mechanism used.
- Evidence anchors:
  - [abstract]: "we observe... that i) aggressively decaying the learning rate, and ii) greatly increasing the batch size through gradient accumulation, both have positive effects on the stability of training"
  - [section]: "we observe (Section 3.2) that i) aggressively decaying the learning rate, and ii) greatly increasing the batch size through gradient accumulation, both have positive effects on the stability of training"
  - [corpus]: 'PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning' suggests averaging techniques can accelerate optimization.

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) and its variance properties
  - Why needed here: Understanding how minibatch noise propagates through the training process is crucial to understanding GVA
  - Quick check question: How does the variance of minibatch gradients scale with batch size, and what is the relationship between gradient variance and parameter trajectory variance?

- Concept: Exponential Moving Average (EMA) and its variance reduction properties
  - Why needed here: EMA is the primary mitigation technique for GVA, so understanding its mathematical properties is essential
  - Quick check question: How does the EMA parameter (γ) affect the effective variance reduction, and what is the relationship between EMA and optimal averaging schemes in stochastic optimization?

- Concept: Stability analysis of dynamical systems
  - Why needed here: GVA occurs in unstable or marginally stable systems, so understanding concepts like Lyapunov stability and eigenvalue analysis is important
  - Quick check question: What is the condition for a linear dynamical system to be marginally stable, and how does this relate to the amplification of small perturbations over time?

## Architecture Onboarding

- Component map:
  - Behavior cloning (BC) loss: ℓBC(πθ) = E[ℓBC(πθ(x), u)] measures distance between imitator and expert actions
  - Rollout reward: JH(πθ) = E[Σ r(xh, uh)] measures cumulative reward over horizon H
  - Policy network: πθ: S → A parameterized by θ, can be MLP or Transformer
  - Optimizer: AdamW with learning rate schedule and EMA option
  - Evaluation: Compute both ℓBC on validation set and JH via rollouts with fixed initial conditions

- Critical path:
  1. Train expert policy via SAC
  2. Collect expert trajectories
  3. Train imitator policy via BC loss minimization
  4. Evaluate imitator via both BC loss and rollout reward
  5. Apply EMA if GVA is observed

- Design tradeoffs:
  - Batch size vs. learning rate: Larger batches reduce gradient variance but increase computational cost; smaller learning rates reduce noise but slow training
  - EMA vs. learning rate decay: Both reduce variance, but EMA is more computationally efficient and can enable training without full decay
  - Model architecture: Deeper/wider models may be more susceptible to GVA; Transformers may learn non-Markovian policies that are more robust

- Failure signatures:
  - Oscillations in rollout reward JH(πθ) during training, despite smooth BC loss ℓBC(πθ)
  - Negative correlation between JH and ℓBC for nearby checkpoints
  - Large Lipschitz constant of θ → JH(πθ) in regions where θ → ℓBC(πθ) is smooth

- First 3 experiments:
  1. Train behavior cloner with default settings and monitor both BC loss and rollout reward for oscillations
  2. Apply EMA with default parameters (γ=0.99, burn-in=1000 steps) and compare to non-EMA training
  3. Increase batch size via gradient accumulation (e.g., 4x or 16x) and observe effect on GVA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does EMA mitigate gradient variance amplification (GVA) in other feedback loop scenarios beyond behavior cloning and autoregressive language models?
- Basis in paper: [explicit] The paper demonstrates EMA's effectiveness in BC and autoregressive language generation, but suggests the phenomenon is generic to any model with feedback loops.
- Why unresolved: The paper only provides empirical evidence for two specific domains, leaving open the question of EMA's effectiveness in other feedback loop contexts.
- What evidence would resolve it: Experiments applying EMA to other domains with feedback loops, such as recommendation systems or online learning, would provide evidence for or against EMA's general effectiveness in mitigating GVA.

### Open Question 2
- Question: What is the precise mechanism by which EMA reduces the variance of stochastic gradients in the presence of GVA?
- Basis in paper: [inferred] The paper shows EMA's effectiveness empirically but does not provide a detailed theoretical explanation of how it reduces variance in the context of GVA.
- Why unresolved: While the paper provides a theoretical framework for GVA, it does not delve into the specific interaction between EMA and the chaotic dynamics that lead to error amplification.
- What evidence would resolve it: A rigorous theoretical analysis that explicitly models the interaction between EMA and the unstable closed-loop dynamics in the presence of GVA would clarify the mechanism.

### Open Question 3
- Question: Are there alternative stabilization techniques that are more effective than EMA in mitigating GVA, particularly in large-scale settings?
- Basis in paper: [explicit] The paper explores several alternative techniques but finds EMA to be the most effective. However, it acknowledges the need for further research into other potential stabilizers.
- Why unresolved: The paper's empirical study is limited to a specific set of techniques, and the computational cost of EMA at scale is not fully explored.
- What evidence would resolve it: Comparative studies of EMA with other stabilization techniques, such as different averaging schemes or gradient clipping methods, in large-scale settings would determine if more effective alternatives exist.

### Open Question 4
- Question: How does the choice of learning rate schedule interact with EMA's effectiveness in mitigating GVA?
- Basis in paper: [inferred] The paper finds that EMA allows for training at higher learning rates without decay, but does not provide a detailed analysis of how different learning rate schedules affect EMA's performance in the presence of GVA.
- Why unresolved: While the paper demonstrates the benefits of EMA with certain learning rate schedules, it does not explore the full range of possible interactions between learning rate and EMA in the context of GVA.
- What evidence would resolve it: Experiments varying the learning rate schedule while keeping EMA fixed would reveal the optimal interaction between these two factors in mitigating GVA.

### Open Question 5
- Question: To what extent does the presence of GVA impact the generalization performance of deep neural networks in feedback loop scenarios?
- Basis in paper: [inferred] The paper focuses on the training instability caused by GVA but does not explicitly address its impact on the final model's ability to generalize to unseen data.
- Why unresolved: The paper's analysis is primarily focused on the training dynamics, leaving open the question of whether GVA-induced instability translates to poor generalization.
- What evidence would resolve it: Empirical studies comparing the generalization performance of models trained with and without EMA in the presence of GVA would reveal the extent to which GVA impacts generalization.

## Limitations

- The theoretical analysis relies on simplified models that may not capture the full complexity of deep neural networks in unstable feedback loops
- The paper does not extensively explore alternative explanations for GVA, such as optimization pathologies or architecture-specific effects
- The effectiveness of EMA is demonstrated empirically, but the mechanism by which it specifically mitigates GVA versus general variance reduction is not conclusively established

## Confidence

- High confidence in the empirical observation of GVA and its mitigation through EMA
- Medium confidence in the theoretical explanations, particularly the characterization of GVA as arising from marginally unstable closed-loop dynamics
- Medium confidence in the claim that classical convex optimization theory can fully explain EMA's benefits in deep learning

## Next Checks

1. Conduct ablation studies on the learning rate schedule and batch size to isolate the specific contribution of gradient variance to GVA
2. Test EMA's effectiveness on additional unstable closed-loop systems, such as model-based reinforcement learning with model uncertainty
3. Compare EMA to alternative variance reduction techniques (e.g., gradient clipping, trust region methods) to determine if GVA is a unique phenomenon requiring EMA or if general variance reduction suffices