---
ver: rpa2
title: 'PhoGPT: Generative Pre-training for Vietnamese'
arxiv_id: '2311.02945'
source_url: https://arxiv.org/abs/2311.02945
tags:
- vietnamese
- nguyen
- phogpt
- arxiv
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoGPT, a 7.5B-parameter Vietnamese generative
  language model series including a base pre-trained model PhoGPT-7B5 and an instruction-following
  variant PhoGPT-7B5-Instruct. The base model is pre-trained from scratch on 41GB
  of Vietnamese texts, closely matching the scale used for BLOOM/BLOOMZ.
---

# PhoGPT: Generative Pre-training for Vietnamese

## Quick Facts
- arXiv ID: 2311.02945
- Source URL: https://arxiv.org/abs/2311.02945
- Reference count: 5
- PhoGPT-7B5-Instruct demonstrates competitive performance with ChatGPT on Vietnamese instruction-following tasks

## Executive Summary
This paper introduces PhoGPT, a 7.5B-parameter Vietnamese generative language model series including a base pre-trained model PhoGPT-7B5 and an instruction-following variant PhoGPT-7B5-Instruct. The base model is pre-trained from scratch on 41GB of Vietnamese texts, closely matching the scale used for BLOOM/BLOOMZ. The instruction variant is fine-tuned on a combined dataset of 150K Vietnamese prompt-response pairs sourced from Bactrian-X, ShareGPT, safety-awareness prompts, and specialized tasks. A human evaluation experiment compared PhoGPT-7B5-Instruct against ChatGPT and other Vietnamese open-source models on 80 Vietnamese-translated Vicuna questions across eight categories. PhoGPT-7B5-Instruct demonstrated strong competitiveness with ChatGPT in generic, knowledge, common-sense, and writing tasks, and significantly outperformed previous open-source baselines in most categories, except for coding & math where Vietcuna-7B-v3 and URA-LLaMA-13B performed better. The models are publicly released to support Vietnamese NLP research and applications.

## Method Summary
PhoGPT-7B5 is pre-trained from scratch using MosaicML LLM-foundry on a 41GB Vietnamese corpus (1GB Wikipedia + 40GB deduplicated BinhVQ news dataset) with a 20,480-token vocabulary. The model uses a transformer decoder architecture (7.5B parameters, 4096 d_model, 32 heads, 32 layers) with ALiBi positional encoding and flash attention. PhoGPT-7B5-Instruct is created by fine-tuning the base model on 150K Vietnamese prompt-response pairs using the VLLM library. Human evaluation compares the model against ChatGPT and other Vietnamese open-source models on 80 Vietnamese-translated Vicuna questions across eight categories, with three annotators scoring responses on a 1-5 scale.

## Key Results
- PhoGPT-7B5-Instruct achieves competitive performance with ChatGPT on generic, knowledge, common-sense, and writing tasks
- Significantly outperforms previous Vietnamese open-source baselines in most categories
- Underperforms specialized models (Vietcuna-7B-v3, URA-LLaMA-13B) on coding & math tasks
- Base model pre-trained from scratch on 41GB Vietnamese corpus achieves strong foundational language understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PhoGPT-7B5-Instruct achieves competitive performance because it is pre-trained from scratch on a Vietnamese-specific corpus of similar size to BLOOM/BLOOMZ, ensuring strong foundational language understanding.
- Mechanism: Training on 41GB of Vietnamese texts with a 20,480-token vocabulary and 7.5B parameters creates a language model deeply attuned to Vietnamese syntax, morphology, and vocabulary distributions.
- Core assumption: A monolingual pre-training corpus of this size is sufficient to match or exceed multilingual baselines for Vietnamese.
- Evidence anchors: [abstract]: "PhoGPT-7B5, with exactly 3.7B parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens"; [section]: "Utilizing the Mosaicml 'llm-foundry' library, we pre-train PhoGPT from scratch on a 41GB pre-training corpus of Vietnamese texts... which closely resembles the Vietnamese data used by BLOOM/BLOOMZ"; [corpus]: Weak corpus evidence—only mentions that the corpus resembles BLOOM/BLOOMZ's Vietnamese data, but does not detail token diversity or topic coverage.
- Break condition: If the Vietnamese corpus lacks domain diversity (e.g., missing legal, medical, or technical texts), model performance degrades in specialized tasks.

### Mechanism 2
- Claim: Fine-tuning on a large, diverse instruction dataset enables PhoGPT-7B5-Instruct to follow complex prompts effectively.
- Mechanism: Instruction fine-tuning aligns the base model to conversational and task-oriented behaviors using 150K prompt-response pairs spanning safety, QA, creative writing, and summarization.
- Core assumption: The fine-tuning dataset covers enough linguistic patterns and task types to generalize to unseen instructions.
- Evidence anchors: [abstract]: "The chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along with an additional 290K conversations"; [section]: "We fine-tune the pre-trained PhoGPT for instruction following, using a dataset consisting of 150K Vietnamese prompt and response pairs"; [corpus]: Weak corpus evidence—no breakdown of prompt diversity or response quality.
- Break condition: If fine-tuning data overrepresents generic prompts and underrepresents domain-specific instructions, the model fails on specialized tasks.

### Mechanism 3
- Claim: Human evaluation on translated Vicuna questions provides reliable relative performance metrics.
- Mechanism: Three annotators rate responses on a 1-5 scale, with conflict resolution to reduce bias; comparison against ChatGPT and other open-source baselines shows competitive or superior results.
- Core assumption: Human raters can consistently judge Vietnamese responses across categories like generic, knowledge, common-sense, writing, coding, and math.
- Evidence anchors: [abstract]: "A human evaluation experiment compared PhoGPT-7B5-Instruct against ChatGPT and other Vietnamese open-source models on 80 Vietnamese-translated Vicuna questions across eight categories"; [section]: "Each generated response is then independently assessed by 3 annotators on a scale from 1 - Bad... to 5 - Excellent"; [corpus]: No corpus evidence—evaluation design is stated but not validated against rater agreement metrics.
- Break condition: If annotators have inconsistent quality standards or if question translation loses nuance, comparative scores become unreliable.

## Foundational Learning

- Concept: Pre-training objective (causal language modeling)
  - Why needed here: PhoGPT is a decoder-only model that predicts next tokens, requiring LM loss minimization during pre-training.
  - Quick check question: Does the model predict tokens one at a time conditioned on previous tokens?

- Concept: Fine-tuning objective (instruction following)
  - Why needed here: Converts the base model into a conversational agent capable of structured responses.
  - Quick check question: Is the fine-tuning dataset labeled with explicit instruction-response pairs?

- Concept: Evaluation methodology (human judgment)
  - Why needed here: Automatic metrics (BLEU, ROUGE) are inadequate for open-ended generative tasks; human ratings capture semantic adequacy.
  - Quick check question: Are responses rated by multiple annotators with conflict resolution?

## Architecture Onboarding

- Component map: Vietnamese text corpus (41GB) -> MosaicML LLM-foundry pre-training -> Transformer decoder (7.5B parameters) -> VLLM fine-tuning -> Human evaluation
- Critical path: Pre-training → Instruction fine-tuning → Human evaluation → Release
- Design tradeoffs: Larger models (7.5B) offer better performance but require more compute; multilingual tokenizers simplify deployment but may slightly reduce Vietnamese-specific tokenization quality
- Failure signatures: Low generation diversity, repetitive outputs, or poor handling of domain-specific vocabulary indicate corpus or fine-tuning issues
- First 3 experiments:
  1. Validate tokenization coverage on a held-out Vietnamese corpus
  2. Test zero-shot generation on unseen prompt types
  3. Measure instruction-following accuracy on a subset of the 150K fine-tuning pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PhoGPT's performance on coding and math tasks compare to specialized smaller models when trained on Vietnamese-specific coding and math data?
- Basis in paper: [explicit] The paper notes PhoGPT-7B5-Instruct performs worse than Vietcuna-7B-v3 and URA-LLaMA-13B on coding & math tasks, attributing this to lack of such data in its pre-training corpus.
- Why unresolved: The paper only compares performance without exploring whether targeted data augmentation could close this gap.
- What evidence would resolve it: Head-to-head evaluation of PhoGPT variants trained on enriched Vietnamese coding/math datasets against current specialized models.

### Open Question 2
- Question: What is the optimal balance between Vietnamese-specific data and multilingual data for Vietnamese LLMs?
- Basis in paper: [inferred] The paper challenges the notion that language-specific models always outperform multilingual ones, but doesn't systematically explore the trade-offs.
- Why unresolved: The study only compares PhoGPT against BLOOMZ-derived models without exploring intermediate approaches.
- What evidence would resolve it: Systematic experiments varying the ratio of Vietnamese to multilingual data in training.

### Open Question 3
- Question: How does PhoGPT's performance on low-resource Vietnamese dialects compare to standard Vietnamese?
- Basis in paper: [inferred] The paper focuses on standard Vietnamese but Vietnamese has significant dialectal variation not addressed in the evaluation.
- Why unresolved: The evaluation corpus and human evaluation focus on standard Vietnamese without testing dialectal robustness.
- What evidence would resolve it: Performance benchmarks of PhoGPT across multiple Vietnamese dialects and regional variations.

## Limitations

- Human evaluation relies on translated Vicuna questions rather than native Vietnamese benchmarks, potentially losing cultural or linguistic nuances
- Corpus composition details are sparse, particularly regarding the deduplication process for the 40GB BinhVQ dataset and overall linguistic diversity
- Lack of ablation studies to isolate the impact of pre-training scale versus instruction fine-tuning quality on final performance

## Confidence

- **High Confidence**: Technical feasibility of pre-training a 7.5B-parameter Vietnamese model using established libraries (MosaicML LLM-foundry) and standard transformer architectures
- **Medium Confidence**: Competitive performance claims with ChatGPT are supported by human evaluation but lack statistical significance testing and rater reliability metrics
- **Low Confidence**: Specific contribution of the 41GB pre-training corpus size to performance improvements, as no comparisons are provided with models trained on smaller or larger Vietnamese corpora

## Next Checks

1. Compute inter-annotator agreement (e.g., Fleiss' kappa) on a subset of evaluation questions to quantify rater consistency and identify potential sources of evaluation noise

2. Analyze the linguistic diversity and domain coverage of the 41GB pre-training corpus using topic modeling or vocabulary analysis to ensure sufficient representation across technical, medical, legal, and colloquial Vietnamese

3. Evaluate PhoGPT-7B5-Instruct on Vietnamese-specific downstream tasks not included in the fine-tuning dataset (e.g., Vietnamese legal document analysis, medical text processing) to assess true generalization capabilities beyond the evaluation benchmarks