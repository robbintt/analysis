---
ver: rpa2
title: 'MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological
  Graph Data'
arxiv_id: '2310.02275'
source_url: https://arxiv.org/abs/2310.02275
tags:
- gene
- genes
- data
- learning
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MuSe-GNN integrates multimodal biological data by combining GNNs
  with multimodal machine learning, using weighted similarity and contrastive losses
  to learn cross-dataset gene relationships. Tested on 82 datasets from 10 tissues,
  3 species, and 3 sequencing techniques, it generates unified gene embeddings.
---

# MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data

## Quick Facts
- arXiv ID: 2310.02275
- Source URL: https://arxiv.org/abs/2310.02275
- Reference count: 40
- Key outcome: Up to 97.5% improvement over state-of-the-art methods in gene representation learning across 82 datasets from 10 tissues, 3 species, and 3 sequencing techniques

## Executive Summary
MuSe-GNN addresses the challenge of integrating multimodal biological data by learning unified gene representations across different tissues, species, and sequencing techniques. The method combines Graph Neural Networks with multimodal machine learning techniques, using weight-sharing across modalities and incorporating both weighted similarity and contrastive learning losses. Tested on an extensive dataset collection, it demonstrates significant improvements in gene embedding quality and enables downstream analyses like pathway enrichment and gene function prediction.

## Method Summary
MuSe-GNN learns unified gene representations by encoding gene co-expression networks from different biological modalities (scRNA-seq, scATAC-seq, spatial transcriptomics) into a shared embedding space. The model uses weight-sharing Graph Neural Networks with TransformerConv layers, where dataset-specific layers are connected to shared layers across modalities. The training combines three loss components: graph reconstruction loss (BCE), weighted cosine similarity loss for common highly variable genes (HVGs) across datasets, and InfoNCE contrastive loss to distinguish functionally different genes. The model is trained on 82 datasets spanning 10 tissues, 3 species, and 3 sequencing techniques.

## Key Results
- Achieves up to 97.5% improvement over state-of-the-art methods in gene representation learning
- Successfully integrates data from 10 tissues, 3 species, and 3 sequencing techniques into unified embeddings
- Improves gene function prediction accuracy by 2-3% compared to existing methods
- Enables pathway enrichment, causal network, and disease function analyses through learned embeddings

## Why This Works (Mechanism)

### Mechanism 1: Weighted Similarity Learning
Weighted similarity learning ensures genes with similar functions across different datasets are mapped close together in the shared embedding space. The model computes cosine similarity between gene embeddings of common HVGs across datasets and weights them by the overlap of their co-expressed neighbors. This weighted similarity is maximized during training, pulling functionally similar genes together. Core assumption: Common HVGs between datasets represent functionally similar genes, and their neighbor overlap reflects functional similarity.

### Mechanism 2: Contrastive Learning
Contrastive learning distinguishes genes with different functions by maximizing mutual information between functionally related genes and minimizing it for unrelated ones. InfoNCE loss is used to pull together embeddings of genes that are co-expressed in the same graph (positive pairs) and push apart embeddings of genes that are not co-expressed (negative pairs). Core assumption: Co-expression implies functional similarity, and genes not co-expressed are likely to have different functions.

### Mechanism 3: Weight-Sharing Across Modalities
Weight-sharing across graphs from different modalities allows the model to learn shared information and integrate genes from different datasets into a unified embedding space. The model uses the same set of Graph Transformer layers for all graphs, allowing information to be shared across datasets. This ensures that the learned embeddings capture common biological signals across modalities. Core assumption: Different modalities contain shared biological information that can be captured by the same set of parameters.

## Foundational Learning

- **Graph Neural Networks (GNNs)**: Why needed here - GNNs are used to encode the gene co-expression networks and learn node representations (gene embeddings) that capture the local graph structure and functional relationships between genes. Quick check question: What is the key operation in a GNN that allows it to aggregate information from a node's neighbors?

- **Multimodal Machine Learning (MMML)**: Why needed here - MMML is used to integrate information from different biological modalities (scRNA-seq, scATAC-seq, spatial transcriptomics) into a unified gene embedding space. Quick check question: What is the main challenge in integrating information from multiple modalities, and how does MMML address it?

- **Contrastive Learning**: Why needed here - Contrastive learning is used to ensure that functionally similar genes are mapped close together in the embedding space, while functionally different genes are pushed apart. Quick check question: How does the InfoNCE loss function encourage the model to learn embeddings that preserve functional similarity?

## Architecture Onboarding

- **Component map**: Input graph → TransformerConv layers (with weight-sharing) → embeddings → loss computation (BCE reconstruction + weighted cosine similarity + InfoNCE) → backward pass
- **Critical path**: The critical path is the forward pass through the model: input graph → TransformerConv layers (with weight-sharing) → embeddings → loss computation → backward pass
- **Design tradeoffs**: The model trades off between learning modality-specific features (via dataset-specific GT layers) and shared features (via weight-sharing GT layers). The weighted similarity learning and contrastive learning losses add regularization but also increase training complexity.
- **Failure signatures**: If the model fails to integrate genes from different modalities, it could be due to poor graph construction, inadequate weight-sharing, or poorly tuned loss functions. If the model overfits to a specific modality, it could be due to insufficient weight-sharing or regularization.
- **First 3 experiments**:
  1. Train the model on a single modality (e.g., scRNA-seq) to verify that it can learn gene embeddings that capture co-expression relationships.
  2. Train the model on two modalities (e.g., scRNA-seq and scATAC-seq) from the same tissue to verify that it can integrate information from different modalities.
  3. Train the model on multiple modalities from different tissues to verify that it can learn unified embeddings that capture functional similarity across tissues and modalities.

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise mechanisms by which weighted similarity learning and contrastive learning contribute to the improved performance of MuSe-GNN compared to models that use only one or neither of these components? The paper states that "combining all three components to construct our loss function is the optimal choice, achieving a performance that is 224.0% higher than the version without any additional regularization" but does not provide a detailed analysis of the individual contributions of weighted similarity learning and contrastive learning to the overall performance improvement.

### Open Question 2
How does the choice of common highly variable genes (HVGs) as anchors impact the ability of MuSe-GNN to integrate gene information across different modalities, tissues, and species? The paper states that "we employ the shared community score as an indirect measurement, which is incorporated as a weight for the cosine similarity of different common HVG pairs within the final loss function" but does not provide a comprehensive analysis of the impact of using HVGs as anchors on the integration of gene information across different contexts.

### Open Question 3
What are the limitations of MuSe-GNN in handling datasets with nodes other than genes, and how can the model be extended to address these limitations? The paper states that "At present, MuSe-GNN does not accept graphs with nodes other than genes as input" and plans to extend the model in the future, but does not provide a detailed analysis of the current limitations or potential solutions.

## Limitations
- Strong assumptions about functional similarity: Common HVGs between datasets are assumed to be functionally related, which may not always hold true
- Performance claims require verification: The 97.5% improvement claim needs careful scrutiny of the benchmarking methodology and fairness of comparison
- Limited to gene-only graphs: The current model cannot handle graphs with nodes other than genes, limiting its applicability to broader biological networks

## Confidence
- **High confidence**: The weight-sharing architecture and general approach of combining GNNs with multimodal learning are sound and well-motivated
- **Medium confidence**: The weighted similarity learning mechanism is reasonable but the assumption that common HVGs and their neighbor overlap reliably indicate functional similarity needs empirical validation
- **Low confidence**: The 97.5% performance improvement claim - this requires verification that the benchmarking is conducted fairly across all methods and that the metrics chosen are appropriate for evaluating cross-modal integration quality

## Next Checks
1. **Ablation study validation**: Verify that removing the weighted similarity loss or contrastive learning components results in significant performance degradation, confirming their necessity rather than just adding complexity.

2. **Cross-tissue generalization test**: Apply the learned embeddings to predict gene functions in a tissue not seen during training (e.g., from the 11 held-out tissues) to test true generalization rather than memorization of tissue-specific patterns.

3. **Biological validation**: Select a set of known gene modules/pathways and verify that their embeddings cluster together across all tissues and modalities, and that the model correctly identifies novel genes that should belong to these modules based on their embedding proximity.