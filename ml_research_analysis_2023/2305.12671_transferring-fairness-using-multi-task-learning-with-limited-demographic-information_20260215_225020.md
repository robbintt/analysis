---
ver: rpa2
title: Transferring Fairness using Multi-Task Learning with Limited Demographic Information
arxiv_id: '2305.12671'
source_url: https://arxiv.org/abs/2305.12671
tags:
- fairness
- task
- demographic
- loss
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training fair machine learning
  models when demographic data is unavailable for the target task. It proposes leveraging
  multi-task learning (MTL) to transfer fairness from a related task that has demographic
  labels.
---

# Transferring Fairness using Multi-Task Learning with Limited Demographic Information

## Quick Facts
- arXiv ID: 2305.12671
- Source URL: https://arxiv.org/abs/2305.12671
- Authors: 
- Reference count: 24
- Key outcome: Proposes MTL-based fairness transfer from a task with demographic labels to a target task without such data, enabling fairer models and intersectional fairness across tasks.

## Executive Summary
This work addresses the challenge of training fair machine learning models when demographic data is unavailable for the target task. It proposes leveraging multi-task learning (MTL) to transfer fairness from a related task that has demographic labels. The method adapts single-task fairness losses to an MTL setting, enabling fairness objectives from one task to influence the target task during joint training. Experiments across clinical, review, and social media datasets demonstrate that MTL with fairness loss can produce fairer models for the target task, even without its own demographic attributes, and can also enable intersectional fairness by combining single-axis demographics from two tasks. The approach is robust across domains and task similarities, though careful model selection still benefits from target task demographic validation.

## Method Summary
The approach uses multi-task learning with a shared encoder and task-specific heads to transfer fairness from a related task with demographic labels to a target task without such data. The fairness loss (ϵ-DEO) is adapted to the MTL setting by applying it to the auxiliary task while jointly training both tasks. For intersectional fairness, fairness losses are applied to both tasks to combine single-axis demographics. Models are trained using Adam optimizer with grid search over hyperparameters, and best models are selected based on validation performance and fairness.

## Key Results
- MTL with fairness loss improves fairness on target tasks without demographic labels, across clinical, review, and social media domains
- Fairness transfer works across domains and task similarities, not requiring domain similarity
- Combining single-axis demographic losses in MTL can achieve intersectional fairness even when neither task has full demographic coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MTL fairness loss transfers fairness from a task with demographic data to a target task without such data.
- Mechanism: The shared encoder parameters in the MTL model are updated by the fairness loss applied to the auxiliary task, causing representations learned for that task to improve fairness on the target task even without its own demographic labels.
- Core assumption: The tasks share useful representational overlap so that fairness improvements in one domain translate to fairness gains in the other.
- Evidence anchors:
  - [abstract]: "We adapt a single-task fairness loss to a multi-task setting to exploit demographic labels from a related task in debiasing a target task..."
  - [section]: "Since there exist similarities between tasks A and B, we wish to utilize the demographic attributes available for task B to obtain a fair classifier for task A."
  - [corpus]: Weak; corpus neighbors discuss fairness in multi-task RL but do not directly anchor the fairness-transfer mechanism described here.
- Break condition: If task domains are too dissimilar, the shared encoder may not encode overlapping features, preventing fairness transfer.

### Mechanism 2
- Claim: Using two single-axis demographic losses in MTL can achieve intersectional fairness even when neither task has full demographic coverage.
- Mechanism: Each task contributes its single-axis fairness loss to the shared encoder, and the combined effect induces fairer behavior across intersectional groups defined by the union of the two axes.
- Core assumption: Single-axis fairness objectives are compatible and complementary, so their joint application in MTL yields intersectional fairness without needing both attributes on the same task.
- Evidence anchors:
  - [abstract]: "...we show that this approach enables intersectional fairness by transferring between two datasets with different single-axis demographics."
  - [section]: "We seek an intersectionally fair classifier on both tasks with respect to z = s1×s2."
  - [corpus]: Weak; no corpus evidence directly supports the intersectional MTL fairness claim.
- Break condition: If the two demographic axes are not independent or their joint distribution is highly skewed, the combined loss may fail to enforce fairness on intersectional groups.

### Mechanism 3
- Claim: Fairness can transfer across domains and tasks when MTL performance on both tasks is strong, even without domain similarity.
- Mechanism: The fairness loss applied to one task shapes the shared representation in a way that improves fairness for the other task, provided the auxiliary task is learnable and the MTL setup can jointly optimize both tasks effectively.
- Core assumption: Domain similarity is not strictly required if the MTL system can achieve competitive performance for both tasks, allowing the fairness objective to influence shared parameters meaningfully.
- Evidence anchors:
  - [section]: "We observe that adding a fairness loss to the MTL settings helps in fairness with tasks across domains and task similarities..."
  - [abstract]: "We explore different data domains to show how our loss can improve fairness domains and tasks."
  - [corpus]: Weak; corpus neighbors discuss fairness generalization but do not provide direct evidence for cross-domain fairness transfer in MTL.
- Break condition: If MTL performance on the auxiliary task is poor, the fairness loss will have little effect, and fairness gains will not transfer.

## Foundational Learning

- Concept: Fairness metrics (e.g., ϵ-DEO) and their role in model training.
  - Why needed here: The paper's approach hinges on using a differentiable fairness loss to guide MTL training toward fairer representations.
  - Quick check question: What does ϵ-DEO measure and why is it used instead of demographic parity?
- Concept: Multi-task learning (MTL) architecture and shared encoder training.
  - Why needed here: Understanding how shared parameters are updated jointly by multiple task losses is key to grasping how fairness can transfer between tasks.
  - Quick check question: How do shared encoder parameters differ from task-specific heads in MTL, and how does this enable fairness transfer?
- Concept: Intersectionality in fairness and single-axis vs. intersectional group definitions.
  - Why needed here: The paper's intersectional fairness experiments rely on combining single-axis demographic attributes from different tasks.
  - Quick check question: Why might combining two single-axis fairness losses in MTL approximate intersectional fairness?

## Architecture Onboarding

- Component map: Shared encoder -> Task-specific heads -> Task losses + Fairness loss -> Parameter updates
- Critical path: Forward pass through shared encoder → task-specific predictions → compute task losses + fairness loss (if applicable) → backward pass updates shared encoder + task heads
- Design tradeoffs: Choosing between stronger fairness loss weight (λ) and model performance; deciding which task gets the fairness loss in MTL; balancing task-specific vs. shared representation capacity
- Failure signatures: Poor MTL performance on one task indicates loss conflict or insufficient shared capacity; lack of fairness improvement suggests weak task overlap or improper fairness loss scaling
- First 3 experiments:
  1. Train STL-base and STL-fair on each task to establish performance/fairness baselines
  2. Train MTL-base on in-domain task pairs to confirm MTL can match or improve STL performance
  3. Train MTL-fair with fairness loss on auxiliary task only and compare fairness/performance against baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fairness generalization capability of MTL change when using tasks with significantly different domains or task similarities, and what is the underlying mechanism?
- Basis in paper: [explicit] The paper explores the relationship between task similarity and fairness generalization, finding that the transfer of fairness is not dependent on domain or task similarity but rather related to the performance of the secondary task.
- Why unresolved: While the paper shows that fairness generalizes across domains and tasks, the exact mechanism by which this occurs and the factors that influence the strength of fairness transfer remain unclear.
- What evidence would resolve it: Further experiments that systematically vary task domains, similarities, and the performance of the secondary task, along with detailed analysis of the learned representations, could help elucidate the underlying mechanisms.

### Open Question 2
- Question: Can the MTL fairness transfer approach be extended to scenarios where demographic attributes are not binary or are continuous, and how would this impact fairness generalization?
- Basis in paper: [inferred] The paper primarily focuses on binary demographic attributes (e.g., gender, race) and does not explicitly address scenarios with non-binary or continuous demographic attributes.
- Why unresolved: The current approach may not be directly applicable to non-binary or continuous demographic attributes, and it is unclear how the fairness generalization would be affected in such cases.
- What evidence would resolve it: Experiments that apply the MTL fairness transfer approach to tasks with non-binary or continuous demographic attributes, along with analysis of the fairness generalization performance, could provide insights into the applicability and effectiveness of the method in these scenarios.

### Open Question 3
- Question: How does the choice of fairness metric (e.g., demographic parity vs. equalized odds) impact the effectiveness of the MTL fairness transfer approach, and are there scenarios where certain metrics are more suitable?
- Basis in paper: [explicit] The paper uses a specific fairness metric (ϵ-DEO) based on equalized odds and mentions that other metrics (e.g., demographic parity, equalized opportunity) could be adapted in a similar way.
- Why unresolved: The paper does not investigate the impact of using different fairness metrics on the effectiveness of the MTL fairness transfer approach, and it is unclear whether certain metrics are more suitable for specific scenarios or tasks.
- What evidence would resolve it: Experiments that compare the performance of the MTL fairness transfer approach using different fairness metrics, along with analysis of the fairness generalization and task performance, could provide insights into the suitability and effectiveness of different metrics in various scenarios.

## Limitations
- Domain similarity assumption: Fairness transfer relies on overlapping representations between tasks, but the paper does not provide systematic analysis of how domain similarity affects transfer efficacy.
- Weak corpus anchoring: Several core claims about fairness transfer mechanisms and intersectional fairness in MTL are not well-supported by the corpus evidence.
- Hyperparameter sensitivity: The method's effectiveness depends on careful tuning of fairness loss weight (λ) and other hyperparameters, but sensitivity analysis is lacking.

## Confidence
- **High confidence**: Claims about MTL architecture and basic experimental setup (data preparation, model training, evaluation metrics). The technical implementation details are clearly specified and reproducible.
- **Medium confidence**: Claims about fairness transfer between tasks and domains. While experimental results support these claims, the mechanisms lack theoretical justification and corpus evidence.
- **Low confidence**: Claims about intersectional fairness via combined single-axis losses. This is a novel theoretical contribution with minimal supporting evidence from either experiments or related work.

## Next Checks
1. **Domain similarity analysis**: Systematically vary task domain similarity (e.g., same domain vs. cross-domain pairs) and measure fairness transfer effectiveness to establish when and why transfer succeeds or fails.
2. **Intersectional fairness ablation**: Test intersectional fairness claims by comparing MTL with combined single-axis losses against models with full demographic labels on intersectional groups to quantify approximation quality.
3. **Hyperparameter robustness**: Conduct sensitivity analysis across a wider range of fairness loss weights and training configurations to identify stable regions and failure modes when demographic validation is unavailable.