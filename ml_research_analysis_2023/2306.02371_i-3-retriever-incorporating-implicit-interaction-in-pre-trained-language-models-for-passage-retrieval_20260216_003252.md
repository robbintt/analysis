---
ver: rpa2
title: 'I^3 Retriever: Incorporating Implicit Interaction in Pre-trained Language
  Models for Passage Retrieval'
arxiv_id: '2306.02371'
source_url: https://arxiv.org/abs/2306.02371
tags:
- passage
- query
- retrieval
- interaction
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes I3 Retriever, a novel PLM-based dense retrieval
  method that incorporates implicit interaction into dual-encoders. Unlike existing
  methods that require explicit query terms for interaction, I3 leverages a query
  reconstructor to generate pseudo-query vectors from passages, which are then used
  by a query-passage interactor to encode query-aware information in passage representations.
---

# I^3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval

## Quick Facts
- arXiv ID: 2306.02371
- Source URL: https://arxiv.org/abs/2306.02371
- Authors: 
- Reference count: 40
- Key outcome: Proposes I3 Retriever achieving state-of-the-art performance on MSMARCO and TREC2019 with MRR@10 of 0.366 vs 0.360, and NDCG@10 of 0.727 vs 0.694

## Executive Summary
I3 Retriever introduces a novel dense retrieval method that incorporates implicit interaction into pre-trained language models. Unlike existing dual-encoder approaches that require explicit query terms for interaction, I3 leverages a query reconstructor to generate pseudo-query vectors from passages, which are then used by a query-passage interactor to encode query-aware information in passage representations. This allows offline pre-computation of all passage vectors while maintaining high online efficiency comparable to vanilla dual-encoders.

## Method Summary
I3 Retriever combines dual-encoder efficiency with late-interaction effectiveness through a novel implicit interaction paradigm. The method generates pseudo-queries from passages using a query reconstructor, then applies a query-passage interactor to encode query-relevant information into passage representations. This approach enables offline pre-computation of query-aware passage vectors that can be stored and retrieved efficiently during online inference using simple dot-product operations. The model is trained using a contrastive loss with hard negative mining and an auxiliary reconstruction loss to guide pseudo-query generation.

## Key Results
- Achieves state-of-the-art performance on MSMARCO and TREC2019 datasets
- Improves MRR@10 from 0.360 to 0.366 compared to existing dense retrievers
- Increases NDCG@10 from 0.694 to 0.727, demonstrating better ranking quality
- Maintains inference speed comparable to vanilla dual-encoders through offline precomputation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: I3's implicit interaction improves retrieval performance by generating query-aware passage vectors offline.
- Mechanism: The query reconstructor generates pseudo-query vectors from passages, which are then used by the query-passage interactor to encode query-relevant information into the passage representations. This allows pre-computation of all passage vectors offline while maintaining high online efficiency.
- Core assumption: The pseudo-query vectors generated by the reconstructor capture the key concepts and terms that real queries might ask for in a passage.
- Evidence anchors:
  - [abstract] "our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction"
  - [section 4.2] "The interactor leverages the pseudo-query to encode important knowledge in the query-aware passage vector that might be relevant to real queries"
  - [corpus] No direct corpus evidence found, but the method is novel and the paper provides experimental results showing effectiveness
- Break condition: If the pseudo-query generation fails to capture relevant query concepts, the implicit interaction would not improve retrieval performance.

### Mechanism 2
- Claim: I3 maintains dual-encoder efficiency while achieving late-interaction effectiveness.
- Mechanism: By conducting implicit interaction on the passage side offline, I3 can precompute query-aware passage vectors that are stored as single vectors. Online inference then only requires simple dot-product computation between query vectors and precomputed passage vectors.
- Core assumption: The query-aware passage vectors can be compressed into single vectors without significant loss of the interaction information.
- Evidence anchors:
  - [abstract] "It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders"
  - [section 4.4] "the time complexity of I3 retriever is O (E + G), where E and G are the cost of query encoding and MIPS operation over the corpus G, respectively"
  - [corpus] No direct corpus evidence found, but the method is novel and the paper provides efficiency comparisons
- Break condition: If the single-vector representation of query-aware passages loses too much interaction information, the efficiency gains would not justify the potential effectiveness loss.

### Mechanism 3
- Claim: Joint optimization of query reconstruction and retrieval tasks improves the quality of implicit interaction.
- Mechanism: The query reconstructor is trained with a reconstruction loss to generate relevant pseudo-queries, while the entire model is optimized with a contrastive retrieval loss. This joint training ensures that the pseudo-queries are tailored for the retrieval task.
- Core assumption: The reconstruction task guides the query reconstructor to generate pseudo-queries that are more relevant for retrieval than generic query generation methods.
- Evidence anchors:
  - [section 4.3] "We also introduce an auxiliary reconstruction loss to guide the query reconstructor, which is defined as L_r = - sum(y_wi log(W_R K_q(p+)_q))"
  - [section 6.3] "the reconstructed query terms can address several key concepts and terms that a query might ask for in a long passage"
  - [corpus] No direct corpus evidence found, but the method is novel and the paper provides experimental results showing effectiveness
- Break condition: If the reconstruction loss does not effectively guide the generation of relevant pseudo-queries, the joint optimization would not provide additional benefits.

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs) and their applications in information retrieval
  - Why needed here: Understanding how PLMs like BERT are used for encoding queries and passages in dense retrieval models
  - Quick check question: What is the main advantage of using PLMs like BERT for dense retrieval compared to traditional methods like BM25?

- Concept: Dual-encoder architecture and its limitations
  - Why needed here: Understanding the baseline model that I3 advances, including its efficiency advantages and effectiveness limitations due to lack of interaction
  - Quick check question: What is the key limitation of dual-encoders that I3 aims to address through implicit interaction?

- Concept: Contrastive learning and negative sampling in dense retrieval
  - Why needed here: Understanding the training objective used to optimize I3, including the use of hard negatives
  - Quick check question: How does the use of hard negative passages in the contrastive loss help improve the retrieval performance of I3?

## Architecture Onboarding

- Component map:
  - Query Encoder -> Query Reconstructor -> Query-Passage Interactor -> Passage Encoder
  - Query Encoder -> Passage Encoder -> Query-Passage Interactor -> Relevance Computation

- Critical path:
  1. Offline: Precompute query-aware passage vectors for all passages in the corpus
  2. Online: Encode query, perform maximum inner product search (MIPS) over precomputed passage vectors, return top-k results

- Design tradeoffs:
  - Effectiveness vs. Efficiency: I3 aims to achieve late-interaction effectiveness while maintaining dual-encoder efficiency through offline precomputation
  - Single-vector vs. Multi-vector representations: I3 uses single-vector representations for query-aware passages to minimize storage cost, potentially at the expense of some interaction information

- Failure signatures:
  - Low retrieval performance: Indicates issues with the quality of implicit interaction or the joint optimization
  - High storage requirements: Suggests problems with the single-vector compression of query-aware passages
  - Slow online inference: Implies inefficiencies in the query encoding or MIPS search

- First 3 experiments:
  1. Implement the query reconstructor and evaluate its ability to generate relevant pseudo-queries for a set of passages
  2. Integrate the query-passage interactor and assess the quality of the query-aware passage vectors compared to original passage vectors
  3. Combine all components and compare the retrieval performance of I3 against a baseline dual-encoder on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of I3 retriever scale with corpus size, particularly for very large document collections beyond the 8.8 million passages tested?
- Basis in paper: [explicit] The paper demonstrates I3's effectiveness on MSMARCO (8.8M passages) but doesn't explore scalability to much larger corpora that exist in production systems
- Why unresolved: The experiments only cover a moderate-sized corpus, leaving questions about performance degradation, efficiency maintenance, and storage requirements at massive scale
- What evidence would resolve it: Systematic experiments testing I3 on progressively larger corpora (100M+ passages) with detailed analysis of latency, memory usage, and recall@K metrics across different corpus sizes

### Open Question 2
- Question: Can the implicit interaction paradigm be extended to handle multi-turn conversational queries or cross-lingual retrieval scenarios?
- Basis in paper: [inferred] The current I3 retriever is designed for single-turn retrieval tasks and doesn't explore more complex query scenarios that are common in real-world applications
- Why unresolved: The paper focuses on standard passage retrieval benchmarks without addressing the architectural modifications needed for conversational context or language mismatch
- What evidence would resolve it: Experiments comparing I3's performance on conversational query sets and cross-lingual retrieval tasks, along with architectural adaptations for handling query history and language translation

### Open Question 3
- Question: What is the optimal balance between the number of pseudo-query vectors generated per passage and retrieval effectiveness, and how does this trade-off vary across different domains?
- Basis in paper: [explicit] The paper uses a fixed length of 32 for generated queries but doesn't systematically explore the parameter space or domain-specific variations
- Why unresolved: The experimental setup uses a single configuration without exploring sensitivity to this hyperparameter or testing across diverse domains with varying information density
- What evidence would resolve it: Comprehensive ablation studies varying the pseudo-query length across multiple domains (news, scientific literature, social media) with corresponding effectiveness metrics and efficiency measurements

### Open Question 4
- Question: How robust is I3 retriever to adversarial queries designed to exploit the pseudo-query generation mechanism?
- Basis in paper: [inferred] The paper doesn't investigate potential vulnerabilities in the implicit interaction approach that could be exploited by carefully crafted queries
- Why unresolved: Security and robustness analysis is absent from the evaluation, leaving questions about the model's resilience to query attacks targeting the reconstruction module
- What evidence would resolve it: Adversarial attack experiments using query generation techniques to create queries that specifically target weaknesses in the pseudo-query reconstruction, followed by defensive adaptations to improve robustness

## Limitations

- The reliance on synthetic pseudo-queries raises uncertainty about whether they capture the full diversity of real user queries, particularly for rare or complex information needs
- The single-vector compression of query-aware passages may lose some interaction information that could be important for ranking, though this trade-off enables efficiency
- The paper does not provide detailed analysis of storage requirements for precomputed passage vectors, making it difficult to assess the true efficiency advantage

## Confidence

- **High confidence**: The core mechanism of implicit interaction through query reconstruction is well-defined and the experimental results demonstrate significant performance improvements over strong baselines
- **Medium confidence**: The claim that I3 maintains dual-encoder efficiency while achieving late-interaction effectiveness is supported by complexity analysis, but the actual storage requirements for precomputed passage vectors are not discussed in detail
- **Medium confidence**: The joint optimization of query reconstruction and retrieval tasks is conceptually sound, but the specific impact of the reconstruction loss on retrieval performance is not thoroughly evaluated through ablation studies

## Next Checks

1. Conduct an ablation study to measure the impact of the reconstruction loss on retrieval performance by comparing I3 with and without the auxiliary reconstruction loss
2. Analyze the quality of generated pseudo-queries by evaluating their relevance to real queries using human judgment or automated metrics like BLEU or ROUGE
3. Measure the storage requirements for precomputed query-aware passage vectors and compare them with other dense retrieval methods to validate the claimed efficiency advantage