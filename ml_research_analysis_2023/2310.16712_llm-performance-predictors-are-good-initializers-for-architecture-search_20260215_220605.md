---
ver: rpa2
title: LLM Performance Predictors are good initializers for Architecture Search
arxiv_id: '2310.16712'
source_url: https://arxiv.org/abs/2310.16712
tags:
- search
- performance
- hs-nas
- architecture
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel use of large language models (LLMs)
  to build performance predictors for deep neural network architectures. The authors
  design prompts for LLMs containing role descriptions, instructions, hyperparameter
  definitions, and demonstrations to predict the performance of specific architectures
  on machine translation tasks.
---

# LLM Performance Predictors are good initializers for Architecture Search

## Quick Facts
- arXiv ID: 2310.16712
- Source URL: https://arxiv.org/abs/2310.16712
- Reference count: 40
- This paper presents a novel use of large language models (LLMs) to build performance predictors for deep neural network architectures.

## Executive Summary
This paper introduces a novel approach to using large language models (LLMs) as performance predictors for neural architecture search (NAS). The authors design specialized prompts for LLMs containing role descriptions, instructions, hyperparameter definitions, and demonstrations to predict the performance of specific architectures on machine translation tasks. They show that GPT-4 with these prompts achieves state-of-the-art mean absolute error and propose a hybrid search algorithm that combines LLM-based predictors for initial search stages with traditional predictors for later stages, achieving similar performance to state-of-the-art NAS while reducing search time by approximately 50%.

## Method Summary
The method involves three key components: (1) LLM-PP, where GPT-4 is prompted with carefully designed prompts containing role descriptions, instructions, hyperparameter definitions, and demonstrations to predict architecture performance on machine translation tasks; (2) LLM-Distill-PP, where predictions from LLM-PP are distilled into a compact regression model (MLP-based) that retains much of the performance at lower cost; and (3) HS-NAS, a hybrid search algorithm that uses LLM-Distill-PP for initial search iterations and a baseline predictor (supernet) for later iterations. The approach is evaluated on three machine translation tasks (WMT'14 En-De, WMT'14 En-Fr, WMT'19 En-De) using MAE and Kendall rank correlation coefficient metrics.

## Key Results
- LLM-PP achieves state-of-the-art mean absolute error and slight degradation in rank correlation compared to baseline predictors
- LLM-Distill-PP retains much of LLM-PP's performance while drastically reducing computational cost
- HS-NAS performs similarly to state-of-the-art NAS while reducing search hours by approximately 50%
- The hybrid approach shows improved latency, GFLOPs, and model size in some cases compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-PP leverages the general understanding of DNN architectures acquired by LLMs during pretraining on research papers and GitHub repositories.
- Mechanism: By providing carefully designed prompts that specify the role, instructions, hyperparameters, and demonstrations, the LLM can map DNN architectures to their performance scores without requiring task-specific fine-tuning.
- Core assumption: The LLM's pretraining data contains sufficient architectural descriptions and performance relationships for it to generalize to new architectures.
- Evidence anchors: [abstract] "We hypothesize that LLMs have a 'general understanding' of DNN architectures, which is learned from relevant training data sources such as DNN research papers and GitHub repositories." [section 5] "These architecture understanding capabilities can be tested by prompting LLM to generate definition of hyperparameters, and generate design principles for architecture search."
- Break condition: If the pretraining corpus lacks sufficient coverage of the target architecture family or if the LLM's pretraining was domain-restricted, the general understanding assumption fails.

### Mechanism 2
- Claim: Distillation of LLM-PP predictions into a regression model (LLM-Distill-PP) preserves much of the performance while drastically reducing cost.
- Mechanism: The MLP-based regressor learns to approximate the mapping from architecture hyperparameters to predicted performance scores, capturing the essential relationships without the overhead of LLM inference.
- Core assumption: The relationship between architecture hyperparameters and performance is sufficiently regular and low-dimensional to be captured by a simple regression model.
- Evidence anchors: [abstract] "we show that predictions from LLM-PP can be distilled to a small regression model (LLM-Distill-PP), which surprisingly retains much of the performance of LLM-PP." [section 6] "Despite the simple model design, LLM-Distill-PP can largely perform similarly or better than LLM-PP for both ChatGPT and GPT-4."
- Break condition: If the relationship between hyperparameters and performance is highly non-linear or involves complex interactions that require the LLM's full capacity to capture, distillation quality will degrade.

### Mechanism 3
- Claim: Hybrid search combining LLM-Distill-PP for initial iterations and a supernet for later iterations achieves similar performance to SOTA NAS while reducing search time by ~50%.
- Mechanism: LLM-Distill-PP provides good initial exploration due to its superior MAE, while the supernet's better Kendall-Tau ranking ensures refined selection in later stages.
- Core assumption: The strengths of different predictors (MAE vs Kendall-Tau) complement each other in different search phases.
- Evidence anchors: [abstract] "HS-NAS performs similarly to SoTA NAS, reducing search hours by approximately 50%." [section 7.3] "Using LLM-Distill-PP for all search iterations achieves lower performance, which indicates that marginal degradation in Kendall-Tau prevents LLM-Distill-PP from fully handling the search."
- Break condition: If the predictor performance profiles don't align with search phase requirements, or if the search space has characteristics that favor one predictor type throughout, the hybrid approach loses its advantage.

## Foundational Learning

- Concept: Prompt engineering for LLMs
  - Why needed here: The performance of LLM-PP critically depends on how well the prompts communicate the task to the LLM
  - Quick check question: What are the four key components of the PP prompts and why is each necessary?

- Concept: Neural architecture search (NAS) algorithms
  - Why needed here: Understanding how HS-NAS modifies existing NAS approaches is crucial for implementation and further development
  - Quick check question: How does the hybrid approach differ from standard evolutionary search in NAS?

- Concept: Performance prediction metrics (MAE and Kendall-Tau)
  - Why needed here: Different metrics capture different aspects of predictor quality, and understanding their implications is essential for evaluation
  - Quick check question: Why might a predictor with better MAE not necessarily be better for NAS?

## Architecture Onboarding

- Component map: Architecture descriptions → LLM-PP → Distillation dataset → LLM-Distill-PP → HS-NAS → Final architecture
- Critical path: Architecture → LLM-PP → Distillation dataset → LLM-Distill-PP → HS-NAS → Final architecture
- Design tradeoffs: LLM-PP offers high accuracy but high cost; LLM-Distill-PP offers lower cost but potentially lower accuracy; HS-NAS balances these through staged usage
- Failure signatures: High MAE indicates poor absolute performance prediction; poor Kendall-Tau indicates poor ranking ability; high search time despite hybrid approach indicates suboptimal predictor selection
- First 3 experiments:
  1. Test LLM-PP with minimal prompts (demonstrations only) to verify baseline capability
  2. Train LLM-Distill-PP on a small distillation dataset and compare to LLM-PP
  3. Run HS-NAS with LLM-Distill-PP for all iterations to establish baseline performance before hybrid implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum search iteration range where LLM-Distill-PP can maintain or improve NAS performance compared to baseline predictors?
- Basis in paper: [explicit] The paper shows HS-NAS with LLM-Distill-PP for iterations 1-15 performs similarly to SOTA, but doesn't explore ranges like 1-20 or 5-30.
- Why unresolved: The paper only tests one specific range (1-15) and shows marginal degradation when used for all 30 iterations.
- What evidence would resolve it: Systematic testing of different start/end iteration pairs to find the optimal range where LLM-Distill-PP maximizes search efficiency without compromising quality.

### Open Question 2
- Question: Can LLM-based performance predictors be effectively used for candidate architecture generation in addition to performance prediction?
- Basis in paper: [explicit] The conclusion mentions future research should explore "application of LLM for candidate architecture generation and performance prediction jointly."
- Why unresolved: The paper only uses LLMs for performance prediction, not for generating new candidate architectures during search.
- What evidence would resolve it: Integration of LLM-generated architecture proposals into the search process and comparison of end-to-end performance against traditional methods.

### Open Question 3
- Question: How do LLM-based performance predictors generalize to other architecture families beyond Transformers (e.g., CNNs, RNNs, or other backbone architectures)?
- Basis in paper: [inferred] The paper focuses exclusively on Transformer-based encoder-decoder architectures for machine translation tasks.
- Why unresolved: The evaluation is limited to one specific architecture family and task type, leaving generalization to other domains unexplored.
- What evidence would resolve it: Testing LLM-PP prompts and distillation methods on diverse architecture families and tasks to establish broader applicability.

## Limitations

- The approach assumes LLMs have sufficient "general understanding" of DNN architectures from pretraining, but this is not empirically validated across diverse architecture families.
- The distillation process quality depends heavily on the architecture encoding method, which is not fully specified in the paper.
- The 50% search time reduction is demonstrated on specific search spaces and may not generalize to other domains or architecture families.

## Confidence

- **High confidence**: LLM-PP outperforms baseline predictors on MAE for the tested tasks, supported by direct experimental results
- **Medium confidence**: LLM-Distill-PP retains performance while reducing cost, though the exact retention rate varies by task
- **Medium confidence**: Hybrid search achieves 50% time reduction, but this is based on limited experimental conditions

## Next Checks

1. Test LLM-PP and LLM-Distill-PP across diverse architecture families (CNNs, RNNs, etc.) to validate the "general understanding" assumption
2. Systematically vary the architecture encoding method to identify optimal representations for distillation
3. Benchmark HS-NAS on additional search spaces and tasks to verify the claimed 50% time reduction generalizes beyond MT tasks