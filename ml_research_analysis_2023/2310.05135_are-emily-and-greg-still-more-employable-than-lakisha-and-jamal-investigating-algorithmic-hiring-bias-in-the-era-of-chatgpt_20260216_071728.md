---
ver: rpa2
title: Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating
  Algorithmic Hiring Bias in the Era of ChatGPT
arxiv_id: '2310.05135'
source_url: https://arxiv.org/abs/2310.05135
tags:
- resumes
- gender
- bias
- race
- resume
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) like GPT-3.5, Bard, and Claude are
  increasingly used in algorithmic hiring to classify or summarize resumes. The study
  investigates whether these models exhibit bias on legally protected attributes (gender,
  race, maternity leave, pregnancy, political affiliation) by replicating the landmark
  Bertrand & Mullainathan (2003) resume audit experiment.
---

# Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT

## Quick Facts
- arXiv ID: 2310.05135
- Source URL: https://arxiv.org/abs/2310.05135
- Reference count: 32
- Large language models (LLMs) like GPT-3.5, Bard, and Claude exhibit bias on protected attributes in resume classification, with Claude showing significant bias on maternity leave, pregnancy, and political affiliation.

## Executive Summary
This study investigates algorithmic hiring bias in large language models by replicating the landmark Bertrand & Mullainathan (2003) resume audit experiment. The authors evaluate three state-of-the-art LLMs (GPT-3.5, Bard, Claude) on resume classification and summarization tasks using a dataset of 334 resumes annotated with stereotypical names and sensitive attributes. They measure bias using the Equal Opportunity Gap (EOG) metric and find that while GPT-3.5 and Bard are largely fair on gender and race, Claude exhibits significant bias on maternity leave, pregnancy, and political affiliation when classifying full resumes. However, bias is reduced when classifying LLM-generated resume summaries, suggesting that summarization may mitigate bias by focusing on relevant information.

## Method Summary
The authors constructed a resume dataset of 334 resumes from IT, Teacher, and Construction job categories, adding stereotypical names and sensitive attribute flags (gender, race, maternity leave, pregnancy, political affiliation). They evaluated three LLMs (GPT-3.5, Bard, Claude) on two tasks: (1) classifying full resumes into job categories, and (2) generating summaries followed by classification. Bias was measured using the Equal Opportunity Gap (EOG) comparing True Positive Rates between demographic groups, with statistical significance assessed using Fisher exact tests. The study also examined open-source models (Alpaca-7B, Llama-2) where white-box access enabled contrastive input decoding to identify potential sources of bias.

## Key Results
- Claude exhibits significant bias on pregnancy and political affiliation, with TPR gaps frequently exceeding 30% in favor of Democrats and against pregnant women
- GPT-3.5 and Bard show lower bias levels overall, with GPT-3.5 providing summaries for nearly all resumes while Bard blocks content in 10-46% of cases
- Bias is reduced when classifying LLM-generated resume summaries compared to full resumes, suggesting summarization may mitigate bias by focusing on relevant information
- All models maintain high overall accuracy (>94%) on the classification task, but fairness metrics reveal differential performance across demographic groups

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-based resume summarization reduces bias on sensitive attributes by filtering irrelevant information.
- **Mechanism**: When LLMs generate summaries, they compress the resume text, potentially removing or de-emphasizing sensitive attribute mentions that are not directly relevant to job qualifications. This compression makes it harder for the LLM to detect and use these attributes during subsequent classification.
- **Core assumption**: Sensitive attribute information is less central to the summary content than job-relevant details, so the model focuses on the latter.
- **Evidence anchors**:
  - [abstract] "bias is reduced when classifying LLM-generated resume summaries, suggesting that summarization may mitigate bias by focusing on relevant information."
  - [section 3.2] "Claude mentions sensitive information more frequently overall than the other two models... The starkest difference is for pregnancy status, as it is mentioned in 80% to 94.12% of the summaries generated."
  - [corpus] Weak evidence - corpus does not directly address summarization effects.
- **Break condition**: If sensitive attributes are frequently central to resume content (e.g., if pregnancy status is a core qualification requirement), summarization may not filter them out effectively.

### Mechanism 2
- **Claim**: Claude exhibits higher bias on pregnancy and political affiliation because these attributes are more contextually salient in the training data.
- **Mechanism**: The LLM training corpus may contain more examples where pregnancy or political affiliation strongly influences hiring decisions, causing the model to overfit to these patterns.
- **Core assumption**: The training data distribution reflects societal biases that amplify the impact of pregnancy and political affiliation.
- **Evidence anchors**:
  - [section 3.1] "Claude is biased on political affiliation, with bias in favor of Democrats. In most instances, TPR Gap exceeds the 15% threshold and frequently exceeds 30%."
  - [section 3.1] "Claude has a statistically significant bias against women with maternity-based employment gaps, and pregnant women."
  - [corpus] Weak evidence - corpus does not detail training data composition.
- **Break condition**: If the training data is heavily sanitized or balanced to remove these biases, the observed bias would diminish.

### Mechanism 3
- **Claim**: Bard's refusal to generate summaries when sensitive attributes are present reduces bias by avoiding exposure to these attributes.
- **Mechanism**: By blocking or refusing to process resumes containing sensitive information, Bard prevents the model from learning or using these attributes in its outputs, thereby reducing downstream bias.
- **Core assumption**: The model's refusal mechanism is triggered by the presence of sensitive attributes, which prevents them from influencing the summary or classification.
- **Evidence anchors**:
  - [section 3.2] "Bard does not provide a summary and outputs an error message: 'Content was blocked, but the reason is uncategorized.'"
  - [section 3.2] "Unlike GPT-3.5, which summarized (almost) every resume, Bard provides a summary for about 54% to 90% of resumes."
  - [corpus] Weak evidence - corpus does not provide details on Bard's refusal behavior.
- **Break condition**: If the refusal mechanism is inconsistent or the model sometimes processes sensitive attributes despite blocking, bias could still occur.

## Foundational Learning

- **Concept**: Equal Opportunity Gap (EOG) as a fairness metric.
  - **Why needed here**: EOG quantifies bias by measuring differences in true positive rates between demographic groups, which is essential for detecting and comparing algorithmic bias.
  - **Quick check question**: How does EOG differ from other fairness metrics like demographic parity or equal odds?

- **Concept**: Fisher exact test for statistical significance.
  - **Why needed here**: Used to determine if observed differences in TPR gaps are statistically significant, providing confidence in the bias detection results.
  - **Quick check question**: Why is Fisher exact test preferred over chi-squared test for small sample sizes?

- **Concept**: Resume summarization as a preprocessing step.
  - **Why needed here**: Summarization can reduce bias by focusing the model on job-relevant information, but also risks losing important context if not done carefully.
  - **Quick check question**: What are the trade-offs between using full resumes versus summaries in algorithmic hiring?

## Architecture Onboarding

- **Component map**: Raw resume dataset → Annotated resumes with sensitive attributes → LLM API calls (GPT-3.5, Bard, Claude, Llama) → Summary generation (if applicable) → Classification task → Output: TPR, TNR, EOG metrics
- **Critical path**: Resume annotation → LLM classification/summarization → Bias measurement (EOG, statistical tests)
- **Design tradeoffs**: Full-text classification captures all details but may amplify bias; summarization reduces bias but risks losing important information; using multiple LLMs increases robustness but adds complexity
- **Failure signatures**: High variance in TPR gaps across job categories; LLM refusal to process certain resumes; inconsistent bias patterns across sensitive attributes
- **First 3 experiments**:
  1. Run classification on full resumes with sensitive attributes to establish baseline bias levels
  2. Generate summaries and re-run classification to compare bias reduction
  3. Use contrastive input decoding on open-source LLMs to identify sources of bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on bias detection for gender and racial groups beyond the cis-gendered White and African American individuals studied?
- Basis in paper: [explicit] The authors acknowledge that their study focused only on cis-gendered individuals for two racial groups and state that future work should investigate other groups, especially as they are historically marginalized.
- Why unresolved: The paper explicitly states it did not examine other gender identities or racial groups beyond White and African American, so the performance of LLMs on these groups remains unknown.
- What evidence would resolve it: Experimental results from a study evaluating LLM bias on transgender, non-binary, and other racial groups using similar methodology.

### Open Question 2
- Question: What are the causes of bias in LLMs for attributes like maternity leave, pregnancy status, and political affiliation?
- Basis in paper: [inferred] The authors used contrastive input decoding on the white-box Alpaca model and found potential explanations like "including personal information about maternity leave is not relevant to the job," but this analysis was limited and could not be extended to black-box models.
- Why unresolved: The authors were unable to fully explain the sources of bias due to the black-box nature of the state-of-the-art models tested, and contrastive input decoding only sometimes provided reasons for rejection.
- What evidence would resolve it: Detailed analysis of model attention mechanisms or internal representations to identify why certain attributes lead to biased outcomes.

### Open Question 3
- Question: How does the placement and phrasing of sensitive attribute flags affect LLM bias?
- Basis in paper: [explicit] The authors note that Bard and Claude were more biased when employment gaps and pregnancy status were placed at the beginning of resumes, and that using an Equal Opportunity Employer statement in the prompt reduced bias in some cases.
- Why unresolved: The paper only tested a few variations in flag placement and prompt wording, so the full impact of these factors on bias is unknown.
- What evidence would resolve it: Experiments systematically varying the location, phrasing, and context of sensitive attributes in resumes and prompts to measure the effect on bias.

## Limitations

- The dataset of 334 resumes across three job categories provides limited coverage of the broader hiring landscape and may not generalize to all industries
- The study focuses primarily on GPT-3.5, Bard, and Claude, with limited analysis of open-source models beyond Llama-2 and Alpaca-7B
- Statistical significance testing at p ≤ 0.05 provides reasonable confidence but the multiple comparisons across job categories and sensitive attributes increase false positive risk

## Confidence

- High confidence in the finding that Claude exhibits significant bias on pregnancy and political affiliation
- Medium confidence in the claim that summarization reduces bias
- Medium confidence in the overall conclusion that LLM-based hiring tools can introduce bias

## Next Checks

1. Expand the evaluation to include more diverse job categories and resume samples to improve external validity and assess whether bias patterns hold across different industries
2. Conduct controlled experiments varying the prominence of sensitive attribute mentions in resumes to determine whether summarization consistently reduces bias or if the effect depends on attribute salience
3. Perform a detailed analysis of the training data distribution of the LLMs to identify potential sources of bias and validate the mechanism by which summarization reduces bias