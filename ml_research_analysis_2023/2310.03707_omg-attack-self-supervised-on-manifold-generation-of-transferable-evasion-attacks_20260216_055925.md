---
ver: rpa2
title: 'OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion
  Attacks'
arxiv_id: '2310.03707'
source_url: https://arxiv.org/abs/2310.03707
tags:
- adversarial
- attacks
- examples
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OMG-ATTACK, a self-supervised method for
  generating transferable evasion attacks on neural networks. The approach uses a
  generator-discriminator architecture inspired by representation learning frameworks
  to create adversarial examples that remain on the data manifold.
---

# OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks

## Quick Facts
- arXiv ID: 2310.03707
- Source URL: https://arxiv.org/abs/2310.03707
- Reference count: 40
- Primary result: Self-supervised method for generating transferable adversarial attacks using generator-discriminator architecture with on-manifold constraint

## Executive Summary
OMG-ATTACK introduces a novel self-supervised method for generating transferable evasion attacks on neural networks. The approach uses a generator-discriminator architecture inspired by representation learning frameworks to create adversarial examples that remain on the data manifold. By constraining attacks to the data distribution rather than model-specific parameters, the method achieves superior transferability across different architectures, defended models, and unseen classes. The authors demonstrate effectiveness across MNIST, GTSRB, and CUB-200 datasets, showing that on-manifold attacks outperform traditional methods like FGSM, PGD, and AdvGAN++.

## Method Summary
OMG-ATTACK employs a generator-discriminator architecture where a ViewMaker-based encoder-decoder generates perturbations constrained by an L1 norm budget. The discriminator uses a PatchGAN structure to enforce on-manifold constraints through a contrastive loss between representations. The training is entirely self-supervised, leveraging two independent objectives: a contrastive loss between target model representations and an on-manifold loss from the discriminator. The method uses pre-trained frozen models as targets and generates attacks through three positive pair permutations. Budget constraints vary by dataset (0.3 for MNIST, 0.015 for GTSRB, 0.025 for CUB-200).

## Key Results
- Achieves attack success rates of 80.8% on defended models compared to 69.9% for AdvGAN++
- Maintains high transferability to unseen models (76.7% ASR) versus 54.8% for AdvGAN++
- Demonstrates strong performance on unseen classes with minimal degradation compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
On-manifold adversarial examples transfer better because they are constrained by the data distribution rather than model-specific parameters. By forcing generated attacks to remain on the data manifold through a discriminator, the perturbations capture data-driven features rather than model artifacts. This works because the data manifold is more consistent across different models than model-specific decision boundaries.

### Mechanism 2
Self-supervised training without labels allows attack generation on unseen classes. The contrastive loss operates on representations rather than class labels, enabling the model to learn perturbations that work across different classes. This is possible because representation space contains class-agnostic features that can be exploited for adversarial generation.

### Mechanism 3
L1 norm constraint enables better manifold adherence than L∞. L1 norm allows concentration of perturbation budget on strategic regions rather than uniform pixel-wise changes, enabling more natural-looking attacks. This works because natural images have sparse perturbation patterns that L1 norm better captures.

## Foundational Learning

- **Adversarial attack transferability**: Understanding why some attacks work across models while others don't is fundamental to OMG-ATTACK's design. Quick check: Why do gradient-based attacks typically have better transferability than optimization-based attacks?

- **Self-supervised learning frameworks**: OMG-ATTACK uses contrastive loss without labels, requiring understanding of self-supervised approaches. Quick check: How does contrastive loss work in self-supervised learning, and what makes it suitable for attack generation?

- **Manifold learning and GAN discriminators**: The discriminator component enforces on-manifold constraints, requiring understanding of how discriminators learn data distributions. Quick check: How does a discriminator learn to distinguish real from fake data in GAN training?

## Architecture Onboarding

- **Component map**: Input → Generator → Budget projection → Target model + Discriminator → Loss computation → Generator update
- **Critical path**: The perturbation generation flows through the generator, budget constraint, target model for representation extraction, and discriminator for manifold validation before loss computation and generator update
- **Design tradeoffs**: 
  - L1 vs L∞ norm: Better manifold adherence vs potentially weaker attacks
  - Self-supervised vs supervised: Works on unseen classes vs potentially stronger attacks on seen classes
  - Generator architecture: Simpler ViewMaker vs more complex GAN generators
- **Failure signatures**:
  - Training doesn't converge: Likely discriminator is too strong or weak
  - Low attack success rates: Budget too small or manifold constraint too strict
  - Visible artifacts: Budget too large or L1 norm not appropriate for dataset
- **First 3 experiments**:
  1. Train OMG-ATTACK on MNIST with a simple CNN target, test white-box attack success rate
  2. Generate attacks on MNIST and test transferability to a different CNN architecture
  3. Train on 80% of classes, test attack success on remaining 20% to verify unseen class performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but implies several through its limitations and discussion sections.

## Limitations
- Lacks specific architectural details for the generator and discriminator beyond general descriptions
- No specific hyperparameter values provided for temperature τ, loss coefficients α and β, or training configurations
- Claim about superior performance on unseen classes lacks theoretical justification for why self-supervised approach specifically enables this capability

## Confidence

- **High confidence**: The core mechanism that on-manifold attacks transfer better due to being constrained by data distribution rather than model parameters, supported by clear experimental evidence across multiple datasets
- **Medium confidence**: The claim that L1 norm is superior to L∞ for on-manifold attacks, as the paper provides experimental evidence but doesn't explore alternative norm formulations
- **Low confidence**: The generalizability claim that the approach works across "various models, unseen data categories" - while supported by experiments, the paper doesn't provide theoretical guarantees or extensive ablation studies

## Next Checks

1. Reproduce the OMG-ATTACK architecture on MNIST with a simple CNN target and verify that training converges with L1 norm but fails with L∞ norm as claimed
2. Test transferability to a defended model (such as a model with adversarial training) to verify the claim of effectiveness against defended architectures
3. Evaluate attack success on a dataset not used in training (e.g., CIFAR-10) to test true generalizability claims beyond the three specified datasets