---
ver: rpa2
title: Improved POS tagging for spontaneous, clinical speech using data augmentation
arxiv_id: '2307.05796'
source_url: https://arxiv.org/abs/2307.05796
tags:
- data
- parser
- speech
- tags
- restart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving POS tagging of transcripts
  of speech from clinical populations. In contrast to prior work on parsing and POS
  tagging of transcribed speech, we do not make use of an in domain treebank for training.
---

# Improved POS tagging for spontaneous, clinical speech using data augmentation

## Quick Facts
- arXiv ID: 2307.05796
- Source URL: https://arxiv.org/abs/2307.05796
- Authors: 
- Reference count: 10
- Primary result: Data augmentation techniques improve POS tagging accuracy for clinical speech transcripts by bridging the domain gap between newswire and spontaneous speech

## Executive Summary
This paper addresses the challenge of POS tagging for spontaneous clinical speech, where traditional parsers trained on newswire text struggle due to the presence of dysfluencies like repetitions, partial words, and fillers. The authors propose a data augmentation approach that transforms Penn Treebank structures to resemble clinical speech patterns. By applying syntactic transformations including repetition, partial words, fillers, and restarts to the training data, they create augmented versions that better match the distribution of spontaneous clinical speech. The approach is evaluated on manually validated POS tags from clinical speech produced by patients with neurodegenerative conditions, showing significant improvements particularly for tags representing dysfluencies.

## Method Summary
The authors train a parser on an out-of-domain treebank of newswire (Penn Treebank) using data augmentation techniques to make these structures resemble natural, spontaneous speech. They employ a pipeline of syntactic transformations including repetition, partial words, fillers, and restarts to introduce dysfluency-like patterns into the training data. The parser architecture is based on the Berkeley Neural Parser (Kitaev et al. 2019) using RoBERTa embeddings, which jointly optimizes POS tagging with parsing. The augmented data is compared against standard training by evaluating POS tagging performance on a manually validated test set of clinical speech transcripts from patients with various neurodegenerative conditions.

## Key Results
- F1 scores increase across all POS tags when using augmented training data compared to standard training
- Three tags show particularly notable improvements: PT (partial words), UH (fillers), and SYM (symbols)
- The parser trained with augmentations maintains performance on standard PTB data while improving on clinical speech
- The improvement is attributed to the model's exposure to synthetic dysfluency patterns that are common in clinical speech but absent in newswire text

## Why This Works (Mechanism)

### Mechanism 1
Data augmentation using syntactic transformations (repetition, partial words, fillers) aligns out-of-domain training data with the distribution of spontaneous clinical speech. The augmentation pipeline introduces dysfluency-like patterns into Penn Treebank trees, making them structurally similar to clinical speech transcripts. This bridges the domain gap by adding variability and noise typical of spontaneous speech. The core assumption is that the augmented syntactic structures are semantically valid and preserve parseability while increasing resemblance to clinical speech. While the paper shows improved POS accuracy, it does not directly verify that the augmented trees match real clinical speech distributions.

### Mechanism 2
Augmentations improve POS tagging accuracy by exposing the model to dysfluency patterns not present in the original Penn Treebank. By injecting fillers (UH), partial words (PT), and repetitions into the training trees, the parser learns to recognize and correctly tag these tokens, which are common in clinical speech but absent in newswire text. The core assumption is that the model can generalize from synthetic dysfluency patterns to real-world clinical speech variations. The paper shows F1 improves for PT and UH tags, but does not demonstrate that the synthetic patterns match real clinical dysfluencies.

### Mechanism 3
The parser architecture (Kitaev et al. 2019) can effectively leverage augmented data because it is transformer-based and jointly optimizes POS tagging with parsing. The model's self-attention mechanism and span-based scoring allow it to adapt to the augmented syntactic structures without requiring architectural changes. The joint POS classifier benefits from exposure to varied input patterns. The core assumption is that the transformer encoder can learn from the augmented training data without overfitting or losing accuracy on standard data. The paper reports that the model maintains performance on standard PTB data while improving on clinical data.

## Foundational Learning

- **Concept**: Out-of-domain adaptation through data augmentation
  - Why needed here: The paper lacks in-domain training data (clinical speech treebanks), so augmentation is used to bridge the domain gap between newswire and spontaneous speech.
  - Quick check question: What is the main challenge in applying a parser trained on newswire to clinical speech, and how does data augmentation address it?

- **Concept**: Dysfluency modeling in syntactic parsing
  - Why needed here: Clinical speech contains high rates of dysfluencies (repetitions, restarts, fillers), which are rare in newswire text. The parser must learn to handle these patterns.
  - Quick check question: Why are dysfluency patterns more frequent in clinical speech than in newswire, and how does the augmentation pipeline simulate them?

- **Concept**: Joint optimization of POS tagging and parsing
  - Why needed here: The parser jointly learns POS tags and syntactic structures, allowing improvements in POS accuracy to benefit parsing performance and vice versa.
  - Quick check question: How does joint optimization of POS tagging and parsing differ from training separate models, and what advantage does it provide in this context?

## Architecture Onboarding

- **Component map**: Data augmentation pipeline (repetition, partial words, fillers, restarts) -> Berkeley Neural Parser (Kitaev et al. 2019) with RoBERTa embeddings -> POS tagging classifier (jointly optimized with span classifier) -> Evaluation on clinical speech test set (CLINpos) and augmented PTB

- **Critical path**: 1. Apply augmentations to PTB training data 2. Train parser on augmented data 3. Parse clinical speech test set 4. Evaluate POS accuracy and dysfluency detection

- **Design tradeoffs**: Augmentation complexity vs. model generalization; Synthetic dysfluencies vs. real clinical speech patterns; Joint optimization vs. separate POS and parsing models

- **Failure signatures**: Large drop in POS accuracy on standard PTB data; Poor dysfluency detection F1 on clinical speech; Overfitting to synthetic patterns not present in real speech

- **First 3 experiments**: 1. Train parser on unaugmented PTB, evaluate on CLINpos to establish baseline 2. Apply only repetition augmentation, train and evaluate to isolate its effect 3. Apply all augmentations, train and evaluate to measure cumulative impact

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed data augmentation techniques compare to training on in-domain materials (when available)? The paper suggests comparing the two approaches on Switchboard conversational telephone speech, but does not provide this direct comparison.

### Open Question 2
Are the data augmentation techniques and in-domain training complementary, and can they be combined to further improve performance? The paper suggests exploring the combination of both approaches but does not provide results on their potential complementarity.

### Open Question 3
How can the data augmentation methods be improved, particularly the RESTART augmentation? The paper mentions intentions to improve these methods by moving away from hand-engineered frequencies and incorporating external sources like chatGPT, but does not provide evidence on the effectiveness of these improvements.

## Limitations
- The evaluation relies entirely on a manually validated test set without clear information about inter-annotator agreement or annotation guidelines
- The augmented trees are generated through deterministic rules without quantitative validation that they accurately reflect real clinical speech distributions
- The paper does not report whether the parser maintains performance on standard text when trained with augmentations

## Confidence

- **High confidence**: The parser architecture can handle augmented data without catastrophic forgetting of standard PTB structures
- **Medium confidence**: The specific POS tag improvements (PT, UH, SYM) are attributable to the augmentation pipeline
- **Low confidence**: The augmented training data accurately represents the distribution of spontaneous clinical speech

## Next Checks
1. Compare the frequency distributions of PT, UH, and SYM tags in the augmented training data versus the manually validated clinical test set to verify that augmentation creates realistic patterns
2. Evaluate the parser trained on augmented data on a held-out subset of standard PTB to confirm that improvements on clinical speech don't come at the cost of degraded performance on newswire text
3. Conduct a qualitative analysis of POS tagging errors on clinical speech, categorizing them by type to understand whether improvements are concentrated in the targeted tags or represent broader gains