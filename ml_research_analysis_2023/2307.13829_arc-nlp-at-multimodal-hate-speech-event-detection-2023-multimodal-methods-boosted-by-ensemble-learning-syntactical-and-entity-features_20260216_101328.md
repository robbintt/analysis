---
ver: rpa2
title: 'ARC-NLP at Multimodal Hate Speech Event Detection 2023: Multimodal Methods
  Boosted by Ensemble Learning, Syntactical and Entity Features'
arxiv_id: '2307.13829'
source_url: https://arxiv.org/abs/2307.13829
tags:
- hate
- speech
- multimodal
- subtask
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the detection of hate speech and propaganda
  in text-embedded images from the Russia-Ukraine war. The authors propose multimodal
  deep learning models enhanced with ensemble learning, syntactical features, and
  named entity recognition (NER) features.
---

# ARC-NLP at Multimodal Hate Speech Event Detection 2023: Multimodal Methods Boosted by Ensemble Learning, Syntactical and Entity Features

## Quick Facts
- arXiv ID: 2307.13829
- Source URL: https://arxiv.org/abs/2307.13829
- Reference count: 15
- Primary result: First place in both hate speech detection (85.65% F1) and target detection (76.34% F1) subtasks at CASE 2023 Multimodal Hate Speech Event Detection

## Executive Summary
This paper presents multimodal deep learning models enhanced with ensemble learning, syntactical features, and named entity recognition (NER) features for detecting hate speech and propaganda in text-embedded images from the Russia-Ukraine war. The authors propose a comprehensive approach that combines visual and textual information with structured features, achieving state-of-the-art performance on both subtasks of the CASE 2023 competition. Their ensemble methods, which integrate multiple model architectures and feature types, outperform all baselines including unimodal approaches.

## Method Summary
The proposed method combines multimodal deep learning with ensemble learning and structured features. For hate speech detection (Subtask A), they use an ensemble of multimodal models and text-based tabular models trained on syntactical features extracted from OCR text. For target detection (Subtask B), they combine multimodal embeddings with NER features. The approach involves OCR text extraction, feature engineering (syntactical attributes and BoW n-grams for Subtask A, named entity counts for Subtask B), multimodal model training (CLIP or text+vision encoders), and ensemble learning for final predictions.

## Key Results
- Achieved first place on both subtasks of the CASE 2023 competition
- Subtask A: 85.65% F1 score for hate speech detection
- Subtask B: 76.34% F1 score for target detection
- Outperformed all textual, visual, and multimodal baselines

## Why This Works (Mechanism)

### Mechanism 1
Ensemble learning combining multimodal embeddings with syntactical and NER features improves hate speech detection performance over single-modality baselines. Multiple diverse models capture different aspects of hate speech, and the ensemble leverages complementary strengths through weighted combination.

### Mechanism 2
Named entity recognition features improve target detection by providing explicit indicators of individuals, communities, and organizations mentioned in text. Entity count vectors capture the presence and prominence of potential targets, which are then concatenated with CLIP embeddings for classification.

### Mechanism 3
Syntactical features (word counts, character ratios, capitalization patterns, symbol usage) provide discriminative information for hate speech detection independent of semantic content. Text structure patterns characteristic of hate speech are captured by tabular classifiers trained on these features.

## Foundational Learning

- **Multimodal learning and fusion techniques**: Why needed here - The task requires understanding both visual content and text within images, which cannot be adequately captured by single-modality models. Quick check: What are the three main approaches to multimodal fusion, and which one does this paper use?

- **Ensemble learning and model weighting**: Why needed here - Combining diverse models improves robustness and performance by leveraging complementary strengths of different architectures. Quick check: How does weighted ensemble differ from simple majority voting, and why is it beneficial here?

- **Named Entity Recognition and feature engineering**: Why needed here - NER provides explicit structural information about potential targets (individuals, communities, organizations) that can be used as discriminative features. Quick check: What are the three entity types extracted in this work, and how do they map to the target detection classes?

## Architecture Onboarding

- **Component map**: OCR text extraction → Text encoder (ELECTRA) + Vision encoder (Swin) → MLP fusion → Syntactical/BoW features → Tabular classifiers → Weighted ensemble → Final prediction (Subtask A). OCR → Text encoder + Vision encoder → CLIP fine-tuning → NER extraction → Entity count vector → Fusion with CLIP embeddings → Tabular classifier → Final prediction (Subtask B).

- **Critical path**: For Subtask A: OCR → Text encoder + Vision encoder → MLP fusion → Syntactical/BoW features → Tabular classifiers → Weighted ensemble → Final prediction. For Subtask B: OCR → Text encoder + Vision encoder → CLIP fine-tuning → NER extraction → Entity count vector → Fusion with CLIP embeddings → Tabular classifier → Final prediction.

- **Design tradeoffs**: Multimodal approaches provide better performance but require more computational resources and complex training pipelines compared to single-modality approaches. Ensemble methods improve accuracy but increase inference latency and complexity. Using pretrained models (CLIP, SpaCy NER) provides strong baselines but may not be optimally tuned for the specific domain.

- **Failure signatures**: Poor performance on either modality (text or image) will limit overall system performance. If OCR quality is low, text-based features become unreliable. If entity recognition misses important targets, target detection accuracy will suffer. If ensemble weights are poorly optimized, the ensemble may perform worse than individual models.

- **First 3 experiments**: 
  1. Train and evaluate individual text-only and image-only models to establish baseline performance and identify which modality contributes more
  2. Implement multimodal fusion with CLIP and evaluate performance improvement over unimodal baselines
  3. Add syntactical features to tabular models and evaluate whether this improves hate speech detection performance beyond semantic models alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed multimodal models perform on hate speech detection datasets from conflicts other than the Russia-Ukraine war?
- Basis in paper: [explicit] The paper focuses on the Russia-Ukraine war dataset, but does not evaluate performance on other conflicts or time periods.
- Why unresolved: The study's evaluation is limited to a single conflict, which may not generalize to other contexts or types of hate speech.
- What evidence would resolve it: Testing the models on hate speech datasets from different conflicts, time periods, or regions to assess generalization and robustness.

### Open Question 2
- Question: How do the syntactical features and named entity recognition (NER) features compare in terms of computational efficiency and training time?
- Basis in paper: [inferred] The paper combines syntactical features and NER features with multimodal models but does not provide a detailed comparison of their computational costs or training efficiency.
- Why unresolved: While both feature types improve performance, their impact on model training time and resource usage is not discussed.
- What evidence would resolve it: Benchmarking the training time, memory usage, and inference speed of models using syntactical features versus NER features to determine trade-offs.

### Open Question 3
- Question: Can the ensemble learning approach be extended to dynamically adjust model weights during inference for real-time applications?
- Basis in paper: [explicit] The paper uses a weighted ensembler for combining model predictions but does not explore dynamic weight adjustment during inference.
- Why unresolved: Static ensemble weights are effective for offline evaluation but may not adapt well to evolving data patterns in real-time applications.
- What evidence would resolve it: Implementing and testing a dynamic ensemble method that updates weights based on incoming data streams or feedback loops to improve real-time performance.

## Limitations
- Domain-specific nature of dataset (Russia-Ukraine war images) may limit generalizability to other contexts
- Reliance on OCR accuracy - poor text extraction would directly degrade performance of text-based features
- Lack of ablation studies to quantify individual contribution of each component (syntactical features, NER features, ensemble learning)

## Confidence
- **High confidence**: The core methodology of using multimodal fusion with CLIP and ensemble learning is well-established in the literature
- **Medium confidence**: The effectiveness of syntactical features and NER features for hate speech detection, while plausible, lacks direct empirical validation within this paper
- **Low confidence**: The assumption that ensemble learning will generalize well to other hate speech datasets without extensive hyperparameter tuning

## Next Checks
1. Conduct ablation studies removing syntactical features, NER features, and ensemble components individually to quantify their specific contributions to performance gains
2. Test the model on hate speech datasets from different domains (e.g., general social media, different geopolitical contexts) to assess generalizability beyond the Russia-Ukraine war corpus
3. Implement cross-validation on the training set to evaluate model stability and ensure that the high F1 scores aren't artifacts of specific train/test splits or overfitting to the competition dataset