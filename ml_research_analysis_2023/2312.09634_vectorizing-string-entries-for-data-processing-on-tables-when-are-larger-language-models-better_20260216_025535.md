---
ver: rpa2
title: 'Vectorizing string entries for data processing on tables: when are larger
  language models better?'
arxiv_id: '2312.09634'
source_url: https://arxiv.org/abs/2312.09634
tags:
- language
- entries
- text
- embeddings
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates vectorization strategies for string entries
  in tabular data processing. The authors benchmark 14 supervised learning tasks and
  a fuzzy join benchmark to compare different text embedding approaches.
---

# Vectorizing string entries for data processing on tables: when are larger language models better?

## Quick Facts
- arXiv ID: 2312.09634
- Source URL: https://arxiv.org/abs/2312.09634
- Reference count: 40
- Primary result: Language models improve vectorization for diverse string entries but not for "dirty categories"

## Executive Summary
This paper evaluates vectorization strategies for string entries in tabular data processing, comparing 14 supervised learning tasks and fuzzy join benchmarks across different embedding approaches. The authors introduce a metric based on unique n-grams per row to characterize columns as either "dirty categories" (low diversity) or "diverse entries" (high diversity). They find that for dirty categories, simple string models like MinHashEncoder perform comparably to large language models, while for diverse entries, larger language models consistently outperform simpler approaches. The study concludes that fine-tuned models like e5 (v2) provide an excellent balance of performance and efficiency for tabular analytics tasks.

## Method Summary
The authors benchmark various text embedding approaches on tabular datasets, using ROC-AUC for analytics tasks and F1 score for fuzzy joins. They classify columns based on unique n-gram counts per row, applying MinHashEncoder to "dirty categories" and language model embeddings to "diverse entries." Embeddings are reduced to 30 dimensions using PCA before concatenation with numerical features and training with GradientBoostingClassifier or 1-NearestNeighbor. The study compares pretrained and fine-tuned models across different sizes, from 330M to 7B parameters.

## Key Results
- Language models show minimal benefit over simpler string models for "dirty categories" (low n-gram diversity)
- For "diverse entries" (high n-gram diversity), larger language models consistently improve performance
- Fine-tuned models like e5 (v2) achieve performance comparable to much larger models while being more efficient
- The unique n-gram count per row (threshold ~3000 for 1000 rows) effectively separates dirty categories from diverse entries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger language models improve vectorization for diverse string entries but not for "dirty categories"
- Mechanism: Language models capture semantic relationships and background knowledge that substring-based methods cannot, particularly when strings are diverse and not easily reducible to simple categories
- Core assumption: Diverse entries contain meaningful variation that requires semantic understanding beyond surface-level n-gram patterns
- Evidence anchors:
  - [abstract] "For dirty categories, pretrained language models bring little-to-no benefit compared to simpler string models. For diverse entries, we show that larger language models improve data processing."
  - [section] "dirty categories setting, where strings share much similarities across entries, and conversely 2) a diverse entries setting... For dirty categories, pretrained language models bring little-to-no benefit compared to simpler string models. For diverse entries, we show that larger language models improve data processing."
  - [corpus] Corpus evidence is weak here - only 8 of 25 related papers are directly relevant, suggesting this specific mechanism hasn't been extensively studied in tabular contexts
- Break condition: When string entries are highly repetitive or follow predictable patterns that can be captured by substring statistics

### Mechanism 2
- Claim: The diversity of strings can be predicted by the number of unique n-grams per row
- Mechanism: Low unique n-gram counts indicate "dirty categories" where entries are variations of the same concept, while high counts indicate diverse entries requiring semantic understanding
- Core assumption: The relationship between n-gram diversity and semantic complexity is monotonic and predictable
- Evidence anchors:
  - [section] "We introduce a simple characterization of a column that reveals two settings... 1) a dirty categories setting, where strings share much similarities across entries... 2) a diverse entries setting... we can separate the columns in two groups, based on a simple metric, the number of unique ngrams in the column for 1000 rows"
  - [section] "empirically below 3000 unique ngrams for 1000 rows" for dirty categories
  - [corpus] Weak evidence - corpus contains related work on table understanding but not specifically on n-gram diversity as a predictive metric
- Break condition: When strings contain rare words or technical terminology that artificially inflates n-gram counts without increasing semantic diversity

### Mechanism 3
- Claim: Fine-tuning language models for sentence similarity tasks improves performance more than increasing model size alone
- Mechanism: Fine-tuning aligns the model's internal representations with the specific task of comparing and embedding sentences, which is more efficient than scaling model parameters
- Core assumption: The benefits of fine-tuning transfer to the specific task of vectorizing table entries
- Evidence anchors:
  - [section] "larger models tend to perform better, but it is useful to fine tune them for embedding purposes"
  - [section] "in Figure 5, we see that a better and newer finetuning procedure translates to bigger gain on the tabular analytics task"
  - [section] "small finetuned models like bge or e5 arrive at close or better performances than the largest models in our table while being an order of magnitude smaller (330M vs 7B parameters)"
  - [corpus] No direct corpus evidence for this specific claim about fine-tuning vs scaling for tabular data
- Break condition: When fine-tuning data is insufficient or unrepresentative of the target task distribution

## Foundational Learning

- Concept: Vectorization of high-cardinality categorical features
  - Why needed here: The paper focuses on converting string entries into numerical vectors for downstream ML tasks
  - Quick check question: What are the limitations of one-hot encoding for high-cardinality categorical features?

- Concept: N-gram analysis for text similarity
  - Why needed here: The paper uses unique n-gram counts per row to classify columns as dirty categories or diverse entries
  - Quick check question: How does Jaccard similarity between n-gram sets relate to string similarity?

- Concept: Contrastive learning for embeddings
  - Why needed here: The paper discusses how fine-tuned models like e5 and bge use contrastive learning to create better sentence embeddings
  - Quick check question: What is the key difference between pretraining and fine-tuning approaches for language models?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Text vectorization module -> Numerical feature encoding -> ML model training
  Vectorization module: Choice between MinHashEncoder, TF-IDF, or language model embeddings
  Post-processing: Optional PCA dimensionality reduction before concatenation with numerical features

- Critical path:
  1. Identify text columns and classify as dirty categories vs diverse entries using n-gram diversity metric
  2. Apply appropriate vectorization method (MinHashEncoder for dirty categories, language model for diverse entries)
  3. Reduce embedding dimensions with PCA if needed
  4. Concatenate with numerical features
  5. Train ML model on combined features

- Design tradeoffs:
  - MinHashEncoder: Fast and lightweight but cannot capture semantic relationships
  - Language models: Better semantic understanding but higher computational cost and latency
  - Fine-tuned vs pretrained: Fine-tuned models are smaller and more efficient but may overfit to specific tasks

- Failure signatures:
  - Low ROC-AUC gains (< 1%) when including text features suggest dirty categories or poor vectorization
  - High variance in model performance across folds indicates overfitting to specific string patterns
  - Long embedding computation times with minimal performance gains suggest unnecessary use of large language models

- First 3 experiments:
  1. Run the pipeline with MinHashEncoder on all text columns to establish baseline performance
  2. Compute unique n-gram counts per row and classify columns as dirty categories vs diverse entries
  3. Apply language model embeddings only to diverse entry columns and compare performance gains to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do computational costs scale when using language models for text entry vectorization in tabular data processing, particularly when embedding is required for large datasets with millions of records?
- Basis in paper: [explicit] The paper discusses operational complexity and energy consumption concerns when using larger language models for vectorizing text entries in tables.
- Why unresolved: The paper mentions these concerns but does not provide quantitative analysis of computational costs across different model sizes or dataset scales.
- What evidence would resolve it: Detailed benchmarking studies showing runtime, memory usage, and energy consumption for different model sizes across various dataset scales.

### Open Question 2
- Question: Can we develop more efficient methods to extract relevant background knowledge from language models without running the full model, particularly for tabular analytics tasks?
- Basis in paper: [explicit] The paper mentions this as an interesting avenue of research, noting that better information can be extracted from earlier layers in large language models.
- Why unresolved: The paper identifies this as a potential research direction but does not explore or validate any specific methods.
- What evidence would resolve it: Empirical studies comparing knowledge extraction methods that access different layers or parts of language models, showing performance and efficiency trade-offs.

### Open Question 3
- Question: How can we adapt language models to specific databases to achieve better representations while managing the increased computational and operational costs?
- Basis in paper: [explicit] The paper discusses this as a potential future direction, noting that adapting models to specific databases could improve representations but would increase computational costs.
- Why unresolved: The paper identifies this as a challenge but does not explore specific adaptation strategies or cost-benefit analyses.
- What evidence would resolve it: Comparative studies of adapted vs. general-purpose language models across different database types, including detailed analysis of adaptation costs and performance gains.

## Limitations

- The 3000 unique n-grams per 1000 rows threshold for characterizing dirty categories vs diverse entries lacks theoretical grounding and may be dataset-dependent
- The evaluation focuses on supervised learning tasks and fuzzy joins, leaving open questions about performance on unsupervised tasks or regression problems
- The study does not explore hybrid approaches that might combine semantic understanding with n-gram statistics for optimal performance across both categories

## Confidence

- **High confidence**: The core finding that larger language models improve vectorization for diverse string entries but not for dirty categories is well-supported by empirical results across multiple datasets and tasks.
- **Medium confidence**: The claim that fine-tuning provides better performance gains than model scaling alone is supported but based on limited comparisons between specific model families rather than systematic ablation studies.
- **Low confidence**: The assertion that the 3000 unique n-grams per 1000 rows threshold optimally characterizes dirty categories versus diverse entries lacks rigorous validation and may be dataset-dependent.

## Next Checks

1. Test the n-gram diversity threshold (3000 unique n-grams per 1000 rows) across diverse datasets to determine if this value generalizes or requires tuning for different domains.
2. Compare the proposed pipeline against hybrid approaches that use both semantic embeddings and n-gram statistics for columns that fall near the threshold between dirty categories and diverse entries.
3. Evaluate the impact of different dimensionality reduction techniques (e.g., UMAP, autoencoders) versus PCA on the performance of language model embeddings in tabular contexts.