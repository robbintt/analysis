---
ver: rpa2
title: Actor-Critic with variable time discretization via sustained actions
arxiv_id: '2308.04299'
source_url: https://arxiv.org/abs/2308.04299
tags:
- learning
- action
- time
- discretization
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SusACER, a reinforcement learning algorithm
  that leverages variable time discretization to balance training efficiency and performance.
  By initially using sparse time discretization and gradually transitioning to finer
  discretization, SusACER combines the benefits of easier training and better ultimate
  performance.
---

# Actor-Critic with variable time discretization via sustained actions

## Quick Facts
- arXiv ID: 2308.04299
- Source URL: https://arxiv.org/abs/2308.04299
- Reference count: 38
- Primary result: SusACER outperforms state-of-the-art algorithms in robotic control tasks by combining variable time discretization with sustained actions

## Executive Summary
This paper introduces SusACER, a reinforcement learning algorithm that leverages variable time discretization to balance training efficiency and performance. By initially using sparse time discretization and gradually transitioning to finer discretization, SusACER combines the benefits of easier training and better ultimate performance. The algorithm modifies Actor-Critic with Experience Replay (ACER) by incorporating sustained actions, allowing the agent to learn optimal action durations. Experimental results on robotic control environments (Ant, HalfCheetah, Hopper, and Walker2D) demonstrate that SusACER outperforms state-of-the-art algorithms, achieving higher scores and faster learning speeds.

## Method Summary
SusACER extends ACER by introducing sustained actions with variable time discretization. The algorithm samples action durations from a geometric distribution with parameter pt that linearly decays from initial value E0 to 1 over TE training steps. When actions are sustained, the policy maintains the same action for multiple time steps, reducing decision frequency early in training. The method incorporates importance sampling for off-policy corrections and uses soft truncation to prevent weight explosion. Action covariance is scaled inversely with expected duration to stabilize exploration variance.

## Key Results
- Outperforms ACER, SAC, and PPO on robotic control benchmarks with higher final scores and faster learning
- Variable time discretization enables faster initial learning through reduced search space
- Sustained actions improve sample efficiency while maintaining precision through gradual discretization refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variable time discretization improves sample efficiency by allowing longer actions early in training when fewer discrete decisions are needed.
- Mechanism: Early in training, SusACER uses large expected action durations (E0 = 4 or 8), reducing the number of decision points and shrinking the search space. This enables faster learning before transitioning to finer discretization for precision.
- Core assumption: Longer initial actions do not sacrifice ultimate performance because the policy can later adapt to shorter, more precise actions.
- Evidence anchors: [abstract] "Initially, it operates with sparse time discretization and gradually switches to a fine one."

### Mechanism 2
- Claim: Sustained actions reduce exploration variance in continuous control by keeping action covariance inversely proportional to action duration.
- Mechanism: When actions are sustained longer, the state covariance grows proportionally to the square of the duration. By scaling action covariance inversely with expected duration, the total state variance is kept roughly constant, stabilizing learning.
- Core assumption: The underlying continuous-time dynamics can be approximated locally as affine so that covariance propagation is predictable.
- Evidence anchors: [section 4.2] "we set the covariance of the action distribution inversely proportional to the expected time of sustaining actions."

### Mechanism 3
- Claim: Importance sampling with sustained actions remains feasible even when p → 1 because the infinite Dirac delta terms cancel in the density ratio.
- Mechanism: For continuous actions, when p < 1 the density ratio reduces to a finite ratio of actor policies times a ratio of sustain probabilities; when p = 1, the infinite part is shared in numerator and denominator, leaving a well-defined finite ratio.
- Core assumption: Actions drawn from continuous distributions are almost surely distinct, so aτ = aτ−1 has probability zero.
- Evidence anchors: [section 4.1] "Forp < 1 the expression (1 − p)δ(0) is infinite. However, for pτ < 1 the infinite part is in both the nominator and denominator of this expression and the density ratio reduces to the following form."

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of reinforcement learning
  - Why needed here: The algorithm relies on MDP assumptions to justify importance sampling and value function estimation.
  - Quick check question: In an MDP, does the next state depend only on the current state and action, or also on earlier history?

- Concept: Actor-Critic architecture with experience replay
  - Why needed here: SusACER extends ACER, so understanding how actor and critic networks interact and how trajectories are stored/replayed is essential.
  - Quick check question: What is the role of the importance sampling ratio in off-policy actor-critic updates?

- Concept: Geometric distribution and its properties
  - Why needed here: The probability of sustaining an action follows a geometric distribution, which determines the expected action duration.
  - Quick check question: If the success probability is p, what is the expected number of trials until the first success?

## Architecture Onboarding

- Component map: Environment -> Action Sustain Scheduler -> Actor Network -> Critic Network -> Experience Replay Buffer -> Training Loop
- Critical path:
  1. Environment step → store transition in buffer
  2. When enough samples, sample trajectory of length n
  3. Compute IS weights with soft truncation
  4. Compute multi-step TD targets
  5. Update critic (ν) and actor (θ) using ADAM
  6. Repeat

- Design tradeoffs:
  - Longer initial E0 speeds early learning but risks suboptimal final control granularity
  - Smaller TE accelerates discretization refinement but may destabilize learning
  - Trajectory length n balances bias-variance in TD estimates

- Failure signatures:
  - Training collapse: IS weights become NaN or explode → check soft truncation parameter b
  - Poor final performance: final E0 too large → reduce initial E0
  - Slow convergence: trajectory length n too short → increase n
  - Unstable exploration: action covariance not scaled with duration → verify scaling logic

- First 3 experiments:
  1. Run SusACER with E0=2, TE=3e4 on HalfCheetah; compare AULC to ACER baseline
  2. Vary E0 (2, 4, 8) with fixed TE=1e5 on Walker2D; plot learning curves and final scores
  3. Disable soft truncation (b→inf) and observe IS weight stability; confirm necessity of truncation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal initial action sustain length (E0) and decay time (TE) vary across different robotic control environments, and can these parameters be predicted or optimized without extensive empirical testing?
- Basis in paper: [explicit] The paper shows that optimal E0 and TE values vary between environments (Ant, HalfCheetah, Hopper, Walker2D) and require empirical testing.
- Why unresolved: The paper empirically determines optimal settings for each environment but does not provide a systematic method for predicting or optimizing these parameters for new environments.
- What evidence would resolve it: A predictive model or optimization algorithm that can determine optimal E0 and TE values based on environment characteristics (e.g., dynamics, action space, reward structure) without requiring extensive empirical testing.

### Open Question 2
- Question: How does the performance of SusACER compare to other state-of-the-art RL algorithms (e.g., SAC, PPO) when applied to environments with different time discretization requirements or continuous action spaces beyond robotic control?
- Basis in paper: [explicit] The paper compares SusACER to SAC and PPO on robotic control environments but does not explore other domains or time discretization requirements.
- Why unresolved: The paper focuses on robotic control environments with specific time discretization needs, leaving the generalizability of SusACER's performance to other domains and discretization requirements unexplored.
- What evidence would resolve it: Extensive empirical testing of SusACER on diverse environments with varying time discretization requirements and continuous action spaces, comparing its performance to other state-of-the-art RL algorithms.

### Open Question 3
- Question: What is the theoretical foundation for the optimal action sustain length (E0) and decay time (TE) in SusACER, and how do these parameters relate to the underlying continuous control process and Markov Decision Process?
- Basis in paper: [inferred] The paper empirically determines optimal E0 and TE values but does not provide a theoretical explanation for why these values are optimal or how they relate to the underlying continuous control process.
- Why unresolved: The paper relies on empirical results to determine optimal parameters but lacks a theoretical framework that explains the relationship between these parameters and the underlying control process.
- What evidence would resolve it: A theoretical analysis that derives optimal E0 and TE values based on the characteristics of the continuous control process and Markov Decision Process, explaining the relationship between these parameters and the underlying system dynamics.

## Limitations

- Performance claims based on limited environment set without statistical significance testing
- Lack of ablation studies to isolate contribution of individual components
- Theoretical mechanisms only partially validated with untested assumptions about affine dynamics

## Confidence

- Performance claims (outperformance of baselines): Medium - Results are promising but based on a limited environment set without statistical significance testing
- Theoretical mechanism (variance stabilization via inverse covariance scaling): Low - Core assumption of affine dynamics approximation untested
- Importance sampling derivation for continuous actions: Medium - Theoretical derivation appears sound but lacks empirical validation of edge cases

## Next Checks

1. Implement ablation studies comparing SusACER against variants with fixed discretization, disabled sustained actions, and removed soft truncation to isolate performance drivers
2. Test the algorithm across a broader range of continuous control tasks including non-robotic domains to assess generalizability
3. Conduct sensitivity analysis on the transition schedule parameters (E0, TE) to identify optimal configurations and failure modes