---
ver: rpa2
title: Robust Adversarial Attacks Detection for Deep Learning based Relative Pose
  Estimation for Space Rendezvous
arxiv_id: '2311.05992'
source_url: https://arxiv.org/abs/2311.05992
tags:
- pose
- adversarial
- relative
- attack
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel approach for detecting adversarial
  attacks in deep learning-based relative pose estimation for spacecraft rendezvous.
  A convolutional neural network (CNN) is designed to estimate the target's relative
  position and rotation from onboard camera images.
---

# Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous

## Quick Facts
- arXiv ID: 2311.05992
- Source URL: https://arxiv.org/abs/2311.05992
- Reference count: 40
- Detection accuracy of 99.21% in simulations and 96.29% in laboratory experiments

## Executive Summary
This paper presents a novel approach for detecting adversarial attacks in deep learning-based relative pose estimation for spacecraft rendezvous. The method combines a CNN-based pose estimator with an LSTM-based detector that monitors SHAP values to identify adversarial perturbations. The system achieves high detection accuracy in both synthetic and real-world laboratory settings, demonstrating its potential for enhancing the safety and reliability of autonomous spacecraft operations.

## Method Summary
The approach involves a CNN-based pose estimator (modified Darknet-19) that predicts relative position and rotation from camera images. Adversarial attacks are generated using FGSM to perturb input images. SHAP values are computed from the GAP layer outputs using DeepSHAP, and these values are fed into an LSTM network for attack detection. The system is trained on synthetic data generated in Blender and validated on real data from a laboratory-designed setup using a 1/9 scale Jason-1 spacecraft model.

## Key Results
- Achieves 99.21% detection accuracy on synthetic data
- Achieves 96.29% detection accuracy on real ASMI Lab data
- CNN-based pose estimator achieves position error < 0.5m and attitude error < 1 degree on synthetic data
- Detection performance degrades gracefully with decreasing FGSM perturbation magnitude (ε)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The LSTM-based adversarial detector successfully identifies adversarial attacks by learning temporal dependencies in SHAP value sequences.
- **Mechanism:** SHAP values from the GAP layer of the CNN-based pose estimator capture feature importance changes caused by adversarial perturbations. The LSTM processes these sequences to detect subtle anomalies.
- **Core assumption:** SHAP value patterns differ consistently between normal and adversarial inputs across time steps.
- **Evidence anchors:**
  - [abstract] "An LSTM-based detector exploiting the explainable Shap values of the CNN based estimator is then proposed to detect the adversarial attacks acting on the input images"
  - [section] "To detect any incoming adversarial attacks on the spacecraft deep relative pose estimator through the onboard camera, an LSTM-based adversarial attacks detector is proposed. The detector aims to monitor the SHAP values generated from the output of the GAP layer and detect any slight anomaly changes that could result based on an adversarial attack."
  - [corpus] Weak evidence - no directly comparable work found in corpus, but adversarial detection via LSTM and SHAP values is a novel approach.
- **Break condition:** If adversarial perturbations do not consistently alter SHAP value patterns, or if attack patterns become too diverse for LSTM to generalize.

### Mechanism 2
- **Claim:** FGSM-generated adversarial perturbations degrade pose estimation accuracy, enabling detection.
- **Mechanism:** Small, human-imperceptible perturbations to input images maximize CNN loss, causing significant errors in relative pose estimates. This degradation is measurable and detectable.
- **Core assumption:** The relationship between perturbation magnitude (ε) and pose estimation error is monotonic and detectable.
- **Evidence anchors:**
  - [abstract] "We perturb seamlessly the input images using adversarial attacks that are generated by the Fast Gradient Sign Method (FGSM)"
  - [section] "The FGSM attacks aim to add small perturbations to the input images where original and perturbed images look similar in human vision but can significantly impact the CNNs' predictions"
  - [section] "We can clearly see that the adversarial attack can result in a significant impact on the guidance based DNN-based relative navigator typically when the distance between the camera and the target is smaller than 30m"
  - [corpus] Weak evidence - no directly comparable work found in corpus, but FGSM's impact on pose estimation is demonstrated in this paper.
- **Break condition:** If adversarial attacks become imperceptible to both humans and detection systems, or if attack methods evolve beyond FGSM.

### Mechanism 3
- **Claim:** DeepSHAP efficiently computes SHAP values for CNN feature maps, enabling real-time detection.
- **Mechanism:** Instead of computing SHAP values for all pixels (computationally expensive), DeepSHAP is applied to the GAP layer output (1000 neurons), significantly reducing computation while preserving detection capability.
- **Core assumption:** SHAP values computed at the GAP layer sufficiently capture feature importance changes relevant to attack detection.
- **Evidence anchors:**
  - [section] "To train the adversarial attack detector, we generated 15,000 sets of SHAP values for normal samples and an additional 15,000 sets of SHAP values for adversarial samples... These samples serve as the background samples for the DeepSHAP explainer"
  - [section] "Therefore, in this work, we consider computing the SHAP values for the subsampling layer in the pose estimator, instead of computing them for the input image... As a result, SHAP values are generated for the outputs of the GAP layer that only need to compute 1000 features"
  - [corpus] Weak evidence - no directly comparable work found in corpus, but DeepSHAP application to CNN feature maps is a novel approach.
- **Break condition:** If GAP layer SHAP values lose critical information needed for attack detection, or if computational savings compromise detection accuracy.

## Foundational Learning

- **Concept:** Spacecraft relative pose estimation
  - Why needed here: The entire system aims to detect attacks on a CNN that estimates spacecraft position and attitude from camera images
  - Quick check question: What are the six degrees of freedom in spacecraft pose estimation, and how are they typically represented?

- **Concept:** Adversarial attacks on neural networks
  - Why needed here: Understanding how FGSM and similar methods work is crucial for both attack implementation and detection
  - Quick check question: How does the Fast Gradient Sign Method (FGSM) generate adversarial examples, and what parameter controls perturbation magnitude?

- **Concept:** Explainable AI (XAI) and SHAP values
  - Why needed here: SHAP values are the core detection signal; understanding how they work is essential for the detection mechanism
  - Quick check question: What is the theoretical basis of SHAP values, and how does DeepSHAP make them computationally tractable for deep networks?

## Architecture Onboarding

- **Component map:**
  - CNN-based pose estimator (Darknet-19 backbone, GAP layer, FC layers for position/attitude)
  - FGSM attack generator
  - DeepSHAP explainer (computes SHAP values from GAP layer)
  - LSTM-based adversarial detector (processes SHAP value sequences)
  - Integration layer (coordinates data flow between components)

- **Critical path:** Image → CNN → GAP → DeepSHAP → SHAP values → LSTM → Detection decision
  - The GAP layer is the critical bottleneck for both pose estimation and attack detection

- **Design tradeoffs:**
  - GAP layer vs. full-pixel SHAP computation: 1000 vs. 355,200 features (computation vs. potential information loss)
  - LSTM sequence length (9 timesteps) vs. detection latency
  - FGSM attack strength (ε) vs. detection accuracy

- **Failure signatures:**
  - High false positives: LSTM detecting attacks when none exist (possibly due to training data imbalance)
  - High false negatives: Missing actual attacks (possibly due to attack sophistication or ε values too small)
  - Low pose estimation accuracy: CNN failing even without attacks (possibly due to overfitting or data quality issues)

- **First 3 experiments:**
  1. Test CNN pose estimation accuracy on synthetic data (position error < 0.5m, attitude error < 1 degree)
  2. Apply FGSM attacks with varying ε values and measure pose estimation degradation
  3. Train LSTM detector on SHAP value sequences and measure detection accuracy (target > 99% on synthetic data)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the FGSM perturbation parameter $\epsilon$ that balances attack effectiveness and detectability in spacecraft pose estimation?
- Basis in paper: [explicit] The paper tests various $\epsilon$ values (0.01, 0.05, 0.1, 0.3, 0.5, 1.0) and observes their impact on both attack success and detection accuracy.
- Why unresolved: Different $\epsilon$ values show varying trade-offs between attack effectiveness and detection rates. The paper doesn't identify an optimal value that balances these competing factors.
- What evidence would resolve it: Systematic experiments varying $\epsilon$ across the full range while measuring both attack success rate and detection accuracy, identifying the sweet spot where attacks are effective but still detectable.

### Open Question 2
- Question: How does the performance of the proposed adversarial attack detection method compare to alternative detection approaches for spacecraft pose estimation?
- Basis in paper: [inferred] The paper proposes an LSTM-based detector using SHAP values but doesn't compare it against other potential detection methods.
- Why unresolved: The paper demonstrates high detection accuracy but lacks comparative analysis against other detection strategies like traditional anomaly detection, other XAI methods, or black-box detection approaches.
- What evidence would resolve it: Head-to-head comparisons of the proposed method against multiple alternative detection approaches using the same datasets and metrics.

### Open Question 3
- Question: How well does the proposed method generalize to different spacecraft geometries and orbital scenarios beyond the Jason-1 and ASMI Lab setups tested?
- Basis in paper: [inferred] The paper tests on synthetic data and real data from a 1/9 scale Jason-1 model, but doesn't explore performance across diverse spacecraft types or orbital conditions.
- Why unresolved: The experiments are limited to specific spacecraft models and scenarios, leaving uncertainty about performance in varied real-world conditions with different target geometries, lighting conditions, or orbital parameters.
- What evidence would resolve it: Testing the complete system (pose estimator + detector) across multiple spacecraft types, orbital scenarios, and environmental conditions while measuring both pose estimation accuracy and detection performance.

## Limitations
- Detection accuracy degrades from 99.21% to 96.29% when moving from synthetic to real data, indicating potential domain adaptation challenges
- Computational overhead of SHAP value computation may be prohibitive for real-time spacecraft operations
- Limited testing against stronger adversarial attacks beyond basic FGSM perturbations

## Confidence
- **High Confidence:** CNN-based pose estimation architecture and FGSM attack generation are well-established techniques with clear implementation details
- **Medium Confidence:** LSTM-based detection using SHAP values shows strong performance but relies on assumptions about SHAP value pattern consistency
- **Low Confidence:** Real-world applicability and robustness against adaptive adversaries who might specifically target the detection mechanism

## Next Checks
1. Test the detection system against stronger adversarial attacks (PGD, CW) to evaluate robustness beyond basic FGSM perturbations
2. Conduct ablation studies removing the GAP layer optimization to quantify the trade-off between computational efficiency and detection accuracy
3. Implement the complete pipeline on the ASMI Lab testbed to validate end-to-end performance under realistic operational conditions, including varying lighting and occlusion scenarios