---
ver: rpa2
title: Discrete Messages Improve Communication Efficiency among Isolated Intelligent
  Agents
arxiv_id: '2312.15985'
source_url: https://arxiv.org/abs/2312.15985
tags:
- communication
- agents
- loss
- discrete
- codebook
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates communication efficiency among intelligent
  agents using discrete versus continuous messages. The authors hypothesize that discrete
  messages are more effective than continuous ones when agents have diverse experiences,
  and that multiple discrete tokens are more advantageous than a single token.
---

# Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents

## Quick Facts
- arXiv ID: 2312.15985
- Source URL: https://arxiv.org/abs/2312.15985
- Reference count: 40
- Primary result: Multi-token discretization reduces communication loss by 32.1%, 10.6%, and 3.7% compared to continuous semantics across three datasets

## Executive Summary
This paper investigates communication efficiency among intelligent agents using discrete versus continuous messages. The authors hypothesize that discrete messages are more effective than continuous ones when agents have diverse experiences, and that multiple discrete tokens are more advantageous than a single token. Through multi-agent machine learning experiments using image datasets (MNIST, CIFAR10, CelebA), the study demonstrates that communication through sentences composed of discrete tokens offers superior inter-agent communication efficiency when agents are exposed to different data. The results show that as the number of discrete tokens increases, communication loss decreases and the codebook is utilized more evenly.

## Method Summary
The paper employs a multi-agent communication framework where isolated agents must learn to share information through encoded messages. The core method uses VQVAE with multi-token discretization, where encoder outputs are segmented into multiple tokens that map to a shared codebook. The experimental setup involves training multiple agent pairs simultaneously with cross-training protocols, where speakers and listeners are randomly paired from different agent pairs during training to promote language alignment. Communication efficiency is measured through reconstruction loss between original and reconstructed images during cross-validation, with agents trained on overlapping but distinct subsets of the data.

## Key Results
- Multi-token discretization reduces communication loss by 32.1%, 10.6%, and 3.7% compared to continuous semantics under three different datasets
- As the number of discrete tokens increases, communication loss decreases and the codebook is utilized more evenly
- Cross-training with different agent pairs improves language alignment, with decreasing Euclidean distances between latent codebooks as learning progresses

## Why This Works (Mechanism)

### Mechanism 1
- Discrete messages outperform continuous messages when agents have diverse experiences
- Discrete encoding forces agents to create a shared codebook that can represent multiple experiences through distinct symbols, while continuous encoding allows information to blend and become less distinguishable
- Core assumption: Agents with different training data will converge on different continuous representations but can still align on discrete codebook entries
- Evidence: [abstract]: "discrete messages are more effective than continuous ones when agents have diverse personal experiences"; [section 4.1]: "the average loss incurred by using multiple discrete tokens for communication is 32.1%, 10.6%, and 3.7% lower than that incurred by using continuous semantics"
- Break condition: When agents have identical experiences or when the codebook becomes too large to learn efficiently

### Mechanism 2
- Multi-token discretization reduces communication loss compared to single-token
- Breaking the encoded representation into multiple segments allows the codebook to capture more granular features and reduces the quantization bottleneck
- Core assumption: The original continuous vector contains separable information that can be encoded more efficiently when divided
- Evidence: [abstract]: "communications using multiple discrete tokens are more advantageous than those using a single token"; [section 4.1]: "as the number of discrete tokens increases, the communication loss decreases"; [section 4.3]: "multi-token discretization facilitates the full utilization of the codebook"
- Break condition: When token count becomes too high relative to codebook size, causing overfitting or inefficient use of capacity

### Mechanism 3
- Cross-training with different agent pairs improves language alignment
- By randomly pairing speakers and listeners from different agent pairs during training, the system forces development of more generalizable communication protocols
- Core assumption: Different agent pairs develop slightly different language interpretations, and cross-training helps align these interpretations
- Evidence: [section 3.3]: "cross-training becomes necessary because it allows different agents to have the most similar understanding of the same language"; [section 4.3]: "as the iterative learning progresses, the Euclidean distances between the latent codebooks of different agents decrease"
- Break condition: When cross-training introduces too much noise and prevents specialization

## Foundational Learning

- **Vector quantization and discrete representation learning**: The core innovation relies on discretizing continuous encoder outputs into codebook entries. *Quick check: What happens to gradient flow in VQ-VAE when using straight-through estimator?*

- **Multi-agent reinforcement learning and emergent communication**: The experimental setup involves multiple agents learning to communicate through a shared protocol. *Quick check: How does the Lewis Game framework relate to the communication protocol being learned?*

- **Autoencoder architecture and reconstruction loss**: The communication efficiency is measured by reconstruction error between original and reconstructed images. *Quick check: What components make up the total loss in the VQ-VAE model?*

## Architecture Onboarding

- **Component map**: Input → Encoder → Multi-token Discretization → Codebook Quantization → Decoder → Reconstruction
- **Critical path**: Input → Encoder → Multi-token Discretization → Codebook Quantization → Decoder → Reconstruction
- **Design tradeoffs**: More tokens improve communication but increase computational cost and require larger codebooks
- **Failure signatures**: High reconstruction loss indicates poor communication; uneven codebook usage suggests suboptimal tokenization
- **First 3 experiments**:
  1. Implement single-token VQ-VAE baseline and verify it outperforms continuous AE
  2. Add multi-token discretization and measure communication loss reduction
  3. Test cross-training by randomly pairing agents during training and measuring alignment

## Open Questions the Paper Calls Out

- **Does the multi-token discretization approach consistently outperform the VAE model across all types of datasets, including non-image data?**
  - Basis: The authors mention that the multi-token discretization approach does not perform as well as the VAE model and acknowledge the lack of evaluation on non-image datasets
  - Why unresolved: The paper does not provide sufficient evidence or systematic comparisons with the VAE model across diverse datasets, particularly non-image data
  - What evidence would resolve it: Conducting experiments on various non-image datasets and comparing the performance of multi-token discretization with the VAE model would provide clarity

- **What specific reasons cause the VAE model to outperform the multi-token discretization approach in certain scenarios?**
  - Basis: The authors note that the VAE model's effectiveness surpasses that of multi-token discretization, hinting at potential optimization difficulties
  - Why unresolved: The paper does not delve into the underlying reasons for the VAE model's superior performance, leaving this aspect unexplored
  - What evidence would resolve it: A detailed analysis of the optimization processes and model architectures could reveal the factors contributing to the VAE model's performance advantage

- **How does the size of the latent codebook space affect the efficiency of discrete communication as the number of agents increases?**
  - Basis: The authors suggest that increasing the size of the latent codebook space improves discrete communication efficiency among agents
  - Why unresolved: The paper does not explore the scalability of this approach with a larger number of agents or varying dataset complexities
  - What evidence would resolve it: Experiments with varying numbers of agents and dataset sizes could determine the scalability and limitations of the codebook size on communication efficiency

## Limitations

- The paper lacks systematic comparison against more sophisticated continuous communication models beyond basic autoencoders
- Evaluation is limited to image datasets only, leaving unclear whether findings generalize to other data modalities
- The mechanism explanations lack formal theoretical grounding that would strengthen claims about why discrete communication works better for diverse experiences

## Confidence

**High Confidence**: The experimental results showing that multi-token discrete communication outperforms single-token discrete and continuous communication are well-supported by the data presented.

**Medium Confidence**: The hypothesis that discrete messages are superior when agents have diverse experiences is supported by experiments, but lack of comparison to more sophisticated continuous models limits confidence.

**Low Confidence**: The claim about cross-training improving language alignment lacks sufficient empirical validation; evidence provided is indirect.

## Next Checks

1. **Model Architecture Comparison**: Implement and compare against VAE-based communication models with learned continuous priors to determine whether observed advantages are specifically due to discretization or could be achieved with continuous models using appropriate regularization.

2. **Cross-Modal Validation**: Replicate the experiments on non-image datasets (text, audio, or tabular data) to verify whether the discrete communication advantage generalizes across different data modalities and communication tasks.

3. **Hyperparameter Sensitivity Analysis**: Conduct systematic ablation studies varying codebook size, token dimensions, overlap ratios between agent datasets, and cross-training frequency to identify the optimal configuration and understand which factors most strongly influence communication efficiency.