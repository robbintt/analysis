---
ver: rpa2
title: Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented
  Visual Question Answering
arxiv_id: '2309.17133'
source_url: https://arxiv.org/abs/2309.17133
tags:
- retrieval
- knowledge
- flmr
- vision
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes Fine-grained Late-interaction Multi-modal Retrieval
  (FLMR) to improve knowledge retrieval in Retrieval-Augmented Visual Question Answering
  (RA-VQA). FLMR addresses two limitations in RA-VQA: incomplete image understanding
  from image-to-text transforms and lossy compression of visual scenes into one-dimensional
  embeddings.'
---

# Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering

## Quick Facts
- **arXiv ID**: 2309.17133
- **Source URL**: https://arxiv.org/abs/2309.17133
- **Reference count**: 29
- **Primary result**: FLMR improves knowledge retrieval PRRecall@5 by approximately 8% and achieves 61% VQA score on OK-VQA dataset

## Executive Summary
This paper introduces Fine-grained Late-interaction Multi-modal Retrieval (FLMR) to address limitations in knowledge retrieval for Retrieval-Augmented Visual Question Answering (RA-VQA). The method tackles two key issues: incomplete image understanding from image-to-text transforms and lossy compression of visual scenes into one-dimensional embeddings. FLMR employs token-level visual and textual features in multi-dimensional embeddings with late interaction formulas to compute relevance scores, enabling finer-grained assessment of relevance. The approach also incorporates large vision models aligned with text-based retrievers to provide more complete image understanding, resulting in significant improvements over baseline methods.

## Method Summary
FLMR improves RA-VQA by encoding images and questions using multi-dimensional embeddings that capture finer-grained relevance between queries and documents. The method employs a late interaction formula similar to ColBERT but in a multi-modal context, computing relevance scores by aggregating maximum similarity between every pair of query and document token embeddings. To address incomplete image understanding, FLMR uses a vision model aligned with existing text-based retrievers through a simple alignment network that projects visual features into the same latent space as the language model. The approach combines text-based vision representations (objects, attributes, captions) with feature-based vision representations (global and regional features) to provide more complete image understanding.

## Key Results
- Improves knowledge retrieval PRRecall@5 by approximately 8% compared to baseline methods
- Achieves VQA score of 61% on OK-VQA dataset, surpassing systems with similar parameter sizes
- Incorporates text-based vision and feature-based vision, increasing PRRecall@5 to 85.99 and 85.08 respectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Late interaction captures finer-grained relevance between query and document tokens, leading to better retrieval performance.
- **Mechanism**: Computes relevance score by aggregating maximum similarity between every pair of query and document token embeddings, contrasting with DPR's compressed vector approach.
- **Core assumption**: Token-level interactions preserve more information than compressed vector representations, especially for complex multi-modal queries.
- **Evidence anchors**: Abstract states FLMR "encodes images and questions using multi-dimensional embeddings to capture finer-grained relevance"; section 3.1 describes the late interaction formula similar to ColBERT; corpus shows average neighbor FMR=0.395.
- **Break condition**: If token-level embeddings are noisy or documents are extremely long, max similarity computation could become computationally expensive or less meaningful.

### Mechanism 2
- **Claim**: Vision-language alignment through a simple mapping network improves retrieval by providing more complete image understanding.
- **Mechanism**: Mapping network FM projects visual features from vision model FV into the same latent space as language model FL, enabling cross-modality token-level interactions.
- **Core assumption**: Visual features from pre-trained vision model can be effectively aligned with text-based features through lightweight mapping network without expensive multi-modal pre-training.
- **Evidence anchors**: Abstract mentions "obtaining image representations that complement those from image-to-text transforms using a vision model aligned with an existing text-based retriever"; section 3.1 details the mapping network training; corpus indicates extensibility through "PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers."
- **Break condition**: If vision and text models have too different latent representations, simple mapping network may not provide sufficient alignment.

### Mechanism 3
- **Claim**: Combining text-based and feature-based vision representations provides more complete image understanding than either modality alone.
- **Mechanism**: Extracts both textual descriptions (captions, objects, attributes) and visual features (global and regional) from images, concatenated and processed through late interaction mechanism.
- **Core assumption**: Text-based vision captures semantic information while feature-based vision captures visual details; together they provide more complete image representation.
- **Evidence anchors**: Abstract discusses complementing image-to-text transforms with vision model representations; section 3.1 describes extraction of objects, attributes using VinVL, and captions using Oscar; section 5.2 shows PRRecall@5 improvements to 85.99 and 85.08 with different vision types.
- **Break condition**: If vision model produces noisy features or text-based vision is incomplete, combination might introduce more noise than signal.

## Foundational Learning

- **Concept: Dense Passage Retrieval (DPR)**
  - Why needed here: DPR is the baseline retrieval method being improved upon
  - Quick check question: How does DPR compute relevance scores between queries and documents?

- **Concept: Vision Transformers (ViT)**
  - Why needed here: Vision model FV uses ViT architecture to extract visual features
  - Quick check question: What type of visual features does ViT extract, and how are they different from traditional CNN features?

- **Concept: Late Interaction Models (ColBERT)**
  - Why needed here: FLMR extends ColBERT late interaction paradigm to multi-modal context
  - Quick check question: What is the key difference between late interaction and early fusion in multi-modal models?

## Architecture Onboarding

- **Component map**: Image → Vision Model → Mapping Network → Token Embeddings → Late Interaction → Relevance Score → Retrieved Documents

- **Critical path**: Vision model extracts global and regional image features, mapping network aligns vision features to language model space, late interaction engine computes token-level relevance scores, PLAID index enables fast retrieval

- **Design tradeoffs**: FLMR trades increased computational complexity (token-level interactions) for improved retrieval performance; vision-language alignment adds small overhead but significantly improves results

- **Failure signatures**: 
  - Poor retrieval performance: Check if mapping network is properly aligned or if token embeddings are too noisy
  - High computational cost: Verify if number of tokens is optimized or if PLAID indexing is working correctly
  - Inconsistent results across runs: Check random seeds and initialization

- **First 3 experiments**:
  1. Replace FLMR with DPR baseline and compare PRRecall@5 to establish baseline improvement
  2. Remove mapping network to test importance of vision-language alignment
  3. Remove either text-based or feature-based vision to test complementarity of two modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FLMR's performance scale with the size and diversity of the knowledge corpus?
- Basis in paper: [inferred] Paper tests FLMR on two knowledge corpora but does not explore performance across varying sizes or domains
- Why unresolved: Only uses Google Search corpus and Wikipedia corpus for OK-VQA, limiting insights into FLMR's scalability and adaptability to different knowledge domains
- What evidence would resolve it: Experiments comparing FLMR's retrieval and VQA performance across corpora of different sizes, domains, and levels of domain-specific knowledge

### Open Question 2
- Question: What is the impact of different vision models on FLMR's retrieval and VQA performance?
- Basis in paper: [explicit] Paper uses CLIP ViT-base for vision encoding but does not explore other vision models or architectures
- Why unresolved: Only uses one vision model (CLIP ViT-base), limiting understanding of how different vision models affect FLMR's performance
- What evidence would resolve it: Experiments comparing FLMR's performance using different vision models (e.g., CLIP ViT-large, Swin Transformer) and analyzing impact on retrieval and VQA accuracy

### Open Question 3
- Question: How does FLMR handle ambiguous or multi-modal queries that require understanding both visual and textual context?
- Basis in paper: [inferred] Paper focuses on knowledge retrieval but does not extensively explore FLMR's ability to handle complex, multi-modal queries
- Why unresolved: Paper does not provide detailed analysis of FLMR's performance on queries requiring both visual and textual understanding, such as those involving object attributes, spatial relationships, or contextual reasoning
- What evidence would resolve it: Experiments evaluating FLMR's performance on a dataset of multi-modal queries requiring both visual and textual understanding, and analyzing its ability to correctly retrieve relevant knowledge

## Limitations
- Generalization to other datasets beyond OK-VQA remains uncertain
- Computational complexity increases due to token-level interactions
- Performance may vary depending on quality and coverage of knowledge corpus

## Confidence
- **High Confidence**: 8% improvement in PRRecall@5 is well-supported by experimental results
- **Medium Confidence**: 61% VQA score is supported but comparisons are limited
- **Low Confidence**: Simple mapping network sufficiency for alignment lacks detailed analysis

## Next Checks
1. Evaluate FLMR on additional VQA datasets beyond OK-VQA (VQA-CP, VizWiz, GQA) to assess generalization
2. Replace simple mapping network with more sophisticated vision-language alignment approaches to quantify optimality
3. Test FLMR with different knowledge corpora of varying quality and coverage to understand sensitivity to knowledge source