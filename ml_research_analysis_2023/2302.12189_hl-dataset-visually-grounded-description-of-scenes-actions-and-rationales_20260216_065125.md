---
ver: rpa2
title: 'HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales'
arxiv_id: '2302.12189'
source_url: https://arxiv.org/abs/2302.12189
tags:
- captions
- high-level
- scene
- dataset
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the High-Level Dataset (HL), a new resource
  for Vision & Language (V&L) modeling that extends the COCO dataset with 134,973
  human-annotated high-level captions along three axes: scenes, actions, and rationales.
  Unlike object-centric captions, HL captions capture human interpretations of scenes,
  enabling research on abstract concepts and their grounding in vision.'
---

# HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales

## Quick Facts
- arXiv ID: 2302.12189
- Source URL: https://arxiv.org/abs/2302.12189
- Reference count: 17
- Introduces High-Level Dataset (HL) with 134,973 human-annotated high-level captions extending COCO dataset

## Executive Summary
The High-Level Dataset (HL) introduces a new resource for Vision & Language modeling by extending COCO images with human-annotated captions along three abstract dimensions: scenes, actions, and rationales. Unlike traditional object-centric captions, HL captures human interpretations and world knowledge about images, enabling research on abstract concept grounding in vision. The dataset includes confidence scores from independent annotators and narrative captions generated by combining the three axes. Analysis reveals that high-level concepts are connected to low-level objects through PMI analysis, and the dataset enables new image-to-text generation tasks and multimodal grounding analyses.

## Method Summary
The HL dataset was constructed by collecting 134,973 human-annotated high-level captions for 14,997 COCO images containing people. Workers on Amazon Mechanical Turk provided captions for three axes: scenes, actions, and rationales. Each image received one caption per axis from workers who passed a pilot study with inter-annotator agreement threshold of 0.5. Independent annotators then rated the confidence of each caption on a 1-5 scale. The dataset was analyzed using PMI to connect high-level concepts with low-level objects, and evaluated for semantic and lexical diversity using BLEURT and BLEU scores.

## Key Results
- High-level captions are shorter but lexically diverse compared to object-centric captions
- PMI analysis reveals connections between high-level concepts and supporting low-level objects
- Confidence scores effectively distinguish shared commonsense interpretations from idiosyncratic ones
- The dataset enables new image-to-text generation tasks and multimodal grounding analyses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-level captions enable grounding of abstract concepts that object-centric captions cannot express
- Mechanism: The dataset decouples the linguistic representation of a scene from its purely visual description by collecting scene, action, and rationale captions that draw on world knowledge and subjective experience
- Core assumption: Human interpretation of images involves abstract concepts not directly observable in the visual input
- Evidence anchors: Abstract states HL captions capture human interpretations enabling research on abstract concepts and their grounding in vision

### Mechanism 2
- Claim: Confidence scores distinguish shared commonsense interpretations from idiosyncratic ones
- Mechanism: Independent annotators rate the likelihood of high-level captions given the image, providing a measure of how widely a particular interpretation is shared
- Core assumption: Different human interpretations of the same scene vary in how much they rely on shared vs. personal knowledge
- Evidence anchors: Abstract mentions dataset includes confidence scores from independent annotators

### Mechanism 3
- Claim: PMI analysis reveals connections between high-level concepts and supporting low-level objects
- Mechanism: Pointwise Mutual Information quantifies how informative specific objects are for particular high-level concepts, revealing the visual basis for abstract interpretations
- Core assumption: High-level concepts can be grounded in vision through their association with specific objects in the scene
- Evidence anchors: Abstract states analysis shows high-level concepts are connected to low-level objects through PMI analysis

## Foundational Learning

- Concept: Multimodal grounding of language in vision
  - Why needed here: The dataset is fundamentally about connecting linguistic concepts to visual content, requiring understanding of how language and vision interact
  - Quick check question: What is the difference between object-centric and high-level captions in terms of what they capture about an image?

- Concept: Crowdsourcing data collection protocols
  - Why needed here: The dataset was collected through Amazon Mechanical Turk with specific quality control mechanisms
  - Quick check question: How does the agreement-based worker selection ensure quality in the confidence score collection?

- Concept: PMI and information theory for concept relationships
  - Why needed here: PMI analysis is used to quantify the relationship between high-level concepts and supporting objects
  - Quick check question: What does a high PMI score between a concept and an object indicate about their relationship?

## Architecture Onboarding

- Component map: Image selection → Caption collection (scene/action/rationale) → Confidence score collection → PMI analysis
- Critical path: Image → Three high-level captions → Confidence scores → Analysis → Dataset release
- Design tradeoffs: Object-centric vs. high-level captions (coverage vs. abstraction), single vs. multiple annotations per axis (cost vs. reliability), automatic vs. human confidence scoring (scalability vs. quality)
- Failure signatures: Low inter-annotator agreement indicating unclear instructions, high grammatical error rates suggesting poor annotation quality, weak correlation between confidence scores and diversity measures
- First 3 experiments:
  1. Train a model to generate high-level captions from images and evaluate using confidence score distribution
  2. Analyze PMI relationships for a specific concept (e.g., "enjoy") and verify against human intuition
  3. Test model generalization from object-centric to high-level captions using the aligned data structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of high-level captions (scenes, actions, rationales) improve downstream task performance compared to object-centric captions alone?
- Basis in paper: Explicit - The paper mentions that the dataset enables new or extended downstream tasks and that high-level captions can be used to generate more natural captions and explanations
- Why unresolved: The paper presents baseline results for high-level captioning but does not provide comprehensive experiments on downstream tasks using the full dataset
- What evidence would resolve it: Empirical results showing improved performance on tasks like visual storytelling, image paragraph generation, and visual commonsense reasoning when using high-level captions in combination with or instead of object-centric captions

### Open Question 2
- Question: To what extent do high-level captions rely on world knowledge and personal experience versus direct visual information?
- Basis in paper: Explicit - The paper states that high-level captions capture human interpretation based on personal experience and world knowledge, and that confidence scores were collected to measure the degree of shared/common sense assumptions
- Why unresolved: While confidence scores provide some measure, they don't fully quantify the balance between world knowledge and visual information in generating high-level captions
- What evidence would resolve it: Controlled experiments manipulating the availability of world knowledge (e.g., limiting access to certain types of information) and measuring the resulting changes in high-level caption generation

### Open Question 3
- Question: How do different levels of abstraction in captions (high-level vs. object-centric) affect the learning and generalization capabilities of vision-language models?
- Basis in paper: Inferred - The paper discusses the relationship between low-level (object-centric) and high-level concepts and mentions that exposure to high-level concepts affects models' attentional resource allocation
- Why unresolved: The paper does not provide detailed experiments on how training with different types of captions affects model performance and generalization
- What evidence would resolve it: Comparative studies training vision-language models on datasets with varying proportions of high-level and object-centric captions, measuring performance on tasks requiring different levels of abstraction

## Limitations

- The inherent subjectivity in high-level captioning introduces significant variance that cannot be fully mitigated by confidence scores
- The three-axis structure (scenes, actions, rationales) may not capture all dimensions of human visual interpretation
- Reliance on COCO images limits diversity and inherits biases toward Western contexts and common objects

## Confidence

- High confidence: Dataset construction methodology and basic analysis results are well-supported and reproducible
- Medium confidence: Claim that HL captions enable meaningful grounding of abstract concepts in vision is supported but needs more extensive model training results
- Medium confidence: Effectiveness of confidence scores in distinguishing interpretations is theoretically sound but needs more validation

## Next Checks

1. Train and evaluate a model on the HL dataset to generate high-level captions, then measure how well the generated captions' confidence score distribution matches human-annotated distributions, validating whether the dataset enables learning of abstract concept grounding.

2. Conduct a targeted analysis of specific high-level concepts (e.g., "relaxation" or "enjoyment") by manually verifying whether the top PMI-ranked objects truly represent the visual basis for these concepts, checking if the automated analysis aligns with human intuition.

3. Perform cross-dataset validation by testing whether models trained on HL captions can successfully transfer to other high-level captioning datasets or tasks, assessing the generalizability of the abstract concept representations learned from this dataset.