---
ver: rpa2
title: 'MIM4DD: Mutual Information Maximization for Dataset Distillation'
arxiv_id: '2312.16627'
source_url: https://arxiv.org/abs/2312.16627
tags:
- real
- dataset
- information
- synthetic
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIM4DD, a method that integrates mutual information
  (MI) maximization into dataset distillation (DD). While previous DD methods focus
  on heuristic matching of network indicators, MIM4DD treats DD as a compression problem
  requiring preservation of shared information between synthetic and real datasets.
---

# MIM4DD: Mutual Information Maximization for Dataset Distillation

## Quick Facts
- **arXiv ID**: 2312.16627
- **Source URL**: https://arxiv.org/abs/2312.16627
- **Reference count**: 40
- **Primary result**: MIM4DD achieves 41.5% accuracy on CIFAR100 with 10 images per class, outperforming previous state-of-the-art by 1.4%

## Executive Summary
This paper introduces MIM4DD, a method that integrates mutual information (MI) maximization into dataset distillation (DD). While previous DD methods focus on heuristic matching of network indicators, MIM4DD treats DD as a compression problem requiring preservation of shared information between synthetic and real datasets. The authors formulate DD as an MI maximization problem and solve it via a contrastive learning framework. Experiments show MIM4DD consistently outperforms state-of-the-art DD methods on MNIST, CIFAR10, and CIFAR100 datasets, with particular improvements on CIFAR100 where it achieves 41.5% accuracy compared to 40.1% for the previous best method.

## Method Summary
MIM4DD integrates mutual information maximization into dataset distillation by treating the task as a compression problem. The method defines positive pairs as samples with identical labels across real and synthetic datasets, and negative pairs as those with different labels. Using a contrastive learning framework with NCE loss, MIM4DD pulls positive pairs closer and pushes negative pairs apart in a contrastive space. This transforms the MI maximization goal into a numerically feasible lower bound. The method can be implemented as an add-on module to existing DD techniques like MTT and BPTT, combining the MI loss with their primary objectives through a weighted sum.

## Key Results
- Achieves 41.5% accuracy on CIFAR100 with 10 images per class, outperforming previous best by 1.4%
- Consistently outperforms state-of-the-art DD methods across MNIST, CIFAR10, and CIFAR100
- Demonstrates effectiveness as an add-on module, improving performance of existing DD methods like MTT

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MIM4DD improves dataset distillation by maximizing mutual information between synthetic and real datasets.
- **Mechanism**: Transforms DD task into MI maximization solved via contrastive learning framework using NCE loss, pulling same-label samples together while pushing different-label samples apart.
- **Core assumption**: Maximizing MI between feature maps of real and synthetic data is equivalent to maximizing MI between datasets themselves.
- **Evidence anchors**: [abstract] introduces MI as metric; [section] states MI invariance theorem; [corpus] weak - mentions related DD papers
- **Break condition**: If network layers don't preserve MI invariance (e.g., non-smooth transformations), theoretical foundation fails.

### Mechanism 2
- **Claim**: Contrastive learning framework effectively enhances heterogeneity in synthetic samples.
- **Mechanism**: Pulling same-label samples closer and different-label samples apart creates more diverse and class-distinguishable synthetic images.
- **Core assumption**: Contrastive objective lower bounds MI and captures statistical dependencies.
- **Evidence anchors**: [abstract] describes pulling/pushing in contrastive space; [section] identifies negative/positive pair definition as obstacle; [corpus] weak - mentions contrastive learning
- **Break condition**: If insufficient negative pairs or embedding function fails to separate classes, contrastive objective breaks down.

### Mechanism 3
- **Claim**: MIM4DD serves as effective add-on module to existing DD methods.
- **Mechanism**: MI maximization loss combines with existing DD losses (MTT/BPTT) to improve performance without replacing core optimization.
- **Core assumption**: MI loss acts as regularization term that enhances rather than conflicts with existing DD objectives.
- **Evidence anchors**: [abstract] states can be implemented as add-on module; [section] emphasizes add-on capability; [corpus] weak - lacks direct add-on evidence
- **Break condition**: If MI loss overwhelms primary DD loss (λ too high), optimization may diverge or prioritize MI over distillation accuracy.

## Foundational Learning

- **Concept: Mutual Information (MI)**
  - Why needed here: MI provides theoretically grounded metric for measuring information preservation between synthetic and real datasets, unlike heuristic distance metrics used in previous DD methods.
  - Quick check question: What is the key difference between mutual information and traditional correlation metrics in the context of dataset distillation?

- **Concept: Contrastive Learning**
  - Why needed here: Contrastive learning provides tractable way to maximize MI by pulling positive pairs together and pushing negative pairs apart, numerically feasible compared to direct MI computation.
  - Quick check question: How does NCE loss in contrastive learning relate to lower bound of mutual information?

- **Concept: Dataset Distillation**
  - Why needed here: Understanding DD fundamentals (synthetic dataset generation, training performance comparison) is essential to appreciate why MI maximization improves the task.
  - Quick check question: What is primary goal of dataset distillation and how does it differ from dataset compression?

## Architecture Onboarding

- **Component map**: Real dataset pipeline → Feature extraction network → MI maximization module → Synthetic dataset update; Synthetic dataset pipeline → Feature extraction network → Contrastive loss computation → Synthetic dataset update; MI maximization module: Embedding function, temperature parameter τ, NCE loss computation; Integration point: Combines with existing DD losses (MTT/BPTT) via weighted sum

- **Critical path**: 1) Extract features from both real and synthetic datasets using same network architecture; 2) Compute positive and negative pairs based on label matching; 3) Apply embedding function and NCE loss to compute contrastive objective; 4) Update synthetic dataset to maximize contrastive loss (minimize NCE); 5) Combine with primary DD loss and optimize end-to-end

- **Design tradeoffs**: λ (MI loss weight) vs. primary DD loss: Higher λ emphasizes information preservation but may slow convergence; Embedding function complexity: More complex embeddings capture better representations but increase computation; Batch size: Larger batches provide more negative pairs but increase memory usage; Temperature τ: Controls similarity score concentration; too high reduces gradient signal, too low may cause collapse

- **Failure signatures**: Poor synthetic dataset quality: Synthetic images lack diversity or class separation; Training instability: Oscillating loss or exploding gradients; Convergence to trivial solution: Synthetic dataset collapses to few prototypes; Performance degradation: Test accuracy worse than baseline DD methods

- **First 3 experiments**: 1) Ablation study on λ: Test MIM4DD with λ ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} to find optimal balance between MI maximization and primary DD objective; 2) Feature visualization: Use t-SNE or UMAP to visualize feature embeddings from real vs. synthetic data to qualitatively assess class separation improvement; 3) Add-on validation: Apply MIM4DD to MTT baseline and measure performance improvement on CIFAR10 with 10 IPC setting

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Theoretical foundation relies on MI invariance under reparameterization, which may not hold for all network architectures
- Computational complexity of NCE loss across all real-synthetic sample pairs may limit scalability to larger datasets
- Lacks extensive ablation studies on temperature parameter τ and embedding function architecture choices

## Confidence
- **High Confidence**: Experimental results showing consistent improvement over baselines across multiple datasets with statistical significance
- **Medium Confidence**: Theoretical claim that MI maximization through contrastive learning is equivalent to maximizing shared information between datasets
- **Low Confidence**: Claim that MIM4DD serves as effective add-on module to existing DD methods, as only one baseline is mentioned without detailed results

## Next Checks
1. **Ablation Study on Temperature Parameter**: Systematically vary temperature parameter τ in NCE loss across {0.07, 0.1, 0.2, 0.5} and measure impact on synthetic dataset quality and training stability
2. **Scalability Analysis**: Test MIM4DD on larger-scale datasets (e.g., ImageNet-10/100) to evaluate computational efficiency and performance degradation when scaling from CIFAR100
3. **Embedding Function Analysis**: Replace current embedding function g_φ(·) with alternative architectures (e.g., MLP with different widths, residual connections) and compare resulting synthetic dataset quality to assess sensitivity to this component