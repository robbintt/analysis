---
ver: rpa2
title: Goal-Oriented Prompt Attack and Safety Evaluation for LLMs
arxiv_id: '2309.11830'
source_url: https://arxiv.org/abs/2309.11830
tags:
- llms
- attack
- prompt
- attacking
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CPAD, a Chinese prompt attack dataset for
  evaluating the safety of large language models (LLMs). The dataset includes 10,050
  high-quality prompt attack samples designed to induce LLMs to generate harmful content.
---

# Goal-Oriented Prompt Attack and Safety Evaluation for LLMs

## Quick Facts
- arXiv ID: 2309.11830
- Source URL: https://arxiv.org/abs/2309.11830
- Reference count: 7
- Primary result: CPAD dataset achieves 70% attack success rate against GPT-3.5

## Executive Summary
This paper introduces CPAD, a Chinese prompt attack dataset designed to evaluate the safety of large language models by inducing them to generate harmful content. The dataset contains 10,050 high-quality prompt attack samples constructed across three dimensions: content, attacking methods, and goals. The authors employ a multi-model voting filter using three Chinese LLMs to ensure attack robustness, then evaluate success using a goal-based assessment framework with GPT-3.5-turbo. The dataset achieves a 70% attack success rate against GPT-3.5 and is publicly available to encourage further research on prompt attacks and defenses.

## Method Summary
The CPAD dataset is constructed through a pipeline involving manual seed content creation, automated generation via ChatGPT using various templates, and filtering through three Chinese LLMs (ChatGLM-2-6B, Baichuan-7B, Chinese-LLaMa-2-7B). Only prompts that successfully attack all three models are retained. The final dataset is evaluated using an evaluation LLM (gpt-3.5-turbo-0301) that checks whether generated responses fulfill the attacker's intended goals. The dataset covers multiple attacking contents (fraud, pornography, violence, politics) and attacking goals (Goal Hijacking, Description of unsafe scenarios, etc.).

## Key Results
- CPAD dataset contains 10,050 high-quality Chinese prompt attack samples
- Achieved 70% attack success rate against GPT-3.5
- Successfully evaluated across three different Chinese LLMs
- Dataset publicly available for research purposes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attack prompts succeed by embedding malicious goals within contextually plausible conversational templates that bypass alignment filters.
- **Mechanism**: Templates like "Wrap with shell," "Conversation completion," and "Assign roles and then issue commands" disguise harmful instructions inside benign dialogue structures, lowering guardrails before harmful content appears.
- **Core assumption**: LLMs align based on explicit keyword matching and surface-level safety patterns; structured templates can evade detection if harmful content is deferred or embedded in indirect roles.
- **Evidence anchors**: [abstract]: "We construct the prompts considering three dimensions: contents, attacking methods and goals." [section]: "Different from previous studies, our dataset contains the attacking targets and aims to investigate the defense capability of various Large Language Models against carefully designed prompt attacks from the perspective of attackers."
- **Break condition**: If alignment training incorporates deeper semantic understanding of role-play and conversation completion contexts, template-based evasion will degrade.

### Mechanism 2
- **Claim**: Explicit attacking goals allow evaluation models to detect whether the LLM's response actually fulfills the attacker's intent, rather than merely containing harmful keywords.
- **Mechanism**: After generating a response, a separate evaluation LLM is prompted with both the original content and the goal, outputting "Yes" if the response delivers the intended harmful behavior.
- **Core assumption**: LLMs can be instructed to evaluate semantic intent reliably when given both the prompt and a goal label.
- **Evidence anchors**: [section]: "We introduce the attacking goals which can accurately reflect the behavior the attacker wants the model to exhibit. This allows us to assess whether the responses of the LLMs align with the intentions of the attacker." [abstract]: "Different from previous datasets involving safety estimation, we construct the prompts considering three dimensions: contents, attacking methods and goals."
- **Break condition**: If the evaluation model itself is vulnerable to prompt injection or biased, goal-based success detection will misclassify attacks.

### Mechanism 3
- **Claim**: High-quality prompts are retained through a voting filter across three independently aligned Chinese LLMs; only prompts that successfully attack all three are kept.
- **Mechanism**: Each extended prompt is tested against ChatGLM-6B, Ziya-13B, and ChatYuan-Large-v2. Prompts failing any one model are discarded, ensuring only universally effective prompts survive.
- **Core assumption**: Alignment strategies differ across models enough that a prompt evading one will likely be caught by another; consensus filtering ensures only universally effective prompts survive.
- **Evidence anchors**: [section]: "We employ three different Chinese LLMs... We keep only the prompts which successfully attacked all of the models." [table 1]: Quantitative success rates for each model (e.g., ChatGLM: 39.87%, ChatYuan: 46.51%, Ziya: 39.28%) support the filtering process.
- **Break condition**: If future models converge on similar alignment techniques, consensus filtering may no longer provide sufficient diversity to isolate truly robust attacks.

## Foundational Learning

- **Concept**: Prompt injection and jailbreak attacks
  - **Why needed here**: The paper's entire contribution relies on constructing and evaluating adversarial prompts; understanding the attack surface is prerequisite to building defenses.
  - **Quick check question**: What is the difference between a jailbreak attack and a prompt injection attack?

- **Concept**: Reinforcement Learning from Human Feedback (RLHF) and safety alignment
  - **Why needed here**: The dataset's effectiveness depends on exploiting gaps in RLHF-based safety tuning; knowing how RLHF works clarifies why certain prompts succeed.
  - **Quick check question**: How does RLHF typically reduce harmful content generation in LLMs?

- **Concept**: Multi-modal evaluation and goal-based safety scoring
  - **Why needed here**: The paper introduces a novel evaluation scheme where goals are explicitly checked; understanding this requires familiarity with structured safety benchmarks.
  - **Quick check question**: Why is evaluating based on attacker-intended goals more informative than binary harm/no-harm classification?

## Architecture Onboarding

- **Component map**: Seed content library → ChatGPT extension → Template formatting → Voting filter (3 Chinese LLMs) → Goal-based evaluation (GPT-3.5-turbo) → Final dataset
- **Critical path**: 
  1. Prompt generation (seed → extended)
  2. Multi-model attack testing
  3. Voting filter to retain only universally successful prompts
  4. Goal-based response evaluation
- **Design tradeoffs**:
  - Higher attack success vs. ethical risk of releasing effective attack prompts
  - Consensus filtering reduces dataset size but increases quality and generalizability
  - Using ChatGPT for generation may embed subtle model-specific patterns
- **Failure signatures**:
  - Low voting consensus across the three Chinese LLMs → prompt is model-specific or weak
  - Evaluation LLM misclassifies goal fulfillment → evaluation pipeline error
  - Generated responses are too short or off-topic → filtering threshold too lenient
- **First 3 experiments**:
  1. Run a subset of CPAD prompts against a fresh, unseen LLM to validate generalization.
  2. Test goal-based evaluation pipeline in isolation with synthetic inputs to verify classification accuracy.
  3. Perform ablation: remove the voting filter and measure impact on attack success rate and dataset size.

## Open Questions the Paper Calls Out

- **Question**: How can the effectiveness of prompt attacks be further improved against larger language models like GPT-4?
  - **Basis in paper**: [explicit] The paper mentions that larger models like ChatGPT have a lower probability of being attacked but are also more harmful when they are attacked.
  - **Why unresolved**: The paper does not explore specific strategies to enhance the effectiveness of prompt attacks against larger models.
  - **What evidence would resolve it**: Developing and testing new prompt attack techniques specifically designed for larger models, and measuring their success rates.

- **Question**: What are the long-term implications of using datasets like CPAD for fine-tuning language models to defend against prompt attacks?
  - **Basis in paper**: [explicit] The paper demonstrates that fine-tuning with CPAD significantly reduces the success rate of prompt attacks.
  - **Why unresolved**: The paper does not address the potential downsides or limitations of using such datasets for fine-tuning, such as model performance on benign tasks.
  - **What evidence would resolve it**: Conducting comprehensive evaluations of fine-tuned models on both safety and general performance metrics.

- **Question**: How can the balance between model safety and utility be optimized when defending against prompt attacks?
  - **Basis in paper**: [explicit] The paper discusses the trade-off between model safety and utility, as overly restrictive defenses may limit the model's usefulness.
  - **Why unresolved**: The paper does not provide a detailed framework for optimizing this balance.
  - **What evidence would resolve it**: Developing and validating metrics to quantify the trade-off, and proposing strategies to achieve an optimal balance.

## Limitations

- The effectiveness of CPAD's attack prompts may be overestimated due to reliance on ChatGPT for both generation and evaluation, potentially introducing model-specific biases.
- The voting filter mechanism may have removed edge-case attacks that could succeed against models with different alignment strategies.
- The dataset's focus on Chinese language may limit its applicability to multilingual or cross-lingual safety evaluations.

## Confidence

- **High Confidence**: The dataset construction methodology (multi-model voting filter, goal-based evaluation) is technically sound and well-documented. The reported 70% attack success rate against GPT-3.5 is a specific, testable claim.
- **Medium Confidence**: The claim that CPAD provides more accurate safety evaluation than previous datasets is plausible but requires independent validation across diverse model architectures and languages.
- **Low Confidence**: The assumption that template-based attacks will remain effective against future LLMs with evolved alignment techniques is speculative and may not hold as safety training advances.

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate CPAD prompts against a diverse set of 5-10 LLMs (including non-Chinese models) that were not used in the original dataset creation to verify the claimed universality of the attacks.

2. **Temporal Robustness Analysis**: Re-test the dataset against the same models 6-12 months after initial publication to measure how quickly alignment improvements degrade attack success rates.

3. **Human Evaluation Benchmark**: Conduct blinded human assessments of CPAD attack success to establish ground truth and compare against the automated evaluation pipeline's accuracy in detecting goal fulfillment.