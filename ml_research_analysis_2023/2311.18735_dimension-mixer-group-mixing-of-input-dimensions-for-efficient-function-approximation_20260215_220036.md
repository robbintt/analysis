---
ver: rpa2
title: 'Dimension Mixer: Group Mixing of Input Dimensions for Efficient Function Approximation'
arxiv_id: '2311.18735'
source_url: https://arxiv.org/abs/2311.18735
tags:
- butterfly
- mixing
- attention
- block
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a generalized signal processing framework
  called the Dimension Mixer model, which unifies various neural architectures like
  CNNs, Transformers, and MLP-Mixers under the concept of dimension mixing. The core
  contribution is extending the butterfly sparsity structure beyond linear transforms
  to non-linear functions, allowing efficient and structured mixing of input dimensions.
---

# Dimension Mixer: Group Mixing of Input Dimensions for Efficient Function Approximation

## Quick Facts
- arXiv ID: 2311.18735
- Source URL: https://arxiv.org/abs/2311.18735
- Reference count: 40
- Key outcome: Butterfly Attention achieves sub-quadratic complexity for long sequences while maintaining competitive accuracy on CIFAR-10/100 and LRA benchmarks

## Executive Summary
This paper introduces the Dimension Mixer model, a generalized signal processing framework that unifies neural architectures like CNNs, Transformers, and MLP-Mixers under the concept of dimension mixing. The core contribution extends butterfly sparsity structures from linear transforms to non-linear functions, enabling efficient block-wise mixing of input dimensions. The authors propose Butterfly MLP and Butterfly Attention, which apply butterfly sparsity to MLP and attention mechanisms respectively, achieving significant computational savings while maintaining or improving accuracy on standard benchmarks.

## Method Summary
The Dimension Mixer model generalizes butterfly structures from linear to non-linear mixing functions, creating a framework where input dimensions are grouped and processed through layered parallel select and mix stages. Butterfly MLP applies this structure to MLP mechanisms, while Butterfly Attention implements block-wise attention with butterfly permutations to reduce complexity from O(S²) to O(S). The paper also introduces Patch-Only MLP-Mixer, which uses patch-wise mixing at multiple scales without common factors, providing a middle ground between MLP-Mixer and CNN for 2D image processing.

## Key Results
- Butterfly Attention achieves competitive accuracy on LRA benchmarks while reducing self-attention complexity from O(S²) to O(S)
- Butterfly MLP and Attention can sparsify dense layers while maintaining or improving accuracy, with significant savings in parameters and computations
- Patch-Only MLP-Mixer produces comparable or better results than original MLP-Mixer on CIFAR-10 and CIFAR-100 datasets
- Butterfly Attention successfully solves the challenging Pathfinder-X task with 16K tokens, demonstrating effectiveness for long sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimension mixing through sparse, structured groupings can approximate dense matrix operations while reducing computational complexity.
- Mechanism: The Dimension Mixer model generalizes butterfly structures from linear to non-linear mixing functions, enabling efficient block-wise mixing of input dimensions. This creates a path from each input dimension to each output dimension through layered parallel select and mix stages.
- Core assumption: Partial, hierarchical signal mixing schemes are sufficient for expressive function approximation and can maintain accuracy while reducing computation.
- Evidence anchors:
  - [abstract] "Research on coupling flows and the butterfly transform shows that partial and hierarchical signal mixing schemes are sufficient for efficient and expressive function approximation."
  - [section II] "Coupling Flows [8], [45], [46], [47] and Reversible ResNet [9], use split and process mechanism that enables invertibility. [48] shows partial mixing of signals can approximate any diffeomorphic function."
  - [corpus] Weak evidence - neighbor papers focus on MLP-Mixer variants but don't directly support butterfly structure generalization.

### Mechanism 2
- Claim: Butterfly Attention reduces self-attention complexity from O(S²) to O(S) while maintaining competitive accuracy.
- Mechanism: Butterfly Attention applies block-wise attention using the butterfly structure, where each layer performs attention on permuted blocks of tokens. Complete mixing is achieved through multiple layers with log_a(S) permutations.
- Core assumption: Structured sparse attention can approximate full attention for certain tasks, especially when the structure aligns with data properties (like spatial locality in images).
- Evidence anchors:
  - [abstract] "We propose an efficient Butterfly Attention Mixer, an approximation of the self-attention mechanism, with sub-quadratic complexity in Sequence Length(S)."
  - [section III-A2] "To solve this issue, we apply partial block-wise attention using the butterfly structure, which reduces the complexity of Attention to just S, however it would require loga(S) layers of Attention for complete mixing of tokens where a is Radix or block size."
  - [section IV-b] "Experiments show that Butterfly Attention scales better with longer sequence lengths, and the accuracy is comparable to dense attention."

### Mechanism 3
- Claim: Patch-Only MLP-Mixer provides a middle ground between MLP-Mixer and CNN by mixing patches without channel mixing.
- Mechanism: Instead of mixing channels as in MLP-Mixer, Patch-Only MLP-Mixer uses patch-wise mixing at multiple scales (different patch sizes with no common factors), mimicking the receptive field growth of CNNs while maintaining MLP-Mixer's flexibility.
- Core assumption: Patch-wise mixing alone can capture sufficient spatial relationships for image tasks, and using multiple patch sizes without common factors ensures complete patch mixing.
- Evidence anchors:
  - [abstract] "We propose Patch-Only MLP-Mixer for processing spatial 2D signals demonstrating a different dimension mixing strategy."
  - [section III-B] "Patch Only MLP-Mixer lies in between the original MLP-Mixer and a Convolution as it uses MLP for processing patches but only uses patches for overall input mixing."
  - [section IV-d] "Experiments show that our method produces comparative or even better results than the original MLP mixer on CIFAR-10 and CIFAR-100 datasets."

## Foundational Learning

- Concept: Butterfly transform and FFT butterfly structure
  - Why needed here: Understanding the butterfly structure is crucial as it forms the basis for the proposed Dimension Mixer model's efficiency and scalability.
  - Quick check question: How does the FFT butterfly structure achieve O(N log N) complexity, and how is this generalized to non-linear functions?

- Concept: Coupling flows and invertible neural networks
  - Why needed here: These architectures demonstrate that partial signal mixing can be sufficient for function approximation while providing invertibility, which motivates the Dimension Mixer's approach.
  - Quick check question: Why does partial mixing in coupling flows enable invertibility, and how does this relate to the Dimension Mixer's select-and-mix stages?

- Concept: Self-attention mechanism and its computational complexity
  - Why needed here: Understanding the quadratic complexity of self-attention is essential to appreciate why Butterfly Attention is proposed as an efficient alternative.
  - Quick check question: What is the computational complexity of standard self-attention, and how does Butterfly Attention reduce this complexity?

## Architecture Onboarding

- Component map: Input → Select stage (dimension grouping) → Mix stage (butterfly-structured processing) → Output
- Critical path: Input → Select stage (dimension grouping) → Mix stage (butterfly-structured processing) → Output. Multiple layers ensure complete mixing through permutations.
- Design tradeoffs:
  - Sparsity vs. accuracy: More sparse structures reduce computation but may lose some representational power
  - Block size: Larger blocks reduce layers needed for complete mixing but increase per-layer complexity
  - Radix choice: Affects the number of layers and the granularity of mixing
- Failure signatures:
  - Poor accuracy: May indicate insufficient mixing (block size too large, not enough layers) or inappropriate radix for the task
  - Memory issues: Could suggest block size too large for available resources
  - Slow training: Might indicate inefficient implementation or suboptimal block structure for the hardware
- First 3 experiments:
  1. Implement Butterfly MLP on a simple MLP-Mixer task (like CIFAR-10) to verify parameter and compute savings while maintaining accuracy
  2. Test Butterfly Attention on a moderate sequence length task (like CIFAR-100 with larger patches) to validate the attention approximation
  3. Implement Patch-Only MLP-Mixer with two patch sizes (e.g., 5x7 and 7x5 on 35x35 image) to verify the patch mixing strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Butterfly Attention mechanism perform on extremely long sequences beyond 16K tokens, such as in video or audio processing tasks?
- Basis in paper: [explicit] The paper demonstrates Butterfly Attention's effectiveness on Pathfinder-X with 16K tokens, but does not explore longer sequences.
- Why unresolved: The paper focuses on benchmarks up to 16K tokens, leaving the scalability to much longer sequences untested.
- What evidence would resolve it: Experiments comparing Butterfly Attention to dense attention on sequences exceeding 16K tokens in video or audio datasets.

### Open Question 2
- Question: What is the impact of randomizing token order in Butterfly Attention for tasks that rely heavily on positional information, such as language modeling?
- Basis in paper: [explicit] The paper shows that randomizing tokens in Butterfly ViT still performs well, suggesting sparse mixing may act as an inductive bias.
- Why unresolved: The experiments focus on vision tasks without positional encoding, leaving the effect on language tasks unclear.
- What evidence would resolve it: Comparative studies of Butterfly Attention with and without positional encoding on language modeling benchmarks.

### Open Question 3
- Question: How does the Patch-Only MLP-Mixer compare to CNNs in tasks requiring translation equivariance, such as object detection or segmentation?
- Basis in paper: [explicit] The paper positions Patch-Only MLP-Mixer as intermediate between MLP-Mixer and CNN but does not test it on equivariance-sensitive tasks.
- Why unresolved: The experiments focus on image classification, not tasks where translation equivariance is critical.
- What evidence would resolve it: Benchmarking Patch-Only MLP-Mixer against CNNs on object detection or semantic segmentation datasets.

## Limitations
- The paper's claims about butterfly structure generalization from linear to non-linear functions rely heavily on empirical validation rather than rigorous mathematical analysis
- Performance claims on Pathfinder-X task with 16K tokens lack detailed ablation studies showing specific contributions of the butterfly structure
- The assumption that partial mixing schemes are sufficient for expressive function approximation needs more theoretical justification

## Confidence

- **High Confidence**: The computational complexity claims (O(S) vs O(S²) for attention) and parameter efficiency metrics are well-supported by theoretical analysis and can be directly verified through implementation.
- **Medium Confidence**: The accuracy comparisons on CIFAR-10/100 and LRA benchmarks are reproducible, but the claim that butterfly attention maintains "competitive" accuracy with dense attention may vary depending on specific task requirements and hyperparameter tuning.
- **Low Confidence**: The generalization claims about butterfly structures being applicable to any non-linear function and the assertion that Patch-Only MLP-Mixer provides an optimal middle ground between MLP-Mixer and CNN lack sufficient theoretical grounding and comparative analysis with other sparse architectures.

## Next Checks

1. **Ablation study on mixing depth**: Systematically vary the number of layers in Butterfly Attention and measure the impact on accuracy for different sequence lengths to verify the log_r(S) mixing requirement claim.

2. **Comparison with other sparse attention methods**: Implement and compare against other efficient attention mechanisms (like Performer, Nyströmformer, or local attention) on LRA benchmarks to contextualize the butterfly attention performance claims.

3. **Mathematical analysis of mixing completeness**: Provide a formal proof or counterexample for the claim that the proposed butterfly structure ensures complete mixing of all input dimensions across all output dimensions, particularly for non-linear mixing functions.