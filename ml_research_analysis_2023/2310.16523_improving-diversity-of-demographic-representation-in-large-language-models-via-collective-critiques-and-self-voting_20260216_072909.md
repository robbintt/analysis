---
ver: rpa2
title: Improving Diversity of Demographic Representation in Large Language Models
  via Collective-Critiques and Self-Voting
arxiv_id: '2310.16523'
source_url: https://arxiv.org/abs/2310.16523
tags:
- diversity
- shot
- baseline
- response
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new prompting technique called collective-critique
  and self-voting (CCSV) to improve diversity of representation in large language
  models (LLMs). The approach leverages the model's ability to self-critique and revise
  its responses, sampling multiple critiques and revision drafts, and using self-voting
  to select the most diverse response.
---

# Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting

## Quick Facts
- arXiv ID: 2310.16523
- Source URL: https://arxiv.org/abs/2310.16523
- Authors: 
- Reference count: 40
- Key outcome: Zero-shot CCSV achieves highest diversity gains without handcrafted examples

## Executive Summary
This paper introduces Collective-Critique and Self-Voting (CCSV), a novel prompting technique that significantly improves demographic diversity in LLM responses. The method leverages the model's ability to self-critique its outputs and iteratively refine them through multiple critique and revision cycles, with self-voting to select the most diverse response. Extensive experiments demonstrate that CCSV outperforms all baseline methods, including few-shot prompting and constitutional AI approaches, with the zero-shot variant achieving the highest gains without requiring any task-specific exemplars.

## Method Summary
CCSV works by first generating an initial response, then producing multiple self-critiques of that response identifying diversity gaps. These critiques are aggregated and used to generate multiple revised drafts, from which the model self-votes to select the most diverse output. The approach operates on the insight that LLMs can reason about and improve their own diversity limitations when prompted appropriately. The method uses entropy and max-gap metrics to quantify diversity improvements across people and culture dimensions, with automated evaluation using knowledge graph entity extraction and human side-by-side comparisons.

## Key Results
- CCSV significantly improves people and culture diversity in LLM responses compared to all baseline methods
- Zero-shot CCSV outperforms all 5-shot approaches while requiring no handcrafted exemplars
- The method demonstrates stronger robustness and generalization properties than existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can reason about and critique its own lack of diversity in generated responses
- Mechanism: Self-reflection capability where the LLM can identify ways in which its response lacks diversity across demographic dimensions and provide concrete suggestions for improvement
- Core assumption: LLMs have sufficient understanding of diversity concepts and can introspect on their own outputs
- Evidence anchors:
  - [abstract] "We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal."
  - [section 2] "Our work can be seen as an extension of in-context prompting methods to the problem of diversity and inclusion."
- Break condition: If the model lacks sufficient knowledge of diverse demographic groups or cannot perform self-critique effectively

### Mechanism 2
- Claim: Aggregating multiple critiques and revisions improves diversity gains over greedy selection
- Mechanism: Collective-critiquing where multiple decoded critiques are aggregated to address multiple diversity gaps simultaneously, followed by self-voting to select the most diverse revision
- Core assumption: Multiple critique perspectives provide more comprehensive coverage of diversity issues than single critique
- Evidence anchors:
  - [section 3.2] "Our hypothesis is by aggregating insights from all the decoded critiques we can boost the model's ability to improve its revised drafts by addressing multiple critiques at the same time."
  - [section 6.3] "We see similar trends for gender and the max-gap metric. We observe that all components of critique-revision approach contribute positively to the performance"
- Break condition: If the aggregated critiques contain conflicting or contradictory suggestions that confuse the revision process

### Mechanism 3
- Claim: Zero-shot prompting can achieve diversity gains comparable to few-shot methods without requiring task-specific exemplars
- Mechanism: Direct instruction to improve diversity without relying on hand-crafted examples, leveraging the model's inherent understanding of diversity concepts
- Core assumption: LLMs can generalize diversity reasoning to new prompts without specific training examples
- Evidence anchors:
  - [abstract] "Our proposed approach includes a number of modeling improvements and insights which can be useful to advance state-of-the-art in-context reasoning approaches beyond diversity"
  - [section 6.1] "Remarkably, even though our 0-shotCCSV operates under a much more challenging setup without any few-shot exemplars, it outperforms all 5-shot approaches"
- Break condition: If zero-shot fails to generalize to prompts requiring nuanced understanding of specific demographic groups

## Foundational Learning

- Concept: Entropy as a measure of diversity
  - Why needed here: To quantify how uniformly different demographic groups are represented in model responses
  - Quick check question: If a response contains 50% male and 50% female names, what is the entropy value compared to 90% male and 10% female?

- Concept: In-context learning and self-consistency
  - Why needed here: The proposed method builds on these foundations to improve diversity through iterative self-critique and voting
  - Quick check question: How does sampling multiple revisions and voting differ from standard greedy decoding in terms of diversity outcomes?

- Concept: Knowledge graph entity extraction for demographic labeling
  - Why needed here: To automatically identify demographic attributes of people mentioned in model responses for evaluation
  - Quick check question: What limitations might arise when using knowledge graphs to label gender or ethnicity in generated text?

## Architecture Onboarding

- Component map:
  - Initial response generator → Critique generator → Revision generator → Self-voting selector → Final response
  - Knowledge graph integration for entity extraction and demographic labeling
  - Automated metrics computation (entropy, max-gap) and human evaluation pipeline

- Critical path: Initial response → Multiple critique generation → Collective critique aggregation → Multiple revision generation → Self-voting → Final diverse response

- Design tradeoffs:
  - Computational cost vs diversity gains (multiple decodes increase cost but improve outcomes)
  - Automated vs human evaluation (scalability vs nuance in measuring diversity perception)
  - Zero-shot simplicity vs few-shot potential for task-specific improvements

- Failure signatures:
  - Persistent low entropy despite multiple iterations
  - Max-gap remaining high while entropy increases (indicating distribution imbalance)
  - Human evaluation ratings not correlating with automated metrics
  - Knowledge graph unable to extract entities or assign demographic labels

- First 3 experiments:
  1. Run baseline vs 0-shot CCSV on people-diversity dataset measuring entropy and max-gap
  2. Test zero-shot CCSV vs few-shot variants on cultural-diversity dataset with human evaluation
  3. Perform ablation study comparing greedy vs collective critiquing and self-voting variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the zero-shot CCSV approach generalize to languages other than English?
- Basis in paper: [inferred] The paper states that CCSV was only tested on English language and acknowledges the need for future work on multilingual setups.
- Why unresolved: The current experiments only demonstrate effectiveness on English prompts, leaving the approach's performance on other languages unknown.
- What evidence would resolve it: Running CCSV on multilingual datasets and comparing results across different languages would determine if the approach transfers effectively.

### Open Question 2
- Question: How does CCSV perform on smaller language models with limited reasoning capabilities?
- Basis in paper: [explicit] The paper acknowledges that CCSV may be less effective on smaller models due to their limited critiquing capabilities.
- Why unresolved: The experiments only used a large 540B parameter model, so the approach's effectiveness on smaller models remains unknown.
- What evidence would resolve it: Testing CCSV on various sizes of language models (e.g., 7B, 20B, 100B parameters) and measuring diversity improvements would show the approach's scalability.

### Open Question 3
- Question: Can CCSV be effectively combined with fine-tuning approaches to improve efficiency?
- Basis in paper: [inferred] The paper discusses CCSV's computational cost and suggests using it offline to generate synthetic data for fine-tuning smaller models.
- Why unresolved: While mentioned as a future direction, the paper doesn't explore whether synthetic data generated by CCSV can effectively train smaller models to produce diverse outputs without iterative prompting.
- What evidence would resolve it: Demonstrating that fine-tuning smaller models on CCSV-generated data produces comparable diversity improvements to direct CCSV application would validate this approach.

## Limitations

- Knowledge graph entity extraction limitations: Automated diversity metrics rely on knowledge graphs that may miss rare names or fictional characters
- Human evaluation sample size concerns: The exact sample sizes and participant demographics for human evaluations are not fully specified
- Computational overhead trade-offs: CCSV requires multiple decoding passes, significantly increasing computational cost compared to baseline methods

## Confidence

**High confidence**: The core finding that CCSV improves entropy and max-gap metrics compared to baselines is well-supported by experimental results across multiple datasets.

**Medium confidence**: The zero-shot CCSV outperforming few-shot approaches is promising but based on a limited set of exemplars for the few-shot baselines.

**Medium confidence**: Human evaluation results showing CCSV responses are preferred for diversity are encouraging but limited by sample size and participant selection transparency.

## Next Checks

1. **Knowledge graph accuracy validation**: Conduct systematic evaluation of the entity extraction pipeline by manually annotating a subset of generated responses to measure accuracy rates across different demographic groups.

2. **Computational cost analysis**: Measure and report actual inference time and token costs for CCSV versus baselines across different batch sizes, analyzing the relationship between computational overhead and diversity improvement gains.

3. **Cross-cultural generalization study**: Test CCSV on prompts specific to different cultural contexts and demographic groups not represented in current evaluation datasets to assess robustness and potential cultural biases in the self-critique mechanism.