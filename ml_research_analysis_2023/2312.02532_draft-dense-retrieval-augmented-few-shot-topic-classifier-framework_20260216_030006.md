---
ver: rpa2
title: 'DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework'
arxiv_id: '2312.02532'
source_url: https://arxiv.org/abs/2312.02532
tags:
- draft
- dataset
- topic
- arxiv
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRAFT addresses few-shot topic classification by using a dense
  retriever model to construct a custom dataset from a few topic examples as queries.
  It retrieves passages relevant to these queries and fine-tunes a classifier on the
  resulting positive and negative samples.
---

# DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework

## Quick Facts
- **arXiv ID**: 2312.02532
- **Source URL**: https://arxiv.org/abs/2312.02532
- **Reference count**: 24
- **Key outcome**: DRAFT outperforms GPT-3 175B on few-shot topic classification despite having 177x fewer parameters

## Executive Summary
DRAFT is a novel framework for few-shot topic classification that leverages dense retrieval to construct custom training datasets. By using a few topic examples as queries, DRAFT retrieves relevant passages from Wikipedia to build a specialized dataset for training a classifier. The framework employs a multi-query retrieval algorithm to handle multiple related queries simultaneously and can be enhanced with negative queries to improve robustness. Experiments demonstrate that DRAFT achieves superior performance compared to large language models like GPT-3 175B on few-shot topic classification tasks while requiring significantly fewer parameters.

## Method Summary
DRAFT uses a dense retriever model to construct a customized dataset for few-shot topic classification. The framework takes a few examples of a target topic as queries and retrieves relevant passages from Wikipedia using a bi-encoder dense retriever. The Multi-Query Retrieval (MQR) algorithm then constructs positive samples from the retrieved passages and negative samples from random passages. A classifier is fine-tuned on this customized dataset. The method can be enhanced with negative queries to improve performance on semantically similar topics. The framework is evaluated on benchmark datasets (AGNews, DBpedia, TREC) and manually constructed datasets.

## Key Results
- DRAFT outperforms GPT-3 175B on few-shot topic classification despite having 177x fewer parameters
- The framework achieves strong performance across diverse real-world topics including Religion, South Korea, and FactsNet
- Multi-query retrieval effectively handles multiple topic-related queries simultaneously
- Negative query enhancement improves classifier robustness to hard negatives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DRAFT builds effective few-shot classifiers by retrieving relevant passages for topic examples
- **Mechanism**: Uses a bi-encoder dense retriever to construct a customized dataset by retrieving relevant passages from Wikipedia using topic examples as queries. The retrieved passages and topic examples form positive samples, while random passages form negative samples.
- **Core assumption**: Dense retriever can effectively retrieve passages semantically related to topic examples
- **Evidence anchors**: Abstract states DRAFT "uses a few examples of a specific topic as queries to construct Customized dataset with a dense retriever model"

### Mechanism 2
- **Claim**: Multi-query retrieval handles multiple queries simultaneously for single topics
- **Mechanism**: MQR algorithm retrieves passages for each query and filters them based on similarity to query vectors, retaining only passages exceeding a threshold similarity score
- **Core assumption**: Similarity between query vectors can effectively filter relevant passages
- **Evidence anchors**: Abstract mentions MQR "effectively handles multiple queries related to a specific topic"

### Mechanism 3
- **Claim**: Negative queries improve performance on hard negatives
- **Mechanism**: Introduces negative queries semantically similar to target topic but with different content to construct negative samples, making classifier more robust
- **Core assumption**: Negative queries provide useful information for distinguishing between semantically similar topics
- **Evidence anchors**: Abstract states DRAFT's performance can be improved by "manually incorporate negative queries that belong to a semantically similar category but are different from the target topic"

## Foundational Learning

- **Dense retrieval using bi-encoders**: DRAFT relies on bi-encoders for efficient dense retrieval. Understanding how bi-encoder architectures enable efficient retrieval compared to other methods is crucial for implementation.
  - *Quick check*: How does a bi-encoder architecture enable efficient dense retrieval compared to other retrieval methods?

- **Contrastive learning**: The dense retriever is pre-trained using contrastive learning to learn effective representations. Understanding contrastive learning's application to dense retrieval is important for understanding retriever performance.
  - *Quick check*: How does contrastive learning help the dense retriever learn to retrieve relevant passages for a given query?

- **Few-shot learning**: DRAFT addresses few-shot topic classification where only a few examples are available. Understanding few-shot learning challenges is important for evaluating DRAFT's effectiveness.
  - *Quick check*: What are the main challenges in few-shot learning, and how does DRAFT address them?

## Architecture Onboarding

- **Component map**: Dense Retriever (bi-encoder) -> MQR Algorithm -> Customized Dataset Constructor -> Classifier -> Topic Classification
- **Critical path**: Construction of the customized dataset using dense retriever and MQR algorithm is the critical path, as dataset quality directly impacts classifier performance
- **Design tradeoffs**: Tradeoff between number of queries and subspace size in MQR algorithm - more queries provide more information but may introduce noise, while larger subspace retrieves more relevant passages but may include irrelevant ones
- **Failure signatures**: Poor performance on topics not well-represented in Wikipedia, difficulty handling semantically similar topics, sensitivity to hyperparameters like number of queries and subspace size
- **First 3 experiments**:
  1. Evaluate DRAFT performance on a small set of diverse topics to understand strengths and weaknesses
  2. Experiment with different numbers of queries and subspace sizes to find optimal configuration
  3. Compare performance with and without negative queries to understand impact on classifier robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does DRAFT's performance scale with size and diversity of the knowledge base used?
- **Basis in paper**: Inferred from discussion about importance of knowledge base choice and its impact on performance in Section 5
- **Why unresolved**: Paper mentions performance varies across datasets but lacks detailed analysis of scaling relationship
- **What evidence would resolve it**: Experiments showing DRAFT's performance on different sizes/diversities of knowledge bases with detailed analysis

### Open Question 2
- **Question**: What are potential limitations of bi-encoder dense retriever model in DRAFT and how could they be addressed?
- **Basis in paper**: Explicit discussion about lack of token-level interaction between query and document tokens in Related Works section
- **Why unresolved**: Acknowledges limitation but doesn't explore potential solutions or alternative approaches
- **What evidence would resolve it**: Comparative study of DRAFT's performance using different retriever models with trade-off analysis

### Open Question 3
- **Question**: How does query choice impact DRAFT's performance and what strategies select optimal queries?
- **Basis in paper**: Inferred from importance of queries in performance and potential for improvement with increased queries in Section 5
- **Why unresolved**: Lacks detailed analysis of different query selection strategies or generation methods
- **What evidence would resolve it**: Experiments comparing DRAFT's performance using different query selection strategies and generation methods

### Open Question 4
- **Question**: What are challenges and solutions for implementing DRAFT in real-world applications with many topics and users?
- **Basis in paper**: Inferred from discussion about implementation challenges in real-world scenarios in Section 5
- **Why unresolved**: Mentions challenge of storing weights for individually trained classifiers but lacks detailed exploration of solutions
- **What evidence would resolve it**: Detailed analysis of computational/storage requirements with proposed solutions or alternative approaches

## Limitations
- Performance comparison relies on single F1 score metric that may not capture full complexity of topic classification
- Reliance on Wikipedia as knowledge base may not represent all domains equally well
- Negative query enhancement requires manual curation, limiting scalability
- MQR algorithm's threshold determination method is not fully specified

## Confidence
- **High Confidence**: DRAFT outperforms GPT-3 175B in few-shot topic classification while using fewer parameters
- **Medium Confidence**: Multi-query retrieval effectively handles multiple topic-related queries simultaneously
- **Medium Confidence**: Negative query enhancement improves classifier robustness to hard negatives

## Next Checks
1. **Cross-domain validation**: Test DRAFT on specialized domains (medical, legal, technical) to verify effectiveness beyond general topics
2. **Threshold sensitivity analysis**: Systematically vary MQR threshold parameter to understand impact on classification performance
3. **Query efficiency study**: Compare DRAFT's performance using different numbers of topic examples (1-10) to determine optimal query efficiency