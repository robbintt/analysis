---
ver: rpa2
title: Impact of time and note duration tokenizations on deep learning symbolic music
  modeling
arxiv_id: '2310.08497'
source_url: https://arxiv.org/abs/2310.08497
tags:
- music
- tokens
- note
- online
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the representation of time and note duration
  in symbolic music tokenization affects model performance. The authors analyze four
  common tokenization methods that differ in their time representation (TimeShift
  vs Bar+Position) and note duration representation (Duration vs NoteOff).
---

# Impact of time and note duration tokenizations on deep learning symbolic music modeling

## Quick Facts
- **arXiv ID**: 2310.08497
- **Source URL**: https://arxiv.org/abs/2310.08497
- **Reference count**: 0
- **Primary result**: Choice of time and note duration representation significantly impacts model performance, with optimal choices varying by task

## Executive Summary
This paper investigates how different symbolic music tokenization methods affect deep learning model performance across multiple tasks. The authors systematically compare four common tokenization approaches that vary in their representation of time (TimeShift vs Bar+Position) and note duration (Duration vs NoteOff). Through experiments on composer classification, emotion classification, music generation, and sequence representation learning, they demonstrate that explicit note duration tokens improve classification accuracy while TimeShift tokens enhance generation quality. The findings reveal that the optimal tokenization strategy depends critically on the specific task requirements.

## Method Summary
The study employs Transformer-based models (BERT for classification and sequence representation, GPT2 for generation) with a fixed architecture (12 layers, 768 units, 12 attention heads). Four tokenization schemes are compared: TimeShift+Duration, TimeShift+NoteOff, Bar+Position+Duration, and Bar+Position+NoteOff. Models are pretrained for 100k steps then finetuned for 50k steps on POP909 (generation), GiantMIDI (classification and representation), and EMOPIA (emotion classification) datasets. Training uses Adam optimizer with specific hyperparameters including learning rate schedules, dropout, and gradient clipping. Evaluation metrics include classification accuracy, Token Syntax Error ratios for generation, and cosine similarity plus intrinsic dimension for representation learning.

## Key Results
- Explicit Duration tokens outperform NoteOff tokens in classification tasks by providing direct melodic and harmonic patterns
- TimeShift tokens achieve better generation performance by encoding explicit time distances between notes
- Bar+Position tokens produce more isotropic embeddings for sequence representation learning through explicit positional information
- Task-specific tokenization choices are critical, with no single best approach across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit note duration tokens improve classification accuracy because they provide direct melodic and harmonic patterns for the model to capture.
- Mechanism: Duration tokens encode absolute note lengths, allowing the model to directly model melodic and harmonic structures. In contrast, NoteOff tokens require the model to infer durations from combinations of NoteOn, NoteOff, and time tokens, adding complexity and potential uncertainty.
- Core assumption: Classification tasks rely heavily on capturing melodic and harmonic patterns, which are more easily represented with explicit durations.
- Evidence anchors:
  - [abstract] "explicit information leads to better results depending on the task"
  - [section] "Explicitly representing note duration leads to better classification accuracy as it helps the models to capture the melodies and harmonies of a music"
  - [corpus] Weak evidence - no direct support in corpus
- Break Condition: If the music dataset contains mostly long, sustained notes or complex rhythms where duration is less discriminative, the advantage of explicit duration tokens might diminish.

### Mechanism 2
- Claim: TimeShift tokens improve music generation by providing explicit time distances between successive notes.
- Mechanism: TimeShift tokens represent explicit time movements, especially the time distances between successive notes. This explicit information helps the model predict the timing of the next note more accurately, reducing prediction errors in time.
- Core assumption: Music generation benefits from explicit temporal information to maintain rhythmic coherence and avoid overlapping notes.
- Evidence anchors:
  - [abstract] "TimeShift tokens perform better for generation"
  - [section] "TimeShift tokens represent explicit time movements, and especially the time distances between successive notes"
  - [corpus] Weak evidence - no direct support in corpus
- Break Condition: If the generation task prioritizes complex rhythmic patterns or polyrhythms, the fixed TimeShift representation might limit the model's flexibility.

### Mechanism 3
- Claim: Bar+Position tokens yield more isotropic embeddings for sequence representation learning because they provide explicit note onset and offset positions.
- Mechanism: Bar+Position tokens bring explicit information on the absolute positions (within bars) of the notes, but not the onset distances between notes. This explicit positional information helps the model create more uniformly distributed embeddings in the representation space.
- Core assumption: Sequence representation learning benefits from explicit positional information to create more distinct and distinguishable embeddings.
- Evidence anchors:
  - [abstract] "For sequence representation learning, Bar+Position tokens yield more isotropic embeddings"
  - [section] "Unlike classification, the contrastive learning objective models the similarities and dissimilarities between examples in the same batch. In this context, note onset and offset positions appear to be helpful for the models to distinguish music"
  - [corpus] Weak evidence - no direct support in corpus
- Break Condition: If the sequence representation task focuses on capturing melodic patterns rather than rhythmic structure, the explicit positional information might be less critical.

## Foundational Learning

- Concept: Music tokenization
  - Why needed here: Tokenization is the process of converting symbolic music into sequences of discrete tokens that can be processed by deep learning models. Understanding tokenization is crucial for this paper as it analyzes different tokenization methods and their impact on model performance.
  - Quick check question: What are the two main aspects of music tokenization analyzed in this paper?

- Concept: Transformer architecture
  - Why needed here: Transformers are the primary deep learning models used in this paper for various tasks. Understanding their architecture and limitations is important for interpreting the results and conclusions.
  - Quick check question: What are the key limitations of Transformers mentioned in the paper that motivate the study of tokenization methods?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is used in the sequence representation learning task to train the model to learn sequence representations where similar inputs have higher cosine similarities. Understanding this technique is crucial for interpreting the results of this task.
  - Quick check question: What is the main objective of contrastive learning in the context of sequence representation learning?

## Architecture Onboarding

- Component map: MIDI files -> Tokenization -> Transformer model -> Task-specific training -> Evaluation metrics
- Critical path: Tokenizing music data -> Training model on specific task -> Evaluating with task-specific metrics
- Design tradeoffs:
  - Tokenization method: The choice of time and note duration representation impacts model performance differently for each task.
  - Model architecture: Using different Transformer variants (BERT vs GPT2) for different tasks.
  - Training strategy: Balancing pretraining and finetuning steps for optimal performance.
- Failure signatures:
  - High prediction error ratios in generation tasks indicate issues with the tokenization method or model architecture.
  - Low accuracy in classification tasks might suggest the need for more discriminative tokenization or additional training data.
  - Non-isotropic embeddings in sequence representation tasks could indicate the need for a different tokenization method or contrastive learning approach.
- First 3 experiments:
  1. Reproduce the composer classification results to verify the superiority of TS+Dur tokenization.
  2. Test the generation performance of TS+NOff tokenization to confirm the higher note duplication and prediction errors.
  3. Evaluate the isotropy of embeddings produced by Pos+Dur tokenization using PCA and intrinsic dimension estimation.

## Open Questions the Paper Calls Out
- How do different time and note duration tokenization strategies affect music transcription performance compared to classification and generation tasks?
- How do music tokenization strategies impact the performance of music reasoning tasks that require logical deductions from the data?
- How do the results of the studied tokenization strategies generalize to other music genres and styles beyond the ones used in the experiments?

## Limitations
- Results are based on specific datasets (POP909, GiantMIDI, EMOPIA) that may not generalize to other musical styles
- All experiments use a fixed Transformer architecture without exploring how tokenization effects might vary with different model sizes
- The study focuses on four specific tasks, potentially missing other relevant applications
- Some metrics may not fully capture the semantic quality of learned representations

## Confidence
**High Confidence**:
- Explicit duration tokens improve classification accuracy over NoteOff representations
- TimeShift tokens outperform positional encodings for music generation tasks
- The four-tokenization framework provides a useful lens for understanding representation tradeoffs

**Medium Confidence**:
- Bar+Position tokens yield more isotropic embeddings for sequence representation learning
- Task-specific optimization of tokenization choices is necessary rather than universal rules

**Low Confidence**:
- The relative performance differences between specific tokenization pairs in generation tasks
- Whether the observed effects would persist with different model scales or alternative deep learning architectures

## Next Checks
1. Test the four tokenization methods on additional datasets representing different musical genres to assess generalizability of the findings.
2. Repeat key experiments with both smaller (4-layer) and larger (24-layer) Transformer variants to determine if tokenization effects scale with model capacity.
3. Systematically remove either time or duration information from each tokenization scheme to quantify the individual contribution of each component to task performance.