---
ver: rpa2
title: 'SHAMSUL: Systematic Holistic Analysis to investigate Medical Significance
  Utilizing Local interpretability methods in deep learning for chest radiography
  pathology prediction'
arxiv_id: '2307.08003'
source_url: https://arxiv.org/abs/2307.08003
tags:
- score
- segmentation
- interpretability
- methods
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the medical significance of interpretability
  methods for pathology prediction in chest radiography. It employs four well-established
  local interpretability methods - LIME, SHAP, Grad-CAM, and LRP - to interpret predictions
  of a deep learning model trained on the CheXpert dataset.
---

# SHAMSUL: Systematic Holistic Analysis to investigate Medical Significance Utilizing Local interpretability methods in deep learning for chest radiography pathology prediction

## Quick Facts
- arXiv ID: 2307.08003
- Source URL: https://arxiv.org/abs/2307.08003
- Reference count: 40
- Key outcome: This paper investigates the medical significance of interpretability methods for pathology prediction in chest radiography using LIME, SHAP, Grad-CAM, and LRP, finding that Grad-CAM performs best quantitatively while LIME shows highest medical significance.

## Executive Summary
This study systematically evaluates four local interpretability methods (LIME, SHAP, Grad-CAM, and LRP) for explaining deep learning predictions of chest radiography pathologies. Using the CheXpert dataset and CheXlocalize human annotations, the research quantifies heatmap segmentation quality through IoU scores and conducts qualitative analysis on Lung Opacity and Support Devices classes. The findings reveal that while Grad-CAM achieves the highest quantitative performance, LIME demonstrates superior medical significance in visualization, highlighting the importance of balancing both quantitative and qualitative assessment approaches in medical AI interpretability.

## Method Summary
The study employs a pre-trained DenseNet-121 model for multi-label pathology classification on chest radiography images from the CheXpert dataset. Four local interpretability methods (LIME, SHAP, Grad-CAM, and LRP) are applied to generate heatmap segmentations highlighting regions relevant to model predictions. The quantitative evaluation uses IoU scores comparing heatmaps against human expert annotations from CheXlocalize, while qualitative analysis examines selected instances with highest IoU and best prediction probabilities for Lung Opacity and Support Devices classes.

## Key Results
- Grad-CAM demonstrates the most favorable performance in quantitative IoU evaluation across most pathology classes
- LIME heatmap segmentation visualization exhibits the highest level of medical significance in qualitative assessment
- Lung Opacity class achieves highest average IoU (0.3591), while Edema shows lowest (0.0253)
- Support Devices class shows consistent performance across interpretability methods with IoU range of 0.0991-0.1470

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LIME's local surrogate model provides interpretable explanations for individual predictions.
- Mechanism: LIME approximates the local behavior of a complex model by training a sparse linear model on data points in the neighborhood of the instance being explained.
- Core assumption: The local behavior of a complex model is simpler and more interpretable than its global behavior.
- Evidence anchors:
  - [abstract]: "LIME operates under the assumption that the local behavior of a model is simpler to compute than its global behavior. Based on this assumption, LIME approximates the local behavior of the model to provide explanations for individual predictions."
  - [section]: "LIME focuses on explaining the prediction for a single instance by considering its local neighborhood. It expands the input data around the specific instance to gather more information about the local area."
- Break condition: If the local neighborhood does not capture the relevant features or the surrogate model is too complex, LIME's explanations may be unreliable.

### Mechanism 2
- Claim: Grad-CAM uses gradients to identify important regions in the input image for a specific class prediction.
- Mechanism: Grad-CAM computes the gradient of the class score with respect to the activation maps of the last convolutional layer, then uses these gradients to weight the activation maps and generate a heatmap highlighting important regions.
- Core assumption: The gradients with respect to the activation maps provide information about the importance of different regions in the input image for the class prediction.
- Evidence anchors:
  - [abstract]: "Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique that assesses the significance of individual neurons in an input using a feature map. This feature map is generated by leveraging the gradient scores propagated to the final convolutional layer of a Convolutional Neural Network (CNN)."
  - [section]: "Let ð‘¦ð‘ denote the class score for a specific class ð‘. Each neuron's significance can be interpreted as ð‘¦ð‘ and is derived by calculating the gradient of ð‘¦ð‘ with respect to the activation ð´. This process yields an importance weight ð›¼ð‘ð‘˜ for each feature mapð‘˜, given by: ð›¼ð‘ð‘˜ = 1/ð‘§ âˆ‘ð‘– âˆ‘ð‘— ðœ•ð‘¦ð‘/ðœ•ð´ð‘˜ð‘–ð‘—"
- Break condition: If the gradients are noisy or the last convolutional layer does not capture the relevant features, Grad-CAM's heatmaps may be inaccurate.

### Mechanism 3
- Claim: SHAP uses game theory concepts to attribute the prediction to individual features.
- Mechanism: SHAP calculates the Shapley value for each feature, which represents the average marginal contribution of that feature across all possible coalitions of features.
- Core assumption: The prediction can be viewed as a game where features are players, and the Shapley value fairly distributes the "payout" (prediction) among the features based on their contributions.
- Evidence anchors:
  - [abstract]: "SHAP (Shapley Additive exPlanations) method... employs KernelSHAP, which combines the concepts of LIME and Shapley values. In KernelSHAP, a linear model ð‘” âˆ¶ â„ð‘€ â†’ â„ is trained using a weighted least squares (WLS) regression method, with its optimal coefficients serving as the Shapley values."
  - [section]: "The attribution score, determined by the Shapley value, can be calculated for each feature1 â‰¤ ð‘– â‰¤ ð‘€. Here, the cardinality operatorâˆ£ â‹… âˆ£ represents the number of non-zero elements in a set. To approximate the Shapley values in our work, we utilized KernelSHAP."
- Break condition: If the feature space is too large or the model is highly non-linear, calculating the exact Shapley values may be computationally infeasible, and approximations may be inaccurate.

## Foundational Learning

- Concept: Intersection over Union (IoU)
  - Why needed here: IoU is used to quantitatively evaluate the overlap between the heatmap segmentations generated by the interpretability methods and the ground truth annotations provided by human experts.
  - Quick check question: If the predicted segmentation perfectly overlaps with the ground truth segmentation, what is the IoU score?

- Concept: Transfer learning
  - Why needed here: The study uses a pre-trained DenseNet-121 model as the base architecture for transfer learning, leveraging the learned features from a large dataset (ImageNet) to improve performance on the medical imaging task.
  - Quick check question: What are the advantages of using transfer learning in this context?

- Concept: Multi-label classification
  - Why needed here: The pathology prediction task involves identifying multiple pathologies in a single chest radiography image, requiring a multi-label classification approach.
  - Quick check question: How does multi-label classification differ from multi-class classification?

## Architecture Onboarding

- Component map: Input images -> DenseNet-121 model -> Interpretability methods (LIME, SHAP, Grad-CAM, LRP) -> Heatmap segmentations -> IoU evaluation and qualitative analysis

- Critical path:
  1. Load and preprocess the chest radiography images
  2. Train the DenseNet-121 model on the CheXpert dataset
  3. Apply the interpretability methods to generate heatmap segmentations
  4. Evaluate the heatmap segmentations using IoU and qualitative analysis

- Design tradeoffs:
  - Using a pre-trained model vs. training from scratch
  - Choosing interpretability methods that are model-agnostic vs. model-specific
  - Balancing quantitative and qualitative evaluation approaches

- Failure signatures:
  - Poor model performance (low AUROC/AUPRC scores)
  - Interpretability methods failing to identify relevant regions (low IoU scores)
  - Qualitative analysis revealing misleading or irrelevant heatmap segmentations

- First 3 experiments:
  1. Train the DenseNet-121 model on the CheXpert dataset and evaluate its performance using AUROC and AUPRC.
  2. Apply LIME to a sample of correctly predicted instances and generate heatmap segmentations. Evaluate the segmentations using IoU.
  3. Compare the heatmap segmentations generated by LIME, SHAP, Grad-CAM, and LRP for a sample of instances, and analyze the differences in their interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of interpretability methods differ when applied to multimodal data sources (e.g., clinical notes, laboratory tests) versus single modality chest radiography images?
- Basis in paper: [inferred] The paper suggests that a multimodal-based approach could offer additional insights for enhancing interpretability, but does not test this hypothesis.
- Why unresolved: The study only investigates interpretability methods using chest radiography images as input, without incorporating other data sources.
- What evidence would resolve it: Empirical comparison of interpretability methods' performance on multimodal data versus single modality data, showing quantitative differences in interpretability metrics and qualitative differences in medical significance.

### Open Question 2
- Question: What is the relationship between the quality of the pathology prediction model and the interpretability of its predictions, particularly when using different model architectures like vision transformers versus CNNs?
- Basis in paper: [inferred] The paper notes that the DenseNet-121 model used may not be optimal and suggests that vision transformers could perform better, but does not test this.
- Why unresolved: The study uses a fixed DenseNet-121 architecture for all interpretability methods, preventing analysis of how model architecture affects interpretability.
- What evidence would resolve it: Comparative study of interpretability methods applied to predictions from different model architectures (CNN vs. vision transformer), with analysis of correlation between prediction accuracy and interpretability quality.

### Open Question 3
- Question: How does the medical significance of interpretability method outputs vary across different pathology classes with varying characteristics (e.g., well-defined shapes vs. amorphous opacities)?
- Basis in paper: [explicit] The paper discusses how classes like Lung Opacity and Support Devices have different characteristics and explores their interpretability qualitatively, but does not provide a systematic quantitative comparison.
- Why unresolved: The study provides qualitative analysis of two specific classes but does not quantify how interpretability performance varies systematically across all pathology classes with different characteristics.
- What evidence would resolve it: Quantitative analysis of interpretability method performance across all pathology classes, categorized by characteristics such as shape definition, prevalence, and prediction difficulty, showing correlations between class characteristics and interpretability quality.

## Limitations
- Small dataset size (1,256 images across 10 pathologies) in CheXlocalize limits generalizability of IoU-based quantitative evaluation
- Qualitative analysis restricted to only two pathology classes (Lung Opacity and Support Devices) despite 14-class prediction task
- No investigation of computational efficiency differences between interpretability methods

## Confidence
- Quantitative findings: Medium confidence - IoU metric provides standardization but limited by dataset size and focus on two classes
- Comparative analysis: Low-Medium confidence - Does not investigate computational efficiency or performance across all pathology classes
- Medical significance assessment: Medium confidence - Assumes larger IoU scores indicate better medical significance without explicit clinical validation

## Next Checks
1. Conduct comprehensive expert review across all 14 pathology classes to validate medical significance of heatmap segmentations beyond the two selected classes
2. Evaluate computational efficiency and runtime performance of each interpretability method to provide complete comparison framework
3. Test interpretability methods on independent external dataset to assess generalizability across different clinical settings