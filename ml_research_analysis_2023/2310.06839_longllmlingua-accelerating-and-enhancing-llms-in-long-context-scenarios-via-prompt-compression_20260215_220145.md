---
ver: rpa2
title: 'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
  Prompt Compression'
arxiv_id: '2310.06839'
source_url: https://arxiv.org/abs/2310.06839
tags:
- compression
- prompt
- information
- what
- longllmlingua
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of high computational cost,
  long latency, and performance degradation in long-context scenarios for large language
  models (LLMs). The authors propose LongLLMLingua, a prompt compression method that
  enhances LLMs' perception of key information by incorporating question-aware coarse-to-fine
  compression, document reordering, dynamic compression ratios, and subsequence recovery.
---

# LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression

## Quick Facts
- arXiv ID: 2310.06839
- Source URL: https://arxiv.org/abs/2310.06839
- Reference count: 40
- Improves LLM performance in long-context scenarios with up to 21.4% accuracy gain and 94% cost reduction

## Executive Summary
LongLLMLingua addresses the computational challenges of long-context scenarios for large language models by introducing a prompt compression method that enhances information density while reducing token count. The approach combines question-aware coarse-to-fine compression, document reordering, dynamic compression ratios, and subsequence recovery to improve LLM performance with fewer tokens. Extensive evaluation shows significant improvements in accuracy, cost reduction, and latency across multiple benchmarks when using GPT-3.5-Turbo and LongChat-13B-16k.

## Method Summary
LongLLMLingua implements a four-stage prompt compression pipeline: budget controller allocates compression resources based on document relevance scores, coarse-grained compression removes irrelevant tokens using question-conditioned perplexity, document reordering positions important content at optimal locations, and fine-grained compression applies adaptive granular control. The subsequence recovery mechanism restores original content when compressed prompts alter key entities. The system dynamically adjusts compression ratios based on document importance, allowing more tokens for relevant content while aggressively compressing less important material.

## Key Results
- Achieves up to 21.4% performance improvement with ~4x fewer tokens in GPT-3.5-Turbo
- Reduces costs by 94.0% in the LooGLE benchmark while maintaining accuracy
- Speeds up end-to-end latency by 1.4x-2.6x when compressing ~10k token prompts at 2x-6x ratios
- Outperforms baselines including BM25, Gzip, SBERT, and LLMLingua across NaturalQuestions, LongBench, and ZeroSCROLLS benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question-aware coarse-to-fine compression improves key information density in prompts
- Mechanism: Uses question-conditioned perplexity (contrastive perplexity) to distinguish tokens relevant to the question, retaining more of these while removing less relevant tokens
- Core assumption: Tokens with higher contrastive perplexity are more relevant to answering the question
- Evidence anchors: LLM performance hinges on density and position of key information in input prompt; question-aware coarse-to-fine compression method improves key information density
- Break condition: If contrastive perplexity fails to distinguish relevant tokens, performance degrades as noise increases

### Mechanism 2
- Claim: Document reordering based on relevance scores reduces information loss in middle positions
- Mechanism: Documents are reordered by importance scores so more relevant documents appear earlier in the prompt, leveraging LLMs' better performance on edge-positioned information
- Core assumption: LLMs perform better on information at beginning and end of prompts
- Evidence anchors: LLMs' ability to capture relevant information depends on their positions in the prompt; document reordering mechanism reduces information loss in the middle
- Break condition: If relevance scoring is inaccurate, reordering may place irrelevant documents at positions where LLMs perform best

### Mechanism 3
- Claim: Dynamic compression ratios bridge coarse and fine-grained compression for adaptive granular control
- Mechanism: Allocates different compression budgets to documents based on relevance scores, giving more budget (less compression) to more relevant documents
- Core assumption: Key information density varies across documents, requiring different compression rates
- Evidence anchors: Dynamic compression ratios bridge coarse-grained compression and fine-grained compression for adaptive granular control; importance scores guide budget allocation
- Break condition: If linear scheduler for budget allocation is suboptimal, relevant information may be over-compressed

## Foundational Learning

- **Perplexity as token importance measure**: Why needed here - system uses token perplexity to determine which tokens to remove during compression. Quick check: What does a lower perplexity value indicate about a token's contribution to the overall context?

- **Contrastive learning for relevance scoring**: Why needed here - system calculates contrastive perplexity by comparing perplexity with and without the question as context. Quick check: How does conditioning on the question change the perplexity scores of tokens?

- **Subsequence recovery for information integrity**: Why needed here - system restores original content from LLM responses when compression has altered key entities. Quick check: What is the relationship between compressed and original prompts that enables subsequence recovery?

## Architecture Onboarding

- **Component map**: Budget Controller → Coarse-grained Compression → Document Reordering → Fine-grained Compression → Subsequence Recovery → LLM Inference
- **Critical path**: Document compression and reordering directly impact LLM performance
- **Design tradeoffs**: Higher compression ratios reduce cost but may remove critical information
- **Failure signatures**: Performance drops when relevant tokens are removed or positioned poorly
- **First 3 experiments**:
  1. Test question-aware coarse compression alone to verify relevance scoring
  2. Test document reordering with fixed compression to verify position sensitivity
  3. Test dynamic compression ratios to verify adaptive granular control

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LongLLMLingua's performance scale with different compression ratios and token lengths beyond the evaluated benchmarks?
- Basis in paper: [inferred] Paper evaluates performance on specific benchmarks with defined compression ratios (2x-10x) and token lengths (~10k tokens), but does not explore full range of possible ratios or token lengths
- Why unresolved: Paper focuses on demonstrating effectiveness within specific range, leaving open question of how method performs at extreme compression ratios or with much longer or shorter prompts
- What evidence would resolve it: Comprehensive experiments evaluating LongLLMLingua across wider range of compression ratios (1.5x-20x) and token lengths (1k-100k tokens) on diverse benchmarks

### Open Question 2
- Question: How does LongLLMLingua compare to other prompt compression methods that use different techniques, such as soft prompt tuning or token pruning?
- Basis in paper: [inferred] Paper compares to retrieval-based methods and information-entropy-based approaches like LLMLingua, but does not directly compare to soft prompt tuning or token pruning methods
- Why unresolved: Different prompt compression techniques have different strengths and weaknesses, and direct comparison would provide more comprehensive understanding
- What evidence would resolve it: Empirical studies comparing LongLLMLingua to other prompt compression methods on variety of tasks and benchmarks

### Open Question 3
- Question: How does LongLLMLingua's performance vary across different types of questions and document domains?
- Basis in paper: [inferred] Paper evaluates on diverse set of tasks and benchmarks, but does not explicitly analyze how performance varies across different question types or document domains
- Why unresolved: Effectiveness of prompt compression methods may depend on specific characteristics of questions and documents being processed
- What evidence would resolve it: Detailed analyses of performance on different question types and document domains using fine-grained categorization and statistical testing

### Open Question 4
- Question: What is the impact of LongLLMLingua's document reordering strategy on the performance of different types of LLMs?
- Basis in paper: [explicit] Paper introduces document reordering mechanism based on importance scores and shows improvements across different tasks and baselines
- Why unresolved: Effectiveness of document reordering may vary depending on specific architecture and training data of different LLMs
- What evidence would resolve it: Experiments evaluating LongLLMLingua with different document reordering strategies and variety of LLMs, including both open-source and proprietary models

## Limitations
- Limited validation beyond retrieval-augmented long-context scenarios
- Potential information loss with extreme compression ratios not fully explored
- Performance dependent on quality of pre-trained perplexity models used

## Confidence
- **High Confidence**: Core mechanism of using question-aware perplexity for token importance scoring is well-grounded and validated through ablation studies
- **Medium Confidence**: Document reordering and dynamic compression ratio mechanisms are logically sound but have limited direct empirical validation
- **Low Confidence**: Subsequence recovery mechanism's effectiveness in maintaining information integrity during extreme compression is not thoroughly validated

## Next Checks
1. Evaluate LongLLMLingua on non-retrieval-augmented long-context tasks (document summarization, extended dialogue) to assess broader applicability
2. Conduct detailed study on information loss and performance degradation when applying compression ratios beyond 6x, particularly for complex queries
3. Test system's performance using different pre-trained models for perplexity calculation across multiple domains to quantify impact of model choice