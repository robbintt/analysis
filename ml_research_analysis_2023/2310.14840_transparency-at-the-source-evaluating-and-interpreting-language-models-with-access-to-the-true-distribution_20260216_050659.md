---
ver: rpa2
title: 'Transparency at the Source: Evaluating and Interpreting Language Models With
  Access to the True Distribution'
arxiv_id: '2310.14840'
source_url: https://arxiv.org/abs/2310.14840
tags:
- language
- data
- pcfg
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel experimental setup for training, evaluating
  and interpreting neural language models using artificially generated language-like
  data. The data is produced by a massive probabilistic grammar (based on state-split
  PCFGs) that is itself derived from a large natural language corpus, providing full
  control over the generative process while maintaining naturalistic properties.
---

# Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution

## Quick Facts
- **arXiv ID**: 2310.14840
- **Source URL**: https://arxiv.org/abs/2310.14840
- **Reference count**: 20
- **Key outcome**: Derives closed-form expressions for exact perplexity bounds on PCFG-generated data, showing causal LMs align better with true distributions than masked LMs

## Executive Summary
This paper introduces a novel experimental framework for training, evaluating, and interpreting neural language models using artificially generated language-like data from state-split probabilistic context-free grammars (PCFGs). The approach provides researchers with direct access to the true underlying distribution, enabling precise measurement of model alignment and stronger interpretability baselines. By leveraging PCFG structure, the authors derive efficient closed-form expressions for exact perplexity bounds, overcoming previous computational intractability. Experiments demonstrate that causal language models better approximate the true PCFG distribution than masked models, and reveal distinct learning dynamics across word types.

## Method Summary
The methodology involves inducing a state-split PCFG from a parsed natural language corpus, then using this grammar to generate synthetic training and evaluation data. Researchers train various transformer architectures (BERT, RoBERTa, DeBERTa, GPT-2, OPT) on this data and compute exact lower bounds on perplexity using specialized parsing algorithms. The approach leverages inside-outside algorithms for masked LMs and Earley-style parsing for causal LMs, enabling efficient computation that was previously intractable for large grammars. The framework also allows direct comparison between learned representations and symbolic grammar rules, facilitating interpretability research.

## Key Results
- Causal language models achieve lower perplexity and better alignment with PCFG distributions than masked language models
- Different word types exhibit distinct learning dynamics during training
- Closed-form expressions enable efficient computation of exact perplexity bounds for large grammars
- Generated data maintains naturalistic properties (Zipfian distributions, selectional preferences) while providing full control over the generative process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closed-form expressions for exact perplexity bounds become tractable by leveraging PCFG structure
- Mechanism: State-split PCFGs allow decomposition of token probabilities into inside/outside terms, avoiding explicit enumeration of all parse trees. This yields O(n³) complexity per sentence instead of exponential.
- Core assumption: The grammar is in Chomsky Normal Form and parse forests can be represented via dynamic programming (inside-outside algorithm)
- Evidence anchors:
  - [section] Derives Eq. 15 using inside/outside probabilities, demonstrating that marginalisation over non-terminals enables efficient masked LM computation
  - [abstract] States that "closed-form expressions to efficiently compute exact lower bounds on perplexity" are a key contribution
- Break condition: If the grammar deviates from CNF or has excessive unary chains, complexity reverts toward exponential

### Mechanism 2
- Claim: Splitting pre-terminal symbols captures word-level dependencies without exploding rule count
- Mechanism: State-splitting creates fine-grained lexical categories tied to syntactic contexts, allowing the grammar to model selectional preferences implicitly through rule probabilities
- Core assumption: Syntactic contexts are sufficient statistics for word co-occurrence patterns relevant to LM evaluation
- Evidence anchors:
  - [section] Notes that open-class pre-terminals lead to more splits, reflecting richer lexical dependencies
  - [corpus] Zipfian rank-frequency plots in Figure 3 show generated data matches natural language statistics, implying effective modelling of word distributions
- Break condition: If semantic context beyond syntax dominates, the grammar will underfit and perplexity bounds will be loose

### Mechanism 3
- Claim: Transformer LM alignment with PCFG distributions correlates strongly with lower perplexity
- Mechanism: Models learn representations that approximate the PCFG's true token conditionals; alignment measured by R² and Spearman's ρ directly predicts perplexity performance
- Core assumption: The LM's training objective gradients push its output distribution toward the true PCFG distribution when data is representative
- Evidence anchors:
  - [section] Table 1 shows that higher Spearman's ρ and R² yield lower perplexity for all tested architectures
  - [abstract] States that causal LMs "approximate the true PCFG distribution more closely than masked language models"
- Break condition: If the model capacity is too low or training data is insufficient, alignment plateaus and perplexity remains above the bound

## Foundational Learning

- Concept: Inside-outside algorithm for PCFG parsing
  - Why needed here: Provides the mathematical machinery to compute exact token probabilities without enumerating all derivations
  - Quick check question: Can you derive the outside probability αⱼ(p,q) in terms of the grammar rules spanning positions outside the span [p,q]?

- Concept: State-splitting procedure
  - Why needed here: Enables compact yet expressive grammars that capture linguistic phenomena without combinatorial explosion
  - Quick check question: Given a non-terminal A with productions A→BC and A→a, how does splitting A into A₁, A₂, ... affect the probability model?

- Concept: Perplexity as a normalised inverse probability
  - Why needed here: Serves as the evaluation metric; understanding its computation is essential to interpret the bounds
  - Quick check question: If a model assigns probability 0.01 to a 10-token sentence, what is its perplexity?

## Architecture Onboarding

- Component map: Grammar induction -> Data generation -> LM training -> Evaluation -> Interpretability probing
- Critical path: Grammar induction -> Data generation -> LM training (masked/causal) -> Perplexity computation -> Correlation analysis
- Design tradeoffs: Large grammars yield tighter bounds but slow parsing; small models train faster but align poorly with PCFG
- Failure signatures: Perplexity significantly above PCFG bound -> underfitting; correlation near zero -> distribution mismatch; Zipf residuals high -> unnatural word frequency
- First 3 experiments:
  1. Train a tiny GPT-2 (2 layers) on a 10k-sentence PCFG corpus; compute perplexity vs PCFG bound; observe alignment
  2. Induce a grammar on a 50k-sentence subset; generate data; compare Zipf residuals to full grammar
  3. Replace inside-outside with naive enumeration on a tiny grammar; measure runtime blowup to validate efficiency claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of parsing framework (state-split PCFGs vs. alternatives like non-parametric Bayesian methods or data-oriented parsing) affect the naturalness and learnability of the generated artificial language data?
- Basis in paper: [explicit] The paper discusses state-split PCFGs as one approach and mentions other frameworks could be explored in future work
- Why unresolved: The paper only uses state-split PCFGs and does not empirically compare to alternative grammar induction methods
- What evidence would resolve it: Systematic experiments training LMs on data generated from different grammar induction frameworks (e.g., non-parametric Bayesian, data-oriented parsing) and comparing their performance and alignment with natural language properties

### Open Question 2
- Question: What specific linguistic cues are lost during the state-split PCFG induction process that result in lower selectional preference and higher perplexity compared to the original treebank?
- Basis in paper: [explicit] The paper notes that state-split PCFG induction leads to a sacrifice of linguistic cues, demonstrated by lower selectional preference and higher perplexity on PCFG-generated data vs. original treebank
- Why unresolved: The paper identifies that cues are lost but does not specify which ones
- What evidence would resolve it: Detailed linguistic analysis comparing the properties of the original treebank vs. PCFG-generated data, potentially using tools like the BLiMP benchmark mentioned in the paper

### Open Question 3
- Question: How does the performance of causal vs. masked language models on PCFG-generated data relate to their performance on natural language, and what does this reveal about their inductive biases?
- Basis in paper: [explicit] The paper finds striking differences in how closely causal vs. masked LMs approximate the PCFG's optimal perplexity, with causal LMs converging more slowly but ultimately performing better
- Why unresolved: While the paper observes these differences on PCFG data, it does not investigate how they translate to natural language performance
- What evidence would resolve it: Training both causal and masked LMs on natural language corpora and comparing their performance and learning dynamics to those observed on PCFG data

## Limitations

- Computational scalability: Inside-outside and Earley-style parsing have cubic time complexity, limiting applicability to very large grammars or longer sequences
- Distributional assumptions: PCFGs may not capture semantic context and long-range discourse phenomena adequately
- Generalization gap: Methodology validated only on artificial data; relationship to natural language performance remains untested

## Confidence

**High confidence**: The mathematical derivation of closed-form expressions for perplexity bounds is sound, and computational efficiency claims are demonstrable through the inside-outside and Earley algorithms.

**Medium confidence**: The claim that state-split PCFGs capture sufficient linguistic structure for meaningful LM evaluation requires more validation, particularly regarding semantic phenomena beyond syntax.

**Low confidence**: The assertion that this provides superior interpretability research capabilities compared to existing benchmarks is primarily conceptual, lacking comparative validation studies.

## Next Checks

1. **Cross-corpus generalization test**: Train the same PCFG on different treebank sources (e.g., Penn Treebank vs. Universal Dependencies) and compare the resulting perplexity bounds and alignment patterns to validate whether findings are corpus-dependent or reflect general properties.

2. **Scaling experiment with larger grammars**: Systematically increase grammar size from current scale to grammars 10× and 100× larger, measuring computational overhead of exact perplexity computation and tightness of bounds to establish practical limits.

3. **Semantic extension validation**: Augment the PCFG with semantic features (e.g., WordNet hypernyms or distributional semantics) and measure whether this improves alignment between generated data and natural language, particularly for semantic phenomena not captured by syntax alone.