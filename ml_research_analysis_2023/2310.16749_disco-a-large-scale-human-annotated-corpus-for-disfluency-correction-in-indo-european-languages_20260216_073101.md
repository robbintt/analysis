---
ver: rpa2
title: 'DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European
  Languages'
arxiv_id: '2310.16749'
source_url: https://arxiv.org/abs/2310.16749
tags:
- language
- disfluency
- disfluent
- english
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DISCO, a large-scale human-annotated corpus
  for disfluency correction in four Indo-European languages: English, Hindi, German,
  and French. The corpus contains over 12,000 disfluent-fluent sentence pairs across
  various domains.'
---

# DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages

## Quick Facts
- arXiv ID: 2310.16749
- Source URL: https://arxiv.org/abs/2310.16749
- Authors: 
- Reference count: 26
- Primary result: Introduces DISCO corpus with over 12,000 disfluent-fluent pairs across four Indo-European languages, achieving high F1 scores (97.55 for English, 94.29 for Hindi, 95.89 for German, 92.97 for French) and improving downstream MT quality by 5.65 BLEU points on average.

## Executive Summary
This paper introduces DISCO, a large-scale human-annotated corpus for disfluency correction across four Indo-European languages: English, Hindi, German, and French. The corpus contains over 12,000 disfluent-fluent sentence pairs and is designed to address the challenge of correcting disfluencies (like fillers, repetitions, and corrections) in spoken language. The authors demonstrate that training state-of-the-art models on DISCO achieves high F1 scores for disfluency correction and leads to significant improvements in downstream machine translation tasks, with an average increase of 5.65 BLEU points.

## Method Summary
The paper presents a methodology for creating and using the DISCO corpus to train disfluency correction models. The approach involves collecting disfluent-fluent sentence pairs across four languages, preprocessing the data for sequence tagging, and training models using conditional random fields, RNNs, and transformer-based architectures. The authors fine-tune multilingual transformers (mBERT, XLM-R, MuRIL) with a classification head for sub-word level binary prediction, and implement Seq-GAN-BERT with adversarial training using helper datasets. The best models achieve high F1 scores for disfluency correction and are integrated with machine translation systems to demonstrate downstream improvements.

## Key Results
- DISCO corpus contains over 12,000 disfluent-fluent sentence pairs across English, Hindi, German, and French
- State-of-the-art models trained on DISCO achieve F1 scores of 97.55 (English), 94.29 (Hindi), 95.89 (German), and 92.97 (French)
- Disfluency correction leads to 5.65 points average increase in BLEU scores when used with machine translation systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing disfluent words improves downstream machine translation quality.
- Mechanism: Disfluencies like fillers, repetitions, and corrections add noise to text that confuses translation models, leading to poor output. Removing these elements produces cleaner input that the MT model can translate more accurately.
- Core assumption: Disfluencies in source text negatively impact translation quality, and removing them will improve BLEU scores.
- Evidence anchors:
  - [abstract]: "DC leads to 5.65 points increase in BLEU scores on average when used in conjunction with a state-of-the-art Machine Translation (MT) system."
  - [section]: "Table 10 summarises our results in both language pairs. DC improves downstream MT for Hindi-English by 6.44 points and for German-English by 4.85 points in BLEU score."
  - [corpus]: The DISCO corpus includes disfluent-fluent sentence pairs that can be used to train disfluency correction models.
- Break condition: If disfluencies are part of the intended meaning or style, removing them might lose important information and reduce translation quality.

### Mechanism 2
- Claim: Large-scale human-annotated datasets enable training of high-accuracy disfluency correction models.
- Mechanism: With sufficient labeled data, transformer models can learn complex patterns of disfluencies across different languages and domains, achieving high F1 scores.
- Core assumption: More training data leads to better model performance for sequence tagging tasks.
- Evidence anchors:
  - [abstract]: "We provide extensive analysis of results of state-of-the-art DC models across all four languages obtaining F1 scores of 97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French)."
  - [section]: "Our best models (fine-tuned multilingual transformers) achieve an F1 score of 97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French)."
  - [corpus]: DISCO contains over 12,000 disfluent-fluent sentence pairs across four languages.
- Break condition: If the data is not representative of real-world disfluencies or if the annotation quality is poor, model performance will suffer.

### Mechanism 3
- Claim: Multilingual training on diverse languages improves generalization of disfluency correction models.
- Mechanism: Training on multiple Indo-European languages with shared linguistic features allows models to learn common disfluency patterns and transfer knowledge across languages.
- Core assumption: Languages within the same family share enough structural similarities that knowledge can transfer between them.
- Evidence anchors:
  - [abstract]: "Towards the goal of multilingual disfluency correction, we present a high-quality human-annotated DC corpus covering four important Indo-European languages: English, Hindi, German and French."
  - [section]: "We experiment with three multilingual transformers: mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020) and MuRIL (Khanuja et al., 2021)."
  - [corpus]: The corpus covers four languages from different branches of Indo-European (Germanic, Romance, Indo-Aryan).
- Break condition: If languages are too dissimilar or if disfluency patterns are language-specific, multilingual training might not provide benefits and could even harm performance.

## Foundational Learning

- Concept: Sequence tagging for disfluency correction
  - Why needed here: Disfluency correction requires identifying which words in a sentence are disfluent, which is a token-level classification task.
  - Quick check question: What label would you assign to the word "um" in the sentence "I want um to go home"? (Answer: Disfluent/1)

- Concept: Multilingual transformer models
  - Why needed here: Models need to handle multiple languages and learn shared representations across them.
  - Quick check question: What advantage does a multilingual transformer have over training separate monolingual models? (Answer: Shared parameters and knowledge transfer between languages)

- Concept: BLEU score calculation for machine translation evaluation
  - Why needed here: BLEU scores measure the quality of machine translation, which is the downstream task being improved by disfluency correction.
  - Quick check question: If a corrected sentence produces a translation that matches the reference translation more closely, what happens to the BLEU score? (Answer: It increases)

## Architecture Onboarding

- Component map: ASR -> Disfluency Correction -> Machine Translation
- Critical path: 1. Collect and annotate disfluent-fluent sentence pairs 2. Preprocess data for sequence tagging 3. Train disfluency correction model 4. Integrate with MT system 5. Evaluate BLEU score improvement
- Design tradeoffs: Single annotator per language vs. inter-annotator agreement, Transformer size vs. training time and resources, Language-specific vs. multilingual models
- Failure signatures: Low F1 scores on test sets indicate poor model performance, BLEU scores not improving suggests disfluency correction isn't helping MT, High false positive rate means model is removing fluent words incorrectly
- First 3 experiments: 1. Train a baseline CRF model on English DISCO data and evaluate F1 score 2. Fine-tune mBERT on all four languages and compare F1 scores 3. Integrate the best DC model with NLLB MT and measure BLEU score improvement on Hindi-English test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do disfluencies in German and French differ from those in English and Hindi in terms of linguistic structure and frequency?
- Basis in paper: [inferred] The paper notes that German and French sentences are shorter on average than English and Hindi, and that certain disfluency types (like false starts) are more challenging to correct in Hindi and German.
- Why unresolved: The paper does not provide a detailed linguistic analysis comparing disfluency patterns across these languages.
- What evidence would resolve it: A cross-linguistic analysis of disfluency types, frequencies, and correction difficulties across all four languages.

### Open Question 2
- Question: What is the impact of multimodal DC (using both speech and text features) on disfluency correction accuracy compared to text-only methods?
- Basis in paper: [explicit] The paper mentions multimodal DC as a promising future direction in the conclusion.
- Why unresolved: The paper only experiments with text-based DC methods and does not explore multimodal approaches.
- What evidence would resolve it: Experiments comparing text-only DC models with multimodal DC models that incorporate speech features.

### Open Question 3
- Question: How do DC models perform on disfluent speech transcripts from low-resource languages, and what challenges arise in adapting existing models to these languages?
- Basis in paper: [inferred] The paper discusses the need for DC datasets in low-resource languages and mentions future work in this area.
- Why unresolved: The paper focuses on Indo-European languages and does not experiment with low-resource languages.
- What evidence would resolve it: Experiments evaluating DC models on disfluent speech transcripts from low-resource languages and analysis of adaptation challenges.

## Limitations

- Dataset size is relatively modest for deep learning models, potentially limiting generalization
- Annotation quality and inter-annotator agreement metrics are not established, raising consistency concerns
- Generalizability of BLEU score improvements is limited to specific language pairs and MT systems tested

## Confidence

- High Confidence: The core claim that disfluency correction can improve machine translation quality is well-supported by experimental results showing 5.65 average BLEU score improvements
- Medium Confidence: F1 scores achieved by transformer models are likely accurate but may be inflated due to small test sets and specific evaluation methodology
- Low Confidence: Claims about multilingual training benefits are not thoroughly validated without ablation studies comparing multilingual vs. monolingual models

## Next Checks

1. **Inter-annotator Agreement Study**: Conduct a formal inter-annotator agreement analysis on a subset of the DISCO corpus across all four languages to establish annotation reliability, particularly for complex disfluency types like clarifications and multi-word false starts.

2. **Cross-Domain Generalization Test**: Evaluate the trained disfluency correction models on test sets from different domains (e.g., news, technical documentation) not represented in the original DISCO corpus to assess domain robustness and identify potential overfitting to conversational data.

3. **Alternative MT System Validation**: Reproduce the BLEU score improvement experiments using different MT systems (e.g., Google Translate API, MarianNMT) and additional language pairs to verify that the improvements are not specific to the NLLB model or the tested language pairs.