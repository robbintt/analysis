---
ver: rpa2
title: Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation
arxiv_id: '2305.09651'
source_url: https://arxiv.org/abs/2305.09651
tags:
- teacher
- student
- distillation
- training
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a common problem in knowledge distillation
  (KD): teacher models with superior performance do not necessarily yield stronger
  students. This is attributed to the misalignment between teacher training practices
  and effective knowledge transfer.'
---

# Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation

## Quick Facts
- arXiv ID: 2305.09651
- Source URL: https://arxiv.org/abs/2305.09651
- Reference count: 34
- Outperforms 10 common KD baselines on GLUE benchmark with average accuracy of 83.4

## Executive Summary
This paper addresses a fundamental challenge in knowledge distillation (KD): teacher models with superior performance don't necessarily produce stronger students due to misalignment between teacher training and effective knowledge transfer. The authors propose Learning Good Teacher Matters (LGTM), a framework that incorporates distillation influence into the teacher's learning process. By quantifying how distilling from each training sample impacts student generalization and reweighting the teacher's loss accordingly, LGTM consistently outperforms 10 KD baselines on 6 GLUE benchmark tasks, achieving an average accuracy of 83.4 compared to the best baseline at 82.5.

## Method Summary
LGTM is a knowledge distillation framework that incorporates distillation influence into teacher training. The method computes per-sample distillation influence using finite difference approximation to estimate how each training sample affects student generalization on validation data. The teacher's loss is then reweighted based on these influences, prioritizing samples that enhance student performance. This approach balances teacher self-evolution with effective knowledge transfer. The framework was evaluated on 6 GLUE benchmark tasks, transferring knowledge from BERT-Base to a 6-layer BERT model.

## Key Results
- LGTM achieves average accuracy of 83.4 on GLUE tasks, outperforming best baseline at 82.5
- The method mitigates student overfitting compared to standard KD approaches
- LGTM enables better balance between teacher self-evolution and knowledge transfer effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distillation influence quantifies how distilling from each training sample impacts the student's generalization ability on validation data.
- Mechanism: For each training sample, compute the gradient similarity between the distillation loss from that sample and the validation loss after student update. Samples with high similarity contribute more to student generalization.
- Core assumption: Gradient similarity between training and validation losses is a valid proxy for sample importance in knowledge transfer.
- Evidence anchors:
  - [abstract] "Distillation influence quantifies how distilling from each training sample impacts the student's generalization ability on validation data."
  - [section 4] "Inspired by the concept of influence function (Pruthi et al., 2020; Koh and Liang, 2017), we propose distillation influence to estimate how much will the student's performance on validation data change if we put one training sample in the knowledge distillation process."
  - [corpus] Weak - no direct corpus evidence for this specific gradient similarity approach.
- Break condition: If gradient similarity becomes an unreliable measure due to training dynamics or optimization instability.

### Mechanism 2
- Claim: Reweighting teacher loss based on distillation influence prioritizes samples that enhance student generalization.
- Mechanism: Assign each training sample a weight equal to its distillation influence, then compute teacher loss as weighted average of sample-wise distillation losses.
- Core assumption: The teacher can learn to focus on samples with higher influence weights without degrading its own performance.
- Evidence anchors:
  - [section 4] "In order to incorporate the per-sample influence into knowledge distillation, we adjust the loss weight of each sample based on its distillation influence."
  - [section 4] "By including the influence in the knowledge distillation loss function, we can tailor the training process to better suit the characteristics of the target task."
  - [corpus] Weak - limited corpus evidence for this specific reweighting strategy.
- Break condition: If reweighting causes teacher to neglect essential learning signals or converge poorly.

### Mechanism 3
- Claim: Finite difference approximation enables efficient computation of per-sample distillation influences.
- Mechanism: Approximate per-sample gradients by computing forward passes with slightly perturbed student parameters, avoiding expensive per-sample backward passes.
- Core assumption: Taylor expansion approximation is sufficiently accurate for influence estimation.
- Evidence anchors:
  - [section 4] "We propose an efficient method for updating the teacher with the distillation influence by utilizing finite difference (Gleich, 2005), a technique commonly used in numerical analysis for approximating the derivative of a function at a given point."
  - [section 4] "Our proposed method for evaluating the finite difference is computationally efficient, as it only requires two forward passes for θs and one backward pass for θt for a single batch."
  - [corpus] Weak - no direct corpus evidence for this specific approximation in KD context.
- Break condition: If approximation error becomes too large relative to influence signal.

## Foundational Learning

- Concept: Influence functions in machine learning
  - Why needed here: LGTM builds directly on influence function theory to measure training sample impact
  - Quick check question: What does an influence function measure in the context of model training?

- Concept: Knowledge distillation objectives and losses
  - Why needed here: Understanding standard KD losses (cross-entropy, KL divergence) is essential for implementing LGTM's modifications
  - Quick check question: How does the standard KD loss differ from the teacher auxiliary loss in LGTM?

- Concept: Finite difference methods for gradient approximation
  - Why needed here: The efficiency of LGTM depends on understanding how finite differences approximate derivatives
  - Quick check question: What is the mathematical basis for using finite differences to approximate gradients?

## Architecture Onboarding

- Component map: Teacher model (BERT-base) -> Student model (BERT-6L) -> Validation set -> Distillation influence computation module -> Weighted loss aggregation

- Critical path: 1. Sample training batch 2. Compute student update 3. Estimate distillation influences via finite difference 4. Reweight teacher loss 5. Update teacher parameters 6. Update student parameters

- Design tradeoffs:
  - Accuracy vs. efficiency in influence approximation
  - Teacher self-evolution vs. knowledge transfer balance
  - Batch size impact on influence stability

- Failure signatures:
  - Student overfitting (validation loss increasing)
  - Teacher performance degradation
  - Influence values becoming unstable or noisy

- First 3 experiments:
  1. Verify influence computation matches analytical gradients on small dataset
  2. Test reweighting strategy on synthetic data with known sample importance
  3. Compare student performance with/without influence-based reweighting on GLUE tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would Learning Good Teacher Matters (LGTM) perform on text generation tasks, which are more complex than text classification?
- Basis in paper: [inferred] The authors acknowledge that their experiments were limited to text classification tasks and suggest that future work should explore the application of LGTM to more complex text generation tasks.
- Why unresolved: The paper does not provide any experimental results or analysis for text generation tasks, leaving the effectiveness of LGTM in these scenarios unknown.
- What evidence would resolve it: Conducting experiments on text generation tasks using LGTM and comparing the results with other knowledge distillation methods would provide evidence of its effectiveness in more complex scenarios.

### Open Question 2
- Question: How would combining LGTM with pre-training knowledge distillation impact the performance of language models?
- Basis in paper: [inferred] The authors mention that while LGTM has demonstrated superior performance in task-specific knowledge distillation, it is worth investigating the potential benefits of combining LGTM with pre-training knowledge distillation.
- Why unresolved: The paper does not explore the combination of LGTM with pre-training knowledge distillation, leaving the potential benefits of this approach unknown.
- What evidence would resolve it: Implementing LGTM in conjunction with pre-training knowledge distillation and evaluating the performance of the resulting models on various tasks would provide evidence of the potential benefits of this combination.

### Open Question 3
- Question: How does the choice of the validation set (existing vs. newly separated from the training set) impact the performance of LGTM?
- Basis in paper: [explicit] The authors investigate the impact of utilizing feedback derived from a new validation set separated from the original training set, but find that the performance is not significantly different from using an existing validation set.
- Why unresolved: The paper does not provide a definitive answer on the impact of the choice of validation set on LGTM's performance, as the results show only minor differences between the two approaches.
- What evidence would resolve it: Conducting more extensive experiments with different sizes of newly separated validation sets and comparing the results with those obtained using existing validation sets would provide a clearer understanding of the impact of the choice of validation set on LGTM's performance.

## Limitations
- Missing complete hyperparameter specifications needed for exact reproduction
- Limited ablation studies on influence approximation accuracy
- Reliance on approximations whose accuracy in high-dimensional transformer models is not validated

## Confidence

**High confidence**: The core mathematical framework of using influence functions for KD is sound and well-established in ML literature

**Medium confidence**: The experimental results showing LGTM outperforming 10 baselines on GLUE benchmarks appear robust, though with limited ablation studies

**Medium confidence**: The claim that reweighting improves knowledge transfer is supported by experiments, but the mechanism's reliability across different model architectures needs more validation

## Next Checks

1. Implement influence function verification: Test whether computed influences match analytical gradients on a small, controlled dataset where ground truth influence is known

2. Conduct ablation study on influence approximation: Compare student performance using exact vs. finite difference approximations across different batch sizes

3. Evaluate teacher degradation: Systematically measure teacher performance degradation across multiple runs with varying influence weight magnitudes