---
ver: rpa2
title: 'MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria'
arxiv_id: '2311.13951'
source_url: https://arxiv.org/abs/2311.13951
tags:
- evaluation
- arxiv
- language
- mllms
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MLLM-Bench, a comprehensive evaluation benchmark
  for multimodal large language models (MLLMs) that addresses the challenge of evaluating
  open-ended, subjective tasks lacking definitive answers. The benchmark spans six
  cognitive levels - Perception, Understanding, Applying, Analyzing, Evaluating, and
  Creation - with 42 capabilities and 420 curated image-instruction pairs, emphasizing
  ethical considerations.
---

# MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria

## Quick Facts
- arXiv ID: 2311.13951
- Source URL: https://arxiv.org/abs/2311.13951
- Reference count: 8
- Primary result: 88.02% agreement with human evaluation when using GPT-4V as judge

## Executive Summary
This paper introduces MLLM-Bench, a comprehensive evaluation benchmark for multimodal large language models (MLLMs) that addresses the challenge of evaluating open-ended, subjective tasks lacking definitive answers. The benchmark spans six cognitive levels - Perception, Understanding, Applying, Analyzing, Evaluating, and Creation - with 42 capabilities and 420 curated image-instruction pairs, emphasizing ethical considerations. To validate the benchmark, 21 popular MLLMs were evaluated in a pairwise-comparison fashion using GPT-4V as the judge, achieving 88.02% agreement with human evaluation. The results show significant performance gaps between existing open-source models and GPT-4V, with the latter outperforming others across all capabilities. The study also analyzes positional and length biases in the evaluation process. MLLM-Bench aims to drive the development of more user-centric and contextually adept vision-language models by providing a more holistic assessment of MLLM capabilities.

## Method Summary
MLLM-Bench evaluates MLLMs using pairwise voting and scoring protocols with GPT-4V and LLaVA as anchor models. The benchmark consists of 420 curated image-instruction pairs across 6 cognitive levels (Perception, Understanding, Applying, Analyzing, Evaluating, Creation) and 42 capabilities. Evaluation involves comparing two model outputs side-by-side and letting the judge pick the better one, sidestepping the need for ground-truth answers in subjective tasks. The system includes bias analysis focusing on positional and length bias, with results aggregated as wins/ties/losses and score ratios per model.

## Key Results
- Achieved 88.02% agreement with human evaluation using GPT-4V as judge
- GPT-4V significantly outperformed all other models across all capabilities
- Identified and analyzed positional and length biases in the evaluation process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can serve as a consistent judge for pairwise comparisons across diverse multimodal tasks by focusing on semantic quality and image alignment
- Mechanism: The evaluation protocol instructs GPT-4V to compare two answers strictly on semantic alignment with the image, ignoring formatting and presentation. This creates repeatable judgments based on content quality
- Core assumption: GPT-4V's internal understanding of image semantics and reasoning is stable enough to serve as a reliable evaluation reference across multiple tasks
- Evidence anchors: [abstract] and [section] references to benchmark design and per-sample criteria
- Break condition: If GPT-4V's internal model shifts or its judgment criteria change over time, the consistency of evaluations will degrade

### Mechanism 2
- Claim: Using pairwise voting with anchor models (GPT-4V and LLaVA) reduces the difficulty of creating ground truth labels for open-ended multimodal tasks
- Mechanism: Instead of defining absolute correctness, the system compares two model outputs side-by-side and lets the judge pick the better one. This sidesteps the need for ground-truth answers in subjective tasks
- Core assumption: Pairwise comparison is easier and more reliable than absolute scoring when ground truth is ambiguous
- Evidence anchors: [abstract] and [section] references to per-sample criteria and benchmark design
- Break condition: If the judge model has strong biases toward certain answer styles, the comparisons will systematically favor or disfavor specific model behaviors

### Mechanism 3
- Claim: The hierarchical taxonomy based on Bloom's cognitive levels ensures broad and balanced coverage of multimodal capabilities
- Mechanism: Tasks are organized into six cognitive levels (Perception to Creation) with 42 capabilities, ensuring that the benchmark probes different reasoning depths and modalities
- Core assumption: Bloom's taxonomy captures the full spectrum of multimodal cognitive tasks relevant to MLLMs
- Evidence anchors: [section] references to Bloom's Taxonomy adoption for the benchmark
- Break condition: If Bloom's levels do not map well to multimodal reasoning, the benchmark may over- or under-weight certain cognitive skills

## Foundational Learning

- Concept: Pairwise comparison evaluation
  - Why needed here: Traditional benchmarks require ground truth answers, which are impossible to define for open-ended multimodal tasks. Pairwise comparison sidesteps this by letting a judge pick the better of two outputs
  - Quick check question: In pairwise voting, if both answers are equally good, what should the judge output?
    - Answer: Tie

- Concept: Anchor-based relative scoring
  - Why needed here: Direct absolute scoring of model outputs is unreliable without ground truth. Using an anchor model as a reference point allows relative performance measurement
  - Quick check question: What is the role of the anchor model in the evaluation protocol?
    - Answer: The anchor model's answer serves as a baseline for comparison, allowing relative ranking of other models

- Concept: Cognitive level taxonomy
  - Why needed here: To ensure the benchmark covers the full range of capabilities MLLMs should possess, from basic perception to creative synthesis
  - Quick check question: Which cognitive level includes tasks like "Visual Storytelling" and "Coding Capability with Vision"?
    - Answer: Creation

## Architecture Onboarding

- Component map: Data collection -> Quality control -> Pairwise evaluation (with anchors) -> Bias analysis -> Result aggregation
- Critical path: 1. Data curation → quality control → pairwise evaluation (with anchors) → bias analysis → result reporting. 2. Any delay in data quality or anchor model performance directly impacts downstream evaluation validity
- Design tradeoffs: Pairwise vs absolute scoring (pairwise avoids ground truth issues but may be less granular), Anchor selection (using a strong anchor ensures discriminative power but may amplify performance gaps), Open-ended prompts (allow realistic assessment but introduce variability in answer formats)
- Failure signatures: Large variance in pairwise results when swapping answer order (→ positional bias), Systematic preference for longer answers (→ length bias), Low inter-model differentiation (→ anchor too strong or benchmark too easy)
- First 3 experiments: 1. Run a small pilot with 10 tasks and swap answer order to check positional bias, 2. Compare GPT-4V pairwise judgments with human judges on 20 samples to measure agreement, 3. Vary the anchor model (e.g., use LLaVA instead of GPT-4V) to see if rankings change significantly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4V as an evaluator compare to human experts in terms of reliability and consistency?
- Basis in paper: [inferred] The paper mentions that the authors will conduct human evaluation to validate how much the GPT-4V evaluator aligns with human experts
- Why unresolved: The paper does not provide the results of the human evaluation, leaving the question of GPT-4V's reliability and consistency compared to human experts unanswered
- What evidence would resolve it: The results of the human evaluation comparing GPT-4V's performance to human experts would provide evidence to answer this question

### Open Question 2
- Question: How does the selection of anchor models affect the ranking and evaluation of other models in the MLLM-Bench?
- Basis in paper: [explicit] The paper discusses the use of anchors in pairwise voting and scoring methods and how it affects the evaluation results
- Why unresolved: While the paper mentions the use of anchors and their impact on evaluation, it does not provide a comprehensive analysis of how different anchor selections might influence the overall rankings and performance assessments of the models
- What evidence would resolve it: A detailed study comparing the results of using different anchor models and their impact on the rankings and evaluations of other models would provide evidence to answer this question

### Open Question 3
- Question: What are the potential biases introduced by the evaluators, and how can they be mitigated to ensure fair and accurate assessments?
- Basis in paper: [explicit] The paper discusses the analysis of positional and length biases in the evaluation process, indicating that biases exist and need to be addressed
- Why unresolved: The paper identifies the presence of biases but does not provide a comprehensive solution or strategy to mitigate these biases effectively
- What evidence would resolve it: A detailed study on the impact of different biases on the evaluation results and the development of strategies to mitigate these biases would provide evidence to answer this question

## Limitations

- The reliability of GPT-4V as a consistent judge across time and tasks remains untested, with no cited precedent for this evaluation paradigm in the corpus
- The pairwise comparison mechanism may introduce biases that favor certain answer styles, though the paper claims to detect and control for positional and length biases
- The Bloom's taxonomy adaptation for multimodal tasks lacks validation that the cognitive levels map appropriately to MLLM capabilities

## Confidence

- High confidence: The benchmark architecture (420 tasks across 6 cognitive levels with pairwise evaluation) is well-specified and reproducible
- Medium confidence: The 88.02% agreement with human evaluation is promising but based on a limited sample size
- Low confidence: The generalizability of GPT-4V as an evaluation judge without ground truth answers

## Next Checks

1. Conduct a longitudinal study to verify GPT-4V maintains consistent judgment criteria over multiple evaluation sessions
2. Expand human evaluation agreement testing to include diverse annotator pools and larger sample sizes
3. Test the benchmark with additional anchor models beyond GPT-4V and LLaVA to validate ranking stability