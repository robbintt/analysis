---
ver: rpa2
title: Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks
  for Wood Quality Classification
arxiv_id: '2310.13490'
source_url: https://arxiv.org/abs/2310.13490
tags:
- feature
- hyperparameter
- wood
- features
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the problem of wood quality classification
  in the sawmill industry, which is traditionally performed by human operators. The
  authors propose a method that combines hyperparameter tuning and feature selection
  for Artificial Neural Networks (ANNs) using Particle Swarm Optimization (PSO).
---

# Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks for Wood Quality Classification

## Quick Facts
- arXiv ID: 2310.13490
- Source URL: https://arxiv.org/abs/2310.13490
- Reference count: 27
- One-line primary result: PSO-based joint feature selection and hyperparameter tuning achieved 0.80 balanced accuracy on wood quality classification

## Executive Summary
This study addresses wood quality classification in the sawmill industry by proposing a method that combines feature selection and hyperparameter tuning for artificial neural networks using Particle Swarm Optimization (PSO). The approach simultaneously optimizes MLP hyperparameters and selects relevant features from wood board images described using GLCM and LBP descriptors. Evaluated on a private dataset of 374 wood board images, the method achieves 0.80 balanced accuracy, outperforming baseline approaches. The research demonstrates that jointly optimizing features and hyperparameters provides significant performance improvements over optimizing either task independently.

## Method Summary
The method employs PSO to jointly optimize the hyperparameters of a Multilayer Perceptron (MLP) neural network and select the most relevant features from GLCM and LBP descriptors. PSO particles encode both continuous hyperparameter values (number of neurons, learning rate, momentum) and binary feature masks. The optimization process uses nested stratified k-fold cross-validation to evaluate fitness based on balanced accuracy. Feature extraction produces 38-dimensional vectors from GLCM (6 measures × 2 angles) and LBP (26 neighbors, radius 3). The PSO algorithm runs for 300 iterations with 30 particles, using a logistic function to binarize positions for feature selection. The best solution is then evaluated on held-out test data.

## Key Results
- Achieved 0.80 balanced accuracy on wood quality classification task
- Outperformed baseline methods including MLP with default parameters, PSO-tuned hyperparameters, and default parameters with selected features
- Demonstrated that joint optimization of features and hyperparameters yields better performance than either task alone
- Showed balanced accuracy is more appropriate than accuracy for imbalanced wood quality datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining feature selection and hyperparameter tuning yields better balanced accuracy than either task alone
- Mechanism: Feature selection removes irrelevant or redundant features, reducing model complexity and noise, while hyperparameter tuning adapts the model architecture to the remaining feature set, allowing better generalization
- Core assumption: The optimal set of features depends on the hyperparameter configuration, and vice versa
- Evidence anchors:
  - [abstract]: "Experimental results suggest that hyperparameters should be adjusted according to the feature set, or the features should be selected considering the hyperparameter values"
  - [section]: "The main goal of combining hyperparameter tuning and feature subset selection is to find the hyperparameter setting λ* ∈ Λ and the feature subset κ* ⊆ x which has the lowest misclassification rate among all HP settings and features subsets"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Weak corpus support for this specific mechanism

### Mechanism 2
- Claim: PSO is effective at simultaneously optimizing continuous (hyperparameters) and discrete (feature selection) search spaces
- Mechanism: PSO particles encode both hyperparameter values and binary feature masks; the velocity update guides exploration of both continuous and combinatorial spaces, with a logistic function mapping continuous velocities to binary decisions
- Core assumption: PSO's adaptive velocity and social learning can balance exploration and exploitation across heterogeneous search spaces
- Evidence anchors:
  - [section]: "Since the particle's decision variables employed for feature selection assume binary values, it is necessary to binarize each position, ψij such that ψij = 1 if s(vij) > r 3 and 0 otherwise"
  - [section]: "The optimization process is performed until meeting the stop criterion, i.e., the maximum number of iterations, which was set in 300"
  - [corpus]: Weak corpus support for this specific PSO adaptation; related works focus on hyperparameter tuning or feature selection separately

### Mechanism 3
- Claim: Balanced accuracy is a more appropriate evaluation metric than accuracy for imbalanced wood quality datasets
- Mechanism: Balanced accuracy averages recall over classes, mitigating the bias toward majority classes present in standard accuracy
- Core assumption: The class distribution in the wood quality dataset is significantly imbalanced, making accuracy misleading
- Evidence anchors:
  - [section]: "Since the dataset used in this work is composed of an imbalanced class distribution, i.e., there is a considerably smaller number of examples labeled as 'C' class. Therefore, measures that acknowledge this imbalance are more suitable for the task"
  - [corpus]: No direct corpus support; related works do not emphasize evaluation metrics

## Foundational Learning

- Concept: Particle Swarm Optimization (PSO) fundamentals
  - Why needed here: PSO drives both hyperparameter tuning and feature selection in a unified framework
  - Quick check question: What are the roles of inertia weight, cognitive and social coefficients in PSO's velocity update equation?

- Concept: Feature extraction using GLCM and LBP
  - Why needed here: These descriptors generate the initial high-dimensional feature space that PSO later prunes
  - Quick check question: How many features are extracted per image from GLCM (6 measures × 2 angles) and LBP (26 neighbors)?

- Concept: Nested stratified cross-validation
  - Why needed here: Provides unbiased performance estimates while tuning hyperparameters and selecting features
  - Quick check question: Why is an inner CV loop required when optimizing hyperparameters on a dataset?

## Architecture Onboarding

- Component map: Raw wood images -> GLCM/LBP feature extraction (38D) -> PSO population (particles with MLP params + binary feature masks) -> Fitness evaluation (inner CV balanced accuracy) -> Outer CV (final balanced accuracy)

- Critical path:
  1. Extract GLCM and LBP features from raw wood board images
  2. Initialize PSO particles with random hyperparameters and feature subsets
  3. Evaluate each particle via nested CV, using validation folds for fitness
  4. Update particle positions/velocities per PSO equations
  5. After convergence, train MLP on full training data with best hyperparameters and features
  6. Test on held-out test set

- Design tradeoffs:
  - PSO iterations (300) vs. computational budget: More iterations may improve solution quality but increase runtime
  - Number of PSO particles (30) vs. exploration: More particles improve coverage but increase evaluations
  - Inner CV folds (10) vs. overfitting risk: More folds give better hyperparameter estimates but cost more evaluations

- Failure signatures:
  - Stagnant PSO fitness over many iterations → premature convergence or poor parameter settings
  - High variance in balanced accuracy across CV folds → unstable model or insufficient data
  - Feature selection masks converging to all zeros or all ones → exploration-exploitation imbalance

- First 3 experiments:
  1. Run M1 (default hyperparameters, all features) to establish baseline performance
  2. Run M2 (PSO-tuned hyperparameters, all features) to isolate hyperparameter effect
  3. Run M3 (default hyperparameters, PSO-selected features) to isolate feature selection effect

## Open Questions the Paper Calls Out
- How does the proposed method compare to state-of-the-art deep learning approaches (e.g., convolutional neural networks) for wood quality classification?
- What is the impact of using different feature extraction techniques beyond GLCM and LBP for wood quality classification?
- How does the proposed method's performance generalize to larger, more diverse datasets of wood images?
- What is the computational efficiency of the proposed method compared to baseline approaches, and how does it scale with dataset size?

## Limitations
- Private dataset of 374 wood board images prevents independent verification and limits generalizability
- PSO implementation details for handling mixed continuous-discrete optimization are described but not fully specified
- Limited comparison against only three baseline methods, lacking comprehensive benchmarking against modern AutoML approaches

## Confidence
- **Medium**: Experimental methodology appears sound with proper nested cross-validation, and the mathematical framework for joint optimization is clearly articulated. However, the limited dataset size and lack of comparison with more recent automated machine learning approaches reduce confidence in the generality of the findings.

## Next Checks
1. Conduct experiments on publicly available wood quality datasets to verify generalizability beyond the private dataset
2. Implement ablation studies testing each mechanism independently (PSO-only feature selection, PSO-only hyperparameter tuning) on the same dataset
3. Compare against state-of-the-art AutoML frameworks like Auto-Sklearn or TPOT to benchmark relative performance improvements