---
ver: rpa2
title: 'HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning'
arxiv_id: '2311.00321'
source_url: https://arxiv.org/abs/2311.00321
tags:
- post
- offensive
- hate
- answer
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HARE, a hate speech detection framework that
  uses large language models (LLMs) to generate step-by-step reasoning explanations,
  addressing the challenge of detecting implicit hate speech. HARE leverages LLM-generated
  rationales via chain-of-thought prompting to fill reasoning gaps in human annotations,
  enabling better supervision of detection models.
---

# HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning

## Quick Facts
- arXiv ID: 2311.00321
- Source URL: https://arxiv.org/abs/2311.00321
- Authors: 
- Reference count: 20
- Key outcome: HARE framework outperforms baselines using human annotations on hate speech detection datasets, with higher accuracy and F1 scores.

## Executive Summary
This paper introduces HARE, a hate speech detection framework that uses large language models (LLMs) to generate step-by-step reasoning explanations, addressing the challenge of detecting implicit hate speech. HARE leverages LLM-generated rationales via chain-of-thought prompting to fill reasoning gaps in human annotations, enabling better supervision of detection models. Experiments on SBIC and Implicit Hate datasets show that HARE consistently outperforms baselines using human annotations, achieving higher accuracy and F1 scores. Analysis reveals that HARE enhances explanation quality, improves generalization to unseen datasets, and aligns more closely with human intuition.

## Method Summary
HARE uses GPT-3.5-turbo with chain-of-thought prompting to generate rationales for hate speech detection. The framework fine-tunes detection models (Flan-T5, T5, GPT-2) on text-classification-rationale tuples extracted from SBIC and Implicit Hate datasets. Two variants are explored: Fr-HARE (using zero-shot CoT prompting) and Co-HARE (incorporating human annotations into CoT prompts). The models are evaluated on both in-domain and cross-dataset performance.

## Key Results
- HARE consistently outperforms baseline methods using human annotations across multiple model sizes and datasets
- Co-HARE rationales align better with human intuition than C+T+I baselines according to GPT-4 pairwise comparisons
- HARE demonstrates improved generalization to unseen datasets (HateXplain, DynaHate) compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated rationales with CoT reasoning fill logical gaps in human annotations, improving detection performance.
- Mechanism: By prompting GPT-3.5-turbo with chain-of-thought instructions that incorporate human-written rationales (target groups and implied statements), the model generates more detailed and logically sequenced explanations that bridge the gap between the input text and the annotations.
- Core assumption: Human annotations contain inherent reasoning gaps that hinder effective supervision of detection models.
- Evidence anchors:
  - [abstract] "Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models."
  - [section 2.2] "Due to a logical gap between the speech P and the annotations T and I, training a model with these annotated rationales does not significantly enhance the model's ability to comprehend hate speech."
- Break condition: If LLM-generated rationales introduce new biases or errors not present in human annotations, or if the CoT prompting fails to generate coherent reasoning sequences.

### Mechanism 2
- Claim: The quality of rationales has a stronger impact on classification performance than simply using human-written annotations.
- Mechanism: Fine-tuning models with LLM-generated rationales (Fr-HARE and Co-HARE) consistently outperforms baselines trained with human annotations (C+T+I), regardless of model size, indicating that better quality explanations lead to better detection.
- Core assumption: More detailed and logically-sequenced explanations improve model understanding of hate speech.
- Evidence anchors:
  - [section 3.2] "Our strategies Fr-HARE and Co-HARE consistently exhibit superior performance over other baseline methods, regardless of the model size."
  - [table 1] Performance metrics showing Fr-HARE and Co-HARE outperform C+T+I across multiple model configurations.
- Break condition: If the performance improvement is due to factors other than rationale quality (e.g., increased training data or different prompting strategies).

### Mechanism 3
- Claim: Co-HARE generates rationales more aligned with human-written annotations by incorporating them into CoT prompts.
- Mechanism: By conditioning the CoT prompt on human-written target groups and implied statements, the LLM generates rationales that follow similar logical structures to human annotations, resulting in better alignment with human intuition.
- Core assumption: Guiding LLM reasoning with human annotations produces more contextually appropriate explanations.
- Evidence anchors:
  - [section 2.2] "Co-HARE integrates human-written rationales about target groups T and implied statements I into the CoT prompt."
  - [section 3.2] "While Fr-HARE and Co-HARE exhibit similar performance, Co-HARE has a slight edge in most cases."
  - [figure 2(b)] GPT-4 pairwise comparison showing Co-HARE rationales align better with human-written labels than C+T+I.
- Break condition: If the conditioning process introduces bias or limits the LLM's ability to generate novel reasoning patterns.

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: Enables LLMs to generate step-by-step reasoning that fills logical gaps in human annotations for hate speech detection.
  - Quick check question: What is the key difference between zero-shot CoT prompting and standard prompting in the context of hate speech detection?

- Concept: Rationale-based supervision
  - Why needed here: Allows detection models to learn not just classification but also the reasoning behind why text constitutes hate speech.
  - Quick check question: How does incorporating rationales into training objectives differ from standard classification training?

- Concept: Dataset bias and generalization
  - Why needed here: Understanding how models trained on SBIC and Implicit Hate perform on unseen datasets (HateXplain, DynaHate) reveals their ability to generalize beyond specific annotation patterns.
  - Quick check question: What does cross-dataset evaluation reveal about the effectiveness of LLM-generated rationales versus human annotations?

## Architecture Onboarding

- Component map: Input text → LLM (GPT-3.5-turbo) with CoT prompts → rationale generation → fine-tuning dataset creation → detection model (Flan-T5, T5, GPT-2) → classification + explanation output

- Critical path: Post → LLM CoT prompting → rationale generation → model fine-tuning → detection and explanation

- Design tradeoffs:
  - Using LLM-generated rationales adds computational cost and potential LLM biases vs. using only human annotations
  - Co-HARE requires human annotations for conditioning vs. Fr-HARE's universal applicability
  - Two-stage extraction process (class + rationale, then class given rationale) vs. single-stage training

- Failure signatures:
  - Detection performance degrades when moving to unseen datasets
  - Generated explanations become incoherent or introduce factual errors
  - Model overfits to specific LLM reasoning patterns rather than learning hate speech concepts

- First 3 experiments:
  1. Implement Fr-HARE baseline: Fine-tune Flan-T5-small with GPT-3.5-turbo-generated rationales using zero-shot CoT prompting
  2. Implement Co-HARE variant: Fine-tune Flan-T5-small with GPT-3.5-turbo-generated rationales using human-annotation-conditioned CoT prompts
  3. Cross-dataset evaluation: Test models trained on SBIC against HateXplain and DynaHate to measure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the HARE framework maintain its effectiveness when applied to other types of harmful content beyond hate speech, such as misinformation or cyberbullying?
- Basis in paper: [inferred] The paper focuses on hate speech detection but discusses the potential for LLM-generated rationales to fill reasoning gaps in human annotations, which could apply to other domains.
- Why unresolved: The paper only evaluates HARE on hate speech datasets (SBIC and Implicit Hate) and does not test its generalizability to other harmful content types.
- What evidence would resolve it: Experiments applying HARE to detect and explain other harmful content types, comparing its performance to existing methods.

### Open Question 2
- Question: How does the performance of HARE compare when using different LLM models (e.g., GPT-4, Claude) as the rationale generator compared to GPT-3.5-turbo?
- Basis in paper: [explicit] The paper uses GPT-3.5-turbo to generate rationales but does not compare its effectiveness against other LLM models.
- Why unresolved: The choice of LLM model could impact the quality and diversity of generated rationales, potentially affecting HARE's performance.
- What evidence would resolve it: Direct comparisons of HARE's performance when using different LLM models for rationale generation, with statistical analysis of the differences.

### Open Question 3
- Question: Can HARE be effectively adapted for real-time hate speech detection on social media platforms, considering the computational resources required for LLM-based rationale generation?
- Basis in paper: [inferred] The paper demonstrates HARE's effectiveness in offline experiments but does not address its feasibility for real-time deployment.
- Why unresolved: The computational cost of generating LLM-based rationales could be prohibitive for real-time applications on large-scale social media platforms.
- What evidence would resolve it: Benchmark tests measuring HARE's latency and resource usage in real-time scenarios, with comparisons to existing real-time detection methods.

## Limitations
- The framework's effectiveness depends heavily on the quality of the underlying LLM's reasoning capabilities and may not generalize across different languages or cultural contexts
- Potential for introduced biases through LLM-generated rationales that weren't present in human annotations
- The paper doesn't thoroughly address adversarial robustness or performance under noisy/ambiguous inputs

## Confidence
- High confidence in empirical performance improvements (accuracy and F1 score increases) on benchmark datasets
- Medium confidence in interpretability claims, as pairwise comparison with GPT-4 is limited
- Medium confidence in generalizability claims across datasets due to limited cross-dataset evaluation scope

## Next Checks
1. **Adversarial Robustness Testing**: Evaluate HARE's performance on intentionally crafted hate speech examples that exploit common LLM reasoning patterns, measuring both detection accuracy and rationale coherence under adversarial conditions.

2. **Cross-Lingual and Cultural Generalization**: Test the framework on non-English datasets and culturally diverse hate speech examples to assess whether LLM-generated rationales maintain their effectiveness across different linguistic and cultural contexts.

3. **Human Interpretability Validation**: Conduct user studies with hate speech experts and laypersons to verify whether the LLM-generated rationales actually improve understanding and trust in the detection decisions, beyond just appearing aligned with human annotations.