---
ver: rpa2
title: 'Federated Learning for Computationally-Constrained Heterogeneous Devices:
  A Survey'
arxiv_id: '2307.09182'
source_url: https://arxiv.org/abs/2307.09182
tags:
- devices
- learning
- training
- federated
- heterogeneity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive analysis of federated learning
  (FL) under computational heterogeneity, focusing on how devices with varying computational
  capabilities can participate effectively in FL systems. The paper categorizes FL
  techniques into two groups: those addressing heterogeneity through neural network
  architecture level adaptations (using methods like dropout, submodels, and distillation)
  and those tackling it at the system level (through client selection, flexible aggregation,
  and asynchronous methods).'
---

# Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey

## Quick Facts
- arXiv ID: 2307.09182
- Source URL: https://arxiv.org/abs/2307.09182
- Reference count: 40
- Primary result: Comprehensive survey analyzing federated learning under computational heterogeneity, categorizing techniques into neural network architecture adaptations and system-level optimizations

## Executive Summary
This survey provides a comprehensive analysis of federated learning (FL) systems operating under computational heterogeneity, where devices with varying computational capabilities must participate effectively. The paper systematically categorizes FL techniques into two main approaches: those addressing heterogeneity through neural network architecture level adaptations (using methods like dropout, submodels, and distillation) and those tackling it at the system level (through client selection, flexible aggregation, and asynchronous methods). The survey identifies key challenges including maintaining effectiveness under fine-grained granularity or large-scale heterogeneity, comparability issues due to inconsistent resource modeling, and unexplored trade-offs in non-IID data scenarios.

## Method Summary
The paper conducts a systematic survey of federated learning techniques for heterogeneous devices, analyzing approaches from both neural network architecture and system-level perspectives. The methodology involves categorizing existing techniques based on their primary mechanism for handling computational heterogeneity, examining their theoretical foundations, and identifying gaps in current research. The survey draws from 40 references to provide a comprehensive overview of the field, with particular focus on how different approaches address the challenges of varying device capabilities in FL systems.

## Key Results
- FedAvg maintains effectiveness under heterogeneity by allowing devices to train only subsets of the full neural network through techniques like Federated Dropout
- Distillation-based approaches enable model architecture heterogeneity by sharing knowledge through soft labels rather than model weights
- System-level approaches address heterogeneity through optimized client selection and aggregation schemes to minimize the impact of stragglers
- Key challenges include maintaining effectiveness under fine-grained granularity or large-scale heterogeneity and ensuring fair contribution from low-resource devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedAvg maintains effectiveness under heterogeneity by allowing devices to train only subsets of the full neural network.
- Mechanism: Techniques like Federated Dropout (FD) and Ordered Dropout (OD) enable devices to select a subset of model parameters to train in each round, reducing the computational burden while preserving the overall model structure on the server.
- Core assumption: Averaging subsets of model weights does not significantly degrade the quality of the aggregated model.
- Evidence anchors:
  - [abstract] "techniques that adapt the NN architecture and works that approach heterogeneity on a system level"
  - [section] "devices can choose from a limited set of model architectures or submodels"
  - [corpus] No direct evidence found in corpus; claims rely on [abstract] and [section] anchors.
- Break condition: If averaging subsets leads to model collapse or significantly worse performance compared to full model averaging.

### Mechanism 2
- Claim: Distillation-based approaches allow for model architecture heterogeneity by sharing knowledge through soft labels rather than model weights.
- Mechanism: Devices train on their private data and generate soft labels for a public dataset, which are then averaged on the server. This allows devices to use different model architectures as long as they share the same output representation.
- Core assumption: Soft labels contain sufficient information to transfer knowledge between different model architectures.
- Evidence anchors:
  - [abstract] "knowledge is not shared through model weights but through soft labels of a public dataset"
  - [section] "FedMD, which utilizes KD for FL, directly addressing the heterogeneous computational capabilities of devices"
  - [corpus] No direct evidence found in corpus; claims rely on [abstract] and [section] anchors.
- Break condition: If the soft labels do not capture the necessary knowledge or if the public dataset is not representative.

### Mechanism 3
- Claim: System-level approaches address heterogeneity by optimizing client selection and aggregation schemes to minimize the impact of stragglers.
- Mechanism: Techniques like FedCS and Oort select devices based on their resource availability and impact on the global model, while asynchronous aggregation allows devices to update the model at any time, reducing the effect of stragglers.
- Core assumption: Devices' resource availability can be accurately estimated and used for client selection.
- Evidence anchors:
  - [abstract] "works that approach heterogeneity on a system level, covering Federated Averaging (FedAvg), distillation, and split learning-based approaches, as well as synchronous and asynchronous aggregation schemes"
  - [section] "Oort, a client selection framework with the aim to optimize convergence speed (w.r.t. time) by allowing for rounds with variable lengths"
  - [corpus] No direct evidence found in corpus; claims rely on [abstract] and [section] anchors.
- Break condition: If the resource estimation is inaccurate or if the selected devices do not contribute meaningfully to the global model.

## Foundational Learning

- Concept: Neural network architectures and their computational requirements
  - Why needed here: Understanding the resource constraints of different devices and how they affect the choice of neural network architecture is crucial for designing effective federated learning systems.
  - Quick check question: What are the key factors that determine the computational requirements of a neural network during training?

- Concept: Federated learning algorithms and their aggregation schemes
  - Why needed here: Familiarity with different federated learning approaches (e.g., FedAvg, FedMD, split learning) and their aggregation mechanisms is essential for understanding how to adapt them to heterogeneous environments.
  - Quick check question: How do the aggregation schemes in FedAvg and FedMD differ, and what are the implications for handling heterogeneity?

- Concept: Device heterogeneity and its sources
  - Why needed here: Recognizing the various sources of heterogeneity (e.g., hardware capabilities, power constraints, data distributions) and their impact on federated learning is crucial for designing effective solutions.
  - Quick check question: What are the main sources of heterogeneity in federated learning environments, and how do they affect the performance of the global model?

## Architecture Onboarding

- Component map:
  Server -> Clients -> Communication -> Resource monitoring

- Critical path:
  1. Server initializes global model and distributes it to clients
  2. Clients train local models on private data using their available resources
  3. Clients upload updates to the server
  4. Server aggregates updates and updates the global model
  5. Server selects clients for the next round based on their resources and impact

- Design tradeoffs:
  - Model complexity vs. resource constraints: Balancing the need for accurate models with the limited computational capabilities of devices
  - Aggregation frequency vs. communication overhead: Determining the optimal frequency of aggregation to minimize communication costs while maintaining model quality
  - Client selection vs. model fairness: Ensuring that all devices have the opportunity to contribute to the global model, even if they have limited resources

- Failure signatures:
  - Slow convergence or poor model quality: Indicates that the current approach is not effectively handling heterogeneity
  - High communication overhead: Suggests that the aggregation frequency or model size is not optimized for the given environment
  - Uneven contribution from devices: May indicate that the client selection or resource monitoring is not accurately capturing devices' capabilities

- First 3 experiments:
  1. Implement FedAvg with a fixed dropout rate and evaluate its performance on a heterogeneous dataset
  2. Compare the effectiveness of FedAvg and FedMD in handling model architecture heterogeneity
  3. Implement a client selection algorithm that accounts for devices' computational capabilities and evaluate its impact on model quality and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum achievable scale and granularity of heterogeneity that federated learning techniques can effectively support while maintaining meaningful contributions from low-resource devices?
- Basis in paper: [explicit] The paper discusses various techniques supporting heterogeneity scales from 4× to 250× but questions whether devices with low resources can make meaningful contributions to the global model, especially in iid settings.
- Why unresolved: Current research shows effectiveness in specific scenarios but lacks comprehensive evaluation of effectiveness across the full range of supported heterogeneity scales and granularities, particularly for low-resource devices.
- What evidence would resolve it: Comparative studies testing federated learning techniques across different heterogeneity scales and granularities, with metrics measuring both convergence speed and meaningful contributions from low-resource devices.

### Open Question 2
- Question: How does non-iid data distribution that correlates with device resource constraints affect federated learning performance, and what mitigation strategies are effective?
- Basis in paper: [explicit] The paper identifies unexplored trade-offs in non-iid data scenarios, particularly when data distribution correlates with device resources, such as outdoor sensors being more constrained and having different data distributions than indoor sensors.
- Why unresolved: Current research primarily focuses on iid scenarios or non-iid scenarios without considering the coupling between data distribution and resource constraints, leaving unknown how this correlation affects model fairness and performance.
- What evidence would resolve it: Empirical studies measuring federated learning performance in scenarios where data distribution correlates with device resource constraints, testing various mitigation strategies for fairness and accuracy.

### Open Question 3
- Question: What are the optimal trade-offs between computation, communication, and memory constraints in federated learning for resource-constrained heterogeneous devices?
- Basis in paper: [explicit] The paper identifies unexplored trade-offs between communication and computation, and between computation and memory, where intermediate results can either be stored or recomputed, which are unexplored in resource-constrained federated learning.
- Why unresolved: Current research focuses primarily on either computation or communication efficiency, with limited exploration of how these constraints interact and what optimal strategies exist for balancing all three constraints simultaneously.
- What evidence would resolve it: Systematic evaluations of federated learning techniques that explicitly model and optimize trade-offs between computation, communication, and memory constraints, identifying Pareto-optimal solutions for different deployment scenarios.

## Limitations
- No consistent model for representing device capabilities across different FL implementations, making direct comparisons challenging
- Effectiveness of techniques under extreme heterogeneity (both fine-grained and large-scale) remains underexplored
- Trade-offs between computational constraints and data heterogeneity (non-IID distributions) have not been thoroughly investigated

## Confidence

**High Confidence**: The categorization of FL techniques into NN architecture-level and system-level approaches is well-supported by existing literature

**Medium Confidence**: Claims about FedAvg's effectiveness under heterogeneity are based on theoretical foundations but lack extensive empirical validation across diverse scenarios

**Low Confidence**: Assertions about distillation-based approaches and split learning in heterogeneous environments are primarily theoretical, with limited practical implementation evidence

## Next Checks
1. Implement a controlled experiment comparing FedAvg with dropout-based approaches (ELFISH/FedDrop) across devices with varying computational capabilities to measure performance degradation
2. Conduct a systematic literature review to quantify the frequency of different resource modeling approaches in existing FL implementations
3. Design a benchmark framework that standardizes device capability representation to enable consistent comparison across different FL techniques