---
ver: rpa2
title: Task-Robust Pre-Training for Worst-Case Downstream Adaptation
arxiv_id: '2306.12070'
source_url: https://arxiv.org/abs/2306.12070
tags:
- tasks
- downstream
- pre-training
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces task-robust pre-training to improve worst-case
  downstream performance by minimizing the maximum expected risk over multiple upstream
  tasks. It proposes softmax weighted gradient descent for efficient minimax optimization
  and provides theoretical convergence guarantees in the convex setting.
---

# Task-Robust Pre-Training for Worst-Case Downstream Adaptation

## Quick Facts
- arXiv ID: 2306.12070
- Source URL: https://arxiv.org/abs/2306.12070
- Authors: 
- Reference count: 40
- Primary result: Task-robust pre-training improves worst-case downstream performance by minimizing maximum expected risk over multiple upstream tasks, achieving 9.2% accuracy improvement on CoLA.

## Executive Summary
This paper introduces task-robust pre-training to address the challenge of worst-case performance across diverse downstream tasks. The approach separates upstream tasks into representative subtasks and applies a minimax loss for pre-training, optimizing for the worst-case expected risk rather than average risk. The authors propose a softmax weighted gradient descent algorithm for efficient minimax optimization and provide theoretical convergence guarantees in the convex setting. Experiments demonstrate significant improvements in worst-case downstream performance on both NLP (GLUE benchmark) and CV (ImageNet) datasets, with the approach particularly effective on challenging tasks.

## Method Summary
The method involves first separating the upstream task into several representative tasks, then applying a minimax loss for pre-training using softmax weighted gradient descent. This optimization algorithm uses softmax-type weights to approximate the subgradient in minimax optimization, avoiding non-differentiability while maintaining computational efficiency. The pre-trained model is then adapted to downstream tasks using standard fine-tuning protocols. For NLP tasks, the approach uses a Part-of-Speech Mask BERT model, while for CV tasks it employs a Multi-Modal Mask MAE approach. The key innovation is optimizing for worst-case expected risk across upstream tasks rather than average risk, which leads to better initialization for challenging downstream tasks.

## Key Results
- On the CoLA task, worst-case performance improved by 9.2% accuracy compared to average-risk minimization
- The approach outperforms average expected risk minimization across all GLUE tasks, with particular gains on the most challenging tasks
- Theoretical analysis shows that fewer samples are required for the hardest downstream task when initialized with the minimax-pre-trained model
- The softmax weighted gradient descent algorithm converges in the convex setting with theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing worst-case expected risk over upstream tasks leads to better worst-case downstream performance
- Mechanism: The minimax pre-training approach ensures the foundation model performs well on the most challenging downstream tasks by optimizing for the worst-case scenario across upstream tasks
- Core assumption: The loss functions of downstream tasks are convex combinations of the loss functions of upstream tasks
- Evidence anchors:
  - [abstract]: "We call this goal as downstream-task robustness. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training."
  - [section 5]: "We consider a simplification of the model and the task relationship. Such a simplification makes our analysis convenient and intuitive."
  - [corpus]: Weak evidence - no direct citations found, but related work on minimax optimization in deep learning exists
- Break condition: If the downstream tasks are not related to the upstream tasks or if the loss functions are not convex combinations

### Mechanism 2
- Claim: Softmax weighted gradient descent provides an efficient approximation of the subgradient for minimax optimization
- Mechanism: By using softmax-type weights, the algorithm avoids non-differentiability caused by the maximum operator while maintaining computational efficiency
- Core assumption: The softmax weighted gradient is a good approximation for the subgradient in minimax optimization
- Evidence anchors:
  - [section 3]: "The motivation behind softmax weighted gradient descent is to use the softmax weighted gradient to approximate the subgradient in the classic subgradient descent for the minimax optimization of (6)."
  - [section 3]: "One advantage of the softmax approximation is that it avoids the non-differentiability caused by the maximum operator via the softmax approximation, making the algorithm easily implementable for pre-training applications."
  - [corpus]: Weak evidence - related work on softmax in optimization exists but not specifically for this application
- Break condition: If the softmax approximation becomes too loose or if the problem becomes too large for the computational efficiency to matter

### Mechanism 3
- Claim: Good initialization through minimax pre-training serves as implicit regularization for downstream tasks
- Mechanism: By finding a parameter that minimizes worst-case expected risk, the initialization places the model closer to optimal solutions for challenging downstream tasks
- Core assumption: The parameter space for downstream tasks is constrained by the initial parameter and the strongly-convex nature of the loss functions
- Evidence anchors:
  - [section 5]: "We then illustrate that a good initialization can serve as an implicit regularization. We suppose that the downstream tasks are trained with gradient descent."
  - [section 5]: "Proposition 5.3. Suppose that a function f : Rd → R is µf-strongly-convex and Lf-smooth for all x ∈ Rd and x∗ ∈ arg minx∈Rd f(x)."
  - [corpus]: Weak evidence - implicit regularization is a known concept but not specifically applied in this way to pre-training
- Break condition: If the loss functions are not strongly-convex or if the downstream tasks are not related to the upstream tasks

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: DRO provides the theoretical foundation for minimizing worst-case expected risk, which is the core idea behind task-robust pre-training
  - Quick check question: What is the difference between minimizing average expected risk and minimizing worst-case expected risk in the context of pre-training?

- Concept: Strongly-convex and smooth functions
  - Why needed here: The theoretical analysis relies on these properties to prove convergence rates and characterize sample complexity
  - Quick check question: Why are strongly-convex and smooth functions important for the convergence analysis of optimization algorithms?

- Concept: Covering numbers and sample complexity
  - Why needed here: The analysis of how many samples are needed for downstream tasks relies on covering number arguments to bound the generalization error
  - Quick check question: How do covering numbers relate to the sample complexity of finding an approximately optimal parameter in a constrained parameter space?

## Architecture Onboarding

- Component map:
  Upstream tasks -> Softmax weighted gradient descent -> Downstream tasks -> Parameter space

- Critical path:
  1. Separate the upstream task into representative tasks
  2. Apply minimax loss for pre-training using softmax weighted gradient descent
  3. Prove convergence in the convex setting
  4. Adapt the foundation model to downstream tasks
  5. Compare performance with average expected risk minimization

- Design tradeoffs:
  - Computational efficiency vs. approximation accuracy in softmax weighted gradient descent
  - Number of upstream tasks vs. representativeness of the worst-case scenario
  - Strong convexity assumptions vs. practical applicability to non-convex deep learning models

- Failure signatures:
  - Poor performance on downstream tasks despite good upstream task performance
  - Convergence issues in softmax weighted gradient descent
  - Increased sample complexity for downstream tasks compared to average expected risk minimization

- First 3 experiments:
  1. Compare minimax pre-training with average expected risk minimization on a simple convex problem
  2. Evaluate the effect of different numbers of upstream tasks on downstream performance
  3. Test the robustness of the approach to violations of the strongly-convex assumption in practice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the softmax weighted gradient descent algorithm perform in non-convex settings, particularly in large-scale deep learning models?
- Basis in paper: [inferred] The paper provides convergence analysis for the softmax weighted gradient descent algorithm in the convex setting and mentions that it works well in non-convex models, but does not provide detailed analysis or empirical evidence for non-convex cases.
- Why unresolved: The paper's convergence analysis is limited to the convex setting, and the non-convex case is only briefly mentioned without further elaboration or experimental validation.
- What evidence would resolve it: Conducting experiments with large-scale deep learning models and providing convergence analysis for non-convex settings would help resolve this question.

### Open Question 2
- Question: How does the choice of upstream tasks affect the downstream-task robustness of the pre-trained model?
- Basis in paper: [explicit] The paper mentions that the choice of upstream tasks allows incorporating prior knowledge of the domain and problems, but does not provide a detailed analysis of how different upstream task choices impact downstream-task robustness.
- Why unresolved: The paper does not explore the relationship between upstream task selection and downstream-task robustness in depth, leaving the question open for further investigation.
- What evidence would resolve it: Conducting experiments with various upstream task choices and analyzing their impact on downstream-task robustness would help answer this question.

### Open Question 3
- Question: How does the softmax weighted gradient descent algorithm compare to other minimax optimization algorithms in terms of computational efficiency and effectiveness?
- Basis in paper: [inferred] The paper introduces the softmax weighted gradient descent algorithm as a simple and efficient alternative to other minimax optimization algorithms, but does not provide a direct comparison of their performance.
- Why unresolved: The paper does not compare the softmax weighted gradient descent algorithm to other minimax optimization algorithms, making it unclear how it fares in terms of computational efficiency and effectiveness.
- What evidence would resolve it: Conducting experiments comparing the softmax weighted gradient descent algorithm to other minimax optimization algorithms in terms of computational efficiency and effectiveness would help resolve this question.

## Limitations

- The theoretical analysis relies on strong assumptions about convexity and smoothness that are not typically satisfied in practical deep learning scenarios
- The relationship between downstream task loss functions and upstream task loss functions is assumed but not empirically validated
- The quality of the softmax approximation and its degradation as the number of upstream tasks increases is not theoretically or empirically analyzed

## Confidence

- **High Confidence**: The experimental results showing improved worst-case performance over average-risk minimization on the GLUE and ImageNet datasets
- **Medium Confidence**: The convergence guarantees in the convex setting, though limited by strong assumptions
- **Low Confidence**: The claim that good initialization serves as implicit regularization for downstream tasks, supported only by theoretical analysis under strong assumptions

## Next Checks

1. Conduct empirical validation to measure how closely downstream task loss functions can be represented as convex combinations of upstream task loss functions, validating a core theoretical assumption.

2. Systematically evaluate how the quality of the softmax approximation degrades as the number of upstream tasks increases, and measure the impact on both convergence speed and final performance.

3. Implement and test the softmax weighted gradient descent algorithm on non-convex deep learning models (e.g., standard BERT or ResNet architectures) to assess how well the theoretical insights translate to practical settings.