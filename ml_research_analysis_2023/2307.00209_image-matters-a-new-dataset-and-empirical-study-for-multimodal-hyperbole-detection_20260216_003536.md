---
ver: rpa2
title: 'Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole
  Detection'
arxiv_id: '2307.00209'
source_url: https://arxiv.org/abs/2307.00209
tags:
- hyperbole
- image
- text
- multimodal
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal hyperbole detection, creating a
  new Chinese dataset from Weibo. It analyzes how text and images contribute to hyperbole,
  finding images mostly assist rather than convey hyperbole directly.
---

# Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection

## Quick Facts
- arXiv ID: 2307.00209
- Source URL: https://arxiv.org/abs/2307.00209
- Reference count: 10
- Primary result: Images mostly assist rather than convey hyperbole directly in multimodal detection

## Executive Summary
This paper addresses multimodal hyperbole detection by creating a new Chinese dataset from Weibo posts. The study analyzes how text and images contribute to hyperbole detection, finding that images mostly assist rather than convey hyperbole directly. Through empirical comparison of fusion methods including BERT, ResNet, concatenation, gating, and attention-based approaches, the research demonstrates that image modality is useful for detection. The study also reveals that pre-trained multimodal encoders like CLIP and BriVL perform poorly, likely due to weak text-image alignment and abstract concepts in hyperbole. Cross-domain experiments expose systematic keyword-based biases that limit generalization, especially for deep fusion methods.

## Method Summary
The research creates a Chinese Weibo dataset with 2,160 balanced hyperbole/non-hyperbole posts, analyzing five disjoint keyword domains. Text is processed with Chinese BERT/Cui et al., images with ResNet50, and various fusion methods (concat, gating, attention-based) are compared. The study employs 10-fold cross-validation with consistent hyperparameters and evaluates pre-trained multimodal encoders (CLIP, BriVL) alongside custom architectures. Cross-domain performance is tested by training on three keywords, validating on one, and testing on another, rotating keywords systematically.

## Key Results
- Image modality is useful for hyperbole detection, not misleading, with simple concatenation improving performance
- Gating mechanisms help avoid background noise by dynamically weighting text and image features
- Attention-based fusion enables fine-grained cross-modal understanding, particularly for complex hyperbole cases
- Pre-trained multimodal encoders (CLIP, BriVL) perform poorly, likely due to weak text-image alignment and abstract concepts
- Cross-domain experiments reveal systematic keyword-based biases that limit generalization, especially for deep fusion methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Images are more useful than misleading for hyperbole detection when combined with text
- Mechanism: Text and image modalities provide complementary information - text often contains hyperbolic expressions while images can provide factual context or visual cues that confirm or reveal hyperbole
- Core assumption: Most images are not hyperbolic themselves but can assist in identifying hyperbolic text
- Evidence anchors:
  - [abstract]: "Empirical studies compare fusion methods: unimodal BERT and ResNet, simple concatenation, gating, and attention-based fusion. Results show image modality is useful, not misleading"
  - [section 5.1]: "With single modality, text-only model (BERT) performs much better than image-only model (ResNet)... introducing image modality, even simply giving it the same significance as text modality (concat), still apparently increases the performance"
  - [corpus]: Weak - corpus mentions multimodal detection but lacks specific evidence about image utility
- Break condition: If images consistently contain irrelevant or misleading information that outweighs their assistive value

### Mechanism 2
- Claim: Gating mechanism improves performance by reducing background noise
- Mechanism: Text and image features are weighted dynamically, allowing the model to suppress features that serve only as background information
- Core assumption: Some features from both modalities serve only as background information without contributing to hyperbole detection
- Evidence anchors:
  - [abstract]: "gating helps avoid background noise"
  - [section 5.1]: "Gating mechanism is important to increase model performance... Since under most circumstances, there are probably some features serving as background information, we should give these features less focus"
  - [section 5.1]: "Comparing with BERT, this improvement is statistically significant"
- Break condition: If gating suppresses useful features or if background noise is minimal

### Mechanism 3
- Claim: Attention mechanism improves fine-grained cross-modal understanding
- Mechanism: Attention identifies specific relationships between text tokens and image patches, capturing case-II and case-III hyperbole expressions
- Core assumption: Hyperbolic meaning often emerges from specific interactions between text and image elements
- Evidence anchors:
  - [abstract]: "attention improves fine-grained cross-modal understanding"
  - [section 5.1]: "Attention is a widely used method which can automatically find out which part of the feature representation is more important... attn-gate is able to extract fine-grained features between two modalities"
  - [section 5.1]: "Figure 5a gives an example. All other methods except attn-gate failed on this case"
- Break condition: If attention focuses on irrelevant cross-modal pairs or if hyperbole is primarily text-based

## Foundational Learning

- Concept: Modality fusion techniques
  - Why needed here: The paper compares different ways to combine text and image information for hyperbole detection
  - Quick check question: What are the three main fusion approaches tested in the paper?

- Concept: Cross-domain generalization
  - Why needed here: The paper evaluates model performance across different topics (keywords) to test generalization
  - Quick check question: Why does performance degrade when testing on cross-domain data?

- Concept: Pre-trained multimodal models
  - Why needed here: The paper tests CLIP and BriVL on the hyperbole detection task to evaluate their effectiveness
  - Quick check question: Why do pre-trained multimodal models perform poorly on this task?

## Architecture Onboarding

- Component map: Text encoder (BERT/Cui et al.) → Image encoder (ResNet50) → Fusion module (concat/gate/attn-gate) → Classification layers
- Critical path: Text → Image → Fusion → Classification
- Design tradeoffs: Simple concatenation is faster but less effective than attention-based fusion; gating adds complexity but reduces noise
- Failure signatures: Poor performance on case-II and case-III hyperbole (requiring cross-modal understanding); failure on abstract concepts; systematic keyword biases
- First 3 experiments:
  1. Test unimodal BERT and ResNet performance separately to establish baseline
  2. Test simple concatenation of BERT and ResNet outputs
  3. Test attention-based fusion with gating to evaluate fine-grained cross-modal understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the true effectiveness of pre-trained multimodal models like CLIP and BriVL for hyperbole detection?
- Basis in paper: [explicit] The paper explicitly tests CLIP and BriVL on this task and finds they perform significantly worse than simple fusion methods, suggesting they are ineffective for hyperbole detection.
- Why unresolved: While the paper shows poor performance, it doesn't fully explore why these models fail. It mentions possible reasons like abstract concepts and weak text-image correlation but doesn't systematically investigate alternative prompts, fine-tuning strategies, or model architectures that might improve performance.
- What evidence would resolve it: Systematic ablation studies testing different prompt strategies