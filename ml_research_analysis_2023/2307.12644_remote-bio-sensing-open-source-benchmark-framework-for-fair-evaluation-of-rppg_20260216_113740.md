---
ver: rpa2
title: 'Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of
  rPPG'
arxiv_id: '2307.12644'
source_url: https://arxiv.org/abs/2307.12644
tags:
- ubfc
- rppg
- pure
- evaluation
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive open-source benchmark framework
  for fair evaluation of remote photoplethysmography (rPPG) methods. The framework
  addresses key challenges in rPPG research including dataset bias, reproducibility
  issues, and lack of standardized evaluation protocols.
---

# Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG

## Quick Facts
- arXiv ID: 2307.12644
- Source URL: https://arxiv.org/abs/2307.12644
- Reference count: 40
- Key outcome: Comprehensive open-source benchmark framework for fair evaluation of remote photoplethysmography (rPPG) methods

## Executive Summary
This paper introduces a comprehensive open-source benchmark framework for fair evaluation of remote photoplethysmography (rPPG) methods. The framework addresses key challenges in rPPG research including dataset bias, reproducibility issues, and lack of standardized evaluation protocols. By providing preprocessing techniques, dataset analysis tools, implementations of both conventional non-DNN and deep learning methods, and standardized evaluation metrics, the benchmark enables researchers to compare rPPG algorithms on equal footing. The framework was tested on multiple datasets including UBFC and PURE, demonstrating its ability to facilitate fair comparisons across different rPPG algorithms.

## Method Summary
The benchmark framework consists of five main layers: Dataset Analysis, Preprocessing, Methods, Train and Evaluation, and Application. The Dataset Analysis layer handles skin tone analysis using Fitzpatrick scale classification and dataset structure. The Preprocessing layer implements four methods: DiffNorm, Z-score normalization, Spatio-Temporal maps (STmap), and Raw data processing. The Methods layer contains implementations of both non-DNN methods (Green, ICA, PCA, CHROM, PBV, POS, SSR, LGI) and DNN methods (DeepPhys, MTTS, EfficientPhys, BigSmall, PhysNet, PhysFormer, APNET, RhythmNet). The Train and Evaluation layer handles model training and standardized metrics including MAE, RMSE, MAPE, and Pearson correlation. The Application layer provides demos and examples for using the framework.

## Key Results
- Framework successfully implemented on UBFC and PURE datasets with standardized preprocessing and evaluation protocols
- Demonstrated ability to compare both non-DNN and DNN methods using consistent metrics across diverse datasets
- Identified skin tone bias issues through Fitzpatrick scale analysis of dataset demographics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework addresses reproducibility issues by providing standardized preprocessing and evaluation protocols.
- Mechanism: The preprocessing layer standardizes data preparation across diverse datasets, while the evaluation layer implements consistent metrics (correlation, RMSE, MAPE, etc.) that enable fair comparisons between methods.
- Core assumption: Different preprocessing approaches and evaluation metrics significantly impact rPPG performance comparison.
- Evidence anchors:
  - [abstract] "lack available code or reproducibility, making it difficult to fairly evaluate and compare performance"
  - [section 2] "Our goal is to tackle the problem by implementing and openly sharing as many rPPG methods as possible"
  - [corpus] Weak - related papers focus on method improvements rather than reproducibility frameworks
- Break condition: If preprocessing standardization fails to account for dataset-specific characteristics, performance comparisons may still be biased.

### Mechanism 2
- Claim: The dataset analysis layer addresses skin color bias in rPPG research.
- Mechanism: The framework implements Fitzpatrick scale classification to identify skin tone distributions in datasets, helping researchers understand potential bias sources in their evaluations.
- Core assumption: Skin tone significantly affects rPPG signal quality and algorithm performance.
- Evidence anchors:
  - [section 3.1.1] "The degree of variation in reflected light due to light absorption is influenced by the thickness of the skin and the content of melanin"
  - [section 3.1.1] "To address this issue, when evaluating the performance of algorithms using the PPG labels, it is important to take into consideration the mismatch that can occur between the HR labels and the PPG labels derived from datasets"
  - [corpus] Weak - related papers mention skin tone challenges but lack systematic bias analysis frameworks
- Break condition: If Fitzpatrick classification fails to capture relevant skin color variations affecting rPPG performance.

### Mechanism 3
- Claim: The comprehensive method implementation enables fair comparison across different rPPG approaches.
- Mechanism: The framework implements both non-DNN and DNN methods with consistent evaluation, allowing direct performance comparison across different methodological approaches.
- Core assumption: Different rPPG methods (traditional vs deep learning) can be fairly compared using the same evaluation metrics and datasets.
- Evidence anchors:
  - [abstract] "provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods"
  - [section 3.3] "Our goal is to tackle the problem by implementing and openly sharing as many rPPG methods as possible"
  - [section 4] "We provide evidence of the reproducibility of our code and present benchmark results for a fair evaluation of each model"
- Break condition: If methodological differences between approaches (e.g., input requirements, training needs) prevent meaningful comparison.

## Foundational Learning

- Concept: Remote photoplethysmography principles
  - Why needed here: Understanding how rPPG extracts physiological signals from facial videos is essential for implementing preprocessing and evaluation methods
  - Quick check question: How does hemoglobin absorption at different wavelengths affect rPPG signal quality?

- Concept: Deep learning model architectures for temporal data
  - Why needed here: Many rPPG methods use temporal models (CNNs, Transformers) that require understanding of 3D convolutions and attention mechanisms
  - Quick check question: What is the difference between spatial and temporal feature extraction in video-based physiological measurement?

- Concept: Signal processing and spectral analysis
  - Why needed here: rPPG methods rely on frequency analysis to extract heart rate from pulsatile signals, requiring knowledge of FFT and filtering techniques
  - Quick check question: How does the choice between FFT and peak detection affect heart rate estimation accuracy?

## Architecture Onboarding

- Component map: Dataset Analysis -> Preprocessing -> Methods -> Train and Evaluation -> Application
- Critical path: For evaluating a new rPPG method, the critical path involves: 1) preprocessing the dataset using the standardized pipeline, 2) running the method through the Methods layer, 3) training if applicable via the Train and Evaluation layer, and 4) generating evaluation metrics for comparison.
- Design tradeoffs: The framework trades off flexibility for standardization - while it provides standardized preprocessing and evaluation, it may not accommodate all possible methodological variations. The choice between different preprocessing methods (DiffNorm vs Raw) represents a key tradeoff in signal quality versus computational efficiency.
- Failure signatures: Common failure modes include: 1) preprocessing artifacts that degrade signal quality, 2) evaluation metrics that don't capture method-specific strengths, 3) skin tone analysis that fails to identify relevant biases, and 4) method implementations that don't generalize across datasets.
- First 3 experiments:
  1. Run the framework's baseline evaluation on the UBFC dataset using the default preprocessing settings to verify the basic pipeline works
  2. Compare the performance of different preprocessing methods (DiffNorm vs Z-score vs Raw) on the same dataset to understand preprocessing impact
  3. Evaluate a single non-DNN method (e.g., CHROM) and a single DNN method (e.g., DeepPhys) on both UBFC and PURE datasets to test cross-dataset generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we create a standardized dataset collection process that accounts for diverse skin tones and lighting conditions?
- Basis in paper: [explicit] The paper mentions that existing datasets often lack Fitzpatrick scale labels and discusses the importance of measuring skin type for fair evaluation
- Why unresolved: Current datasets have inconsistent labeling and collection protocols, making it difficult to ensure fair representation across different demographics
- What evidence would resolve it: Development and validation of a standardized protocol for dataset collection that includes comprehensive skin tone measurement and controlled lighting conditions

### Open Question 2
- Question: What is the optimal preprocessing method that balances computational efficiency with signal quality across diverse real-world conditions?
- Basis in paper: [explicit] The paper discusses various preprocessing methods (DiffNorm, Z-score normalize, STmap, Raw) but doesn't determine which performs best across all scenarios
- Why unresolved: Different preprocessing methods show varying performance depending on dataset characteristics, and there's no clear winner for all situations
- What evidence would resolve it: Comprehensive benchmarking of preprocessing methods across diverse datasets with varying noise conditions and demographic factors

### Open Question 3
- Question: How can we develop more robust evaluation metrics that better reflect real-world performance of rPPG systems?
- Basis in paper: [explicit] The paper mentions current evaluation metrics (Correlation, Bland-Altman, RMSE, MAE, MAPE, SNR) but notes discrepancies between PPG labels and actual measurements
- Why unresolved: Existing metrics may not fully capture the practical usability and reliability of rPPG systems in real-world applications
- What evidence would resolve it: Development and validation of new evaluation metrics that better account for practical deployment challenges and user experience

## Limitations
- Limited to RGB video data, excluding other modalities like thermal or depth cameras
- Framework coverage may not include all existing rPPG methods or dataset types
- Evaluation metrics may not fully capture practical deployment challenges in real-world scenarios

## Confidence

High confidence in framework's ability to provide standardized preprocessing and evaluation protocols.
Medium confidence in claims about addressing skin color bias through Fitzpatrick classification.
Low confidence in claims about fair comparison across all methodological approaches.

## Next Checks

1. Validate the preprocessing pipeline by comparing output signal quality metrics across different preprocessing methods on the same dataset
2. Test framework reproducibility by running the same method configuration on different hardware and comparing results
3. Evaluate the framework's ability to detect known performance differences by testing methods with established superiority on controlled datasets