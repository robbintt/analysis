---
ver: rpa2
title: On Regression in Extreme Regions
arxiv_id: '2303.03084'
source_url: https://arxiv.org/abs/2303.03084
tags:
- extreme
- regression
- conditional
- risk
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a statistical learning framework for regression
  in extreme regions of the input space. The core idea is to perform nonparametric
  least squares regression on a subsample of extreme observations, focusing on their
  angular components.
---

# On Regression in Extreme Regions

## Quick Facts
- arXiv ID: 2303.03084
- Source URL: https://arxiv.org/abs/2303.03084
- Reference count: 28
- One-line primary result: Empirical risk minimization on extreme observations yields angular predictors with good generalization capacity in extreme regions

## Executive Summary
This paper develops a statistical learning framework for regression in extreme regions of the input space, where observations have large norms. The core idea is to perform nonparametric least squares regression on a subsample of extreme observations, focusing on their angular components. Under conditional regular variation assumptions, the authors establish that the optimal predictive function depends only on the angular component of the input, and prove that empirical risk minimization on extreme observations yields functions with good generalization capacity.

## Method Summary
The method performs regression in extreme regions by first standardizing marginal distributions to unit-Pareto, then selecting the k largest observations by norm, transforming inputs to their angular components on the unit sphere, and finally minimizing empirical quadratic risk over a hypothesis class of angular functions. The approach is nonparametric and can be combined with any regression method, though the theoretical analysis focuses on least squares regression. The key innovation is shifting focus from radial information to angular components under regular variation assumptions.

## Key Results
- Angular-based regression functions achieve asymptotic optimality in extreme regions under conditional regular variation
- Empirical risk minimization on extreme observations yields finite-sample risk bounds with clear bias-variance decomposition
- Marginal standardization preserves the theoretical framework while allowing analysis to focus on angular components
- Numerical experiments show improved performance in extreme regions compared to standard regression methods

## Why This Works (Mechanism)

### Mechanism 1
Angular-based regression functions achieve asymptotic optimality in extreme regions. Under conditional regular variation, the optimal predictive function depends only on the angular component of the input. Empirical risk minimization on extreme observations yields a function that converges to the optimal angular predictor. This relies on the conditional distribution of X given Y belonging to the maximal domain of attraction of a multivariate extreme value distribution. Break condition: If the optimal predictor depends significantly on radial components beyond asymptotic regions.

### Mechanism 2
Non-asymptotic risk bounds with clear bias-variance decomposition are achieved by using a fraction k/n of the largest observations. The empirical risk minimizer achieves excess risk bounds that decompose into stochastic error (O(1/√k)), model bias, and approximation bias. This requires the hypothesis class H to have finite VC dimension and be uniformly bounded. Break condition: If k is too small relative to n or if the hypothesis class is too restrictive or too complex.

### Mechanism 3
Marginal standardization preserves the theoretical framework by transforming marginal distributions to unit-Pareto while maintaining the regular variation property. This allows the analysis to focus on angular components without loss of generality. Break condition: If marginal transformations must be estimated, introducing additional statistical error.

## Foundational Learning

- **Concept**: Multivariate regular variation and conditional regular variation
  - Why needed here: Provides theoretical foundation for why angular components contain all relevant information for prediction in extreme regions
  - Quick check question: What does it mean for a distribution to be multivariate regularly varying, and how does this relate to the convergence of normalized vectors to a spectral measure on the sphere?

- **Concept**: Empirical risk minimization (ERM) and VC dimension
  - Why needed here: The approach uses ERM on a truncated sample of extreme observations, and finite VC dimension is required for generalization bounds
  - Quick check question: How does the VC dimension of a function class relate to uniform convergence of empirical risks, and why is this important for the non-asymptotic analysis?

- **Concept**: Concentration inequalities and empirical processes
  - Why needed here: Proving uniform convergence of empirical conditional risks over the hypothesis class requires concentration bounds for dependent statistics
  - Quick check question: What type of concentration inequality is needed when the empirical risk is an average of dependent random variables (ordered by norm)?

## Architecture Onboarding

- **Component map**: Data preprocessing (marginal standardization) → Extreme observation selection (truncation) → Angular transformation → Empirical risk minimization → Output angular regression function
- **Critical path**: The transformation from raw data to standardized form, selection of k extreme observations, computation of angular components, and optimization over the hypothesis class H
- **Design tradeoffs**: Larger k reduces stochastic error but increases approximation bias; smaller k captures more extreme regime but increases variance
- **Failure signatures**: Poor performance when true optimal predictor depends significantly on radial information, or when standardization estimates are poor
- **First 3 experiments**:
  1. Verify Assumption 2 holds for synthetic data by checking convergence of normalized extreme observations to spectral measure
  2. Test sensitivity to k by plotting MSE vs k for various sample sizes and comparing to theoretical O(1/√k) rate
  3. Compare angular regression against standard regression on datasets where extremes follow different angular distributions

## Open Questions the Paper Calls Out

### Open Question 1
How can we theoretically quantify the bias error terms in Corollary 2's bound that arise from substituting the conditional quadratic risk for its asymptotic limit? The paper mentions that extending the conditional multivariate regular variation property and specifying second-order conditions would be required, but this extension is beyond the scope of the current paper.

### Open Question 2
What are the optimal strategies for choosing the number of extreme observations k in high-dimensional settings, beyond simple rules of thumb? The paper acknowledges the importance of this issue for high-dimensional data but does not provide a solution, deferring it to future research.

### Open Question 3
How can the proposed methodology be effectively combined with regularization techniques to handle high-dimensional data while maintaining theoretical guarantees? The authors suggest combining with regularization techniques but do not provide details on how this would be done or what impact it would have on the theoretical framework.

## Limitations
- Theoretical framework relies heavily on multivariate regular variation assumptions that may not hold for many real-world datasets
- Marginal standardization introduces additional complexity when dealing with unknown marginal distributions
- Choice of k/n ratio critically impacts bias-variance tradeoff but lacks definitive guidelines beyond the stability region concept

## Confidence
- **High confidence**: The asymptotic optimality of angular-based predictors under regular variation assumptions
- **Medium confidence**: The non-asymptotic risk bounds with bias-variance decomposition, as these depend on empirical verification of finite VC dimension and uniform boundedness
- **Medium confidence**: The effectiveness of marginal standardization in preserving the theoretical framework, particularly when marginal distributions must be estimated

## Next Checks
1. **Assumption validation**: Test whether Assumption 2 holds for real-world datasets by checking the convergence of normalized extreme observations to a spectral measure using the statistic $\hat{A}_k = \sum_{i=1}^k \mathbf{1}_{\{\Theta(X_{(i)}) \in A\}}/k$ for various sets A on the unit sphere.

2. **Sensitivity analysis**: Conduct a systematic study of MSE performance across different k/n ratios and sample sizes to empirically verify the theoretical O(1/√k) rate and identify the stability region where estimates become robust to k variations.

3. **Robustness to estimation errors**: Evaluate the method's performance when marginal distributions must be estimated rather than known, comparing results to the ideal case where true Pareto margins are used.