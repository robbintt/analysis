---
ver: rpa2
title: 'Machine Learning needs Better Randomness Standards: Randomised Smoothing and
  PRNG-based attacks'
arxiv_id: '2306.14043'
source_url: https://arxiv.org/abs/2306.14043
tags:
- random
- test
- attack
- distribution
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates novel attacks on Randomised Smoothing, a
  key technique for certifying robustness in machine learning models. Attackers can
  manipulate randomness generators to distort confidence estimates, leading to over-
  or under-estimation of model robustness by up to a factor of 81.
---

# Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks

## Quick Facts
- arXiv ID: 2306.14043
- Source URL: https://arxiv.org/abs/2306.14043
- Reference count: 40
- Key outcome: PRNG-based attacks can distort Randomised Smoothing robustness certifications by factors up to 81x while evading NIST randomness tests

## Executive Summary
This paper demonstrates novel attacks on Randomised Smoothing, a key technique for certifying robustness in machine learning models. Attackers can manipulate pseudo-random number generators to distort confidence estimates, leading to over- or under-estimation of model robustness by up to a factor of 81. The attacks are extremely difficult to detect using standard NIST randomness tests, revealing critical gaps in current randomness standards for safety-critical ML applications. The work highlights the need for updated randomness standards that account for ML-specific vulnerabilities where randomness is transformed into non-uniform distributions.

## Method Summary
The authors implement bit-flipping attacks on PRNGs by modifying the bitstream produced by numpy's PCG64 generator. They skew, add kurtosis, or negate bits in the PRNG output to alter the distribution of noise added to input data during Randomised Smoothing. The attacks target the Gaussian noise sampling process that underlies robustness certification. They evaluate these attacks on CIFAR-10 using 500 test images with a ResNet-110 base model, measuring certified radius ratios (R'/R) and accuracy changes. The experiments compare baseline certification with attacked versions across different manipulation strategies.

## Key Results
- PRNG manipulation can distort Randomised Smoothing certified radii by factors up to 81x
- NIST randomness tests with default parameters fail to detect the attacks
- Attacks maintain near-baseline model accuracy while significantly altering robustness estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attackers can manipulate randomness generators to distort confidence estimates in Randomised Smoothing.
- Mechanism: By modifying the bit stream produced by a pseudo-random number generator (PRNG), an attacker can alter the distribution of noise added to input data, leading to overestimation or underestimation of model robustness.
- Core assumption: The PRNG is a trusted component that generates randomness for the Randomised Smoothing process.
- Evidence anchors:
  - [abstract] "Attackers can manipulate randomness generators to distort confidence estimates, leading to over- or under-estimation of model robustness by up to a factor of 81."
  - [section] "We demonstrate an entirely novel attack against it, where an attacker backdoors the supplied randomness to falsely certify either an overestimate or an underestimate of robustness."
- Break condition: If the PRNG is cryptographically secure and its output is verified for randomness before being used in Randomised Smoothing.

### Mechanism 2
- Claim: NIST randomness tests fail to detect certain PRNG-based attacks.
- Mechanism: The NIST test suite, designed for cryptographic security, may not be sensitive enough to detect subtle manipulations in the PRNG output that affect the distribution of noise in Randomised Smoothing.
- Core assumption: The NIST test suite is sufficient to detect all malicious manipulations of PRNG output.
- Evidence anchors:
  - [abstract] "The attacks are extremely hard to detect using standard NIST randomness tests, requiring updates to randomness standards for safety-critical ML applications."
  - [section] "We show that NIST's randomness tests with default parameters fail to catch our attacks and argue for updated randomness standards."
- Break condition: If the NIST test suite is updated to include tests specifically designed to detect manipulations that affect the distribution of noise in Randomised Smoothing.

### Mechanism 3
- Claim: Randomised Smoothing relies on high-quality randomness to provide accurate robustness certifications.
- Mechanism: Randomised Smoothing samples Gaussian noise to explore the volume around a data point, and the quality of this noise directly impacts the accuracy of the robustness certification.
- Core assumption: The randomness used in Randomised Smoothing is of high quality and not manipulated by an attacker.
- Evidence anchors:
  - [abstract] "Under the hood, it relies on sampling Gaussian noise to explore the volume around a data point to certify that a model is not vulnerable to adversarial examples."
  - [section] "We choose Randomised Smoothing since it is used for both security and safety – to counteract adversarial examples and quantify uncertainty respectively."
- Break condition: If the randomness used in Randomised Smoothing is verified for quality and not manipulated by an attacker.

## Foundational Learning

- Concept: Pseudo-random number generators (PRNGs)
  - Why needed here: PRNGs are used to generate the randomness required for Randomised Smoothing, and attacks target the PRNG to manipulate the noise distribution.
  - Quick check question: What is the difference between a true random number generator and a PRNG, and why is this distinction important for understanding the attack?

- Concept: Statistical tests for randomness
  - Why needed here: Statistical tests, such as the NIST test suite, are used to verify the quality of randomness generated by PRNGs, and understanding their limitations is crucial for understanding why the attacks are hard to detect.
  - Quick check question: What are the limitations of the NIST test suite in detecting manipulations that affect the distribution of noise in Randomised Smoothing?

- Concept: Randomised Smoothing
  - Why needed here: Randomised Smoothing is the target of the attacks, and understanding how it works is essential for understanding the impact of the attacks on model robustness certification.
  - Quick check question: How does Randomised Smoothing use randomness to provide robustness certifications, and what is the role of the noise distribution in this process?

## Architecture Onboarding

- Component map: PRNG -> Randomised Smoothing -> NIST test suite
- Critical path: 1. PRNG generates randomness. 2. Randomised Smoothing samples Gaussian noise using the PRNG output. 3. Randomised Smoothing provides robustness certifications based on the sampled noise.
- Design tradeoffs:
  - Speed vs. security: Cryptographically secure PRNGs may be slower but provide better security against attacks.
  - Detection vs. performance: More thorough randomness tests may detect attacks but also impact the performance of the system.
- Failure signatures: Inflated or deflated robustness certifications. PRNG output fails randomness tests. Randomised Smoothing fails to provide accurate robustness certifications.
- First 3 experiments:
  1. Implement a PRNG-based attack on Randomised Smoothing and observe the impact on robustness certifications.
  2. Run the NIST test suite on the PRNG output and observe the detection rate of the attack.
  3. Implement a defense against the PRNG-based attack and evaluate its effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal statistical tests and parameters needed to reliably detect PRNG-based attacks on machine learning systems that use randomness for certification?
- Basis in paper: [explicit] The paper demonstrates that NIST's standard randomness tests with default parameters fail to detect the proposed attacks, and advocates for updated randomness standards specifically for ML applications.
- Why unresolved: The paper shows that different attacks are detected by different tests, and that current NIST recommendations are insufficient. It also shows that some attacks (like the positive kurtosis attack with γ=4) pass even specialized normality tests.
- What evidence would resolve it: A comprehensive evaluation of multiple statistical tests (Shapiro-Wilk, Kolmogorov-Smirnov, D'Agostino's K-squared, etc.) with various sample sizes and parameters on a wide range of PRNG attacks, establishing which combinations reliably detect all attack types.

### Open Question 2
- Question: How can we design PRNGs that are both efficient for ML workloads and resistant to the specific class of attacks demonstrated in this paper?
- Basis in paper: [explicit] The paper notes that cryptographic PRNGs are too slow for ML applications that make intensive use of randomness, and that even NIST-certified generators like PCG64 can be compromised.
- Why unresolved: Current PRNGs prioritize speed over security for ML, and the paper shows that even cryptographic standards don't account for ML-specific vulnerabilities where randomness is transformed into non-uniform distributions.
- What evidence would resolve it: Development and benchmarking of new PRNG designs that maintain ML performance requirements while incorporating detection mechanisms for the bit-flipping and distribution manipulation attacks described.

### Open Question 3
- Question: What is the fundamental trade-off between randomness quality and model performance in certified robustness techniques like Randomised Smoothing?
- Basis in paper: [inferred] The paper demonstrates that attacks can manipulate certified radii by factors up to 81x while maintaining acceptable model accuracy, suggesting there's a complex relationship between randomness quality and both robustness certification and performance.
- Why unresolved: While the paper shows attacks can be highly effective while maintaining baseline accuracy, it doesn't explore the full spectrum of what happens as randomness quality degrades - does performance always degrade linearly, or are there thresholds where small changes have outsized effects?
- What evidence would resolve it: Systematic studies varying randomness quality parameters and measuring the resulting changes in both certified robustness accuracy and model performance metrics across different ML tasks and datasets.

## Limitations
- The exact implementation details of PRNG modifications are not fully specified, requiring reconstruction from descriptions.
- The paper focuses on a single base model (ResNet-110) and dataset (CIFAR-10), limiting generalizability.
- Parameter values for attacks are described qualitatively but not precisely quantified.

## Confidence
- High confidence: The core claim that PRNG manipulation can significantly distort Randomised Smoothing robustness estimates (up to factor of 81) is well-supported by experimental results showing measurable R'/R ratio changes across multiple attack types.
- Medium confidence: The claim about NIST test suite inadequacy is supported but could benefit from testing against newer NIST versions or alternative randomness test suites beyond the default parameters used.
- Medium confidence: The assertion that attacks maintain near-baseline accuracy while manipulating robustness estimates is supported by confusion matrix analysis, though the trade-off between attack strength and accuracy degradation could be more thoroughly characterized.

## Next Checks
1. Reproduce with exact parameters: Implement the PRNG bit-flipping attacks using the described modification patterns and verify the R'/R ratios match paper claims (e.g., 81x underestimation, 18x overestimation) on CIFAR-10 with ResNet-110.

2. Expand NIST test coverage: Test the attacked PRNG outputs against the full NIST SP 800-22 suite with various sample sizes and parameter configurations to determine if specific tests can detect the manipulations beyond the default settings used.

3. Cross-model generalization: Apply the same attack methodology to different model architectures (e.g., WideResNet, VGG) and datasets (e.g., ImageNet subsets) to verify if the attack effectiveness scales across diverse ML applications beyond the ResNet-110/CIFAR-10 combination.