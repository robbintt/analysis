---
ver: rpa2
title: 'LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated
  Multiple Instance Learning'
arxiv_id: '2308.01413'
source_url: https://arxiv.org/abs/2308.01413
tags:
- classification
- code
- large
- learning
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LaFiCMIL, a large file classification framework\
  \ based on correlated multiple instance learning. The method addresses the input\
  \ length limitation of transformer models by splitting large files into smaller\
  \ chunks, extracting features using BERT-family models, and learning correlations\
  \ among chunks via Nystr\xF6mformer."
---

# LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning

## Quick Facts
- arXiv ID: 2308.01413
- Source URL: https://arxiv.org/abs/2308.01413
- Reference count: 22
- Key outcome: Achieves new state-of-the-art performance across seven benchmark datasets spanning long document classification, code defect detection, and Android malware detection, notably improving accuracy on datasets with high proportions of long documents

## Executive Summary
LaFiCMIL introduces a large file classification framework based on correlated multiple instance learning (c-MIL) that addresses the input length limitation of transformer models. By splitting large files into smaller chunks, extracting features using BERT-family models, and learning correlations among chunks via Nyströmformer, the method achieves new state-of-the-art performance across diverse classification tasks. The approach scales BERT to process up to 20,000 tokens on a single 32GB GPU, demonstrating both efficiency and effectiveness in handling large files.

## Method Summary
LaFiCMIL processes large files by first splitting them into smaller chunks (typically 512 tokens for text/code, class-level for APKs). Features are extracted using pre-trained BERT-family models like BERT, CodeBERT, or DexBERT. The method then applies Nyströmformer layers to learn correlations among chunks through an efficient attention approximation mechanism. These correlations are used to construct a category vector for each large file, which is then passed through an MLP for final classification. The framework treats each large file as a bag of correlated instances, leveraging the dependencies between chunks to improve classification accuracy while maintaining computational efficiency.

## Key Results
- Achieves new state-of-the-art performance across all seven benchmark datasets
- Improves accuracy by +4.41 percentage points on Paired Book Summary dataset
- Scales BERT to process up to 20,000 tokens on a single 32GB GPU

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: LaFiCMIL extends the input length limit of transformer models from 512 tokens to nearly 20,000 tokens on a single GPU by chunking large files and using Nyströmformer to capture correlations among chunks.
**Mechanism**: The method splits large files into smaller chunks, extracts features using BERT-family models, and learns correlations among chunks via Nyströmformer's efficient attention approximation. This allows processing of complete large files without truncation.
**Core assumption**: Large files contain meaningful correlations between their constituent chunks that can be leveraged for improved classification performance.
**Evidence anchors**: 
- [abstract] "LaFiCMIL achieves new state-of-the-art performance across all tasks, notably improving accuracy on datasets with high proportions of long documents (e.g., +4.41 percentage points on Paired Book Summary)"
- [section] "The proposed LaFiCMIL can be generalized to various research domains, addressing problems such as long document classification, code defect detection, and Android malware detection"
- [corpus] Weak - corpus contains only tangentially related papers about file classification but not transformer-based approaches
**Break condition**: If correlations between chunks are weak or non-existent, the performance gains from LaFiCMIL would diminish significantly.

### Mechanism 2
**Claim**: LaFiCMIL's correlated multiple instance learning (c-MIL) framework improves classification by treating each large file as a bag of correlated instances (chunks) rather than independent ones.
**Mechanism**: By learning dependencies between chunks within the same file, LaFiCMIL captures contextual relationships that basic MIL misses. The Nyströmformer layer models these correlations through attention mechanisms.
**Core assumption**: Instances (chunks) within the same large file exhibit dependencies or ordering relationships that are relevant to classification.
**Evidence anchors**:
- [section] "We claim that the small chunks from the same large file are correlated in some way (e.g., semantic dependencies in text or code snippets, invocation relationships between Android classes)"
- [section] "According to Theorem 2, these correlations can be exploited to reduce uncertainty in prediction"
- [corpus] Weak - corpus doesn't contain papers specifically addressing correlated MIL for file classification
**Break condition**: If the dataset contains files where chunks are truly independent (no semantic ordering or dependencies), the c-MIL framework provides no advantage over basic MIL.

### Mechanism 3
**Claim**: LaFiCMIL's efficiency comes from the Nyströmformer's ability to approximate full self-attention with linear complexity, enabling processing of thousands of chunks on limited GPU memory.
**Mechanism**: Nyströmformer approximates the softmax attention matrix using landmark tokens and Moore-Penrose inverse, reducing computational complexity from O(n²) to O(n) while maintaining performance.
**Core assumption**: The landmark-based approximation in Nyströmformer is sufficiently accurate for capturing inter-chunk relationships in large file classification.
**Evidence anchors**:
- [section] "Nyströmformer approximates S by sof tmax( QeK T p dq)Z ∗sof tmax( eQK T p dq)"
- [section] "Following their empirical choice, we run 6 iterations in order to achieve a good approximation of the pseudoinverse"
- [corpus] Weak - corpus lacks papers specifically about Nyströmformer's application to large file classification
**Break condition**: If the approximation error becomes too large for complex inter-chunk relationships, the classification accuracy would degrade.

## Foundational Learning

- **Concept**: Multiple Instance Learning (MIL)
  - **Why needed here**: MIL provides the theoretical framework for treating large files as bags of instances (chunks) where only bag-level labels are available
  - **Quick check question**: In MIL, if a bag contains at least one positive instance, what is the bag's label? (Answer: Positive)

- **Concept**: Correlated Multiple Instance Learning (c-MIL)
  - **Why needed here**: c-MIL extends basic MIL by modeling dependencies between instances within the same bag, which is crucial for capturing relationships between chunks in large files
  - **Quick check question**: What key assumption does c-MIL make that basic MIL does not? (Answer: Instances within the same bag are correlated or dependent)

- **Concept**: Nyström approximation
  - **Why needed here**: Nyström approximation enables efficient computation of attention mechanisms over thousands of chunks, making the approach feasible on limited GPU resources
  - **Quick check question**: What computational complexity does Nyströmformer reduce attention from and to? (Answer: From O(n²) to O(n))

## Architecture Onboarding

- **Component map**: Large file -> Chunk division -> BERT feature extraction -> Nyströmformer correlation learning -> Category vector -> MLP classification
- **Critical path**: Large file -> Chunk division -> BERT feature extraction -> Nyströmformer correlation learning -> Category vector -> MLP classification
- **Design tradeoffs**:
  - Memory vs. completeness: Processing all chunks vs. truncation (LaFiCMIL chooses completeness)
  - Model complexity vs. efficiency: Multiple Nyströmformer layers vs. single attention layer
  - Frozen backbone vs. fine-tuning: Faster training but potentially less adaptation
- **Failure signatures**:
  - Poor performance on datasets with independent chunks (positional embedding provides no benefit)
  - Memory overflow when processing extremely large files (>20K tokens)
  - Degraded accuracy if BERT feature extraction quality is low
- **First 3 experiments**:
  1. Baseline test: Run LaFiCMIL on a dataset with only short documents (<512 tokens) to verify it doesn't degrade performance
  2. Chunk dependency test: Run with and without chunk positional embedding on a document dataset to measure ordering impact
  3. Memory efficiency test: Process a 20K token file and measure GPU memory usage vs. standard BERT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do correlations between instances in large file classification tasks generalize beyond the domains explored in the paper?
- Basis in paper: [explicit] The paper discusses correlations in text/code snippets and Android Smali classes, but does not explore other potential domains.
- Why unresolved: The paper focuses on three specific domains (NLP, PLP, Android), leaving the applicability to other domains unexplored.
- What evidence would resolve it: Empirical results demonstrating LaFiCMIL's effectiveness on additional domains, such as image classification or audio processing, would clarify the generalizability of the approach.

### Open Question 2
- Question: What is the impact of varying the number of chunks on the performance of LaFiCMIL in large file classification tasks?
- Basis in paper: [inferred] The paper mentions splitting large files into smaller chunks but does not investigate the optimal number of chunks for different tasks.
- Why unresolved: The paper does not provide a systematic analysis of how the number of chunks affects model performance across different datasets.
- What evidence would resolve it: A comprehensive study varying the number of chunks and measuring the impact on classification accuracy would provide insights into the optimal chunk configuration.

### Open Question 3
- Question: How does the choice of the Nyströmformer's hyperparameters, such as the number of landmarks, affect the performance of LaFiCMIL?
- Basis in paper: [inferred] The paper uses Nyströmformer but does not explore the sensitivity of its performance to hyperparameter changes.
- Why unresolved: The paper does not report experiments with different Nyströmformer configurations, leaving the impact of hyperparameters on performance unclear.
- What evidence would resolve it: Experiments varying the number of landmarks and other hyperparameters, along with their effects on classification accuracy, would clarify the sensitivity of LaFiCMIL to these settings.

## Limitations
- Dataset heterogeneity and task-specific performance variations are not thoroughly analyzed
- Approximation accuracy vs. computational efficiency trade-off is not quantified
- Positional embedding necessity varies by dataset without clear guidelines for when to include it

## Confidence
- **High confidence**: The fundamental approach of chunking large files and using Nyströmformer for efficient attention computation is technically sound
- **Medium confidence**: The claimed state-of-the-art performance across all seven datasets, though heterogeneous nature of tasks makes comprehensive assessment difficult
- **Low confidence**: The universal applicability of the correlated MIL framework, given the paper's own admission that positional embeddings aren't always useful

## Next Checks
1. **Chunk independence test**: Create a synthetic dataset where chunks within files are truly independent (randomly shuffled text passages) and run LaFiCMIL with and without the correlated MIL framework to quantify the performance penalty when chunk correlations are absent.

2. **Approximation fidelity analysis**: Systematically vary the number of Nyströmformer iterations (1, 3, 6, 9, 12) and measure both computational efficiency and classification accuracy on a representative large file classification task to quantify the approximation error trade-off.

3. **Dataset-specific ablation study**: Run LaFiCMIL on each of the seven benchmark datasets with all components enabled, then systematically disable correlated MIL features (remove Nyströmformer layers, remove positional embeddings) to identify which components drive performance gains on which dataset types.