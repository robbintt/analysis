---
ver: rpa2
title: On the importance of catalyst-adsorbate 3D interactions for relaxed energy
  predictions
arxiv_id: '2310.06682'
source_url: https://arxiv.org/abs/2310.06682
tags:
- adsorbate
- catalyst
- energy
- independent
- disconnected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores whether relaxed energy predictions for adsorbate-catalyst\
  \ systems can be made without modeling the 3D spatial relationship between adsorbate\
  \ and catalyst. The authors propose four modifications of GNN architectures\u2014\
  removing inter-component edges, pooling independent representations, using separate\
  \ backbones, and attention-based non-geometric communication\u2014to create \"disconnected\"\
  \ models."
---

# On the importance of catalyst-adsorbate 3D interactions for relaxed energy predictions

## Quick Facts
- arXiv ID: 2310.06682
- Source URL: https://arxiv.org/abs/2310.06682
- Reference count: 11
- Primary result: Disconnected GNN models achieve 20-31% higher MAE than geometric models but still predict relaxed energies reasonably well

## Executive Summary
This paper investigates whether 3D geometric coupling between adsorbates and catalysts is necessary for accurate relaxed energy predictions. The authors propose four modifications to GNN architectures—removing inter-component edges, pooling independent representations, using separate backbones, and attention-based non-geometric communication—to create "disconnected" models that don't explicitly model spatial relationships. Experiments on the OC20 dataset show these models incur performance drops but maintain reasonable accuracy, suggesting that relaxed energy can be predicted without explicit geometric binding site information. This finding opens possibilities for configuration-agnostic materials discovery approaches.

## Method Summary
The authors modify GNN architectures to remove explicit geometric coupling between adsorbate and catalyst components while preserving energy prediction accuracy. Four disconnected approaches are explored: removing inter-component edges entirely, pooling independent component representations, using separate GNN backbones for each component, and implementing attention-based communication without geometric information. The models are evaluated on the OC20 IS2RE dataset using mean absolute error across multiple validation splits, comparing against baseline geometric models.

## Key Results
- Disconnected GNN models achieve 20-31% higher MAE than geometric models on OC20
- Independent backbone architectures perform best among disconnected approaches
- Attention-based models with z-axis proximity information show moderate success
- Disconnected models maintain reasonable accuracy despite losing explicit geometric information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disconnected GNNs can predict relaxed energies without explicit adsorbate-catalyst geometric coupling
- Mechanism: By removing edges between adsorbate and catalyst, the model loses geometric binding site information but retains independent structural features from each component, allowing energy prediction via separate component modeling and learned non-geometric interactions
- Core assumption: Relaxed energy can be inferred from component-level features alone without direct geometric coupling information
- Evidence anchors:
  - [abstract] "four modifications of GNN architectures—removing inter-component edges, pooling independent representations, using separate backbones, and attention-based non-geometric communication—to create 'disconnected' models"
  - [section 2.1] "baseline Disconnected GNN model does not create any edges between the catalyst and the adsorbate in the graph creation step"
  - [corpus] weak - no direct neighbor evidence, but related papers on binding affinity prediction without explicit geometry support this direction
- Break condition: When binding site geometry becomes the dominant factor in energy differences, or when input configurations are highly varied within the same system

### Mechanism 2
- Claim: Separate backbone architectures capture distinct structural patterns in adsorbates vs catalysts
- Mechanism: Using independent GNNs for adsorbates and catalysts allows each model to specialize in different structural motifs and size distributions, improving component-level feature extraction
- Core assumption: Adsorbate and catalyst structures are sufficiently distinct that specialized models outperform shared architectures
- Evidence anchors:
  - [section 2.3] "the adsorbate and the catalyst have very different sizes and roles. Hence, it might be beneficial to have independent GNNs"
  - [section 3] "independent model architecture performs best among all proposed disconnected GNNs"
  - [corpus] weak - no direct neighbor evidence, but aligns with general GNN practice of using separate encoders for heterogeneous graphs
- Break condition: When geometric interactions between components become more informative than component-specific features

### Mechanism 3
- Claim: Attention-based inter-component communication can approximate geometric relationships using non-geometric features
- Mechanism: Weighted edges based on z-axis positions and GAT convolution layers enable cross-component information flow without explicit distance or orientation data
- Core assumption: Surface proximity information (z-axis) is sufficient to approximate binding site effects on energy
- Evidence anchors:
  - [section 2.4] "The motivation behind these weighted edges is to make the model aware of the closeness of a node to the catalyst's surface"
  - [section 2.4] "this model can run without locating the adsorbate and catalyst in the same 3D plane"
  - [corpus] weak - no direct neighbor evidence, but aligns with attention mechanisms in protein-ligand binding prediction
- Break condition: When 3D spatial relationships beyond surface proximity significantly impact energy predictions

## Foundational Learning

- Graph Neural Networks
  - Why needed here: The entire approach builds on GNN architectures that can process atomic systems as graphs
  - Quick check question: What are the three main steps in the standard GNN pipeline for atomic systems?

- Equivariance and Invariance
  - Why needed here: The models must respect E(3)-invariance for energy predictions while handling 3D atomic positions
  - Quick check question: Why does removing edges between adsorbate and catalyst preserve translation equivariance?

- Message Passing
  - Why needed here: The proposed architectures modify how information propagates between atoms in the system
  - Quick check question: How does removing edges between components affect message passing in GNNs?

## Architecture Onboarding

- Component map:
  Input -> Graph creation (separate vs connected) -> Embedding -> Interaction layers -> Pooling (separate vs combined) -> Output

- Critical path:
  1. Graph construction (separate vs connected)
  2. Independent vs shared backbone processing
  3. Pooling strategy (separate vs combined)
  4. Optional inter-component communication layers

- Design tradeoffs:
  - Separate backbones: More parameters but better specialization vs shared backbones: Fewer parameters but may miss component-specific patterns
  - Independent pooling: Simple but may lose cross-component interactions vs attention-based: More complex but can capture non-geometric relationships
  - No inter-component edges: Fastest but loses all cross-component information vs attention with z-axis: Moderate complexity with partial information retention

- Failure signatures:
  - High MAE on OOD splits: Model struggles with unseen adsorbate/catalyst combinations
  - Performance drops with increased dataset size: Model can't generalize across multiple configurations
  - Attention models underperforming: Z-axis proximity insufficient for energy prediction

- First 3 experiments:
  1. Compare baseline GNN vs disconnected baseline on OC20 to quantify geometric information loss
  2. Test independent backbones vs shared backbones to measure benefit of component specialization
  3. Evaluate attention-based communication vs independent pooling to assess value of cross-component information flow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance gap between disconnected and connected GNN models change when trained and evaluated on datasets that only contain unique adsorbate-catalyst pairs without multiple binding site configurations?
- Basis in paper: [explicit] The authors note that 11% of OC20 IS2RE has multiple targets for the same sample, which impairs training dynamics for disconnected models, and show in Appendix D that disconnected models match or outperform baselines on the 10k dataset which only has unique pairs
- Why unresolved: While initial results on the 10k dataset are promising, a comprehensive evaluation across multiple datasets with only unique pairs is needed to establish whether the performance gap is consistently reduced or eliminated
- What evidence would resolve it: Systematic experiments on multiple datasets (including OC20-Dense) comparing disconnected vs connected models' performance, particularly focusing on cases with single vs multiple binding sites per pair

### Open Question 2
- Question: What architectural modifications to attention-based disconnected GNNs would enable them to consistently outperform baseline models while maintaining their configuration-agnostic properties?
- Basis in paper: [explicit] The authors state that attention models "do not consistently outperform the independent backbone models" and suggest "it is likely that the way in which attention models use GAT convolution layers hinders the model's prediction"
- Why unresolved: The paper identifies the problem but doesn't explore alternative attention mechanisms or architectural changes that could resolve the issue
- What evidence would resolve it: Experiments comparing different attention mechanisms (e.g., modified GAT, transformer-based attention, or other graph attention variants) that show consistent performance improvements over both baseline and independent backbone models

### Open Question 3
- Question: Can disconnected GNN models effectively predict the global minimum relaxed energy across all possible binding site configurations without explicitly exploring each configuration?
- Basis in paper: [explicit] The authors propose that "Building upon [OC20-Dense], we will be able to train disconnected GNN models to predict the minimum relaxed energy of each adslab over all possible input configurations"
- Why unresolved: This represents a theoretical capability mentioned for future work, but no experimental validation exists to show whether disconnected models can actually marginalize over configurations to find global minima
- What evidence would resolve it: Experiments demonstrating that disconnected models trained on OC20-Dense can accurately predict minimum energies across all configurations, compared against ground truth DFT calculations of global minima

## Limitations
- Performance degradation of 20-31% higher MAE indicates significant information loss from geometric relationships
- Results primarily validated on OC20 dataset where geometric effects are less dominant
- Focus on energy prediction rather than structural optimization or reaction pathway identification

## Confidence
- High confidence: The core finding that disconnected models can predict relaxed energies with reasonable accuracy is well-supported by systematic experiments across multiple model architectures and validation splits
- Medium confidence: The claim that this approach enables configuration-agnostic materials discovery is plausible but not directly demonstrated - the paper shows it's possible but doesn't prove it's beneficial for discovery
- Medium confidence: The ranking of disconnected architectures (independent > attention > baseline) is well-established within the paper's experiments, though results may vary with different datasets

## Next Checks
1. Test disconnected models on datasets where geometric binding site information is known to be more critical (e.g., systems with multiple adsorption sites or competing configurations)
2. Evaluate whether disconnected models maintain performance when trained on larger, more diverse datasets that include multiple configurations per adsorbate-catalyst pair
3. Compare disconnected approaches with traditional surrogate models that use geometric featurization to determine if GNN complexity is necessary for this task