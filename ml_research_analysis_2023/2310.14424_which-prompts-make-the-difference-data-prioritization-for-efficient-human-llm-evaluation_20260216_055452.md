---
ver: rpa2
title: Which Prompts Make The Difference? Data Prioritization For Efficient Human
  LLM Evaluation
arxiv_id: '2310.14424'
source_url: https://arxiv.org/abs/2310.14424
tags:
- evaluation
- human
- language
- completion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the resource-intensive nature of human evaluation
  for large language models by introducing a data prioritization approach to reduce
  required annotations while maintaining robust performance evaluation. The method
  uses KL divergence and Cross-Entropy metrics to rank prompts based on predicted
  dissimilarity between model completions, aiming to minimize tie outcomes in pairwise
  comparisons.
---

# Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation

## Quick Facts
- arXiv ID: 2310.14424
- Source URL: https://arxiv.org/abs/2310.14424
- Reference count: 40
- Ranking prompts by KL divergence reduces tie rates by up to 54% compared to random selection

## Executive Summary
This work addresses the resource-intensive nature of human evaluation for large language models by introducing a data prioritization approach to reduce required annotations while maintaining robust performance evaluation. The method uses KL divergence and Cross-Entropy metrics to rank prompts based on predicted dissimilarity between model completions, aiming to minimize tie outcomes in pairwise comparisons. Results show that ranking by KL divergence reduces tie rates by up to 54% compared to random selection in the top-20 percentile of prioritized instances. The approach also improves Elo score robustness, allowing reliable model rankings with fewer annotations.

## Method Summary
The approach generates completions for each prompt from all models in the evaluation pool, then computes KL divergence and Cross-Entropy between completion pairs using log probabilities. Prompts are ranked by these metrics to prioritize those likely to produce decisive model preferences. Human annotators evaluate the top k% of ranked prompts in pairwise comparisons, and Elo ratings are computed from these preferences. The method includes min-max normalization to handle different probability scales across model families, and uses soft voting thresholds (0.2 for same family, 0.1 for different families) to define tie outcomes.

## Key Results
- KL divergence ranking reduces tie rates by up to 54% compared to random selection in top-20 percentile
- The approach improves Elo score robustness, enabling reliable model rankings with fewer annotations
- Effectiveness demonstrated across widely used model families including Flan-T5 and Dolly-V2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KL divergence and Cross-Entropy ranking effectively prioritize prompts that yield decisive model preference outcomes.
- **Mechanism**: These metrics quantify the dissimilarity between model completions, identifying prompts where completions diverge significantly. Higher divergence corresponds to clearer preference signals from human annotators, reducing tie outcomes.
- **Core assumption**: Completion dissimilarity directly correlates with human preference decisiveness.
- **Evidence anchors**:
  - [abstract] "ranking by KL divergence reduces tie rates by up to 54% compared to random selection"
  - [section] "We show that our method is effective across widely used model families, reducing instances of indecisive (or "tie") outcomes by up to 54% compared to a random sample"
  - [corpus] "Label-Efficient Model Selection for Text Generation" shows similar data prioritization approaches

### Mechanism 2
- **Claim**: The proposed offline ranking approach enables reliable model comparison with fewer annotations while maintaining Elo score stability.
- **Mechanism**: By ordering prompts from most to least dissimilar completions, evaluators can make confident model comparisons early in the evaluation process. This reduces the number of annotations needed to establish reliable Elo ratings.
- **Core assumption**: Early preference signals from highly dissimilar completions accurately reflect overall model quality differences.
- **Evidence anchors**:
  - [abstract] "This potential reduction in required human effort positions our approach as a valuable strategy in future large language model evaluations"
  - [section] "Our proposed evaluation provides a fast and thorough strategy to assess models performance relative to each other, enhancing trust in results even when working with smaller datasets"
  - [corpus] "Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation" demonstrates importance of diverse evaluation sets

### Mechanism 3
- **Claim**: Normalization of probability ranges is critical when comparing models from different families.
- **Mechanism**: Min-max scaling ensures probability distributions from different model families are comparable, preventing systematic bias in dissimilarity metrics due to different output probability scales.
- **Core assumption**: Probability distributions from different model families span different numerical ranges due to architectural and training differences.
- **Evidence anchors**:
  - [section] "In the inter-family comparisons, we observe significant variations in the range of log probabilities across models, given the same evaluation prompts"
  - [section] "This standardization method enables the computation of our metrics on a comparable scale across different model families"
  - [corpus] "PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models" shows importance of normalization in reward-based comparisons

## Foundational Learning

- **Concept**: KL Divergence and Cross-Entropy as dissimilarity metrics
  - Why needed here: These metrics quantify how different model completions are, enabling prioritization of prompts that maximize model differentiation
  - Quick check question: What is the fundamental difference between KL divergence and Cross-Entropy in measuring distribution dissimilarity?

- **Concept**: Elo rating system for model comparison
  - Why needed here: Provides a systematic framework for aggregating pairwise human preferences into overall model rankings
  - Quick check question: How does the Elo rating update formula incorporate both expected and actual outcomes in pairwise comparisons?

- **Concept**: Min-max normalization for probability distributions
  - Why needed here: Ensures probability distributions from different model families are comparable when computing dissimilarity metrics
  - Quick check question: What happens to KL divergence calculations if probability distributions have vastly different scales and aren't normalized?

## Architecture Onboarding

- **Component map**: Prompt sampling -> Completion generation -> Probability extraction -> Normalization -> KL divergence/Cross-Entropy calculation -> Prompt ranking -> Human annotation -> Elo score computation
- **Critical path**: Completion generation -> probability extraction -> normalization -> dissimilarity metric computation -> prompt ranking -> human evaluation -> Elo score update
- **Design tradeoffs**: 
  - Random vs. prioritized prompt ordering (efficiency vs. potential bias)
  - Intra-family vs. inter-family normalization approaches (accuracy vs. complexity)
  - Soft voting threshold selection (sensitivity vs. robustness to annotator disagreement)
- **Failure signatures**:
  - High tie rates persisting across all ranking percentiles indicates metrics aren't capturing meaningful differences
  - Elo scores diverging significantly from random baseline suggests ranking approach is introducing bias
  - Inconsistent normalization across model families leading to unreliable cross-family comparisons
- **First 3 experiments**:
  1. Compare tie rates for a single model pair (flan-t5-xxl vs. flan-t5-xl) using random vs. KL divergence ranking across 5%, 10%, 20%, 30%, 50% of data
  2. Validate normalization approach by computing tie rates for inter-family comparison (flan-t5-xxl vs. dolly-v2-12b) with and without min-max scaling
  3. Track Elo score convergence for both intra-family and inter-family comparisons to verify ranking approach maintains accurate model ordering with fewer annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KL divergence and Cross-Entropy metrics compare across different model families and tasks in terms of reducing tie outcomes?
- Basis in paper: [explicit] The paper compares the effectiveness of KL divergence and Cross-Entropy metrics in reducing tie outcomes for different model families and tasks.
- Why unresolved: While the paper shows that both metrics outperform random selection, it does not provide a detailed comparison of their performance across different model families and tasks.
- What evidence would resolve it: A comprehensive analysis comparing the performance of KL divergence and Cross-Entropy metrics across different model families and tasks in terms of reducing tie outcomes.

### Open Question 2
- Question: What is the impact of prompt prioritization on the overall efficiency and cost-effectiveness of human evaluation in large language model development?
- Basis in paper: [explicit] The paper demonstrates that prompt prioritization can significantly reduce tie outcomes and improve the efficiency of human evaluation.
- Why unresolved: The paper does not provide a detailed analysis of the impact of prompt prioritization on the overall efficiency and cost-effectiveness of human evaluation in large language model development.
- What evidence would resolve it: A comprehensive study comparing the efficiency and cost-effectiveness of human evaluation with and without prompt prioritization in large language model development.

### Open Question 3
- Question: How does the performance of the proposed metrics vary with different annotation aggregation methods and threshold settings?
- Basis in paper: [explicit] The paper uses a soft voting aggregation mechanism and threshold settings to define tie outcomes.
- Why unresolved: The paper does not explore the impact of different annotation aggregation methods and threshold settings on the performance of the proposed metrics.
- What evidence would resolve it: An investigation comparing the performance of the proposed metrics with different annotation aggregation methods and threshold settings.

## Limitations
- The approach relies on the assumption that completion dissimilarity correlates with human preference decisiveness
- Normalization across model families introduces complexity and may mask genuine differences
- The 20% ranking threshold appears somewhat arbitrary and may not be optimal for all evaluation scenarios

## Confidence

**High Confidence:** The core finding that KL divergence ranking reduces tie rates by up to 54% compared to random selection. This is directly supported by experimental results and the mechanism (higher completion dissimilarity leads to clearer human preferences) is well-grounded.

**Medium Confidence:** The claim about Elo score robustness and reliable model rankings with fewer annotations. While tie rate reduction is demonstrated, the direct impact on Elo score stability and model ranking accuracy needs more validation, particularly across different dataset types and model combinations.

**Low Confidence:** The generalizability of the approach across all LLM evaluation scenarios. The results are based on specific model families and datasets, and may not translate directly to other evaluation contexts or model architectures.

## Next Checks

1. **Elo Score Stability Validation**: Replicate the Elo score convergence analysis for both intra-family and inter-family comparisons using progressively larger subsets (5%, 10%, 20%, 30%, 50%) of the ranked data. Compare the final rankings and Elo scores against those obtained from random sampling to quantify the accuracy trade-off.

2. **Cross-Dataset Generalization**: Test the approach on a dataset not included in the original study (e.g., a different reasoning or summarization task) to validate whether the KL divergence and Cross-Entropy metrics maintain their effectiveness in prioritizing prompts across different evaluation domains.

3. **Ranking Threshold Sensitivity Analysis**: Systematically vary the ranking percentile threshold (e.g., test 10%, 15%, 25%, 30%) to identify the optimal balance between annotation efficiency and evaluation accuracy. Analyze how tie rates and Elo score stability change with different thresholds to establish practical guidelines for real-world deployment.