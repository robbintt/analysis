---
ver: rpa2
title: Do Smaller Language Models Answer Contextualised Questions Through Memorisation
  Or Generalisation?
arxiv_id: '2311.12337'
source_url: https://arxiv.org/abs/2311.12337
tags:
- evaluation
- samples
- training
- datasets
- similar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the question of whether smaller language models
  can answer contextualised questions through memorisation or generalisation. The
  authors propose a method to identify evaluation samples unlikely to have been memorised
  by training data, based on semantic similarity of input and output tokens.
---

# Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?

## Quick Facts
- **arXiv ID**: 2311.12337
- **Source URL**: https://arxiv.org/abs/2311.12337
- **Reference count**: 20
- **Primary result**: Smaller language models can generalise rather than memorise when answering contextualised questions, as demonstrated by performance improvements on unmemorisable subsets of evaluation datasets.

## Executive Summary
This paper addresses the challenge of distinguishing between memorisation and generalisation in smaller language models when answering contextualised questions. The authors propose a novel method to identify evaluation samples unlikely to be memorised by training data, based on semantic similarity of input and output tokens. By training two models - one on standard QA datasets (UQA) and another with additional numerical reasoning datasets (UQA+TDND) - they demonstrate that significant performance improvements on unmemorisable subsets of DROP and ROPES datasets indicate genuine generalisation. The method effectively identifies memorisable samples through semantic similarity analysis and answer term overlap, providing a framework for evaluating model generalisation capabilities.

## Method Summary
The study trains two BART-based language models in multitask fashion: a baseline UQA model using standard QA datasets (SQUAD 1.1, SQUAD2, NarrativeQA, RACE, ARC, Regents, OpenbookQA, MCTest, BoolQ) and an intervention UQA+TDND model that adds two numerical reasoning datasets (TD and ND). The authors identify unmemorisable evaluation samples by computing semantic similarity between evaluation and training samples using sentence embeddings, filtering for samples with similarity scores below 60 and no answer term overlap. Both models are evaluated on these unmemorisable subsets, and performance differences are measured using F1 scores for reading comprehension datasets and exact match for multiple-choice datasets.

## Key Results
- On unmemorisable subsets of DROP, the UQA+TDND model shows a 9.0% performance improvement over the UQA baseline.
- On unmemorisable subsets of ROPES, the UQA+TDND model achieves a 25.7% performance improvement over the UQA baseline.
- Other evaluation datasets (NewsQA, PIQA, CSQA, QASC) show no significant performance changes between the two models, suggesting the intervention specifically benefits numerical reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1
Semantic similarity between input and output tokens identifies memorisable evaluation-train pairs better than n-gram overlap. The method uses sentence embeddings to compute similarity between both question/context and answer components of evaluation and training samples, capturing both contiguous and discontiguous token overlaps. Core assumption: Semantic similarity in embeddings correlates with potential memorisation risk even when token sequences are not identical.

### Mechanism 2
Adding TDND datasets improves performance on unmemorisable subsets by imparting numerical reasoning strategies. TDND datasets contain synthetic samples designed to teach numerical reasoning, which transfers to similar reasoning required in DROP and ROPES datasets. Core assumption: Numerical reasoning strategies learned from synthetic data generalize to similar reasoning tasks in evaluation datasets.

### Mechanism 3
Using the same unmemorisable subset before and after intervention avoids comparing disparate populations. By identifying unmemorisable samples based on similarity thresholds and answer term overlap, the same subset can be used to measure performance changes attributable to the intervention. Core assumption: The unmemorisable subset remains stable and representative across both model versions.

## Foundational Learning

- **Semantic similarity and sentence embeddings**: Why needed here: The method relies on comparing semantic similarity between evaluation and training samples using sentence embeddings to identify potential memorisation. Quick check: How does cosine similarity between sentence embeddings capture semantic similarity between text sequences?

- **Numerical reasoning and generalisation**: Why needed here: The TDND datasets are designed to impart numerical reasoning strategies that should generalize to similar tasks in evaluation datasets. Quick check: What types of numerical reasoning strategies are taught by synthetic datasets, and how might they transfer to real-world tasks?

- **Intervention studies and experimental design**: Why needed here: The study uses an intervention (adding TDND datasets) to measure performance changes on the same subset, avoiding issues with comparing disparate populations. Quick check: How does using the same evaluation subset before and after intervention help isolate the effects of the intervention?

## Architecture Onboarding

- **Component map**: BART model -> Sentence transformer for similarity computation -> Training datasets (UQA and UQA+TDND) -> Evaluation pipeline (similarity scoring, subset selection, performance measurement)
- **Critical path**: Train BART model on UQA datasets → Compute similarity scores between evaluation and training samples → Identify unmemorisable subsets → Train second BART model on UQA+TDND datasets → Evaluate both models on the same unmemorisable subsets → Compare performance improvements
- **Design tradeoffs**: Using semantic similarity vs. n-gram overlap for memorisation detection; adding synthetic datasets vs. relying on natural data; using the same evaluation subset vs. comparing different populations
- **Failure signatures**: Poor performance improvement on unmemorisable subsets; high similarity scores between evaluation and training samples; inconsistent results across model runs
- **First 3 experiments**:
  1. Train a BART model on UQA datasets and evaluate on all evaluation datasets to establish baseline performance.
  2. Compute similarity scores between evaluation and training samples, identify unmemorisable subsets, and verify no memorisable samples are present.
  3. Train a second BART model on UQA+TDND datasets and evaluate on the same unmemorisable subsets to measure performance improvements.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed semantic similarity method compare to other memorization detection techniques in terms of accuracy and computational efficiency? The authors propose a method based on semantic similarity of input and output tokens between training and evaluation samples but do not provide a direct comparison with other memorization detection techniques.

### Open Question 2
Can the proposed method for identifying unmemorisable samples be effectively applied to larger language models, or does its effectiveness scale with model size? The study focuses on smaller language models, and the authors suggest that memorization capacity scales with model parameter count, but do not investigate applicability to larger models.

### Open Question 3
What are the underlying mechanisms responsible for the observed performance improvements on unmemorisable subsets of DROP and ROPES when using TDND datasets? The paper shows significant performance improvements but does not investigate the specific mechanisms or reasoning processes involved.

## Limitations
- The semantic similarity threshold (60%) may not generalize across different domains or model architectures.
- The effectiveness of synthetic TDND datasets for numerical reasoning transfer may be limited to specific types of numerical reasoning problems.
- Results may not generalize to other QA datasets beyond DROP, ROPES, and the specific evaluation sets used.

## Confidence
- **High Confidence**: The methodology for identifying unmemorisable subsets through semantic similarity and answer overlap is well-defined and reproducible. The experimental design of using the same evaluation subset before and after intervention is sound.
- **Medium Confidence**: The assumption that semantic similarity in sentence embeddings correlates with memorisation risk requires further validation across different embedding models and domains.
- **Low Confidence**: The generalisability of these findings to other QA datasets beyond those specifically tested.

## Next Checks
1. Apply the semantic similarity method to identify unmemorisable subsets in a different domain (e.g., biomedical QA or multilingual QA) to test the robustness of the approach across different types of language understanding tasks.

2. Repeat the unmemorisable subset identification using different sentence embedding models (e.g., SBERT, Universal Sentence Encoder) to verify that the results are not dependent on the specific sentence-transformers/stsb-roberta-large model used.

3. Conduct an ablation study by training models with only TD datasets, only ND datasets, and various combinations to isolate which components of the TDND intervention are most responsible for the observed performance improvements on DROP and ROPES.