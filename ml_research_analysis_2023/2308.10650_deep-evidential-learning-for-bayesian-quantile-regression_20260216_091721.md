---
ver: rpa2
title: Deep Evidential Learning for Bayesian Quantile Regression
arxiv_id: '2308.10650'
source_url: https://arxiv.org/abs/2308.10650
tags:
- uncertainty
- evidential
- quantile
- distribution
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a deep Bayesian quantile regression model that
  can estimate the quantiles of a continuous target distribution without assuming
  a Gaussian distribution. The method is based on evidential learning, which allows
  the model to capture aleatoric and epistemic uncertainty with a single deterministic
  forward-pass model, making it efficient and scalable.
---

# Deep Evidential Learning for Bayesian Quantile Regression

## Quick Facts
- arXiv ID: 2308.10650
- Source URL: https://arxiv.org/abs/2308.10650
- Reference count: 40
- Primary result: Deep evidential quantile regression model estimates non-Gaussian quantiles with calibrated uncertainties using single forward pass

## Executive Summary
This paper proposes a deep Bayesian quantile regression approach that estimates quantiles of continuous target distributions without assuming Gaussian noise. The method uses evidential learning with a Normal-Inverse-Gamma prior to produce Student-t predictive distributions, enabling modeling of non-Gaussian aleatoric uncertainty while disentangling it from epistemic uncertainty. The approach achieves calibrated uncertainty estimates with computational efficiency through single-pass inference, validated on synthetic data, UCI regression datasets, and monocular depth estimation tasks.

## Method Summary
The method implements deep evidential quantile regression by training a neural network to output Normal-Inverse-Gamma (NIG) parameters (γ, ν, α, β) for each quantile. These parameters characterize the evidential distribution, from which Student-t predictive distributions are derived. The model is trained using a combination of negative log-likelihood and evidence regularization loss, with the regularization strength λ tuned per dataset. During inference, a single forward pass produces both quantile predictions and separate estimates of aleatoric (data) and epistemic (model) uncertainty without requiring sampling or multiple passes.

## Key Results
- Achieves calibrated uncertainty estimates on non-Gaussian distributions including exponential, gamma, and Laplace noise
- Disentangles aleatoric and epistemic uncertainty in a single forward pass, outperforming ensemble and dropout baselines
- Demonstrates robustness to out-of-distribution samples while maintaining computational efficiency
- Outperforms state-of-the-art Bayesian neural network uncertainty estimation techniques on UCI regression datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model estimates quantiles without Gaussian assumptions by using NIG evidential prior to derive Student-t predictive distribution
- Mechanism: NIG prior on likelihood parameters produces analytical Student-t distribution through marginalization, modeling non-Gaussian aleatoric uncertainty via quantile-specific offset
- Core assumption: Quantile regression problems follow asymmetric Laplace distribution that can be reformulated as scalar mixture of Gaussians
- Break condition: If quantile regression assumption fails, Student-t distribution may not accurately capture target distribution

### Mechanism 2
- Claim: Disentangles aleatoric and epistemic uncertainty in single forward pass through evidential framework
- Mechanism: Aleatoric uncertainty captured by expected value of inverse gamma distribution (E[σi]), epistemic uncertainty by variance of mean (Var[µi])
- Core assumption: Evidential framework allows separate quantification through prior distribution parameters
- Break condition: If NIG prior inadequately represents true uncertainty structure, disentanglement becomes inaccurate

### Mechanism 3
- Claim: Computational efficiency achieved through single forward pass instead of sampling or multiple passes
- Mechanism: Neural network directly outputs evidential distribution parameters, avoiding weight sampling required by dropout or ensemble methods
- Core assumption: Single deterministic model can accurately approximate evidential parameters for both uncertainty types
- Break condition: Insufficient model capacity may prevent accurate evidential parameter estimation in single pass

## Foundational Learning

- Concept: Quantile Regression
  - Why needed here: Extends Bayesian quantile regression to evidential learning for non-Gaussian aleatoric uncertainty modeling
  - Quick check question: What is the tilted loss function used in quantile regression, and how does it differ from mean squared error?

- Concept: Bayesian Inference and Uncertainty Quantification
  - Why needed here: Essential for understanding how to model both aleatoric and epistemic uncertainty in evidential framework
  - Quick check question: How do aleatoric and epistemic uncertainties differ, and why is it important to disentangle them?

- Concept: Normal-Inverse-Gamma Distribution and Marginalization
  - Why needed here: NIG prior is central to deriving Student-t predictive distribution used in the model
  - Quick check question: What are the parameters of the NIG distribution, and how do they relate to the moments of the Student-t distribution?

## Architecture Onboarding

- Component map: Input features → Dense neural network layers (e.g., 128 units, Leaky ReLU) → Four output neurons per quantile (γ, ν, α, β) → NIG parameters → Student-t predictive distribution

- Critical path: 1) Forward pass: Input → NN → NIG parameters → Student-t predictive distribution; 2) Loss computation: NLL + λ · evidence regularization; 3) Backward pass: Gradient descent update

- Design tradeoffs: Single forward pass vs. multiple passes (faster inference but requires accurate parameter estimation); evidence regularization strength λ (balances uncertainty inflation vs. model fit); quantile selection (more quantiles provide better coverage but increase complexity)

- Failure signatures: High aleatoric uncertainty everywhere (model underestimating true distribution); low epistemic uncertainty on OOD data (poor epistemic uncertainty capture); poor quantile predictions (insufficient model capacity or inappropriate regularization)

- First 3 experiments: 1) Synthetic data with known non-Gaussian noise (exponential, gamma, Laplace) to test theoretical quantile recovery; 2) UCI regression datasets to compare TL, NLL, and inference speed against baselines; 3) Monocular depth estimation to evaluate uncertainty on in-distribution and OOD data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does regularization strength λ affect trade-off between uncertainty calibration and quantile estimation accuracy?
- Basis in paper: [explicit] Paper discusses importance of λ in Equation 14 and shows varying λ affects estimated quantiles and uncertainty in Figures 7 and 8
- Why unresolved: Paper mentions λ should be optimized per use case but provides no systematic method or comprehensive study across datasets and noise distributions
- What evidence would resolve it: Comprehensive ablation study across multiple datasets with varying noise distributions showing impact of different λ values on quantile accuracy and uncertainty calibration metrics

### Open Question 2
- Question: How does proposed method perform on high-dimensional datasets or problems with complex feature interactions?
- Basis in paper: [inferred] Experiments focus on relatively simple regression tasks and monocular depth estimation; scalability claims not empirically validated on high-dimensional data
- Why unresolved: Paper does not test on high-dimensional datasets like image classification, NLP tasks, or high-dimensional regression benchmarks
- What evidence would resolve it: Testing on high-dimensional datasets (e.g., CIFAR-10, high-dimensional regression benchmarks) comparing performance and uncertainty estimates with baseline methods

### Open Question 3
- Question: How sensitive is method to choice of quantile values q when estimating multiple quantiles simultaneously?
- Basis in paper: [explicit] Paper mentions common practice of estimating multiple quantiles simultaneously but does not discuss how quantile choice affects performance
- Why unresolved: Paper does not investigate whether certain quantile values are more difficult to estimate than others or how quantile choice impacts overall performance
- What evidence would resolve it: Experiment varying quantile values (5th, 25th, 50th, 75th, 95th) analyzing estimation accuracy and uncertainty calibration changes across different quantiles

## Limitations
- Distributional assumptions may not hold for all real-world datasets, particularly highly complex or multi-modal distributions
- Evidence regularization parameter λ requires careful per-dataset tuning with only general guidelines provided
- Computational overhead increases due to four evidential parameters per quantile compared to standard quantile regression

## Confidence
- High Confidence: Evidential learning mechanism using NIG priors to produce Student-t distributions is mathematically sound and well-established in Bayesian statistics
- Medium Confidence: Disentanglement of aleatoric and epistemic uncertainty through evidential parameters is theoretically valid but needs further validation in complex real-world scenarios
- Low Confidence: Generalizability to highly non-linear, multi-modal, or heavy-tailed distributions beyond tested cases remains uncertain without extensive empirical validation

## Next Checks
1. Cross-domain robustness test: Evaluate method on synthetic datasets with varying distribution shapes (mixture models, heavy-tailed distributions) to assess NIG prior's ability to capture non-Gaussian uncertainties across diverse scenarios

2. Ablation study on evidence regularization: Systematically vary λ across wide range (0.1 to 1.0) on multiple datasets to quantify impact on quantile accuracy, uncertainty calibration, and model stability, providing clearer selection guidelines

3. Comparison with alternative priors: Implement and test alternative evidential distributions (Normal-Wishart, Student-t) as priors to determine if NIG prior is optimal or if other choices yield better uncertainty quantification