---
ver: rpa2
title: Language Models for Novelty Detection in System Call Traces
arxiv_id: '2309.02206'
source_url: https://arxiv.org/abs/2309.02206
tags:
- system
- language
- detection
- networks
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new open-source dataset of kernel traces
  comprising over 2 million web requests with seven distinct behaviors, and a new
  approach for detecting novelties based on language models. Language models are probability
  distributions over sequences of tokens, and since novelties deviate from previously
  observed behaviors by definition, they would be unlikely under the model.
---

# Language Models for Novelty Detection in System Call Traces

## Quick Facts
- **arXiv ID**: 2309.02206
- **Source URL**: https://arxiv.org/abs/2309.02206
- **Reference count**: 40
- **Primary result**: Achieves F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic

## Executive Summary
This paper introduces a novel approach for detecting novelties in system call traces using language models. The authors propose that novelties, by definition, deviate from previously observed behaviors and should therefore have low likelihood under a language model trained on normal system behaviors. Three architectures are evaluated - LSTM, Transformer, and Longformer - with the proposed methodology requiring minimal expert hand-crafting. The approach achieves high performance metrics while being data- and task-agnostic, with source code and trained models publicly available.

## Method Summary
The methodology involves training language models on system call traces to estimate the likelihood of sequences. The models predict the next token given previous ones, and novelty detection is performed using perplexity thresholds. Three architectures are evaluated: LSTM, Transformer, and Longformer. The system call traces include not only the call names but also arguments like return values, process names, and timestamps. These are represented using embeddings and encodings before being fed into the language models. The trained models are then used to detect novelties by calculating perplexity scores and comparing them against thresholds.

## Key Results
- Achieves F-score and AuROC greater than 95% on most novelties
- Language models effectively detect deviations from learned system call patterns
- All three architectures (LSTM, Transformer, Longformer) can process batches in under 100ms on a V100 GPU
- The approach is data- and task-agnostic, requiring minimal expert hand-crafting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language models assign low likelihood to sequences containing novelties because novelties deviate from the learned distribution of normal system call sequences.
- **Mechanism**: A left-to-right language model is trained on known behaviors to maximize the joint probability of sequences. Novel behaviors, by definition, do not match this distribution and therefore have a lower joint probability, which translates into higher perplexity.
- **Core assumption**: The perplexity metric effectively captures novelty because it accounts for sequence length, avoiding bias toward longer sequences.
- **Evidence anchors**:
  - [abstract] "Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model."
  - [section III-C] "The perplexity of a language model is a widely popular metric [22, 24] that measures its degree of uncertainty when a new token is generated, averaged over very long sequences."
- **Break condition**: If the novel behavior is similar in statistical distribution to normal behaviors (e.g., new users), the model may not detect it as novel.

### Mechanism 2
- **Claim**: Attention-based models like the Transformer and Longformer can model long-term dependencies in system call traces, improving detection of subtle novelties.
- **Mechanism**: The self-attention mechanism in Transformers computes pairwise compatibility scores between tokens, allowing the model to consider dependencies of arbitrary length. This is crucial for kernel traces, which are typically much longer than the effective context length of LSTMs.
- **Core assumption**: Long-term dependencies are important for novelty detection in system call traces.
- **Evidence anchors**:
  - [section II-B] "recurrent networks are unable to efficiently model long-term dependencies due to their iterative nature."
  - [section V-A] "The attention activation patterns of the Transformer on the in-distribution validation set... most of the attention learned is along the diagonal."
- **Break condition**: If the novel behavior relies on local patterns, the benefit of attention mechanisms may be minimal, as shown by the LSTM's comparable performance on some behaviors.

### Mechanism 3
- **Claim**: Using system call arguments (e.g., return values, process names) in the trace representation improves the model's ability to distinguish novel behaviors.
- **Mechanism**: Embeddings are used to represent inherently meaningful arguments, while cosine and sine encodings are applied to context-dependent arguments. This rich representation allows the model to capture more nuanced patterns in the system call sequences.
- **Core assumption**: The inclusion of system call arguments provides additional semantic information that aids in novelty detection.
- **Evidence anchors**:
  - [section III-A] "Our methodology diverges from that of [11] and [22] in three aspects... the addition requires the vectors to have the same dimension and preserves that dimension, which may be too small to store all the information, thus creating a bottleneck."
  - [section II-A] "arguments are valuable data that allow the model to make more informed and, ultimately, more accurate predictions."
- **Break condition**: If the system call arguments are not relevant to the novelty being detected, their inclusion may not provide a significant benefit.

## Foundational Learning

- **Concept**: Language models and perplexity
  - Why needed here: Understanding how language models assign probabilities to sequences and how perplexity is used to detect novelties.
  - Quick check question: How does the perplexity metric account for sequence length, and why is this important for novelty detection?

- **Concept**: Attention mechanisms and long-term dependencies
  - Why needed here: Understanding how attention mechanisms in Transformers and Longformers allow modeling of long-term dependencies, which is crucial for kernel traces.
  - Quick check question: What is the key difference between the attention mechanisms in LSTMs and Transformers, and how does this impact their ability to model long-term dependencies?

- **Concept**: System call trace representation
  - Why needed here: Understanding how system call traces are represented as sequences of tokens, including system call names and arguments, and how this representation is used as input to language models.
  - Quick check question: What are the advantages and disadvantages of using different system call trace representations (e.g., system call names only, bag-of-words, joint representation with arguments)?

## Architecture Onboarding

- **Component map**: Data collection (wrk2, Apache2, MySQL, LTTng) -> Data processing (trace parsing, argument embedding/encoding, sequence generation) -> Model training (LSTM, Transformer, Longformer) -> Novelty detection (perplexity calculation, threshold-based classification)
- **Critical path**: Data collection → Data processing → Model training → Novelty detection
- **Design tradeoffs**:
  - Model complexity vs. real-time performance: Larger models (e.g., Transformer) may offer better accuracy but require more computational resources and may not be suitable for real-time detection.
  - Trace representation: Using system call arguments improves model performance but increases data complexity and processing requirements.
  - Dataset size and diversity: A larger and more diverse dataset improves model generalization but requires more resources for data collection and processing.
- **Failure signatures**:
  - High false positive rate: The model may be too sensitive to normal variations in system behavior.
  - High false negative rate: The model may not be able to detect subtle novelties or novelties that are similar to normal behaviors.
  - Computational bottlenecks: The model may not be able to process traces in real-time due to computational constraints.
- **First 3 experiments**:
  1. Train and evaluate a simple n-gram baseline model on the dataset to establish a performance baseline.
  2. Train and evaluate an LSTM model with and without system call arguments to assess the impact of argument inclusion on novelty detection performance.
  3. Train and evaluate a Transformer model with different sequence truncation lengths to determine the optimal trade-off between model performance and computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do LSTM, Transformer, and Longformer architectures compare in terms of novelty detection performance when processing longer sequences with richer system call arguments?
- **Basis in paper**: [explicit] The paper explicitly compares these three architectures on novelty detection performance, noting that LSTM performs better than Transformer and Longformer on 3 out of 6 behaviors due to its inductive bias toward local dependencies.
- **Why unresolved**: The paper only considers relatively short sequences and does not explore how these architectures would perform with significantly longer sequences or with richer system call arguments beyond the basic set used.
- **What evidence would resolve it**: Empirical evaluation of these architectures on sequences an order of magnitude longer with comprehensive system call arguments would provide clear evidence of their relative performance.

### Open Question 2
- **Question**: Can the proposed perplexity-based novelty detection methodology scale effectively to handle the vast diversity of behaviors in real-world production environments?
- **Basis in paper**: [inferred] The paper demonstrates high performance on a controlled dataset with seven distinct behaviors, but acknowledges that real-world environments contain much greater behavioral diversity and potential novel patterns.
- **Why unresolved**: The current dataset, while large, is limited to specific server-side scenarios and doesn't capture the full spectrum of potential novel behaviors that could occur in complex production systems.
- **What evidence would resolve it**: Testing the methodology on diverse production datasets spanning multiple application domains and system types would demonstrate its scalability and generalization capabilities.

### Open Question 3
- **Question**: What is the optimal balance between computational efficiency and detection accuracy when implementing this novelty detection approach in real-time monitoring systems?
- **Basis in paper**: [explicit] The paper notes that all three architectures can process batches in under 100ms on a V100 GPU, but doesn't explore the trade-offs between model complexity, detection accuracy, and computational requirements for different deployment scenarios.
- **Why unresolved**: The paper doesn't investigate how these trade-offs change with different hardware configurations, data volumes, or detection latency requirements that would be critical for practical deployment.
- **What evidence would resolve it**: Systematic evaluation of detection accuracy and computational requirements across different hardware platforms and operational constraints would provide guidance for practical implementation decisions.

## Limitations
- The approach assumes novelties will always manifest as low-probability sequences, which may not hold for subtle novelties similar to normal behaviors.
- Evaluation is conducted on a single dataset generated under controlled conditions, limiting generalizability to real-world environments.
- Manual tuning of hyperparameters introduces potential bias and limits reproducibility of results.

## Confidence
- **High confidence**: The core mechanism of using language model perplexity for novelty detection is well-established and the empirical results are robust across multiple architectures (LSTM, Transformer, Longformer).
- **Medium confidence**: The superiority of attention-based models for long system call traces is supported but the margin of improvement varies significantly across different novelty types.
- **Low confidence**: Claims about real-time applicability are not thoroughly validated, as computational requirements for larger models are not fully characterized under different hardware constraints.

## Next Checks
1. Test the models on a more diverse dataset including system call traces from different applications and operating systems to assess generalizability.
2. Evaluate detection performance on simulated subtle novelties that are statistically similar to normal behaviors to identify potential blind spots in the approach.
3. Conduct runtime profiling of all three architectures under different hardware configurations to determine practical deployment constraints and identify the optimal model for real-time applications.