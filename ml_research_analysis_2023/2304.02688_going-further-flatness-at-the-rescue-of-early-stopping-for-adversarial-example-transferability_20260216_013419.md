---
ver: rpa2
title: 'Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example
  Transferability'
arxiv_id: '2304.02688'
source_url: https://arxiv.org/abs/2304.02688
tags:
- transferability
- surrogate
- rate
- trained
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the hypothesis that early stopping benefits
  transferability by reducing non-robust features. Instead, it shows transferability
  improves when the learning rate decays, coinciding with a drop in sharpness in the
  loss landscape.
---

# Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability

## Quick Facts
- arXiv ID: 2304.02688
- Source URL: https://arxiv.org/abs/2304.02688
- Reference count: 40
- Primary result: RFN improves adversarial transferability by up to 47 percentage points over early stopping

## Executive Summary
This work challenges the established belief that early stopping improves adversarial transferability by selecting robust features. Instead, it demonstrates that transferability improves when learning rate decays, coinciding with reduced sharpness in the loss landscape. Based on this insight, the authors propose RFN, a method that explicitly minimizes sharpness over large neighborhoods using SAM, achieving state-of-the-art transferability without requiring early stopping.

## Method Summary
The paper proposes RFN (Robust Feature Normalization), which modifies SAM by using a large neighborhood size (ρ=0.4) to minimize sharpness during training. Unlike early stopping, RFN trains models for the full number of epochs while maintaining high transferability. The method is evaluated on CIFAR-10 and ImageNet using ResNet architectures, comparing transferability against early stopping, standard SAM, and other state-of-the-art techniques. RFN is shown to consistently outperform early stopping by up to 47 percentage points.

## Key Results
- RFN achieves up to 47 percentage point improvement in transferability over early stopping
- Transferability peaks during learning rate decay, not at early stopping points
- RFN provides a better base model for complementary transferability techniques
- Sharpness-aware minimization with large neighborhoods is key to improved transferability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early stopping benefits transferability by affecting loss landscape dynamics, not by selecting robust features
- Mechanism: Learning rate decay causes SGD to transition from "crossing the valley" to "crawling down to the valley," temporarily reducing sharpness and maximizing transferability
- Core assumption: Sharpness in parameter space is inversely correlated with transferability
- Evidence anchors: [abstract] "RFN...minimizes loss sharpness during training in order to maximize transferability"; [section 4] "Transferability peaks when the LR decays"
- Break condition: If sharpness and transferability become uncorrelated in certain architectures or datasets

### Mechanism 2
- Claim: RFN improves transferability by explicitly minimizing sharpness over unusually large neighborhoods
- Mechanism: Maximizing flat neighborhood size (ρ=0.4) avoids deep, sharp holes where representations become too specific, maintaining generic representations that transfer better
- Core assumption: Larger flat neighborhoods correspond to more generic representations
- Evidence anchors: [abstract] "We discover that the strong SAM regularization from large flat neighborhoods tightly links to transferability"; [section 5] "RFN significantly increases the transferability over the original SAM"
- Break condition: If large flat neighborhoods degrade natural accuracy too much

### Mechanism 3
- Claim: RFN provides a better base model for complementary transferability techniques
- Mechanism: RFN's flatter minima provide more stable gradients that complementary techniques (GN, SGM, DI, etc.) can exploit
- Core assumption: Complementary techniques perform better when applied to flatter minima
- Evidence anchors: [abstract] "the best sharpness-aware minimizers prove competitive with other training methods and complement existing transferability techniques"
- Break condition: If complementary techniques show no improvement or degradation when applied to RFN-trained models

## Foundational Learning

- Concept: Sharpness-aware minimization (SAM)
  - Why needed here: SAM is the core optimizer underlying RFN, and understanding how it minimizes maximum loss in neighborhoods is essential
  - Quick check question: What does SAM do differently from standard SGD during each update step?

- Concept: Transferability in adversarial examples
  - Why needed here: The paper's entire premise is about improving cross-model adversarial example effectiveness
  - Quick check question: How does early stopping traditionally improve transferability according to the RFs/NRFs hypothesis?

- Concept: Loss landscape geometry
  - Why needed here: The paper's insights depend on understanding how training dynamics explore the parameter space
  - Quick check question: What is the difference between "crossing the valley" and "crawling down to the valley" phases in SGD?

## Architecture Onboarding

- Component map: Dataset/Model -> SAM Optimizer (ρ=0.4) -> Training Loop -> Transferability Evaluation -> Attack Generation
- Critical path: 1) Load dataset and model 2) Initialize SAM optimizer with ρ=0.4 3) Train for full epochs without early stopping 4) Evaluate transferability on validation targets 5) Generate adversarial examples using combined techniques
- Design tradeoffs: Larger ρ values improve transferability but may degrade natural accuracy; full training vs. early stopping eliminates need for stopping criteria; computational cost requires 2x backward passes per iteration vs. SGD
- Failure signatures: Natural accuracy drops significantly below baseline; transferability plateaus or decreases despite larger ρ; training becomes unstable or diverges with high ρ values
- First 3 experiments: 1) Train RFN with ρ=0.4 on CIFAR-10 and compare transferability to early-stopped SGD 2) Vary ρ from 0.05 to 1.0 and measure trade-off between transferability and natural accuracy 3) Combine RFN with GN and MI techniques and evaluate on ImageNet targets

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Optimal ρ value (0.4) appears somewhat arbitrary and may not generalize across all architectures and datasets
- Computational overhead of RFN (2x backward passes per iteration) is a practical limitation not fully addressed in terms of training time or resource requirements
- The causal link between sharpness minimization and transferability remains inferential rather than definitively proven

## Confidence
- Sharpness-transferability correlation: High
- RFN effectiveness on tested benchmarks: High
- Mechanism explanation (why flatness improves transferability): Medium
- Generalization across architectures/datasets: Low

## Next Checks
1. Test RFN on diverse architectures beyond ResNet (e.g., Vision Transformers, ConvNext) to assess architecture dependence
2. Conduct ablation studies varying ρ across a wider range to better understand the sharpness-transferability trade-off
3. Implement theoretical analysis connecting loss landscape flatness to gradient alignment across models