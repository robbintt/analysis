---
ver: rpa2
title: Is post-editing really faster than human translation?
arxiv_id: '2312.12660'
source_url: https://arxiv.org/abs/2312.12660
tags:
- translation
- speed
- values
- revision
- outliers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This large-scale study analyzed real-world translation productivity
  data from 879 linguists across 11 language pairs over 2.5 years, covering 90 million
  words. It compared post-editing (PE) and human translation (HT) speeds at both translation
  and revision stages.
---

# Is post-editing really faster than human translation?

## Quick Facts
- arXiv ID: 2312.12660
- Source URL: https://arxiv.org/abs/2312.12660
- Reference count: 14
- PE was 66% faster than HT on average at translation stage

## Executive Summary
This large-scale study analyzed real-world translation productivity data from 879 linguists across 11 language pairs over 2.5 years, covering 90 million words. The research compared post-editing (PE) and human translation (HT) speeds at both translation and revision stages. Results showed PE was significantly faster than HT on average, with PE revision being 38% faster than HT revision. The study found high variability in PE speeds across language pairs and challenged the use of edit distance as a reliable productivity assessment metric.

## Method Summary
The study used real-world productivity data from a language service provider, analyzing words processed per hour (WPH) and edit distance metrics collected through CAT tool memoQ. The analysis employed nonparametric statistical methods including Kolmogorov-Smirnov normality tests and Kendall's tau-b correlations due to non-normal data distribution. Outliers were identified using interquartile range (IQR) calculations, and correlations between temporal effort (WPH) and technical effort (edit distance) were examined.

## Key Results
- PE was 66% faster than HT on average at translation stage
- PE revision was 38% faster than HT revision
- PE speeds ranged 530-1,440 WPH (translation) and 1,990-5,540 WPH (revision)
- Weak correlation between edit distance and PE speed challenges its use for productivity assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-editing speed improvements depend heavily on edit distance magnitude.
- Mechanism: The correlation between temporal effort (WPH) and technical effort (edit distance) determines whether PE is faster than HT. When edit distance is low, PE is faster; when high, PE may slow down.
- Core assumption: Edit distance is a meaningful proxy for PE productivity.
- Evidence anchors:
  - [abstract] "edit distance showed weak correlation with PE speed, challenging its use for productivity assessment."
  - [section] "Overall, there was a moderate, negative correlation between speed and PED – which accounts for edits to the MT output only – whilst there was only a weak, negative correlation between speed and PED-TMR."
- Break condition: If edit distance increases but speed remains constant or improves, the correlation mechanism breaks.

### Mechanism 2
- Claim: PE speed variability is higher than HT due to MT quality fluctuations.
- Mechanism: The quality of MT output varies across language pairs and domains, causing inconsistent PE speed. HT has more consistent speed because human translators control the initial quality.
- Core assumption: MT quality variability directly impacts PE speed consistency.
- Evidence anchors:
  - [section] "There was great variability in the number of words processed in one hour, with sometimes high differences across language pairs."
  - [section] "WPH variability was considerably higher in post-editing than in human translation."
- Break condition: If MT quality becomes uniformly high across all language pairs, PE speed variability would decrease.

### Mechanism 3
- Claim: Revision speed differs between PE and HT due to perceived translation quality.
- Mechanism: Reviewers may perceive PE outputs as higher quality, leading to faster revision. Alternatively, some reviewers may treat PE as inferior, causing slower, more thorough revision.
- Core assumption: Reviewer perception of PE quality affects revision speed.
- Evidence anchors:
  - [section] "It is surprising to see such a striking difference between HT and PE at the revision stage, because (a) the same quality standards apply to the translation and revision of both HT and PE, and (b) the revision process is not supposed to present significant differences whether carried out as part of a PE or HT workflow."
  - [section] "I hypothesise that some of the possible reasons behind this result may be that (1) although linguists are requested to apply the same quality standards to HT and PE, some of them might perceive the post-editing practice as inferior to the human translation practice, which could lead them to be less accurate and complete PE revision more quickly; and/or (2) the quality of PE outputs might be frequently higher than that of HT outputs, enabling linguists to work faster on PE revision."
- Break condition: If quality standards are strictly enforced and perception biases eliminated, the speed difference would disappear.

## Foundational Learning

- Concept: Non-parametric statistical tests
  - Why needed here: The data distribution was not normal, requiring non-parametric methods like Kolmogorov-Smirnov and Kendall's tau-b.
  - Quick check question: When should you use non-parametric tests instead of parametric ones?

- Concept: Interquartile range (IQR) for outlier detection
  - Why needed here: To identify extreme WPH values that could skew averages and correlations.
  - Quick check question: How do you calculate the IQR and use it to identify outliers?

- Concept: Edit distance metrics (Levenshtein algorithm)
  - Why needed here: To measure technical effort in PE and correlate it with temporal effort.
  - Quick check question: What does edit distance measure, and how is it calculated?

## Architecture Onboarding

- Component map:
  - CAT tool (memoQ) -> Data collection (WPH and edit distance) -> SPSS analysis (Kolmogorov-Smirnov, Kendall's tau-b) -> Manual validation

- Critical path:
  1. Collect WPH and edit distance data from CAT tool
  2. Perform normality tests to determine statistical approach
  3. Calculate correlations between WPH and edit distance
  4. Identify outliers and recalculate statistics without them
  5. Compare speed metrics across language pairs and stages

- Design tradeoffs:
  - Using real-world data vs. experimental data: Real-world data is more representative but less controlled
  - Excluding time spent on research vs. including it: Excludes breaks but may underestimate true effort
  - Using average WPH vs. median: Averages are sensitive to outliers, medians are more robust

- Failure signatures:
  - High standard deviation in WPH values indicating inconsistent productivity
  - Weak or no correlation between edit distance and speed suggesting edit distance is not a good productivity proxy
  - Outliers that are valid cases (not measurement errors) indicating true variability in performance

- First 3 experiments:
  1. Compare average WPH including vs. excluding outliers to see impact on PE vs. HT speed comparison
  2. Calculate Kendall's tau-b correlation between WPH and edit distance both including and excluding TM matches
  3. Analyze WPH variability using IQR and standard deviation to determine if PE or HT is more consistent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying reasons for the observed speed differences between post-editing and human translation at the revision stage, given that the same quality standards apply to both workflows?
- Basis in paper: [explicit] The paper notes that PE revision was 38% faster than HT revision on average, but the reasons for this difference are not definitively established, only hypothesized.
- Why unresolved: The study did not experimentally test the hypotheses (e.g., perceived quality differences, text structure) but only speculated about potential explanations.
- What evidence would resolve it: Controlled experimental studies comparing revision quality and speed under identical conditions, possibly including eye-tracking or think-aloud protocols to understand revisers' cognitive processes.

### Open Question 2
- Question: How do individual translator skills and working practices specifically impact translation speed, and can these factors be systematically measured or optimized?
- Basis in paper: [explicit] The paper found high variability in speed among individual linguists, but did not investigate which specific skills or practices contributed to faster or slower performance.
- Why unresolved: The study focused on comparing PE vs HT at aggregate levels rather than analyzing individual translator characteristics or strategies.
- What evidence would resolve it: Detailed analysis of translator profiles, working methods, and their correlation with speed metrics, possibly using longitudinal data tracking the same translators across different projects.

### Open Question 3
- Question: To what extent do source text features (e.g., complexity, domain, repetitiveness) influence translation speed in post-editing versus human translation?
- Basis in paper: [inferred] The paper mentions that source texts in PE were typically "well-structured [and] often repetitive," but did not systematically analyze how different text features affect speed.
- Why unresolved: The exploratory analysis did not control for or measure specific source text characteristics across the dataset.
- What evidence would resolve it: Controlled experiments varying source text features while keeping other factors constant, or statistical analysis correlating text feature metrics with translation speed.

## Limitations
- Edit distance as productivity metric shows weak correlation with actual translation speed
- High variability in post-editing speeds across language pairs limits generalizability
- Dataset represents single language service provider context

## Confidence
- PE being faster than HT at translation stage: High confidence
- PE revision being faster than HT: Medium confidence
- Edit distance correlation with PE speed: Low confidence

## Next Checks
1. Replicate correlation analysis with larger dataset spanning multiple LSPs to verify edit distance relationship stability
2. Conduct controlled experiment isolating MT quality as variable to quantify its impact on PE speed variability
3. Perform qualitative interviews with linguists to validate hypothesized reasons for revision speed differences between PE and HT