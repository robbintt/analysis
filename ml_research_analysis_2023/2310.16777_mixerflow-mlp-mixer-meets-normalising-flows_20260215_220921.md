---
ver: rpa2
title: 'MixerFlow: MLP-Mixer meets Normalising Flows'
arxiv_id: '2310.16777'
source_url: https://arxiv.org/abs/2310.16777
tags:
- mixerflow
- layer
- image
- layers
- flows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MixerFlow is a novel normalising flow architecture based on the
  MLP-Mixer architecture, designed to unify generative and discriminative modelling.
  By leveraging weight sharing and an efficient mechanism for patch-based transformations,
  MixerFlow improves density estimation on image datasets and scales well with increasing
  resolution.
---

# MixerFlow: MLP-Mixer meets Normalising Flows

## Quick Facts
- arXiv ID: 2310.16777
- Source URL: https://arxiv.org/abs/2310.16777
- Authors: 
- Reference count: 9
- Key outcome: MixerFlow achieves 3.46 bits per dimension on CIFAR-10, outperforming Glow-based architectures while using fewer parameters.

## Executive Summary
MixerFlow is a novel normalizing flow architecture that integrates the MLP-Mixer design with flow-based generative models. By leveraging weight sharing and efficient patch-based transformations, it improves density estimation on image datasets while maintaining scalability. The architecture uses alternating channel-mixing and patch-mixing flows, enhanced by shift layers to handle boundary artifacts. Experiments show superior performance on CIFAR-10, ImageNet32, and structured datasets, with better downstream task embeddings than Glow-based models.

## Method Summary
MixerFlow partitions images into patches, projects them into a mixer-matrix, and applies alternating channel-mixing and patch-mixing flows. Weight sharing across patches and channels reduces parameters while maintaining expressiveness. Shift layers dynamically redefine patch extraction to mitigate boundary artifacts. The architecture integrates various flow methods (coupling, Lipschitz-constrained, FFJORD) and supports hybrid training with supervised classification heads.

## Key Results
- Achieves 3.46 bits per dimension on CIFAR-10, outperforming Glow (3.62 bpd)
- Scales well with resolution, using ~half the parameters of Glow-based baselines
- Better downstream task performance with MixerFlow embeddings (lower classification loss) compared to Glow/Neural Spline embeddings

## Why This Works (Mechanism)

### Mechanism 1
MixerFlow leverages MLP-Mixer's weight sharing to efficiently model patch-based transformations in normalizing flows. The architecture partitions images into patches, projects them into lower-dimensional vectors forming a mixer-matrix, and applies channel-mixing and patch-mixing flows iteratively. Weight sharing across patches and channels reduces parameter count while maintaining expressiveness.

### Mechanism 2
Shift layers mitigate boundary artifacts by dynamically redefining patch extraction during training. After initial patch partitioning, shift layers reshape the mixer-matrix back to image dimensions, carve a frame leaving the inner part, and apply flows to a "shifted-mixer-matrix" before reinserting the frame. This distributes transformations near boundaries.

### Mechanism 3
MixerFlow embeddings provide more informative representations than Glow-based embeddings for downstream tasks. By training hybrid models (flow + classifier head) with MixerFlow, lower classification loss and higher accuracy are achieved compared to Glow/Neural Spline embeddings, indicating better feature learning.

## Foundational Learning

- **Normalizing Flows and the change of variables theorem**
  - Why needed: MixerFlow is a normalizing flow; understanding invertibility and Jacobian determinants is essential to grasp its design
  - Quick check: Why must normalizing flows use bijective transformations, and how does the change of variables theorem enable density estimation?

- **MLP-Mixer architecture and patch-based processing**
  - Why needed: MixerFlow is built on MLP-Mixer; understanding patch partitioning, token-mixing, and channel-mixing is key to its flow design
  - Quick check: How does MLP-Mixer's weight sharing between patches and channels differ from convolution, and why is this beneficial for flow models?

- **Coupling layers and affine transformations**
  - Why needed: MixerFlow uses affine coupling layers; understanding how they split inputs and apply learned transformations is critical
  - Quick check: In an affine coupling layer, which partition undergoes the identity transformation and which undergoes the affine transformation, and why?

## Architecture Onboarding

- **Component map:** Input → 1x1 conv → Patch partitioning → Mixer-matrix → Channel-mixing flow → Patch-mixing flow → Repeat (alternating) → Output
- **Critical path:** 1. Patch partitioning and mixer-matrix formation 2. Alternating channel-mixing and patch-mixing flows 3. Shift layers to handle boundaries 4. Flow layers with residual blocks and ActNorm 5. Final output through invertible transformations
- **Design tradeoffs:** Weight sharing vs. parameter efficiency (reduces parameters but may lose local spatial detail); Patch size choice (larger patches reduce parameter count but may miss fine structure); Shift layer frequency (more shifts help boundaries but increase compute and complexity)
- **Failure signatures:** Training instability (check ActNorm initialization and identity initialization of linear layers); Poor density estimation (verify patch size and mixing layer count; inspect for checkerboard artifacts); Weak downstream performance (confirm hybrid training setup and classifier head integration)
- **First 3 experiments:** 1. CIFAR-10 density estimation with default 30 layers, patch size 4, shift every 4 layers; compare NLL to Glow baseline 2. Ablation: Remove shift layers; measure impact on boundary artifacts and NLL 3. Hybrid training: Add linear classifier head to pre-trained MixerFlow; measure classification loss and accuracy vs Glow embeddings

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the scalability of MixerFlow compare to Glow-based architectures when increasing image resolution beyond 64x64?
  - Basis: Paper mentions scalability but only tested up to 64x64
  - Resolution: Experiments on 128x128 or 256x256 datasets comparing performance and parameter efficiency

- **Open Question 2:** What is the impact of different types of flow layers (e.g., coupling, Lipschitz-constrained, FFJORD) on the performance of MixerFlow?
  - Basis: Paper states MixerFlow can integrate various flow methods but lacks experimental comparison
  - Resolution: Implementing and evaluating MixerFlow with different flow layer types on standard datasets

- **Open Question 3:** How does the performance of MixerFlow on datasets with strong structural correlations (e.g., periodic or permutation-invariant) compare to Glow-based architectures?
  - Basis: Paper demonstrates performance on specific structural correlations but doesn't extensively explore other types
  - Resolution: Testing MixerFlow on datasets with different structural correlations (hierarchical, spatial, temporal) and comparing performance

## Limitations
- Shift layer mechanism lacks extensive empirical validation beyond preliminary experiments
- Hybrid training approach requires careful balancing between generative and discriminative objectives
- Performance on datasets requiring highly localized features may be suboptimal compared to convolution-based approaches

## Confidence
- High confidence: Core density estimation results on CIFAR-10 and ImageNet32 (3.46 bpd on CIFAR-10)
- Medium confidence: Shift layer effectiveness and downstream task performance
- Medium confidence: Architectural design choices (patch size, shift frequency)

## Next Checks
1. Ablation study: Remove shift layers from MixerFlow and measure impact on CIFAR-10 density estimation performance and boundary artifact visualization
2. Scalability test: Train MixerFlow on 128×128 resolution images to verify architectural scalability claims beyond 64×64
3. Downstream task comparison: Implement hybrid training with Glow-based embeddings on the same datasets and tasks to directly compare embedding quality for classification