---
ver: rpa2
title: Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned
  Reinforcement Learning
arxiv_id: '2312.04736'
source_url: https://arxiv.org/abs/2312.04736
tags:
- feedback
- task
- language
- generalisation
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether providing agents with natural language
  feedback can improve their ability to generalize in sparse-reward environments with
  language-specified goals. The authors propose the Feedback Decision Transformer
  (FDT), which extends the Decision Transformer architecture to condition action generation
  on automatically generated language feedback, in addition to or instead of traditional
  numerical feedback like return-to-go or goal instructions.
---

# Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.04736
- Source URL: https://arxiv.org/abs/2312.04736
- Authors: 
- Reference count: 40
- One-line primary result: Language feedback can improve generalization in sparse-reward goal-conditioned RL by providing complementary information to numerical rewards

## Executive Summary
This paper investigates whether natural language feedback can improve an agent's ability to generalize in sparse-reward environments with language-specified goals. The authors propose the Feedback Decision Transformer (FDT), which extends the Decision Transformer architecture to condition action generation on automatically generated language feedback in addition to traditional numerical feedback. Experiments on the BabyAI benchmark demonstrate that language feedback improves generalization performance, particularly when extrapolating to new goal locations and for levels with longer horizons. The results suggest that language feedback provides a useful complementary signal to numerical rewards and goal instructions, and can even replace them in some cases.

## Method Summary
The Feedback Decision Transformer (FDT) extends the Decision Transformer architecture by incorporating language feedback as an additional conditioning signal. The method uses a frozen SentenceBERT encoder to convert mission strings and feedback into fixed-length embeddings, which are then concatenated with state embeddings and fed into a GPT2-based transformer decoder to predict actions. Feedback is automatically generated using hand-crafted rules that decompose complex language instructions into granular sub-goals, with Task Feedback indicating progress toward goals and Rule Feedback explaining why actions fail. The model is trained with cross-entropy loss on action prediction using datasets generated by random policies in the BabyAI environment.

## Key Results
- Conditioning on language feedback improves generalization performance compared to baselines using only return-to-go or mission strings
- Task Feedback is most effective for simple tasks and single-room environments, while Rule Feedback benefits complex tasks with longer horizons
- Language feedback can substitute for numerical rewards in simple environments, but combining both yields the best performance overall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language feedback provides complementary information to sparse numerical rewards by explicitly describing the agent's progress toward sub-goals
- Mechanism: Feedback generation rules decompose complex instructions into granular sub-goals, with Task Feedback stating when sub-goals are completed and Rule Feedback explaining action failures
- Core assumption: Feedback generation rules accurately reflect true progress and action validity
- Evidence anchors: [abstract] "automatically generated language feedback from the environment dynamics and goal condition success" [section 4] "decompose high-level goal instructions into granular sub-goals"

### Mechanism 2
- Claim: Language feedback can substitute for missing or insufficient numerical reward signals
- Mechanism: Acts as dense intermediate reward signal when environment provides only sparse terminal rewards
- Core assumption: Language feedback contains sufficient information to guide the agent toward the goal
- Evidence anchors: [abstract] "conditioning on language feedback can boost generalisation performance" [section 5.2] "Replacing RTG with feedback improves OOD performance"

### Mechanism 3
- Claim: Different types of language feedback are suited for different task complexities
- Mechanism: Task Feedback better for simple tasks and single rooms; Rule Feedback better for complex tasks and mazes
- Core assumption: Agent can learn to distinguish and use different feedback types appropriately
- Evidence anchors: [section 5.2] "Combining RTG with Task Feedback boosts performance, especially on single-room levels" and "Combining mission with Rule Feedback boosts performance, especially on levels with longer horizons"

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Agent has access only to partial observations (images) and must learn policy mapping observations to actions
  - Quick check question: What is the key difference between a POMDP and an MDP, and why is it relevant in the BabyAI environment?

- Concept: Goal-Conditioned Reinforcement Learning
  - Why needed here: Agent must learn to achieve goals specified in natural language
  - Quick check question: How does goal-conditioning differ from standard RL with a fixed reward function, and what challenges does it introduce?

- Concept: Sequence Modeling with Transformers
  - Why needed here: Decision Transformer treats RL as sequence modeling problem using transformer's ability to model long-range dependencies
  - Quick check question: How does the transformer's attention mechanism help in modeling the sequential nature of RL trajectories, and what are the limitations of using a fixed context length?

## Architecture Onboarding

- Component map: Image encoder -> Transformer decoder -> Action; RTG/Mission/Feedback -> SentenceBERT encoder -> Transformer decoder
- Critical path: Observation → Image encoder → Transformer decoder → Action; RTG/Mission/Feedback → SentenceBERT encoder → Transformer decoder
- Design tradeoffs:
  - Using frozen SentenceBERT vs. training custom language model: Faster training but may not be optimally tuned for BabyAI domain
  - Concatenating all inputs vs. separate attention heads: Simpler architecture but may not capture complex interactions between modalities
  - Using fixed context length: Limits memory requirements but may miss long-range dependencies
- Failure signatures:
  - Poor performance on IID tasks: Likely issues with core Decision Transformer architecture or training procedure
  - Poor performance on OOD tasks: Likely issues with feedback generation or agent's ability to generalize from feedback
  - Collapse to single action: Likely issues with action space or loss function
- First 3 experiments:
  1. Train and evaluate baseline Decision Transformer (RTG-only) on simple BabyAI level (e.g., GoToObj)
  2. Add Task Feedback to baseline and evaluate if it improves performance, particularly on OOD tasks
  3. Add Rule Feedback to baseline and evaluate if it improves performance, particularly on complex tasks with physical constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to more complex environments beyond 2D gridworlds?
- Basis in paper: [explicit] Paper acknowledges this limitation and suggests it as future work
- Why unresolved: Experiments limited to BabyAI 2D gridworld environment
- What evidence would resolve it: Implementing and evaluating method on 3D navigation or robotic manipulation benchmarks

### Open Question 2
- Question: How does quality and diversity of automatically generated feedback impact learning performance?
- Basis in paper: [inferred] Paper suggests replacing hand-crafted rules with LLM-generated feedback could provide additional flexibility
- Why unresolved: Paper doesn't explore impact of varying feedback quality or diversity
- What evidence would resolve it: Experiments varying feedback quality/diversity and measuring impact on learning performance

### Open Question 3
- Question: How does the proposed method perform in the online setting where agent can interact with environment during training?
- Basis in paper: [inferred] Paper focuses on offline RL and mentions feedback approach would "translate directly to the online setting"
- Why unresolved: Paper doesn't evaluate method in online setting
- What evidence would resolve it: Implementing and evaluating method in online setting on benchmark environments

## Limitations
- Evaluation confined to BabyAI environment with synthetic feedback rules, limiting generalizability to real-world applications
- Feedback generation based on known environment dynamics rather than human-provided feedback
- Experiments focus on relatively simple instruction-following tasks

## Confidence
- Language feedback improves generalization in sparse-reward goal-conditioned RL: High
- Task and Rule Feedback serve different purposes across task complexities: Medium
- Feedback can completely replace numerical rewards in complex environments: Low

## Next Checks
1. Replace synthetic feedback generation rules with human-annotated feedback on subset of trajectories to evaluate FDT performance with real feedback
2. Test FDT on more challenging instruction-following benchmarks like ALFWorld or VirtualHome to assess benefits in environments with longer horizons and complex goal structures
3. Systematically vary granularity of Task and Rule Feedback to determine optimal feedback density for different task complexities