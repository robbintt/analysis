---
ver: rpa2
title: Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries
arxiv_id: '2312.02079'
source_url: https://arxiv.org/abs/2312.02079
tags:
- data
- learning
- machine
- deep
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We demonstrate that imputation and alignment procedures are unnecessary
  for Deep Learning models when dealing with sparse and irregular bioprocess data.
  Deep Set Neural Networks equipped with triplet encoding of the input data can successfully
  handle bio-process data without any need for imputation or alignment procedures.
---

# Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries

## Quick Facts
- arXiv ID: 2312.02079
- Source URL: https://arxiv.org/abs/2312.02079
- Authors: 
- Reference count: 6
- Primary result: Deep Set Neural Networks with triplet encoding successfully handle sparse, irregular bioprocess data without imputation or alignment, performing on par with or better than traditional methods.

## Executive Summary
This paper demonstrates that Deep Set Neural Networks equipped with triplet encoding can directly process sparse and irregular bioprocess time series data without requiring imputation or alignment procedures. The method represents each measurement as a (timestamp, channel indicator, value) triplet and uses permutation-invariant aggregation to make predictions. The approach is tested on synthetic data generated from macrokinetic growth models and shows performance comparable to or better than traditional fitting procedures and imputation-based methods.

## Method Summary
The method uses triplet encoding to represent asynchronous time series data, where each measurement is encoded as a tuple of (timestamp, channel indicator, measured value). A Deep Set Neural Network architecture processes these triplets through an extractor network that maps them to a latent space, followed by permutation-invariant summation aggregation. The aggregated representation is then conditioned on the target timestamp to produce forecasts. The entire architecture is trained end-to-end without requiring data alignment or imputation.

## Key Results
- Deep Set Networks with triplet encoding successfully handle sparse and irregular bioprocess data without imputation or alignment
- The method performs on par with or better than traditional fitting procedures and imputation-based methods
- All tested machine learning models outperformed mechanistic model fitting on synthetic data, particularly for the E. coli model
- Performance differences between Deep Set Networks and alignment-based methods were statistically significant but practically negligible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep Set Networks with triplet encoding can directly process sparse and irregular time series without imputation or alignment
- Mechanism: The triplet encoding represents each measurement as a tuple (timestamp, channel indicator, value), preserving temporal structure while eliminating the need for a regular grid. The Deep Set architecture then uses a permutation-invariant summation over these triplets, making the model agnostic to measurement order and timing irregularities.
- Core assumption: The triplet format captures all necessary information for forecasting, and the Deep Set architecture can learn meaningful representations from these unordered triplets.
- Evidence anchors:
  - [abstract] "Deep Set Neural Networks equipped with triplet encoding of the input data can successfully handle bio-process data without any need for imputation or alignment procedures"
  - [section 2] "We represent timeseries by the so-called triplet format (Horn et al., 2020; Yalavarthi et al., 2022), which encodes each single measurement by the tuple of timestamp, channel indicator and measured value"
  - [corpus] Weak evidence - no direct corpus support for triplet encoding effectiveness, but the approach is mentioned in related work on asynchronous time series processing
- Break condition: If the model requires cross-channel dependencies that cannot be captured by individual triplets, or if the temporal dynamics are too complex for the permutation-invariant summation to preserve

### Mechanism 2
- Claim: The Deep Set architecture with timestamp conditioning can forecast future values from past observations
- Mechanism: The extractor network processes each (timestamp, channel, value) triplet into a latent representation. The aggregator then combines these representations via summation and conditions on the target timestamp to produce predictions. This allows the model to learn temporal dependencies without requiring a fixed time grid.
- Core assumption: The combination of timestamp conditioning and permutation-invariant aggregation can capture the necessary temporal dynamics for forecasting
- Evidence anchors:
  - [section 2] "To employ Deep Sets for forecasting, we adopt a structure similar to that of Conditional Neural Processes... the aggregator is additionally supplied with a timestamp τ in which predictions p are requested"
  - [section 3] "we focus on forecasting... asked to predict future states of the system ('forecasting' part)"
  - [corpus] Moderate evidence - Conditional Neural Processes are established for similar tasks, but specific effectiveness for bioprocess forecasting needs validation
- Break condition: If the forecasting horizon is too long for the model to maintain temporal context through the summation operation

### Mechanism 3
- Claim: Deep Learning models can outperform mechanistic models even when data is generated from the same underlying model
- Mechanism: The Deep Learning models learn direct mappings from observations to predictions without being constrained by the assumptions of the mechanistic model. This allows them to capture complex patterns and noise characteristics that the mechanistic model fitting might miss due to local minima or model misspecification.
- Core assumption: The data contains sufficient information for the Deep Learning model to learn accurate mappings without explicit mechanistic structure
- Evidence anchors:
  - [section 3] "Surprisingly, all Machine Learning models surpass fitting with mechanistic models, especially for the E. coli model. We believe the mean reason behind this poor performance is the local minima problem"
  - [section 2] "both ML and MCMC can naturally handle sparsity and irregularity of the measurement data" but then shows ML approaches performing better
  - [corpus] Weak evidence - the claim about outperforming mechanistic models is specific to this paper's experiments and may not generalize
- Break condition: If the mechanistic model is correctly specified and the data is noise-free, or if the amount of training data is insufficient for the Deep Learning model to learn accurate mappings

## Foundational Learning

- Concept: Permutation-invariant functions
  - Why needed here: Deep Set Networks rely on the ability to aggregate information in a way that doesn't depend on the order of inputs
  - Quick check question: What mathematical property ensures that a function f(x₁, x₂, ..., xₙ) gives the same output regardless of the order of its inputs?

- Concept: Time series forecasting with irregular sampling
  - Why needed here: The data has measurements at irregular time intervals, requiring models that can handle non-uniform temporal spacing
  - Quick check question: How does the triplet encoding handle the fact that different channels may be measured at completely different times?

- Concept: Neural network conditioning on external variables
  - Why needed here: The forecasting model needs to condition its predictions on the target timestamp, not just the input observations
  - Quick check question: In the architecture described, how is the target timestamp incorporated into the prediction process?

## Architecture Onboarding

- Component map: Input triplet encoding → extractor network → permutation-invariant summation → aggregator with timestamp conditioning → output prediction
- Critical path: The data flows from raw triplet encoding through the extractor network, undergoes permutation-invariant summation, receives timestamp conditioning in the aggregator, and produces final predictions
- Design tradeoffs: The triplet encoding provides flexibility for irregular sampling but may lose some temporal continuity information that could be captured by alignment methods. The permutation-invariant aggregation is robust to measurement order but may struggle with very long sequences or complex temporal dependencies.
- Failure signatures: Poor performance on forecasting tasks with long horizons, inability to capture cross-channel dependencies, sensitivity to noise in the input data, failure to generalize to different sampling patterns than seen during training.
- First 3 experiments:
  1. Test the model on a simple synthetic time series with known ground truth to verify basic functionality
  2. Compare performance with and without timestamp conditioning to understand its importance
  3. Evaluate the model's robustness to different levels of sparsity and irregularity in the input data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact limit of Deep Set Neural Networks in handling the number of observations and the dimensionality of the latent space for approximating permutation invariant statistics?
- Basis in paper: [explicit] The paper mentions that with an upper limit on the number of observations and sufficiently large dimensionality of the latent space, Deep Set Networks can approximate any permutation invariant statistics with arbitrary precision.
- Why unresolved: The paper does not specify the exact limit or provide a detailed analysis of the trade-off between the number of observations, latent space dimensionality, and the approximation accuracy.
- What evidence would resolve it: Experimental results showing the performance of Deep Set Networks with varying numbers of observations and latent space dimensions, along with a theoretical analysis of the approximation limits.

### Open Question 2
- Question: How do Deep Set Neural Networks perform on real-world bioprocess data compared to simulated data?
- Basis in paper: [inferred] The paper demonstrates the performance of Deep Set Networks on data generated from macrokinetic growth models under realistic conditions, but it does not mention testing on actual real-world bioprocess data.
- Why unresolved: The paper focuses on simulated data, which may not capture all the complexities and noise present in real-world bioprocess data.
- What evidence would resolve it: Application of Deep Set Neural Networks to real-world bioprocess datasets and comparison of their performance with other methods, including traditional fitting procedures.

### Open Question 3
- Question: What are the specific biases and errors introduced by imputation and alignment procedures that are transferred to the target model?
- Basis in paper: [explicit] The paper states that biases and errors introduced by imputation and alignment are translated into the model trained on the imputed data.
- Why unresolved: The paper does not provide a detailed analysis of the specific types of biases and errors that occur during imputation and alignment, nor does it quantify their impact on the final model performance.
- What evidence would resolve it: A comprehensive study comparing the performance of models trained on imputed data versus models trained on raw data, along with an analysis of the specific biases and errors introduced during imputation and alignment.

## Limitations
- The experiments rely on synthetic data generated from known models, which may not capture real-world complexity and noise characteristics
- Network architectures and training procedures are not fully specified, making exact reproduction challenging
- Performance gains over alignment-based methods are described as "practically negligible" despite being "statistically better"

## Confidence

- **High Confidence**: The core mechanism of triplet encoding for representing asynchronous time series data is well-established and the mathematical foundation of Deep Set Networks is sound
- **Medium Confidence**: The experimental results showing Deep Sets outperforming mechanistic models on synthetic data, though this may not generalize to real-world conditions
- **Low Confidence**: Claims about the practical significance of avoiding imputation/alignment procedures, given the described negligible difference in performance metrics

## Next Checks

1. Apply the Deep Set Neural Network approach to real-world bioprocess datasets (not synthetic) to verify performance on actual measurement noise and complexity
2. Systematically vary network architecture parameters (depth, width, activation functions) to determine robustness of results to architectural choices
3. Evaluate model performance on forecasting tasks with extended prediction horizons to assess temporal dependency capture limitations