---
ver: rpa2
title: Selective experience replay compression using coresets for lifelong deep reinforcement
  learning in medical imaging
arxiv_id: '2302.11510'
source_url: https://arxiv.org/abs/2302.11510
tags:
- learning
- compression
- coreset
- experience
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in lifelong deep reinforcement
  learning for medical imaging by proposing a reward distribution-preserving coreset
  compression technique for experience replay buffers. The method uses k-means++ clustering
  to partition experiences based on rewards, selecting cluster centers as compressed
  experiences with weights proportional to cluster sizes.
---

# Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging

## Quick Facts
- arXiv ID: 2302.11510
- Source URL: https://arxiv.org/abs/2302.11510
- Reference count: 10
- Selective experience replay compression using coresets maintains performance up to 10x compression ratio in lifelong medical imaging DRL

## Executive Summary
This paper addresses catastrophic forgetting in lifelong deep reinforcement learning for medical imaging by proposing a reward distribution-preserving coreset compression technique for experience replay buffers. The method uses k-means++ clustering to partition experiences based on rewards, selecting cluster centers as compressed experiences with weights proportional to cluster sizes. This allows asynchronous compression without access to training models or parameters. Evaluated on brain tumor segmentation (BRATS) and whole-body MRI datasets for anatomical landmark localization, the approach achieved mean pixel error distances of 12.93 (10x compression), 13.46 (20x), 17.75 (30x), and 18.55 (40x) on BRATS, compared to 10.87 for uncompressed models.

## Method Summary
The proposed method implements coreset compression for experience replay buffers in lifelong deep reinforcement learning. Using k-means++ clustering based on reward values, experiences are partitioned into N/R clusters where N is buffer size and R is compression ratio. Cluster centers become compressed experiences, each assigned a weight equal to its cluster size. During training, weighted experiences are repeated to approximate the original reward distribution. The technique operates asynchronously without requiring model parameters or Q-function values, enabling flexible integration with existing DRL systems. The method was tested on BRATS dataset for ventricle localization across 10 task-environment pairs and on whole-body MRI for five landmark localization tasks.

## Key Results
- 10x compressed models showed no significant performance difference from uncompressed models (p=0.28) on whole-body MRI with mean distances of 25.30 vs 19.24
- Mean pixel error distances of 12.93 (10x), 13.46 (20x), 17.75 (30x), and 18.55 (40x) on BRATS compared to 10.87 for uncompressed models
- Experience replay buffers can be compressed up to 10x without significant performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering experiences by reward distribution preserves task-relevant information while compressing storage.
- Mechanism: k-means++ clustering partitions experiences into N/R clusters based on rewards, selecting cluster centers with weights proportional to cluster sizes. This maintains reward distribution approximation.
- Core assumption: Reward values correlate strongly with task-relevant state-action outcomes.
- Evidence anchors:
  - [abstract] "use k-means++ clustering to partition experiences based on rewards, selecting cluster centers as compressed experiences with weights proportional to cluster sizes"
  - [section] "We use k-means++ clustering because we want to manually pick the number of clusters to be N/R as it reflects the compression ratio"
- Break condition: If reward distribution is multimodal with sparse modes, clustering may merge distinct behavioral patterns, causing loss of critical experiences.

### Mechanism 2
- Claim: Weighted sampling preserves approximate reward distribution after unpacking.
- Mechanism: Each selected cluster center receives a weight equal to its cluster size. During unpacking, experiences are repeated according to their weights to approximate original reward frequencies.
- Core assumption: Reward distribution is the primary factor determining experience importance for lifelong learning.
- Evidence anchors:
  - [abstract] "The coreset lifelong learning models trained on a sequence of 10 different brain MR imaging environments demonstrated excellent performance localizing the ventricle with a mean pixel error distance of 12.93 for the compression ratio of 10x"
  - [section] "We pick the closest point to the center of these clusters to be in the compressed ERB. Additionally, we give these points in the compressed ERB a weight parameter that is equal to the number of points in the clusters they are in"
- Break condition: If reward values are not the primary differentiator between experiences, weighting by cluster size will not preserve task-relevant information.

### Mechanism 3
- Claim: Asynchronous compression without model access enables flexible integration with existing DRL systems.
- Mechanism: Compression operates solely on ERB data without requiring Q-function values, model parameters, or training access. This decouples compression from training cycles.
- Core assumption: Experience replay buffers contain sufficient information for meaningful compression without additional model context.
- Evidence anchors:
  - [abstract] "This allows asynchronous compression without access to training models or parameters"
  - [section] "Furthermore, the proposed technique only requires the experience replay buffer for compression. This is an asynchronous process"
- Break condition: If compression requires understanding of state-action dynamics beyond reward values, the model-agnostic approach will fail to preserve critical information.

## Foundational Learning

- Concept: Catastrophic forgetting in lifelong learning
  - Why needed here: The paper addresses catastrophic forgetting by maintaining representative experiences from previous tasks through compressed ERBs.
  - Quick check question: What happens to model performance when training on new tasks without preserving experiences from previous tasks?

- Concept: Coreset theory and approximation guarantees
  - Why needed here: The compression method relies on coreset principles to guarantee that compressed experience sets approximate original data distributions within bounded error.
  - Quick check question: How does the (1+Îµ) multiplicative error guarantee in coreset theory relate to the performance degradation observed in experiments?

- Concept: k-means++ clustering properties
  - Why needed here: The method uses k-means++ to partition experiences by reward, assuming this clustering preserves meaningful task structure.
  - Quick check question: Under what reward distribution conditions does k-means++ clustering optimally preserve task-relevant experience groupings?

## Architecture Onboarding

- Component map: Experience Replay Buffer -> Compression Module -> Compressed ERB -> Unpacking Module -> Weighted Experiences -> DRL Agent
- Critical path:
  1. Collect experiences during DRL training
  2. Periodically compress ERB using clustering
  3. Store compressed experiences with weights
  4. During training, unpack weighted experiences to approximate original buffer
  5. Sample from unpacked buffer for experience replay

- Design tradeoffs:
  - Compression ratio vs. performance: 10x compression shows no significant performance loss (p=0.28), while 40x shows substantial degradation
  - Reward-only clustering vs. multi-dimensional clustering: Simpler implementation but potentially loses state/action information
  - Asynchronous compression vs. online compression: More flexible but may miss temporal patterns

- Failure signatures:
  - Performance degradation correlated with compression ratio exceeding 10x
  - Clustering producing highly imbalanced cluster sizes (some weights much larger than others)
  - Reward distributions with sparse modes that get merged during clustering

- First 3 experiments:
  1. Test compression on synthetic ERB with known reward distribution to verify distribution preservation
  2. Compare 10x compressed performance against baseline across multiple random seeds
  3. Measure cluster size distribution to identify if certain experiences are overrepresented in compression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on compression ratio beyond which the coreset method's performance degrades significantly?
- Basis in paper: [explicit] The paper shows 10x compression maintains performance but 40x shows significant degradation, with authors stating "experience replay buffers can be compressed up to 10x without any significant drop in performance."
- Why unresolved: The paper only tests compression ratios of 10x, 20x, 30x, and 40x, leaving the exact threshold unclear.
- What evidence would resolve it: Systematic testing of intermediate compression ratios (e.g., 5x, 15x, 25x) to map the performance degradation curve.

### Open Question 2
- Question: How would incorporating state and action information into the coreset compression method affect performance compared to using reward distribution alone?
- Basis in paper: [explicit] "This work approximates the joint distribution of state-action-reward-next state using the reward distribution alone, resulting in a potential loss of information... We plan to address this limitation by incorporating state and action into the distribution preserving coreset compression method in the future."
- Why unresolved: The current method only uses reward distribution, which the authors acknowledge may lose information when multiple state-action-reward-next state tuples share the same reward.
- What evidence would resolve it: Experimental comparison between the current reward-only coreset method and an extended version that incorporates state and action dimensions into the clustering process.

### Open Question 3
- Question: How does the coreset compression method perform in multi-agent reinforcement learning settings where experience replay buffers grow linearly with the number of agents?
- Basis in paper: [explicit] "The second limitation of this study is the limited focus on single-agent deep reinforcement learning models... we plan to evaluate the coreset-based ERB compression technique in a multi-agent setup where the size of the ERB linearly increases with the number of agents."
- Why unresolved: The paper only evaluates the method on single-agent scenarios, leaving the scalability to multi-agent systems untested.
- What evidence would resolve it: Testing the coreset compression method on multi-agent reinforcement learning benchmarks with varying numbers of agents to measure performance and compression efficiency.

## Limitations

- The method assumes reward distribution alone captures all task-relevant information, potentially losing state-action information when multiple experiences share similar rewards
- Limited evaluation to single-agent scenarios without testing scalability to multi-agent reinforcement learning settings
- No systematic exploration of compression ratio thresholds between optimal performance and significant degradation

## Confidence

- **High Confidence**: The empirical results showing 10x compression performance (p=0.28 vs uncompressed) and the basic mechanism of k-means++ clustering for coreset formation
- **Medium Confidence**: The generalization of findings to other medical imaging tasks beyond the tested brain tumor segmentation and landmark localization
- **Medium Confidence**: The claim that asynchronous compression without model access is sufficient for preserving lifelong learning capabilities

## Next Checks

1. Test compression on synthetic ERB with controlled reward distributions to isolate the effect of clustering on distribution preservation
2. Compare 10x compressed performance against baseline across at least 5 random seeds to establish statistical robustness
3. Measure cluster size distribution and inter-cluster reward variance to identify potential information loss patterns during compression