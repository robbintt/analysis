---
ver: rpa2
title: When is a Foundation Model a Foundation Model
arxiv_id: '2309.11510'
source_url: https://arxiv.org/abs/2309.11510
tags:
- foundation
- clip
- data
- pathology
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether foundation models trained on non-clinical
  data sources can provide optimal embeddings for digital pathology tasks. Three foundation
  models (CLIP, BiomedCLIP, PLIP) and two conventional models (KimiaNet, DinoSSLPath)
  were evaluated on WSI-to-WSI matching tasks across four internal and two public
  datasets.
---

# When is a Foundation Model a Foundation Model

## Quick Facts
- arXiv ID: 2309.11510
- Source URL: https://arxiv.org/abs/2309.11510
- Reference count: 13
- Primary result: Conventional models (KimiaNet, DinoSSLPath) outperform foundation models on WSI-to-WSI matching tasks, with KimiaNet achieving 65% total F1 score versus CLIP's 55%

## Executive Summary
This study investigates whether foundation models trained on non-clinical data sources can provide optimal embeddings for digital pathology tasks. The research compares three foundation models (CLIP, BiomedCLIP, PLIP) against two conventional models (KimiaNet, DinoSSLPath) on whole slide image-to-whole slide image matching tasks across four internal and two public datasets. Results demonstrate that conventional models trained on robust clinical datasets significantly outperform foundation models, with KimiaNet achieving 65% total F1 score compared to CLIP's 55%. The findings suggest that the quality and relevance of training data are more critical than model architecture or parameter count for achieving optimal performance in medical image analysis tasks.

## Method Summary
The study employs a WSI-to-WSI matching pipeline using Yottixel's mosaic patching approach with median-of-min distance measurements. Five models are evaluated: three CLIP-based foundation models (CLIP, BiomedCLIP, PLIP) and two conventional models (KimiaNet, DinoSSLPath). Patch-level embeddings are extracted from whole slide images, then combined into WSI representations for matching using leave-one-patient-out validation. Performance is measured using F1 scores (MV@k - majority vote among top-k retrieved WSIs) across four internal datasets (breast tumors, fatty liver, skin cancer, colorectal polyps) and two public datasets (DigestPath, WSSS4LUAD).

## Key Results
- KimiaNet achieved 65% total F1 score versus CLIP's 55% on patient matching tasks
- Conventional models (KimiaNet, DinoSSLPath) consistently outperformed foundation models across all tested datasets
- Foundation models trained on non-clinical data sources (Twitter, general internet) failed to capture domain-specific pathology features effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP-based foundation models underperform conventional models on WSI-to-WSI matching due to weaker representation learning from non-clinical data sources
- Core assumption: Quality and relevance of training data outweighs architectural advantages of larger foundation models
- Evidence anchors: Abstract conclusion about conventional models outperforming foundation models; section noting KimiaNet/DinoSSLPath superiority despite fewer parameters
- Break condition: Fine-tuning foundation models on high-quality clinical data could close or reverse performance gap

### Mechanism 2
- Claim: The Yottixel-based WSI-to-WSI matching pipeline relies heavily on patch-level embedding quality
- Core assumption: Poor patch representations cascade into poor WSI matching performance
- Evidence anchors: Section noting WSI matching depends on patch comparison quality; abstract showing conventional model superiority
- Break condition: Modifying matching algorithm to be less dependent on patch-level embeddings

### Mechanism 3
- Claim: Model size and complexity do not guarantee superior performance when training data lacks domain relevance
- Core assumption: Data quality and domain alignment are secondary to model architecture for specialized medical tasks
- Evidence anchors: Section comparing KimiaNet/DinoSSLPath (fewer parameters) to CLIP architectures (~100 million parameters); abstract showing performance differences
- Break condition: Training foundation models on similarly large and high-quality clinical datasets

## Foundational Learning

- Concept: Domain-specific representation learning
  - Why needed here: Understanding how models learn domain-specific features explains why foundation models underperform when trained on non-clinical data
  - Quick check question: Why do models trained on Twitter histopathology images perform worse than those trained on TCGA diagnostic slides for patient matching?

- Concept: Zero-shot learning vs. fine-tuning
  - Why needed here: The study evaluates zero-shot retrieval capabilities, highlighting when fine-tuning becomes necessary
  - Quick check question: What is the difference between zero-shot retrieval and fine-tuned model performance in medical image analysis tasks?

- Concept: Embedding quality and downstream task performance
  - Why needed here: The paper demonstrates that embedding quality directly impacts downstream tasks like patient matching
  - Quick check question: How does the quality of patch-level embeddings affect the performance of WSI-to-WSI matching algorithms?

## Architecture Onboarding

- Component map: Whole Slide Images -> Patch Extraction (mosaic) -> Embedding Extraction (CLIP/KimiaNet/DinoSSLPath) -> WSI Representation -> Matching (median-of-min distance) -> Evaluation (MV@k F1 scores)

- Critical path:
  1. Extract patch-level embeddings using selected models
  2. Build WSI representations via mosaic patching
  3. Perform WSI-to-WSI matching using median-of-min distance
  4. Evaluate matching performance using leave-one-patient-out validation and F1 scores

- Design tradeoffs:
  - Model complexity vs. data quality: Foundation models have more parameters but perform worse due to less relevant training data
  - Zero-shot vs. fine-tuned: Zero-shot retrieval with foundation models underperforms compared to conventionally trained models
  - Patch-based vs. full WSI processing: Patch-based approach is necessary due to WSI size but introduces dependency on patch embedding quality

- Failure signatures:
  - Poor patient matching performance with foundation models despite architectural advantages
  - Inconsistent performance across different datasets and tasks
  - Inability to generalize from non-clinical to clinical data domains

- First 3 experiments:
  1. Replicate WSI-to-WSI matching pipeline using only KimiaNet and DinoSSLPath on subset of internal data
  2. Compare patch-level embeddings from CLIP and KimiaNet using t-SNE visualization
  3. Perform ablation study by removing different components of Yottixel pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of clinical training data make it superior for foundation models in pathology compared to non-clinical sources like Twitter?
- Basis in paper: Explicit - paper demonstrates conventional models trained on TCGA outperform foundation models trained on Twitter for WSI-to-WSI matching
- Why unresolved: Paper shows performance differences but doesn't identify which aspects of clinical data (annotation quality, image resolution, case diversity) drive this gap
- What evidence would resolve it: Comparative analysis of feature representations showing which characteristics correlate with downstream task performance

### Open Question 2
- Question: How does the performance gap between foundation models and conventional models change with increasing model size and training data volume?
- Basis in paper: Inferred - paper shows conventional models outperforming foundation models but doesn't explore scaling effects
- Why unresolved: Study uses relatively small conventional models and doesn't test larger foundation models or different training strategies
- What evidence would resolve it: Systematic scaling experiments comparing models of varying sizes trained on both clinical and non-clinical datasets

### Open Question 3
- Question: What specific architectural modifications to foundation models could improve their performance on pathology tasks without requiring clinical training data?
- Basis in paper: Explicit - paper suggests foundation models trained on non-clinical data struggle with pathology tasks
- Why unresolved: Study only tests existing foundation models without exploring architectural adaptations specific to pathology domain
- What evidence would resolve it: Development and testing of modified foundation model architectures incorporating pathology-specific inductive biases

## Limitations

- Evaluation restricted to zero-shot retrieval capabilities without exploring fine-tuning scenarios
- Single matching pipeline (Yottixel) with specific distance metrics may not be optimal for all foundation models
- Dataset composition may not fully represent real-world pathology practice complexity

## Confidence

**High Confidence**: Empirical results showing KimiaNet and DinoSSLPath outperforming CLIP-based models on patient matching tasks are well-supported by presented data and methodology (65% vs 55% F1 scores).

**Medium Confidence**: Interpretation that data quality outweighs architectural sophistication for foundation models is logically consistent with results but lacks direct corpus support.

**Low Confidence**: Claim that foundation models are inherently unsuitable for pathology tasks is too broad given limited zero-shot evaluation scope and absence of fine-tuning experiments.

## Next Checks

1. Fine-tuning experiment: Replicate study with foundation models fine-tuned on clinical datasets to determine if performance improves and potentially surpasses conventional models.

2. Cross-dataset generalization: Evaluate model performance when trained on one dataset type and tested on another to assess robustness of observed data quality effects.

3. Embedding space analysis: Conduct t-SNE or UMAP visualizations of patch-level embeddings from all models to qualitatively assess representational differences.