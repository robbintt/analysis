---
ver: rpa2
title: 'My Art My Choice: Adversarial Protection Against Unruly AI'
arxiv_id: '2309.03198'
source_url: https://arxiv.org/abs/2309.03198
tags:
- diffusion
- image
- mamc
- images
- protection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces My Art My Choice (MAMC), a method to protect
  images from being used by generative diffusion models. The core idea is to adversarially
  perturb an image so that when it is input to a diffusion model, the output is degraded
  and no longer resembles the original.
---

# My Art My Choice: Adversarial Protection Against Unruly AI

## Quick Facts
- arXiv ID: 2309.03198
- Source URL: https://arxiv.org/abs/2309.03198
- Reference count: 16
- This paper introduces MAMC, a method to protect images from being used by generative diffusion models.

## Executive Summary
This paper introduces My Art My Choice (MAMC), a method to protect images from being used by generative diffusion models. The core idea is to adversarially perturb an image so that when it is input to a diffusion model, the output is degraded and no longer resembles the original. A U-Net generator is trained to create these protected images, using a combination of reconstruction, content, style, and noise losses to balance fidelity and robustness. The amount of distortion is controlled by a user-defined balance factor.

## Method Summary
MAMC trains a U-Net generator to create protected images by applying adversarial perturbations that degrade diffusion model outputs. The method uses a multi-objective loss combining reconstruction (LPIPS + ℓ2), content, style, and noise losses. A balance factor controls the trade-off between image fidelity and protection strength. The method is evaluated on three datasets (Wiki Art, Historic Art, and FaceForensics++) using metrics like PSNR, SSIM, and FID.

## Key Results
- MAMC significantly degrades diffusion outputs while keeping protected images visually similar to originals
- Average PSNRs around 25-35 dB and FIDs exceeding 90 indicate outputs are far from natural images
- Cross-dataset evaluations confirm generalization of the protection method
- Ablation studies validate the loss design and its components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAMC uses adversarial perturbations to degrade diffusion model outputs by maximizing dissimilarity between original and protected image outputs.
- Mechanism: The U-Net generator learns perturbations δ such that when input image I is modified to I + δ, the diffusion model's output M(I + δ) becomes maximally dissimilar from M(I), measured via perceptual and generative losses.
- Core assumption: The black-box diffusion model will process the protected image differently than the original, producing qualitatively worse outputs.
- Evidence anchors:
  - [abstract] "MAMC learns to generate adversarially perturbed 'protected' versions of images which can in turn 'break' diffusion models."
  - [section 3.1] "max_δI ||M(I + δI) - M(I)||, s.t. |δI| < ϕ I + ϵ" - formalizes the attack objective.
- Break condition: If the diffusion model learns to ignore or compensate for the perturbations, the protection fails.

### Mechanism 2
- Claim: Multi-objective loss combining reconstruction, content, style, and noise losses creates robust protection across different diffusion tasks.
- Mechanism: Reconstruction loss (LPIPS + ℓ2) ensures protected images stay visually similar to originals, while content loss pushes diffusion outputs away from protected images, style loss disrupts style transfer, and noise loss pushes outputs toward Gaussian noise.
- Core assumption: Different diffusion tasks (inpainting, style transfer, personalization) rely on preserving different aspects of the input image, and attacking all these aspects provides comprehensive protection.
- Evidence anchors:
  - [section 3.3] "We formulate a multi-objective function that combine additional losses to satisfy the initial assumptions."
  - [section 3.3] Equations 2-5 show the four loss components explicitly.
- Break condition: If one loss component dominates or becomes ineffective, the protection may fail for tasks relying on that aspect.

### Mechanism 3
- Claim: User-controllable balance factor allows artists to trade off between image fidelity and protection strength.
- Mechanism: The balance factor ϕ controls the magnitude of perturbations allowed, with higher values creating stronger protection but more visible distortion.
- Core assumption: Artists need different levels of protection depending on their specific use case and tolerance for distortion.
- Evidence anchors:
  - [abstract] "The perturbation amount is decided by the artist to balance distortion vs. protection of the content."
  - [section 3.4] "we experiment with predefined sets of α* hyper-parameters and prepare MAMC versions with different strengths."
- Break condition: If the balance factor is set too low, protection becomes ineffective; if too high, the protected image becomes unusable.

## Foundational Learning

- Concept: Adversarial machine learning and perturbation attacks
  - Why needed here: MAMC is fundamentally an adversarial attack on diffusion models, requiring understanding of how to craft perturbations that fool neural networks.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks, and which type does MAMC use?

- Concept: Diffusion models and their architecture
  - Why needed here: Understanding how diffusion models work (forward/noise addition, reverse/denoising) is crucial for designing effective attacks.
  - Quick check question: What are the key components of a diffusion model, and how do they differ from GANs?

- Concept: Loss function design and multi-objective optimization
  - Why needed here: MAMC combines multiple loss terms (reconstruction, content, style, noise) that must be balanced correctly for effective protection.
  - Quick check question: How do you balance competing loss terms in a multi-objective optimization problem?

## Architecture Onboarding

- Component map: Original image I -> U-Net generator -> Protected image I' = I + δ -> Frozen pre-trained diffusion model M -> Degraded output

- Critical path: I → U-Net → I' → M → Degraded output
  - The U-Net must learn perturbations that consistently degrade M's output across different inputs and tasks.

- Design tradeoffs:
  - Perturbation magnitude vs. visual fidelity: Larger perturbations provide better protection but make images look unnatural.
  - Loss weight balancing: Different tasks may require different weightings of the four loss components.
  - Training time vs. protection quality: More training epochs may improve protection but increase computational cost.

- Failure signatures:
  - Protected images look too similar to originals (insufficient protection)
  - Protected images look too different from originals (excessive distortion)
  - Diffusion outputs of protected images still resemble expected outputs (attack failure)
  - Model overfits to specific dataset and fails on new images (lack of generalization)

- First 3 experiments:
  1. Train MAMC on Wiki Art dataset with default parameters, evaluate PSNR/SSIM/FID between original/protected images and their diffusion outputs.
  2. Test cross-dataset generalization by training on Wiki Art and testing on Historic Art, measuring same metrics.
  3. Vary the balance factor ϕ and visualize the tradeoff between protection strength and image fidelity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does MAMC protect against diffusion models that are not used for image-to-image tasks, such as text-to-image generation?
- Basis in paper: [inferred] The paper focuses on image-to-image tasks and assumes that diffusion models are used for guided generation. It does not explicitly test MAMC against text-to-image models.
- Why unresolved: The paper does not provide evidence of MAMC's effectiveness against text-to-image models, which are a different type of diffusion model.
- What evidence would resolve it: Experiments showing MAMC's performance against text-to-image models would resolve this question.

### Open Question 2
- Question: How does the effectiveness of MAMC vary with the size and complexity of the dataset used for training?
- Basis in paper: [explicit] The paper mentions that MAMC is trained on datasets with 1K and 5K subsets, but does not provide a detailed analysis of how dataset size affects performance.
- Why unresolved: The paper does not explore the relationship between dataset size and MAMC's effectiveness, leaving uncertainty about its scalability.
- What evidence would resolve it: Experiments varying dataset sizes and analyzing MAMC's performance would provide clarity.

### Open Question 3
- Question: What is the impact of using different pre-trained diffusion models on MAMC's performance?
- Basis in paper: [explicit] The paper uses a standard pre-trained diffusion model but does not explore the effects of using different models.
- Why unresolved: The choice of diffusion model could affect MAMC's ability to generate adversarial perturbations, but this is not investigated in the paper.
- What evidence would resolve it: Testing MAMC with various pre-trained diffusion models and comparing results would address this question.

## Limitations

- Implementation specifics missing: The exact values for loss weights are not provided, only stated to be set "experimentally" based on ablation studies.
- Evaluation scope: The evaluation focuses primarily on quantitative metrics without extensive qualitative analysis of real-world protection scenarios.
- Generalization concerns: The paper doesn't test against different diffusion model architectures or examine how quickly protection degrades as diffusion models evolve.

## Confidence

**High confidence**: The core mechanism of using adversarial perturbations to degrade diffusion outputs is technically sound and well-supported by the mathematical formulation and experimental results showing significant FID increases.

**Medium confidence**: The effectiveness of the multi-objective loss design is demonstrated through ablation studies, but the specific weight configurations are not fully specified, requiring experimental determination.

**Low confidence**: The practical real-world effectiveness against determined adversaries who might adapt their diffusion models or use alternative approaches to bypass MAMC protection.

## Next Checks

1. **Loss weight sensitivity analysis**: Systematically vary each loss weight (αR1, αR2, αC, αS, αN) to determine their optimal values and understand their individual contributions to protection effectiveness.

2. **Adversarial robustness testing**: Test MAMC-protected images against adaptive diffusion models that attempt to learn and compensate for the adversarial perturbations during fine-tuning.

3. **Cross-architecture generalization**: Evaluate MAMC protection against multiple diffusion model architectures (Stable Diffusion, DALL-E, Midjourney) to assess the breadth of protection rather than just depth on one model type.