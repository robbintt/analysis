---
ver: rpa2
title: Why Not? Explaining Missing Entailments with Evee (Technical Report)
arxiv_id: '2308.07294'
source_url: https://arxiv.org/abs/2308.07294
tags:
- rule
- explanation
- ontology
- missing
- abduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new version of Evee, a Prot\xE9g\xE9 plugin\
  \ that explains missing entailments in OWL ontologies. It offers counterexamples\
  \ and abduction-based explanations for non-entailed axioms, visualizing counterexamples\
  \ as graphs and providing hypotheses to repair missing entailments."
---

# Why Not? Explaining Missing Entailments with Evee (Technical Report)

## Quick Facts
- arXiv ID: 2308.07294
- Source URL: https://arxiv.org/abs/2308.07294
- Reference count: 32
- Key outcome: Evee plugin explains missing entailments in OWL ontologies using counterexamples and abduction, visualizing results and suggesting repairs.

## Executive Summary
This paper presents a new version of the Evee plugin for Protégé, designed to explain missing entailments in OWL ontologies. The plugin provides two main explanation types: counterexamples showing models where a missing entailment fails, and abduction-based hypotheses that, when added to the ontology, would make the entailment hold. It integrates external tools like CAPI and Lethe-Abduction, supporting EL and ALC description logics, and visualizes counterexamples as interactive directed graphs to help users understand and debug their ontologies.

## Method Summary
The Evee plugin extends Protégé with explanation services for non-entailed axioms. Counterexamples are generated using tableau methods or canonical models, visualized as directed graphs highlighting relevant elements. Abduction solvers compute minimal hypotheses using external tools (CAPI and Lethe-Abduction) to repair missing entailments. The plugin provides a unified interface, handles caching and user interaction, and supports vocabulary restrictions to focus explanations.

## Key Results
- Evee successfully generates counterexamples and abduction hypotheses for missing entailments in EL and ALC ontologies
- Visual counterexamples highlight important model elements and support interactive exploration
- Integration with CAPI and Lethe-Abduction provides powerful abduction capabilities within Protégé

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The plugin uses both counterexamples and abduction to explain missing entailments, allowing users to understand why a desired axiom does not follow from the ontology.
- Mechanism: Counterexamples are generated using tableau methods or canonical models to show a model where the left-hand side of the GCI is true but the right-hand side is false. Abduction computes minimal hypotheses (sets of axioms) that, when added to the ontology, would make the missing entailment hold.
- Core assumption: The ontology is consistent and the missing entailment is a single GCI or a set of axioms expressible in the supported DLs (EL, ALC).
- Evidence anchors:
  - [abstract] The plugin provides explanations for missing entailments via techniques based on abduction and counterexamples.
  - [section 3.1] The Small Model Counterexample Generator uses a tableau-based algorithm for EL⊥ to generate complete models that serve as counterexamples.
  - [section 4.1] The Complete Signature-Based Abduction solver uses LETHE to compute complete signature-based hypotheses for ALC.
- Break condition: If the ontology is inconsistent, or if the missing entailment is not expressible in the supported DLs, the mechanisms may fail to provide valid explanations.

### Mechanism 2
- Claim: The plugin visualizes counterexamples as directed graphs, highlighting important elements and allowing users to interactively explore the model.
- Mechanism: Domain elements are depicted as circles, with the root element (satisfying the left-hand side of the GCI) marked in black. Users can select nodes to see all concept names the element belongs to, and add disjointness axioms to refine the model.
- Core assumption: The visualization library (GraphStream) correctly renders the model and the user can interpret the graph to understand the counterexample.
- Evidence anchors:
  - [section 3] The plug-ins visualize counterexamples as directed graphs, where nodes are individuals labeled by concept names and edges are labeled by role names.
  - [section 3.1] The generated graphs highlight the root element and allow users to see all concept names for a selected element.
  - [section 3] The graphical model view allows zoom and node movement to facilitate exploring large graphs.
- Break condition: If the model is too large or complex, the visualization may become difficult to interpret, reducing its effectiveness as an explanation.

### Mechanism 3
- Claim: The plugin integrates external tools (CAPI and LETHE-Abduction) to compute abduction-based explanations, providing a unified interface for different explanation services.
- Mechanism: The Evee core plugin provides extension points for new explanation services. The abstract base classes for abduction and counterexample services handle the integration with the user interface, caching results, and displaying explanations.
- Core assumption: The external tools are correctly installed and configured, and the abstract base classes correctly handle the integration with the user interface.
- Evidence anchors:
  - [abstract] The plugin integrates the functionality provided by the external tools CAPI and LETHE-Abduction for abduction.
  - [section 5.2] The abstract base class AbstractAbductionSolver handles caching results, creating the result component, and handling user input for abduction solvers.
  - [section 5.1] The abstract base class AbstractCounterexampleGenerationService handles the visualization of counterexamples and implements the INonEntailmentExplanationService interface.
- Break condition: If the external tools are not installed or configured correctly, or if the abstract base classes have bugs, the integration may fail and the plugin may not provide abduction-based explanations.

## Foundational Learning

- Concept: Description Logics (DLs) and their semantics
  - Why needed here: The plugin is designed to explain missing entailments in OWL ontologies, which are based on Description Logics. Understanding the syntax and semantics of DLs is crucial for interpreting the explanations provided by the plugin.
  - Quick check question: What is the difference between a TBox and an ABox in Description Logics?

- Concept: Tableau methods for reasoning in Description Logics
  - Why needed here: The plugin uses tableau methods to generate counterexamples for missing entailments. Understanding how tableau methods work is important for understanding how the counterexamples are generated and why they are valid explanations.
  - Quick check question: What is the purpose of the expansion rules in a tableau algorithm for Description Logics?

- Concept: Abduction and its application to ontology debugging
  - Why needed here: The plugin uses abduction to compute hypotheses that, when added to the ontology, would make the missing entailment hold. Understanding abduction and its application to ontology debugging is crucial for interpreting the abduction-based explanations provided by the plugin.
  - Quick check question: What is the difference between a complete signature-based hypothesis and a connection-minimal hypothesis in abduction?

## Architecture Onboarding

- Component map:
  - Evee core plugin -> Counterexample generation services -> Abduction solvers -> Abstract base classes -> User interface
  - Evee core plugin -> External tools (CAPI, LETHE-Abduction) -> Abduction solvers -> User interface

- Critical path:
  1. User specifies the missing entailment and optional vocabulary in the Evee tab.
  2. User selects an explanation service and clicks "Generate Explanation".
  3. The selected service computes the explanation (counterexample or abduction hypotheses).
  4. The explanation is displayed in the Evee tab, allowing the user to explore and interact with it.

- Design tradeoffs:
  - The plugin supports multiple explanation services (counterexamples and abduction), providing flexibility but also increasing complexity.
  - The visualization of counterexamples as directed graphs allows for interactive exploration but may become difficult to interpret for large or complex models.
  - The integration with external tools (CAPI and LETHE-Abduction) provides powerful abduction capabilities but also introduces dependencies and potential configuration issues.

- Failure signatures:
  - If the ontology is inconsistent, the counterexample generation services may fail to produce a valid counterexample.
  - If the external tools (CAPI and LETHE-Abduction) are not installed or configured correctly, the abduction solvers may fail to compute hypotheses.
  - If the model is too large or complex, the visualization of counterexamples may become difficult to interpret, reducing its effectiveness as an explanation.

- First 3 experiments:
  1. Load the plugin in Protégé and open an OWL ontology. Specify a missing entailment (e.g., a GCI that does not follow from the ontology) and generate a counterexample using the Small Model Counterexample Generator. Explore the visualization and verify that it correctly represents a model where the left-hand side of the GCI is true but the right-hand side is false.
  2. Repeat experiment 1 using the Relevant Counterexample Generator and compare the results. Verify that the relevant counterexample focuses on the most important parts of the model for explaining the missing entailment.
  3. Specify a missing entailment and generate abduction hypotheses using the Complete Signature-Based Abduction solver. Verify that the hypotheses, when added to the ontology, would make the missing entailment hold. Explore the visualization of the hypotheses and verify that they are correctly represented.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the scalability of the Small Model Counterexample Generator be improved for large EL⊥ ontologies?
- Basis in paper: [inferred] The paper mentions that the generator tries to keep models small, but does not discuss scalability limitations for very large ontologies.
- Why unresolved: The paper focuses on correctness and completeness of the algorithm but does not provide performance analysis or optimization strategies for large-scale applications.
- What evidence would resolve it: Empirical performance benchmarks comparing the generator's runtime and memory usage across ontologies of varying sizes, along with proposed optimizations.

### Open Question 2
- Question: What are the trade-offs between using canonical models versus tableau-based models for counterexample generation in terms of explainability and computational efficiency?
- Basis in paper: [explicit] The paper contrasts the Small Model Counterexample Generator (tableau-based) with the Relevant Counterexample Generator (canonical model-based) but does not evaluate their relative strengths.
- Why unresolved: The paper presents both methods but lacks a comparative analysis of their effectiveness in different scenarios.
- What evidence would resolve it: A user study or systematic evaluation measuring user comprehension of explanations from both methods, alongside runtime and memory usage comparisons.

### Open Question 3
- Question: How can the framework be extended to support more expressive description logics beyond EL and ALC?
- Basis in paper: [explicit] The paper states that the current implementation supports EL and ALC but does not discuss plans or challenges for extending to more expressive logics.
- Why unresolved: The paper focuses on the current implementation and does not address future directions or technical barriers for broader DL support.
- What evidence would resolve it: A technical report or proof-of-concept implementation demonstrating the adaptation of the framework to a more expressive DL, such as SROIQ or Horn-SHIQ.

### Open Question 4
- Question: What is the impact of signature-based restrictions on the quality and completeness of abduction explanations?
- Basis in paper: [explicit] The paper describes signature-based abduction but does not evaluate how restricting the vocabulary affects the relevance or completeness of the generated hypotheses.
- Why unresolved: The paper explains the mechanism but lacks empirical validation of its effectiveness in practice.
- What evidence would resolve it: A case study or experiment comparing abduction results with and without vocabulary restrictions across diverse ontologies, measuring hypothesis quality and user satisfaction.

## Limitations
- DL expressivity limited to EL and ALC, restricting applicability to more complex ontologies
- Heavy dependency on external tool installation and configuration for abduction capabilities
- Visualization may become impractical for large or complex models, reducing explainability

## Confidence
- High confidence in counterexample generation mechanisms and visualization functionality
- Medium confidence in abduction hypothesis generation quality
- Medium confidence in practical usability

## Next Checks
1. Test the plugin with ontologies containing mixed EL and ALC constructs to verify DL expressivity handling and identify any unexpected behavior at language boundaries.
2. Evaluate counterexample visualization performance with progressively larger models (10, 100, 1000+ nodes) to establish scalability limits and identify failure thresholds.
3. Verify abduction hypothesis minimality and completeness by systematically testing with ontologies where ground truth minimal repairs are known, comparing generated hypotheses against expected results.