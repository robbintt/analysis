---
ver: rpa2
title: Distributed Matrix-Based Sampling for Graph Neural Network Training
arxiv_id: '2311.02909'
source_url: https://arxiv.org/abs/2311.02909
tags:
- sampling
- each
- graph
- matrix
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents distributed algorithms for graph neural network
  (GNN) training that reduce communication in the sampling step. The key idea is to
  express sampling algorithms as sparse matrix multiplications (SpGEMM) and sample
  multiple minibatches in bulk.
---

# Distributed Matrix-Based Sampling for Graph Neural Network Training

## Quick Facts
- arXiv ID: 2311.02909
- Source URL: https://arxiv.org/abs/2311.02909
- Authors: 
- Reference count: 14
- 2.5× speedup over Quiver on 128 GPUs for 3-layer GraphSAGE on largest OGB datasets

## Executive Summary
This paper presents distributed algorithms for graph neural network (GNN) training that reduce communication in the sampling step by expressing sampling algorithms as sparse matrix multiplications (SpGEMM) and sampling multiple minibatches in bulk. The approach works for both cases when the graph topology fits on one GPU (matrix-based bulk sampling) and when it does not (distributed SpGEMM). A communication-avoiding feature fetching step using all-to-allv exchanges with replicated feature data is also proposed. Experimental results show significant speedups on large datasets using up to 128 GPUs.

## Method Summary
The paper's distributed GNN training pipeline works by expressing sampling algorithms as sparse matrix multiplications and leveraging distributed SpGEMM algorithms to sample multiple minibatches in bulk. When the graph fits on one GPU, the adjacency matrix is replicated across all GPUs while the sampler matrix is partitioned. For larger graphs, a distributed SpGEMM approach with graph replication or partitioning is used. The feature fetching step employs a 1.5D partitioning scheme with all-to-allv communication to efficiently gather features for sampled minibatches. The pipeline is evaluated on GraphSAGE and LADIES sampling algorithms across several OGB datasets.

## Key Results
- Achieves 2.5× speedup over Quiver on a 3-layer GraphSAGE network using 128 GPUs on largest OGB datasets
- Demonstrates 8.46× speedup on 128 GPUs in per-epoch time for other datasets
- Shows strong scaling behavior with near-linear speedup up to 128 GPUs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matrix-based bulk sampling reduces communication in the sampling step by expressing sampling as a sparse matrix multiplication (SpGEMM) and sampling multiple minibatches at once.
- Mechanism: The sampling algorithm is formulated as matrix operations where the adjacency matrix A is multiplied by a sampler-dependent matrix QL. This allows leveraging existing distributed SpGEMM algorithms, which are communication-avoiding, to perform sampling across multiple minibatches in bulk rather than individually.
- Core assumption: The graph topology can be represented as a sparse matrix and that SpGEMM algorithms can be efficiently distributed across GPUs.
- Evidence anchors:
  - [abstract] "expresses sampling as a sparse matrix multiplication (SpGEMM) and samples multiple minibatches at once"
  - [section 4] "we show how to express node-wise and layer-wise sampling algorithms in the language of linear algebra"
  - [corpus] Weak - related works mention distributed training but not matrix-based sampling specifically
- Break condition: If the graph cannot be efficiently represented as a sparse matrix or if SpGEMM algorithms don't scale well for the graph size.

### Mechanism 2
- Claim: When the graph topology fits on one GPU, sampling can be performed without communication by replicating the adjacency matrix across all GPUs.
- Mechanism: By replicating the adjacency matrix A on each GPU and partitioning the sampler matrix QL across devices, each GPU can compute its portion of the SpGEMM locally without any inter-GPU communication during the sampling step.
- Core assumption: The adjacency matrix fits in GPU memory when replicated across all GPUs.
- Evidence anchors:
  - [abstract] "When the input graph topology (but not the embeddings) fits in the memory of one GPU, our approach (1) performs sampling without communication"
  - [section 5.1] "we partition Ql across devices and replicate the adjacency matrix A on each device"
  - [corpus] Weak - related works discuss distributed training but not this specific replication strategy
- Break condition: If the replicated adjacency matrix exceeds GPU memory capacity.

### Mechanism 3
- Claim: A communication-avoiding feature fetching step using all-to-allv exchanges with replicated feature data outperforms current methods for distributed GNN training.
- Mechanism: The feature matrix H is partitioned using a 1.5D partitioning scheme where each block row is replicated across multiple processes. An all-to-allv call allows each process to gather the necessary feature vectors for its minibatches efficiently by communicating only within its process column.
- Core assumption: Replicating feature data with a simple all-to-allv exchange is more efficient than alternative feature fetching strategies.
- Evidence anchors:
  - [abstract] "we show that judiciously replicating feature data with a simple all-to-all exchange can outperform current methods for the feature extraction step in distributed GNN training"
  - [section 6] "We partition the overall feature matrix H with a 1.5D partitioning scheme... we run a single all-to-allv call on each process column P (:, j)"
  - [corpus] Weak - related works mention distributed training but not this specific feature fetching optimization
- Break condition: If the communication cost of all-to-allv exceeds the benefit of replication, or if memory constraints prevent sufficient replication.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message-passing architecture
  - Why needed here: The paper's techniques are specifically designed for GNN training, which requires understanding how GNNs operate on graph data through message-passing
  - Quick check question: What are the two main steps in the GNN message-passing model?

- Concept: Sparse matrix multiplication (SpGEMM) and its distributed algorithms
  - Why needed here: The core innovation relies on expressing sampling as SpGEMM operations and leveraging distributed SpGEMM algorithms
  - Quick check question: What is the primary computational bottleneck in distributed SpGEMM that this work addresses?

- Concept: Graph sampling algorithms (node-wise, layer-wise, graph-wise)
  - Why needed here: The paper implements specific sampling algorithms (GraphSAGE, LADIES) using matrix operations, requiring understanding of how these algorithms work
  - Quick check question: How does LADIES differ from GraphSAGE in terms of sampling strategy?

## Architecture Onboarding

- Component map:
  - Input: Graph topology (adjacency matrix A), feature matrix H, sampler matrix QL
  - Processing: Matrix-based bulk sampling (SpGEMM operations), feature fetching (all-to-allv), forward/backward propagation
  - Output: Trained GNN model
  - Key components: 1.5D partitioning scheme, replication factor c, bulk sampling count k

- Critical path:
  1. Partition and replicate data (A, H, QL) across GPUs
  2. Generate probability distributions via SpGEMM (QlA)
  3. Sample from distributions using Inverse Transform Sampling
  4. Extract sampled adjacency matrices via SpGEMM
  5. Fetch features via all-to-allv
  6. Run forward/backward propagation

- Design tradeoffs:
  - Memory vs. Communication: Replicating data reduces communication but increases memory usage
  - Replication factor c: Higher c reduces communication in feature fetching but increases memory pressure
  - Bulk sampling count k: Larger k amortizes sampling overhead but requires more memory

- Failure signatures:
  - Out of memory errors: Graph/adjacency matrix too large for replication, or too many minibatches in bulk
  - Poor scaling: Insufficient replication factor c, or graph partitioning that creates load imbalance
  - High communication overhead: Suboptimal partitioning leading to excessive data movement

- First 3 experiments:
  1. Single-node validation: Run on a single GPU with a small graph to verify matrix-based sampling implementation matches baseline
  2. Weak scaling test: Fix problem size per GPU and increase GPU count to measure parallel efficiency
  3. Strong scaling test: Fix total problem size and increase GPU count to measure speedup in per-epoch time

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but several limitations and future directions are implied by the work.

## Limitations
- The replication strategy for feature fetching may face memory constraints on larger graphs or with higher replication factors
- Performance comparisons are made against Quiver but lack comparisons with other recent distributed GNN training systems
- The scalability claims are primarily demonstrated on relatively small cluster sizes (128 GPUs) compared to modern exascale systems

## Confidence
- High confidence: The mathematical formulation of sampling as SpGEMM operations and the basic communication-avoiding mechanisms
- Medium confidence: The claimed 2.5× speedup over Quiver, as this depends heavily on specific hardware configurations and dataset characteristics
- Low confidence: The generalization of results to significantly larger GPU counts or different GNN architectures beyond GraphSAGE and LADIES

## Next Checks
1. **Memory Scaling Analysis**: Systematically vary the replication factor c and measure both memory usage and communication costs to identify optimal configurations across different graph sizes
2. **Larger Scale Validation**: Test the approach on a 1024+ GPU system to verify if the communication-avoiding benefits scale to modern exascale environments
3. **Cross-Algorithm Generalization**: Implement and benchmark the matrix-based sampling approach with additional GNN architectures (e.g., Graph Attention Networks) to assess broader applicability