---
ver: rpa2
title: Reducing Gender Bias in Machine Translation through Counterfactual Data Generation
arxiv_id: '2311.16362'
source_url: https://arxiv.org/abs/2311.16362
tags:
- gender
- data
- bias
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses gender bias in neural machine translation (NMT)
  systems, where translations often reflect stereotypical gender roles. The authors
  propose a novel domain-adaptation technique using counterfactual data generation
  to create gender-balanced training data from the original training corpus.
---

# Reducing Gender Bias in Machine Translation through Counterfactual Data Generation

## Quick Facts
- **arXiv ID**: 2311.16362
- **Source URL**: https://arxiv.org/abs/2311.16362
- **Reference count**: 5
- **Primary result**: NMT gender bias reduced by 19-23% in WinoMT accuracy without significant BLEU score loss

## Executive Summary
This paper addresses gender bias in neural machine translation systems, where translations often reflect stereotypical gender roles for professions and activities. The authors propose a novel domain-adaptation technique using counterfactual data generation to create gender-balanced training data from the original training corpus. They fine-tune base NMT models on this data, avoiding catastrophic forgetting by including random in-domain samples. Experiments on English→French, English→Spanish, and English→Italian show significant improvements in WinoMT accuracy (19-23%) without significant BLEU score loss. The approach is purely data-centric, requiring no modification to training objectives or inference models, and leverages diverse data through counterfactual generation.

## Method Summary
The method generates gender-balanced fine-tuning datasets by identifying gendered sentences from the training corpus and creating counterfactual pairs using Zmigrod et al. (2019) techniques. These counterfactual pairs are sentences that are semantically equivalent but have opposite gender representations. The base NMT models are then fine-tuned on the combined gender-balanced dataset, supplemented with random in-domain samples to prevent catastrophic forgetting. The approach uses the Marian toolkit for NMT and evaluates performance using WinoMT accuracy and BLEU scores.

## Key Results
- WinoMT accuracy improved by 19-23% across all three language pairs (English→French, English→Spanish, English→Italian)
- BLEU scores remained stable, showing no significant degradation compared to baseline models
- Counterfactual data generation using in-domain data proved more effective than handcrafted datasets while avoiding catastrophic forgetting
- The method successfully balanced masculine and feminine representations in profession nouns without over-correcting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual data generation preserves in-domain fluency while balancing gender
- Mechanism: By swapping gendered pronouns and inflectional morphology within the same sentence context, the method creates synthetic sentence pairs that are semantically equivalent but have opposite gender representations. This avoids the domain shift problem seen when using external datasets
- Core assumption: The original training corpus contains sufficient instances of gendered profession nouns in varied contexts to produce high-quality counterfactuals
- Evidence anchors: Abstract mentions "novel domain-adaptation technique that leverages in-domain data"; corpus analysis shows related work on gender bias in MT

### Mechanism 2
- Claim: Random in-domain sampling mitigates catastrophic forgetting
- Mechanism: Supplementing fine-tuning data with random samples from the original training corpus keeps the model exposed to the full distribution of the original domain, preventing degradation in general translation quality
- Core assumption: The random sample is representative of the original training corpus and contains no significant gender imbalance
- Evidence anchors: Abstract states "simply supplementing the handcrafted dataset with a random sample from the base model training corpus is enough to significantly reduce the catastrophic forgetting"
- Break condition: If the random sample is too small or unrepresentative, the model may still experience significant forgetting

### Mechanism 3
- Claim: The gender-balanced dataset guides the model toward gender neutrality rather than over-correction
- Mechanism: By including both original and counterfactual sentence pairs with identical contexts but opposite gender, the model learns to associate gender roles with context rather than arbitrary features
- Core assumption: The model can learn from paired contrastive examples and generalize this balance to unseen sentences
- Evidence anchors: Methodology section describes combining original and counterfactual data to yield balanced profession words
- Break condition: If the model overfits to the training pairs without generalizing, it may not effectively balance gender in new contexts

## Foundational Learning

- **Neural Machine Translation architecture and training**: Understanding how transformer models learn representations and how fine-tuning affects them is crucial for grasping why catastrophic forgetting occurs and how the proposed methods mitigate it. Quick check: What is catastrophic forgetting in the context of fine-tuning NMT models, and why does it occur when training on a narrow domain?

- **Counterfactual data generation and morphological inflection**: The paper's core innovation relies on generating synthetic sentence pairs with opposite gender inflections. Understanding how tools like Zmigrod et al. (2019) work is essential for implementing and extending this approach. Quick check: How does the Markov Random Field (MRF) model identify tokens that need gender changes beyond the primary animate noun in morphologically rich languages?

- **Gender bias evaluation in NLP**: The paper uses WinoMT challenge set to quantify gender bias. Understanding the metrics (Accuracy, Pro, Anti, ∆S, ∆G) and their implications is necessary for interpreting results and comparing with other methods. Quick check: What does a high ∆S value indicate about a translation system's gender bias, and why is it important to consider both pro- and anti-stereotypical subsets?

## Architecture Onboarding

- **Component map**: Base NMT model -> Counterfactual data generation pipeline -> Random in-domain sampling module -> Fine-tuning training loop -> WinoMT evaluation module

- **Critical path**: 1) Load base model and training corpus, 2) Generate counterfactual sentence pairs, 3) Sample random in-domain data, 4) Combine datasets for fine-tuning, 5) Fine-tune base model, 6) Evaluate on WinoMT and BLEU

- **Design tradeoffs**: Using in-domain data vs. external gender-balanced datasets (in-domain preserves fluency but may have limited diversity); counterfactual generation complexity vs. quality (more sophisticated changes improve accuracy but increase computational cost); random sampling size (larger samples better prevent forgetting but increase fine-tuning time)

- **Failure signatures**: Significant BLEU score drop (catastrophic forgetting not adequately mitigated); persistent gender bias in WinoMT (counterfactual generation producing low-quality data); erroneous gender swapping (issues in counterfactual generation pipeline)

- **First 3 experiments**: 1) Fine-tune base model on handcrafted dataset only (expect significant WinoMT improvement but BLEU degradation), 2) Fine-tune base model on counterfactual dataset only (expect similar or better WinoMT improvement with less BLEU degradation), 3) Fine-tune base model on combined handcrafted + random dataset (expect WinoMT improvement close to handcrafted only, with BLEU scores closer to baseline)

## Open Questions the Paper Calls Out

- **Question**: How can the proposed method be extended to handle non-binary gender identities effectively?
- **Basis**: The paper mentions this as future work, noting current techniques work well for male and female genders but don't address non-binary gender identities
- **Why unresolved**: The current approach relies on binary gender swapping without proposing solutions for non-binary scenarios
- **What evidence would resolve it**: Experiments showing improved WinoMT accuracy for non-binary gender scenarios with modified dataset generation including non-binary pronouns and nouns

## Limitations
- Counterfactual data generation relies heavily on quality and diversity of original training corpus
- Random sampling approach assumes sampled data is representative and gender-balanced
- Method requires significant computational resources for generating counterfactual data
- Current approach only addresses binary gender identities, not non-binary options

## Confidence
- **High Confidence**: Effectiveness of counterfactual data generation in improving WinoMT accuracy (19-23% improvement) is well-supported by experimental results
- **Medium Confidence**: Random in-domain sampling effectively mitigates catastrophic forgetting is supported by results but exact sample size relationship is not fully explored
- **Low Confidence**: Claim that gender-balanced dataset guides model toward gender neutrality rather than over-correction is inferred from methodology and results without direct evidence of learning process

## Next Checks
1. **Corpus Diversity Analysis**: Analyze original training corpus to quantify diversity and frequency of gendered profession nouns to validate assumption about sufficient instances for high-quality counterfactual generation

2. **Sample Size Impact Study**: Conduct experiments varying the size of random in-domain sample used to mitigate catastrophic forgetting to understand relationship between sample size and forgetting mitigation effectiveness

3. **Generalization Evaluation**: Test fine-tuned models on held-out test set with diverse gender contexts not present in training data to assess whether models have generalized gender balance to unseen sentences or overfitted to training pairs