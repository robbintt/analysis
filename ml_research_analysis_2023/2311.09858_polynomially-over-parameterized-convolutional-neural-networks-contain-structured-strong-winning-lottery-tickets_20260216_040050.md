---
ver: rpa2
title: Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured
  Strong Winning Lottery Tickets
arxiv_id: '2311.09858'
source_url: https://arxiv.org/abs/2311.09858
tags:
- random
- pruning
- lemma
- neural
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves the existence of structured subnetworks within
  randomly-initialized CNNs that can approximate any smaller CNN without training.
  The key innovation is a new multidimensional Random Subset-Sum result that handles
  dependencies arising from parameter-sharing in CNNs.
---

# Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets

## Quick Facts
- arXiv ID: 2311.09858
- Source URL: https://arxiv.org/abs/2311.09858
- Authors: [Not specified in source]
- Reference count: 40
- Key outcome: Proves structured subnetworks exist in random CNNs without training, using polynomial over-parameterization and a new multidimensional Random Subset-Sum result.

## Executive Summary
This paper establishes that polynomially over-parameterized random convolutional neural networks contain structured subnetworks that can approximate any smaller target CNN without any training. The key innovation is a new multidimensional Random Subset-Sum result that handles the stochastic dependencies arising from parameter-sharing in CNNs. This enables proving that polynomially over-parameterized random CNNs contain structured subnetworks (obtained via block and neuron/filter pruning) that approximate any target network with accuracy inversely proportional to the degree of over-parameterization.

## Method Summary
The method constructs random CNNs with specific architectural layers and leverages a new multidimensional Random Subset-Sum result to handle dependencies from parameter sharing. The proof shows that with sufficient width and depth, random normal tensors contain structured subnetworks that can approximate any target network. Structured pruning via block and neuron/filter removal creates smaller, dense networks that directly reduce computational costs while maintaining approximation capability.

## Key Results
- Proves the existence of structured subnetworks within randomly-initialized CNNs that can approximate any smaller CNN without training
- Introduces a new multidimensional Random Subset-Sum result handling dependencies from parameter sharing
- Achieves sub-exponential bounds for structured pruning under the Strong Lottery Ticket Hypothesis
- Shows approximation error is inversely proportional to the degree of over-parameterization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polynomial over-parameterization guarantees the existence of structured subnetworks that approximate any smaller target CNN without training.
- Mechanism: The proof constructs a random CNN with specific architectural layers and leverages a new multidimensional Random Subset-Sum result that handles stochastic dependencies from parameter sharing. This allows pruning via block and neuron/filter removal to recover the target network.
- Core assumption: Random normal tensors with sufficient width and depth contain structured subnetworks that can approximate any target network with accuracy inversely proportional to the degree of over-parameterization.
- Evidence anchors:
  - [abstract] "we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH"
  - [section] "Our main result is the following. Theorem 2 (Structured SLTH). Let D, c 0, ℓ ∈ N, and ε ∈ R>0. For i ∈ [ℓ], let di, c i, n i ∈ N... Then, there exists a universal constant C > 0, such that if, for i ∈ [ℓ], ni ≥ Cd13i c6i log3 d2ici ci− 1ℓ ε, then, with probability at least 1 − ε, we have that, for all f ∈ F, supX∈ [− 1, 1]D× D× c0 min g∈G ∥f (X) − g(X)∥max ≤ ε."
- Break condition: If the over-parameterization bound (polynomial in dimensions) is not met, or if the random initialization deviates significantly from normal distribution assumptions, the structured subnetwork may not exist with the required approximation guarantees.

### Mechanism 2
- Claim: The Normally-scaled MRSS theorem supports NSN vectors, which capture the stochastic dependencies inherent in CNN parameter sharing.
- Mechanism: The theorem proves that d-dimensional i.i.d. NSN random vectors can approximate any target vector within ε error with high probability, where NSN vectors model the structure of CNN weights after parameter sharing.
- Core assumption: NSN vectors (Y where Yi = Z · Zi with Z, Z1,...,Zd i.i.d. standard normal) correctly model the statistical dependencies created by parameter sharing in CNNs.
- Evidence anchors:
  - [abstract] "we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH"
  - [section] "A key technical contribution of ours is a Multidimensional Random Subset Sum (MRSS) result that supports NSN vectors"
- Break condition: If the parameter sharing structure in CNNs creates dependencies beyond what NSN vectors can model, or if the number of vectors n is insufficient relative to the target vector complexity, the approximation guarantee fails.

### Mechanism 3
- Claim: Structured pruning via block and neuron/filter removal achieves computational efficiency gains without sacrificing approximation accuracy.
- Mechanism: The proof shows that removing entire filters and contiguous blocks of parameters creates smaller, dense networks that directly reduce computational costs while maintaining the ability to approximate target networks through the existence of appropriate subnetworks in the over-parameterized random CNN.
- Core assumption: Structured pruning patterns (like 2n-channel-blocked masks and filter removal) preserve the approximation capability of the subnetwork while delivering computational efficiency.
- Evidence anchors:
  - [abstract] "we show that, with high probability and for a wide class of architectures, polynomially over-parameterized random networks can be pruned in a structured manner to approximate any target network"
  - [section] "Our results cover CNNs, which generalise fully-connected networks as well as many layer types commonly used in modern architectures, such as pooling and normalisation layers"
- Break condition: If the structured pruning pattern is too restrictive (e.g., removing too many parameters or using overly coarse block structures), the remaining subnetwork may lack sufficient capacity to approximate the target network accurately.

## Foundational Learning

- Concept: Random Subset-Sum Problem and its multidimensional generalization
  - Why needed here: The proof fundamentally relies on showing that sums of random subsets can approximate any target value, extended to multiple dimensions to handle the vector-valued outputs of neural networks
  - Quick check question: What is the key difference between the classical Random Subset-Sum Problem and the multidimensional version used in this paper?

- Concept: Parameter sharing in convolutional neural networks
  - Why needed here: Parameter sharing creates stochastic dependencies between different coordinates of weight tensors, which must be accounted for in the mathematical analysis of structured pruning
  - Quick check question: How does parameter sharing in CNNs differ from the independent parameter assumption typically used in theoretical analyses of neural networks?

- Concept: Concentration inequalities and the second moment method
  - Why needed here: The proof uses concentration inequalities to bound the probability of successful approximation and the second moment method to show that the expected number of good subnetworks is positive
  - Quick check question: What role does the second moment method play in proving the existence of structured subnetworks in random CNNs?

## Architecture Onboarding

- Component map: Random CNN (normal tensors) -> Structured pruning (block masks, filter removal) -> Approximated target network
- Critical path: Random CNN → Apply structured pruning → Verify approximation to target network
- Design tradeoffs:
  - Width vs depth: The paper requires polynomial over-parameterization in width and a specific depth structure (2ℓ layers for ℓ-layer target networks)
  - Sparsity pattern choice: Block sparsity vs filter sparsity vs neuron sparsity tradeoffs for computational efficiency vs approximation accuracy
  - Distribution assumptions: Normal initialization vs other distributions and their impact on the proof
- Failure signatures:
  - Approximation error exceeds ε: Likely due to insufficient over-parameterization or inappropriate pruning pattern
  - Pruning removes too many parameters: Indicates the bound on n is not met or the target network is too complex
  - Computational efficiency gains not realized: Suggests the structured sparsity pattern is not well-suited to the hardware or implementation
- First 3 experiments:
  1. Implement the random CNN construction with normal initialization and verify the shapes match the theorem requirements
  2. Apply 2n-channel-blocked masks and filter removal to create structured subnetworks from the random CNN
  3. Measure the approximation error between pruned subnetworks and target networks of varying sizes to validate the ε bound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the polynomial width overhead in Theorem 2 be reduced to logarithmic, similar to unstructured pruning results?
- Basis in paper: [explicit] The paper notes that "the width overhead could be greatly reduced by sampling parameters from a hyperbolic distribution" as shown in [OHR20], but this is for unstructured pruning.
- Why unresolved: The paper's structured pruning approach relies on multidimensional random subset sum results that inherently require polynomial bounds due to dependencies in CNNs, unlike unstructured pruning.
- What evidence would resolve it: A proof showing that structured pruning can achieve logarithmic width overhead while maintaining approximation guarantees, or a lower bound proving polynomial width is necessary.

### Open Question 2
- Question: Can the depth overhead of one extra layer be eliminated entirely for structured pruning?
- Basis in paper: [explicit] "The hypothesis on the shape can be a relevant limitation for such use cases. The constructions proposed by [Bur22a, Bur22b] appear as a promising direction to overcome this limitation, with the added benefit of reducing the depth overhead."
- Why unresolved: The current proof requires specific kernel shapes (1×1) in intermediate layers as an artifact of the analysis, which may not be necessary for all structured pruning patterns.
- What evidence would resolve it: A proof showing that structured pruning can work without any depth overhead while maintaining approximation guarantees, or demonstrating that depth overhead is necessary for certain structured patterns.

### Open Question 3
- Question: Can the results be extended to activation functions beyond ReLU?
- Basis in paper: [explicit] "Another limitation of our results is the restriction to ReLU as the activation function... Our analysis, on the other hand, does not rely on such property, so adapting the approach of [Bur22a] to our setting is not straightforward."
- Why unresolved: The proof technique relies heavily on ReLU's properties (x = φ(x) - φ(-x)), and extending to other activations would require new mathematical tools.
- What evidence would resolve it: A proof extending the structured pruning results to a broad class of activation functions (e.g., leaky ReLU, Swish), or proving that ReLU is necessary for the current approach.

## Limitations
- The polynomial over-parameterization bounds may be overly conservative in practice, with constants that could be quite large
- Assumes normal initialization, which may not hold for real-world networks using other initialization schemes
- Structured pruning patterns considered may not capture all practical pruning strategies used in modern architectures

## Confidence
- High confidence: The existence of structured subnetworks in random CNNs (Theorem 2) - supported by rigorous proof and multiple validation steps
- Medium confidence: The polynomial over-parameterization bounds are tight and practical - theoretical bounds may be conservative
- Medium confidence: The NSN vector model accurately captures parameter sharing dependencies - assumption appears reasonable but not empirically validated

## Next Checks
1. Implement empirical validation of the polynomial over-parameterization bounds across different CNN architectures to determine if the theoretical bounds are loose in practice
2. Test the approximation guarantees with non-normal initialization schemes (e.g., He, Xavier) to verify robustness beyond the theoretical assumptions
3. Compare the structured pruning patterns to unstructured pruning in terms of both approximation accuracy and computational efficiency to validate the practical tradeoffs