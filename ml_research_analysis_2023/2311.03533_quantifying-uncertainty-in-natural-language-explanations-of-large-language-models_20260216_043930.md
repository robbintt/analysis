---
ver: rpa2
title: Quantifying Uncertainty in Natural Language Explanations of Large Language
  Models
arxiv_id: '2311.03533'
source_url: https://arxiv.org/abs/2311.03533
tags:
- confidence
- explanations
- uncertainty
- answer
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes two novel metrics to quantify uncertainty in
  natural language explanations (NLEs) generated by large language models (LLMs).
  The first metric, verbalized uncertainty, directly prompts the LLM to express its
  confidence in the explanations it generates.
---

# Quantifying Uncertainty in Natural Language Explanations of Large Language Models

## Quick Facts
- arXiv ID: 2311.03533
- Source URL: https://arxiv.org/abs/2311.03533
- Reference count: 40
- Key outcome: Proposes two metrics to quantify uncertainty in LLM-generated natural language explanations, finding that probing uncertainty is more reliable than verbalized uncertainty.

## Executive Summary
This work addresses the critical question of whether large language models can reliably quantify their own uncertainty when providing natural language explanations for their outputs. The authors propose two novel metrics: verbalized uncertainty, which directly prompts the LLM to express confidence, and probing uncertainty, which measures explanation consistency under input perturbations. Through extensive experiments on five benchmark datasets and three LLM variants, the study reveals that verbalized uncertainty is often unreliable, with LLMs exhibiting high overconfidence. In contrast, probing uncertainty shows meaningful correlation with explanation faithfulness, with lower uncertainty corresponding to more reliable explanations.

## Method Summary
The authors develop two uncertainty quantification methods for natural language explanations. Verbalized uncertainty directly prompts the LLM to express confidence in its explanations. Probing uncertainty uses two strategies: sample probing (generating semantically equivalent paraphrases of questions and measuring explanation consistency) and model probing (generating multiple explanations with stochastic sampling at controlled temperature). The study evaluates these methods across five datasets (GSM8K, SVAMP, ASDiv, StrategyQA, Sports Understanding) using three LLM variants (InstructGPT, GPT-3.5, GPT-4). Faithfulness is assessed using counterfactual intervention for token importance explanations and early-answering strategies for chain-of-thought explanations.

## Key Results
- Verbalized uncertainty is unreliable, with average confidence scores of 94.46% across all explanations
- Probing uncertainty estimates correlate with explanation faithfulness, with lower uncertainty corresponding to higher faithfulness
- Explanations for correct answers show significantly lower uncertainty compared to those for incorrect answers
- Among perturbation strategies, sample probing with 10 paraphrases yields the highest correlation with faithfulness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Verbalized uncertainty fails because LLMs are trained to produce confident-sounding outputs regardless of internal certainty.
- **Mechanism**: LLMs optimize for generating plausible-sounding text; confidence cues in natural language are learned patterns that correlate with correctness in training data but do not reflect internal probabilistic reasoning.
- **Core assumption**: Training data contains explicit confidence annotations or patterns that the model learns to mimic.
- **Evidence anchors**:
  - [abstract]: "verbalized uncertainty is not a reliable estimate of explanation confidence and LLMs often exhibit very high verbalized confidence in the explanations they generate"
  - [section]: "Our results in Figure 4 show that, on average, across both explanation methods and five datasets, the verbalized confidence is 94.46%"
  - [corpus]: Weak; no direct training data analysis provided
- **Break condition**: If confidence annotations are sparse or absent in pretraining data, verbalized uncertainty cannot be learned as a reliable signal.

### Mechanism 2
- **Claim**: Probing uncertainty via input perturbations captures explanation stability, which correlates with faithfulness.
- **Mechanism**: When explanations remain consistent across semantically equivalent perturbations, the model's reasoning process is more stable and thus more faithful to the underlying decision process.
- **Core assumption**: Faithful explanations are those that would remain stable under input perturbations that preserve the answer.
- **Evidence anchors**:
  - [abstract]: "probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness"
  - [section]: "We define the uncertainty in token importance explanations as the mean agreement between perturbed explanations and the original explanation"
  - [corpus]: Weak; correlation claim not directly validated with faithfulness ground truth in corpus
- **Break condition**: If perturbations introduce semantic drift or if the model's reasoning is inherently unstable, probing uncertainty may not correlate with faithfulness.

### Mechanism 3
- **Claim**: Explanation confidence correlates with prediction correctness because correct reasoning paths are more internally consistent.
- **Mechanism**: When LLMs arrive at correct answers, the intermediate reasoning steps (CoT) or token importance assignments are more coherent and thus generate higher confidence scores.
- **Core assumption**: Correct answers have more coherent reasoning processes than incorrect ones.
- **Evidence anchors**:
  - [abstract]: "explanations for correct answers tend to have lower uncertainty compared to those for incorrect answers"
  - [section]: "Fig. 5 shows that explanations of correct answers have higher explanation confidence compared to explanations of wrong answers"
  - [corpus]: Weak; correlation observed empirically but mechanism not explicitly modeled in corpus
- **Break condition**: If the model uses post-hoc rationalization or if correct answers arise from lucky guesses, confidence may not correlate with correctness.

## Foundational Learning

- **Concept**: Semantic equivalence and paraphrase detection
  - **Why needed here**: Sample probing requires generating semantically equivalent questions to measure explanation stability
  - **Quick check question**: Can you generate three paraphrases of "The cat sat on the mat" that preserve meaning but differ in wording?

- **Concept**: Temperature sampling and stochastic generation in LLMs
  - **Why needed here**: Model probing leverages temperature to generate multiple explanations and measure consistency
  - **Quick check question**: What happens to output diversity when you increase temperature from 0.0 to 1.0 in an LLM?

- **Concept**: Faithfulness metrics for natural language explanations
  - **Why needed here**: The work evaluates whether uncertainty correlates with faithfulness, requiring understanding of counterfactual tests and early-answering strategies
  - **Quick check question**: How would you test if a token importance explanation faithfully reflects the model's actual reasoning?

## Architecture Onboarding

- **Component map**: Input preprocessor -> LLM interface -> Paraphrase generator -> Explanation parser -> Agreement calculator -> Statistical analyzer
- **Critical path**: Question ‚Üí Prompt generation ‚Üí LLM execution ‚Üí Explanation parsing ‚Üí Uncertainty calculation ‚Üí Statistical analysis
- **Design tradeoffs**: 
  - Verbalized uncertainty is computationally cheap but unreliable
  - Probing uncertainty is more reliable but requires multiple LLM calls
  - Token importance vs CoT explanations have different agreement calculation complexities
- **Failure signatures**:
  - Verbalized confidence consistently near 100% across all explanations
  - Probing uncertainty fails to correlate with faithfulness when perturbations are too aggressive
  - Agreement metrics produce NaNs when explanations have incompatible formats
- **First 3 experiments**:
  1. Generate 10 paraphrases of a sample question and verify semantic equivalence
  2. Run verbalized uncertainty on a simple math problem and observe confidence distribution
  3. Compare token agreement scores for correct vs incorrect answers on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of temperature parameter ùúè affect the reliability of model probing uncertainty estimates across different LLM architectures?
- Basis in paper: [explicit] The paper discusses using temperature parameter ùúè to control stochasticity in model probing, but doesn't explore how different ùúè values impact uncertainty reliability.
- Why unresolved: The authors only use ùúè=1.0 for their experiments, leaving open how other temperature settings might affect the correlation between probing uncertainty and explanation faithfulness.
- What evidence would resolve it: Systematic experiments varying ùúè across multiple LLM architectures, measuring the correlation between uncertainty estimates and explanation faithfulness at each temperature setting.

### Open Question 2
- Question: Can uncertainty quantification methods be extended to more complex explanation types beyond token importance and chain-of-thought, such as free-form natural language explanations?
- Basis in paper: [inferred] The paper focuses on two specific explanation methods but acknowledges that other NLE types exist, suggesting the need for generalization.
- Why unresolved: The proposed metrics are tailored to token importance and CoT explanations, and the paper doesn't address how to adapt these methods to more free-form explanation types.
- What evidence would resolve it: Development and validation of uncertainty quantification methods for free-form NLEs, with empirical comparison to the proposed metrics.

### Open Question 3
- Question: How do the proposed uncertainty metrics perform when applied to domain-specific tasks outside of math word problems and commonsense reasoning?
- Basis in paper: [explicit] The authors note their experiments are limited to math word problems and commonsense reasoning datasets.
- Why unresolved: The paper's evaluation is confined to five specific datasets, leaving the generalizability of the uncertainty metrics to other domains unexplored.
- What evidence would resolve it: Application and evaluation of the uncertainty metrics on diverse domain-specific tasks, measuring their effectiveness in quantifying explanation reliability across different contexts.

## Limitations
- Verbalized uncertainty consistently shows high overconfidence, failing as a reliable metric
- Probing uncertainty correlation with faithfulness remains weak (Pearson correlation < 0.4)
- Limited evaluation to five specific datasets (math word problems and commonsense reasoning)
- Does not address whether explanations are post-hoc rationalizations versus faithful reasoning

## Confidence
- **High confidence**: Probing uncertainty is more reliable than verbalized uncertainty for measuring explanation confidence
- **Medium confidence**: Lower uncertainty correlates with higher faithfulness and correctness of answers
- **Low confidence**: The specific mechanisms explaining why verbalized uncertainty fails or why probing uncertainty works are fully understood

## Next Checks
1. Cross-dataset validation: Test the uncertainty-faithfulness correlation on additional domains beyond the five current datasets to verify generalizability
2. Ablation study: Compare different perturbation strategies (number of paraphrases, temperature settings) to identify optimal configurations for probing uncertainty
3. Human evaluation: Conduct controlled human studies to validate whether lower uncertainty explanations are perceived as more trustworthy and faithful than high-uncertainty ones