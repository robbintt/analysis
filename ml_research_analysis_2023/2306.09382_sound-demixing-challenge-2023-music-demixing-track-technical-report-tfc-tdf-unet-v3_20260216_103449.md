---
ver: rpa2
title: 'Sound Demixing Challenge 2023 Music Demixing Track Technical Report: TFC-TDF-UNet
  v3'
arxiv_id: '2306.09382'
source_url: https://arxiv.org/abs/2306.09382
tags:
- music
- training
- source
- separation
- leaderboard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report presents TFC-TDF-UNet v3, a time-efficient music source
  separation model that achieves state-of-the-art performance on the MUSDB18-HQ benchmark
  with 8.12 median SDR. The model improves upon its predecessor through architectural
  enhancements including residual U-Net structure, time-domain filter blocks, channel-wise
  sub-bands, and instance normalization.
---

# Sound Demixing Challenge 2023 Music Demixing Track Technical Report: TFC-TDF-UNet v3

## Quick Facts
- arXiv ID: 2306.09382
- Source URL: https://arxiv.org/abs/2306.09382
- Reference count: 0
- Primary result: Achieved 1st place on Leaderboard B (bleeding) and 3rd place on Leaderboard A (label-noise) at SDX23

## Executive Summary
This report presents TFC-TDF-UNet v3, a time-efficient music source separation model that achieves state-of-the-art performance on the MUSDB18-HQ benchmark with 8.12 median SDR. The model improves upon its predecessor through architectural enhancements including residual U-Net structure, time-domain filter blocks, channel-wise sub-bands, and instance normalization. For the Sound Demixing Challenge 2023, we developed noise-robust training approaches for both label-noise and bleeding scenarios using loss masking techniques. These methods successfully filtered corrupted training data by discarding high-loss chunks, achieving 1st place on Leaderboard B (bleeding) and 3rd place on Leaderboard A (label-noise). The final ensemble model combines multiple TFC-TDF-UNet v3 instances with different random seeds and augmentation strategies. All code is publicly available at github.com/kuielab/sdx23.

## Method Summary
The approach uses TFC-TDF-UNet v3 architecture with time-domain filter blocks and residual U-Net structure for music source separation. For noise-robust training, loss masking techniques filter corrupted training chunks based on high reconstruction loss, effectively training only on cleaner data. The final submission uses an ensemble of three independently trained models with different random seeds and augmentation strategies to reduce variance and improve performance.

## Key Results
- Achieved 1st place on Leaderboard B (bleeding scenario) at SDX23
- Achieved 3rd place on Leaderboard A (label-noise scenario) at SDX23
- 8.12 median SDR on MUSDB18-HQ benchmark
- Successfully implemented loss masking techniques that filter corrupted training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss masking improves robustness to label-noise and bleeding by filtering corrupted training chunks based on high training loss.
- Mechanism: During training, chunks with high reconstruction loss are assumed to contain more noise and are discarded from the gradient update. This effectively trains the model only on cleaner chunks.
- Core assumption: Chunks with higher training loss contain more noise because the model struggles to reconstruct corrupted sources.
- Evidence anchors:
  - [abstract]: "These methods successfully filtered corrupted training data by discarding high-loss chunks"
  - [section]: "To reduce the negative effects of these noisy chunks and train only with clean chunks, we use a loss masking scheme"
  - [corpus]: Weak evidence - corpus contains related music source separation papers but no specific mention of loss masking techniques
- Break condition: If the noise distribution is uniform across all chunks, loss masking would discard both clean and noisy chunks indiscriminately, reducing training effectiveness.

### Mechanism 2
- Claim: Ensemble models with different random seeds and augmentation strategies improve final performance by reducing variance.
- Mechanism: Multiple independently trained models with different initialization and augmentation provide diverse predictions that are averaged for the final output, reducing overfitting to specific training conditions.
- Core assumption: Individual models will have different strengths and weaknesses based on their training randomness, and averaging their predictions will smooth out individual model errors.
- Evidence anchors:
  - [abstract]: "The final ensemble model combines multiple TFC-TDF-UNet v3 instances with different random seeds and augmentation strategies"
  - [section]: "For each Leaderboard, the final submission is an ensemble of three TFC-TDF-UNet v3 models trained with the noise-robust training loss"
  - [corpus]: No direct evidence in corpus about ensemble methods for music source separation
- Break condition: If all models converge to very similar solutions, the ensemble provides minimal benefit and just increases computational cost.

### Mechanism 3
- Claim: Time-domain filter blocks (TDF) combined with residual U-Net structure improve feature extraction efficiency for music source separation.
- Mechanism: TDF blocks operate directly on time-domain features while the residual connections help propagate information through the network, allowing efficient learning of temporal patterns in music signals.
- Core assumption: Music source separation benefits from both time-domain and frequency-domain feature extraction, and residual connections help maintain gradient flow in deep networks.
- Evidence anchors:
  - [abstract]: "TFC-TDF-UNet v3, a time-efficient music source separation model"
  - [section]: "Change overall structure to a ResUnet-like structure and add a TDF block to each Residual Block"
  - [corpus]: Weak evidence - corpus contains related papers but no specific mention of TDF blocks or their effectiveness
- Break condition: If the TDF blocks don't capture relevant temporal features for the specific instruments being separated, they may add unnecessary complexity without benefit.

## Foundational Learning

- Concept: Signal processing fundamentals (STFT, time-frequency representations)
  - Why needed here: The model uses both time-domain and frequency-domain processing, requiring understanding of how audio signals are transformed and represented
  - Quick check question: What is the relationship between window size, hop length, and time-frequency resolution in STFT?

- Concept: Loss functions and gradient-based optimization
  - Why needed here: The noise-robust training relies on loss-based filtering, and understanding how different loss functions affect model behavior is crucial
  - Quick check question: How does L2 loss behave differently from L1 loss when applied to audio signals with outliers?

- Concept: Ensemble methods and model averaging
  - Why needed here: The final solution uses ensemble averaging, requiring understanding of how and why multiple models can improve performance
  - Quick check question: What are the conditions under which ensemble averaging provides the most benefit for deep learning models?

## Architecture Onboarding

- Component map: Raw waveform -> TFC-TDF-UNet v3 (Encoder with TDF blocks, Instance Normalization, GELU activation) -> Decoder with skip connections -> Separated source estimates

- Critical path: Data loading and augmentation -> Forward pass through TFC-TDF-UNet v3 -> Loss computation with masking -> Backpropagation and parameter update -> Ensemble prediction during inference

- Design tradeoffs:
  - TDF blocks vs pure spectrogram-based methods: TDF adds time-domain processing capability but increases model complexity
  - Loss masking parameters (q value): Higher q filters more data but may remove useful information; lower q keeps more data but may include noise
  - Ensemble size: Larger ensembles provide better performance but increase inference time and memory usage

- Failure signatures:
  - High variance in leaderboard performance across different runs
  - Degradation in SDR when using loss masking with extreme q values
  - Memory issues when trying to train with larger batch sizes

- First 3 experiments:
  1. Train a single TFC-TDF-UNet v3 without loss masking on clean MUSDB data to establish baseline performance
  2. Implement and test loss masking with different q values on noisy training data to find optimal filtering threshold
  3. Create ensemble of 3 models with different random seeds and compare performance against single model baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of loss masking techniques vary with different quantile thresholds (q) across different types of noise corruption (label-noise vs bleeding)?
- Basis in paper: [explicit] The paper mentions using different q values for label-noise (no specific q given) and bleeding (q=0.93) and shows an ablation study in Table 2 comparing models with and without loss masking.
- Why unresolved: The paper does not explore a range of q values systematically for either type of noise, nor does it compare optimal q values across the two noise types in a controlled manner.
- What evidence would resolve it: A systematic ablation study varying q from 0.5 to 1.0 for both label-noise and bleeding scenarios, comparing SDR performance across the range.

### Open Question 2
- Question: What is the impact of temporal vs batch dimension loss masking on bleeding corruption specifically, and why does the temporal approach work better for bleeding than for label-noise?
- Basis in paper: [explicit] The paper states that for bleeding, they used a fine-grained masking scheme along both temporal and batch dimensions, with q=0.93 (meaning 7% of temporal bins discarded), while for label-noise they only masked along batch dimension.
- Why unresolved: The paper does not explain the theoretical or empirical justification for why temporal masking is more effective for bleeding than label-noise, nor does it test whether temporal masking would improve label-noise performance.
- What evidence would resolve it: Controlled experiments testing temporal masking on label-noise data and batch-only masking on bleeding data, with analysis of the noise characteristics in each case.

### Open Question 3
- Question: How does the ensemble strategy (combining multiple models with different seeds and augmentations) compare to other ensembling methods like model averaging or stacking in terms of SDR improvement and computational efficiency?
- Basis in paper: [explicit] The paper mentions that final submissions are ensembles of three TFC-TDF-UNet v3 models trained with different random seeds, but does not compare this approach to other ensembling strategies.
- Why unresolved: The paper does not provide a comparison of different ensemble methods or analyze the marginal benefit of each ensemble component.
- What evidence would resolve it: A comparative study of different ensemble strategies (seed-based, augmentation-based, model averaging, stacking) on the same base model, measuring both SDR improvement and computational overhead.

## Limitations

- The effectiveness of loss masking relies on the assumption that high-loss chunks correlate with noisier data, which requires further validation
- Limited ablation studies on critical hyperparameters like the q value for loss masking thresholds
- Lack of testing on diverse real-world scenarios beyond the specific SDX23 competition datasets

## Confidence

- High confidence: The core TFC-TDF-UNet v3 architecture and its performance on standard MUSDB18-HQ benchmarks (8.12 median SDR)
- Medium confidence: The effectiveness of loss masking for noise-robust training, as the report provides limited ablation studies and the assumption that high-loss chunks correlate with noisier data requires further validation
- Low confidence: The generalizability of the approach beyond the specific SDX23 datasets, given the lack of testing on diverse real-world scenarios

## Next Checks

1. Conduct ablation studies on the TDF block effectiveness by comparing performance with and without TDF components on the same hardware
2. Validate the loss masking assumption by analyzing the correlation between loss magnitude and actual noise levels in training chunks
3. Test the trained models on additional benchmark datasets (e.g., MUSDB18-HQ test set, other source separation benchmarks) to assess generalizability beyond the SDX23 competition data