---
ver: rpa2
title: Clustering and Uncertainty Analysis to Improve the Machine Learning-based Predictions
  of SAFARI-1 Control Follower Assembly Axial Neutron Flux Profiles
arxiv_id: '2312.14193'
source_url: https://arxiv.org/abs/2312.14193
tags:
- uni00000014
- uni00000013
- uni00000015
- uni00000010
- uni00000026
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning (ML) framework that combines
  unsupervised clustering analysis with supervised regression models to improve predictions
  of axial neutron flux profiles in the SAFARI-1 research reactor. The approach addresses
  the challenge of poor ML prediction accuracy for control follower assemblies, which
  was attributed to the presence of multiple distinct clusters of axial profile shapes
  in the measurement data.
---

# Clustering and Uncertainty Analysis to Improve the Machine Learning-based Predictions of SAFARI-1 Control Follower Assembly Axial Neutron Flux Profiles

## Quick Facts
- arXiv ID: 2312.14193
- Source URL: https://arxiv.org/abs/2312.14193
- Reference count: 40
- Primary result: Clustering reduces NRMSE prediction error for control assemblies from 23% to <11%, achieving comparable accuracy to fuel assemblies

## Executive Summary
This paper presents a machine learning framework that combines unsupervised clustering with supervised regression models to improve predictions of axial neutron flux profiles in the SAFARI-1 research reactor. The approach addresses poor ML prediction accuracy for control follower assemblies, which was attributed to multiple distinct clusters of axial profile shapes in the measurement data. By applying k-means and Affinity Propagation clustering algorithms to identify clusters, then training separate Deep Neural Network (DNN) and Gaussian Process (GP) regression models for each cluster, the method achieves significant improvements in prediction accuracy while providing uncertainty quantification.

## Method Summary
The method combines unsupervised clustering with supervised regression to predict axial neutron flux profiles. First, historical copper-wire measurement data undergoes preprocessing including decay correction, z-score normalization, Savitzky-Golay smoothing, and interpolation to fill missing values. k-means and Affinity Propagation clustering algorithms identify distinct clusters in the axial flux profiles of control follower assemblies. Separate DNN (with Monte Carlo Dropout for uncertainty quantification) and GP regression models are then trained for each cluster. The GP model provides prediction uncertainty through its covariance matrix, while the DNN uncertainty is estimated using Monte Carlo Dropout. Finally, prediction accuracy is evaluated using Normalized Root Mean Square Error (NRMSE) metrics.

## Key Results
- Clustering reduces NRMSE prediction error for control assemblies from 23% to <11%, achieving comparable accuracy to fuel assemblies
- Both DNN and GP models show significant performance improvements after clustering the dataset
- Clustering reduces GP training time by a factor of three, providing computational efficiency
- DNN and GP prediction uncertainties exhibit large differences at certain assembly axial locations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering separates heterogeneous axial profile shapes, allowing regression models to fit each cluster more accurately.
- Mechanism: The control follower assemblies contain two or three distinct clusters of axial neutron flux profiles. Training separate models for each cluster reduces model confusion between incompatible shapes, leading to better predictions.
- Core assumption: The clusters represent meaningful structural differences in flux shapes that are not due to noise.
- Evidence anchors:
  - [abstract] "The approach addresses the challenge of poor ML prediction accuracy for control follower assemblies, which was attributed to the presence of multiple distinct clusters of axial profile shapes in the measurement data."
  - [section] "Our recent analyses [5] revealed that the reduced ML model prediction accuracy of the CAs can be attributed to the presence of at least two distinct, yet unexplained, groups... of axial profile shapes in the CAs' measured data."
  - [corpus] Weak: No direct corpus match, but the paper references prior clustering studies in nuclear engineering.
- Break condition: If clusters are not stable across cycles or if the number of clusters varies unpredictably.

### Mechanism 2
- Claim: GP regression naturally provides prediction uncertainty, enabling quantitative comparison with DNN uncertainty estimates.
- Mechanism: GP models output both a mean prediction and a covariance matrix. The diagonal of this covariance gives per-point uncertainty, while the off-diagonal terms capture correlations between points.
- Core assumption: The kernel choice (squared exponential) adequately captures the smoothness and correlation structure of the flux profiles.
- Evidence anchors:
  - [section] "GP prediction uncertainty mainly reflect the noises in the data as well as data coverage, while DNN prediction uncertainty may also be notably influenced by model architecture (the model being too simple/complex) and randomness in training..."
  - [section] "The quantified uncertainties in GP and DNN predictions exhibited, however, large differences in certain assembly axial locations."
  - [corpus] Weak: No direct corpus match for SAFARI-1 GP applications.
- Break condition: If the chosen kernel does not match the true covariance structure of the data.

### Mechanism 3
- Claim: Monte Carlo Dropout (MCD) in DNNs approximates Bayesian inference, providing uncertainty estimates without changing the model architecture.
- Mechanism: By applying dropout during both training and inference, MCD samples from an approximate posterior over the network weights. Averaging over multiple forward passes yields mean predictions and variance estimates.
- Core assumption: Dropout at inference approximates sampling from a variational posterior over weights.
- Evidence anchors:
  - [section] "In MCD, for a DNN of depth L, with a dropout probability p, the loss function is given by..." (describes the MCD training procedure).
  - [section] "The DNN training process only provides deterministic estimates of the DNN parameters (weights and biases). In this study, the MCD technique [20] is employed to quantify uncertainties of the DNN predictions."
  - [corpus] Weak: No direct corpus match for SAFARI-1 DNN with MCD.
- Break condition: If dropout rate is too high or too low, the uncertainty estimates become unreliable.

## Foundational Learning

- Concept: Clustering algorithms (k-means and Affinity Propagation) for unsupervised learning.
  - Why needed here: To separate the heterogeneous axial flux profile shapes into clusters before training regression models.
  - Quick check question: What is the main difference between k-means and Affinity Propagation clustering?

- Concept: Gaussian Process regression and kernel functions.
  - Why needed here: GP provides both predictions and uncertainty estimates, serving as a benchmark for DNN uncertainty quantification.
  - Quick check question: How does the length-scale parameter in a squared exponential kernel affect the smoothness of the predicted function?

- Concept: Monte Carlo Dropout for uncertainty quantification in neural networks.
  - Why needed here: To estimate prediction uncertainty for DNN models without changing the model architecture.
  - Quick check question: Why does applying dropout at inference time help estimate uncertainty?

## Architecture Onboarding

- Component map: Data preprocessing -> Clustering (k-means, AP) -> Separate model training per cluster (DNN with MCD, GP) -> Uncertainty quantification -> Evaluation (NRMSE)
- Critical path: Data preprocessing → Clustering → Separate model training per cluster → Uncertainty quantification → Evaluation
- Design tradeoffs:
  - Clustering choice: k-means is simpler but requires specifying k; AP automatically finds clusters but is computationally expensive.
  - GP kernel: Squared exponential is smooth but may not capture all data patterns; other kernels could be explored.
  - DNN architecture: Deeper networks may capture complex patterns but risk overfitting; dropout rate affects uncertainty estimates.
- Failure signatures:
  - Clustering instability: Different runs produce different cluster assignments.
  - Poor NRMSE: Models fail to capture the flux profile shapes, possibly due to insufficient training data or model complexity.
  - Inconsistent uncertainty: DNN and GP uncertainties differ significantly without a clear reason.
- First 3 experiments:
  1. Run clustering on a small subset of data and visualize the resulting clusters to verify separation.
  2. Train a single DNN and GP model on the full dataset (without clustering) and compare NRMSE to the clustered approach.
  3. Generate predictions and uncertainties for one assembly and one cycle, then plot the results to visually inspect fit quality and uncertainty bands.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact nature of the external factor (if any) that causes the formation of two distinct clusters in the control follower assembly axial flux profiles?
- Basis in paper: [inferred] The paper mentions that the clusters appear to be randomly distributed over historical cycles and control bank positions, suggesting a stochastic nature. However, the exact cause is unknown.
- Why unresolved: The paper only speculates about potential causes like inadvertent misplacement of the copper wire during measurement, but does not provide definitive evidence.
- What evidence would resolve it: Systematic investigation of the measurement process, including careful examination of the physical setup and potential sources of error, could provide evidence for the cause of the clustering.

### Open Question 2
- Question: Why do the DNN and GP models exhibit large differences in their prediction uncertainties for certain assembly axial locations?
- Basis in paper: [explicit] The paper explicitly states that the quantified uncertainties in GP and DNN predictions exhibit large differences in certain assembly axial locations.
- Why unresolved: The paper does not provide a detailed explanation for this observation. It mentions that the DNN uncertainty may be influenced by model architecture and randomness in training, while GP uncertainty mainly reflects noise and data coverage.
- What evidence would resolve it: Further investigation into the specific sources of uncertainty in both DNN and GP models, including sensitivity analysis and comparison of model architectures, could help explain the differences in uncertainty predictions.

### Open Question 3
- Question: How can the computational efficiency of the GP model be further improved without sacrificing prediction accuracy?
- Basis in paper: [explicit] The paper mentions that splitting the training dataset into clusters resulted in a significant acceleration of the GP training time, but does not explore other potential optimization techniques.
- Why unresolved: The paper does not explore alternative methods for improving the computational efficiency of the GP model, such as sparse approximations or parallel processing.
- What evidence would resolve it: Comparison of different optimization techniques and their impact on the computational efficiency and prediction accuracy of the GP model would provide evidence for the most effective approach.

## Limitations
- Cluster stability is not verified across all 76 historical cycles, raising questions about generalizability
- Large differences between DNN and GP uncertainty estimates lack mechanistic explanations
- Results are specific to SAFARI-1 reactor data and may not generalize to other reactor designs

## Confidence
- **High confidence**: The core methodology of using clustering to improve regression performance for heterogeneous data is well-established in ML literature and the implementation details are sufficiently specified for reproduction.
- **Medium confidence**: The reported performance improvements are convincing for the SAFARI-1 dataset, but the stability of clusters and the validity of uncertainty comparisons require further investigation.
- **Low confidence**: The mechanistic explanation for why GP and DNN uncertainties differ significantly in certain locations, and which uncertainty estimate should be trusted.

## Next Checks
1. Apply the clustering algorithm to successive 10-cycle subsets of the historical data to verify that the same number and characteristics of clusters emerge consistently.
2. Generate synthetic flux profiles with known uncertainty characteristics and test whether GP or DNN uncertainty estimates better capture the true uncertainty in these controlled scenarios.
3. Repeat the entire analysis pipeline without the Savitzky-Golay smoothing and interpolation steps to quantify their impact on clustering quality and final prediction accuracy.