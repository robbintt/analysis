---
ver: rpa2
title: 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension
  Tasks'
arxiv_id: '2310.09036'
source_url: https://arxiv.org/abs/2310.09036
tags:
- multimodal
- performance
- instructions
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MM-BigBench, a comprehensive evaluation framework
  for assessing the performance of multimodal large language models (MLLMs) on multimodal
  content comprehension tasks. Unlike previous works that primarily focus on unimodal
  content understanding, MM-BigBench evaluates MLLMs on a diverse set of multimodal
  tasks, including multimodal sentiment analysis, multimodal aspect-based sentiment
  analysis, multimodal hateful memes recognition, multimodal sarcasm recognition,
  multimodal relation extraction, and visual question answering.
---

# MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks

## Quick Facts
- arXiv ID: 2310.09036
- Source URL: https://arxiv.org/abs/2310.09036
- Reference count: 40
- Key outcome: MM-BigBench framework evaluates MLLMs on multimodal content comprehension tasks, showing Flan-T5-XXL based models outperform LLaMA series and instruction tuning improves stability

## Executive Summary
This paper introduces MM-BigBench, a comprehensive evaluation framework for assessing multimodal large language models (MLLMs) on multimodal content comprehension tasks. Unlike previous works focusing on unimodal understanding, MM-BigBench evaluates models across six diverse tasks including sentiment analysis, hateful memes recognition, and visual question answering. The framework introduces multiple metrics (Best Performance, Mean Relative Gain, Stability, Adaptability) to comprehensively assess model and instruction performance. Extensive experiments with 20 language models (14 MLLMs) on 14 multimodal datasets demonstrate that models with Flan-T5-XXL backbone outperform LLaMA series, instructions in 'Question-Answer' format yield better results, and instruction tuning enhances model stability across tasks.

## Method Summary
The study evaluates 20 language models (6 pure LLMs and 14 MLLMs) on 14 multimodal datasets spanning six tasks using zero-shot evaluation. For each dataset, 10 manually designed multimodal instructions are used, following a structured format with components including task name, task definition, text context, image context, output format, and optional elements like questions and options. The evaluation employs multiple metrics: Accuracy as the primary metric, Best Performance to measure optimal instruction performance, Mean Relative Gain to quantify instruction impact, Stability to assess performance consistency across instructions, and Adaptability to evaluate performance across different tasks. Video datasets are converted to text-image format by randomly selecting frames from videos.

## Key Results
- Models with Flan-T5-XXL as backbone (Encoder-Decoder architecture) outperform LLaMA series (Decoder-only architecture) across multimodal tasks
- Instructions in 'Question-Answer' format consistently perform better than other formats for both pure LLMs and MLLMs
- MLLMs trained using instruction tuning exhibit greater stability across various tasks compared to models without instruction tuning
- The addition of options further improves model performance on certain tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flan-T5-XXL based models outperform LLaMA series in multimodal content comprehension tasks.
- Mechanism: The Encoder-Decoder architecture of Flan-T5-XXL provides advantages in multimodal representation learning compared to the Decoder-only architecture of LLaMA series.
- Core assumption: Encoder-Decoder architecture is inherently better suited for multimodal content comprehension than Decoder-only architecture.
- Evidence anchors:
  - [abstract] "models with Flan-T5-XXL as the backbone, based on the Encoder-Decoder architecture, outperform the LLaMA series models with the Decoder-only architecture"
  - [section 4.1] "The encoder module of the latter provides significant advantages in multimodal representation learning for multimodal content comprehension tasks"
- Break condition: If the tasks evaluated don't actually require the multimodal fusion capabilities that the Encoder-Decoder architecture provides.

### Mechanism 2
- Claim: Instructions in a 'Question-Answer' format perform better than other formats.
- Mechanism: Models have been trained on QA tasks, so specific words like 'Question' and 'Answer' aid in enhancing the model's performance.
- Core assumption: The models' training data included significant exposure to QA-formatted instructions.
- Evidence anchors:
  - [abstract] "Instructions in a 'Question-Answer' format perform better, and the addition of options further improves the model's performance on certain tasks"
  - [section 4.2] "This phenomenon is consistent among both pure LLMs and MLLMs. It further demonstrates that most LMs are better suited for instructions designed in a Question-answer format"
- Break condition: If the models were primarily trained on non-QA tasks or if the instruction tuning didn't include QA formats.

### Mechanism 3
- Claim: MLLMs trained using instruction tuning exhibit greater stability across various tasks.
- Mechanism: Instruction tuning reduces the sensitivity of models to different instructions in multimodal content comprehension tasks.
- Core assumption: Instruction tuning creates more generalizable models that are less sensitive to instruction variations.
- Evidence anchors:
  - [abstract] "The performance of MLLMs trained using instruction tuning exhibits greater stability across various tasks when compared to models that do not undergo instruction tuning"
  - [section 4.4] "It demonstrates that instruction tuning can mitigate the sensitivity of models to different instructions in multimodal content comprehension tasks"
- Break condition: If the instruction tuning process doesn't generalize well to new tasks or if the instruction tuning data is too narrow.

## Foundational Learning

- Concept: Multimodal content comprehension tasks
  - Why needed here: Understanding the distinction between multimodal reasoning and multimodal content comprehension is crucial for evaluating the models correctly
  - Quick check question: What is the key difference between multimodal reasoning tasks and multimodal content comprehension tasks according to the paper?

- Concept: Instruction tuning
  - Why needed here: The paper shows that instruction tuning significantly impacts model stability and performance across different tasks
  - Quick check question: How does instruction tuning affect the stability of MLLMs across various tasks?

- Concept: Encoder-Decoder vs Decoder-only architectures
  - Why needed here: The paper attributes performance differences to architectural choices, specifically comparing these two architectures
  - Quick check question: What architectural difference between Flan-T5-XXL and LLaMA series is cited as the reason for performance differences?

## Architecture Onboarding

- Component map: Language Model Backbone (LLM) -> Visual Model Backbone (PVM) -> Multimodal Fusion -> Language Model -> Output Decoder
- Critical path: Input ‚Üí Visual Model ‚Üí Multimodal Fusion ‚Üí Language Model ‚Üí Output
- Design tradeoffs:
  - Encoder-Decoder vs Decoder-only: Better multimodal fusion vs simpler architecture
  - Instruction format complexity vs model performance
  - Model size vs inference speed and resource requirements
- Failure signatures:
  - Poor performance on MNRE task suggests issues with handling long-tail distributions
  - Large variance in performance across instructions indicates sensitivity to prompt engineering
  - Lower performance on video datasets suggests challenges with temporal understanding
- First 3 experiments:
  1. Test Flan-T5-XXL vs LLaMA-13B on MSA task with identical instructions to verify architectural advantage
  2. Compare QA format vs non-QA format instructions on the same model and task to validate instruction format impact
  3. Test instruction-tuned vs non-instruction-tuned versions of the same model on stability metrics across multiple tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MLLMs vary across different datasets within the same task (e.g., MVSA-Single vs. MVSA-Multiple for MSA)?
- Basis in paper: [explicit] The paper evaluates models on 14 datasets spanning 6 tasks and presents the best performance (ùê¥ Àúùëñ) for each model on each dataset.
- Why unresolved: While the paper provides the best performance for each model on each dataset, it does not delve into a detailed comparison of model performance across different datasets within the same task.
- What evidence would resolve it: A detailed analysis comparing the performance of MLLMs across different datasets within the same task, including factors that may contribute to performance differences.

### Open Question 2
- Question: What are the key factors that influence the stability of MLLMs across different instructions and tasks?
- Basis in paper: [explicit] The paper introduces the Stability metric to assess the stability of models and instructions. It also mentions that models with Flan-T5-XXL as the backbone, based on the Encoder-Decoder architecture, outperform the LLaMA series models with the Decoder-only architecture.
- Why unresolved: While the paper provides insights into model stability, it does not explore the underlying factors that contribute to stability differences across different instructions and tasks.
- What evidence would resolve it: An in-depth analysis of the factors that influence model stability, including model architecture, training data, and task complexity.

### Open Question 3
- Question: How does the performance of MLLMs on multimodal content comprehension tasks compare to their performance on unimodal tasks?
- Basis in paper: [inferred] The paper focuses on evaluating MLLMs on multimodal content comprehension tasks, but it does not directly compare their performance on unimodal tasks.
- Why unresolved: The paper does not provide a direct comparison of MLLM performance on multimodal and unimodal tasks, leaving the question of how well MLLMs perform on unimodal tasks unanswered.
- What evidence would resolve it: A comparative analysis of MLLM performance on multimodal and unimodal tasks, using the same evaluation metrics and datasets.

## Limitations

- The architectural comparison between Encoder-Decoder and Decoder-only models is based on observed performance differences rather than controlled ablation studies isolating architectural factors
- The instruction format findings lack ablation studies to definitively prove that specific formatting elements drive performance improvements rather than other factors like instruction clarity or length
- The poor performance on video datasets may stem from temporal comprehension limitations or suboptimal frame selection rather than fundamental architectural issues

## Confidence

- **High confidence**: The core finding that instruction tuning improves stability across tasks is well-supported by empirical evidence and shows consistent patterns across multiple datasets and models
- **Medium confidence**: The architectural advantage of Flan-T5-XXL over LLaMA series is observed but could be influenced by factors beyond architecture, such as model size, training data, or instruction tuning effects
- **Medium confidence**: The instruction format preferences are consistent but lack mechanistic explanations or controlled experiments isolating specific formatting elements

## Next Checks

1. **Controlled architectural ablation**: Replicate the performance comparison between Flan-T5-XXL and LLaMA-13B using models of identical size (e.g., Flan-T5-base vs LLaMA-7B) and identical instruction tuning procedures to isolate architectural effects from scale and training differences

2. **Instruction format ablation study**: Systematically test instruction formats where only specific elements vary (e.g., keeping all other aspects constant but changing whether the word "Question" appears) to identify which formatting elements drive the performance improvements rather than assuming the entire QA format is responsible

3. **Temporal video understanding test**: Conduct focused experiments on the video datasets (MOSI, MOSEI) with frame selection strategies beyond random sampling, such as key-frame extraction based on motion or content changes, to determine if the poor performance stems from temporal comprehension limitations or simply suboptimal frame selection