---
ver: rpa2
title: 'MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning'
arxiv_id: '2311.10537'
source_url: https://arxiv.org/abs/2311.10537
tags:
- medical
- domain
- arxiv
- question
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedAgents introduces a multi-disciplinary collaboration framework
  to enhance large language models (LLMs) in medical reasoning. The framework simulates
  domain experts who analyze medical questions and options, summarize insights, discuss
  iteratively, and reach consensus for final decisions.
---

# MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning

## Quick Facts
- arXiv ID: 2311.10537
- Source URL: https://arxiv.org/abs/2311.10537
- Authors: Multiple authors
- Reference count: 14
- Key outcome: MedAgents achieves comparable performance to few-shot methods and significantly outperforms zero-shot baselines on nine medical QA datasets

## Executive Summary
MedAgents introduces a multi-disciplinary collaboration framework that enhances large language models (LLMs) for medical reasoning by simulating domain experts who analyze questions, options, and reach consensus through iterative discussion. The framework addresses challenges in medical LLMs such as domain-specific terminology and specialized knowledge requirements. Experiments on nine datasets show significant improvements over zero-shot baselines like chain-of-thought and self-consistency prompting, achieving performance comparable to few-shot methods. Human evaluation reveals that most errors stem from lack of domain knowledge, mis-retrieval of knowledge, and consistency issues, highlighting both the effectiveness and limitations of multi-agent collaboration in leveraging medical expertise within LLMs.

## Method Summary
MedAgents is a training-free framework that simulates multi-disciplinary expert collaboration for medical reasoning. The method consists of five steps: expert gathering (LLMs categorize questions into medical subfields), analysis proposition (experts analyze questions and options from their specialized perspectives), report summarization (synthesized analysis), collaborative discussion (iterative voting and modification until consensus), and decision making. The framework uses 5-6 domain experts for question analysis and 2 experts for option analysis, depending on the dataset. Temperature and top_p are set to 1.0, and experiments use GPT-3.5-Turbo and GPT-4 from Azure OpenAI Service.

## Key Results
- Significantly outperforms zero-shot baselines (chain-of-thought, self-consistency) on MedQA, MedMCQA, PubMedQA, and MMLU medical subtasks
- Achieves comparable performance to few-shot methods while requiring no training data
- Most errors identified through human evaluation stem from lack of domain knowledge (77%), mis-retrieval of knowledge, and consistency issues
- Optimal performance achieved with 4-5 question domain experts depending on dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-disciplinary collaboration leverages distributed domain expertise to improve reasoning accuracy
- Mechanism: The framework simulates domain experts who analyze the medical question from their specialized perspectives, then collaboratively discuss and reach consensus. This multi-round interaction helps surface implicit medical knowledge and improves reasoning quality
- Core assumption: LLMs contain distributed medical expertise that can be activated through role-playing and structured discussion
- Evidence anchors:
  - [abstract] "MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion"
  - [section 3.4] "the objective of the Collaborative Consultation stage is to engage distinct domain experts in multiple rounds of discussions and ultimately render a summary report that is recognized by all experts"
  - [corpus] Weak - related papers discuss similar collaborative frameworks but don't directly test the distributed expertise hypothesis
- Break condition: If LLMs lack sufficient medical knowledge in their training corpus, role-playing cannot compensate for this absence

### Mechanism 2
- Claim: Structured analysis phases (question → option → report) improve reasoning coherence
- Mechanism: By separating analysis into question-focused and option-focused phases, the framework ensures comprehensive evaluation of both the clinical scenario and each answer choice before synthesis
- Core assumption: LLMs benefit from structured decomposition of complex reasoning tasks
- Evidence anchors:
  - [section 3.2] "we manage to attain a set of question analyses QA = {qa1, qa2, . . . , qam}" and "we acquire a series of option analyses OA = {oa1, oa2, . . . , oan}"
  - [section 3.3] "compose a summarized report on the basis of a previous series of analyses"
  - [corpus] Moderate - similar decomposition approaches appear in related agent frameworks
- Break condition: If the intermediate analysis steps introduce errors or inconsistencies that compound in the final synthesis

### Mechanism 3
- Claim: Iterative refinement through expert voting and modification improves answer quality
- Mechanism: Each expert provides feedback on the synthesized report through voting (yes/no) and modification suggestions, with the report revised iteratively until consensus is reached
- Core assumption: Multiple rounds of expert critique and revision lead to better outcomes than single-pass analysis
- Evidence anchors:
  - [section 3.4] "During each round of discussions, the experts give their personal votes (yes/no) as well as modification opinions if they vote no for the current report"
  - [section 3.4] "the discussions are held iteratively until all experts vote yes for the final report"
  - [corpus] Moderate - collaborative refinement appears in related work but specific voting mechanisms are novel
- Break condition: If expert disagreements cannot be resolved or if modification suggestions introduce new errors

## Foundational Learning

- Concept: Role-based prompting in LLMs
  - Why needed here: The framework depends on LLMs acting as domain experts with specific medical knowledge
  - Quick check question: How does the framework specify which domain an LLM should simulate when analyzing a question?

- Concept: Chain-of-thought reasoning
  - Why needed here: The framework builds on CoT principles but extends them through multi-agent collaboration
  - Quick check question: What distinguishes this framework's reasoning approach from standard CoT prompting?

- Concept: Consensus mechanisms in multi-agent systems
  - Why needed here: The framework requires agents to reach agreement through iterative discussion and voting
  - Quick check question: How does the framework handle situations where experts cannot reach consensus?

## Architecture Onboarding

- Component map: Expert gathering → Question analysis → Option analysis → Report summarization → Collaborative discussion → Decision making
- Critical path: The collaborative discussion phase is critical - errors here propagate to the final decision
- Design tradeoffs: More agents provide broader expertise but increase complexity and potential for disagreement
- Failure signatures: Consensus failure, inconsistent reasoning across agents, domain knowledge gaps
- First 3 experiments:
  1. Test expert gathering accuracy with varied question types
  2. Measure impact of agent count on performance
  3. Evaluate consensus quality through agreement rates and error analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MedAgents vary with different numbers of question domain experts (m) and option domain experts (n)?
- Basis in paper: [explicit] The paper states that "The numbers of domain experts for the question and options are set as: m = 5, n = 2 except for MedMCQA (m = 4, n = 2)" and mentions an experiment varying the number of question agents from 1 to 8, finding the optimal number to be 5 for MedQA and MedMCQA and 4 for PubMedQA.
- Why unresolved: The paper only provides the optimal numbers for specific datasets and does not explore the full range of possible values for m and n or their impact on other datasets.
- What evidence would resolve it: Conducting experiments with different values of m and n on all datasets and analyzing the impact on performance would provide a more comprehensive understanding of the optimal configuration.

### Open Question 2
- Question: What are the specific characteristics of the "lack of domain knowledge" errors identified in the human evaluation?
- Basis in paper: [explicit] The paper mentions that "the majority (77%) of the error examples are due to confusion about the domain knowledge" in the human evaluation of error cases.
- Why unresolved: The paper does not provide detailed information on the nature of these errors or examples of specific knowledge gaps that led to incorrect answers.
- What evidence would resolve it: Providing detailed examples of questions where the model lacked domain knowledge, along with the correct knowledge required to answer those questions, would help identify the specific areas where the model needs improvement.

### Open Question 3
- Question: How does the performance of MedAgents compare to fine-tuned models specifically trained on medical data?
- Basis in paper: [inferred] The paper focuses on zero-shot and few-shot settings, comparing MedAgents to other prompting methods, but does not compare it to models that have been fine-tuned on medical data.
- Why unresolved: Fine-tuned models are a common approach in the medical domain, and comparing MedAgents to them would provide a more complete picture of its effectiveness.
- What evidence would resolve it: Conducting experiments comparing MedAgents to state-of-the-art fine-tuned models on the same datasets would allow for a direct comparison of their performance and help determine the relative strengths and weaknesses of each approach.

## Limitations

- Framework performance depends heavily on LLM's pre-existing medical knowledge, with 77% of errors attributed to domain knowledge gaps
- Limited evaluation across different LLM architectures, tested only with GPT-3.5-Turbo and GPT-4
- Performance varies significantly across datasets, with notably lower accuracy on PubMedQA compared to other medical QA tasks

## Confidence

- Multi-disciplinary collaboration improves zero-shot medical reasoning accuracy: Medium
- Structured analysis phases improve reasoning coherence: Medium-High
- Iterative refinement through expert voting improves answer quality: Medium

## Next Checks

1. **Error Type Analysis**: Conduct detailed error categorization to determine whether failures stem from domain knowledge gaps, reasoning flaws, or framework-specific issues like expert disagreement resolution.

2. **Cross-LLM Validation**: Test the framework with diverse LLM architectures (open-source models, different proprietary models) to assess whether improvements are model-dependent or framework-generalizable.

3. **Knowledge Boundary Testing**: Systematically evaluate the framework's performance across different medical subspecialties to identify where distributed expertise assumptions hold versus where they break down.