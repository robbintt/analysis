---
ver: rpa2
title: 'VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers'
arxiv_id: '2305.13417'
source_url: https://arxiv.org/abs/2305.13417
tags:
- attention
- each
- output
- block
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISIT, a tool for visualizing the information
  flow in transformer-based language models. The tool uses the logit lens method to
  project hidden states and neurons into the vocabulary space, making them interpretable
  as words.
---

# VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers

## Quick Facts
- arXiv ID: 2305.13417
- Source URL: https://arxiv.org/abs/2305.13417
- Reference count: 34
- One-line primary result: Introduces VISIT, a visualization tool for analyzing transformer information flow using logit lens projections

## Executive Summary
This paper presents VISIT, a novel tool for visualizing and interpreting the semantic information flow within transformer-based language models. By leveraging the logit lens method, VISIT projects hidden states and neurons into the vocabulary space, making them interpretable as words. This approach enables researchers to identify patterns in the attention mechanism and understand the role of different model components in the final prediction. The tool was demonstrated through three case studies analyzing GPT-2, revealing insights about indirect object identification, layer normalization as semantic filters, and the discovery of regularization neurons.

## Method Summary
The VISIT tool implements the logit lens method to project hidden states and neurons into vocabulary space using the tied embedding-decoding matrix (D = E⊤). It creates flow-graph visualizations by calculating intersection scores between projected tokens and pruning connections to retain only the top 2% most significant. The tool analyzes attention heads by ranking them with the OV circuit and examining their semantic contributions through norm-based dominance. Layer normalization effects are studied by comparing token probability distributions before and after LN transformations.

## Key Results
- Demonstrated indirect object identification in GPT-2 by tracking semantic information flow through attention heads
- Identified layer normalization as a semantic filter that suppresses function words while promoting content words
- Discovered regularization neurons that consistently decrease the probability of unlikely next tokens
- Showed that attention blocks function as selective association gates, with dominant heads controlling semantic content flow

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The logit lens method projects hidden states into vocabulary space, making them interpretable as words
- Mechanism: By using the tied embedding-decoding matrix (D = E⊤), any hidden state vector can be projected to a probability distribution over the vocabulary through a layer norm and softmax
- Core assumption: The tied embedding-decoding matrix preserves semantic meaning when projecting hidden states back to vocabulary space
- Evidence anchors:
  - [abstract] "Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them human interpretable"
  - [section] "The Logit Lens (LL): nostalgebraist (2020) observed that, since the decoding matrix in GPTs is tied to the embedding matrix, D = E⊤, we can examine HS from the model throughout its computation"
  - [corpus] Weak - corpus doesn't directly address logit lens validity
- Break condition: If the embedding-decoding matrix isn't tied or if the semantic alignment breaks down for deeper layers

### Mechanism 2
- Claim: Attention heads function as selective association gates that promote specific semantic content
- Mechanism: Attention heads with larger norms dominate the final attention output, filtering which semantic information flows through to the residual stream
- Core assumption: The norm of attention heads correlates with their semantic influence on the final prediction
- Evidence anchors:
  - [section] "we separately ranked each attention block's heads with the OV circuit... only the few heads with the largest norm have a common vocabulary with their attention block output"
  - [section] "This suggests that the attention block operates as a selective association gate: by making some of the heads much more dominant than others, this gate chooses which heads' semantics to promote into the residual"
  - [corpus] Weak - corpus doesn't directly address attention head dominance
- Break condition: If attention heads with smaller norms show similar semantic influence, or if norm correlation breaks down

### Mechanism 3
- Claim: Layer normalization acts as semantic filters that influence token probabilities
- Mechanism: LN transforms token probability distributions by suppressing function words and promoting content words, effectively filtering semantic content
- Core assumption: The learned weights in LN layers have semantic rather than purely numerical purposes
- Evidence anchors:
  - [section] "tokens whose probability decreases the most are function words like 'the', 'a' or 'not'... Conversely, tokens that gain most probability from LN are content words like 'Microsoft' or 'subsidiaries'"
  - [section] "These results suggest that the model uses LN to introduce new tokens into the top tokens that it compares at each block"
  - [corpus] Weak - corpus doesn't directly address LN semantic filtering
- Break condition: If LN consistently preserves or amplifies function words, or if the effect is random rather than systematic

## Foundational Learning

- Concept: Attention mechanism with query-key-value structure
  - Why needed here: Understanding how attention heads retrieve and combine information from previous tokens is crucial for interpreting the information flow
  - Quick check question: What is the difference between a query, key, and value in the attention mechanism, and how do they interact?

- Concept: Residual connections and layer stacking
  - Why needed here: The residual stream carries information through multiple layers, and understanding how each component modifies it is key to interpreting model behavior
  - Quick check question: How does the residual connection work in transformer blocks, and why is it important for information flow?

- Concept: Matrix multiplication as neuron activation
  - Why needed here: Interpreting static weights (neurons) requires understanding how matrix multiplication translates to semantic content through projection
  - Quick check question: How does projecting a neuron's weight vector through the logit lens reveal its semantic meaning?

## Architecture Onboarding

- Component map: Input token embeddings → stacked transformer blocks (attention → MLP) → final hidden state → tied embedding-decoding matrix → softmax → output distribution
- Critical path: The information flows from input embeddings through each transformer block's attention and MLP sub-blocks, with residual connections preserving information across layers
- Design tradeoffs: The model prioritizes interpretability through the logit lens approach over potentially more accurate but less interpretable projection methods. The pruning strategy simplifies visualization but may miss less prominent semantic contributions.
- Failure signatures: If the logit lens projections show no semantic alignment, if attention head norms don't correlate with semantic influence, or if LN doesn't systematically filter token types, the interpretability claims may not hold.
- First 3 experiments:
  1. Run VISIT on a simple GPT-2 completion task and verify that projected hidden states show meaningful word relationships
  2. Test the attention head dominance hypothesis by modifying head norms and observing changes in output
  3. Examine LN effects by comparing token distributions before and after layer normalization across different layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different languages affect the interpretation of hidden states and neurons using the logit lens method?
- Basis in paper: [inferred] The paper mentions limitations to English LMs and the assumption that insights might not generalize to other languages.
- Why unresolved: The study only analyzed English language models, and the authors acknowledge that their findings might not apply to other languages.
- What evidence would resolve it: Conducting similar analyses on language models trained on different languages and comparing the results to the English language models would provide insights into the generalizability of the findings.

### Open Question 2
- Question: What is the role of the attention mechanism in LMs when predicting function words versus subject tokens?
- Basis in paper: [inferred] The paper mentions that the model's attention mechanism works similarly for correctly recalling factual knowledge and when predicting tokens of function words, but the probability scores given to the final prediction differ.
- Why unresolved: The study does not delve into the specific differences in the attention mechanism's behavior when predicting function words versus subject tokens.
- What evidence would resolve it: Analyzing the attention mechanism's behavior in more detail when predicting function words and subject tokens, and comparing the results, would provide insights into the differences in the attention mechanism's role.

### Open Question 3
- Question: How do the semantics of WQ and WK outputs differ from the rest of the model, and why do they not align with the rest of the model's outputs?
- Basis in paper: [explicit] The paper mentions that the output of WQ and WK does not align with the rest of the model's outputs when projected using the logit lens or the QK circuit.
- Why unresolved: The study does not provide a clear explanation for the lack of alignment between WQ, WK outputs and the rest of the model's outputs.
- What evidence would resolve it: Investigating the embedding space in which WQ and WK operate and comparing it to the rest of the model's embedding space would provide insights into the differences in semantics and the reasons for the lack of alignment.

## Limitations
- The pruning strategy retaining only top 2% of connections may oversimplify model behavior
- Limited validation across different model architectures and layer depths
- Findings may not generalize to languages other than English

## Confidence

**High Confidence:**
- The technical feasibility of implementing the logit lens projection method
- The general utility of visualization tools for model interpretability

**Medium Confidence:**
- The semantic interpretability of logit lens projections
- The dominance of attention heads with larger norms
- The semantic filtering role of layer normalization

**Low Confidence:**
- The universality of identified regularization neurons
- The scalability of the visualization approach to larger models

## Next Checks

1. **Cross-model validation**: Apply VISIT to multiple transformer architectures (BERT, RoBERTa, different GPT versions) to test whether logit lens projections consistently yield semantically interpretable results across varying model depths and architectures.

2. **Statistical validation of attention dominance**: Systematically manipulate attention head norms through controlled experiments (e.g., weight scaling) and measure the correlation between norm magnitude and semantic influence on output predictions, including significance testing.

3. **LN semantic filtering quantification**: Conduct ablation studies on layer normalization layers, measuring the statistical significance of function word suppression and content word promotion across multiple layers and diverse input types.