---
ver: rpa2
title: Pseudo-Bayesian Optimization
arxiv_id: '2310.09766'
source_url: https://arxiv.org/abs/2310.09766
tags:
- optimization
- function
- where
- have
- property
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pseudo-Bayesian Optimization (PseudoBO),
  an axiomatic framework that establishes minimal requirements for black-box optimization
  convergence beyond Gaussian process (GP)-based methods. The framework decomposes
  algorithms into surrogate predictor (SP), uncertainty quantifier (UQ), and acquisition
  function (AF), requiring local consistency, generalized no-empty-ball (GNEB) property,
  and improvement property respectively.
---

# Pseudo-Bayesian Optimization

## Quick Facts
- arXiv ID: 2310.09766
- Source URL: https://arxiv.org/abs/2310.09766
- Reference count: 40
- Key outcome: Introduces PseudoBO framework achieving sublinear cumulative regret while being 4-60× faster than state-of-the-art methods

## Executive Summary
This paper introduces Pseudo-Bayesian Optimization (PseudoBO), an axiomatic framework that establishes minimal requirements for black-box optimization convergence beyond Gaussian process-based methods. The framework decomposes algorithms into surrogate predictor (SP), uncertainty quantifier (UQ), and acquisition function (AF), requiring local consistency, generalized no-empty-ball (GNEB) property, and improvement property respectively. Leveraging this flexibility, the authors construct simple algorithms using local regression and randomized prior for uncertainty quantification that consistently outperform state-of-the-art methods across synthetic functions, hyperparameter tuning, and robotic tasks.

## Method Summary
PseudoBO decomposes Bayesian Optimization into three components: surrogate predictor (SP) for function approximation, uncertainty quantifier (UQ) for exploration guidance, and acquisition function (AF) for decision making. The framework establishes three axiomatic properties - local consistency for SP, GNEB property for UQ, and improvement property for AF - that guarantee convergence. The authors implement three variants: PseudoBO-RP using randomized prior for uncertainty, PseudoBO-KR-Hyb combining kernel regression with hybrid uncertainty quantification, and PseudoBO-KR-Hyb-TR with trust region adaptation. These are tested against baselines on synthetic benchmarks, hyperparameter tuning, and robotic tasks.

## Key Results
- PseudoBO-KR-Hyb achieves sublinear cumulative regret on benchmark functions
- 4-60× faster runtime than competitors while attaining better or comparable objective values
- Superior performance across synthetic functions, neural network hyperparameter tuning, and robotic tasks
- Simple approaches match or exceed sophisticated GP-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generalized no-empty-ball (GNEB) property ensures exploration by requiring positive uncertainty for unexplored regions
- Mechanism: GNEB stipulates that uncertainty quantifiers (UQ) must remain positive when there are no nearby evaluated points, preventing premature convergence to local optima
- Core assumption: The objective function f is continuous and the search space X is compact
- Evidence anchors:
  - [abstract]: "require local consistency, generalized no-empty-ball (GNEB) property, and improvement property respectively"
  - [section 3.1]: "Roughly speaking, the first part of Assumption 3.1 stipulates that as along as there is no infinitesimally close evaluated point in the neighborhood of x, then the EW of x is positive"
  - [corpus]: Weak evidence - no direct citations found
- Break condition: If the UQ becomes zero or negative in unexplored regions, exploration fails and the algorithm may converge prematurely

### Mechanism 2
- Claim: Local consistency of surrogate predictors (SP) enables accurate function approximation in explored regions
- Mechanism: Local consistency ensures that SP predictions converge to true function values as evaluation points approach any target point, providing reliable exploitation guidance
- Core assumption: The objective function f is continuous and SP can fit the data points with increasing precision
- Evidence anchors:
  - [abstract]: "We derive the axiomatic properties of SP, UQ and AF to attain theoretical convergence"
  - [section 3.2]: "SP aims to provide point predictions at different design points" and "This assumption stipulates that the true function value at a target point can be approximated with increasing precision by ˆf constructed at evaluation points converging to this target"
  - [corpus]: Weak evidence - no direct citations found
- Break condition: If SP predictions diverge from true function values in regions with sufficient data, exploitation guidance becomes unreliable

### Mechanism 3
- Claim: The improvement property of acquisition functions (AF) balances exploration and exploitation
- Mechanism: AF converts SP and UQ into evaluation worthiness, ensuring zero worthiness for points with certain non-improvement while maintaining positive worthiness for uncertain points
- Core assumption: The acquisition function g_n satisfies the improvement property conditions
- Evidence anchors:
  - [abstract]: "AF transforms SP and UQ into the decision on which design point to evaluate next"
  - [section 3.2]: "The assumption states that if there is, with eventual certainty, no improvement, then the worthiness to evaluate is zero. In contrast, as long as there is uncertainty, then there is some worthiness to evaluate more points"
  - [corpus]: Weak evidence - no direct citations found
- Break condition: If AF assigns positive worthiness to points with certain non-improvement or zero worthiness to uncertain points, the exploration-exploitation balance fails

## Foundational Learning

- Concept: Bayesian Optimization fundamentals
  - Why needed here: Understanding BO principles is essential for grasping why PseudoBO decomposes the problem into SP, UQ, and AF components
  - Quick check question: What are the three main components of Bayesian Optimization and how do they interact?

- Concept: Uncertainty quantification methods
  - Why needed here: PseudoBO relies on GNEB property for UQ, requiring understanding of how different methods (GP posterior variance, minimum distance, randomized prior) quantify uncertainty
  - Quick check question: How does GP posterior variance differ from minimum distance in terms of uncertainty quantification?

- Concept: Convergence analysis in optimization
  - Why needed here: The theoretical guarantees in PseudoBO require understanding of consistency concepts and how to prove algorithmic convergence
  - Quick check question: What is the difference between algorithmic consistency and convergence rate analysis?

## Architecture Onboarding

- Component map: Surrogate Predictor (SP) → Uncertainty Quantifier (UQ) → Acquisition Function (AF) → Evaluation Worthiness → Point Selection → Function Evaluation → Data Update → Repeat

- Critical path: SP → UQ → AF → Evaluation Worthiness → Point Selection → Function Evaluation → Data Update → Repeat

- Design tradeoffs:
  - SP complexity vs accuracy: Simple local regression is faster but less accurate than GP
  - UQ calibration vs exploration: Well-calibrated UQ improves exploration but requires validation data
  - AF exploration-exploitation balance: Different AFs prioritize exploration vs exploitation differently

- Failure signatures:
  - SP failure: Poor predictions in explored regions, indicated by high regret
  - UQ failure: Uncertainty collapses prematurely, indicated by zero uncertainty in unexplored regions
  - AF failure: Poor point selection, indicated by slow convergence or getting stuck in local optima

- First 3 experiments:
  1. Implement PseudoBO with local regression SP and minimum distance UQ on a simple 1D function
  2. Add randomized prior UQ and compare uncertainty calibration using coverage rate metric
  3. Test different AFs (EI vs PI) on a 2D synthetic benchmark and measure convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between exploitation and exploration in PseudoBO algorithms when using different acquisition functions like EI, PI, and UCB?
- Basis in paper: [explicit] The paper discusses various acquisition functions (EI, PI, UCB) and their improvement properties, but does not provide empirical comparisons of their performance across different tasks.
- Why unresolved: The paper mentions that different acquisition functions satisfy the improvement property, but does not systematically evaluate their relative performance or identify conditions under which one might outperform others.
- What evidence would resolve it: Empirical studies comparing the performance of EI, PI, and UCB across various benchmark functions and real-world tasks, measuring both final objective values and cumulative regret.

### Open Question 2
- Question: How does the choice of kernel bandwidth in kernel regression affect the performance of PseudoBO algorithms in high-dimensional spaces?
- Basis in paper: [explicit] The paper mentions using kernel regression with specific bandwidth choices (h(l)₀, h(u)₀, h'₀) but does not provide a systematic study of how different bandwidth choices affect performance.
- Why unresolved: While the paper provides specific bandwidth values used in experiments, it does not explore the sensitivity of PseudoBO performance to these choices or provide guidance on selecting optimal bandwidths.
- What evidence would resolve it: Sensitivity analysis showing PseudoBO performance across different bandwidth choices, potentially with theoretical bounds on optimal bandwidth selection for different dimensionalities.

### Open Question 3
- Question: Can PseudoBO algorithms be extended to handle non-stationary objective functions effectively?
- Basis in paper: [inferred] The paper focuses on stationary objectives and local consistency assumptions, but real-world functions often exhibit non-stationarity.
- Why unresolved: The current framework assumes local consistency and continuity, which may not hold for non-stationary functions. The paper does not address how to modify the framework for such cases.
- What evidence would resolve it: Experimental results showing PseudoBO performance on benchmark functions with known non-stationarity, and/or theoretical analysis extending the framework to handle changing function properties over the search space.

## Limitations
- GNEB property requires stronger assumptions than traditional BO methods, potentially limiting applicability to non-continuous or noisy functions
- Limited empirical validation beyond presented benchmarks, particularly for noisy objective functions
- Weak evidence supporting local consistency mechanism despite its theoretical importance

## Confidence

- Mechanism 1 (GNEB property): Medium - supported by axiomatic derivation but lacks empirical stress-testing
- Mechanism 2 (Local consistency): Low - minimal evidence despite theoretical importance
- Mechanism 3 (Improvement property): Medium - well-defined mathematically but practical implementation details are sparse

## Next Checks
1. Test PseudoBO on noisy objective functions to evaluate GNEB property robustness beyond the assumed continuity
2. Conduct ablation studies comparing local regression SP performance against GP-based SP on high-dimensional problems to validate the claimed 4-60× speedup
3. Evaluate coverage rate and width metrics for different UQ methods across diverse function classes to verify uncertainty calibration claims