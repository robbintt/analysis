---
ver: rpa2
title: 'ASTER: Automatic Speech Recognition System Accessibility Testing for Stutterers'
arxiv_id: '2308.15742'
source_url: https://arxiv.org/abs/2308.15742
tags:
- stuttering
- audio
- systems
- speech
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASTER, the first automatic testing technique
  for evaluating the accessibility of ASR systems to stutterers. ASTER generates stuttering
  audio samples by injecting five types of stuttering into benign audio files and
  uses multi-objective optimization to enhance the quality of the generated test cases.
---

# ASTER: Automatic Speech Recognition System Accessibility Testing for Stutterers

## Quick Facts
- arXiv ID: 2308.15742
- Source URL: https://arxiv.org/abs/2308.15742
- Reference count: 40
- Key outcome: Increases ASR error rates by 23.12% (WER), 21.45% (MER), and 33.34% (WIL) for stutterers

## Executive Summary
This paper introduces ASTER, the first automatic testing technique for evaluating ASR system accessibility to stutterers. ASTER generates realistic stuttering audio by injecting five types of stuttering (block, prolongation, sound repetition, word repetition, and interjection) at syllable boundaries. The system uses multi-objective optimization to balance between exposing ASR failures and maintaining audio realism. Evaluation on four open-source and three commercial ASR systems demonstrates significant error rate increases, while a user study confirms the generated stuttering audio is indistinguishable from real stuttering.

## Method Summary
ASTER operates through three main components: Phonetic Alignment extracts word and syllable timing from benign audio using PocketSphinx; Speech Mutation injects stuttering using five mutators (block adds silence, prolongation extends syllables, sound repetition duplicates syllables, word repetition duplicates words, and interjection inserts filler words); and Multi-Objective Optimization-based seed pool update uses Pareto frontier to select test cases that maximize ASR output divergence while maintaining semantic similarity to original audio. The metamorphic relation oracle compares ASR outputs for original and mutated audio to identify failures.

## Key Results
- ASTER increases word error rate (WER) by 23.12%, match error rate (MER) by 21.45%, and word information loss (WIL) by 33.34%
- Generated stuttering audio is indistinguishable from real stuttering audio in user study
- Identified and categorized 1,069 recognition errors into five bug types
- Demonstrated significant performance differences between commercial and open-source ASR systems on stuttering speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting stuttering at the syllable level increases ASR error rates by forcing recognition of non-fluent patterns.
- Mechanism: Phonetic alignment identifies word and syllable boundaries; mutators inject block, prolongation, sound repetition, word repetition, or interjection at precise syllable timing.
- Core assumption: Stuttering manifests at the syllable level and ASR systems are sensitive to temporal disruptions within syllables.
- Evidence anchors:
  - [abstract] "ASTER can generate valid test cases by injecting five different types of stuttering... can both simulate realistic stuttering speech and expose failures in ASR systems."
  - [section] "First, ASTER determines the word timing in the audio file... Then, based on the word timing, ASTER determines the syllable timing for each word."
  - [corpus] Weak: no direct neighbor evidence; this is original contribution.
- Break condition: If ASR models are trained to normalize syllable-level jitter or if phonetic alignment is inaccurate.

### Mechanism 2
- Claim: Multi-objective optimization (MOO) balances failure exposure with realism by favoring test cases that differ across ASR outputs but remain semantically close to the original.
- Mechanism: MOO selects seeds that minimize cosine similarity between ASR outputs (M1) and maximize similarity to the original audio (M2) using a Pareto frontier.
- Core assumption: Divergent ASR outputs indicate genuine recognition errors rather than malformed audio; maintaining semantic similarity ensures realism.
- Evidence anchors:
  - [abstract] "ASTER can further enhance the quality of the test cases with a multi-objective optimization-based seed updating algorithm."
  - [section] "ASTER uses a multi-objective optimization (MOO) algorithm to balance between two properties: the difference between the results of the ASR systems and the similarity to the benign audio."
  - [corpus] Weak: no neighbor papers describe MOO for ASR test case selection; original design.
- Break condition: If ASR systems converge on the same erroneous output for a stuttering case.

### Mechanism 3
- Claim: Metamorphic relation oracle detects failures by comparing ASR output for original vs. mutated audio, flagging large semantic divergence.
- Mechanism: Cosine similarity between BERT embeddings of ground-truth text and ASR output below threshold θ indicates failure.
- Core assumption: Valid stuttering audio should still map to the same underlying meaning; large divergence implies recognition failure.
- Evidence anchors:
  - [abstract] "ASTER uses the distance to the original speech text plus manual checks as the test oracle to capture the failures of the ASR systems under test."
  - [section] "The metamorphic relation is that the output text of an ASR system should be the same for both the original audio and the mutated audio."
  - [corpus] Weak: no neighbor evidence; this is original metamorphic oracle design.
- Break condition: If stuttering audio naturally distorts meaning beyond threshold, causing false positives.

## Foundational Learning

- Concept: Phonetic alignment and syllable boundary detection
  - Why needed here: Enables precise injection of stuttering at linguistically valid points.
  - Quick check question: How does the system determine where to split words into syllables before applying mutators?

- Concept: Multi-objective optimization and Pareto frontiers
  - Why needed here: Balances two competing goals—realism and error exposure—without arbitrary weighting.
  - Quick check question: What happens to a test case that is highly realistic but produces identical ASR outputs across all systems?

- Concept: Metamorphic testing
  - Why needed here: Provides a test oracle for speech outputs that lack ground-truth for stuttered forms.
  - Quick check question: Why compare mutated and original audio outputs rather than comparing to ground-truth stuttered transcriptions?

## Architecture Onboarding

- Component map:
  Phonetic Alignment -> Word & Syllable Timing Extraction -> Speech Mutation -> Multi-Objective Optimization-based seed pool update -> Metamorphic Oracle -> Test Bed

- Critical path:
  1. Load benign audio -> 2. Extract timing -> 3. Apply random mutator -> 4. Run through all ASR systems -> 5. Compute M1/M2 -> 6. Update Pareto frontier -> 7. Filter by oracle -> 8. Output failures

- Design tradeoffs:
  - Mutation variety vs. realism: More stutter types increase error exposure but may produce less natural speech.
  - MOO selection vs. exploration: Pareto frontier focuses on best candidates but may miss rare failure modes.
  - Manual oracle verification vs. automation: Ensures validity but slows feedback loop.

- Failure signatures:
  - Low M1 + High M2: Test case is realistic but fails to differentiate ASR outputs—likely not challenging enough.
  - High M1 + Low M2: Test case is unrealistic or malformed—discard.
  - Low M1 + Low M2: Likely semantic divergence—flag for manual review.

- First 3 experiments:
  1. Run single mutator (e.g., Block) on one audio file; verify syllable-level silence insertion.
  2. Run all mutators on small seed pool; check Pareto frontier selects diverse, realistic cases.
  3. Apply oracle to generated cases; manually verify true positives vs. false positives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for the cosine similarity between Bert-embeddings of ASR system results and ground truth text to balance between identifying true stuttering faults and avoiding false positives?
- Basis in paper: [explicit] The paper mentions using a threshold of 0.8 but notes that around 31.43% of failures are false positives with this default value, indicating a need for optimization.
- Why unresolved: The paper does not explore alternative threshold values or provide an analysis of how different thresholds affect the precision and recall of stuttering fault detection.
- What evidence would resolve it: Empirical testing of various threshold values on a labeled dataset of stuttering and non-stuttering speech to determine the threshold that maximizes the F1-score or another appropriate metric.

### Open Question 2
- Question: How do the performance differences between commercial and open-source ASR systems on stuttering speech translate to real-world usage scenarios for people who stutter?
- Basis in paper: [explicit] The paper compares commercial and open-source ASR systems and finds significant differences in their performance on stuttering audio, but does not address real-world implications.
- Why unresolved: The study uses synthesized stuttering audio and does not include real-world usage data or feedback from actual users who stutter.
- What evidence would resolve it: Longitudinal studies with participants who stutter using various ASR systems in their daily lives, coupled with user satisfaction surveys and error analysis of real-world interactions.

### Open Question 3
- Question: Which mutator combination and sequence produce the most effective test cases for exposing ASR system vulnerabilities to stuttering?
- Basis in paper: [inferred] The paper applies mutators randomly and individually but does not explore the potential synergies or effectiveness of specific combinations or sequences of mutators.
- Why unresolved: The current approach uses random selection and individual mutator application, which may not fully exploit the potential of the mutators to create challenging test cases.
- What evidence would resolve it: Systematic testing of all possible mutator combinations and sequences, measuring the resulting ASR system errors and comparing the effectiveness of different approaches in exposing vulnerabilities.

### Open Question 4
- Question: How can the ASTER framework be adapted to generate stuttering audio samples for languages other than English?
- Basis in paper: [explicit] The paper focuses on English speech datasets and ASR systems, but the generalizability to other languages is not discussed.
- Why unresolved: The paper does not address the challenges or requirements for adapting the framework to other languages, such as different phonetic structures or the availability of stuttering speech datasets.
- What evidence would resolve it: Implementation of the ASTER framework for at least one non-English language, using appropriate speech datasets and ASR systems, and evaluating its effectiveness in generating realistic stuttering audio samples and detecting ASR system errors.

## Limitations

- The implementation details of the multi-objective optimization algorithm, particularly Pareto frontier calculation, are not fully specified
- User study demonstrating audio indistinguishability is limited to only 15 participants
- Error categorization relies on manual inspection that may introduce subjective bias
- Specific hyperparameters for stuttering injection parameters are not provided

## Confidence

- High confidence: The core mechanism of syllable-level stuttering injection and its impact on ASR error rates
- Medium confidence: The effectiveness of the multi-objective optimization in balancing realism and error exposure
- Low confidence: The user study results on audio indistinguishability due to small sample size

## Next Checks

1. Implement the phonetic alignment component using PocketSphinx and verify syllable boundary detection accuracy on diverse audio samples before proceeding to stuttering injection.
2. Create a controlled experiment comparing ASR error rates with and without stuttering injection to isolate the specific impact of each stutter type.
3. Conduct a larger-scale user study (n > 50) with diverse participants to validate whether generated stuttering audio is truly indistinguishable from real stuttering recordings.