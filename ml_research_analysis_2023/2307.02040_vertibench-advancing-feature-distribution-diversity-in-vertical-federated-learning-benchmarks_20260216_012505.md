---
ver: rpa2
title: 'VertiBench: Advancing Feature Distribution Diversity in Vertical Federated
  Learning Benchmarks'
arxiv_id: '2307.02040'
source_url: https://arxiv.org/abs/2307.02040
tags:
- datasets
- learning
- correlation
- feature
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VertiBench introduces two key factors\u2014feature importance\
  \ and feature correlation\u2014to systematically evaluate VFL algorithms across\
  \ diverse scenarios. It proposes novel feature-splitting methods based on these\
  \ factors, generates synthetic datasets reflecting different importance and correlation\
  \ levels, and introduces a new real-world image-image VFL dataset (Satellite) addressing\
  \ a critical gap."
---

# VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks

## Quick Facts
- arXiv ID: 2307.02040
- Source URL: https://arxiv.org/abs/2307.02040
- Authors: [List of authors from paper]
- Reference count: 40
- Key outcome: Introduces feature importance and correlation as key factors for VFL evaluation, proposes novel splitting methods, and provides comprehensive benchmarking across diverse scenarios

## Executive Summary
This paper addresses a critical gap in vertical federated learning (VFL) evaluation by introducing a systematic framework that captures feature distribution diversity. VertiBench proposes two key factors—feature importance and feature correlation—that significantly influence VFL algorithm performance. The framework provides novel dataset splitting methods based on these factors, generates synthetic datasets with controllable importance and correlation levels, and introduces a real-world image-image VFL dataset. Through comprehensive evaluation of state-of-the-art VFL algorithms, the work demonstrates significant performance variations under different data partition scenarios and highlights the importance of communication efficiency in imbalanced datasets.

## Method Summary
VertiBench introduces a systematic approach to evaluate VFL algorithms across diverse feature distribution scenarios. The method uses Dirichlet distribution-based feature splitting to control feature importance across parties, where larger α values lead to higher expected importance for specific parties. For correlation-based splitting, it employs a novel metric (Pcor) based on singular values of correlation matrices and uses Biased Random-Key Genetic Algorithm (BRKGA) optimization to achieve target correlation levels. The framework generates synthetic datasets reflecting different importance and correlation configurations and integrates a real-world image-image VFL dataset adapted from the WorldStrat dataset. VFL algorithms are then evaluated using standard metrics like accuracy, RMSE, and communication size across these diverse scenarios.

## Key Results
- Empirical results demonstrate significant performance variations across VFL algorithms under different feature importance (α) and correlation (β) configurations
- Communication efficiency emerges as a critical factor, particularly for algorithms like C-VFL in imbalanced datasets
- The benchmark reveals that VFL algorithms show different strengths depending on data distribution, validating the need for diverse evaluation scenarios
- Scalability analysis shows potential for VFL algorithms to handle large-scale problems with appropriate parameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature importance and correlation are the two key factors influencing VFL algorithm performance.
- Mechanism: The paper introduces feature importance through a Dirichlet distribution parameterized by α values, where larger αi values lead to higher expected importance for party Pi. Correlation is measured using a novel metric (Pcor) based on the variance of singular values of the correlation matrix.
- Core assumption: Feature importance and correlation are orthogonal factors that can be independently manipulated to generate diverse VFL scenarios.
- Evidence anchors:
  - [abstract] "introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods"
  - [section 4.1] "we highlight feature importance and correlation as two crucial factors that could potentially influence the performance of VFL algorithms"
  - [corpus] Weak - corpus papers focus on privacy, differential privacy, and specific VFL algorithm improvements rather than feature importance/correlation as fundamental factors
- Break condition: If feature importance and correlation are not actually independent factors, or if other factors (like privacy mechanisms) have greater impact on VFL performance.

### Mechanism 2
- Claim: The Dirichlet distribution-based feature splitting method generates datasets with controlled importance variance across parties.
- Mechanism: By sampling probabilities from Dir(α1,...,αK) and allocating features accordingly, the method ensures that larger αi values result in higher expected importance for party Pi, while smaller ∥{αi}K i=1∥2 values create greater variance in importance across parties.
- Core assumption: The expected cumulative importance of each party is proportional to the ratio generated by the Dirichlet distribution.
- Evidence anchors:
  - [section 4.2] "we propose the implementation of the Dirichlet distribution parameterized by {αi}K i=1 for feature splitting"
  - [section 4.2] "Theorem 1. Consider a feature index set A = {1, 2, ..., m} and a characteristic function v : 2 A → R such that v(∅) = 0"
  - [corpus] Missing - corpus papers don't discuss Dirichlet-based feature importance splitting methods
- Break condition: If the Dirichlet distribution doesn't actually produce the claimed proportional relationships between α values and feature importance distributions.

### Mechanism 3
- Claim: The correlation-based feature splitting algorithm using BRKGA optimization generates datasets with controlled inter-party correlation levels.
- Mechanism: The algorithm uses a permutation-based optimization approach with Biased Random-Key Genetic Algorithm (BRKGA) to find feature allocations that achieve target correlation levels β, where β=0 represents minimum correlation and β=1 represents maximum correlation.
- Core assumption: The Icor metric accurately captures inter-party correlation and can be optimized to achieve specific β values.
- Evidence anchors:
  - [section 4.3] "we propose a novel metric to examine the correlation when the parties involved possess unequal numbers of features"
  - [section 4.3] "Algorithm 1: Feature Splitting by Correlation" describes the BRKGA-based optimization approach
  - [corpus] Weak - corpus papers don't discuss BRKGA-based correlation optimization for feature splitting
- Break condition: If the optimization algorithm fails to find good solutions, or if the Icor metric doesn't accurately represent inter-party correlation in practice.

## Foundational Learning

- Concept: Vertical Federated Learning (VFL) fundamentals
  - Why needed here: Understanding the difference between VFL (feature-partitioned data) and HFL (instance-partitioned data) is crucial for grasping why feature importance and correlation matter differently in VFL.
  - Quick check question: What distinguishes vertical federated learning from horizontal federated learning in terms of data partitioning?

- Concept: Dirichlet distribution properties
  - Why needed here: The Dirichlet distribution is used to control feature importance distribution across parties, so understanding its properties (mean, variance, concentration parameters) is essential for implementing and tuning the importance-based splitting method.
  - Quick check question: How does changing the concentration parameter α in a Dirichlet distribution affect the variance of the resulting probability distribution?

- Concept: Correlation matrix analysis and singular value decomposition
  - Why needed here: The correlation-based splitting method relies on computing singular values of correlation matrices to measure inter-party correlation, requiring understanding of SVD and correlation matrix properties.
  - Quick check question: Why does the variance of singular values of a correlation matrix serve as a good measure of overall correlation between two feature sets?

## Architecture Onboarding

- Component map: Dataset → Feature Importance Analysis → Dirichlet-based Feature Splitting → VFL Algorithm Training → Performance Evaluation → Communication Analysis
- Critical path: Global dataset → feature importance/correlation analysis → dataset splitting (synthetic or real) → VFL algorithm evaluation → performance analysis across α and β dimensions
- Design tradeoffs: Importance-based splitting offers computational efficiency but may not capture complex correlation patterns; correlation-based splitting captures realistic scenarios but is computationally expensive and may not scale well to many parties
- Failure signatures: If VFL algorithms show minimal performance variation across different α and β values, this suggests either the splitting methods aren't creating meaningful differences or the evaluation framework isn't sensitive enough to capture them
- First 3 experiments:
  1. Generate synthetic datasets with varying α values (0.1, 1, 10, 100) using importance-based splitting and verify the distribution of feature importance across parties matches theoretical expectations
  2. Generate synthetic datasets with β values (0, 0.3, 0.6, 1.0) using correlation-based splitting and visualize correlation matrices to confirm inter-party correlation levels
  3. Run VFL algorithms on datasets with extreme parameter values (α=0.1, β=0) and (α=100, β=1) to observe maximum performance differences and validate the framework's sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can correlation-based feature splitting be scaled to work with a large number of parties?
- Basis in paper: [explicit] The paper states that the correlation-based split method may face challenges when applied to a large number of parties due to the exponential increase in potential feature splits.
- Why unresolved: Current optimization methods like BRKGA struggle to find the minimum and maximum Icor and the optimal split for a given β as the number of parties increases.
- What evidence would resolve it: Development and demonstration of advanced permutation-based optimization algorithms that can efficiently handle feature splits for a large number of parties.

### Open Question 2
- Question: Is there a relationship between feature importance and correlation that should be considered in VFL benchmarking?
- Basis in paper: [explicit] The paper treats importance and correlation as orthogonal factors but acknowledges that this might overlook potential correlations between them.
- Why unresolved: The paper does not provide empirical evidence or a theoretical framework to address the potential relationship between feature importance and correlation.
- What evidence would resolve it: Empirical studies or theoretical models demonstrating how feature importance and correlation are related and how they jointly affect VFL performance.

### Open Question 3
- Question: How can the privacy of different VFL algorithms and models be quantitatively evaluated?
- Basis in paper: [explicit] The paper mentions that VertiBench does not provide a quantitative evaluation of privacy and highlights the need for such evaluation.
- Why unresolved: The paper does not propose a method or framework for quantitatively assessing the privacy of VFL algorithms.
- What evidence would resolve it: Development of a quantitative privacy evaluation framework and its application to various VFL algorithms to compare their privacy levels.

## Limitations

- The correlation-based splitting algorithm using BRKGA optimization is computationally expensive and may not scale well to scenarios with many parties or extremely high-dimensional features
- The framework treats feature importance and correlation as independent factors, which may overlook potential relationships between them that could affect VFL performance
- The real-world Satellite dataset, while addressing a critical gap, is adapted from an existing dataset and may not fully represent the complexity of true multi-party VFL scenarios

## Confidence

- Feature importance as a key factor: **High** - supported by both theoretical analysis and empirical validation
- Correlation as a key factor: **Medium** - theoretical framework is sound, but empirical validation is limited to specific scenarios
- Dirichlet distribution effectiveness: **Medium** - theoretical properties are well-established, but real-world applicability varies
- BRKGA optimization scalability: **Low** - computationally expensive and not tested beyond moderate problem sizes

## Next Checks

1. Test the feature importance splitting method on real-world datasets with known feature importance patterns (e.g., medical data with established biomarkers) to validate the Dirichlet-based approach.
2. Evaluate the correlation-based splitting algorithm with 5+ parties to assess scalability and identify computational bottlenecks in the BRKGA optimization.
3. Conduct ablation studies removing either feature importance or correlation factors to quantify their relative contributions to VFL performance variation.