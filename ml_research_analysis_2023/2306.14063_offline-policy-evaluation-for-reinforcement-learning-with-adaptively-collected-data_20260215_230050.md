---
ver: rpa2
title: Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected
  Data
arxiv_id: '2306.14063'
source_url: https://arxiv.org/abs/2306.14063
tags:
- learning
- policy
- data
- adaptive
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of offline policy evaluation (OPE)
  in reinforcement learning when data is collected adaptively by potentially changing
  logging policies, as opposed to the standard i.i.d. trajectory assumption.
---

# Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data

## Quick Facts
- arXiv ID: 2306.14063
- Source URL: https://arxiv.org/abs/2306.14063
- Reference count: 40
- This paper develops theoretical guarantees for the TMIS estimator in adaptive data settings for tabular MDPs, showing that minimax-optimal bounds from the non-adaptive case can be recovered and that adaptive data does not inherently degrade performance.

## Executive Summary
This paper addresses offline policy evaluation (OPE) in reinforcement learning when data is collected adaptively by changing logging policies, rather than under the standard i.i.d. trajectory assumption. The authors develop novel theoretical guarantees for the TMIS estimator in this adaptive setting for tabular MDPs. Their main contributions include deriving high-probability, instance-dependent bounds on estimation error that decompose the error into terms reflecting the target policy's visitation frequency and transition variance, showing that minimax-optimal bounds can be recovered by discarding data, and providing empirical evidence that adaptive data does not inherently degrade performance compared to non-adaptive data when using the same logging policies.

## Method Summary
The paper analyzes the TMIS (Trajectory Monte Carlo Importance Sampling) estimator for offline policy evaluation in the presence of adaptively collected data. The key innovation is a novel "tape model" that allows concentration inequalities to be applied to individual state-action transitions despite the overall dataset being dependent. By introducing this model where each (h,s,a) tuple has a tape containing i.i.d. samples from the transition distribution, the authors can isolate individual transition errors that can be concentrated over. The method derives high-probability, instance-dependent bounds on the estimation error that decompose into terms reflecting the target policy's visitation frequency and the variance of state transitions. The paper also shows that by selectively throwing away data (keeping only the first N transitions out of each (sh, ah)), all transitions become independent and minimax-optimal bounds from the non-adaptive setting can be recovered.

## Key Results
- The TMIS estimator achieves high-probability, instance-dependent bounds on estimation error in adaptive data settings through the novel tape model approach
- Adaptive data does not inherently degrade the performance of the TMIS estimator compared to non-adaptive data when using identical logging policies
- Under reasonable exploration assumptions, minimax-optimal bounds from the non-adaptive setting can be recovered in the adaptive setting by selectively discarding data

## Why This Works (Mechanism)

### Mechanism 1
The TMIS estimator achieves low estimation error in adaptive data settings by leveraging a "tape" model that allows concentration inequalities to be applied to individual state-action transitions. The paper introduces a novel data-collection model where each (h,s,a) tuple has a "tape" containing n i.i.d. samples from the transition distribution. When the logging algorithm visits (h,s,a), it reads from the tape and advances it. This allows the error decomposition to isolate individual transition errors that can be concentrated over, despite the overall dataset being dependent. The core assumption is that for any fixed (h,s,a), the number of visitations nh,s,a is bounded, and the transitions read from the tape are i.i.d. samples from Ph+1(·|s,a).

### Mechanism 2
Adaptive data does not inherently degrade the performance of the TMIS estimator compared to non-adaptive data when using the same logging policies. The paper shows through empirical simulations that the magnitude of estimation error |ˆvπ − vπ| is indistinguishable between adaptive and non-adaptive data when the logging policies are identical. This suggests that the adaptivity itself is not the source of additional error, but rather the exploration quality of the logging policies. The core assumption is that the logging policies used to generate adaptive and non-adaptive data are identical, and the only difference is whether future policies can depend on previous trajectories.

### Mechanism 3
Under reasonable exploration assumptions, minimax-optimal bounds from the non-adaptive setting can be directly ported to the adaptive setting by selectively throwing away data. The paper shows that by considering a revised dataset that keeps only the first N transitions out of each (sh, ah) for all sh, ah (where N = minh,s,a nh,s,a), all transitions become independent conditioned on N. This reduces the problem to a generative-model type setting where existing minimax-optimal results can be applied. The core assumption is that the logging process satisfies an exploration assumption that ensures nh,s,a ≥ n ¯dm for all h,s,a with high probability.

## Foundational Learning

- **Martingale concentration inequalities**
  - Why needed here: The paper uses martingale concentration to bound the error of the TMIS estimator in the non-adaptive setting, and this technique is adapted to handle adaptive data through the "tape" model
  - Quick check question: What is the key property of a martingale that allows concentration inequalities to be applied?

- **Importance sampling in reinforcement learning**
  - Why needed here: The TMIS estimator is a form of importance sampling that estimates the value of a target policy using data collected under different logging policies
  - Quick check question: What is the main challenge in applying importance sampling to reinforcement learning compared to the bandit setting?

- **Sample complexity in reinforcement learning**
  - Why needed here: The paper derives sample complexity bounds for the TMIS estimator in the adaptive data setting, which is a key measure of the estimator's efficiency
  - Quick check question: What is the relationship between the exploration assumption and the sample complexity of an offline RL algorithm?

## Architecture Onboarding

- **Component map**: MDP (tabular, finite-horizon) -> Logging process (adaptive algorithms) -> Dataset {τi} -> TMIS estimator -> Estimation error bounds
- **Critical path**: 1) Collect dataset {τi} using logging process, 2) Compute plug-in estimates ˆP, ˆr, and ˆd1 from the dataset, 3) Apply the TMIS estimator to compute ˆvπ, 4) Use concentration arguments to bound |ˆvπ − vπ|
- **Design tradeoffs**: Exploration vs. exploitation (more exploration leads to better bounds but may reduce immediate reward), Adaptivity vs. independence (adaptive logging can lead to better exploration but complicates analysis due to dependence between trajectories), Instance-dependent vs. worst-case bounds (instance-dependent bounds can be tighter for specific MDPs but may not provide worst-case guarantees)
- **Failure signatures**: High variance in nh,s,a across different (h,s,a) tuples indicating poor exploration, Large estimation error |ˆvπ − vπ| suggesting the TMIS estimator is not performing well, Violations of the exploration assumption leading to unbounded error
- **First 3 experiments**: 1) Verify that the TMIS estimator achieves low error on a simple MDP with known transition dynamics, 2) Test the impact of adaptivity on the TMIS estimator by comparing its performance on adaptive vs. non-adaptive data with identical logging policies, 3) Investigate the effect of the exploration assumption on the TMIS estimator's error by varying the logging process's exploration parameters

## Open Questions the Paper Calls Out

### Open Question 1
Can instance-dependent bounds on estimation error be recovered in the adaptive setting that are tighter than the worst-case bounds? The paper discusses that the instance-dependent bounds (Theorems 3.1 and 3.3) may significantly outperform the minimax-optimal bound on certain instances, but fail to recover optimal worst-case rates. This remains unresolved as the paper does not provide a definitive answer on whether tighter instance-dependent bounds can be obtained in the adaptive setting. Further theoretical analysis or empirical results demonstrating tighter instance-dependent bounds in the adaptive setting compared to the worst-case bounds would resolve this question.

### Open Question 2
How does adaptivity affect the behavior of the TMIS estimator for OPE compared to non-adaptive data? The paper mentions that empirical simulations suggest measurable differences in the signed behavior of the estimator for adaptive data vs non-adaptive data, even with the same logging policies. This remains unresolved as the paper only provides weak empirical evidence and acknowledges the need for more systematic understanding of how datasets generated by different exploration algorithms interact with existing OPE estimators. Comprehensive empirical studies comparing the performance of the TMIS estimator under different exploration algorithms and logging policies in both adaptive and non-adaptive settings would resolve this question.

### Open Question 3
Is pessimism provably efficient for offline RL in the adaptive setting? While the paper discusses the development of theoretical guarantees for the TMIS estimator in the adaptive setting, it does not address the efficiency of pessimism in offline RL under adaptive data. This remains unresolved as the paper focuses on the TMIS estimator and does not explore the efficiency of pessimism in the adaptive setting. Theoretical analysis or empirical results demonstrating the efficiency of pessimism in offline RL under adaptive data, possibly building on the results from Jin et al. (2021) for linear MDPs, would resolve this question.

## Limitations

- The theoretical results are primarily for tabular MDPs, leaving open questions about generalization to function approximation settings
- Empirical validation is limited to basic simulations with computational constraints, providing only weak evidence for some claims
- The exploration assumption (nh,s,a ≥ n d^m_h) may not hold in practice, especially for complex MDPs or poorly designed logging policies

## Confidence

- **Theoretical concentration arguments**: High - The mathematical proofs for the concentration inequalities and error bounds are rigorous and well-established
- **Empirical observations**: Medium - The simulations provide some validation but are limited by computational resources and scope
- **Practical applicability**: Medium - While the theory is sound, the assumptions may be challenging to satisfy in real-world applications

## Next Checks

1. **Coverage Analysis**: Systematically analyze the distribution of nh,s,a values across different adaptive logging strategies to verify the exploration assumption holds with high probability in practice.

2. **Instance-Dependent Bounds**: Conduct controlled experiments comparing the actual estimation error to the theoretical bounds across different MDP instances to validate the tightness of the instance-dependent guarantees.

3. **Computational Scaling**: Test the TMIS estimator on larger MDPs (beyond tabular) with adaptive data to understand how the bounds scale and whether the tape model approach remains tractable.