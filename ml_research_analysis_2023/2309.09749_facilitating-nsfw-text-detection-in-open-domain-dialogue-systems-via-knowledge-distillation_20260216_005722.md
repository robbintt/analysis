---
ver: rpa2
title: Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge
  Distillation
arxiv_id: '2309.09749'
source_url: https://arxiv.org/abs/2309.09749
tags:
- nsfw
- chatgpt
- dialogue
- gpt-4
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for detecting NSFW (Not Safe for Work)
  content in open-domain dialogue systems. The authors introduce a new dataset, CensorChat,
  constructed using knowledge distillation with GPT-4 and ChatGPT.
---

# Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation

## Quick Facts
- arXiv ID: 2309.09749
- Source URL: https://arxiv.org/abs/2309.09749
- Reference count: 0
- Primary result: BERT model achieves 0.91 accuracy in detecting NSFW content using knowledge distillation from GPT-4 and ChatGPT

## Executive Summary
This paper addresses the challenge of detecting NSFW (Not Safe for Work) content in open-domain dialogue systems by introducing a novel dataset construction approach using knowledge distillation. The authors leverage GPT-4 and ChatGPT to generate pseudo-labels for dialogue data, then train a BERT classifier on this labeled data. The approach achieves high accuracy while providing a cost-effective alternative to using large language models directly for NSFW detection, balancing user safety with freedom of expression.

## Method Summary
The authors construct the CensorChat dataset by collecting dialogue data from a social media platform and splitting it into utterance-level and context-level formats. They use ChatGPT and GPT-4 with a self-criticism strategy to annotate the data as NSFW or SFW, creating pseudo-labels for training. A BERT model is then fine-tuned on this pseudo-labeled data using cross-entropy loss. The self-criticism strategy involves prompting the LLMs to assess and potentially revise their labels when disagreements occur, with Cohen's kappa values above 0.8 indicating strong agreement between annotators.

## Key Results
- BERT model achieves 0.91 accuracy in detecting NSFW content
- Cohen's kappa values between ChatGPT and GPT-4 exceed 0.8 with self-criticism strategy
- The approach provides a cost-effective means of constructing NSFW detectors without sacrificing accuracy
- Dataset includes both utterance-level and context-level annotations for comprehensive coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from GPT-4 and ChatGPT enables cost-effective NSFW detection without sacrificing accuracy.
- Mechanism: The authors use high-performance LLMs to generate pseudo-labels for a large unlabeled dialogue dataset, then train a smaller BERT model on this pseudo-labeled data. This transfers the NSFW detection capability from expensive LLMs to a lightweight classifier.
- Core assumption: GPT-4 and ChatGPT can reliably label NSFW content in dialogue contexts.
- Evidence anchors:
  - [abstract] "Leveraging knowledge distillation techniques involving GPT-4 and ChatGPT, this dataset offers a cost-effective means of constructing NSFW content detectors."
  - [section 3.2.3] "We observe that the Cohen's κ values between ChatGPT and GPT-4 with the self-criticism strategy... are larger than 0.8, indicating almost perfect agreement."
  - [corpus] Found 25 related papers, average neighbor FMR=0.365, indicating moderate relevance to NSFW detection tasks.
- Break condition: If GPT-4 and ChatGPT fail to agree on labels or produce inconsistent annotations, the pseudo-labels become unreliable, degrading downstream BERT performance.

### Mechanism 2
- Claim: The self-criticism strategy improves label quality by resolving disagreements between annotators.
- Mechanism: When GPT-4 and ChatGPT disagree on a label, the self-criticism strategy prompts the models to re-evaluate their outputs, identify potential errors, and revise their labels before final assignment.
- Core assumption: LLMs can accurately self-assess and correct their own labeling mistakes when prompted appropriately.
- Evidence anchors:
  - [section 3.2.3] "The self-criticism strategy involves prompting the LLM to assess its output for potential inaccuracies or areas of improvement."
  - [section 3.2.3] "We observe that the Cohen's κ values between ChatGPT and GPT-4 with the self-criticism strategy... are larger than 0.8, indicating almost perfect agreement."
  - [corpus] No direct corpus evidence for self-criticism effectiveness, but related work exists on LLM self-correction (Madaan et al., 2023).
- Break condition: If the self-criticism prompts are ineffective or the models remain confident in incorrect labels, the strategy fails to improve annotation quality.

### Mechanism 3
- Claim: Separating data into utterance-level and context-level formats captures different aspects of NSFW detection challenges.
- Mechanism: Utterance-level annotation treats each utterance independently, while context-level annotation considers the conversational context when determining if a chatbot response is NSFW. This dual approach addresses both standalone offensive content and context-dependent inappropriate responses.
- Core assumption: NSFW content in dialogue systems requires both standalone utterance analysis and context-aware response evaluation.
- Evidence anchors:
  - [section 2.3] "For utterance-level content, we split the dialogue into utterances... For context-level content, we divide the dialogue into single-turn sessions..."
  - [section 4.1] "At the utterance level, xi = {u} represents an utterance produced by a user or a dialogue system. At the context level, we focus on whether the model response is NSFW or not, conditioning on user input."
  - [corpus] No direct corpus evidence, but this approach aligns with standard NLP practices for context-dependent classification.
- Break condition: If the context-level annotation fails to capture the relevant conversational dynamics, or if utterance-level annotation misses context-dependent NSFW content, detection performance suffers.

## Foundational Learning

- Concept: Knowledge distillation in machine learning
  - Why needed here: The paper relies on transferring knowledge from large LLMs to a smaller BERT classifier through pseudo-label generation.
  - Quick check question: What is the primary benefit of using knowledge distillation in this NSFW detection task?

- Concept: Cohen's kappa for inter-annotator agreement
  - Why needed here: The authors use Cohen's kappa to measure agreement between GPT-4 and ChatGPT annotations, validating their annotation quality.
  - Quick check question: What does a Cohen's kappa value above 0.8 indicate about annotator agreement?

- Concept: BERT fine-tuning for text classification
  - Why needed here: The final NSFW detection model is a fine-tuned BERT classifier trained on pseudo-labeled data.
  - Quick check question: What is the role of the [CLS] token in BERT-based text classification?

## Architecture Onboarding

- Component map: Data collection → Data annotation (ChatGPT/GPT-4) → Self-criticism strategy → Pseudo-label generation → BERT fine-tuning → NSFW detection model
- Critical path: The annotation and self-criticism stages are critical, as poor-quality pseudo-labels directly impact downstream BERT performance.
- Design tradeoffs: Using LLMs for annotation provides high accuracy but increases computational cost; using a smaller BERT model reduces inference costs but may sacrifice some detection capability.
- Failure signatures: Low Cohen's kappa values between annotators, poor BERT classification metrics (precision/recall below acceptable thresholds), or high false positive/negative rates in NSFW detection.
- First 3 experiments:
  1. Test annotation agreement between GPT-4 and ChatGPT on a small validation set without self-criticism.
  2. Evaluate the impact of self-criticism strategy on annotation agreement and label quality.
  3. Fine-tune BERT on pseudo-labeled data and measure classification performance on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the BERT model on CensorChat compare to other state-of-the-art NSFW detection models?
- Basis in paper: [explicit] The paper fine-tunes a BERT model on the CensorChat dataset and evaluates its performance.
- Why unresolved: The paper does not compare the BERT model's performance to other existing NSFW detection models, making it unclear how it stands against the state-of-the-art.
- What evidence would resolve it: Comparative studies with other NSFW detection models on the same dataset or similar datasets would provide insights into the BERT model's relative performance.

### Open Question 2
- Question: What is the impact of the self-criticism strategy on the quality of annotations in the dataset?
- Basis in paper: [explicit] The paper employs a self-criticism strategy to correct discrepancies in labeling between ChatGPT and GPT-4.
- Why unresolved: While the paper mentions the use of the self-criticism strategy, it does not provide detailed analysis on how this strategy affects the quality of annotations or the overall performance of the model.
- What evidence would resolve it: A comparative analysis of model performance with and without the self-criticism strategy would elucidate its impact on annotation quality and model efficacy.

### Open Question 3
- Question: How does the CensorChat dataset handle the balance between user safety and freedom of expression in NSFW content detection?
- Basis in paper: [explicit] The paper emphasizes the importance of AI systems prioritizing user safety while respecting freedom of expression.
- Why unresolved: The paper does not delve into specific mechanisms or strategies employed to maintain this balance, leaving the approach to this critical aspect of NSFW detection unclear.
- What evidence would resolve it: Detailed case studies or examples from the dataset illustrating how different types of NSFW content are handled would provide clarity on the balance between safety and expression.

## Limitations

- The approach depends critically on the reliability of LLM-based pseudo-annotations, which lack ground truth validation
- Dataset construction methodology is not fully reproducible due to unspecified data sources and preprocessing steps
- Reliance on proprietary LLMs (GPT-4 and ChatGPT) creates practical barriers for widespread adoption

## Confidence

**High confidence**: The BERT fine-tuning methodology and standard text classification metrics (accuracy of 0.91) are well-established approaches with predictable performance characteristics.

**Medium confidence**: The knowledge distillation approach is theoretically sound, but the quality of pseudo-labels directly impacts downstream performance. While Cohen's κ values suggest reasonable annotation agreement, we lack ground truth validation to confirm label accuracy.

**Low confidence**: The self-criticism strategy's effectiveness is primarily supported by the same Cohen's κ metric used for overall annotation agreement. Without independent evaluation of whether this strategy actually improves label quality versus simply enforcing consistency between models, its true contribution remains uncertain.

## Next Checks

1. **Ground truth validation**: Manually annotate a small subset (100-200 samples) of the CensorChat dataset and compare against GPT-4/ChatGPT labels to establish actual annotation accuracy and identify systematic biases or error patterns.

2. **Cross-cultural robustness test**: Evaluate the BERT model on dialogue data from different cultural contexts or languages to assess whether NSFW detection performance degrades when moving beyond the training domain, identifying potential cultural bias limitations.

3. **Cost-benefit analysis**: Measure the actual computational cost of generating pseudo-labels with GPT-4/ChatGPT versus the inference cost savings from using the fine-tuned BERT model, quantifying the claimed cost-effectiveness and identifying the break-even point in terms of dataset size.