---
ver: rpa2
title: 'Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant'
arxiv_id: '2307.04276'
source_url: https://arxiv.org/abs/2307.04276
tags:
- essay
- training
- attention
- feedback
- discourse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of automated essay scoring, specifically
  the need to evaluate argumentative writing for persuasiveness. While existing tools
  focus on grammar, spelling, and organization, they lack the ability to assess the
  strength of arguments, leaving teachers to provide feedback on persuasiveness alone.
---

# Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant

## Quick Facts
- arXiv ID: 2307.04276
- Source URL: https://arxiv.org/abs/2307.04276
- Reference count: 25
- Primary result: DeBERTeTeachingAssistant achieves above-human accuracy in predicting persuasiveness ratings for discourse elements in argumentative essays

## Executive Summary
This paper addresses the challenge of automated essay scoring for argumentative writing, specifically the need to evaluate persuasiveness of discourse elements. While existing tools can assess grammar, spelling, and organization, they cannot evaluate the strength of arguments. The authors develop DeBERTeTeachingAssistant, a transformer-based architecture that leverages the DeBERTaV3 model to predict persuasiveness ratings for discourse elements in essays. The approach includes preprocessing essays to include discourse type tokens, formulating the task as token classification, and employing techniques like ensemble learning, adversarial weight perturbation, and scale-invariant fine-tuning to enhance performance.

## Method Summary
The method uses a DeBERTaV3-large model with token classification head to predict persuasiveness ratings for discourse elements in argumentative essays. The preprocessing step includes all discourse elements from the same essay and adds special tokens to mark discourse elements and their types. The model is trained using 5-fold cross-validation with bagging to create an ensemble of five models, and applies adversarial weight perturbation and scale-invariant fine-tuning techniques. The ensemble approach reduces variance and improves log-loss performance compared to single models.

## Key Results
- Achieved log-loss of 0.5806 on persuasiveness prediction task
- Outperformed BERT and bag-of-words baseline models
- Demonstrated above-human accuracy in predicting effectiveness ratings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating content and position embeddings (disentangled attention) improves persuasiveness detection
- Mechanism: The DeBERTa model represents tokens using separate content embeddings and relative position embeddings, then computes cross-attention scores using both. This allows the model to learn distinct representations for semantic meaning and positional relationships, improving its ability to capture context for argumentative discourse elements
- Core assumption: Argumentative writing requires understanding both the meaning of words and their relative positions within the essay structure
- Evidence anchors:
  - [section 3.2.1]: Describes the disentangled attention mechanism where content and position embeddings are kept separate
  - [section 5.1]: States that the disentangled attention mechanism "creates a more robust representation of text for assessing its persuasiveness"
- Break condition: If the disentangled attention provides no advantage over standard attention in other persuasive text datasets, or if positional information becomes less important in shorter argumentative pieces

### Mechanism 2
- Claim: Ensemble learning with cross-validation folds reduces variance and improves log-loss performance
- Mechanism: The model splits training data into five folds, trains five identical DeBERTaV3 models, and averages their predictions. This bagging approach combines multiple models trained on different data subsets to reduce overfitting and variance
- Core assumption: Models trained on different subsets of the same data will make complementary errors that averaging can smooth out
- Evidence anchors:
  - [section 3.4.1]: Describes using bagging by averaging predictions from five models trained on different folds
  - [section 4.2]: Shows that the ensemble of five DeBERTaV3 models achieves log-loss of 0.5806, significantly better than single models
- Break condition: If ensemble performance doesn't improve with more folds, or if the dataset is too small for effective cross-validation

### Mechanism 3
- Claim: Using essay-level context (including all discourse elements from the same essay) improves prediction accuracy for individual elements
- Mechanism: Instead of evaluating each discourse element in isolation, the model preprocesses essays by adding special tokens to mark discourse elements and their types, then concatenates all elements from the same essay. This provides full contextual information for each element's prediction
- Core assumption: The persuasiveness of a discourse element depends on its relationship to other elements within the same essay
- Evidence anchors:
  - [section 3.1.1]: Describes preprocessing that includes all discourse elements from the same essay when evaluating each element
  - [section 3.1.2]: Formulates the problem as token classification where each token is labeled with the effectiveness rating of its discourse element
- Break condition: If element-level predictions don't benefit from essay-level context, or if the computational cost outweighs the performance gain

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model is built on DeBERTaV3, which uses transformer layers with attention mechanisms. Understanding how transformers process sequences and compute attention scores is essential for modifying or debugging the architecture
  - Quick check question: How does the disentangled attention in DeBERTa differ from standard self-attention in BERT?

- Concept: Token classification vs sequence classification
  - Why needed here: The problem is formulated as token classification rather than sequence classification, where each token gets a label and predictions are averaged. This approach differs from standard text classification and affects how the model is trained and evaluated
  - Quick check question: What are the advantages of formulating essay persuasiveness prediction as token classification rather than sequence classification?

- Concept: Ensemble methods and cross-validation
  - Why needed here: The model uses K-fold cross-validation combined with bagging to create an ensemble of five models. Understanding these techniques is crucial for implementing, modifying, or evaluating the ensemble approach
  - Quick check question: How does K-fold cross-validation help reduce variance compared to training a single model on all data?

## Architecture Onboarding

- Component map:
  Input preprocessing -> DeBERTaV3-large model -> Token classification head -> Ensemble layer -> Post-processing

- Critical path:
  1. Preprocess essay to include discourse markers
  2. Tokenize and feed through DeBERTaV3
  3. Apply token classification head
  4. Ensemble predictions from five models
  5. Average token scores for final element predictions

- Design tradeoffs:
  - Memory vs. performance: Large model size requires fp16 and gradient checkpointing
  - Context vs. efficiency: Essay-level processing provides better context but increases input length
  - Ensemble size vs. training time: Five models provide better performance but increase training time by 5x

- Failure signatures:
  - Poor performance on long essays: May indicate attention mechanism limitations
  - High variance between ensemble models: Suggests insufficient data or unstable training
  - Memory errors during training: May require adjusting batch size or gradient accumulation

- First 3 experiments:
  1. Test single model performance vs. ensemble to quantify variance reduction
  2. Compare essay-level context vs. element-only context to measure context benefit
  3. Test different ensemble sizes (3, 5, 7 models) to find optimal tradeoff between performance and training cost

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The model was only tested on one dataset (PERSUADE corpus) and may not generalize to other argumentative writing styles or domains
- The ensemble approach increases computational cost by 5x and may not scale well for real-time applications
- The study focuses on predicting persuasiveness ratings but does not address how these predictions translate into actionable feedback for students

## Confidence

- High confidence: The core mechanism of using disentangled attention in DeBERTa for capturing both content and positional information is well-established in the literature and demonstrated in the results
- Medium confidence: The ensemble learning approach shows clear benefits on this specific dataset, but the optimal number of models and the generalizability to other datasets remain uncertain
- Medium confidence: The preprocessing approach of including essay-level context for element-level predictions appears beneficial, but the specific impact relative to simpler approaches is not quantified

## Next Checks

1. Test model performance on a held-out test set from a different argumentative writing corpus to assess generalizability beyond the PERSUADE dataset
2. Compare ensemble performance with simpler approaches like single models with more training data or different regularization techniques to isolate the specific benefit of the ensemble approach
3. Evaluate the computational efficiency trade-off by measuring inference time and memory usage for different ensemble sizes to determine the practical scalability of the approach