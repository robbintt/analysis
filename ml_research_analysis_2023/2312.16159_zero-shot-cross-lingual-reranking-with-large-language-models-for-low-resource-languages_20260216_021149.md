---
ver: rpa2
title: Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource
  Languages
arxiv_id: '2312.16159'
source_url: https://arxiv.org/abs/2312.16159
tags:
- reranking
- languages
- llms
- query
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the effectiveness of large language models
  (LLMs) as listwise rerankers for low-resource African languages. It investigates
  cross-lingual and monolingual reranking scenarios using English queries and African
  language passages.
---

# Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages

## Quick Facts
- arXiv ID: 2312.16159
- Source URL: https://arxiv.org/abs/2312.16159
- Reference count: 6
- Key outcome: LLMs improve cross-lingual reranking for low-resource African languages, with English reranking showing up to 7 points better nDCG@20 than cross-lingual reranking.

## Executive Summary
This paper investigates the effectiveness of large language models (LLMs) as listwise rerankers for low-resource African languages. The study examines cross-lingual and monolingual reranking scenarios using English queries and African language passages. Three models - RankGPT4, RankGPT3.5, and RankZephyr - are evaluated on the CIRAL cross-lingual IR dataset covering Hausa, Somali, Swahili, and Yoruba. The research demonstrates that while reranking in English via translation yields the best results, cross-lingual reranking can be competitive with monolingual approaches depending on the LLM's multilingual capabilities.

## Method Summary
The study employs listwise reranking using three LLMs: RankGPT4, RankGPT3.5, and RankZephyr. The approach uses BM25 as a first-stage retriever with either query translation (BM25-QT) or document translation (BM25-DT). Retrieved passages are segmented using a sliding window technique with size 20 and stride 10. Reranking is performed using prompt templates that instruct the LLM to order passages by relevance. The CIRAL dataset provides English queries and passages in four African languages. Translation quality is measured using BLEU scores, and effectiveness is evaluated using nDCG@20 and MRR@100 metrics.

## Key Results
- Reranking in English yields up to 7 points improvement in nDCG@20 over cross-lingual reranking
- Cross-lingual reranking is consistently more effective than reranking in African languages
- Using the same LLM for query translation and reranking improves African language reranking effectiveness, particularly for RankGPT4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Listwise reranking with LLMs is more effective than pointwise approaches for low-resource languages.
- Mechanism: LLMs process multiple documents simultaneously in a single prompt, allowing them to capture document-to-document relationships and relative relevance ordering that pointwise approaches miss.
- Core assumption: The LLM's attention mechanism can effectively model inter-document relationships even when processing documents in low-resource languages.
- Evidence anchors:
  - [abstract] "The large context size of LLMs makes listwise approaches particularly attractive because the model attends to multiple documents and produces a relative ordering."
  - [section] "As this approach has been proven to be more effective than pointwise and pairwise reranking (Ma et al., 2023b; Pradeep et al., 2023a), we solely employ listwise reranking in this work."
- Break condition: When the LLM's context window is too small to include enough documents for meaningful comparison, or when document quality is too low for the model to extract meaningful relationships.

### Mechanism 2
- Claim: Reranking in English yields better results than reranking directly in African languages.
- Mechanism: LLMs are better aligned and trained on English data, so their understanding of relevance is more accurate in English, even when documents are translations.
- Core assumption: The quality of relevance assessment is proportional to the amount of training data in that language.
- Evidence anchors:
  - [abstract] "While reranking remains most effective in English, our results reveal that cross-lingual reranking may be competitive with reranking in African languages depending on the multilingual capability of the LLM."
  - [section] "Improved reranking effectiveness with English translations is expected, given that LLMs, despite being multilingual, are more attuned to English."
- Break condition: When LLM models are specifically fine-tuned on low-resource languages or when translation quality degrades the original document meaning significantly.

### Mechanism 3
- Claim: Using the same LLM for both query translation and reranking improves monolingual reranking effectiveness in African languages.
- Mechanism: The LLM maintains semantic consistency between its own translations and its ranking decisions, reducing semantic drift that occurs when different systems handle translation and ranking.
- Core assumption: The LLM's internal representation of meaning is consistent across its translation and ranking capabilities.
- Evidence anchors:
  - [abstract] "When reranking in African languages, we gain improvements for RankGPT4 when we perform query translation using GPT-4 itself."
  - [section] "In the process, we also establish that good translations obtained from the LLMs do improve its reranking effectiveness in the African language reranking scenario as discovered with RankGPT4."
- Break condition: When the LLM's translation quality is poor or when the translation task is outsourced to a more specialized translation system.

## Foundational Learning

- Concept: Cross-lingual information retrieval (CLIR)
  - Why needed here: The paper operates in a CLIR setting where queries are in English but documents are in African languages.
  - Quick check question: What are the three main approaches to CLIR mentioned in the paper?
  - Answer: Query translation, document translation, and language-independent representations.

- Concept: Listwise reranking
  - Why needed here: The paper exclusively uses listwise reranking rather than pointwise or pairwise approaches.
  - Quick check question: How does listwise reranking differ from pointwise reranking in terms of input to the model?
  - Answer: Listwise provides multiple documents simultaneously for relative ranking, while pointwise evaluates each document independently.

- Concept: BLEU score for translation quality
  - Why needed here: The paper evaluates translation quality using BLEU to understand its correlation with reranking effectiveness.
  - Quick check question: What does a higher BLEU score indicate about translation quality?
  - Answer: A higher BLEU score indicates that the translation is more similar to human reference translations.

## Architecture Onboarding

- Component map: BM25 retrieval -> Sliding window segmentation -> LLM reranker -> Evaluation
- Critical path: Document retrieval -> Reranking decision -> Final ranked list
- Design tradeoffs: Translation quality vs. computational cost, window size vs. context utilization, LLM choice vs. availability
- Failure signatures: Poor nDCG scores, large gaps between cross-lingual and monolingual performance, inconsistent translation quality
- First 3 experiments:
  1. Compare BM25-QT vs BM25-DT as first-stage retrievers with the same LLM reranker
  2. Test different sliding window sizes (10, 20, 30) with the same LLM
  3. Compare GPT-4 vs GPT-3.5 vs RankZephyr on the same retrieval setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which query translation quality influences reranking effectiveness in low-resource languages?
- Basis in paper: [explicit] The paper observes that RankGPT4 achieves better monolingual reranking effectiveness in African languages when using its own query translations, and correlates this with translation quality measured by BLEU scores.
- Why unresolved: The paper demonstrates a correlation but does not establish causation or identify specific linguistic or semantic features that drive this relationship.
- What evidence would resolve it: Controlled experiments varying translation quality while holding other factors constant, along with detailed linguistic analysis of translation errors and their impact on ranking decisions.

### Open Question 2
- Question: Why does cross-lingual reranking consistently outperform monolingual reranking in African languages despite lower translation quality?
- Basis in paper: [explicit] The paper reports that cross-lingual reranking is consistently more effective than reranking in African languages, though it provides limited theoretical explanation for this finding.
- Why unresolved: The paper identifies the phenomenon but does not explore underlying mechanisms such as model biases, attention patterns, or differences in semantic representation across languages.
- What evidence would resolve it: Comparative analysis of model attention patterns and semantic embeddings across cross-lingual versus monolingual scenarios, plus controlled experiments isolating specific factors.

### Open Question 3
- Question: What architectural modifications to open-source LLMs would most effectively improve their zero-shot reranking performance on low-resource languages?
- Basis in paper: [explicit] The paper notes that RankZephyr, an open-source model, shows competitive performance with proprietary models but has room for improvement in low-resource languages.
- Why unresolved: The paper demonstrates current limitations but does not systematically explore which model components or training approaches would yield the greatest improvements.
- What evidence would resolve it: Empirical comparison of various architectural modifications (e.g., multilingual pretraining, domain adaptation, instruction tuning) on reranking performance across multiple low-resource languages.

## Limitations

- The study assumes translation quality is static across different LLM models, but translation quality varies significantly between models, which could confound the reranking results.
- The sliding window approach with fixed size (20) and stride (10) may not be optimal for all languages or document types, potentially limiting the generalizability of findings.
- The experiments exclusively evaluate English-to-African language translation quality using BLEU, without assessing whether BLEU correlates with downstream reranking effectiveness.

## Confidence

- **High Confidence:** The finding that English reranking outperforms cross-lingual reranking is well-supported by the experimental results across multiple metrics and languages.
- **Medium Confidence:** The claim that using the same LLM for translation and reranking improves African language reranking effectiveness is supported but based on limited evidence (primarily RankGPT4).
- **Low Confidence:** The generalizability of findings to other low-resource language pairs beyond the four African languages studied remains uncertain.

## Next Checks

1. Conduct correlation analysis between BLEU scores and reranking effectiveness metrics to determine if translation quality directly impacts ranking performance.
2. Test the sliding window approach with variable sizes optimized per language to assess whether current fixed parameters limit performance.
3. Expand experiments to include additional low-resource language pairs (e.g., Asian or indigenous American languages) to validate the cross-lingual effectiveness findings beyond African languages.