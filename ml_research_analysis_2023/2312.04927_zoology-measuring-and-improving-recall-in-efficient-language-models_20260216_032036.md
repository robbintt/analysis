---
ver: rpa2
title: 'Zoology: Measuring and Improving Recall in Efficient Language Models'
arxiv_id: '2312.04927'
source_url: https://arxiv.org/abs/2312.04927
tags:
- attention
- baseconv
- sequence
- input
- gid00001
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gated-convolution language models are efficient but underperform
  attention on associative recall, a key capability for language modeling. We identify
  that the gap is due to each model's ability to recall previously seen associations
  in context.
---

# Zoology: Measuring and Improving Recall in Efficient Language Models

## Quick Facts
- arXiv ID: 2312.04927
- Source URL: https://arxiv.org/abs/2312.04927
- Authors: 
- Reference count: 40
- Primary result: Gated-convolution models require dimension scaling with sequence length for associative recall while attention achieves constant dimensionality

## Executive Summary
This paper identifies a fundamental capability gap between efficient gated-convolution language models and attention-based models: associative recall. Through extensive experiments on the Pile dataset and synthetic tasks, the authors demonstrate that attention significantly outperforms gated-convolutions on recalling previously seen associations in context, even when using much smaller models. The key insight is that attention's input-dependent token-to-token interactions enable efficient associative recall, while gated-convolutions' fixed filters struggle with this task. The authors propose a new formalization called multi-query associative recall (MQAR) and show that hybrids combining convolutions with sparse, input-dependent attention can close 97.4% of the gap to full attention while maintaining sub-quadratic scaling.

## Method Summary
The paper systematically evaluates 17 language models across five architectures (attention, Hyena, H3, RWKV, and BaseConv) at four scales (70M-1.4B parameters) on the Pile dataset. They introduce synthetic MQAR tasks to isolate associative recall capabilities and analyze perplexity on specific AR tokens. The authors then develop BaseConv-attention hybrid models with input-dependent sparse attention patterns targeting repeated bigrams. These hybrids are evaluated both on the Pile and synthetic MQAR datasets to quantify the recall gap closure.

## Key Results
- Attention outperforms gated-convolutions on associative recall by large margins, even with much smaller models
- Gated-convolutions require model dimension to scale with sequence length to solve MQAR, while attention can solve it with constant dimensionality
- Hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention on MQAR tasks
- Input-dependent sequence mixing is crucial for efficient associative recall in efficient architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention outperforms gated-convolutions on associative recall because attention uses input-dependent token-to-token interactions, while gated-convolutions use fixed filters defined by model weights.
- Core assumption: The ability to dynamically adapt sequence mixing based on input content is critical for associative recall performance.
- Evidence anchors:
  - [abstract] "Attention outperforms gated-convolutions on this task by a large margin, even with much smaller models"
  - [section] "Attention achieves input-dependence since it computes all token-to-token interactions when determining how to mix information in the sequence"
- Break condition: If input-independent convolution architectures with gating can achieve attention-level associative recall performance, this mechanism would be invalid.

### Mechanism 2
- Claim: Gated-convolutions require model dimension to scale with sequence length to solve multi-query associative recall, while attention can solve it with constant dimensionality.
- Core assumption: The number of unique token-interaction distances in a sequence directly impacts the model's representational requirements.
- Evidence anchors:
  - [abstract] "We show that gated-convolutions require model dimension to scale with sequence length to solve MQAR, while attention can solve it with constant dimensionality"
  - [section] "The model needs to adapt the sequence mixing weights based on the token-interaction distances required for each new example"
- Break condition: If gated-convolutions can solve MQAR with constant dimensionality independent of sequence length, this mechanism would be invalid.

### Mechanism 3
- Claim: Input-dependent sequence mixing is essential for efficient multi-query associative recall.
- Core assumption: Associative recall primarily requires input-dependent operations on a small subset of tokens rather than full input-dependent computation.
- Evidence anchors:
  - [abstract] "We show that input-dependent sequence mixing is important to solve MQAR efficiently"
  - [section] "we show that simply inserting input-dependent operator — e.g., a convolution filter that shifts the sequence based on the bigram positions or sparse attention placed only on repeated bigram positions — to the BaseConv architecture at < 10% of layers suffices to outperform the Transformer baseline"
- Break condition: If associative recall performance cannot be significantly improved by targeted input-dependent operations on specific tokens, this mechanism would be invalid.

## Foundational Learning

- Concept: Associative recall
  - Why needed here: This is the core capability that explains the quality gap between attention and gated-convolution architectures
  - Quick check question: Can you explain the difference between single-query associative recall and multi-query associative recall?

- Concept: Sequence mixing
  - Why needed here: Understanding how different architectures aggregate token embeddings is crucial for analyzing their recall capabilities
  - Quick check question: What is the key difference between how attention and gated-convolutions perform sequence mixing?

- Concept: Input-dependence
  - Why needed here: This property distinguishes architectures that can adapt to input content from those that use fixed operations
  - Quick check question: How does input-dependence in attention compare to input-independence in gated-convolutions?

## Architecture Onboarding

- Component map: Attention -> QKV projections -> Token-to-token interactions -> Sequence mixing; BaseConv -> Fixed convolution filters -> Gating -> Sequence mixing; Hybrids -> Convolution + Sparse input-dependent attention -> Sequence mixing

- Critical path: Understanding the associative recall gap requires analyzing: 1) How each architecture performs on downstream perplexity, 2) How they handle AR tokens specifically, 3) Their performance on synthetic MQAR tasks, and 4) How hybrid approaches with input-dependent operations perform.

- Design tradeoffs: Attention provides excellent associative recall but has quadratic complexity. Gated-convolutions offer sub-quadratic scaling but struggle with recall. Hybrids with sparse attention can balance these tradeoffs.

- Failure signatures: Poor performance on AR tokens (repeated bigrams in context), especially for rare bigrams seen infrequently during training. Large perplexity gaps between attention and gated-convolutions on these specific tokens.

- First 3 experiments:
  1. Train attention, Hyena, H3, and RWKV models on Pile and measure overall perplexity and AR-specific perplexity
  2. Create synthetic MQAR data and evaluate all architectures on varying sequence lengths and model dimensions
  3. Implement BaseConv hybrids with programmatic sparse attention and evaluate on both Pile and synthetic MQAR tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do input-dependent gating mechanisms in gated-convolution architectures compare to attention in terms of parameter efficiency for multi-query associative recall (MQAR) tasks?
- Basis in paper: [explicit] The paper discusses the importance of input-dependence for solving MQAR efficiently and shows that attention achieves this property naturally, while gated-convolutions require careful design of input-dependent filters or selective attention patterns.
- Why unresolved: While the paper provides theoretical analysis and experiments on specific hybrid architectures, it does not provide a comprehensive comparison of different input-dependent gating mechanisms in terms of their parameter efficiency for MQAR.
- What evidence would resolve it: A systematic study comparing the parameter efficiency of various input-dependent gating mechanisms (e.g., learned selection, programmatic selection, different types of input-dependent convolutions) against attention on a range of MQAR tasks with varying sequence lengths and number of queries.

### Open Question 2
- Question: Can the multi-query associative recall (MQAR) gap between gated-convolutions and attention be closed entirely through architectural modifications, or are there fundamental limitations to the representational power of gated-convolutions for this task?
- Basis in paper: [explicit] The paper demonstrates that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention on MQAR, but it does not definitively prove whether the remaining gap can be closed through further architectural modifications or if there are inherent limitations to gated-convolutions.
- Why unresolved: The paper provides strong empirical evidence for the effectiveness of input-dependent sparse attention patterns, but it does not explore the full space of potential architectural modifications or provide a formal proof of the limitations of gated-convolutions for MQAR.
- What evidence would resolve it: A combination of theoretical analysis (e.g., proving lower bounds on the representational power of gated-convolutions for MQAR) and extensive empirical evaluation of a wide range of architectural modifications (e.g., different types of gating, convolutions, and attention mechanisms) on MQAR tasks with varying difficulty levels.

### Open Question 3
- Question: How does the multi-query associative recall (MQAR) gap between gated-convolutions and attention change as model scale increases beyond the billion-parameter level?
- Basis in paper: [explicit] The paper evaluates the AR gap at the 70M, 160M, 360M, and 1.4B parameter scales and finds that it persists even at the billion-parameter level, but it does not explore scales beyond this point.
- Why unresolved: The paper provides evidence that the AR gap is not solely due to model scale, but it does not investigate whether the gap continues to persist or diminishes as models become even larger.
- What evidence would resolve it: Training and evaluating gated-convolution and attention models at scales significantly larger than 1.4B parameters on MQAR tasks and downstream language modeling benchmarks, comparing their performance and analyzing the trends in the AR gap as scale increases.

## Limitations

- The synthetic MQAR tasks may not fully capture the complexity and statistical properties of natural language associative recall
- The analysis focuses primarily on repeated bigram patterns, which may not represent all forms of associative recall in real language
- The paper does not explore whether architectural modifications beyond simple attention hybrids could achieve similar input-dependence in gated-convolutions

## Confidence

**High Confidence**: The empirical observation that attention models achieve lower perplexity on associative recall tokens (AR hits) compared to gated-convolution models, even when the latter are larger. This is directly measured on real language data and the effect size is substantial.

**Medium Confidence**: The theoretical argument that gated-convolutions require model dimension to scale with sequence length for MQAR, while attention can solve it with constant dimensionality. The synthetic experiments support this, but real language may have different statistical properties.

**Medium Confidence**: The claim that input-dependent sequence mixing is important for efficient MQAR. The hybrid experiments show promising results, but the specific mechanism (sparse attention on repeated bigrams) may be a special case rather than a general principle.

## Next Checks

1. **Scale-up validation**: Evaluate the proposed BaseConv-attention hybrids on larger model scales (5B+ parameters) to verify that the 97.4% gap closure holds at scale, and test whether attention's advantage persists or diminishes as model capacity increases.

2. **Alternative input-dependent mechanisms**: Implement and compare alternative input-dependent operations for gated-convolutions beyond sparse attention on repeated bigrams, such as dynamic convolution filters or learned gating patterns, to determine if the sparse attention approach is optimal.

3. **Real-world language complexity**: Design a more complex natural language benchmark that isolates associative recall capabilities while controlling for other linguistic factors, to verify that the synthetic MQAR task captures the essential challenges of real language modeling.