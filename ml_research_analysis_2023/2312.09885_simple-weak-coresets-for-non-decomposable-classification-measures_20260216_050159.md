---
ver: rpa2
title: Simple Weak Coresets for Non-Decomposable Classification Measures
arxiv_id: '2312.09885'
source_url: https://arxiv.org/abs/2312.09885
tags:
- coreset
- score
- uniform
- size
- coresets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work initiates the study of coresets for non-decomposable
  classification measures, focusing on the F1 score and Matthews Correlation Coefficient
  (MCC). It demonstrates that stratified uniform sampling can create weak coresets
  with theoretical guarantees for these measures, achieving performance comparable
  to more complex coreset construction strategies.
---

# Simple Weak Coresets for Non-Decomposable Classification Measures

## Quick Facts
- arXiv ID: 2312.09885
- Source URL: https://arxiv.org/abs/2312.09885
- Reference count: 40
- Primary result: Stratified uniform sampling creates weak coresets with theoretical guarantees for F1 and MCC, achieving performance comparable to sophisticated methods while being faster

## Executive Summary
This work initiates the study of coresets for non-decomposable classification measures, focusing on F1 score and Matthews Correlation Coefficient (MCC). The authors demonstrate that stratified uniform sampling can create weak coresets with theoretical guarantees for these measures, achieving performance comparable to more complex coreset construction strategies. The study establishes lower bounds showing that strong coresets for F1 and MCC require the full dataset size, thus proving that uniform sampling is optimal. Experimental results on real datasets with various classifiers confirm that uniform sampling performs as well as or better than sophisticated methods like leverage score and k-means based coresets, while being significantly faster.

## Method Summary
The method employs stratified uniform sampling to construct weak coresets for F1 and MCC measures. The approach samples proportionally from each class to maintain class balance, then applies Li-Long-Srinivasan VC-dimension bounds to ensure empirical TP, FP, TN, FN counts approximate true counts within error bounds for all queries in Qγ. For MCC specifically, negative samples are reweighted by the Y-/Y+ ratio to compensate for class imbalance. The coreset is then used to train classifiers (logistic regression, SVM, MLP) and evaluate performance on the full dataset.

## Key Results
- Stratified uniform sampling preserves F1 score and MCC up to small additive error for a large set of queries including the optimal
- Lower bounds show strong coresets for F1 and MCC require the full dataset size, proving uniform sampling is optimal
- Uniform sampling coresets achieve performance comparable to sophisticated methods (leverage score, k-means) while being significantly faster
- The method works across three real datasets (COVERTYPE, Adult, KDDCUP '99) and three classifier types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stratified uniform sampling preserves F1 score and MCC up to small additive error for a large set of queries including the optimal.
- Mechanism: The method samples proportionally from each class, maintaining class balance. By applying the Li-Long-Srinivasan VC-dimension bound, it ensures that empirical TP, FP, TN, FN counts approximate true counts within error bounds for all queries in Qγ.
- Core assumption: Queries in Qγ achieve high F1/MCC and sufficient TP/TN counts; VC-dimension of Qγ is finite and known.
- Evidence anchors:
  - [abstract] "show that stratified uniform sampling based coresets have excellent empirical performance that are backed by theoretical guarantees too."
  - [section 5] Formal proof of Theorem 5.2 using VC-dimension and additive error bounds.
  - [corpus] No direct evidence; must infer from methodology description.
- Break condition: If TP or TN drops below the lower bound threshold, or if the VC-dimension is too large, the additive error guarantee fails.

### Mechanism 2
- Claim: Uniform sampling is optimal because strong coresets for F1 and MCC require the full dataset size.
- Mechanism: Lower bound constructions show that any strong coreset must preserve the exact contingency table for each point's optimal classifier, forcing inclusion of all points.
- Core assumption: The lower bound construction holds for arbitrary query sets, implying no smaller strong coreset exists.
- Evidence anchors:
  - [section 4] Theorem 4.1 and 4.2 provide explicit lower bound proofs for strong coresets.
  - [abstract] "show a lower bound for strong coresets for the F1 and the MCC scores, implying that we cannot do much better than uniform sampling."
  - [corpus] No direct evidence; relies on theoretical proofs.
- Break condition: If query set is restricted or decomposable measures are used, stronger coresets may exist.

### Mechanism 3
- Claim: Reweighting negative samples by Y-/Y+ ratio compensates for imbalance and preserves MCC.
- Mechanism: By assigning each negative sample a weight of Y-/Y+, the coreset simulates the full dataset's negative class distribution in MCC calculation.
- Core assumption: MCC's definition depends on class proportions; reweighting correctly scales TN and FP contributions.
- Evidence anchors:
  - [section 6] Theorem 6.1 proof showing reweighting preserves MCC approximation.
  - [abstract] "show that uniform coresets attain a lower bound for coreset size, and have good empirical performance, comparable with 'smarter' coreset construction strategies."
  - [corpus] No direct evidence; inferred from proof methodology.
- Break condition: If class imbalance is extreme or reweighting factor is misestimated, MCC preservation fails.

## Foundational Learning

- Concept: VC-dimension and its role in uniform convergence bounds.
  - Why needed here: The proof of weak coreset guarantees relies on VC-dimension to bound the number of samples needed for uniform approximation of TP, FP, TN, FN counts across all queries in Qγ.
  - Quick check question: What does the VC-dimension of a hypothesis class represent, and how does it affect the sample complexity in uniform convergence?
- Concept: Contingency table-based evaluation measures (F1, MCC).
  - Why needed here: Understanding how F1 and MCC are computed from TP, FP, TN, FN is essential to see why stratified uniform sampling preserves them.
  - Quick check question: How is MCC defined in terms of the contingency table, and why is it non-decomposable?
- Concept: Strong vs weak coresets.
  - Why needed here: The paper distinguishes between coresets that work for all queries (strong) versus a subset (weak), which is key to interpreting the results.
  - Quick check question: What is the difference between strong and weak coresets, and why can weak coresets be smaller?

## Architecture Onboarding

- Component map:
  - Data ingestion -> Stratified uniform sampler -> Reweighter -> Classifier trainer -> Evaluator
- Critical path:
  1. Stratified sampling → 2. Reweighting → 3. Model training → 4. Evaluation
- Design tradeoffs:
  - Uniform sampling is fast but may be suboptimal for very imbalanced data; leverage/k-means coresets are slower but potentially more accurate
  - Weak coresets save space but only guarantee performance for high-performing queries
- Failure signatures:
  - MCC/F1 degrades sharply if TP/TN fall below thresholds
  - VC-dimension bounds too loose → more samples needed than practical
  - Reweighting factor misestimated → class imbalance not properly compensated
- First 3 experiments:
  1. Compare uniform vs leverage score coresets on CovType with logistic regression for MCC
  2. Vary coreset size (1%-10%) and plot MCC/F1 vs full dataset baseline
  3. Test weak coreset guarantee by evaluating on queries with varying F1/MCC values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can coresets with better additive guarantees be developed for non-decomposable classification measures beyond the current theoretical bounds?
- Basis in paper: [explicit] The authors conclude by noting that "it would be interesting to see whether coresets with better additive guarantees and lesser assumptions on the query vector can be developed."
- Why unresolved: Current work only provides weak coresets with additive error guarantees for F1 and MCC, and the authors establish lower bounds showing strong coresets require the full dataset size.
- What evidence would resolve it: Development of new coreset construction algorithms that provide stronger theoretical guarantees (either smaller additive errors or fewer assumptions on the query set) with empirical validation on benchmark datasets.

### Open Question 2
- Question: How do uniform sampling coresets perform for other non-decomposable measures like AUC-ROC compared to F1 and MCC?
- Basis in paper: [explicit] The authors state "The question of tackling other measures, e.g., AUC-ROC, also remains open."
- Why unresolved: The current work focuses exclusively on F1 and MCC measures, leaving the performance on other non-decomposable metrics unexplored.
- What evidence would resolve it: Empirical studies applying uniform sampling coresets to AUC-ROC and other non-decomposable measures across multiple datasets, comparing performance to both sophisticated coreset methods and the current theoretical expectations.

### Open Question 3
- Question: Can algorithm-specific subset selection strategies provide more efficient coresets for particular classifier types?
- Basis in paper: [explicit] The authors suggest "algorithm-specific subset selection strategies could be explored for more efficiency."
- Why unresolved: The current experiments use general coreset methods (uniform, leverage score, k-means, etc.) without tailoring to specific classifier characteristics or optimization requirements.
- What evidence would resolve it: Development and testing of classifier-specific coreset algorithms that exploit properties of particular models (e.g., neural networks, SVMs, gradient boosting) showing improved performance or efficiency compared to general methods.

## Limitations
- The lower bound proofs assume adversarial query sets, which may not reflect real-world applications with more structured query spaces
- VC-dimension bounds may be loose for complex classifiers like MLPs, potentially requiring more samples than reported
- Experimental validation is limited to three datasets and three classifier types, raising questions about generalizability

## Confidence
- Theoretical guarantees for weak coresets: Medium
- Optimality claim for uniform sampling: Medium (depends on lower bound construction)
- Empirical superiority over sophisticated methods: Medium (limited dataset/algorithm scope)

## Next Checks
1. Test uniform sampling on highly imbalanced datasets (imbalance ratio > 100:1) to verify MCC preservation under extreme conditions
2. Evaluate coresets with deeper neural networks (e.g., ResNet) to check if VC-dimension bounds remain valid for modern architectures
3. Apply the method to time-series or image classification tasks to assess performance beyond tabular data