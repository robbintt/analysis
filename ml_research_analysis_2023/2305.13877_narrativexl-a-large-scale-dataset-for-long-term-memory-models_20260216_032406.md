---
ver: rpa2
title: 'NarrativeXL: A Large-scale Dataset For Long-Term Memory Models'
arxiv_id: '2305.13877'
source_url: https://arxiv.org/abs/2305.13877
tags:
- memory
- questions
- book
- dataset
- long-term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NarrativeXL, a large-scale reading comprehension
  dataset for long-term memory models. It contains nearly a million questions based
  on summaries of 1,500 fiction books, with an average document length exceeding 50,000
  words.
---

# NarrativeXL: A Large-scale Dataset For Long-Term Memory Models

## Quick Facts
- arXiv ID: 2305.13877
- Source URL: https://arxiv.org/abs/2305.13877
- Authors: 
- Reference count: 14
- Key outcome: Introduces NarrativeXL, a large-scale reading comprehension dataset with nearly a million questions based on 1,500 fiction books, designed to train and evaluate long-term memory models

## Executive Summary
This paper presents NarrativeXL, a novel dataset specifically designed to train and evaluate long-term memory models in language systems. The dataset contains approximately one million reading comprehension questions derived from summaries of 1,500 fiction books, with each question tagged with a "retention demand" metric indicating the memory length required to answer it. The questions come in multiple formats including multiple-choice scene recognition and free-form narrative reconstruction, allowing for comprehensive evaluation of different memory architectures. Three validation experiments demonstrate that the dataset accurately represents source material, can diagnose model memory capacity, and presents meaningful challenges even for modern language models within their context windows.

## Method Summary
The dataset creation process involves downloading 1,500 fiction books from Project Gutenberg, manually filtering out non-narrative content, and splitting each book into 20,000-symbol chunks with 300-symbol overlap. GPT-3.5 is then used to generate scene-level summaries for each chunk, producing approximately 150 summaries per book. From these summaries, reading comprehension questions are generated in multiple formats including true/false scene recognition, lookahead scenes, other-book scenes, and distorted scenes. The dataset also includes free-form narrative reconstruction questions, with each question tagged with retention demand metadata indicating how many scenes back the relevant information was presented.

## Key Results
- Created 990,595 reading comprehension questions across 1,500 fiction books with an average document length exceeding 50,000 words
- Each question includes "retention demand" metadata indicating memory length required for answering
- Three validation experiments confirmed dataset quality: human validation achieved 48/50 accuracy, baseline model performance within context windows was as expected, and questions proved challenging for modern language models
- Dataset includes both multiple-choice scene recognition questions and free-form narrative reconstruction questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset creation method leverages GPT-3.5's short-term memory competence to generate long-term memory tasks.
- Mechanism: GPT-3.5 summarizes scenes from books, then generates reading comprehension questions that require recalling information from earlier scenes, effectively converting short-term competence into a long-term memory evaluation tool.
- Core assumption: GPT-3.5's ability to understand and summarize text translates into the ability to create valid reading comprehension questions that test memory retention.
- Evidence anchors:
  - [abstract]: "Using GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book."
  - [section]: "Despite their tremendous successes, most large language models do not have any long-term memory mechanisms, which restricts their applications."
  - [corpus]: Weak - related papers focus on VQA and general comprehension, not long-term memory evaluation.
- Break condition: If GPT-3.5 fails to accurately capture scene content or generate coherent questions, the dataset validity would be compromised.

### Mechanism 2
- Claim: The "retention demand" metadata enables diagnostic evaluation of memory capacity.
- Mechanism: Each question includes a timestamp indicating how many scenes back the relevant information was read, allowing precise measurement of model forgetting curves.
- Core assumption: The distance between when information was presented and when it's queried directly correlates with memory capacity requirements.
- Evidence anchors:
  - [abstract]: "Crucially, most questions have a known 'retention demand', indicating how long-term of a memory is needed to answer it, which should aid long-term memory performance evaluation."
  - [section]: "Each question has a clearly defined 'memory load': how long ago was the target scene read."
  - [corpus]: Weak - related works don't discuss memory demand metadata or forgetting curve analysis.
- Break condition: If models can answer questions through inference rather than recall, the retention demand metric would lose diagnostic value.

### Mechanism 3
- Claim: The multi-format question structure (multiple-choice + free-form) provides complementary evaluation approaches.
- Mechanism: Multiple-choice questions offer controlled evaluation with clear answer boundaries, while free-form questions allow richer semantic understanding assessment.
- Core assumption: Different question formats stress different aspects of memory and comprehension, providing a more complete evaluation than either format alone.
- Evidence anchors:
  - [abstract]: "We created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as free-form narrative reconstruction questions."
  - [section]: "While our multiple-choice questions provide a controlled and interpretable way to measure memory performance, free-form answers might sometimes provide a richer learning signal to the model."
  - [corpus]: Weak - related papers don't discuss multi-format evaluation approaches for memory assessment.
- Break condition: If one format becomes significantly easier or harder than intended, it would skew the overall evaluation.

## Foundational Learning

- Concept: Scene-level summarization
  - Why needed here: The dataset relies on breaking books into manageable scene summaries that preserve narrative continuity while enabling memory evaluation.
  - Quick check question: Can you explain why scene-level rather than chapter-level summaries are used for this task?

- Concept: Named entity substitution
  - Why needed here: Randomizing character names prevents models from using prior knowledge of popular books, ensuring evaluation measures learned memory rather than pre-existing knowledge.
  - Quick check question: How does named entity substitution help address data contamination concerns?

- Concept: Retention demand calculation
  - Why needed here: Understanding how to compute and use retention demand is crucial for interpreting model performance and comparing different memory architectures.
  - Quick check question: Given a book with 100 scenes, how would you calculate the retention demand for a question asked at scene 75 about content from scene 25?

## Architecture Onboarding

- Component map: Book preprocessing pipeline -> GPT-3.5 summarization engine -> Question generation module -> Data validation framework -> Named entity substitution system

- Critical path: Book preprocessing → GPT-3.5 summarization → Question generation → Validation → Named entity substitution → Dataset export

- Design tradeoffs: Using GPT-3.5 for summarization trades computational cost for scalability, while the multiple question formats balance controlled evaluation with rich semantic assessment.

- Failure signatures: Common failure modes include summary distortion bias (GPT-3.5 consistently misrepresenting certain scene types), retention demand miscalibration (questions too easy/hard relative to stated demand), and named entity substitution errors (failed substitutions or inconsistent mappings).

- First 3 experiments:
  1. Validate question generation by testing GPT-4 on questions within its context window to confirm baseline performance
  2. Run human validation study comparing true vs. false summaries to verify summary quality
  3. Test BERT on scene distortion questions with no context to identify systematic biases in summary generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific performance gains can long-term memory transformers achieve when trained on NarrativeXL compared to existing datasets?
- Basis in paper: [inferred] The paper introduces NarrativeXL as a dataset specifically designed for training long-term memory models, suggesting it may lead to better performance.
- Why unresolved: The paper validates the dataset's quality but does not provide direct performance comparisons between models trained on NarrativeXL versus other datasets.
- What evidence would resolve it: Experiments comparing model performance on long-term memory tasks using NarrativeXL versus other datasets, showing relative improvements.

### Open Question 2
- Question: How do different long-term memory architectures perform on NarrativeXL's scene recognition and narrative reconstruction tasks?
- Basis in paper: [explicit] The paper mentions three validation experiments with existing language models but does not specify performance differences between architectures.
- Why unresolved: The paper validates data quality but doesn't systematically compare different long-term memory approaches on the dataset.
- What evidence would resolve it: Systematic evaluation of multiple long-term memory architectures (e.g., compressive transformers, updater-extractor) on both multiple-choice and free-form questions in NarrativeXL.

### Open Question 3
- Question: What is the optimal retention demand threshold for balancing task difficulty and model capability?
- Basis in paper: [explicit] The paper introduces "retention demand" as a key feature but doesn't specify optimal values.
- Why unresolved: While the dataset includes retention demand information, the paper doesn't analyze how different threshold values affect model performance or task design.
- What evidence would resolve it: Analysis of model performance curves across different retention demand thresholds, identifying points where performance drops significantly.

## Limitations

- Dataset creation relies entirely on GPT-3.5, which may introduce systematic biases in summarization and question generation that could affect dataset quality
- Human validation was limited to only 48/50 summaries, which may not be sufficient to ensure consistent quality across the entire dataset
- The paper assumes GPT-3.5's short-term memory competence translates effectively to long-term memory evaluation without empirical validation

## Confidence

- **High Confidence**: The dataset creation methodology is clearly specified and follows logical steps. The validation experiments showing baseline performance on questions within context windows are well-designed and executed.

- **Medium Confidence**: The claim that the dataset accurately represents the source material and can diagnose model memory capacity is supported by validation experiments, but the scale of human validation is limited.

- **Low Confidence**: The claim that questions are not trivial for modern language models even within their context windows is based on a single experiment with BERT, and the paper does not explore performance across different model architectures or scales.

## Next Checks

1. **Bias Analysis**: Conduct a systematic analysis of GPT-3.5's summarization patterns to identify any systematic biases in how different types of scenes (dialogue-heavy, action scenes, descriptive passages) are summarized.

2. **Cross-Model Validation**: Test the dataset with multiple language model architectures (not just BERT) including both transformer-based and non-transformer models to assess whether the dataset consistently challenges different memory architectures as claimed.

3. **Genre-Specific Performance Analysis**: Evaluate model performance across different fiction genres represented in the dataset to determine if certain narrative structures or writing styles affect the difficulty of memory tasks.