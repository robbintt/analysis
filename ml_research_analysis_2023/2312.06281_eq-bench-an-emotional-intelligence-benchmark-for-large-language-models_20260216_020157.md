---
ver: rpa2
title: 'EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models'
arxiv_id: '2312.06281'
source_url: https://arxiv.org/abs/2312.06281
tags:
- scores
- eq-bench
- score
- emotional
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EQ-Bench is a new benchmark designed to measure emotional understanding
  (EU) in large language models. It does this by asking models to rate the intensity
  of four emotions felt by characters in a dialogue.
---

# EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2312.06281
- Source URL: https://arxiv.org/abs/2312.06281
- Reference count: 7
- Primary result: A benchmark that measures emotional understanding by having models rate emotion intensity in dialogues, showing strong correlation with general intelligence benchmarks

## Executive Summary
EQ-Bench is a new benchmark designed to measure emotional understanding in large language models through dialogue-based scenarios. The benchmark asks models to rate the intensity of four emotions felt by characters in a dialogue, then normalizes these ratings to sum to 10. Results show that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (r=0.97) and Chatbot Arena ELO scores (r=0.94), suggesting emotional understanding is a good proxy for general intelligence in LLMs.

## Method Summary
EQ-Bench measures emotional understanding by presenting models with 60 dialogue scenarios depicting conflict or tension, asking them to rate four emotions from 0-10. Models use zero-shot prompting with a critique-and-revision technique to provide ratings. The benchmark normalizes scores to sum to 10, then calculates a final score by comparing model ratings to reference answers. The approach focuses on relative rather than absolute emotional intensity to reduce subjectivity.

## Key Results
- Strong correlation with MMLU (r=0.97) indicating emotional understanding aligns with general intelligence
- Strong correlation with Chatbot Arena ELO scores (r=0.94) showing alignment with human perception
- Able to differentiate a wide range of model capabilities
- Difficult to game, suggesting genuine emotional understanding rather than pattern matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EQ-Bench's focus on rating emotional intensity in conflict scenarios provides a more nuanced assessment than traditional multiple-choice formats.
- Mechanism: By requiring models to rate multiple emotions rather than select a single answer, the benchmark captures subtle gradations in emotional understanding. The dialogue-based format presents complex social dynamics that demand deeper comprehension than simple emotion recognition.
- Core assumption: Models that perform well on this task demonstrate broader cognitive capabilities that transfer to other intelligence measures.
- Evidence anchors:
  - [abstract] "We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue."
  - [section 2.2] "Aspects of these tests which are poorly suited to a LLM benchmark include: self-rating of abilities; multi-modal questions (i.e. pictures); incorporating feedback from colleagues"
  - [corpus] Weak - corpus lacks direct validation of emotional intensity rating effectiveness
- Break condition: If models learn to game the system by optimizing for specific numerical patterns rather than genuine emotional understanding.

### Mechanism 2
- Claim: Normalizing scores to sum to 10 eliminates absolute intensity bias while preserving relative emotional understanding.
- Mechanism: By focusing on the relative intensity of emotions rather than their absolute values, the benchmark removes subjective interpretation of the 0-10 scale. This normalization ensures that the assessment measures understanding of emotional relationships rather than calibration to arbitrary scales.
- Core assumption: Relative emotional intensity is a more reliable indicator of understanding than absolute intensity ratings.
- Evidence anchors:
  - [section 3.8] "To nullify the inherent subjectivity of deciding how intense an emotion ought to be on the 0-10 scale, we normalise both the subject's answers and the reference answers such that all four emotions sum to 10."
  - [section 2.6] "Removing the summation requirement... This circumvents the problem of sequential reasoning"
  - [corpus] Weak - corpus doesn't provide evidence on normalization effectiveness
- Break condition: If normalization artificially constrains model responses in ways that don't reflect genuine understanding.

### Mechanism 3
- Claim: Strong correlation with MMLU (r=0.97) indicates EQ-Bench measures broad intelligence, not just emotional understanding.
- Mechanism: The high correlation suggests that emotional understanding is deeply intertwined with general cognitive abilities in LLMs. Since all models share similar transformer architectures and are trained on broad datasets, performance on emotional tasks likely reflects general reasoning capabilities.
- Core assumption: Emotional understanding and general intelligence are closely coupled in transformer-based LLMs due to shared underlying cognitive mechanisms.
- Evidence anchors:
  - [abstract] "We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97)"
  - [section 2.4] "We suggest that it is reasonable to expect that LLM performance on EI tests may scale closely with benchmarks that assess broad intelligence"
  - [corpus] Moderate - corpus shows related work on EI benchmarks but lacks direct validation of this specific correlation claim
- Break condition: If the correlation is driven by models overfitting to specific patterns in both benchmarks rather than genuine capability transfer.

## Foundational Learning

- Concept: Emotional intelligence as a multi-branch construct
  - Why needed here: Understanding the theoretical framework helps interpret why EQ-Bench focuses specifically on emotional understanding rather than broader EI
  - Quick check question: What are the four branches of emotional intelligence according to Mayer & Salovey (1997)?

- Concept: Benchmark design tradeoffs
  - Why needed here: The paper makes specific design choices (dialogue format, normalization, emotion selection) that require understanding of benchmarking principles
  - Quick check question: Why might multiple-choice formats be less effective than rating scales for assessing emotional understanding?

- Concept: Correlation interpretation in benchmark validation
  - Why needed here: The paper relies heavily on correlation with other benchmarks to validate its approach, requiring understanding of what correlations actually demonstrate
  - Quick check question: What does a correlation coefficient of 0.97 between EQ-Bench and MMLU actually tell us about the relationship between emotional understanding and general intelligence?

## Architecture Onboarding

- Component map: Dialogue generation (GPT-4) -> Question parsing -> Emotion rating -> Normalization -> Score calculation -> Result aggregation
- Critical path: Dialogue generation → Question parsing → Emotion rating → Normalization → Score calculation → Result aggregation
- Design tradeoffs: Question complexity vs. computational efficiency, normalization vs. preserving absolute intensity information, zero-shot prompting vs. chain-of-thought prompting
- Failure signatures: Low parse rates (>10% unparseable answers), high variance between benchmark runs (>5% CV), poor correlation with established benchmarks (<0.7 Pearson)
- First 3 experiments:
  1. Run EQ-Bench on a small set of models (3-5) to verify basic functionality and check parse rates
  2. Test normalization procedure by creating synthetic answers with known relationships
  3. Compare zero-shot vs. few-shot prompting to assess impact on scores and parse rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design questions to further extend the upper range of emotional understanding that the benchmark can measure as model capabilities improve?
- Basis in paper: [inferred] The authors note that "we expect scores to compress as model capabilities approach the upper limit that the test is able to measure" and suggest that "in order to increase the ceiling of EU that the benchmark is able to measure, we may in future work employ experts in EI to craft more complex questions."
- Why unresolved: The current question set was designed by the paper's authors, who acknowledge that their ability to create challenging questions may limit the benchmark's upper range. Employing domain experts could potentially create more complex scenarios, but the specific methods for doing so are not yet defined.
- What evidence would resolve it: Testing the benchmark with questions designed by emotional intelligence experts and comparing the score distributions to the current results. Evidence of a wider spread of scores at the upper end would indicate success in extending the benchmark's ceiling.

### Open Question 2
- Question: How does the choice of dialogue generation method (GPT-4 vs. human writers) impact the benchmark's effectiveness and potential biases?
- Basis in paper: [explicit] The authors state "While the benchmark reference answers were determined by the authors of this paper, all dialogues were generated by GPT-4. The benchmark could be improved by employing human writers to create the dialogues, eliminating a potential source of bias and improving the emotional depth and complexity of the scenes."
- Why unresolved: The impact of using GPT-4-generated dialogues versus human-written ones on the benchmark's results and potential biases has not been tested. It's unclear whether human-written dialogues would lead to more nuanced scenarios or introduce different types of biases.
- What evidence would resolve it: Comparing EQ-Bench scores and score distributions when using human-written dialogues versus GPT-4-generated ones. Analysis of potential biases introduced by each method and their impact on model differentiation would provide insights into the optimal approach.

### Open Question 3
- Question: What is the effect of model size and architecture on emotional understanding as measured by EQ-Bench, beyond simple parameter count comparisons?
- Basis in paper: [inferred] The paper compares models of different sizes (7B vs 70B parameters) and notes differences in scores, but doesn't delve into how architecture or training methods might influence emotional understanding.
- Why unresolved: While the paper establishes correlations between model size and EQ-Bench scores, it doesn't explore how different architectural choices or training approaches might impact emotional understanding capabilities.
- What evidence would resolve it: Systematic testing of models with similar parameter counts but different architectures or training methods, comparing their EQ-Bench scores and analyzing performance across different types of emotional scenarios. This could reveal whether certain architectural features or training approaches are particularly beneficial for emotional understanding.

## Limitations
- High correlation with MMLU (r=0.97) raises questions about whether EQ-Bench measures emotional understanding specifically or general intelligence
- Reliance on GPT-4 for question generation may introduce circularity and potential bias
- Normalization procedure may mask important differences in how models calibrate emotional intensity
- Limited question set (60 questions) may not capture the full complexity of emotional understanding

## Confidence
- High Confidence: The benchmark's design and implementation details, including the prompt format, scoring mechanism, and correlation with Chatbot Arena ELO scores (r=0.94)
- Medium Confidence: The claim that EQ-Bench measures emotional understanding specifically rather than general intelligence, given the extremely high MMLU correlation
- Medium Confidence: The assertion that EQ-Bench is difficult to game, based on correlation patterns and lack of obvious optimization strategies
- Low Confidence: The claim that EQ-Bench is a better proxy for general intelligence than comprehensive multi-domain benchmarks like MMLU

## Next Checks
1. **Construct Validity Test**: Run EQ-Bench alongside a battery of emotion-specific tests (like EmoBench or EmotionQueen) to determine whether it captures unique variance in emotional understanding beyond general intelligence measures.

2. **Cross-Generation Validation**: Evaluate models from different architectural families (not just transformer-based LLMs) on EQ-Bench to test whether the strong correlation with MMLU is specific to the current LLM paradigm or generalizes across AI systems.

3. **Adversarial Benchmark Design**: Create a modified version of EQ-Bench where questions are generated by a different model family or human experts, then compare performance and correlations to test for potential bias from using GPT-4 for question generation.