---
ver: rpa2
title: The potential of LLMs for coding with low-resource and domain-specific programming
  languages
arxiv_id: '2307.13018'
source_url: https://arxiv.org/abs/2307.13018
tags:
- function
- matrix
- code
- gretl
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of large language models
  (LLMs) to the low-resource and domain-specific programming language hansl used in
  the econometrics software gretl. The research explores whether LLMs can effectively
  assist with tasks such as code documentation, explanation, variable naming, code
  generation, and refactoring.
---

# The potential of LLMs for coding with low-resource and domain-specific programming languages

## Quick Facts
- arXiv ID: 2307.13018
- Source URL: https://arxiv.org/abs/2307.13018
- Reference count: 5
- Key outcome: LLMs can successfully perform code documentation, explanation, variable naming, code generation, and refactoring for hansl (low-resource econometrics DSL) with mostly accurate results

## Executive Summary
This study investigates the application of large language models (LLMs) to hansl, a low-resource domain-specific programming language used in gretl econometrics software. The research demonstrates that LLMs can effectively assist with multiple software development tasks including generating descriptive docstrings, translating docstrings into executable code, providing precise explanations of complex econometric functions, and suggesting improved variable and function names. The LLM successfully refactored code to enhance readability and maintainability, showing promise for reducing entry barriers in specialized domains. However, limitations were identified including inability to write accurate unit tests and occasional failures in code improvement tasks.

## Method Summary
The study used a proprietary GPT-3.5-based LLM (from you.com) to perform various coding tasks on hansl code snippets, including docstring generation, code explanation, variable naming, code generation from descriptions, and refactoring. The method involved systematic testing of LLM capabilities through iterative prompting, with outputs evaluated for accuracy and syntactical correctness. The evaluation process relied on manual review to identify successes and limitations, though specific hansl datasets and detailed prompt specifications were not provided.

## Key Results
- LLMs successfully generated accurate docstrings for hansl functions and provided precise explanations of complex econometric code
- The model demonstrated code generation capabilities from docstrings, producing functional hansl code with minor syntactical errors
- Variable naming and code refactoring tasks were completed successfully, improving code readability and maintainability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generalize well to low-resource domain-specific languages even without explicit training on them.
- Mechanism: The model leverages multilingual pretraining and transfer learning, allowing it to recognize structural and semantic patterns across programming languages.
- Core assumption: Core syntactic and semantic structures of programming languages share enough commonality for cross-lingual transfer to be effective.
- Evidence anchors:
  - [abstract] "This indicates that publicly existing models are able to generalize well to low-resource programming languages (Zügner et al., 2021)."
  - [section] "Despite these limitations, we demonstrate that LLMs can still be applied successfully to certain coding tasks using gretl."
- Break Condition: If the domain-specific language has fundamentally different paradigms or relies on domain-specific abstractions not present in training data.

### Mechanism 2
- Claim: LLMs can assist in code understanding tasks such as documentation generation and explanation, even for poorly documented code.
- Mechanism: The model uses its understanding of natural language and programming patterns to infer intent from code structure and generate human-readable explanations.
- Core assumption: Code patterns and naming conventions provide sufficient context for the model to infer functionality accurately.
- Evidence anchors:
  - [abstract] "LLMs can be a useful tool for writing, understanding, improving, and documenting gretl code, which includes generating descriptive docstrings for functions and providing precise explanations for abstract and poorly documented econometric code."
  - [section] "The LLM correctly identifies the function as executing a kernel regression... Despite the non-meaningful and single-letter naming of the parameters, the LLM still manages to accurately describe them."
- Break Condition: If code is highly obfuscated or uses unconventional patterns.

### Mechanism 3
- Claim: LLMs can generate functional code from natural language descriptions (docstrings) for low-resource languages.
- Mechanism: The model maps natural language intent to code structure by leveraging its pretraining on code-natural language pairs, even when the target language is underrepresented.
- Core assumption: Natural language descriptions contain sufficient information to reconstruct the algorithmic intent.
- Evidence anchors:
  - [abstract] "While the LLM showcased promoting docstring-to-code translation capability..."
  - [section] "We show that the LLM also writes gretl code for computing the Fibonacci sequence or the root mean squared error including only minor syntactical errors."
- Break Condition: If the docstring is ambiguous or requires deep domain expertise not captured in training.

## Foundational Learning

- Concept: Understanding of transformer architecture and self-attention mechanisms
  - Why needed here: The paper relies on GPT-3.5's ability to process and generate code through attention-based modeling
  - Quick check question: What is the primary innovation of transformer models that makes them effective for sequence-to-sequence tasks?

- Concept: Domain-specific language (DSL) characteristics and challenges
  - Why needed here: Gretl/hansl is a DSL for econometrics with specialized functions and limited public code availability
  - Quick check question: How does the scarcity of training data for DSLs typically impact LLM performance compared to general-purpose languages?

- Concept: Code documentation standards (docstrings) and their role in software development
  - Why needed here: The study heavily relies on generating and interpreting docstrings for code understanding and generation tasks
  - Quick check question: What are the key elements that make a docstring effective for both human readers and automated processing?

## Architecture Onboarding

- Component map: Input prompts -> GPT-3.5 transformer processing -> Code/explanation generation -> Manual validation -> Iterative refinement
- Critical path: Prompt → LLM processing → Output generation → Manual validation → Iteration
- Design tradeoffs: Using a general-purpose LLM vs fine-tuning on hansl-specific data; tradeoff between output creativity and adherence to hansl syntax
- Failure signatures: Syntax errors in generated code, incorrect understanding of domain-specific functions, overconfidence in explanations for complex econometric concepts
- First 3 experiments:
  1. Test docstring generation for a simple hansl function with clear inputs/outputs
  2. Evaluate code explanation capabilities for a moderately complex function with linear algebra
  3. Assess code generation from docstring for a basic statistical calculation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of fine-tuning LLMs specifically on low-resource domain-specific programming languages like hansl?
- Basis in paper: [explicit] The paper mentions that future research could explore ways to fine-tune LLM models for gretl code.
- Why unresolved: The paper does not provide any empirical evidence or results from fine-tuning LLMs on hansl code.
- What evidence would resolve it: Empirical studies comparing the performance of fine-tuned LLMs versus general LLMs on tasks such as code generation, documentation, and explanation for hansl code.

### Open Question 2
- Question: How effective are LLMs in detecting and correcting errors in low-resource programming language code?
- Basis in paper: [inferred] The paper notes that LLMs may not always be successful in improving code and failed to write a correct unit test, suggesting limitations in error detection and correction.
- Why unresolved: The paper does not provide detailed analysis or results on the LLM's ability to detect and correct errors in hansl code.
- What evidence would resolve it: Empirical studies measuring the accuracy of LLMs in identifying and fixing errors in hansl code, compared to human programmers.

### Open Question 3
- Question: Can LLMs effectively translate code from high-resource general-purpose programming languages to low-resource domain-specific languages like hansl?
- Basis in paper: [explicit] The paper mentions that translating code from another language into gretl is a topic under active research.
- Why unresolved: The paper does not provide any results or analysis on the effectiveness of LLMs in translating code between languages.
- What evidence would resolve it: Empirical studies evaluating the accuracy and efficiency of LLMs in translating code from languages like Python or R to hansl, including the quality of the translated code and the preservation of functionality.

## Limitations
- Evaluation relies on proprietary LLM interface (you.com) without open-source alternatives, making exact replication challenging
- Specific hansl dataset and detailed prompt specifications are not publicly available, limiting reproducibility
- The study acknowledges failures in writing accurate unit tests and improving certain code sections, suggesting incomplete coverage of software development tasks

## Confidence
- **High confidence**: LLMs can generate accurate docstrings and provide precise explanations of hansl functions
- **Medium confidence**: LLMs can generate functional code from natural language descriptions for hansl
- **Medium confidence**: LLMs can suggest improved variable and function names

## Next Checks
1. **Syntax Error Analysis**: Systematically catalog and categorize all syntactical errors produced when generating hansl code from docstrings across 50+ diverse functions to quantify error rates and patterns.

2. **Domain Expert Review**: Have econometrics domain experts evaluate the precision of LLM-generated explanations for complex statistical functions (e.g., kernel regression, maximum likelihood estimation) to validate claims of "precise explanations."

3. **Cross-LLM Comparison**: Repeat the same hansl coding tasks using both proprietary (GPT-3.5) and open-source LLMs (CodeLlama, StarCoder) to determine whether observed capabilities are model-specific or generalizable across architectures.