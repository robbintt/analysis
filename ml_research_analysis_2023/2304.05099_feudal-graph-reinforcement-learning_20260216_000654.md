---
ver: rpa2
title: Feudal Graph Reinforcement Learning
arxiv_id: '2304.05099'
source_url: https://arxiv.org/abs/2304.05099
tags:
- learning
- graph
- each
- hierarchy
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Feudal Graph Reinforcement Learning (FGRL),
  a framework for learning hierarchical, graph-based policies in reinforcement learning.
  The core idea is to represent an agent's morphology as a graph and introduce a hierarchy
  of policies where high-level decisions are propagated down through a layered graph
  structure.
---

# Feudal Graph Reinforcement Learning

## Quick Facts
- arXiv ID: 2304.05099
- Source URL: https://arxiv.org/abs/2304.05099
- Reference count: 38
- One-line primary result: FGRL achieves NMI 0.64 on Cora dataset and competitive performance on MuJoCo locomotion tasks with hierarchical graph-based policies.

## Executive Summary
Feudal Graph Reinforcement Learning (FGRL) introduces a hierarchical framework for learning graph-based policies in reinforcement learning. The method represents agent morphology as a graph and introduces a layered hierarchy where high-level decisions propagate down through sub-managers to worker nodes. This enables task decomposition and improves coordination compared to flat graph neural network approaches. FGRL was evaluated on MuJoCo locomotion tasks and graph clustering, showing competitive performance and improved sample efficiency in some environments.

## Method Summary
FGRL combines hierarchical reinforcement learning with graph neural networks to create modular policies for graph-based control of physical agents. The method extracts graph representations from agent morphologies and implements a 2-level hierarchy with a manager, sub-managers, and workers. Message-passing GNNs propagate information between same-level nodes and across hierarchical levels, while MLPs generate goals and actions. The framework is trained using CMA-ES optimization on MuJoCo environments, with evaluation on both locomotion tasks and graph clustering problems.

## Key Results
- Achieved NMI score of 0.64 on Cora graph clustering dataset
- Competitive performance on MuJoCo locomotion tasks compared to DeepSetMLP and SMP baselines
- Demonstrated improvements in learning stability and sample efficiency in certain environments
- Showed potential for transferring learned behaviors across different agent morphologies

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical message-passing mitigates local information bottlenecks by allowing high-level decisions to propagate from manager → sub-managers → workers, enabling each layer to process abstracted information relevant to its scale rather than forcing low-level nodes to attend to both actuator control and global coordination simultaneously.

### Mechanism 2
Feudal-style goal decomposition enables task abstraction and temporal credit assignment by having managers set high-level goals that are recursively decomposed into lower-level subgoals, enabling temporal abstraction and localized credit assignment without requiring full state access at every level.

### Mechanism 3
Graph-based modular policy sharing enables zero-shot transfer across morphologically similar agents by allowing worker policies to be directly applied to agents with similar but not identical morphologies through simple graph topology adjustment and node re-mapping.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire RL formulation depends on defining states, actions, rewards, and transition dynamics as an MDP.
  - Quick check question: What tuple defines an MDP in the paper, and how are state representations structured for graph-based agents?

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The policy architecture is built on GNNs where each node aggregates messages from neighbors to update its representation.
  - Quick check question: In the propagation equation (3), what are the two types of message functions φ₁ and φ₂ used for?

- Concept: Feudal Reinforcement Learning (FRL)
  - Why needed here: FGRL is explicitly built on FRL's hierarchical control paradigm where higher-level agents set goals for lower-level agents.
  - Quick check question: How does the manager in FGRL differ from sub-managers in terms of reward access and goal-setting authority?

## Architecture Onboarding

- Component map:
  Manager → Sub-managers → Workers → Environment
  (Hierarchical goal propagation with message-passing at each level)

- Critical path:
  Manager → goal → sub-managers → goal → workers → action → environment → reward → propagate back up

- Design tradeoffs:
  - Hierarchical depth vs. training complexity: Deeper hierarchies allow finer-grained goals but increase training difficulty
  - Message-passing rounds vs. coordination quality: More rounds improve coordination but add computational cost
  - Graph pooling method vs. transferability: Different clustering strategies affect how well policies transfer across morphologies

- Failure signatures:
  - Degenerate reward maximization: Agents learn to set trivial goals that maximize synthetic rewards without solving the task
  - Information bottleneck: Upper layers cannot propagate meaningful goals due to poor message-passing design
  - Transfer collapse: Policies fail when applied to morphologies with significantly different graph structures

- First 3 experiments:
  1. Train on standard Walker2D, evaluate transfer to Walker2D variants with missing limbs
  2. Train on Humanoid, evaluate transfer to Humanoid variants with different arm configurations
  3. Train on Snake (modified Swimmer), evaluate transfer across different numbers of limbs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the depth of the hierarchy beyond the 2-level structure affect the performance of FGRL in complex environments?
- Basis in paper: The paper mentions that future works will address more complex learning architectures with deeper hierarchies.
- Why unresolved: The current study only evaluates a 2-level hierarchy, leaving the impact of deeper hierarchies unexplored.
- What evidence would resolve it: Empirical results comparing FGRL with varying hierarchy depths on complex tasks, showing performance trends and trade-offs.

### Open Question 2
- Question: What are the specific challenges in transferring learned policies from agents with different morphologies, and how can FGRL address these challenges?
- Basis in paper: The paper conducts zero-shot transfer learning experiments but acknowledges that all methods struggle with certain morphologies, such as the 'Hop-3' agent.
- Why unresolved: The experiments show limitations in transfer learning, but the underlying causes and potential solutions are not fully explored.
- What evidence would resolve it: Detailed analysis of transfer failures and successes, identifying key factors that influence transfer performance, and testing FGRL's ability to adapt to diverse morphologies.

### Open Question 3
- Question: How does the choice of aggregation functions and message-passing mechanisms in FGRL impact its performance and stability?
- Basis in paper: The paper uses specific aggregation functions and message-passing mechanisms but does not explore alternative choices.
- Why unresolved: The impact of different aggregation and message-passing strategies on FGRL's effectiveness is not investigated.
- What evidence would resolve it: Comparative studies using various aggregation functions and message-passing mechanisms, demonstrating their effects on learning efficiency and policy quality.

## Limitations

- Cora graph clustering results (NMI 0.64) appear without proper baselines or statistical significance testing
- Transfer learning capabilities remain largely theoretical with limited empirical validation across substantially different morphologies
- Absence of comparison to modern graph-based RL methods limits interpretability of MuJoCo results

## Confidence

**High confidence**: The core architectural contribution (hierarchical GNN with message passing) is technically sound and builds on well-established feudal RL principles.

**Medium confidence**: The MuJoCo performance claims are reasonable given the competitive results shown, but the absence of modern graph RL baselines limits interpretability.

**Low confidence**: The transfer learning capabilities remain largely theoretical with insufficient empirical evidence of successful transfer between substantially different agent types.

## Next Checks

1. **Transfer Validation**: Test FGRL policies trained on Walker2D on modified morphologies with missing or additional limbs to verify actual transfer capability beyond theoretical claims.

2. **Hierarchical Learning Analysis**: Conduct ablation studies removing upper hierarchy levels to determine whether high-level managers learn meaningful abstractions or merely pass through received goals.

3. **Reward Function Design**: Systematically vary goal reward functions (e.g., from identity to threshold-based) to identify conditions that prevent degenerate goal-setting behavior where agents maximize synthetic rewards without solving tasks.