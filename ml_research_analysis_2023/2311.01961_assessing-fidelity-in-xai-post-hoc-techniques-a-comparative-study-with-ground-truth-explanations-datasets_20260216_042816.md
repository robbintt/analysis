---
ver: rpa2
title: 'Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground
  Truth Explanations Datasets'
arxiv_id: '2311.01961'
source_url: https://arxiv.org/abs/2311.01961
tags:
- methods
- dataset
- explanations
- proposed
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the fidelity of thirteen post-hoc eXplainable
  Artificial Intelligence (XAI) methods by introducing three novel image datasets
  with reliable ground truth for explanations. The authors generate synthetic datasets
  using a texture-based approach, allowing for objective comparison of XAI methods.
---

# Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets

## Quick Facts
- arXiv ID: 2311.01961
- Source URL: https://arxiv.org/abs/2311.01961
- Reference count: 40
- Key outcome: Backpropagation-based XAI methods (GBP, LRP, DeepLIFT, Integrated Gradients) achieve higher fidelity than sensitivity analysis or CAM methods on synthetic datasets with ground truth

## Executive Summary
This study evaluates the fidelity of thirteen post-hoc XAI methods using three novel synthetic image datasets with reliable ground truth explanations. The authors introduce a texture-based approach for generating synthetic datasets that enables objective comparison of XAI methods. Results show that backpropagation-based methods consistently outperform sensitivity analysis and CAM-based methods in terms of fidelity to ground truth, though they produce noisier saliency maps. The study provides a framework for fair and objective evaluation of XAI techniques, enabling the elimination of unreliable explanations.

## Method Summary
The study generates three synthetic image datasets (TXUXIv1, TXUXIv2, TXUXIv3) using a texture-based approach where objects are placed on textured backgrounds. A CNN model is trained on each dataset to achieve near-perfect accuracy. Thirteen XAI methods are then applied to explain the model's predictions, and their outputs are compared against ground truth explanations using Earth Mover Distance (EMD) and Similarity Metric (MIN). The comparison includes methods like LIME, SHAP, RISE, GradCAM variants, SIDU, and backpropagation-based methods (GBP, LRP, DeepLIFT, Integrated Gradients, SmoothGrad).

## Key Results
- Backpropagation-based methods (GBP, LRP, DeepLIFT, Integrated Gradients) consistently achieve higher fidelity to ground truth than sensitivity analysis or CAM methods
- Sensitivity analysis methods (LIME, SHAP, RISE) perform poorly due to generation of out-of-distribution samples
- CAM methods show misaligned explanations due to upsampling operations, resulting in lower fidelity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backpropagation-based XAI methods achieve higher fidelity than sensitivity analysis or CAM methods on synthetic datasets with ground truth.
- Mechanism: These methods propagate the output information backward through the network layers to assign importance scores to input pixels, capturing the model's internal reasoning more directly.
- Core assumption: The ground truth explanations generated from texture-based datasets accurately represent the model's decision process.
- Evidence anchors:
  - [abstract] "Our results demonstrate that XAI methods based on the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on sensitivity analysis or Class Activation Maps (CAM)."
  - [section] "In Figure 6 we can see an example of the results obtained for the second experiment. The results are consistent with the metrics, with the back propagation-based methods producing the best results, while the sensitivity analysis based methods (RISE [27], LIME [29], SHAP [20], SIDU [26], and ScoreCAM [42]) the worst ones."
  - [corpus] Weak corpus evidence - no direct matches found for backpropagation fidelity claims.
- Break condition: If the synthetic datasets do not accurately represent real-world model behavior, or if backpropagation methods fail to capture non-linear interactions properly.

### Mechanism 2
- Claim: Sensitivity analysis methods perform poorly due to generation of out-of-distribution (OOD) samples.
- Mechanism: These methods perturb inputs to measure changes in output, but the perturbations create data points that fall outside the training distribution, leading to unreliable explanations.
- Core assumption: The synthetic datasets are sufficiently different from training data that perturbation-based methods generate OOD samples.
- Evidence anchors:
  - [abstract] "However, all these metrics tend to generate out-of-domain (OOD) samples [28, 14], and this is the factor why these metrics are unreliable [40]."
  - [section] "We can also see that all the good methods had small dispersions, indicated by the standard deviation, while some methods with bad results also had large dispersions, as LIME [29] and SHAP [20]."
  - [corpus] No direct corpus evidence for OOD generation claims.
- Break condition: If sensitivity analysis methods can be tuned to avoid OOD generation, or if the synthetic datasets are too simple to trigger OOD issues.

### Mechanism 3
- Claim: CAM methods have misaligned explanations due to the upsampling operation.
- Mechanism: CAM methods use upsampling to map low-resolution activation maps to input resolution, but this process can misalign important regions with their true locations.
- Core assumption: The misalignment is significant enough to affect explanation quality metrics.
- Evidence anchors:
  - [section] "The CAM methods [34, 10], also had bad results cause by the misalignment of the explanations that generate the UpSampling operation, as studied and demonstrated by Xia et al. [43]."
  - [corpus] No corpus evidence for CAM misalignment claims.
- Break condition: If upsampling methods are improved to preserve spatial relationships, or if the effect is dataset-dependent.

## Foundational Learning

- Concept: Synthetic dataset generation with ground truth
  - Why needed here: Provides objective evaluation without human bias, essential for comparing XAI methods
  - Quick check question: How does the texture-based approach ensure ground truth explanations are reliable?

- Concept: Fidelity metrics (EMD and MIN)
  - Why needed here: Quantifies how well explanations match ground truth, enabling objective comparison
  - Quick check question: Why do EMD and MIN metrics sometimes produce conflicting results?

- Concept: Backpropagation vs sensitivity analysis
  - Why needed here: Understanding the fundamental difference in how methods capture model behavior
  - Quick check question: What makes backpropagation methods more sensitive to noise than sensitivity analysis methods?

## Architecture Onboarding

- Component map: Dataset generator -> CNN model -> 13 XAI methods -> Fidelity metrics -> Results analysis
- Critical path: Ground truth generation -> Model training -> Explanation extraction -> Metric calculation -> Comparison
- Design tradeoffs: Simple synthetic datasets vs realistic complexity, computational cost vs comprehensive comparison
- Failure signatures: High standard deviation across metrics, inconsistent rankings between EMD and MIN, poor performance on multiple datasets
- First 3 experiments:
  1. Run all 13 methods on TXUXIv1 with EMD and MIN metrics to establish baseline performance
  2. Compare backpropagation vs non-backpropagation methods to identify performance gaps
  3. Analyze noise patterns in explanations using visual inspection of saliency maps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do backpropagation-based XAI methods perform on real-world datasets with more complex backgrounds and multiple objects compared to the TXUXI datasets used in this study?
- Basis in paper: [inferred] The study used synthetic datasets with ground truth explanations to compare XAI methods. While the TXUXI datasets introduced more complexity than previous synthetic datasets, they still have limitations in representing real-world scenarios.
- Why unresolved: The study focused on synthetic datasets to ensure ground truth explanations. Evaluating XAI methods on real-world datasets would provide a more comprehensive assessment of their performance in practical applications.
- What evidence would resolve it: Conducting experiments using XAI methods on real-world datasets with complex backgrounds and multiple objects, comparing their performance to the results obtained from the TXUXI datasets.

### Open Question 2
- Question: Can the noise generated by backpropagation-based XAI methods be reduced without compromising their fidelity to the underlying model?
- Basis in paper: [explicit] The study found that backpropagation-based methods (GBP, LRP, DeepLIFT, Integrated Gradients) yielded higher accuracy and reliability but tended to generate more noisy saliency maps.
- Why unresolved: While backpropagation methods showed good fidelity, the noise in their explanations could affect user trust and interpretability. Finding ways to reduce this noise without sacrificing fidelity is an important challenge.
- What evidence would resolve it: Developing and evaluating new techniques or modifications to existing backpropagation methods that reduce noise in the saliency maps while maintaining or improving their fidelity to the underlying model.

### Open Question 3
- Question: How do the findings of this study generalize to other types of neural network architectures beyond the CNN used in the experiments?
- Basis in paper: [inferred] The study used a CNN as the underlying model for generating explanations. However, the performance of XAI methods may vary depending on the architecture of the neural network.
- Why unresolved: The study's results are specific to the CNN architecture used. To draw more general conclusions about the performance of XAI methods, it is necessary to evaluate them on different neural network architectures.
- What evidence would resolve it: Conducting experiments using XAI methods on various neural network architectures, such as recurrent neural networks (RNNs), transformers, or graph neural networks, and comparing their performance to the results obtained from the CNN in this study.

## Limitations
- Synthetic datasets may not capture the complexity of real-world scenarios with non-linear interactions and hierarchical feature representations
- Lack of corpus evidence for several core claims reduces confidence in mechanistic explanations
- Does not address potential adversarial attacks on XAI methods or impact of different network architectures

## Confidence
- **High confidence**: Comparative ranking of XAI methods using EMD and MIN metrics on synthetic datasets
- **Medium confidence**: Backpropagation methods outperform sensitivity analysis methods
- **Low confidence**: Mechanistic explanations for method performance differences (OOD generation, CAM misalignment)

## Next Checks
1. **Cross-architecture validation**: Test the same methodology on ResNet and Vision Transformer architectures to verify if the ranking holds across different network types.
2. **Real-world dataset validation**: Apply the best-performing methods from synthetic tests to real datasets with human-annotated ground truth to assess ecological validity.
3. **Adversarial robustness test**: Evaluate how well the top-performing XAI methods maintain fidelity under adversarial perturbations of the input images.