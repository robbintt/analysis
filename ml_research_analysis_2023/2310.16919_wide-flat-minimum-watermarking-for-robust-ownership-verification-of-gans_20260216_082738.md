---
ver: rpa2
title: Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs
arxiv_id: '2310.16919'
source_url: https://arxiv.org/abs/2310.16919
tags:
- watermark
- watermarking
- image
- loss
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a robust, box-free, multi-bit GAN watermarking
  method for intellectual property protection. The method embeds a watermark by adding
  a watermarking loss term during GAN training, ensuring the watermark is present
  in generated images and retrievable by a pre-trained decoder.
---

# Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs

## Quick Facts
- arXiv ID: 2310.16919
- Source URL: https://arxiv.org/abs/2310.16919
- Reference count: 39
- Primary result: Robust, box-free multi-bit GAN watermarking method using wide flat minimum training for IPR protection

## Executive Summary
This paper presents a novel GAN watermarking method that embeds multi-bit watermarks during training by adding a watermarking loss term. The key innovation is enforcing convergence to a wide flat minimum of the watermarking loss, which makes the watermark robust to parameter modifications from model-level attacks like pruning and fine-tuning. The method uses a frozen pre-trained decoder and includes a noise layer to simulate image processing attacks during training, achieving negligible impact on image quality while demonstrating superior robustness.

## Method Summary
The method embeds watermarks by jointly optimizing GAN training with a watermarking loss term. During training, random noise is added to generator parameters, and the watermarking loss is computed across multiple perturbed versions. The averaged gradient encourages the generator to find a wide flat minimum where the watermarking loss remains approximately constant despite parameter modifications. A noise layer between generated images and the decoder simulates image processing attacks, while the decoder itself is pre-trained and frozen during GAN training.

## Key Results
- Bit accuracy (Bit Acc) remains high after model modification attacks (pruning, quantization, fine-tuning)
- Image quality metrics (FID, PSNR) show negligible degradation after watermark embedding
- Method demonstrates superior robustness compared to baseline watermarking techniques
- Architecture- and dataset-agnostic approach works across multiple GAN models and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding random noise vectors to generator parameters forces convergence to a wide flat minimum of the watermarking loss.
- Mechanism: During training, multiple perturbed versions of generator parameters are sampled, and the watermarking loss is computed across all perturbed versions. The averaged gradient update encourages the generator to find a region where the watermarking loss is locally flat.
- Core assumption: The watermarking loss surface has sufficient redundancy and dimensionality to allow for a flat region that maintains high watermark accuracy.
- Break condition: If perturbation range `b` is too large, the minimum becomes too wide and model performance degrades significantly.

### Mechanism 2
- Claim: The watermarking loss gradient averaged over perturbed parameters increases robustness to model-level attacks.
- Mechanism: By forcing the generator to find a flat minimum, small or moderate parameter changes from pruning, quantization, etc. do not substantially increase the watermarking loss, preserving watermark accuracy.
- Core assumption: Model-level attacks induce parameter changes smaller than the width of the flat minimum.
- Break condition: If an attack causes parameter changes larger than the flat region width, watermark accuracy will drop sharply.

### Mechanism 3
- Claim: The noise layer in the watermarking loss simulates image processing attacks during training.
- Mechanism: A noise layer is added between generated images and the watermark decoder during training, simulating operations like Gaussian noise, blurring, and JPEG compression. The generator learns to embed watermarks that survive these simulated attacks.
- Core assumption: Simulated image processing during training generalizes to unseen image processing operations at inference time.
- Break condition: If simulated attacks during training don't cover real attack types or intensities, the watermark may fail to survive.

## Foundational Learning

- Concept: Hypothesis testing for watermark verification
  - Why needed here: To set detection thresholds and evaluate missed detection probability under different watermark accuracies.
  - Quick check question: If the watermark has 100 bits and the threshold is set for Pf=10^-4, how many bits must match the expected watermark to verify ownership?

- Concept: Wasserstein GAN training and adversarial loss
  - Why needed here: Understanding the baseline GAN training loss is essential for integrating the watermarking loss term without degrading image quality.
  - Quick check question: What is the role of the discriminator in the adversarial loss, and how does it differ from the watermark decoder's role?

- Concept: Neural network loss landscape and flat minima
  - Why needed here: The core innovation relies on finding a flat minimum of the watermarking loss, so understanding loss landscapes is critical.
  - Quick check question: How does the width of a flat minimum affect generalization and robustness to parameter perturbations?

## Architecture Onboarding

- Component map: Watermark encoder -> Watermark decoder (frozen) -> Noise layer -> Generator -> Discriminator
- Critical path: Train encoder/decoder → Freeze decoder → Train GAN with combined loss → Apply noise layer → Sample perturbed parameters → Update generator
- Design tradeoffs:
  - Larger perturbation range `b` increases robustness but may hurt image quality
  - More noise sampling iterations `k` improves flat minimum search but increases training cost
  - Balancing adversarial and watermarking loss weights is critical to avoid degrading image quality
- Failure signatures:
  - High FID after watermark embedding indicates poor image quality
  - Sharp drop in Bit Acc after model-level attacks indicates insufficient flat minimum width
  - Low Bit Acc even before attacks indicates poor watermark embedding
- First 3 experiments:
  1. Train encoder/decoder on CIFAR-10 and verify watermark extraction accuracy
  2. Train DCGAN on MNIST with only watermarking loss, measure Bit Acc and FID
  3. Apply pruning and fine-tuning attacks to watermarked GAN, measure Bit Acc and image quality

## Open Questions the Paper Calls Out

- Question: How does the choice of noise layer parameters (e.g., noise range b, number of perturbations k) affect the robustness of the watermark against model-level attacks?
- Question: Can the proposed method be extended to protect the intellectual property of diffusion models?
- Question: How does the proposed method perform against more sophisticated watermark removal attacks, such as adversarial training or model inversion attacks?

## Limitations

- Method effectiveness highly sensitive to perturbation range parameter `b`
- Limited evaluation against sophisticated adversarial watermark removal techniques
- Computational overhead from multiple perturbed parameter evaluations during training

## Confidence

- High confidence: Watermark embedding mechanism and negligible impact on image quality
- Medium confidence: Wide flat minimum improves robustness to model modifications
- Medium confidence: Box-free watermarking being more practical

## Next Checks

1. Test robustness against adaptive attacks specifically designed to target the watermarking loss surface
2. Evaluate performance across a wider range of image processing operations not included in training noise layer
3. Conduct systematic hyperparameter sensitivity analysis varying `b` and `k` across different GAN architectures