---
ver: rpa2
title: Negotiated Representations to Prevent Forgetting in Machine Learning Applications
arxiv_id: '2312.00237'
source_url: https://arxiv.org/abs/2312.00237
tags:
- learning
- tasks
- task
- class
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel approach to prevent catastrophic forgetting
  in class incremental continual learning by introducing negotiated representations.
  The method involves creating vector representations with constant Hamming distance
  between classes and employing a negotiation mechanism to balance learning new tasks
  while retaining previous knowledge.
---

# Negotiated Representations to Prevent Forgetting in Machine Learning Applications

## Quick Facts
- arXiv ID: 2312.00237
- Source URL: https://arxiv.org/abs/2312.00237
- Authors: 
- Reference count: 37
- Primary result: Proposed method achieves 82.3% average accuracy on MNIST, 54.8% on Fashion MNIST, 46.5% on CIFAR-10, and 34.9% on CIFAR-100, outperforming traditional one-hot encoding approaches.

## Executive Summary
This study introduces negotiated representations as a novel approach to address catastrophic forgetting in class incremental continual learning. The method employs vector representations with constant Hamming distance between classes, created using Walsh matrices, combined with a negotiation mechanism that balances learning new tasks while preserving knowledge of previous tasks. Experimental results across four benchmark datasets demonstrate significant improvements in average classification accuracy compared to baseline models, with the method showing particular effectiveness in preventing catastrophic forgetting while maintaining competitive performance on new tasks.

## Method Summary
The proposed method tackles catastrophic forgetting by incorporating negotiated representations into the learning process. It uses Walsh matrix-derived vector representations where each class has a constant Hamming distance from other classes, facilitating robust class separability. A negotiation mechanism constrains model weight movement during training, preventing the model from completely abandoning previous knowledge while still allowing adaptation to new tasks. The approach also includes a negotiation rate scheduler that gradually allocates model capacity across tasks, ensuring equal task memory distribution through an optimal negotiation plasticity formula.

## Key Results
- Average accuracy of 82.3% on MNIST dataset
- Average accuracy of 54.8% on Fashion MNIST dataset
- Average accuracy of 46.5% on CIFAR-10 dataset
- Average accuracy of 34.9% on CIFAR-100 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negotiated representations prevent catastrophic forgetting by constraining model weight movement during training.
- Mechanism: The method introduces a negotiation paradigm that limits how much the model can adjust its weights when learning new tasks. This prevents the model from completely abandoning its previous knowledge neighborhood.
- Core assumption: Some degree of weight restriction is necessary to preserve prior task knowledge while still allowing adaptation to new tasks.
- Evidence anchors:
  - [abstract] "Our proposed method tackles the catastrophic forgetting problem by incorporating negotiated representations into the learning process, which allows the model to maintain a balance between retaining past experiences and adapting to new tasks."
  - [section] "We attempted to limit the movement of the model by setting up a negotiation table between the model and the labels that correspond to the samples."
- Break condition: If the negotiation rate is set too high, the model cannot learn new tasks effectively. If set too low, forgetting occurs.

### Mechanism 2
- Claim: Vector representations with constant Hamming distance between classes enable robust class separability.
- Mechanism: The method uses Walsh matrix-derived vector representations where each class has a constant Hamming distance from other classes. This structure allows the model to learn shared and distinctive features across classes.
- Core assumption: Maintaining constant Hamming distance between class representations preserves feature relationships that generalize across tasks.
- Evidence anchors:
  - [section] "We propose a novel method for preventing catastrophic forgetting... by creating vector representations with constant Hamming distance between classes."
  - [section] "This approach leverages vector representations that maintain a constant Hamming distance between each pair of classes, facilitating robust class separability."
- Break condition: If vector representations are exhausted (exceeding available Walsh vectors), the method cannot assign unique representations to new classes.

### Mechanism 3
- Claim: Gradual allocation of model capacity through negotiation rate scheduling ensures equal task memory distribution.
- Mechanism: The negotiation rate scheduler allocates model capacity progressively across tasks, preventing the most recent task from monopolizing learning capacity. The optimal negotiation plasticity formula ensures equal capacity allocation.
- Core assumption: Without capacity scheduling, the model will allocate disproportionate resources to recent tasks at the expense of earlier ones.
- Evidence anchors:
  - [section] "we have developed the negotiation rate scheduler... enables to allocate a significant part of the model's memory, which was initially not very effective, for new tasks."
  - [section] "optimal negotiation plasticity = 1/2 · neg − neg2"
- Break condition: If the negotiation plasticity formula is not properly implemented, task capacity allocation becomes imbalanced.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding the problem being solved is essential for implementing the solution correctly.
  - Quick check question: Why does standard gradient descent cause catastrophic forgetting in sequential task learning?

- Concept: Class-incremental continual learning
  - Why needed here: The method specifically addresses this learning scenario where classes are added sequentially without task identifiers.
  - Quick check question: How does class-incremental learning differ from task-incremental learning in terms of evaluation metrics?

- Concept: Vector representation and Hamming distance
  - Why needed here: The core innovation relies on constant Hamming distance vector representations for class discrimination.
  - Quick check question: What property of Walsh matrices makes them suitable for creating constant Hamming distance vectors?

## Architecture Onboarding

- Component map:
  Input preprocessing layer -> Convolutional feature extractor (CNN layers) -> Negotiation mechanism controller -> Vector representation generator (Walsh matrix) -> Minimum Distance Classifier (MDN) -> Custom sigmoid activation function -> Negotiation rate scheduler

- Critical path:
  1. Input samples → CNN feature extraction
  2. Predicted outputs → Distance calculation to available representations
  3. Nearest vector assignment → Negotiation with model predictions
  4. Loss calculation → Backpropagation with capacity constraints
  5. Model evaluation across all learned tasks

- Design tradeoffs:
  - Higher negotiation rates improve new task learning but increase forgetting risk
  - Larger vector sizes provide more unique representations but increase model complexity
  - More convolutional layers improve feature extraction but increase computational cost

- Failure signatures:
  - Rapid accuracy decline on new tasks: Negotiation rate too restrictive
  - Catastrophic forgetting of old tasks: Negotiation rate too permissive or inadequate capacity allocation
  - Inability to learn new classes: Exhausted vector representations (exceeded Walsh matrix capacity)

- First 3 experiments:
  1. Train on single class pair with varying negotiation rates to find optimal range
  2. Add second class pair with vector representation assignment to test negotiation mechanism
  3. Run full incremental learning on Split MNIST to validate catastrophic forgetting prevention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal negotiation rate for each dataset to maximize average accuracy while minimizing catastrophic forgetting?
- Basis in paper: [explicit] The paper mentions finding the optimal initial negotiation rate for each dataset to balance new knowledge acquisition and previous knowledge retention.
- Why unresolved: The paper presents graphs showing the relationship between negotiation rate and accuracy for each dataset, but does not provide a definitive optimal rate. The optimal rate may vary depending on dataset complexity and model architecture.
- What evidence would resolve it: Additional experiments varying the negotiation rate in finer increments for each dataset, along with statistical analysis to determine the rate that consistently produces the highest average accuracy across multiple runs.

### Open Question 2
- Question: How does the proposed method compare to other state-of-the-art approaches for preventing catastrophic forgetting in continual learning?
- Basis in paper: [inferred] The paper mentions several related works in the literature review section, but does not provide a direct comparison of their method to these approaches.
- Why unresolved: The paper focuses on presenting their novel approach and its results, without benchmarking against other methods. A direct comparison would require implementing and evaluating other approaches on the same datasets and tasks.
- What evidence would resolve it: Conducting experiments comparing the proposed method to other state-of-the-art approaches for preventing catastrophic forgetting, using the same datasets and evaluation metrics.

### Open Question 3
- Question: How does the proposed method scale to more complex datasets with a larger number of classes and more intricate visual concepts?
- Basis in paper: [explicit] The paper mentions that the CIFAR-100 dataset, with 100 classes, presented challenges for their approach due to its complexity.
- Why unresolved: The paper only experiments with datasets containing up to 100 classes. It is unclear how the method would perform on even more complex datasets with thousands or millions of classes.
- What evidence would resolve it: Experimenting with the proposed method on larger and more complex datasets, such as ImageNet or other real-world datasets with a high number of classes, to assess its scalability and performance.

## Limitations
- Limited validation on simple image classification tasks (MNIST, Fashion MNIST, CIFAR variants) with untested performance on more complex, real-world datasets
- Reliance on Walsh matrices for vector representations may become impractical as the number of classes grows, potentially limiting scalability
- Optimal negotiation rate scheduling mechanism requires careful tuning that may not generalize well across different problem domains

## Confidence
- High confidence in the theoretical framework and mechanism design
- Medium confidence in empirical results due to limited dataset diversity
- Low confidence in scalability claims beyond tested datasets

## Next Checks
1. **Scalability Test**: Evaluate the method on larger, more complex datasets (e.g., ImageNet) to assess performance degradation and vector representation exhaustion limits.
2. **Cross-Domain Transfer**: Test the negotiated representations approach on non-image data (e.g., text or audio) to verify domain generalization.
3. **Resource Efficiency Analysis**: Compare computational and memory requirements against baseline methods to ensure practical applicability in resource-constrained environments.