---
ver: rpa2
title: A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs
arxiv_id: '2310.11917'
source_url: https://arxiv.org/abs/2310.11917
tags:
- entities
- context
- information
- descriptions
- semi-inductive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Wikidata5M-SI, a new benchmark for semi-inductive
  link prediction (SI-LP) in knowledge graphs. SI-LP aims to predict facts for previously
  unseen entities based on contextual information, avoiding expensive retraining for
  large-scale KGs.
---

# A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2310.11917
- **Source URL:** https://arxiv.org/abs/2310.11917
- **Reference count:** 14
- **Key outcome:** Wikidata5M-SI benchmark reveals semi-inductive LP performance lags far behind transductive performance for long-tail entities, highlighting need for better integration of context and textual data

## Executive Summary
This paper introduces Wikidata5M-SI, a new benchmark for semi-inductive link prediction in knowledge graphs that enables predicting facts for previously unseen entities without retraining. The benchmark extends Wikidata5M with transductive, k-shot, and 0-shot settings, providing varying amounts of contextual facts and textual information (mentions and descriptions). Experimental results with recent LP approaches show that SI performance is significantly worse than transductive performance for long-tail entities across all settings, demonstrating the challenge of generalizing to unseen entities. The study highlights the importance of textual information and suggests that better integration of context and textual data is needed for effective semi-inductive link prediction.

## Method Summary
The benchmark provides transductive, k-shot (1, 3, 5, 10), and 0-shot settings for link prediction on long-tail entities (degree 11-20) from Wikidata5M. It includes varying levels of information: KG structure only, KG structure with context triples, and KG structure with context plus textual mentions or descriptions. Multiple models are evaluated including graph-only approaches (ComplEx, DistMult, HittER) and text-based models (SimKGC, KGT5 variants). Performance is measured using filtered MRR and Hits@K metrics, with the goal of predicting missing relations for unseen entities based on contextual information rather than requiring retraining.

## Key Results
- SI performance lags significantly behind transductive performance across all experimental settings for long-tail entities
- Textual descriptions provide the most valuable information for SI-LP, improving performance considerably compared to mentions or atomic representations
- Less common relations as context triples yield better SI performance than common relations, which often describe high-level concepts
- There is generally a trade-off between transductive and semi-inductive performance, with the best SI models not necessarily being the best TD models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using factual context triples with unseen entities significantly improves semi-inductive link prediction (SI-LP) performance compared to relying solely on KG structure.
- **Mechanism:** The benchmark provides 11 context triples for each unseen entity, allowing models to infer entity properties and relationships from neighborhood information. This addresses the challenge of predicting links for entities without direct embeddings.
- **Core assumption:** Context triples are informative enough to represent unseen entities for prediction tasks.
- **Evidence anchors:**
  - [abstract] "SI performance was far behind TD performance in all experiments for long-tail entities"
  - [section] "As no information was provided for unseen entities, 0-shot was not reasonably possible. Without text-based information, context was a necessity."
  - [corpus] Weak evidence - no direct citations available
- **Break condition:** If context triples are too sparse or uninformative (e.g., all relations are highly generic like "instanceOf"), the improvement may be minimal or nonexistent.

### Mechanism 2
- **Claim:** Detailed textual descriptions of entities provide more valuable information for SI-LP than simple mentions or atomic representations.
- **Mechanism:** Descriptions contain richer semantic information that helps models infer entity properties without requiring graph context. This enables better performance even in 0-shot settings.
- **Core assumption:** Textual descriptions contain information relevant to link prediction tasks and are available for unseen entities.
- **Evidence anchors:**
  - [abstract] "there was generally a trade-off between TD and SI performance" and "textual information was highly valuable"
  - [section] "Integrating descriptions improved performance for both settings, TD and SI, considerably"
  - [corpus] Weak evidence - no direct citations available
- **Break condition:** If descriptions are generic or don't contain link-relevant information, the benefit disappears. Also fails if descriptions aren't available for new entities.

### Mechanism 3
- **Claim:** The selection strategy for context triples significantly impacts SI-LP performance, with less common relations providing more useful context.
- **Mechanism:** Using less common relations as context provides more specific information about entities, while common relations often describe high-level concepts that are less discriminative.
- **Core assumption:** Specific relations provide more discriminative information than generic relations for entity representation.
- **Evidence anchors:**
  - [section] "we found that the less common the relations of the provided context, the better the SI performance"
  - [section] "More common context relations often described high-level concepts, while less common provided further detail"
  - [corpus] Weak evidence - no direct citations available
- **Break condition:** If specific relations are too sparse or the dataset structure makes common relations actually more informative, this strategy may backfire.

## Foundational Learning

- **Concept:** Knowledge Graph Embeddings (KGE)
  - Why needed here: KGE models form the foundation for both transductive and semi-inductive link prediction approaches evaluated in the benchmark.
  - Quick check question: What is the difference between ComplEx and DistMult embeddings, and why might one be preferred over the other for link prediction?

- **Concept:** Few-shot learning
  - Why needed here: The benchmark evaluates k-shot settings (1, 3, 5, 10) where models must predict links using limited context information about unseen entities.
  - Quick check question: How does the k-shot setting differ from 0-shot and transductive settings in terms of available information?

- **Concept:** Contrastive learning
  - Why needed here: Text-based models like SimKGC use contrastive learning approaches to measure similarity between query and answer entity representations.
  - Quick check question: What is the role of contrastive learning in the SimKGC model, and how does it differ from traditional KGE approaches?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Model training -> Evaluation -> Analysis
- **Critical path:**
  1. Load and preprocess Wikidata5M-SI data
  2. Select appropriate context triples for each unseen entity
  3. Train model with selected hyperparameters
  4. Evaluate using filtered MRR and Hits@K metrics
  5. Analyze performance differences across settings
- **Design tradeoffs:**
  - Graph-only vs text-based models: Text-based models show better performance but require textual information availability
  - Context selection strategy: Common relations vs specific relations tradeoff between realism and informativeness
  - Training complexity: More complex models (KGT5-context) vs simpler baselines (ERAvg)
- **Failure signatures:**
  - Low performance in 0-shot settings indicates insufficient textual information or model capability
  - Large gap between transductive and semi-inductive performance suggests poor generalization to unseen entities
  - Inconsistent performance across different k-shot settings may indicate context selection issues
- **First 3 experiments:**
  1. Run baseline ComplEx model with fold-in on 1-shot setting to establish minimum performance
  2. Evaluate KGT5-context model on descriptions dataset for 0-shot and 10-shot to compare best vs worst case
  3. Test DistMult + ERAvg with mentions dataset using random context selection to validate context selection strategy importance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do different knowledge graph structures affect the performance of semi-inductive link prediction models?
- **Basis in paper:** [explicit] The paper mentions that the benchmark is based on Wikidata5M, a subset of a single knowledge graph, and notes that model performance may vary if the graph structure is different.
- **Why unresolved:** The paper does not explore different knowledge graph structures or their impact on model performance.
- **What evidence would resolve it:** Experiments comparing model performance on different knowledge graph structures, such as different datasets or synthetic graphs with varying properties.

### Open Question 2
- **Question:** What is the impact of entity degree on semi-inductive link prediction performance?
- **Basis in paper:** [explicit] The paper states that long-tail entities (degree â‰¤ 20) are used as unseen entities in the benchmark, implying that entity degree is a factor in model performance.
- **Why unresolved:** The paper does not analyze the relationship between entity degree and model performance in detail.
- **What evidence would resolve it:** Experiments measuring model performance on entities with different degrees, potentially revealing a correlation between entity degree and prediction accuracy.

### Open Question 3
- **Question:** How do different context selection strategies affect semi-inductive link prediction performance?
- **Basis in paper:** [explicit] The paper explores three context selection strategies (most common, least common, and random relations) and finds that less common relations provide better context.
- **Why unresolved:** The paper does not explore other potential context selection strategies or provide a comprehensive analysis of the impact of context selection on model performance.
- **What evidence would resolve it:** Experiments comparing a wider range of context selection strategies, such as those based on relation types, entity types, or semantic similarity, to determine the most effective approach.

## Limitations
- The benchmark is based on a single knowledge graph (Wikidata5M), limiting generalizability to other KG structures
- Semi-inductive performance remains substantially behind transductive performance, indicating current approaches are insufficient
- The mechanism explaining why less common relations provide better context is based on qualitative observations without rigorous statistical validation

## Confidence
- **High confidence**: The benchmark construction methodology and the general observation that SI performance lags behind TD performance are well-supported by experimental results.
- **Medium confidence**: The claims about the value of textual information and context triples are supported by ablation studies, but the specific impact may vary depending on the model architecture and dataset characteristics.
- **Low confidence**: The mechanism explaining why less common relations provide better context is based on qualitative observations without rigorous statistical validation.

## Next Checks
1. Conduct statistical significance tests on performance differences between common and specific context relations to validate the observed trend.
2. Evaluate model performance on a held-out validation set with different context selection strategies to assess robustness.
3. Test the benchmark with additional model architectures not included in the original study to determine if the observed performance gaps are model-specific or general phenomena.