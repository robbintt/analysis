---
ver: rpa2
title: Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language
  Model with Sparse Human Supervision
arxiv_id: '2310.20153'
source_url: https://arxiv.org/abs/2310.20153
tags:
- annotation
- human
- annotations
- gpt-3
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IMFL, an interactive multi-fidelity learning
  framework for cost-effective adaptation of language models with sparse human supervision.
  IMFL addresses the challenge of high data annotation costs in developing domain-specific
  language models by formulating the fine-tuning process as a multi-fidelity learning
  problem that balances low-fidelity automatic LLM annotations and high-fidelity human
  annotations.
---

# Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision

## Quick Facts
- arXiv ID: 2310.20153
- Source URL: https://arxiv.org/abs/2310.20153
- Authors: 
- Reference count: 40
- This paper introduces IMFL, an interactive multi-fidelity learning framework that achieves superior performance with significantly reduced human annotation costs through strategic balancing of LLM and human annotations.

## Executive Summary
This paper addresses the challenge of high data annotation costs in domain-specific language model adaptation by introducing an interactive multi-fidelity learning framework (IMFL). The framework strategically combines low-fidelity automatic LLM annotations with high-fidelity human annotations, using uncertainty scores and prompt retrieval to optimize the allocation of limited human annotation budgets. Extensive experiments on financial and medical tasks demonstrate that IMFL can achieve performance comparable to 5x human annotation using only 3x human annotation, representing a significant reduction in annotation costs while maintaining model quality.

## Method Summary
The IMFL framework formulates domain-specific fine-tuning as a multi-fidelity learning problem that balances automatic LLM annotations with human annotations based on uncertainty scores. It employs an exploration-exploitation query strategy where uncertainty sampling prioritizes human annotation for samples with least confidence, while diversity sampling ensures representativeness through cluster-based selection. The framework uses prompt retrieval with Sentence-BERT embeddings to improve LLM annotation quality by providing in-context examples from previously annotated samples. A variable batch-size strategy allocates larger human annotation budgets in early rounds to establish a robust human-annotated pool, which then serves as a better source for prompt retrieval in subsequent LLM annotations. The framework integrates these components with LoRA fine-tuning to adapt pre-trained language models to domain-specific tasks efficiently.

## Key Results
- IMFL significantly outperforms 3x human annotation baselines in all four tasks (financial and medical datasets)
- IMFL achieves very close performance to 5x human annotation baselines on two of the four tasks
- The framework demonstrates effective cost reduction by achieving comparable performance with fewer human annotations supplemented with LLM annotations

## Why This Works (Mechanism)

### Mechanism 1
The interactive multi-fidelity learning framework achieves superior performance by balancing low-fidelity LLM annotations with high-fidelity human annotations based on uncertainty scores. The framework queries samples using an exploration-exploitation strategy where uncertainty sampling prioritizes human annotation for samples with least confidence, while diversity sampling ensures representativeness through cluster-based selection. This approach recognizes that different data samples inherently exhibit different levels of hardness for learning, making it unnecessary to request human annotation for every sample.

### Mechanism 2
Prompt retrieval with similarity-based selection significantly improves LLM annotation quality by leveraging high-fidelity human annotations as in-context examples. The framework retrieves annotated samples most similar to the queried instance using Sentence-BERT embeddings and cosine similarity, then uses these examples to construct prompts for the LLM annotator. This leverages LLMs' ability to perform few-shot learning through carefully constructed prompts using retrieved examples.

### Mechanism 3
Variable batch sizes with human-first annotation order enhances knowledge distillation and stabilizes LLM annotations throughout the interactive learning process. The framework allocates larger human annotation budgets in early rounds to establish a robust human-annotated pool, which then serves as a better source for prompt retrieval in subsequent LLM annotations. This assumes that early human annotations provide higher quality supervision signals that can guide LLM annotations more effectively.

## Foundational Learning

- **Multi-fidelity optimization**: Why needed here - treats human and LLM annotations as different fidelity sources with different costs and accuracies, requiring optimization strategies to balance their usage. Quick check - What is the key difference between multi-fidelity optimization and traditional active learning approaches?

- **In-context learning and prompt engineering**: Why needed here - framework relies on LLMs' ability to perform few-shot learning through carefully constructed prompts using retrieved examples, which is central to improving annotation quality. Quick check - How does the choice of in-context examples affect the quality of LLM-generated annotations?

- **Uncertainty quantification in language models**: Why needed here - framework uses uncertainty scores to determine which samples should be annotated by humans versus LLMs, requiring methods to estimate model confidence. Quick check - What are common methods for estimating uncertainty in language model predictions?

## Architecture Onboarding

- **Component map**: Unannotated data pool -> Query Strategy (uncertainty + diversity) -> Human Annotation -> Prompt Retrieval -> LLM Annotation -> Fine-tuning -> Updated Model -> Repeat
- **Critical path**: Data pool → Query Strategy (uncertainty + diversity) → Human Annotation → Prompt Retrieval → LLM Annotation → Fine-tuning → Updated Model → Repeat
- **Design tradeoffs**: Framework trades off between annotation cost (human vs LLM), annotation quality (high-fidelity vs low-fidelity), and computational efficiency (batch sizes, rounds). Variable batch size approach prioritizes early human effort for better prompt retrieval later.
- **Failure signatures**: Poor performance could stem from (1) inaccurate uncertainty estimation leading to wrong annotation assignments, (2) ineffective prompt retrieval due to poor embedding similarity, (3) insufficient human annotation budget for establishing quality examples, or (4) instability in LLM annotations despite prompt retrieval.
- **First 3 experiments**:
  1. Run a baseline with random query strategy and equal batch sizes to establish the importance of the exploration-exploitation approach.
  2. Test prompt retrieval with fixed batch sizes to isolate the impact of similarity-based in-context examples.
  3. Implement the full framework with variable batch sizes and human-first ordering to evaluate the complete synergistic effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of human annotations to LLM-generated annotations for maximizing performance under different budget constraints?
- Basis in paper: The paper conducts an ablation study on different human annotation ratios (20/80, 10/90, 5/95) but does not explore the full space of possible ratios or provide a principled method for determining the optimal ratio for specific tasks and budget constraints.
- Why unresolved: The paper only tests three specific ratios and does not provide a general framework for determining the optimal ratio. The optimal ratio may vary depending on task complexity, domain, and budget constraints.
- What evidence would resolve it: Experiments testing a wider range of ratios across multiple tasks and domains, or a theoretical framework for determining the optimal ratio based on task and budget characteristics.

### Open Question 2
- Question: How does the performance of IMFL scale with increasing annotation budget beyond the tested limits?
- Basis in paper: The paper tests a maximum annotation budget of 1000 but does not explore performance beyond this limit. The paper also does not provide a clear analysis of how performance scales with increasing budget.
- Why unresolved: The paper does not test budgets beyond 1000 and does not provide a detailed analysis of the relationship between budget and performance. The point of diminishing returns and the optimal budget for different tasks are unknown.
- What evidence would resolve it: Experiments testing larger annotation budgets (e.g., 2000, 5000) and a detailed analysis of the relationship between budget and performance, including the identification of the point of diminishing returns.

### Open Question 3
- Question: How does the choice of embedding model for prompt retrieval affect the performance of IMFL?
- Basis in paper: The paper uses Sentence-BERT as the embedding model for prompt retrieval but does not explore the impact of using different embedding models or fine-tuning the embedding model on domain-specific data.
- Why unresolved: The paper does not compare the performance of IMFL using different embedding models or explore the potential benefits of fine-tuning the embedding model on domain-specific data.
- What evidence would resolve it: Experiments comparing the performance of IMFL using different embedding models (e.g., BioBERT for medical tasks, FinBERT for financial tasks) and experiments fine-tuning the embedding model on domain-specific data.

### Open Question 4
- Question: How does the performance of IMFL compare to other active learning or few-shot learning approaches in the low-resource setting?
- Basis in paper: The paper compares IMFL to single fidelity annotations but does not compare it to other active learning or few-shot learning approaches that are designed for the low-resource setting.
- Why unresolved: The paper does not provide a comparison to other active learning or few-shot learning approaches that are specifically designed for the low-resource setting. It is unclear how IMFL compares to these approaches in terms of performance and efficiency.
- What evidence would resolve it: Experiments comparing the performance of IMFL to other active learning or few-shot learning approaches (e.g., entropy-based active learning, diversity-based active learning, meta-learning approaches) on the same tasks and under the same budget constraints.

## Limitations

- The framework's effectiveness depends heavily on the quality of uncertainty estimation and prompt retrieval, which may not generalize well across all domains or tasks.
- The variable batch size approach assumes that early human annotations provide superior supervision signals, but this may not hold for all domains or model architectures.
- The claim that IMFL can achieve "very close performance as 5x human annotation" with only 3x human annotation is uncertain and may depend heavily on specific domain characteristics.

## Confidence

- **High Confidence**: The general framework architecture (exploration-exploitation with uncertainty sampling, prompt retrieval, variable batch sizes) is well-specified and logically coherent.
- **Medium Confidence**: The claim that IMFL achieves "superior performance" compared to single fidelity annotations is supported by experimental results, but the magnitude of improvement may vary significantly across different domains and tasks.
- **Low Confidence**: The claim that IMFL can achieve "very close performance as 5x human annotation" with only 3x human annotation is the most uncertain, as this represents a 40% reduction in annotation costs.

## Next Checks

1. **Uncertainty Calibration Test**: Conduct experiments to measure the correlation between predicted uncertainty scores and actual annotation difficulty by having human annotators rate sample difficulty independently. This would validate whether the exploration-exploitation strategy is allocating human effort optimally.

2. **Prompt Retrieval Ablation Study**: Systematically test the effectiveness of prompt retrieval by comparing different similarity metrics, embedding models, and prompt construction methods. Include cases where retrieved examples are deliberately irrelevant to measure the impact on LLM annotation quality.

3. **Domain Transfer Experiment**: Apply the framework to a new domain (e.g., legal documents or scientific literature) with different semantic characteristics than financial and medical domains to test generalizability and identify domain-specific limitations or necessary adaptations.