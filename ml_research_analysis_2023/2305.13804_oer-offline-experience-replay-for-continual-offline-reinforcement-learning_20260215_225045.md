---
ver: rpa2
title: 'OER: Offline Experience Replay for Continual Offline Reinforcement Learning'
arxiv_id: '2305.13804'
source_url: https://arxiv.org/abs/2305.13804
tags:
- learning
- offline
- tasks
- policy
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new problem called continual offline reinforcement
  learning (CORL), where an agent learns a sequence of tasks from pre-collected offline
  datasets without any environment exploration. The key challenge is to overcome catastrophic
  forgetting while dealing with distribution shifts between offline data and learned
  policies.
---

# OER: Offline Experience Replay for Continual Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.13804
- Source URL: https://arxiv.org/abs/2305.13804
- Reference count: 40
- Primary result: Introduces continual offline RL with model-based experience selection and dual behavior cloning to prevent catastrophic forgetting

## Executive Summary
This paper addresses the challenge of continual offline reinforcement learning (CORL), where an agent must learn a sequence of tasks from pre-collected offline datasets without environment exploration. The key difficulty lies in overcoming catastrophic forgetting while managing distribution shifts between offline data and learned policies. The authors propose Offline Experience Replay (OER), which combines Model-Based Experience Selection (MBES) for building effective replay buffers and Dual Behavior Cloning (DBC) to prevent optimization conflicts. Experimental results on Mujoco environments demonstrate that OER significantly outperforms state-of-the-art baselines in both average performance and backward transfer metrics, particularly excelling at maintaining performance across sequential tasks while minimizing forgetting.

## Method Summary
OER addresses continual offline RL through two main components: MBES constructs replay buffers by using a learned transition model to simulate optimal policy trajectories and selects offline data points most similar to these simulated states, while DBC employs a dual-policy architecture with separate networks for current task optimization and continual learning. The method uses TD3+BC as a backbone offline RL algorithm with ensemble dynamic models, and implements a multi-head policy network where each task has its own output head. The transition model learns to approximate environment dynamics, and MBES uses this model to generate pseudo-trajectories by recursively sampling actions and finding closest states in the offline dataset. The DBC architecture trains two policy networks (π for continual learning and μₙ for current task optimization) to prevent optimization conflicts between behavior cloning and Q-learning objectives.

## Key Results
- Significant performance improvement over state-of-the-art baselines on Mujoco environments (Ant-Dir, Walker-Par, Cheetah-Vel)
- Superior backward transfer metrics demonstrating reduced catastrophic forgetting across sequential tasks
- Effective performance maintenance across tasks with both Medium and Medium-Replay quality offline datasets

## Why This Works (Mechanism)

### Mechanism 1: Model-Based Experience Selection (MBES) reduces distribution shift between replay buffer and learned policy
- Uses learned transition model to simulate optimal policy trajectories and selects offline data points closest to these simulated states
- Assumes transition model can approximate optimal policy's state distribution well enough for effective replay selection
- Evidence: Transition model bridges distribution bias between replay buffer and learned model by filtering relevant data
- Break condition: Transition model accuracy degrades significantly or cannot approximate optimal policy's state distribution

### Mechanism 2: Dual Behavior Cloning (DBC) prevents optimization conflict between Q-learning and continual learning
- Separates policy networks for current task optimization (μₙ) and continual learning (π) to prevent interference
- Assumes two-policy architecture can maintain stable optimization without interfering with each other's objectives
- Evidence: DBC scheme solves inconsistency by requiring continual learning policy to clone both previous experiences and current task policy
- Break condition: Two policy networks diverge significantly or cannot be effectively synchronized

### Mechanism 3: Selective replay buffer population prioritizes valuable experiences
- Uses transition model to simulate optimal policy trajectories and selects offline data points that best approximate these trajectories
- Assumes trajectories similar to optimal policy simulations provide better learning signals than random selection
- Evidence: MBES considers both learned optimal policy and task-specific dynamic model for experience selection
- Break condition: Selection criterion becomes too restrictive or fails to capture diverse experiences needed for robust learning

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Core problem OER addresses is maintaining performance on previously learned tasks while learning new ones
  - Quick check question: What happens to a neural network's performance on task A when it's trained on task B without any special mechanisms?

- Concept: Distribution shift between behavior policy and learned policy
  - Why needed here: Offline RL suffers from overestimation when applying Q-learning to out-of-distribution state-action pairs
  - Quick check question: Why might Q-learning on offline data lead to overly optimistic value estimates?

- Concept: Replay buffer management in experience replay
  - Why needed here: Size-limited replay buffer must balance retaining valuable old experiences while making room for new task data
  - Quick check question: How does the choice of experiences stored in a replay buffer affect the stability of continual learning?

## Architecture Onboarding

- Component map:
  - Main policy network π (multi-head for different tasks)
  - Current task policy μₙ (independent network)
  - Q-network Qₙ (task-specific)
  - Transition model ensemble (5 individual models)
  - Replay buffers B₁...Bₙ for each task

- Critical path:
  1. Learn current task using μₙ and Qₙ
  2. Use learned models to select valuable experiences for Bₙ
  3. Update π to clone both μₙ and previous buffers
  4. Repeat for next task

- Design tradeoffs:
  - Model complexity vs. computational efficiency
  - Buffer size vs. forgetting prevention
  - Selection strictness vs. diversity of experiences
  - Weight of BC term vs. new task learning speed

- Failure signatures:
  - Performance degradation on earlier tasks
  - Instability during policy updates
  - Buffer selection not capturing optimal trajectories
  - Q-learning diverging due to BC interference

- First 3 experiments:
  1. Implement MBES alone with random buffer selection to test distribution alignment
  2. Implement DBC alone with uniform buffer selection to test optimization conflict resolution
  3. Combine MBES and DBC with varying buffer sizes to find optimal capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MBES scheme perform when applied to tasks with significantly different state spaces or dynamics, and what modifications would be needed to handle such cases?
- Basis in paper: The paper mentions that sequential tasks share the same state space, action space, and discounting factor for simplicity, but does not explore cases with different state spaces or dynamics
- Why unresolved: Current MBES method relies on finding similar states in offline dataset, which may not be feasible when state spaces differ significantly
- What evidence would resolve it: Experimental results comparing MBES performance across tasks with varying state spaces and dynamics, along with proposed modifications

### Open Question 2
- Question: What is the impact of replay buffer size on the trade-off between forgetting old tasks and learning new tasks, and is there an optimal buffer size that balances these competing objectives?
- Basis in paper: The paper discusses importance of replay buffer size and shows results for different buffer sizes (1K vs 10K), but does not provide systematic analysis
- Why unresolved: Paper only shows results for two buffer sizes without exploring full range or providing theoretical analysis of the trade-off
- What evidence would resolve it: Comprehensive study varying buffer sizes across multiple orders of magnitude, combined with theoretical analysis

### Open Question 3
- Question: How does the proposed OER method compare to online continual learning methods in terms of sample efficiency and final performance, and under what conditions might online methods be preferable?
- Basis in paper: The paper focuses on offline continual learning and compares against other offline methods, but does not compare against online continual learning methods
- Why unresolved: Paper does not provide experimental comparison with online continual learning methods, which could offer different trade-offs
- What evidence would resolve it: Experimental results comparing OER against state-of-the-art online continual learning methods on same tasks, measuring both sample efficiency and final performance

## Limitations

- Limited experimental validation to Mujoco environments raises questions about generalization to other domains
- Heavy dependence on transition model quality may limit effectiveness in high-dimensional state spaces or with limited offline data
- Dual policy architecture introduces additional complexity that could lead to training instability if networks fail to remain synchronized

## Confidence

- **High confidence**: Problem formulation for CORL is well-defined and baseline comparisons are methodologically sound
- **Medium confidence**: Theoretical framework for MBES and DBC is plausible but practical implementation details are sparse
- **Low confidence**: Claims about significant performance improvements require more extensive hyperparameter ablation studies

## Next Checks

1. **Model robustness test**: Evaluate transition model accuracy on held-out validation trajectories to quantify approximation error and its impact on MBES performance
2. **Policy divergence monitoring**: Implement online monitoring of the distance between π and μₙ during training to detect potential optimization conflicts in DBC
3. **Buffer size sensitivity analysis**: Systematically vary replay buffer capacities to identify optimal balance between retaining old experiences and accommodating new task data across different environment complexities