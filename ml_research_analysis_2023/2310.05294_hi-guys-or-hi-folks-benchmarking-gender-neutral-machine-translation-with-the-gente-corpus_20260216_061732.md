---
ver: rpa2
title: Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the
  GeNTE Corpus
arxiv_id: '2310.05294'
source_url: https://arxiv.org/abs/2310.05294
tags:
- translation
- neutral
- gender
- language
- gendered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GeNTE, the first natural benchmark for evaluating
  gender-neutral machine translation (MT) from English to Italian. To address the
  lack of resources for inclusive MT, the authors conducted a survey on the acceptability
  of neutral translations and created a bilingual test set with parallel sentences
  requiring either gender-neutral or gendered forms.
---

# Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus

## Quick Facts
- arXiv ID: 2310.05294
- Source URL: https://arxiv.org/abs/2310.05294
- Reference count: 36
- The paper introduces GeNTE, the first natural benchmark for evaluating gender-neutral machine translation (MT) from English to Italian.

## Executive Summary
This paper addresses the lack of resources for evaluating gender-neutral machine translation by introducing the GeNTE corpus, a bilingual test set with parallel English-Italian sentences requiring either gender-neutral or gendered forms. The authors conducted a survey on the acceptability of neutral translations and created three different neutral references per sentence, enabling contrastive evaluation. Through experiments with reference-based and reference-free metrics, they demonstrate that n-gram overlap metrics can distinguish gendered from neutral translations at corpus level but fail at sentence level. A reference-free classifier trained on synthetic data achieves balanced accuracy and proves more robust to neutralization strategies, offering a viable alternative for automated evaluation.

## Method Summary
The authors created the GeNTE corpus through a multi-step process: extracting sentence pairs from Europarl, manually editing to create gender-neutral alternatives, and having three translators create neutral references for each sentence. They evaluated reference-based metrics (BLEU, TER, METEOR, BERTscore, COMET, BLEURT) on this corpus and developed a reference-free classifier trained on synthetic data generated using GPT-3.5-turbo. The synthetic corpus was created through seedword triplets and sentence generation, followed by a second round of rewriting to ensure syntactic variety. The classifier was then evaluated on the test-bed to assess its ability to distinguish gendered from neutral translations.

## Key Results
- N-gram overlap metrics (BLEU, TER, METEOR) can detect gendered vs. neutral translations at corpus level but fail at sentence-level classification
- Neural metrics (BERTscore, COMET, BLEURT) are less effective for gender-neutral evaluation due to their focus on semantic similarity
- A reference-free classifier trained on synthetic data achieves balanced accuracy across both gendered and neutral classes
- The classifier outperforms the best n-gram overlap metric (TER) by up to 22.67 points for DeepL translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference-based n-gram metrics can distinguish gendered from neutral translations at corpus level but fail at sentence level
- Mechanism: N-gram overlap metrics penalize differences in lexical surface forms, so gendered references reward gendered outputs and neutral references reward neutral outputs when evaluated at scale
- Core assumption: Gender-neutralizations primarily alter surface forms without changing core meaning, making them detectable by surface-sensitive metrics when aggregated
- Evidence anchors: [abstract] and [section] showing corpus-level detection but sentence-level failure

### Mechanism 2
- Claim: Neural metrics are less effective for gender-neutral evaluation because they focus on semantic similarity
- Mechanism: Neural metrics rely on contextual embeddings that capture meaning, so gender-neutral paraphrases that preserve semantics are scored similarly regardless of gender form
- Core assumption: Gender-neutral translations are semantically equivalent to their gendered counterparts, so neural metrics cannot differentiate them
- Evidence anchors: [abstract] and [section] showing divergence between n-gram and neural metrics in COMMON SET-N

### Mechanism 3
- Claim: A reference-free classifier trained on synthetic data can achieve balanced accuracy for gendered and neutral translation classification
- Mechanism: The classifier learns to distinguish gendered from neutral forms based on lexical and morphological cues in the Italian target, regardless of reference availability
- Core assumption: Synthetic data generated with balanced gendered/neutral forms and varied syntax can train a model to generalize to real MT outputs
- Evidence anchors: [abstract] and [section] showing classifier outperforming TER by up to 22.67 points

## Foundational Learning

- Concept: Gender-neutral translation strategies (e.g., synonymy, periphrasis) in Italian
  - Why needed here: The evaluation relies on detecting these strategies; understanding them is essential to interpret metric behavior
  - Quick check question: What are two common Italian neutralization strategies mentioned in the paper?

- Concept: Contrastive evaluation protocol using gendered vs. neutral references
  - Why needed here: The paper's main evaluation method depends on comparing scores across reference types; knowing how this works is key to understanding results
  - Quick check question: In the contrastive protocol, when should a metric give higher scores: for gendered outputs vs. gendered references, or for neutral outputs vs. neutral references?

- Concept: Synthetic data generation for classifier training
  - Why needed here: The reference-free approach depends on quality synthetic data; understanding generation helps assess reliability
  - Quick check question: How did the authors ensure syntactic variety in the synthetic corpus?

## Architecture Onboarding

- Component map: Europarl → Set-N/Set-G extraction → manual editing → neutral reference creation (3 translators) → COMMON SET → MT outputs (DeepL + Amazon Translate) → manual post-edit → reference-based metrics → sentence-level classification → reference-free classifier ← synthetic data (GPT-3.5-turbo prompts → seedword triplets → sentence generation → second round rewriting)

- Critical path: 1) Build COMMON SET with 3 neutral references per sentence 2) Generate MT outputs and post-edit neutral ones 3) Run reference-based metrics at corpus and sentence level 4) Train classifier on synthetic data 5) Evaluate classifier on test-bed

- Design tradeoffs: Reference-based offers simplicity and interpretability but depends on reference quality; reference-free is more robust to neutralization strategy variability but requires synthetic data and training

- Failure signatures: Reference-based: Low accuracy on Set-N indicates inability to detect neutralizations; high accuracy on Set-G suggests bias toward gendered forms. Reference-free: Low balanced accuracy suggests classifier overfits or synthetic data poorly represents real outputs

- First 3 experiments: 1) Run all reference-based metrics on COMMON SET at corpus level to confirm directionality 2) Perform sentence-level classification using BLEU/TER/METEOR to check per-sentence accuracy 3) Train classifier on full synthetic corpus and evaluate on COMMON SET to compare balanced accuracy

## Open Questions the Paper Calls Out

## Open Question 1
- Question: How do neural metrics like BERTScore, BLEURT, and COMET perform in evaluating gender-neutral translations compared to n-gram overlap metrics like BLEU, TER, and METEOR?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that neural metrics are less effective in detecting gender-neutral translations due to their lower frequency in training data and sensitivity to semantic variations. However, the specific reasons and potential improvements for neural metrics are not fully explored.
- What evidence would resolve it: Detailed analysis of the training data distribution and semantic representation of neutral vs. gendered translations, along with experiments to improve neural metrics' performance on gender-neutral translation evaluation.

## Open Question 2
- Question: What are the specific challenges and limitations of implementing gender-neutral language in machine translation for other grammatical gender languages beyond Italian?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on English to Italian translation and acknowledges that the resources and evaluation methods may not directly apply to other languages. However, it does not provide a comprehensive analysis of the challenges and limitations specific to other grammatical gender languages.
- What evidence would resolve it: Comparative studies on gender-neutral translation in multiple grammatical gender languages, identifying common challenges and language-specific limitations.

## Open Question 3
- Question: How can the proposed reference-free classifier be further improved to handle more complex and nuanced gender-neutral translation scenarios?
- Basis in paper: [explicit]
- Why unresolved: The paper introduces a reference-free classifier for gender-neutral translation evaluation and shows promising results. However, it does not discuss potential improvements or extensions to handle more complex translation scenarios.
- What evidence would resolve it: Experiments with the classifier on more diverse and challenging translation datasets, along with techniques to enhance its performance and generalization capabilities.

## Limitations
- The evaluation relies on a relatively small benchmark corpus (1,500 sentences), limiting statistical power for sentence-level classification
- The synthetic data generation process introduces potential domain mismatch between training and real MT outputs
- The focus on English-to-Italian translation restricts generalizability to other language pairs or broader gender-neutralization scenarios

## Confidence
- **High confidence**: Corpus-level performance of n-gram metrics (BLEU, TER, METEOR) correctly distinguishing gendered from neutral translations
- **Medium confidence**: Reference-free classifier achieving balanced accuracy, though synthetic data quality remains uncertain
- **Low confidence**: Neural metrics (BERTscore, COMET, BLEURT) being ineffective for gender-neutral evaluation due to semantic focus

## Next Checks
1. Evaluate classifier performance on a held-out subset of COMMON SET not used during synthetic data generation to test generalization
2. Test n-gram metric performance on varying corpus sizes to determine minimum threshold for reliable gendered/neutral distinction
3. Compare classifier performance when trained on real MT outputs (where available) versus synthetic data to quantify domain adaptation requirements