---
ver: rpa2
title: Reasons to Reject? Aligning Language Models with Judgments
arxiv_id: '2312.14591'
source_url: https://arxiv.org/abs/2312.14591
tags:
- judgments
- arxiv
- judgment
- alignment
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic exploration of aligning
  large language models (LLMs) with language feedback (judgments) rather than scalar
  rewards. The authors propose a novel framework, Contrastive Unlikelihood Training
  (CUT), which enables fine-grained inappropriate content detection and correction
  based on judgments.
---

# Reasons to Reject? Aligning Language Models with Judgments

## Quick Facts
- arXiv ID: 2312.14591
- Source URL: https://arxiv.org/abs/2312.14591
- Reference count: 25
- Key outcome: CUT framework achieves 62.56% winning rate against 175B DaVinci003 using only 1317 judgment data points

## Executive Summary
This paper introduces Contrastive Unlikelihood Training (CUT), a novel framework for aligning large language models using language judgments rather than scalar rewards. CUT identifies inappropriate tokens by contrasting generation probabilities under positive and negative judgment conditions, then applies unlikelihood training to those tokens. The method demonstrates strong performance on AlpacaEval (62.56% winning rate) while requiring significantly less data than traditional reward-based approaches. The authors show that judgments provide finer-grained feedback than scalar rewards, enabling more precise alignment.

## Method Summary
CUT combines maximum likelihood estimation (MLE) with unlikelihood training (UT) in a contrastive framework. For each instruction-response-judgment triplet, CUT generates responses under both authentic negative judgments and fake positive judgments, identifies tokens with substantially higher probability under negative conditions, and applies UT to those tokens. The framework supports both offline alignment with pre-collected judgment data and online iterative alignment where the model generates responses, receives judgments, and is fine-tuned in repeated cycles. The method uses LLaMA2 variants as base models and evaluates on benchmarks including AlpacaEval, TruthfulQA, and ARC.

## Key Results
- CUT (LLaMA2-13b) achieves 62.56% winning rate against 175B DaVinci003 on AlpacaEval using only 1317 judgment data points
- CUT improves LLaMA2-13b by 13.08 points on TruthfulQA and 9.76 points on ARC
- Online iterative alignment shows steady improvement from 81.09 to 91.36 points on AlpacaEval
- CUT outperforms direct preference optimization (DPO) by 4.56 points on AlpacaEval using the same 4000 UltraFeedback examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CUT improves alignment by detecting and penalizing inappropriate tokens through contrastive generation probabilities
- Mechanism: CUT generates two versions of a response - one with authentic negative judgment and one with fake positive judgment. It identifies tokens with substantially higher probability under negative judgment and applies unlikelihood training to those tokens
- Core assumption: LLMs exhibit measurable probability shifts when conditioned on negative vs. positive judgments, particularly for tokens related to errors
- Evidence anchors: [abstract] "fine-grained inappropriate content detection and correction based on judgments" [section 4.2] "We contrast the response generation under different conditions to shed light on the appropriate behavior that the LLM should maintain"
- Break condition: If the probability shift between negative and positive judgments is insufficient to reliably detect inappropriate tokens, the UT objective becomes ineffective

### Mechanism 2
- Claim: CUT leverages both positive and negative alignment signals to prevent the model from overfitting to one type of example
- Mechanism: CUT combines likelihood training on aligned examples (both positive and negative judgments) with unlikelihood training on misaligned examples to balance learning
- Core assumption: The model can learn from both well-formed and flawed responses to improve overall quality
- Evidence anchors: [section 4.2] "We train on this comparison with the following MLE objective" for aligned examples [section 5.1] "CUT improves the base model by 13.08 points on TruthfulQA" showing effectiveness of combined approach
- Break condition: If the model becomes too conservative from excessive unlikelihood training, it may generate overly safe but less useful responses

### Mechanism 3
- Claim: CUT's iterative online alignment mimics human learning by continuously refining the model with model-specific judgments
- Mechanism: The model generates responses, receives judgments, and is fine-tuned on this feedback in repeated cycles, allowing for progressive improvement
- Core assumption: Model-specific judgments are more effective than generic ones because they address the current model's weaknesses
- Evidence anchors: [section 5.2.1] "online alignment results demonstrate that CUT can align LLMs in an iterative fashion using model-specific judgment data" [section 5.2.1] "steady performance improvement from 81.09 to 91.36 points on AlpacaEval"
- Break condition: If judgments become redundant or the model reaches a performance ceiling where further iterations provide diminishing returns

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) vs. Unlikelihood Training (UT)
  - Why needed here: CUT combines MLE for aligned examples and UT for misaligned examples, requiring understanding of both objectives
  - Quick check question: What is the mathematical difference between the MLE loss and UT loss in CUT's framework?

- Concept: Contrastive learning principles
  - Why needed here: CUT's core mechanism relies on contrasting generation probabilities under different judgment conditions
  - Quick check question: How does CUT's contrastive approach differ from standard contrastive learning in representation learning?

- Concept: In-context learning capabilities of LLMs
  - Why needed here: CUT leverages LLMs' ability to respond differently to negative vs. positive judgments in the same input
  - Quick check question: Why does CUT depend on strong in-context learning rather than just parameter updates?

## Architecture Onboarding

- Component map: Base LLM (LLaMA2 variants) -> Judgment annotator (human or GPT-4) -> CUT training pipeline with two-stage training -> Evaluation suite (AlpacaEval, TruthfulQA, etc.)

- Critical path:
  1. Generate responses for instructions
  2. Obtain judgments (positive or negative)
  3. Create Align-P, Align-N, and Misalign examples
  4. Compute generation probabilities under different judgment conditions
  5. Apply MLE and UT objectives
  6. Update model parameters

- Design tradeoffs:
  - Judgment quality vs. annotation cost (human vs. GPT-4)
  - Balance between MLE and UT objectives to prevent overfitting
  - Downsampling ratio for Align-P examples in online alignment
  - Number of iterations in online alignment

- Failure signatures:
  - Performance plateaus or degrades after iterations (judgment quality issues)
  - Model generates overly generic responses (excessive UT)
  - Poor performance on TruthfulQA (insufficient UT)
  - Training instability (imbalanced loss weights)

- First 3 experiments:
  1. Validate that probability shifts occur between negative and positive judgments for misaligned examples
  2. Test different λ and α hyperparameters to find optimal detection and training balance
  3. Compare CUT performance with and without inappropriate token detection on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CUT compare to other methods when using different base models (e.g., GPT-3, BLOOM, or other open-source LLMs)?
- Basis in paper: [inferred] The paper evaluates CUT using LLaMA2-13b and LLaMA2-chat-13b, but does not explore other base models.
- Why unresolved: The authors only use LLaMA2 models for their experiments, limiting the generalizability of their findings to other large language models.
- What evidence would resolve it: Conduct experiments using CUT with different base models, such as GPT-3, BLOOM, or other open-source LLMs, and compare the results to those obtained with LLaMA2 models.

### Open Question 2
- Question: How does the performance of CUT change when using different types of judgments, such as those generated by humans or other AI models?
- Basis in paper: [explicit] The paper mentions that judgments can be provided by human annotators or AI judge models, but does not explore the impact of using different types of judgments on CUT's performance.
- Why unresolved: The authors only use judgments generated by GPT-4 for their experiments, leaving the question of how other types of judgments might affect CUT's performance unanswered.
- What evidence would resolve it: Conduct experiments using CUT with judgments generated by different sources (e.g., humans, other AI models) and compare the results to those obtained with GPT-4 judgments.

### Open Question 3
- Question: How does the performance of CUT change when using different amounts of judgment data during the alignment process?
- Basis in paper: [explicit] The paper mentions that CUT can be trained with as few as 1317 judgment data points, but does not explore the impact of using different amounts of data on its performance.
- Why unresolved: The authors only use a fixed amount of judgment data for their experiments, leaving the question of how varying the amount of data might affect CUT's performance unanswered.
- What evidence would resolve it: Conduct experiments using CUT with different amounts of judgment data (e.g., 500, 1000, 2000, 3000) and compare the results to those obtained with the original 1317 data points.

## Limitations

- The framework's performance heavily depends on judgment quality, but the paper provides limited analysis of how judgment quality affects results
- Evaluation focuses primarily on AlpacaEval and a limited set of other tasks, with uncertain generalization to diverse domains
- Computational overhead of generating two response versions for each training example is not thoroughly analyzed or compared to baseline methods

## Confidence

**High Confidence:**
- CUT's mathematical framework for combining MLE and UT objectives is sound
- The contrastive approach for detecting inappropriate tokens is theoretically valid
- Offline alignment results showing 62.56% winning rate against DaVinci003 are reproducible given access to the Shepherd dataset

**Medium Confidence:**
- The claim that judgments hold "greater potential than rewards" for LLM alignment, based on the 4.56-point improvement on AlpacaEval
- The effectiveness of iterative online alignment, given that only three iterations were tested
- The superiority of CUT over existing alignment methods, considering the limited comparison scope

**Low Confidence:**
- The assertion that CUT can "effectively mitigate the misalignment problem" without comprehensive testing across diverse failure modes
- The scalability of CUT to much larger models (e.g., 70B+ parameters) without performance degradation
- The framework's robustness to noisy or adversarial judgments

## Next Checks

**Validation Check 1: Judgment Quality Impact Study**
Run controlled experiments varying judgment quality (human vs. GPT-4 vs. noisy labels) while keeping CUT's implementation constant. Measure performance degradation rates to quantify how sensitive CUT is to judgment quality. This addresses Unknown 1 and provides practical guidelines for deployment.

**Validation Check 2: Cross-Domain Generalization Test**
Evaluate CUT on at least 10 diverse tasks spanning creative writing, code generation, mathematical reasoning, and domain-specific knowledge (medicine, law, etc.). Compare performance against baseline alignment methods to validate the "greater potential" claim beyond AlpacaEval. This addresses Unknown 2 and tests real-world applicability.

**Validation Check 3: Computational Overhead Analysis**
Benchmark CUT's training time and memory usage against standard RLHF implementations across different model sizes (7B, 13B, 70B). Include analysis of the probability computation overhead for the contrastive step. This addresses Unknown 3 and informs practical deployment decisions.