---
ver: rpa2
title: 'Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example'
arxiv_id: '2308.00720'
source_url: https://arxiv.org/abs/2308.00720
tags:
- adam
- example
- constant
- algorithm
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs a unidimensional nonconvex function with
  Lipschitz continuous gradient that causes the ADAM algorithm to diverge when using
  a fixed stepsize, regardless of the choice of method parameters. The function is
  designed such that the ADAM iterates starting from the origin produce a constant
  nonzero gradient at every iteration, violating convergence.
---

# Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example

## Quick Facts
- arXiv ID: 2308.00720
- Source URL: https://arxiv.org/abs/2308.00720
- Reference count: 5
- Key outcome: ADAM diverges on a unidimensional nonconvex function with fixed stepsize for all (β₁, β₂) ∈ [0,1)²

## Executive Summary
This paper constructs a simple unidimensional nonconvex function with Lipschitz continuous gradient that causes the ADAM algorithm to diverge when using a fixed stepsize, regardless of the choice of method parameters. The function is designed such that ADAM iterates starting from the origin produce a constant nonzero gradient at every iteration, leading to unbounded growth. This example demonstrates that ADAM can fail to converge on deterministic problems even with arbitrarily small Lipschitz constants, extending previous work by showing divergence for all parameter choices rather than specific ones.

## Method Summary
The paper constructs a specific nonconvex function f(t) defined on ℝ⁺ using Hermite interpolation. The function is designed to produce constant gradient values gₖ = -1 at each iterate xₖ = kα starting from x₀ = 0. When ADAM is applied with any fixed stepsize α > 0 and parameters β₁, β₂ ∈ [0,1), the momentum mₖ and variance vₖ terms also become constant (mₖ = -1, vₖ = 1), causing the update rule to produce iterates that grow linearly without bound. The construction ensures the Lipschitz constant remains bounded below by a constant depending only on α.

## Key Results
- ADAM diverges on the constructed function for all (β₁, β₂) ∈ [0,1)²
- The gradient magnitude |gₖ| = 1 remains constant at every iteration
- Iterates grow without bound: xₖ → ∞ as k → ∞
- The Lipschitz constant L = 6/α remains bounded below by a constant depending only on α

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADAM diverges because the adaptive stepsize calculation maintains a constant nonzero gradient magnitude throughout iterations.
- Mechanism: The function is constructed so that at every iteration k, the gradient gₖ = -1 remains constant. This creates a feedback loop where the momentum mₖ = -1 and the variance term vₖ = 1 also remain constant. The update rule x_{k+1} = x_k + α ensures the iterate moves rightward by a fixed amount α each step, causing unbounded growth.
- Core assumption: The Lipschitz continuous gradient property holds for the constructed function across all iterations.
- Evidence anchors:
  - [abstract]: "the ADAM iterates starting from the origin produce a constant nonzero gradient at every iteration"
  - [section]: "We now show that there exists a (nonconvex) univariate function f1 defined on R+ with Lipschitz continuous gradient such that fk = f(xk) = 0 and gk = ∇1xf1(xk) = -1 for all k ≥ 0"
- Break condition: If the gradient were to change magnitude or sign, the constant feedback loop would be disrupted and convergence might occur.

### Mechanism 2
- Claim: The function's piecewise construction with Hermite interpolation guarantees the required gradient and function value conditions at each iterate.
- Mechanism: The function f1(t) is defined piecewise using Hermite interpolation conditions that ensure f(xk) = 0 and ∇f1(xk) = -1 at each iteration point. This construction works because the interval between consecutive iterates is fixed at length α, allowing the interpolation to maintain the required properties across all iterations.
- Core assumption: The Hermite interpolation conditions can be satisfied simultaneously for all iterate positions.
- Evidence anchors:
  - [section]: "a simple Hermite interpolation calculation based of these conditions yields that, for all t ≥ 0, f1(t) = -(t - xk(t)) + 3/sk(t)(t - xk(t))2 - 2/s2k(t)(t - xk(t))3"
  - [section]: "We may then define f(t) = { f1(t) if t ≥ 0, -t if t < 0, so that f(t) is well-defined on the whole of R"
- Break condition: If the iterate spacing were not constant or if the interpolation could not be maintained, the gradient conditions would fail.

### Mechanism 3
- Claim: The divergence occurs for all (β1, β2) ∈ [0,1)² because the construction makes the momentum and variance terms independent of these parameters.
- Mechanism: The recurrence relations for mₖ and vₖ simplify to constants regardless of β1 and β2 values because gₖ is constant. Specifically, mₖ = -1 and vₖ = 1 for all k, making the ADAM update x_{k+1} = x_k + α independent of the algorithm parameters.
- Core assumption: The constant gradient gₖ = -1 causes the parameter-dependent terms to collapse to constants.
- Evidence anchors:
  - [abstract]: "This example extends previous work by showing divergence for all (β1, β2) ∈ [0,1)²"
  - [section]: "For k ≥ 0, let the sequence of function values and gradients be defined by fk = 0 and gk = -1"
  - [section]: "mk = β1mk-1 + (1 - β1)gk = -1" and "vk = β2vk-1 + (1 - β2)g2k = 1"
- Break condition: If the gradient were not constant, the parameter-dependent terms would vary and potentially prevent divergence.

## Foundational Learning

- Concept: Lipschitz continuous gradient
  - Why needed here: The construction requires the function to have a Lipschitz continuous gradient to satisfy the assumptions under which ADAM is typically analyzed, yet still exhibit divergence.
  - Quick check question: What does it mean for a function to have a Lipschitz continuous gradient, and how is the Lipschitz constant related to the second derivative?

- Concept: Hermite interpolation
  - Why needed here: The function is constructed using Hermite interpolation to simultaneously satisfy function value and gradient conditions at multiple points.
  - Quick check question: How does Hermite interpolation differ from standard polynomial interpolation, and what additional conditions does it enforce?

- Concept: Adaptive learning rate mechanisms
  - Why needed here: Understanding how ADAM's adaptive learning rate calculation works is crucial to seeing why it fails in this construction.
  - Quick check question: How do the momentum (mₖ) and variance (vₖ) terms in ADAM affect the effective learning rate at each iteration?

## Architecture Onboarding

- Component map: ADAM consists of momentum update (mₖ) → variance update (vₖ) → parameter update (x_{k+1})
- Critical path: Constant gradient → constant momentum → constant variance → constant update step → divergent iterates
- Design tradeoffs: The construction trades off convexity for simplicity and parameter independence. A convex function with similar properties would be more complex and might only work for specific parameter choices.
- Failure signatures: Constant nonzero gradient magnitude across iterations, combined with iterates that grow without bound. Secondary signatures include the collapse of adaptive components to constants.
- First 3 experiments:
  1. Implement the function f(t) as defined and verify that ∇f(xₖ) = -1 for iterates xₖ = kα starting from x₀ = 0.
  2. Run ADAM with various (β₁, β₂) pairs on this function and confirm that xₖ grows linearly with k for all parameter choices.
  3. Measure the Lipschitz constant L = 6/α and verify that it remains bounded below by a constant depending only on α.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ADAM converge for the proposed function when using a decreasing step size (αₖ → 0) rather than a fixed step size?
- Basis in paper: [explicit] The paper explicitly notes that their conclusions do not apply when using decreasing stepsizes αₖ → 0, as is done in [5, Proposition 1.1].
- Why unresolved: The paper only analyzes the fixed step size case and explicitly states that their construction fails for decreasing step sizes.
- What evidence would resolve it: Constructing a similar counterexample for the decreasing step size case, or proving convergence for some specific decreasing step size schedule.

### Open Question 2
- Question: What is the convergence behavior of ADAM on other classes of nonconvex functions with Lipschitz continuous gradients?
- Basis in paper: [inferred] The paper demonstrates divergence on one specific nonconvex function but does not characterize the broader class of functions where ADAM fails.
- Why unresolved: The paper provides only a single counterexample without generalizing to a broader class of functions.
- What evidence would resolve it: Identifying necessary and/or sufficient conditions on the objective function that guarantee or prevent ADAM convergence.

### Open Question 3
- Question: How does the convergence behavior of ADAM change when using the variants with ε > 0 in the denominator of the update rule?
- Basis in paper: [explicit] The paper mentions that their conclusions would also hold if the update rule included ε in the denominator, but does not analyze this case in detail.
- Why unresolved: The paper only mentions these variants but does not provide a detailed analysis of their convergence properties.
- What evidence would resolve it: Proving whether these variants converge or diverge on the proposed counterexample, or constructing new counterexamples specifically for these variants.

## Limitations
- The example is purely theoretical using an artificial, piecewise-constructed function unlikely to arise in practice
- Divergence requires starting exactly at x₀ = 0; small perturbations might change the behavior
- The function is unidimensional, limiting generalizability to high-dimensional settings where ADAM is typically used

## Confidence

**Confidence Assessment:**
- **High Confidence**: The construction of the specific counterexample function and the proof that ADAM diverges on it for all (β₁, β₂) ∈ [0,1)²
- **Medium Confidence**: The claim that this represents the "simplest possible" example, as simplicity can be subjective
- **Medium Confidence**: The practical implications for ADAM usage, as real-world problems rarely exhibit such pathological behavior

## Next Checks

1. Implement numerical simulations of ADAM on the constructed function to verify the theoretical divergence predictions
2. Test whether adding small random perturbations to the initial point or gradient calculations prevents divergence
3. Analyze whether similar constructions can be made for multidimensional functions while maintaining simplicity