---
ver: rpa2
title: Non-autoregressive Streaming Transformer for Simultaneous Translation
arxiv_id: '2310.14883'
source_url: https://arxiv.org/abs/2310.14883
tags:
- nast
- translation
- source
- latency
- simt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a non-autoregressive streaming Transformer (NAST)
  for simultaneous translation. It addresses the issue of aggressive anticipation
  in autoregressive models by using a unidirectional encoder and a non-autoregressive
  decoder with intra-chunk parallelism.
---

# Non-autoregressive Streaming Transformer for Simultaneous Translation

## Quick Facts
- **arXiv ID**: 2310.14883
- **Source URL**: https://arxiv.org/abs/2310.14883
- **Reference count**: 22
- **Primary result**: Proposes NAST, a non-autoregressive streaming Transformer that outperforms autoregressive baselines on WMT benchmarks while reducing latency through parallel decoding and adaptive READ/WRITE strategies.

## Executive Summary
This paper addresses the challenge of aggressive anticipation in autoregressive simultaneous machine translation by proposing a non-autoregressive streaming Transformer (NAST). The key innovation is using a unidirectional encoder and non-autoregressive decoder with intra-chunk parallelism to eliminate target-side token dependencies, enabling parallel generation and reducing latency. NAST generates blank or repetitive tokens to flexibly adjust its READ/WRITE strategy, and is trained with an alignment-based latency loss to maximize non-monotonic latent alignment while producing source-monotonic translations.

## Method Summary
NAST consists of a unidirectional encoder with causal self-attention and a non-autoregressive decoder that generates target tokens in parallel using intra-chunk parallelism (λ=3). The model employs a chunk wait-k strategy and uses CTC-based alignment with blank tokens to model implicit READ actions. Training occurs in two stages: first with CTC loss, then with non-monotonic latent alignment loss combined with alignment-based latency loss. The model extends the output vocabulary to include blank tokens and uses a collapsing function to convert alignments into final translations.

## Key Results
- NAST achieves higher BLEU scores than strong autoregressive baselines on WMT15 De-En and WMT16 En-Ro benchmarks
- Demonstrates lower Average Lagging and improved latency metrics compared to autoregressive models
- Shows effective quality-latency tradeoff through chunk wait-k strategy and alignment-based latency loss
- Generates source-monotonic-aligned translations comparable to human interpreters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-autoregressive decoding removes target-side token dependency, enabling parallel generation and eliminating teacher forcing bias.
- Mechanism: By factorizing the probability as the product of independent token probabilities conditioned only on the source, the decoder can generate tokens in parallel without waiting for previous tokens. This removes the need for teacher forcing during training, preventing source information leakage.
- Core assumption: Target tokens can be predicted independently without significant loss of translation quality.
- Evidence anchors:
  - [abstract] "We enable NAST to generate the blank token or repetitive tokens to adjust its READ/WRITE strategy flexibly"
  - [section 2.2.1] "Non-autoregressive generation (Gu et al., 2018) is originally introduced to reduce decoding latency1. It removes the autoregressive dependency and generates target tokens in a parallel way."
- Break condition: If target-side dependencies are crucial for translation quality, independent generation will lead to significant quality degradation.

### Mechanism 2
- Claim: The CTC-based alignment loss with blank tokens enables adaptive READ/WRITE strategy through non-monotonic alignments.
- Mechanism: By extending the output space with blank tokens and using CTC marginalization, the model can generate partial alignments that implicitly encode when to read or write. The collapsing function removes consecutive duplicates and blanks to produce the final output.
- Core assumption: The model can learn to generate appropriate sequences of blanks and tokens that collapse to correct translations.
- Evidence anchors:
  - [abstract] "We enable NAST to generate the blank token or repetitive tokens to adjust its READ/WRITE strategy flexibly"
  - [section 3.1] "Following CTC (Graves et al., 2006), we extend the vocabulary to allow NAST generating the blank token ϵ or repeated tokens from decoder states to model an implicit READ action."
- Break condition: If the alignment space becomes too large or the collapsing function too complex, the model may fail to learn meaningful READ/WRITE patterns.

### Mechanism 3
- Claim: Non-monotonic latent alignment loss enables source-monotonic translations by maximizing bigram matching between target and alignments.
- Mechanism: Instead of forcing monotonic mapping during training, the model maximizes expected bigram matching using non-monotonic alignments. This allows the model to generate translations that are aligned with the source in a monotonic manner during inference.
- Core assumption: The reference corpus contains non-monotonic alignments that, when properly learned, enable monotonic generation.
- Evidence anchors:
  - [abstract] "train it to maximize the non-monotonic latent alignment with an alignment-based latency loss"
  - [section 3.3] "we apply the bigram-based non-monotonic latent alignment loss (Shao and Feng, 2022) to train our NAST, which maximizes the F1 score of expected bigram matching between target and alignments"
- Break condition: If the reference corpus is already monotonic or if the non-monotonic alignment cannot be properly learned, this mechanism provides no benefit.

## Foundational Learning

- **Concept**: CTC (Connectionist Temporal Classification) and its collapsing function
  - Why needed here: NAST uses CTC to handle variable-length output and enable blank token generation for adaptive READ/WRITE
  - Quick check question: How does the collapsing function β⁻¹ work to convert alignments with blanks into final translations?

- **Concept**: Non-monotonic alignment and its difference from monotonic alignment
  - Why needed here: NAST uses non-monotonic alignment loss to learn flexible generation patterns that can still produce monotonic translations
  - Quick check question: What is the key difference between monotonic and non-monotonic alignment in the context of simultaneous translation?

- **Concept**: Teacher forcing and its limitations in streaming scenarios
  - Why needed here: NAST avoids teacher forcing to prevent source information leakage, which is crucial for streaming translation
  - Quick check question: Why does teacher forcing cause source information leakage in simultaneous translation?

## Architecture Onboarding

- **Component map**: Source tokens → Unidirectional encoder → Decoder chunks → Partial alignments → Collapsing function → Final translation

- **Critical path**: Source token → Encoder → Decoder chunks → Partial alignments → Collapsing function → Final translation

- **Design tradeoffs**:
  - Parallel decoding vs. target-side dependency capture
  - Fixed vs. adaptive READ/WRITE strategies
  - Translation quality vs. latency control
  - Training complexity vs. inference efficiency

- **Failure signatures**:
  - High hallucination rate indicates source-info leakage issues
  - Poor fluency suggests target dependency capture problems
  - Inability to handle low latency indicates alignment mechanism issues

- **First 3 experiments**:
  1. Verify CTC collapsing function works correctly with simple blank/repetition patterns
  2. Test chunk wait-k strategy impact on translation quality vs latency tradeoff
  3. Validate non-monotonic alignment loss improves source-monotonic generation on a small dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations section, several unresolved questions emerge:
- How does NAST's performance scale to very long sentences compared to autoregressive models?
- How does NAST handle low-resource language pairs with limited training data?
- How does NAST's latency-quality trade-off compare to human interpreters in real-time scenarios?
- How does NAST handle rare or out-of-vocabulary words during simultaneous translation?

## Limitations

- Limited empirical validation of core mechanisms through ablation studies
- Lack of extensive comparisons across diverse latency regimes and language pairs
- Unclear computational overhead of CTC alignment mechanism for real-time deployment
- Weakest claim: non-monotonic alignment training leading to source-monotonic generation lacks sufficient empirical support

## Confidence

- **High Confidence**: Basic architecture design is technically sound and reported BLEU scores are verifiable
- **Medium Confidence**: Claims about non-autoregressive decoding reducing source information leakage are theoretically supported but lack direct empirical validation
- **Low Confidence**: Assertion that non-monotonic alignment training leads to source-monotonic generation during inference has limited empirical support

## Next Checks

1. **Ablation Study on Autoregressive Dependencies**: Implement a controlled experiment comparing NAST against a variant that retains limited autoregressive dependencies to quantify the exact contribution of complete non-autoregressive decoding to latency improvements.

2. **Cross-Lingual Generalization Test**: Evaluate NAST on additional language pairs with varying word order structures to assess the robustness of the non-monotonic alignment mechanism across different syntactic alignments.

3. **Real-Time Deployment Benchmark**: Conduct a comprehensive analysis measuring the actual computational overhead of the CTC alignment mechanism, including GPU memory usage and inference time breakdown, to evaluate practical deployment feasibility under strict latency constraints.