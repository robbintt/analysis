---
ver: rpa2
title: Learning to Reconstruct Accelerated MRI Through K-space Cold Diffusion without
  Noise
arxiv_id: '2311.10162'
source_url: https://arxiv.org/abs/2311.10162
tags:
- image
- diffusion
- k-space
- reconstruction
- cold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a k-space cold diffusion model for accelerated
  MRI reconstruction that performs image degradation and restoration in k-space without
  the need for Gaussian noise. Unlike previous diffusion-based MRI reconstruction
  models that rely on Gaussian noise, the proposed method degrades images by gradually
  increasing the down-sampling ratio in k-space.
---

# Learning to Reconstruct Accelerated MRI Through K-space Cold Diffusion without Noise

## Quick Facts
- arXiv ID: 2311.10162
- Source URL: https://arxiv.org/abs/2311.10162
- Reference count: 38
- Key outcome: K-space cold diffusion model achieves higher PSNR and SSIM than baseline deep learning models on accelerated MRI reconstruction

## Executive Summary
This paper introduces a novel k-space cold diffusion approach for accelerated MRI reconstruction that replaces traditional Gaussian noise-based diffusion with structured k-space under-sampling. The method performs degradation and restoration directly in k-space, gradually increasing the under-sampling ratio over time steps. Evaluated on the fastMRI dataset using Cartesian and Gaussian sampling masks with 4-fold and 8-fold acceleration, the model demonstrates superior performance compared to established baselines including U-Net, W-Net, and E2E-VarNet.

## Method Summary
The approach implements k-space cold diffusion by applying progressively more aggressive under-sampling masks to k-space data during the forward diffusion process. A U-Net architecture learns to reverse this degradation through a linear scheduling of mask proportions across time steps. The model uses an improved sampling strategy that combines current predictions with differences between degraded versions to stabilize reconstruction. Training employs L1 loss on the fastMRI single-coil knee dataset for 700,000 iterations with Adam optimization.

## Key Results
- Cartesian sampling with 4-fold acceleration: 30.58 PSNR and 0.7150 SSIM
- Cartesian sampling with 8-fold acceleration: 29.51 PSNR and 0.6414 SSIM
- Gaussian sampling with 4-fold acceleration: 30.31 PSNR and 0.7059 SSIM
- Gaussian sampling with 8-fold acceleration: 29.59 PSNR and 0.6416 SSIM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: K-space cold diffusion avoids Gaussian noise, using structured k-space down-sampling as the degradation instead.
- Mechanism: By gradually increasing the k-space under-sampling ratio across time steps, the model learns to invert this deterministic degradation, which is more directly aligned with the physics of MRI than random noise.
- Core assumption: K-space under-sampling is a sufficient and effective surrogate for Gaussian noise in diffusion-based training.
- Evidence anchors:
  - [abstract] "Our model performs degradation in k-space during the diffusion process" and "performs image degradation and restoration in k-space without the need for Gaussian noise."
  - [section] "Our model performs degradation in k-space during the diffusion process... A deep neural network is trained to perform the reverse process to recover the original fully sampled image."
  - [corpus] Weak evidence: no direct comparisons to Gaussian noise-based diffusion in cited papers; claims are self-reported.
- Break condition: If the k-space under-sampling fails to sufficiently "blur" or degrade high-frequency information, the network may not learn a meaningful inverse mapping.

### Mechanism 2
- Claim: Linear scheduling of k-space mask proportions over time steps allows smooth degradation and stable training.
- Mechanism: At each time step, a subset of the masked-out k-space data is progressively restored, enabling the network to learn a continuous mapping from highly under-sampled to fully sampled images.
- Core assumption: Linear interpolation of the sampling mask over time steps is adequate for effective degradation and reconstruction.
- Evidence anchors:
  - [section] "Let Mt=0 = J, Mt=T = M... the corresponding image is xt = F −1(Mt ◦ k) = D(x0, t)" and "the sampling mask proportion for intermediate steps are scheduled linearly according to the step number t."
  - [corpus] Weak evidence: no reported experiments testing non-linear schedules or their impact.
- Break condition: If the linear schedule is too coarse or too fine, the network may either not learn meaningful intermediate steps or waste capacity on redundant reconstructions.

### Mechanism 3
- Claim: The improved sampling strategy (Eq. 5) stabilizes reconstruction when the restoration operator is imperfect.
- Mechanism: Instead of directly sampling from the predicted full image, the model adjusts the next step by combining the current prediction with the difference between two degraded versions, reducing drift from the target distribution.
- Core assumption: The degradation function is sufficiently smooth and differentiable for the Taylor expansion to hold.
- Evidence anchors:
  - [section] "Cold diffusion proposed an improved sampling strategy... This sampling strategy enables more reliable reconstructions for cold diffusion models with smaller total step number T."
  - [corpus] Weak evidence: no ablation studies showing impact of this sampling strategy on MRI reconstruction quality.
- Break condition: If the degradation is too non-linear or discontinuous, the sampling adjustment may introduce instability or artifacts.

## Foundational Learning

- Concept: Fourier transform relationship between k-space and image space.
  - Why needed here: The model explicitly operates on k-space data, so understanding how under-sampling in k-space translates to aliasing in image space is critical.
  - Quick check question: If you zero-fill a highly under-sampled k-space, what artifact appears in the image domain?

- Concept: Diffusion models and score-based generative modeling.
  - Why needed here: The approach adapts cold diffusion, a variant of diffusion models, so familiarity with forward/reverse processes and denoising steps is essential.
  - Quick check question: In standard diffusion, what is the role of the learned denoising network during the reverse process?

- Concept: Sampling patterns and their effect on MRI reconstruction quality.
  - Why needed here: Different sampling masks (Cartesian vs Gaussian) impact the aliasing structure and thus the difficulty of reconstruction.
  - Quick check question: How does the distribution of sampled k-space points differ between Cartesian and Gaussian masks, and what does that mean for image aliasing?

## Architecture Onboarding

- Component map:
  Input k-space data -> Apply mask Mt -> Inverse FFT -> Image -> U-Net -> Predicted full image -> L1 loss

- Critical path:
  1. Load under-sampled k-space -> apply mask Mt -> inverse FFT -> image
  2. Pass image through U-Net -> predicted full image
  3. Compute L1 loss -> backpropagate

- Design tradeoffs:
  - Linear vs non-linear mask scheduling: Linear is simpler and faster but may miss finer control over degradation.
  - Single vs multiple sampling: Multiple samples improve robustness but increase compute and may smooth fine details.

- Failure signatures:
  - Poor PSNR/SSIM despite training: Likely issues with mask scheduling, insufficient network capacity, or data normalization.
  - High-frequency artifacts: Possible under-fitting to high-frequency components or instability in sampling strategy.
  - Slow convergence: May indicate suboptimal learning rate or batch size.

- First 3 experiments:
  1. Compare Cartesian vs Gaussian mask reconstructions with fixed network and parameters.
  2. Ablate the improved sampling strategy (Eq. 5) vs naive sampling to measure impact on stability.
  3. Vary total time steps T (e.g., 125, 250, 1000) to assess trade-off between reconstruction quality and inference speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of k-space cold diffusion models scale with different acceleration factors beyond the tested 4-fold and 8-fold?
- Basis in paper: [explicit] The paper tests 4-fold and 8-fold acceleration but does not explore higher or lower acceleration factors.
- Why unresolved: The paper does not provide results for other acceleration factors, and it's unclear if the model maintains its advantage at different acceleration levels.
- What evidence would resolve it: Testing the model with various acceleration factors (e.g., 2-fold, 16-fold) and comparing the results with baseline models would clarify the scalability.

### Open Question 2
- Question: How does the choice of degradation scheduling strategy affect the performance of k-space cold diffusion models?
- Basis in paper: [inferred] The paper uses a linear scheduling for k-space degradation, but it mentions that the degradation process could be further accommodated to encounter differences in frequency regions.
- Why unresolved: The paper does not explore different degradation scheduling strategies or their impact on the model's performance.
- What evidence would resolve it: Experimenting with different degradation scheduling strategies (e.g., non-linear, frequency-aware) and comparing their performance would provide insights into the optimal approach.

### Open Question 3
- Question: How does the k-space cold diffusion model perform with different sampling masks, such as variable density masks or radial masks?
- Basis in paper: [explicit] The paper tests Cartesian and Gaussian sampling masks but does not explore other types of sampling masks.
- Why unresolved: The paper does not provide results for other sampling mask types, and it's unclear if the model maintains its advantage with different masks.
- What evidence would resolve it: Testing the model with various sampling masks (e.g., variable density, radial) and comparing the results with baseline models would clarify the model's adaptability to different sampling strategies.

## Limitations

- The evidence for superiority over Gaussian noise-based diffusion is self-reported without direct comparative experiments.
- The improved sampling strategy's contribution is described but lacks ablation studies demonstrating its specific impact.
- The paper doesn't explore non-linear degradation scheduling or alternative sampling mask types.

## Confidence

- **High**: Reported PSNR/SSIM numbers on fastMRI, U-Net architecture specification, and training setup details.
- **Medium**: Claims about avoiding Gaussian noise and the superiority of k-space degradation over traditional noise-based diffusion.
- **Low**: Claims of being the first to apply cold diffusion to MRI without comparative ablation on noise vs. k-space degradation.

## Next Checks

1. **Baseline ablation**: Compare k-space cold diffusion against a Gaussian noise-based diffusion model on the same dataset and metrics.
2. **Sampling strategy ablation**: Train identical models with and without the improved sampling strategy (Eq. 5) to quantify its impact on reconstruction quality.
3. **Schedule sensitivity**: Test non-linear mask scheduling (e.g., exponential, logarithmic) against the linear schedule to assess sensitivity and potential improvements.