---
ver: rpa2
title: What's "up" with vision-language models? Investigating their struggle with
  spatial reasoning
arxiv_id: '2310.19785'
source_url: https://arxiv.org/abs/2310.19785
tags:
- spatial
- table
- image
- what
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We introduce three benchmarks to isolate and evaluate vision-language
  models'' spatial reasoning abilities: What''sUp (manually captured photos of household
  objects), COCO-spatial (curated from COCO), and GQA-spatial (curated from GQA).
  Across 18 models, all perform poorly on these tasks, with average accuracies far
  below human performance (e.g., 56% vs 99%).'
---

# What's "up" with vision-language models? Investigating their struggle with spatial reasoning

## Quick Facts
- arXiv ID: 2310.19785
- Source URL: https://arxiv.org/abs/2310.19785
- Reference count: 11
- Key outcome: Vision-language models perform poorly on spatial reasoning tasks, with average accuracies around 56% versus human performance at 99% on the authors' benchmarks

## Executive Summary
Vision-language models struggle with spatial reasoning tasks involving prepositions like "under," "left of," and "right of," despite achieving high performance on standard vision-language benchmarks. The authors introduce three new benchmarks (What'sUp, COCO-spatial, and GQA-spatial) to isolate and evaluate spatial reasoning abilities. Across 18 models, all perform significantly worse than humans, suggesting that spatial understanding is not adequately captured during pretraining despite the abundance of spatial information in natural images.

## Method Summary
The authors created three benchmark datasets to evaluate spatial reasoning: What'sUp (manually captured photos of household objects), COCO-spatial (curated from COCO), and GQA-spatial (curated from GQA). They evaluated 18 vision-language models (including CLIP, BLIP, BLIP2, CoCa, and others) using zero-shot inference on these benchmarks. The primary metric was the percentage of images for which models selected the correct caption with the highest image-text matching score. Additional analyses included frequency analysis of spatial prepositions in LAION-2B, vocabulary substitution experiments, and finetuning studies to investigate the source of the performance gap.

## Key Results
- All 18 evaluated models achieved average accuracies around 56% on spatial reasoning benchmarks, compared to human performance of 99%
- Spatial prepositions appear in less than 0.2% of captions in LAION-2B pretraining data
- Models finetuned on downstream tasks (like VQAv2) show near-human performance on those tasks but drop to ~56% on spatial reasoning benchmarks
- Vocabulary sensitivity experiments show significant performance differences when using synonyms like "background" versus "behind"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models fail to learn spatial prepositions because these words are rare and ambiguous in pretraining data, and contrastive learning objectives don't require precise spatial understanding to succeed.
- Mechanism: The LAION-2B corpus contains common spatial prepositions like "under" or "left of" in less than 0.2% of captions. When present, these prepositions can be ambiguous (viewer vs. subject perspective) or extraneous to the image content. Contrastive learning with large batch sizes allows models to perform well by matching general visual concepts without needing to precisely encode spatial relationships.
- Core assumption: The contrastive objective can be optimized without capturing fine-grained spatial relationships between objects.
- Evidence anchors:
  - [abstract]: "that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships"
  - [section 3]: "We find that captions in the corpus contain common spatially specific prepositions like 'under' or 'left of' only 0.2% of the time"
  - [corpus]: LAION-2B prepositions represent less than 0.22% of captions when combined, after removing noise

### Mechanism 2
- Claim: Models can achieve high performance on general vision-language tasks without developing spatial reasoning capabilities because the tasks don't strictly require it.
- Mechanism: Models like BLIP finetuned on VQAv2 achieve near-human performance (56% accuracy) on the authors' benchmarks but only 99% on VQAv2. This suggests models can rely on other cues (object recognition, attribute detection) to succeed at VQAv2 without truly understanding spatial prepositions. The What'sUp benchmark isolates spatial reasoning by keeping objects fixed and varying only prepositions.
- Core assumption: Success on existing vision-language benchmarks doesn't require spatial reasoning.
- Evidence anchors:
  - [abstract]: "BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%"
  - [section 2.3]: "Detailed results including pair and set accuracy for What'sUp, and one- and two-object accuracy for COCO-spatial and GQA-spatial are presented in Appendix Table 3"
  - [corpus]: Models show very poor pair and set accuracy, showing their lack of understanding of the concept of each preposition

### Mechanism 3
- Claim: Models' spatial understanding may be present but inaccessible due to vocabulary mismatch between training data and evaluation prompts.
- Mechanism: The word "background" appears in 0.84% of LAION captions and provides spatial information, while "behind" (tested in What'sUp) appears much less frequently. When models are given prompts using "background" instead of "behind," performance improves significantly (67% vs 52% for CLIP ViT-B/32). This suggests spatial knowledge exists but is tied to specific vocabulary learned during training.
- Core assumption: Models can learn spatial concepts but only associate them with words that appear frequently in training data.
- Evidence anchors:
  - [section 4.2]: "This word alone appears in 0.84% of the captions, four times more than all of the other prepositions we study combined"
  - [section 4.2]: "performance on (1) is an average of 52%, just two points above random chance, whereas performance on (2) is an average of 67%"
  - [corpus]: LAION-2B frequency data shows "background" appears much more frequently than spatial prepositions

## Foundational Learning

- Concept: Preposition semantics and perspective
  - Why needed here: Models must understand that spatial prepositions depend on perspective (viewer vs. subject) and that the same preposition can have different visual instantiations (e.g., "under" for a ball under a desk vs. under water)
  - Quick check question: Can you explain why "the mug is under the table" could be true from the camera's perspective but false from the mug's perspective if the mug is on a glass table?

- Concept: Contrastive learning objectives
  - Why needed here: Understanding how contrastive learning works (matching image embeddings to text embeddings in large batches) explains why models can succeed without spatial reasoning - they just need to match general visual concepts
  - Quick check question: In CLIP training, if an image of a dog under a table is paired with "a dog on a table," could the model still learn to match them correctly based on the dog alone?

- Concept: Data distribution and bias
  - Why needed here: Understanding that LAION captions reflect real-world biases (mugs usually on tables, not under them) explains why models develop priors that hurt spatial reasoning performance
  - Quick check question: If you search Google Images for "a mug under a table," why might you get few relevant results, and how would this affect model training?

## Architecture Onboarding

- Component map: Image encoder (ViT, ResNet, etc.) -> Text encoder (Transformer) -> Similarity function (cosine similarity/dot product) -> Prediction
- Critical path: Image/text encoding -> embedding generation -> similarity scoring -> prediction
- Design tradeoffs:
  - Two-stack (separate encoders) vs. one-stack (cross-attention): Two-stack is more modular but may miss fine-grained interactions; one-stack can better capture multimodal interactions but is more complex
  - Contrastive vs. generative objectives: Contrastive is efficient for retrieval but may not encourage compositional understanding; generative can better capture language structure but requires more compute
  - Model size: Larger models have more capacity but diminishing returns for spatial reasoning in current scale

- Failure signatures:
  - Random or near-random performance across all spatial prepositions
  - Consistent bias toward certain prepositions (e.g., always predicting "on" or "under")
  - Good performance on one-object spatial relations but poor on two-object relations
  - Performance improves when vocabulary is changed to match training data (e.g., "background" vs "behind")

- First 3 experiments:
  1. Evaluate model on What'sUp pair-wise accuracy: measure if model correctly distinguishes opposing prepositions (on vs under, left vs right) for the same object pair
  2. Test vocabulary sensitivity: evaluate model performance when spatial prepositions are replaced with synonyms or related terms from LAION (e.g., "background" vs "behind")
  3. Measure text-only priors: calculate the average similarity of each caption option to a large set of unrelated images to quantify how much performance depends on caption priors vs image understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model size beyond the scales studied lead to improved spatial reasoning performance on the benchmarks?
- Basis in paper: [inferred] The paper observes that scaling up model size does not necessarily improve spatial reasoning capabilities (e.g., CLIP ViT-B/32 outperforms CLIP ViT-L/14).
- Why unresolved: The study only examined models up to 129M parameters. Larger models might have different spatial reasoning abilities.
- What evidence would resolve it: Scaling experiments with much larger models (e.g., 1B+ parameters) evaluated on the same benchmarks.

### Open Question 2
- Question: Can spatial reasoning be learned effectively through contrastive training with auto-generated hard negatives?
- Basis in paper: [explicit] The authors attempted finetuning CLIP ViT-B/32 on LAION-4M with programmatically switched prepositions as hard negatives, but observed very high training loss and inability to consistently disambiguate between correct and incorrect captions.
- Why unresolved: The failure could be due to the specific training regime, dataset size, or inherent limitations of contrastive learning for spatial relations.
- What evidence would resolve it: Successful training of models with contrastive objectives using various hard negative generation strategies and supervision signals.

### Open Question 3
- Question: Is spatial reasoning a fundamental capability that requires specific architectural inductive biases rather than just more data?
- Basis in paper: [explicit] The authors note that XVLM (with bounding-box supervision) outperforms other models, suggesting supervision matters. They also observe that finetuning on relevant data only provides marginal improvements.
- Why unresolved: The paper tests several modeling interventions but finds none dramatically improve performance, leaving open whether architecture or data is the limiting factor.
- What evidence would resolve it: Comparative studies of models with different architectural inductive biases (e.g., explicit spatial attention mechanisms) trained on identical data.

## Limitations
- The evaluation relies on zero-shot inference without task-specific fine-tuning, which may underestimate models' true spatial reasoning capabilities
- The LAION-2B frequency analysis is based on a 2-billion subset rather than the full corpus, potentially introducing sampling bias
- The study doesn't explore whether larger models or longer training schedules would improve spatial reasoning performance

## Confidence
- **High Confidence**: The empirical finding that all 18 evaluated models perform significantly worse than humans on spatial reasoning benchmarks (average ~56% vs 99% accuracy). The data clearly shows this performance gap across multiple datasets and model architectures.
- **Medium Confidence**: The explanation that spatial prepositions are rare and ambiguous in pretraining data, leading to poor spatial reasoning. While the frequency analysis is sound, the causal link between data scarcity and performance requires additional validation.
- **Medium Confidence**: The claim that models can succeed on VQAv2 without spatial reasoning. The performance comparison is valid, but alternative explanations (such as VQAv2's different question distribution) need consideration.

## Next Checks
1. **Vocabulary Generalization Test**: Create a systematic evaluation where spatial prepositions are systematically replaced with synonyms and related terms from LAION-2B. Measure whether performance improvements correlate with term frequency in pretraining data, distinguishing between true spatial knowledge and vocabulary memorization.

2. **Controlled Data Augmentation**: Generate synthetic training data with balanced spatial preposition distributions and retrain a subset of models. Compare spatial reasoning performance to original models to isolate the effect of preposition frequency on learning spatial concepts.

3. **Attention Pattern Analysis**: For models that perform relatively better on spatial tasks, analyze attention maps when processing images with clear spatial relationships. Identify whether attention consistently focuses on relevant object pairs and spatial configurations, providing evidence for or against the presence of spatial reasoning mechanisms.