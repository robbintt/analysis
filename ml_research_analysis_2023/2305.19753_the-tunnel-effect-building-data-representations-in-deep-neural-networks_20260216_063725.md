---
ver: rpa2
title: 'The Tunnel Effect: Building Data Representations in Deep Neural Networks'
arxiv_id: '2305.19753'
source_url: https://arxiv.org/abs/2305.19753
tags:
- rank
- numerical
- tunnel
- linear
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates a phenomenon in deep neural networks called
  the "tunnel effect", where the network splits into two distinct parts during training:
  an "extractor" that builds linearly-separable representations and a "tunnel" that
  compresses these representations with minimal impact on final performance. The tunnel
  emerges early in training and its depth depends on the relation between network
  capacity and task complexity.'
---

# The Tunnel Effect: Building Data Representations in Deep Neural Networks

## Quick Facts
- arXiv ID: 2305.19753
- Source URL: https://arxiv.org/abs/2305.19753
- Reference count: 40
- Key outcome: Deep networks naturally split into an extractor that builds linearly-separable representations and a tunnel that compresses these representations with minimal impact on final performance, degrading OOD generalization and hindering continual learning through catastrophic forgetting.

## Executive Summary
This paper investigates a phenomenon in deep neural networks called the "tunnel effect", where networks split into two distinct parts during training: an "extractor" that builds linearly-separable representations and a "tunnel" that compresses these representations with minimal impact on final performance. The tunnel emerges early in training and its depth depends on the relation between network capacity and task complexity. The paper shows that the tunnel degrades out-of-distribution generalization and can hinder continual learning by causing catastrophic forgetting. The tunnel is found to be task-agnostic, meaning it can be freely mixed with different extractors without affecting performance. These findings offer new perspectives on studying catastrophic forgetting at specific layers and suggest that training shallower networks with at least the same capacity as the extractor part of the original network can reduce forgetting.

## Method Summary
The study investigates the tunnel effect across multiple architectures (VGG-19, ResNet-34, MLP) trained on CIFAR-10, CIFAR-100, and CINIC-10 datasets. The method involves training networks with standard hyperparameters, saving intermediate checkpoints, computing layer-wise weight changes, and tracking the evolution of numerical rank throughout training. Key metrics include linear probing accuracy, numerical rank of representations, CKA similarity, and inter-class/intra-class variance. The analysis focuses on identifying the tunnel start point, evaluating linear probing performance on OOD datasets, and analyzing representation rank and similarity.

## Key Results
- Deep networks naturally split into an extractor (early layers) that builds linearly-separable representations and a tunnel (later layers) that compresses these representations with minimal impact on final performance
- The tunnel degrades out-of-distribution generalization by compressing representations to low numerical rank
- The tunnel exhibits task-agnostic behavior in continual learning, leading to higher catastrophic forgetting when the number of classes differs between tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep networks naturally split into an extractor that builds linearly-separable representations and a tunnel that compresses these representations with minimal impact on final performance.
- Mechanism: During training, early layers form task-specific, linearly separable representations (extractor), while later layers progressively compress these representations into a low-rank form (tunnel) without improving linear separability.
- Core assumption: Overparameterized networks can allocate fixed capacity for representation building independent of overall model depth.
- Evidence anchors:
  - [abstract]: "The initial layers create linearly-separable representations, while the subsequent layers... compress these representations and have a minimal impact on the overall performance."
  - [section]: "The first part, which we call the extractor, builds representations, while the other, dubbed the tunnel, propagates the representations further to the model's output, compressing them significantly."
  - [corpus]: Weak—no direct corpus match for "tunnel effect" or similar terms.
- Break condition: If representations rank does not collapse in deeper layers or if linear probe accuracy continues to improve beyond the extractor layers.

### Mechanism 2
- Claim: The tunnel degrades out-of-distribution (OOD) generalization by compressing representations to a low numerical rank.
- Mechanism: Low-rank representations in the tunnel discard task-irrelevant features, making them less transferable to OOD data that require broader feature coverage.
- Core assumption: OOD performance is tightly coupled with the numerical rank of representations.
- Evidence anchors:
  - [section]: "We show that the tunnel deteriorates the generalization ability on out-of-distribution data."
  - [section]: "The tunnel degrades the out-of-distribution performance of the model which is tightly coupled with the drop of representations rank."
  - [corpus]: Weak—no corpus evidence for this specific claim.
- Break condition: If OOD performance does not drop in correspondence with representation rank collapse, or if low-rank representations perform equally well on OOD tasks.

### Mechanism 3
- Claim: The tunnel exhibits task-agnostic behavior in continual learning, causing catastrophic forgetting.
- Mechanism: The tunnel compresses representations in a task-agnostic way, allowing it to be mixed with different extractors without affecting performance. However, this leads to higher forgetting as the extractor part is task-specific.
- Core assumption: The tunnel is not specific to the training task and compresses representations in a task-agnostic way.
- Evidence anchors:
  - [section]: "We show that the tunnel exhibits task-agnostic behavior in a continual learning scenario. Simultaneously it leads to higher catastrophic forgetting of the model."
  - [section]: "In any combination changing T1 to T2 or vice versa have a marginal impact on the performance. This is quite remarkable, and suggests that the tunnel is not specific to the training task."
  - [corpus]: Weak—no corpus evidence for this claim.
- Break condition: If the tunnel performance degrades significantly when combined with different extractors, or if catastrophic forgetting is not observed.

## Foundational Learning

- Concept: Linear separability and numerical rank of representations.
  - Why needed here: To understand how the extractor builds task-specific features and how the tunnel compresses them.
  - Quick check question: Can you explain how linear separability and numerical rank are measured and why they matter for network performance?

- Concept: Continual learning and catastrophic forgetting.
  - Why needed here: To understand the implications of the tunnel's task-agnostic behavior on model adaptability.
  - Quick check question: What is catastrophic forgetting, and how does it relate to the extractor and tunnel parts of the network?

- Concept: Out-of-distribution (OOD) generalization.
  - Why needed here: To grasp why the tunnel degrades performance on OOD data.
  - Quick check question: How does the numerical rank of representations affect a model's ability to generalize to OOD data?

## Architecture Onboarding

- Component map: Extractor (early layers) -> Tunnel (later layers)
- Critical path: Train the network, monitor linear probe accuracy and numerical rank at each layer, identify the tunnel start point
- Design tradeoffs: Deeper or wider networks result in longer tunnels, potentially improving in-distribution performance but degrading OOD generalization and increasing forgetting in continual learning
- Failure signatures: If the numerical rank does not collapse in deeper layers, or if linear probe accuracy continues to improve beyond the extractor layers, the tunnel effect may not be present
- First 3 experiments:
  1. Train a VGG-19 on CIFAR-10, attach linear probes to each layer, and plot linear probe accuracy vs. layer number
  2. Compute the numerical rank of representations at each layer and compare it with the linear probe accuracy
  3. Evaluate the model on OOD data (e.g., CIFAR-100) and observe the correlation between performance drop and representation rank collapse

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms drive the tunnel effect in overparameterized deep neural networks, and how can they be theoretically modeled or explained?
- Basis in paper: [explicit] The paper identifies the tunnel effect but acknowledges that further theoretical research is needed to explain the underlying mechanisms.
- Why unresolved: While empirical evidence supports the existence of the tunnel effect, a comprehensive theoretical framework explaining its origins and dynamics is lacking.
- What evidence would resolve it: Development of a theoretical model that accurately predicts the emergence and characteristics of the tunnel effect across different architectures and datasets, supported by rigorous mathematical proofs and extensive empirical validation.

### Open Question 2
- Question: How does the tunnel effect influence the generalization capabilities of neural networks on out-of-distribution data, and can this impact be mitigated or leveraged for improved performance?
- Basis in paper: [explicit] The paper demonstrates that the tunnel degrades out-of-distribution generalization and suggests that understanding this relationship could lead to improved model robustness.
- Why unresolved: While the correlation between the tunnel effect and reduced OOD performance is established, the exact causal mechanisms and potential mitigation strategies remain unexplored.
- What evidence would resolve it: Experimental results showing how modifying the tunnel characteristics (e.g., through architectural changes or training techniques) affects OOD generalization, along with theoretical insights into why these modifications work.

### Open Question 3
- Question: In what ways does the tunnel effect impact catastrophic forgetting in continual learning scenarios, and how can this knowledge be used to design more effective continual learning algorithms?
- Basis in paper: [explicit] The paper shows that the tunnel exhibits task-agnostic behavior, which provides immunity against catastrophic forgetting when the number of classes is equal, but also leads to higher forgetting when the number of classes differs.
- Why unresolved: The paper identifies the tunnel's role in catastrophic forgetting but does not provide a comprehensive framework for leveraging this knowledge to design better continual learning algorithms.
- What evidence would resolve it: Development and empirical validation of continual learning algorithms that explicitly account for the tunnel effect, demonstrating improved performance on tasks with varying numbers of classes compared to existing methods.

## Limitations

- The empirical observation of the tunnel effect is well-supported, but the generality across different model families and tasks remains unclear
- The mechanistic explanation connecting representation rank collapse to OOD degradation and forgetting is plausible but not rigorously proven
- The sample size of tasks used to demonstrate task-agnostic behavior is limited

## Confidence

- High: The empirical observation of the tunnel effect itself (layer-wise linear probe accuracy plateaus, rank collapse in later layers)
- Medium: The claim that tunnel depth scales with network capacity vs task complexity
- Low: The specific mechanistic explanations for OOD degradation and catastrophic forgetting

## Next Checks

1. Test whether training shallower networks with equal extractor capacity (rather than equal total capacity) consistently reduces forgetting across multiple continual learning benchmarks
2. Verify if the rank collapse in the tunnel layers is causal to OOD performance by attempting to regularize rank during training and measuring impact
3. Check if the tunnel effect appears in transformer architectures and other non-convolutional models to establish generality beyond CNNs