---
ver: rpa2
title: Distilling ChatGPT for Explainable Automated Student Answer Assessment
arxiv_id: '2305.12962'
source_url: https://arxiv.org/abs/2305.12962
tags:
- student
- rationale
- answer
- chatgpt
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for explainable automated
  student answer assessment using ChatGPT. The approach leverages ChatGPT to generate
  rationales for student answers, which are then refined and used to fine-tune a smaller
  language model for scoring and rationale generation.
---

# Distilling ChatGPT for Explainable Automated Student Answer Assessment

## Quick Facts
- arXiv ID: 2305.12962
- Source URL: https://arxiv.org/abs/2305.12962
- Reference count: 40
- Key outcome: Achieves 11% improvement in QWK score and generates more detailed, comprehensible rationales for student answer assessment

## Executive Summary
This paper introduces a novel framework that leverages ChatGPT's few-shot prompting capability to generate rationales for student answer assessment. The generated rationales are refined and used to fine-tune a smaller language model, creating an efficient system that both scores answers and provides explanations. The approach demonstrates significant improvements over using ChatGPT directly and traditional text classification methods, while maintaining explainability in the assessment process.

## Method Summary
The framework uses ChatGPT to generate rationales for student answers through few-shot prompting with demonstration examples. These rationales are then refined to align with marking standards and used to fine-tune a smaller language model (such as T5) for practical deployment. The system also employs semantic confidence intervals to identify potentially noisy data and human labeling errors by computing confidence from multiple ChatGPT outputs.

## Key Results
- Achieves 11% improvement in QWK score compared to ChatGPT baseline
- Generates more detailed and comprehensible rationales than traditional text classification methods
- Demonstrates the effectiveness of knowledge distillation from ChatGPT to smaller models for practical deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can generate high-quality rationales for student answer assessment through few-shot prompting
- Mechanism: By providing example student answers with correct scores and rationales, ChatGPT learns the assessment pattern and generates rationales following the same structure
- Core assumption: ChatGPT's in-context learning capability is sufficient to capture the relationship between student answers, key elements, rubrics, and appropriate scores

### Mechanism 2
- Claim: Fine-tuning a smaller language model on ChatGPT-generated rationales produces a more efficient assessment system
- Mechanism: ChatGPT acts as a teacher model, generating rationales that are used to train a smaller student model (like T5), transferring knowledge while maintaining efficiency
- Core assumption: The rationales generated by ChatGPT contain sufficient information to train a smaller model for both scoring and rationale generation

### Mechanism 3
- Claim: Semantic confidence intervals can identify noisy data and human label errors in the training set
- Mechanism: By computing confidence intervals from multiple ChatGPT outputs, instances with low confidence or high disagreement can be flagged as potentially problematic
- Core assumption: When ChatGPT is uncertain about an answer, it often indicates either data quality issues or human labeling errors

## Foundational Learning

- **Concept: In-context learning**
  - Why needed here: The framework relies on ChatGPT's ability to learn from few examples without fine-tuning
  - Quick check question: Can you explain the difference between in-context learning and traditional fine-tuning?

- **Concept: Knowledge distillation**
  - Why needed here: The approach transfers knowledge from the large ChatGPT model to a smaller, more efficient model for practical deployment
  - Quick check question: What are the key differences between teacher-student knowledge distillation and simple model compression?

- **Concept: Rationale generation evaluation**
  - Why needed here: The framework needs to assess the quality of generated rationales without ground truth rationales available
  - Quick check question: How would you evaluate the quality of free-form rationales when no human-annotated rationales exist?

## Architecture Onboarding

- **Component map**: Input (Question, Key Elements, Rubric, Student Answer) → ChatGPT Teacher (generates rationales) → Confidence Filter (computes semantic confidence intervals) → Data Refinement (corrects errors) → Student Model Trainer (fine-tunes T5) → Output (Score and rationale)

- **Critical path**: ChatGPT → Confidence Filtering → Data Refinement → Student Model Training → Inference

- **Design tradeoffs**:
  - Accuracy vs efficiency: Using ChatGPT directly vs fine-tuned smaller model
  - Data quantity vs quality: More ChatGPT outputs vs filtered, higher-quality subset
  - Complexity vs interpretability: Simple prompts vs comprehensive instructions

- **Failure signatures**:
  - Poor student model performance: Indicates issues with ChatGPT rationales quality or insufficient training data
  - High variance across runs: Suggests sensitivity to random seeds or unstable training process
  - Confidence intervals not correlating with accuracy: Indicates the confidence measure isn't capturing true uncertainty

- **First 3 experiments**:
  1. Test ChatGPT's few-shot prompting capability with a small subset of data to verify it can generate reasonable rationales
  2. Implement semantic confidence interval calculation and validate it identifies known problematic instances
  3. Fine-tune a small language model on ChatGPT-generated rationales and measure performance on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of rationales generated by ChatGPT compare to human-written rationales in terms of accuracy and comprehensiveness?
- Basis in paper: The paper mentions that rationales generated by ChatGPT are compared to those of the model, but does not provide a direct comparison to human-written rationales.
- Why unresolved: The paper does not provide a quantitative comparison between ChatGPT-generated rationales and human-written rationales, making it difficult to assess the relative quality of the two.
- What evidence would resolve it: A direct comparison of ChatGPT-generated rationales to human-written rationales using metrics such as BLEU score or human evaluation would provide insight into the relative quality of the two.

### Open Question 2
- Question: How does the performance of the fine-tuned model vary across different datasets and subjects (e.g., Science, Biology, English, Art)?
- Basis in paper: The paper mentions that the performance of the fine-tuned model is evaluated on four subsets of the ASAP-SAS dataset, covering Science and Biology subjects.
- Why unresolved: The paper does not provide a detailed analysis of the performance differences across different datasets and subjects, making it difficult to understand the model's generalizability.
- What evidence would resolve it: A more comprehensive evaluation of the model's performance across different datasets and subjects, along with an analysis of the factors contributing to performance differences, would provide insights into the model's generalizability.

### Open Question 3
- Question: How does the inclusion of additional information (e.g., Question, Key Elements, Rubric) in the input affect the performance of the model in student answer assessment?
- Basis in paper: The paper mentions that the Longformer-all model, which includes additional information in the input, does not show improved performance compared to the Longformer model.
- Why unresolved: The paper does not provide a detailed analysis of the impact of including additional information on the model's performance, making it difficult to understand the optimal input format for student answer assessment.
- What evidence would resolve it: A systematic evaluation of the model's performance with different input formats, including varying amounts of additional information, would provide insights into the optimal input format for student answer assessment.

## Limitations

- The framework's performance heavily depends on ChatGPT's ability to generate accurate rationales through few-shot prompting, which is not extensively validated
- The semantic confidence interval mechanism for identifying noisy data lacks empirical validation
- The approach assumes provided rubrics and key elements are sufficient for ChatGPT to generate accurate rationales, which may not hold for highly subjective or complex assessment criteria

## Confidence

- **High Confidence**: The core concept of using ChatGPT to generate rationales and fine-tuning smaller models for efficiency is technically sound and aligns with established knowledge distillation principles
- **Medium Confidence**: The few-shot prompting effectiveness and confidence interval utility are plausible but lack sufficient empirical validation
- **Low Confidence**: The claim about generating more detailed and comprehensible rationales than traditional methods is subjective and not directly compared with specific baselines

## Next Checks

1. **Few-shot prompting validation**: Systematically test ChatGPT's rationale generation quality across different numbers of examples (0, 1, 3, 5, 10 shots) on a held-out validation set

2. **Confidence interval correlation**: Measure the actual correlation between ChatGPT's semantic confidence intervals and prediction accuracy on a validation set

3. **Generalization stress test**: Evaluate the fine-tuned student model on questions with significantly different formats or domains than the training data to assess robustness