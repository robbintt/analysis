---
ver: rpa2
title: Out-Of-Domain Unlabeled Data Improves Generalization
arxiv_id: '2310.00027'
source_url: https://arxiv.org/abs/2310.00027
tags:
- unlabeled
- data
- have
- samples
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel framework for incorporating unlabeled
  out-of-domain data into semi-supervised classification problems, focusing on both
  robust and non-robust loss functions. The key idea is to combine Distributionally
  Robust Optimization (DRO) with self-supervised training, allowing unlabeled samples
  to deviate slightly from the in-domain distribution.
---

# Out-Of-Domain Unlabeled Data Improves Generalization

## Quick Facts
- arXiv ID: 2310.00027
- Source URL: https://arxiv.org/abs/2310.00027
- Authors: 
- Reference count: 40
- Primary result: RSS training achieves improved generalization bounds compared to ERM when unlabeled data is out-of-domain

## Executive Summary
This paper presents a novel framework for incorporating unlabeled out-of-domain data into semi-supervised classification problems. The method, called Robust Self-Supervised (RSS) training, combines Distributionally Robust Optimization (DRO) with self-supervised training to allow unlabeled samples to deviate slightly from the in-domain distribution. Theoretical analysis on a binary classification problem involving two Gaussian models in Rd shows that RSS training achieves improved generalization bounds compared to Empirical Risk Minimization (ERM), particularly when the number of unlabeled samples is sufficiently large relative to labeled samples and dimensionality.

## Method Summary
The RSS framework addresses semi-supervised classification by leveraging both labeled and unlabeled data through a novel combination of DRO and self-training. The method works by artificially labeling unlabeled data using the current model, then applying robust optimization techniques that allow for small distribution shifts. The core innovation is the use of Wasserstein distance to define an uncertainty neighborhood around the data distribution, enabling controlled adversarial perturbations that improve generalization. The framework achieves better generalization bounds than traditional ERM approaches when unlabeled data is abundant relative to the dimensionality of the problem space.

## Key Results
- RSS training improves generalization bounds when unlabeled data is out-of-domain
- The method achieves faster convergence rates compared to empirical risk minimization
- Experimental results on synthetic and real-world datasets validate the theoretical improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RSS training improves generalization bounds when unlabeled data is out-of-domain
- Mechanism: The framework combines DRO with self-supervised training, allowing unlabeled samples to deviate slightly from the in-domain distribution. This creates regularization that forces the classifier to avoid crowded areas and improves robustness.
- Core assumption: The true data distribution adheres to a "cluster assumption" where classes form distinct clusters in feature space
- Evidence anchors:
  - [abstract]: "The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training."
  - [section]: "Theorem 4.1 presents a generalization bound for the proposed estimator when one considers the robust loss under an adversarial budget..."
  - [corpus]: Weak - only general semi-supervised learning papers found, no direct mention of DRO + self-training combination
- Break condition: If the cluster assumption fails or if the distributional shift between labeled and unlabeled data is too large

### Mechanism 2
- Claim: The adversarial budget γ controls the tradeoff between bias and variance in classification error
- Mechanism: By regulating the diameter of the uncertainty neighborhood based on the number of samples, RSS training achieves a balance that leads to faster convergence rates compared to ERM
- Core assumption: The cost function c is strictly convex and the loss function ℓ is sufficiently smooth
- Evidence anchors:
  - [abstract]: "we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution"
  - [section]: "The sensitivity of such regularization is controlled by both λ and also γ ′"
  - [corpus]: Weak - no direct evidence about uncertainty neighborhood diameter regulation found
- Break condition: If the cost function is not strictly convex or if the loss function is not smooth enough for the gradient to exist

### Mechanism 3
- Claim: The trade-off between labeled (m) and unlabeled (n) sample sizes follows n ≥ Ω(m²/d) for improved generalization
- Mechanism: When n is sufficiently large relative to m and the dimensionality d, the residual term in the generalization bound decays faster, surpassing traditional techniques that solely rely on labeled data
- Core assumption: The unlabeled data distribution is within a controlled Wasserstein distance α from the in-domain distribution
- Evidence anchors:
  - [abstract]: "Using only the labeled data, it is known that the generalization error can be bounded by ∝(d/m)¹/². However, using our method... one can derive a new set of analytically explicit and non-asymptotic bounds which show substantial improvement on the generalization error compared ERM"
  - [section]: "Our results show that as long as n ≥ Ω(m²/d), our proposed algorithm surpasses traditional techniques that solely rely on labeled data"
  - [corpus]: Weak - no direct mention of the specific m²/d relationship found
- Break condition: If n < Ω(m²/d) or if α exceeds the threshold where unlabeled data becomes harmful

## Foundational Learning

- Concept: VC-dimension and generalization bounds
  - Why needed here: The paper builds on classical VC-theory results to show how RSS training improves upon the O(VCdim(H)/m)¹/² bound for generalization error
  - Quick check question: What is the VC-dimension of linear classifiers in Rd, and how does it affect the generalization bound?

- Concept: Wasserstein distance and distributionally robust optimization
  - Why needed here: The framework uses Wasserstein distance to define an uncertainty neighborhood around the data distribution, allowing for adversarial perturbations
  - Quick check question: How does the Wasserstein distance between two distributions relate to their support and cost function?

- Concept: Self-training and pseudo-labeling
  - Why needed here: RSS training uses the current model to generate pseudo-labels for unlabeled data, which are then incorporated into the loss function
  - Quick check question: What are the risks of self-training, and how does the robust regularization term mitigate them?

## Architecture Onboarding

- Component map:
  Labeled data stream -> Robust loss computation φγ (X,y;θ)
  Unlabeled data stream -> Pseudo-label generation hθ(X′) -> Robust regularization φγ′ (X′,hθ(X′);θ)
  Optimization loop -> Inner maximization (adversarial perturbation) -> Outer minimization (parameter update)
  Hyperparameter controller -> Manages γ, γ′, λ values based on sample sizes and distributional shift

- Critical path:
  1. Load labeled and unlabeled data batches
  2. Compute adversarial perturbations for both streams
  3. Calculate robust losses φγ and φγ′
  4. Combine losses with weighting λ
  5. Backpropagate and update model parameters
  6. Repeat until convergence

- Design tradeoffs:
  - Larger γ provides more robustness but may increase bias
  - Smaller λ reduces unlabeled contribution but may miss regularization benefits
  - Higher-dimensional data requires exponentially more unlabeled samples for benefit

- Failure signatures:
  - Training loss decreases but validation accuracy plateaus or degrades
  - Model becomes overconfident on pseudo-labeled data, ignoring true labels
  - Gradient norms explode during adversarial perturbation computation

- First 3 experiments:
  1. Synthetic Gaussian mixture model with d=50, m=100 labeled, n=10000 unlabeled, varying α
  2. Same setup but with non-isotropic covariances to test general GMM case
  3. Histopathology dataset with labeled from NCT-CRC-HE-100K and unlabeled from PatchCamelyon

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Theoretical guarantees assume the cluster assumption holds for data distributions, which may not be valid for all real-world datasets
- Analysis is limited to binary classification with Gaussian mixture models, extension to multi-class or non-Gaussian distributions remains unclear
- Sensitivity to hyperparameters γ, γ′, and λ is not fully characterized, particularly for datasets with complex decision boundaries

## Confidence
- High confidence: The mathematical framework combining DRO with self-training is sound and the theoretical bounds are correctly derived for the binary Gaussian case
- Medium confidence: Experimental results showing accuracy improvements on histopathology datasets are promising but limited in scope
- Low confidence: Generalization of results to high-dimensional, non-Gaussian real-world data without further empirical validation

## Next Checks
1. Test the RSS framework on multi-class classification problems with non-Gaussian distributions to verify theoretical bounds extend beyond the binary case
2. Conduct ablation studies systematically varying γ, γ′, and λ to identify optimal hyperparameter regimes across different dataset characteristics
3. Evaluate performance when unlabeled data distribution shift α exceeds the controlled perturbation threshold to determine failure conditions