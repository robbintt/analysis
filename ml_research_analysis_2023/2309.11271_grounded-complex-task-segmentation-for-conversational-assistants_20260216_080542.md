---
ver: rpa2
title: Grounded Complex Task Segmentation for Conversational Assistants
arxiv_id: '2309.11271'
source_url: https://arxiv.org/abs/2309.11271
tags:
- conversational
- steps
- task
- step
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structuring web-based cooking
  instructions into a format suitable for conversational assistants, where users have
  shorter attention spans and benefit from information presented in smaller, manageable
  steps. The authors collected and annotated a new corpus, ConvRecipes, where recipes
  are segmented into dialogue-suited steps.
---

# Grounded Complex Task Segmentation for Conversational Assistants

## Quick Facts
- arXiv ID: 2309.11271
- Source URL: https://arxiv.org/abs/2309.11271
- Authors: 
- Reference count: 40
- Primary result: T5-3B Enc-only model improved conversational structure of 86% of evaluated tasks with F1 score of 80.0% on test set

## Executive Summary
This paper addresses the challenge of structuring web-based cooking instructions into a format suitable for conversational assistants, where users have shorter attention spans and benefit from information presented in smaller, manageable steps. The authors collected and annotated a new corpus, ConvRecipes, where recipes are segmented into dialogue-suited steps. A human study revealed that users prioritize manageable complexity and length in conversational steps. To model this task computationally, they proposed the Dialogue-Task Segmenter Transformer (DTS-Transformer), a token-level Transformer architecture that outperforms existing baselines. Experiments showed that the best model, T5-3B Enc-only, improved the conversational structure of 86% of evaluated tasks, with an F1 score of 80.0% on the test set, demonstrating the effectiveness of the proposed methodology.

## Method Summary
The authors created the ConvRecipes corpus by annotating 300 recipes with conversational step segmentation, using 1930 recipes for training and 424 for validation. They proposed a token-level Transformer-based Dialogue-Task Segmenter (DTS) that processes the entire recipe text and predicts segmentation points at the token level. The model is trained using cross-entropy loss with variants including BERT and T5 backbones, with T5-3B Enc-only achieving the best performance. The approach grounds segmentation in the original task text rather than generating new instructions, avoiding potential hallucinations. Model performance is evaluated using F1 score, Pk metric, and a user study comparing conversational suitability between original and segmented recipes.

## Key Results
- T5-3B Enc-only achieved F1 score of 80.0% on the test set, outperforming sentence-level baselines
- Model improved conversational structure of 86% of evaluated tasks in user study (scored 3.72 vs 2.63 for original recipes)
- Token-level segmentation outperformed sentence-level approaches by better capturing conversational step characteristics
- The ConvRecipes corpus enabled effective training of transformer models for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversational steps should be short, simple, and aligned with users' limited attention spans.
- Mechanism: The paper converts web-based recipes into dialogue-suited ones by splitting long steps into smaller ones, ensuring each step is concise and contains fewer verbs and nouns, reducing cognitive load.
- Core assumption: Users of conversational assistants have shorter attention and memory spans than when reading instructions.
- Evidence anchors:
  - [abstract] "Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions."
  - [section] "The distribution of linguistic characteristics such as length, verbs (which cover actions), and nouns (covering ingredients, tools, etc.), confirms that dialog-suited instructions should avoid overwhelming users’ short-term memory (Miller, 1956; Cowan, 2001)."
- Break condition: If the complexity or length of steps is not managed, users may struggle to follow instructions, leading to task abandonment.

### Mechanism 2
- Claim: Grounding segmentation on the original task's steps avoids introducing hallucinations or errors common in generative approaches.
- Mechanism: Instead of rewriting or generating new instructions, the model segments the original task text into conversational steps, maintaining the integrity of the original content.
- Core assumption: Rewriting approaches can introduce mistakes or hallucinations in step-by-step procedures.
- Evidence anchors:
  - [abstract] "We do this to avoid the risk of introducing hallucinations or mistakes in step-by-step procedures (Choi et al., 2022)."
  - [section] "This approach makes the segmentation grounded on the original task, avoiding the introduction of mistakes prone to happen when using rewriting approaches."
- Break condition: If the original task text is ambiguous or poorly structured, grounding may not fully resolve conversational suitability issues.

### Mechanism 3
- Claim: Token-level segmentation with Transformers captures conversational step structure better than sentence-level or cross-segment approaches.
- Mechanism: The Dialogue-Task Segmenter Transformer (DTS-Transformer) processes the entire task text at the token level, allowing the model to consider the global task structure and make segmentation decisions based on fine-grained contextual information.
- Core assumption: Modeling at a token-level granularity abstracts less information than sentence-based or cross-segment approaches, enabling better capture of conversational step characteristics.
- Evidence anchors:
  - [section] "Distinct from previous work, we follow a token-level approach which by modeling steps’ text at a finer granularity, is capable of better modeling the inherent structural characteristics of conversational tasks."
  - [section] "A fundamental difference between DTS and other supervised approaches is that it tackles the conversational recipe structuring task at a token-level granularity."
- Break condition: If the model is not sufficiently large or well-trained, token-level granularity alone may not lead to better segmentation.

## Foundational Learning

- Concept: Attention mechanisms in Transformers
  - Why needed here: The DTS-Transformer relies on self-attention to contextualize token embeddings across the entire recipe text, enabling it to make informed segmentation decisions.
  - Quick check question: How does the self-attention mechanism in Transformers allow the model to consider the global task structure when making segmentation decisions?

- Concept: Text segmentation metrics (Pk, Precision, Recall, F1)
  - Why needed here: These metrics are used to evaluate the model's performance in identifying the correct locations of conversational steps, comparing the predicted segmentation with the ground-truth labels.
  - Quick check question: What is the difference between Pk and F1 score in evaluating text segmentation models, and why are both important?

- Concept: Conversational traits and their importance
  - Why needed here: Understanding the importance of traits like complexity, clarity, length, and naturalness helps in designing models that align with user preferences in a conversational task-guiding setting.
  - Quick check question: Based on the user study results, which conversational traits are most important for users when following instructions in a conversational assistant, and how might this influence model design?

## Architecture Onboarding

- Component map:
  Input -> Tokenizer (Spacy) -> Transformer Backbone (BERT/T5) -> Segmentation Head (Binary Classification) -> Output (Segmentation Points)

- Critical path:
  1. Preprocess recipe text using Spacy for tokenization and sentence identification.
  2. Feed the entire recipe text into the Transformer backbone.
  3. Apply the segmentation head to each token embedding to predict the probability of it being a segmentation point.
  4. Generate the final segmentation based on the predicted segmentation points.

- Design tradeoffs:
  - Token-level vs. sentence-level segmentation: Token-level provides finer granularity but may be more computationally intensive.
  - Encoder-only vs. encoder-decoder models: Encoder-only models are faster and simpler, while encoder-decoder models may capture more complex relationships but at the cost of increased complexity and training time.
  - Model size: Larger models (e.g., T5-3B) may achieve better performance but require more computational resources.

- Failure signatures:
  - High precision but low recall: The model is conservative in making segmentation predictions, potentially missing some valid segmentation points.
  - Low precision but high recall: The model is overly aggressive in making segmentation predictions, potentially introducing false positives.
  - Poor performance on recipes with unique structures: The model may struggle with recipes that deviate significantly from the training data distribution.

- First 3 experiments:
  1. Compare the performance of token-level segmentation (DTS-Transformer) with sentence-level segmentation on a held-out test set.
  2. Evaluate the impact of model size (BERT-Base vs. T5-3B) on segmentation performance and training/inference time.
  3. Analyze the attention patterns of the best-performing model to understand how it identifies segmentation points and ensure alignment with conversational step characteristics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do users perceive the quality of recipes when segmented by the DTS model versus the original web format in terms of task completion success and satisfaction?
- Basis in paper: [explicit] The paper mentions that the user evaluation showed the model's segmentation was preferred (86%) and scored higher (3.72 vs 2.63) for conversational suitability, but does not directly assess task completion success or overall satisfaction.
- Why unresolved: The study focused on the suitability of the segmentation for conversational agents but did not measure the impact on actual task performance or user satisfaction with the completed task.
- What evidence would resolve it: Conducting a user study where participants attempt to complete the recipes using both the original and DTS-segmented versions, measuring completion rates, time taken, and subjective satisfaction ratings.

### Open Question 2
- Question: Can the DTS model be effectively applied to other domains beyond cooking, such as DIY tasks or educational tutorials, and what adjustments might be necessary?
- Basis in paper: [inferred] The paper suggests generalizing experiments to other domains like DIY tasks and tutorials in the conclusion, implying uncertainty about the model's applicability to different instructional text types.
- Why unresolved: The ConvRecipes corpus is specific to cooking instructions, and the linguistic patterns and complexity of other domains may differ significantly, requiring adaptation of the model or training data.
- What evidence would resolve it: Applying the DTS model to instructional texts from other domains, evaluating its performance with domain-specific metrics, and potentially fine-tuning the model on domain-specific corpora to improve results.

### Open Question 3
- Question: What is the impact of the number of steps generated by the DTS model on user comprehension and task performance, and is there an optimal step length or number of steps for conversational assistants?
- Basis in paper: [explicit] The paper notes that the model tends to overestimate the number of steps and that breaking too often may result in incomplete steps, as shown by the Every1 baseline. However, it does not determine an optimal balance.
- Why unresolved: While the paper identifies the importance of manageable complexity and length, it does not experimentally determine the optimal number of steps or step length that maximizes user comprehension and task performance.
- What evidence would resolve it: Conducting experiments varying the number of steps and their lengths in the DTS model's output, measuring user comprehension through recall tests and task performance through completion rates and error frequencies.

## Limitations
- The ConvRecipes corpus focuses exclusively on cooking instructions, limiting generalizability to other task domains.
- The user study had a relatively small sample size (25 participants) and relied on self-reported ratings rather than behavioral task completion measures.
- The T5-3B model's computational requirements may limit practical deployment in real-world conversational assistants.

## Confidence
**High Confidence**: The assertion that users prefer shorter, simpler steps in conversational settings is well-supported by both the user study results and established cognitive psychology literature on attention spans. The improvement over baselines using the T5-3B Enc-only model is statistically significant and consistently demonstrated across multiple metrics.

**Medium Confidence**: The claim that token-level segmentation outperforms sentence-level approaches is supported by experimental results, but the difference in performance between token-level and sentence-level methods may not be as substantial as suggested. The grounding advantage over generative approaches is logically sound but lacks direct empirical comparison.

**Low Confidence**: The generalizability of the ConvRecipes corpus findings to non-cooking domains remains largely speculative, as the paper does not provide cross-domain validation or discuss domain transfer challenges.

## Next Checks
1. **Cross-Domain Evaluation**: Test the DTS-Transformer model on non-cooking instructional domains (e.g., furniture assembly, technical procedures) to assess generalizability of the segmentation approach across different types of tasks.

2. **A/B User Testing with Behavioral Metrics**: Conduct a larger-scale user study (n > 100) that measures not just subjective ratings but actual task completion rates, error frequencies, and time-to-completion when following original vs. segmented instructions in a conversational interface.

3. **Efficiency Benchmarking**: Evaluate the inference-time performance and memory usage of the T5-3B Enc-only model versus smaller alternatives on a realistic workload, and assess whether the performance gains justify the computational overhead for production deployment.