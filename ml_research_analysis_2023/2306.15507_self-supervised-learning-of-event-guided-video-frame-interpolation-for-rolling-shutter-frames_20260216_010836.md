---
ver: rpa2
title: Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling
  Shutter Frames
arxiv_id: '2306.15507'
source_url: https://arxiv.org/abs/2306.15507
tags:
- frames
- frame
- time
- shutter
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first self-supervised framework for arbitrary
  frame rate global shutter (GS) frame interpolation from consecutive rolling shutter
  (RS) frames using event cameras. The key idea is to estimate a displacement field
  (DF) capturing non-linear dense 3D spatiotemporal motion information from events,
  and then use this DF to reciprocally reconstruct between RS and GS frames for self-supervision.
---

# Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames

## Quick Facts
- arXiv ID: 2306.15507
- Source URL: https://arxiv.org/abs/2306.15507
- Reference count: 40
- Primary result: Achieves 94% bandwidth reduction and 16 ms per frame at 32× interpolation using self-supervised event-guided RS-to-GS frame interpolation

## Executive Summary
This paper introduces the first self-supervised framework for arbitrary frame rate global shutter (GS) frame interpolation from consecutive rolling shutter (RS) frames using event cameras. The method estimates a displacement field (DF) capturing non-linear dense 3D spatiotemporal motion from events, then uses this DF for reciprocal reconstruction between RS and GS frames to enable self-supervision. The framework achieves comparable performance to supervised frame-based methods while reducing bandwidth usage by 94% and processing at 16 ms per frame for 32× interpolation.

## Method Summary
The framework consists of three main modules: Displacement Field Estimation (DFE), Latent GS Frames Generation, and Reciprocal Reconstruction. The DFE module estimates spatiotemporal motion from events using E-RAFT by dividing events into time bins and voxel grids, then estimating optical flow between consecutive event moments. The Latent GS Frames Generation module generates GS frames at arbitrary frame rates using RS frames and the estimated displacement field through mask-based warping and refinement networks. The Reciprocal Reconstruction module imposes self-supervision by reconstructing RS frames from the generated GS frames and warping RS frames based on the displacement field, creating a bidirectional mapping that eliminates the need for ground truth GS frames.

## Key Results
- Reduces bandwidth usage by 94% compared to traditional frame-based methods
- Achieves 16 ms per frame processing time at 32× interpolation rates
- Performs comparably to supervised frame-based methods on synthetic datasets (Gev-RS, Fastec-RS) and real-world data (ERS-VFI)

## Why This Works (Mechanism)

### Mechanism 1: Displacement Field Estimation
- Claims to model non-linear dense 3D spatiotemporal motion by splitting events into voxel grids and moments, then estimating optical flow between consecutive moments
- Core assumption: Short time intervals in event-based optical flow can approximate non-linear motion with piecewise linear segments
- Break condition: Optical flow estimation fails on complex non-linear motion or voxel grid resolution is insufficient

### Mechanism 2: Reciprocal Reconstruction for Self-Supervision
- Uses GS frames generated from RS frames and DF to reconstruct input RS frames, with reconstruction loss serving as self-supervision signal
- Core assumption: Displacement field contains sufficient information for bidirectional reconstruction between RS and GS frames
- Break condition: Reconstruction loss does not converge or generated GS frames contain significant artifacts

### Mechanism 3: Mask-Based Warping
- Designs masks based on RS and GS exposure mechanisms to transform between frame types using displacement field information
- Core assumption: Uniform sampling within mask elements can accurately estimate transformation weights
- Break condition: Uniform sampling insufficient for complex exposure patterns or mask resolution limits accuracy

## Foundational Learning

- **Rolling shutter vs global shutter exposure mechanisms**: Understanding how RS distortion occurs is fundamental to correcting it and generating accurate GS frames
  - Quick check: How does the exposure time difference between adjacent rows in RS cameras create distortion?

- **Event camera characteristics and event representation**: Events provide high temporal resolution data that enables accurate motion estimation between frames
  - Quick check: What information does each event contain and how does this differ from traditional frame-based data?

- **Optical flow estimation and displacement field concepts**: Displacement field estimation relies on optical flow between event moments to model motion
  - Quick check: How does estimating optical flow between event moments differ from traditional frame-to-frame optical flow?

## Architecture Onboarding

- **Component map**: RS frames + events → Displacement Field Estimation → Displacement Field → Latent GS Frames Generation → GS frames → Reciprocal Reconstruction → RS reconstruction loss
- **Critical path**: RS frames + events → Displacement Field → GS frames → RS reconstruction loss
- **Design tradeoffs**:
  - Time bins vs. displacement field resolution: More bins capture more complex motion but increase computation and error accumulation
  - Mask resolution vs. warping accuracy: Higher resolution masks provide better transformation but increase memory usage
  - Pre-trained vs. learned optical flow: Pre-trained models provide good initialization but may not adapt to specific dataset characteristics
- **Failure signatures**:
  - Displacement field fails to capture complex motion → visible artifacts in interpolated frames
  - Reciprocal reconstruction does not converge → training instability or poor performance
  - Mask-based warping introduces discontinuities → visible seams or distortions at frame boundaries
- **First 3 experiments**:
  1. Validate displacement field estimation on synthetic data with known ground truth motion
  2. Test reciprocal reconstruction with ground truth GS frames to verify bidirectional mapping works
  3. Evaluate ablation of time bins and mask resolution to find optimal configuration

## Open Questions the Paper Calls Out

- How does the proposed self-supervised framework perform on real-world RS-events-GS triplet datasets compared to supervised methods? The paper evaluates on a real-world dataset but lacks quantitative metrics and full evaluation.

- Can the proposed displacement field estimation module handle non-linear motion in more complex and diverse real-world scenes beyond the synthetic and simple real-world datasets used in the paper?

- How does the proposed self-supervised framework compare to other self-supervised or unsupervised VFI methods in terms of performance and computational efficiency?

## Limitations

- Performance claims based entirely on synthetic datasets without comprehensive real-world validation
- Mask design and uniform sampling approach lacks detailed technical specification and validation
- Missing direct comparison with other self-supervised or unsupervised VFI methods

## Confidence

- Displacement field estimation: **High** - Clear technical descriptions using established event processing techniques
- Reciprocal reconstruction self-supervision: **Medium** - Mechanism clearly described but relies entirely on synthetic validation
- Mask-based warping: **Low** - Limited technical detail and no validation experiments or ablation studies

## Next Checks

1. Validate displacement field accuracy on synthetic data with known ground truth motion trajectories across different motion patterns

2. Test self-supervision convergence by training with varying noise levels and monitoring reconstruction losses

3. Apply trained model to real RS video sequences without GS ground truth and evaluate through user studies against traditional RS correction methods