---
ver: rpa2
title: 'Lyra: Orchestrating Dual Correction in Automated Theorem Proving'
arxiv_id: '2309.15806'
source_url: https://arxiv.org/abs/2309.15806
tags:
- proof
- correction
- atpwithtc
- have
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents Lyra, a framework for formal theorem proving
  that employs two correction mechanisms: Tool Correction and Conjecture Correction.
  Tool Correction mitigates hallucinations by using predefined prover tools to replace
  incorrect tools in the formal proof.'
---

# Lyra: Orchestrating Dual Correction in Automated Theorem Proving

## Quick Facts
- arXiv ID: 2309.15806
- Source URL: https://arxiv.org/abs/2309.15806
- Reference count: 40
- State-of-the-art performance on miniF2F with 55.3% validation and 51.2% test success rates

## Executive Summary
Lyra introduces a dual-correction framework for formal theorem proving that addresses hallucinations and refinement challenges in LLM-generated proofs. The system employs Tool Correction to replace incorrect tactics with predefined prover tools and Conjecture Correction to refine proof conjectures using Isabelle error messages. Lyra achieves state-of-the-art performance on the miniF2F dataset, improving over previous methods by 7.3% and 5.7% respectively, while also solving three IMO problems.

## Method Summary
Lyra uses GPT-4 to generate informal proofs, which are then formalized using Isabelle. The framework employs two correction mechanisms: Tool Correction replaces incorrect tactics with predefined powerful tools (sledgehammer, auto, simp, blast, etc.) when validation fails, and Conjecture Correction iteratively refines proofs by incorporating Isabelle error messages without collecting paired generation-refinement prompts. The system also includes a reset mechanism that regenerates initial proofs at regular intervals to prevent error propagation. Evaluation on the miniF2F dataset demonstrates significant improvements over baselines like DSP and Subgoal-Learning.

## Key Results
- Achieves 55.3% success rate on miniF2F validation set
- Achieves 51.2% success rate on miniF2F test set
- Solves three IMO problems, demonstrating effectiveness on challenging formal proofs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool Correction replaces incorrect tactics with predefined powerful tools to mitigate hallucinations.
- Mechanism: When a tactic fails, the framework attempts substitution with sledgehammer or 11 heuristic tools (auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, auto simp: field simps).
- Core assumption: LLM-generated conjectures are often correct but paired with ineffective tactics.
- Evidence anchors:
  - [abstract]: "Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof."
  - [section 3.2]: "We introduce the Tool Correction as a remedy to alleviate the generation errors stemming from Large Language Models (LLMs). Through empirical observation, it becomes evident that despite the factual accuracy of conjectures, LLMs at times adopt misguided heuristics that do not withstand validation by theorem provers."
- Break condition: If the correct tactic is not among the predefined set, or if the conjecture itself is wrong.

### Mechanism 2
- Claim: Conjecture Correction refines formal proof conjectures by integrating error messages from the prover.
- Mechanism: Iteratively regenerate proofs by appending Isabelle error messages to the prompt, without collecting paired (generation, error & refinement) prompts.
- Core assumption: Prover error messages contain sufficient information to guide meaningful refinements.
- Evidence anchors:
  - [abstract]: "Conjecture Correction refines generation with instruction but does not collect paired (generation, error & refinement) prompts."
  - [section 3.3]: "We design a framework that can easily integrate previous formal sketches and error messages from the prover to improve sketch generation."
- Break condition: If error messages are too cryptic or do not point to actionable fixes.

### Mechanism 3
- Claim: Reset initial round generation prevents compounding errors from poor initial proofs.
- Mechanism: Regenerate initial proof at interaction rounds K, 2K, 3K, etc., refining in remaining rounds.
- Core assumption: Quality of initial proof strongly influences subsequent refinements.
- Evidence anchors:
  - [section 3.3]: "To ensure that a potentially subpar initial round proof does not negatively affect subsequent proofs, we regenerate the initial round proof at interaction rounds K, 2K, 3K and so on."
- Break condition: If reset frequency K is too low, error propagation dominates; if too high, refinement benefit is lost.

## Foundational Learning

- Concept: Isabelle theorem prover syntax and tactics
  - Why needed here: Core environment for validating and refining proofs
  - Quick check question: What is the difference between "by auto" and "by simp" in Isabelle?
- Concept: GPT-4 prompt engineering for mathematical reasoning
  - Why needed here: Generates both informal and formal proofs
  - Quick check question: How does temperature affect diversity of generated proofs?
- Concept: Automated theorem proving (ATP) heuristics
  - Why needed here: Basis for Tool Correction predefined tactics
  - Quick check question: Why is sledgehammer often more powerful than individual tactics?

## Architecture Onboarding

- Component map: GPT-4 (proof generation) → Tool Correction (tactic replacement) → Isabelle prover (validation) → Conjecture Correction (refinement) → Output
- Critical path: GPT-4 → Isabelle → Success/Failure → Tool Correction (if fail) → Isabelle → Conjecture Correction (if fail) → Isabelle
- Design tradeoffs: Number of attempts vs. computation cost; reset frequency K vs. refinement quality
- Failure signatures: Persistent tactic failures despite Tool Correction; stagnant improvement across refinement rounds
- First 3 experiments:
  1. Run DSP baseline on miniF2F with 100 attempts, record success rate
  2. Add Tool Correction only, compare success rate and analyze failed cases
  3. Add both Tool Correction and Conjecture Correction with K=5, evaluate convergence and success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Tool Correction scale with increasingly complex formal proofs beyond miniF2F?
- Basis in paper: [explicit] The paper demonstrates Tool Correction improves performance on miniF2F, but doesn't test scalability to more complex proofs.
- Why unresolved: The evaluation is limited to miniF2F dataset, which contains problems of relatively uniform difficulty.
- What evidence would resolve it: Testing Tool Correction on more challenging theorem proving datasets or real-world formal verification problems.

### Open Question 2
- Question: What is the theoretical limit of Conjecture Correction's effectiveness given infinite attempts and a sufficiently powerful prover?
- Basis in paper: [inferred] The paper shows Conjecture Correction improves performance but requires more attempts to converge, suggesting diminishing returns.
- Why unresolved: The paper doesn't explore the asymptotic behavior of Conjecture Correction with unlimited resources.
- What evidence would resolve it: Extensive experiments varying attempt numbers and prover capabilities to identify convergence patterns.

### Open Question 3
- Question: How do Tool Correction and Conjecture Correction interact when applied to problems requiring both correct tool selection and iterative refinement?
- Basis in paper: [explicit] The paper shows both mechanisms improve performance individually but doesn't analyze their combined effect on different problem types.
- Why unresolved: The ablation studies focus on each mechanism separately rather than their interaction.
- What evidence would resolve it: Detailed analysis of success rates across problem categories when using both mechanisms versus individual mechanisms.

## Limitations

- The predefined set of 11 heuristic tools may not be comprehensive enough for all proof scenarios
- Performance on broader theorem proving tasks beyond miniF2F remains to be validated
- Computational cost of multiple refinement attempts and resets is not fully characterized

## Confidence

- **High confidence**: The core mechanism of Tool Correction (replacing failed tactics with predefined tools) is well-established and demonstrably effective based on the empirical results.
- **Medium confidence**: The effectiveness of Conjecture Correction relies on the quality and informativeness of Isabelle error messages, which may vary across different proof contexts.
- **Medium confidence**: The reset mechanism's optimal frequency (K) is likely problem-dependent, and the chosen value of 5 may not be universally optimal.

## Next Checks

1. **Ablation study**: Systematically remove Tool Correction and Conjecture Correction components to quantify their individual contributions to the 7.3% improvement over DSP.

2. **Error message analysis**: Manually categorize and analyze a sample of Isabelle error messages used in Conjecture Correction to assess their informativeness and actionability for proof refinement.

3. **Generalization test**: Evaluate Lyra on a different theorem proving dataset (e.g., HolStep or CoqGym) to assess performance beyond miniF2F and test robustness across domains.