---
ver: rpa2
title: Fine-tuning and aligning question answering models for complex information
  extraction tasks
arxiv_id: '2309.14805'
source_url: https://arxiv.org/abs/2309.14805
tags:
- text
- data
- document
- question
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated fine-tuning German extractive QA models
  for complex information extraction from business documents like insurance reports
  and medical leaflets. The approach combined rule-based preprocessing with fine-tuned
  QA models to extract features of varying complexity.
---

# Fine-tuning and aligning question answering models for complex information extraction tasks

## Quick Facts
- arXiv ID: 2309.14805
- Source URL: https://arxiv.org/abs/2309.14805
- Reference count: 4
- Primary result: Fine-tuning German QA models on domain-specific datasets significantly improves extraction accuracy for complex linguistic features from business documents.

## Executive Summary
This study investigates fine-tuning German extractive QA models for complex information extraction from business documents like insurance reports and medical leaflets. The approach combines rule-based preprocessing with fine-tuned QA models to extract features of varying complexity. Using only small annotated datasets, fine-tuning significantly improved performance across tasks, with the largest gains seen for extracting visual descriptions of medications. A weighted average metric combining Levenshtein distance, ROUGE-L, and F1-score was proposed to better approximate human expert evaluation, while exact match proved less useful. The method achieved notable accuracy improvements and demonstrated applicability for industrial document analysis tasks.

## Method Summary
The study fine-tuned German QA models (specifically gelectra-large-germanquad) on domain-specific datasets using 5-fold cross-validation and grid search for hyperparameter optimization. Documents were first processed through OCR and region detection, then rule-based preprocessing identified candidate text regions using keywords or headlines. The QA model was then applied to these restricted regions to extract answers. Evaluation combined multiple metrics (Levenshtein distance, ROUGE-L, F1-score, and Exact Match) into a weighted average that better approximated human expert assessment than any single metric.

## Key Results
- Fine-tuning significantly improved performance across all extraction tasks, with the largest gains for visual medication descriptions
- Exact Match proved insufficient for evaluating complex, multi-word answers, while the weighted average metric achieved 93.87% accuracy in reconstructing human scoring
- The approach demonstrated practical applicability for industrial document analysis with notable accuracy improvements using only small annotated datasets

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning German QA models on domain-specific datasets significantly improves extraction accuracy for complex linguistic features like damage cause descriptions or medication appearance. Domain-specific fine-tuning adapts the general GermanQuAD model to recognize and extract domain-relevant patterns, enabling it to handle free-form, multi-sentence answers typical in business documents. The core assumption is that the base model has sufficient general language understanding, and fine-tuning on small annotated datasets can effectively transfer that understanding to specialized domains.

### Mechanism 2
Combining multiple evaluation metrics (Levenshtein, ROUGE-L, F1-score) provides a more accurate approximation of human expert assessment than any single metric. Different metrics capture different aspects of similarity—Levenshtein measures character-level edit distance, ROUGE-L focuses on longest common subsequence, and F1 balances precision and recall—so their weighted combination better reflects human judgment on fuzzy, multi-word answers. The core assumption is that human experts implicitly balance multiple criteria when judging answer quality, and these criteria can be approximated by a linear combination of automated metrics.

### Mechanism 3
Restricting the QA model's input to relevant document regions (via rule-based preprocessing) improves performance and reduces noise. By identifying candidate text regions containing relevant features before querying the QA model, the system narrows the context and avoids irrelevant passages that could confuse the model. The core assumption is that the rule-based preprocessing can reliably identify regions likely to contain the answer, and the QA model benefits from reduced input length and focused context.

## Foundational Learning

- **Question Answering (QA) models**: Why needed here: The system relies on extractive QA models to locate answers within document boundaries, ensuring outputs are verifiable and not hallucinated. Quick check: What distinguishes extractive QA models from generative models in terms of output reliability?
- **Fine-tuning vs. zero-shot inference**: Why needed here: Fine-tuning adapts pre-trained models to domain-specific tasks with limited data, whereas zero-shot inference relies on general capabilities that may not capture specialized patterns. Quick check: How does fine-tuning a model on a small annotated dataset improve performance compared to using the base model directly?
- **Evaluation metrics for NLP**: Why needed here: Automated metrics (Levenshtein, ROUGE-L, F1) are used to assess model performance, but they must be chosen carefully to reflect human judgment on fuzzy, multi-word answers. Quick check: Why might exact match (EM) be insufficient for evaluating answers to complex questions in natural language?

## Architecture Onboarding

- **Component map**: OCR and region detection -> Rule-based preprocessing (keyword/region selection) -> Fine-tuned QA model (gelectra-large-germanquad) -> Weighted metric evaluation (Levenshtein, ROUGE-L, F1, EM) -> Rule-based post-validation
- **Critical path**: Input document → OCR and region detection → Rule-based region selection based on keywords/headlines → QA model inference on selected region → Answer validation and output
- **Design tradeoffs**: Rule-based preprocessing vs. full-document QA: Rules improve efficiency and reduce noise but risk missing relevant context. Metric combination vs. single metric: Weighted average better approximates human judgment but requires additional training and tuning. Fine-tuning vs. zero-shot: Fine-tuning improves domain accuracy but requires annotated data and computational resources.
- **Failure signatures**: Low Levenshtein/ROUGE-F1 scores but high manual expert scores: Indicates model outputs are semantically correct but differ in wording from ground truth. High EM scores but low manual scores: Suggests model outputs are overly rigid and fail on fuzzy answers. Rule-based preprocessing misses relevant regions: QA model receives incomplete context, leading to poor answers.
- **First 3 experiments**: 1) Fine-tune base QA model on leaflet dataset, evaluate on held-out test set using Levenshtein, ROUGE-L, F1, and manual assessment. 2) Compare fine-tuned model performance on damage report dataset vs. base model, focusing on complex features like damage cause. 3) Test weighted average metric against individual metrics to verify it better approximates human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of fine-tuned QA models compare to larger generative language models (like GPT-4) for complex information extraction tasks in German business documents? The paper focuses on extractive QA models and does not compare their performance to larger generative language models.

### Open Question 2
What is the optimal strategy for determining the most relevant text regions to limit the query scope for the QA model in a document analysis pipeline? The paper mentions the importance of limiting the query scope but does not provide a detailed strategy for determining the most relevant text regions.

### Open Question 3
How can the proposed weighted average metric be further improved to better approximate human expert evaluation across different domains and tasks? The paper introduces a weighted average metric but notes that the weighting factors are not generalizable across datasets and tasks.

## Limitations

- Evaluation relies on small annotated datasets (leaflets: 101 samples, damage reports: 117 samples) that may not capture full variability of real-world business documents
- Rule-based preprocessing depends on domain-specific keywords and formatting that may not transfer well to other document types or languages
- Weighted metric combination shows strong correlation with human judgment but was validated on a limited set of features and may not generalize to all extraction tasks

## Confidence

- **High Confidence**: Fine-tuning improves performance across all tested tasks is well-supported by consistent metric improvements and cross-validation results
- **Medium Confidence**: Exact match is insufficient for complex answers is supported by qualitative observations but lacks systematic comparison across all task types
- **Low Confidence**: Generalizability to other domains, languages, or document types remains untested

## Next Checks

1. **Dataset Size Sensitivity**: Conduct experiments varying training dataset sizes to determine the minimum effective sample size for fine-tuning and whether performance plateaus or degrades with additional data
2. **Cross-Domain Transfer**: Test the fine-tuned models on document types from different domains (e.g., legal contracts, financial reports) to assess generalizability and identify domain-specific limitations
3. **Metric Robustness**: Validate the weighted average metric on additional feature types and document domains, comparing its performance against human experts across a broader range of extraction tasks