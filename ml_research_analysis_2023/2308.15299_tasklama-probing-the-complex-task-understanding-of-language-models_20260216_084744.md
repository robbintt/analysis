---
ver: rpa2
title: 'TaskLAMA: Probing the Complex Task Understanding of Language Models'
arxiv_id: '2308.15299'
source_url: https://arxiv.org/abs/2308.15299
tags:
- task
- steps
- tasks
- graph
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of structured complex task decomposition
  (SCTD) - breaking down complex real-world tasks into directed acyclic graphs over
  individual steps with temporal dependencies. The authors create a high-quality human-annotated
  dataset called TaskLAMA for this problem, develop metrics to evaluate SCTD performance,
  and compare several baseline approaches including those based on crowd-sourcing,
  search queries, and summarization.
---

# TaskLAMA: Probing the Complex Task Understanding of Language Models

## Quick Facts
- **arXiv ID**: 2308.15299
- **Source URL**: https://arxiv.org/abs/2308.15299
- **Reference count**: 13
- **Primary result**: LLMs significantly outperform baselines at structured complex task decomposition, with 15%-280% relative improvement in step generation but struggle with pairwise temporal dependencies.

## Executive Summary
This paper addresses structured complex task decomposition (SCTD) - breaking down complex real-world tasks into directed acyclic graphs over individual steps with temporal dependencies. The authors create TaskLAMA, a high-quality human-annotated dataset of 1,612 tasks, and develop comprehensive evaluation metrics. They demonstrate that large language models significantly outperform baseline approaches at generating task steps, achieving 15%-280% relative improvement. However, while LLMs excel at generating coherent step sequences, they struggle to predict pairwise temporal dependencies, revealing gaps in their understanding of complex task structures.

## Method Summary
The authors create the TaskLAMA dataset with 1,612 complex tasks annotated with steps and temporal dependencies. They develop multiple evaluation metrics including Hungarian matching for step alignment, in-degree/out-degree/step proximity for temporal dependencies, and F1/F2 scores for step generation quality. The study compares LLMs against baselines using in-context learning, soft-prompt tuning, and multiple sequence generation approaches. Experiments use PaLM 62B as the base model, with performance measured across both step generation and temporal dependency prediction tasks.

## Key Results
- LLMs achieve 15%-280% relative improvement over baselines in generating task steps
- LLMs struggle with pairwise temporal dependency prediction despite good step sequence generation
- Proposed LLM-based approaches (ICL, soft-prompt tuning, multiple sequences) provide 7%-37% additional improvement over base model
- Multiple sequence generation with deduplication improves recall compared to single sequence approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can effectively decompose complex tasks into individual steps
- Mechanism: LLMs have learned task decomposition patterns from their training data, enabling them to break down complex tasks into actionable steps when prompted with in-context learning
- Core assumption: The training data contains sufficient examples of task decomposition patterns
- Evidence anchors:
  - [abstract] "LLMs are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15% to 280% over the best baseline"
  - [section] "Our results reveal that the steps generated by an off-the-shelf LLM have higher quality than the baselines"
- Break condition: If the task requires domain-specific knowledge not present in the training data, or if the task complexity exceeds what was represented in the training corpus

### Mechanism 2
- Claim: Soft-prompt tuning can improve LLM performance on structured complex task decomposition
- Mechanism: Learning task-specific prompt embeddings during fine-tuning allows the model to better understand the SCTD task structure and generate more relevant steps
- Core assumption: The task structure can be captured through learned prompt embeddings that guide the generation process
- Evidence anchors:
  - [abstract] "We also propose a number of approaches to further improve their performance, with a relative improvement of 7% to 37% over the base model"
  - [section] "Soft-Prompt Tuning (SPT): In the case of ICL, the in-context demonstrations we provide as input get mapped to the corresponding token embeddings... Recently, it has been shown that instead of using a fixed set of token embeddings as the in-context demonstrations, one can learn those embeddings based on training data"
- Break condition: If the training data is too limited or the task structure is too complex to be captured by learned embeddings

### Mechanism 3
- Claim: LLMs struggle with predicting pairwise temporal dependencies despite generating good step sequences
- Mechanism: While LLMs can generate coherent sequences of steps, they lack the explicit reasoning about temporal relationships between individual steps
- Core assumption: Generating a coherent sequence requires different capabilities than determining pairwise temporal dependencies
- Evidence anchors:
  - [abstract] "However, we find that LLMs still struggle to predict pairwise temporal dependencies, which reveals a gap in their understanding of complex tasks"
  - [section] "We also measure the quality of the temporal dependencies produced by LLMs and observe that while LLMs are good at generating good sequences of steps, their ability in predicting pairwise temporal dependency remains unsatisfactory"
- Break condition: If the model is evaluated on tasks with very clear temporal dependencies or if additional training specifically focused on temporal reasoning is provided

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL allows the model to learn task decomposition patterns from a few examples without extensive fine-tuning
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in the context of ICL?

- Concept: Directed acyclic graphs (DAGs)
  - Why needed here: Task graphs are represented as DAGs where nodes are steps and edges are temporal dependencies
  - Quick check question: Why must task graphs be directed acyclic graphs rather than arbitrary directed graphs?

- Concept: Hungarian matching algorithm
  - Why needed here: Used to match generated steps with golden steps while enforcing one-to-one mapping to avoid duplicate counting
  - Quick check question: What problem does Hungarian matching solve in the context of evaluating generated steps?

## Architecture Onboarding

- Component map:
  TaskLAMA dataset (tasks, steps, dependencies) -> Base LLM (PaLM 62B) with enhancement techniques -> Evaluation pipeline (multiple metrics)

- Critical path:
  1. Load task and context
  2. Generate steps using ICL or enhanced methods
  3. Generate temporal dependencies
  4. Evaluate using custom metrics

- Design tradeoffs:
  - Single vs multiple sequences: Multiple sequences provide better recall but require deduplication
  - Soft-prompt tuning vs ICL: Soft-prompt tuning provides better performance but requires training data
  - Linear order vs explicit dependency prediction: Linear order is simpler but less precise for complex dependencies

- Failure signatures:
  - High precision but low recall: Model is generating too few or overly specific steps
  - High recall but low precision: Model is generating many irrelevant or duplicate steps
  - Poor edge prediction: Model can generate good sequences but struggles with pairwise dependencies

- First 3 experiments:
  1. Test ICL baseline on a small subset of tasks to verify basic functionality
  2. Implement and test soft-prompt tuning with different prompt sizes
  3. Compare single sequence vs multiple sequence generation approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be improved to better predict pairwise temporal dependencies between task steps, rather than just generating a good sequence of steps?
- Basis in paper: [explicit] The paper states that while LLMs are good at generating sequences of steps for a task in the right order, their ability in predicting pairwise temporal dependencies still lags behind.
- Why unresolved: The paper only experimented with simple approaches like ICL and soft-prompt tuning for predicting temporal dependencies, which were found to be insufficient. More advanced methods or architectural changes may be needed.
- What evidence would resolve it: Developing and testing new approaches, such as using different model architectures, incorporating additional knowledge sources, or leveraging more sophisticated reasoning techniques, that can significantly improve the accuracy of pairwise temporal dependency prediction compared to the baselines tested in this paper.

### Open Question 2
- Question: How can the quality of generated task graphs be improved to reduce the number of errors and inconsistencies, such as irrelevant steps, parsing issues, and duplicate steps?
- Basis in paper: [explicit] The paper mentions several quality issues found in existing datasets, including irrelevant steps, parsing issues, and duplicate steps. It also provides qualitative examples of LLM-generated task graphs with errors.
- Why unresolved: The paper does not propose any specific solutions to address these quality issues. It only presents the problem and suggests that future work can find ways to improve the quality of generated task graphs.
- What evidence would resolve it: Developing and evaluating techniques, such as using more advanced filtering or validation methods, incorporating human feedback, or leveraging external knowledge sources, that can significantly reduce the number of errors and inconsistencies in generated task graphs.

### Open Question 3
- Question: How can language models be extended to handle conditional task graphs, where the steps and dependencies may vary based on the outcomes of previous steps?
- Basis in paper: [explicit] The paper mentions that conditional task graphs are a limitation of the current work and suggests that future work can develop probes and benchmark models for conditional task graph generation.
- Why unresolved: The paper does not provide any specific approaches or experimental results for handling conditional task graphs. It only acknowledges the limitation and suggests that future work can address this issue.
- What evidence would resolve it: Developing and testing methods that can generate and reason about conditional task graphs, such as using probabilistic models, incorporating decision trees or flow charts, or leveraging reinforcement learning techniques, and evaluating their performance on benchmark datasets.

## Limitations

- The TaskLAMA dataset contains only 1,612 tasks, which may not capture the full diversity of real-world task structures and dependencies
- Evaluation is conducted on a small subset of 11 tasks, potentially limiting statistical robustness of results
- The Hungarian matching algorithm's one-to-one constraint may artificially constrain evaluation of models that generate multiple valid decompositions

## Confidence

**High Confidence Claims**:
- LLMs significantly outperform baseline approaches at generating task steps (15%-280% relative improvement)
- LLMs struggle with pairwise temporal dependency prediction despite generating good step sequences
- Multiple sequence generation with deduplication provides better recall than single sequence approaches

**Medium Confidence Claims**:
- Soft-prompt tuning provides 7%-37% improvement over base ICL performance
- The TaskLAMA dataset represents a meaningful benchmark for SCTD evaluation
- Linear ordering of steps provides reasonable approximation of task structure

**Low Confidence Claims**:
- Specific performance thresholds across different task categories
- Optimal prompt size for soft-prompt tuning across all task types
- Generalization capabilities beyond the 11-task evaluation subset

## Next Checks

1. **Dataset Diversity Validation**: Conduct a comprehensive analysis of task diversity across the 1,612-task dataset by clustering tasks based on structural similarity and measuring the coverage of different task domains. This will help quantify whether the dataset provides sufficient variety to train robust SCTD models.

2. **Robustness Testing**: Evaluate model performance across different task complexity levels by stratifying tasks based on step count, dependency density, and domain specificity. This stratified analysis will reveal whether the reported improvements hold consistently across different task characteristics.

3. **Metric Validation**: Design and implement an ablation study that systematically varies the one-to-one constraint in Hungarian matching and tests alternative edge evaluation metrics that don't rely on proximity assumptions. This will help determine whether current metrics accurately capture model performance or impose artificial constraints.