---
ver: rpa2
title: 'ConDistFL: Conditional Distillation for Federated Learning from Partially
  Annotated Data'
arxiv_id: '2308.04070'
source_url: https://arxiv.org/abs/2308.04070
tags:
- condistfl
- loss
- segmentation
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConDistFL introduces a federated learning framework for multi-organ
  and tumor segmentation using partially annotated medical data. It combines supervised
  learning with knowledge distillation from global models to handle unlabeled regions
  in local datasets.
---

# ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data

## Quick Facts
- arXiv ID: 2308.04070
- Source URL: https://arxiv.org/abs/2308.04070
- Authors: 
- Reference count: 32
- Key outcome: ConDistFL achieves average Dice scores of 0.77 on partially labeled data and 0.78 on external AMOS22 data while reducing communication costs through infrequent aggregation

## Executive Summary
ConDistFL addresses the challenge of multi-organ and tumor segmentation in federated learning settings where client data has partial annotations. The method combines supervised learning on labeled regions with conditional distillation from a global model to learn from unlabeled areas. By using a conditional distillation loss with foreground filtering and background grouping, ConDistFL effectively transfers knowledge from the global model without conflicts. The approach outperforms standard federated learning baselines on MSD and KiTS19 datasets while maintaining performance with reduced aggregation frequency.

## Method Summary
ConDistFL implements a federated learning framework for medical image segmentation using partially annotated data. The method combines supervised learning with knowledge distillation, using a conditional distillation loss that only applies to background regions while avoiding conflicts with supervised loss on labeled areas. It employs marginal loss design to handle label space heterogeneity across clients, and uses a global model as a teacher to provide soft labels for unlabeled regions. The framework is implemented with NVIDIA FLARE and tested on 3D medical imaging datasets.

## Key Results
- ConDistFL achieves average Dice scores of 0.77 on partially labeled data and 0.78 on external AMOS22 validation data
- Outperforms FedAvg, FedOpt, and FedProx baselines on multi-organ and tumor segmentation tasks
- Maintains performance with reduced aggregation frequency, lowering communication costs while preventing model divergence
- Demonstrates effectiveness of conditional distillation in handling unlabeled regions in federated settings

## Why This Works (Mechanism)

### Mechanism 1
Conditional distillation allows local models to learn from global model predictions on unlabeled regions without knowledge conflicts. The global model acts as a teacher, providing soft labels for unlabeled organs and tumors in local data. The conditional distillation loss only applies to background regions, using foreground filtering to avoid conflicts with supervised loss on labeled areas. Core assumption: The global model's predictions on unlabeled regions are sufficiently accurate to guide local model learning.

### Mechanism 2
Marginal loss design resolves label space heterogeneity across clients with different annotation tasks. By merging background class probabilities and treating unlabeled classes uniformly, the loss function prevents conflicts when different clients have annotated different organ subsets. Core assumption: Treating all unlabeled classes as background is sufficient for multi-task learning without explicit label alignment.

### Mechanism 3
Reduced aggregation frequency with conditional distillation maintains performance while lowering communication costs. The conditional distillation provides a continuous learning signal between global model updates, allowing longer local training without degradation. Core assumption: Knowledge distillation between rounds provides sufficient regularization to prevent local model drift.

## Foundational Learning

- Concept: Knowledge distillation with temperature scaling
  - Why needed here: Softening model outputs with temperature τ=0.5 enhances confidence calibration for unlabeled region learning
  - Quick check question: What happens to the probability distribution when temperature approaches zero versus infinity?

- Concept: Dice loss with cross-entropy combination
  - Why needed here: Handles class imbalance in medical segmentation while providing smooth gradient for both foreground and background regions
  - Quick check question: How does Dice loss behave when there are very few positive samples versus many background samples?

- Concept: Marginal loss for partial supervision
  - Why needed here: Prevents label conflict when different clients have annotated different organ subsets by treating unlabeled classes as background
  - Quick check question: What would happen if you used standard cross-entropy loss instead of marginal loss with partial annotations?

## Architecture Onboarding

- Component map: Global server -> Aggregates model updates, maintains teacher model; Client models -> Local segmentation networks, train with supervised + conditional distillation loss; Loss functions -> Supervised loss (marginal loss + DiceCELoss) + Conditional distillation loss; Data pipeline -> Partial annotations distributed across clients, background grouping by organ

- Critical path: Client forward pass → supervised loss on labeled regions + conditional distillation loss on unlabeled regions → local backpropagation → model upload → server aggregation → global model update → distribution to clients

- Design tradeoffs: Marginal loss vs explicit label alignment, conditional distillation vs pseudo-labeling, aggregation frequency vs communication cost

- Failure signatures: Performance plateau despite training (insufficient distillation), sudden accuracy drops (model divergence), communication bottlenecks (aggregation overhead)

- First 3 experiments:
  1. Standalone training on single dataset with marginal loss to verify basic segmentation capability
  2. Two-client federated training with overlapping and non-overlapping annotations to test label space handling
  3. Increasing local training steps with fixed aggregation frequency to identify divergence thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the conditional distillation loss perform when applied to other medical imaging tasks beyond multi-organ and tumor segmentation, such as cardiac or brain imaging?
- Basis in paper: The paper focuses on abdominal CT data but suggests generalizability of the approach
- Why unresolved: The paper only evaluates the method on abdominal CT datasets and does not explore its applicability to other anatomical regions or imaging modalities
- What evidence would resolve it: Experiments applying ConDistFL to cardiac or brain segmentation tasks using partially annotated data would demonstrate its broader applicability

### Open Question 2
- Question: What is the impact of varying the number of clients and their data heterogeneity on the performance of ConDistFL?
- Basis in paper: The paper mentions that data heterogeneity is a challenge in FL and that ConDistFL addresses this issue
- Why unresolved: The paper uses a fixed number of clients (4) with specific data distributions and does not explore how the method scales with different numbers of clients or levels of data heterogeneity
- What evidence would resolve it: Experiments varying the number of clients and their data distributions would show how ConDistFL performs under different FL scenarios

### Open Question 3
- Question: How does ConDistFL compare to other knowledge distillation approaches in federated learning settings?
- Basis in paper: The paper introduces conditional distillation but does not compare it to other distillation methods in FL
- Why unresolved: While the paper compares ConDistFL to standard FL baselines, it does not explore how its distillation approach compares to other knowledge distillation techniques in FL
- What evidence would resolve it: Implementing and comparing ConDistFL with other FL-specific knowledge distillation methods would clarify its relative performance

## Limitations
- The approach relies heavily on the global model's prediction quality on unlabeled regions, which may be unreliable in early training stages
- The marginal loss simplification assumes background grouping is sufficient for multi-task learning, but this may not hold for complex annotation scenarios
- The study uses only abdominal CT data, limiting generalizability to other imaging modalities or anatomical regions

## Confidence
- Core methodology and experimental results: High confidence
- Theoretical foundations (marginal loss, conditional distillation mechanisms): Medium confidence
- Generalizability to other medical imaging tasks: Low confidence

## Next Checks
1. Evaluate performance degradation when clients have highly divergent annotation patterns (>50% task overlap) to stress-test the marginal loss assumption
2. Test early-round distillation reliability by comparing results when initializing global model from pretrained weights versus random initialization
3. Validate robustness across imaging modalities by replicating experiments on brain MRI or chest CT datasets with similar annotation patterns