---
ver: rpa2
title: 'Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis
  and Comparison with Existing Models'
arxiv_id: '2312.07592'
source_url: https://arxiv.org/abs/2312.07592
tags:
- chatgpt
- question
- questions
- evaluation
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study comprehensively evaluates ChatGPT as a Question Answering\
  \ System (QAS) by comparing its performance with existing task-specific models.\
  \ The primary focus is on ChatGPT\u2019s ability to extract answers from provided\
  \ paragraphs, a core QAS capability, as well as its performance in scenarios without\
  \ surrounding passages."
---

# Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models

## Quick Facts
- arXiv ID: 2312.07592
- Source URL: https://arxiv.org/abs/2312.07592
- Reference count: 5
- ChatGPT performs worse than task-specific models in question answering, especially on "how" and "why" questions.

## Executive Summary
This study comprehensively evaluates ChatGPT's performance as a Question Answering System (QAS) by comparing it with existing task-specific models. The research focuses on ChatGPT's ability to extract answers from provided paragraphs and its performance in scenarios without surrounding passages. Experiments were conducted on ChatGPT using well-known QA datasets (SQuAD, NewsQA, PersianQuAD) in English and Persian, employing metrics such as F-score, exact match, and accuracy. The study reveals that while ChatGPT demonstrates competence as a generative model, it is less effective in question answering compared to task-specific models. Providing context improves its performance, and prompt engineering enhances precision, particularly for questions lacking explicit answers in provided paragraphs.

## Method Summary
The study employed a three-component framework: Prompt Builder, ChatGPT API, and Answer Evaluator. Researchers collected and preprocessed QA datasets including SQuAD 1.1, SQuAD 2.0, NewsQA, and PersianQuAD, normalizing questions, paragraphs, and answers. They implemented various prompt engineering techniques, including single-step and two-step querying approaches. The OpenAI API was used to send prompts to ChatGPT (GPT-3.5 Turbo) and collect responses. Responses were then normalized and compared to ground truth answers to calculate F-score, Exact Match, and Recall metrics. Performance was analyzed across different question types and difficulty levels.

## Key Results
- ChatGPT performs worse than task-specific models in question answering tasks, particularly on complex "how" and "why" questions.
- Providing context significantly improves ChatGPT's performance, with notable gains in F1 score, Exact Match, and Precision.
- Two-step prompting, which first checks for answer presence before requesting it, reduces hallucinations and improves precision on unanswerable questions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT benefits from explicit context paragraphs when answering questions.
- Mechanism: Providing surrounding text supplies lexical and semantic cues that align the model's internal knowledge with the specific scenario, improving F1 score and Exact Match.
- Core assumption: The model can leverage context when the answer exists within the paragraph, rather than relying solely on pre-trained knowledge.
- Evidence anchors:
  - [abstract]: "Providing context improves its performance, and prompt engineering enhances precision..."
  - [section]: "A notable observation is the significant improvement in F1 score, Exact Match, and Precision when the language model is provided with the paragraph."
  - [corpus]: Weak evidence—no directly related papers discussing context impact on ChatGPT Q&A.
- Break condition: If the paragraph is irrelevant or too noisy, performance may degrade or hallucinations may increase.

### Mechanism 2
- Claim: Two-step prompting improves precision when the answer is not in the paragraph.
- Mechanism: First ask if the answer exists; if "Yes", then request the answer. This avoids forcing a response when none exists.
- Core assumption: ChatGPT's generative tendency can be moderated by explicitly checking answer presence before generating.
- Evidence anchors:
  - [abstract]: "...a two-step prompt querying the presence of the answer prior to requesting it yields superior performance..."
  - [section]: "In the next step, questions were asked in two stages, and the evaluation metrics improved."
  - [corpus]: Weak evidence—no corpus papers discussing two-step prompting with ChatGPT for QA.
- Break condition: If the first step is misclassified (e.g., "Yes" when the answer is absent), it may produce hallucinated answers.

### Mechanism 3
- Claim: ChatGPT performs worse on "how" and "why" questions compared to factual "what" questions.
- Mechanism: "How" and "why" require causal inference and multi-step reasoning, which are harder for generative models to synthesize accurately from context.
- Core assumption: The model's strengths lie in surface-level fact retrieval, not deep reasoning about mechanisms or motivations.
- Evidence anchors:
  - [abstract]: "ChatGPT excels at simpler factual questions compared to 'how' and 'why' question types."
  - [section]: "ChatGPT exhibits lower F1 score, Precision, and Recall scores for questions beginning with 'Why' and 'How'..."
  - [corpus]: Weak evidence—no corpus papers analyzing ChatGPT's performance on different question types.
- Break condition: If the model is fine-tuned or augmented with reasoning modules, this gap may narrow.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: Small changes in prompt wording or structure (single-step vs. two-step) lead to measurable differences in accuracy and hallucination rates.
  - Quick check question: What happens if you replace "what is the answer" with "does this paragraph contain the answer"?

- Concept: Context utilization in language models
  - Why needed here: Understanding how large language models process and use context is key to interpreting performance gains when paragraphs are provided.
  - Quick check question: If a paragraph is provided but the answer is not there, will the model still generate a plausible-sounding answer?

- Concept: Evaluation metrics for QA
  - Why needed here: Exact Match, F1, and Recall are used to compare ChatGPT with task-specific models; knowing how they differ is crucial for correct interpretation.
  - Quick check question: What is the difference between Exact Match and F1 in this context?

## Architecture Onboarding

- Component map: Prompt Builder -> ChatGPT API -> Answer Evaluator
- Critical path: Generate prompt -> Call ChatGPT -> Normalize ground truth and response -> Compute metrics
- Design tradeoffs: Token limits require careful prompt length management; two-step prompting increases latency but improves precision; context inclusion boosts accuracy but may not always be available.
- Failure signatures: High hallucination rates when questions are unanswerable; low F1 on complex "how"/"why" questions; performance drops without context.
- First 3 experiments:
  1. Compare single-step vs. two-step prompting on SQuAD 2.0 unanswerable questions.
  2. Test context presence/absence effect on SQuAD 1.1 factual questions.
  3. Evaluate F1 score variation across "what", "how", and "why" question types in SQuAD datasets.

## Open Questions the Paper Calls Out

- How can ChatGPT be fine-tuned specifically for question answering tasks to improve its performance?
  - Basis in paper: explicit
  - Why unresolved: The paper identifies ChatGPT's limitations in question answering compared to task-specific models, but doesn't explore fine-tuning strategies.
  - What evidence would resolve it: Comparative experiments showing performance improvements after fine-tuning ChatGPT on question answering datasets.

- What are the most effective prompt engineering techniques for improving ChatGPT's performance on unanswerable questions?
  - Basis in paper: explicit
  - Why unresolved: While the paper mentions that prompt engineering improves performance on unanswerable questions, it doesn't provide detailed analysis of specific techniques.
  - What evidence would resolve it: Systematic evaluation of different prompt engineering approaches on unanswerable questions across multiple datasets.

- How does ChatGPT's performance vary across different types of complex questions (e.g., multi-hop, temporal, causal) compared to simpler factual questions?
  - Basis in paper: explicit
  - Why unresolved: The paper identifies that ChatGPT struggles with "how" and "why" questions, but doesn't provide detailed analysis of different types of complex questions.
  - What evidence would resolve it: Detailed performance analysis of ChatGPT on various types of complex questions with comparison to factual questions.

## Limitations

- The study lacks detailed prompt formulations and token limit specifications, making replication challenging.
- Cross-lingual comparison with PersianQuAD is limited by unclear preprocessing and annotation methods.
- The research relies on a single GPT-3.5 Turbo model version without exploring alternative model sizes or configurations.

## Confidence

- High Confidence: ChatGPT's superior performance on factual "what" questions compared to "how" and "why" questions is well-supported by the abstract and results sections.
- Medium Confidence: The effectiveness of two-step prompting in reducing hallucinations and improving precision is supported by the abstract and results, but the exact prompt formulations and their impact on different question types require further validation.
- Low Confidence: The cross-lingual performance comparison with PersianQuAD is limited by the lack of detailed information on the dataset's preprocessing and annotation, making it difficult to assess the validity of the cross-lingual results.

## Next Checks

1. Replicate the study using the exact prompt formulations and token limits specified in the original research to verify the observed performance improvements and hallucination rates.

2. Conduct a detailed analysis of the PersianQuAD dataset's preprocessing and annotation methods to ensure accurate cross-lingual comparison and to identify any potential biases or inconsistencies.

3. Evaluate ChatGPT's performance using different model sizes and configurations (e.g., GPT-4) to determine if the observed trends hold across various model architectures and capabilities.