---
ver: rpa2
title: A Survey on Multi-Objective Neural Architecture Search
arxiv_id: '2307.09099'
source_url: https://arxiv.org/abs/2307.09099
tags:
- search
- architecture
- neural
- network
- objectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of Multi-Objective Neural
  Architecture Search (MONAS), a subfield of automated machine learning that optimizes
  neural network architectures based on multiple objectives such as accuracy, computational
  complexity, power consumption, and network size. The authors provide a well-defined
  formulation for NAS and correct classification mistakes found in previous surveys.
---

# A Survey on Multi-Objective Neural Architecture Search

## Quick Facts
- arXiv ID: 2307.09099
- Source URL: https://arxiv.org/abs/2307.09099
- Reference count: 40
- Primary result: Comprehensive survey of Multi-Objective Neural Architecture Search (MONAS) that corrects classification errors in previous surveys and provides detailed analysis of objectives, methods, and open questions

## Executive Summary
This survey provides a comprehensive overview of Multi-Objective Neural Architecture Search (MONAS), a subfield of automated machine learning that optimizes neural network architectures across multiple objectives simultaneously. The authors correct classification mistakes found in previous surveys and provide a well-defined formulation for NAS as a multi-objective optimization problem. They present a detailed list of objectives used in MONAS, including accuracy, computational complexity, power consumption, and network size, and discuss the stochastic nature of objectives like accuracy and latency. The paper highlights the importance of MONAS in addressing trade-offs between different objectives and improving the efficiency and robustness of the search process.

## Method Summary
The paper synthesizes existing MONAS approaches by categorizing them based on their optimization methods (evolutionary algorithms, reinforcement learning, gradient-based methods), performance estimation techniques (weight-sharing/supernets, surrogate models, direct training), and search space designs (global, cell-based). The authors provide a unified formulation where the cost function is treated as a vector of objectives rather than a scalar, enabling Pareto-optimal exploration. They analyze how different methods handle stochastic objectives through sampling and distribution modeling, and discuss techniques for reducing computational cost while maintaining search quality.

## Key Results
- MONAS enables explicit optimization of trade-offs between accuracy and resource constraints through multi-dimensional objective functions
- Stochastic objectives like accuracy and latency require sampling-based estimation and distribution modeling rather than point estimates
- Weight-sharing and supernet-based performance estimation can dramatically reduce evaluation costs while maintaining search quality
- The survey corrects classification errors in previous works and provides the first comprehensive categorization of MONAS objectives and methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MONAS can explicitly optimize trade-offs between accuracy and resource constraints by defining multi-dimensional objective functions rather than relying on a single scalar proxy.
- **Mechanism:** The survey frames MONAS as a bilevel optimization problem where the outer loop searches over architectural hyperparameters and the inner loop trains network weights. The cost function is replaced with a vector of objectives (accuracy, model size, latency, etc.), enabling Pareto-optimal exploration instead of a single trade-off.
- **Core assumption:** A well-defined and differentiable search space exists, and all objectives can be evaluated for candidate architectures within that space.
- **Evidence anchors:**
  - [abstract] states MONAS "considers more goals such as computational complexity, power consumption, and size of the network for optimization, reaching a trade-off between the accuracy and other features like the computational cost."
  - [section] defines the cost function as a vector: "MONAS is when cost(.) is a vector of possibly two or more objectives."
- **Break condition:** If the search space is too coarse or the objectives are not correlated in a meaningful way, Pareto fronts may collapse to trivial solutions or fail to guide search.

### Mechanism 2
- **Claim:** Stochasticity in objectives like accuracy and latency is handled by sampling multiple trials and using distributions rather than point estimates in the multi-objective optimization loop.
- **Mechanism:** For objectives with inherent randomness (e.g., accuracy varying due to weight initialization or latency varying due to thread scheduling), MONAS aggregates multiple evaluations to produce mean/variance estimates. These distributions are then used in the Pareto ranking to avoid biasing toward unstable architectures.
- **Core assumption:** The variance across trials is small enough that the distribution is informative; the sampling cost is acceptable within the search budget.
- **Evidence anchors:**
  - [section] explicitly contrasts deterministic vs stochastic objectives: "Some optimization goals like network size... are deterministic properties. On the other hand, some other objectives, especially the accuracy, and the inference latency are stochastic."
  - [section] provides a figure showing distribution of error rate and inference time over trials, supporting the need to model variability.
- **Break condition:** If trial variance is too high, the Pareto front becomes noisy and the search method cannot distinguish meaningful trade-offs.

### Mechanism 3
- **Claim:** Weight-sharing and supernet-based performance estimation dramatically reduce the cost of evaluating multiple objectives, enabling deeper exploration of the Pareto front within practical time budgets.
- **Mechanism:** A single supernet is trained once, and sub-networks are evaluated by masking unused connections. This allows simultaneous estimation of accuracy and resource metrics (e.g., FLOPs, latency) without separate full training runs.
- **Core assumption:** The supernet is representative of the search space and the masked sub-networks inherit accurate performance estimates from the shared weights.
- **Evidence anchors:**
  - [section] lists "Weight-sharing/ Supernets" as a performance estimation method and notes "In this manner, given a trained supernet, the performance estimation takes the time of only evaluating a network on a test set."
  - [section] references ENAS achieving "50000x speedup" by using weight-sharing.
- **Break condition:** If the supernet underfits or overfits certain sub-networks, the performance estimates become unreliable, leading to poor Pareto-optimal selections.

## Foundational Learning

- **Concept:** Multi-objective optimization and Pareto optimality
  - Why needed here: MONAS fundamentally relies on exploring trade-offs rather than a single optimal solution; understanding Pareto sets is essential for interpreting results and designing search algorithms.
  - Quick check question: What is the difference between a Pareto-optimal solution and a dominated solution in a multi-objective space?

- **Concept:** Stochastic optimization and sampling distributions
  - Why needed here: Many MONAS objectives are inherently stochastic; the framework must handle variance and uncertainty in evaluations to avoid misleading comparisons.
  - Quick check question: Why might two identical architectures yield different accuracy scores across training trials?

- **Concept:** Search space design and representation
  - Why needed here: The efficiency and quality of MONAS depend heavily on how architectures are encoded and what operations are allowed; poor design can lead to huge search spaces with few good candidates.
  - Quick check question: What is the difference between a global (variable macro structure) and a cell-based search space in NAS?

## Architecture Onboarding

- **Component map:** Search Space Module -> Search Method Module -> Performance Estimation Module -> Pareto Analysis Module -> Controller/Orchestrator
- **Critical path:** Initialize architecture hyperparameters → evaluate via performance estimation → rank using Pareto criteria → generate new candidates → repeat until termination
- **Design tradeoffs:**
  - Search space granularity vs. tractability: Finer-grained spaces allow better architectures but explode search cost
  - Objective determinism vs. sampling cost: Stochastic objectives give realistic estimates but require multiple trials
  - Supernet fidelity vs. search diversity: High-fidelity supernets improve estimates but may bias toward explored regions
- **Failure signatures:**
  - Pareto front collapses to a single point (objectives are redundant or search space too small)
  - High variance in objective estimates leading to noisy rankings
  - Search gets stuck in local optima due to poor exploration-exploitation balance
- **First 3 experiments:**
  1. Implement a small search space (e.g., depth, width) and use a simple EA to optimize accuracy vs. FLOPs; verify Pareto front emerges
  2. Train the same architecture multiple times, collect accuracy distributions, and compare Pareto rankings using mean vs. full distributions
  3. Build a differentiable supernet for the same space, share weights, and compare evaluation speed and Pareto front quality against full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stochastic nature of objectives like accuracy and inference latency impact the optimization process and final Pareto frontier in MONAS?
- Basis in paper: [explicit] The paper discusses the stochastic properties of some objectives in Table III and Figure 6, noting that accuracy and inference latency are stochastic, unlike deterministic objectives like model size.
- Why unresolved: The paper acknowledges the existence of stochastic objectives but does not delve into how this stochasticity affects the optimization process or the quality of the Pareto frontier.
- What evidence would resolve it: Empirical studies comparing the Pareto frontiers obtained when treating stochastic objectives as deterministic versus when accounting for their stochastic nature. Analysis of the impact of stochasticity on the convergence and diversity of the Pareto frontier.

### Open Question 2
- Question: What are the most effective methods for estimating the values of stochastic objectives like accuracy and inference latency in MONAS?
- Basis in paper: [explicit] The paper mentions the challenge of estimating stochastic objectives and briefly discusses methods like lookup tables and prediction models, but does not provide a comprehensive comparison or evaluation of these methods.
- Why unresolved: The paper acknowledges the difficulty of estimating stochastic objectives but does not provide a definitive answer on the most effective methods. Different methods may have varying levels of accuracy and computational cost, and the optimal choice may depend on the specific MONAS problem.
- What evidence would resolve it: Comparative studies evaluating the accuracy, computational cost, and scalability of different methods for estimating stochastic objectives in MONAS. Analysis of the trade-offs between different methods and their suitability for different MONAS problems.

### Open Question 3
- Question: How can the search space be effectively designed and explored in MONAS to balance between finding high-quality architectures and reducing computational cost?
- Basis in paper: [explicit] The paper discusses the importance of search space design in NAS and mentions the use of techniques like progressive search and weight sharing to reduce computational cost. However, it does not provide a comprehensive framework or guidelines for designing and exploring search spaces in MONAS.
- Why unresolved: The paper acknowledges the challenge of balancing search space design and computational cost but does not provide a definitive answer on how to achieve this balance. Different MONAS problems may require different search space designs and exploration strategies.
- What evidence would resolve it: Empirical studies comparing the effectiveness and efficiency of different search space design and exploration strategies in MONAS. Analysis of the trade-offs between search space size, diversity, and computational cost, and their impact on the quality of the final architectures.

## Limitations

- The survey presents theoretical discussions without empirical validation of MONAS approaches through quantitative comparisons
- Claims about MONAS effectiveness assume multi-objective frameworks will consistently find better architectures, but this depends heavily on search space design and algorithm choice
- The assertion that stochasticity significantly impacts MONAS outcomes lacks experimental evidence showing improved search quality from uncertainty quantification

## Confidence

- **High confidence**: The formulation of MONAS as multi-objective optimization and the categorization of objectives are well-established concepts in the field
- **Medium confidence**: The claim that MONAS provides better trade-offs between accuracy and resource constraints assumes the multi-objective framework will consistently find architectures that single-objective methods miss
- **Low confidence**: The assertion that stochasticity in objectives significantly impacts MONAS outcomes lacks empirical validation in the survey

## Next Checks

1. Implement a simple MONAS framework on CIFAR-10 comparing multi-objective search against single-objective baselines, measuring both solution quality and search efficiency
2. Systematically vary the number of trials used to estimate stochastic objectives and measure how this affects Pareto front stability and search performance
3. Compare architectures selected via weight-sharing-based performance estimation against those selected via full training to quantify the trade-off between search speed and solution quality