---
ver: rpa2
title: 'Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional
  Reward Model'
arxiv_id: '2310.09520'
source_url: https://arxiv.org/abs/2310.09520
tags:
- reward
- language
- text
- generation
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RAD, a method to control text generation\
  \ using a unidirectional reward model. RAD rescales the top-k sampling probabilities\
  \ during decoding based on a reward model\u2019s score of how well a sequence aligns\
  \ with a desired attribute."
---

# Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model

## Quick Facts
- arXiv ID: 2310.09520
- Source URL: https://arxiv.org/abs/2310.09520
- Reference count: 16
- Key outcome: RAD outperforms weighted decoding methods and matches performance of state-of-the-art methods involving additional training while incurring minimal computational overhead

## Executive Summary
This paper introduces Reward-Augmented Decoding (RAD), a method for controlled text generation that uses a unidirectional reward model to steer language model outputs toward desired attributes. RAD rescales top-k sampling probabilities during decoding based on reward model scores of how well sequences align with target attributes. The method demonstrates effectiveness on detoxification and sentiment control tasks while maintaining computational efficiency through activation caching in the unidirectional reward model.

## Method Summary
RAD combines a base language model with a unidirectional reward model during decoding. At each generation step, RAD computes reward scores for top-k candidate tokens, then adds β times these scores to the logits before softmax normalization. The unidirectional reward model caches hidden states from previous generation steps, requiring computation only for newly added tokens rather than reprocessing entire sequences. This architecture enables efficient controlled generation without additional training of the base model.

## Key Results
- RAD outperforms weighted decoding methods on detoxification and sentiment control tasks
- RAD matches performance of state-of-the-art methods involving additional training (PPLM, GeDi, DExperts)
- RAD scales effectively to large language models (LLaMA variants) with minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1
RAD reduces toxicity by scaling top-k token probabilities based on reward model scores. At each decoding step, RAD computes reward scores for top-k candidate tokens, then adds β times these scores to the logits before softmax normalization. This rescales token probabilities to favor high-reward tokens.

### Mechanism 2
Caching intermediate activations in unidirectional reward model reduces computational overhead. The unidirectional reward model caches hidden states from previous generation steps, requiring computation only for newly added tokens rather than reprocessing entire sequences.

### Mechanism 3
Cumulative squared error loss encourages reward model to capture attribute alignment at all prefix lengths. The loss function weights earlier prefixes more heavily and trains the model to predict correct rewards for every prefix of text sequences.

## Foundational Learning

- Concept: Top-k sampling and weighted decoding
  - Why needed here: RAD builds on these established text generation techniques by adding reward-based probability rescaling
  - Quick check question: How does top-k sampling differ from nucleus sampling, and why might top-k be preferred for controlled generation?

- Concept: Transformer decoder architecture with causal masking
  - Why needed here: Understanding causal masking is essential for grasping how the unidirectional reward model caches activations
  - Quick check question: What prevents a unidirectional decoder from accessing future tokens during attention computation?

- Concept: Reinforcement learning concepts (rewards, policies)
  - Why needed here: RAD uses reward signals to guide text generation, similar to RL approaches but without training the base model
  - Quick check question: How does RAD's approach differ from RLHF (Reinforcement Learning from Human Feedback)?

## Architecture Onboarding

- Component map: Base LM (GPT-2 Large or LLaMA) -> Unidirectional reward model (GPT-2 Small) -> RAD decoding algorithm -> Caching mechanism

- Critical path:
  1. Initialize with generation prefix
  2. Generate top-k token logits from base LM
  3. Compute reward scores for top-k sequences using cached reward model states
  4. Rescale probabilities by adding β times reward scores to logits
  5. Sample next token and append to sequence
  6. Update reward model cache with new activations

- Design tradeoffs:
  - Larger k values increase diversity but reduce fluency due to considering more unlikely tokens
  - Higher β values increase attribute alignment but may reduce text quality
  - Smaller reward models reduce computational overhead but may be less accurate
  - Trade-off between computational cost and attribute control effectiveness

- Failure signatures:
  - Text quality degradation when β is too high
  - Attribute alignment failure when reward model is poorly trained
  - Computational inefficiency when reward model size approaches base LM size
  - Caching errors when sequence length exceeds reward model context window

- First 3 experiments:
  1. Verify basic RAD functionality: Run with k=10, β=1 on simple prompts and confirm attribute alignment
  2. Tune hyperparameters: Test different k and β values on detoxification task to find optimal balance
  3. Compare against baselines: Run RAD alongside PPLM, GeDi, and DExperts on same tasks for performance comparison

## Open Questions the Paper Calls Out
The paper mentions that RAD could be applied to more sophisticated tasks like encouraging language models to follow instructions, but does not provide experimental results for such applications.

## Limitations
- Limited exploration of cross-domain transfer and few-shot adaptation capabilities
- Highly dependent on hyperparameter tuning with no systematic analysis of parameter interactions
- Practical computational overhead may vary significantly based on implementation details

## Confidence
- High Confidence: The fundamental mechanism of RAD and its computational efficiency claims are well-supported
- Medium Confidence: Effectiveness on large language models demonstrated but with limited sample size
- Low Confidence: Performance claims relative to state-of-the-art methods based on specific baselines without broader comparison

## Next Checks
1. Evaluate RAD's performance on a third, qualitatively different attribute control task (e.g., politeness, formality) using the same implementation and hyperparameters
2. Implement RAD with detailed profiling to measure actual computational overhead across different sequence lengths and model sizes
3. Test RAD's effectiveness on tasks requiring long-range attribute consistency over extended text passages (multiple paragraphs)