---
ver: rpa2
title: 'SafetyBench: Evaluating the Safety of Large Language Models'
arxiv_id: '2309.07045'
source_url: https://arxiv.org/abs/2309.07045
tags:
- safety
- llms
- questions
- safetybench
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafetyBench, the first comprehensive benchmark
  for evaluating the safety of large language models (LLMs) using multiple-choice
  questions. The benchmark covers 7 distinct categories of safety concerns, including
  offensiveness, unfairness and bias, physical health, mental health, illegal activities,
  ethics and morality, and privacy and property.
---

# SafetyBench: Evaluating the Safety of Large Language Models

## Quick Facts
- arXiv ID: 2309.07045
- Source URL: https://arxiv.org/abs/2309.07045
- Reference count: 9
- Key outcome: First comprehensive benchmark for LLM safety using multiple-choice questions across 7 categories, showing GPT-4 significantly outperforms other models

## Executive Summary
SafetyBench introduces the first comprehensive benchmark for evaluating large language model safety using multiple-choice questions. The benchmark covers 7 distinct safety categories including offensiveness, bias, physical health, mental health, illegal activities, ethics, and privacy. It comprises 11,435 questions in both Chinese and English and evaluates 25 popular LLMs in zero-shot and few-shot settings. Results show most models achieve less than 80% average accuracy, indicating substantial room for safety improvement.

## Method Summary
SafetyBench uses multiple-choice questions with single correct answers to enable automated safety evaluation of LLMs. The benchmark includes 11,435 diverse questions across 7 safety categories in both Chinese and English. The evaluation pipeline tests 25 LLMs using both zero-shot and few-shot settings, with automated extraction of model responses to calculate accuracy scores. The benchmark is publicly available on GitHub for community use and future development.

## Key Results
- GPT-4 significantly outperforms other LLMs, especially in physical health and ethics/morality categories
- Most LLMs achieve less than 80% average accuracy across all safety categories
- Chinese and English versions show different performance patterns, with Chinese models performing better on Chinese data and GPT models showing more balanced cross-lingual performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multiple-choice format with single correct answers enables automated, cost-effective, and accurate safety evaluation of LLMs.
- **Mechanism:** By structuring safety questions as multiple-choice items with only one correct answer, evaluation can be fully automated without human judgment, reducing cost and latency while maintaining high accuracy.
- **Core assumption:** LLMs can be reliably prompted to select from provided options and their responses can be programmatically extracted without ambiguity.
- **Evidence anchors:**
  - [abstract] "exclusively features multiple-choice questions, each with a single correct answer, which enables automated and cost-effective evaluations of LLMs' safety with exceptional accuracy"
  - [section 1] "In line with well-known benchmarks such as MMLU (Hendrycks et al., 2021b), SafetyBench exclusively features multiple-choice questions"
- **Break condition:** If LLMs frequently generate non-standard responses or refuse to answer in the required format, automated extraction becomes unreliable and human intervention would be needed.

### Mechanism 2
- **Claim:** Covering 7 distinct safety categories with diverse question types provides comprehensive safety assessment across different real-world contexts.
- **Mechanism:** By including multiple safety categories and various question types (dialogue scenarios, real-life situations, safety comparisons, knowledge inquiries), the benchmark tests LLMs' safety understanding in a wide range of situations.
- **Core assumption:** Safety understanding is domain-specific and LLMs may perform differently across categories; comprehensive coverage reveals these differences.
- **Evidence anchors:**
  - [abstract] "comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns"
  - [section 1] "Test questions in SafetyBench encompass a diverse array of types, spanning dialogue scenarios, real-life situations, safety comparisons, safety knowledge inquiries"
- **Break condition:** If certain safety categories have insufficient data diversity or the questions don't reflect real-world scenarios, the benchmark may not accurately assess LLM safety in practice.

### Mechanism 3
- **Claim:** Evaluating both Chinese and English versions ensures broader and more inclusive assessment of LLM safety across languages and cultural contexts.
- **Mechanism:** By creating parallel Chinese and English versions through translation and cultural adaptation, the benchmark can evaluate safety understanding across different linguistic and cultural contexts, revealing language-specific safety gaps.
- **Core assumption:** Safety issues manifest differently across languages and cultures, and translation alone may not capture all nuances.
- **Evidence anchors:**
  - [abstract] "Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages"
  - [section 1] "SafetyBench offers both Chinese and English data, which could facilitate the evaluation of both Chinese and English LLMs"
- **Break condition:** If translation introduces significant noise or cultural differences create divergent answers, the benchmark may not provide fair cross-language comparisons.

## Foundational Learning

- **Concept:** Safety understanding in LLMs
  - **Why needed here:** The benchmark measures whether LLMs can correctly identify safe vs. unsafe content across multiple categories, which is fundamental to building safe AI systems.
  - **Quick check question:** What is the difference between a model's ability to understand safety concepts versus its ability to generate safe responses in open-ended scenarios?

- **Concept:** Multiple-choice question design
  - **Why needed here:** SafetyBench relies on well-designed multiple-choice questions with one correct answer to enable automated evaluation.
  - **Quick check question:** How does the presence of plausible but incorrect options in multiple-choice questions affect the difficulty and validity of safety assessment?

- **Concept:** Cross-lingual evaluation methodology
  - **Why needed here:** The benchmark includes both Chinese and English versions, requiring understanding of translation quality and cultural adaptation in safety evaluation.
  - **Quick check question:** What challenges arise when translating safety-related questions between languages with different cultural norms and legal frameworks?

## Architecture Onboarding

- **Component map:** Question collection -> Quality control -> Translation/adaptation -> Evaluation pipeline -> Automated extraction -> Analysis
- **Critical path:**
  1. Collect diverse safety questions across 7 categories
  2. Apply quality control and human verification
  3. Create Chinese and English versions
  4. Evaluate 25 LLMs using automated extraction
  5. Analyze results by category and language

- **Design tradeoffs:**
  - Multiple-choice format vs. open-ended responses (automation vs. realism)
  - Translation vs. native question creation (efficiency vs. cultural accuracy)
  - Automated extraction vs. human evaluation (cost vs. accuracy)
  - Broad category coverage vs. depth in specific safety domains

- **Failure signatures:**
  - High variance in extraction success rates across models
  - Disproportionate performance gaps between languages
  - Inconsistent results when evaluating the same model multiple times
  - Unexpected correlations between safety understanding and general capabilities

- **First 3 experiments:**
  1. Test extraction accuracy on a subset of questions across all 25 models to establish baseline reliability
  2. Evaluate a few models on both Chinese and English versions to identify translation or cultural adaptation issues
  3. Run the full benchmark on GPT-4 to establish the reference performance level across all categories

## Open Questions the Paper Calls Out

Based on my understanding of the paper, here are 3 open questions:

### Open Question 1
- Question: How does the performance of LLMs on SafetyBench correlate with their performance on other established safety evaluation benchmarks like RealToxicityPrompts or BBQ?
- Basis in paper: [inferred] The paper mentions that SafetyBench covers 7 distinct categories of safety concerns, including offensiveness, unfairness and bias, physical health, mental health, illegal activities, ethics and morality, and privacy and property. It also states that GPT-4 significantly outperforms other LLMs, especially in categories like physical health and ethics and morality. However, the paper does not compare the performance of LLMs on SafetyBench with other established safety evaluation benchmarks.
- Why unresolved: The paper does not provide a direct comparison between the performance of LLMs on SafetyBench and other established safety evaluation benchmarks. This could be due to the fact that SafetyBench is the first comprehensive benchmark for evaluating the safety of LLMs, and therefore, there might not be enough data available for such a comparison.
- What evidence would resolve it: A study comparing the performance of LLMs on SafetyBench with their performance on other established safety evaluation benchmarks like RealToxicityPrompts or BBQ would provide evidence to resolve this question.

### Open Question 2
- Question: How does the performance of LLMs on SafetyBench vary across different languages and cultures?
- Basis in paper: [explicit] The paper mentions that SafetyBench incorporates both Chinese and English data, facilitating the evaluation in both languages. It also states that the GPT series from OpenAI exhibit more balanced performances on Chinese and English data, while LLMs created by Chinese organizations perform significantly better on Chinese data.
- Why unresolved: While the paper provides some insights into the performance of LLMs on SafetyBench across different languages, it does not delve into how the performance varies across different cultures. This could be due to the fact that the paper primarily focuses on the overall performance of LLMs on SafetyBench, rather than the nuances of their performance across different languages and cultures.
- What evidence would resolve it: A study examining the performance of LLMs on SafetyBench across different languages and cultures would provide evidence to resolve this question.

### Open Question 3
- Question: How can the SafetyBench benchmark be used to guide the development of safer LLMs?
- Basis in paper: [explicit] The paper mentions that SafetyBench enables automated, efficient safety evaluation of LLMs. It also states that the benchmark could play an important role in evaluating the safety of LLMs and facilitating the rapid development of safer LLMs.
- Why unresolved: While the paper provides some insights into how SafetyBench can be used to evaluate the safety of LLMs, it does not provide a detailed explanation of how the benchmark can be used to guide the development of safer LLMs. This could be due to the fact that the paper primarily focuses on introducing the SafetyBench benchmark, rather than its practical applications in guiding the development of safer LLMs.
- What evidence would resolve it: A study exploring the practical applications of SafetyBench in guiding the development of safer LLMs would provide evidence to resolve this question.

## Limitations

- The multiple-choice format may not fully capture real-world safety behaviors in open-ended scenarios
- Translation quality and cultural adaptation between Chinese and English versions remain uncertain
- The benchmark's ability to accurately reflect real-world safety performance is limited by its structured format

## Confidence

**High Confidence:** The benchmark successfully achieves automated evaluation through multiple-choice format. The paper provides clear evidence that GPT-4 significantly outperforms other models across most categories, with quantitative results supporting this claim.

**Medium Confidence:** The claim that most LLMs achieve less than 80% average accuracy indicating substantial room for improvement. While results are presented, the benchmark's ability to accurately reflect real-world safety performance remains uncertain due to the multiple-choice format limitations.

**Low Confidence:** The claim about comprehensive coverage across all 7 safety categories. While the benchmark includes questions from all categories, the depth and quality of coverage for each category is not fully demonstrated, particularly for more nuanced categories like ethics and morality.

## Next Checks

1. **Extraction Reliability Test:** Conduct a detailed analysis of answer extraction success rates across all 25 models to quantify how often models respond in the required format versus generating refusals or ambiguous answers.

2. **Cross-Lingual Validation:** Evaluate a subset of models on both Chinese and English versions using identical questions to measure performance consistency and identify potential translation or cultural adaptation issues.

3. **Open-Ended Comparison:** Compare SafetyBench scores with human evaluation results on open-ended safety prompts for a subset of models to assess how well multiple-choice performance correlates with real-world safety behavior.