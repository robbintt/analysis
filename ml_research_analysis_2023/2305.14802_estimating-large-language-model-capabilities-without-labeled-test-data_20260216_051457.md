---
ver: rpa2
title: Estimating Large Language Model Capabilities without Labeled Test Data
arxiv_id: '2305.14802'
source_url: https://arxiv.org/abs/2305.14802
tags:
- accuracy
- test
- confidence
- dataset
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the problem of in-context learning (ICL)
  accuracy estimation: predicting how well a large language model will perform on
  a new task given only unlabeled test data and a few labeled training examples. The
  proposed solution is a meta-model that uses LLM confidence score distributions (confidence
  profiles) as features to predict ICL accuracy on out-of-distribution datasets.'
---

# Estimating Large Language Model Capabilities without Labeled Test Data

## Quick Facts
- arXiv ID: 2305.14802
- Source URL: https://arxiv.org/abs/2305.14802
- Reference count: 12
- Key outcome: A meta-model using LLM confidence score distributions achieves 6.27 average MAE on ICL accuracy estimation, outperforming baselines in 7 of 12 settings

## Executive Summary
This paper addresses the challenge of estimating how well a large language model will perform on a new task given only unlabeled test data and a few labeled training examples. The proposed solution uses LLM confidence score distributions (confidence profiles) as features to train a meta-model that predicts in-context learning accuracy at the dataset level. The method is evaluated across 12 settings spanning 4 LLMs, 3 task collections, and multiple baselines. Results show the meta-model achieves an average mean absolute error of 6.27, outperforming all baselines in 7 out of 12 settings, with the most promise shown for closed-book QA tasks where it outperforms the strongest baseline by 65% in one setting.

## Method Summary
The method trains a meta-model to predict ICL accuracy using LLM confidence score distributions as features. The approach extracts confidence profiles from both in-distribution datasets (with known ICL accuracies) and unlabeled test datasets, then uses these profiles to estimate accuracy on new tasks. The meta-model is trained on confidence vectors representing the distribution of confidence scores across examples in a dataset, learning the mapping from these distributions to dataset-level ICL accuracy. The approach is evaluated using three meta-model architectures (XGBoost, MLP, k-NN) across multiple LLMs and task types, comparing performance against several strong baselines.

## Key Results
- Average mean absolute error of 6.27 across all 12 evaluation settings
- Outperforms all baselines in 7 out of 12 settings
- Best performance: matches oracle evaluation using 128 labeled test examples on OPT-13B for MCQA
- Shows 65% improvement over strongest baseline for closed-book QA on OPT-6.7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence profiles serve as a task-level calibration signal that correlates with ICL accuracy
- Mechanism: The meta-model learns a mapping from LLM confidence score distributions to dataset-level ICL accuracy by capturing how confidence varies across examples
- Core assumption: The distribution of confidence scores across a dataset is predictive of the dataset's ICL accuracy and generalizes across tasks
- Evidence anchors:
  - [abstract] "we propose a method that trains a meta-model using LLM confidence scores as features"
  - [section] "we focus on dataset-level calibrationâ€”whether we can use overall model confidence distributions to estimate dataset-level accuracies"
- Break condition: If confidence distributions don't correlate with accuracy across tasks or the relationship is highly task-specific

### Mechanism 2
- Claim: Out-of-distribution accuracy estimation can be performed without labeled test data by leveraging confidence features from in-distribution datasets
- Mechanism: The meta-model trained on in-distribution datasets with known ICL accuracies can predict accuracy on new, unlabeled datasets using their confidence profiles
- Core assumption: The confidence-accuracy relationship learned on in-distribution datasets generalizes to OOD datasets
- Evidence anchors:
  - [abstract] "we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled data for that task"
  - [section] "We propose a method to learn out-of-distribution (OOD) model calibration patterns based on observations of LLMs' performance at the dataset level"
- Break condition: If the confidence-accuracy relationship doesn't transfer to OOD datasets or the meta-model overfits to the training distribution

### Mechanism 3
- Claim: The meta-model architecture (XGBoost, MLP, or k-NN) can effectively learn the mapping from confidence features to ICL accuracy
- Mechanism: Different meta-model architectures are evaluated for their ability to learn the non-linear relationship between confidence profiles and ICL accuracy
- Core assumption: The relationship between confidence profiles and ICL accuracy is learnable by the chosen meta-model architectures
- Evidence anchors:
  - [abstract] "we compare our method to several strong accuracy estimation baselines on a comprehensive set of 12 evaluation settings"
  - [section] "We choose meta-models that are easy to train and contain far fewer parameters than LLMs for computational efficiency"
- Break condition: If the chosen architectures cannot capture the relationship or a different architecture would perform significantly better

## Foundational Learning

- Concept: In-context learning (ICL) and its task-dependent nature
  - Why needed here: Understanding ICL is crucial because the paper proposes a method to estimate ICL accuracy without labeled test data
  - Quick check question: What are the two main approaches for ICL mentioned in the related work section?

- Concept: Model calibration and its extension to dataset-level calibration
  - Why needed here: The proposed method relies on using confidence score distributions as features, which extends model calibration concepts to the dataset level
  - Quick check question: How does dataset-level calibration differ from example-level calibration?

- Concept: Out-of-distribution (OOD) prediction and its challenges
  - Why needed here: The proposed method aims to estimate ICL accuracy on OOD datasets without labeled test data, which is a form of OOD prediction
  - Quick check question: What are some common approaches to OOD prediction mentioned in the related work section?

## Architecture Onboarding

- Component map:
  LLM (OPT, LLaMA models) -> Confidence profile extraction -> Meta-model (XGBoost, MLP, k-NN) -> ICL accuracy prediction

- Critical path:
  1. Extract confidence profiles from in-distribution datasets using the LLM
  2. Train meta-model on confidence profiles and ICL accuracies from in-distribution datasets
  3. Extract confidence profile from unlabeled test dataset using the LLM
  4. Use meta-model to predict ICL accuracy on the test dataset

- Design tradeoffs:
  - Using confidence profiles vs. other meta-features (e.g., model embeddings)
  - Dimensionality of confidence vectors (dc) and its impact on performance and computational cost
  - Choice of meta-model architecture based on performance and efficiency

- Failure signatures:
  - High MAE in accuracy estimation compared to baselines
  - Meta-model overfitting to in-distribution datasets, leading to poor OOD performance
  - Confidence profiles not being predictive of ICL accuracy for certain tasks or datasets

- First 3 experiments:
  1. Implement confidence profile extraction for a single LLM and dataset, and visualize the distribution
  2. Train a simple meta-model (e.g., k-NN) on confidence profiles and ICL accuracies from a small set of in-distribution datasets, and evaluate on a held-out dataset
  3. Compare the performance of different meta-model architectures (XGBoost, MLP, k-NN) on a larger set of datasets and LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the ideal number of unlabeled test examples (m) that optimizes the tradeoff between estimation accuracy and computational cost for confidence profile-based ICL accuracy estimation?
- Basis in paper: Inferred from Figure 3, which shows the relationship between m and MAE, and the discussion about computational cost constraints
- Why unresolved: The paper demonstrates that increasing m reduces MAE but does not identify an optimal point that balances accuracy gains against inference costs
- What evidence would resolve it: Empirical studies comparing MAE reduction rates with computational cost increases across different m values, identifying the point of diminishing returns

### Open Question 2
- Question: How do different meta-model architectures perform on ICL accuracy estimation tasks beyond the three tested (k-NN, MLP, XGBoost)?
- Basis in paper: Inferred from the discussion of meta-model architecture ablation and the statement that "substantial room for improvement still exists"
- Why unresolved: The paper only tests three relatively simple architectures, leaving open the question of whether more sophisticated models could achieve better performance
- What evidence would resolve it: Comparative studies testing additional architectures (e.g., transformer-based models, ensemble methods) on the same benchmark

### Open Question 3
- Question: What is the relationship between the number of in-context examples (k) and the accuracy of ICL performance estimation?
- Basis in paper: Explicit mention in Section 5.3 discussing "Effect of number of shots" and the comparison between 4-shot and 5-shot settings
- Why unresolved: The paper only compares two specific k values (4-shot vs 5-shot) for one LLM-dataset combination, not providing a comprehensive understanding of how k affects estimation accuracy
- What evidence would resolve it: Systematic experiments varying k across a wider range of values and multiple LLM-dataset combinations to establish the relationship

## Limitations
- Limited task coverage: Only evaluates on multiple-choice QA and closed-book QA tasks, leaving uncertainty about generalizability to other NLP tasks
- Prompt sensitivity: Uses a "default prompt template" without exploring how different prompt formulations affect confidence profile quality
- Confidence score assumptions: Relies on the ability to compute meaningful confidence scores from probability distributions, which may not generalize to all task types

## Confidence
- High confidence: The core insight that confidence profiles can provide accuracy estimates without labeled test data is well-supported by experimental results
- Medium confidence: The claim of generalizability across different LLMs and task types is partially supported but limited by the narrow task scope
- Low confidence: The assertion that this approach is "useful in practice" given "substantial room for improvement" reflects uncertainty about real-world applicability

## Next Checks
1. **Cross-task generalization test**: Evaluate the method on at least two additional task types (e.g., text classification and generation tasks) to verify whether confidence profile effectiveness generalizes beyond QA tasks

2. **Prompt sensitivity analysis**: Systematically vary prompt templates (number of examples, example ordering, prompt phrasing) and measure how confidence profile quality and accuracy estimation performance change

3. **Real-world deployment simulation**: Simulate a realistic deployment scenario where the meta-model must estimate ICL accuracy for tasks from completely unseen domains (e.g., medical QA, legal document analysis) with no domain overlap with training data