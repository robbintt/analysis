---
ver: rpa2
title: Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive
  Solvers for Vehicle Routing Problems with Faster Inference Speed
arxiv_id: '2312.12469'
source_url: https://arxiv.org/abs/2312.12469
tags:
- gnarkd
- node
- student
- greedy
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the inference speed
  of neural construction models for Vehicle Routing Problems (VRPs) while maintaining
  high solution quality. The proposed method, Guided Non-Autoregressive Knowledge
  Distillation (GNARKD), transforms Autoregressive (AR) models into Non-Autoregressive
  (NAR) ones through knowledge distillation.
---

# Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed

## Quick Facts
- arXiv ID: 2312.12469
- Source URL: https://arxiv.org/abs/2312.12469
- Reference count: 9
- Primary result: 4-5× faster inference with 2-3% solution quality drop on TSP and CVRP

## Executive Summary
This paper introduces Guided Non-Autoregressive Knowledge Distillation (GNARKD), a method that transforms Autoregressive (AR) models into Non-Autoregressive (NAR) ones for Vehicle Routing Problems (VRPs). By modifying the decoder structure and using AR model solutions as guidance, GNARKD enables parallel inference while preserving learned knowledge. The method achieves significant speed improvements (4-5× faster) with acceptable performance degradation (2-3%) on TSP and CVRP instances, making it promising for real-world applications requiring immediate solutions.

## Method Summary
GNARKD modifies the AR decoder to remove sequential dependencies while preserving the encoder architecture. The method uses knowledge distillation with a low temperature (T1=0.1) to transfer order-dependent information from AR to NAR models. The NAR decoder employs unmasked attention and post-processing constraints to maintain solution feasibility. During training, the AR model's action distributions guide the NAR model's decoding process, enabling it to learn confident action selection while maintaining parallel processing capability.

## Key Results
- 4-5× faster inference speed compared to AR models
- 2-3% solution quality degradation on TSP and CVRP problems
- On-par solution quality with state-of-the-art models
- Effective performance on both synthetic and real-world instances (51-159 nodes)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing sequential dependency enables parallel decoding without losing learned representations
- Mechanism: The GNARKD method keeps the encoder unchanged while transforming the decoder to remove action history dependency, allowing node embeddings to be processed in parallel
- Core assumption: Encoder representations are sufficiently rich to support direct decoding without sequential context
- Evidence anchors: Abstract states the approach "removes the constraint of sequential generation in AR models while preserving the learned pivotal components"

### Mechanism 2
- Claim: Guided knowledge distillation enables NAR models to learn order-dependent information
- Mechanism: Using the AR model's action probability distribution as a teacher signal, the NAR model learns to replicate confident actions while maintaining parallel decoding capability
- Core assumption: AR model's action probability distribution contains valuable order-dependent information that can be transferred to NAR models
- Evidence anchors: Paper states they "use the solution generated by the AR model as the teacher network to provide decoding guidance"

### Mechanism 3
- Claim: Modified decoder output processing maintains solution feasibility in NAR decoding
- Mechanism: Instead of using masks in attention layers, the NAR decoder uses post-decoder processing with constrained sets that update based on remaining capacity and visited nodes
- Core assumption: Solution feasibility can be maintained through post-processing rather than during decoding attention steps
- Evidence anchors: Paper explains the student decoder "does not employ such masks because it only relies on the post-decoder output processing for generating valid solutions"

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper builds on Transformer-based AR models and modifies their decoder structure
  - Quick check question: Can you explain the difference between self-attention and cross-attention in the Transformer decoder?

- Concept: Knowledge distillation principles
  - Why needed here: The core technique is transforming knowledge from AR to NAR models through distillation
  - Quick check question: What is the purpose of using temperature scaling in knowledge distillation?

- Concept: Combinatorial optimization problem formulation
  - Why needed here: VRPs are combinatorial optimization problems where solution quality is measured against optimal or near-optimal solutions
  - Quick check question: How is solution quality typically measured for TSP and CVRP problems?

## Architecture Onboarding

- Component map: Encoder → Decoder input transformation → Parallel attention computation → Post-processing constraints → Solution generation
- Critical path: Encoder processes node features → Modified decoder input removes action history → Parallel attention computes node relationships → Post-processing enforces constraints → Solution generated
- Design tradeoffs: Maintaining encoder unchanged preserves learned representations but limits architectural innovation; removing attention masks speeds up inference but requires careful post-processing constraint handling; low distillation temperature improves confidence but may reduce solution diversity
- Failure signatures: Solution quality drops significantly when input size increases beyond training distribution; training instability when distillation temperature is not properly tuned; constraint violations in CVRP solutions due to post-processing errors
- First 3 experiments:
  1. Train GNARKD-AM on TSP-50 with varying distillation temperatures (0.1, 0.5, 1.0) and measure solution quality
  2. Compare inference speed of GNARKD-AM vs AM on TSP instances ranging from 50 to 500 nodes
  3. Test GNARKD-AM on CVRP instances with different capacity constraints to verify feasibility maintenance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GNARKD's performance change when handling more complex constraints beyond capacity and demand?
- Basis in paper: The paper mentions that the current version may not perform well with complex constraints and suggests modeling constraints directly on the decoder instead of post-processing
- Why unresolved: Only tested on TSP and CVRP with basic constraints, not more complex variants
- What evidence would resolve it: Testing GNARKD on VRPs with time windows, multiple depots, or other complex constraints and comparing performance to baseline methods

### Open Question 2
- Question: Would using multiple teacher models improve GNARKD student performance beyond what single teachers achieve?
- Basis in paper: The conclusion states they plan to use multiple teachers to train students for further performance gain
- Why unresolved: Only single teacher models were used in current experiments
- What evidence would resolve it: Experiments comparing GNARKD students trained with one teacher versus multiple teachers on the same VRP instances

### Open Question 3
- Question: What is the theoretical limit of how much inference speed can be improved through non-autoregressive knowledge distillation?
- Basis in paper: The paper shows significant speed improvements but doesn't explore theoretical bounds or diminishing returns
- Why unresolved: Experiments show practical improvements but don't establish theoretical limits
- What evidence would resolve it: Mathematical analysis or extensive experiments varying model sizes and problem complexities to determine speed improvement ceilings

## Limitations
- Performance claims rely heavily on synthetic datasets and small-scale real-world instances (up to 159 nodes)
- Method requires pre-trained AR models as teachers, creating dependency chain that may limit practical applicability
- Does not demonstrate effectiveness on large-scale industrial VRP instances with thousands of nodes

## Confidence

- **High Confidence**: Inference speed improvement (4-5× faster) - straightforward computational measurement
- **Medium Confidence**: Solution quality degradation (2-3% drop) - reported gap is within acceptable ranges but methodology could vary
- **Low Confidence**: Generalizability to larger problem instances - only tested up to 500 nodes, making real-world applicability claims speculative

## Next Checks

1. **Scale Test**: Validate GNARKD performance on VRP instances with 1000+ nodes using real-world logistics datasets to assess scalability claims
2. **Teacher Dependency Analysis**: Test GNARKD performance when using different quality teacher models (from excellent to mediocre) to quantify the method's robustness to teacher quality
3. **Real-world Deployment Simulation**: Implement GNARKD in a simulated logistics scenario with time-varying demands and compare operational cost savings against traditional heuristics