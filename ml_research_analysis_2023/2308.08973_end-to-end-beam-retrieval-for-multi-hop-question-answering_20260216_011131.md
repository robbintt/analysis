---
ver: rpa2
title: End-to-End Beam Retrieval for Multi-Hop Question Answering
arxiv_id: '2308.08973'
source_url: https://arxiv.org/abs/2308.08973
tags:
- beam
- retrieval
- multi-hop
- passages
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Beam Retrieval, a general end-to-end retrieval
  framework for multi-hop question answering. It addresses the limitations of previous
  retrievers which are often customized for two-hop questions and lack supervision
  over the entire multi-hop retrieval process.
---

# End-to-End Beam Retrieval for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2308.08973
- Source URL: https://arxiv.org/abs/2308.08973
- Reference count: 10
- This paper introduces Beam Retrieval, a general end-to-end retrieval framework for multi-hop question answering that achieves state-of-the-art performance.

## Executive Summary
This paper introduces Beam Retrieval, a general end-to-end retrieval framework for multi-hop question answering. It addresses the limitations of previous retrievers which are often customized for two-hop questions and lack supervision over the entire multi-hop retrieval process. Beam Retrieval jointly optimizes an encoder and two classification heads across all hops while maintaining multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages.

## Method Summary
Beam Retrieval implements a beam search paradigm for multi-hop retrieval by maintaining multiple partial hypotheses at each hop. The framework jointly optimizes an encoder (DeBERTa) and two classification heads (classifier1 for first hop, classifier2 for subsequent hops) across all hops in an end-to-end manner. During training and inference, the model uses a consistent beam size to prevent distribution shift, and expands the search space by considering all passages from top hypotheses at each subsequent hop. The method handles variable-length sequences through truncation and uses shuffled inner orders of concatenated passages within hypotheses.

## Key Results
- Achieves nearly 50% improvement compared with baselines on the challenging MuSiQue-Ans dataset
- Surpasses all previous retrievers on HotpotQA
- Achieves 99.9% precision on 2WikiMultiHopQA
- Helps supervised reader achieve new state-of-the-art performance and improves zero-shot GPT-3.5 performance by up to 28.8 points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Beam search paradigm expands search space by maintaining multiple partial hypotheses at each hop.
- Mechanism: At each hop, Beam Retrieval keeps B highest-scoring hypotheses instead of just one, allowing exploration of alternative passage sequences.
- Core assumption: Maintaining multiple hypotheses reduces the risk of missing relevant passages that might be overlooked with single-passage selection.
- Evidence anchors: [abstract]: "maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages"

### Mechanism 2
- Claim: End-to-end training with consistent beam size reduces the gap between training and reasoning.
- Mechanism: Beam Retrieval trains and reasons with the same beam size B, optimizing encoder and classification heads across all hops simultaneously.
- Core assumption: Consistent training and inference beam sizes prevent distribution shift and improve generalization.
- Evidence anchors: [abstract]: "jointly optimizes an encoder and two classification heads across all hops"

### Mechanism 3
- Claim: Two classification heads (one for first hop, one for subsequent hops) are optimal for the task.
- Mechanism: classifier1 handles the first hop with fixed n passages, while classifier2 handles variable-length sequences in expanded search space.
- Core assumption: The first hop has different characteristics than subsequent hops due to different input formats and search spaces.
- Evidence anchors: [section 4]: "At the first hop... At subsequent hop t... We use the same encoder but another classification head named classifier2"

## Foundational Learning

- Concept: Auto-regressive language generation
  - Why needed here: The paper draws a direct analogy between auto-regressive generation and multi-hop retrieval
  - Quick check question: How does the paper map language generation concepts to the retrieval task?

- Concept: Beam search algorithm
  - Why needed here: Beam Retrieval is fundamentally built on the beam search paradigm
  - Quick check question: What is the key difference between beam search and greedy search in this context?

- Concept: Multi-hop reasoning
  - Why needed here: The paper addresses multi-hop QA which requires reasoning across multiple documents
  - Quick check question: Why do previous two-hop methods fail on more complex scenarios?

## Architecture Onboarding

- Component map: Question → Encoder → classifier1 → First hop passages → Expanded search space → classifier2 → Subsequent hops → Final passage chain → Reader → Answer

- Critical path: Question → Encoder → classifier1 → First hop passages → Expanded search space → classifier2 → Subsequent hops → Final passage chain → Reader → Answer

- Design tradeoffs:
  - Beam size vs. computational cost (memory and speed)
  - Single vs. multiple classification heads
  - Fixed vs. variable input sequence lengths
  - End-to-end training vs. stage-wise training

- Failure signatures:
  - Poor retrieval EM/F1 on development sets
  - High variance in training loss across hops
  - Memory overflow with large beam sizes
  - Slow training due to multiple encoder calls

- First 3 experiments:
  1. Test different beam sizes (1, 2, 3, 4) on MuSiQue-Ans to find optimal balance
  2. Compare single vs. dual classification heads on retrieval performance
  3. Evaluate end-to-end vs. stage-wise training on HotpotQA

## Open Questions the Paper Calls Out
Based on my understanding of the paper, here are 3-5 concrete open questions:

### Open Question 1
- Question: How does Beam Retrieval's performance scale with increasingly longer multi-hop questions (e.g. 5-10 hops)?
- Basis in paper: [inferred] The paper mentions MuSiQue-Ans has 2-4 hop questions and demonstrates strong performance, but does not explore performance on even longer hop questions.
- Why unresolved: The paper does not conduct experiments with multi-hop questions beyond 4 hops.

### Open Question 2
- Question: How does Beam Retrieval compare to other end-to-end multi-hop retrieval methods that jointly optimize across hops, such as using reinforcement learning?
- Basis in paper: [explicit] The paper introduces Beam Retrieval as an end-to-end retrieval method that jointly optimizes an encoder and classification heads across all hops.
- Why unresolved: The paper does not compare Beam Retrieval to other end-to-end multi-hop retrieval methods.

### Open Question 3
- Question: How does Beam Retrieval's performance change when using different backbone models like GPT or T5 instead of BERT-based models?
- Basis in paper: [inferred] The paper uses BERT-based models as the backbone for Beam Retrieval.
- Why unresolved: The paper does not explore using different backbone models.

## Limitations
- Reliance on DeBERTa encoder may not generalize optimally to all retrieval scenarios
- Truncation method for handling long concatenated sequences could discard relevant information
- Fixed beam size throughout training and inference may not be most efficient for all dataset characteristics

## Confidence
- High confidence: The core mechanism of maintaining multiple hypotheses through beam search is well-established
- Medium confidence: The claim of 50% improvement on MuSiQue-Ans is well-supported by experimental results
- Medium confidence: The claim about GPT-3.5 improvements (up to 28.8 points) is supported by results but has variability

## Next Checks
1. Replicate the beam size ablation study: Systematically test beam sizes 1-4 on MuSiQue-Ans to verify performance degradation
2. Test alternative encoder configurations: Evaluate whether other transformer architectures can match or exceed DeBERTa's performance
3. Validate truncation impact: Conduct experiments with varying truncation thresholds to quantify impact on retrieval accuracy