---
ver: rpa2
title: Off-Policy Evaluation for Large Action Spaces via Policy Convolution
arxiv_id: '2310.15433'
source_url: https://arxiv.org/abs/2310.15433
tags:
- convolution
- estimators
- quality
- tree
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Policy Convolution (PC) framework for
  off-policy evaluation in large action spaces, addressing the high variance and common
  support violations that plague traditional importance sampling methods. PC leverages
  action embeddings to strategically convolve logging and target policies, introducing
  a bias-variance trade-off controlled by the amount of convolution.
---

# Off-Policy Evaluation for Large Action Spaces via Policy Convolution

## Quick Facts
- arXiv ID: 2310.15433
- Source URL: https://arxiv.org/abs/2310.15433
- Reference count: 40
- This paper introduces the Policy Convolution (PC) framework for off-policy evaluation in large action spaces, addressing the high variance and common support violations that plague traditional importance sampling methods.

## Executive Summary
This paper introduces the Policy Convolution (PC) framework for off-policy evaluation in large action spaces, addressing the high variance and common support violations that plague traditional importance sampling methods. PC leverages action embeddings to strategically convolve logging and target policies, introducing a bias-variance trade-off controlled by the amount of convolution. Four convolution functions are proposed: kernel smoothing, tree smoothing, ball smoothing, and kNN smoothing. Extensive experiments on synthetic and real-world datasets demonstrate that PC achieves up to 5-6 orders of magnitude improvement in mean squared error compared to existing estimators, particularly when action space size is large, policy mismatch is high, or the common support assumption is violated.

## Method Summary
The Policy Convolution framework replaces traditional importance sampling with a convolution operation over action embeddings. Given a context, the logging policy's action probabilities are smoothed using the target policy's probabilities weighted by action similarity in embedding space. This creates a new, smoothed logging policy that better aligns with the target policy while reducing variance. The convolution amount is controlled by hyperparameters œÑ1 and œÑ2, allowing explicit bias-variance trade-off management. Four convolution functions are proposed: kernel smoothing, tree smoothing, ball smoothing, and kNN smoothing. These are applied to backbone estimators including IPS, SNIPS, DR, and SNDR.

## Key Results
- PC achieves up to 5-6 orders of magnitude improvement in mean squared error compared to existing estimators
- The framework is particularly effective when action space size is large, policy mismatch is high, or common support assumption is violated
- PC demonstrates ability to fill blind spots created by deficient support in logging policy through action similarity-based smoothing
- Extensive experiments validate PC's effectiveness across synthetic and real-world datasets (Movielens-100k)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy Convolution reduces variance by smoothing importance weights via action embeddings.
- Mechanism: The PC framework replaces raw importance weights with weighted averages over similar actions in embedding space, effectively reducing the maximum propensity weight growth from linear to sublinear in action space size.
- Core assumption: Action embeddings capture reward-relevant similarity structure.
- Evidence anchors:
  - [abstract] "These methods leverage latent structure within actions‚Äîmade available through action embeddings‚Äîto strategically convolve the logging and target policies."
  - [section] "the variance of these importance sampling based estimators grows roughly linearly w.r.t. the maximum propensity weight in D. And the maximum propensity weight can grow linearly w.r.t. the size of action space Œ©(|A |)"
- Break condition: If embeddings do not capture reward-relevant similarity, convolution may increase bias without variance reduction benefit.

### Mechanism 2
- Claim: PC provides bias-variance trade-off control through convolution amount.
- Mechanism: By adjusting convolution parameters (ùúè1, ùúè2), PC can shift between unbiased but high-variance estimation (minimal convolution) and low-variance but biased estimation (heavy convolution), finding optimal MSE balance.
- Core assumption: There exists a non-trivial optimal convolution amount for each scenario.
- Evidence anchors:
  - [abstract] "This convolution introduces a unique bias-variance trade-off, which can be controlled by adjusting the amount of convolution."
  - [section] "We observe the best bias-variance trade-off, leading to the lowest MSE" (from motivating example)
- Break condition: If optimal convolution point is at extreme (0 or maximum), framework offers no advantage over baseline.

### Mechanism 3
- Claim: PC addresses common support violations by filling blind spots through action similarity.
- Mechanism: When logging policy has zero probability on certain actions (deficient support), PC's convolution assigns non-zero smoothed probabilities based on similar actions' embeddings, reducing irrecoverable bias.
- Core assumption: Similar actions have similar rewards for given contexts.
- Evidence anchors:
  - [section] "when Assumption 2.2 is violated, the variance of such importance sampling based estimators becomes unbounded, in addition to incurring a bias of Eùë•{Œ£ùëé‚ààU(ùë•) œÄ(ùëé|ùë•)Œ¥(ùëé|ùë•)}"
  - [section] "PC is able to accurately leverage action-embeddings as a guide for appropriately filling-in the blind spots"
- Break condition: If similar actions have divergent rewards, filling blind spots introduces large bias.

## Foundational Learning

- Concept: Importance sampling and its variance properties in large action spaces
  - Why needed here: Understanding why vanilla IS fails in large action spaces is crucial for appreciating PC's contribution
  - Quick check question: Why does IS variance grow linearly with action space size?

- Concept: Action embeddings and their role in capturing similarity
  - Why needed here: PC's effectiveness depends on action embeddings capturing reward-relevant structure
  - Quick check question: What properties should action embeddings have for PC to work effectively?

- Concept: Bias-variance trade-off in statistical estimation
  - Why needed here: PC explicitly trades bias for variance reduction, requiring understanding of this fundamental trade-off
  - Quick check question: When is it beneficial to introduce bias to reduce variance?

## Architecture Onboarding

- Component map:
  - Data pipeline: context-action-reward tuples with action embeddings
  - Convolution engine: implements kernel, tree, ball, or kNN smoothing functions
  - Backbone estimator: IPS, SNIPS, DR, or SNDR implementation
  - Hyperparameter tuner: finds optimal convolution parameters (ùúè1, ùúè2)
  - Evaluation module: computes MSE, bias, and variance

- Critical path:
  1. Load logged data and action embeddings
  2. Implement convolution function (choose one: kernel, tree, ball, kNN)
  3. Implement backbone estimator with convolution wrapper
  4. Add hyperparameter optimization for convolution parameters
  5. Validate on synthetic data before real-world application

- Design tradeoffs:
  - Convolution function choice: kernel offers smoothness but requires bandwidth tuning; tree offers interpretability but needs hierarchical structure; ball and kNN are simpler but may create discontinuities
  - Convolution symmetry: convolving both policies differently (ùúè1 ‚â† ùúè2) provides more flexibility than equal convolution
  - Computational cost: kNN and tree methods scale better than kernel smoothing for large action spaces

- Failure signatures:
  - Performance similar to baseline: suggests embeddings don't capture relevant structure
  - Worsening MSE with increased convolution: indicates bias increase outweighs variance reduction
  - High sensitivity to convolution parameters: suggests model instability

- First 3 experiments:
  1. Implement kernel smoothing PC-IPS on synthetic data with known optimal convolution parameter
  2. Compare PC with different convolution functions (kernel, tree, ball, kNN) on synthetic data
  3. Test PC performance under varying support deficiency ratios to validate blind spot filling capability

## Open Questions the Paper Calls Out
- How should the amount of convolution (œÑ1, œÑ2) be automatically selected in practice without validation data?
- The paper acknowledges understanding and developing principled techniques for automatically selecting the level of convolution is an interesting research direction.

## Limitations
- The effectiveness critically depends on quality of action embeddings, which is not fully explored
- Computational complexity of kNN and kernel methods may become prohibitive for extremely large action spaces
- The framework hasn't been validated on continuous action spaces or other domains beyond contextual bandits and recommendation systems

## Confidence
**High Confidence Claims:**
- PC provides significant MSE improvement over baseline estimators in large action spaces (supported by extensive experiments across synthetic and real-world datasets)
- The variance reduction mechanism through action similarity smoothing is mathematically sound (derived from importance sampling variance properties)
- Common support violations can be mitigated through convolution when similar actions have similar rewards (validated through synthetic experiments)

**Medium Confidence Claims:**
- The bias-variance trade-off can be effectively controlled through convolution amount tuning (supported by experiments but dependent on embedding quality)
- The four convolution functions (kernel, tree, ball, kNN) are approximately equivalent in performance (based on comparative results but not exhaustively tested across all scenarios)
- PC's performance improvements hold across diverse domains (validated on synthetic and Movielens data but limited to two real-world datasets)

**Low Confidence Claims:**
- PC will generalize to other domains beyond contextual bandits and recommendation systems (extrapolation beyond tested domains)
- The framework extends naturally to continuous action spaces (mentioned as future work but not validated)
- Computational costs remain manageable for industrial-scale applications (acknowledged as trade-off but not rigorously analyzed)

## Next Checks
1. **Embedding Sensitivity Analysis**: Systematically test PC performance with embeddings of varying quality (learned vs random vs scrambled) across different domains to quantify the sensitivity to embedding quality and establish minimum embedding requirements.

2. **Continuous Action Space Extension**: Implement and validate PC on a continuous action space problem (e.g., robotics control) to verify the framework's extensibility beyond discrete action spaces, addressing the paper's future work claim.

3. **Large-Scale Industrial Validation**: Deploy PC on a real-world large-scale recommendation system with millions of actions to empirically validate the computational trade-offs and performance claims in production settings, particularly testing kNN and kernel smoothing scalability.