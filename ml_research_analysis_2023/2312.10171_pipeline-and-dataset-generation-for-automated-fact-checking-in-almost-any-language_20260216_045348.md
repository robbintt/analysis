---
ver: rpa2
title: Pipeline and Dataset Generation for Automated Fact-checking in Almost Any Language
arxiv_id: '2312.10171'
source_url: https://arxiv.org/abs/2312.10171
tags:
- data
- claim
- evidence
- qacg
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pipeline for automated fact-checking that
  is easily deployable in various languages, including Czech, English, Polish, and
  Slovak. The pipeline consists of evidence retrieval and claim veracity evaluation
  modules, which are trained using synthetically generated data.
---

# Pipeline and Dataset Generation for Automated Fact-checking in Almost Any Language

## Quick Facts
- arXiv ID: 2312.10171
- Source URL: https://arxiv.org/abs/2312.10171
- Reference count: 40
- Primary result: QACG-based synthetic data generation enables multilingual fact-checking pipelines deployable in any language with Wikipedia presence

## Executive Summary
This paper presents a pipeline for automated fact-checking that works across multiple languages by combining synthetic data generation with state-of-the-art retrieval and classification models. The key innovation is the use of Question Answering for Claim Generation (QACG) to create training data without requiring large-scale human annotations in target languages. The pipeline processes claims at the paragraph level rather than sentence level, simplifying architecture while maintaining context. Evaluation on Czech, English, Polish, and Slovak shows that multilingual models trained on aggregated QACG data outperform monolingual models, achieving promising results for automated fact-checking tasks across different languages.

## Method Summary
The pipeline consists of three main components: synthetic data generation using a modified QACG method, evidence retrieval using ColBERTv2, and claim veracity evaluation using a multilingual NLI classifier. QACG extracts named entities from Wikipedia paragraphs to generate synthetic claims, which are labeled as SUPPORTS, REFUTES, or NEI through controlled perturbations. The ColBERTv2 model retrieves relevant evidence paragraphs for each claim, and the XLM-RoBERTa-based NLI classifier evaluates claim veracity. Temperature scaling is applied to calibrate confidence scores. The pipeline is trained on synthetic data generated from Wikipedia snapshots across four languages, with multilingual models trained on aggregated datasets from all languages.

## Key Results
- Multilingual models trained on aggregated QACG data outperform monolingual models across all tested languages
- The pipeline achieves promising results in automated fact-checking tasks with paragraph-level processing
- Temperature scaling successfully calibrates overconfident NLI predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QACG generates high-quality training data for fact-checking without requiring large-scale human annotations in the target language
- Mechanism: Named entities are extracted from source paragraphs, used as question-answer pairs to generate synthetic claims, which are then labeled as SUPPORTS, REFUTES, or NEI based on controlled perturbations
- Core assumption: Named entities and their semantic relationships can be reliably extracted and used to generate plausible claims that mirror real-world fact-checking tasks
- Evidence anchors:
  - [abstract] "our solution builds on the Question Answering for Claim Generation (QACG) method, which we adapt and use to generate the data for all models of the pipeline."
  - [section] "Our version of QACG differs from [7] in two ways: it works on the paragraph level and simplifies the generation of REFUTES claims, making it language-agnostic."
  - [corpus] Strong - QACG data generation and evaluation is thoroughly described and tested across four languages

### Mechanism 2
- Claim: Multilingual models trained on aggregated QACG data outperform monolingual models and generalize better across languages
- Mechanism: A single multilingual model is trained on the concatenated QACG datasets from all four languages, leveraging shared linguistic patterns and reducing per-language data requirements
- Core assumption: Linguistic similarities between Czech, English, Polish, and Slovak allow a single model to learn cross-lingual representations that improve performance on individual languages
- Evidence anchors:
  - [abstract] "The multilingual models trained on the aggregated QACG data perform the best..."
  - [section] "This result supports the previous research, showing that multilingual models typically achieve better results, given enough capacity and enough data [31]."
  - [corpus] Strong - NLI model comparison explicitly shows QACG-sum outperforming monolingual models on all languages

### Mechanism 3
- Claim: Paragraph-level processing simplifies the pipeline and improves evidence retrieval quality compared to sentence-level approaches
- Mechanism: Each evidence document corresponds to a single Wikipedia paragraph, providing more context for semantic understanding and eliminating the need for a separate sentence selection step
- Core assumption: Modern language models can effectively process longer contexts, and the additional paragraph-level context improves both retrieval and NLI performance
- Evidence anchors:
  - [abstract] "Unlike most similar pipelines, which work with evidence sentences, our pipeline processes data on a paragraph level, simplifying the overall architecture and data requirements."
  - [section] "The motivation of the paragraph-level approach lies mainly in the extended context of the paragraph when compared to an individual sentence."
  - [corpus] Strong - The paper explicitly compares this approach to FEVER's sentence-level method and argues for its advantages

## Foundational Learning

- Concept: Question Answering for Claim Generation (QACG)
  - Why needed here: QACG is the core method for generating synthetic training data without human annotation, enabling deployment in low-resource languages
  - Quick check question: How does QACG generate REFUTES claims differently from SUPPORTS claims?

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI is the task framework used to evaluate claim veracity given retrieved evidence, classifying claims as SUPPORTS, REFUTES, or NEI
  - Quick check question: What are the three possible classification outcomes in the NLI task as used in this pipeline?

- Concept: ColBERTv2 document retrieval
  - Why needed here: ColBERTv2 provides the evidence retrieval module with state-of-the-art performance while being computationally efficient for large corpora
  - Quick check question: What is the key architectural difference between ColBERTv2 and traditional two-tower retrieval models?

## Architecture Onboarding

- Component map: QACG data generation → Evidence Retrieval (ColBERTv2) → Veracity Evaluation (NLI with temperature scaling) → FactSearch UI
- Critical path: Claim → Evidence Retrieval → NLI Classification → User Output
- Design tradeoffs: Paragraph-level vs. sentence-level processing (context vs. noise), multilingual aggregation vs. monolingual specialization (data efficiency vs. potential quality loss), synthetic data vs. human annotation (scalability vs. authenticity)
- Failure signatures: Poor evidence retrieval → irrelevant documents, Noisy NLI predictions → overconfident wrong classifications, QACG data issues → malformed or mislabeled claims
- First 3 experiments:
  1. Generate QACG data for a small Wikipedia sample in one language and train basic NLI model to verify data quality
  2. Test ColBERTv2 retrieval on the generated QACG claims vs. BM-25 baseline to confirm performance gains
  3. Evaluate multilingual vs. monolingual NLI models on the same language to observe transfer benefits

## Open Questions the Paper Calls Out

- Question: How can the high misclassification rate of REFUTES samples in the QACG-generated data be reduced?
- Basis in paper: [explicit] The paper mentions that the high misclassification rate of REFUTES samples is a limiting property of the QACG-generated data.
- Why unresolved: The paper proposes analyzing and improving the selection of semantically aligned alternatives but does not provide a concrete solution.
- What evidence would resolve it: A study comparing the performance of different methods for selecting semantically aligned alternatives and their impact on the misclassification rate of REFUTES samples.

- Question: How does the performance of the QACG-based fact-checking pipeline compare to human fact-checkers in real-world scenarios?
- Basis in paper: [inferred] The paper focuses on the development and evaluation of the QACG-based pipeline, but does not provide a direct comparison with human fact-checkers.
- Why unresolved: Evaluating the pipeline's performance against human fact-checkers requires access to human fact-checkers and a suitable real-world dataset, which is not provided in the paper.
- What evidence would resolve it: A study comparing the performance of the QACG-based pipeline with human fact-checkers on a real-world fact-checking task.

- Question: How can the pipeline be extended to handle multi-hop claims that require evidence from multiple documents?
- Basis in paper: [explicit] The paper mentions that the current QACG method does not involve multi-hop claims and proposes extending it by adding another retrieval module to generate chains of related named entities.
- Why unresolved: The paper does not provide a concrete implementation or evaluation of this extension.
- What evidence would resolve it: A study implementing and evaluating the proposed extension to handle multi-hop claims, including a comparison with the current pipeline's performance on single-hop claims.

## Limitations

- Language Generalization Gap: The pipeline's success relies heavily on shared Indo-European features among the test languages, with unclear performance on languages with significantly different structures
- Synthetic Data Quality: The QACG method may not fully capture the complexity and nuance of real-world claims, with generated REFUTES claims sometimes failing
- Wikipedia Dependency: The pipeline is trained and evaluated on Wikipedia-derived datasets, raising questions about performance on claims requiring domain-specific knowledge outside Wikipedia's scope

## Confidence

- High Confidence: The multilingual NLI model outperforms monolingual variants across all tested languages; ColBERTv2 achieves superior MRR scores; temperature scaling successfully calibrates overconfidence
- Medium Confidence: QACG data generation effectiveness across diverse linguistic structures; paragraph-level processing consistently outperforming sentence-level approaches; pipeline being "easily deployable" in any language
- Low Confidence: Performance guarantees for languages outside the Indo-European family; scalability to very low-resource languages; effectiveness against coordinated disinformation campaigns

## Next Checks

1. Cross-Linguistic Transfer Test: Evaluate the multilingual NLI model on a truly diverse language set (e.g., Turkish, Japanese, and Swahili) to quantify generalization limits beyond Indo-European languages

2. Real-World Claim Validation: Test the pipeline on a curated set of real-world claims from fact-checking organizations, comparing performance against human fact-checkers to identify gaps between synthetic and authentic claim distributions

3. Robustness Against Adversarial Claims: Create a benchmark of deliberately misleading claims designed to exploit potential weaknesses in the QACG-generated training data to assess the system's resilience to sophisticated disinformation