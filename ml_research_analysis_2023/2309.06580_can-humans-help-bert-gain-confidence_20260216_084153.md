---
ver: rpa2
title: Can humans help BERT gain "confidence"?
arxiv_id: '2309.06580'
source_url: https://arxiv.org/abs/2309.06580
tags:
- attention
- features
- word
- cognitive
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This research investigates augmenting BERT with cognitive features\
  \ from EEG and eye-tracking data recorded during natural reading tasks. The goal\
  \ is to improve BERT\u2019s text classification performance by integrating human\
  \ brain activity patterns and attention data."
---

# Can humans help BERT gain "confidence"?

## Quick Facts
- arXiv ID: 2309.06580
- Source URL: https://arxiv.org/abs/2309.06580
- Reference count: 16
- Adding EEG and eye-tracking tokens to BERT's embedding layer improved accuracy by 1.5% and 0.7%, respectively

## Executive Summary
This research investigates augmenting BERT with cognitive features from EEG and eye-tracking data recorded during natural reading tasks. The goal is to improve BERT's text classification performance by integrating human brain activity patterns and attention data. Multiple methods were explored, including adding cognitive tokens to the embedding layer, modifying the self-attention mask, and incorporating sentence-level EEG features into the classification head. Results showed that adding EEG and eye-tracking tokens to the embedding layer improved accuracy by 1.5% and 0.7%, respectively, with the best model reaching 70% accuracy. The attention analysis revealed that cognitive models assigned higher weights to keywords defining relation types, correlating well with LIME explanations.

## Method Summary
The study augments BERT's architecture at different levels with cognitive features from the Zurich Cognitive Corpus (ZuCo) dataset. The approach involves extracting word-level EEG tokens (scaled mean of 105-channel power values across 8 frequency bands), eye-tracking tokens (product of fixation count and gaze duration), and sentence-level EEG vectors. These features are integrated into BERT through embedding layer augmentation, self-attention mask modification, and neural network addition to the classification head. The models are trained and evaluated on a relation classification task, comparing performance against baseline BERT using precision, recall, F1-score, and accuracy metrics.

## Key Results
- Adding EEG tokens to the embedding layer improved accuracy by 1.5%
- Adding eye-tracking tokens to the embedding layer improved accuracy by 0.7%
- The best model with sentence-level EEG integration reached 70% accuracy
- A word-EEG lexicon enabled benchmarking on external datasets without cognitive features, achieving 73.8% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EEG tokens improve BERT classification by encoding brain activity patterns for fixated words
- Mechanism: EEG token values are derived from fixation-related potentials across multiple frequency bands and aggregated into a single scalar per word. This scalar is passed through an embedding layer and added to the standard word and position embeddings, allowing BERT to condition on human reading-related brain activity.
- Core assumption: Higher EEG token values for a word correlate with increased attention/importance for classification.
- Evidence anchors:
  - [abstract]: "adding EEG and eye-tracking tokens to the embedding layer improved accuracy by 1.5% and 0.7%, respectively"
  - [section]: "EEG tokens are calculated as a scaled mean of 105-channel power values across 8 frequency bands for fixated words"
  - [corpus]: Weak—no direct corpus citation, inferred from experimental setup.
- Break condition: If fixation-related potentials do not correlate with classification-relevant information, EEG tokens add noise rather than signal.

### Mechanism 2
- Claim: Eye tokens help BERT learn human relevance patterns for classification
- Mechanism: Eye tokens are computed as a product of fixation count and gaze duration measures, producing a relevance score for each word. This score is embedded and summed with word and position embeddings, biasing the model toward human-attended content.
- Core assumption: Words with more fixations and longer gaze duration are more informative for relation classification.
- Evidence anchors:
  - [abstract]: "adding EEG and eye-tracking tokens to the embedding layer improved accuracy by 1.5% and 0.7%, respectively"
  - [section]: "Eye tokens combine number of fixations, first fixation duration, total reading time, gaze duration, and go-past time"
  - [corpus]: Weak—experimental results only.
- Break condition: If eye-tracking data does not capture semantic relevance (e.g., if fixations are driven by low-level visual factors), the model's attention will be misled.

### Mechanism 3
- Claim: Sentence-level EEG features integrated into pooled output improve classification via richer feature representation
- Mechanism: Sentence-level EEG vectors (105-dimensional) are passed through a small neural network and added to BERT's [CLS] pooled output before classification. This provides complementary neural activity signals to the learned contextual embedding.
- Core assumption: Sentence-level EEG captures global comprehension signals that complement BERT's local context.
- Evidence anchors:
  - [abstract]: "the best model reaching 70% accuracy" with neural network addition
  - [section]: "EEG vector representation trainable through the neural network so it can be updated during training"
  - [corpus]: Weak—relies on observed performance increase.
- Break condition: If sentence-level EEG is too coarse or noisy, it may degrade rather than improve pooled representations.

## Foundational Learning

- Concept: EEG frequency bands and their cognitive correlates
  - Why needed here: Understanding how theta, alpha, beta, gamma bands relate to attention and language comprehension explains why EEG tokens might encode useful information.
  - Quick check question: Which EEG band is most associated with memory and language comprehension in the literature cited?
- Concept: BERT self-attention mechanism
  - Why needed here: The experiments modify how attention is computed or constrained; understanding scaled dot-product attention is essential to interpret mask changes.
  - Quick check question: In BERT's self-attention, what is the shape of the attention score tensor before softmax?
- Concept: Embedding layer summation in BERT
  - Why needed here: Cognitive tokens are added to the standard embeddings; knowing that word, position, and segment embeddings are summed is key to understanding the augmentation.
  - Quick check question: What are the three components summed in BERT's embedding function before feeding to self-attention?

## Architecture Onboarding

- Component map: Data preprocessing -> Cognitive feature extraction -> Embedding augmentation -> Self-attention modification -> Classification head -> Training and evaluation
- Critical path:
  1. Load ZuCo dataset and extract cognitive features
  2. Build augmented BERT model with chosen modifications
  3. Train and evaluate on relation classification task
- Design tradeoffs:
  - EEG tokens vs Eye tokens: EEG provides neural signal but may be noisier; Eye tokens are simpler but may miss deeper comprehension cues
  - Attention mask vs embedding augmentation: Mask changes model's attention distribution but can hurt performance; embedding augmentation preserves attention flexibility
  - Sentence-level EEG addition vs word-level: Sentence-level simpler and faster but less granular; word-level richer but more complex
- Failure signatures:
  - Training loss plateaus early: likely feature noise or insufficient model capacity
  - Evaluation accuracy drops below baseline: cognitive features misaligned with task or added too much variance
  - Attention weights collapse to zero for many words: attention mask too aggressive
- First 3 experiments:
  1. Add EEG tokens to embedding layer and compare to baseline
  2. Replace attention mask with cognitive mask derived from fixation counts
  3. Integrate sentence-level EEG via neural network addition to pooled output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of cognitive features (EEG tokens, eye tokens, attention masks) affect BERT's ability to learn semantic versus keyword-based representations in text classification tasks?
- Basis in paper: [explicit] The paper discusses experiments with EEG tokens, eye tokens, and cognitive attention masks, showing different effects on model performance and attention patterns.
- Why unresolved: The paper provides initial insights but doesn't fully explore the nuanced impact of each cognitive feature on the type of representation learned by BERT (semantic vs. keyword-based).
- What evidence would resolve it: Systematic ablation studies varying combinations of cognitive features and analyzing their impact on attention weights, classification accuracy, and explainability metrics like LIME scores for different types of NLP tasks.

### Open Question 2
- Question: Can synthetic cognitive data (EEG, eye-tracking) generated from computational models effectively replace real cognitive data for augmenting NLP models like BERT?
- Basis in paper: [inferred] The paper mentions the potential for generating synthetic cognitive data but doesn't explore this avenue experimentally.
- Why unresolved: The paper doesn't test the effectiveness of synthetic cognitive data in improving NLP model performance or understanding.
- What evidence would resolve it: Comparative studies using real vs. synthetic cognitive data to augment NLP models, measuring performance improvements and analyzing the quality of the synthetic data in capturing relevant cognitive patterns.

### Open Question 3
- Question: How does the integration of cognitive features at different levels of BERT's architecture (embedding, self-attention, downstream task) impact the model's ability to generalize to unseen data or adapt to new tasks?
- Basis in paper: [explicit] The paper experiments with cognitive feature integration at embedding and downstream levels but doesn't thoroughly investigate generalization or adaptation capabilities.
- Why unresolved: The experiments focus on performance improvement on a specific dataset but don't explore the broader implications for model generalization and task adaptation.
- What evidence would resolve it: Cross-dataset evaluation and transfer learning experiments to assess how cognitive feature integration affects BERT's ability to generalize and adapt to new tasks and datasets.

### Open Question 4
- Question: Can the accumulated incoming attention weights method proposed in the paper serve as a reliable and interpretable alternative to model-agnostic explainability techniques like LIME for understanding BERT's predictions?
- Basis in paper: [explicit] The paper suggests that accumulated attention weights correlate well with LIME scores but doesn't establish it as a definitive alternative.
- Why unresolved: The paper provides initial evidence of correlation but doesn't conduct a comprehensive comparison or establish the reliability and interpretability of the attention weights method.
- What evidence would resolve it: Systematic comparison of accumulated attention weights and LIME across various NLP tasks, evaluating their agreement, reliability, and interpretability in explaining model predictions.

## Limitations
- The 1.5% and 0.7% improvements, while statistically detectable, represent relatively modest absolute gains from baseline performance
- The word-EEG lexicon approach for external datasets is validated indirectly and untested on truly independent data
- The ZuCo dataset's limited size (24 subjects) may constrain generalizability of the findings

## Confidence

**High confidence**: The core experimental methodology (integrating cognitive features at embedding level, attention mask, and classification head) is well-specified and reproducible.

**Medium confidence**: The magnitude of performance improvements (1.5% for EEG, 0.7% for eye-tracking) appears robust based on the reported ablation study, though the absolute baseline accuracy is not explicitly stated.

**Low confidence**: The explainability claims regarding attention weights correlating with LIME explanations lack quantitative validation metrics (correlation coefficients, statistical tests).

## Next Checks

1. **Statistical validation**: Compute confidence intervals and statistical significance tests (paired t-tests or Wilcoxon signed-rank) across multiple random seeds for the reported accuracy improvements.

2. **Independent dataset testing**: Apply the word-EEG lexicon approach to an external relation classification dataset not used in training to verify the 73.8% accuracy claim.

3. **Feature importance ablation**: Systematically remove individual frequency bands or eye-tracking measures to quantify their marginal contributions and test the core assumption that higher values indicate classification relevance.