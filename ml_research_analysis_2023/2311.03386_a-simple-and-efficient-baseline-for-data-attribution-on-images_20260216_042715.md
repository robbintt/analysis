---
ver: rpa2
title: A Simple and Efficient Baseline for Data Attribution on Images
arxiv_id: '2311.03386'
source_url: https://arxiv.org/abs/2311.03386
tags:
- data
- training
- samples
- attribution
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple, efficient baseline for data attribution
  on images using features from a self-supervised model. The key idea is to use a
  MoCo pretrained model to extract features from a target image and compute attribution
  scores based on the distance in feature space to training images of the same class.
---

# A Simple and Efficient Baseline for Data Attribution on Images

## Quick Facts
- arXiv ID: 2311.03386
- Source URL: https://arxiv.org/abs/2311.03386
- Authors: 
- Reference count: 40
- Primary result: Proposes a simple, efficient baseline using MoCo features and ESVM distance that rivals state-of-the-art data attribution methods

## Executive Summary
This paper introduces a simple and efficient baseline for data attribution on images that uses features from a MoCo pre-trained model to identify training samples that most influence a model's prediction. The method computes attribution scores based on distance in feature space to training images of the same class, requiring no information about the model training process. On CIFAR-10 and ImageNet, this approach achieves strong performance that rivals or outperforms state-of-the-art approaches like DataModels and TRAK, while using only a single model instead of thousands. The results reinforce the intuition that a model's prediction is most impacted by visually similar training samples.

## Method Summary
The method extracts features from a MoCo pre-trained ResNet-18 model for both the target image and same-class training images. It then computes attribution scores using ESVM distance between the target features and each training sample's features. The approach uses bisection search to find minimal subsets of training images that, when removed or mislabeled, cause misclassification of the target. This model-agnostic approach requires no information about the model training process and focuses exclusively on same-class training samples to reduce noise from unrelated classes.

## Key Results
- Achieves strong performance on CIFAR-10 and ImageNet that rivals state-of-the-art approaches
- Uses only a single MoCo model versus thousands of models required by competing methods
- ESVM distance outperforms ℓ2 distance for measuring similarity in feature space
- Same-class filtering significantly improves attribution accuracy compared to all-class approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised features (MoCo) are more effective than supervised features for identifying influential training samples
- Mechanism: MoCo learns representations that capture visual similarity without relying on class labels, creating feature spaces where similar images cluster more naturally than in supervised feature spaces
- Core assumption: Visual similarity is a strong proxy for influence on model predictions
- Evidence anchors:
  - [abstract]: "Our method does not use any information about the model training process, and yet still rivals the performance of state-of-the-art approaches"
  - [section 3.1]: "We find that the learning paradigm used to train a feature extractor heavily influences the estimation of data support... MoCo features outperform other self-supervised approaches"
  - [corpus]: Missing - no related papers directly address this comparison
- Break condition: If visual similarity is not actually correlated with influence, or if class-specific features are needed for attribution

### Mechanism 2
- Claim: Exemplar SVM (ESVM) distance is more effective than ℓ2 distance for comparing embeddings
- Mechanism: ESVM creates a decision boundary that separates the target embedding from all other embeddings of the same class, capturing unique dimensions of the target relative to its class
- Core assumption: The hyperplane defined by ESVM better captures the "distinctiveness" of a training sample's influence than simple Euclidean distance
- Evidence anchors:
  - [section 3.1]: "we find that measuring distance as distance to the hyperplane of an Exemplar SVM (ESVM) improves image similarity... yields better removal support estimates than ℓ2 distance"
  - [figure 2]: Visual comparison showing ESVM outperforming ℓ2 distance
  - [corpus]: Missing - no related papers mention ESVM for data attribution
- Break condition: If the class similarity assumption breaks down, or if ESVM overfits to noise in the embedding space

### Mechanism 3
- Claim: Focusing on same-class training samples is critical for effective data attribution
- Mechanism: By restricting comparison to training samples of the same class as the target, the method eliminates noise from unrelated classes and focuses on samples that are actually relevant to the prediction
- Core assumption: Influential training samples for a prediction are predominantly from the same class as the target
- Evidence anchors:
  - [section 3.1]: "choosing a support set from training images of class the same class y as the target zt = (x, y) is critical"
  - [figure 11]: Comparison showing same-class selection outperforms all-class selection
  - [corpus]: Missing - no related papers discuss this same-class restriction
- Break condition: If influential samples come from other classes (e.g., in cases of dataset bias or label noise)

## Foundational Learning

- Concept: Self-supervised learning and MoCo
  - Why needed here: Understanding how MoCo creates feature representations that capture visual similarity without labels is crucial to why this method works
  - Quick check question: What is the key difference between how MoCo and supervised learning create feature representations?

- Concept: Feature space similarity metrics
  - Why needed here: The choice between ℓ2, cosine, and ESVM distances directly impacts the method's effectiveness
  - Quick check question: How does ESVM distance differ fundamentally from Euclidean distance in measuring similarity?

- Concept: Counterfactual reasoning in machine learning
  - Why needed here: The method's evaluation relies on understanding how removing/mislabeling training samples affects predictions
  - Quick check question: What does "data brittleness" measure in the context of model predictions?

## Architecture Onboarding

- Component map:
  - Feature extractor (MoCo ResNet-18)
  - Feature extraction pipeline
  - ESVM distance computation module
  - Same-class filtering mechanism
  - Attribution scoring system
  - Evaluation framework (bisection search for data support)

- Critical path:
  1. Extract features from target image and same-class training images
  2. Compute ESVM distance between target and each training sample
  3. Rank training samples by distance (influence score)
  4. Use bisection search to find minimal data support

- Design tradeoffs:
  - Single model vs. ensemble approaches: Much faster and more scalable, but potentially less nuanced
  - Same-class restriction: Reduces noise but may miss cross-class influences
  - ESVM vs. other distances: More effective but requires training SVMs for each target

- Failure signatures:
  - Poor performance on data mislabel support vs. removal support
  - Degradation when applied to different architectures (as shown in Fig. 6)
  - Inability to identify influential samples from other classes

- First 3 experiments:
  1. Compare ESVM vs. ℓ2 distance on a small subset of CIFAR-10
  2. Test same-class vs. all-class filtering on a validation set
  3. Evaluate transfer to a different architecture (e.g., MobileNetV2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of distance function in the feature space impact the accuracy of data attribution?
- Basis in paper: [explicit] The paper compares the effectiveness of different distance functions (Euclidean distance, cosine distance, and Exemplar SVM) in measuring similarity of embeddings.
- Why unresolved: While the paper shows that using distance to the hyperplane of an Exemplar SVM yields better removal support estimates than ℓ2 distance, it does not provide a comprehensive analysis of other possible distance metrics or their impact on data attribution accuracy.
- What evidence would resolve it: A systematic evaluation of various distance metrics (e.g., Mahalanobis distance, Earth Mover's Distance) on their ability to accurately attribute data would help determine the optimal distance function for data attribution tasks.

### Open Question 2
- Question: How does the performance of the proposed baseline approach generalize to larger and more diverse datasets beyond CIFAR-10 and ImageNet?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the baseline approach on CIFAR-10 and ImageNet, but it does not explore its performance on larger or more diverse datasets.
- Why unresolved: The scalability and generalization capabilities of the approach to larger and more diverse datasets remain unknown, which is crucial for real-world applications.
- What evidence would resolve it: Conducting experiments on larger and more diverse datasets (e.g., COCO, OpenImages) and comparing the performance of the baseline approach with other state-of-the-art methods would provide insights into its generalization capabilities.

### Open Question 3
- Question: Can the proposed baseline approach be extended to other modalities beyond images, such as text or audio?
- Basis in paper: [explicit] The paper focuses on data attribution for images and does not explore its applicability to other modalities.
- Why unresolved: The effectiveness of the approach in other modalities, where feature extraction and similarity measurement may differ, is unknown.
- What evidence would resolve it: Adapting the approach to other modalities (e.g., text, audio) and evaluating its performance on data attribution tasks specific to those modalities would demonstrate its versatility and potential for broader applications.

## Limitations
- Performance degrades when applied to architectures different from the MoCo ResNet-18 used for feature extraction
- Evaluation relies on synthetic scenarios rather than real-world attribution needs
- Computational efficiency, while better than methods requiring thousands of models, still requires training multiple models during bisection search

## Confidence
- **High Confidence**: The core claim that MoCo features with ESVM distance provide effective data attribution (supported by strong quantitative results on CIFAR-10 and ImageNet)
- **Medium Confidence**: The assertion that this approach rivals or outperforms state-of-the-art methods (valid within tested scenarios but may not generalize to all attribution contexts)
- **Low Confidence**: The scalability and practical utility claims, as real-world applicability depends on factors not tested in the paper

## Next Checks
1. Test transferability by applying the method to attribution tasks on architectures not used during feature extraction (e.g., ViT or ConvNeXt models)
2. Evaluate performance on real-world attribution scenarios where training data provenance is known, rather than synthetic removal/mislabeling tests
3. Benchmark computational efficiency against a broader range of attribution methods, including those using different feature extractors or distance metrics