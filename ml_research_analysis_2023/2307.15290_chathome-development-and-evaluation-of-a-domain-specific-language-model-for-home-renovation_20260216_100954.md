---
ver: rpa2
title: 'ChatHome: Development and Evaluation of a Domain-Specific Language Model for
  Home Renovation'
arxiv_id: '2307.15290'
source_url: https://arxiv.org/abs/2307.15290
tags:
- data
- domain
- home
- language
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChatHome, a domain-specific language model
  for home renovation. The model combines domain-adaptive pretraining and instruction-tuning
  on a comprehensive dataset of professional articles, standards, and web content.
---

# ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation

## Quick Facts
- arXiv ID: 2307.15290
- Source URL: https://arxiv.org/abs/2307.15290
- Reference count: 34
- Key outcome: ChatHome achieves 69.03 on EvalHome benchmark, significantly outperforming baseline models

## Executive Summary
This paper introduces ChatHome, a domain-specific language model for home renovation developed through a combination of domain-adaptive pretraining (DAPT) and instruction tuning. The model is trained on a comprehensive dataset including professional articles, national standards, and web content. Experimental results demonstrate that ChatHome significantly improves domain-specific performance while maintaining general language capabilities, achieving a score of 69.03 on the newly introduced EvalHome benchmark compared to baseline models scoring around 30-50.

## Method Summary
The ChatHome model is developed using a two-stage approach: first, domain-adaptive pretraining (DAPT) on a combined corpus of 26.6M domain-specific tokens and 276.6M general tokens, with various data ratio experiments (1:0 to 1:10); second, instruction tuning using approximately 25k question-answer pairs. A novel Multi-Task Instruction Pre-Training (MIP) strategy is introduced, incorporating instruction data during the DAPT phase. The model is evaluated using the EvalHome benchmark (113 questions across 3 difficulty levels) as well as general capability benchmarks C-Eval and CMMLU.

## Key Results
- ChatHome achieves 69.03 score on EvalHome benchmark, significantly outperforming baseline models
- Incorporating instruction data during DAPT (MIP approach) yields best results
- A 1:5 ratio of domain to general data during pretraining provides optimal balance
- Model maintains general capabilities while improving domain-specific performance

## Why This Works (Mechanism)

### Mechanism 1
Domain-adaptive pretraining (DAPT) followed by instruction tuning improves domain-specific performance while maintaining general capabilities. DAPT on domain corpus updates model parameters with domain-specific knowledge, creating a foundation that instruction tuning can build upon to learn task-specific patterns. Core assumption: domain corpus contains sufficient domain-specific knowledge and base model has enough capacity to incorporate this knowledge without catastrophic forgetting. Evidence: DAPT model achieves highest scores on both EvalHome and general benchmarks.

### Mechanism 2
Incorporating downstream instruction data during DAPT (MIP) yields better results than separate DAPT and instruction tuning stages. Instruction data during pretraining allows model to learn both domain knowledge and instruction-following patterns simultaneously, creating better representations for both. Core assumption: instruction data contains sufficient domain-relevant patterns that benefit from early integration during pretraining. Evidence: MIP strategy achieves unexpected score of 69.03 on EvalHome, outperforming all other models.

### Mechanism 3
Balancing domain and general data ratios during training preserves general capabilities while enhancing domain performance. Maintaining a 1:5 ratio of domain to general data provides sufficient domain exposure while preventing catastrophic forgetting of general knowledge. Core assumption: base model's general knowledge can be preserved with sufficient general data exposure during domain adaptation. Evidence: 1:5 ratio yields best performance on EvalHome and shows least general capability loss.

## Foundational Learning

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: Domain adaptation can cause models to lose previously learned general knowledge, which must be prevented
  - Quick check question: What training strategies can prevent the model from forgetting general knowledge during domain adaptation?

- Concept: Instruction tuning for task generalization
  - Why needed here: Model needs to learn how to follow instructions in home renovation domain while maintaining general instruction-following capabilities
  - Quick check question: How does instruction tuning differ from standard fine-tuning in terms of learning objectives and data requirements?

- Concept: Data balancing in multi-domain training
  - Why needed here: Model must balance learning domain-specific knowledge with preserving general capabilities through appropriate data ratios
  - Quick check question: What metrics can be used to evaluate whether general capabilities are being maintained during domain adaptation?

## Architecture Onboarding

- Component map: Base model (Baichuan-13B) → DAPT stage (domain corpus + instruction data) → Instruction tuning stage (question-answer pairs) → Evaluation (EvalHome + general benchmarks)
- Critical path: Data collection → DAPT with balanced ratios → MIP implementation → Instruction tuning → Evaluation and iteration
- Design tradeoffs: More domain data improves domain performance but risks general capability loss; more general data preserves general capabilities but may dilute domain specificity
- Failure signatures: General capability scores dropping significantly during domain adaptation; poor performance on EvalHome indicating insufficient domain knowledge; overfitting to training data
- First 3 experiments:
  1. Test different data ratios (1:0, 1:1, 1:2, 1:5, 1:10) during DAPT to find optimal balance
  2. Compare separate DAPT + instruction tuning vs MIP approach with instruction data during pretraining
  3. Evaluate model performance on both EvalHome and general benchmarks to ensure domain improvement doesn't come at cost of general capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data ratio between domain-specific and general data for achieving the best performance in domain adaptation for language models in the home renovation domain?
- Basis in paper: The paper discusses various data ratios tested (1:0, 1:1, 1:2, 1:5, and 1:10) and concludes that a 1:5 ratio yields the best performance on the EvalHome benchmark and general capability evaluation benchmarks.
- Why unresolved: While the paper identifies a 1:5 ratio as optimal for the current base model and dataset, it is unclear if this ratio would remain optimal for different base models, domains, or evaluation benchmarks. The optimal ratio may vary depending on the specific characteristics of the domain and the capabilities of the base model.
- What evidence would resolve it: Conducting experiments with different base models and domains, while systematically varying the data ratio, would provide insights into the generalizability of the 1:5 ratio. Additionally, exploring the impact of data quality and diversity on the optimal ratio would be valuable.

### Open Question 2
- Question: How does the incorporation of downstream instruction data during the domain-adaptive pretraining (DAPT) phase impact the performance of language models in the home renovation domain compared to traditional DAPT followed by instruction tuning?
- Basis in paper: The paper introduces the MIP (Multi-Task Instruction Pre Training) strategy, which incorporates downstream instruction data during the DAPT phase. This approach achieves a score of 69.03 on EvalHome, outperforming all other models tested.
- Why unresolved: While the paper demonstrates the effectiveness of the MIP strategy, it does not provide a detailed analysis of the impact of incorporating instruction data during DAPT compared to the traditional approach of DAPT followed by instruction tuning. The reasons behind the superior performance of MIP and the potential trade-offs are not fully explored.
- What evidence would resolve it: Conducting a detailed ablation study comparing the performance of models trained using traditional DAPT followed by instruction tuning with those trained using the MIP strategy would provide insights into the benefits and limitations of incorporating instruction data during DAPT. Additionally, analyzing the learned representations and attention patterns of the models could shed light on the mechanisms behind the improved performance.

### Open Question 3
- Question: What are the potential sources of hallucination in the ChatHome model, and what strategies can be employed to mitigate these issues?
- Basis in paper: The paper acknowledges that the ChatHome model still exhibits hallucination defects and mentions the need for further experimental explorations, including the implementation of knowledge base and reinforcement learning techniques, to enhance performance.
- Why unresolved: While the paper identifies hallucination as a limitation, it does not provide a detailed analysis of the specific sources of hallucination in the model. The potential impact of data quality, model architecture, and training strategies on hallucination is not fully explored.
- What evidence would resolve it: Conducting a systematic analysis of the types and frequencies of hallucinations produced by the model, along with an investigation of the underlying causes, would provide insights into the sources of hallucination. Additionally, exploring different strategies for mitigating hallucination, such as knowledge grounding, fact-checking, and uncertainty estimation, would help identify effective approaches for improving model reliability.

## Limitations

- The EvalHome benchmark is not publicly available, preventing independent verification of results
- Domain-specific corpus (26.6M tokens) is relatively small compared to general corpus (276.6M tokens)
- Long-term stability and cross-domain generalization capabilities are not thoroughly examined

## Confidence

**High Confidence**: The effectiveness of domain-adaptive pretraining (DAPT) in improving domain-specific performance while maintaining general capabilities. Well-supported by comparative experiments across multiple data ratios and evaluation benchmarks.

**Medium Confidence**: The superiority of the MIP approach (incorporating instruction data during DAPT) over separate pretraining and instruction tuning stages. While experimental results show improvement, the unexpected nature of this finding and limited ablation studies reduce confidence.

**Low Confidence**: The specific 1:5 data ratio as optimal for all similar domain adaptation tasks. This conclusion is based on experiments with a single base model and domain, and may not generalize to other domains or model architectures.

## Next Checks

1. **Independent Benchmark Replication**: Recreate the EvalHome benchmark using publicly available home renovation resources and independently evaluate the model's performance to verify the claimed 69.03 score and comparison with baseline models.

2. **Long-term Performance Stability**: Conduct extended testing over multiple epochs and different random seeds to assess the model's stability and identify any performance degradation patterns that may emerge over time.

3. **Cross-domain Generalization**: Test the model's ability to handle unseen home renovation subdomains (e.g., specialized areas like sustainable renovation or smart home integration) to evaluate its true domain coverage and generalization capabilities.