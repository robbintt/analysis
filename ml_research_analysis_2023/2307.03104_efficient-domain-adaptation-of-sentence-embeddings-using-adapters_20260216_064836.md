---
ver: rpa2
title: Efficient Domain Adaptation of Sentence Embeddings Using Adapters
arxiv_id: '2307.03104'
source_url: https://arxiv.org/abs/2307.03104
tags:
- adapter
- sentence
- adapters
- layer
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using lightweight adapters to efficiently adapt
  sentence embedding models to different domains. Instead of fine-tuning the entire
  model, adapters add a small number of new parameters while keeping the base model
  fixed.
---

# Efficient Domain Adaptation of Sentence Embeddings Using Adapters

## Quick Facts
- **arXiv ID**: 2307.03104
- **Source URL**: https://arxiv.org/abs/2307.03104
- **Reference count**: 20
- **One-line primary result**: Adapter-based domain adaptation achieves near state-of-the-art performance while training only 3.6% of parameters

## Executive Summary
This paper proposes using lightweight adapters to efficiently adapt sentence embedding models to different domains. Instead of fine-tuning the entire model, adapters add a small number of new parameters while keeping the base model fixed. Three adapter architectures are explored: Houlsby-Adapter, Pfeiffer-Adapter, and K-Adapter. Results show that using the Houlsby-Adapter with a contrastive loss function achieves performance within 1% of a fully fine-tuned model while only training about 3.6% of the parameters. This demonstrates adapters as an effective approach for parameter-efficient domain adaptation of sentence embeddings.

## Method Summary
The method involves inserting lightweight adapter modules into a pretrained sentence embedding model (SimCSE-sup-bert-base) and training only these adapters on domain-specific data while keeping the base model frozen. Three adapter architectures are evaluated: Houlsby-Adapter (bottleneck modules after both attention and feed-forward layers), Pfeiffer-Adapter (bottleneck modules after feed-forward layers only), and K-Adapter (external plug-in layers). The adapters transform inputs into lower-dimensional representations and upsample them, allowing domain-specific knowledge injection without altering base model weights. Two loss functions are compared: triplet margin loss (ℓ1) and contrastive objective (ℓ2). The approach is evaluated on two domain-specific datasets: SciDocs (scientific papers) and AskUbuntu (technical forum posts).

## Key Results
- Houlsby-Adapter with contrastive loss achieves performance within 1% of fully fine-tuned models while training only 3.6% of parameters
- The Houlsby adapter, despite being the smallest among investigated adapters, yields the best results for both loss functions
- Contrastive loss function ℓ2 performs consistently better than triplet margin loss ℓ1 across both domains and adapter architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapter-based domain adaptation achieves near state-of-the-art performance while training only a small fraction of parameters.
- Mechanism: Adapters introduce lightweight bottleneck modules that transform inputs into lower-dimensional representations and upsample them, allowing domain-specific knowledge injection without altering base model weights.
- Core assumption: The base sentence embedding model captures general semantic information sufficiently to be adapted via small parameter additions.
- Evidence anchors:
  - [abstract] "using the Houlsby-Adapter with a contrastive loss function achieves performance within 1% of a fully fine-tuned model while only training about 3.6% of the parameters"
  - [section] "The adapter layers transform their input into a very low-dimensional representation and upsample it again to the same dimension in the output. This generates a parameter-efficient lower-dimensional representation while most information is kept."
  - [corpus] Weak evidence - no directly related corpus entries specifically validate this mechanism.
- Break condition: If the base model's general embeddings are insufficient for the target domain, adapters cannot compensate with such a small parameter budget.

### Mechanism 2
- Claim: The contrastive loss function (ℓ2) is more effective than triplet margin loss (ℓ1) for adapter-based domain adaptation.
- Mechanism: Contrastive learning ensures embeddings are distributed across the entire embedding space, while triplet loss may collapse representations into narrow cones, limiting expressiveness.
- Core assumption: Proper embedding distribution is critical for downstream semantic textual similarity tasks.
- Evidence anchors:
  - [abstract] "the contrastive loss function ℓ2 performs consistently better than ℓ1"
  - [section] "Our results align with the observations of Gao et al. (2021) who conclude that the contrastive objective ensures a distribution of embeddings around the entire embedding space. In contrast, ℓ1 may yield learned representations occupying a narrow vector space cone, which severely limits their expressiveness."
  - [corpus] No corpus evidence directly supports this specific comparison.
- Break condition: If the dataset characteristics favor triplet loss (e.g., specific negative sampling patterns), the performance advantage may not hold.

### Mechanism 3
- Claim: The Houlsby-Adapter architecture is most suitable for parameter-efficient domain adaptation of sentence embeddings.
- Mechanism: The Houlsby-Adapter uses bottleneck modules added after both multi-head attention and feed-forward blocks, providing optimal parameter efficiency and performance.
- Core assumption: The architecture's placement and bottleneck design are optimal for sentence embedding tasks.
- Evidence anchors:
  - [abstract] "the Houlsby adapter, although the smallest among the adapters investigated, yields the best results for both loss functions"
  - [section] "We conclude that the bottleneck architecture is more suitable than the external plug-in architecture for domain adaptation of sentence embedding models"
  - [corpus] No corpus evidence specifically validates Houlsby-Adapter superiority in this context.
- Break condition: If the target domain requires very deep adaptation or the base model architecture differs significantly, other adapter designs might outperform.

## Foundational Learning

- Concept: Contrastive learning objectives for sentence embeddings
  - Why needed here: The paper explicitly compares two loss functions and concludes ℓ2 (contrastive) outperforms ℓ1 (triplet), making understanding contrastive learning essential for replicating results.
  - Quick check question: What is the key difference between how contrastive loss and triplet loss handle negative samples in sentence embedding training?

- Concept: Adapter architectures and bottleneck designs
  - Why needed here: The paper investigates three adapter architectures (Houlsby, Pfeiffer, K-Adapter) and concludes bottleneck designs are more suitable, requiring understanding of how these architectures differ.
  - Quick check question: How does the placement of adapter modules (after attention vs. feed-forward layers) affect the parameter efficiency and performance in transformer-based models?

- Concept: Domain adaptation vs. general pretraining
  - Why needed here: The paper's core contribution is adapting general sentence embeddings to specific domains, requiring understanding when and why domain adaptation is necessary versus using out-of-the-box models.
  - Quick check question: Under what conditions would an out-of-the-box sentence embedding model fail to perform adequately on domain-specific semantic textual similarity tasks?

## Architecture Onboarding

- Component map: Base model -> Adapter insertion -> Loss function selection -> Training on domain-specific data -> Evaluation on domain-specific test sets
- Critical path: Base model initialization → Adapter insertion → Loss function selection → Training on domain-specific data → Evaluation on domain-specific test sets
- Design tradeoffs:
  - Parameter efficiency vs. performance: Smaller adapters train fewer parameters but may sacrifice some accuracy
  - Architecture choice: Houlsby adds adapters after both attention and feed-forward layers, Pfeiffer only after feed-forward, K-Adapter uses external plug-in layers
  - Loss function: Contrastive loss provides better embedding distribution but may require more careful negative sampling
- Failure signatures:
  - Performance close to out-of-the-box baseline indicates adapter architecture or training configuration issues
  - Large gap between adapter and full fine-tuning suggests insufficient adapter capacity or poor domain data quality
  - Poor performance with contrastive loss but good with triplet loss suggests embedding collapse issues
- First 3 experiments:
  1. Train Houlsby-Adapter with contrastive loss (ℓ2) on AskUbuntu dataset and compare against out-of-the-box SimCSE baseline
  2. Train Pfeiffer-Adapter with contrastive loss on SciDocs dataset to validate architecture performance across domains
  3. Compare Houlsby-Adapter with both loss functions on the same dataset to confirm contrastive loss superiority

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different adapter architectures (e.g., low-rank, parallel, hierarchical) compare to the three explored (Houlsby, Pfeiffer, K-Adapter) for domain adaptation of sentence embeddings?
- Basis in paper: [explicit] The paper states "three adapter architectures are explored: Houlsby-Adapter, Pfeiffer-Adapter, and K-Adapter" but notes "many different adapter architectures have been proposed for various domains and tasks."
- Why unresolved: The paper only compares three specific adapter architectures, leaving open the question of how other architectures might perform.
- What evidence would resolve it: A systematic comparison of various adapter architectures (e.g., low-rank, parallel, hierarchical) on the same domain adaptation tasks.

### Open Question 2
- Question: Can the proposed adapter-based domain adaptation approach be effectively scaled to larger models (e.g., RoBERTa-large, T5) and more diverse domains?
- Basis in paper: [explicit] The paper states "We use SimCSEsup−bert−base (Gao et al., 2021) as the base sentence embedding model" and evaluates on two domains (SciDocs, AskUbuntu).
- Why unresolved: The experiments only use a single base model (BERT-base) and two domains, leaving open questions about scalability and generalizability.
- What evidence would resolve it: Experiments with larger base models (e.g., RoBERTa-large, T5) and a wider range of diverse domains (e.g., biomedical, legal, social media).

### Open Question 3
- Question: How does the performance of adapter-based domain adaptation compare to other parameter-efficient fine-tuning methods (e.g., prefix tuning, LoRA) for sentence embeddings?
- Basis in paper: [explicit] The paper focuses solely on adapters as a parameter-efficient fine-tuning method.
- Why unresolved: The paper does not compare adapters to other parameter-efficient methods, leaving open the question of their relative effectiveness.
- What evidence would resolve it: A head-to-head comparison of adapters with other parameter-efficient fine-tuning methods (e.g., prefix tuning, LoRA) on the same domain adaptation tasks.

## Limitations
- Narrow scope of evaluated domains (only scientific and technical domains)
- Absence of ablation studies on adapter capacity scaling
- Performance claims rely on a single base model without testing generalization to other architectures

## Confidence
- High confidence: The parameter efficiency claim (3.6% of parameters achieving 99% of full fine-tuning performance) is well-supported by empirical results across two datasets.
- Medium confidence: The superiority of Houlsby-Adapter over other architectures is demonstrated but limited to the specific experimental setup and may not generalize to all transformer-based embedding models.
- Medium confidence: The contrastive loss superiority claim is supported by results but lacks ablation studies on temperature scaling and negative sampling strategies.

## Next Checks
1. **Cross-domain generalization**: Test the adapter approach on diverse domains (legal, biomedical, financial) to verify the claimed parameter efficiency holds across different semantic similarity tasks and vocabulary distributions.

2. **Adapter capacity scaling**: Systematically vary adapter bottleneck dimensions (from 1/4 to 1/32 of hidden size) to determine the minimum parameter budget required for near-full fine-tuning performance, establishing a Pareto frontier.

3. **Failure case analysis**: Identify and characterize specific scenarios where adapter-based domain adaptation fails (e.g., domain shift magnitude, data scarcity) to understand the practical limitations and guide future improvements.