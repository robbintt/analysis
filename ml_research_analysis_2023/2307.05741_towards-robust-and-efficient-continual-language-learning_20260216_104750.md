---
ver: rpa2
title: Towards Robust and Efficient Continual Language Learning
arxiv_id: '2307.05741'
source_url: https://arxiv.org/abs/2307.05741
tags:
- transfer
- tasks
- task
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark for continual language learning
  that tests models' ability to leverage past tasks to learn new tasks more efficiently.
  The benchmark includes sequences of tasks with varying transfer potential (positive,
  negative, neutral).
---

# Towards Robust and Efficient Continual Language Learning

## Quick Facts
- arXiv ID: 2307.05741
- Source URL: https://arxiv.org/abs/2307.05741
- Reference count: 40
- Key outcome: Proposed selective checkpoint selection method achieved 42.4% relative PerfAUC compared to 34.5% for naive sequential fine-tuning

## Executive Summary
This paper introduces a new benchmark for continual language learning that tests models' ability to leverage past tasks to learn new tasks more efficiently. The benchmark includes sequences of tasks with varying transfer potential (positive, negative, neutral). The authors propose a simple method that selects the best model checkpoint to initialize new tasks from based on task similarity features. This approach successfully exploits positive transfer while avoiding negative transfer in many cases. On the benchmark, the method achieved 42.4% relative PerfAUC on average, compared to 34.5% for naive sequential fine-tuning and 43.6% for an oracle.

## Method Summary
The paper proposes a sequential fine-tuning approach where each new task can be initialized from any previous task checkpoint or the original pre-trained model. A gradient boosted decision tree is trained to predict positive transfer potential between task pairs using features like relative performance, weight changes, and gradient similarity. During continual learning, this selector chooses the most promising checkpoint for each new task, defaulting to the pre-trained model when no positive transfer is predicted. The method is evaluated on a benchmark of 55 diverse NLP tasks from the FLAN collection.

## Key Results
- Selective checkpoint selection achieved 42.4% relative PerfAUC compared to 34.5% for naive sequential fine-tuning
- The method successfully exploits positive transfer while avoiding negative transfer in many cases
- Performance varied significantly across task sequences, with some showing strong improvements and others showing limited benefit
- Oracle selection achieved 43.6% relative PerfAUC, providing an upper bound for the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter initialization transfer works when gradients from past tasks align directionally with gradients needed for new tasks.
- Mechanism: The paper shows that initializing from a previously fine-tuned model (task A) can accelerate learning on task B when the weight updates during A's training have similar directional alignment to those needed for B.
- Core assumption: The cosine similarity between parameter updates from different tasks serves as a proxy for beneficial transfer potential.
- Evidence anchors: