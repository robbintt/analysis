---
ver: rpa2
title: 'SCREWS: A Modular Framework for Reasoning with Revisions'
arxiv_id: '2309.13075'
source_url: https://arxiv.org/abs/2309.13075
tags:
- answer
- selection
- resampling
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose SCREWS, a modular framework for reasoning
  with revisions that enables exploration of different reasoning strategies. It consists
  of three main modules: Sampling (to generate initial answers), Conditional Resampling
  (to revise answers if needed), and Selection (to choose the best answer).'
---

# SCREWS: A Modular Framework for Reasoning with Revisions

## Quick Facts
- arXiv ID: 2309.13075
- Source URL: https://arxiv.org/abs/2309.13075
- Reference count: 25
- Improves reasoning accuracy by 10-15% over vanilla strategies

## Executive Summary
SCREWS is a modular framework for reasoning with revisions that explores different reasoning strategies through three main components: Sampling, Conditional Resampling, and Selection. The framework demonstrates significant accuracy improvements (10-15%) on three reasoning tasks—arithmetic, multi-hop QA, and code debugging—by leveraging heterogeneous revision strategies and a selection mechanism that prevents error propagation. The modular design allows researchers to systematically explore the space of possible reasoning strategies by combining different methods.

## Method Summary
SCREWS implements a three-module architecture where Sampling generates initial answers using methods like Chain of Thought (CoT) or subquestion decomposition, Conditional Resampling decides whether to revise answers and generates alternatives when needed, and Selection chooses between original and revised candidates. The framework is evaluated using ChatGPT API (gpt-3.5-turbo-0301) across three reasoning tasks: GSM8K arithmetic problems (1319 test samples), StrategyQA multi-hop QA (490 training subset samples), and Big Bench AutoDebugging code debugging (33 test samples). Experiments show that heterogeneous revision strategies combined with model-based selection yield the best performance, with tool-based conditional resampling providing additional gains when external knowledge is needed.

## Key Results
- SCREWS improves accuracy by 10-15% over vanilla strategies across all three reasoning tasks
- Heterogeneous revision strategies outperform homogeneous ones, with subquestion decomposition sampling + CoT resampling showing 81.34% accuracy on StrategyQA
- Tool-based conditional resampling provides 2-point improvements by correcting model hallucinations through external facts
- Model-based selection prevents error introduction from conditional resampling, validating the importance of the Selection module

## Why This Works (Mechanism)

### Mechanism 1
Heterogeneous revision strategies improve reasoning accuracy more than homogeneous ones. By allowing different sampling methods (e.g., CoT and subquestion decomposition) to be used for initial sampling and conditional resampling, the model can correct errors from one method using the strengths of another. Core assumption: Different reasoning methods have complementary error patterns. Evidence: Experiments show subquestion decomposition sampling + CoT resampling achieves 81.34% accuracy on StrategyQA.

### Mechanism 2
Selection between original and revised answers prevents error introduction from conditional resampling. A selection module evaluates both the original sample and the conditionally resampled revision, choosing the better one, which allows rollback when resampling introduces errors. Core assumption: The selection module can accurately identify which answer is correct. Evidence: Experiments demonstrate that selection prevents degradation from conditional resampling errors.

### Mechanism 3
Tool-based conditional resampling provides external knowledge to correct model hallucinations. When the model lacks factual knowledge, incorporating external facts during resampling helps identify and correct incorrect claims. Core assumption: External tools can provide accurate information that the model lacks. Evidence: Tool-based resampling yields 2-point improvements by providing correct facts during revision.

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: CoT is one of the primary sampling methods in SCREWS that generates step-by-step reasoning.
  - Quick check question: Can you explain how CoT differs from direct answer generation and why it might help with complex reasoning tasks?

- Concept: Conditional resampling
  - Why needed here: This is the core mechanism that allows the model to revise its initial answer when needed.
  - Quick check question: What triggers conditional resampling in the SCREWS framework, and how does it decide whether to revise?

- Concept: Ensemble methods and selection
  - Why needed here: Selection is crucial for choosing between original and revised answers, preventing error propagation.
  - Quick check question: How does model-based selection differ from simple majority voting in the context of SCREWS?

## Architecture Onboarding

- Component map: Input → Sampling → Conditional Resampling → Selection → Final Answer
- Critical path: The conditional resampling step only executes if the model decides revision is needed, creating a conditional execution path.
- Design tradeoffs: Heterogeneous methods provide better accuracy but increase complexity and token cost. Tool-based resampling improves accuracy but adds latency and cost. Simple selection methods are faster but may be less accurate than model-based selection.
- Failure signatures: Poor accuracy despite SCREWS usage may indicate: (1) selection module failing to identify correct answers, (2) conditional resampling not being triggered when needed, or (3) homogeneous methods being used when heterogeneous would be better.
- First 3 experiments:
  1. Compare CoT vs. Answer Only sampling on a simple arithmetic task to establish baseline performance.
  2. Test heterogeneous resampling by combining CoT sampling with subquestion decomposition resampling.
  3. Evaluate the impact of selection by comparing "always select revised" vs. "select better" strategies.

## Open Questions the Paper Calls Out

1. How does the performance of SCREWS scale with increasing model size or capability? (Basis: inferred from discussion of future work)
2. Can the modules in SCREWS be fine-tuned end-to-end for improved performance? (Basis: explicit suggestion in discussion)
3. How does SCREWS perform on tasks requiring deeper, multi-step reasoning beyond the evaluated datasets? (Basis: inferred from current dataset limitations)

## Limitations

- Heterogeneous resampling advantage primarily demonstrated through synthetic oracle conditions rather than real-world knowledge gaps
- Selection module effectiveness depends heavily on quality of few-shot examples, which are abbreviated in the paper
- Tool-based resampling improvements don't quantify practical cost implications of external API calls

## Confidence

**High Confidence**: The modular framework design and the core insight that selection between original and revised answers prevents error propagation are well-supported by controlled experiments across three distinct tasks.

**Medium Confidence**: The claim that heterogeneous revision strategies consistently outperform homogeneous ones is supported by oracle experiments but needs validation with real knowledge gaps and noisy tools.

**Medium Confidence**: The observation that conditional resampling can introduce errors and requires selection is demonstrated, but the paper doesn't explore failure modes where selection itself might fail to identify correct answers.

## Next Checks

1. **Cost-Benefit Analysis**: Measure actual token costs and latency introduced by tool-based resampling across all three tasks, comparing against accuracy gains to establish practical deployment thresholds.

2. **Selection Robustness Test**: Create adversarial examples where the selection module consistently chooses incorrect answers to identify failure modes and calibrate selection thresholds.

3. **Real-World Knowledge Gap Experiment**: Design a task where models demonstrably lack specific factual knowledge, then measure whether tool-based resampling with external APIs provides meaningful accuracy improvements over synthetic oracle conditions.