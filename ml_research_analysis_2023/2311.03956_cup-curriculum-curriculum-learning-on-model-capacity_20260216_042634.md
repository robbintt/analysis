---
ver: rpa2
title: 'Cup Curriculum: Curriculum Learning on Model Capacity'
arxiv_id: '2311.03956'
source_url: https://arxiv.org/abs/2311.03956
tags:
- curriculum
- training
- capacity
- best
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a curriculum learning approach called the
  "cup curriculum" that manipulates model capacity during training to improve performance.
  The method consists of two phases: first, reducing model capacity using iterative
  magnitude pruning (IMP), and second, reintroducing capacity in a structured manner.'
---

# Cup Curriculum: Curriculum Learning on Model Capacity

## Quick Facts
- arXiv ID: 2311.03956
- Source URL: https://arxiv.org/abs/2311.03956
- Reference count: 40
- Key outcome: 0.5% to 2% performance improvement over early stopping with best strategy using Best rewinding, Random initialization, and Identical updating

## Executive Summary
The cup curriculum is a novel curriculum learning approach that manipulates model capacity during training to improve transformer model performance. It consists of two phases: first reducing capacity through iterative magnitude pruning (IMP), then gradually reintroducing capacity in a structured manner. Experiments on WikiText2 with transformer models of varying sizes show that the best cup curriculum strategy achieves reliable performance improvements of 0.5% to 2% over early stopping baselines.

## Method Summary
The cup curriculum approach involves two training phases. First, iterative magnitude pruning reduces model capacity over 20 cycles of 50 epochs each, pruning 20% of remaining capacity per cycle using magnitude change as the criterion. Second, capacity is reintroduced through 20 introduction steps of 50 epochs each, with weights rewound to their best-performing state of each cycle, randomly reinitialized, and updated identically using standard backpropagation. The method is tested on WikiText2 with Small, Medium, and Large transformer models.

## Key Results
- Best strategy achieves 0.5% to 2% performance improvement over early stopping with 99% confidence
- Performance improvements scale with model size, from 0.5% (Small) to 2% (Large)
- Best rewinding, Random initialization, and Identical updating strategy demonstrates resilience to overfitting
- Training loss spikes every 50 epochs due to weight rewinding and pruning operations

## Why This Works (Mechanism)

### Mechanism 1
Iterative pruning followed by capacity reintroduction creates a "cup-shaped" capacity curve that improves model generalization. During pruning, weights are removed based on magnitude change, forcing reliance on essential features. During growth, pruned weights are reintroduced in reverse order, allowing gradual capacity restoration while maintaining learned essential features.

### Mechanism 2
Best rewinding strategy provides better initialization for reintroduced weights than initial or warm rewinding. By rewinding to the best performing model state of each pruning cycle, reintroduced weights start from a state that has already proven effective for the current capacity level, rather than less optimal initial or warm-up states.

### Mechanism 3
Random initialization of reintroduced weights combined with identical updating provides the best performance improvement. Random initialization prevents bias toward specific weight values, while identical updating (standard backpropagation) allows all weights to be updated uniformly, enabling the model to adapt to reintroduced capacity.

## Foundational Learning

- **Iterative Magnitude Pruning (IMP)**
  - Why needed here: Core algorithm for systematic capacity reduction based on weight importance
  - Quick check question: How does IMP determine which weights to prune in each cycle?

- **Lottery Ticket Hypothesis**
  - Why needed here: Suggests dense networks contain sparse subnetworks that can be trained in isolation to achieve comparable performance
  - Quick check question: What is the relationship between IMP and the lottery ticket hypothesis?

- **Curriculum Learning**
  - Why needed here: The cup curriculum applies curriculum learning principles to model capacity rather than dataset or task
  - Quick check question: How does curriculum learning on model capacity differ from curriculum learning on dataset or task?

## Architecture Onboarding

- **Component map**: Pruning Phase -> Growth Phase -> Rewinding Strategies (Initial, Warm, Best, No) -> Initialization Schemes (Random, Original, Old, Top) -> Update Schemes (Freezing, Identical, Dynamic)

- **Critical path**:
  1. Initialize model and data
  2. Run pruning phase with chosen rewinding strategy
  3. Run growth phase with chosen initialization and update schemes
  4. Evaluate final model performance

- **Design tradeoffs**:
  - More aggressive pruning may lead to better generalization but harder recovery
  - Best rewinding requires tracking best models, adding memory overhead
  - Random initialization is simple but may require more training time to converge

- **Failure signatures**:
  - Training loss spikes every 50 epochs due to weight rewinding and pruning
  - Validation loss plateaus after certain number of pruning cycles
  - Performance degradation if pruning is too aggressive

- **First 3 experiments**:
  1. Compare early stopping vs. cup curriculum with Best rewinding, Random initialization, and Identical updating on Small model
  2. Test different rewinding strategies (Initial, Warm, Best, No) with Random initialization on Medium model
  3. Evaluate impact of different initialization schemes (Original, Old, Top) with Best rewinding on Large model

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the cup curriculum scale with increasingly larger transformer models (e.g., beyond the Large model size tested)?
Basis: The paper shows performance improvements increase with model size and suggests future work should test on state-of-the-art LLMs.
Why unresolved: Experiments only tested three model sizes with modest parameter counts.
What evidence would resolve it: Testing cup curriculum on increasingly larger transformer models (1B, 10B, 100B+ parameters) and measuring performance improvements over early stopping baselines.

### Open Question 2
What is the impact of different learning rate schedules on the effectiveness of the cup curriculum?
Basis: The paper mentions future work should analyze the impact of different learning rate schedulers given insights from related work.
Why unresolved: All experiments used a fixed learning rate scheduler starting at 5.0.
What evidence would resolve it: Comparing cup curriculum performance using various learning rate schedules (cosine decay, linear warmup, etc.) against the fixed schedule.

### Open Question 3
How sensitive is the cup curriculum to the specific pruning criterion used (magnitude change vs other criteria)?
Basis: The paper used magnitude change as the pruning criterion based on Zhou et al.'s analysis but didn't explore alternatives.
Why unresolved: Only one pruning criterion was tested.
What evidence would resolve it: Testing cup curriculum with alternative pruning criteria (gradient-based, random, structured pruning) and comparing performance.

## Limitations
- Modest performance improvements (0.5% to 2%) may not justify added complexity
- Limited testing to transformer models on WikiText2 dataset only
- Theoretical mechanism for why cup-shaped capacity curves improve generalization lacks direct support

## Confidence
- **High Confidence**: Experimental methodology and statistical significance testing are well-defined and rigorously applied
- **Medium Confidence**: Claim that Best rewinding, Random initialization, and Identical updating is optimal is supported by results but relies on limited hyperparameter tuning
- **Low Confidence**: Theoretical mechanism for why cup-shaped capacity curves improve generalization remains speculative with no direct corpus support

## Next Checks
1. **Generalization Test**: Evaluate cup curriculum on different transformer architectures (BERT, GPT variants) and diverse NLP tasks (GLUE benchmark) to assess generalizability beyond WikiText2

2. **Ablation Study**: Systematically test contribution of each component (rewinding strategy, initialization scheme, update scheme) by evaluating all combinations, not just the claimed best one

3. **Mechanism Investigation**: Conduct controlled experiments to isolate effect of cup-shaped capacity curves on generalization, including comparison against other capacity manipulation strategies and analysis of learned representations at different pruning levels