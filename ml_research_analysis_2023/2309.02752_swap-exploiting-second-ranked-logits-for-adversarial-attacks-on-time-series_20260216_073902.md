---
ver: rpa2
title: 'SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series'
arxiv_id: '2309.02752'
source_url: https://arxiv.org/abs/2309.02752
tags:
- logits
- attack
- adversarial
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SW AP, a novel adversarial attack method
  for time series classification models that improves upon existing gradient-based
  attacks. The key innovation is targeting the second-ranked logit instead of random
  logit selection, achieved by minimizing Kullback-Leibler divergence between perturbed
  and target logit distributions.
---

# SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series

## Quick Facts
- arXiv ID: 2309.02752
- Source URL: https://arxiv.org/abs/2309.02752
- Reference count: 25
- Key outcome: Achieves 50% attack success rate, 18% improvement over baselines

## Executive Summary
This paper introduces SWAP, a novel adversarial attack method for time series classification models that improves upon existing gradient-based attacks. The key innovation is targeting the second-ranked logit instead of random logit selection, achieved by minimizing Kullback-Leibler divergence between perturbed and target logit distributions. This approach reduces noise while maintaining attack effectiveness. Experiments on the UCR Archive-2018 dataset show SWAP achieves a 50% attack success rate, representing an 18% improvement over existing methods like FGSM, BIM, and GM.

## Method Summary
SWAP is an adversarial attack method for time series classification that perturbs input time series to cause misclassification while minimizing detectable noise. The method works by identifying the second-ranked logits (the second most likely class predictions) and designing a target distribution where these logits replace the predicted class logits. It then minimizes KL divergence between this target distribution and the perturbed distribution using gradient-based optimization. The approach includes L2 regularization to control perturbation magnitude and clipping to ensure perturbations remain within acceptable bounds. The method is evaluated on the UCR Archive-2018 dataset using InceptionTime as the target classifier.

## Key Results
- SWAP achieves 50% attack success rate on UCR Archive-2018 datasets
- 18% improvement over existing methods (FGSM, BIM, GM at 32%)
- Produces more subtle perturbations with lower average distance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing KL divergence between target and perturbed logit distributions enables stealthy adversarial attacks by reducing noise while maintaining attack success.
- Mechanism: The SW AP method designs a target logit distribution where the second-ranked logits replace the predicted class logits, then minimizes KL divergence to achieve this rank swap with minimal perturbation to other logits.
- Core assumption: The second-ranked logits are sufficiently close in value to the predicted logits that a small perturbation can swap their ranks without requiring large noise.
- Evidence anchors:
  - [abstract]: "This is achieved by minimizing Kullback-Leibler divergence between the target logit distribution and the predictive logit distribution"
  - [section]: "By minimizing the Kullback-Leibler divergence ( DKL) between the target distribution f (Xi)t and the perturbed distribution f (X′i ), we achieve a more effective perturbation strategy"
  - [corpus]: No direct evidence found in neighboring papers about KL divergence optimization for time series adversarial attacks
- Break condition: If the second-ranked logits are too distant from the predicted logits, the required perturbation would need to be too large, making the attack detectable or failing.

### Mechanism 2
- Claim: Targeting the second-ranked logits rather than random logits improves attack success rate by focusing perturbation on the most relevant alternative class.
- Mechanism: The method specifically enhances confidence of the second-largest logits while minimizing manipulation on other logits, focusing the attack on the most plausible alternative classification.
- Core assumption: The second-ranked logits represent the most likely misclassification target, making them the optimal choice for adversarial perturbation.
- Evidence anchors:
  - [abstract]: "SW AP focuses on enhancing the confidence of the second-ranked logits while minimizing the manipulation of other logits"
  - [section]: "The SW AP framework specifically aims to enhance the confidence of the second-largest logits while minimizing manipulation on the other logits"
  - [corpus]: No direct evidence found in neighboring papers about second-ranked logit targeting strategy
- Break condition: If the second-ranked logits are not meaningful alternatives (e.g., in highly imbalanced classification), this targeting strategy would fail.

### Mechanism 3
- Claim: The balance factor γ controls the trade-off between attack success and perturbation magnitude, enabling fine-tuning of attack stealthiness.
- Mechanism: γ determines the ratio between original prediction logits and second-ranked logits in the target distribution, with values near 0.48 providing optimal balance.
- Core assumption: There exists an optimal γ value that maximizes attack success while minimizing perturbation magnitude.
- Evidence anchors:
  - [section]: "The typical value of γ is 0.48" and "When γ was set to 0.5, the ASR was extremely low because the prediction was near the decision boundary"
  - [section]: "Therefore, a suitable γ value could be chosen within the range of 0.4 to 0.49 to maintain good consistency"
  - [corpus]: No direct evidence found in neighboring papers about γ parameter tuning for adversarial attacks
- Break condition: If γ is set too close to 0 or 1, either the attack becomes too aggressive (detectable) or too weak (ineffective).

## Foundational Learning

- Concept: KL divergence as a measure of distribution similarity
  - Why needed here: The method relies on minimizing KL divergence between target and perturbed distributions to achieve stealthy attacks
  - Quick check question: What does a high KL divergence value indicate about two probability distributions?

- Concept: Softmax activation and logit distributions
  - Why needed here: Understanding how logits transform to probabilities and how perturbation affects classification decisions
  - Quick check question: How does increasing a single logit value affect the softmax output distribution?

- Concept: Gradient-based optimization for adversarial attacks
  - Why needed here: The method uses gradient descent to minimize KL divergence with respect to the input perturbations
  - Quick check question: What is the relationship between the loss function gradient and the direction of adversarial perturbation?

## Architecture Onboarding

- Component map: Pre-trained TSC model (InceptionTime) -> Adversarial perturbation generator (SW AP algorithm) -> Target logit distribution designer -> KL divergence calculator -> L2 regularization module

- Critical path: 1. Forward pass through TSC model to get original logits 2. Design target logit distribution using second-ranked logits 3. Compute KL divergence between target and perturbed distributions 4. Backpropagate to update perturbations 5. Apply L2 regularization and clipping 6. Repeat until convergence or max iterations

- Design tradeoffs:
  - γ value vs. attack success rate and stealthiness
  - L2 regularization strength vs. perturbation magnitude and attack effectiveness
  - Number of iterations vs. computational cost and convergence quality

- Failure signatures:
  - ASR drops significantly when γ approaches 0.5
  - Average distance increases dramatically with aggressive perturbation
  - Divergence between target and perturbed distributions fails to converge

- First 3 experiments:
  1. Validate that SW AP achieves higher ASR than baseline methods on a single dataset with default parameters
  2. Test sensitivity of ASR to γ parameter values (0.4, 0.45, 0.48, 0.49, 0.5)
  3. Compare average distance metrics between SW AP and baseline methods on successfully attacked samples

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important unresolved issues emerge from the research:

- The paper mentions that SWAP achieves "over 50% ASR" but does not explore whether this represents a theoretical limit or if further optimization could significantly improve this rate.

- While the paper demonstrates effectiveness against InceptionTime, it does not investigate how SWAP performs against ensemble models or other TSC architectures.

- The specific choice of γ=0.48 is presented as effective but lacks theoretical justification for why this value is optimal.

## Limitations

- The method's effectiveness depends on the assumption that second-ranked logits are meaningful misclassification targets, which may not hold in highly imbalanced classification scenarios.

- The balance factor γ=0.48 is presented without theoretical justification and may not generalize to all time series classification problems or different model architectures.

- The method has only been tested against a single model architecture (InceptionTime), limiting understanding of its broader applicability.

## Confidence

*High Confidence Claims:*
- The SW AP method can achieve higher attack success rates than baseline methods like FGSM, BIM, and GM
- The method produces perturbations with lower average distance metrics compared to existing approaches
- The KL divergence minimization framework is correctly implemented and mathematically sound

*Medium Confidence Claims:*
- The 18% improvement in attack success rate (from 32% to 50%) is robust across different datasets and models
- The second-ranked logit targeting strategy is superior to random logit selection in all scenarios
- The L2 regularization parameter α=0.01 provides optimal balance between attack success and stealthiness

*Low Confidence Claims:*
- The specific γ=0.48 value is universally optimal across all time series classification tasks
- The method maintains effectiveness against all defense mechanisms currently known
- The perturbation stealthiness guarantees hold under all possible detection methods

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary γ across its full range (0.4 to 0.49) on multiple datasets to identify the optimal value for different classification scenarios and determine if 0.48 is truly universal.

2. **Cross-Architecture Generalization**: Test SW AP against multiple TSC architectures beyond InceptionTime (e.g., ResNet, FCN, and Xception) to validate that the second-ranked logit targeting strategy maintains effectiveness across different model families.

3. **Defense Robustness Testing**: Evaluate SW AP's effectiveness against common adversarial defense mechanisms including adversarial training, input preprocessing, and certified robustness approaches to assess real-world applicability.