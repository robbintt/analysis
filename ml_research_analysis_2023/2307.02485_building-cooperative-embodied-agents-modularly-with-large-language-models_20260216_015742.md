---
ver: rpa2
title: Building Cooperative Embodied Agents Modularly with Large Language Models
arxiv_id: '2307.02485'
source_url: https://arxiv.org/abs/2307.02485
tags:
- agents
- communication
- module
- embodied
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a framework that leverages Large Language Models
  to build cooperative embodied agents, enabling them to plan, communicate, and collaborate
  with other agents and humans to accomplish long-horizon tasks efficiently. The proposed
  framework consists of five modules: observation, belief, communication, reasoning,
  and planning.'
---

# Building Cooperative Embodied Agents Modularly with Large Language Models

## Quick Facts
- arXiv ID: 2307.02485
- Source URL: https://arxiv.org/abs/2307.02485
- Reference count: 40
- Key outcome: LLM-based cooperative embodied agents can surpass strong planning-based methods and exhibit effective communication in multi-agent tasks

## Executive Summary
This paper presents CoELA, a framework for building cooperative embodied agents using Large Language Models (LLMs). The framework consists of five modules: observation, belief, communication, reasoning, and planning. It enables agents to plan, communicate, and collaborate with other agents and humans to accomplish long-horizon tasks efficiently. Experiments on two extended embodied multi-agent cooperation challenges, C-WAH and TDW-MAT, demonstrate that LLM-based agents can outperform strong planning-based methods and exhibit emergent effective communication. A user study further reveals that agents communicating in natural language can earn more trust and cooperate more effectively with humans.

## Method Summary
The CoELA framework leverages GPT-4 for reasoning and communication, while a separate planning module handles low-level actions. The observation module processes raw sensory data, the belief module maintains and updates the agent's understanding of the environment and other agents, the communication module generates and interprets natural language messages, the reasoning module synthesizes information to generate high-level plans, and the planning module executes these plans through low-level actions. The framework is evaluated on two extended embodied multi-agent cooperation challenges: C-WAH (Communicative Watch-And-Help) and TDW-MAT (ThreeDWorld Multi-Agent Transport).

## Key Results
- LLM-based agents surpass strong planning-based methods on C-WAH and TDW-MAT benchmarks
- Agents exhibit emergent effective communication without hand-designed heuristics
- Natural language communication with LLMs earns more trust and enables more effective human-agent cooperation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based agents can reason about other agents' states without hand-designed heuristics, leading to improved multi-agent cooperation efficiency.
- Mechanism: The reasoning module uses GPT-4 to synthesize information from observation, belief, communication, and planning modules to generate high-level plans that consider both the agent's own state and the inferred state of other agents. This allows for more effective labor division and task allocation compared to rule-based or planning-based methods.
- Core assumption: GPT-4 can accurately infer the states of other agents from limited information (e.g., past actions, communication history) and reason about their likely future actions.
- Evidence anchors:
  - [abstract]: "Experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication."
  - [section 4.2.1]: "cooperating with the LLM agent boosts the speed-up to 45% and 34% respectively, even without any knowledge of the inner working mechanism of the others, which shows LLMs can reason about the other agent's state well without hand-designed heuristics."
  - [corpus]: Weak - only 5 out of 8 related papers are found, and none have citations yet.
- Break condition: If GPT-4 fails to accurately infer other agents' states or reasons incorrectly about their future actions, the labor division and task allocation will be suboptimal, reducing cooperation efficiency.

### Mechanism 2
- Claim: Natural language communication with LLMs enables more effective human-agent cooperation compared to template-based communication.
- Mechanism: The communication module uses GPT-4 to generate and understand natural language messages, allowing agents to express their intents, share progress, and request help in a flexible and context-aware manner. This leads to better alignment of goals and actions between humans and agents, increasing trust and cooperation effectiveness.
- Core assumption: Humans can understand and respond to natural language messages from LLMs, and the generated messages are clear and relevant to the task at hand.
- Evidence anchors:
  - [abstract]: "Furthermore, a user study reveals that LLM-based agents communicating in natural language can earn more trust and cooperate more effectively with humans."
  - [section 4.2.2]: "Humans trust LLM agents who can communicate in natural language more and cooperate more efficiently with them."
  - [corpus]: Weak - only 5 out of 8 related papers are found, and none have citations yet.
- Break condition: If the natural language messages generated by GPT-4 are unclear, irrelevant, or misunderstood by humans, the cooperation effectiveness and trust will decrease, potentially leading to task failures.

### Mechanism 3
- Claim: The belief module is essential for storing and updating the agent's understanding of the environment, other agents, and task progress, enabling long-horizon planning and effective communication.
- Mechanism: The belief module maintains four types of information: task progress, ego-state, others-state, and scene memory. This information is used by the reasoning and communication modules to make informed decisions and generate relevant messages. Without the belief module, agents would lack the necessary context to plan effectively and communicate meaningfully.
- Core assumption: The belief module can accurately track and update the agent's understanding of the environment, other agents, and task progress based on the observations and messages received.
- Evidence anchors:
  - [section 3.2.2]: "Since LLMs have no intrinsic memory of the previous observations or interactions, it's crucial to find a way to effectively store and update the belief of the physical scenes and the states of the other agents."
  - [section 4.3]: "As shown in Figure 5c, the steps needed to finish the task for the agent with no Belief Module nearly double, showing the importance of our Belief Module to store and update the belief of the scene and the other agents."
  - [corpus]: Weak - only 5 out of 8 related papers are found, and none have citations yet.
- Break condition: If the belief module fails to accurately track or update the agent's understanding of the environment, other agents, or task progress, the reasoning and communication modules will make suboptimal decisions, leading to inefficient planning and ineffective communication.

## Foundational Learning

- Concept: Dec-POMDP (Decentralized Partially Observable Markov Decision Process)
  - Why needed here: The problem setup is formalized as a Dec-POMDP augmented with communication, where multiple agents have partial observability of the environment and must cooperate to achieve a common goal.
  - Quick check question: What is the main difference between a Dec-POMDP and a regular POMDP?

- Concept: Chain-of-thought prompting
  - Why needed here: The reasoning module uses zero-shot chain-of-thought prompting to encourage GPT-4 to generate more detailed and step-by-step reasoning before arriving at a final plan, leading to better decision-making.
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of the reasoning process?

- Concept: Theory of Mind
  - Why needed here: The reasoning module needs to infer the states and likely future actions of other agents based on limited information, which requires a form of theory of mind - the ability to attribute mental states to others and understand their perspective.
  - Quick check question: What is the key difference between first-order and second-order theory of mind?

## Architecture Onboarding

- Component map: Observation -> Belief -> Reasoning -> Planning -> Action
- Critical path: Observation -> Belief -> Reasoning -> Planning -> Action
  - The observation module provides the initial input, which is then processed by the belief module to update the agent's understanding. The reasoning module uses this information to generate high-level plans, which are then executed by the planning module through low-level actions.

- Design tradeoffs:
  - Using GPT-4 for reasoning and communication vs. smaller, more specialized models: GPT-4 provides strong reasoning and language understanding capabilities but may be more expensive and slower than smaller models.
  - Natural language communication vs. template-based communication: Natural language allows for more flexible and context-aware communication but may be more prone to misunderstandings compared to predefined templates.

- Failure signatures:
  - Inefficient cooperation: If the reasoning module fails to accurately infer other agents' states or generate effective plans, the cooperation efficiency will decrease.
  - Poor human-agent interaction: If the communication module generates unclear or irrelevant messages, the human-agent cooperation effectiveness and trust will decrease.
  - Suboptimal planning: If the belief module fails to accurately track the environment, other agents, or task progress, the reasoning module will make suboptimal decisions, leading to inefficient planning.

- First 3 experiments:
  1. Evaluate the impact of the belief module on cooperation efficiency by comparing agents with and without the belief module on C-WAH and TDW-MAT.
  2. Assess the effectiveness of natural language communication by comparing human-agent cooperation with LLM-based agents using natural language vs. template-based communication on C-WAH.
  3. Investigate the importance of GPT-4's reasoning capabilities by comparing the performance of agents using GPT-4 vs. ChatGPT for the reasoning module on C-WAH and TDW-MAT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of LLM-based agents change if the framework were adapted to handle 3D spatial information directly, rather than relying on text-based representations?
- Basis in paper: [explicit] The paper mentions that the current framework does not incorporate 3D spatial information due to the challenge of introducing spatial information to pure text language models, and that this limitation may cause agents to come up with semantically sound but time-consuming exploration plans.
- Why unresolved: The paper acknowledges the limitation but does not explore how incorporating spatial information would affect the agents' performance or what methods could be used to effectively integrate it.
- What evidence would resolve it: Experiments comparing the performance of agents using the current framework versus an adapted version that incorporates 3D spatial information, demonstrating improvements in efficiency and planning.

### Open Question 2
- Question: Would allowing LLM-based agents to make low-level control decisions directly, instead of using a separate Planning Module, lead to more effective and efficient task completion?
- Basis in paper: [explicit] The paper notes that while LLMs are effective at making high-level plans, they are poor at making low-level controls. It also mentions that allowing LLMs to make low-level decisions would require significantly more API requests, which is currently cost-prohibitive.
- Why unresolved: The paper does not explore the potential benefits or drawbacks of integrating low-level control decision-making into the LLM-based agents, nor does it investigate cost-effective ways to implement this.
- What evidence would resolve it: Comparative studies showing the performance differences between agents using a separate Planning Module and those with integrated low-level control decision-making, along with an analysis of the cost implications.

### Open Question 3
- Question: How would the performance of LLM-based agents be affected if they were trained with fine-tuning or few-shot prompting, rather than relying solely on zero-shot prompting?
- Basis in paper: [explicit] The paper states that the framework achieves results without any fine-tuning or few-shot prompting, but does not explore how these techniques might improve performance.
- Why unresolved: The paper does not investigate the potential benefits of fine-tuning or few-shot prompting on the agents' ability to plan, communicate, and cooperate effectively.
- What evidence would resolve it: Experiments comparing the performance of agents using zero-shot prompting versus those using fine-tuning or few-shot prompting, demonstrating any improvements in task completion and cooperation.

## Limitations

- Evaluation relies on synthetic benchmarks (C-WAH and TDW-MAT) that may not fully capture real-world human-agent collaboration complexity.
- Performance depends on specific capabilities and biases of GPT-4, which may evolve over time or differ from other LLMs.
- User study sample size and demographic diversity are not specified, raising questions about generalizability.

## Confidence

- **High Confidence**: The framework's modular architecture and the use of LLMs for reasoning and communication are well-defined and supported by experimental results.
- **Medium Confidence**: The effectiveness of natural language communication in enhancing human-agent cooperation is supported by the user study, but generalizability to diverse real-world scenarios is uncertain.
- **Low Confidence**: Framework's performance in complex, real-world environments with diverse tasks and unpredictable human behavior remains to be seen.

## Next Checks

1. Conduct a study with diverse participants to evaluate the framework's performance in a real-world human-agent collaboration scenario, assessing impact on task efficiency, human satisfaction, and trust.

2. Perform an ablation study to quantify the contribution of each component within the belief module to overall performance, identifying the most critical aspects.

3. Evaluate the framework's performance on long-horizon tasks with extended time horizons and increased complexity compared to planning-based methods to assess scalability.