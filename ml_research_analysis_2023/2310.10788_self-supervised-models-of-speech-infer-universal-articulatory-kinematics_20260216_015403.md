---
ver: rpa2
title: Self-Supervised Models of Speech Infer Universal Articulatory Kinematics
arxiv_id: '2310.10788'
source_url: https://arxiv.org/abs/2310.10788
tags:
- speech
- articulatory
- speakers
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether self-supervised learning (SSL)\
  \ models of speech can infer universal articulatory kinematics\u2014the causal articulatory\
  \ dynamics underlying speech signals. The researchers probe SSL models (HuBERT and\
  \ Wav2vec 2.0) trained on multiple languages (English, Mandarin, Korean, French,\
  \ Dutch) using electromagnetic articulography (EMA) data from 62 speakers across\
  \ different languages and dialects."
---

# Self-Supervised Models of Speech Infer Universal Articulatory Kinematics

## Quick Facts
- arXiv ID: 2310.10788
- Source URL: https://arxiv.org/abs/2310.10788
- Authors: 
- Reference count: 0
- Primary result: SSL models trained on speech can infer universal articulatory kinematics across languages with correlation scores exceeding 0.8, demonstrating that articulatory representations are a fundamental property of SSL models.

## Executive Summary
This study investigates whether self-supervised learning (SSL) models of speech can infer universal articulatory kinematics—the causal articulatory dynamics underlying speech signals. The researchers probe SSL models (HuBERT and Wav2vec 2.0) trained on multiple languages using electromagnetic articulography (EMA) data from 62 speakers across different languages and dialects. They find that SSL models can recover articulatory kinematics with high accuracy regardless of the training language, indicating that this ability is a fundamental property of SSL models. Moreover, individual articulatory systems are mutually transferable with affine transformations, demonstrating a canonical basis of articulatory kinematics.

## Method Summary
The study extracts 20ms frame features from SSL models (HuBERT and Wav2vec 2.0) trained on five languages, then applies linear inversion models to project these features into EMA space. A 6Hz low-pass filter removes high-frequency noise, and correlation between predicted and ground truth EMA traces is computed. The researchers evaluate five public EMA datasets covering 62 speakers from English (UK, US, Beijing, Shanghai), Mandarin, and Italian. They also assess transferability between speakers using affine transformations and analyze language-specific patterns in articulatory phonology.

## Key Results
- SSL models recover articulatory kinematics with correlation scores exceeding 0.8 across all tested languages and dialects
- Individual articulatory systems are mutually transferable through simple affine transformations, demonstrating a canonical basis of articulatory kinematics
- Language-specific articulatory phonology exists but does not override the universal articulatory basis learned by SSL models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSL models trained on speech inherently learn articulatory kinematics as a universal representation, independent of language.
- Mechanism: The models map acoustic features to the causal articulatory dynamics (vocal tract shapes) that generate speech. Since all humans share a similar vocal tract anatomy, this mapping is universal across languages.
- Core assumption: The vocal tract anatomy and orofacial muscle structure are consistent across ethnic groups, languages, and dialects.
- Evidence anchors:
  - [abstract] "the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal"
  - [section] "speech acoustics are a result of the resonance associated with vocal tract shapes that add spectral detail to the vocal source"
- Break condition: If vocal tract anatomy or muscle structure varies significantly across populations in ways not captured by the training data, the universal mapping assumption fails.

### Mechanism 2
- Claim: Individual articulatory systems can be aligned through simple affine transformations, indicating a canonical basis of articulatory kinematics.
- Mechanism: Differences between speakers' articulatory systems are primarily geometric (anatomical variations, sensor placement) rather than fundamental differences in the kinematic basis. Affine transformations can align these systems.
- Core assumption: Variability in EMA data across speakers is mainly due to anatomical differences and sensor placement rather than fundamental differences in how speech is produced.
- Evidence anchors:
  - [abstract] "with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects"
  - [section] "We devise a transferability metric, which measures the correlation between two articulatory systems... trained another linear model, gA→B, to align the articulatory systems"
- Break condition: If articulatory differences between speakers cannot be modeled by affine transformations (e.g., if fundamental kinematic patterns differ), the canonical basis assumption fails.

### Mechanism 3
- Claim: Language-specific articulatory phonology exists but does not override the universal articulatory basis learned by SSL models.
- Mechanism: While SSL models learn universal articulatory kinematics, they also capture language-specific articulatory habits that create preferences for certain languages during probing.
- Core assumption: Language-specific articulatory habits can be distinguished from the universal articulatory basis in SSL representations.
- Evidence anchors:
  - [abstract] "This abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system"
  - [section] "While the overall scores are surprisingly high, we find a preference for language or dialect, which is evidence of language-specific articulatory phonology"
- Break condition: If language-specific articulatory habits completely dominate over universal patterns, the model would show strong language-specific performance rather than high universal performance.

## Foundational Learning

- Concept: Articulatory kinematics and vocal tract dynamics
  - Why needed here: Understanding how speech is physically produced is fundamental to grasping why SSL models can learn these representations
  - Quick check question: What are the six main articulators typically tracked in EMA studies, and what aspects of speech do they control?

- Concept: Self-supervised learning and masked prediction objectives
  - Why needed here: The paper's core claim depends on understanding how SSL models learn representations without explicit labels
  - Quick check question: How do HuBERT and Wav2vec 2.0 differ in their approach to masked prediction, and what does each predict?

- Concept: Probing analysis methodology
  - Why needed here: The paper's validation approach relies on correlating SSL representations with articulatory data through linear regression
  - Quick check question: What is the key difference between the probing approach in this paper and previous articulatory probing studies?

## Architecture Onboarding

- Component map: Audio → SSL feature extraction (20ms frames) → Linear inversion model → EMA prediction → Correlation calculation → Affine transformation (transferability analysis)

- Critical path: Audio → SSL feature extraction → Linear inversion → EMA prediction → Correlation evaluation

- Design tradeoffs:
  - Linear vs. non-linear inversion: Linear models are simpler and interpretable but may miss non-linear relationships
  - Layer selection: Different SSL layers capture different levels of abstraction; optimal layer varies by task
  - Frame-level vs. sequence-level analysis: Frame-level is simpler but may miss temporal dynamics

- Failure signatures:
  - Low correlation (<0.7) indicates poor articulatory representation in the SSL model
  - Inconsistent transferability scores suggest non-universal articulatory basis
  - Language-specific bias in cross-lingual probing indicates overfitting to training language

- First 3 experiments:
  1. Probe a new SSL model (not HuBERT or Wav2vec 2.0) on the existing EMA datasets to test generalizability of findings
  2. Test non-linear inversion models (e.g., small MLP) to see if they improve performance over linear models
  3. Apply the same methodology to EMA data from a language not represented in the current datasets (e.g., Arabic or Hindi) to test true universality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the articulatory representations inferred by SSL models compare to the complete articulatory kinematics that include vocal fold vibration and non-mid-sagittal movements?
- Basis in paper: [explicit] The paper notes that EMA data only captures midsagittal plane movements and does not include vocal fold vibration, suggesting that SSL models might capture more complete articulatory information.
- Why unresolved: The paper does not provide direct evidence of SSL models capturing vocal fold information or non-mid-sagittal movements.
- What evidence would resolve it: Direct comparison of SSL model outputs with comprehensive articulatory data that includes vocal fold vibrations and non-mid-sagittal movements.

### Open Question 2
- Question: Do SSL models exhibit different preferences for articulatory representations across languages with significantly different phonological systems (e.g., tonal vs. non-tonal languages)?
- Basis in paper: [inferred] The paper shows language-specific preferences in articulatory phonology but only tests a limited set of languages.
- Why unresolved: The study's language sample is limited, and the paper does not explore extreme phonological differences.
- What evidence would resolve it: Probing SSL models trained on languages with vastly different phonological systems and comparing their articulatory representation preferences.

### Open Question 3
- Question: What is the relationship between the scale of SSL model training (data size and model size) and the fidelity of articulatory representation inference?
- Basis in paper: [explicit] The paper notes that scaling up has minimal effect on articulatory representation learning, suggesting saturation in the SSL regime.
- Why unresolved: The paper only tests a limited range of model sizes and does not explore the upper limits of scaling effects.
- What evidence would resolve it: Systematic testing of articulatory representation fidelity across a wide range of model sizes and training data scales.

### Open Question 4
- Question: Can the articulatory representations inferred by SSL models be used to improve downstream speech tasks beyond acoustic-to-articulatory inversion?
- Basis in paper: [explicit] The paper mentions that articulatory representation correlation can indicate SSL model success in downstream tasks.
- Why unresolved: The paper focuses on articulatory representation itself rather than its application to other speech tasks.
- What evidence would resolve it: Experimental evaluation of SSL models with articulatory representations on various downstream speech tasks (e.g., speech recognition, speaker identification) compared to standard SSL models.

## Limitations
- Linear inversion models may not capture all non-linear relationships between acoustic features and articulatory dynamics
- Dataset composition (62 speakers across 5 languages) may not fully represent global phonetic diversity
- EMA sensor placement and anatomical differences between speakers introduce noise that could affect transferability metrics

## Confidence
- **High Confidence**: The universal articulatory representation across different SSL architectures and training languages. The consistent high correlation scores (over 0.8) across multiple model/language combinations provide robust evidence.
- **Medium Confidence**: The mutual transferability of individual articulatory systems through affine transformations. While the evidence is strong, individual variation and sensor placement differences introduce uncertainty.
- **Medium Confidence**: The language-specific articulatory phonology preferences. The evidence shows preferences but doesn't fully characterize the nature or extent of these differences.

## Next Checks
1. Evaluate whether non-linear models (e.g., small MLPs or transformers) improve articulatory prediction accuracy over linear models, particularly for challenging articulators like lips and jaw.

2. Test the methodology on EMA data from additional languages not represented in the current dataset (e.g., Arabic, Hindi, or African languages) to validate true universality claims.

3. Extend the analysis beyond frame-level correlations to examine how well SSL models capture temporal articulatory patterns and coarticulation effects across different speaking rates and styles.