---
ver: rpa2
title: Preference Optimization for Molecular Language Models
arxiv_id: '2310.12304'
source_url: https://arxiv.org/abs/2310.12304
tags:
- molecules
- fine-tuning
- language
- molecular
- moses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Direct Preference Optimization (DPO)
  to fine-tune molecular language models for generating molecules that satisfy chemist
  preferences. The authors show that by using DPO with automatically computable feedback
  (such as filtering based on SMARTS patterns or predicted bioactivity), they can
  significantly improve the rate of generated molecules that pass desired filters
  or are predicted active against a target, while minimally affecting other metrics
  like diversity and validity.
---

# Preference Optimization for Molecular Language Models

## Quick Facts
- arXiv ID: 2310.12304
- Source URL: https://arxiv.org/abs/2310.12304
- Authors:
- Reference count: 10
- This paper explores using Direct Preference Optimization (DPO) to fine-tune molecular language models for generating molecules that satisfy chemist preferences.

## Executive Summary
This paper demonstrates how Direct Preference Optimization (DPO) can be used to fine-tune molecular language models for generating molecules that align with chemist preferences. The authors show that by using DPO with automatically computable feedback (such as filtering based on SMARTS patterns or predicted bioactivity), they can significantly improve the rate of generated molecules that pass desired filters or are predicted active against a target, while minimally affecting other metrics like diversity and validity.

## Method Summary
The method involves pre-training molecular language models (LSTM or GPT) on SMILES strings, then fine-tuning them using DPO with automatically generated preference data. Preference pairs are created by sampling molecules from the reference model and labeling them based on computable properties like SMARTS filter compliance or bioactivity predictions. The DPO fine-tuning process uses an objective that shifts probability mass from negative to positive sequences while maintaining proximity to the reference model.

## Key Results
- DPO increased the rate of filter-passing molecules by 65-79%
- DPO increased the fraction of molecules predicted active against EGFR by 527-778%
- These improvements came with little-to-no degradation in other metrics like diversity and validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO shifts probability mass from negative to positive sequences while maintaining proximity to the reference model.
- Mechanism: The DPO objective directly maximizes the log-probability of positive sequences (sp) and minimizes that of negative sequences (sn) using a preference-based ranking loss.
- Core assumption: The reference model πref provides a stable starting distribution from which DPO can make controlled, preference-driven updates.
- Evidence anchors:
  - [abstract] "DPO objective shifts probability mass away from negative sequences sn towards positive sequences sp, while not deviating too much from the reference model."
  - [section 2.1] "The DPO objective can be written as LDPO(πθ; πref) = −E(sp,sn) [log σ(β log πθ(sp)/πref(sp) − β log πθ(sn)/πref(sn))]"
- Break Condition: If the ranking labels are noisy or if the reference model is already highly biased, DPO updates may not effectively shift mass or could destabilize training.

### Mechanism 2
- Claim: Automatic computable feedback enables large-scale synthetic preference datasets for efficient fine-tuning.
- Mechanism: Since molecular properties like SMARTS filter compliance and bioactivity predictions can be computed automatically, synthetic datasets of 100k+ labeled molecules can be generated cheaply from the reference model.
- Core assumption: The automated scoring functions (SMARTS patterns, bioactivity classifiers) are sufficiently accurate to generate meaningful preference pairs.
- Evidence anchors:
  - [abstract] "Since many of the properties of interest can be directly and efficiently computed (or estimated), it is possible to cheaply generate large labeled, synthetic datasets for this task using a pre-trained language model."
  - [section 3] Describes using 100k sampled molecules filtered by SMARTS and bioactivity classifiers as training data for DPO.
- Break Condition: If the automated scoring functions have high false-positive/negative rates, the preference pairs will be incorrect, leading DPO to reinforce wrong behaviors.

### Mechanism 3
- Claim: Fine-tuning with DPO minimally degrades diversity and validity while improving targeted metrics.
- Mechanism: By carefully balancing the DPO loss with the reference model via the β hyperparameter, the fine-tuned model retains general generation quality while specializing in desired properties.
- Core assumption: The β parameter can be tuned to achieve the desired trade-off between preference alignment and retention of baseline generation properties.
- Evidence anchors:
  - [abstract] "we find that training with DPO is straightforward and computationally low-cost."
  - [section 3.1] "these improvements come with little-to-no degradation in the other metrics."
- Break Condition: If β is set too high, the model may diverge significantly from the reference and lose general generation capability; if too low, preference improvements may be negligible.

## Foundational Learning

- Concept: Molecular string representations (SMILES, SELFIES) and their parsing into chemical structures.
  - Why needed here: The paper operates on SMILES strings and uses RDKit for parsing/validating molecules; understanding this is crucial for interpreting FracValid and FracUnique metrics.
  - Quick check question: Given the SMILES string "CCO", what is the corresponding chemical structure and is it valid under RDKit?

- Concept: Preference optimization and reinforcement learning in generative modeling.
  - Why needed here: DPO is presented as an alternative to RL for fine-tuning language models using preference data; understanding this helps compare DPO's advantages (no separate reward model) and limitations.
  - Quick check question: How does the DPO objective differ from a standard policy gradient RL objective in terms of model updates?

- Concept: Molecular property computation (SMARTS filtering, bioactivity prediction).
  - Why needed here: The experiments rely on automatically computed properties (SMARTS filter compliance, EGFR activity prediction) to generate preference pairs; understanding these tools is essential for reproducing the results.
  - Quick check question: What is a SMARTS pattern and how would you write one to detect an aromatic ring?

## Architecture Onboarding

- Component map:
  Pre-trained molecular language models (LSTM or GPT) → Reference model πref → Automated property scoring functions (SMARTS filters, bioactivity classifiers) → Generate preference pairs → DPO fine-tuning module → Update model πθ using LDPO loss → Evaluation pipeline → Compute FracValid, FracUnique, FracPassesMCF, IntDiv

- Critical path:
  1. Pre-train or load base molecular language model
  2. Sample 100k molecules from the model
  3. Compute automatic properties for each molecule
  4. Construct preference pairs (sp ≻ sn) based on property thresholds
  5. Fine-tune model with DPO using the preference pairs
  6. Evaluate on held-out samples for targeted and general metrics

- Design tradeoffs:
  - Dataset size vs. quality: Larger synthetic datasets provide more DPO training examples but may include more noisy labels if scoring functions are imperfect.
  - β hyperparameter: Higher β allows more deviation from reference model for stronger preference alignment but risks losing general generation quality.
  - Property computation cost: More complex property evaluations (e.g., docking simulations) yield better preferences but are slower and more expensive.

- Failure signatures:
  - Training loss plateaus early or diverges: Indicates potential issues with preference pair quality or β tuning.
  - FracValid drops significantly after fine-tuning: Suggests the model is generating chemically invalid structures, possibly due to overly aggressive preference optimization.
  - Minimal improvement in targeted metrics: Could indicate noisy preference labels or insufficient model capacity.

- First 3 experiments:
  1. Reproduce the filtering experiment: Pre-train a simple LSTM on MOSES, sample 100k molecules, apply SMARTS filtering, fine-tune with DPO, evaluate FracPassesMCF.
  2. Validate bioactivity experiment: Train a random forest classifier on ChEMBL EGFR data, use it to label synthetic molecules, fine-tune with DPO, evaluate FracPredActive.
  3. Ablation study on β: Run the filtering experiment with multiple β values to observe the trade-off between preference alignment and retention of baseline metrics.

## Open Questions the Paper Calls Out

- Can DPO fine-tuning be performed without binarizing continuous scores to mitigate potential information loss?
- What causes the observed decrease in diversity metrics when using DPO fine-tuning, and can this be mitigated?
- How would human-curated preference labels from chemists compare to automatically computable feedback in DPO fine-tuning of molecular language models?

## Limitations
- Reliance on automated property scoring functions for generating preference pairs without quantifying their error rates
- Limited exploration of the β hyperparameter across different molecular properties and tasks
- No assessment of potential overfitting to synthetic preference data or generalizability to unseen molecular scaffolds

## Confidence

- High confidence: The claim that DPO can improve targeted molecular properties (e.g., SMARTS filter compliance, bioactivity prediction) while maintaining general generation quality.
- Medium confidence: The claim that automated computable feedback enables large-scale synthetic preference datasets.
- Low confidence: The claim that the specific mechanisms of DPO (probability mass shifting, minimal degradation via β tuning) are the primary drivers of the observed improvements.

## Next Checks
1. Quantify the false-positive and false-negative rates of the SMARTS filters and bioactivity classifiers used to generate preference pairs.
2. Run the filtering experiment across a wider range of β values (e.g., 0.01, 0.1, 1.0, 10.0) and plot the trade-off curves between targeted metrics and baseline metrics.
3. Evaluate the fine-tuned models on a held-out set of molecular scaffolds not present in the synthetic preference dataset to test generalizability.