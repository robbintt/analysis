---
ver: rpa2
title: Dual-Granularity Contrastive Learning for Session-based Recommendation
arxiv_id: '2304.08873'
source_url: https://arxiv.org/abs/2304.08873
tags:
- learning
- factor-level
- embeddings
- recommendation
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Dual-Granularity Contrastive Learning for
  Session-based Recommendation (DGCL-GNN), which addresses two limitations of existing
  contrastive learning strategies: inability to enforce finer-grained (e.g., factor-level)
  comparisons and the use of item or segment dropout as data augmentation. DGCL-GNN
  introduces a novel multi-granularity CL framework with two extra augmented embedding
  convolution channels with different granularities.'
---

# Dual-Granularity Contrastive Learning for Session-based Recommendation

## Quick Facts
- arXiv ID: 2304.08873
- Source URL: https://arxiv.org/abs/2304.08873
- Reference count: 39
- P@20 and M@20 scores of 0.7469 and 0.3289 on Yoochoose1/64 dataset

## Executive Summary
This paper introduces DGCL-GNN, a dual-granularity contrastive learning framework for session-based recommendation. The method addresses limitations in existing contrastive learning strategies by introducing two augmentation channels: one for item-level (star graph) and one for factor-level (disentangled representation learning) information. DGCL-GNN achieves statistically significant improvements over existing methods on two benchmark datasets, demonstrating the effectiveness of learning both item-level and finer-grained factor-level representations in sparse session data.

## Method Summary
DGCL-GNN implements a multi-view contrastive learning approach with three embedding convolution channels: the original session graph, a factor-level augmentation channel using disentangled representation learning to obtain independent latent factors, and an item-level augmentation channel using star graph construction. The model employs Graph Gated Neural Networks (GGNNs) for graph convolution across all views, with a BCE contrastive loss comparing original and augmented embeddings at both granularities. Session embeddings are generated through soft attention over item and factor embeddings, with final predictions using inner products of session and candidate embeddings.

## Key Results
- P@20 and M@20 scores of 0.7469 and 0.3289, respectively, on Yoochoose1/64 dataset
- P@20 and M@20 scores of 0.5501 and 0.1911, respectively, on Diginetica dataset
- Statistically significant improvements over existing methods
- Demonstrates effectiveness of dual-granularity contrastive learning in sparse session data

## Why This Works (Mechanism)

### Mechanism 1
Dual-granularity contrastive learning addresses both item-level and factor-level information loss in session data. The model uses two augmentation channels—one that constructs a star graph for item-level augmentation and another that uses disentangled representation learning for factor-level embeddings. These two views are compared with the original view to enforce contrastive learning at both granularities.

### Mechanism 2
Disentangled representation learning replaces missing explicit factor labels in session data. DRL decomposes item embeddings into independent latent factor embeddings, which are then used to create separate convolution channels for each factor. This allows factor-level contrastive learning without needing explicit brand/color annotations.

### Mechanism 3
Star graph augmentation preserves data density while adding useful structural information. A satellite node is added to the session graph, randomly connected to session items. This allows non-adjacent items to interact indirectly, enriching the graph without dropping any real session data.

## Foundational Learning

- Concept: Contrastive learning in self-supervised settings
  - Why needed here: Addresses data sparsity by learning richer embeddings from augmented views without requiring labels.
  - Quick check question: What is the difference between instance-level and factor-level contrastive learning, and why does both matter in session data?

- Concept: Graph neural networks for sequential modeling
  - Why needed here: Sessions are naturally modeled as graphs where items are nodes and transitions are edges; GNNs can capture complex item-item relationships.
  - Quick check question: How does a directed session graph differ from an undirected one, and why is directionality important here?

- Concept: Disentangled representation learning
  - Why needed here: Enables learning of independent latent factors from item embeddings, which can then be used for finer-grained modeling and contrastive learning.
  - Quick check question: How does DRL encourage independence between learned factors, and what metric is used to measure that independence?

## Architecture Onboarding

- Component map: Input session -> Build original and augmented graphs -> GGNN conv (3 views) -> DRL (factor view) -> CL loss -> Session encoder -> Prediction -> Loss
- Critical path: Input session → Build original and augmented graphs → GGNN conv (3 views) → DRL (factor view) → CL loss → Session encoder → Prediction → Loss
- Design tradeoffs:
  - More factors (K) → finer-grained CL but harder to train independent factors
  - Star graph connectivity probability (θ) → balance between augmentation strength and noise
  - α hyperparameter → balance between item-level and factor-level CL
- Failure signatures:
  - CL loss dominates → embeddings collapse to trivial solutions
  - Prediction loss dominates → CL module ineffective
  - Both losses plateau early → poor augmentation quality or factor independence
- First 3 experiments:
  1. Remove factor-level CL (keep only item-level) → verify performance drop
  2. Replace star graph with edge dropout → verify performance drop
  3. Remove all CL → verify performance drop

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed DGCL-GNN model perform on datasets with different levels of data sparsity? The paper mentions that data sparsity is one of the main challenges in session-based recommendation and that the proposed model addresses this issue, but does not provide an analysis of how the model performs on datasets with different levels of data sparsity.

### Open Question 2
How does the proposed DGCL-GNN model perform on datasets with different session lengths? The paper mentions that the proposed model is evaluated on datasets with different session lengths, but does not provide an analysis of how the model performs on datasets with different session lengths.

### Open Question 3
How does the proposed DGCL-GNN model perform on datasets with different numbers of items? The paper mentions that the proposed model is evaluated on datasets with different numbers of items, but does not provide an analysis of how the model performs on datasets with different numbers of items.

## Limitations

- Factor independence in DRL is asserted but not empirically validated with quantitative measures
- Key hyperparameters (K, θ, α) are not specified, making exact reproduction difficult
- Star graph augmentation effectiveness is claimed but not rigorously compared to simpler alternatives with statistical significance

## Confidence

- High: The overall dual-granularity CL framework is logically sound and the two augmentation channels are clearly defined
- Medium: The use of DRL for factor disentanglement is plausible, but lacks empirical validation of factor independence
- Low: The star graph augmentation's effectiveness is asserted but not rigorously compared to alternatives

## Next Checks

1. Measure pairwise correlations or mutual information between learned factors; verify they are below a threshold (e.g., 0.1) to ensure disentanglement
2. Replace the star graph augmentation with edge dropout and report P@K/M@K; confirm that star graph consistently outperforms
3. Sweep K (number of factors) and θ (star graph connectivity); report performance variance to identify stable ranges