---
ver: rpa2
title: 'Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark
  and a Case Study on Summarizing Diverse Information from News Articles'
arxiv_id: '2309.09369'
source_url: https://arxiv.org/abs/2309.09369
tags:
- articles
- answer
- news
- evaluation
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multi-document Diversity Summarization
  (MDDS) task, which challenges models to summarize diverse information from multiple
  news articles covering the same event. The authors create a new dataset, DiverseSumm,
  containing 245 news stories with 10 articles each and human-validated references.
---

# Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles

## Quick Facts
- arXiv ID: 2309.09369
- Source URL: https://arxiv.org/abs/2309.09369
- Reference count: 33
- Key outcome: New dataset DiverseSumm with 245 news stories, each with 10 articles and human-validated references; GPT-4 covers less than 40% of diverse information on average

## Executive Summary
This paper introduces the Multi-document Diversity Summarization (MDDS) task, which challenges models to summarize diverse information from multiple news articles covering the same event. The authors create a new dataset, DiverseSumm, containing 245 news stories with 10 articles each and human-validated references. They propose a pipeline for identifying diverse information using LLMs to generate questions, extract answers, and cluster them into groups. Through fine-grained human evaluation, the authors find that even state-of-the-art LLMs like GPT-4 struggle with the task, covering less than 40% of diverse information on average. The paper provides insights into LLM behavior and recommendations for evaluation practices.

## Method Summary
The authors propose a reference annotation methodology using an LLM-based pipeline to identify diverse information across multiple news articles. The pipeline generates questions about a news story, extracts answers from each article, clusters answers based on semantics, and filters invalid pairs through human validation. The DiverseSumm dataset contains 245 news stories, each with 10 articles and paired human-validated reference QAs. For evaluation, they conduct fine-grained human assessment at sentence level for faithfulness and answer level for coverage, while also analyzing GPT-4-based metrics for bias and correlation with human judgments.

## Key Results
- GPT-4 covers less than 40% of diverse information on average in the MDDS task
- LLMs exhibit position bias, focusing more on initial and final articles
- Question type bias observed, with "How" and "What" questions being more challenging
- Different behaviors for frequent vs. infrequent answers depending on context length
- GPT-4-based metrics show bias toward position and verbosity in evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-based pipeline for reference annotation works because it breaks down the complex task of identifying diverse information into smaller, more manageable sub-tasks (question generation, question answering, answer clustering, and post-processing).
- Mechanism: By using LLMs to generate questions about a news story, extract answers from each article, cluster answers based on semantics, and filter invalid pairs, the pipeline can surface diverse information that would be difficult for humans to identify manually at scale.
- Core assumption: LLMs are capable of generating valid questions that elicit diverse responses from different sources, and of extracting and clustering answers in a way that preserves semantic diversity.
- Evidence anchors:
  - [abstract]: "We propose a reference annotation methodology to identify and gather diverse information dispersed across multiple articles about the same story. Our approach is a pipeline based on gpt-3.5-turbo..."
  - [section]: "Our data collection framework surfaces diverse information across news articles by asking questions about a news story, extracting answers from each news article, clustering the answers based on semantics, and filtering invalid questions and answers."
  - [corpus]: "We propose a multi-document diversity summarization task and curate a dataset named DiverseSumm... The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference."

### Mechanism 2
- Claim: GPT-4 is an effective evaluator for the multi-document diversity summarization task because it can assess both faithfulness and coverage at a fine-grained level.
- Mechanism: By breaking down summaries into sentences for faithfulness evaluation and answers for coverage evaluation, GPT-4 can provide a more robust and reliable assessment of summary quality compared to evaluating the summary as a whole.
- Core assumption: GPT-4's ability to reason about text quality and its correlation with human judgments makes it a suitable evaluator for this task.
- Evidence anchors:
  - [abstract]: "We conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries, as well as their correlation with human assessments."
  - [section]: "Following the recommendations from Krishna et al. (2023), we conducted evaluations at a finer granularity. For faithfulness evaluation, we split each summary into individual sentences and ask annotators to evaluate whether each generated sentence is faithful to at least one input article. Conversely, for coverage evaluation, we ask annotators to rate whether each answer in the reference is covered by a particular summary."
  - [corpus]: "Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover less than 40% of the diverse information on average."

### Mechanism 3
- Claim: The multi-document diversity summarization task is challenging for LLMs because it requires them to effectively incorporate diverse information from multiple sources, which is a different set of challenges compared to traditional multi-document summarization tasks.
- Mechanism: By focusing on summarizing diverse information dispersed across multiple articles about the same event, rather than just collating consensus information, the task requires LLMs to identify and incorporate a wider range of perspectives and opinions into the summary.
- Core assumption: The ability to identify and incorporate diverse information is a distinct skill from traditional summarization, and one that current LLMs struggle with.
- Evidence anchors:
  - [abstract]: "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, to our knowledge, the summarization of diverse information dispersed across multiple articles about an event has not been previously investigated."
  - [section]: "Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover less than 40% of the diverse information on average."
  - [corpus]: "Even state-of-the-art LLMs, such as GPT-4, struggle to achieve high coverage."

## Foundational Learning

- Concept: Understanding the differences between single-document and multi-document summarization.
  - Why needed here: The multi-document diversity summarization task builds upon and extends traditional multi-document summarization, so a solid understanding of the latter is crucial.
  - Quick check question: What are the key differences between extractive and abstractive summarization, and how do these approaches apply to single-document vs. multi-document settings?

- Concept: Familiarity with LLM capabilities and limitations.
  - Why needed here: The paper relies heavily on LLMs for reference annotation, evaluation, and studying LLM behavior on the task. Understanding what LLMs can and cannot do is essential for interpreting the results.
  - Quick check question: What are some common biases and limitations of LLMs, and how might these impact their performance on the multi-document diversity summarization task?

- Concept: Knowledge of evaluation methodologies for summarization tasks.
  - Why needed here: The paper conducts extensive analyses of different evaluation protocols and their correlation with human judgments. Understanding these methodologies is key to interpreting the findings.
  - Quick check question: What are some common metrics and protocols used to evaluate summarization quality, and how do they differ in terms of granularity and correlation with human judgments?

## Architecture Onboarding

- Component map: Reference annotation pipeline (Question generation -> Answer extraction -> Answer clustering -> Post-processing) -> Dataset (DiverseSumm: 245 stories, 10 articles each) -> Evaluation (Faithfulness evaluation -> Coverage evaluation -> Bias analysis -> Correlation analysis) -> LLM behavior analysis (Position bias -> Question type bias -> Answer frequency bias)

- Critical path: The reference annotation pipeline is the most critical component as it directly impacts the quality and diversity of the dataset. Ensuring the pipeline generates valid questions and clusters diverse answers is essential.

- Design tradeoffs: The paper uses LLMs for reference annotation and evaluation due to their scalability and correlation with human judgments. However, this introduces potential biases and limitations that need to be carefully considered and mitigated.

- Failure signatures: If the reference annotation pipeline fails to generate valid questions or cluster diverse answers, the dataset will lack the necessary diversity to support the task. Similarly, if the evaluation protocols exhibit significant biases or low correlation with human judgments, the findings may not be reliable.

- First 3 experiments:
  1. Test the reference annotation pipeline on a small sample of news stories to ensure it generates valid questions and clusters diverse answers.
  2. Conduct a pilot human evaluation of the generated summaries to assess the effectiveness of the evaluation protocols and identify any potential issues.
  3. Analyze the correlation between GPT-4 evaluations and human judgments on a subset of the data to validate the reliability of the LLM-based evaluation.

## Open Questions the Paper Calls Out

- **Question**: How can the proposed task of Multi-document Diversity Summarization (MDDS) be adapted to other domains beyond news articles, such as scientific papers or legal documents?
- **Basis in paper**: [inferred] The paper focuses on news articles, but the concept of summarizing diverse information from multiple sources could be applicable to other domains.
- **Why unresolved**: The paper does not explore the application of MDDS to other domains, leaving open the question of how the task and dataset creation methods would need to be modified for different types of documents.
- **What evidence would resolve it**: Experiments applying MDDS to other domains, along with an analysis of the necessary adaptations to the data collection and evaluation methods.

- **Question**: What are the limitations of using LLMs as evaluators for the MDDS task, and how can these limitations be addressed?
- **Basis in paper**: [explicit] The paper identifies biases in GPT-4-based evaluation protocols, such as position bias and verbosity bias, and suggests best practices to mitigate these issues.
- **Why unresolved**: While the paper provides recommendations for using GPT-4 as an evaluator, it does not fully explore the limitations of LLM-based evaluation or propose alternative evaluation methods.
- **What evidence would resolve it**: A comprehensive analysis of LLM-based evaluation limitations, along with the development and validation of alternative evaluation methods that address these limitations.

- **Question**: How does the performance of LLMs on the MDDS task change when the number of input articles is increased beyond 10?
- **Basis in paper**: [explicit] The paper sets the number of input articles at 10 for the MDDS task, but does not explore the impact of varying this number.
- **Why unresolved**: The paper does not investigate how the performance of LLMs on the MDDS task changes with a different number of input articles, leaving open the question of whether there is an optimal number of articles for this task.
- **What evidence would resolve it**: Experiments varying the number of input articles and analyzing the impact on LLM performance for the MDDS task.

## Limitations

- The dataset focuses exclusively on news articles, limiting generalizability to other domains where diversity manifests differently
- GPT-4's coverage of less than 40% diverse information suggests significant challenges, but results may be model-specific
- The 245-story dataset size may limit statistical power for some analyses and bias assessments

## Confidence

- Reference annotation pipeline: Medium
- GPT-4 coverage finding (<40%): Low
- Bias analysis validity: Medium
- Dataset robustness: Low

## Next Checks

1. Replicate the reference annotation pipeline on a small subset of news stories to verify question generation quality and answer clustering effectiveness.

2. Conduct inter-annotator reliability tests on the human evaluation interfaces to ensure consistent assessment of faithfulness and coverage.

3. Test the evaluation protocols across different LLM models (beyond GPT-4) to assess whether observed biases and correlation patterns are model-specific or general phenomena.