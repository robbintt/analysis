---
ver: rpa2
title: 'Let the Models Respond: Interpreting Language Model Detoxification Through
  the Lens of Prompt Dependence'
arxiv_id: '2309.00751'
source_url: https://arxiv.org/abs/2309.00751
tags:
- language
- detoxification
- association
- prompt
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding how detoxification
  techniques impact the internal mechanisms of language models. The authors apply
  feature attribution methods to quantify changes in prompt dependence after detoxification
  via fine-tuning and reinforcement learning.
---

# Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence

## Quick Facts
- **arXiv ID**: 2309.00751
- **Source URL**: https://arxiv.org/abs/2309.00751
- **Reference count**: 8
- **Primary result**: Counter-narrative fine-tuning encourages more uniform prompt attribution compared to RLHF, with FT slightly outperforming RL in reducing toxic responses.

## Executive Summary
This paper investigates how different detoxification techniques impact language models' reliance on prompt information when generating responses. The authors apply feature attribution methods to quantify changes in prompt dependence after detoxification via fine-tuning and reinforcement learning. They find that counter-narrative fine-tuning leads to more uniform allocation of importance across prompt tokens, while RLHF does not significantly alter the original attribution distribution. The study shows that fine-tuning slightly outperforms RLHF in reducing toxic responses and provides insights into the differences in prompt reliance between the two methods despite their similar detoxification performances.

## Method Summary
The authors fine-tune RedPajama 3B and Falcon 7B models using both counter-narrative fine-tuning and reinforcement learning from human feedback approaches on the DIALOCONAN dataset. They evaluate the pre- and post-detoxification models on the RealToxicityPrompts dataset using PerspectiveAPI for toxicity scoring. Feature attribution methods are applied to quantify changes in prompt dependence, measuring attribution entropy across generated tokens to understand how models rely on different parts of the prompt during toxic response generation.

## Key Results
- Counter-narrative fine-tuning encourages more uniform allocation of importance on prompt tokens compared to RLHF
- RLHF does not noticeably affect the original attribution distribution of base models
- Fine-tuning slightly outperforms RLHF in reducing toxic responses despite similar overall detoxification performance
- Sharp entropy increases in prompt dependence correlate with locations of toxic keywords in model generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature attribution entropy can reveal differences in prompt dependence between fine-tuning and reinforcement learning detoxification
- Mechanism: By measuring how importance is distributed across prompt tokens during generation, we can quantify how much models rely on the prompt for toxic output
- Core assumption: Feature attribution methods like gradients accurately capture prompt importance in transformer-based language models
- Evidence anchors: "quantify their impact on the resulting models' prompt dependence using feature attribution methods"
- Break condition: If feature attribution methods fail to capture true prompt importance, or if the entropy measure doesn't correlate with actual toxic generation patterns

### Mechanism 2
- Claim: Counter-narrative fine-tuning leads to more uniform prompt attribution distributions compared to reinforcement learning
- Mechanism: Counter-narrative fine-tuning explicitly trains models to generate helpful responses that address toxic prompts, requiring distributing attention across the entire prompt
- Core assumption: Counter-narrative training objectives inherently encourage broader prompt engagement than RLHF objectives
- Evidence anchors: "counter-narrative fine-tuning encourages a more uniform allocation of importance on the prompt"
- Break condition: If the observed attribution patterns are due to factors unrelated to the training objective

### Mechanism 3
- Claim: The location of toxic keywords in model generations correlates with sharp prompt dependence patterns
- Mechanism: When models generate toxic content, they show sharp increases in prompt dependence entropy, suggesting specific prompt elements strongly condition the toxic generation
- Core assumption: Toxic generations have identifiable patterns in their feature attribution that differ from non-toxic generations
- Evidence anchors: "We generally observe a steep entropy increase for non-FT models after the first few generated tokens"
- Break condition: If toxic and non-toxic generations show similar attribution patterns

## Foundational Learning

- **Feature attribution methods**: Techniques like integrated gradients and saliency maps that quantify input importance
  - Why needed: To measure how much models rely on different parts of the prompt when generating responses
  - Quick check: What is the difference between gradient-based and perturbation-based feature attribution methods?

- **Reinforcement learning from human feedback (RLHF)**: Training approach using human preference data as reward signal
  - Why needed: The paper compares this detoxification approach with fine-tuning
  - Quick check: How does RLHF differ from standard supervised fine-tuning in terms of training objective?

- **Text toxicity detection**: Automated systems for identifying harmful content in text
  - Why needed: The study evaluates detoxification effectiveness using toxicity scores
  - Quick check: What are the limitations of using automated toxicity detection systems for evaluating model safety?

## Architecture Onboarding

- **Component map**: Language models (RedPajama 3B, Falcon 7B) -> Low-rank adapters -> Feature attribution pipeline -> Toxicity evaluation -> Interpretation
- **Critical path**: Data -> Model preparation -> Detoxification training -> Feature attribution analysis -> Toxicity evaluation -> Interpretation
- **Design tradeoffs**: Fine-tuning vs. RLHF (computational cost vs. alignment quality); attribution method choice (efficiency vs. accuracy)
- **Failure signatures**: Attribution entropy doesn't change post-detoxification; toxicity scores don't decrease despite attribution changes; high variance in attribution patterns
- **First 3 experiments**: 1) Run feature attribution on base models to establish baseline patterns; 2) Apply counter-narrative fine-tuning and measure changes; 3) Apply RLHF and compare attribution changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of counter-narrative fine-tuning compare to reinforcement learning when applied to larger language models (e.g., GPT-3.5, GPT-4)?
- Basis: The paper evaluates detoxification techniques on two models (RedPajama 3B and Falcon 7B) and finds counter-narrative fine-tuning slightly outperforms reinforcement learning
- Why unresolved: The study is limited to smaller models, and it's unclear if these findings generalize to larger models with different architectures
- What evidence would resolve it: Comparative experiments applying both techniques to larger models, measuring both detoxification effectiveness and impact on prompt dependence

### Open Question 2
- Question: Can the entropy-based attention regularization technique be effectively used to accelerate the detoxification process for language models?
- Basis: The paper suggests that entropy increase in attribution scores could indicate a sharp conditioning applied by specific prompt elements, potentially making it a target for regularization
- Why unresolved: The paper only observes this phenomenon but does not test the effectiveness of entropy-based attention regularization
- What evidence would resolve it: Experiments applying entropy-based attention regularization during detoxification and measuring its impact

### Open Question 3
- Question: How does the impact of detoxification techniques on prompt dependence vary across different types of toxic content (e.g., hate speech, profanity, misinformation)?
- Basis: The paper focuses on general toxic content but doesn't analyze variations in detoxification effectiveness for different types of toxicity
- Why unresolved: The study uses a general toxicity metric without differentiating between types of toxic content
- What evidence would resolve it: Comparative analysis of detoxification effectiveness and prompt dependence changes for different categories of toxic content

## Limitations
- The study focuses on two specific model sizes (3B and 7B parameters) which may not capture full spectrum of model behaviors
- Limited to one particular dataset for detoxification, potentially affecting generalizability
- Establishes correlations between attribution entropy patterns and detoxification methods but doesn't definitively prove causation
- Does not differentiate between types of toxic content in the analysis

## Confidence

- **High confidence**: Comparative effectiveness of counter-narrative fine-tuning versus RLHF in reducing toxic outputs
- **Medium confidence**: Interpretation of feature attribution entropy as a measure of prompt dependence
- **Medium confidence**: Specific claim that counter-narrative fine-tuning creates more uniform prompt attribution distributions

## Next Checks

1. Test whether observed attribution entropy patterns persist across different base model architectures (e.g., Llama, Mistral) and scales
2. Conduct ablation studies that isolate effects of training data versus training objective by comparing models trained on identical datasets with different detoxification approaches
3. Validate that attribution entropy patterns correlate with actual changes in toxic generation through human evaluation studies connecting technical metrics to perceived model safety