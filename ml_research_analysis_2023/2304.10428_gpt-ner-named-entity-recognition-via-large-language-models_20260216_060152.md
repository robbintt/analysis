---
ver: rpa2
title: 'GPT-NER: Named Entity Recognition via Large Language Models'
arxiv_id: '2304.10428'
source_url: https://arxiv.org/abs/2304.10428
tags:
- output
- input
- gpt-3
- task
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GPT-NER adapts large language models to named entity recognition
  by reformulating the sequence labeling task as a text-generation task using special
  tokens to mark entities, and addresses hallucination via self-verification. Experiments
  on five NER datasets show performance comparable to fully supervised baselines,
  with significantly better results in low-resource and few-shot settings, demonstrating
  strong real-world applicability when labeled data is scarce.
---

# GPT-NER: Named Entity Recognition via Large Language Models

## Quick Facts
- arXiv ID: 2304.10428
- Source URL: https://arxiv.org/abs/2304.10428
- Reference count: 15
- Key outcome: GPT-NER adapts LLMs to NER by reformulating sequence labeling as text generation using special tokens, achieving performance comparable to supervised baselines and significantly better results in low-resource/few-shot settings.

## Executive Summary
GPT-NER bridges the gap between large language models and named entity recognition by transforming the sequence labeling task into a text generation task. The method uses special tokens (`@@entity##`) to mark entities in generated text, enabling LLMs to naturally produce labeled outputs. To address LLM hallucination tendencies, GPT-NER implements a self-verification strategy that prompts the model to validate extracted entities. Experiments on five NER datasets demonstrate performance comparable to fully supervised baselines, with particularly strong results in low-resource and few-shot scenarios where labeled data is scarce.

## Method Summary
GPT-NER reformulates NER as a text generation task using special tokens to mark entities, addressing the format mismatch between sequence labeling and LLM generation. The method employs entity-level kNN retrieval to find semantically relevant demonstrations, constructs prompts with task descriptions and demonstrations, and uses GPT-3 to generate labeled outputs. A self-verification step filters out hallucinated entities by prompting the LLM to validate extracted entities. The approach is evaluated across five datasets (CoNLL2003, OntoNotes5.0, ACE2004, ACE2005, GENIA) using precision, recall, and F1-score metrics.

## Key Results
- GPT-NER achieves performance comparable to fully supervised baselines on standard NER datasets
- Significant performance gains in low-resource and few-shot settings where training data is extremely limited
- Self-verification strategy effectively reduces LLM hallucination while maintaining entity extraction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance improves when NER task is reformulated as text generation with special tokens.
- Mechanism: Using `@@entity##` markers reduces generation complexity by avoiding token-by-token label alignment.
- Core assumption: LLMs generate coherent spans more easily than per-token labels.
- Evidence anchors:
  - [abstract] "GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text Columbus is a city is transformed to generate the text sequence @@Columbus## is a city, where special tokens @@## marks the entity to extract."
  - [section 4.1.2] "To resolve this issue, we propose the LLM output takes the following format: if the input sequence does not contain any entity, W just copies the input X; for an entity/entities in the input sequence, we use special tokens @@ and ## to surround it/them."
- Break condition: Special tokens become ambiguous with existing text patterns; or model learns to ignore markers during generation.

### Mechanism 2
- Claim: kNN-based demonstration retrieval using entity-level embeddings outperforms random and sentence-level retrieval.
- Mechanism: Retrieves semantically relevant examples at token level, aligning with NER's local evidence needs.
- Core assumption: NER depends more on token/entity similarity than full-sentence semantic similarity.
- Evidence anchors:
  - [abstract] "we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models."
  - [section 4.2.2] "To resolve the issue above, we need to retrieve kNN examples based on token-level representations rather than sentence-level representations. We first extract entity-level representations for all tokens of all training examples as the datastore using a fine-tuned NER tagging model."
- Break condition: Entity-level embeddings fail to capture domain-specific entity types; or retrieval cost outweighs benefits.

### Mechanism 3
- Claim: Self-verification reduces LLM hallucination in NER by filtering overpredicted entities.
- Mechanism: Post-extraction verification step prompts LLM to validate extracted entity labels, countering overconfidence.
- Core assumption: LLMs benefit from explicit self-consistency checks for high-stakes decisions.
- Evidence anchors:
  - [abstract] "To efficiently address the hallucination issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag."
  - [section 4.3] "To address this issue, we propose the self-verification strategy. Given an extracted entity by LLMs, we ask the LLM to further verify whether the extracted entity is correct, answered by yes or no."
- Break condition: Verification step introduces latency or false negatives; or LLM fails to understand verification prompt.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: GPT-NER relies on ICL to adapt LLMs to NER without fine-tuning.
  - Quick check question: What are the limitations of ICL compared to fine-tuning in terms of task performance and resource constraints?

- Concept: Token-level vs sentence-level similarity in retrieval
  - Why needed here: kNN retrieval performance depends on choosing the right embedding granularity.
  - Quick check question: Why might sentence-level embeddings fail to retrieve helpful NER demonstrations?

- Concept: Sequence labeling vs text generation task formats
  - Why needed here: NER is sequence labeling, LLMs are generative; bridging this gap is central to GPT-NER.
  - Quick check question: How does the `@@entity##` format simplify the alignment problem compared to BMES?

## Architecture Onboarding

- Component map:
  Prompt Construction -> LLM Generation -> Self-Verification -> Output Parsing
  (Optional: kNN Retrieval Module for entity-level or sentence-level retrieval)

- Critical path:
  1. Build entity-level embedding datastore from training set.
  2. For each input sentence and entity type:
     - Retrieve kNN demonstrations.
     - Construct prompt with task description, demos, and input.
     - Generate labeled sentence.
     - Run self-verification on each extracted entity.
  3. Aggregate verified entities as final NER output.

- Design tradeoffs:
  - Retrieval granularity (entity-level vs sentence-level) vs retrieval speed.
  - Prompt length limit (4096 tokens) vs number/quality of demonstrations.
  - Zero-shot vs few-shot vs self-verification vs performance vs latency.

- Failure signatures:
  - Low precision: LLM overpredicts entities → check self-verification effectiveness.
  - Low recall: LLM underpredicts entities → check demonstration relevance and prompt clarity.
  - Slow inference: Long retrieval or large kNN → optimize datastore or reduce k.

- First 3 experiments:
  1. Compare entity-level vs sentence-level kNN retrieval on a small CoNLL sample.
  2. Test impact of removing self-verification on precision/recall trade-off.
  3. Vary the number of demonstrations (k) and measure F1 plateau behavior.

## Open Questions the Paper Calls Out

- What is the maximum number of demonstrations GPT-NER can use before performance plateaus, and how does this change with different LLMs (e.g., GPT-4)?
- How does GPT-NER's performance degrade when applied to languages other than English, especially those with different scripts or morphological structures?
- What is the computational efficiency of GPT-NER compared to traditional supervised NER models, particularly in terms of inference time and resource usage?

## Limitations

- Dependency on fine-tuned BERT-based model for entity-level embeddings introduces reproducibility challenges
- Self-verification mechanism adds inference latency and may reduce recall through false negatives
- Scalability to nested NER with overlapping entities remains unproven
- Reliance on GPT-3 API limits practical deployment due to costs and rate limits

## Confidence

- **High confidence**: The core mechanism of transforming NER into text generation using special tokens is well-supported by experimental results
- **Medium confidence**: Entity-level kNN retrieval's superiority depends on fine-tuned model quality and may not generalize across domains
- **Medium confidence**: Self-verification effectively reduces hallucination but impact on precision-recall trade-offs needs more quantification
- **Low confidence**: Claims about superior few-shot performance are based on limited dataset experiments without zero-shot testing

## Next Checks

1. **Ablation study on kNN retrieval granularity**: Compare entity-level vs sentence-level kNN retrieval on a small sample of CoNLL and OntoNotes data, measuring F1-score and inference time to quantify the trade-off between retrieval quality and computational cost.

2. **Self-verification impact analysis**: Remove the self-verification step from GPT-NER and measure changes in precision, recall, and hallucination rate on the test sets. Analyze whether the drop in precision is offset by improved recall or if hallucination increases significantly.

3. **Scalability to nested NER**: Test GPT-NER on a nested NER dataset (e.g., ACE2005) with multiple overlapping entities, evaluating whether the `@@##` format can accurately capture nested spans and whether self-verification handles ambiguous cases.