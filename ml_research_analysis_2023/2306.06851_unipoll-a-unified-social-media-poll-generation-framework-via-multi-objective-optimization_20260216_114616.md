---
ver: rpa2
title: 'UniPoll: A Unified Social Media Poll Generation Framework via Multi-Objective
  Optimization'
arxiv_id: '2306.06851'
source_url: https://arxiv.org/abs/2306.06851
tags:
- generation
- question
- unipoll
- poll
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UniPoll, a unified framework for automatically
  generating social media polls from posts. UniPoll uses multi-objective optimization
  and a pre-trained encoder-decoder model to generate questions and answers by learning
  context-poll, context-question, and context-answer relations.
---

# UniPoll: A Unified Social Media Poll Generation Framework via Multi-Objective Optimization

## Quick Facts
- arXiv ID: 2306.06851
- Source URL: https://arxiv.org/abs/2306.06851
- Reference count: 40
- Key outcome: UniPoll achieves ROUGE-1 scores of 47.92 and 46.24 for question and answer generation respectively on social media poll generation tasks

## Executive Summary
This paper introduces UniPoll, a unified framework for automatically generating social media polls from posts using multi-objective optimization. The framework employs a pre-trained encoder-decoder model (T5) with prompt tuning to generate questions and answers by learning context-poll, context-question, and context-answer relations. UniPoll is evaluated on Chinese Weibo and English Reddit datasets, outperforming previous state-of-the-art models in both automatic and human evaluations. The study demonstrates the importance of incorporating comments as context and shows how multi-objective optimization can effectively handle the challenges of noisy social media data.

## Method Summary
UniPoll uses a T5 encoder-decoder backbone with multi-objective optimization to jointly learn context-poll, context-question, and context-answer relations. The framework employs prompt tuning with task-specific prefixes to differentiate between generating questions, answers, and complete polls while sharing model parameters. Comments are incorporated as enriched context to reveal implicit "poll-worthy" points. The model is trained using a weighted combination of main task loss (context-poll generation) and auxiliary task losses (context-question and context-answer generation), optimizing for both overall poll generation and the individual components.

## Key Results
- UniPoll outperforms previous state-of-the-art models on Chinese Weibo and English Reddit datasets
- Achieves ROUGE-1 scores of 47.92 for question generation and 46.24 for answer generation
- Demonstrates significant improvements in human evaluation metrics for relevance, fluency, and engagingness
- Shows that incorporating comments as context significantly improves poll generation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective optimization helps balance question and answer generation by jointly optimizing their losses rather than training separate models.
- Mechanism: By combining main task loss (context-poll) with auxiliary task losses (context-question, context-answer) using weighted sum, the model learns inter-task dependencies and avoids overfitting to one objective.
- Core assumption: Question-answer consistency improves when both are trained together with shared encoder representations.
- Evidence anchors:
  - [abstract]: "UniPoll employs prompt tuning with multi-objective optimization to bolster the connection exploration between contexts (posts and comments) and polls (questions and answers)."
  - [section]: "Our training engages multiple tasks, including the main task to learn context-poll relations and the auxiliary tasks to explore context-question and context-answer relations."
  - [corpus]: Weak - corpus neighbors discuss engagement prediction and federated learning, not multi-objective optimization for poll generation.

### Mechanism 2
- Claim: Prompt tuning with task-specific prefixes allows the model to differentiate between generating questions, answers, and polls while maintaining shared parameters.
- Mechanism: Special tokens like "<question>", "<answers>", and task prompts ("generate <question> then <answers>") guide the decoder to produce appropriate output formats for each task without requiring separate models.
- Core assumption: The pre-trained decoder can generalize task-specific generation when provided with explicit formatting cues.
- Evidence anchors:
  - [abstract]: "It employs prompt tuning with multi-objective optimization to bolster the connection exploration between contexts (posts and comments) and polls (questions and answers)."
  - [section]: "We designed two task prompts: T PQ 'generate <question>' and T PA 'generate <answers>', which correspond to QG and AG, respectively."
  - [corpus]: Weak - corpus neighbors focus on engagement prediction and federated learning, not prompt-based multi-task learning.

### Mechanism 3
- Claim: Incorporating comments as enriched context helps the model identify "poll-worthy" points that are implicit in the original post.
- Mechanism: Comments provide supplementary perspectives and keywords that reveal reader interests and potential discussion angles, making implicit context-question-answer relations more explicit.
- Core assumption: Comments contain valuable semantic signals about what aspects of a post would make engaging poll questions.
- Evidence anchors:
  - [abstract]: "To tackle these challenges, we enrich a post's context with its comments and propose a novel unified poll generation framework called UniPoll."
  - [section]: "Inspired by that, we enrich the post's context with its comments, which may reveal keywords indicative of the readers' possible interests related to the post."
  - [corpus]: Weak - corpus neighbors discuss engagement prediction but not the specific role of comments in poll generation.

## Foundational Learning

- Concept: Multi-task learning with shared representations
  - Why needed here: Poll generation requires simultaneous learning of context understanding, question generation, and answer generation, which are interdependent tasks.
  - Quick check question: Can you explain how sharing encoder parameters across tasks might help capture question-answer consistency?

- Concept: Prompt engineering for task differentiation
  - Why needed here: The model needs to generate different output formats (questions vs. answers vs. complete polls) using the same architecture.
  - Quick check question: How would you design task prefixes to help a model distinguish between generating a question and generating answer choices?

- Concept: Handling noisy social media data
  - Why needed here: Social media posts and comments contain informal language, implicit relations, and context that may not directly map to poll questions.
  - Quick check question: What preprocessing steps might help a model better understand implicit context-question-answer relations in social media data?

## Architecture Onboarding

- Component map: Encoder (T5 backbone) → Memory bank (context encoding) → Decoder (with task-specific prompts) → Output (poll/question/answers)
- Critical path: Context → Encoder → Memory bank → Decoder (prompt-guided) → Poll generation
- Design tradeoffs: Joint training improves consistency but adds complexity; using auxiliary tasks helps balance objectives but requires careful weight tuning
- Failure signatures: QA inconsistency, off-topic generation, duplicated answer choices, grammar errors in questions
- First 3 experiments:
  1. Train UniPoll with only main task (no auxiliary tasks) to establish baseline performance
  2. Add one auxiliary task (either QG or AG) to measure incremental improvement
  3. Vary task weights (γQ, γA) to find optimal balance between main and auxiliary objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the evaluation metrics for poll generation to better capture alternative but valid questions?
- Basis in paper: [explicit] The paper mentions that alternative questions are a significant challenge in question generation (12/100 errors) and that current evaluation metrics may indicate an error when the generated question is appropriate but not similar to the ground truth.
- Why unresolved: Current evaluation metrics like ROUGE and BLEU focus on surface-level similarity and may not capture the semantic validity of alternative questions.
- What evidence would resolve it: Development and validation of new evaluation metrics that can better assess the semantic quality and relevance of generated questions, even when they differ from the ground truth.

### Open Question 2
- Question: How can we effectively incorporate external knowledge to improve context encoding and reduce information insufficiency errors in answer generation?
- Basis in paper: [explicit] The paper mentions that information insufficiency errors (2/100) are caused by insufficient information available in short contexts and suggests that improving models' common sense reasoning capabilities or leveraging external knowledge could help.
- Why unresolved: Current models may struggle to generate complete and informative answers when the context is limited, and the paper suggests that external knowledge could be beneficial but doesn't provide a specific solution.
- What evidence would resolve it: Experiments comparing the performance of models with and without external knowledge incorporation in handling short and limited contexts.

### Open Question 3
- Question: How can we further reduce QA inconsistency errors in poll generation, especially when the context contains multiple topics?
- Basis in paper: [explicit] The paper mentions that QA inconsistency errors (8/100) still exist despite the multi-objective optimization approach and suggests that limited poll samples to learn question-answer consistency may be a reason.
- Why unresolved: The current approach may not be sufficient to fully capture the complex relationships between questions and answers, especially in cases with multiple potential topics in the context.
- What evidence would resolve it: Experiments evaluating the effectiveness of data augmentation techniques or more advanced modeling approaches in improving question-answer consistency across diverse contexts.

## Limitations
- The paper introduces a new English RedditPolls dataset but lacks detailed statistics about dataset quality and size
- Performance comparison relies heavily on automatic metrics without sufficient ablation studies to isolate individual component contributions
- Prompt templates and task weight tuning process are not fully specified, making exact replication challenging

## Confidence

- **High Confidence**: The core architecture of using multi-objective optimization with T5 backbone and the effectiveness of incorporating comments as context. The reported ROUGE-1 scores of 47.92 and 46.24 for question and answer generation are specific and verifiable.

- **Medium Confidence**: The claim that multi-objective optimization significantly improves over single-task training, as the paper does not provide direct comparison with baseline models trained without auxiliary tasks. The human evaluation results showing superior relevance, fluency, and engagingness are promising but based on limited sample size.

- **Low Confidence**: The generalizability of results to other social media platforms beyond Weibo and Reddit, and the scalability of the approach to longer posts or more complex comment threads, as these scenarios are not explicitly tested.

## Next Checks

1. **Ablation Study**: Train UniPoll variants with only main task, only auxiliary tasks, and various combinations to quantify the individual contribution of each component to the overall performance.

2. **Cross-Platform Evaluation**: Test UniPoll on a third social media platform (e.g., Twitter or Facebook) to assess generalization beyond Chinese and English datasets, measuring performance degradation or improvement.

3. **Prompt Template Analysis**: Systematically vary the prompt templates and task weight configurations (γQ, γA) to identify optimal settings and understand the sensitivity of the model to these hyperparameters.