---
ver: rpa2
title: 'DelucionQA: Detecting Hallucinations in Domain-specific Question Answering'
arxiv_id: '2312.05200'
source_url: https://arxiv.org/abs/2312.05200
tags:
- answer
- retrieval
- question
- context
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in retrieval-augmented
  question answering (QA), where large language models (LLMs) generate non-factual
  responses despite access to relevant context. The authors introduce DelucionQA,
  a dataset of 2,038 examples from car manual QA that captures hallucination cases
  in retrieval-augmented LLM outputs.
---

# DelucionQA: Detecting Hallucinations in Domain-specific Question Answering

## Quick Facts
- **arXiv ID:** 2312.05200
- **Source URL:** https://arxiv.org/abs/2312.05200
- **Reference count:** 12
- **Primary result:** Introduces DelucionQA dataset with 2,038 examples for hallucination detection in domain-specific QA, achieving 70-71% Macro F1 with similarity-based methods

## Executive Summary
This paper addresses the critical problem of hallucination in retrieval-augmented question answering (QA), where large language models (LLMs) generate non-factual responses despite access to relevant context. The authors introduce DelucionQA, a specialized dataset of 2,038 examples from car manual QA that captures hallucination cases in retrieval-augmented LLM outputs. They propose two baseline hallucination detection methods: a similarity-based approach using sentence embeddings and overlap ratios, and a keyword-based method. The similarity-based methods achieve 70-71% Macro F1 on the test set, while the keyword-based method achieves 53%, demonstrating that simple keyword matching is insufficient for hallucination detection and more sophisticated similarity measures are needed.

## Method Summary
The paper proposes a comprehensive approach to detecting hallucinations in domain-specific QA by first creating a specialized dataset (DelucionQA) through question generation, multiple retrieval methods, answer generation using ChatGPT, and human annotation. The authors then develop two baseline hallucination detection methods: a similarity-based approach that compares generated answer sentences against context sentences using sentence embeddings and overlap ratios, and a keyword-based method that checks if answer keywords exist in the retrieval context. The similarity-based methods achieve 70-71% Macro F1 on the test set, while the keyword-based method achieves 53%, demonstrating that simple keyword matching is insufficient for hallucination detection and more sophisticated similarity measures are needed.

## Key Results
- Similarity-based methods achieve 70-71% Macro F1 on the test set, outperforming keyword-based approaches
- Keyword-based method achieves only 53% Macro F1, indicating its insufficiency for hallucination detection
- The dataset reveals that hallucination can occur due to insufficient context, excessive context, or LLM over-reliance on pre-trained knowledge
- The best-performing baseline achieves only 71.09% Macro F1, suggesting substantial room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level similarity detection effectively identifies hallucinated content by comparing generated answer sentences against context sentences.
- Mechanism: The system calculates two types of similarity measures - sentence-embedding-based similarity and sentence overlapping ratio. If an answer sentence fails both similarity checks against all context sentences, it is flagged as hallucinated.
- Core assumption: Hallucinated content lacks semantic and syntactic similarity to any context sentence, making it detectable through comprehensive similarity matching.
- Evidence anchors:
  - [abstract] "We propose a set of hallucination detection methods to serve as baselines for future works from the research community."
  - [section] "The sentence-embedding-based similarity calculates the cosine similarity of the embedding vectors generated by a language model for each of the two sentences in focus."
  - [corpus] Weak - only 5 related papers found with no citations, suggesting limited prior work on this specific mechanism.
- Break condition: This mechanism fails when answer sentences contain paraphrases or information that is semantically similar but syntactically different from context, or when the LLM generates content that is plausible but factually incorrect.

### Mechanism 2
- Claim: Keyword extraction-based detection identifies hallucinations by checking if answer keywords exist in the retrieval context.
- Mechanism: The system extracts keywords from the generated answer and checks if a significant portion of these keywords are present in the retrieval context. If many keywords are missing from the context, the answer is flagged as hallucinated.
- Core assumption: Generated answers should primarily contain keywords that are grounded in the retrieval context, and hallucinated content introduces keywords not present in the context.
- Evidence anchors:
  - [abstract] "Furthermore, we propose a set of hallucination detection methods to serve as baselines for future works from the research community."
  - [section] "Here, the main idea of this approach is that given an answer generated by an LLM, if a significant portion of the keywords in the answer does not exist in the retrieved context, the LLM is deemed to have hallucinated the answer."
  - [corpus] Weak - only 5 related papers found with no citations, suggesting limited prior work on this specific mechanism.
- Break condition: This mechanism fails when the answer legitimately uses synonyms or related concepts not explicitly present in the context, or when the context is relevant but uses different terminology than the answer.

### Mechanism 3
- Claim: Combining multiple retrieval methods reduces hallucination by providing more comprehensive context coverage.
- Mechanism: The system employs multiple retrieval strategies (sparse retrieval, ensemble retrieval, adaptive ensemble retrieval) to ensure relevant context is captured from different angles, reducing the likelihood of the LLM generating hallucinated content.
- Core assumption: Different retrieval methods capture different aspects of relevant information, and combining them provides a more complete context that reduces the LLM's reliance on pre-trained knowledge.
- Evidence anchors:
  - [abstract] "The amount of hallucination can be reduced by leveraging information retrieval to provide relevant background information to the LLM."
  - [section] "We hypothesize that the context retrieved will be more relevant to the question by retrieving with more sophisticated methods, which meanwhile makes the answer less likely to contain hallucinated content."
  - [corpus] Weak - only 5 related papers found with no citations, suggesting limited prior work on this specific mechanism.
- Break condition: This mechanism fails when the retrieval methods themselves are flawed or when the LLM over-relies on pre-trained knowledge regardless of context quality.

## Foundational Learning

- Concept: Cosine similarity and vector embeddings
  - Why needed here: The sentence-embedding-based similarity measure relies on calculating cosine similarity between vector representations of sentences to determine semantic similarity.
  - Quick check question: Can you explain how cosine similarity between two vectors indicates semantic similarity, and what range of values indicates high similarity?

- Concept: Dynamic programming for sequence alignment
  - Why needed here: The sentence overlapping ratio method uses dynamic programming to find optimal alignment paths between sentences to calculate overlap length, particularly useful for detecting partial similarities.
  - Quick check question: How does dynamic programming help in finding the optimal overlap between two sentences, and why is this important for detecting partial similarities?

- Concept: Keyword extraction and frequency analysis
  - Why needed here: The keyword-based detection method requires understanding how to extract meaningful keywords from text and analyze their frequency distribution in context.
  - Quick check question: What are the key considerations when extracting keywords from generated text, and how do you determine if a keyword is "significant" for hallucination detection?

## Architecture Onboarding

- Component map: Question Generation -> Context Retrieval (4 methods) -> Answer Generation -> Human Annotation -> Hallucination Detection
- Critical path: Question → Context Retrieval (4 methods) → Answer Generation → Human Annotation → Hallucination Detection
- Design tradeoffs: Simple keyword matching (fast, low accuracy) vs. sophisticated similarity measures (slower, higher accuracy); single retrieval method (efficient, limited coverage) vs. multiple methods (comprehensive, resource-intensive).
- Failure signatures: High false positives when context contains different terminology; high false negatives when answer contains plausible but incorrect information; performance degradation with longer contexts; sensitivity to threshold selection.
- First 3 experiments:
  1. Test similarity-based methods on a small subset with known hallucinations to validate threshold tuning
  2. Compare keyword-based vs. similarity-based methods on the development set to confirm performance differences
  3. Analyze failure cases where both methods miss hallucinations to identify common patterns and improve detection logic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hallucination detection methods be improved to achieve higher accuracy in domain-specific QA tasks?
- Basis in paper: Explicit - The authors report a best-performing baseline with only 71.09% Macro F1 on the test set, indicating substantial room for improvement.
- Why unresolved: The current methods, including similarity-based and keyword-based approaches, show limitations in capturing one-to-many or many-to-one mappings between answer and context sentences, and simple keyword matching is insufficient.
- What evidence would resolve it: Developing and testing new hallucination detection methods that can better handle complex mappings and semantic relationships between answers and contexts, potentially incorporating more advanced NLP techniques or multimodal approaches.

### Open Question 2
- Question: What are the most effective strategies to prevent LLM hallucination when retrieval results contain excessive or irrelevant information?
- Basis in paper: Inferred - The authors note that even when retrieval results include the correct context, LLMs may still hallucinate due to additional information in the retrieval results, as seen in their case study.
- Why unresolved: The paper demonstrates that simply improving retrieval methods may not guarantee solving the hallucination problem, as the LLM's tendency to rely on pre-learned knowledge can still lead to hallucination.
- What evidence would resolve it: Experiments comparing different strategies such as advanced filtering of retrieval results, context summarization, or prompt engineering techniques to guide LLM focus on relevant information.

### Open Question 3
- Question: How do different LLM architectures and training approaches affect hallucination rates in retrieval-augmented QA systems?
- Basis in paper: Explicit - The authors note that their dataset contains answers generated by ChatGPT, and it's unclear whether methods developed on this dataset will generalize to other LLMs.
- Why unresolved: The paper focuses on a single LLM (ChatGPT), leaving open questions about how other models might perform and whether different architectures would be more or less prone to hallucination.
- What evidence would resolve it: Comparative studies across multiple LLM architectures (e.g., GPT, Claude, LLaMA) in the same domain-specific QA task, measuring hallucination rates and effectiveness of detection methods across models.

## Limitations
- Weak corpus signals with only 5 related papers found and no citations, suggesting limited benchmarking against existing techniques
- Domain-specific dataset (car manuals) restricts generalizability to other QA scenarios
- Threshold selection process for detection methods lacks transparency in the paper

## Confidence
- **High confidence** in the experimental methodology and evaluation metrics used
- **Medium confidence** in the claimed superiority of similarity-based methods over keyword-based approaches
- **Low confidence** in the generalizability of findings beyond the car manual domain

## Next Checks
1. Replicate the experiment using a more diverse dataset from multiple domains to assess generalizability
2. Compare the proposed methods against established hallucination detection techniques from related papers in the corpus
3. Conduct ablation studies to determine the impact of different threshold values on detection performance and identify optimal settings