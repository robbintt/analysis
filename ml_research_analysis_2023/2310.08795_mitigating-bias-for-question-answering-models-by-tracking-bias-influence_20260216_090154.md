---
ver: rpa2
title: Mitigating Bias for Question Answering Models by Tracking Bias Influence
arxiv_id: '2310.08795'
source_url: https://arxiv.org/abs/2310.08795
tags:
- bias
- instance
- instances
- answer
- mitigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of social bias in multiple-choice
  question answering (QA) models. The authors propose BMBI, a bias mitigation method
  that tracks bias influence through in-context learning.
---

# Mitigating Bias for Question Answering Models by Tracking Bias Influence

## Quick Facts
- arXiv ID: 2310.08795
- Source URL: https://arxiv.org/abs/2310.08795
- Reference count: 26
- This paper proposes BMBI, a method that tracks bias influence through in-context learning to mitigate social bias in multiple-choice QA models, achieving significant bias reduction while maintaining or improving accuracy.

## Executive Summary
This paper addresses social bias in multiple-choice question answering models by proposing BMBI, a novel bias mitigation method that tracks bias influence through in-context learning. The method detects bias by observing how a query instance influences the prediction of a ruler instance, and uses this detected bias level as an additional training objective in a multi-task learning setup. Experiments on the BBQ dataset demonstrate that BMBI significantly reduces bias magnitude across all 9 bias categories while maintaining or improving QA accuracy, outperforming or matching a counterfactual data augmentation baseline.

## Method Summary
BMBI uses in-context learning to detect bias by measuring how a query instance influences a ruler instance's predictions. The method samples reference instances (neutral and ruler) from the BBQ dataset to create bias axes. For each query instance, bias level is detected by comparing the ruler's prediction distribution when influenced by the query versus the neutral instance. This bias level is then used as an additional loss in a multi-task learning framework, where the model is fine-tuned on both the original QA task and the bias mitigation task. The approach works for both classification-based and generation-based QA models without requiring instance-level bias annotations.

## Key Results
- BMBI significantly reduces bias magnitude across all 9 bias categories on the BBQ dataset
- The method maintains or improves QA accuracy while reducing bias
- BMBI outperforms or matches a counterfactual data augmentation baseline
- The approach generalizes across bias categories without relying on explicit attribute words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns bias patterns through in-context learning by observing how a query instance influences a ruler instance's predictions.
- Mechanism: By concatenating verbalized query instance with the input of ruler instance, the model replicates behavior shown in context, allowing influence of query to be traced through ruler's prediction distribution.
- Core assumption: Language models can learn and simulate behavior from demonstration instances provided in context.
- Evidence anchors:
  - [abstract]: "we measure the bias level of a query instance by observing its influence on another instance"
  - [section]: "we use the influence by concatenating verbalized query instance with the input of ruler instance following the in-context learning paradigm"
- Break condition: If the model cannot effectively learn from context examples or if the ruler instance is not sensitive enough to detect bias influence.

### Mechanism 2
- Claim: The bias detection module can identify bias levels without instance-level annotations by using a small set of reference instances.
- Mechanism: Reference instances (neutral and ruler) create bias axes. Ruler instance defines bias direction, neutral instance provides baseline. Difference in predictions when influenced by query versus neutral indicates bias level.
- Core assumption: A small set of carefully designed reference instances can represent bias axes for various categories.
- Evidence anchors:
  - [abstract]: "the bias detection module enables us to obtain bias level estimation without the need for instance-level bias annotation in an unsupervised manner"
  - [section]: "Using different ruler instances targeting different protected groups and underlying bias reasons, our framework enables us to reflect the bias of multiple perspectives"
- Break condition: If reference instances are not representative or if bias axes are too complex to be captured by reference set.

### Mechanism 3
- Claim: Multi-task learning with an additional bias mitigation loss reduces bias while maintaining QA performance.
- Mechanism: Detected bias level is used as optimization objective. Model is fine-tuned on both original QA task and bias mitigation task iteratively.
- Core assumption: Bias level signal is strong enough to guide model toward less biased predictions without degrading ability to answer questions correctly.
- Evidence anchors:
  - [abstract]: "we use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task"
  - [section]: "To avoid the performance decay on the original QA task while mitigating the bias, we perform multi-task learning"
- Break condition: If bias mitigation loss interferes with QA task or if bias level signal is not accurate enough.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Method relies on model's ability to learn from context examples without explicit training on bias labels.
  - Quick check question: Can the model replicate behavior shown in demonstration instances provided in context?

- Concept: Bias axes and protected groups
  - Why needed here: Understanding how bias is measured and what constitutes protected group is essential for designing reference instances and interpreting bias levels.
  - Quick check question: How are bias axes defined for different bias categories, and how do they relate to answer candidates in ruler instances?

- Concept: Multi-task learning
  - Why needed here: Method combines original QA task with bias mitigation task to ensure reducing bias doesn't harm model's performance.
  - Quick check question: How does model balance two objectives during training, and what is impact on both tasks?

## Architecture Onboarding

- Component map: Base QA model -> Bias detection module -> Bias mitigation module -> Reference dataset (neutral and ruler instances)

- Critical path:
  1. Sample reference instances from BBQ dataset
  2. For each query instance, detect bias level using bias detection module
  3. Use detected bias level as additional loss in multi-task learning setup
  4. Fine-tune model on both QA task and bias mitigation task
  5. Evaluate model's performance on BBQ dataset

- Design tradeoffs:
  - Using small set of reference instances reduces need for extensive annotations but may limit method's ability to capture all possible bias scenarios
  - Multi-task learning approach aims to balance bias mitigation and QA performance, but balance may vary across different bias categories

- Failure signatures:
  - If bias level detection is inaccurate, bias mitigation may not be effective or may even increase bias
  - If multi-task learning is not properly balanced, model may prioritize one task over other, leading to degraded performance on neglected task

- First 3 experiments:
  1. Evaluate bias detection module's accuracy in identifying bias levels across different bias categories using BBQ dataset
  2. Test bias mitigation module's effectiveness in reducing bias while maintaining or improving QA accuracy on BBQ dataset
  3. Analyze impact of different reference instance selections on bias mitigation results to ensure robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of reference instances needed to achieve effective bias mitigation across different bias categories?
- Basis in paper: [explicit] The paper states that "We show that 5 pairs of reference instances are good enough to mitigate the bias significantly" but acknowledges this may not be optimal for all categories.
- Why unresolved: Paper only tested 5 pairs of reference instances and did not explore whether fewer or more instances would yield better results or be more efficient.
- What evidence would resolve it: Systematic experiments varying number of reference instances from 1-10+ for each bias category to identify minimum effective number while maintaining bias mitigation performance.

### Open Question 2
- Question: How does the proposed bias mitigation method generalize to languages other than English?
- Basis in paper: [inferred] The paper explicitly states "We focus on an English monolingual setup" and does not test cross-linguistic generalization.
- Why unresolved: Method relies on verbalizing query instances and using specific bias categories, which may not translate directly to other languages with different linguistic structures and cultural contexts.
- What evidence would resolve it: Applying BMBI to multiple languages with different grammatical structures and evaluating its effectiveness using comparable bias evaluation datasets.

### Open Question 3
- Question: What is the impact of bias mitigation on long-term model performance and generalization to unseen data?
- Basis in paper: [inferred] The paper only evaluates bias mitigation on BBQ dataset and does not examine how bias-mitigated models perform on other datasets or over extended periods.
- Why unresolved: Study focuses on immediate performance on BBQ dataset without considering whether bias mitigation might negatively affect model's ability to learn new information or generalize to different domains.
- What evidence would resolve it: Longitudinal studies tracking model performance on diverse QA datasets before and after bias mitigation, and examining how quickly bias might re-emerge during continued training on new data.

## Limitations
- The method's effectiveness depends heavily on the quality and representativeness of reference instances, which may not capture all possible bias scenarios
- Evaluation is limited to the BBQ dataset, leaving open questions about generalization to other QA benchmarks or real-world scenarios
- The approach requires careful balancing in multi-task learning to avoid degrading QA performance while mitigating bias

## Confidence
- High Confidence: Experimental results showing reduced bias magnitude on BBQ across all 9 categories are well-supported by presented data
- Medium Confidence: Claim that bias can be detected without instance-level annotations through in-context learning is supported but relies on assumptions about model's ability to learn from context
- Medium Confidence: Generalizability of method to different QA model architectures is demonstrated but not extensively explored across diverse datasets

## Next Checks
1. Evaluate BMBI on additional QA benchmarks beyond BBQ to test robustness across different domains and bias distributions
2. Conduct ablation study on reference instances by systematically varying number and selection of neutral/ruler instances to quantify their impact on bias detection accuracy and mitigation effectiveness
3. Assess whether bias mitigation persists after fine-tuning on other tasks or during continued training, checking for potential bias regression over time