---
ver: rpa2
title: Teaching Arithmetic to Small Transformers
arxiv_id: '2307.03381'
source_url: https://arxiv.org/abs/2307.03381
tags:
- addition
- digit
- data
- scratchpad
- digits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how small transformer models can learn arithmetic
  operations using the next-token prediction objective. The authors find that conventional
  data formatting is suboptimal for learning arithmetic, and that reversing the output
  or using chain-of-thought style data with intermediate steps significantly improves
  accuracy and sample efficiency.
---

# Teaching Arithmetic to Small Transformers

## Quick Facts
- arXiv ID: 2307.03381
- Source URL: https://arxiv.org/abs/2307.03381
- Reference count: 40
- Key outcome: Small transformers can learn arithmetic operations more effectively using reversed output formatting and chain-of-thought data with intermediate steps, improving both accuracy and sample efficiency.

## Executive Summary
This paper investigates how small transformer models can learn arithmetic operations through next-token prediction. The authors find that conventional data formatting is suboptimal because it forces the model to generate the most significant digit first, requiring global information. By reversing the output order (generating least significant digit first) and using chain-of-thought style data with intermediate steps, the model can learn simpler local functions instead of complex global ones. These approaches significantly improve accuracy and sample efficiency, even in the complete absence of pretraining. The study also demonstrates that balancing training data and using few-shot prompting further enhance performance.

## Method Summary
The authors train small transformer models (NanoGPT, GPT-2, GPT-3) from random initialization using next-token prediction loss on arithmetic examples. They experiment with different data formats including plain (MSB-first), reversed (LSB-first), simplified scratchpad, and detailed scratchpad with intermediate steps. Datasets are balanced in terms of digit lengths and carry operations. The models use character-level tokenization and absolute positional encoding. Training is performed with AdamW optimizer and evaluated on held-out examples. The study systematically compares performance across data formats, sampling strategies, and model scales.

## Key Results
- Reversed output formatting (LSB-first) significantly improves accuracy by enabling the model to learn local digit-wise functions instead of complex global ones
- Chain-of-thought data with intermediate steps dramatically improves sample efficiency and convergence speed
- Balanced sampling prevents overfitting to common patterns and ensures robust learning across different arithmetic scenarios
- The model struggles with length generalization, failing to perform addition on digits it has not been trained on

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reversing the output order allows the model to learn a simpler, local function instead of a complex global one.
- Mechanism: In next-token prediction, generating the least significant digit first means each step only depends on the current digit pair and carry, not on all digits globally.
- Core assumption: The model can learn the standard digit-wise addition algorithm when prompted to produce digits from least to most significant.
- Evidence anchors:
  - [abstract] "we then explore the potential benefits of chain-of-thought (CoT) data during training" and "reversing the output or using chain-of-thought style data with intermediate steps significantly improves accuracy and sample efficiency"
  - [section] "By training on samples with reversed results, i.e., 'A3A2A1 + B3B1B1 = C1C2C3', we enable the model to learn a simpler function"
  - [corpus] Weak — no corpus neighbor directly discusses reversing outputs for transformers.
- Break condition: If the model must output the most significant digit first, it cannot avoid learning a global function.

### Mechanism 2
- Claim: Chain-of-thought formatting dramatically improves sample efficiency by decomposing arithmetic into simpler intermediate steps.
- Mechanism: Providing step-by-step intermediate results (digit sums, carries) lets the model learn a higher-dimensional but easier-to-learn function map, reducing complexity per step.
- Core assumption: The model can generalize from learning each intermediate step to composing them into the full operation.
- Evidence anchors:
  - [abstract] "Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed."
  - [section] "This format includes step-by-step operations and intermediate results, allowing the model to learn the individual components of complex tasks."
  - [corpus] Moderate — "Compositional Capabilities of Autoregressive Transformers" discusses decomposition of tasks.
- Break condition: If intermediate steps are noisy or overly complex, the benefit disappears or reverses.

### Mechanism 3
- Claim: Balanced data sampling prevents the model from overfitting to common patterns and ensures robust learning.
- Mechanism: By equalizing the number of samples per digit length and per carry count, the model sees a uniform distribution of addition scenarios, avoiding skew toward certain patterns.
- Core assumption: Transformers can learn uniformly from balanced datasets rather than being biased by the natural skew of random sampling.
- Evidence anchors:
  - [abstract] "Additionally, we find that balancing the training data and using few-shot prompting"
  - [section] "we employ a structured sampling approach...balance digits by assigning higher weights to lower-digit numbers...balance carry-ons by ensuring an equal distribution"
  - [corpus] Weak — no corpus neighbor directly discusses balanced sampling for transformers.
- Break condition: If the model is forced to rely on memorization of frequent patterns, balancing becomes irrelevant.

## Foundational Learning

- Concept: Low-rank matrix completion connection
  - Why needed here: Explains why sharp phase transitions occur as training data increases — the addition map is equivalent to completing a rank-2 matrix.
  - Quick check question: If addition is rank-2, how many revealed entries are theoretically needed to guarantee completion?

- Concept: Next-token prediction objective bias
  - Why needed here: Shows why plain formatting is suboptimal — the model is forced to output the most significant digit first, requiring global information.
  - Quick check question: Why does reversing output order align better with the autoregressive generation order?

- Concept: Chain-of-thought decomposition
  - Why needed here: Demonstrates how breaking complex operations into simpler sub-steps improves learning efficiency and accuracy.
  - Quick check question: What happens if intermediate steps are noisy or incorrect?

## Architecture Onboarding

- Component map: Data formatting -> Balanced sampling -> Model training -> Evaluation
- Critical path: Data formatting → balanced sampling → model training → evaluation on held-out examples
- Design tradeoffs: Token efficiency vs. sample efficiency (scratchpad formats use more tokens but require fewer samples); model scale vs. pretraining needs
- Failure signatures: Poor accuracy on unseen digit lengths; inability to generalize beyond trained sequence lengths; catastrophic forgetting when fine-tuning
- First 3 experiments:
  1. Train on plain 3-digit addition vs. reverse 3-digit addition, compare accuracy and sample efficiency
  2. Add balanced sampling to 3-digit addition, observe improvement
  3. Introduce simplified scratchpad formatting, measure reduction in required samples for 100% accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum model scale required for small transformers to reliably learn arithmetic operations without pretraining?
- Basis in paper: [explicit] The authors explore scaling from 10.6M to 124M parameters and find that scale aids learning but is not strictly necessary.
- Why unresolved: The study did not test larger models like GPT-3/4 without pretraining, leaving uncertainty about the minimum effective scale.
- What evidence would resolve it: Systematic experiments training models of various scales (e.g., 1B, 10B parameters) from scratch on arithmetic tasks.

### Open Question 2
- Question: How does the complexity of intermediate steps in chain-of-thought data affect learning efficiency for different arithmetic operations?
- Basis in paper: [explicit] The authors find detailed scratchpad improves sample efficiency but not necessarily token efficiency, and CoT is less effective for sine and square root due to complex intermediate steps.
- Why unresolved: The paper does not provide a systematic analysis of how varying the complexity of intermediate steps impacts learning across operations.
- What evidence would resolve it: Controlled experiments varying the complexity of intermediate steps in CoT data and measuring learning efficiency for each arithmetic operation.

### Open Question 3
- Question: What data formatting strategies can improve length generalization in arithmetic learning?
- Basis in paper: [inferred] The authors find models struggle with length generalization, suggesting current formatting methods do not promote algorithmic understanding.
- Why unresolved: The paper does not explore alternative data formatting approaches that might encourage length generalization.
- What evidence would resolve it: Experiments testing novel data formats designed to explicitly encourage length generalization, such as curriculum learning or positional encoding schemes.

## Limitations
- The experiments are limited to small, randomly initialized transformers without pretraining, making it unclear whether findings generalize to larger, pretrained models
- The study focuses on synthetic arithmetic tasks, which may not reflect the complexity of real-world mathematical reasoning
- The paper does not investigate whether the benefits of reversed output and chain-of-thought formatting persist when scaling to longer sequences or more complex operations

## Confidence

**High Confidence**: The empirical results showing improved accuracy and sample efficiency with reversed output formatting and balanced data sampling. The authors provide clear experimental evidence and ablation studies supporting these claims.

**Medium Confidence**: The theoretical interpretation of why reversed output works better (local vs. global function learning) and the connection to low-rank matrix completion. While plausible, these explanations are somewhat speculative and could benefit from more rigorous mathematical analysis.

**Low Confidence**: The generalizability of findings to larger models and real-world applications. The experiments are conducted on small, randomly initialized transformers, making it difficult to extrapolate to production-scale systems.

## Next Checks

1. **Scale Validation**: Replicate the core experiments (plain vs. reversed output, with/without balanced sampling) on a pretrained GPT-2 or GPT-3 model to test whether the observed benefits persist at scale.

2. **Generalization Test**: Train on addition up to 5 digits and evaluate performance on 6-8 digit addition to quantify the model's ability to generalize beyond its training distribution.

3. **Alternative Task Test**: Apply the same data formatting techniques (reversed output, chain-of-thought) to a different mathematical operation like multiplication or exponentiation to verify whether the benefits are task-specific or generalizable.