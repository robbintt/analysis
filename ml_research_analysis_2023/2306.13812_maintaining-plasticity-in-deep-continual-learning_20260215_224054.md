---
ver: rpa2
title: Maintaining Plasticity in Deep Continual Learning
arxiv_id: '2306.13812'
source_url: https://arxiv.org/abs/2306.13812
tags:
- learning
- plasticity
- continual
- loss
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that modern deep learning systems suffer
  from a loss of plasticity in continual learning settings, meaning they lose their
  ability to adapt to new data over time. The authors show this loss of plasticity
  occurs with various architectures, optimizers, and activation functions on ImageNet,
  MNIST, and regression tasks.
---

# Maintaining Plasticity in Deep Continual Learning

## Quick Facts
- arXiv ID: 2306.13812
- Source URL: https://arxiv.org/abs/2306.13812
- Reference count: 15
- One-line primary result: Continual backpropagation maintains 90% accuracy on ImageNet after 5000 tasks versus 77% for standard backpropagation

## Executive Summary
This paper identifies and addresses a critical limitation in deep continual learning: the loss of plasticity, where neural networks progressively lose their ability to adapt to new data over time. The authors demonstrate this phenomenon occurs across various architectures, optimizers, and tasks, showing it's not merely catastrophic forgetting but a more fundamental degradation of learning capacity. To address this, they propose continual backpropagation, which selectively reinitializes low-utility units during training. This approach maintains network plasticity indefinitely and significantly outperforms existing methods on ImageNet, MNIST, and regression tasks, with continual backpropagation maintaining 90% accuracy after 5000 tasks while standard backpropagation drops to 77%.

## Method Summary
The method involves standard forward and backward passes with Adam optimization, but adds a utility computation step after each update. The algorithm calculates contribution-utility (based on output magnitude and outgoing weights) and adaptation-utility (based on input weight magnitude) for each hidden unit. Units with low combined utility are selectively reinitialized with small random weights, while their outgoing weights are reset to zero. New units are protected from replacement for a maturity threshold period. This process injects fresh random weights into the network while preserving learned functionality, preventing the weight magnitude growth and effective rank reduction that characterize loss of plasticity.

## Key Results
- Continual backpropagation maintained 90% accuracy on ImageNet after 5000 tasks, while standard backpropagation dropped to 77%
- The approach successfully mitigated all three correlates of loss of plasticity: eliminated dead units, prevented weight magnitude growth, and maintained high effective rank
- Commonly used techniques like Adam, dropout, and normalization can worsen loss of plasticity, contrary to their benefits in stationary learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective reinitialization of low-utility units maintains diversity in neural network representations over time
- Mechanism: The continual backpropagation algorithm identifies and replaces units with low contribution-utility (based on magnitude of output and outgoing weights) and low adaptation-utility (based on small input weights). This injects new random weights into the network while preserving learned functionality.
- Core assumption: Unit utility can be effectively measured by the product of contribution-utility and adaptation-utility, where contribution-utility captures the unit's impact on downstream neurons and adaptation-utility captures how quickly the unit can change.
- Evidence anchors:
  - [abstract] "continual backpropagation extends the initialization to all time steps, by selectively reinitializing a small fraction of low-utility hidden units at every presentation of a training example"
  - [section 7] "Continual backpropagation selectively reinitializes low-utility units in the network... After each task, the learning algorithm finds a low-rank solution for the current task, which then serves as the initialization for the next task"
  - [corpus] Weak - no direct citations found supporting this specific utility measure
- Break condition: If the utility measure fails to identify truly low-utility units, the network may either waste resources replacing useful units or fail to replace truly useless ones, negating the benefits.

### Mechanism 2
- Claim: Maintaining small weight magnitudes prevents the network from entering a low-rank, saturated state
- Mechanism: The algorithm's reinitialization process continually resets weights to small random values, preventing the exponential growth of weight magnitudes that occurs during standard backpropagation. This keeps the network in a high-rank, diverse state.
- Core assumption: Large weight magnitudes are correlated with loss of plasticity because they lead to saturated activations and reduced representational diversity.
- Evidence anchors:
  - [section 5] "we found that the degradation of online classification accuracy of backpropagation observed in Figure 3 was associated with an increase in the average magnitude of the weights"
  - [section 7] "Continual backpropagation... mitigated all three correlates of loss of plasticity. It had almost no dead units, stopped the network weights from growing, and maintained a high effective rank across tasks"
  - [corpus] Weak - no direct citations found supporting this specific correlation
- Break condition: If the network encounters tasks requiring very large weights to solve, the constraint on weight magnitude could prevent effective learning.

### Mechanism 3
- Claim: Continuous injection of random units prevents the network from getting trapped in local minima
- Mechanism: By continually adding new random units, the algorithm creates a dynamic architecture that can escape suboptimal configurations. The new units provide fresh pathways for learning while mature units (protected for a threshold period) ensure stability.
- Core assumption: The initial random weight distribution contains useful starting points for learning that are gradually lost through standard training, and this diversity can be maintained by periodic reinitialization.
- Evidence anchors:
  - [section 7] "Continual backpropagation makes conventional backpropagation continual by performing similar computations at all times"
  - [section 7] "Continual backpropagation selectively reinitializes low-utility units in the network... Continual backpropagation extends the initialization to all time steps"
  - [corpus] Weak - no direct citations found supporting this specific escape mechanism
- Break condition: If the replacement rate is too high, the network may never stabilize on useful representations; if too low, it may not escape local minima effectively.

## Foundational Learning

- Concept: Catastrophic forgetting - the phenomenon where neural networks lose previously learned information when trained on new tasks
  - Why needed here: Understanding catastrophic forgetting is essential context for grasping why loss of plasticity is a distinct but related problem that requires different solutions
  - Quick check question: What is the key difference between catastrophic forgetting and loss of plasticity as described in the paper?

- Concept: Effective rank - a measure of the number of linearly independent dimensions in a matrix representation
  - Why needed here: The paper uses effective rank as a key metric to quantify the diversity of neural network representations and how it degrades over time
  - Quick check question: How does the effective rank of a neural network's representation change as it experiences loss of plasticity?

- Concept: Gradient-based optimization and its limitations in non-stationary environments
  - Why needed here: The paper critiques standard backpropagation for its failure to adapt in continual learning settings, requiring understanding of how gradient descent behaves in these scenarios
  - Quick check question: Why does standard backpropagation struggle to maintain plasticity in continual learning environments?

## Architecture Onboarding

- Component map:
  - Forward pass: Standard neural network computation
  - Backward pass: Standard backpropagation with Adam optimizer
  - Utility computation: Calculate contribution-utility and adaptation-utility for each unit
  - Selective reinitialization: Replace low-utility units with random initialization
  - Protection mechanism: New units are protected from replacement for a maturity threshold

- Critical path:
  1. Forward pass to compute predictions
  2. Backward pass to compute gradients
  3. Update weights using Adam
  4. Compute utilities for all hidden units
  5. Identify low-utility units (below replacement threshold)
  6. Replace selected units with new random initialization
  7. Reset outgoing weights of new units to zero
  8. Protect new units from replacement for maturity threshold steps

- Design tradeoffs:
  - Replacement rate vs. stability: Higher replacement rates maintain more plasticity but risk destabilizing learned representations
  - Utility measure complexity vs. computational cost: More sophisticated utility measures may improve performance but increase computation per update
  - Maturity threshold length vs. adaptability: Longer thresholds provide more stability but slower adaptation to new patterns

- Failure signatures:
  - Performance degradation over time similar to standard backpropagation
  - Oscillating performance if replacement rate is too high
  - Failure to adapt if replacement rate is too low
  - Computational slowdown if utility computation is inefficient

- First 3 experiments:
  1. Implement the utility computation module and verify it correctly identifies dead units and units with small input weights
  2. Test the reinitialization mechanism on a simple network with known dead units to ensure they are replaced
  3. Run a small-scale version of Online Permuted MNIST to verify the algorithm maintains performance over multiple tasks

## Open Questions the Paper Calls Out

- Question: What is the optimal utility measure for continual backpropagation that could replace the heuristic-based approach?
  - Basis in paper: [explicit] The paper acknowledges its utility measure is heuristic-based and suggests developing more principled approaches.
  - Why unresolved: The paper uses a specific utility measure based on contribution and adaptation utilities but doesn't explore alternatives or prove optimality.
  - What evidence would resolve it: Development and validation of utility measures derived from theoretical principles (e.g., information theory, control theory) that outperform the heuristic approach across diverse continual learning problems.

- Question: How does the effectiveness of continual backpropagation vary across different types of non-stationary data distributions beyond permuted MNIST and slowly-changing regression?
  - Basis in paper: [inferred] The paper demonstrates effectiveness on permuted MNIST and slowly-changing regression but doesn't explore other distribution types.
  - Why unresolved: The experiments focus on specific synthetic non-stationary problems. Real-world data streams may have different characteristics (e.g., gradual vs abrupt changes, cyclical patterns) that could affect performance.
  - What evidence would resolve it: Systematic testing of continual backpropagation on diverse real-world datasets with various types of non-stationarity (e.g., weather data, financial data, sensor data) showing consistent performance improvements over baseline methods.

- Question: What is the theoretical relationship between loss of plasticity and the lottery ticket hypothesis in deep networks?
  - Basis in paper: [explicit] The paper mentions this connection in the discussion section, noting that reduced randomness in trained networks might mean fewer "winning tickets."
  - Why unresolved: The paper only suggests this connection as a hypothesis without empirical investigation or theoretical analysis.
  - What evidence would resolve it: Empirical studies showing correlation between network plasticity over time and the number/density of lottery tickets, plus theoretical analysis explaining the mechanistic connection.

## Limitations
- The computational overhead of continual utility computation and unit replacement could become prohibitive for larger networks or more complex tasks
- The effectiveness of selective reinitialization depends critically on the utility measure's ability to identify truly low-utility units, which isn't extensively validated across diverse scenarios
- The paper demonstrates effectiveness on synthetic tasks but real-world data streams may have different characteristics that could affect performance

## Confidence
- High confidence: The empirical observation that weight magnitudes increase and effective rank decreases during standard continual learning
- Medium confidence: The proposed continual backpropagation algorithm effectively maintains plasticity based on the presented experiments
- Medium confidence: The relationship between dead units, weight magnitude growth, and loss of plasticity as general correlates

## Next Checks
1. **Utility Measure Robustness**: Test the utility measure's effectiveness across different network architectures (RNNs, transformers) and tasks beyond image classification to validate its general applicability
2. **Computational Overhead Analysis**: Measure and compare the wall-clock time and memory requirements of continual backpropagation versus standard methods, particularly for larger networks
3. **Ablation Studies**: Systematically vary the replacement rate, maturity threshold, and utility computation parameters to identify optimal settings and understand sensitivity to hyperparameters