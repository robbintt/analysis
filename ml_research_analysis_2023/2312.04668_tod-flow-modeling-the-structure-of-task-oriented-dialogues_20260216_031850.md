---
ver: rpa2
title: 'TOD-Flow: Modeling the Structure of Task-Oriented Dialogues'
arxiv_id: '2312.04668'
source_url: https://arxiv.org/abs/2312.04668
tags:
- system
- dialog
- graph
- user
- tod-flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TOD-Flow, a novel approach for inferring the
  underlying task structure in task-oriented dialogues using a TOD-Flow graph. The
  graph learns what a model can, should, and should not predict, effectively reducing
  the search space and providing a rationale for the model's prediction.
---

# TOD-Flow: Modeling the Structure of Task-Oriented Dialogues

## Quick Facts
- **arXiv ID:** 2312.04668
- **Source URL:** https://arxiv.org/abs/2312.04668
- **Reference count:** 16
- **Primary result:** TOD-Flow improves next system action prediction F1 by up to 33.2% on SGD and 13.4% on MultiWOZ

## Executive Summary
TOD-Flow introduces a novel approach for inferring the underlying task structure in task-oriented dialogues through a graph-based representation that learns what actions a model can, should, and should not predict. The inferred TOD-Flow graph reduces the search space for valid next actions by encoding causal constraints from dialogue trajectories, even when annotations are noisy or incomplete. This graph can be seamlessly integrated with any dialogue model to improve prediction performance, transparency, and controllability, with experiments showing significant improvements in both dialog act classification and end-to-end response generation.

## Method Summary
The TOD-Flow approach infers task structure from dialogue trajectories using decision tree classifiers to learn three relationships (Can, Should, ShouldNot) between dialog acts without direct supervision. The graph is trained by maximizing conditional likelihoods from observed trajectories, capturing causal dependencies even in noisy annotations. For prediction, the graph conditions dialogue models by filtering, adding, and removing candidate actions based on the learned constraints. In end-to-end response generation, multiple beam-search responses are annotated with dialog acts via few-shot prompted LLMs, then ranked by graph compliance scores without retraining the base model.

## Key Results
- TOD-Flow improves average F1 score of next system action prediction by up to 33.2% on SGD and 13.4% on MultiWOZ
- The approach significantly improves combined score of response generation by up to 1.24 on MultiWOZ
- TOD-Flow graphs better resemble human-annotated graphs compared to prior approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inferring the TOD-Flow graph without direct supervision captures the underlying task structure from noisy dialogue annotations.
- **Mechanism:** The graph learns three relationships (can, should, should not) between dialog acts by maximizing conditional likelihoods from observed trajectories. This reduces the search space for valid next actions by encoding causal constraints.
- **Core assumption:** Dialogue trajectories implicitly encode the true dependency structure, even when annotations are incomplete or noisy.
- **Evidence anchors:** [abstract] "Our TOD-Flow graph learns what a model can, should, and should not predict..."; [section 3.3] Objectives JShd, JShdnt, JCan maximize true positives; [corpus] Weak: neighbor papers focus on annotation schemes, not unsupervised graph inference.

### Mechanism 2
- **Claim:** Conditioning a dialogue model with the inferred graph improves prediction accuracy by filtering, adding, and removing candidate actions.
- **Mechanism:** The graph defines admissible actions (Can), required actions (Shd), and prohibited actions (Shdnt) given the current completion vector. The base model's predictions are post-processed to enforce these constraints before ranking.
- **Core assumption:** The base model can generate a diverse set of candidates, some of which violate the true constraints; the graph can identify and correct these.
- **Evidence anchors:** [abstract] "Our TOD-Flow graph can be easily integrated with any dialogue model..."; [section 4.1] Algorithm 1 shows explicit filtering (Can ∧ ¬Shdnt) and addition (Shd); [section 5.4] Table 2 shows consistent F1 improvements.

### Mechanism 3
- **Claim:** Using the graph for candidate selection in end-to-end response generation improves task success metrics without retraining the base model.
- **Mechanism:** Multiple beam-search responses are annotated with dialog acts by a few-shot prompted LLM, then ranked by the graph's compliance score (violation rate). The best response is selected without modifying the generator.
- **Core assumption:** The few-shot annotated dialog acts are sufficiently accurate to compute meaningful compliance scores, and the base model's candidates include at least one valid response.
- **Evidence anchors:** [abstract] "when combined with several dialogue policies and end-to-end dialogue models..."; [section 4.2] Describes annotation and graph-based selection; [section 5.5] Table 4 shows Info and Succ metrics improve.

## Foundational Learning

- **Concept: Boolean function representation of task dependencies**
  - Why needed here: The TOD-Flow graph encodes causal relationships as Boolean functions over completion vectors, enabling efficient inference and integration with dialogue models.
  - Quick check question: Given subtasks A and B, write the Boolean function for "C can only happen if A and B are done" and its equivalent graph edges.

- **Concept: Behavioral cloning vs. constraint-based learning**
  - Why needed here: BC directly mimics demonstrations, while TOD-Flow learns constraints (Can/Shd/Shdnt) that generalize better to unseen states.
  - Quick check question: If a demonstration shows "BOOK after INFORM_PRICE", what would BC learn vs. what would TOD-Flow infer about the relationship?

- **Concept: Graph-conditioned policy improvement**
  - Why needed here: Post-processing base model predictions with the inferred graph enforces valid action sequences without retraining.
  - Quick check question: If the base model predicts {BOOK, CONFIRM}, and the graph says BOOK is not allowed now, what is the final predicted action set?

## Architecture Onboarding

- **Component map:** Data preprocessing → trajectory construction → Graph inference module → Base dialogue model → Graph conditioning layer → Evaluation
- **Critical path:** 1. Build trajectories from annotated dialogues. 2. Train three decision trees (Can, Shd, Shdnt) on (completion, action) pairs. 3. Sample candidates from base model. 4. Apply graph constraints (Algorithm 1 for actions, GPT-NLU + ranking for responses). 5. Output final prediction.
- **Design tradeoffs:** Decision tree depth vs. overfitting; number of sampled candidates vs. diversity; few-shot NLU accuracy vs. graph ranking quality.
- **Failure signatures:** High variance in decision tree predictions, low training F1; selected actions violate graph constraints or are empty sets; BLEU improves but Info/Succ do not.
- **First 3 experiments:** 1. Train Can/Shd/Shdnt trees on a small domain, inspect learned rules vs. ground truth. 2. Run base model alone vs. with graph conditioning on validation set, compare F1. 3. Generate 5 responses from base model, annotate with GPT, rank by graph, check if top improves over greedy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How well do the inferred TOD-Flow graphs generalize to unseen domains or domain combinations?
- **Basis in paper:** [inferred] The paper mentions limitations in handling multiple domains and unseen domain combinations.
- **Why unresolved:** The paper only tested the approach on single domains and did not explore its performance on unseen domain combinations.
- **What evidence would resolve it:** Experiments evaluating the approach on dialogs involving multiple domains, including unseen combinations, and comparing its performance to baselines.

### Open Question 2
- **Question:** Can the TOD-Flow graphs be inferred from raw, unannotated dialogue data?
- **Basis in paper:** [explicit] The paper mentions that it relies on action annotations from the datasets to infer graphs, which limits its applicability.
- **Why unresolved:** The paper does not explore the possibility of inferring graphs from raw dialogue data without annotations.
- **What evidence would resolve it:** Experiments demonstrating the approach's ability to infer graphs from raw dialogue data and comparing its performance to using annotated data.

### Open Question 3
- **Question:** How does the quality of the NLU process affect the performance of the TOD-Flow approach?
- **Basis in paper:** [explicit] The paper mentions that the NLU process has an average F1 score of 77.6% when predicting actions from ground truth responses, which is not perfect.
- **Why unresolved:** The paper does not explore the impact of NLU quality on the overall performance of the TOD-Flow approach.
- **What evidence would resolve it:** Experiments varying the quality of the NLU process and measuring its impact on the performance of the TOD-Flow approach in downstream tasks like next action prediction and response generation.

## Limitations
- Graph inference quality degrades with high annotation noise, potentially overfitting to spurious patterns
- Performance depends on base model's ability to generate diverse, partially valid candidate sets
- Few-shot NLU annotation accuracy is critical but uncertain for complex dialog act types

## Confidence

**Mechanism 1:** Medium - Unsupervised inference relies on noisy annotations, ablation studies on noise levels are missing
**Mechanism 2:** High - Post-processing approach is straightforward with robust improvements across multiple models
**Mechanism 3:** Medium - Few-shot annotation accuracy is critical and may not scale to complex dialog acts

## Next Checks

1. **Ablation study on annotation noise:** Systematically vary the noise level in training annotations and measure how graph inference quality and downstream performance degrade.
2. **Base model candidate diversity analysis:** For each dataset, measure the overlap between the base model's candidate sets and the graph's Can constraints.
3. **Few-shot NLU accuracy benchmarking:** Evaluate the annotation accuracy of the few-shot prompted LLM across different dialog act types and complexity levels.