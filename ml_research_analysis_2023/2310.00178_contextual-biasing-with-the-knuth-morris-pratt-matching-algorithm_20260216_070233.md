---
ver: rpa2
title: Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm
arxiv_id: '2310.00178'
source_url: https://arxiv.org/abs/2310.00178
tags:
- biasing
- matching
- search
- phrases
- match
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes efficient on-TPU algorithms for contextual
  biasing in ASR, based on the Knuth-Morris-Pratt algorithm for pattern matching.
  The approach efficiently maintains status of matching with biasing phrases during
  beam search and assigns biasing bonuses to token extensions.
---

# Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm

## Quick Facts
- arXiv ID: 2310.00178
- Source URL: https://arxiv.org/abs/2310.00178
- Reference count: 6
- Key outcome: Efficient KMP-based contextual biasing achieves significant WER reductions on biasing test sets

## Executive Summary
This paper presents an efficient on-device algorithm for contextual biasing in automatic speech recognition (ASR) based on the Knuth-Morris-Pratt (KMP) string matching algorithm. The approach integrates seamlessly with beam search decoding, tracking partial matches to biasing phrases and assigning linear bonus scores to token extensions. The method is evaluated on TPU hardware and demonstrates substantial WER improvements compared to baseline ASR models, with further gains when combined with model-based biasing techniques.

## Method Summary
The paper proposes a KMP-based contextual biasing approach that operates efficiently during beam search decoding. The method precomputes failure functions for biasing phrases and uses vectorized forward functions to track partial matches incrementally. Two integration strategies are explored: shallow fusion (biasing before pruning) and on-the-fly rescoring. The bonus computation uses a linear scoring function parameterized by δ, with additional hyperparameters F (expansion count) and λ (interpolation weight). The approach is implemented in TensorFlow and evaluated on TPU hardware.

## Key Results
- Shallow fusion with KMP biasing achieves 1.7% absolute WER reduction on Contact-Tag test set
- Optimal performance requires tuning δ to balance biasing effectiveness with general accuracy
- Shallow fusion consistently outperforms on-the-fly rescoring, especially with larger F values
- Combined KMP and model-based biasing yields additional WER improvements

## Why This Works (Mechanism)

### Mechanism 1
Matching biasing phrases incrementally during beam search and assigning bonus scores based on partial match lengths improves recognition of rare contextual phrases. The algorithm tracks partial match lengths for each biasing phrase and awards a bonus equal to the difference in maximum phrase-match potential before and after consuming a token. This boosts hypotheses that are extending matches into biasing phrases, increasing their survival through beam pruning.

Core assumption: A linear per-token bonus per matched biasing phrase is sufficient to guide beam search toward correct biasing phrase recognition without degrading general performance.

### Mechanism 2
Using the failure function from the KMP algorithm allows efficient partial match tracking across multiple biasing phrases without backtracking in the input token sequence. The failure function encodes, for each partial match length, the next possible match position if a mismatch occurs, avoiding re-scanning of the token sequence.

Core assumption: The failure function can be precomputed and stored efficiently for thousands of biasing phrases without excessive memory cost.

### Mechanism 3
Shallow fusion integration yields better WER reduction than on-the-fly rescoring because biasing bonuses are incorporated earlier in the decoding process. In shallow fusion, biasing bonuses are computed for top expansions before pruning, directly influencing which hypotheses survive.

Core assumption: The biasing model scores are sufficiently reliable to guide pruning decisions effectively when integrated early in the search.

## Foundational Learning

- Concept: Knuth-Morris-Pratt string matching algorithm
  - Why needed here: It provides the theoretical foundation for efficient multi-pattern matching during beam search without backtracking in the input sequence.
  - Quick check question: What is the worst-case time complexity of KMP matching, and how does it compare to naive matching?

- Concept: Weighted Finite State Transducers (WFSTs) and their limitations on TPUs
  - Why needed here: Understanding why traditional WFST-based biasing is inefficient on TPUs motivates the KMP-based approach.
  - Quick check question: Why are FST adjacency matrices typically sparse, and how does this sparsity affect TPU efficiency?

- Concept: Beam search decoding in sequence-to-sequence models
  - Why needed here: The biasing method must integrate seamlessly with beam search mechanics, including hypothesis expansion, pruning, and score updates.
  - Quick check question: In beam search, what determines which hypotheses are expanded and which are pruned at each step?

## Architecture Onboarding

- Component map:
  Base ASR model (RNN-T) -> KMP failure function table (precomputed) -> Biasing bonus computation module (vectorized) -> Shallow fusion or on-the-fly rescoring integration point

- Critical path:
  1. Precompute failure functions for all biasing phrases
  2. During beam search, for each expansion token:
     - Run forward function on all biasing phrases in parallel
     - Compute bonus as potential difference
     - Add bonus to token score (shallow fusion) or hypothesis score (OTF rescoring)
  3. Prune and continue decoding

- Design tradeoffs:
  - Memory vs. speed: Precomputing failure functions saves time but uses O(m) memory per phrase
  - Accuracy vs. efficiency: Larger F in shallow fusion improves accuracy but increases computation
  - Model-agnosticism vs. specialization: KMP biasing works with any ASR model but may not exploit model-specific biasing capabilities

- Failure signatures:
  - Degraded general WER with high δ values
  - Memory overflow with very large biasing phrase sets
  - Computational bottleneck in vectorized forward function when γ is large

- First 3 experiments:
  1. Baseline: Run RNN-T with no biasing, measure WER on Contact-Tag set
  2. KMP shallow fusion with F=50, tune δ to minimize WER on With-Prefix set
  3. KMP OTF rescoring, compare WER and computation time against shallow fusion baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of KMP biasing scale when the number of biasing phrases exceeds 3,000? Are there specific thresholds where performance degradation becomes significant?

### Open Question 2
What is the optimal way to integrate external neural language models into the bonus computation function for improved biasing accuracy?

### Open Question 3
How does KMP biasing perform in streaming scenarios where prefixes and biasing phrases can arrive dynamically during inference?

## Limitations
- Performance evaluated only on Contact-Tag test set (7.6K utterances), limiting generalizability
- Computational efficiency gains demonstrated through theoretical analysis rather than detailed profiling
- No ablation study of individual components or comparison with alternative biasing methods
- Limited exploration of hyperparameter sensitivity and statistical significance testing

## Confidence
**High Confidence Claims:**
- KMP algorithm provides O(m+n) time complexity for pattern matching
- Shallow fusion integration yields better WER reduction than on-the-fly rescoring
- Failure function correctly encodes longest proper prefix that matches proper suffix

**Medium Confidence Claims:**
- Specific WER reduction values are reproducible with described methodology
- Linear bonus scoring function is optimal for guiding beam search
- Vectorized implementation provides stated computational advantages on TPUs

**Low Confidence Claims:**
- Approach will generalize equally well to languages other than English
- Memory overhead of precomputing failure functions is negligible in practice
- Performance gains will scale proportionally with larger biasing phrase sets

## Next Checks
1. **Robustness Across Domains**: Evaluate KMP-based biasing on diverse ASR test sets including conversational speech, accented speech, and noisy environments to verify generalization beyond the Contact-Tag domain.

2. **Failure Mode Characterization**: Systematically explore identified failure conditions by running controlled experiments with varying δ values, extremely long biasing phrases, and massive biasing phrase sets (e.g., 10K+ phrases).

3. **Ablation Study of Integration Points**: Perform comprehensive ablation analysis comparing shallow fusion, on-the-fly rescoring, and alternative integration strategies with detailed computational profiling.