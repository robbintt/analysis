---
ver: rpa2
title: Convolutional Neural Operators for robust and accurate learning of PDEs
arxiv_id: '2302.01178'
source_url: https://arxiv.org/abs/2302.01178
tags:
- operator
- neural
- functions
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Convolutional Neural Networks have been largely ignored for operator
  learning due to their inconsistency in function space. This paper demonstrates that
  by reinterpreting and adapting CNNs, they can be made suitable for learning operators
  between function spaces.
---

# Convolutional Neural Operators for robust and accurate learning of PDEs

## Quick Facts
- arXiv ID: 2302.01178
- Source URL: https://arxiv.org/abs/2302.01178
- Reference count: 19
- Primary result: CNOs outperform standard CNNs and other operator learning models (FNO, DeepONet) on Navier-Stokes benchmarks by a factor of almost 4, with superior resolution invariance.

## Executive Summary
Convolutional Neural Networks have been largely ignored for operator learning due to their inconsistency in function space. This paper demonstrates that by reinterpreting and adapting CNNs, they can be made suitable for learning operators between function spaces. The resulting architecture, termed Convolutional Neural Operators (CNO), incorporates bandlimited approximations and carefully designed upsampling/downsampling operations to maintain consistency in function space. CNO is tested on two benchmark problems involving the Navier-Stokes equations, showing significantly better performance than competing models like FNO and DeepONet. The method is stable across different resolutions and provides a promising alternative framework for robust and accurate operator learning.

## Method Summary
The method involves reinterpreting CNNs through the lens of bandlimited function spaces. CNOs use sinc-based interpolation filters and carefully designed upsampling/downsampling operations to maintain the continuous-discrete equivalence at every layer. The architecture consists of encoder-decoder blocks with convolution, modified activation, and proper interpolation filtering. The model is trained on datasets generated from Navier-Stokes simulations, learning the operator mapping between initial conditions and solutions at later times.

## Key Results
- CNOs achieve significantly lower relative median L1-error than CNN, FNO, and DeepONet baselines on Navier-Stokes benchmarks
- CNOs demonstrate superior stability across different resolutions compared to FNOs
- The architecture outperforms competing models by a factor of almost 4 on the tested problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNOs outperform standard CNNs on PDE operator learning by preserving the continuous-discrete equivalence at every layer through bandlimited approximations and interpolation filtering.
- Mechanism: Standard CNNs operate directly on discrete grids without enforcing bandlimitedness, causing aliasing errors when the underlying function is not sufficiently sampled. CNOs enforce bandlimitedness by applying low-pass interpolation filters (sinc-based) at each up/down-sampling step, ensuring the Shannon-Whittaker-Kotel'nikov theorem holds. This guarantees that operations in the discrete domain correspond uniquely to operations in the continuous function space.
- Core assumption: Functions of interest (solutions to PDEs) have rapidly decaying Fourier spectra and can be well-approximated by bandlimited functions for sufficiently large bandwidth.
- Evidence anchors:
  - [abstract] states that "convolution based neural network architectures – believed to be inconsistent in function space – have been largely ignored in the context of learning solution operators of PDEs" and that CNOs "are designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer."
  - [section] explains that "The motivation behind the use of bandlimited functions is twofold: (1) the fourier coefficients of Sobolev functions f ∈ Hr(D) decay (rapidly). Therefore, they can be well approximated by bandlimited functions ˜f∈Bw(D) with a large enough bandw... (2) for a sufficiently resolved grid i.e. s > 2w, there exists a direct equivalence between the function and its grid (point) values... This exact correspondence between the continuous and discrete representations of the functions... is a necessary condition for working with continuous objects such as functions."
  - [corpus] shows no direct anchor; however, the absence of aliasing artifacts in CNO predictions (as shown in Figure 6) supports the claim.
- Break condition: If the underlying function has significant high-frequency content beyond the bandlimit imposed by the grid, or if the sinc interpolation filter is not properly designed (e.g., insufficient taps or incorrect cutoff), aliasing errors will re-emerge and degrade performance.

### Mechanism 2
- Claim: CNOs are resolution-invariant, meaning their accuracy does not degrade significantly when tested on grids with different resolutions than the training data.
- Mechanism: By enforcing bandlimitedness and proper interpolation at every layer, CNOs avoid the scale-dependent artifacts that plague standard CNNs. This allows the model to generalize across resolutions because the continuous function space equivalence is maintained regardless of discretization.
- Core assumption: The continuous operators defined in the CNO architecture (convolution, upsampling, downsampling, activation) are resolution-independent when implemented with bandlimited approximations.
- Evidence anchors:
  - [abstract] reports that CNOs "are stable across different resolutions and provides a promising alternative framework for robust and accurate operator learning."
  - [section] demonstrates this experimentally: "We test this issue for CNO by considering the same setup as the NS2 experiment... Results are illustrated in Figure 2. On the left plot... the CNN model performs very poorly... On the right of the same plot... we compare CNO and FNO at different test resolutions and observe that CNO... not only is very stable to changes in the resolution, but seems to be even more stable than FNO."
  - [corpus] provides no direct anchor; the claim is supported entirely by experimental results in the paper.
- Break condition: If the model is trained on data that violates the bandlimitedness assumption (e.g., high-frequency noise), or if the interpolation filters are not properly matched to the sampling rate, resolution invariance will break down.

### Mechanism 3
- Claim: CNOs significantly outperform competing operator learning architectures (FNO, DeepONet, CNN) on benchmark PDE problems by leveraging the inductive bias of locality and hierarchical feature extraction inherent in CNNs.
- Mechanism: The encoder-decoder structure of CNOs allows for multi-scale feature extraction: the encoder progressively downsamples the input, capturing global features, while the decoder upsamples and refines the output. This hierarchical approach, combined with the bandlimited consistency, leads to more accurate and stable approximations of the underlying PDE operators.
- Core assumption: The PDE solution operators exhibit hierarchical structure that can be effectively captured by convolutional layers with proper bandlimitedness.
- Evidence anchors:
  - [abstract] states that "CNOs are tested on a novel suite of benchmarks... and are observed to significantly outperform baselines."
  - [section] provides quantitative results: "We compare the performance of CNO to CNN, FNO and DeepONet baselines and present the test errors in Table 3. We observe from this table that not only is CNO the best performing architectures among all the models compared here, it outperforms CNN by a factor of almost 4."
  - [corpus] provides no direct anchor; the claim is supported by the experimental results in the paper.
- Break condition: If the PDE operator does not exhibit hierarchical structure, or if the convolutional filters are not appropriately sized or initialized, the performance advantage may diminish.

## Foundational Learning

- Concept: Bandlimited function spaces and the Shannon-Whittaker-Kotel'nikov sampling theorem
  - Why needed here: These concepts are the theoretical foundation for ensuring that discrete operations correspond uniquely to continuous operations, which is essential for operator learning in function spaces.
  - Quick check question: What is the relationship between the bandwidth of a function and the minimum sampling rate required to avoid aliasing?

- Concept: Convolutional neural network architectures and their inductive biases
  - Why needed here: CNOs are built upon CNNs, inheriting their locality and hierarchical feature extraction properties. Understanding these biases is crucial for appreciating why CNOs can effectively learn PDE operators.
  - Quick check question: How do convolutional layers exploit spatial locality in data?

- Concept: Fourier Neural Operators and DeepONets
  - Why needed here: These are the primary baselines against which CNOs are compared. Understanding their strengths and weaknesses is essential for evaluating the contributions of CNOs.
  - Quick check question: What is the key difference between the convolution operation in FNO and CNO?

## Architecture Onboarding

- Component map: Input -> Encoder (D blocks) -> Optional ResNet (R blocks) -> Decoder (U blocks) -> Output
- Critical path:
  1. Design appropriate sinc-based interpolation filters (cutoff frequency, number of taps)
  2. Implement the downsampling block (D) with convolution, modified activation, and downsampling
  3. Implement the upsampling block (U) with convolution, upsampling, and modified activation
  4. Stack encoder, optional ResNet, and decoder blocks
  5. Train the model on PDE operator learning tasks
- Design tradeoffs:
  - Filter design: More taps in the sinc filter lead to better approximation of the ideal low-pass filter but increase computational cost.
  - Number of channels: More channels allow for richer feature representation but increase memory usage and training time.
  - Depth of encoder/decoder: Deeper networks can capture more complex operators but are harder to train and more prone to overfitting.
- Failure signatures:
  - Aliasing artifacts in the output: Indicates that the bandlimitedness assumption is violated or the interpolation filters are not properly designed.
  - Poor performance on high-resolution data: Suggests that the model is not resolution-invariant, possibly due to inadequate bandlimitedness enforcement.
  - Overfitting: Indicates that the model is too complex for the available data, leading to poor generalization.
- First 3 experiments:
  1. Replicate the NS1 experiment from the paper: Learn the operator mapping initial vorticity to vorticity at time T=5 for the Navier-Stokes equations with viscosity ν=10^-3.
  2. Replicate the NS2 experiment from the paper: Learn the operator mapping initial velocity to velocity at time T=1 for the Navier-Stokes equations with viscosity ν=4·10^-4 and a thin shear layer initial condition.
  3. Test the resolution invariance of CNO: Train the model on data downsampled to a 64x64 grid and evaluate its performance on data downsampled to various resolutions (e.g., 32x32, 128x128, 256x256).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Convolutional Neural Operators (CNOs) perform when applied to PDEs with discontinuities or sharp gradients in the solutions?
- Basis in paper: [inferred] The paper mentions that the use of windowed-sinc filters in CNOs is motivated by the need to avoid ringing artifacts around high-gradient points, such as discontinuities, due to the Gibbs phenomenon.
- Why unresolved: The paper does not provide explicit experimental results or theoretical analysis of CNOs' performance on PDEs with discontinuities or sharp gradients.
- What evidence would resolve it: Experimental results comparing CNOs to other operator learning models on PDEs with known discontinuities or sharp gradients, along with theoretical analysis of the stability and accuracy of CNOs in such scenarios.

### Open Question 2
- Question: Can the stability of CNOs across different resolutions be further improved, and if so, how?
- Basis in paper: [explicit] The paper demonstrates that CNOs are stable across different resolutions and even more stable than FNOs, but it does not explore potential improvements to this stability.
- Why unresolved: The paper does not discuss methods for enhancing the resolution stability of CNOs beyond their current implementation.
- What evidence would resolve it: Investigation of alternative interpolation filters, downsampling techniques, or architectural modifications that could enhance the resolution stability of CNOs, along with experimental validation of these improvements.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the number of Fourier features or the sequence of channels in the encoder, affect the performance of CNOs?
- Basis in paper: [inferred] The paper provides details on the hyperparameters used in the experiments but does not explore the impact of varying these hyperparameters on the performance of CNOs.
- Why unresolved: The paper does not include a systematic study of the sensitivity of CNOs to different hyperparameter settings.
- What evidence would resolve it: A comprehensive study varying the hyperparameters of CNOs and analyzing the resulting performance on benchmark problems, potentially leading to guidelines for optimal hyperparameter selection.

## Limitations

- The theoretical guarantees of bandlimited approximations may not hold for all types of PDE solutions, particularly those with significant high-frequency content.
- The computational cost of sinc-based interpolation filters may be prohibitive for very large-scale problems.
- The generalizability of CNOs to other types of PDEs beyond Navier-Stokes is not extensively explored in the paper.

## Confidence

- High: The experimental results demonstrating the resolution-invariance of CNOs and their superior performance compared to baselines.
- Medium: The theoretical framework of bandlimited approximations and their application to CNN architectures for operator learning.
- Low: The generalizability of CNOs to other types of PDEs and the computational efficiency of the proposed architecture.

## Next Checks

1. Replicate the NS1 and NS2 experiments on a different dataset or with a different numerical solver to assess the robustness of the results.
2. Conduct ablation studies to determine the individual contributions of the bandlimited approximations, the encoder-decoder structure, and the specific choice of activation functions to the overall performance.
3. Test the CNO architecture on a different type of PDE (e.g., diffusion equation, wave equation) to evaluate its generalizability beyond the Navier-Stokes equations.