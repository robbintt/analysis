---
ver: rpa2
title: 'MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive Impairment
  in older adults using facial videos'
arxiv_id: '2304.05292'
source_url: https://arxiv.org/abs/2304.05292
tags:
- loss
- facial
- features
- videos
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Multi-branch Classifier-Video Vision
  Transformer (MC-ViViT) model to detect Mild Cognitive Impairment (MCI) from facial
  videos. The model integrates a Transformer backbone with a multi-branch classifier
  and a hybrid Focal/AD-CORRE loss to address inter-class and intra-class imbalances
  in the I-CONECT dataset.
---

# MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive Impairment in older adults using facial videos

## Quick Facts
- arXiv ID: 2304.05292
- Source URL: https://arxiv.org/abs/2304.05292
- Reference count: 40
- Primary result: Novel MC-ViViT model achieves up to 90.63% accuracy in detecting MCI from facial videos using hybrid Focal/AD-CORRE loss and multi-branch classifier

## Executive Summary
This paper proposes MC-ViViT, a Multi-branch Classifier-Video Vision Transformer model for detecting Mild Cognitive Impairment (MCI) from facial videos. The model addresses the challenging I-CONECT dataset's class imbalance by integrating a multi-branch classifier and a hybrid HP loss combining Focal and AD-CORRE losses. Experimental results across four conversational themes demonstrate superior performance with high accuracy, F1 scores, and balanced sensitivity/specificity, showing the model's effectiveness in distinguishing MCI from normal cognition using facial video features.

## Method Summary
MC-ViViT processes 16-frame video segments through Tubelet Embedding to create 3D patches, which are then fed into a Factorised Transformer Encoder with spatial and temporal self-attention mechanisms. The model incorporates a multi-branch classifier with four parallel fully connected layers to enrich feature representation from different perspectives. A hybrid HP loss function (combining Focal loss and AD-CORRE loss) addresses both inter-class and intra-class imbalances in the dataset. The model is trained using Adam optimizer with learning rate 1e-6 and Cyclic scheduler, with 3-fold cross-validation for evaluation.

## Key Results
- Achieves up to 90.63% accuracy in detecting MCI from facial videos across four conversational themes
- Demonstrates strong F1 scores and balanced sensitivity/specificity metrics
- Ablation studies confirm the effectiveness of multi-branch classifier and hybrid HP loss in improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-branch classifier (MC) improves feature representation by providing diverse linear projections of the same feature space.
- Mechanism: MC applies four parallel fully connected layers with different dimensional transformations (64→16→[8,8,8,8]→32→num_class), allowing the model to learn complementary feature views rather than collapsing information too early.
- Core assumption: Different linear projections of intermediate features capture distinct aspects of the data that a single projection would miss.
- Evidence anchors:
  - [section]: "MC takes multi-branch structure to provide different views and enrich representation as well."
  - [section]: "MC can provide more features and view the object from different angles."

### Mechanism 2
- Claim: The hybrid HP loss (Focal + AD-CORRE) addresses both inter-class and intra-class imbalance in the I-CONECT dataset.
- Mechanism: Focal loss down-weights easy examples and emphasizes hard-to-classify NC samples (minority class), while AD-CORRE loss focuses on correlation between samples within mini-batches to handle positive-negative sample imbalance within each class.
- Core assumption: The I-CONECT dataset has both class imbalance (fewer NC samples) and within-class imbalance (variable MCI symptom presentation across video segments).
- Evidence anchors:
  - [abstract]: "The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples"
  - [section]: "The I-CONECT dataset has both inter- and intra-class variations...subjects with MCI may behave normal cognitive sometime or show the symptom of MCI in one second."

### Mechanism 3
- Claim: Tubelet embedding with temporal dimension t=4 optimally captures spatiotemporal features for MCI detection.
- Mechanism: By dividing video clips into 3D cubes (t×h×w×3) with temporal dimension t=4, the model preserves short-term temporal dynamics while maintaining computational efficiency.
- Core assumption: MCI-related facial expressions and behaviors manifest within short temporal windows rather than requiring long-term context.
- Evidence anchors:
  - [section]: "To keep Tubelet Embedding and reduce the burden of computation, we set the temporal dimension t as [2, 4, 8] alternatively to study the best value for this research."
  - [section]: "Table III shows that the t = 4 predicts the best under the current configuration on four themes."

## Foundational Learning

- Concept: Video Vision Transformers (ViViT)
  - Why needed here: Standard 2D vision transformers cannot capture temporal dependencies crucial for analyzing facial expressions and behaviors in video data.
  - Quick check question: What architectural component in ViViT allows it to process video as a sequence of 3D patches rather than individual frames?

- Concept: Class imbalance handling in deep learning
  - Why needed here: The I-CONECT dataset has significantly fewer NC samples than MCI samples, and within each class, video segments vary in their MCI symptom expression.
  - Quick check question: How does Focal loss specifically address class imbalance differently from standard cross-entropy?

- Concept: Transformer self-attention mechanisms
  - Why needed here: Self-attention allows the model to weigh the importance of different spatiotemporal regions in facial videos when detecting MCI indicators.
  - Quick check question: In the context of Factorised Encoder, what is the difference between spatial and temporal self-attention?

## Architecture Onboarding

- Component map: Input (16-frame video segments) → Tubelet Embedding → Class Token + Positional Embeddings → Transformer Encoder (Spatial + Temporal) → Multi-branch Classifier → Output (MCI/NC prediction)
- Critical path: The spatiotemporal feature extraction pipeline (Tubelet Embedding → Transformer Encoder) is most critical for model performance
- Design tradeoffs: Higher temporal dimensions (t) provide better temporal modeling but increase computational cost; the multi-branch classifier adds parameters but improves accuracy
- Failure signatures: Poor performance on NC samples suggests Focal loss isn't properly emphasizing hard examples; inconsistent predictions across similar videos suggests Tubelet Embedding parameters need adjustment
- First 3 experiments:
  1. Test different temporal dimensions (t=2,4,8) on a single theme to find optimal temporal context
  2. Compare single-branch vs multi-branch classifier performance to validate feature enrichment benefit
  3. Evaluate Focal loss vs AD-CORRE vs HP loss on balanced subset to understand individual component contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in video quality (e.g., occlusion, lighting, resolution) quantitatively affect MC-ViViT's prediction accuracy for MCI detection?
- Basis in paper: [inferred] The paper mentions that video quality variations (occlusion, lightness problems) can decrease the quality of extracted spatio-temporal features and negatively influence predictions.
- Why unresolved: The authors acknowledge this as a challenge but do not provide empirical analysis of how specific quality factors correlate with prediction performance.
- What evidence would resolve it: Controlled experiments varying video quality parameters and measuring corresponding changes in MC-ViViT accuracy, ideally with quality metrics (e.g., PSNR, SSIM) correlated to prediction scores.

### Open Question 2
- Question: Does the HP loss function maintain its effectiveness when applied to datasets with different class imbalance ratios compared to the I-CONECT dataset?
- Basis in paper: [explicit] The paper introduces HP loss specifically to address inter-class (Hard-Easy) and intra-class (Positive-Negative) imbalance in the I-CONECT dataset, which has an uneven distribution of MCI and NC subjects.
- Why unresolved: The effectiveness of HP loss is demonstrated only on the I-CONECT dataset; its generalizability to other datasets with different imbalance characteristics remains untested.
- What evidence would resolve it: Testing MC-ViViT with HP loss on multiple datasets with varying imbalance ratios (e.g., synthetic imbalance adjustments, other medical imaging datasets) and comparing performance against standard loss functions.

### Open Question 3
- Question: What is the optimal temporal dimension (t) size for MC-ViViT across different conversational themes or domains beyond the I-CONECT dataset?
- Basis in paper: [explicit] The paper conducts an ablation study on temporal dimension t, finding t=4 optimal for the four I-CONECT themes tested, but acknowledges that t=16 (frame number) can be divided by [1, 2, 4, 8, 16].
- Why unresolved: The study only tests t values of [2, 4, 8] and within a single dataset; optimal t may vary with different video characteristics or conversational contexts.
- What evidence would resolve it: Systematic testing of all possible t values across multiple datasets with different video lengths and conversational dynamics, identifying patterns in optimal t selection.

## Limitations

- Dataset-specific performance may not generalize to other MCI detection datasets or clinical settings
- High computational complexity may limit real-time deployment on resource-constrained clinical devices
- Lack of interpretability regarding which specific facial features or temporal patterns indicate MCI

## Confidence

- High Confidence: The effectiveness of the multi-branch classifier in improving feature representation is well-supported by ablation studies showing performance degradation when using single-branch alternatives. The Focal loss component's effectiveness in handling class imbalance is also well-established in the literature.
- Medium Confidence: The hybrid HP loss (Focal + AD-CORRE) shows promising results, but the individual contribution of each component is not fully isolated. The specific benefits of combining these two loss functions over using either alone requires further investigation.
- Low Confidence: The optimal temporal dimension (t=4) is based on limited experiments with only three values tested. Different video characteristics or MCI manifestations might require different temporal contexts for optimal detection.

## Next Checks

1. **Cross-Dataset Validation**: Test MC-ViViT on an independent MCI detection dataset (e.g., ADNI) to assess generalizability and potential overfitting to I-CONECT-specific characteristics.

2. **Feature Importance Analysis**: Implement attention visualization techniques to identify which facial regions and temporal patterns the model uses for MCI detection, providing clinical interpretability.

3. **Real-Time Performance Evaluation**: Benchmark the model's inference speed and memory requirements on edge devices to assess practical deployment feasibility in clinical settings.