---
ver: rpa2
title: Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with
  Skeleton-Motion-Informed Gradient
arxiv_id: '2308.05681'
source_url: https://arxiv.org/abs/2308.05681
tags:
- attack
- no-box
- samples
- adversarial
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for hard no-box adversarial
  attacks on skeleton-based human action recognition. The proposed method addresses
  the challenge of attacking models without access to the victim model, training data,
  or labels.
---

# Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient

## Quick Facts
- **arXiv ID:** 2308.05681
- **Source URL:** https://arxiv.org/abs/2308.05681
- **Reference count:** 40
- **Primary result:** Achieves higher fooling rates than baselines in hard no-box adversarial attacks on skeleton-based action recognition

## Executive Summary
This paper presents a novel approach for hard no-box adversarial attacks on skeleton-based human action recognition. The method addresses the challenge of attacking models without access to the victim model, training data, or labels. The core innovation involves learning a motion manifold using contrastive learning and defining a no-box adversarial loss to guide the attack. A skeleton-motion-informed (SMI) gradient is introduced that explicitly considers temporal dependencies in skeletal motions, improving both transferability and imperceptibility of adversarial samples.

## Method Summary
The proposed method trains a contrastive learning encoder to learn a motion manifold from unlabeled skeletal data. For each attack, negative samples are selected from K-means clusters to define a no-box adversarial loss that guides the perturbation. The SMI gradient is computed using time-varying autoregressive models that capture motion dynamics (velocity and acceleration), replacing the raw gradient assumption that treats each dimension independently. This gradient is then applied to iterative attack methods like I-FGSM and MI-FGSM to generate adversarial samples within a pre-defined perturbation budget.

## Key Results
- Achieves higher fooling rates compared to existing baselines in hard no-box settings
- SMI gradient improves transferability of adversarial samples across different victim models
- Enhanced imperceptibility of adversarial samples while maintaining attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The skeleton-motion-informed (SMI) gradient improves attack transferability by explicitly modeling temporal dependencies in skeletal motion data.
- Mechanism: The SMI gradient replaces the raw gradient assumption (treating each dimension as independent) with a dynamics-aware computation using time-varying autoregressive (TV-AR) models that capture motion velocity and acceleration.
- Core assumption: Skeletal motion sequences exhibit Markovian or autoregressive dynamics that can be accurately modeled to improve gradient computation for adversarial attacks.

### Mechanism 2
- Claim: Contrastive learning without labels can create effective decision boundaries for adversarial attacks in no-box settings.
- Mechanism: By training a query encoder to maximize similarity between augmented views of the same sample while minimizing similarity to negative samples, the method creates a latent manifold where similar motions cluster together, enabling adversarial perturbation towards dissimilar samples.
- Core assumption: Similar skeletal motions have similar high-level features that can be learned without labels through augmentation and contrastive objectives.

### Mechanism 3
- Claim: The no-box adversarial loss effectively guides attacks without requiring labels or victim model access.
- Mechanism: The loss maximizes dissimilarity between adversarial samples and positive samples (clean samples) while minimizing similarity to negative samples (samples from different clusters), driving the adversarial sample across decision boundaries.
- Core assumption: Negative samples selected through clustering represent high-density areas of other classes, making them effective targets for adversarial perturbations.

## Foundational Learning

- **Concept:** Time-varying autoregressive (TV-AR) models for temporal sequence modeling
  - Why needed here: TV-AR models capture the temporal dependencies in skeletal motion data that are crucial for computing the SMI gradient
  - Quick check question: What is the difference between TV-AR(1) and TV-AR(2) in terms of the temporal relationships they model?

- **Concept:** Contrastive learning without labels
  - Why needed here: Standard supervised learning requires labels which are unavailable in hard no-box settings, making contrastive learning essential for creating decision boundaries
  - Quick check question: How does InfoNCE loss encourage the encoder to learn discriminative features without using class labels?

- **Concept:** Adversarial attack gradient computation
  - Why needed here: Understanding how gradients are typically computed for adversarial attacks (raw gradient vs. dynamics-aware) is crucial for grasping the SMI gradient innovation
  - Quick check question: Why does treating each dimension as independent fail for skeletal motion data?

## Architecture Onboarding

- **Component map:** Contrastive Learning Module -> SMI Gradient Computation -> No-Box Adversarial Loss -> Attack Strategies
- **Critical path:** Training CL encoder → Computing SMI gradient → Applying adversarial loss → Generating adversarial samples via iterative gradient updates
- **Design tradeoffs:** Using TV-AR models adds computational overhead but improves motion dynamics capture; selecting negative samples via clustering avoids label requirements but may introduce clustering errors; the 2nd-order SMI gradient captures acceleration but requires more complex TV-AR(2) modeling
- **Failure signatures:** Low fooling rates despite successful CL training indicate poor SMI gradient computation or inappropriate negative sample selection; high perceptual deviation suggests the attack is not staying close to the motion manifold; poor transferability across victim models indicates the SMI gradient may not generalize well
- **First 3 experiments:**
  1. Train the CL encoder with different numbers of clusters (60, 80, 100, 120) and evaluate the impact on no-box fooling rates
  2. Compare first-order vs. second-order SMI gradients on a single victim model to validate the importance of acceleration modeling
  3. Test the perceptual quality (∆p) of S1I-FGSM vs. S2I-FGSM vs. I-FGSM to verify the imperceptibility claims

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the transferability of adversarial attacks be further improved beyond the proposed SMI gradient approach?
  - Basis in paper: The paper mentions that boosting transferability is still an open problem and suggests exploring this further with the SMI gradient.
  - Why unresolved: The paper only demonstrates that SMI gradient improves transferability, but does not explore other potential methods or combinations.
  - What evidence would resolve it: Experiments comparing the SMI gradient approach with other methods or combinations thereof on multiple datasets and victim models.

- **Open Question 2:** What is the theoretical explanation for the variance in fooling rates across different victim models and datasets?
  - Basis in paper: The paper notes a variance in fooling rates across different victims and datasets but leaves the theoretical analysis for future work.
  - Why unresolved: The paper does not provide a theoretical framework to explain why certain models or datasets are more susceptible to adversarial attacks than others.
  - What evidence would resolve it: A comprehensive theoretical analysis of the factors contributing to differences in fooling rates, potentially including model architecture, dataset characteristics, and attack method interactions.

- **Open Question 3:** How effective are other models, such as diffusion models, for generating adversarial attacks on time-series data like skeletal motions?
  - Basis in paper: The paper mentions the potential exploration of other models (e.g., diffusion models) for adversarial attacks on time-series data in the conclusions.
  - Why unresolved: The paper only proposes using the SMI gradient with existing attack methods and does not explore other model architectures.
  - What evidence would resolve it: Experiments comparing the performance of diffusion models and other architectures against the SMI gradient approach on skeletal motion data.

## Limitations

- The effectiveness of TV-AR models for skeletal motion dynamics capture is assumed but not empirically validated
- The contrastive learning approach relies on creating meaningful clusters without labels, but the quality of these clusters and their relationship to actual action classes is not examined
- The SMI gradient computation assumes Markovian dynamics that may not hold for all skeletal motion sequences

## Confidence

- **High Confidence:** The overall framework combining contrastive learning with adversarial attacks is sound and addresses a real problem in no-box settings
- **Medium Confidence:** The SMI gradient concept is innovative, but the empirical evidence for its superiority over raw gradients is limited to relative improvements without ablation studies on the TV-AR components
- **Medium Confidence:** The no-box adversarial loss formulation appears theoretically sound, but the assumption that K-means clusters provide appropriate negative samples needs further validation

## Next Checks

1. Conduct ablation studies comparing SMI gradients with different TV-AR orders (AR(1) vs AR(2)) on the same victim models to isolate the impact of acceleration modeling
2. Analyze the quality of clusters produced by contrastive learning by examining the distribution of true action classes within each cluster
3. Test attack transferability across a wider range of model architectures (not just different training runs of the same architecture) to verify generalization claims