---
ver: rpa2
title: Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large
  Language Models
arxiv_id: '2305.14705'
source_url: https://arxiv.org/abs/2305.14705
tags:
- direct
- arxiv
- tasks
- flan
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FLAN-MOE, a set of instruction-finetuned sparse
  Mixture-of-Experts (MoE) language models. The authors find that while MoE models
  underperform dense models when finetuned on individual downstream tasks, they surpass
  dense models when instruction-finetuned.
---

# Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models

## Quick Facts
- arXiv ID: 2305.14705
- Source URL: https://arxiv.org/abs/2305.14705
- Authors: 
- Reference count: 40
- Key outcome: FLAN-MOE-32B outperforms FLAN-PALM-62B on four benchmarks using only one-third of the FLOPs

## Executive Summary
This paper presents FLAN-MOE, a set of instruction-finetuned sparse Mixture-of-Experts (MoE) language models that dramatically outperform dense models when instruction-tuned. The authors demonstrate that while MoE models underperform dense models when finetuned on individual downstream tasks, instruction tuning transforms them into state-of-the-art performers. Specifically, FLAN-MOE-32B surpasses FLAN-PALM-62B on four benchmarks while utilizing only one-third of the FLOPs. The paper establishes instruction tuning as essential for MoE models to achieve strong performance on both downstream and held-out tasks, encouraging a rethinking of large-scale language model design under task-agnostic learning settings.

## Method Summary
The method involves converting dense T5 models to MoE by replacing every other feed-forward network with sparse MoE layers containing 64 experts and top-2 routing. These MoE models are then instruction-finetuned on the FLAN collective dataset (1,836 tasks) using a prefix language modeling objective with sequence length 2048 input and 512 output. Training employs dropout (0.05), expert dropout (0.2), learning rate 1e-4, and batch size 32. The models are evaluated on held-out benchmarks including MMLU, BBH, reasoning tasks, and QA tasks using few-shot and zero-shot settings, and compared against dense T5 models of equivalent compute.

## Key Results
- FLAN-MOE-32B outperforms FLAN-PALM-62B on four benchmark tasks while using only one-third of the FLOPs
- MoE models benefit more from instruction-finetuning than dense models and are more sensitive to the number of instruction-tuning tasks
- Instruction tuning is essential for MoE models to achieve strong performance on both downstream and held-out tasks
- FLAN-MOE models show significantly better performance on held-out tasks compared to task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Instruction tuning compensates for MoE's inherent task-specific overfitting by providing diverse, general-purpose training signals that regularize the routing and expert specialization.
- **Mechanism**: Without instruction tuning, MoE models overfit to single downstream tasks, causing expert collapse and poor generalization. Instruction tuning with diverse tasks forces the gating network to learn more robust, transferable routing patterns, improving both downstream and held-out performance.
- **Core assumption**: Diverse instruction data prevents the MoE from specializing too narrowly on single-task patterns, which dense models are less prone to due to their full-layer activation.
- **Evidence anchors**: [abstract] MoE models underperform dense models when finetuned without instruction tuning, but performance dramatically changes with instruction tuning. [section] MoE models benefit more from instruction-finetuning and are more sensitive to instruction-tuning tasks.
- **Break condition**: If instruction data is too homogeneous or limited in scope, the MoE may still overfit and fail to outperform dense models.

### Mechanism 2
- **Claim**: Sparse activation in MoE allows for greater model capacity without proportional computational cost, but only when the gating network is properly trained via instruction tuning.
- **Mechanism**: MoE layers contain many more parameters than dense layers, but only a subset is active per token. Instruction tuning ensures the gating network learns to route inputs effectively across experts, unlocking this capacity advantage. Without it, the sparse structure underutilizes available parameters.
- **Core assumption**: The gating network can learn meaningful routing strategies when exposed to varied instruction data, distributing workload across experts effectively.
- **Evidence anchors**: [abstract] FLAN-MOE-32B surpasses FLAN-PALM-62B on four benchmark tasks while utilizing only one-third of the FLOPs. [section] Each MoE layer's learnable gating network is trained to use its input to activate the best two experts for each token.
- **Break condition**: If the gating network fails to learn meaningful routing (e.g., due to poor initialization or insufficient diversity in instruction data), the sparse structure offers no advantage.

### Mechanism 3
- **Claim**: Instruction tuning mitigates the instability of MoE models during fine-tuning by regularizing both expert and gating parameters through exposure to diverse task distributions.
- **Mechanism**: MoE models are prone to overfitting and expert collapse during fine-tuning on limited data. Instruction tuning with a large, varied dataset provides a stable pre-fine-tuning stage that regularizes both experts and gating, leading to better generalization on downstream tasks.
- **Core assumption**: The stability gained from instruction tuning generalizes to subsequent task-specific fine-tuning, preventing the MoE from diverging or collapsing.
- **Evidence anchors**: [section] Sparse models have performed remarkably well in the regime of large datasets, but have sometimes performed poorly when fine-tuning data is limited. Instruction finetuning can also be viewed as a continual fine-tuning stage. [section] The incorporation of auxiliary loss helps mitigate the risk of overfitting by promoting the diversification of the experts' knowledge.
- **Break condition**: If the instruction tuning stage is too short or the data too limited, the regularization effect may be insufficient to prevent instability.

## Foundational Learning

- **Concept**: Mixture-of-Experts (MoE) architecture and conditional computation
  - **Why needed here**: Understanding how MoE layers work (sparse activation, gating, expert specialization) is essential to grasp why instruction tuning is particularly beneficial for MoE models versus dense models.
  - **Quick check question**: What is the role of the gating network in an MoE layer, and how does it differ from a standard feed-forward layer in a dense transformer?

- **Concept**: Instruction tuning and its impact on model generalization
  - **Why needed here**: The paper's central claim is that instruction tuning is essential for MoE success. Knowing how instruction tuning works (e.g., multi-task fine-tuning with diverse prompts) and its effects on model behavior is critical.
  - **Quick check question**: How does instruction tuning with diverse tasks differ from single-task fine-tuning in terms of model adaptation and generalization?

- **Concept**: Routing strategies in MoE (token-choice vs. expert-choice)
  - **Why needed here**: The paper compares different routing strategies (Switch, GShard, expert-choice). Understanding these strategies and their trade-offs is important for interpreting experimental results.
  - **Quick check question**: What is the difference between token-choice and expert-choice routing, and how might each affect model performance and efficiency?

## Architecture Onboarding

- **Component map**: Pre-trained dense model -> MoE conversion (64 experts, top-2 routing) -> Instruction tuning on FLAN dataset -> Evaluation on held-out benchmarks

- **Critical path**: 1) Pre-training dense model (e.g., T5) 2) Convert to MoE by replacing feed-forward layers 3) Instruction tune on FLAN dataset with auxiliary losses 4) Evaluate on downstream and held-out tasks

- **Design tradeoffs**:
  - Expert number: More experts increase capacity but may lead to saturation or overfitting; fewer experts are more stable but less expressive
  - Routing strategy: Token-choice (token selects experts) vs. expert-choice (experts select tokens); affects load balancing and specialization
  - Auxiliary loss: Balancing loss vs. Z-loss; impacts expert utilization and stability
  - Finetuning scope: Updating all parameters vs. freezing gating or experts; affects overfitting risk and generalization

- **Failure signatures**:
  - Poor downstream performance: Likely due to insufficient instruction tuning or expert collapse
  - High variance across runs: May indicate instability in MoE training, possibly from poor routing or unbalanced expert loads
  - Low expert utilization: Suggests gating network not learning meaningful routing, or experts not specialized enough

- **First 3 experiments**:
  1. **Instruction tuning ablation**: Compare MoE performance with and without instruction tuning on a held-out task to confirm its necessity
  2. **Routing strategy comparison**: Test token-choice vs. expert-choice routing on a small MoE model to identify which yields better performance and stability
  3. **Expert number scaling**: Train MoE models with varying numbers of experts (e.g., 32, 64, 128) on the same instruction tuning data to find the point of diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the specific limitations of MoE models when it comes to multilingual language processing?
- **Basis in paper**: The paper mentions that FLAN-MOE models perform poorly on multilingual benchmarks like TyDiQA and MGSM, achieving significantly lower scores compared to their dense counterparts.
- **Why unresolved**: The paper suggests that the issue might stem from over-optimization towards English during finetuning, but it doesn't delve into the specific mechanisms behind this limitation or propose solutions.
- **What evidence would resolve it**: Detailed analysis of the MoE model's performance across different languages, including error analysis and ablation studies to identify the root cause of the multilingual performance gap. Research into methods to improve multilingual capabilities, such as incorporating diverse linguistic data during training or developing language-specific expert routing strategies.

### Open Question 2
- **Question**: How does the performance of FLAN-MOE scale with the number of experts beyond the saturation point observed in the paper?
- **Basis in paper**: The paper mentions that the performance of FLAN-MOE models tends to saturate beyond a certain threshold of experts, but it doesn't explore the behavior beyond this point.
- **Why unresolved**: The paper only investigates a limited range of expert numbers and doesn't provide insights into the potential benefits or drawbacks of using an even larger number of experts.
- **What evidence would resolve it**: Experiments with FLAN-MOE models using a significantly larger number of experts, evaluating their performance on various benchmarks and analyzing the trade-offs between model size, computational efficiency, and task performance.

### Open Question 3
- **Question**: What are the optimal hyperparameters for instruction finetuning FLAN-MOE models at different scales?
- **Basis in paper**: The paper mentions that hyperparameter sensitivity varies across tasks and scales, but it doesn't provide a comprehensive study of the optimal hyperparameter settings for different model sizes.
- **Why unresolved**: The paper only briefly mentions hyperparameter sensitivity and doesn't offer specific recommendations for hyperparameter tuning at different scales.
- **What evidence would resolve it**: Systematic experiments with different hyperparameter configurations (e.g., learning rate, batch size, expert dropout) for FLAN-MOE models of various sizes, identifying the optimal settings for each scale and task type.

## Limitations

- Exact pre-trained MoE model checkpoints used (Switch Transformer, GShard, Expert-Choice variants) are not clearly identified, making precise reproduction difficult
- Routing strategy (token-choice vs. expert-choice) and specific auxiliary loss implementation are not fully detailed, potentially affecting experimental outcomes
- No statistical significance testing or error bars provided for benchmark comparisons, limiting confidence in the magnitude of performance gains
- Limited exploration of why certain routing strategies or expert counts perform better, leaving mechanisms partially unexplained

## Confidence

**High Confidence**: The core claim that instruction tuning significantly improves MoE model performance compared to task-specific fine-tuning is well-supported by experimental results. The finding that FLAN-MOE-32B outperforms FLAN-PALM-62B using only one-third of the FLOPs is a clear, measurable outcome with direct evidence in the paper.

**Medium Confidence**: The assertion that MoE models are "more sensitive to the number of instruction-tuning tasks" is supported by ablation studies but lacks detailed statistical analysis. The claim that MoE models benefit more from instruction tuning than dense models is plausible given the architecture differences but requires more controlled experiments to definitively establish.

**Low Confidence**: The paper's discussion of specific failure modes (expert collapse, routing instability) and their mitigation through instruction tuning is largely theoretical, with limited empirical validation of these mechanisms. The exact reasons why certain routing strategies outperform others are not fully explored or explained.

## Next Checks

1. **Routing Strategy Impact**: Conduct controlled experiments comparing token-choice, expert-choice, and Switch routing strategies on identical MoE architectures with the same instruction tuning protocol to determine which yields optimal performance and stability.

2. **Expert Count Scaling Analysis**: Systematically vary the number of experts (32, 64, 128) in MoE layers while maintaining consistent instruction tuning to identify the point of diminishing returns and optimal expert utilization.

3. **Instruction Tuning Data Diversity**: Test the hypothesis that diverse instruction data is crucial for MoE success by comparing models instruction-tuned on homogeneous versus heterogeneous task distributions, measuring downstream generalization and expert utilization.