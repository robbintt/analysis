---
ver: rpa2
title: 'Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative
  Analysis'
arxiv_id: '2309.15656'
source_url: https://arxiv.org/abs/2309.15656
tags:
- feedback
- dialogues
- spontaneous
- dialogue
- expressing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides a quantitative analysis of conversational feedback
  and grounding phenomena in scripted dialogues (movie/TV subtitles) compared to spontaneous
  dialogues across eight languages. Using lexical statistics and a neural dialogue
  act tagger, the researchers found that communicative feedback is significantly less
  frequent in subtitles than in spontaneous dialogues, and subtitles contain a higher
  proportion of negative feedback.
---

# Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis

## Quick Facts
- arXiv ID: 2309.15656
- Source URL: https://arxiv.org/abs/2309.15656
- Reference count: 40
- One-line primary result: Subtitles contain significantly fewer feedback signals than spontaneous dialogues, with more negative feedback and fewer acknowledgments.

## Executive Summary
This study provides a quantitative analysis of conversational feedback and grounding phenomena in scripted dialogues (movie/TV subtitles) compared to spontaneous dialogues across eight languages. Using lexical statistics and a neural dialogue act tagger, the researchers found that communicative feedback is significantly less frequent in subtitles than in spontaneous dialogues, and subtitles contain a higher proportion of negative feedback. When fine-tuned on different dialogue corpora, large language models reproduced these patterns, generating more feedback cues when trained on spontaneous dialogues. The study also found that while recent conversational models understand feedback cues well, they struggle to produce short feedback signals.

## Method Summary
The study analyzed eight spontaneous dialogue corpora (English Switchboard, French CID, German HAMATAC, Hungarian BUSZI-2, Italian CLIPS, Japanese CallHome, Norwegian NoTa-Oslo, Chinese CALLHOME) and subtitle corpora from OpenSubtitles 2018 (filtered for recent movies â‰¥1990, excluding non-dialogue genres). The researchers employed lexical analysis using language-specific cue word lists, neural dialogue act tagging trained on Switchboard-DAMSL mapped to 5 coarse classes, and fine-tuning of GPT-2 models on each corpus type to generate synthetic dialogues for comparison.

## Key Results
- Subtitles contain significantly fewer feedback signals than spontaneous dialogues across all eight languages studied
- Subtitles show a higher proportion of negative feedback compared to spontaneous dialogues
- Language models trained on subtitle data generate dialogues with fewer feedback cues than those trained on spontaneous dialogues
- Conversational AI models understand feedback cues well but struggle to produce short feedback signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subtitles contain fewer conversational feedback signals because they are designed for brevity and narrative focus.
- Mechanism: Subtitle constraints (max 40-50 characters per line, 1-6 seconds display time) force condensation, omitting non-essential feedback elements.
- Core assumption: The condensation process in subtitle creation systematically removes feedback cues while preserving core narrative content.
- Evidence anchors:
  - [abstract]: "subtitles tend to only transcribe the most salient information conveyed in each utterance"
  - [section]: "This is due to constraints on the length and the readability of the subtitles as well as adherence to writing conventions"
  - [corpus]: Weak - no direct subtitle corpus analysis showing omitted feedback; based on design constraints
- Break condition: If subtitles were created with feedback preservation as a priority, or if feedback cues were found to be narrative-relevant.

### Mechanism 2
- Claim: Large language models trained on subtitles reproduce lower feedback frequencies because they learn from the data distribution.
- Mechanism: Language models learn patterns from training data; when trained on subtitle corpora with fewer feedback signals, they generate text with similar properties.
- Core assumption: The language model's training data distribution directly influences its generation patterns for conversational phenomena.
- Evidence anchors:
  - [abstract]: "dialogues generated by standard LLMs lie much closer to scripted dialogues than spontaneous interactions in terms of communicative feedback"
  - [section]: "We then apply the neural dialogue act tagger on those dialogues (translated into English) to assess the relative frequency of various dialogue act groups"
  - [corpus]: Strong - direct comparison of synthetic dialogues from subtitle-trained vs spontaneous-trained models
- Break condition: If fine-tuning on spontaneous data doesn't increase feedback generation, or if other factors dominate generation patterns.

### Mechanism 3
- Claim: Current conversational AI models struggle with feedback production because their training data lacks sufficient spontaneous dialogue examples.
- Mechanism: Models trained primarily on scripted data learn to replicate scripted dialogue patterns, which contain fewer and different types of feedback cues than spontaneous speech.
- Core assumption: The scarcity of spontaneous dialogue in training data limits models' ability to understand and generate natural feedback patterns.
- Evidence anchors:
  - [abstract]: "we also offer a small-scale qualitative analysis of how they deal with such communicative signals"
  - [section]: "The limited evidence summarized here thus indicates that recent conversational models might have some difficulties producing and handling certain short communicative feedback signs"
  - [corpus]: Moderate - qualitative analysis shows ChatGPT struggles with short feedback cues despite understanding them
- Break condition: If models show strong feedback handling after fine-tuning on spontaneous data, or if other training approaches overcome this limitation.

## Foundational Learning

- Concept: Dialogue act classification
  - Why needed here: The study uses a neural dialogue act tagger to quantify feedback phenomena across different corpora
  - Quick check question: What are the main dialogue act categories used in the Switchboard DAMSL framework?

- Concept: Corpus linguistics and genre differences
  - Why needed here: Understanding how different dialogue genres (scripted vs spontaneous) have distinct linguistic properties is central to the study
  - Quick check question: How do constraints on subtitle creation (length, display time) affect the linguistic features present?

- Concept: Language model fine-tuning and domain adaptation
  - Why needed here: The study examines how models trained on different data sources (subtitles vs spontaneous dialogues) produce different feedback patterns
  - Quick check question: What happens to a language model's generation patterns when fine-tuned on a corpus with different stylistic properties?

## Architecture Onboarding

- Component map: Data collection -> preprocessing -> cue word extraction -> neural tagging -> analysis
- Critical path: Corpus preprocessing -> cue word list creation -> dialogue act tagger training -> model fine-tuning -> synthetic dialogue generation -> analysis
- Design tradeoffs: Using machine translation for non-English corpora enables cross-linguistic comparison but may introduce errors in feedback cue detection; neural tagging provides more precise classification than cue words alone but requires more resources
- Failure signatures: Low feedback detection accuracy across corpora suggests tagger generalization issues; synthetic dialogues showing unexpected patterns may indicate training data quality problems
- First 3 experiments:
  1. Apply the dialogue act tagger to a held-out portion of Switchboard to verify baseline performance before applying to other corpora
  2. Compare lexical cue word detection vs neural tagger output on the same corpus to assess consistency
  3. Generate synthetic dialogues from base models vs fine-tuned models and compare feedback frequencies to validate the fine-tuning effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do subtitles in different genres (e.g., action vs. romance) differ in their use of conversational feedback, and what narrative functions do these differences serve?
- Basis in paper: explicit - The paper mentions that different genres of subtitles were analyzed and found to have varying amounts of feedback phenomena.
- Why unresolved: The paper provides an overview of genre-based differences but does not deeply analyze the narrative reasons behind these patterns.
- What evidence would resolve it: A detailed comparative analysis of feedback usage across genres, examining how feedback contributes to narrative pacing, character development, and audience engagement in each genre.

### Open Question 2
- Question: How do different types of nonverbal feedback (e.g., nods, smiles, head shakes) vary in their frequency and function across different movie genres, and how does this correlate with their representation in subtitles?
- Basis in paper: explicit - The paper mentions that nonverbal feedback is often omitted from subtitles and provides some quantitative data on its occurrence in specific movies.
- Why unresolved: The paper only examines a small sample of movies and does not systematically analyze how different types of nonverbal feedback vary across genres.
- What evidence would resolve it: A large-scale analysis of multiple movies across various genres, with detailed annotations of different types of nonverbal feedback and their correlation with subtitle content.

### Open Question 3
- Question: How does the performance of conversational AI systems in handling feedback cues differ when trained on spontaneous dialogue versus scripted dialogue, and what specific aspects of feedback understanding or generation are most affected?
- Basis in paper: explicit - The paper discusses how language models trained on different types of dialogue data perform differently in generating feedback cues.
- Why unresolved: The paper only provides a preliminary evaluation of a few conversational models and does not conduct a comprehensive comparison of their performance on various feedback-related tasks.
- What evidence would resolve it: A systematic evaluation of multiple conversational AI systems, trained on different types of dialogue data, on a range of tasks involving both understanding and generating feedback cues, including both verbal and nonverbal signals.

## Limitations
- The study relies on machine translation to compare dialogue act patterns across eight languages, introducing potential noise in feedback cue detection
- The spontaneous dialogue corpora vary considerably in size and collection context, affecting generalizability of findings across languages
- Cue word lists may not capture all feedback phenomena, particularly subtle or context-dependent feedback signals

## Confidence
- High Confidence: Subtitles contain significantly fewer feedback signals than spontaneous dialogues (supported by both lexical statistics and neural tagging across multiple languages)
- Medium Confidence: Subtitles contain a higher proportion of negative feedback (supported but sensitive to tagging thresholds and class definitions)
- Low Confidence: Qualitative assessment of LLM feedback handling (based on limited examples and manual inspection)

## Next Checks
1. Manually translate a small sample of spontaneous dialogues from two non-English corpora and compare dialogue act tagger outputs between machine-translated and human-translated versions
2. Conduct a focused analysis on a subset of spontaneous dialogues to identify feedback signals missed by current cue word lists, then expand the lists and measure the impact
3. Evaluate the neural dialogue act tagger's performance on each spontaneous corpus individually using available gold-standard annotations to identify potential bias sources