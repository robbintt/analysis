---
ver: rpa2
title: 'SCOTT: Self-Consistent Chain-of-Thought Distillation'
arxiv_id: '2305.01879'
source_url: https://arxiv.org/abs/2305.01879
tags:
- rationales
- student
- answer
- more
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning a small, faithful
  chain-of-thought (CoT) model from a large teacher model. The key challenges are
  that teacher models often generate inconsistent rationales and students may treat
  rationale generation and answer prediction as independent processes.
---

# SCOTT: Self-Consistent Chain-of-Thought Distillation

## Quick Facts
- arXiv ID: 2305.01879
- Source URL: https://arxiv.org/abs/2305.01879
- Reference count: 3
- Key outcome: SCOTT improves chain-of-thought rationale faithfulness while maintaining task performance through contrastive decoding and counterfactual reasoning

## Executive Summary
This paper addresses the challenge of distilling faithful chain-of-thought (CoT) reasoning from large teacher models to smaller student models. The key insight is that standard distillation approaches often produce rationales that are inconsistent with final answers, undermining their utility. SCOTT introduces two complementary techniques: contrastive decoding to generate more consistent rationales from the teacher, and counterfactual reasoning to train students that actually use the rationales rather than treating them as independent from answer prediction.

## Method Summary
SCOTT works by first generating training data using a large teacher model with contrastive decoding, which prefers tokens that become more plausible when the gold answer is considered. This produces rationales that are more consistent with their corresponding answers. The student model is then trained using both factual and counterfactual reasoning objectives - the latter requiring the student to generate different answers when given rationales that lead to incorrect answers. This dual approach addresses both the quality of the training data and how students utilize that data.

## Key Results
- SCOTT achieves 86.3% LAS (faithfulness) score compared to 82.1% for standard greedy decoding on StrategyQA
- The method maintains comparable end-task performance while improving rationale faithfulness across all tested datasets
- Ablation studies show contrastive decoding improves LAS by 3.6% and counterfactual training adds another 1.5% improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive decoding leads to more consistent rationales by penalizing tokens that are plausible without considering the answer
- Mechanism: The contrastive decoding approach calculates the plausibility growth of each token when the gold answer is considered versus when a perturbed answer is used. Tokens with higher plausibility growth are preferred, encouraging the model to generate rationales that are more grounded in the answer.
- Core assumption: The teacher model can distinguish between tokens that are generally plausible versus tokens that are plausible specifically when the answer is considered
- Evidence anchors:
  - [abstract] "we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered"
  - [section] "we incorporate the plausibility growth into Eq. 1 by aggregation as our final contrastive decoding strategy"
  - [corpus] Weak - the corpus provides related papers but doesn't directly validate the contrastive decoding mechanism

### Mechanism 2
- Claim: Counterfactual reasoning training removes the reasoning shortcut between questions and answers
- Mechanism: By training the student to answer differently when given rationales that lead to incorrect answers, the model learns to actually use the rationale rather than just correlating questions with answers
- Core assumption: The student model has sufficient capacity to learn the counterfactual mapping without simply memorizing answer patterns
- Evidence anchors:
  - [abstract] "we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions"
  - [section] "we train the model to generate a' when r' is directly fed to the decoder as teacher-forcing"
  - [corpus] Weak - related papers discuss CoT distillation but don't specifically validate counterfactual reasoning

### Mechanism 3
- Claim: The combination of contrastive decoding and counterfactual training produces students that are both more faithful and maintain task performance
- Mechanism: The contrastive decoding ensures the training data is higher quality (more consistent rationales), while counterfactual training ensures the student actually uses these rationales. Together they address both sources of inconsistency.
- Core assumption: The improvements from higher-quality training data and better utilization of that data are additive rather than conflicting
- Evidence anchors:
  - [abstract] "Experiments show that, while yielding comparable end-task performance, our method can generate CoT rationales that are more faithful than baselines do"
  - [section] "we observe that students trained with the rationales from contrastive decoding... generally achieve higher LAS scores compared to the baselines"
  - [corpus] Weak - the corpus provides context but doesn't directly validate the combined effect

## Foundational Learning

- Concept: Contrastive learning principles
  - Why needed here: Understanding how contrasting positive and negative examples can shape model behavior is fundamental to grasping why contrastive decoding works
  - Quick check question: What is the key difference between how contrastive decoding and standard greedy decoding select tokens?

- Concept: Knowledge distillation fundamentals
  - Why needed here: The entire approach relies on transferring knowledge from a large teacher to a smaller student model, which requires understanding the basics of distillation
  - Quick check question: In standard knowledge distillation, what is typically transferred from teacher to student?

- Concept: Counterfactual reasoning
  - Why needed here: The counterfactual training objective is central to making the student actually use the rationales rather than just memorizing answer patterns
  - Quick check question: How does counterfactual reasoning in this context differ from standard supervised learning?

## Architecture Onboarding

- Component map:
  Teacher model (GPT-neox) with contrastive decoding module -> Training data generation pipeline (rationale + answer pairs) -> Student model (T5 variants) with counterfactual reasoning training objective -> Evaluation framework (LAS metric, accuracy metrics)

- Critical path:
  1. Generate training data using contrastive decoding on teacher
  2. Train student with factual and counterfactual objectives
  3. Evaluate student faithfulness and task performance
  4. Iterate with different model sizes and decoding strategies

- Design tradeoffs:
  - Larger student models achieve better task performance but lower faithfulness
  - Contrastive decoding with wrong answers vs empty strings affects consistency differently
  - Adding counterfactual training improves faithfulness but requires more training data and computation

- Failure signatures:
  - Student performance drops significantly compared to baselines
  - LAS scores remain low despite using contrastive decoding
  - Counterfactual training causes instability or convergence issues

- First 3 experiments:
  1. Compare LAS scores between greedy decoding and contrastive decoding baselines to validate the teacher improvement
  2. Test student faithfulness with and without counterfactual training objective on a small dataset
  3. Evaluate performance trade-offs across different student model sizes (T5-base, T5-large, T5-3B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SCOTT compare to larger models when using the same amount of compute resources?
- Basis in paper: [explicit] The paper mentions that larger student models achieve higher performance but lower faithfulness, and discusses the trade-off between model size and faithfulness.
- Why unresolved: The paper does not directly compare the performance of SCOTT to larger models using the same compute resources.
- What evidence would resolve it: A controlled experiment comparing SCOTT's performance to larger models while keeping the compute resources constant.

### Open Question 2
- Question: Can SCOTT be applied to other types of reasoning tasks beyond open-domain question answering, such as mathematical or logical reasoning?
- Basis in paper: [inferred] The paper focuses on open-domain question answering tasks but does not explicitly discuss the applicability of SCOTT to other types of reasoning tasks.
- Why unresolved: The paper does not explore the potential of SCOTT in other reasoning domains.
- What evidence would resolve it: Experiments applying SCOTT to different types of reasoning tasks and comparing its performance to baseline methods.

### Open Question 3
- Question: How does the performance of SCOTT change when using different teacher models with varying sizes and capabilities?
- Basis in paper: [explicit] The paper uses GPT-neox with 20B parameters as the teacher model but does not explore the impact of using different teacher models.
- Why unresolved: The paper does not investigate the effect of using teacher models with different sizes and capabilities on the performance of SCOTT.
- What evidence would resolve it: Experiments using different teacher models with varying sizes and capabilities and comparing the performance of the resulting student models.

## Limitations

- The paper relies on a specific faithfulness metric (LAS) that may not fully capture the practical utility of rationales in real-world applications
- The scalability analysis shows fundamental trade-offs between model size and faithfulness that aren't fully explained
- The contrastive decoding mechanism's effectiveness depends on the quality of the teacher model's ability to distinguish between plausible and implausible tokens

## Confidence

- Confidence: Medium - The paper demonstrates improved faithfulness metrics but the human evaluation setup has limitations
- Confidence: Low - The scalability analysis shows unexplained trade-offs between performance and faithfulness
- Confidence: Medium - The contrastive decoding mechanism relies on assumptions about token plausibility that aren't thoroughly validated

## Next Checks

1. Conduct ablation studies testing different perturbation strategies in contrastive decoding (random answers, unanswerable questions, contradictory statements) to determine optimal approaches across different question types.

2. Perform granular scaling analysis with additional model sizes between T5-large and T5-3B to better understand the architectural factors affecting faithfulness-performance trade-offs.

3. Evaluate rationale stability across paraphrased inputs and additional context to assess practical robustness beyond static test set evaluation.