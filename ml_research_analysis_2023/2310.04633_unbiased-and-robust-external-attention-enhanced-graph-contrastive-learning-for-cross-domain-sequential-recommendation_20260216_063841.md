---
ver: rpa2
title: 'Unbiased and Robust: External Attention-enhanced Graph Contrastive Learning
  for Cross-domain Sequential Recommendation'
arxiv_id: '2310.04633'
source_url: https://arxiv.org/abs/2310.04633
tags:
- uni00000003
- graph
- learning
- ea-gcl
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles two types of bias in cross-domain sequential
  recommendation: inter-domain density bias (caused by asymmetric interaction densities
  across domains) and training bias from batch-based self-attention. To address these
  issues, the authors propose EA-GCL, a framework that uses graph contrastive learning
  and an external attention mechanism.'
---

# Unbiased and Robust: External Attention-enhanced Graph Contrastive Learning for Cross-domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2310.04633
- Source URL: https://arxiv.org/abs/2310.04633
- Reference count: 37
- Key outcome: EA-GCL improves NDCG@10 by up to 5.2%, MRR@10 by 3.4%, and Recall@10 by 4.7% in the sparser domain over state-of-the-art baselines.

## Executive Summary
This paper addresses two critical biases in cross-domain sequential recommendation: inter-domain density bias caused by asymmetric interaction densities and training bias from batch-based self-attention. The authors propose EA-GCL, a framework that combines graph contrastive learning with an external attention mechanism. The method constructs cross-domain sequential graphs, applies multi-task learning with a self-supervised auxiliary task to remove inter-domain density bias, and uses an MLP-based external attention network to learn sequence representations globally across batches. Experiments on real-world datasets demonstrate significant improvements over state-of-the-art baselines, particularly in the sparser domain.

## Method Summary
EA-GCL constructs Cross-Domain Sequential (CDS) graphs from hybrid interaction sequences of overlapped users, modeling both user-item interactions and sequential relationships. It uses NGCF as the base graph encoder to learn node representations, then applies graph contrastive learning with Item Dropout and Sequence Reorder augmentations to generate self-supervised signals. An external attention mechanism with MLP-based memory-sharing structure learns sequence representations across batches. The model is trained using multi-task learning with both supervised cross-entropy loss and self-supervised InfoNCE loss to jointly optimize recommendation accuracy and bias mitigation.

## Key Results
- Up to 5.2% improvement on NDCG@10 in the sparser domain
- 3.4% improvement on MRR@10 compared to state-of-the-art baselines
- 4.7% improvement on Recall@10 in the sparser domain
- Demonstrated effectiveness in addressing both inter-domain density bias and batch-based training bias

## Why This Works (Mechanism)

### Mechanism 1
The external attention network removes batch-based training bias by using a memory-sharing structure that can attend across batches. Unlike self-attention, which only computes correlations within a single batch, the external attention mechanism stores and reuses attention weights in an MLP-based memory unit. This allows each item to attend to all items in the sequence regardless of batch boundaries, avoiding the loss of global item-item correlations during training. The external MLP memory can be effectively shared and reused across different batches without introducing significant noise or computational overhead.

### Mechanism 2
The graph contrastive learning auxiliary task mitigates inter-domain density bias by aligning node representations across perturbed views. Two augmentation strategies—Item Dropout and Sequence Reorder—are applied to the denser domain's subgraph to generate positive pairs. The contrastive loss maximizes agreement between these views while minimizing agreement with views from other batches, forcing the model to learn domain-invariant node representations. Perturbing the denser domain is sufficient to create informative contrastive pairs that help align the sparser domain without losing meaningful structure.

### Mechanism 3
Multi-task learning with the SSL auxiliary task and supervised loss jointly reduces both inter-domain density bias and improves recommendation accuracy. The supervised loss learns from interaction data, while the SSL loss aligns representations across domains. Their combination under a weighted sum allows the model to balance between fitting the observed data and learning robust, unbiased features. The supervised and self-supervised signals are complementary rather than conflicting, and their balance can be tuned via a single hyper-parameter (β).

## Foundational Learning

- **Graph Neural Networks (GNNs) for collaborative filtering**: EA-GCL uses NGCF to propagate user-item interactions in the cross-domain sequential graph, capturing high-order connectivity. Quick check: What is the difference between first-order and high-order connectivity in a user-item bipartite graph?

- **Contrastive learning and InfoNCE loss**: The auxiliary SSL task relies on maximizing agreement between augmented views of the same graph while minimizing agreement with other views, formalized by InfoNCE. Quick check: In contrastive learning, what is the role of negative samples, and how are they constructed in this work?

- **Attention mechanisms and the distinction between self-attention and external attention**: The sequence encoder uses an external attention mechanism to compute item-item correlations globally, unlike self-attention which is confined to a batch. Quick check: How does the attention computation in external attention differ from standard self-attention in terms of scope and data flow?

## Architecture Onboarding

- **Component map**: Cross-Domain Sequential (CDS) Graph Construction → Graph Encoder (NGCF) → Node Representations (eU, eA, eB) → Graph Contrastive Learning (GCL) → External Attention Sequence Encoder → Sequence Representations (HSA, HSB) → Prediction Layers → Loss (supervised + SSL) → Joint Training

- **Critical path**: Graph construction → NGCF propagation → node embeddings → external attention → final prediction; GCL is a parallel path that updates node embeddings via contrastive signals.

- **Design tradeoffs**: Using NGCF vs. more complex GNN layers: simpler but still captures high-order connectivity; more layers may overfit. External attention vs. self-attention: removes batch bias but may increase memory usage. Perturbing only domain A: safer for sparser domain but may limit diversity of contrastive views.

- **Failure signatures**: Node embeddings collapse or become domain-biased → check GCL loss and perturbation strategy. Sequence representations are noisy or inconsistent across batches → inspect external attention memory and batch processing logic. Training instability or slow convergence → tune β and SSL regularization, check learning rate.

- **First 3 experiments**: 1) Train EA-GCL without the GCL auxiliary task to confirm the role of contrastive learning. 2) Replace external attention with self-attention and compare batch bias effects. 3) Vary the perturbation ratio α and measure the effect on domain B performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of external attention-based sequence encoding compare to other attention mechanisms in CSR scenarios beyond batch-based training bias? The authors compare external attention to self-attention and claim it alleviates batch-based training bias, but do not explore other attention mechanisms. Comparative studies evaluating external attention against other attention variants (e.g., sparse attention, local attention) in diverse CSR settings and bias conditions would resolve this.

### Open Question 2
What is the optimal balance between supervised and self-supervised loss components in multi-task learning for CSR, and how does this vary across different domain density imbalances? The authors use a multi-task learning approach with supervised and self-supervised components, but the optimal balance is not explored. Empirical studies varying the balance of supervised and self-supervised losses across different CSR datasets with varying domain density characteristics would resolve this.

### Open Question 3
How does the performance of graph contrastive learning for bias mitigation in CSR generalize to domains with more than two domains or non-sequential recommendation scenarios? The framework is designed for two-domain sequential recommendation with specific bias types, but its generalizability is not tested. Experimental validation of the framework in multi-domain settings and non-sequential recommendation tasks to assess generalizability would resolve this.

## Limitations
- Implementation details of the external attention mechanism are not fully specified, making exact reproduction challenging
- The sensitivity of the model to hyperparameter choices (α, β) is not thoroughly explored
- Generalizability to datasets with different density patterns or more than two domains is not discussed

## Confidence

**High**: The identification of inter-domain density bias and batch-based training bias as problems is well-justified. The general framework architecture is clearly described.

**Medium**: The specific implementation details of the external attention mechanism are somewhat vague, making exact reproduction challenging. The choice of perturbing only the denser domain for contrastive learning is reasonable but not extensively validated.

**Low**: The ablation studies are mentioned but not detailed in the provided text. The specific impact of each component on the final performance gains is not quantified.

## Next Checks

1. Implement a variant of EA-GCL using standard self-attention and compare batch bias effects on sequence representations
2. Vary the perturbation degree α systematically and measure its effect on domain B's recommendation performance
3. Conduct an ablation study removing the GCL auxiliary task to quantify its contribution to the overall performance gain