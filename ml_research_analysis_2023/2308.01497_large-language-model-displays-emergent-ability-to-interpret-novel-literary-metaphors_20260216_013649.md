---
ver: rpa2
title: Large Language Model Displays Emergent Ability to Interpret Novel Literary
  Metaphors
arxiv_id: '2308.01497'
source_url: https://arxiv.org/abs/2308.01497
tags:
- metaphors
- metaphor
- gpt-4
- interpretations
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested GPT-4\u2019s ability to interpret novel literary\
  \ metaphors by comparing its performance to human college students. Researchers\
  \ used 55 metaphors translated from Serbian poetry to ensure novelty, as these were\
  \ unlikely to have been in GPT-4\u2019s training data."
---

# Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors

## Quick Facts
- arXiv ID: 2308.01497
- Source URL: https://arxiv.org/abs/2308.01497
- Reference count: 0
- Primary result: GPT-4's metaphor interpretations were rated superior to human college students

## Executive Summary
This study demonstrates that GPT-4 can interpret novel literary metaphors at a level approaching or exceeding human performance. Researchers tested GPT-4 using 55 metaphors from Serbian poetry, ensuring novelty by using translations unlikely to be in the model's training data. Both GPT-4 and human participants generated interpretations for metaphors in their original and reversed forms. Blind human judges rated GPT-4's interpretations as superior to those of the students. Additionally, GPT-4's performance on standard metaphor benchmarks approached human-level accuracy, suggesting an emergent ability to interpret complex metaphors in unfamiliar poetic contexts.

## Method Summary
The study used 55 Serbian literary metaphors translated into English, testing them in both canonical (X is Y) and reversed (Y is X) forms. GPT-4 generated interpretations using a zero-shot prompt: "Please provide an expression for the following expression: <METAPHOR>". Human college students provided interpretations for each metaphor in either canonical or reversed form. Three blind human judges scored all interpretations on a 0-2 scale for aptness and appropriateness, without knowing whether they came from AI or humans. The study also included standard metaphor benchmarks (RAT and CMT) to compare GPT-4's performance against human norms.

## Key Results
- GPT-4's metaphor interpretations were rated as superior to human college students by blind judges
- GPT-4's performance on standard metaphor benchmarks approached human-level accuracy
- GPT-4 restored canonical meanings of reversed metaphors at rates comparable to humans, demonstrating sensitivity to pragmatic constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can interpret novel literary metaphors by leveraging its training on large text corpora, including diverse linguistic patterns.
- Mechanism: The model's transformer architecture allows it to transform embeddings based on differential attention to context, enabling it to understand and generate interpretations of novel metaphors.
- Core assumption: GPT-4 has been exposed to a wide range of linguistic patterns during training, allowing it to recognize and interpret novel metaphors.
- Evidence anchors:
  - [abstract] GPT-4's ability to interpret novel literary metaphors drawn from Serbian poetry.
  - [section] The transformer algorithm underlying GPT-4 transforms embeddings based on differential attention to context.
- Break condition: If GPT-4 has not been exposed to sufficient linguistic diversity during training, it may struggle to interpret novel metaphors.

### Mechanism 2
- Claim: GPT-4's performance on metaphor interpretation is enhanced by its ability to integrate conceptual combination with analogical reasoning.
- Mechanism: GPT-4 uses a process of conceptual combination, modifying semantic vectors to increase similarity between source and target concepts, while also employing analogical reasoning to draw parallels between different domains.
- Core assumption: GPT-4 has developed the ability to perform both conceptual combination and analogical reasoning, allowing it to interpret complex metaphors.
- Evidence anchors:
  - [abstract] GPT-4's interpretations of metaphors were rated as superior to those of human college students.
  - [section] GPT-4's performance on metaphor interpretation approaches human-level accuracy.
- Break condition: If GPT-4 lacks the ability to perform either conceptual combination or analogical reasoning, its performance on metaphor interpretation may be limited.

### Mechanism 3
- Claim: GPT-4's ability to interpret reversed metaphors demonstrates its sensitivity to pragmatic constraints on communication.
- Mechanism: GPT-4 restores the canonical meaning of reversed metaphors, indicating an understanding of the Gricean cooperative principle and the importance of effective communication.
- Core assumption: GPT-4 has developed an understanding of pragmatic constraints and the importance of maintaining coherence in communication.
- Evidence anchors:
  - [abstract] GPT-4's interpretations of reversed metaphors restored their canonical meaning at about the same rate as humans.
  - [section] GPT-4's sensitivity to pragmatic constraints is evident in its interpretation of reversed metaphors.
- Break condition: If GPT-4 does not understand pragmatic constraints, it may struggle to interpret reversed metaphors effectively.

## Foundational Learning

- Concept: Understanding of metaphorical language
  - Why needed here: To interpret novel literary metaphors, GPT-4 must understand the underlying principles of metaphorical language and how it differs from literal language.
  - Quick check question: Can GPT-4 distinguish between literal and metaphorical language in a given context?

- Concept: Analogical reasoning
  - Why needed here: GPT-4's ability to interpret metaphors relies on its capacity to draw parallels between different domains and concepts.
  - Quick check question: Can GPT-4 identify and apply analogical relationships between seemingly unrelated concepts?

- Concept: Pragmatic constraints on communication
  - Why needed here: GPT-4's interpretation of reversed metaphors demonstrates its understanding of the importance of maintaining coherence and effective communication.
  - Quick check question: Can GPT-4 identify and apply pragmatic constraints in a given communication context?

## Architecture Onboarding

- Component map: Transformer model with multiple layers -> Context-aware embedding transformations -> Metaphor interpretation output
- Critical path: Metaphor recognition -> Conceptual combination/analogical reasoning -> Pragmatic constraint application -> Interpretation generation
- Design tradeoffs: Model performance depends on training data diversity and quality vs. computational efficiency and response coherence
- Failure signatures: Struggles with metaphor interpretation may indicate insufficient exposure to diverse linguistic patterns or limitations in applying conceptual combination and analogical reasoning
- First 3 experiments:
  1. Test GPT-4's ability to distinguish between literal and metaphorical language in various contexts
  2. Evaluate GPT-4's performance on interpreting metaphors with varying levels of complexity and novelty
  3. Assess GPT-4's sensitivity to pragmatic constraints by testing its interpretation of reversed metaphors and its ability to maintain coherence in communication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific computational processes enable GPT-4 to interpret novel metaphors, and are they analogous to human metaphor comprehension mechanisms like conceptual combination or analogical reasoning?
- Basis in paper: [explicit] The paper discusses theories of human metaphor comprehension including conceptual combination and analogical reasoning, and notes that GPT-4's mechanisms remain unknown due to lack of transparency about its training data and internal representations.
- Why unresolved: GPT-4's developers have not provided detailed information about its training data or internal representations, making it impossible to determine how it processes metaphors.
- What evidence would resolve it: Access to GPT-4's internal representations during metaphor interpretation, or detailed information about its training data and architectural mechanisms that would allow researchers to trace how it processes metaphorical language.

### Open Question 2
- Question: Can large language models generate genuinely novel metaphors rather than variations of existing ones, and if so, what distinguishes their creative process from human poetic metaphor creation?
- Basis in paper: [explicit] The paper explicitly states that while LLMs can interpret metaphors, they have not been shown to create genuinely novel metaphors, and raises the question of whether they can invent metaphors that don't belong to accepted patterns.
- Why unresolved: The paper acknowledges this capability has not been demonstrated, and previous attempts at having LLMs write poetry have been criticized as producing derivative work rather than truly novel metaphors.
- What evidence would resolve it: Systematic evaluation of LLM-generated metaphors by literary critics to determine if they contain genuinely novel conceptual mappings not derived from existing human-created metaphors.

### Open Question 3
- Question: How does GPT-4's ability to interpret reversed metaphors relate to its understanding of pragmatic principles like Grice's cooperative principle, and is this understanding fundamental or superficial?
- Basis in paper: [explicit] The paper notes that GPT-4, like humans, frequently restored the canonical meaning of reversed metaphors, suggesting sensitivity to pragmatic constraints on communication, but questions whether this reflects genuine understanding.
- Why unresolved: While GPT-4 shows similar behavior to humans in restoring canonical meanings, it's unclear whether this reflects actual comprehension of pragmatic principles or learned patterns from training data.
- What evidence would resolve it: Testing GPT-4's behavior on novel pragmatic violations beyond metaphor reversal, such as implicature, conversational maxims, and other Gricean principles, to determine if its responses reflect systematic understanding or pattern matching.

## Limitations
- The metaphors were drawn from Serbian poetry and translated into English, raising questions about the novelty claim given GPT-4's multilingual training
- The study used a single prompt format without exploring prompt engineering variations that might affect performance
- Human judges were not given specific training on metaphor interpretation criteria before scoring

## Confidence
- High confidence: GPT-4 outperforms college students on metaphor interpretation tasks when using the specific prompt and scoring methodology described
- Medium confidence: GPT-4's performance approaches human-level accuracy on standard metaphor benchmarks (RAT and CMT)
- Medium confidence: The claim of "emergent ability" is supported, though the study doesn't definitively establish whether this ability was present in earlier models or emerged specifically in GPT-4

## Next Checks
1. Replicate the study using metaphors from different linguistic traditions and cultures to test the robustness of GPT-4's metaphorical reasoning across domains
2. Conduct a systematic prompt engineering study to determine optimal prompting strategies for metaphor interpretation tasks
3. Compare GPT-4's performance against domain experts (literature professors or professional poets) rather than just college students to establish a more rigorous benchmark