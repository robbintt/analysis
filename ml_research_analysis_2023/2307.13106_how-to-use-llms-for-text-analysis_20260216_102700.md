---
ver: rpa2
title: How to use LLMs for Text Analysis
arxiv_id: '2307.13106'
source_url: https://arxiv.org/abs/2307.13106
tags:
- text
- llms
- data
- populism
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a practical guide to using Large Language Models
  (LLMs) for text analysis in social science research. The authors demonstrate how
  to use LLMs like ChatGPT to analyze political texts for populism, a challenging
  task that has traditionally required manual coding.
---

# How to use LLMs for Text Analysis

## Quick Facts
- arXiv ID: 2307.13106
- Source URL: https://arxiv.org/abs/2307.13106
- Reference count: 23
- Primary result: LLMs achieve Krippendorff's alpha of 0.635 on populism detection, approaching human coder reliability of 0.827

## Executive Summary
This paper provides a practical guide for using Large Language Models (LLMs) like ChatGPT in social science text analysis. The authors demonstrate that LLMs can effectively analyze political texts for populism, a task traditionally requiring manual coding. Through API setup, prompt engineering, and iterative validation, LLMs achieve high inter-coder agreement with human coders while offering speed and cost advantages over traditional methods. The guide covers essential steps from initial setup to validation, making LLM-based analysis accessible to social scientists.

## Method Summary
The method involves using OpenAI's ChatGPT API with Python to analyze political speeches for populism. Researchers first set up API access and load text data into pandas DataFrames. They then engineer prompts based on established populism definitions and run LLM analysis on speech texts. Results are parsed and validated using Krippendorff's alpha against human-coded benchmarks. The process emphasizes iterative refinement of both prompts and conceptual understanding through comparison with human coding.

## Key Results
- LLMs achieve Krippendorff's alpha of 0.635 on populism detection, approaching human-human agreement of 0.827
- The method works well for complex interpretive tasks that traditional NLP methods struggle with
- LLMs offer a fast, cost-effective alternative to manual coding while maintaining high reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve high inter-coder agreement (Krippendorff's alpha of 0.635) on populism detection, approaching human coder reliability (0.827).
- Mechanism: LLMs leverage their training on vast text corpora to understand complex contextual and interpretive tasks that traditional NLP methods struggle with, such as identifying latent ideological concepts like populism.
- Core assumption: The model's training data includes sufficient political discourse examples to capture the nuanced features of populist rhetoric.
- Evidence anchors:
  - [abstract] "LLMs can achieve high inter-coder agreement (Krippendorff's alpha of 0.635) when measuring populism, approaching the reliability of human coders (0.827)"
  - [section] "The resulting a Krippendorﬀs alpha of 0.635. This is a relatively high correspondence – at least considering that the LLM was not even given the same coding instructions as the original coders."
- Break condition: If the training corpus lacks sufficient diversity in populist discourse across languages and contexts, or if populist rhetoric evolves beyond the model's training cutoff.

### Mechanism 2
- Claim: LLMs can handle complex interpretive tasks like detecting populism that require contextual understanding and knowledge of country-specific political periods.
- Mechanism: LLMs use their transformer architecture to process long text passages and understand relationships between concepts, allowing them to identify the dual requirements of people-centrism and anti-elitism that define populism.
- Core assumption: The model's context window (32,000 tokens for GPT-4) is sufficient to capture the full rhetorical structure of populist speeches.
- Evidence anchors:
  - [abstract] "LLMs move beyond the existing state-of-the-art" for measuring populism
  - [section] "scholars argue that measuring populism requires context-dependent understanding, knowledge of the country-government period, and the need to use longer passage of text as unit of measurement"
- Break condition: If text passages exceed the context window, or if the model cannot access relevant contextual knowledge not present in the text itself.

### Mechanism 3
- Claim: Iterative prompt engineering combined with validation creates a robust method for LLM-based text analysis.
- Mechanism: Researchers refine their conceptual understanding through an iterative process of prompt development, testing against validation data, and examining disagreements to construct rigorous and reproducible definitions of scientific concepts.
- Core assumption: The iterative process of comparing LLM outputs with human coding reveals meaningful insights about both the concept being measured and the model's capabilities.
- Evidence anchors:
  - [section] "When using LLMs for text analysis, it is recommended to use an iterative process of simultaneous prompt and concept development"
  - [section] "A useful strategy is to engage in a closer examination of the cases where the LLM and the coders disagree"
- Break condition: If researchers treat human coding as an unquestioned gold standard rather than engaging in genuine mutual learning, or if validation data is unavailable or of poor quality.

## Foundational Learning

- Concept: Krippendorff's alpha for measuring inter-coder reliability
  - Why needed here: The paper uses Krippendorff's alpha to quantify how well the LLM's populism classifications align with human-coded data, providing an objective measure of the method's validity
  - Quick check question: What does a Krippendorff's alpha of 0.635 indicate about the agreement between LLM and human coders compared to human-human agreement of 0.827?

- Concept: Context window and token limitations
  - Why needed here: Understanding that LLMs have finite context windows (2,000 tokens for GPT-3, 32,000 for GPT-4) is crucial for determining whether texts need to be split or truncated before analysis
  - Quick check question: If a political speech is 5,000 tokens long and you're using GPT-3, what preprocessing step must you take before analysis?

- Concept: Prompt engineering as qualitative method
  - Why needed here: The paper emphasizes that formulating precise instructions for the LLM is itself a form of qualitative method, encoding how researchers conceptualize social science concepts
  - Quick check question: Why does the paper suggest starting prompt development by looking at instructions given to human coders for similar tasks?

## Architecture Onboarding

- Component map: OpenAI API access with API key management -> Python environment with pandas and openai libraries -> Data loading into pandas DataFrame -> Prompt engineering module for creating analysis instructions -> LLM analysis function that calls the API -> Result parsing function to convert API output to structured data -> Validation module using Krippendorff's alpha -> Iterative refinement loop

- Critical path: API setup -> Data loading -> Prompt engineering -> LLM analysis -> Result parsing -> Validation -> Iteration. The most time-consuming step is typically the LLM analysis itself due to API call latency and costs.

- Design tradeoffs: Using LLMs trades computational efficiency for interpretive capability. While traditional NLP methods can process millions of data points quickly, LLMs require seconds per analysis but can handle tasks like irony detection and contextual inference that traditional methods cannot. Cost scales linearly with number of API calls.

- Failure signatures: High disagreement between LLM and validation data suggests prompt engineering issues or insufficient training data diversity. API call failures indicate network issues or rate limiting. Unexpected results may indicate model bias or training data limitations. Slow processing suggests the context window is being fully utilized, requiring text segmentation.

- First 3 experiments:
  1. Run a simple sentiment analysis on a small sample of texts using a basic prompt to verify API connectivity and basic functionality
  2. Test the populism prompt on 10 known populist and non-populist texts to evaluate initial performance before scaling up
  3. Compare LLM results with human-coded validation sample to establish baseline Krippendorff's alpha and identify disagreement patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform compared to human coders on tasks requiring subjective interpretation, such as identifying sarcasm or irony in political texts?
- Basis in paper: [explicit] The paper states that LLMs can handle complex interpretive tasks beyond traditional NLP approaches, but notes they may fail in unexpected ways on specific tasks.
- Why unresolved: While the paper shows LLMs can measure populism with good reliability, it doesn't directly compare their performance on more nuanced interpretive tasks like sarcasm or irony detection.
- What evidence would resolve it: A direct comparison study measuring LLM performance against human coders on tasks specifically designed to test sarcasm and irony detection in political texts.

### Open Question 2
- Question: What are the long-term effects of using LLMs for text analysis on the development of qualitative research methods in social sciences?
- Basis in paper: [inferred] The paper discusses how LLMs challenge the conventional division between quantitative and qualitative methods, but doesn't explore long-term methodological implications.
- Why unresolved: The paper presents LLMs as a new tool but doesn't examine how their widespread adoption might change research practices and methodological development over time.
- What evidence would resolve it: Longitudinal studies tracking changes in research methodologies and practices as LLMs become more integrated into social science research.

### Open Question 3
- Question: How can we develop more robust validation methods for LLM-based text analysis that account for the models' potential biases and limitations?
- Basis in paper: [explicit] The paper emphasizes the need for careful validation but acknowledges current methods may not fully capture the models' limitations and biases.
- Why unresolved: While the paper suggests comparing LLM results with human coding, it doesn't provide a comprehensive framework for validating LLMs that accounts for their unique characteristics.
- What evidence would resolve it: Development and testing of new validation frameworks specifically designed for LLM-based text analysis that incorporate bias detection and limitation assessment.

## Limitations
- Krippendorff's alpha of 0.635 for LLM-human agreement still shows notable gap compared to human-human agreement of 0.827
- Method performance on texts from different political contexts or time periods not represented in training data is unknown
- Computational costs and scalability for larger datasets are not quantified

## Confidence
- **High confidence**: The practical implementation steps (API setup, prompt engineering, result parsing) are clearly specified and reproducible
- **Medium confidence**: The claim that LLMs can achieve "high" inter-coder agreement (0.635) is supported by the reported data, but this metric alone may not fully capture the quality of analysis
- **Low confidence**: The paper does not address potential systematic biases in LLM outputs or how well the method generalizes beyond the specific populism dataset used

## Next Checks
1. Test the method on a diverse sample of political texts from different countries and time periods to assess generalizability across contexts
2. Conduct a systematic comparison of different prompt formulations to quantify their impact on inter-coder agreement and identify best practices
3. Analyze the computational costs and processing times for scaling up to larger datasets, including implementation of parallel processing or batch analysis strategies