---
ver: rpa2
title: Tensor Decomposition Based Attention Module for Spiking Neural Networks
arxiv_id: '2310.14576'
source_url: https://arxiv.org/abs/2310.14576
tags:
- attention
- tensor
- neural
- networks
- spiking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Projected Full-Attention (PFA) module
  for Spiking Neural Networks (SNNs) that leverages tensor decomposition to achieve
  temporal-channel-spatial attention. Unlike previous works that process event streams
  as tensors without considering their properties, PFA uses a linear projection of
  spike tensors (LPST) to compress the original spike tensor into three lower-dimensional
  tensors, followed by an attention map composing (AMC) module that reconstructs the
  attention map using the inverse process of tensor decomposition.
---

# Tensor Decomposition Based Attention Module for Spiking Neural Networks

## Quick Facts
- **arXiv ID**: 2310.14576
- **Source URL**: https://arxiv.org/abs/2310.14576
- **Reference count**: 40
- **Primary result**: Introduces Projected Full-Attention (PFA) module for SNNs achieving SOTA on static and dynamic datasets with linearly growing parameters

## Executive Summary
This paper presents a novel Projected Full-Attention (PFA) module that leverages tensor decomposition to enable efficient temporal-channel-spatial attention in Spiking Neural Networks. The approach compresses 4D spike tensors into three 2D projections using learnable parameters, then reconstructs attention maps through inverse CP decomposition. PFA achieves state-of-the-art performance on both static (CIFAR10/100) and dynamic (CIFAR10DVS, NCaltech-101) datasets while maintaining linear parameter scaling, making it computationally efficient for larger architectures.

## Method Summary
PFA integrates two modules: the Linear Projection of Spike Tensor (LPST) module and the Attention Map Composing (AMC) module. LPST projects the 4D input tensor (H×W×C×T) into three 2D matrices along temporal, channel, and spatial dimensions using learnable parameters. AMC then composes these projections into an attention map through inverse CP decomposition. The attention map is applied to the original input via Hadamard product. The method uses a small connecting factor r to maintain linear parameter scaling, with parameter count growing as C×r + T×r + k²×T×r. The approach is integrated into standard VGG/ResNet architectures with LIF neuron models and trained using spatial-temporal backpropagation.

## Key Results
- Achieves SOTA performance on CIFAR10 (97.06%), CIFAR100 (84.24%), CIFAR10DVS (86.62%), and NCaltech-101 (91.15%)
- Demonstrates linearly growing parameters with tensor rank (scales as C×r + T×r + k²×T×r)
- Ablation studies show each attention dimension contributes to accuracy, with spatial attention having largest impact
- Maintains high accuracy while achieving 2.6× parameter reduction compared to existing SNN models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor decomposition reduces dimensionality while preserving essential information for attention computation
- Mechanism: LPST module compresses the 4D input tensor into three 2D projections (temporal, channel, spatial) using learnable parameters, enabling efficient attention map reconstruction via inverse CP decomposition
- Core assumption: The input spike tensor has low-rank structure that can be effectively captured by three projections
- Evidence anchors:
  - [abstract] "we start by compressing the original spike tensor into three projected tensors using a single property-preserving strategy with learnable parameters for each dimension"
  - [section 3.2] "we adopt the reverse process of the tensor CP decomposition to generate the attention map"
  - [corpus] Weak correlation with related tensor decomposition papers (FMR ~0.0-0.0)
- Break condition: If input tensor rank is too high for low-rank approximation to capture sufficient information, attention quality degrades

### Mechanism 2
- Claim: Linear parameter growth makes PFA scalable for larger datasets and architectures
- Mechanism: Parameter count scales as C×r + T×r + k²×T×r, where r is a small fixed connecting factor, making it linear in time steps T and channels C
- Core assumption: The connecting factor r remains small (typically < T/2) based on theoretical analysis of tensor rank
- Evidence anchors:
  - [abstract] "demonstrates excellent results with linearly growing parameters"
  - [section 3.2] "the parameter amount is C×r + T×r + k²×T×r"
  - [section 3.3] "we propose a principle for selecting r: For dynamic datasets, the search for the optimal value of r is performed within the range of no more than T"
  - [corpus] No direct evidence, but weak FMR with tensor optimization papers suggests independent approach
- Break condition: If r needs to scale with input size for accuracy, parameter efficiency is lost

### Mechanism 3
- Claim: Three-dimensional attention (temporal-channel-spatial) improves SNN performance over single-dimension approaches
- Mechanism: AMC module composes attention map from three separate projections, allowing each dimension to contribute independently to the final attention
- Core assumption: Different dimensions contain complementary information that benefits from separate processing before fusion
- Evidence anchors:
  - [abstract] "PFA is composed by the linear projection of spike tensor (LPST) module and attention map composing (AMC) module"
  - [section 4.4] "whichever dimension we ablate, the performance drops" showing each dimension contributes to accuracy
  - [section 4.5] "The attention maps corresponding to the temporal and channel dimensions...are obtained from the deep layer" indicating different dimensional importance
  - [corpus] Weak FMR with attention-related SNN papers (0.523) suggests novel approach
- Break condition: If attention computation becomes bottleneck or if one dimension dominates attention map composition

## Foundational Learning

- Concept: Tensor decomposition (CP decomposition)
  - Why needed here: Provides mathematical framework for dimensionality reduction while preserving tensor structure
  - Quick check question: What does the connecting factor r represent in CP decomposition?

- Concept: Leaky Integrate and Fire (LIF) neuron model
  - Why needed here: Standard spiking neuron model used for simulation and backpropagation
  - Quick check question: How does the LIF model differ from standard artificial neurons in terms of information encoding?

- Concept: Neuromorphic computing principles
  - Why needed here: Understanding event-driven computation and temporal encoding is crucial for SNN design
  - Quick check question: Why are spiking neural networks considered more energy-efficient than traditional ANNs?

## Architecture Onboarding

- Component map:
  - Input: 4D spike tensor (H×W×C×T)
  - LPST module: Three linear projections (temporal, channel, spatial)
  - AMC module: Inverse CP decomposition to compose attention map
  - Fusion: Hadamard product between attention map and input tensor
  - Output: Refined tensor for downstream processing

- Critical path: Input → LPST → AMC → Fusion → Output
  - LPST creates projections in parallel
  - AMC combines projections into attention map
  - Fusion applies attention to input

- Design tradeoffs:
  - LPST uses simple linear layers rather than complex nonlinear mappings to reduce parameters
  - r is fixed and small to maintain linear parameter scaling
  - Single property-preserving strategy used across all dimensions for consistency

- Failure signatures:
  - Accuracy drops when r is too small (insufficient rank) or too large (overfitting)
  - Training instability if membrane potential distribution is poorly regularized
  - Memory issues if time steps T becomes very large without architectural adjustments

- First 3 experiments:
  1. Verify parameter scaling: Measure parameter count as T and C vary
  2. Ablation study: Remove each projection dimension to confirm contribution
  3. Rank analysis: Compute tensor rank of sample input data to validate theoretical assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal connecting factor (r) value for PFA modules across different datasets and architectures?
- Basis in paper: [explicit] The paper shows r values of 3, 1, 8, 8 yield best results for CIFAR10, CIFAR100, CIFAR10DVS, and NCaltech-101 respectively, but doesn't provide a general selection principle.
- Why unresolved: The paper only tested a limited range of r values and different datasets showed different optimal values, suggesting the relationship between r and dataset characteristics needs further investigation.
- What evidence would resolve it: Systematic experiments varying r across diverse datasets and architectures, combined with theoretical analysis linking optimal r to dataset properties like temporal dynamics and spatial complexity.

### Open Question 2
- Question: How does the rank of the attention map relate to the sparsity of the SNN and network performance?
- Basis in paper: [explicit] The paper mentions that even when SNNs exhibit considerable sparsity, the rank of the attention map can remain low, but doesn't explore this relationship in depth.
- Why unresolved: The paper only provides a toy example showing rank around 30 for CIFAR10DVS, but doesn't establish a general relationship between SNN sparsity, attention map rank, and classification accuracy.
- What evidence would resolve it: Experiments systematically varying SNN sparsity levels and measuring resulting attention map ranks and classification performance across multiple datasets.

### Open Question 3
- Question: How does the attention distribution across temporal, channel, and spatial dimensions affect different classification tasks?
- Basis in paper: [explicit] The ablation study shows spatial attention has the largest impact, followed by temporal and channel, but doesn't explore how this varies by task.
- Why unresolved: The ablation study only tested one architecture (VGGSNN) on one dataset (CIFAR10DVS), leaving unclear whether the attention distribution priorities generalize.
- What evidence would resolve it: Ablation studies across multiple architectures and diverse classification tasks, comparing attention distribution impacts on different types of visual recognition problems.

## Limitations
- Implementation details for LPST and AMC modules are not fully specified, particularly the inverse CP decomposition process
- Mathematical assumption of low-rank tensor structure may not hold for all input data types
- Empirical validation across diverse architectures beyond VGG and ResNet is limited

## Confidence
- **High Confidence**: The linear parameter scaling property and its mathematical derivation are well-established
- **Medium Confidence**: The effectiveness of three-dimensional attention is demonstrated through ablation studies, though the relative importance of each dimension varies by dataset
- **Low Confidence**: The specific implementation details of LPST and AMC modules, particularly the inverse CP decomposition process, are not fully specified

## Next Checks
1. **Rank Analysis Validation**: Compute the actual tensor rank of sample input data from each dataset to verify the assumption that r << T holds in practice
2. **Architecture Transferability Test**: Implement PFA in architectures beyond VGG and ResNet (e.g., MobileNet, EfficientNet) to assess generalizability
3. **Parameter Sensitivity Analysis**: Systematically vary r across a wider range and measure the trade-off between accuracy and parameter efficiency to identify optimal values for different dataset types