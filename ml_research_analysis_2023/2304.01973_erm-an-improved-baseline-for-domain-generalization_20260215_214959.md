---
ver: rpa2
title: 'ERM++: An Improved Baseline for Domain Generalization'
arxiv_id: '2304.01973'
source_url: https://arxiv.org/abs/2304.01973
tags:
- training
- performance
- domain
- data
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of Domain Generalization (DG)
  where a model is trained on multiple source domains and evaluated on unseen target
  domains. The core method, ERM++, improves upon the standard Empirical Risk Minimization
  (ERM) baseline by incorporating several key techniques: 1) Utilizing the full training
  data, 2) Implementing long training and early stopping based on validation data,
  3) Leveraging strong pre-trained model initialization, 4) Unfreezing BatchNorm parameters,
  and 5) Applying model parameter averaging for weight-space regularization.'
---

# ERM++: An Improved Baseline for Domain Generalization

## Quick Facts
- arXiv ID: 2304.01973
- Source URL: https://arxiv.org/abs/2304.01973
- Reference count: 40
- One-line primary result: ERM++ achieves over 5% improvement over state-of-the-art on standard DG benchmarks using ResNet-50 and over 15% with ViT-B/16

## Executive Summary
ERM++ presents a significantly improved baseline for Domain Generalization (DG) by enhancing the standard Empirical Risk Minimization (ERM) approach. The method incorporates five key techniques: full utilization of training data, long training with early stopping based on validation data, strong pre-trained model initialization, unfreezing BatchNorm parameters, and model parameter averaging for weight-space regularization. These modifications result in substantial performance gains across multiple DG benchmarks, outperforming previous state-of-the-art methods by over 5% on standard datasets and over 15% with Vision Transformer models.

## Method Summary
ERM++ builds upon the standard ERM baseline by implementing five key improvements. First, it utilizes the full training dataset including validation data to maximize learning from available samples. Second, it employs long training durations (4× standard) with early stopping determined by validation performance rather than fixed iteration counts. Third, it leverages strong pre-trained initialization using AugMix weights. Fourth, it unfreezes BatchNorm parameters during training to prevent overfitting to source domain statistics. Finally, it applies model parameter averaging after a burn-in period to regularize the weight space. These modifications work synergistically to create a more robust and generalizable model.

## Key Results
- ERM++ achieves over 5% improvement over state-of-the-art on standard DG benchmarks using ResNet-50
- With ViT-B/16 architecture, ERM++ demonstrates over 15% improvement compared to previous methods
- The approach shows consistent performance gains across five standard DG datasets (OfficeHome, PACS, VLCS, DomainNet, TerraIncognita) and the challenging WILDS-FMOW dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the full training dataset (Full Data) improves DG performance by preventing underfitting due to data loss.
- Mechanism: Instead of splitting data into train/validation sets, the entire dataset is used for final model training, maximizing the information available for learning generalizable features.
- Core assumption: The performance gain from more training data outweighs the loss of a separate validation set for hyperparameter tuning.
- Evidence anchors:
  - [abstract] "We propose to employ all training samples from the source, including validation data, which expands knowledge about the task."
  - [section 3.1] "By creating a separate validation set we are sacrificing a significant portion of our labeled data."
- Break condition: If the dataset is extremely large and training on all data becomes computationally infeasible, or if the validation split is crucial for preventing overfitting on very small datasets.

### Mechanism 2
- Claim: Long training (LT) and early stopping based on validation data prevent premature convergence and improve DG performance.
- Mechanism: Training is extended beyond typical fixed step counts, and the optimal stopping point is determined by monitoring validation performance, ensuring the model has converged to a generalizable solution.
- Core assumption: Standard fixed step counts are insufficient for model convergence across diverse datasets with varying complexities.
- Evidence anchors:
  - [abstract] "we utilized an adaptive training procedure that would automatically determine the sufficient training length for a model to obtain the best performance."
  - [section 3.1] "This approach has two major drawbacks... Second, by training under a fixed (relatively small) number of iterations we ignore the varying convergence rates of different models..."
- Break condition: If the validation set is not representative of the true target distribution, leading to overfitting on the validation data instead of the source domains.

### Mechanism 3
- Claim: Unfreezing BatchNorm parameters (UBN) acts as a regularizer, preventing overfitting to source domains.
- Mechanism: Allowing BatchNorm parameters to be updated during training introduces noise into the feature normalization process, which prevents the model from overfitting to the specific statistics of the source domains.
- Core assumption: Frozen BatchNorm parameters can lead to overfitting by forcing the model to rely too heavily on the source domain statistics.
- Evidence anchors:
  - [abstract] "we find that frozen batch normalization leads to quick overfitting in the long-training regime."
  - [section 3.2.2] "With frozen BatchNorm, the initial training is faster but it overfits."
- Break condition: If the source domains are extremely diverse and BatchNorm updates cause instability in the training process, or if the target domains have very different statistics than the source domains.

## Foundational Learning

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: ERM++ is built upon the ERM baseline, so understanding its principles is crucial for grasping the improvements.
  - Quick check question: What is the main difference between ERM and ERM++ in terms of how they handle training data?

- Concept: Domain Generalization (DG)
  - Why needed here: DG is the core problem being addressed, and ERM++ aims to improve generalization across unseen domains.
  - Quick check question: How does DG differ from domain adaptation, and why is generalization to unseen domains more challenging?

- Concept: Catastrophic Forgetting
  - Why needed here: ERM++ uses techniques like warmstart to prevent catastrophic forgetting of pre-trained features during fine-tuning.
  - Quick check question: What are the potential consequences of catastrophic forgetting in the context of DG, and how does warmstart mitigate this issue?

## Architecture Onboarding

- Component map: Data pipeline (Full Data, train/validation splits, early stopping) -> Model initialization (Pre-trained weights selection, warmstart, BatchNorm unfreezing) -> Training loop (Long training, model parameter averaging, weight-space regularization)
- Critical path: Data preparation → Model initialization → Training with full data and long training → Validation and early stopping → Final model with parameter averaging
- Design tradeoffs:
  - Full data vs. validation split: Maximizing training data vs. hyperparameter tuning capability
  - Long training vs. computational cost: Improved performance vs. increased training time
  - Unfreezing BatchNorm vs. stability: Regularization effect vs. potential instability
- Failure signatures:
  - Underfitting: Poor performance on both source and target domains
  - Overfitting: Good performance on source domains but poor generalization to target domains
  - Instability: Erratic training behavior due to BatchNorm updates or other factors
- First 3 experiments:
  1. Implement ERM++ on a simple dataset (e.g., OfficeHome) and compare performance to standard ERM.
  2. Experiment with different pre-trained weights (e.g., AugMix vs. TorchVision) and observe their impact on DG performance.
  3. Test the effect of unfreezing BatchNorm on a small dataset and analyze its impact on overfitting and generalization.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis, several important questions remain:

### Open Question 1
- Question: Does ERM++ perform equally well on datasets with larger domain shifts compared to datasets with subtle domain shifts?
- Basis in paper: [explicit] The paper shows ERM++ outperforms state-of-the-art on diverse datasets including TerraIncognita (realistic large domain shift) and VLCS (subtle domain shift), but doesn't directly compare performance across different magnitudes of domain shift.
- Why unresolved: The paper doesn't analyze the relationship between domain shift magnitude and ERM++ performance, leaving uncertainty about its effectiveness on extremely large domain shifts.
- What evidence would resolve it: A controlled experiment comparing ERM++ performance across datasets with systematically varied domain shift magnitudes (e.g., using synthetic domain shift generators) would clarify this.

### Open Question 2
- Question: How does ERM++ compare to specialized domain generalization methods when computational resources are severely limited?
- Basis in paper: [inferred] The paper demonstrates ERM++ achieves strong performance with less computational overhead than MIRO and DIW A, but doesn't explore scenarios with extreme resource constraints (e.g., mobile devices).
- Why unresolved: While the paper shows ERM++ is more efficient than competing methods, it doesn't evaluate its performance under strict computational budgets that might favor simpler, less accurate methods.
- What evidence would resolve it: Benchmarking ERM++ against specialized DG methods on resource-constrained hardware (e.g., mobile GPUs) with varying computational budgets would provide clarity.

### Open Question 3
- Question: Does ERM++ maintain its performance advantage when applied to tasks beyond image classification?
- Basis in paper: [inferred] The paper demonstrates ERM++ effectiveness on image classification datasets, but doesn't explore its applicability to other domains like natural language processing or speech recognition.
- Why unresolved: The success of ERM++ on image classification doesn't guarantee similar performance on other data modalities that may have different characteristics and domain shift patterns.
- What evidence would resolve it: Applying ERM++ to non-image tasks (e.g., text classification, speech recognition) and comparing its performance to domain-specific DG methods would resolve this uncertainty.

## Limitations
- Performance gains rely heavily on specific implementation choices, particularly AugMix pre-trained weights
- Evaluation is limited to computer vision benchmarks, with untested effectiveness on other data types
- Computational cost increases with 4× longer training duration compared to standard ERM

## Confidence
- High confidence in the core claims about full data utilization, long training, and unfrozen BatchNorm improving standard ERM
- Medium confidence in the practical significance for real-world applications, as the evaluation is limited to curated academic datasets
- Low confidence in generalizability to non-image domains or scenarios with extremely limited training data

## Next Checks
1. Reproduce ERM++ performance using different pre-trained models (e.g., standard ImageNet pre-training) to quantify the contribution of AugMix weights
2. Evaluate ERM++ on a domain shift task outside computer vision (e.g., text classification) to test generalizability
3. Conduct an ablation study on the impact of validation set size when using full data training, to understand the tradeoff between data utilization and hyperparameter tuning capability