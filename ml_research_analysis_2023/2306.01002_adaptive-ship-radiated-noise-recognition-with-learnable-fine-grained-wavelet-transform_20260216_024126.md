---
ver: rpa2
title: Adaptive ship-radiated noise recognition with learnable fine-grained wavelet
  transform
arxiv_id: '2306.01002'
source_url: https://arxiv.org/abs/2306.01002
tags:
- wavelet
- agnet
- underwater
- recognition
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adaptive generalized network (AGNet) for
  ship-radiated noise recognition. AGNet employs a learnable fine-grained wavelet
  transform that adaptively learns optimal wavelet parameters for different center
  frequencies, enabling robust feature extraction in varying underwater environments.
---

# Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform

## Quick Facts
- **arXiv ID:** 2306.01002
- **Source URL:** https://arxiv.org/abs/2306.01002
- **Reference count:** 40
- **Key outcome:** AGNet achieves state-of-the-art recognition accuracy (85.48%, 77.09%, 95.76%) on three underwater acoustic datasets

## Executive Summary
This paper proposes AGNet, an adaptive generalized network for ship-radiated noise recognition that employs a learnable fine-grained wavelet transform to extract robust features from underwater acoustic signals. The network learns optimal wavelet parameters at different center frequencies, enabling adaptive feature extraction in varying underwater environments. A ResNet classifier with parallel convolution attention modules focuses on informative time-frequency components. Experiments demonstrate superior recognition accuracy and robustness against low signal-to-noise ratios, with significant performance gains from transfer learning.

## Method Summary
AGNet processes underwater acoustic signals through a learnable fine-grained wavelet transform using Fbsp basis with parameters (m, fb, fc) that are updated via backpropagation. The resulting 2D spectrograms are classified using a ResNet50 backbone with parallel convolution attention modules that apply depth-wise separable convolutions to generate attention matrices. The system is trained end-to-end using Adam optimizer (lr=5e-4, weight_decay=1e-6) for 100 epochs on 4 V100 GPUs, with optional pre-training on AudioSet for transfer learning. Experiments are conducted on three datasets (Shipsear, DeepShip, Thousand Island Lake) with 4-fold cross-validation.

## Key Results
- Achieves 85.48% recognition accuracy on Shipsear dataset (9 vessel types, 90 recordings)
- Demonstrates 77.09% accuracy on DeepShip dataset (265 ships, 47h audio)
- Shows 95.76% accuracy on Thousand Island Lake dataset with strong robustness to low SNR
- Benefits significantly from transfer learning with AudioSet pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learnable wavelet parameters improve generalization across diverse underwater acoustic environments.
- Mechanism: The network converts fixed wavelet parameters into fine-grained learnable parameters, enabling adaptive feature extraction that responds to background noise, transmission channel variations, and other environmental factors.
- Core assumption: Wavelet parameters at different center frequencies can be optimized through gradient-based learning to capture more relevant information than fixed-parameter approaches.
- Evidence anchors:
  - [abstract] "By converting fixed wavelet parameters into fine-grained learnable parameters, AGNet learns the characteristics of underwater sound at different frequencies."
  - [section] "AGNet converts wavelet parameters into learnable ones. It allows the wavelet basis at different center frequencies to learn differential parameters in a data-driven manner, so as to achieve fine-grained wavelet transformation."
  - [corpus] Weak evidence - no direct citations in corpus neighbors discussing learnable wavelet parameters specifically.
- Break condition: If the learning rate is too high or too low, the wavelet parameters may fail to converge to optimal values, potentially degrading recognition performance.

### Mechanism 2
- Claim: Parallel convolution attention modules enhance the classifier's ability to focus on informative time-frequency components.
- Mechanism: Attention blocks use depth-wise separable convolutions to generate attention matrices that multiply with feature maps, emphasizing informative regions while suppressing noise and irrelevant information.
- Core assumption: The spectrogram contains regions with varying levels of information density, and adaptive attention can effectively identify and amplify the informative regions.
- Evidence anchors:
  - [abstract] "To utilize the implicit information in wavelet spectrograms, AGNet adopts the convolutional neural network with parallel convolution attention modules as the classifier."
  - [section] "Each attention block corresponds to a residual layer... The attention block could help the network focus the attention on the informative parts (e.g., beat or rhythm)."
  - [corpus] Weak evidence - corpus neighbors mention attention mechanisms but not specifically for wavelet spectrograms in underwater acoustic recognition.
- Break condition: If the attention modules are not properly initialized or if the depth-wise separable convolutions fail to capture the relevant spatial relationships, the attention mechanism may not effectively improve recognition accuracy.

### Mechanism 3
- Claim: Transfer learning from large audio datasets significantly improves recognition performance on underwater acoustic tasks.
- Mechanism: Pre-training on AudioSet provides rich prior knowledge about audio patterns that can be transferred to underwater acoustic recognition through weight initialization, enabling faster convergence and better generalization.
- Core assumption: There exist commonalities between general audio domains and underwater acoustic domains that can be leveraged through transfer learning.
- Evidence anchors:
  - [abstract] "Moreover, AGNet could benefit more from transfer learning."
  - [section] "Based on a large-scale dataset - AudioSet, we conduct experiments on transfer learning... With the help of AudioSet, we pre-train the AGNet on the source domain (audio domain) and then transfer the model weight as the initial value on the underwater acoustic recognition task."
  - [corpus] Weak evidence - corpus neighbors discuss transfer learning but not specifically for underwater acoustic recognition with AudioSet.
- Break condition: If the source domain (AudioSet) is too dissimilar from the target domain (underwater acoustics), the transferred knowledge may be irrelevant or even harmful, leading to worse performance than training from scratch.

## Foundational Learning

- Concept: Wavelet Transform
  - Why needed here: Wavelet transform provides adaptive time-frequency resolution, which is crucial for analyzing non-stationary underwater acoustic signals with varying frequency content across different ship types.
  - Quick check question: What is the key advantage of wavelet transform over Fourier transform for analyzing signals with transient components?

- Concept: Depth-wise Separable Convolution
  - Why needed here: Depth-wise separable convolutions reduce the number of parameters and computational complexity while maintaining sufficient representational power for attention mechanisms in the classifier.
  - Quick check question: How does depth-wise separable convolution differ from standard convolution in terms of parameter count and computational efficiency?

- Concept: Transfer Learning
  - Why needed here: Transfer learning leverages knowledge from large-scale audio datasets to overcome the scarcity of underwater acoustic training data, improving both convergence speed and final recognition accuracy.
  - Quick check question: What are the key considerations when selecting a source domain for transfer learning to a target domain?

## Architecture Onboarding

- Component map:
  - Raw signal → learnable wavelet parameters (m, fb, fc)
  - Wavelet transform → 2D spectrogram generation
  - Spectrogram → ResNet50 with parallel convolution attention blocks
  - Classifier output → cross-entropy loss computation
  - Backpropagation → update of all parameters (wavelet and classifier)

- Critical path:
  1. Raw underwater acoustic signal → learnable wavelet parameters
  2. Wavelet transform → 2D spectrogram generation
  3. Spectrogram → ResNet50 with attention blocks
  4. Classifier output → cross-entropy loss computation
  5. Backpropagation → update of all parameters (wavelet and classifier)

- Design tradeoffs:
  - Learnable parameters vs. fixed parameters: More learnable parameters increase flexibility but also computational cost and risk of overfitting
  - Attention modules vs. plain ResNet: Attention improves focus on informative regions but adds complexity and parameters
  - Transfer learning vs. training from scratch: Transfer learning speeds up convergence and improves accuracy but requires careful domain alignment

- Failure signatures:
  - Poor convergence: Learning rate too high/low, inadequate initialization, or insufficient training data
  - Overfitting: Too many learnable parameters relative to training data size, insufficient regularization
  - Attention modules not helping: Improper initialization, inappropriate attention mechanism design for spectrogram features

- First 3 experiments:
  1. Baseline comparison: Implement fixed wavelet transform with ResNet50 (no attention) and compare against learnable wavelet transform with attention modules
  2. Ablation study: Remove attention blocks to quantify their contribution to overall performance
  3. Robustness test: Add colored noise at varying SNR levels to evaluate system performance degradation compared to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AGNet's learnable fine-grained wavelet transform perform on underwater acoustic signals with non-stationary noise patterns compared to stationary noise patterns?
- Basis in paper: [inferred] The paper mentions AGNet's robustness against colored noise, but does not specifically address non-stationary noise patterns.
- Why unresolved: The paper does not provide experimental results or analysis comparing the performance of AGNet on non-stationary versus stationary noise patterns.
- What evidence would resolve it: Experimental results comparing AGNet's performance on datasets with non-stationary and stationary noise patterns, with metrics such as accuracy, F1-score, and signal-to-noise ratio.

### Open Question 2
- Question: What is the computational complexity of the AGNet's learnable fine-grained wavelet transform, and how does it scale with the size of the input signal?
- Basis in paper: [inferred] The paper mentions that AGNet uses a fine-grained wavelet transform, but does not provide details on its computational complexity.
- Why unresolved: The paper does not provide an analysis of the computational complexity of the learnable fine-grained wavelet transform, nor does it discuss how it scales with input signal size.
- What evidence would resolve it: A theoretical analysis of the computational complexity of the learnable fine-grained wavelet transform, along with empirical results showing how it scales with input signal size.

### Open Question 3
- Question: How does the performance of AGNet on underwater acoustic signal recognition tasks compare to other deep learning-based methods that use different feature extraction techniques, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)?
- Basis in paper: [explicit] The paper compares AGNet's performance to several baseline methods, but does not provide a comprehensive comparison to other deep learning-based methods.
- Why unresolved: The paper does not provide a comprehensive comparison of AGNet's performance to other deep learning-based methods that use different feature extraction techniques.
- What evidence would resolve it: A comprehensive comparison of AGNet's performance to other deep learning-based methods, using the same datasets and evaluation metrics, to determine which method performs best for underwater acoustic signal recognition tasks.

## Limitations
- Implementation details for parallel convolution attention modules are insufficient for exact reproduction
- Dataset splitting methodology lacks specificity, particularly for DeepShip's 4-fold cross-validation
- No quantitative comparison against non-adaptive wavelet baselines to isolate the benefit of learnability
- Limited analysis of parameter sensitivity for the learnable wavelet parameters

## Confidence
- Learnable wavelet transform: Medium confidence - supported by experimental results but lacks ablation studies
- Transfer learning: Medium confidence - shows improvement but domain alignment analysis is incomplete
- Attention mechanism: Low confidence - insufficient architectural details and unclear contribution to performance

## Next Checks
1. Implement a baseline system using fixed wavelet parameters with identical classifier architecture to isolate the contribution of learnable parameters to the 85.48% accuracy improvement
2. Conduct ablation studies systematically removing attention modules, transfer learning, and learnable parameters to quantify individual contributions
3. Perform sensitivity analysis on learnable wavelet parameters (m, fb, fc) to determine optimal initialization ranges and learning rate constraints for stable training