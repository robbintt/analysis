---
ver: rpa2
title: Spatially Resolved Gene Expression Prediction from H&E Histology Images via
  Bi-modal Contrastive Learning
arxiv_id: '2306.01859'
source_url: https://arxiv.org/abs/2306.01859
tags:
- expression
- bleep
- image
- gene
- profiles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BLEEP introduces a bi-modal contrastive learning framework that
  predicts spatially resolved gene expression profiles from H&E histology images by
  learning a joint embedding space between image patches and their paired spatial
  transcriptomics data. It uses a ResNet50 encoder for histology images and an expression
  encoder for gene profiles, trained via a modified contrastive loss to align paired
  representations while mitigating batch effects.
---

# Spatially Resolved Gene Expression Prediction from H&E Histology Images via Bi-modal Contrastive Learning

## Quick Facts
- arXiv ID: 2306.01859
- Source URL: https://arxiv.org/abs/2306.01859
- Reference count: 26
- BLEEP achieves up to 120% improvement in correlation with ground-truth expression compared to existing methods

## Executive Summary
BLEEP introduces a bi-modal contrastive learning framework that predicts spatially resolved gene expression profiles from H&E histology images by learning a joint embedding space between image patches and their paired spatial transcriptomics data. It uses a ResNet50 encoder for histology images and an expression encoder for gene profiles, trained via a modified contrastive loss to align paired representations while mitigating batch effects. Expression prediction is achieved through k-nearest neighbor imputation in the joint embedding space. Evaluated on a human liver Visium dataset, BLEEP achieves significantly higher correlation (up to 120% improvement) with ground-truth expression across marker genes, highly expressed genes, and highly variable genes compared to HisToGene and ST-Net, while preserving biological heterogeneity and being robust to experimental artifacts.

## Method Summary
BLEEP learns a joint embedding space between H&E histology image patches and their paired spatial transcriptomics expression profiles using a modified contrastive learning objective. The framework employs a ResNet50 encoder for image features and a three-layer fully connected network for expression profiles, projecting both modalities into a shared 256-dimensional space. During training, the model aligns paired representations while accounting for internal similarities to preserve biological heterogeneity. For prediction, query image patches are projected into this embedding space and their gene expression is imputed by finding the k nearest reference expression profiles and linearly combining them.

## Key Results
- Achieves up to 120% improvement in correlation with ground truth for marker genes compared to HisToGene and ST-Net
- Shows 39% improvement for highly expressed genes and 90% improvement for highly variable genes
- Maintains biological heterogeneity and robustness to experimental artifacts while improving prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
Joint embedding space learned via contrastive learning enables gene expression imputation by aligning paired histology images and spatial transcriptomics data. The model uses ResNet50 for image features and an FCN for expression profiles, then aligns them in a shared embedding space through a modified contrastive loss that pulls paired representations together while pushing others apart. This alignment creates a bridge where similar-looking image patches correspond to similar gene expression patterns.

### Mechanism 2
Query-reference imputation through k-nearest neighbors in the joint embedding space overcomes the curse of dimensionality inherent in direct gene expression regression. Instead of predicting each gene independently, the model projects query image patches into the learned embedding space, finds the k closest reference expression profiles by Euclidean distance, and imputes the query expression as a weighted average of these neighbors.

### Mechanism 3
Contrastive learning with similarity-adjusted targets preserves biological heterogeneity while mitigating batch effects. The modified contrastive loss incorporates internal similarities within image and expression batches, preventing the model from pulling apart spots with similar expression profiles. This maintains variance in the learned embeddings while reducing batch-specific artifacts.

## Foundational Learning

- Concept: Contrastive learning fundamentals (CLIP-style loss, temperature scaling, positive/negative pairs)
  - Why needed here: The entire BLEEP framework depends on learning meaningful cross-modal representations through contrastive objectives rather than supervised regression.
  - Quick check question: What is the role of the temperature parameter τ in contrastive loss, and how does it affect the learned embedding space?

- Concept: Spatial transcriptomics data preprocessing (normalization, highly variable gene selection, batch correction)
  - Why needed here: BLEEP requires properly preprocessed paired data; understanding normalization and batch correction is essential for reproducing the results and troubleshooting.
  - Quick check question: Why is Harmony batch correction applied before training, and what would happen if it were omitted?

- Concept: Nearest-neighbor imputation and distance metrics in high-dimensional spaces
  - Why needed here: The query-reference imputation step relies on finding nearest neighbors in the joint embedding space; understanding distance metrics and k-selection is critical for implementation.
  - Quick check question: How does the choice of k affect the bias-variance tradeoff in the imputed expression profiles?

## Architecture Onboarding

- Component map: Image patches (224×224) → ResNet50 encoder → image embedding; Expression profiles (3467 genes) → FCN encoder → expression embedding; Both embeddings → joint embedding space (256-dim) → k-NN imputation → predicted expression

- Critical path: Paired data → image encoder → expression encoder → contrastive alignment → joint embedding → k-NN imputation → predicted expression

- Design tradeoffs:
  - Smaller encoder (ResNet50 vs ViT) trades capacity for better generalization on limited data
  - Contrastive learning vs supervised regression trades explicit supervision for implicit alignment
  - k-NN imputation vs direct prediction trades computational cost for preservation of heterogeneity

- Failure signatures:
  - Poor contrastive training: embeddings show no clear clustering of paired samples
  - Inappropriate k value: predictions are either too smooth (large k) or too noisy (small k)
  - Batch effect leakage: predicted expressions show sample-specific artifacts not present in original

- First 3 experiments:
  1. Train with only the contrastive loss (no imputation) and visualize t-SNE of image vs expression embeddings to verify alignment.
  2. Test k-NN imputation on a held-out reference subset to measure imputation accuracy before full training.
  3. Ablate the similarity-adjusted target and compare correlation metrics to confirm its contribution to performance.

## Open Questions the Paper Calls Out

- What is the upper limit on reference dataset size before BLEEP's query-reference imputation performance plateaus or degrades? The authors suggest increasing reference dataset size could improve performance by reducing query-reference distances, implying current dataset size may be limiting. This remains unresolved as the paper only evaluates BLEEP on a single human liver Visium dataset.

- Can BLEEP's joint embedding space be effectively transferred to predict expression from H&E images of different tissue types not seen during training? The authors mention BLEEP's potential for integrating new datasets into the shared embedding space but do not demonstrate cross-tissue generalization. This requires testing BLEEP trained on one tissue type against H&E images from completely different tissues.

- How does BLEEP's performance compare when predicting expression for genes with poor morphological correlation versus genes with strong morphological correlation? The authors note that some genes may be poorly correlated with morphological features, potentially limiting predictability, but do not stratify performance by gene-morphology correlation strength.

## Limitations
- Performance depends on critical hyperparameter choices (k for k-NN, temperature for contrastive loss) that were not explicitly specified
- Evaluation limited to single tissue type (human liver), raising questions about generalizability across different tissues and staining protocols
- Reliance on paired training data constrains applicability in settings where spatial transcriptomics data is unavailable

## Confidence
- High confidence: The core mechanism of using contrastive learning to align image and expression embeddings is theoretically sound and well-supported by ablation studies
- Medium confidence: The reported performance improvements are substantial but depend on specific hyperparameter choices and dataset characteristics that may not generalize
- Medium confidence: The claim of preserving biological heterogeneity while mitigating batch effects is supported by the similarity-adjusted contrastive loss but requires further validation across diverse datasets

## Next Checks
1. Perform systematic hyperparameter sensitivity analysis for k (range 1-20) and temperature τ (range 0.01-0.5) to identify optimal values and assess robustness to these critical choices.
2. Evaluate BLEEP on a multi-tissue dataset (e.g., STARmap or MERFISH data) to test generalizability beyond human liver tissue and validate cross-tissue performance.
3. Conduct biological validation by comparing predicted expression patterns of known marker genes against independent experimental validation (e.g., immunofluorescence or RNAscope) to assess clinical relevance beyond correlation metrics.