---
ver: rpa2
title: Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement
  Learning
arxiv_id: '2307.08794'
source_url: https://arxiv.org/abs/2307.08794
tags:
- agent
- policy
- multi-timescale
- agents
- coma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-timescale multi-agent reinforcement learning
  (MARL), where agents operate on different timescales and need to coordinate time-dependent
  actions. The core method introduces a periodic time encoding using phase-functioned
  neural networks to learn non-stationary multi-agent policies that can handle time
  dependencies induced by multiple timescales.
---

# Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.08794
- Source URL: https://arxiv.org/abs/2307.08794
- Reference count: 20
- Key outcome: Phasic COMA outperforms standard methods in multi-timescale MARL tasks, achieving 100% success in easy gridworld and superior performance in harder tasks and building energy management

## Executive Summary
This paper addresses the challenge of multi-timescale multi-agent reinforcement learning (MARL), where agents operate on different timescales and must coordinate time-dependent actions. The core contribution is a periodic time encoding method using phase-functioned neural networks (PFNNs) that enables learning non-stationary policies capable of handling the repeating action patterns induced by multiple timescales. The approach is validated on both a gridworld "Move Box" task and a building energy management environment, demonstrating superior performance compared to standard MARL methods.

## Method Summary
The proposed method extends COMA with phase-functioned neural networks to create "phasic COMA" agents that can learn K-periodic non-stationary policies. The PFNN architecture uses learnable spline control points to smoothly vary weights as a function of phase (2πt/K), creating an inductive bias that similar phases should use similar weights. This enables agents to capture the periodic nature of optimal actions in multi-timescale environments. The method also includes a theoretical framework proving convergence of policy iteration in the space of K-periodic non-stationary joint policies under cooperative assumptions and full observability.

## Key Results
- Phasic COMA achieved 100% success rate in the easy gridworld task vs. 0% for basic COMA
- Phasic COMA showed superior performance in harder gridworld tasks with appropriate period settings (K=2 for easy, K=6 for hard)
- In building energy management, phasic COMA was the only method able to reliably learn near-optimal joint policies for coordinating HVAC and energy storage agents

## Why This Works (Mechanism)

### Mechanism 1: Periodic Time Encoding via PFNNs
Phase-functioned neural networks enable learning non-stationary policies that capture repeating action patterns induced by multiple timescales. The PFNN architecture smoothly varies weights as a function of phase, creating an inductive bias for periodicity. Break condition occurs when the environment's periodicity K doesn't match the PFNN period or when optimal policy is not truly periodic.

### Mechanism 2: Theoretical Convergence Framework
The paper establishes that K-periodic non-stationary policies can converge to the optimal multi-timescale policy under cooperative assumptions and full observability. Policy iteration in the space of K-periodic non-stationary joint policies converges through a contraction mapping argument. Break condition occurs when cooperative assumption is violated or partial observability prevents reduction to multi-agent MDP.

### Mechanism 3: Observation Aliasing Solution
Phase-aware observation augmentation solves observation aliasing by providing agents with time information to distinguish between observations that should lead to different actions at different times. By encoding current phase, agents can condition policies on temporal information that disambiguates aliased observations. Break condition occurs when environment's temporal structure is too complex for simple phase encoding or when aliasing is not primary source of non-stationarity.

## Foundational Learning

- **Multi-agent reinforcement learning with partial observability**: The paper builds on MARL foundations while extending them to multi-timescale settings where agents have different action frequencies. Quick check: What is the key difference between a standard Dec-POMDP and a MT-DEC-POMDP as defined in this paper?

- **Policy gradient methods and actor-critic architectures**: The proposed solution uses a policy gradient approach with actor-critic architecture, specifically adapting COMA with PFNNs. Quick check: How does the phasic COMA agent differ from standard COMA in terms of network architecture?

- **Neural network weight modulation and spline-based architectures**: PFNNs use spline-based weight modulation that smoothly varies with phase, which is the key innovation for handling periodicity. Quick check: What is the role of the Catmull-Rom spline in the PFNN architecture?

## Architecture Onboarding

- **Component map**: Environment -> Agent observation -> Phase encoding -> PFNN weight computation -> Policy network forward pass -> Action selection -> Environment transition -> Reward computation -> Value estimation -> Policy gradient update

- **Critical path**: Environment step → Agent observation → Phase encoding → PFNN weight computation → Policy network forward pass → Action selection → Environment transition → Reward computation → Value estimation → Policy gradient update

- **Design tradeoffs**: PFNNs vs. one-hot encoding (PFNNs provide smoother generalization but require more complex weight computation; one-hot encoding is simpler but may not capture complex phase-action relationships); Period selection (K = LCM of agent timescales and environment period is theoretically justified but may be computationally expensive); Observability assumption (theoretical convergence requires full observability, but experiments use partial observability)

- **Failure signatures**: Basic COMA fails completely (0% success) on time-dependent tasks due to observation aliasing; Recurrent COMA shows slow learning and unreliable convergence; PFNNs with incorrect period (e.g., 4 instead of K) show degraded but still functional performance; Training instability when phase encoding is inconsistent across agents

- **First 3 experiments**: 1) Implement basic COMA on Move Box easy environment to verify observation aliasing problem (should fail completely); 2) Implement one-hot phase-aware COMA on same environment to verify simple phase encoding solves problem; 3) Implement phasic COMA with correct period K on building energy management environment to verify superior performance on real-world multi-timescale coordination

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following remain unresolved based on the analysis:

- **Scalability to larger agent populations**: How does the method scale to environments with more than two timescales and a larger number of agents?
- **Sensitivity to incorrect period assumptions**: How sensitive is the method to incorrect assumptions about the environment period C?
- **Extension to partially observable settings**: Can the method be extended to partially observable multi-timescale environments where agents do not have full observability?

## Limitations
- Theoretical framework assumes full observability but experiments use partial observability settings
- Period K selection relies on knowing LCM of agent timescales and environment period, which may not be practical
- PFNN architecture's complexity introduces computational overhead not fully characterized for scalability
- Limited experimental validation on relatively simple environments with known periodicity

## Confidence
- **High confidence**: Identification of observation aliasing problem and its impact on basic MARL methods in multi-timescale settings
- **Medium confidence**: Theoretical convergence proof for full observability case, though limited by gap between theory and practice
- **Medium confidence**: Empirical results showing phasic COMA's superiority on demonstrated benchmarks
- **Low confidence**: Method's generalization to environments with unknown or varying timescales

## Next Checks
1. **Generalization test**: Implement phasic COMA on a variant of Move Box environment where agent timescales are not perfectly periodic or are initially unknown
2. **Scalability assessment**: Scale building energy management environment to include more than two agents with diverse timescales, measuring computational overhead and performance degradation
3. **Competitive baseline comparison**: Compare phasic COMA against state-of-the-art multi-agent RL methods designed for non-stationarity on same benchmarks