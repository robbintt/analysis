---
ver: rpa2
title: Benchmarking Adversarial Robustness of Compressed Deep Learning Models
arxiv_id: '2308.08160'
source_url: https://arxiv.org/abs/2308.08160
tags:
- adversarial
- pruning
- base
- pruned
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark evaluating the impact
  of neural network pruning on adversarial robustness. The study examines models pruned
  using filter importance criteria (L1, L2, and geometric median) across three popular
  architectures (ResNet50, DenseNet121, MobileNetV1) on CIFAR-10 and CIFAR-100 datasets.
---

# Benchmarking Adversarial Robustness of Compressed Deep Learning Models

## Quick Facts
- arXiv ID: 2308.08160
- Source URL: https://arxiv.org/abs/2308.08160
- Reference count: 40
- Key outcome: Pruning up to 50% maintains adversarial robustness while reducing inference time (8-10% improvement) and model size (approximately 66% reduction)

## Executive Summary
This paper presents a comprehensive benchmark evaluating the impact of neural network pruning on adversarial robustness across three popular architectures (ResNet50, DenseNet121, MobileNetV1) on CIFAR-10 and CIFAR-100 datasets. The study uses seven attack methods (FGSM, DeepFool, PGD, BIM, APGD, CW, Universal Perturbation) to generate adversarial examples from base models and tests them against both base and pruned versions. Results demonstrate that pruning up to 50% maintains adversarial robustness comparable to base models while providing benefits of reduced inference time and smaller model sizes. The research also investigates transferability of adversarial examples across different model architectures, finding consistent robustness patterns between base and pruned models.

## Method Summary
The study trains base models on CIFAR-10 and CIFAR-100, generates adversarial examples using seven attack methods, and applies NNCF filter pruning with L1, L2, and geometric median criteria at 10-50% pruning rates. After each pruning step, models undergo fine-tuning to optimize performance. The evaluation measures accuracy on both clean and adversarial test sets, inference time using OpenVINO, and model size reduction. Transferability analysis tests whether adversarial examples generated from one architecture family affect other families, comparing robustness patterns between base and pruned models.

## Key Results
- Pruning up to 50% maintains adversarial accuracy within ±1% of base models
- Inference time improves by 8-10% after pruning and OpenVINO optimization
- Model size reduces by approximately 66% with 50% pruning
- Adversarial examples show consistent transferability patterns between base and pruned models
- Different pruning criteria (L1, L2, geometric median) yield comparable robustness results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filter pruning removes unimportant filters while preserving critical features needed for both accuracy and adversarial robustness
- Mechanism: The pruning algorithm identifies and removes filters with lowest importance scores based on L1, L2, or geometric median criteria. Since adversarial examples exploit the same decision boundaries that regular inputs do, removing redundant filters doesn't substantially alter the model's vulnerability profile
- Core assumption: Filters most vulnerable to adversarial manipulation are also most important for correct classification
- Evidence anchors: Experimental results show minimal impact on adversarial accuracy (±1% change); neighboring papers discuss compression vs robustness trade-offs

### Mechanism 2
- Claim: Iterative fine-tuning after pruning helps restore any temporary degradation in robustness
- Mechanism: After each pruning step, the model undergoes fine-tuning which allows it to adapt remaining parameters to compensate for removed filters
- Core assumption: Fine-tuning can effectively redistribute representational burden across remaining filters
- Evidence anchors: Minimal accuracy changes despite pruning; fine-tuning is explicitly part of the pruning procedure

### Mechanism 3
- Claim: Adversarial robustness is primarily determined by overall architecture and training data rather than specific filter configurations
- Mechanism: Since base models are not adversarially trained, their inherent vulnerability is baked into architecture and training process
- Core assumption: Susceptibility to adversarial examples is a property of fundamental structure rather than specific filter weights
- Evidence anchors: Pruning neither significantly degrades nor enhances robustness; neighboring papers discuss architecture's role in robustness

## Foundational Learning

- Concept: Adversarial examples and attack methods (FGSM, PGD, CW, etc.)
  - Why needed here: The paper evaluates robustness against multiple attack types
  - Quick check question: What is the key difference between white-box and black-box attacks, and which does this paper use?

- Concept: Neural network pruning techniques (magnitude pruning, filter pruning, iterative pruning)
  - Why needed here: The study uses NNCF's filter pruning algorithm with different importance criteria
  - Quick check question: How does iterative pruning differ from one-shot pruning, and why might iterative be preferred?

- Concept: Transferability of adversarial examples across model architectures
  - Why needed here: The paper investigates whether adversarial examples generated from one architecture family affect other families
  - Quick check question: What does it mean for an adversarial example to be "transferable," and why is this important for security analysis?

## Architecture Onboarding

- Component map: Base models (ResNet50, DenseNet121, MobileNetV1, VGG19) trained on CIFAR-10/CIFAR-100 → Adversarial attack generation module (ART library) → Pruning module (NNCF library with L1/L2/Geometric Median criteria) → Fine-tuning module → Evaluation module (accuracy, inference time, model size)
- Critical path: 1) Train base models → 2) Generate adversarial examples from base models → 3) Apply pruning to create compressed versions → 4) Evaluate both base and pruned models on original and adversarial test sets → 5) Measure inference time and model size
- Design tradeoffs: Choice between pruning criteria involves balancing computation ease, compression effectiveness, and feature preservation; white-box attacks with surrogate models trade realism for controlled experimentation
- Failure signatures: Pruned models showing significantly different accuracy drops compared to base models on adversarial examples; inference time improvements not materializing; model sizes not reducing as expected
- First 3 experiments:
  1. Verify pruning preserves base accuracy: Apply 50% pruning to ResNet50 and confirm accuracy remains within 1-2% of base model
  2. Test single attack transferability: Generate FGSM examples from ResNet50 and test on both base and pruned DenseNet121 to verify comparable accuracy drops
  3. Measure inference time reduction: Compare inference times of base vs 50% pruned MobileNet on CPU to confirm 8-10% improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does adversarial training combined with pruning affect adversarial robustness differently than pruning without adversarial training?
- Basis in paper: [explicit] The paper states "Previous work has largely focused on pruning models that have already undergone adversarial training" and mentions "large computation cost of robust/adversarial training" as a reason for studying non-adversarially trained models
- Why unresolved: The paper specifically excludes adversarially trained models from its study
- What evidence would resolve it: A direct comparison between pruning adversarially trained models versus pruning non-adversarially trained models while measuring adversarial robustness

### Open Question 2
- Question: How does model pruning affect adversarial robustness when the attacker has access to the pruned model parameters?
- Basis in paper: [inferred] The paper assumes a relaxed threat model where the adversary lacks knowledge of victim's trained base model parameters
- Why unresolved: The study only evaluates attacks using base model parameters to generate adversarial examples
- What evidence would resolve it: Experiments where the attacker generates adversarial examples using the pruned model itself as the surrogate

### Open Question 3
- Question: Do different pruning criteria (L1, L2, geometric median) have varying impacts on adversarial transferability across different model architectures?
- Basis in paper: [explicit] The paper tests three pruning criteria but only briefly mentions transferability results without detailed analysis of pruning criterion effects
- Why unresolved: The transferability analysis is limited and doesn't differentiate how pruning criteria affect cross-architecture adversarial example effectiveness
- What evidence would resolve it: Systematic testing of transferability patterns for each pruning criterion across all model architecture combinations

## Limitations

- The paper does not specify exact adversarial attack parameters (epsilon values, step sizes) which could affect reproducibility
- Fine-tuning schedules and exact optimizer parameters after pruning are not detailed
- No information on random seed settings, which may impact consistency of pruning outcomes

## Confidence

**High Confidence**: Claims about maintaining accuracy and robustness during pruning up to 50% are well-supported by multiple experiments across different architectures and datasets.

**Medium Confidence**: Inference time improvements (8-10%) and model size reduction (~66%) are plausible given OpenVINO optimization, but exact measurements depend on hardware configuration.

**Medium Confidence**: Transferability patterns between architectures are demonstrated but limited to specific model families without broader architectural diversity.

## Next Checks

1. Verify pruning implementation by comparing baseline accuracy retention: Apply 50% pruning to ResNet50 and measure if accuracy stays within 1-2% of base model

2. Test single attack transferability: Generate FGSM examples from ResNet50 and evaluate on both base and pruned DenseNet121 to confirm comparable accuracy degradation

3. Measure inference time reduction: Compare CPU inference times of base vs 50% pruned MobileNet to validate 8-10% improvement claim