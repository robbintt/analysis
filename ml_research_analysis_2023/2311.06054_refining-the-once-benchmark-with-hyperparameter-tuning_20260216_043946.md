---
ver: rpa2
title: Refining the ONCE Benchmark with Hyperparameter Tuning
arxiv_id: '2311.06054'
source_url: https://arxiv.org/abs/2311.06054
tags:
- object
- learning
- semi-supervised
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the claimed improvements of semi-supervised
  learning methods for 3D object detection on the ONCE dataset. The authors find that
  prior work used suboptimal hyperparameters for the supervised pretraining step,
  leading to underfitting.
---

# Refining the ONCE Benchmark with Hyperparameter Tuning

## Quick Facts
- arXiv ID: 2311.06054
- Source URL: https://arxiv.org/abs/2311.06054
- Reference count: 40
- One-line primary result: Optimal hyperparameter tuning of supervised models on ONCE dataset significantly improves baseline performance, reducing the gains attributed to semi-supervised learning.

## Executive Summary
This paper challenges the claimed improvements of semi-supervised learning methods for 3D object detection on the ONCE dataset. The authors find that prior work used suboptimal hyperparameters for the supervised pretraining step, leading to underfitting. They perform grid search to find optimal hyperparameters for the SECOND and CenterPoint models, resulting in state-of-the-art performance when trained only on labeled data. When applying semi-supervised methods to these well-tuned models, the improvement from unlabeled data is much smaller than previously reported, suggesting that much of the prior performance gain was due to better supervised pretraining rather than the unlabeled data. The paper concludes that a well-fit supervised model is critical for effective semi-supervised learning, and highlights the importance of proper hyperparameter tuning in benchmarking.

## Method Summary
The authors perform grid search over batch size (16, 32, 64, 128), learning rate (0.0001, 0.0005, 0.001, 0.003, 0.006, 0.01), number of epochs (80, 150, 500, 1000), and NMS threshold (0.01, 0.25, 0.65) for SECOND and CenterPoint models on the ONCE dataset. They train supervised models using the optimal hyperparameters found, then apply Mean Teacher and Proficient Teacher semi-supervised learning methods on Small, Medium, and Large unlabeled subsets. Performance is evaluated using mAP with 3D IoU thresholds of 0.7, 0.3, and 0.5 for Vehicle, Pedestrian, and Cyclist classes respectively.

## Key Results
- Grid search hyperparameter tuning on supervised models achieves state-of-the-art performance on ONCE dataset without using unlabeled data
- Semi-supervised learning improvements are significantly smaller when applied to well-tuned supervised models compared to previous reports
- The paper demonstrates that underfitting in supervised pretraining was a major factor in previously reported semi-supervised gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperparameter tuning of the supervised pretraining model improves the baseline performance more than the subsequent semi-supervised learning step improves it further.
- Mechanism: Optimal batch size, learning rate, number of epochs, and NMS threshold enable the model to reach a better local minimum, reducing underfitting and thus providing higher-quality pseudo-labels in the semi-supervised phase.
- Core assumption: The ONCE benchmark's default hyperparameters are suboptimal and cause underfitting.
- Evidence anchors:
  - [abstract] "Our results suggest that simple grid search hyperparameter tuning applied to a supervised model can lead to state-of-the-art performance on the ONCE dataset..."
  - [section] "We perform a straightforward grid search operation to maximize the quality of training and inference for these models."
  - [corpus] Weak/no direct support; neighboring papers discuss LiDAR tasks but not hyperparameter tuning impact.
- Break condition: If the default parameters were already optimal, the grid search would yield negligible gains, invalidating the claim.

### Mechanism 2
- Claim: A well-fitted supervised model yields better pseudo-labels, making the semi-supervised improvement from unlabeled data smaller than previously reported.
- Mechanism: High-quality pseudo-labels from a well-trained model reduce the noise in the semi-supervised training process, so the relative gain from unlabeled data diminishes.
- Core assumption: The quality of pseudo-labels is the bottleneck for semi-supervised learning effectiveness.
- Evidence anchors:
  - [abstract] "...much of the prior performance gain was due to better supervised pretraining rather than the unlabeled data."
  - [section] "A supervised model must be pretrained on labeled data to acquire initial pseudo labels... the effectiveness of semi-supervised learning depends on the quality of the pretrained model."
  - [corpus] No direct evidence; neighboring papers focus on LiDAR datasets but not pseudo-label quality.
- Break condition: If pseudo-label quality were not the limiting factor, further gains from unlabeled data would still be substantial even with a well-fitted model.

### Mechanism 3
- Claim: Optimizing the NMS threshold during inference is critical for improving detection performance.
- Mechanism: The NMS threshold controls the suppression of overlapping bounding boxes; tuning it balances precision and recall, directly impacting mAP.
- Core assumption: Default NMS thresholds are suboptimal for the dataset/task.
- Evidence anchors:
  - [section] "â€¢ NMS threshold... Raising the threshold boosts precision but may compromise recall... Lowering it might enhance recall but could lead to more false positives."
  - [section] "grid search over batch size, learning rate, number of epochs, and NMS threshold..."
  - [corpus] No evidence; neighboring papers do not discuss NMS tuning.
- Break condition: If detection performance were insensitive to NMS threshold, tuning it would have no effect.

## Foundational Learning

- Concept: Role of batch size in gradient estimation and convergence stability
  - Why needed here: The paper compares different batch sizes (16, 32, 64, 128) and finds larger batches improve performance, so understanding batch size's effect on training dynamics is essential.
  - Quick check question: How does increasing batch size typically affect the variance of gradient estimates and the stability of convergence?

- Concept: Learning rate scheduling and the One Cycle policy
  - Why needed here: The experiments use the One Cycle policy; knowing how learning rate schedules affect underfitting/overfitting is crucial to interpret results.
  - Quick check question: What is the primary advantage of the One Cycle policy compared to fixed or step-decay schedules?

- Concept: NMS (Non-Maximum Suppression) in object detection
  - Why needed here: NMS threshold tuning is explicitly tested; understanding how it trades off precision vs. recall is necessary to grasp its impact.
  - Quick check question: What happens to the precision-recall curve when the NMS threshold is increased?

## Architecture Onboarding

- Component map: LiDAR point cloud -> Voxelization -> VFE (Voxel Feature Encoding) -> Sparse Conv -> RPN (Region Proposal Network) -> NMS -> Final boxes
- Critical path:
  1. Data loading -> voxelization
  2. Feature extraction (VFE + sparse conv)
  3. Proposal generation (RPN)
  4. Post-processing (NMS)
  5. (Optional) Semi-supervised fine-tuning
- Design tradeoffs:
  - Voxel size vs. computational cost vs. detail preservation
  - Batch size vs. memory constraints vs. convergence stability
  - NMS threshold vs. precision vs. recall
- Failure signatures:
  - Underfitting: Low training/validation mAP, large gap to tuned models
  - Overfitting: High training mAP, low validation mAP
  - Poor NMS: Too many false positives or missed detections due to aggressive/weak suppression
- First 3 experiments:
  1. Run baseline SECOND/CenterPoint with default ONCE hyperparameters and record mAP.
  2. Tune batch size and learning rate (keeping epochs fixed) and measure impact on validation mAP.
  3. Tune NMS threshold post-training and measure change in final mAP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific hyperparameters in the pretraining phase are most critical for the success of semi-supervised 3D object detection methods on point cloud data?
- Basis in paper: [explicit] The paper demonstrates that well-tuned supervised pretraining significantly improves semi-supervised learning performance, suggesting certain hyperparameters are more influential than others.
- Why unresolved: While the paper shows that hyperparameter tuning improves performance, it doesn't identify which specific parameters (learning rate, batch size, epochs, etc.) have the most impact on semi-supervised learning effectiveness.
- What evidence would resolve it: Systematic ablation studies varying individual hyperparameters while keeping others fixed, measuring their impact on semi-supervised learning performance across multiple datasets and models.

### Open Question 2
- Question: How does the distribution mismatch between labeled and unlabeled data affect the performance of semi-supervised 3D object detection methods?
- Basis in paper: [explicit] The paper notes that the assumption of identical distributions between labeled and unlabeled data often doesn't hold in practice, particularly in autonomous driving datasets where certain conditions may be underrepresented.
- Why unresolved: The paper identifies this as a potential issue but doesn't quantify how distribution mismatch specifically impacts semi-supervised learning performance or propose methods to mitigate it.
- What evidence would resolve it: Controlled experiments manipulating the similarity between labeled and unlabeled data distributions, measuring the effect on semi-supervised learning performance, and developing techniques to handle distribution mismatch.

### Open Question 3
- Question: What is the optimal ratio of labeled to unlabeled data for semi-supervised 3D object detection on point cloud data?
- Basis in paper: [inferred] The paper shows that hyperparameter tuning on labeled data alone can achieve state-of-the-art performance, suggesting that the contribution of unlabeled data may depend on having sufficient high-quality labeled data.
- Why unresolved: The paper doesn't explore how varying the amount of labeled versus unlabeled data affects semi-supervised learning performance, or whether there's a threshold where adding more unlabeled data becomes beneficial.
- What evidence would resolve it: Experiments systematically varying the ratio of labeled to unlabeled data while keeping total data volume constant, measuring performance across different ratios to identify optimal balance.

## Limitations

- The paper doesn't provide detailed training curves or ablations showing the progression from underfit to well-fit models, making it difficult to fully verify the underfitting claim.
- The Proficient Teacher method is described as 3D-specific but lacks implementation details that would allow for precise replication.
- The study focuses on two specific models (SECOND and CenterPoint) and may not generalize to all 3D object detection architectures.

## Confidence

- **High confidence**: The finding that hyperparameter tuning improves supervised baseline performance on ONCE (directly measurable from mAP scores).
- **Medium confidence**: The claim that semi-supervised improvements are smaller when applied to well-tuned models (supported by results but requires replication).
- **Low confidence**: The assertion that most prior gains were due to underfitting rather than semi-supervised learning efficacy (inference-heavy, not directly proven).

## Next Checks

1. Replicate the grid search for batch size and learning rate, recording training/validation mAP curves to visually confirm underfitting in the baseline and convergence in the tuned model.
2. Train a well-tuned supervised model, then apply a standard semi-supervised method (e.g., Mean Teacher) and compare the mAP improvement to that reported in prior ONCE work.
3. Perform an ablation where the well-tuned model is intentionally underfitted (e.g., fewer epochs, smaller batch size) and measure whether semi-supervised gains increase, supporting the pseudo-label quality hypothesis.