---
ver: rpa2
title: 'Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing
  Risk Criterion'
arxiv_id: '2310.16546'
source_url: https://arxiv.org/abs/2310.16546
tags:
- risk
- distributional
- learning
- distribution
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to address the issue of biased exploration
  in distributional reinforcement learning, where optimistic decision-making based
  on estimated uncertainty can lead to suboptimal policies. The core idea is to randomize
  the risk criterion used for action selection, avoiding one-sided tendencies toward
  risk-seeking or risk-averse behavior.
---

# Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion

## Quick Facts
- arXiv ID: 2310.16546
- Source URL: https://arxiv.org/abs/2310.16546
- Reference count: 40
- This paper proposes a method to address biased exploration in distributional reinforcement learning by randomizing the risk criterion used for action selection.

## Executive Summary
This paper addresses the issue of biased exploration in distributional reinforcement learning, where using estimated uncertainty for optimistic action selection leads to suboptimal policies. The authors propose perturbing the risk criterion during action selection by sampling from an ambiguity set, which prevents the agent from developing a persistent bias toward either risk-seeking or risk-averse behavior. This approach maintains theoretical convergence guarantees while improving practical performance on Atari games.

## Method Summary
The method introduces perturbed quantile regression (PQR) where action selection uses a randomized risk criterion. At each timestep, a reweighting distribution ξ is sampled from a Dirichlet distribution to perturb the return distribution. The perturbed distributional Bellman optimality operator (PDBOO) is used to update the quantile network, with the perturbation bound scheduled to decay over time. This allows unbiased exploration while still converging to the optimal policy.

## Key Results
- PQR achieves 962% of human baseline score on Atari 55 games
- Outperforms other distribution-based methods including QR-DQN and IQN
- Maintains theoretical convergence guarantees through controlled perturbation bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Randomizing the risk criterion during action selection prevents the agent from developing a persistent bias toward either risk-seeking or risk-averse behavior.
- **Mechanism**: At each timestep, the algorithm samples a reweighting distribution ξ from an ambiguity set defined by a perturbation bound ∆. This ξ changes the effective risk measure used for selecting the greedy action, while the underlying return distribution is updated with the original (unperturbed) target. The sampling introduces variability in the exploration criterion, avoiding over-commitment to a fixed optimism or pessimism.
- **Core assumption**: The sequence of perturbation bounds ∆t can be scheduled to shrink to zero over time (e.g., ∆t = ∆₀·t^-(1+ε)), ensuring eventual convergence to the risk-neutral optimal policy.
- **Evidence anchors**:
  - [abstract] The authors propose "a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk."
  - [section 3.1] "To choose statistically plausible actions... we will generate a distortion risk measure involved in a pre-defined constraint set... We say the sampled reweighting distribution ξ as (distributional) perturbation."
- **Break condition**: If ∆t is kept constant or decays too slowly, the agent may continue selecting suboptimal actions after sufficient exploration, preventing convergence.

### Mechanism 2
- **Claim**: The perturbed distributional Bellman optimality operator (PDBOO) maintains theoretical convergence guarantees without requiring the conventional contraction property.
- **Mechanism**: PDBOO uses a time-varying Bellman operator τ^ξ that is not a contraction in any metric. However, under the assumption that ∑∆n < ∞ and ξn converges to 1 in probability, the expectation of any composition of these operators Eτ^ξₙ:₁ converges to the same fixed point as the standard Bellman optimality operator. This weaker contraction property is sufficient for convergence.
- **Core assumption**: The perturbation gap d(Z; ξ) ≤ ∆ must be controlled such that the accumulated perturbations remain summable.
- **Evidence anchors**:
  - [section 3.2] "Since the infinite composition of time-varying Bellman operators does not necessarily converge... we provide the sufficient condition in this section."
  - [section 3.2] "If Assumption 3.2 holds, then the expectation of any composition of operators Eτ^ξₙ:₁ converges, i.e., Eτ^ξₙ:₁[Z] → E[Z*]."
- **Break condition**: If ∑∆n diverges, the perturbations may accumulate and prevent convergence to the optimal policy.

### Mechanism 3
- **Claim**: The ambiguity set construction using a symmetric Dirichlet distribution with concentration parameter β allows practical sampling of the reweighting distribution ξ while bounding the perturbation gap.
- **Mechanism**: The algorithm samples a random vector x ~ Dir(β) and constructs ξ = (1/N + α(Nx - 1/N)), where α controls the maximum allowable distortion. By setting α ≤ ∆/((N-1)Vmax), the perturbation gap is guaranteed to be within the bound ∆. This construction ensures ξ is a valid probability density function that can be used to reweight the quantiles.
- **Core assumption**: The Dirichlet distribution provides sufficient flexibility to generate perturbations within the required bound while maintaining computational tractability.
- **Evidence anchors**:
  - [section 3.3] "We sample a random vector,x ~ Dir(β), and define the reweight distribution asξ := 1N + α(N x - 1N)."
  - [section 3.3] "Hence, letting α ≤ ∆/(N-1)Vmax is sufficient to obtain d(Z; ξ) ≤ ∆ in the quantile setting."
- **Break condition**: If the concentration parameter β is set too high, the sampled perturbations may become too concentrated around the uniform distribution, reducing exploration effectiveness.

## Foundational Learning

- **Concept**: Distributional reinforcement learning learns the full distribution of returns rather than just the expected value.
  - **Why needed here**: The paper builds on the quantile regression approach to distributional RL, where the return distribution is represented by a set of quantile locations. This representation is necessary to extract both the mean (for risk-neutral optimization) and the variance (for exploration).
  - **Quick check question**: What is the difference between the distributional Bellman operator τ and the expectation-based Bellman operator Eτ?

- **Concept**: Optimism in the face of uncertainty (OFU) is an exploration strategy that selects actions based on upper confidence bounds of estimated uncertainty.
  - **Why needed here**: The paper identifies that using the estimated variance from the return distribution for OFU exploration can lead to biased action selection, as it conflates intrinsic uncertainty with parametric uncertainty. This motivates the need for randomization.
  - **Quick check question**: Why does using the empirical variance from quantiles for optimistic exploration cause biased action selection?

- **Concept**: Distortion risk measures generalize traditional risk measures by applying a distortion function to the cumulative distribution function.
  - **Why needed here**: The paper uses distortion risk measures to formalize how the reweighting distribution ξ changes the effective risk criterion. Understanding this concept is crucial for grasping how randomization affects action selection.
  - **Quick check question**: How does a distortion risk measure differ from a traditional risk measure like Value-at-Risk?

## Architecture Onboarding

- **Component map**: State → Quantile network → Greedy action selection with ξ → Environment step → Store transition → Sample minibatch → Sample ξ → Compute perturbed target → Compute loss → Update network
- **Critical path**: State → Quantile network → Greedy action selection with ξ → Environment step → Store transition → Sample minibatch → Sample ξ → Compute perturbed target → Compute loss → Update network
- **Design tradeoffs**:
  - Perturbation bound schedule: Fixed vs. decaying (affects exploration-exploitation balance)
  - Dirichlet concentration parameter β: Low values allow more diverse perturbations, high values make perturbations more uniform
  - Number of quantiles N: More quantiles provide better distribution approximation but increase computation
- **Failure signatures**:
  - If performance degrades over time: Perturbation bound schedule may be decaying too quickly
  - If learning is unstable: Dirichlet concentration parameter β may be too low, causing excessive variance in perturbations
  - If agent gets stuck in suboptimal policies: Perturbation bound may be too small from the start
- **First 3 experiments**:
  1. Compare PQR with and without perturbation on a simple environment like N-Chain to verify that randomization prevents bias
  2. Test different perturbation bound schedules (fixed vs. decaying) to find optimal exploration-exploitation tradeoff
  3. Vary the Dirichlet concentration parameter β to study its effect on exploration diversity and learning stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed perturbation-based exploration compare to other exploration methods like curiosity-driven or intrinsic motivation approaches?
- Basis in paper: [explicit] The paper focuses on comparing PQR with other distribution-based methods and discusses the limitations of existing exploration strategies.
- Why unresolved: The paper does not directly compare PQR to curiosity-driven or intrinsic motivation approaches.
- What evidence would resolve it: Empirical comparisons of PQR with curiosity-driven or intrinsic motivation approaches on the same set of tasks.

### Open Question 2
- Question: Can the perturbation-based exploration be combined with other advanced techniques like distributed training or model-based RL to further improve performance?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of PQR on its own but does not explore potential synergies with other techniques.
- Why unresolved: The paper focuses on evaluating PQR as a standalone method.
- What evidence would resolve it: Empirical results showing the combined performance of PQR with distributed training or model-based RL on benchmark tasks.

### Open Question 3
- Question: How does the performance of PQR scale with the size of the action space and the complexity of the environment?
- Basis in paper: [explicit] The paper evaluates PQR on Atari 55 games which have a relatively large action space.
- Why unresolved: The paper does not explore the scalability of PQR to environments with even larger action spaces or more complex dynamics.
- What evidence would resolve it: Empirical results of PQR on environments with progressively larger action spaces and more complex dynamics.

## Limitations

- The theoretical convergence proof relies on a specific schedule for the perturbation bound that may be difficult to tune in practice.
- The Dirichlet-based perturbation sampling introduces additional hyperparameters that significantly affect exploration behavior but are not thoroughly studied across different environments.
- While the paper demonstrates improved performance on Atari 55 games, the ablation studies are limited.

## Confidence

- **High confidence**: The core mechanism of randomization preventing bias in exploration is theoretically sound and empirically validated.
- **Medium confidence**: The theoretical convergence guarantees under the specified perturbation schedule, as this requires careful tuning in practice.
- **Medium confidence**: The practical performance improvements, as the ablation studies are limited and the Atari results may not generalize to other domains.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the Dirichlet concentration parameter β and perturbation bound schedule to identify robust settings and understand their impact on exploration-exploitation tradeoffs.
2. **Ablation study**: Compare PQR with alternative exploration strategies (e.g., epsilon-greedy, entropy regularization) to isolate the specific contribution of randomization to performance gains.
3. **Cross-domain validation**: Test PQR on non-Atari domains (e.g., continuous control tasks, multi-agent environments) to evaluate generalizability beyond the original evaluation setting.