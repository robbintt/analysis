---
ver: rpa2
title: 'Visual Program Distillation: Distilling Tools and Programmatic Reasoning into
  Vision-Language Models'
arxiv_id: '2312.03052'
source_url: https://arxiv.org/abs/2312.03052
tags:
- image
- answer
- program
- visual
- pali-x-vpd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Visual Program Distillation (VPD) is a method for training vision-language
  models (VLMs) by distilling the reasoning abilities of large language models (LLMs)
  and vision tools into a single end-to-end model. VPD generates multiple candidate
  programs for a given visual task using an LLM, executes them with specialized vision
  modules, filters for correct programs, and converts their execution traces into
  natural language chain-of-thought (CoT) rationales.
---

# Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models

## Quick Facts
- arXiv ID: 2312.03052
- Source URL: https://arxiv.org/abs/2312.03052
- Reference count: 40
- Key outcome: VPD-trained PaLI-X achieves state-of-the-art results on 8 classical VQA benchmarks and 2 zero-shot multimodal benchmarks

## Executive Summary
Visual Program Distillation (VPD) is a method for training vision-language models (VLMs) by distilling the reasoning abilities of large language models (LLMs) and vision tools into a single end-to-end model. VPD generates multiple candidate programs for visual tasks using an LLM, executes them with specialized vision modules, filters for correct programs, and converts execution traces into natural language chain-of-thought rationales. These rationales are then used to fine-tune VLMs using step-by-step distillation loss. The approach significantly improves VLM performance on compositional reasoning tasks and achieves state-of-the-art results on multiple benchmarks.

## Method Summary
VPD works by first generating multiple candidate programs for a given visual task using an LLM, then executing these programs with specialized vision modules (object detection, depth estimation, OCR, etc.). The system filters out programs that fail execution or produce incorrect answers, keeping only correct reasoning traces. These execution traces are converted into natural language chain-of-thought rationales using another LLM, which are then used to fine-tune VLMs through step-by-step distillation loss. This approach teaches VLMs to mimic structured reasoning similar to program execution, enabling better compositional reasoning abilities.

## Key Results
- PaLI-X trained with VPD achieves state-of-the-art results on 8 classical VQA benchmarks
- Significant performance gains on compositional reasoning tasks (+1.6 on GQA, +1.2 on TallyQA complex)
- Outperforms previous VLMs by large margins on zero-shot multimodal benchmarks (+8.5% on MMBench, +9.8% on TallyQA complex)
- Human evaluation confirms VPD-tuned models generate more consistent and faithful rationales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VPD improves VLMs by training them to mimic executable program reasoning traces.
- Mechanism: The pipeline generates multiple candidate programs, executes them with vision tools, filters for correct answers, and converts execution traces into natural language CoT rationales. These CoTs are then distilled into the VLM using step-by-step loss, teaching the model to follow structured reasoning steps similar to program execution.
- Core assumption: A correct program trace captures the full reasoning process needed to answer the question, and a VLM can learn to reproduce this reasoning through supervised distillation.
- Evidence anchors:
  - [abstract] "VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM."
  - [section 3.1] "We use the pipeline in §3.1 to synthesize CoT reasoning steps for these labels, and tune the PaLI model with these the loss in Equation 1."

### Mechanism 2
- Claim: Sampling multiple candidate programs and filtering for correct ones significantly improves the quality of the training data.
- Mechanism: Instead of relying on a single LLM-generated program, VPD samples k=5 programs and executes each one. Programs that fail execution or produce incorrect answers are filtered out, ensuring that only correct reasoning traces are used for distillation. This avoids propagating errors from incorrect programs.
- Core assumption: LLM-generated programs are error-prone, and filtering for correctness is essential to ensure high-quality training data.
- Evidence anchors:
  - [abstract] "Although innovative, generating explicit programs is computationally expensive in practice, prone to errors, and still underperforms end-to-end models. Programs require loading and executing multiple tools, leading to high latency and computational cost. Moreover, generated programs may omit necessary steps or include spurious ones."
  - [section 4.2] "There is a dramatic increase in success rate from 1 program to 5: +45% on GQA and A-OKVQA, +33% on OK-VQA, and +10% on TallyQA."

### Mechanism 3
- Claim: VPD enables VLMs to learn compositional reasoning by leveraging specialized vision tools within programs.
- Mechanism: The generated programs call specialized vision tools (e.g., object detection, depth estimation, OCR) to perform low-level visual understanding steps. By distilling these tool-based reasoning steps, the VLM learns to compose multiple skills (e.g., counting, spatial reasoning, attribute recognition) in a single forward pass.
- Core assumption: VLMs lack the ability to perform complex compositional reasoning, but can learn it by mimicking the structured use of specialized tools within programs.
- Evidence anchors:
  - [abstract] "Solving complex visual tasks such as 'Who invented the musical instrument on the right?' involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge."
  - [section 4.2] "VPD obtains a +1.6 improvement on GQA (which is heavily focused on compositional questions, spatial relationship, and localization), +1.2 on TallyQA for complex questions, and +1.2 on MMBench."

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: VPD converts program execution traces into CoT rationales, which are then used to train the VLM. Understanding CoT is essential to grasp how VPD represents and distills reasoning steps.
  - Quick check question: What is the difference between a program execution trace and a CoT rationale, and why is converting one to the other important for VPD?

- Concept: Program synthesis and execution
  - Why needed here: VPD relies on LLM-generated programs that call specialized vision tools. Understanding how these programs are generated, executed, and filtered is crucial to understanding the data synthesis pipeline.
  - Quick check question: How does the program filtering step ensure that only correct reasoning traces are used for distillation, and what happens if no program passes the filter?

- Concept: Vision tool integration
  - Why needed here: The programs in VPD call external vision tools (e.g., object detection, depth estimation, OCR) to perform specific visual understanding tasks. Understanding how these tools work and how their outputs are integrated into the reasoning process is important for understanding the overall approach.
  - Quick check question: What are the roles of the different vision tools used in VPD programs, and how do their outputs contribute to the final answer?

## Architecture Onboarding

- Component map: LLM (PaLM-2) → Program generation → Program execution (with vision tools) → Program filtering → CoT conversion → VLM fine-tuning
- Critical path: LLM → Program generation → Program execution (with vision tools) → Program filtering → CoT conversion → VLM fine-tuning
- Design tradeoffs:
  - Sampling multiple programs increases data quality but also computational cost
  - Using specialized vision tools enables complex reasoning but introduces dependency on external models
  - Converting execution traces to CoT rationales makes the data more natural for VLMs but may lose some precision
- Failure signatures:
  - Low success rate in finding correct programs (indicates issues with LLM generation or filtering)
  - VLM performance does not improve despite high-quality CoT data (indicates issues with the distillation process or VLM architecture)
  - Inconsistent or incorrect CoT rationales (indicates issues with the CoT conversion step)
- First 3 experiments:
  1. Run the full VPD pipeline on a small subset of GQA to verify that programs can be generated, executed, and filtered correctly
  2. Fine-tune a VLM with the synthesized CoT data from step 1 and evaluate on a held-out GQA validation set to check for performance improvement
  3. Compare the performance of the VPD-trained VLM with a baseline VLM trained on the same data without CoT (i.e., only the final answers) to isolate the effect of the CoT distillation

## Open Questions the Paper Calls Out

- Question: What is the precise mechanism by which the step-by-step distillation loss in Equation 1 improves model performance, beyond just providing more training data?
  - Basis in paper: [explicit] The paper describes the step-by-step distillation loss as "teaching the VLM to generate faithful reasoning steps similar to program execution traces, and carries more information beyond the label that also helps the VLM in better predicting the label."
  - Why unresolved: The paper doesn't provide a detailed analysis of how the step-by-step distillation loss specifically improves performance. It would be valuable to understand the specific mechanisms by which the loss improves model performance.
  - What evidence would resolve it: A detailed ablation study comparing the performance of models trained with and without the step-by-step distillation loss, along with an analysis of the impact on the model's reasoning abilities.

## Limitations

- Program Generation Quality: The effectiveness of VPD heavily depends on the LLM's ability to generate correct programs, with 45-55% of cases failing to generate usable training data.
- Vision Tool Dependency: VPD's performance is contingent on the accuracy and availability of specialized vision tools, with no detailed error analysis of how tool failures propagate.
- Generalization to New Domains: While strong on established benchmarks, limited evidence exists about transfer to domains with different visual characteristics or reasoning patterns.

## Confidence

- High Confidence: The core distillation mechanism and experimental results showing VPD improves VLM performance compared to baselines.
- Medium Confidence: The claim that VPD achieves state-of-the-art results across all evaluated benchmarks, given that some comparisons may not account for architectural differences.
- Medium Confidence: The assertion that VPD is particularly effective for compositional reasoning tasks, though the evidence is strong for the evaluated datasets.

## Next Checks

1. Conduct a detailed failure mode analysis by categorizing program generation failures and vision tool errors to identify systematic weaknesses in the pipeline.
2. Test VPD's sensitivity to the number of sampled programs (k) and the filtering threshold to determine the optimal tradeoff between computational cost and data quality.
3. Evaluate VPD-trained models on datasets with significantly different visual characteristics or reasoning patterns than the training data to assess generalization capabilities.