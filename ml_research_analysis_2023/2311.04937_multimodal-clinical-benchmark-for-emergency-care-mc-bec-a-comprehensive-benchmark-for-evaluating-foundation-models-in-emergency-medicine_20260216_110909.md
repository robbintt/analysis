---
ver: rpa2
title: 'Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive
  Benchmark for Evaluating Foundation Models in Emergency Medicine'
arxiv_id: '2311.04937'
source_url: https://arxiv.org/abs/2311.04937
tags:
- data
- modalities
- prediction
- multimodal
- multitask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MC-BEC, a comprehensive multimodal clinical
  benchmark for evaluating foundation models in Emergency Medicine using a dataset
  of over 100,000 ED visits. MC-BEC focuses on clinically relevant prediction tasks
  like patient decompensation, disposition, and revisit, using diverse modalities
  like vital signs, waveforms, and clinical notes.
---

# Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine

## Quick Facts
- arXiv ID: 2311.04937
- Source URL: https://arxiv.org/abs/2311.04937
- Reference count: 40
- Primary result: Introduces MC-BEC, a comprehensive multimodal clinical benchmark for evaluating foundation models in Emergency Medicine using over 100,000 ED visits with diverse clinical data modalities

## Executive Summary
This paper introduces MC-BEC, a comprehensive multimodal clinical benchmark for evaluating foundation models in Emergency Medicine using a dataset of over 100,000 ED visits. MC-BEC focuses on clinically relevant prediction tasks like patient decompensation, disposition, and revisit, using diverse modalities like vital signs, waveforms, and clinical notes. The benchmark provides a standardized evaluation framework with train-test splits and metrics. Experiments demonstrate strong performance on decompensation and disposition prediction, moderate monotonicity in modalities, and robustness to missing data. MC-BEC enables robust evaluation of multimodal, multitask models for emergency care, advancing the development of generalizable and accessible foundation models for this domain.

## Method Summary
MC-BEC uses a dataset of over 100,000 continuously monitored Emergency Department visits from 2020-2022. The benchmark includes multimodal clinical data such as vital signs, ECG and PPG waveforms, free-text clinical notes, triage information, and outcomes. Models are evaluated on three prediction tasks: decompensation, disposition, and revisit. The evaluation framework includes metrics for prediction performance (AUPRC), monotonicity in modalities (concordance index), robustness to missing modalities, and bias assessment across demographic groups. Baseline models use pretrained embeddings with a modular approach for modality fusion, employing multitask training inspired by prompting techniques with task descriptions encoded as BERT embeddings.

## Key Results
- Strong performance on decompensation and disposition prediction tasks using multimodal models
- Moderate monotonicity observed in modality integration, with some modalities showing diminishing returns
- Robustness to missing data, though performance degrades predictably when key modalities are absent
- Initial bias assessment shows relatively small TPR differences across demographic groups for most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multimodal integration of vital signs, waveforms, and clinical notes enables capturing complementary temporal and contextual information about patient decompensation.
- Mechanism: Vital signs provide discrete snapshots of physiological state, waveforms reveal underlying cardiac and respiratory dynamics, and clinical notes encode clinical reasoning and contextual cues. Combining these modalities allows the model to detect both overt physiological changes and subtle early warning signs that may be missed by any single modality.
- Core assumption: Different modalities carry non-redundant predictive information about patient outcomes.
- Evidence anchors:
  - [abstract] "includes a wide range of detailed clinical data, including continuously measured vital signs, electrocardiogram and photoplethysmograph waveforms, free-text reports of imaging studies"
  - [section] "MC-BEC contains both categorical and unstructured clinical data. These modalities and data structures are described below."
- Break condition: If modalities are highly correlated or redundant, adding more modalities may not improve performance and could even degrade it through noise or competition.

### Mechanism 2
- Claim: The multitask learning approach with unified task representations allows models to leverage shared information across related clinical prediction tasks.
- Mechanism: By encoding task descriptions as text embeddings and concatenating them with multimodal features, the model learns task-specific decision boundaries within a shared embedding space. This enables knowledge transfer between tasks like decompensation and disposition prediction, which may share underlying physiological or clinical factors.
- Core assumption: Clinical prediction tasks in emergency care share common features or latent factors.
- Evidence anchors:
  - [section] "Our multitask training approach draws inspiration from prompting techniques in language models. Rather than using task-specific prediction heads, we employ a unified task representation combined with task-specific queries."
  - [section] "This allows creating broadly capable models without architectural changes for new tasks."
- Break condition: If tasks are too dissimilar, the shared representation may become a bottleneck, limiting performance compared to task-specific models.

### Mechanism 3
- Claim: Evaluating monotonicity in modalities ensures that the model effectively integrates multimodal information rather than relying on a subset.
- Mechanism: By training models with incrementally more modalities and measuring performance changes, we can detect if adding modalities improves or harms predictions. A monotonic increase suggests effective multimodal integration, while decreases may indicate modality competition or noisy inputs.
- Core assumption: Adding informative modalities should not decrease model performance.
- Evidence anchors:
  - [section] "We hypothesize that a well-designed multimodal model should not perform worse when more data modalities are used for training."
  - [section] "A reduction in performance caused by the inclusion of more modalities may be attributed to modality competition during training, wherein the model learns to rely on only a subset of modalities to make predictions."
- Break condition: If certain modalities consistently degrade performance across tasks, the model architecture or featurization may need redesign.

## Foundational Learning

- Concept: Multimodal featurization strategies
  - Why needed here: The dataset contains diverse data types (time-series, waveforms, text, categorical) requiring specialized preprocessing to extract meaningful features.
  - Quick check question: How would you featurize ECG waveforms differently from vital sign trends?

- Concept: Task-specific vs. multitask modeling tradeoffs
  - Why needed here: The benchmark evaluates both approaches, requiring understanding of when to share vs. separate model capacity.
  - Quick check question: Under what conditions might multitask learning hurt performance despite parameter efficiency?

- Concept: Fairness evaluation in clinical ML
  - Why needed here: The benchmark includes bias metrics across demographic groups, critical for clinical deployment.
  - Quick check question: What are the limitations of using TPR difference as a fairness metric in imbalanced medical datasets?

## Architecture Onboarding

- Component map:
  Data ingestion → Featurization (modality-specific) → Multimodal fusion → Task-specific heads (or shared multitask head) → Predictions → Evaluation

- Critical path:
  1. Load and preprocess each modality according to its type
  2. Apply modality-specific featurizers (ClinicalBERT for text, engineered features for vitals, embeddings for waveforms)
  3. Concatenate features with task embeddings for multitask models
  4. Train LightGBM ensemble on the combined representation
  5. Evaluate on held-out test set using all metrics

- Design tradeoffs:
  - Featurization complexity vs. model flexibility: Heavy featurization (engineered features) vs. end-to-end learning
  - Multitask sharing vs. task-specific heads: Parameter efficiency vs. task specialization
  - Evaluation breadth vs. focus: Comprehensive metrics vs. core accuracy metrics

- Failure signatures:
  - Monotonicity index < 0.5 suggests modality competition or noisy inputs
  - Large AUPRC drop with missing modalities indicates poor robustness
  - Significant TPR differences across groups indicate potential bias requiring investigation

- First 3 experiments:
  1. Train single-task LightGBM models for each prediction task using all modalities; compare to multitask baseline
  2. Evaluate monotonicity in modalities by training models with incrementally more modalities; calculate concordance indices
  3. Test robustness to missing modalities by dropping each modality during inference; measure AUPRC changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the MC-BEC benchmark models change when incorporating a temporal dimension within and across patient visits?
- Basis in paper: [inferred] The authors mention that future models should leverage approaches like deep end-to-end trainable networks and integration of a temporal dimension both within and across visits.
- Why unresolved: The current baseline models do not incorporate a temporal dimension, so the impact of this on performance is unknown.
- What evidence would resolve it: Experiments comparing the performance of models with and without temporal modeling on the MC-BEC tasks.

### Open Question 2
- Question: Can the MC-BEC benchmark be extended to evaluate prediction over a non-discrete set of tasks, such as predicting therapeutic complications or need for post-discharge follow-up?
- Basis in paper: [explicit] The authors suggest that future work should aim to evaluate prediction over a non-discrete set of tasks to enable more granular and comprehensive predictions.
- Why unresolved: The current MC-BEC benchmark is limited to the discrete tasks of decompensation, disposition, and revisit prediction.
- What evidence would resolve it: Extending the MC-BEC dataset and evaluation framework to include additional tasks, and assessing the performance of models on these tasks.

### Open Question 3
- Question: How does the performance of the MC-BEC benchmark models change when incorporating a patient's longitudinal EHR history into their representations?
- Basis in paper: [explicit] The authors conducted an experiment using CLMBR embeddings learned from a patient's past EHR codes, and observed performance that was not too far off from models using all modalities.
- Why unresolved: The experiment was limited to structured EHR data, and did not directly compare to models using all modalities. The potential benefits of incorporating longitudinal history are unclear.
- What evidence would resolve it: Experiments comparing the performance of models with and without access to a patient's longitudinal EHR history on the MC-BEC tasks.

## Limitations
- Single-site dataset may introduce site-specific biases in clinical practices and documentation patterns
- Pretrained embeddings lack detailed specification of training procedures, limiting reproducibility
- Benchmark evaluates robustness to missing modalities but not varying degrees of data quality within each modality

## Confidence
- **High Confidence**: The benchmark's methodology for evaluating monotonicity in modalities and robustness to missing data is well-grounded in multimodal learning theory
- **Medium Confidence**: The fairness evaluation approach using TPR differences across demographic groups provides useful initial insights but may not capture full complexity of algorithmic bias
- **Low Confidence**: Generalization of findings to other emergency departments remains uncertain due to single-site nature and potential variations in clinical workflows

## Next Checks
1. **Cross-site validation**: Evaluate MC-BEC models on emergency department data from multiple health systems to assess generalization across different clinical environments and patient demographics

2. **Bias characterization refinement**: Implement additional fairness metrics beyond TPR differences, including calibration across groups and subgroup analysis by age, comorbidity burden, and socioeconomic factors

3. **Clinical utility assessment**: Conduct prospective evaluation of MC-BEC models in a controlled clinical setting to measure impact on clinical decision-making, workflow integration, and patient outcomes