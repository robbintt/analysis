---
ver: rpa2
title: 'DPSUR: Accelerating Differentially Private Stochastic Gradient Descent Using
  Selective Update and Release'
arxiv_id: '2311.14056'
source_url: https://arxiv.org/abs/2311.14056
tags:
- privacy
- training
- dpsur
- learning
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow convergence issue in differentially
  private stochastic gradient descent (DPSGD) caused by Gaussian noise and random
  sampling. The proposed DPSUR framework introduces selective updates and releases
  to improve convergence speed and model utility.
---

# DPSUR: Accelerating Differentially Private Stochastic Gradient Descent Using Selective Update and Release

## Quick Facts
- **arXiv ID**: 2311.14056
- **Source URL**: https://arxiv.org/abs/2311.14056
- **Reference count**: 40
- **Primary result**: DPSUR achieves 1-2% higher accuracy than second-best method across all datasets and privacy budgets

## Executive Summary
This paper addresses the slow convergence issue in differentially private stochastic gradient descent (DPSGD) caused by Gaussian noise and random sampling. The proposed DPSUR framework introduces selective updates and releases to improve convergence speed and model utility. DPSUR evaluates gradient updates using a validation test and only applies those leading to convergence. The method employs a clipping strategy for update randomization and a threshold mechanism for gradient selection. Experimental results on MNIST, FMNIST, CIFAR-10, and IMDB datasets show that DPSUR significantly outperforms previous works in terms of convergence speed and model utility.

## Method Summary
DPSUR is a differentially private training framework that applies selective updates based on validation performance. The method clips loss differences (ΔE) to a minimal range and adds Gaussian noise to create a randomization mechanism. Only updates that pass a threshold test are applied to the model, consuming privacy budget. The framework uses RDP composition for privacy accounting across both training and validation phases. Key components include Poisson sampling for batch selection, minimal clipping of ΔE with bound Cv, and threshold-based acceptance with parameter β.

## Key Results
- Achieves 1-2% higher accuracy than the second-best method across all datasets and privacy budgets
- Significantly improves convergence speed compared to standard DPSGD
- Maintains same privacy guarantees while improving model utility
- Outperforms baseline methods including DPSGD, DPIS, DPSGD-HF, DPSGD-TS, and DPAGD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective updates based on loss improvement accelerate convergence
- Mechanism: DPSUR evaluates gradient updates using a validation test and only applies those leading to convergence, avoiding useless or harmful updates
- Core assumption: The difference in loss (ΔE) between iterations is a reliable indicator of whether a gradient update will improve the model
- Evidence anchors:
  - [abstract]: "Our key idea to address these issues is to apply selective updates to the model training, while discarding those useless or even harmful updates."
  - [section]: "We propose DPSUR, a Differentially Private training framework based on Selective Updates and Release, where the gradient from each iteration is evaluated based on a validation test, and only those updates leading to convergence are applied to the model."
  - [corpus]: Weak - related works focus on adaptive clipping and sampling, not selective updates

### Mechanism 2
- Claim: Minimal clipping of ΔE reduces privacy budget consumption while maintaining accuracy
- Mechanism: Clip ΔE to a very small range [-Cv, Cv] where Cv ≈ 0.001, making most values fall outside the interval, then add Gaussian noise
- Core assumption: For loss values in gradient descent, ΔE can be effectively discretized to -Cv or Cv with negligible probability of falling within the interval
- Evidence anchors:
  - [section]: "we can use a small clipping bound Cv for uniform clipping such that almost all ΔE are outside the interval [-Cv, Cv]"
  - [section]: "Although in the worst case, there could be some value of ΔE in the interval of [-Cv, Cv], the probability is so small that we can ignore it"
  - [corpus]: Weak - related works focus on gradient clipping, not loss difference clipping

### Mechanism 3
- Claim: Selective release of gradients reduces privacy budget consumption across iterations
- Mechanism: Only consume privacy budget when a selective update occurs (fΔE < β·Cv), not for every iteration
- Core assumption: The probability of accepting updates is independent of the underlying data distribution beyond what's captured by the validation test
- Evidence anchors:
  - [section]: "DPSUR only consumes privacy budget when a selective update occurs, i.e., fΔE < β·Cv"
  - [section]: "The main challenges lie in two aspects — privacy concerns arising from gradient evaluation, and gradient selection strategy for model update"
  - [corpus]: Weak - related works focus on gradient-level privacy, not selective release mechanisms

## Foundational Learning

- Concept: Differential Privacy fundamentals (ε, δ, sensitivity, Gaussian mechanism)
  - Why needed here: DPSUR builds on differential privacy to ensure privacy while improving convergence speed
  - Quick check question: What is the relationship between the noise multiplier σ and the privacy parameters ε and δ?

- Concept: Rényi Differential Privacy (RDP) and its conversion to (ε, δ)-DP
  - Why needed here: DPSUR uses RDP for privacy accounting of both training and validation phases
  - Quick check question: How does the composition property of RDP help in calculating the total privacy loss?

- Concept: Stochastic Gradient Descent (SGD) convergence properties
  - Why needed here: Understanding why DPSUR's selective updates can accelerate convergence compared to standard DPSGD
  - Quick check question: What factors in DPSGD cause slower convergence compared to standard SGD?

## Architecture Onboarding

- Component map: DPSGD training phase -> Validation phase -> Selection mechanism -> Privacy accounting
- Critical path:
  1. Sample batch for training
  2. Compute and clip gradients, add Gaussian noise
  3. Update model parameters temporarily
  4. Sample batch for validation
  5. Compute loss difference ΔE
  6. Apply minimal clipping to ΔE
  7. Add Gaussian noise to clipped ΔE
  8. Compare with threshold and decide to accept/reject update
  9. Update privacy budget if accepted
  10. Repeat until convergence or budget exhausted
- Design tradeoffs:
  - Smaller Cv reduces noise but increases probability of values falling in interval
  - Larger β reduces acceptance rate but may improve model quality
  - Higher validation batch size improves estimate quality but consumes more privacy budget
  - More frequent validation improves selection accuracy but slows training
- Failure signatures:
  - Accuracy plateaus early: Validation test not sensitive enough or Cv too large
  - Privacy budget exhausted quickly: Acceptance rate too high or validation batch size too large
  - Model quality degrades: Threshold β too conservative or validation set not representative
- First 3 experiments:
  1. Test with different Cv values (0.01, 0.001, 0.0001) on MNIST to find optimal discretization
  2. Vary β parameter (-1.0, -0.5, -1.5) to observe impact on acceptance rate and accuracy
  3. Compare privacy budget consumption between DPSUR and standard DPSGD on FMNIST with fixed ε

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed clipping strategy for the loss difference (ΔE) compare in terms of effectiveness and computational efficiency to adaptive clipping techniques used in other differentially private SGD methods?
- Basis in paper: [inferred] The paper introduces a minimal clipping strategy for ΔE, setting the clipping bound Cv to a very small value (e.g., 0.001) to discretize the loss difference. This approach is distinct from adaptive clipping techniques used in other DPSGD variants, which adjust the clipping threshold based on gradient norms during training.
- Why unresolved: The paper does not provide a direct comparison of the proposed clipping strategy to adaptive clipping techniques in terms of effectiveness (e.g., model accuracy) and computational efficiency (e.g., runtime).
- What evidence would resolve it: Conducting experiments comparing the proposed minimal clipping strategy to adaptive clipping techniques on various datasets and models, measuring both model accuracy and computational efficiency, would provide insights into their relative performance.

### Open Question 2
- Question: What is the optimal value for the threshold parameter β in the threshold mechanism, and how does it vary across different datasets, models, and privacy budgets?
- Basis in paper: [explicit] The paper suggests setting β = -1 based on experimental observations, but acknowledges that the optimal value may vary depending on the dataset, model, and privacy budget.
- Why unresolved: The paper does not provide a theoretical analysis or empirical study to determine the optimal value of β for different scenarios. It only reports the results for a fixed value of β = -1.
- What evidence would resolve it: Conducting a comprehensive sensitivity analysis of the threshold parameter β across various datasets, models, and privacy budgets, and identifying the optimal value for each scenario, would help establish guidelines for setting this parameter.

### Open Question 3
- Question: How does the proposed selective update mechanism affect the convergence speed and final accuracy of the model compared to other DPSGD variants that use different techniques to improve convergence?
- Basis in paper: [explicit] The paper claims that DPSUR outperforms other DPSGD variants in terms of convergence speed and model utility, but does not provide a detailed analysis of the convergence properties or compare the final accuracy to other methods.
- Why unresolved: The paper does not provide theoretical analysis of the convergence properties of the selective update mechanism or conduct a thorough comparison of the final accuracy with other DPSGD variants.
- What evidence would resolve it: Conducting theoretical analysis of the convergence properties of the selective update mechanism and performing experiments comparing the final accuracy of DPSUR with other DPSGD variants on various datasets and models would provide insights into its effectiveness.

## Limitations

- The selective update mechanism relies heavily on the assumption that loss difference ΔE is a reliable indicator of gradient quality, which may not hold in complex optimization landscapes
- The minimal clipping strategy assumes ΔE values rarely fall within the clipping interval, but this probability could increase with different learning rates or dataset characteristics
- The validation test introduces additional privacy budget consumption that may not be fully accounted for in all experimental configurations

## Confidence

- **High Confidence**: The theoretical framework for privacy accounting using RDP composition is sound and well-established in the literature
- **Medium Confidence**: The empirical results showing 1-2% accuracy improvements over baselines are convincing for the tested datasets
- **Low Confidence**: The claim that ΔE discretization is "safe" because values rarely fall within [-Cv, Cv] lacks rigorous probability analysis

## Next Checks

1. **Robustness Testing**: Test DPSUR across diverse datasets (e.g., medical imaging, tabular data) and architectures (transformers, graph neural networks) to verify the claimed accuracy improvements are not dataset-specific
2. **Probability Analysis**: Quantitatively analyze the actual probability distribution of ΔE values during training to validate the assumption that values rarely fall within [-Cv, Cv] across different learning rates and batch sizes
3. **Privacy Accounting Verification**: Implement independent privacy accounting to verify that the selective release mechanism correctly only consumes budget when updates are accepted, and test the impact of different validation batch sizes on total privacy consumption