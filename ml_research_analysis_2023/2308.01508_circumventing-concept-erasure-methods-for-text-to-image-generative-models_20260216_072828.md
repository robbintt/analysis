---
ver: rpa2
title: Circumventing Concept Erasure Methods For Text-to-Image Generative Models
arxiv_id: '2308.01508'
source_url: https://arxiv.org/abs/2308.01508
tags:
- concept
- diffusion
- images
- inversion
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of five recently proposed
  concept-erasure methods for text-to-image generative models, including Erased Stable
  Diffusion, Selective Amnesia, Forget-Me-Not, Negative Prompt, and Safe Latent Diffusion.
  The authors show that these methods can be circumvented using a technique called
  Concept Inversion, which involves learning new word embeddings that can retrieve
  the supposedly erased concepts from the sanitized models without any alterations
  to their weights.
---

# Circumventing Concept Erasure Methods For Text-to-Image Generative Models

## Quick Facts
- arXiv ID: 2308.01508
- Source URL: https://arxiv.org/abs/2308.01508
- Reference count: 40
- One-line primary result: Five concept-erasure methods for text-to-image models can be circumvented using Concept Inversion, which learns new word embeddings to retrieve supposedly erased concepts without modifying model weights.

## Executive Summary
This paper investigates the robustness of five recently proposed concept-erasure methods for text-to-image generative models. The authors demonstrate that these methods, including Erased Stable Diffusion, Selective Amnesia, Forget-Me-Not, Negative Prompt, and Safe Latent Diffusion, can be circumvented using a technique called Concept Inversion. This approach learns new word embeddings that can retrieve the supposedly erased concepts from sanitized models without any alterations to their weights. The results highlight the brittleness of post hoc concept erasure methods and question their use in AI safety toolkits.

## Method Summary
The authors employ Concept Inversion to circumvent five concept-erasure methods by learning new word embeddings that can retrieve erased concepts from sanitized models. Using Textual Inversion on fine-tuned Stable Diffusion models, they optimize word embeddings for 1,000 steps while keeping model weights frozen. For each erasure method, they develop specific strategies: for ESD, they use classifier-free guidance with learned embeddings; for SA, they apply TI to both the full model and the erased concept; for FMN, they use TI with increased guidance scale; for NP, they generate images using learned embeddings and evaluate with CLIP; and for SLD, they modify the classifier to remove negative class bias during TI. The effectiveness is evaluated by comparing classification accuracy of erased concepts before and after Concept Inversion.

## Key Results
- Concept Inversion successfully retrieves erased concepts across all five tested methods (ESD, SA, FMN, NP, SLD)
- For object concepts, average classification accuracy increases from 0.3% to 59.7% using Concept Inversion
- For artistic styles, learned embeddings can generate images matching erased artistic concepts
- For NSFW concepts, Concept Inversion increases detected exposed body parts from 0% to 17-38% across different methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept erasure methods only perform input filtering rather than true removal of knowledge from model weights.
- Mechanism: The methods modify the model's response to specific prompts but leave the underlying embeddings intact. Special word embeddings can be learned that bypass these modifications.
- Core assumption: The diffusion model's learned representations contain sufficient information to reconstruct "erased" concepts through alternative prompt formulations.
- Evidence anchors:
  - [abstract] "Specifically, we leverage the existence of special learned word embeddings that can retrieve 'erased' concepts from the sanitized models with no alterations to their weights."
  - [section 3.1] "Using Concept Inversion, we can generate images of the erased objects, which can be seen by an increase in average accuracy from 0.3% to 59.7%."
  - [corpus] Weak - the corpus papers are about improving erasure methods, not explaining why they fail
- Break condition: If the model's representations are completely orthogonalized with respect to erased concepts, preventing any alternative access.

### Mechanism 2
- Claim: Textual Inversion can find new word embeddings that map to the same latent space regions as the erased concepts.
- Mechanism: By optimizing new word embeddings to minimize reconstruction loss while keeping model weights frozen, the method discovers embeddings that trigger the same generative pathways as the original concepts.
- Core assumption: The diffusion model's latent space contains multi-modal representations where different word embeddings can access similar visual concepts.
- Evidence anchors:
  - [section 3.1] "We employ standard Textual Inversion [31] on fine-tuned Stable Diffusion models... to learn a new word embedding that corresponds to the (artistic/object) concept"
  - [section 3.5] "We propose a new strategy to perform Concept Inversion in Algorithm 2" specifically for SLD
  - [corpus] Missing - no corpus papers directly address Textual Inversion's ability to bypass concept erasure
- Break condition: If the concept erasure method implements stronger regularization that prevents multiple embeddings from mapping to the same latent regions.

### Mechanism 3
- Claim: Concept erasure methods leave residual traces in the model that can be accessed through classifier-free guidance manipulation.
- Mechanism: By modifying the guidance scale and conditioning, the methods can be made to produce outputs that still contain traces of the erased concepts.
- Core assumption: The guidance mechanism in diffusion models can be exploited to override the erasure modifications.
- Evidence anchors:
  - [section 3.4] "Instead of updating the weights of the original model, it replaces the unconditional score with the score estimate conditioned on the erased concept"
  - [section 3.5] "The guidance mechanism in diffusion models can be exploited to override the erasure modifications"
  - [corpus] Missing - no corpus papers discuss guidance manipulation as a failure mode
- Break condition: If the concept erasure method implements stronger regularization that prevents multiple embeddings from mapping to the same latent regions.

## Foundational Learning

- Concept: Diffusion models and denoising process
  - Why needed here: Understanding how text-to-image models generate images is crucial for grasping why concept erasure methods can be circumvented
  - Quick check question: What is the difference between the forward and reverse diffusion processes in diffusion models?

- Concept: Word embeddings and vocabulary space
  - Why needed here: The paper relies on finding new word embeddings that can access "erased" concepts
  - Quick check question: How do word embeddings in text-to-image models map to visual concepts in the generated images?

- Concept: Textual Inversion technique
  - Why needed here: The paper uses Textual Inversion to find new word embeddings that can bypass concept erasure
  - Quick check question: What is the objective function optimized during Textual Inversion, and how does it differ from standard training?

## Architecture Onboarding

- Component map: Text prompt → word embeddings → conditioning on U-Net → U-Net denoising steps → latent representation → Decoder → final image

- Critical path:
  1. Text prompt → word embeddings
  2. Embeddings → conditioning on U-Net
  3. U-Net denoising steps → latent representation
  4. Decoder → final image

- Design tradeoffs:
  - Memory vs. quality: Higher resolution images require more memory but provide better quality
  - Guidance scale: Higher scales produce more faithful outputs but may amplify artifacts
  - Batch size: Larger batches improve stability but require more memory

- Failure signatures:
  - Concept erasure methods only work for specific prompts but fail for others
  - Visual artifacts when using learned word embeddings
  - Inconsistent results across different concept erasure methods

- First 3 experiments:
  1. Verify that original concept erasure methods work as intended for their target prompts
  2. Apply Textual Inversion to learn new word embeddings for erased concepts
  3. Test whether the learned embeddings can generate the erased concepts using the sanitized models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective evaluation methodologies for concept erasure in text-to-image models?
- Basis in paper: [explicit] The authors call for stronger evaluation methodologies, stating that measuring the degree of concept erasure is tricky due to the vast number of potential prompts an attacker could use.
- Why unresolved: The paper does not propose a specific solution, only highlighting the need for better evaluation methods beyond the limited test cases used in previous works.
- What evidence would resolve it: Development and demonstration of a comprehensive evaluation framework that can robustly assess the effectiveness of concept erasure across a wide range of prompts and scenarios, including the proposed Concept Inversion attacks.

### Open Question 2
- Question: What is the underlying mechanism that allows concept erasure methods to be circumvented through Concept Inversion?
- Basis in paper: [inferred] The authors hypothesize that concept erasure methods may be performing a form of input filtering, leaving them vulnerable to more sophisticated prompts. They design Concept Inversion techniques to search for word embeddings that can retrieve erased concepts.
- Why unresolved: While the paper demonstrates the effectiveness of Concept Inversion in breaking existing methods, it does not provide a deep understanding of why these methods fail or the specific mechanisms behind the success of Concept Inversion.
- What evidence would resolve it: Detailed analysis of the internal workings of concept erasure methods and Concept Inversion, including investigation of how the learned word embeddings interact with the model's latent space and attention mechanisms.

### Open Question 3
- Question: Can concept erasure methods be designed to be more robust against Concept Inversion and similar attacks?
- Basis in paper: [explicit] The authors conclude that their findings highlight the challenges in sanitizing generative models and suggest that entirely new approaches may be necessary.
- Why unresolved: The paper does not propose any new concept erasure methods or discuss potential strategies for improving robustness against attacks.
- What evidence would resolve it: Development and evaluation of novel concept erasure methods that incorporate defenses against Concept Inversion, such as more thorough exploration of the input space during training or the use of adversarial examples to strengthen the model's resilience.

## Limitations

- The experimental validation focuses primarily on single-concept erasure scenarios, with limited investigation into multi-concept erasure where concepts may be semantically related.
- The evaluation metrics rely heavily on classification accuracy and visual inspection, without exploring more robust perceptual or human evaluation studies.
- The paper does not adequately address potential countermeasures or whether stronger erasure techniques could prevent Concept Inversion.

## Confidence

**High Confidence**: The core finding that concept erasure methods can be circumvented using Textual Inversion is well-supported by multiple experiments across different erasure techniques and concept types. The mechanism by which alternative word embeddings can access latent representations is theoretically sound given the nature of diffusion models.

**Medium Confidence**: The generalizability of Concept Inversion to more complex erasure scenarios (multiple concepts, semantically related concepts) remains uncertain. While the paper demonstrates success on isolated concepts, the robustness in realistic multi-concept scenarios is not thoroughly explored.

**Low Confidence**: The paper does not adequately address potential countermeasures or whether stronger erasure techniques could prevent Concept Inversion. The claim that "post hoc concept erasure methods are brittle" may be overstated without exploring whether future methods could implement more robust protection mechanisms.

## Next Checks

1. **Multi-concept Erasure Test**: Evaluate Concept Inversion effectiveness when multiple related concepts are erased simultaneously (e.g., erasing both "Picasso" and "cubism" together) to test semantic robustness.

2. **Human Evaluation Study**: Conduct blind human evaluation of generated images to assess whether people can detect differences between images from original vs. sanitized models when using Concept Inversion prompts.

3. **Countermeasure Exploration**: Test whether increasing the strength of guidance scaling, implementing stronger regularization during fine-tuning, or using orthogonalization techniques can prevent Concept Inversion from succeeding.