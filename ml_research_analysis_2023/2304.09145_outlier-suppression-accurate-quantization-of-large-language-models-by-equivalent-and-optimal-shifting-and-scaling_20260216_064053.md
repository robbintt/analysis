---
ver: rpa2
title: 'Outlier Suppression+: Accurate quantization of large language models by equivalent
  and optimal shifting and scaling'
arxiv_id: '2304.09145'
source_url: https://arxiv.org/abs/2304.09145
tags:
- quantization
- arxiv
- scaling
- outliers
- shifting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Outlier Suppression+ (OS+), a framework for
  accurate quantization of large language models (LLMs) by addressing the challenges
  posed by detrimental outliers in activations. The core idea involves channel-wise
  shifting and scaling operations to eliminate asymmetric presentation and scale down
  problematic channels, which can be seamlessly migrated into subsequent modules while
  maintaining equivalence.
---

# Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling

## Quick Facts
- arXiv ID: 2304.09145
- Source URL: https://arxiv.org/abs/2304.09145
- Reference count: 18
- Key outcome: Establishes new SOTA for 4-bit BERT with 15.5% improvement, achieving near-floating-point performance on 8/6-bit settings

## Executive Summary
This paper introduces Outlier Suppression+ (OS+), a framework for accurate quantization of large language models by addressing detrimental outliers in activations. The core innovation involves channel-wise shifting and scaling operations that eliminate asymmetric outlier presentation and scale down problematic channels while maintaining floating-point equivalence. The framework includes a fast and stable scheme to calculate effective shifting and scaling values, and quantitatively analyzes changes brought by migration and quantization for better quantization burden balance. OS+ is validated across various tasks and models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA, demonstrating superior performance in both standard and fine-grained quantization settings.

## Method Summary
OS+ operates by first applying channel-wise shifting to eliminate asymmetric outliers by centering each channel around zero, followed by channel-wise scaling to suppress outlier channels by shrinking their values into a bounded range. These operations are then seamlessly migrated into subsequent layers through a unified migration pattern that updates weights and biases while maintaining floating-point equivalence. The framework optimizes an outlier threshold t to balance quantization error between activation and subsequent weight quantization, and employs Token-Wise Clipping for BERT models to further suppress outliers from the token aspect.

## Key Results
- Establishes new state-of-the-art for 4-bit BERT with 15.5% improvement over previous methods
- Achieves near-floating-point performance on both small models and LLMs on 8-bit and 6-bit settings
- Demonstrates superiority across various tasks with comprehensive results on BERT, OPT, BLOOM, BLOOMZ, and LLaMA models

## Why This Works (Mechanism)

### Mechanism 1
Channel-wise shifting eliminates asymmetric outliers by centering each channel around zero, thereby reducing the overall tensor range and improving quantization fidelity. The shifting vector z aligns the center of each channel by computing (max + min)/2 per channel, effectively removing the asymmetric tail of outliers that inflate the quantization range.

### Mechanism 2
Channel-wise scaling suppresses outlier channels by shrinking their values into a bounded range while leaving normal channels unchanged. Scaling factors s are computed as max(1.0, max(channel_range)/threshold), where threshold t is optimized to balance quantization error between activation and subsequent weight.

### Mechanism 3
Unified migration pattern preserves floating-point equivalence by absorbing shifting and scaling effects into subsequent layers' weights and biases. For a linear layer, W and b are updated to W⊙s^T and zW^T + b respectively, effectively reversing the shifting and scaling operations while maintaining the same forward computation.

## Foundational Learning

- Concept: Post-training quantization (PTQ)
  - Why needed here: OS+ operates in the PTQ setting where models are quantized without retraining, requiring minimal calibration data
  - Quick check question: What is the key difference between PTQ and QAT in terms of data requirements and computational cost?

- Concept: Asymmetric quantization
  - Why needed here: The paper uses asymmetric quantization for weights to handle the negative ranges introduced by shifting operations
  - Quick check question: Why is asymmetric quantization necessary when using channel-wise shifting?

- Concept: Hessian-aware quantization
  - Why needed here: Understanding how quantization error propagates through matrix multiplications helps explain why the scaling threshold optimization works
  - Quick check question: How does the Hessian matrix relate to quantization error sensitivity in neural networks?

## Architecture Onboarding

- Component map: LayerNorm → Shifting/Scaling → Quantization → Subsequent layers (Attention/FFN)
- Critical path: Outlier detection → Shifting vector computation → Scaling threshold optimization → Weight/bias migration → Calibration
- Design tradeoffs: Per-channel vs per-tensor quantization, dynamic vs static quantization, computational overhead vs accuracy
- Failure signatures: Accuracy drops when shifting/scaling vectors are not properly optimized, or when subsequent layers cannot absorb the transformations
- First 3 experiments:
  1. Apply only shifting to BERT-base and measure accuracy on GLUE tasks to isolate the effect of asymmetry removal
  2. Apply only scaling with fixed threshold to measure the impact of outlier suppression without shifting
  3. Vary the outlier threshold t systematically to find the optimal value for a specific model/task combination

## Open Questions the Paper Calls Out

- What are the underlying reasons for the emergence of asymmetric outliers in transformer language models, and how can this understanding be leveraged to improve quantization techniques?
- How can the Outlier Suppression+ framework be extended to vision transformers (ViTs) and other transformer-based architectures beyond language models?
- How can the Outlier Suppression+ framework be combined with other quantization schemes, such as per-token quantization, to further improve the accuracy and efficiency of low-bit transformer language models?
- What are the potential limitations of the Outlier Suppression+ framework when applied to even larger language models, such as those with trillions of parameters, and how can these limitations be addressed?

## Limitations

- The unified migration pattern's equivalence preservation relies heavily on assumptions about subsequent linear layers that may break down with complex layer compositions
- Computational overhead of the grid search for optimal scaling threshold t could become prohibitive for extremely large models
- Framework's effectiveness on models with trillions of parameters remains unexplored and may reveal scalability limitations

## Confidence

- High Confidence: Channel-wise shifting for asymmetric outlier removal is well-supported by mathematical derivation and empirical evidence
- Medium Confidence: Scaling threshold optimization approach shows promising results but relies on several assumptions about outlier channel distributions
- Low Confidence: Seamless migration claims are primarily supported by theoretical derivation for linear layers, with limited examination of complex architectures

## Next Checks

1. Apply OS+ to models with diverse layer compositions to verify that the unified migration pattern maintains equivalence across different architectural patterns
2. Systematically vary the grid search parameter K and outlier threshold t across different model sizes and tasks to determine sensitivity to hyperparameters
3. Quantify the additional computational cost introduced by OS+ framework and compare against performance gains to determine practical trade-offs