---
ver: rpa2
title: 'The curse of language biases in remote sensing VQA: the role of spatial attributes,
  language diversity, and the need for clear evaluation'
arxiv_id: '2311.16782'
source_url: https://arxiv.org/abs/2311.16782
tags:
- language
- question
- rsvqa
- visual
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses language biases in remote sensing visual question
  answering (RSVQA) by analyzing their impact on model robustness and proposing evaluation
  metrics. The authors conduct a three-pronged analysis: visual blind models, adversarial
  testing, and dataset analysis.'
---

# The curse of language biases in remote sensing VQA: the role of spatial attributes, language diversity, and the need for clear evaluation

## Quick Facts
- arXiv ID: 2311.16782
- Source URL: https://arxiv.org/abs/2311.16782
- Reference count: 24
- Primary result: Visual blind models achieve high accuracy in RSVQA by exploiting language biases, revealing that simpler vocabulary and question generation strategies make RSVQA more prone to bias than standard VQA

## Executive Summary
This paper investigates language biases in remote sensing visual question answering (RSVQA) and their impact on model robustness. Through a comprehensive three-pronged analysis involving visual blind models, adversarial testing, and dataset analysis, the authors demonstrate that RSVQA suffers from more severe language biases than standard VQA. The study compares RNN-based Skip-thoughts and Transformer-based distilBERT language models, with and without fine-tuning, revealing that while fine-tuning improves performance, it doesn't eliminate underlying biases. The authors propose relative evaluation metrics that account for language biases, providing a more transparent assessment of model capabilities.

## Method Summary
The study employs a comparative analysis of two language model architectures (RNNs and Transformers) in RSVQA tasks, examining both frozen and fine-tuned variants. The methodology includes training visual blind models that use only question text to predict answers, conducting adversarial testing by substituting random images during inference, and performing detailed dataset analysis to identify bias sources. The experiments are conducted across multiple RSVQA datasets including low and high resolution versions, as well as specialized datasets for land cover and flood detection. The evaluation framework incorporates prior accuracy baselines and improvement-over-lower-bound metrics to provide context for model performance.

## Key Results
- Visual blind models achieve significant accuracy (up to 50% in some cases) by exploiting question-answer distribution imbalances
- Fine-tuned distilBERT consistently outperforms other configurations, especially for complex questions
- Adversarial testing reveals that models often rely more on language patterns than visual information
- Language biases in RSVQA are more severe than standard VQA due to simpler vocabulary and question generation strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual blind models exploit imbalanced question-answer distributions to achieve high accuracy without seeing images.
- Mechanism: The model learns to map specific question tokens to the most probable answer class in the training set, bypassing visual reasoning entirely.
- Core assumption: Question-answer pairs have non-uniform distributions where certain answers dominate for certain question types.
- Evidence anchors:
  - [abstract]: "The proposed relative evaluation metrics, such as improvement over a lower-bound of accuracy, provide a more transparent assessment of model performance considering language biases."
  - [section]: "This analysis focuses both on model and data. Moreover, we motivate the use of more informative and complementary evaluation metrics sensitive to the issue."
  - [corpus]: Weak - no direct neighbor studies mention visual blind models; only general VQA bias studies exist.

### Mechanism 2
- Claim: Adversarial testing with random image substitution reveals whether model predictions depend on visual input.
- Mechanism: By replacing the correct image with a random image from the dataset, performance degradation indicates reliance on visual features.
- Core assumption: Random images maintain the same visual distribution but break semantic alignment with the question.
- Evidence anchors:
  - [abstract]: "The gravity of language biases in RSVQA is then exposed for all of these methods with the training of models discarding the image data and the manipulation of the visual input during inference."
  - [section]: "To avoid time-consuming manual modifications of the images triggering different answers, the image can be whited- or blacked-out, or can be changed at random from the entire dataset."
  - [corpus]: Weak - no explicit adversarial testing studies in corpus; only general VQA robustness studies exist.

### Mechanism 3
- Claim: Dataset analysis at object/relation level identifies specific question-answer shortcuts models exploit.
- Mechanism: By decomposing questions into objects, relations, and attributes, the most common answer patterns per subset reveal bias sources.
- Core assumption: Question templates follow predictable patterns that can be systematically deconstructed.
- Evidence anchors:
  - [abstract]: "Finally, a detailed analysis of question-answer distribution demonstrates the root of the problem in the data itself."
  - [section]: "Digging deeper, the question construction scheme of the dataset is investigated. Questions can often be dissected and grouped as the combination of objects, relations and attributes."
  - [corpus]: Weak - no direct neighbor studies on RSVQA dataset analysis; only general VQA bias studies exist.

## Foundational Learning

- Concept: Language model architectures (RNN vs Transformer)
  - Why needed here: Understanding architectural differences is crucial for interpreting why fine-tuning affects bias differently across models.
  - Quick check question: What is the key architectural difference between RNNs and Transformers that affects their ability to capture long-range dependencies?

- Concept: Tokenization and vocabulary handling
  - Why needed here: Different tokenization strategies (dataset-specific vs general-purpose) impact model's ability to generalize to new vocabulary.
  - Quick check question: Why does distilBERT's general tokenizer provide advantages over a dataset-specific Skip-thoughts dictionary?

- Concept: Multimodal fusion strategies
  - Why needed here: Understanding how visual and language features are combined affects model's reliance on either modality.
  - Quick check question: How does element-wise multiplication fusion differ from concatenation in terms of information preservation?

## Architecture Onboarding

- Component map: Remote sensing image + natural language question → CNN visual encoder + Tokenizer + Language Model → Fusion layer → Classification head → Predicted answer

- Critical path: Question → Tokenizer → Language Model → Fusion → Classification → Answer prediction

- Design tradeoffs:
  - Frozen vs fine-tuned language models: Frozen preserves pre-trained knowledge but may not adapt to domain-specific language; fine-tuned adapts but may overfit to dataset biases
  - Fusion strategy choice: Element-wise multiplication is computationally efficient but may lose information compared to concatenation or MUTAN
  - Answer classification vs generation: Classification is simpler but limits output to pre-defined answers

- Failure signatures:
  - High visual blind model accuracy indicates strong language bias
  - Stable performance under adversarial testing indicates lack of visual grounding
  - Large gaps between Prior Acc and Uni Acc indicate dataset imbalance

- First 3 experiments:
  1. Train visual blind model with frozen language model and measure accuracy by question type
  2. Run adversarial testing with random image substitution on reference model with fine-tuned distilBERT
  3. Perform dataset analysis decomposing questions into objects and relations, calculate Prior Acc for each subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific language model architectures and training strategies would be most effective at mitigating language biases in RSVQA?
- Basis in paper: Explicit. The paper compares different language models (RNN and Transformers) with and without fine-tuning and finds that fine-tuning helps improve performance but does not eliminate biases.
- Why unresolved: The paper does not explore a wide range of language model architectures and training strategies, leaving open the question of what would be optimal for RSVQA.
- What evidence would resolve it: Experiments comparing a broader set of language models (e.g. larger Transformers, different pre-training approaches) and training strategies (e.g. domain-specific pre-training, multi-task learning) on RSVQA datasets would help identify the most effective approaches.

### Open Question 2
- Question: How can new RSVQA datasets be designed to reduce language biases while maintaining task relevance and diversity?
- Basis in paper: Explicit. The paper identifies sources of language bias in existing RSVQA datasets and calls for new datasets designed with bias reduction in mind.
- Why unresolved: Designing datasets that are both less biased and still challenging and relevant to real-world RSVQA applications is a complex task that requires further research.
- What evidence would resolve it: Studies evaluating the effectiveness of different dataset design strategies (e.g. more diverse question generation, balanced answer distributions, human-generated questions) in reducing biases while maintaining task quality would provide insights into best practices.

### Open Question 3
- Question: What evaluation metrics and benchmarks are most appropriate for assessing the robustness and generalization of RSVQA models in the presence of language biases?
- Basis in paper: Explicit. The paper proposes new relative evaluation metrics that account for language biases but acknowledges the need for further research in this area.
- Why unresolved: Developing metrics that accurately reflect model performance while considering biases is challenging, especially when comparing across different datasets and model architectures.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different evaluation metrics (e.g. relative accuracy, out-of-distribution performance) in capturing model robustness and generalization would help establish best practices for RSVQA evaluation.

## Limitations

- The study relies on artificial question generation strategies that may not reflect natural human questioning patterns
- Visual blind models may not capture all possible language shortcuts, potentially underestimating the severity of biases
- The adversarial testing methodology using random image substitution may not adequately stress-test models that rely on subtle visual cues

## Confidence

**High Confidence Claims:**
- Visual blind models can achieve non-trivial accuracy in RSVQA, confirming the presence of language biases
- Fine-tuning language models (particularly distilBERT) provides significant performance improvements
- The Prior accuracy baseline is a useful metric for understanding dataset imbalance

**Medium Confidence Claims:**
- Language biases in RSVQA are more severe than in standard VQA due to simpler vocabulary and question generation strategies
- Transformers outperform RNNs when fine-tuned, but not when frozen
- The proposed IO LowerBound metrics provide more informative evaluation than standard accuracy

**Low Confidence Claims:**
- Specific numerical values of bias severity across different datasets and question types
- The relative effectiveness of different fusion strategies under biased conditions
- Generalizability of findings to other remote sensing domains beyond the tested datasets

## Next Checks

1. **Distribution Analysis Verification**: Perform a comprehensive analysis of question-answer distributions across all tested datasets to quantify the severity of imbalance that enables visual blind models. Compare these distributions against standard VQA datasets to validate the claim that RSVQA has more severe biases.

2. **Adversarial Testing Robustness**: Implement alternative adversarial testing strategies including image corruption, region occlusion, and semantic-preserving transformations to verify that random substitution adequately captures model reliance on visual input. Compare results across different stress-testing approaches.

3. **Bias Transfer Analysis**: Train visual blind models on subsets of data with varying levels of question diversity and complexity to determine whether the observed biases are fundamental to the task or artifacts of the specific generation strategies used in the tested datasets.