---
ver: rpa2
title: 'Founder-GPT: Self-play to evaluate the Founder-Idea fit'
arxiv_id: '2312.12037'
source_url: https://arxiv.org/abs/2312.12037
tags:
- founder
- idea
- success
- score
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Founder-GPT, a method for evaluating the
  fit between a startup founder's background and their business idea using large language
  models. The approach combines embeddings, self-play, tree-of-thought prompting,
  and critique-based refinement to assess founder profiles against similar successful
  and failed startups.
---

# Founder-GPT: Self-play to evaluate the Founder-Idea fit

## Quick Facts
- arXiv ID: 2312.12037
- Source URL: https://arxiv.org/abs/2312.12037
- Reference count: 7
- The method generates a composite score (0-1) representing likelihood of startup success by evaluating founder profiles against successful and failed startups using embeddings, self-play, and critique-based refinement.

## Executive Summary
This paper introduces Founder-GPT, a method for evaluating the fit between a startup founder's background and their business idea using large language models. The approach combines embeddings, self-play, tree-of-thought prompting, and critique-based refinement to assess founder profiles against similar successful and failed startups. The system generates a composite score representing the likelihood of startup success, considering founder experience, idea viability, and their alignment. Results on sample cases show scores ranging from 0 (high likelihood of failure) to 0.9 (high likelihood of success), demonstrating the method's ability to differentiate between strong and weak founder-idea matches.

## Method Summary
The Founder-GPT method evaluates founder-idea fit through a multi-stage process combining semantic embeddings, self-play simulation, and hierarchical scoring. Founder profiles and startup ideas are converted into embeddings using pre-trained sentence-transformers models, then compared against databases of successful and failed startups to identify similar cases. A multi-expert LLM orchestrator simulates multiple venture capital analysts who iteratively critique and refine their evaluations through tree-of-thought reasoning. The system calculates separate scores for founder quality, idea viability, and their alignment, then aggregates them into a composite success probability using a multiplicative formula.

## Key Results
- Composite scores range from 0 to 0.9, with 0 indicating high likelihood of failure and 0.9 indicating high likelihood of success
- The method successfully differentiates between strong and weak founder-idea matches in sample evaluations
- Self-play and critique-based refinement techniques show early promising results in improving evaluation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-play approach simulates multiple VC analyst perspectives to iteratively refine founder-idea fit assessments.
- Mechanism: By prompting the LLM to generate multiple "expert" viewpoints, each building upon and critiquing the previous ones, the system converges toward a more balanced and logical evaluation.
- Core assumption: Multiple simulated perspectives can reduce individual LLM biases and surface more comprehensive evaluation criteria.
- Evidence anchors:
  - [abstract] "self-play, tree-of-thought, and critique-based refinement techniques show early promising results"
  - [section] "The main idea is to begin our prompt with the following decorator: Imagine three different Venture Capital analysts are trying to find the successful features of founders..."
  - [corpus] No direct corpus evidence found; this appears to be a novel application of self-play techniques
- Break condition: If the LLM consistently fails to generate meaningful critiques between simulated experts, the self-play mechanism would lose effectiveness.

### Mechanism 2
- Claim: The embedding-based similarity search identifies founder and idea patterns from successful and failed startups.
- Mechanism: By converting founder profiles and startup ideas into dense vector representations, the system finds semantically similar cases from its dataset to extract success/failure patterns.
- Core assumption: Similar founder profiles and ideas will exhibit comparable success patterns across different startups.
- Evidence anchors:
  - [abstract] "Embeddings, self-play, tree-of-thought, and critique-based refinement techniques show early promising results"
  - [section] "The Founder Description and Previous Jobs Held Before Founding fields were subsequently individually converted into embeddings using a pre-trained sentence-transformers model"
  - [corpus] No direct corpus evidence found for this specific embedding application in founder-idea fit evaluation
- Break condition: If the embedding space fails to capture relevant semantic similarities, the pattern extraction would be ineffective.

### Mechanism 3
- Claim: The hierarchical scoring system (founder score × idea score × fit score) captures multiple dimensions of startup success likelihood.
- Mechanism: The system calculates separate scores for founder quality, idea viability, and their alignment, then aggregates them to produce a composite success probability.
- Core assumption: Startup success depends on the multiplicative interaction of founder quality, idea quality, and their fit rather than any single factor.
- Evidence anchors:
  - [abstract] "The approach generates a composite score representing the likelihood of startup success, considering founder experience, idea viability, and their alignment"
  - [section] "Embracing the philosophy that the founder is the heart of a startup... we consider the following formulae for calculating the overall founder suitability score: η = ηfounder × (1/2 × ηfit × ηidea)"
  - [corpus] No direct corpus evidence found for this specific multiplicative scoring approach
- Break condition: If any component score is zero, the entire evaluation becomes zero regardless of other strengths, which may be overly punitive.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding spaces
  - Why needed here: The system uses cosine similarity to measure semantic similarity between founder profiles and ideas
  - Quick check question: What does a cosine similarity of 0.8 indicate about the relationship between two vectors?

- Concept: Tree-of-thought reasoning
  - Why needed here: The system uses ToT prompting to structure the evaluation process into hierarchical reasoning steps
  - Quick check question: How does tree-of-thought differ from simple chain-of-thought prompting?

- Concept: Self-play in reinforcement learning
  - Why needed here: The system applies self-play concepts by having multiple LLM instances critique each other's evaluations
  - Quick check question: In what way does the self-play mechanism in this paper resemble traditional self-play in game-playing AI?

## Architecture Onboarding

- Component map: Data preprocessing → Embedding generation → Similarity search → Multi-expert evaluation → Score aggregation → Result output

- Critical path: Data preprocessing → Embedding generation → Similarity search → Multi-expert evaluation → Score aggregation → Result output

- Design tradeoffs:
  - Embedding quality vs. computational cost: Using pre-trained sentence-transformers balances performance with resource requirements
  - Number of simulated experts vs. evaluation depth: Three experts provide diversity without excessive computational overhead
  - Fixed vs. adaptive similarity thresholds: Current approach uses fixed top-3 retrieval, which may miss relevant cases

- Failure signatures:
  - Uniform scores across all evaluations indicate potential issues with the critique mechanism
  - Extreme scores (0 or 1) suggest either overly harsh or insufficiently critical evaluation criteria
  - High variance between expert evaluations may indicate instability in the self-play process

- First 3 experiments:
  1. Test the embedding similarity search with known similar founder profiles to verify semantic matching accuracy
  2. Validate the self-play mechanism by comparing single-expert vs. multi-expert evaluations on the same input
  3. Stress-test the scoring aggregation by systematically varying component scores to observe composite score behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the Founder-GPT method at predicting startup success across different geographic regions and cultures?
- Basis in paper: [inferred] The paper mentions a dataset bias towards the United States, limiting generalizability of findings.
- Why unresolved: The method hasn't been tested on data from diverse global regions to validate its effectiveness in different cultural and economic contexts.
- What evidence would resolve it: Testing the method on datasets from various countries and comparing prediction accuracy across regions.

### Open Question 2
- Question: Can the Founder-GPT method be improved by incorporating additional data sources beyond LinkedIn profiles?
- Basis in paper: [explicit] The paper discusses using LinkedIn data for founder profiles and mentions the need for further validation.
- Why unresolved: The current method relies solely on LinkedIn data, potentially missing valuable information from other sources like social media, company websites, or industry-specific platforms.
- What evidence would resolve it: Testing the method with additional data sources and measuring improvements in prediction accuracy and comprehensiveness.

### Open Question 3
- Question: How can the subject and degree categorization in the Founder-GPT method be refined to better capture the nuances of different educational backgrounds?
- Basis in paper: [explicit] The paper mentions the need to redefine subject mapping and increase subject and degree coverage.
- Why unresolved: The current categorization may oversimplify complex educational backgrounds, potentially missing important connections between education and startup success.
- What evidence would resolve it: Expert input to refine categorization and testing the method with the improved categorization to measure impact on prediction accuracy.

## Limitations
- The methodology relies heavily on prompt engineering and LLM-based reasoning without sufficient empirical validation or comparison against ground truth startup outcomes
- The embedding-based similarity search assumes semantic equivalence translates to predictive validity, an assumption that requires verification
- The self-play mechanism's effectiveness depends on the LLM's ability to generate meaningful critiques, which is not demonstrated through systematic testing

## Confidence

**Medium Confidence**: The claim that self-play and multi-expert critique mechanisms improve evaluation quality. While the conceptual framework is sound, the paper lacks quantitative evidence showing that multiple simulated perspectives actually outperform single-expert evaluations or reduce bias in practice.

**Low Confidence**: The claim that the composite scoring formula (η = ηfounder × (1/2 × ηfit × ηidea)) accurately represents startup success likelihood. This multiplicative model appears arbitrary without empirical validation showing its predictive power compared to alternative formulations.

**Low Confidence**: The claim that embedding-based similarity search captures relevant patterns for founder-idea fit assessment. The paper does not demonstrate that semantic similarity in embedding space correlates with actual startup success patterns or that the retrieved examples are genuinely informative.

## Next Checks

1. **Cross-validation with real outcomes**: Test the evaluation system against a dataset of startups with known outcomes (success/failure) to measure actual predictive accuracy and compare against baseline methods like random selection or simple heuristics.

2. **Sensitivity analysis of self-play parameters**: Systematically vary the number of simulated experts, iteration counts, and stopping criteria to identify optimal configurations and determine whether the self-play mechanism provides consistent improvements over single-expert evaluations.

3. **Embedding space quality assessment**: Conduct qualitative and quantitative analysis of the embedding similarity search by having human experts evaluate whether the top-3 retrieved examples are genuinely relevant and informative for assessing the target founder-idea pair.