---
ver: rpa2
title: Understanding the Role of Layer Normalization in Label-Skewed Federated Learning
arxiv_id: '2308.09565'
source_url: https://arxiv.org/abs/2308.09565
tags:
- normalization
- layer
- learning
- label
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the role of layer normalization (LN) in federated
  learning under extreme label shift, where each client only has access to data from
  a single class. The authors show that LN helps to address feature collapse and local
  overfitting in such settings by constraining feature norms, thereby accelerating
  global training.
---

# Understanding the Role of Layer Normalization in Label-Skewed Federated Learning

## Quick Facts
- arXiv ID: 2308.09565
- Source URL: https://arxiv.org/abs/2308.09565
- Reference count: 27
- Key outcome: Layer normalization improves federated learning under extreme label shift by constraining feature norms and preventing feature collapse

## Executive Summary
This paper investigates the role of layer normalization (LN) in federated learning under extreme label skew, where each client only has access to data from a single class. The authors demonstrate that LN helps mitigate feature collapse and local overfitting by constraining feature norms, which accelerates global training convergence. They identify feature normalization (FN) as the essential mechanism within LN, showing that normalizing only the last-layer feature embeddings before classification captures most of LN's benefits while being computationally simpler.

## Method Summary
The paper studies federated learning with extreme label skew using the FedAvg framework. The method involves adding layer normalization (LN) or feature normalization (FN) to standard CNN/ResNet architectures before the classification layer. The authors evaluate their approach on CIFAR-10/100, TinyImageNet, and PACS datasets with one-class-per-client and Dirichlet distribution settings, comparing against baselines like FedProx, SCAFFOLD, and FedLC. The training uses SGD optimizer with batch size 32, and experiments test learning rate sensitivity and convergence under different data heterogeneity levels.

## Key Results
- FedLN improves FedAvg by ~32% in absolute test accuracy in the one-class setting on CIFAR-10
- FN captures most of LN's performance gains while being computationally simpler
- LN/FN provide significant robustness to learning rate choices compared to vanilla FedAvg
- Benefits of LN/FN diminish as data heterogeneity decreases (less label skew)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layer normalization (LN) mitigates feature collapse under extreme label shift by constraining feature norms, thereby accelerating global training convergence.
- **Mechanism:** LN normalizes latent feature representations before the classifier head (feature normalization, FN), preventing local overfitting on heavily skewed datasets where each client only sees data from one class.
- **Core assumption:** The feature embedding network is scale equivariant, meaning scaling the input scales the output proportionally.
- **Evidence anchors:**
  - [abstract]: "LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training."
  - [section 4.2]: "Normalization addresses feature collapse. In fact, in the one-class setting, there is no need for the feature embedding to distinguish images from different classes... If the feature norms are constrained (like in LN/FN), then each client cannot overfit by increasing the feature norms, but has to learn the directional information of the feature embeddings."
  - [corpus]: Weak - corpus neighbors focus on label shift in federated learning but do not directly address the feature collapse mechanism.
- **Break condition:** If the neural network architecture lacks scale equivariance (e.g., Vision Transformers with multi-head attention), the simplification of LN to FN may not hold, reducing its effectiveness.

### Mechanism 2
- **Claim:** Feature normalization (FN) is the essential mechanism within LN that drives performance improvements in label-skewed federated learning.
- **Mechanism:** FN simplifies LN by normalizing only the last-layer feature embeddings before classification, retaining similar performance while avoiding computational overhead of per-layer normalization.
- **Core assumption:** The expressive power of FN is equivalent to LN under the given neural network architecture (CNN/ResNet with ReLU activations).
- **Evidence anchors:**
  - [abstract]: "We discover the key mechanism of LN in such problems, feature normalization, which simply normalizes pre-classification layer feature embeddings."
  - [section 4.1]: "Scale normalizing each layer is equivalent to only scale normalizing the last layer... a layer-normalized neural network can be simplified to only MV normalizing the last layer and shifting previous layers."
  - [section 5.1]: "FedFN largely captures the performance gain of FedLN, despite being consistently inferior to FedLN."
- **Break condition:** If the neural network includes biases after the first layer or uses non-Leaky ReLU activations, the scale equivariance assumption breaks, and FN may not fully capture LN's benefits.

### Mechanism 3
- **Claim:** LN/FN improve learning rate robustness in federated learning under label shift by stabilizing the optimization landscape.
- **Mechanism:** Normalization constrains the feature norms, reducing the sensitivity of the loss landscape to learning rate choices and preventing extreme local overfitting.
- **Core assumption:** The optimization dynamics are smoother when feature norms are bounded, leading to more stable gradient updates.
- **Evidence anchors:**
  - [abstract]: "Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices."
  - [section 5.5]: "With different learning rates, the performance of the vanilla method changes severely, while the variation of FN/LN is relatively small within a large range of learning rates."
  - [corpus]: Weak - corpus neighbors do not address learning rate robustness in the context of normalization.
- **Break condition:** If the dataset heterogeneity is low (clients have similar label distributions), the benefit of LN/FN in stabilizing training diminishes.

## Foundational Learning

- **Concept:** Scale equivariance in neural networks
  - Why needed here: Critical for proving that layer-wise normalization can be reduced to last-layer normalization (Proposition 1).
  - Quick check question: If you scale the input of a ReLU network by λ, by what factor does the output scale? (Answer: λ, assuming no biases after the first layer)

- **Concept:** Feature collapse in high-dimensional embeddings
  - Why needed here: Explains why clients with single-class data overfit by mapping all features to a low-dimensional subspace.
  - Quick check question: What happens to the singular values of feature embeddings when feature collapse occurs? (Answer: One singular value dominates, indicating low-rank structure)

- **Concept:** Cross-entropy loss minimization under label skew
  - Why needed here: Shows why minimizing local loss in single-class setting leads to divergent norms (Theorem 1).
  - Quick check question: In a one-class client, what happens to (wc - wk)ᵀgθ(xi) for c ≠ k as training progresses? (Answer: It diverges to -∞, causing norm explosion)

## Architecture Onboarding

- **Component map:** Input data -> Feature embedding network -> Normalization layer -> Classifier head -> Cross-entropy loss
- **Critical path:**
  1. Input data → Feature embedding network → Normalization layer → Classifier head → Cross-entropy loss
  2. Local client training → Model upload → Server aggregation → Global model distribution

- **Design tradeoffs:**
  - LN vs FN: LN provides slightly better performance but higher computational cost; FN is simpler and nearly as effective
  - Per-layer vs last-layer normalization: Per-layer offers more granular control but may not be necessary under scale equivariance
  - Running statistics tracking: Improves stability but adds communication overhead in federated setting

- **Failure signatures:**
  - Performance plateaus early: Likely feature collapse not addressed
  - High variance across learning rates: Normalization not stabilizing optimization
  - Degraded performance on heterogeneous clients: Overfitting to local single-class data

- **First 3 experiments:**
  1. Compare vanilla FedAvg vs FedLN/FedFN on CIFAR-10 one-class setting (10 clients, 1 class each)
  2. Test learning rate sensitivity: Run FedAvg, FedLN, FedFN with learning rates [0.001, 0.01, 0.1]
  3. Validate scale equivariance: Check if scaling input by λ scales output by λ in feature embedding network

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of feature normalization (FN) versus layer normalization (LN) vary depending on the specific activation function used in the neural network?
- Basis in paper: [explicit] The paper mentions Assumption 1, which requires (Leaky) ReLU activation for the theoretical results on scale equivariance. However, it does not explore the impact of other activation functions.
- Why unresolved: The paper's theoretical analysis and empirical results are primarily based on (Leaky) ReLU activation. The effect of other activation functions on the performance of FN and LN remains unexplored.
- What evidence would resolve it: Conducting experiments with different activation functions (e.g., sigmoid, tanh, ELU) and comparing the performance of FN and LN under each activation function would provide insights into the role of activation functions in the effectiveness of these normalization methods.

### Open Question 2
- Question: How does the performance of FN and LN compare under different types of data heterogeneity beyond label shift, such as concept drift or feature shift?
- Basis in paper: [explicit] The paper focuses on label shift, where each client has data from a single class or a skewed distribution of classes. However, it briefly mentions covariate shift and its potential impact on the effectiveness of normalization methods.
- Why unresolved: The paper's experiments and analysis are primarily focused on label shift scenarios. The performance of FN and LN under other types of data heterogeneity, such as concept drift or feature shift, remains unexplored.
- What evidence would resolve it: Conducting experiments with datasets that exhibit concept drift or feature shift and comparing the performance of FN and LN under these scenarios would provide insights into the generalizability of these normalization methods.

### Open Question 3
- Question: Can the theoretical insights on the expressive power of FN and LN be extended to other neural network architectures beyond ResNet and CNN, such as Transformers or Graph Neural Networks?
- Basis in paper: [explicit] The paper discusses the extension of the theoretical results to ResNet but notes that the conclusions do not immediately extend to Vision Transformers due to the lack of scale equivariance in the multi-head attention mechanism.
- Why unresolved: The paper's theoretical analysis and empirical results are primarily based on ResNet and CNN architectures. The applicability of the insights on expressive power to other architectures, such as Transformers or Graph Neural Networks, remains unexplored.
- What evidence would resolve it: Conducting theoretical analysis and empirical experiments on other neural network architectures, such as Transformers or Graph Neural Networks, would provide insights into the generalizability of the expressive power results.

## Limitations
- The analysis assumes ReLU activations and no biases after the first layer, which may not generalize to architectures with different activation functions
- The "scale equivariance" assumption, while reasonable for standard CNNs, requires validation for transformer-based models
- Benefits diminish with less heterogeneous data distributions, leaving questions about optimal normalization for moderate non-IID settings

## Confidence
- **High confidence**: Empirical accuracy improvements, basic mechanism of feature norm constraint
- **Medium confidence**: Theoretical analysis of feature collapse, scale equivariance simplification
- **Medium confidence**: Learning rate robustness claims, diminishing returns in less skewed settings

## Next Checks
1. **Architecture Generalization Test**: Validate the effectiveness of FN/LN on transformer-based architectures (e.g., ViT) where scale equivariance may not hold due to self-attention mechanisms.

2. **Representation Quality Analysis**: Beyond accuracy, evaluate representation quality using downstream tasks, clustering metrics, or out-of-distribution detection to verify that normalized features capture meaningful semantic information.

3. **Moderate Heterogeneity Benchmark**: Design experiments with intermediate levels of label skew (Dirichlet parameter α between 0.1 and 1.0) to map the full spectrum of normalization benefits and identify optimal strategies for different degrees of non-IIDness.