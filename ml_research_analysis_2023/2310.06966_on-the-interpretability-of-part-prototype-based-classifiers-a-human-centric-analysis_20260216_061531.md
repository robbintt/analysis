---
ver: rpa2
title: 'On the Interpretability of Part-Prototype Based Classifiers: A Human Centric
  Analysis'
arxiv_id: '2310.06966'
source_url: https://arxiv.org/abs/2310.06966
tags:
- prototypes
- prototype
- methods
- interpretability
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a human-centric framework to evaluate interpretability
  of part-prototype classifiers. The authors address the lack of rigorous human evaluation
  of interpretability in these models, which are often assumed interpretable without
  validation.
---

# On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis

## Quick Facts
- **arXiv ID**: 2310.06966
- **Source URL**: https://arxiv.org/abs/2310.06966
- **Reference count**: 26
- **Key outcome**: Human evaluation reveals significant variation in interpretability across part-prototype classifiers, with TesNet and ProtoTree showing highest prototype interpretability but poor similarity between prototypes and query activations.

## Executive Summary
This paper introduces a human-centric framework to evaluate the interpretability of part-prototype classifiers, which are often assumed to be interpretable without rigorous validation. The authors conduct extensive human evaluation experiments on six part-prototype methods (ProtoPNet, Deformable ProtoPNet, SPARROW, ProtoTree, TesNet, ProtoPool) and one unsupervised method (ACE) across three datasets (CUB-200, Stanford Cars, ImageNet subset). Their framework evaluates interpretability through three dimensions: prototype interpretability, similarity between prototypes and their activated query regions, and interpretability of the decision-making process. The results demonstrate that interpretability varies significantly across methods and that human evaluation is crucial for assessing true interpretability.

## Method Summary
The authors evaluate interpretability using three experimental paradigms on Amazon Mechanical Turk. First, they assess prototype interpretability through comparative and absolute experiments where workers choose or rate which prototype is more interpretable. Second, they evaluate similarity between prototypes and their activated query regions by showing workers pairs of prototypes and query regions and asking which pair shows better similarity. Third, they test decision-making interpretability by showing workers the top-10 prototypes activated for a query and asking them to predict the model's classification decision. Each experiment includes qualification tests and validation samples to ensure worker quality, with all experiments conducted in a within-subject design.

## Key Results
- TesNet achieved the highest prototype interpretability (95.18%) while ProtoPool performed worst (48.86%)
- All methods showed poor similarity between prototypes and their activated query regions, with TesNet highest at 42.98%
- ProtoPool required an average of 5.58 prototypes per query to make decisions, compared to ~2 for other methods
- ProtoPool's decision-making interpretability was significantly worse than other methods, requiring many prototypes to reach comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The framework evaluates interpretability across three distinct dimensions: prototype interpretability, prototype-query similarity, and decision-making interpretability.
- **Mechanism**: By decomposing interpretability into measurable components, the framework avoids conflating different aspects of interpretability that may behave differently across methods.
- **Core assumption**: Interpretability is not a monolithic property but rather a combination of distinct properties that can be evaluated separately.
- **Evidence anchors**:
  - [abstract] "Our proposed framework consists of three actionable metrics and experiments."
  - [section] "Our experiments were designed to measure the human opinion on the individual properties required for an interpretable method."
  - [corpus] No direct corpus evidence available for this decomposition claim.

### Mechanism 2
- **Claim**: Comparative experiments provide a reference frame that improves human annotator consistency and reliability.
- **Mechanism**: By showing annotators pairs of prototypes or prototype-query pairs from different methods, they can make relative judgments rather than absolute ones, which are known to be less reliable.
- **Core assumption**: Humans make more consistent judgments when comparing items directly rather than rating them in isolation.
- **Evidence anchors**:
  - [abstract] "In the first experiment, each person was shown a pair of prototypes... The annotators had to choose the prototype they found more interpretable out of the two."
  - [section] "This ensured that people had a frame of reference when deciding which method worked better."
  - [corpus] Weak corpus support - the referenced HIVE framework also uses comparative approaches but the paper doesn't explicitly cite research on comparative vs. absolute judgments.

### Mechanism 3
- **Claim**: Using top-k prototypes for decision-making evaluation captures the practical interpretability of the model.
- **Mechanism**: Most part-prototype methods use a small number of high-activation prototypes for final decisions. By testing whether humans can predict the model's decision from these top-k prototypes, the framework measures whether the decision-making process is interpretable in practice.
- **Core assumption**: The interpretability of a model's decision-making process is best assessed by testing whether humans can replicate the model's decision using the same information the model uses.
- **Evidence anchors**:
  - [abstract] "Our final experiment was designed to assess the interpretability of the decision-making process."
  - [section] "If the model, using those same 10 prototypes, is able to classify the query image and come to the same decision as before... then those 10 prototypes are a good proxy for the decision-making process."
  - [corpus] No direct corpus evidence for this specific methodology.

## Foundational Learning

- **Concept**: Prototype-based classifiers
  - Why needed here: The framework evaluates part-prototype methods, which are a specific type of prototype-based classifier. Understanding how these classifiers work is essential for interpreting the experimental design.
  - Quick check question: How do part-prototype methods differ from traditional prototype-based classifiers in terms of what they compare during classification?

- **Concept**: Human-centered evaluation methodologies
  - Why needed here: The framework relies on human annotators to assess interpretability, which requires understanding best practices in experimental design to ensure reliable results.
  - Quick check question: Why might asking humans to rate interpretability on an absolute scale be less reliable than asking them to make comparative judgments?

- **Concept**: Interpretability metrics vs. evaluation
  - Why needed here: The paper distinguishes between automatic metrics (which it argues are insufficient) and human evaluation, highlighting the importance of understanding what different evaluation approaches measure.
  - Quick check question: What is a key limitation of automatic metrics for assessing interpretability that human evaluation can address?

## Architecture Onboarding

- **Component map**: The framework consists of three main experimental components: prototype interpretability evaluation (comparative and absolute versions), prototype-query similarity evaluation (comparative and absolute versions), and decision-making interpretability evaluation. Each uses Amazon Mechanical Turk with qualification tests and validation samples.

- **Critical path**: The most critical path is the decision-making interpretability experiment, as it tests whether the model's actual classification decisions are interpretable to humans using the same information the model uses.

- **Design tradeoffs**: The framework trades off comprehensiveness for focus by evaluating only three aspects of interpretability rather than attempting to capture all possible dimensions. It also uses both comparative and absolute experimental designs to cross-validate results.

- **Failure signatures**: If results show low similarity between prototypes and their activated query regions across all methods, this suggests a fundamental limitation of the part-prototype approach rather than issues with specific implementations. If ProtoPool shows particularly poor decision-making interpretability, this indicates issues with multi-class prototype assignment.

- **First 3 experiments**:
  1. Run the comparative prototype interpretability experiment on a small subset of data to validate the experimental design and qualification process.
  2. Test the prototype-query similarity experiment with a single method and dataset to ensure the instructions are clear and the evaluation captures meaningful differences.
  3. Conduct a pilot of the decision-making interpretability experiment with a simplified version (e.g., using only 5 prototypes instead of 10) to verify that humans can reasonably complete the task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different prototype-based methods compare in terms of interpretability when evaluated using a unified human-centric framework?
- **Basis in paper**: [explicit] The paper evaluates six part-prototype methods (ProtoPNet, Deformable ProtoPNet, SPARROW, ProtoTree, TesNet, ProtoPool) and one unsupervised method (ACE) on three datasets using human evaluations.
- **Why unresolved**: While the paper provides comparative results, it does not establish a definitive ranking of interpretability across all methods due to the subjective nature of human evaluations and the varying contexts in which these methods are applied.
- **What evidence would resolve it**: A large-scale study involving diverse datasets and a broader range of human evaluators to determine consistent patterns in interpretability rankings across different methods.

### Open Question 2
- **Question**: What are the specific factors that contribute to the dissimilarity between prototypes and their corresponding activations in part-prototype methods?
- **Basis in paper**: [explicit] The paper identifies a significant issue with the similarity between prototypes and their activated query regions across all methods tested.
- **Why unresolved**: The paper does not delve into the underlying causes of this dissimilarity, such as the nature of the embedding space or the training process, which could provide insights into improving prototype-query similarity.
- **What evidence would resolve it**: Detailed analysis of the embedding spaces and training dynamics of part-prototype methods, potentially using visualization techniques or mathematical models to understand the factors leading to dissimilarity.

### Open Question 3
- **Question**: How can the interpretability of part-prototype methods be improved without significantly compromising their classification performance?
- **Basis in paper**: [inferred] The paper suggests that while some methods perform well in interpretability tests, they may still require a large number of prototypes to maintain classification accuracy, which could overwhelm human understanding.
- **Why unresolved**: The paper does not explore potential modifications to the methods or training processes that could enhance interpretability while maintaining or improving classification performance.
- **What evidence would resolve it**: Development and evaluation of new methods or modifications to existing ones that balance interpretability and performance, supported by empirical studies comparing these methods to current approaches.

## Limitations
- The evaluation focused on three specific datasets which may not generalize to all domains
- The human evaluation relied on AMT workers who may not represent domain experts
- The framework evaluates only three aspects of interpretability which may not capture all relevant dimensions

## Confidence
- **High**: Human evaluation is essential for assessing interpretability - directly demonstrated through systematic experiments
- **Medium**: Specific ranking of methods by interpretability - potential sampling variability in human evaluations and limited datasets
- **High**: Interpretability varies significantly across methods - consistent patterns observed across multiple experiments

## Next Checks
1. Replicate the experiments on additional datasets from different domains (medical imaging, satellite imagery) to test generalizability of the interpretability rankings.
2. Conduct expert evaluation studies comparing domain experts' interpretability assessments with those of AMT workers to validate the crowdworker approach.
3. Test whether the same framework can detect changes in interpretability when interpretable models are deliberately modified (e.g., by simplifying prototypes or reducing the number of prototypes used for decisions).