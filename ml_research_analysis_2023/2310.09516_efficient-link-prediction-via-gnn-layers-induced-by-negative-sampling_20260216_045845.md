---
ver: rpa2
title: Efficient Link Prediction via GNN Layers Induced by Negative Sampling
arxiv_id: '2310.09516'
source_url: https://arxiv.org/abs/2310.09516
tags:
- negative
- node
- graph
- yinyangnn
- node-wise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency-accuracy tradeoff in graph
  neural networks for link prediction. Node-wise methods are fast but less accurate,
  while edge-wise methods are accurate but slow.
---

# Efficient Link Prediction via GNN Layers Induced by Negative Sampling

## Quick Facts
- arXiv ID: 2310.09516
- Source URL: https://arxiv.org/abs/2310.09516
- Authors: [Not specified in input]
- Reference count: 40
- Primary result: YinYanGNN achieves 93.8% HR@100 on Cora, outperforming node-wise baselines while matching their speed

## Executive Summary
This paper addresses the efficiency-accuracy tradeoff in graph neural networks for link prediction. Node-wise methods are fast but less accurate, while edge-wise methods are accurate but slow. The authors propose YinYanGNN, a novel node-wise architecture that incorporates negative sampling into the forward pass via an energy function. This allows each node embedding to depend on both positive and negative samples, increasing expressiveness without sacrificing efficiency. Experiments on seven datasets show YinYanGNN outperforms existing node-wise methods in accuracy while matching their speed, and outperforms edge-wise methods in speed while maintaining competitive accuracy.

## Method Summary
YinYanGNN is a node-wise GNN architecture that incorporates negative sampling into the forward pass via an energy function. The method processes graph data with node features through a base model, then performs T layers of gradient descent updates on the energy function that combines positive and negative edge regularization. Node embeddings are derived as minimizers of this energy function, which are then passed through a HadamardMLP decoder to compute edge scores. The model is trained by optimizing this energy function to learn node embeddings, with the ability to learn weights for multiple negative graphs to improve expressiveness.

## Key Results
- Achieves 93.8% HR@100 on Cora, significantly outperforming node-wise baselines (66.8% HR@100)
- Matches the speed of node-wise methods while providing accuracy comparable to edge-wise methods
- Outperforms edge-wise methods in inference speed while maintaining competitive accuracy
- Demonstrates effectiveness across seven diverse datasets including Cora, Citeseer, Pubmed, and OGB benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative sampling in the forward pass breaks node isomorphism without sacrificing efficiency.
- Mechanism: By incorporating both positive and negative edges into the energy function during forward propagation, node embeddings are explicitly informed by both connected and non-connected node features. This allows isomorphic nodes to be distinguished based on their relationships to negative samples.
- Core assumption: The energy function's regularization terms effectively separate positive and negative edges during embedding optimization.
- Evidence anchors:
  - [abstract] "we propose a novel GNN architecture whereby the forward pass explicitly depends on both positive (as is typical) and negative (unique to our approach) edges"
  - [section 4] "we seek a node embedding matrix Y = arg minY ℓnode(G, G−) in such a way that the optimal solution decomposes as yi = g(vi, Gi, G−i)"
- Break condition: If the energy function doesn't properly balance positive and negative edge regularization, or if the step size α is not chosen appropriately (α < (I + λL̃ − λK/K L̃−)−1F), convergence to meaningful embeddings fails.

### Mechanism 2
- Claim: Learning to combine multiple negative graphs improves expressiveness over fixed negative sampling.
- Mechanism: Instead of using a single fixed set of negative samples, the model learns weights λk/K for K different negative graphs. This allows the model to adaptively emphasize different negative sampling strategies during training.
- Core assumption: Different negative sampling strategies capture complementary information about node relationships.
- Evidence anchors:
  - [section 4.2] "we sample K negative graphs {G−(k)}k=1,...,K, in which every negative graph consists of one negative edge per positive edge"
  - [section 4.2] "we set learnable weights λk/K for the structure term of each negative graph"
- Break condition: If K is too large, computational complexity becomes prohibitive; if K is too small, the benefits of diverse negative sampling are not realized.

### Mechanism 3
- Claim: The energy function minimization process enables large receptive fields without oversmoothing.
- Mechanism: By recasting node embeddings as minimizers of a graph-regularized energy function, the model inherits properties from prior optimization-based GNN models that allow for large receptive fields without the oversmoothing problem common in traditional GNNs.
- Core assumption: The energy function's regularization terms prevent the embeddings from collapsing to a single point.
- Evidence anchors:
  - [abstract] "by unifying the positive and negative samples within a single energy function minimization process, the implicit receptive field of the embeddings can be arbitrarily large without oversmoothing"
- Break condition: If the energy function is not properly regularized, embeddings may still collapse; if the step size α is too large, the optimization process may diverge.

## Foundational Learning

- Concept: Energy function minimization
  - Why needed here: The paper uses energy function minimization to derive the node embeddings, which is different from standard GNN training that directly optimizes a loss function.
  - Quick check question: How does the energy function ℓnode relate to the link prediction loss Llink?

- Concept: Negative sampling in graph learning
  - Why needed here: Negative sampling is used not just for training signal but as an integral part of the forward pass to increase model expressiveness.
  - Quick check question: What is the difference between how negative samples are used in YinYanGNN versus traditional GNNs?

- Concept: Graph Laplacian and normalized Laplacian
  - Why needed here: The paper uses Laplacian matrices (L and L−) to regularize the energy function and control the influence of positive and negative edges.
  - Quick check question: How do the normalized Laplacians L̃ and L̃− differ from the standard Laplacians L and L−?

## Architecture Onboarding

- Component map:
  Input features → Base model → T gradient descent steps → Node embeddings → Decoder → Edge scores → Loss

- Critical path: Input features → Base model → T gradient descent steps → Node embeddings → Decoder → Edge scores → Loss

- Design tradeoffs:
  - K (number of negative graphs): Larger K increases expressiveness but also computational cost
  - T (number of propagation layers): More layers allow larger receptive fields but increase computation
  - λK (negative edge regularization weight): Higher values emphasize negative edges but may destabilize training

- Failure signatures:
  - Poor performance on node-wise baselines: Likely issue with energy function design or hyperparameter tuning
  - Slow inference: Likely issue with K or T being too large, or not using Flashlight for decoding
  - Training instability: Likely issue with λK being too large or α being too small

- First 3 experiments:
  1. Verify that negative sampling in the forward pass improves accuracy over no negative sampling (Table 3 in paper)
  2. Test different values of K (1, 2, 4) to find the best tradeoff between accuracy and speed
  3. Compare with GCN baseline to ensure the base model is working correctly before adding negative sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does YinYanGNN's performance scale with increasing negative sampling rate K?
- Basis in paper: [explicit] The paper mentions that YinYanGNN's performance can be improved by learning the weights {λk_K} for different negative graphs, but doesn't extensively explore the impact of varying K.
- Why unresolved: The paper only briefly mentions the ability to learn {λk_K} and its potential benefits for smaller K, but doesn't provide a comprehensive analysis of how performance scales with different values of K.
- What evidence would resolve it: Systematic experiments varying K and comparing performance across different graph sizes and types would provide insight into the optimal negative sampling rate.

### Open Question 2
- Question: Can YinYanGNN's negative sampling strategy be extended to heterogeneous graphs with different edge types?
- Basis in paper: [inferred] The paper focuses on homogeneous graphs and doesn't address the applicability of YinYanGNN to heterogeneous graphs.
- Why unresolved: Heterogeneous graphs are common in real-world applications, and it's unclear how YinYanGNN's negative sampling approach would generalize to such scenarios.
- What evidence would resolve it: Experiments applying YinYanGNN to heterogeneous graph datasets and analyzing the impact of different negative sampling strategies for various edge types would provide clarity.

### Open Question 3
- Question: How does YinYanGNN's performance compare to other link prediction methods when incorporating additional node features beyond basic graph structure?
- Basis in paper: [explicit] The paper primarily focuses on graph structure and doesn't extensively explore the impact of incorporating additional node features.
- Why unresolved: Many real-world graphs have rich node features that could potentially improve link prediction performance, but it's unclear how YinYanGNN would handle such features compared to other methods.
- What evidence would resolve it: Experiments comparing YinYanGNN's performance with and without additional node features, and against other methods that incorporate such features, would provide insight into its effectiveness in feature-rich scenarios.

## Limitations
- Limited ablation studies on the choice of K and its impact on both accuracy and efficiency
- No extensive exploration of the energy function's sensitivity to hyperparameters like α and λK
- Experiments are limited to relatively small datasets, raising questions about scalability to larger, more complex graphs

## Confidence

- High confidence: The empirical results showing YinYanGNN's superior accuracy compared to node-wise methods and competitive efficiency relative to edge-wise methods are well-supported by the experimental data.
- Medium confidence: The theoretical justification for why negative sampling in the forward pass improves expressiveness is plausible but not rigorously proven. The paper relies on empirical evidence rather than theoretical guarantees.
- Low confidence: The generalizability of the approach to larger, more complex graphs is uncertain, as the experiments are limited to relatively small datasets.

## Next Checks
1. **Ablation study on K:** Conduct experiments varying K (e.g., 1, 2, 4, 8) to quantify the tradeoff between accuracy and efficiency, and identify the optimal K for different dataset characteristics.
2. **Sensitivity analysis of hyperparameters:** Perform a grid search or random search over α and λK to determine their impact on convergence and final performance, and identify stable ranges for these hyperparameters.
3. **Scalability test:** Evaluate YinYanGNN on larger, more complex graphs (e.g., social networks, knowledge graphs) to assess its scalability and robustness to graph size and complexity.