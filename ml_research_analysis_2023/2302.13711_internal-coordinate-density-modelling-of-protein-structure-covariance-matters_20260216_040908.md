---
ver: rpa2
title: 'Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters'
arxiv_id: '2302.13711'
source_url: https://arxiv.org/abs/2302.13711
tags:
- protein
- structure
- uctuations
- coordinates
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modelling distributions of
  protein structural states, particularly the small-scale fluctuations in internal
  coordinates (bond angles and torsional angles) that can propagate to large changes
  in the 3D structure. The key difficulty is that small changes in internal coordinates
  can have large consequences for the global structure of the protein due to the complex
  covariance structures between the degrees of freedom.
---

# Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters

## Quick Facts
- arXiv ID: 2302.13711
- Source URL: https://arxiv.org/abs/2302.13711
- Reference count: 19
- Key outcome: A VAE-based model that models protein structure fluctuations in internal coordinates using Lagrange multipliers to induce proper covariance structure, validated on both experimental NMR data and simulation data

## Executive Summary
This paper addresses the challenge of modelling distributions of protein structural states, particularly the small-scale fluctuations in internal coordinates (bond angles and torsional angles) that can propagate to large changes in the 3D structure. The key difficulty is that small changes in internal coordinates can have large consequences for the global structure of the protein due to the complex covariance structures between the degrees of freedom. To overcome this, the authors propose a new strategy for inducing the covariance structure by imposing constraints on downstream atom movement using the Lagrange formalism. They construct a variational autoencoder (VAE) that models fluctuations for full-length proteins in internal coordinates.

## Method Summary
The authors construct a variational autoencoder (VAE) that models fluctuations for full-length proteins in internal coordinates. The VAE decoder predicts a mean over internal coordinates, which is converted into Cartesian coordinates using pNeRF. The mean structure is then used to construct constraints on atom fluctuations in 3D space, which are weighed by predicted Lagrange multipliers to obtain a precision matrix. The model is trained using an evidence lower bound (ELBO) objective with an auxiliary loss on the inverse of Lagrange multipliers to ensure the first-order approximation remains valid. The approach is validated on two test cases: a small α-helical protein with NMR structures and a larger protein from molecular dynamics simulation.

## Key Results
- The model provides meaningful density estimates on ensemble data for proteins obtained from experiment and simulation
- Samples are on par with reference structures in terms of sample quality (QMEAN6 score)
- The model successfully captures both local fluctuations (Ramachandran plots, bond angle distributions) and global structure (atom position variance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The induced covariance structure via Lagrange multipliers accurately captures global protein fluctuations from local internal coordinates.
- Mechanism: The model constructs a precision matrix as the sum of a prior and a constraint-derived term, where the constraint term is modulated by Lagrange multipliers λ. These multipliers are predicted by a neural network (U-Net) and enforce that downstream atom fluctuations match imposed constraints in 3D space. This ensures that small changes in internal coordinates do not lead to unphysical global perturbations.
- Core assumption: The first-order approximation of atom displacements as linear functions of internal coordinate changes is valid within the allowed fluctuation range.
- Evidence anchors:
  - [abstract] "The VAE decoder predicts a mean over internal coordinates, which is converted into Cartesian coordinates using pNeRF. The mean structure is then used to construct constraints on atom fluctuations in 3D space, which are weighed by predicted Lagrange multipliers to obtain a precision matrix."
  - [section] "To first order, we can express the displacement vectors ∆xi_m of each atom with respect to the ith internal coordinate as ∆xi_m = ∂xm/∂κi ∆κi where xm is the position of the mth atom, under the condition that the atom is post-rotational, i.e. the location of atom m is downstream of the ith internal coordinate."
  - [corpus] Weak or missing; no direct evidence in cited neighbors that similar constraint-based covariance induction has been validated for full proteins.
- Break condition: If atom fluctuations exceed the linear regime, the first-order approximation breaks down, leading to inaccurate covariance estimates and potentially unphysical samples.

### Mechanism 2
- Claim: The auxiliary loss on Lagrange multipliers keeps the induced covariance within the valid linear approximation range.
- Mechanism: An auxiliary loss term (mean absolute error over λ⁻¹) is added to the training objective to prevent the prior from dominating and to ensure that the Lagrange multipliers stay within a range where the first-order approximation holds. Tuning the weight waux on this auxiliary loss controls the strength of the constraints.
- Core assumption: The inverse of the Lagrange multipliers is a good proxy for controlling the scale of induced fluctuations.
- Evidence anchors:
  - [abstract] "To ensure this, we add an auxiliary loss in the form of a mean absolute error over λ⁻¹, which prevents the κ-prior from dominating."
  - [section] "To ensure this, we add an auxiliary loss in the form of a mean absolute error over λ⁻¹, which prevents the κ-prior from dominating. By tuning the weight waux on the auxiliary loss, we can influence the strength of the constraints."
  - [corpus] Weak or missing; no evidence in neighbors that auxiliary losses on Lagrange multipliers have been used in this context.
- Break condition: If waux is too small, the model may produce large fluctuations violating the linear approximation; if too large, the model may underfit and fail to capture the true covariance structure.

### Mechanism 3
- Claim: Rotational invariance is maintained despite constraints expressed in Cartesian coordinates.
- Mechanism: The constraints are derived from the mean predicted structure in 3D, but the sampling process does not depend on a fixed global coordinate frame. The model operates on internal coordinates, which are inherently rotationally invariant, and the constraints are applied in a way that does not fix the orientation of the protein.
- Core assumption: The mean structure used to derive constraints is sufficient to capture the relevant relative atom positions without fixing global orientation.
- Evidence anchors:
  - [abstract] "Despite the fact that constraints are expressed in terms of Cartesian coordinates, the model is not dependent on a global reference frame (i.e. it is rotationally invariant)."
  - [section] "The predicted mean over κ is used to get a pairwise distance matrix d that serves as the input to a U-Net (Ronneberger et al., 2015), from which we estimate values for the Lagrange multipliers for each constraint."
  - [corpus] Weak or missing; no evidence in neighbors that rotational invariance is preserved in similar internal-coordinate density models.
- Break condition: If the mean structure is poorly estimated or the pairwise distance matrix does not capture the necessary relative positions, the constraints may inadvertently introduce a preferred orientation.

## Foundational Learning

- Concept: Internal coordinates (dihedrals, bond angles, bond lengths) vs Cartesian coordinates.
  - Why needed here: Understanding the trade-offs between representations is crucial for grasping why the authors chose internal coordinates and how they address the covariance challenge.
  - Quick check question: Why do small fluctuations in internal coordinates lead to large changes in Cartesian coordinates for proteins?

- Concept: Variational Autoencoders (VAEs) and latent variable models.
  - Why needed here: The model is implemented as a VAE, so understanding the encoder-decoder architecture, latent space, and evidence lower bound (ELBO) is essential for understanding how the model learns the distribution over internal coordinates.
  - Quick check question: What is the role of the latent variable z in the VAE architecture described in the paper?

- Concept: Lagrange multipliers and constrained optimization.
  - Why needed here: The core innovation involves using Lagrange multipliers to induce the covariance structure by enforcing constraints on atom fluctuations. Understanding this formalism is key to understanding how the model works.
  - Quick check question: How do Lagrange multipliers relate to the allowed fluctuations in atom positions in the model?

## Architecture Onboarding

- Component map: Internal coordinates -> Encoder (60→30→15) -> Latent space z -> Decoder (15→30→60) -> Mean over κ -> pNeRF -> Cartesian mean -> Pairwise distances -> U-Net -> Lagrange multipliers -> Precision matrix -> Sampling from multivariate Gaussian

- Critical path: Internal coordinates → Encoder → Latent space z → Decoder → Mean over κ → pNeRF → Cartesian mean → Pairwise distances → U-Net → Lagrange multipliers → Precision matrix → Sampling from multivariate Gaussian

- Design tradeoffs:
  - Internal vs Cartesian coordinates: Internal coordinates are rotationally invariant and reduce degrees of freedom, but small fluctuations can cause large global changes; Cartesian coordinates avoid this but require handling global orientation.
  - First-order approximation: Simpler and faster but limited to small fluctuations; higher-order methods could be more accurate but computationally expensive.
  - Auxiliary loss weight: Controls the balance between fitting the data and maintaining valid linear approximation; needs tuning per dataset.

- Failure signatures:
  - Large atom fluctuations not matching constraints: Indicates the first-order approximation is breaking down or the auxiliary loss is not strong enough.
  - Poor sample quality (low QMEAN6 score): Suggests the model is not capturing the true distribution, possibly due to an overly strong prior or incorrect Lagrange multipliers.
  - Rotational dependence: If samples show a preferred orientation, the rotational invariance may be violated.

- First 3 experiments:
  1. Train the model on a small protein (like 1unc) with default hyperparameters and visualize the internal coordinate distributions and atom fluctuations to check if they match the reference data.
  2. Vary the prior weight a and auxiliary loss weight waux to see their impact on atom fluctuations and sample quality, using the metrics provided in the paper.
  3. Compare the model's performance to the baselines (prior samples and standard estimator samples) on a larger protein (like 1pga) to verify that the induced covariance structure improves sample quality and maintains global structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for estimating the prior standard deviations of individual degrees of freedom from data?
- Basis in paper: [inferred] The paper mentions that the standard deviations could be estimated from data, either directly or using a preexisting model of protein local structure.
- Why unresolved: The paper does not provide a specific method for estimating these standard deviations, only suggesting that it could be done.
- What evidence would resolve it: A study comparing different methods for estimating the prior standard deviations and their impact on the model's performance would provide evidence for the optimal strategy.

### Open Question 2
- Question: How does the choice of prior weight (a) and auxiliary loss weight (waux) affect the model's performance and the validity of the first-order approximation?
- Basis in paper: [explicit] The paper discusses the impact of these weights on the model's performance and mentions that tuning these hyperparameters is important for good performance.
- Why unresolved: The paper does not provide a definitive answer on the optimal values for these weights or how they affect the model's performance.
- What evidence would resolve it: A systematic study varying the values of a and waux and measuring the model's performance and the validity of the first-order approximation would provide evidence for the optimal values.

### Open Question 3
- Question: How can the model be extended to handle larger conformational changes and more complex proteins?
- Basis in paper: [explicit] The paper mentions that the current model's produced fluctuations are generally too small to match the target densities and suggests constructing a hierarchical VAE to solve this issue.
- Why unresolved: The paper does not provide a specific implementation of a hierarchical VAE or demonstrate its effectiveness on larger conformational changes.
- What evidence would resolve it: A study implementing a hierarchical VAE and testing its performance on proteins with larger conformational changes would provide evidence for the effectiveness of this approach.

## Limitations

- The first-order approximation of atom displacements may break down for larger fluctuations, limiting the model's ability to capture larger conformational changes.
- The model requires careful tuning of hyperparameters (prior weight a and auxiliary loss weight waux) for optimal performance, which may not generalize across different protein datasets.
- The complexity of the model, including the U-Net for Lagrange multiplier prediction and the pNeRF conversion, makes it computationally expensive and potentially difficult to scale to very large proteins.

## Confidence

- Covariance induction via Lagrange multipliers: High
- Auxiliary loss mechanism: Medium
- Rotational invariance preservation: Medium
- Sample quality improvements: High

## Next Checks

1. Test the model's performance on proteins with different secondary structure compositions (β-sheets, loops) to assess generalizability beyond α-helical structures
2. Compare the induced covariance structure against reference covariance matrices computed from molecular dynamics trajectories using statistical tests
3. Perform ablation studies removing the auxiliary loss or using alternative constraint formulations to quantify their contributions to sample quality