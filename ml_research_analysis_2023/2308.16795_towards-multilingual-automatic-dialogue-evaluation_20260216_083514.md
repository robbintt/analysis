---
ver: rpa2
title: Towards Multilingual Automatic Dialogue Evaluation
arxiv_id: '2308.16795'
source_url: https://arxiv.org/abs/2308.16795
tags:
- dialogue
- data
- multilingual
- language
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores leveraging machine translation to train multilingual
  dialogue evaluation models using the large English DialyDialog corpus. Various approaches
  are tested including zero-shot inference with English models, language-specific
  finetuning, multilingual finetuning, and MAD-X.
---

# Towards Multilingual Automatic Dialogue Evaluation

## Quick Facts
- arXiv ID: 2308.16795
- Source URL: https://arxiv.org/abs/2308.16795
- Reference count: 26
- Primary result: Filtering translated dialogue data using MT Quality Estimation metrics achieves strong multilingual dialogue evaluation performance approaching ChatGPT.

## Executive Summary
This work explores leveraging machine translation to train multilingual dialogue evaluation models using the large English DialyDialog corpus. Various approaches are tested including zero-shot inference with English models, language-specific finetuning, multilingual finetuning, and MAD-X adapters. The best performance is achieved by carefully filtering translated data using MT Quality Estimation metrics and training with only the highest quality translations. This approach outperforms both zero-shot inference and finetuning with all translated data, achieving strong correlation with human judgements while approaching the performance of large language models like ChatGPT.

## Method Summary
The method involves translating English dialogue data (DailyDialog) into six target languages using MBART50, then filtering translations using MT Quality Estimation (COMET-QE-DA) to select high-quality examples. XLM-RoBERTa encoder models are finetuned using various strategies: zero-shot inference with English models, language-specific finetuning, multilingual finetuning, and MAD-X adapters. Models are evaluated on human-annotated test sets for Understandability and Sensibleness subqualities using Pearson and Spearman correlation metrics.

## Key Results
- Filtering low-quality translations using MT Quality Estimation scores significantly improves model performance over using all translated data
- Zero-shot inference with English models works best for Understandability (fluency-based), while target-language finetuning excels for Sensibleness (context-awareness)
- The approach achieves strong correlation with human judgements, approaching ChatGPT's performance on Spearman correlation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering low-quality translations using MT Quality Estimation (QE) scores improves multilingual dialogue evaluation model performance by removing noisy training examples that harm model learning.
- Mechanism: QE scoring ranks translations, allowing selective inclusion of high-quality examples during finetuning. This reduces noise in the training data that would otherwise disrupt the model's ability to learn quality patterns.
- Core assumption: Low-quality translations introduce noise that degrades model performance, and this noise can be effectively detected and filtered using QE metrics.
- Evidence anchors:
  - [abstract]: "Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance."
  - [section]: "The effects of noise introduced to the training data is a subject of intense research in the literature... It is expected that, for this task, noise is introduced by low quality translations, reducing the performance of trained models."

### Mechanism 2
- Claim: Zero-shot inference with English models performs well for Understandability because this subquality relies on fluency patterns that transfer across languages.
- Mechanism: Understandability focuses on whether a response is comprehensible without context, which depends on surface-level linguistic features like grammar and coherence that are preserved across translations.
- Core assumption: Fluency-related features are universal across languages and can be recognized by models trained on English data.
- Evidence anchors:
  - [section]: "For Understandability, we note that, on average, the best performing encoder approach is the zero-shot inference using the English model (EN)."

### Mechanism 3
- Claim: Target-language finetuning works best for Sensibleness because this subquality requires understanding context-response relationships in the target language.
- Mechanism: Sensibleness requires understanding whether a response is appropriate given the context, which depends on language-specific discourse patterns and cultural norms that are best captured by models trained on target-language data.
- Core assumption: Context-awareness patterns are language-dependent and require exposure to target-language dialogue data during training.
- Evidence anchors:
  - [section]: "The best performing encoder approach for this subquality is LANG. Intuitively this makes sense, given that during finetuning the model is exposed to target-language data for the language it is being evaluated on."

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper needs to adapt English dialogue evaluation models to work in multiple languages without requiring complete retraining from scratch.
  - Quick check question: What are the three main approaches to cross-lingual transfer mentioned in the paper (zero-shot inference, target-language finetuning, multilingual finetuning)?

- Concept: Machine Translation Quality Estimation
  - Why needed here: QE scores are used to filter low-quality translations that would harm model training, serving as a proxy for translation quality when reference translations aren't available.
  - Quick check question: Why can't we use traditional MT evaluation metrics (like BLEU) for filtering translations in this scenario?

- Concept: Self-supervised learning for dialogue evaluation
  - Why needed here: The models are trained using Next Sentence Prediction and Valid Sentence Prediction tasks that don't require human-labeled dialogue quality data.
  - Quick check question: What are the two self-supervised tasks used to train the dialogue evaluation models, and what dialogue subqualities do they target?

## Architecture Onboarding

- Component map:
  - XLM-RoBERTa encoder (multilingual pretrained model) -> Regression head for scoring -> MT system (MBART50) for translation -> MT Quality Estimation system (COMET-QE-DA) for filtering -> Data processing pipeline for creating context-response pairs -> Training loop with different data subsets

- Critical path:
  1. Preprocess English dialogue data into context-response pairs
  2. Translate pairs to target languages using MT
  3. Score translations using MT QE
  4. Filter translations based on QE scores
  5. Train/finetune models on selected data subsets
  6. Evaluate on human-annotated test set

- Design tradeoffs:
  - Filtering vs. including all data: More filtering reduces noise but may remove useful examples
  - Zero-shot vs. finetuning: Zero-shot preserves English model quality but may miss language-specific patterns
  - Translation quality vs. coverage: Higher quality MT may be slower or cover fewer languages

- Failure signatures:
  - Poor correlation with human judgments indicates model isn't capturing quality patterns
  - Large performance gaps between languages suggest filtering isn't working well for some languages
  - Inconsistent results across seeds indicate sensitivity to training data quality

- First 3 experiments:
  1. Compare zero-shot inference (EN model) vs. finetuning with all translated data (ML-100) to establish baseline performance gap
  2. Test different filtering thresholds (5%, 10%, 20%, 50%, 75%, 100%) to find optimal amount of filtered data
  3. Compare performance across different target languages to identify which benefit most from filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of translated data to use for training multilingual dialogue evaluation models for different languages and submetrics?
- Basis in paper: [explicit] The paper states that the optimal amount of translated data varies by language and subquality, with results showing 5% for Understandability and 75% for Sensibleness.
- Why unresolved: The paper tested only a limited set of languages (PT, DE, FR, ZH, ES, JA) and two specific submetrics. The optimal amounts may differ for other languages or submetrics not tested.
- What evidence would resolve it: Additional experiments testing a wider range of languages and submetrics, with varying amounts of translated data, to determine if the optimal percentages found in this study hold true more broadly.

### Open Question 2
- Question: How does the quality of machine translations impact the performance of multilingual dialogue evaluation models?
- Basis in paper: [explicit] The paper hypothesizes that low-quality translations can introduce noise into the training data, reducing model performance. It tests this by filtering translations using MT Quality Estimation metrics.
- Why unresolved: While the paper shows that filtering out low-quality translations improves performance, it doesn't fully quantify the impact of translation quality on model performance across different scenarios.
- What evidence would resolve it: Experiments comparing model performance using translations of varying quality levels, without any filtering, to determine the precise impact of translation quality on evaluation metrics.

### Open Question 3
- Question: Can multilingual dialogue evaluation models be effectively trained using languages with very limited dialogue data available?
- Basis in paper: [inferred] The paper focuses on six languages with presumably more available dialogue data. It doesn't address the challenge of training models for low-resource languages.
- Why unresolved: The paper's methodology relies on translating existing English dialogue data, which assumes a source language with ample data. This approach may not be viable for truly low-resource languages.
- What evidence would resolve it: Experiments attempting to train multilingual dialogue evaluation models using only minimal dialogue data from low-resource languages, potentially using alternative data augmentation or transfer learning techniques.

## Limitations
- The paper relies on MT Quality Estimation for data curation without validating whether QE scores directly correlate with dialogue evaluation quality
- The optimal filtering threshold varies by submetric and language, suggesting no universal solution exists
- Comparison with ChatGPT uses a much larger model (137B parameters) that isn't directly comparable in terms of computational efficiency

## Confidence
- High confidence: The core finding that zero-shot inference with English models works well for Understandability (fluency-based) while target-language finetuning is superior for Sensibleness (context-awareness)
- Medium confidence: The claim that MT Quality Estimation filtering is the "best approach" for multilingual dialogue evaluation
- Low confidence: The assertion that the resulting models "approach the performance of large language models like ChatGPT"

## Next Checks
1. **Validate QE-score correlation**: Conduct an ablation study to test whether MT Quality Estimation scores directly correlate with dialogue evaluation quality by comparing performance when filtering by QE scores versus random filtering at equivalent data volumes.

2. **Cross-lingual transfer analysis**: Design controlled experiments to isolate which aspects of dialogue quality (fluency vs. context-awareness) transfer across languages versus require target-language training, using languages with varying degrees of similarity to English.

3. **Computational efficiency comparison**: Benchmark the proposed approach against zero-shot inference and finetuning in terms of training time, inference latency, and memory requirements to provide a complete picture of practical deployment considerations beyond correlation metrics.