---
ver: rpa2
title: Online Continual Learning Without the Storage Constraint
arxiv_id: '2305.09253'
source_url: https://arxiv.org/abs/2305.09253
tags:
- learning
- continual
- online
- data
- storage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adaptive Continual Memory (ACM), a simple yet
  effective method for online continual learning that leverages a k-nearest neighbor
  classifier with pretrained feature extractors. ACM addresses the challenge of catastrophic
  forgetting by storing all incoming data compactly using low-dimensional feature
  representations.
---

# Online Continual Learning Without the Storage Constraint

## Quick Facts
- arXiv ID: 2305.09253
- Source URL: https://arxiv.org/abs/2305.09253
- Reference count: 21
- Key outcome: ACM achieves over 20% higher accuracy than existing methods on large-scale OCL datasets while requiring substantially less computational and storage resources.

## Executive Summary
This paper introduces Adaptive Continual Memory (ACM), a novel approach to online continual learning that addresses the challenge of catastrophic forgetting without the storage constraints typically associated with replay-based methods. ACM leverages pretrained feature extractors and a k-nearest neighbor classifier with approximate nearest neighbor search to achieve both perfect retention of previously seen data and rapid adaptation to distribution shifts. The method demonstrates significant performance improvements over existing OCL approaches on two large-scale datasets (CLOC with 39M images and 712 classes, and CGLM with 580K images and 10,788 classes) while maintaining logarithmic computational complexity through efficient approximate kNN search.

## Method Summary
ACM is a simple yet effective method for online continual learning that uses a k-nearest neighbor classifier with pretrained feature extractors. It stores all incoming data compactly using low-dimensional feature representations (256D) and employs the HNSW algorithm for approximate kNN search, achieving O(log n) computational complexity. Unlike existing methods that rely on expensive gradient updates, ACM enables rapid adaptation through one-sample learning and never forgets previously seen data due to the consistency property of kNN classifiers. The method significantly outperforms existing approaches on large-scale datasets while requiring substantially less computational and storage resources.

## Key Results
- ACM achieves over 20% higher accuracy than existing methods on CLOC (39M images, 712 classes) and CGLM (580K images, 10,788 classes) datasets
- Maintains logarithmic computational complexity (O(log n)) through HNSW approximate kNN search despite storing all data
- Achieves perfect retention of previously seen data through kNN classifier's consistency property

## Why This Works (Mechanism)

### Mechanism 1
ACM never forgets previously seen data because the kNN classifier stores features from all seen data points. When a previously seen data point is queried again, the kNN lookup returns the same label as before, ensuring perfect retention. This relies on the pretrained feature extractor producing consistent embeddings for the same input across time.

### Mechanism 2
ACM enables rapid adaptation to distribution shifts by immediately adding new data points to memory. If a point was misclassified previously, the next time it appears the updated kNN will classify it correctly, allowing one-sample learning. This assumes distribution shifts manifest as repeated misclassifications of the same data points over time.

### Mechanism 3
ACM achieves logarithmic computational complexity despite storing all data by using approximate kNN algorithms (HNSW). This scales memory retrieval and insertion operations as O(log n) rather than O(n), where n is the number of stored data points. This assumes the HNSW algorithm maintains its theoretical complexity guarantees in practice.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why traditional neural network approaches fail in continual learning settings is crucial to appreciate ACM's design choices
  - Quick check question: What happens to the weights of a neural network when it learns a new task, and why does this cause forgetting of previous tasks?

- Concept: Feature representations and transfer learning
  - Why needed here: ACM relies on pretrained feature extractors to produce embeddings. Understanding how these work is essential for grasping the method's effectiveness
  - Quick check question: Why might features pretrained on ImageNet1K work well on datasets like CLOC and CGLM, even though they're larger and more complex?

- Concept: k-Nearest Neighbors algorithm
  - Why needed here: ACM's core mechanism is a kNN classifier. Understanding how kNN works, including its consistency property and computational complexity, is fundamental
  - Quick check question: How does kNN classify a new data point, and why does this ensure that previously seen data points are never forgotten?

## Architecture Onboarding

- Component map: Pretrained feature extractor -> 2-layer MLP -> HNSW-based approximate kNN index -> Majority vote classifier
- Critical path: 1) Extract features from incoming image using pretrained model, 2) Retrieve nearest neighbor from HNSW index, 3) Classify using majority vote of retrieved neighbors, 4) Insert new feature-label pair into HNSW index
- Design tradeoffs:
  - Storage vs. computational cost: Storing all data enables perfect retention but requires efficient retrieval mechanisms
  - Embedding dimensionality: Lower dimensions reduce memory and computation but may lose information
  - Choice of k in kNN: k=1 gives perfect consistency but may be more sensitive to noise
- Failure signatures:
  - Degraded accuracy over time: May indicate feature extractor drift or curse of dimensionality
  - Increasing latency: HNSW index may need rebuilding or hyperparameter tuning (ef, m parameters)
  - Memory errors: Check embedding dimension and data type (float32 vs float16)
- First 3 experiments:
  1. Verify feature extractor consistency: Feed the same image multiple times and check if embeddings match
  2. Test HNSW insertion and retrieval: Insert known data points and verify they can be retrieved correctly
  3. Benchmark computational complexity: Measure insertion and retrieval times as a function of stored data size to confirm O(log n) scaling

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Adaptive Continual Memory (ACM) scale with different feature extraction architectures beyond XCIT and ResNet50? The paper compares ACM performance using XCIT DINO and ResNet50 (V2) and ResNet50 (I1B) models, noting significant differences in performance, but doesn't explore other architectures.

### Open Question 2
What is the impact of varying the embedding dimension size on the trade-off between computational efficiency and classification accuracy in ACM? The paper explores embedding sizes of 512, 256, and 128, noting that reducing the size to 256 offers a good balance, but doesn't test a wider range.

### Open Question 3
How does ACM perform in settings where privacy constraints require limiting data storage, as opposed to the unrestricted storage scenario studied? The paper acknowledges that privacy considerations could motivate memory restrictions but does not explore this scenario.

## Limitations
- The consistency property relies on pretrained feature extractors producing identical embeddings for the same input across time, which may not hold in all scenarios
- Practical HNSW performance may degrade for very large datasets, though theoretical O(log n) complexity is claimed
- Scalability to extremely large datasets beyond the tested 39M images remains unverified

## Confidence
- High Confidence: The claim that ACM never forgets previously seen data, given the kNN classifier's fundamental properties and the consistency of pretrained feature extractors
- Medium Confidence: The computational complexity claims (O(log n) for HNSW) and the effectiveness of rapid adaptation to distribution shifts
- Low Confidence: The scalability claims for extremely large datasets beyond those tested, and the robustness of the method to feature extractor drift over extended time periods

## Next Checks
1. Feature extractor consistency test: Feed identical images multiple times across different time steps and measure embedding drift to validate the consistency assumption
2. HNSW complexity benchmarking: Measure insertion and retrieval times as a function of stored data size across multiple orders of magnitude to empirically verify O(log n) complexity
3. Long-term adaptation study: Run ACM on a simulated data stream with gradual concept drift over extended periods (100K+ time steps) to evaluate how the method maintains performance when encountering previously seen data after long intervals