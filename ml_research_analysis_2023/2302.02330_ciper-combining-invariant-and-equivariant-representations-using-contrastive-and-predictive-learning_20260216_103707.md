---
ver: rpa2
title: 'CIPER: Combining Invariant and Equivariant Representations Using Contrastive
  and Predictive Learning'
arxiv_id: '2302.02330'
source_url: https://arxiv.org/abs/2302.02330
tags:
- learning
- augmentations
- contrastive
- representations
- equivariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CIPER, a method that combines contrastive
  invariant learning with predictive equivariant learning to leverage both types of
  features. The key idea is to use a shared encoder with two output heads: one for
  contrastive learning to learn invariant features, and another for predicting augmentation
  parameters to learn equivariant features.'
---

# CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning

## Quick Facts
- arXiv ID: 2302.02330
- Source URL: https://arxiv.org/abs/2302.02330
- Authors: 
- Reference count: 40
- Primary result: Combines contrastive invariant learning with predictive equivariant learning to improve performance on tasks requiring augmentation-related information

## Executive Summary
CIPER introduces a method that combines contrastive invariant learning with predictive equivariant learning to leverage both types of features. The key idea is to use a shared encoder with two output heads: one for contrastive learning to learn invariant features, and another for predicting augmentation parameters to learn equivariant features. This allows the model to capture both semantic meaning invariant to augmentations and augmentation-related information. CIPER outperforms standard contrastive methods on image classification tasks, especially when the downstream task requires encoding augmentation-related information. It also enables learning hierarchically structured representations.

## Method Summary
CIPER uses a shared ResNet18 encoder with two output heads: a projection head for contrastive learning and a prediction head for equivariant learning. The method applies random augmentations to input images, processes both views through the shared encoder, and then branches into the two heads. The projection head generates embeddings for contrastive loss using InfoNCE, while the prediction head processes the difference vector between original and augmented representations to predict augmentation parameters via MSE loss. The combined loss function balances these two objectives.

## Key Results
- CIPER outperforms standard contrastive methods on image classification tasks
- Significant performance gains when downstream tasks require augmentation-related information
- Effective on static image datasets (CIFAR10) and time-augmented datasets (CORe50, TDW)
- Enables learning hierarchically structured representations

## Why This Works (Mechanism)

### Mechanism 1
The shared encoder with two output heads enables simultaneous extraction of both invariant and equivariant features without mutual interference. The encoder processes input through a shared backbone, then branches into two heads: one projection head for contrastive learning (maximizing similarity of augmented views while minimizing with negatives) and one prediction head that estimates augmentation parameters. This allows the encoder to encode semantic meaning invariant to augmentations while also preserving transformation-related information in separate feature subspaces.

### Mechanism 2
The predictive head learns equivariant representations by minimizing reconstruction error of augmentation parameters from the difference vector between anchor and augmented representations. For each augmented view, the prediction head takes h_i - h_t1_i as input and predicts the augmentation parameters r_t1_i. The MSE loss between predicted and actual parameters encourages the encoder to encode transformation information in a way that can be linearly reconstructed.

### Mechanism 3
CIPER's combined objective outperforms pure contrastive learning when downstream tasks require encoding augmentation-related information. By preserving both semantic invariance and transformation equivariance, CIPER representations contain richer information that can be selectively utilized by downstream tasks. This is particularly beneficial for tasks like view parameter regression or session classification where augmentation information is directly relevant.

## Foundational Learning

- **Concept**: Contrastive learning and InfoNCE loss
  - Why needed here: CIPER builds directly on contrastive learning framework, using InfoNCE loss to encourage invariant features
  - Quick check question: What is the key difference between InfoNCE loss and simple cosine similarity loss in contrastive learning?

- **Concept**: Data augmentation and parameterization
  - Why needed here: CIPER requires understanding how augmentations are parameterized to predict their parameters effectively
  - Quick check question: How would you parameterize a rotation augmentation for the predictive head?

- **Concept**: Mutual information maximization in self-supervised learning
  - Why needed here: CIPER's design philosophy is based on maximizing relevant mutual information while minimizing irrelevant information
  - Quick check question: Why is maximizing I(h; g) (representation-task mutual information) the ultimate goal in self-supervised learning?

## Architecture Onboarding

- **Component map**: Input images -> Random augmentations -> Shared encoder -> Projection head + Prediction head -> InfoNCE loss + MSE loss
- **Critical path**: 1. Input images are randomly augmented twice 2. Shared encoder processes both views 3. Projection head generates embeddings for contrastive loss 4. Prediction head processes difference vector for parameter prediction 5. Combined loss drives training
- **Design tradeoffs**: Shared vs. separate encoders (shared saves parameters but may create interference), fixed vs. learnable augmentation parameters (fixed simplifies prediction but may limit flexibility), MSE vs. other prediction losses (MSE is simple but may not capture complex relationships)
- **Failure signatures**: Prediction loss not converging (augmentation parameters not encoded), contrastive loss dominates completely (equivariant features lost), both losses converge but downstream performance poor (architecture not learning useful features)
- **First 3 experiments**: 1. Ablation: Train with only contrastive head vs. only predictive head vs. both heads to verify complementary benefits 2. Hyperparameter sweep: Vary α (weight of predictive loss) to find optimal balance for different downstream tasks 3. Augmentation analysis: Remove each augmentation type to identify which contribute most to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the weighting hyperparameter α impact the trade-off between invariant and equivariant features in CIPER?
- Basis in paper: The paper states that increasing α can lead to higher object classification accuracy on both TDW and CORe50, as well as higher session classification accuracy and lower view regression error. However, the classification accuracy may drop if α is too high.
- Why unresolved: The paper does not provide a clear explanation for why increasing α can lead to better performance up to a certain point, after which performance drops. It is unclear how the weighting hyperparameter α affects the balance between invariant and equivariant features in the learned representations.
- What evidence would resolve it: Experiments that systematically vary α and measure the impact on downstream tasks would provide evidence for how α affects the trade-off between invariant and equivariant features. Additionally, visualizations of the learned representations for different values of α could provide insight into how the representations change as α is varied.

### Open Question 2
- Question: How does the order of applying augmentations impact the performance of CIPER?
- Basis in paper: The paper states that augmentations are performed in a fixed order, but does not explore how changing the order of augmentations impacts the performance of CIPER. This suggests that the order of applying augmentations could be an important factor in the performance of CIPER.
- Why unresolved: The paper does not provide any experiments or analysis of how the order of applying augmentations impacts the performance of CIPER. It is unclear whether the order of applying augmentations has a significant impact on the learned representations and downstream task performance.
- What evidence would resolve it: Experiments that vary the order of applying augmentations and measure the impact on downstream task performance would provide evidence for how the order of augmentations affects the performance of CIPER. Additionally, visualizations of the learned representations for different orders of augmentations could provide insight into how the representations change based on the order of augmentations.

### Open Question 3
- Question: How does the dimensionality of the latent space impact the performance of CIPER?
- Basis in paper: The paper does not explore how the dimensionality of the latent space impacts the performance of CIPER. This suggests that the dimensionality of the latent space could be an important factor in the performance of CIPER.
- Why unresolved: The paper does not provide any experiments or analysis of how the dimensionality of the latent space impacts the performance of CIPER. It is unclear whether the dimensionality of the latent space has a significant impact on the learned representations and downstream task performance.
- What evidence would resolve it: Experiments that vary the dimensionality of the latent space and measure the impact on downstream task performance would provide evidence for how the dimensionality of the latent space affects the performance of CIPER. Additionally, visualizations of the learned representations for different dimensionalities of the latent space could provide insight into how the representations change based on the dimensionality of the latent space.

## Limitations
- Lacks detailed ablations showing individual contributions of invariant vs. equivariant learning components
- Fixed hyperparameter α without justification or sensitivity analysis
- Focus primarily on computer vision tasks limits generalizability to other domains

## Confidence

**High**: Basic implementation of contrastive learning with InfoNCE loss and predictive learning with MSE loss, as these are well-established techniques.

**Medium**: The claim that CIPER outperforms standard contrastive methods, particularly on tasks requiring augmentation-related information. While results are presented, the lack of ablations and hyperparameter tuning reduces confidence.

**Medium**: The mechanism by which shared encoder learns both feature types without interference. The paper describes the architecture but doesn't provide empirical evidence of feature separation or mutual information analysis.

## Next Checks

1. Conduct ablation studies comparing: (a) CIPER with both heads, (b) CIPER with only contrastive head, (c) CIPER with only predictive head, and (d) standard contrastive learning to quantify individual contributions.

2. Perform systematic hyperparameter tuning for α across different downstream tasks to identify optimal weighting and verify robustness to this critical parameter.

3. Analyze feature representations using mutual information estimates or linear probing to verify that the encoder successfully separates invariant and equivariant information in distinct feature subspaces.