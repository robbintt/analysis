---
ver: rpa2
title: 'Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models'
arxiv_id: '2310.07301'
source_url: https://arxiv.org/abs/2310.07301
tags:
- multi-turn
- user
- chat
- assistant
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of improving large language models\u2019\
  \ (LLMs) multi-turn instruction-following capabilities, which are crucial for effective\
  \ human-LM interactions. The authors observe that existing instruction-tuning datasets\
  \ for LLMs are typically single-turn or have quality issues, leading to poor performance\
  \ in multi-turn conversations."
---

# Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models

## Quick Facts
- arXiv ID: 2310.07301
- Source URL: https://arxiv.org/abs/2310.07301
- Authors: 
- Reference count: 40
- Key outcome: A novel approach to enhance multi-turn instruction-following capabilities of LLMs by training a model to generate human-like questions, collecting high-quality multi-turn dialogues, and fine-tuning a chat model on this data, resulting in improved performance on multi-turn benchmarks.

## Executive Summary
This paper addresses the challenge of improving large language models' (LLMs) multi-turn instruction-following capabilities, which are crucial for effective human-LM interactions. The authors propose Parrot, a novel approach that involves training a specialized model (Parrot-Ask) to generate human-like multi-turn questions, using it to engage in multi-turn conversations with ChatGPT, and fine-tuning an LLM on the resulting high-quality dataset (Parrot-40K) to create a chat model (Parrot-Chat). The authors demonstrate that Parrot-40K outperforms existing multi-turn instruction-tuning datasets in terms of topic diversity, number of turns, and resemblance to human conversation, and that Parrot-Chat achieves superior performance among 13B open-source models on multi-turn benchmarks, with up to 7.2% improvement in multi-turn instruction-following.

## Method Summary
The Parrot approach involves three main steps: 1) Train Parrot-Ask on a subset of ShareGPT logs to generate human-like questions given conversation context; 2) Use Parrot-Ask to generate follow-up questions for existing ShareGPT dialogues and UltraChat initial questions, collect responses from ChatGPT to form Parrot-40K dataset (40K sessions); 3) Fine-tune LLaMA2-13B on Parrot-40K dataset to create Parrot-Chat, a chat model that excels in multi-turn instruction-following tasks.

## Key Results
- Parrot-40K dataset demonstrates highest average number of turns, longest average sequence length, and greatest number of topic shifts and transitions compared to existing datasets
- Parrot-Chat achieves superior performance among 13B open-source models across a range of instruction-following benchmarks, particularly excelling in multi-turn capabilities
- With only 40K training examples, Parrot-Chat shows up to 7.2% improvement in multi-turn instruction-following compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training Parrot-Ask to predict question tokens conditioned on answer and context enables more human-like question generation
- Core assumption: Real user question-asking behavior is more varied, specific, and context-dependent than synthetic generation
- Evidence anchors: Abstract and section descriptions of Parrot-Ask training objective and its ability to generate human-like questions
- Break condition: If Parrot-Ask overfits to training data and cannot generalize to diverse topics

### Mechanism 2
- Claim: Using Parrot-Ask to interact with ChatGPT produces longer, more human-like dialogues than self-chat methods
- Core assumption: ChatGPT responds better to human-like questions with more detailed and varied responses
- Evidence anchors: Abstract description of Parrot-40K collection and section analysis of dialogue characteristics
- Break condition: If ChatGPT ignores question quality and defaults to generic responses

### Mechanism 3
- Claim: Fine-tuning on high-quality, human-like multi-turn data improves multi-turn instruction-following performance
- Core assumption: Training data quality and structure directly influence model's ability to handle context dependencies and topic shifts
- Evidence anchors: Abstract and section claims about Parrot-Chat performance on benchmarks
- Break condition: If performance improvement is not significant compared to training on more lower-quality data

## Foundational Learning

- Autoregressive language modeling: Both Parrot-Ask and Parrot-Chat use token-level autoregressive objectives
  - Why needed: Core training methodology for both models
  - Quick check: What's the difference between predicting question vs answer tokens in loss formulation?

- Fine-tuning LLMs on custom datasets: Central to creating specialized chat models
  - Why needed: Transforms base LLaMA2-13B into instruction-following models
  - Quick check: How does training objective change for instruction following vs general language modeling?

- Multi-turn dialogue structure and context management: Essential for generating and maintaining conversations
  - Why needed: Method relies on maintaining conversation history for relevant questions and responses
  - Quick check: How does model handle anaphora and ellipsis in multi-turn conversations?

## Architecture Onboarding

- Component map: Parrot-Ask -> ChatGPT -> Parrot-40K -> Parrot-Chat -> MT-Bench++
- Critical path: 1) Train Parrot-Ask on ShareGPT logs 2) Generate questions and collect ChatGPT responses 3) Fine-tune LLaMA2-13B on Parrot-40K 4) Evaluate on benchmarks
- Design tradeoffs: Data quality vs quantity (40K high-quality vs millions lower-quality), human-likeness vs control, external API dependency
- Failure signatures: Repetitive/irrelevant questions from Parrot-Ask, low topic diversity in Parrot-40K, performance degradation after 6 turns, poor benchmark scores
- First 3 experiments: 1) Train Parrot-Ask on ShareGPT subset and evaluate question quality 2) Generate small dialogue set and analyze for diversity and length 3) Fine-tune base model on small Parrot-40K subset and evaluate on MT-Bench++

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Parrot-Ask handle generation of the first question in a conversation?
- Basis in paper: Explicit - authors leave first question generation as future work
- Why unresolved: Paper relies on real users for initial questions but wants autonomous generation
- What evidence would resolve it: Developing and evaluating autonomous initial question generation method

### Open Question 2
- Question: Can Parrot-Ask be made more controllable for generating instructions with specific restrictions?
- Basis in paper: Inferred - authors note current lack of controllability
- Why unresolved: Current model cannot generate instructions with explicit constraints
- What evidence would resolve it: Creating framework for controlled instruction generation

### Open Question 3
- Question: How does Parrot-Chat performance change on dialogues with more than eight turns?
- Basis in paper: Inferred - MT-Bench++ only goes to eight turns
- Why unresolved: Paper doesn't explore performance on longer dialogues
- What evidence would resolve it: Evaluating on extended dialogues beyond eight turns

## Limitations

- Reliance on ChatGPT for response generation introduces potential biases not fully explored
- Effectiveness of Parrot-Ask in generating truly human-like questions lacks direct validation
- Scalability and generalizability to different dataset sizes and base models not fully addressed

## Confidence

- High confidence: Novel approach of training question generator, collecting dialogues, and fine-tuning chat model
- Medium confidence: Parrot-40K quality superiority over existing datasets
- Low confidence: Improvement primarily due to dataset quality vs other factors

## Next Checks

1. Conduct human evaluation comparing Parrot-Ask question quality to human-generated questions
2. Perform ablation study training models on datasets of varying quality to isolate dataset impact
3. Evaluate approach scalability by training on different dataset sizes and base models