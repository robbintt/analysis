---
ver: rpa2
title: Auto deep learning for bioacoustic signals
arxiv_id: '2311.04945'
source_url: https://arxiv.org/abs/2311.04945
tags:
- acrocephalus
- ciconia
- himantopus
- dataset
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study aimed to improve bird vocalization classification using
  automated deep learning compared to traditional manual models. AutoKeras was used
  to automatically search neural network architectures and hyperparameters, then compared
  to pretrained models like MobileNet, ResNet50 and VGG16 on a 20-class wetland bird
  dataset.
---

# Auto deep learning for bioacoustic signals

## Quick Facts
- arXiv ID: 2311.04945
- Source URL: https://arxiv.org/abs/2311.04945
- Reference count: 0
- The study demonstrates that AutoKeras outperforms traditional deep learning models on a 20-class wetland bird vocalization classification task.

## Executive Summary
This study investigates the application of automated deep learning to bioacoustic signal classification, specifically for bird vocalization identification. The research compares AutoKeras, an automated machine learning framework, against established deep learning models (MobileNetV2, ResNet50, VGG16) on a dataset of 20 wetland bird species. AutoKeras automatically searches for optimal neural architectures and hyperparameters, eliminating manual feature engineering while improving classification performance.

The results show that the AutoKeras-derived Xception model achieves superior F1 scores compared to traditional models on both validation and test datasets, demonstrating the potential of automated approaches to advance bioacoustics research. The study also emphasizes the critical importance of proper stratified sampling when working with multi-class imbalanced datasets and highlights the necessity of separate test sets for evaluating model generalization.

## Method Summary
The study uses the Western Mediterranean Wetland Birds dataset containing 201.6 minutes of audio from 5,795 excerpts across 20 species. Audio files are converted to 1-second spectrograms using mel-scale, then normalized with MinMaxScaler. The dataset is split into training (70%), validation (20%), and test (10%) sets using stratified sampling that accounts for session lengths. AutoKeras searches 100 models for 500 epochs each to find optimal architectures, with the top-performing Xception model retrained 20 times and compared against baseline MobileNetV2, ResNet50, and VGG16 models.

## Key Results
- AutoKeras-derived Xception model achieved higher F1 scores than MobileNetV2, ResNet50, and VGG16 on both validation and test datasets
- The automated approach eliminated manual feature engineering while improving classification performance
- Stratified sampling was critical for handling multi-class imbalanced datasets and ensuring proper class representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoKeras automates neural architecture search and hyperparameter tuning, eliminating manual feature engineering and improving model performance.
- Mechanism: AutoKeras uses Bayesian optimization with a neural network kernel and tree-structured acquisition function to efficiently explore the search space of deep learning models, finding optimal architectures and hyperparameters for the specific bird vocalization classification task.
- Core assumption: The search space explored by AutoKeras contains architectures that significantly outperform manually designed models like MobileNet, ResNet50, and VGG16 for this specific task.
- Evidence anchors:
  - [abstract] "Using the Western Mediterranean Wetland Birds dataset, we investigated the use of AutoKeras, an automated machine learning framework, to automate neural architecture search and hyperparameter tuning."
  - [section] "AutoKeras automatically searches over deep learning models, tuning the architecture and hyperparameters using network morphism (Wei, 2016)."
  - [corpus] No direct evidence in corpus, but related work on automated bioacoustics workflows (Brown et al., 2021) suggests automated approaches can outperform manual ones.

### Mechanism 2
- Claim: Stratified sampling is crucial for handling multi-class imbalanced datasets and ensuring proper representation of all classes in training, validation, and test sets.
- Mechanism: Stratified sampling ensures each class is represented proportionally in each data split, preventing class imbalance from biasing the model during training and evaluation.
- Core assumption: The dataset has significant class imbalance, and proper stratification can mitigate the negative effects of this imbalance on model performance.
- Evidence anchors:
  - [abstract] "Our methodology incorporates stratified sampling and data normalization for multi-class learning."
  - [section] "To address the multi-class nature of this classification problem, we employed stratified sampling in dividing the dataset into training, validation, and test sets."
  - [corpus] No direct evidence in corpus, but general machine learning literature emphasizes the importance of stratified sampling for imbalanced datasets.

### Mechanism 3
- Claim: Proper evaluation using a separate test set is essential for assessing model generalization and avoiding overfitting to the validation set.
- Mechanism: By using a separate test set, the model's performance can be evaluated on unseen data, providing a more accurate estimate of its generalization ability.
- Core assumption: The model's performance on the validation set is not a reliable indicator of its performance on new, unseen data.
- Evidence anchors:
  - [abstract] "Additionally, we underscore the critical need for a separate test set to properly evaluate model generalization, given AutoML's reliance on the validation set for model selection."
  - [section] "The F1 scores on the separate test set validates that the AutoKeras model has better generalization beyond just the training and validation data."
  - [corpus] No direct evidence in corpus, but general machine learning principles emphasize the importance of separate test sets for unbiased evaluation.

## Foundational Learning
- Concept: Deep learning and neural networks
  - Why needed here: The study involves comparing the performance of different deep learning models (MobileNet, ResNet50, VGG16, and AutoKeras-derived models) on a bird vocalization classification task.
  - Quick check question: Can you explain the basic structure and functioning of a convolutional neural network (CNN)?
- Concept: Automated machine learning (AutoML)
  - Why needed here: The study investigates the use of AutoKeras, an AutoML framework, to automate neural architecture search and hyperparameter tuning.
  - Quick check question: What is the main goal of AutoML, and how does it differ from traditional machine learning approaches?
- Concept: Multi-class classification
  - Why needed here: The study involves classifying bird vocalizations into 20 different classes, requiring an understanding of multi-class classification techniques.
  - Quick check question: How does multi-class classification differ from binary classification, and what are some common approaches for handling multi-class problems?

## Architecture Onboarding
- Component map: Data preprocessing -> Model training -> Evaluation
- Critical path: 1. Preprocess data using stratified sampling and normalization 2. Train baseline models on training set 3. Use AutoKeras to search optimal architectures 4. Train best AutoKeras model on training set 5. Evaluate all models on validation and test sets
- Design tradeoffs:
  - AutoKeras offers automated architecture search but requires more computational resources than manually designed models
  - Stratified sampling ensures proper class representation but may introduce bias if not done correctly
  - Using a separate test set provides an unbiased evaluation but requires a larger dataset
- Failure signatures:
  - Poor performance on test set compared to validation set indicates overfitting
  - High variance in F1 scores across runs suggests instability in training process
  - Confusion matrix shows systematic misclassifications for certain classes, indicating potential issues with data or model architecture
- First 3 experiments:
  1. Train baseline models (MobileNet, ResNet50, VGG16) on training set and evaluate performance on validation set to establish baseline
  2. Use AutoKeras to search for optimal architectures and hyperparameters, then train best model on training set and evaluate on validation set
  3. Compare performance of AutoKeras-derived model and baseline models on test set to assess generalization ability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of the AutoKeras-derived Xception model compare to manual feature engineering approaches on the Western Mediterranean Wetland Birds dataset?
- Basis in paper: [explicit] The study aimed to demonstrate that automated deep learning techniques can eliminate the need for manual feature engineering while improving performance compared to traditional models.
- Why unresolved: The study did not include any manual feature engineering approaches for comparison, only comparing the AutoKeras model to pretrained deep learning models.
- What evidence would resolve it: Implementing and evaluating manual feature engineering techniques (e.g. hand-crafted spectrotemporal features) on the same dataset and comparing their performance to the AutoKeras model.

### Open Question 2
- Question: How robust is the AutoKeras model to variations in the input audio preprocessing pipeline, such as window size, mel-scale parameters, and noise reduction techniques?
- Basis in paper: [inferred] The study used a specific preprocessing pipeline (1-second windows, mel-scale, etc.) but did not explore the impact of different preprocessing choices on model performance.
- Why unresolved: The impact of different preprocessing choices on model performance was not investigated, so the robustness of the AutoKeras model to these variations is unknown.
- What evidence would resolve it: Evaluating the AutoKeras model with different preprocessing pipelines and comparing their performance to understand the model's robustness to preprocessing variations.

### Open Question 3
- Question: Can the AutoKeras framework be effectively applied to other bioacoustic datasets with different characteristics, such as larger numbers of classes or longer audio samples?
- Basis in paper: [explicit] The study focused on a specific dataset with 20 classes and 1-second audio samples, but the authors suggested the potential for automated deep learning to advance bioacoustics research more broadly.
- Why unresolved: The study only evaluated the AutoKeras framework on one specific dataset, so its effectiveness on other bioacoustic datasets with different characteristics is unknown.
- What evidence would resolve it: Applying the AutoKeras framework to other bioacoustic datasets with varying characteristics (e.g. different numbers of classes, audio sample lengths) and comparing the results to manual deep learning approaches.

## Limitations
- The study used a single dataset with specific characteristics, limiting generalizability to other bioacoustic classification tasks
- The computational cost of AutoKeras may be prohibitive for many research groups
- Results may not extend to different taxonomic groups or recording conditions

## Confidence
- Confidence level: Medium
- Performance improvements demonstrated but with limitations in generalizability
- Single dataset focus raises questions about broader applicability
- Computational expense may limit practical adoption

## Next Checks
1. **Dataset Diversity Test**: Replicate the experiment using multiple bioacoustic datasets with varying taxonomic groups, signal-to-noise ratios, and recording conditions to assess generalizability of the AutoKeras approach.

2. **Computational Efficiency Analysis**: Compare wall-clock time and computational resources required for AutoKeras versus manual fine-tuning of established architectures, including sensitivity analysis of search parameters.

3. **Transfer Learning Comparison**: Evaluate whether pre-training AutoKeras-derived models on large-scale audio datasets (e.g., AudioSet) improves performance on the wetland bird dataset compared to training from scratch.