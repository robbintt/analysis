---
ver: rpa2
title: 'Scaling MLPs: A Tale of Inductive Bias'
arxiv_id: '2306.13575'
source_url: https://arxiv.org/abs/2306.13575
tags:
- mlps
- learning
- bias
- more
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how far the performance of multi-layer perceptrons
  (MLPs) can be pushed on vision tasks, addressing the question of whether MLPs are
  a valid proxy for more complex architectures in theoretical studies. The authors
  explore various MLP configurations, including standard and inverted bottleneck designs,
  and evaluate their performance on standard vision benchmarks like CIFAR10, CIFAR100,
  and ImageNet, both from scratch and with transfer learning from ImageNet21k.
---

# Scaling MLPs: A Tale of Inductive Bias

## Quick Facts
- arXiv ID: 2306.13575
- Source URL: https://arxiv.org/abs/2306.13575
- Reference count: 31
- Multi-layer perceptrons (MLPs) achieve strong performance on vision tasks when scaled up, challenging their role as theoretical proxies for more complex architectures.

## Executive Summary
This work investigates the performance limits of multi-layer perceptrons (MLPs) on vision tasks by systematically scaling model size and dataset dimensions. The authors demonstrate that MLPs without inherent spatial inductive bias can achieve competitive performance (93% on CIFAR10, 79% on CIFAR100, 69% on TinyImageNet) when appropriately scaled, with a particular emphasis on increasing dataset size rather than model size. Surprisingly, they discover that larger batch sizes improve generalization for MLPs, contrary to the behavior observed in convolutional networks. The study highlights the critical role of data augmentation for MLPs and raises questions about their validity as proxies for theoretical analysis of modern architectures.

## Method Summary
The authors employ inverted bottleneck MLP architectures with depth 6, width 1024, and expansion factor 4, trained using the LION optimizer with learning rate 5e-5 and label smoothing 0.3. All images are downscaled to 64x64 resolution, with data augmentation including random flips, crops, and MixUp. Experiments span CIFAR10, CIFAR100, STL10, TinyImageNet, and ImageNet1k for evaluation, with ImageNet21k used for pre-training. Transfer learning is performed by fine-tuning pre-trained models on downstream tasks. Scaling behavior is analyzed by varying parameter count and dataset size while measuring performance and compute efficiency in FLOPs.

## Key Results
- MLPs achieve 93% accuracy on CIFAR10, 79% on CIFAR100, and 69% on TinyImageNet when scaled appropriately
- Larger batch sizes surprisingly improve generalization for MLPs, contrary to CNN behavior
- Data augmentation plays a critical role in MLP performance, even at large dataset sizes
- MLPs require significantly more compute investment in dataset size compared to model size for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling up MLPs compensates for their lack of inductive bias in vision tasks.
- Mechanism: Increasing model size and training data size improves generalization performance, with a power-law relationship between compute (FLOPs) and test error.
- Core assumption: The benefits of scale for MLPs mirror those observed in CNNs and transformers.
- Evidence anchors:
  - [abstract] "We show that the performance of MLPs drastically improves with scale (95% on CIFAR10, 82% on CIFAR100, 58% on ImageNet ReaL), highlighting that lack of inductive bias can indeed be compensated."
  - [section] "We observe that MLPs mimic the behaviour of their modern counterparts faithfully, with some components in the learning setting however exhibiting stronger or unexpected behaviours."
  - [corpus] Weak; no direct neighbor evidence supporting the scaling claim.
- Break condition: If scaling laws do not follow a power-law relationship or if performance plateaus despite increasing compute.

### Mechanism 2
- Claim: Data augmentation plays a crucial role in improving MLP performance, even at large dataset sizes.
- Mechanism: Augmentation provides indirect inductive bias by introducing invariances, which is especially important for MLPs due to their lack of spatial structure.
- Core assumption: The effectiveness of data augmentation is amplified for MLPs compared to CNNs.
- Evidence anchors:
  - [abstract] "We further investigate how the implicit bias of SGD affects performance, and we make a very counter-intuitive discovery: contrary to CNNs, we find that larger batch sizes generalize significantly better for MLPs."
  - [section] "While regularization in the form of data augmentation is also helpful for CNNs, its role is significantly amplified for MLPs even at large sample sizes, leading to fatal degradation if turned off."
  - [corpus] Weak; no direct neighbor evidence supporting the data augmentation claim.
- Break condition: If data augmentation no longer improves performance or becomes unnecessary at very large scales.

### Mechanism 3
- Claim: The implicit bias of SGD differs for MLPs compared to CNNs, with larger batch sizes generalizing better for MLPs.
- Mechanism: The optimization dynamics of SGD lead to different solutions for MLPs and CNNs, potentially due to their structural differences.
- Core assumption: The implicit bias of SGD is architecture-dependent.
- Evidence anchors:
  - [abstract] "We further investigate how the implicit bias of SGD affects performance, and we make a very counter-intuitive discovery: contrary to CNNs, we find that larger batch sizes generalize significantly better for MLPs."
  - [section] "This result questions the validity of the proxy role that the MLP plays in theoretical works investigating the implicit bias of SGD."
  - [corpus] Weak; no direct neighbor evidence supporting the SGD implicit bias claim.
- Break condition: If larger batch sizes do not generalize better for MLPs or if the implicit bias of SGD becomes similar to that of CNNs.

## Foundational Learning

- Concept: Multi-layer perceptrons (MLPs) and their mathematical formulation.
  - Why needed here: Understanding the basic structure of MLPs is essential for grasping the paper's contributions.
  - Quick check question: What is the mathematical representation of an MLP, and how does it differ from a convolutional neural network?
- Concept: Inductive bias and its role in machine learning models.
  - Why needed here: The paper investigates how MLPs, which lack inductive bias, can achieve good performance through scaling.
  - Quick check question: What is inductive bias, and why is it important for machine learning models, especially in vision tasks?
- Concept: Scaling laws and their relationship to model performance.
  - Why needed here: The paper explores how scaling up MLPs affects their performance and compares it to other architectures.
  - Quick check question: What are scaling laws, and how do they relate to the performance of machine learning models?

## Architecture Onboarding

- Component map: Input -> Linear Layer -> Activation (GELU) -> Linear Layer -> ... (repeated for depth) -> Output
- Critical path: 1) Data preprocessing (centering, normalizing, augmenting), 2) Model training with LION optimizer, 3) Transfer learning on downstream tasks
- Design tradeoffs: MLPs are computationally efficient but lack spatial inductive bias. Inverted bottleneck MLPs improve performance but add some inductive bias.
- Failure signatures: Poor performance due to overfitting (mitigated by data augmentation), slow learning (mitigated by using inverted bottleneck structure)
- First 3 experiments:
  1. Train a standard MLP on CIFAR10 without data augmentation to observe overfitting.
  2. Train an inverted bottleneck MLP on CIFAR100 with data augmentation to see the impact of regularization.
  3. Perform transfer learning from ImageNet21k to CIFAR10 using an MLP to assess feature transferability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MLPs compare to other architectures (like CNNs and Transformers) when trained on extremely large datasets, beyond the scale used in this study?
- Basis in paper: [inferred] The paper discusses scaling MLPs and compares their performance to other architectures, but the authors acknowledge that the scale used in their experiments is smaller than that employed in studies with Transformers and CNNs.
- Why unresolved: The paper does not provide experimental results for MLPs trained on datasets larger than ImageNet21k, which is still smaller than the datasets used in studies with Transformers and CNNs.
- What evidence would resolve it: Experiments comparing the performance of MLPs, CNNs, and Transformers on extremely large datasets, beyond the scale used in this study, would provide a more definitive answer to this question.

### Open Question 2
- Question: What is the optimal way to allocate compute between model size and dataset size for MLPs to achieve the best performance?
- Basis in paper: [explicit] The paper investigates the scaling laws of MLPs and finds that the optimal strategy for MLPs invests significantly more compute into dataset size compared to model size, unlike Transformers.
- Why unresolved: The paper provides empirical evidence for the optimal allocation of compute for MLPs but does not provide a theoretical explanation for this behavior.
- What evidence would resolve it: A theoretical analysis explaining why MLPs require more compute in the form of dataset size compared to model size, and how this differs from other architectures, would resolve this question.

### Open Question 3
- Question: How does the implicit bias of SGD affect the performance of MLPs, and how does this differ from other architectures like CNNs and Transformers?
- Basis in paper: [explicit] The paper observes that MLPs exhibit different behavior regarding the implicit bias of SGD compared to CNNs, with larger batch sizes leading to better generalization for MLPs.
- Why unresolved: The paper provides empirical evidence for the different behavior of MLPs regarding the implicit bias of SGD but does not provide a theoretical explanation for this phenomenon.
- What evidence would resolve it: A theoretical analysis explaining why the implicit bias of SGD affects MLPs differently than other architectures, and how this impacts their performance, would resolve this question.

### Open Question 4
- Question: How do data augmentations contribute to the performance of MLPs, and what specific augmentations are most effective?
- Basis in paper: [explicit] The paper highlights the importance of data augmentations for MLPs, noting that they provide indirect inductive bias and significantly improve performance.
- Why unresolved: The paper demonstrates the effectiveness of data augmentations for MLPs but does not provide a detailed analysis of which specific augmentations are most effective or why they are particularly beneficial for MLPs.
- What evidence would resolve it: A systematic study comparing the effectiveness of different data augmentations for MLPs, along with an analysis of why they are particularly beneficial for this architecture, would resolve this question.

## Limitations

- The study uses 64x64 resolution images, which may not capture the full complexity of natural images
- The focus on a single backbone architecture (inverted bottleneck MLPs) may not generalize to all MLP variants
- The experiments rely heavily on ImageNet21k pre-training, which may not be available to all researchers

## Confidence

- **High Confidence**: The empirical scaling results showing improved MLP performance with increased model and data size are well-supported by extensive experiments
- **Medium Confidence**: The observation about batch size generalization benefits for MLPs is based on controlled experiments but contradicts conventional wisdom and needs theoretical justification
- **Medium Confidence**: The claim that MLPs serve as valid proxies for theoretical analysis of modern architectures is partially supported but limited by observed differences in implicit bias behavior

## Next Checks

1. Test the inverted bottleneck MLP architecture at full resolution (224x224) on ImageNet to verify if scaling benefits persist at higher image resolutions
2. Conduct ablation studies varying initialization schemes and activation function placements to determine their impact on the observed scaling behavior and generalization patterns
3. Compare optimization trajectories and implicit bias outcomes between MLPs and CNNs trained with identical hyperparameters to better understand the fundamental differences in their learning dynamics