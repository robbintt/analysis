---
ver: rpa2
title: Multi-fidelity climate model parameterization for better generalization and
  extrapolation
arxiv_id: '2309.10231'
source_url: https://arxiv.org/abs/2309.10231
tags:
- data
- levels
- tendency
- vertical
- mf-rpn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving machine-learning-based
  parameterizations for climate models to enhance their generalization and extrapolation
  capabilities, which is crucial for accurate climate change projections. The authors
  propose a multi-fidelity approach using Randomized Prior Networks (MF-RPNs) that
  integrates low-fidelity (CAM5) and high-fidelity (SPCAM5) datasets to leverage both
  the abundance of low-fidelity data and the accuracy of high-fidelity data.
---

# Multi-fidelity climate model parameterization for better generalization and extrapolation

## Quick Facts
- arXiv ID: 2309.10231
- Source URL: https://arxiv.org/abs/2309.10231
- Reference count: 40
- The paper demonstrates that multi-fidelity Randomized Prior Networks significantly improve climate model parameterization generalization and extrapolation capabilities.

## Executive Summary
This paper addresses the critical challenge of improving machine-learning-based parameterizations for climate models to enhance their generalization and extrapolation capabilities, particularly for accurate climate change projections. The authors propose a multi-fidelity approach using Randomized Prior Networks (MF-RPNs) that integrates low-fidelity (CAM5) and high-fidelity (SPCAM5) datasets to leverage both the abundance of low-fidelity data and the accuracy of high-fidelity data. The MF-RPNs show significantly improved performance in predicting heat and moisture tendencies across various vertical levels, especially in tropical regions and under climate change scenarios (+4K). The approach also provides reliable uncertainty quantification, with uncertainties increasing appropriately with prediction errors.

## Method Summary
The method employs Multi-fidelity Randomized Prior Networks (MF-RPNs) that combine low-fidelity CAM5 +8K data with high-fidelity SPCAM5 historical data. The architecture consists of two neural networks: a low-fidelity network that maps inputs to low-fidelity outputs, and a high-fidelity network that maps these outputs to high-fidelity predictions. The model uses 128 RPNs with data bootstrapping, trained jointly on normalized data using Adam optimizer. The approach leverages low-fidelity data to learn general patterns while using high-fidelity data to refine these patterns with more accurate physics, enabling better extrapolation to unseen climate regimes.

## Key Results
- MF-RPNs achieved higher R² values (up to 0.73 for moisture tendency) and lower MAE compared to single-fidelity models
- The approach demonstrated superior performance in tropical regions and under climate change scenarios (+4K warming)
- MF-RPNs provide reliable uncertainty quantification that increases appropriately with prediction errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-fidelity RPNs improve generalization by combining abundant low-fidelity data with scarce high-fidelity data.
- Mechanism: The model uses low-fidelity data to learn general patterns and regimes, while high-fidelity data refines these patterns with more accurate physics. The architecture directly learns the mapping from low- to high-fidelity outputs, allowing uncertainty to propagate naturally.
- Core assumption: Low-fidelity data spans broader regimes than high-fidelity data and provides useful information for extrapolation.
- Evidence anchors:
  - [abstract]: "MF-RPNs combine physical parameterization data as low-fidelity and storm-resolving historical run's data as high-fidelity."
  - [section]: "The chosen architecture directly learns the non-linear mapping between the low-to-high fidelity outputs instead of inferring the difference between them as an error bias correction."
- Break condition: If low-fidelity data does not span regimes relevant to the extrapolation scenario, the model's performance degrades.

### Mechanism 2
- Claim: RPNs provide reliable uncertainty quantification that increases with prediction error.
- Mechanism: Each RPN member consists of a trainable network plus a fixed prior network. Data bootstrapping and ensemble averaging provide uncertainty estimates. The uncertainty naturally increases when the model encounters out-of-distribution samples.
- Core assumption: The ensemble method and bootstrapping capture the true posterior distribution without collapse.
- Evidence anchors:
  - [abstract]: "Our approach paves the way for the use of machine-learning based methods that can optimally leverage historical observations or high-fidelity simulations and extrapolate to unseen regimes such as climate change."
  - [section]: "We also verify that the MF-RPN's uncertainty quantification estimated over the ensemble predictions is coherent as it increases with the predictions error."
- Break condition: If the prior networks are poorly chosen or the ensemble size is too small, uncertainty estimates may be unreliable.

### Mechanism 3
- Claim: Data normalization using low-fidelity statistics helps the model account for distribution shift between training and testing data.
- Mechanism: Since the extrapolation scenario is closer to the low-fidelity (CAM5 +8K) data distribution, normalizing all data with respect to low-fidelity statistics helps the model learn the correct mapping for extrapolation.
- Core assumption: The extrapolation scenario (SPCAM5 +4K) is closer in distribution to the low-fidelity training data (CAM5 +8K) than to the high-fidelity training data (SPCAM5 historical).
- Evidence anchors:
  - [section]: "The chosen normalization helps the MF-RPN model account for the distribution shift between the training and testing high-fidelity data, based only on information of the computationally cheaper but valuable (for extrapolation) low-fidelity training data."
  - [section]: "Distributions of the normalized test data using statistics from CAM5 +8K and SPCAM5 historical runs confirm the physically-based motivation of using the former dataset for MF-RPN normalization."
- Break condition: If the extrapolation scenario is not closer in distribution to the low-fidelity data, this normalization strategy would hurt performance.

## Foundational Learning

- Concept: Bayesian uncertainty quantification
  - Why needed here: Climate modeling requires understanding prediction confidence, especially for extrapolation scenarios.
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty in this context?

- Concept: Multi-fidelity modeling
  - Why needed here: High-fidelity climate simulations are computationally expensive and scarce, while low-fidelity simulations are abundant but less accurate.
  - Quick check question: How does the multi-fidelity approach balance the tradeoff between accuracy and computational cost?

- Concept: Data distribution shift and domain adaptation
  - Why needed here: The model must extrapolate from historical climate data to warmer climate scenarios, which involves significant distribution shift.
  - Quick check question: What are the key differences in data distribution between the training and testing datasets?

## Architecture Onboarding

- Component map: Input (52-dim) -> LF network (red) -> HF network (blue) -> Output prediction with uncertainty
- Critical path: Input → LF network → HF network → Output prediction with uncertainty
- Design tradeoffs:
  - Joint vs sequential training: Joint training ensures both networks are optimized together, while sequential training would prioritize the HF network.
  - Normalization strategy: Using low-fidelity statistics helps with extrapolation but may reduce accuracy on in-distribution data.
  - Ensemble size: Larger ensembles provide better uncertainty estimates but increase computational cost.
- Failure signatures:
  - Poor extrapolation: Negative R² values in tropical regions
  - Unreliable uncertainty: Uncertainty that doesn't increase with prediction error
  - Mode collapse: Ensemble members producing very similar predictions
- First 3 experiments:
  1. Train single-fidelity RPN on high-fidelity data only, evaluate on test set
  2. Train LF-RPN on low-fidelity data only, evaluate on test set
  3. Train MF-RPN with joint training, compare performance to experiments 1 and 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MF-RPN model perform in terms of uncertainty quantification compared to other RPN-based models, and does the uncertainty increase with prediction errors?
- Basis in paper: [explicit] The paper states that the MF-RPN model shows improved performance in terms of uncertainty quantification, with uncertainties increasing appropriately with prediction errors.
- Why unresolved: The paper provides some evidence through density plots and longitude-latitude structures, but a more detailed analysis of the uncertainty quantification across different scenarios and variables would be beneficial.
- What evidence would resolve it: Further analysis of the MF-RPN's uncertainty quantification across various scenarios, variables, and error metrics, along with comparisons to other RPN-based models.

### Open Question 2
- Question: Can the MF-RPN model be extended to include more complex physical constraints and observational data, and how would this affect its performance?
- Basis in paper: [inferred] The paper mentions that there is room for improvement by enforcing physical constraints and aggregating observational data, but it does not explore these extensions in detail.
- Why unresolved: The paper focuses on the current implementation of the MF-RPN model and does not delve into potential extensions or modifications.
- What evidence would resolve it: Implementation and evaluation of the MF-RPN model with additional physical constraints and observational data, along with a comparison of its performance to the current version.

### Open Question 3
- Question: How does the MF-RPN model perform in online settings within differentiable solvers, and what are the potential benefits and challenges of this approach?
- Basis in paper: [inferred] The paper mentions the possibility of extending the MF-RPN model to an online setting within differentiable solvers, but it does not explore this approach in detail.
- Why unresolved: The paper focuses on the offline performance of the MF-RPN model and does not discuss its potential application in online settings.
- What evidence would resolve it: Implementation and evaluation of the MF-RPN model in an online setting within differentiable solvers, along with an analysis of its performance and potential benefits and challenges.

## Limitations
- The approach relies heavily on the assumption that low-fidelity data spans relevant extrapolation regimes, which may not hold for all climate scenarios
- The ensemble size of 128 RPNs requires substantial computational resources that may limit broader adoption
- The evaluation focuses primarily on tropical regions, and performance in other climate zones requires further investigation

## Confidence
- **High**: The demonstrated improvements in R² scores (up to 0.73 for moisture tendency) and MAE reduction are robust across multiple experiments and directly measurable from the presented results.
- **Medium**: The uncertainty quantification reliability claim is supported by correlation between uncertainty estimates and prediction errors, but could benefit from additional out-of-distribution testing scenarios.
- **Medium**: The data normalization strategy's effectiveness for extrapolation is theoretically sound and empirically validated, but the approach's generalizability to other climate variables and scenarios needs further validation.

## Next Checks
1. Evaluate MF-RPN performance on additional climate variables (e.g., cloud cover, precipitation) to assess generalizability beyond heat and moisture tendencies.
2. Test the approach on out-of-distribution scenarios beyond +4K warming, including regional climate shifts and extreme weather events.
3. Compare computational efficiency against alternative multi-fidelity approaches while maintaining comparable accuracy levels.