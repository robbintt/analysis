---
ver: rpa2
title: 'UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers'
arxiv_id: '2309.16275'
source_url: https://arxiv.org/abs/2309.16275
tags:
- conspiracy
- data
- theories
- training
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting conspiracy theories
  in online discourse, specifically focusing on Italian Telegram messages. The authors
  employed a method that combines pre-trained Italian sentence Transformer models
  with data augmentation techniques to enhance performance.
---

# UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers

## Quick Facts
- arXiv ID: 2309.16275
- Source URL: https://arxiv.org/abs/2309.16275
- Reference count: 11
- Key outcome: First place in both sub-tasks of ACTI @ EVALITA 2023 with F1 scores of 85.71% (binary) and 91.23% (fine-grained)

## Executive Summary
This paper presents a solution for detecting conspiracy theories in Italian Telegram messages that achieved first place in the ACTI @ EVALITA 2023 shared task. The authors employed a method combining pre-trained Italian sentence Transformer models with data augmentation techniques to enhance performance on both binary classification (conspiratorial vs non-conspiratorial) and fine-grained topic classification (Covid, QAnon, Flat-Earth, Russian). By fine-tuning sentence embeddings using contrastive learning with SetFit and augmenting training data with LLM-generated paraphrases, they achieved F1 scores of 85.71% and 91.23% respectively, demonstrating the effectiveness of leveraging pre-trained models and data augmentation for conspiracy detection.

## Method Summary
The approach combines pre-trained Italian Sentence Transformer models with contrastive fine-tuning using the SetFit framework. The method involves generating positive and negative triplets for contrastive learning, followed by training a classification head on top of the fine-tuned embeddings. Data augmentation is performed using LLM-generated paraphrases to expand the training set and mitigate class imbalance. The pipeline processes Italian Telegram messages through pre-trained models, applies augmentation, performs contrastive fine-tuning, and trains classification heads for both binary and multi-class tasks.

## Key Results
- Achieved first place in both sub-tasks of ACTI @ EVALITA 2023
- Binary classification F1 score: 85.71%
- Fine-grained classification F1 score: 91.23%
- Demonstrated effectiveness of combining pre-trained models with data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained Italian sentence transformers on conspiratorial Telegram messages yields high performance for both binary and multi-class classification.
- Mechanism: The sentence transformer learns contextualized embeddings that capture semantic similarities between messages, enabling accurate separation of conspiratorial vs non-conspiratorial content and fine-grained topic classification.
- Core assumption: Italian language sentence transformers already encode sufficient linguistic and contextual knowledge that can be adapted to the conspiracy detection task with limited training data.
- Evidence anchors:
  - [abstract] "The combination of pre-trained sentence Transformer models and data augmentation techniques enabled us to secure first place in the final leaderboard of both sub-tasks."
  - [section] "We considered an Italian language Sentence Transformer model for our submissions and trained contrastive with SetFit as described by Tunstall et al. (2022)."
- Break condition: If the pre-trained model lacks adequate coverage of conspiratorial language patterns or domain-specific terminology, fine-tuning may not generalize well to unseen messages.

### Mechanism 2
- Claim: Data augmentation via LLM-generated paraphrases increases model robustness and mitigates class imbalance.
- Mechanism: LLM paraphrasing generates semantically equivalent variations of training samples, expanding the effective training set size and exposing the model to linguistic diversity, thereby improving generalization.
- Core assumption: LLM-generated paraphrases preserve the conspiratorial or non-conspiratorial nature of the original text while introducing sufficient variation to be useful for training.
- Evidence anchors:
  - [section] "we integrated a data augmentation step in our classification pipeline... we used an LLM to create paraphrases for our training data"
  - [section] "The distribution for the augmented dataset is shown in Table 2."
- Break condition: If LLM paraphrasing introduces semantic drift or hallucinated content that changes the conspiracy label, augmentation could degrade model performance.

### Mechanism 3
- Claim: Contrastive fine-tuning with SetFit optimizes sentence embeddings specifically for the target classification tasks.
- Mechanism: SetFit generates positive and negative triplets to fine-tune embeddings such that messages of the same class are projected closer together while different classes are pushed apart in the embedding space.
- Core assumption: The triplet-based contrastive learning effectively captures class-specific semantic boundaries in the embedding space, enabling better downstream classification.
- Evidence anchors:
  - [section] "Sentence-Transformers are pretrained Transformer models finetuned in a Siamese network... In our experiments, we used several Italian pretrained Sentence Transformers... The first step in the SetFit training process involves generating positive and negative triplets."
  - [section] "In the second step, a fully connected classification head is trained on top of the Sentence-Transformer to distinguish between the available classes."
- Break condition: If the triplet generation does not adequately represent class boundaries or the embedding space becomes too constrained, contrastive fine-tuning may not improve classification accuracy.

## Foundational Learning

- Concept: Sentence Transformers and Siamese Networks
  - Why needed here: Understanding how sentence transformers learn semantic embeddings and how Siamese networks enable contrastive learning is crucial for grasping the core methodology.
  - Quick check question: What is the difference between a standard classification fine-tuning and contrastive fine-tuning in the context of sentence transformers?

- Concept: Data Augmentation and Class Imbalance
  - Why needed here: Recognizing the impact of class imbalance on model performance and how data augmentation techniques can address this issue is essential for evaluating the experimental design.
  - Quick check question: How does generating paraphrases with an LLM help balance the training dataset, and what are the potential risks of this approach?

- Concept: Fine-grained vs Binary Classification
  - Why needed here: Understanding the distinction between binary classification (conspiratorial vs non-conspiratorial) and fine-grained multi-class classification (specific conspiracy topics) is important for interpreting the results.
  - Quick check question: Why might the same model architecture perform differently on binary classification versus fine-grained topic classification tasks?

## Architecture Onboarding

- Component map:
  - Pre-trained Italian Sentence Transformer models (e.g., efederici/sentence-BERTino, nickprock/sentence-bert-base-italian-xxl-uncased)
  - LLM for data augmentation (e.g., text-davinci-003, mT5)
  - SetFit for contrastive fine-tuning
  - Classification head (fully connected layer)
  - Training pipeline with triplet generation and augmentation

- Critical path:
  1. Load and preprocess Italian Telegram messages
  2. Generate LLM paraphrases for data augmentation
  3. Train Sentence Transformer with contrastive fine-tuning using SetFit
  4. Train classification head on top of fine-tuned embeddings
  5. Evaluate on test sets for both sub-tasks

- Design tradeoffs:
  - Using larger pre-trained models may improve performance but increase computational cost and training time
  - High-temperature LLM paraphrasing increases diversity but risks semantic drift
  - More training iterations and epochs may improve accuracy but also increase overfitting risk

- Failure signatures:
  - Poor performance on one sub-task but not the other may indicate model architecture or fine-tuning parameter mismatch
  - Significant drop in private leaderboard scores vs public leaderboard may indicate data distribution shift or overfitting to public test set
  - Low quality paraphrases may lead to noisy training data and degraded model performance

- First 3 experiments:
  1. Fine-tune a base Italian sentence transformer (e.g., efederici/sentence-BERTino) on the binary classification task without data augmentation to establish a baseline
  2. Apply data augmentation with mT5 paraphrasing to the binary classification task and compare performance to baseline
  3. Fine-tune a larger Italian sentence transformer (e.g., nickprock/sentence-bert-base-italian-xxl-uncased) on the fine-grained topic classification task with contrastive learning and evaluate performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated paraphrases affect the performance of conspiracy theory detection models, particularly when comparing different LLMs like text-davinci-003 and mT5?
- Basis in paper: [explicit] The paper discusses using different LLMs for data augmentation and notes that the mT5 model performed poorly compared to text-davinci-003, suggesting the quality of paraphrases impacts performance.
- Why unresolved: The paper does not provide a detailed analysis of the specific quality differences between the paraphrases generated by different LLMs or how these differences translate to model performance.
- What evidence would resolve it: A comparative study analyzing the semantic accuracy and diversity of paraphrases generated by different LLMs, and correlating these metrics with downstream model performance on conspiracy detection tasks.

### Open Question 2
- Question: What are the long-term effects of data augmentation on the robustness and generalization of conspiracy theory detection models across different datasets and platforms?
- Basis in paper: [inferred] The paper uses data augmentation to address class imbalance and improve model performance, but does not explore the long-term effects of this approach.
- Why unresolved: The study focuses on immediate performance improvements without investigating how augmented data affects model robustness over time or across varied datasets.
- What evidence would resolve it: Longitudinal studies comparing models trained with and without data augmentation across multiple datasets and platforms, assessing performance stability and generalization over time.

### Open Question 3
- Question: How does the choice of pre-trained sentence transformer model influence the detection of nuanced conspiracy topics, and what factors contribute to one model outperforming another in this context?
- Basis in paper: [explicit] The paper mentions using different Italian pre-trained sentence transformer models and notes that different models performed best for different sub-tasks, but does not delve into why.
- Why unresolved: The paper does not analyze the underlying factors that make one transformer model more effective than another for detecting specific conspiracy topics.
- What evidence would resolve it: An in-depth analysis of the architectural and training differences between transformer models, coupled with experiments to isolate the impact of these factors on conspiracy topic detection performance.

## Limitations

- Model Selection Ambiguity: The specific Italian Sentence Transformer model used for the final submission is not clearly specified, making it difficult to assess whether performance gains are due to the fine-tuning approach or the inherent capabilities of the specific pre-trained model.
- Data Augmentation Implementation Details: Critical implementation details of the LLM-based data augmentation pipeline are missing, including prompt structure, temperature settings, and quality control mechanisms.
- Generalizability to Other Languages: The methodology relies heavily on Italian-specific pre-trained models and language-specific conspiratorial discourse patterns, with no evidence of effective transfer to other languages or cultural contexts.

## Confidence

**High Confidence**: The fundamental approach of combining pre-trained sentence transformers with contrastive fine-tuning and data augmentation is sound and well-established in the literature. The achieved F1 scores (85.71% for binary, 91.23% for fine-grained) demonstrate clear performance improvements over baseline methods.

**Medium Confidence**: The specific implementation details of the SetFit contrastive fine-tuning and the LLM-based data augmentation pipeline, while methodologically reasonable, lack sufficient documentation for precise replication. The contribution of each component to the overall performance improvement cannot be fully isolated.

**Low Confidence**: Claims about the robustness and generalization of the approach beyond the specific ACTI dataset and Italian language context are not adequately supported by experimental evidence or cross-validation studies.

## Next Checks

1. **Ablation Study**: Conduct a systematic ablation study to isolate the contribution of each component (pre-trained model choice, contrastive fine-tuning, data augmentation) to the final performance. This would involve training models with different combinations of these components and measuring their individual and combined effects.

2. **Cross-Lingual Transfer**: Test the approach on conspiracy detection datasets in other languages (e.g., English or Spanish) to evaluate its generalizability. This would involve either translating the Italian dataset or applying the trained model to existing multilingual conspiracy detection corpora.

3. **Quality Assessment of Augmented Data**: Implement a manual or semi-automated quality assessment of the LLM-generated paraphrases to measure semantic drift and label preservation. This would help quantify the trade-off between augmentation diversity and data quality, providing insights into optimal augmentation parameters.