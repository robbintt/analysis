---
ver: rpa2
title: 'Rewarded soups: towards Pareto-optimal alignment by interpolating weights
  fine-tuned on diverse rewards'
arxiv_id: '2306.04488'
source_url: https://arxiv.org/abs/2306.04488
tags:
- rewards
- bleu
- learning
- reward
- rewarded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Rewarded Soups, a method for aligning foundation
  models with diverse human preferences by interpolating weights fine-tuned on different
  proxy rewards. The key idea is to first train multiple expert networks, each specialized
  on a different reward, and then interpolate their weights linearly to create a continuum
  of models covering various trade-offs.
---

# Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards

## Quick Facts
- arXiv ID: 2306.04488
- Source URL: https://arxiv.org/abs/2306.04488
- Reference count: 40
- Key outcome: Rewarded Soups interpolates weights fine-tuned on diverse rewards to achieve Pareto-optimal alignment without retraining

## Executive Summary
Rewarded Soups (RS) addresses the challenge of aligning foundation models with diverse human preferences by training multiple expert networks, each specialized on a different proxy reward, and then linearly interpolating their weights to create a continuum of models covering various trade-offs. The method leverages the observation that weights fine-tuned from a shared pre-trained initialization remain linearly connected, allowing interpolation to approximate Pareto-optimal solutions without additional training. Experiments demonstrate that RS can effectively balance between diverse rewards across text-to-text, image-to-text, and control tasks, often matching or exceeding the performance of more expensive multi-policy strategies.

## Method Summary
The method involves first pre-training or loading a foundation model, then independently fine-tuning N copies of this model on different proxy rewards using standard RLHF techniques. The resulting weight parameters from each fine-tuned model are then linearly interpolated to create a continuum of models spanning the reward space. The optimal interpolation coefficients are selected based on validation performance, allowing the method to capture diverse user preferences without retraining. The approach relies on the hypothesis that weights remain linearly connected when fine-tuned on diverse rewards from a shared initialization, enabling effective interpolation without retraining.

## Key Results
- Rewarded Soups achieves Pareto-optimal alignment across diverse rewards without requiring expensive multi-policy optimization
- Weight interpolation maintains linear mode connectivity in RL settings, enabling effective trade-off between competing objectives
- The method provides personalized alignments and handles reward misspecification through continuous coverage of the reward space

## Why This Works (Mechanism)

### Mechanism 1: Linear Mode Connectivity (LMC) in RL
Weights fine-tuned from a shared pre-trained initialization remain linearly connected even when optimized on diverse rewards, enabling effective interpolation without retraining. The LMC hypothesis states that for any convex combination of two fine-tuned weights, the reward for each individual objective is at least as good as the convex combination of the individual rewards. This allows creating a continuum of models that approximate the Pareto front. LMC breaks when weights diverge significantly during fine-tuning, which can happen with poor initialization, unstable RL algorithms, or very diverse reward structures that push parameters in opposite directions.

### Mechanism 2: Weight Interpolation Approximates Functional Ensembling
Linearly interpolating weights in parameter space approximates interpolating predictions in function space, allowing combined models to trade off capabilities of individual experts. When weights remain close, first-order Taylor expansion shows that f(x, θλ) ≈ (1-λ)f(x, θ1) + λf(x, θ2), meaning interpolated weights produce predictions similar to averaging predictions from individual models. Interpolation fails when weights are too far apart (divergence during training) or when the function space is highly non-linear with respect to parameters, making the first-order approximation invalid.

### Mechanism 3: Pareto-Optimal Coverage Through Weight Space Interpolation
Interpolating between N expert weights trained on different rewards creates a continuous set of policies that approximates the Pareto front without requiring explicit multi-objective optimization. By training one expert per reward and linearly interpolating weights, RS creates a continuous coverage set where any preference over rewards can be approximated by selecting appropriate interpolation coefficients. Coverage fails when expert weights are poorly distributed (e.g., clustered in reward space) or when the true Pareto front has complex geometry that cannot be well-approximated by convex combinations.

## Foundational Learning

- Concept: Linear Mode Connectivity
  - Why needed here: LMC is the theoretical foundation that makes weight interpolation effective; without it, interpolated models would perform worse than individual experts.
  - Quick check question: If you fine-tune two models from the same initialization on different rewards and they diverge significantly, what happens to the performance of their linear interpolation?

- Concept: Multi-Objective Optimization and Pareto Optimality
  - Why needed here: Understanding Pareto fronts is crucial for recognizing why RS can provide better trade-offs than single-policy approaches that optimize for consensus preferences.
  - Quick check question: In a two-objective problem where improving one objective necessarily degrades the other, what defines the set of Pareto-optimal solutions?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RS is specifically designed to address reward misspecification in RLHF, where proxy rewards imperfectly capture true human preferences.
  - Quick check question: What is the key difference between supervised fine-tuning and RLHF in terms of how they evaluate model outputs?

## Architecture Onboarding

- Component map:
  Pre-trained foundation model -> N independent RL fine-tuning processes -> N fine-tuned expert weights -> Weight interpolation module -> Validation system -> Inference system

- Critical path:
  1. Pre-train or load foundation model
  2. Define N diverse reward functions/models
  3. Launch N parallel RL fine-tuning runs from shared initialization
  4. Validate each fine-tuned model on held-out data
  5. Interpolate weights across the continuum of [0,1] for N=2 (or simplex for N>2)
  6. Select optimal interpolation coefficients via validation performance
  7. Deploy model using selected interpolated weights

- Design tradeoffs:
  - Number of rewards vs. computational cost: More rewards provide better coverage but require more fine-tuning runs
  - Reward diversity vs. interpolation quality: Rewards should be sufficiently different to capture meaningful trade-offs but not so conflicting that weights diverge
  - Pre-training quality vs. LMC maintenance: Better pre-training initialization helps maintain LMC during fine-tuning
  - Validation strategy vs. deployment performance: Cross-validation of λ is more robust but requires additional data

- Failure signatures:
  - Convexity violation: Interpolated models perform worse than linear interpolation of individual rewards (LMC fails)
  - Pareto front misspecification: Interpolated models don't cover the full range of desired trade-offs
  - Reward hacking: Models exploit weaknesses in proxy rewards rather than learning genuine alignment
  - Divergence: Fine-tuned weights become too far apart for effective interpolation

- First 3 experiments:
  1. Simple two-reward interpolation: Fine-tune LLaMA on two different summarization rewards (e.g., faithfulness vs. coherence) and verify LMC holds by checking that interpolated models outperform linear combinations of individual rewards.
  2. Multi-reward spider map visualization: Fine-tune on 3-5 rewards and create a spider map showing coverage of the reward space, comparing against scalarization baselines.
  3. Reward misspecification test: Use a held-out "true" reward that's a non-linear combination of training rewards and verify RS can approximate it better than single-policy approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the linear mode connectivity hypothesis hold for very deep neural networks with billions of parameters, or does it break down as network depth increases?
- Basis in paper: [explicit] The paper mentions that "linear mode connectivity (LMC) [...] underpins RS's viability" but does not thoroughly test it for extremely large models.
- Why unresolved: Most experiments in the paper use relatively small models (e.g., LLaMA-7b, 2.2B parameter diffusion models). The paper does not explore whether the LMC holds for state-of-the-art models with hundreds of billions of parameters.
- What evidence would resolve it: Systematic experiments interpolating weights of models with increasing depth and parameter counts, measuring reward consistency across interpolated models.

### Open Question 2
- Question: How does the linear mode connectivity hypothesis behave when fine-tuning on diverse rewards in reinforcement learning settings with non-stationary or changing reward distributions?
- Basis in paper: [inferred] The paper focuses on static reward distributions but mentions that "human preferences change from time to time." This suggests that the LMC might not hold for non-stationary reward distributions.
- Why unresolved: The paper does not explore the behavior of LMC when rewards are dynamic or non-stationary, which is a common scenario in real-world applications.
- What evidence would resolve it: Experiments fine-tuning on a sequence of changing reward distributions and measuring the degree of linear mode connectivity between weights trained on different time points.

### Open Question 3
- Question: Can the weight interpolation approach be extended to handle more complex reward combinations beyond linear mixtures, such as non-linear or hierarchical reward structures?
- Basis in paper: [explicit] The paper acknowledges that "such linearization cannot encapsulate all types of (human) preferences" and suggests that "considering more complex combinations [...] is a promising direction."
- Why unresolved: The paper only demonstrates the effectiveness of weight interpolation for linear combinations of rewards, leaving open the question of how to handle more complex reward structures.
- What evidence would resolve it: Experiments using weight interpolation for non-linear or hierarchical reward combinations and comparing the results to other methods like multi-objective reinforcement learning or reward shaping.

## Limitations
- The linear mode connectivity hypothesis in RL settings is not well-validated, particularly for diverse rewards that may push weights in opposite directions
- The method requires significant computational resources for fine-tuning multiple expert models, limiting practical applicability
- The assumption that convex combinations of expert weights adequately cover the Pareto front may not hold for complex reward geometries

## Confidence

- **High confidence**: The basic weight interpolation methodology and its implementation details are well-specified and reproducible. The computational framework for combining models is clearly defined.
- **Medium confidence**: The empirical results showing RS performs comparably to or better than multi-policy baselines across different task types (text-to-text, image-to-text, control). The experiments are well-designed but the sample size is limited.
- **Low confidence**: The theoretical claims about linear mode connectivity in RL settings and the general applicability of weight interpolation for Pareto-optimal alignment across diverse rewards. These claims extend established supervised learning results to RL without sufficient empirical validation.

## Next Checks
1. **LMC Stress Test**: Systematically vary the diversity of rewards and measure how weight divergence affects interpolation quality. Test with increasingly conflicting rewards to identify the breaking point where LMC fails.

2. **Pareto Front Geometry Analysis**: Map the true Pareto front for a multi-reward problem and quantitatively compare it to the coverage provided by RS weight interpolation. Identify cases where convex combinations fail to capture non-convex Pareto front geometries.

3. **Resource Efficiency Benchmark**: Compare the computational cost of RS (N fine-tuning runs + interpolation) against multi-policy baselines across different scales (varying N and model sizes) to establish practical trade-offs and identify sweet spots for application.