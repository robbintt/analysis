---
ver: rpa2
title: 'SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable
  Memory Budget'
arxiv_id: '2308.15030'
source_url: https://arxiv.org/abs/2308.15030
tags:
- experts
- memory
- expert
- pc-moe
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SwapMoE, a framework for efficiently serving
  off-the-shelf MoE-based large language models (LLMs) with tunable memory budgets.
  The key idea is to maintain a small dynamic set of important experts, called Virtual
  Experts, in the main memory for inference while seamlessly mapping them to the actual
  experts.
---

# SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget

## Quick Facts
- arXiv ID: 2308.15030
- Source URL: https://arxiv.org/abs/2308.15030
- Reference count: 40
- Reduces memory consumption from 14.2 GiB to 4.7 GiB while achieving 50% latency reduction

## Executive Summary
This paper proposes SwapMoE, a framework for efficiently serving off-the-shelf MoE-based large language models (LLMs) with tunable memory budgets. The key innovation is maintaining a small dynamic set of important experts (Virtual Experts) in main memory while seamlessly mapping them to actual experts. This approach exploits three key observations about expert activations: temporal locality, exchangeability, and skippable computation. The framework significantly reduces memory consumption and latency while maintaining reasonable accuracy for continuous inference scenarios.

## Method Summary
SwapMoE introduces a Parameter Committee (PC) framework that maintains a small dynamic set of important experts in main memory for inference. The system uses an offline profiling-guided committee planner to determine optimal PC configuration and an online adaptive committee scheduler to manage expert swapping and request handling. The framework leverages three key mechanisms: temporal locality of expert activations (recently activated experts are likely to be activated again), exchangeability of experts (non-PC experts can be substituted with PC experts), and skippable computation (non-PC experts can be skipped or approximated without significant accuracy loss).

## Key Results
- Reduces memory consumption from 14.2 GiB to 4.7 GiB on text summarization tasks with Switch Transformer
- Achieves 50% latency reduction while maintaining reasonable accuracy
- Incurs only a slight Rouge-2 score drop of 0.041 compared to baseline
- Demonstrates effectiveness across multiple MoE architectures including Swin-MoE, GMoE, and GPT-MoE

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dynamic expert swapping reduces memory usage while maintaining accuracy.
- **Mechanism**: SwapMoE maintains a small set of important experts (Virtual Experts) in main memory and maps them to actual experts as needed. This exploits temporal locality, where recently activated experts are likely to be activated again soon.
- **Core assumption**: Expert activation patterns exhibit temporal locality in continuous inference scenarios.
- **Evidence anchors**: Experiments show temporal locality in Swin-MoE model where most frames activate the same experts repeatedly. Related work on expert offloading systems supports feasibility.

### Mechanism 2
- **Claim**: Skippable computation allows ignoring non-critical experts without significant accuracy loss.
- **Mechanism**: Non-PC experts can be skipped or their computation results approximated, leveraging the observation that some expert computations contribute minimally to final predictions.
- **Core assumption**: The contribution of some experts to the final output is optional and can be approximated without major accuracy degradation.
- **Evidence anchors**: Experiments reveal that skipping some expert computations results in only slight performance loss (e.g., from 52.22 to 52.15). Related work on eMoE explores similar expert skipping strategies.

### Mechanism 3
- **Claim**: Exchangeability of experts allows flexible substitution without accuracy loss.
- **Mechanism**: Non-PC experts can be substituted with PC experts during inference, as experts can be interchangeable for most input examples despite being trained for different functionalities.
- **Core assumption**: Different experts within the MoE model can be interchangeable for many input samples, even though they were trained for different knowledge domains.
- **Evidence anchors**: Swap experiments show minimal accuracy loss when randomly swapping 50% of experts (from 52.22 to 52.12). Related work on expert substitution strategies supports this approach.

## Foundational Learning

- **Concept**: Mixture of Experts (MoE) architecture
  - **Why needed here**: Understanding MoE is fundamental to grasping why expert swapping and management strategies work.
  - **Quick check question**: In a typical MoE layer with 8 experts and top-2 gating, how many experts are activated for each input token?

- **Concept**: Temporal locality in expert activation
  - **Why needed here**: This property enables the dynamic swapping strategy to be effective.
  - **Quick check question**: If a video frame sequence activates experts [3,5,3,3,5,5,3], what temporal locality pattern does this demonstrate?

- **Concept**: Importance scoring for experts
  - **Why needed here**: Critical for determining which experts to keep in the Parameter Committee.
  - **Quick check question**: Given three experts with parameter magnitudes [1.2, 0.8, 1.5] and gate outputs [0.4, 0.1, 0.5] for a token, which expert has the highest importance score?

## Architecture Onboarding

- **Component map**: Input token → Gating computation → PC expert selection → Output computation → Optional non-PC handling (skip/exchange)
- **Critical path**: Input token → Gating computation → PC expert selection → Output computation → Optional non-PC handling (skip/exchange)
- **Design tradeoffs**:
  - PC size vs. accuracy: Larger PC improves accuracy but increases memory usage
  - Update frequency vs. latency: More frequent updates improve temporal locality exploitation but increase swapping overhead
  - Skip vs. exchange strategies: Skip reduces computation but may lose information; exchange maintains information but requires more complex routing
- **Failure signatures**:
  - High swapping frequency with minimal accuracy improvement: Indicates poor temporal locality exploitation
  - Significant accuracy drop despite reasonable memory usage: Suggests suboptimal PC configuration or inappropriate skip/exchange strategy
  - Unexpected latency spikes: Could indicate thrashing (frequent eviction and re-loading of the same experts)
- **First 3 experiments**:
  1. **Memory-accuracy tradeoff analysis**: Vary PC size (e.g., 25%, 50%, 75% of total experts) and measure accuracy degradation and memory savings
  2. **Temporal locality validation**: Analyze expert activation sequences from continuous inference to quantify temporal locality (e.g., calculate repeat activation rates)
  3. **Strategy comparison**: Compare skip vs. exchange strategies under identical PC configurations to determine which provides better accuracy-latency tradeoffs for specific tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of PC-MoE vary across different types of continuous input data (e.g., videos with varying temporal locality, non-video data streams)?
- **Basis in paper**: The paper discusses experiments with continuous video input data and mentions testing with faster videos and randomly shuffled video frames, but does not extensively explore other types of continuous data streams.
- **Why unresolved**: The evaluation focuses primarily on computer vision tasks with video data, leaving the generalizability to other continuous data types unexplored.
- **What evidence would resolve it**: Experiments applying PC-MoE to diverse continuous data streams such as sensor data, audio streams, or financial time series, with comprehensive performance analysis across these domains.

### Open Question 2
- **Question**: What is the impact of PC-MoE on model accuracy when deployed on heterogeneous edge devices with varying computational capabilities and memory hierarchies?
- **Basis in paper**: The paper evaluates PC-MoE on two devices (NVIDIA 3090 GPU and Jetson AGX ORIN) and mentions different I/O bandwidths, but does not explore a wide range of heterogeneous edge devices.
- **Why unresolved**: The evaluation is limited to a narrow set of hardware configurations, not reflecting the diversity of edge devices in real-world deployments.
- **What evidence would resolve it**: Comprehensive testing across a spectrum of edge devices including low-power microcontrollers, various GPU architectures, and devices with different memory hierarchies, with detailed performance and accuracy trade-off analysis.

### Open Question 3
- **Question**: How does PC-MoE handle catastrophic forgetting when the input data distribution shifts significantly over time, and what mechanisms can be implemented to mitigate this?
- **Basis in paper**: The paper mentions continuous inference scenarios and uses datasets with domain shift, but does not explicitly address the long-term adaptability of the model to evolving data distributions.
- **Why unresolved**: The paper focuses on immediate performance optimization without considering the long-term stability of the model in dynamic environments.
- **What evidence would resolve it**: Longitudinal studies tracking model performance over extended periods with gradually shifting data distributions, and experimental results demonstrating the effectiveness of mechanisms like incremental learning or elastic weight consolidation in preserving accuracy.

## Limitations

- The framework assumes continuous inference scenarios where temporal locality holds, but provides no analysis of performance under batch processing or diverse workloads where this assumption may break.
- The paper doesn't address the computational overhead of the committee scheduler or the potential cache thrashing from frequent expert swapping.
- Claims about expert interchangeability are based on limited experiments with only one model architecture, lacking comprehensive validation across different MoE architectures.

## Confidence

**High confidence**: The memory reduction claims are well-supported by experimental results showing specific GB reductions across multiple models. The latency improvement measurements are concrete and reproducible.

**Medium confidence**: The accuracy maintenance claims are supported but the statistical significance of the observed accuracy drops (e.g., 0.041 Rouge-2 score reduction) is unclear. The framework's generalizability to different MoE architectures and tasks is demonstrated but not comprehensively validated.

**Low confidence**: The claims about expert interchangeability are based on limited experiments with only one model architecture. The paper doesn't adequately address potential accuracy cliffs that might occur with different input distributions or more aggressive swapping strategies.

## Next Checks

1. **Temporal locality statistical validation**: Analyze expert activation sequences from continuous inference across multiple models to quantify temporal locality using metrics like repeat activation rates and inter-arrival times. Compare these patterns across different task domains and input distributions.

2. **Expert interchangeability boundary testing**: Systematically vary the percentage of experts swapped (25%, 50%, 75%, 100%) and measure accuracy degradation across multiple models and tasks. Identify the breaking point where interchangeability assumptions fail.

3. **Ablation study of core mechanisms**: Isolate and measure the individual contributions of temporal locality exploitation, expert exchangeability, and skippable computation to overall performance improvements. This will reveal which mechanisms provide the most value and where optimization efforts should be focused.