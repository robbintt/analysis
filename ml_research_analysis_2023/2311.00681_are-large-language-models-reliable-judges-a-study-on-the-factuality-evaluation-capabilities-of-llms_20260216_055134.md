---
ver: rpa2
title: Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation
  Capabilities of LLMs
arxiv_id: '2311.00681'
source_url: https://arxiv.org/abs/2311.00681
tags:
- factuality
- llms
- errors
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates whether large language models (LLMs) like
  GPT-3.5, GPT-4, and PaLM-2 can reliably judge the factuality of text summaries.
  Two approaches are tested: (1) using LLMs as a unified QA-based factuality evaluator,
  and (2) having LLMs directly score summary faithfulness.'
---

# Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs

## Quick Facts
- arXiv ID: 2311.00681
- Source URL: https://arxiv.org/abs/2311.00681
- Reference count: 10
- Key outcome: Current LLMs (GPT-3.5, GPT-4, PaLM-2) show no significant correlation with human judgments on factuality evaluation, with GPT-3.5 showing limited correlation only in two subcategories.

## Executive Summary
This study investigates whether large language models (LLMs) can reliably judge the factuality of text summaries. The researchers evaluate two approaches: using LLMs as unified QA-based factuality evaluators and having LLMs directly score summary faithfulness. Experiments are conducted on the FRANK benchmark, which includes human-annotated factuality judgments across multiple error types. The results reveal that current LLMs have fundamental limitations in accurately assessing factuality, with no significant correlation between LLM-based factuality scores and human judgments for GPT-4 and PaLM-2, and limited correlation for GPT-3.5.

## Method Summary
The researchers evaluate the factuality evaluation capabilities of LLMs (GPT-3.5, GPT-4, and PaLM-2) using the FRANK benchmark dataset. Two approaches are tested: (1) a unified QA-based factuality evaluation pipeline where a single LLM performs answer selection, question generation, and question answering, and (2) direct faithfulness scoring where LLMs rate summaries on a scale of 1-5. Pearson, Spearman, and partial correlation coefficients are computed between LLM-generated scores and human annotations across various error types (semantic frame errors, discourse errors, content verifiability errors) to assess performance.

## Key Results
- GPT-4 and PaLM-2 show no significant correlation with human factuality judgments across all error types.
- GPT-3.5 shows limited correlation only in two subcategories (PredE and CircE).
- The unified QA-based approach and direct faithfulness scoring both fail to achieve high correlation with human evaluations.
- These findings suggest fundamental limitations in current LLMs' ability to accurately assess factuality.

## Why This Works (Mechanism)

### Mechanism 1
Using a single LLM to perform all steps of the QA-based factuality scoring pipeline can improve efficiency and reduce complexity compared to using separate models. By leveraging in-context learning and a unified prompt structure, a single LLM can generate both questions and answers from the summary and then answer those questions using the original article, allowing for a streamlined evaluation process.

### Mechanism 2
Direct faithfulness scoring via LLMs can provide a simple and interpretable measure of summary factuality. By prompting an LLM to rate the faithfulness of a summary on a scale from 1 to 5, we obtain a direct human-readable score that can be correlated with human annotations.

### Mechanism 3
Partial correlation analysis can provide a more accurate measure of LLM factuality evaluation by controlling for confounding variables like dataset and system properties. By calculating partial correlation coefficients instead of simple Pearson/Spearman correlations, we isolate the relationship between LLM scores and human judgments while accounting for systematic biases.

## Foundational Learning

- Concept: Question-answering-based factuality evaluation
  - Why needed here: The paper relies on comparing answers generated from the summary versus the article to measure factual consistency.
  - Quick check question: What are the three main steps in a traditional QA-based factuality evaluation pipeline?

- Concept: Correlation coefficients (Pearson, Spearman, partial)
  - Why needed here: The paper uses these statistical measures to quantify the relationship between LLM-generated scores and human judgments.
  - Quick check question: How does partial correlation differ from Pearson correlation in the context of factuality evaluation?

- Concept: Factual error typologies (semantic frame, discourse, content verifiability)
  - Why needed here: The FRANK benchmark categorizes errors to assess LLM performance across different types of factual inconsistencies.
  - Quick check question: Name two subcategories under semantic frame errors and explain their difference.

## Architecture Onboarding

- Component map: Article + Summary → LLM (unified QA) → Generated answers → Similarity score → Correlation with human judgment
- Critical path: Article → LLM (unified QA) → Generated answers → Similarity score → Correlation with human judgment
- Design tradeoffs:
  - Unified LLM vs. specialized models: Simplicity and efficiency vs. potential loss in accuracy
  - Similarity-based vs. direct rating: Granular comparison vs. interpretability
  - Partial vs. simple correlation: Controlling for bias vs. ease of computation
- Failure signatures:
  - Low or inconsistent correlation scores across all error types
  - LLM-generated questions/answers are irrelevant or nonsensical
  - Direct faithfulness ratings do not align with known factual errors
- First 3 experiments:
  1. Test the unified LLM QA pipeline on a small sample of article-summary pairs and manually verify generated questions/answers.
  2. Compare the similarity scores from the unified LLM with a baseline (e.g., ROUGE) to check if they capture different aspects of factuality.
  3. Run the direct faithfulness scoring on the same sample and check if the ratings are consistent and interpretable.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning open-source LLMs for factuality evaluation lead to a more reliable factuality evaluator? The paper states that future work will investigate whether fine-tuning open-source LLMs for factuality evaluation leads to a better factuality evaluator. This remains unresolved as the study only evaluated closed-source LLMs.

### Open Question 2
Can LLMs reliably evaluate factuality in noisy datasets, such as those with domain-specific terminology or errors? The paper mentions that future work will investigate the factuality evaluation capabilities of LLMs using noisy datasets. This remains unresolved as the study only evaluated LLMs on the FRANK benchmark.

### Open Question 3
Can other prompting strategies or few-shot learning improve the factuality evaluation capabilities of LLMs? The paper suggests that future work will investigate the use of few-shot learning and other prompting strategies. This remains unresolved as the study only used a single prompting strategy for each LLM.

## Limitations

- The study's findings are limited to English summaries and may not generalize to other languages or domains.
- The correlation analysis does not explore potential improvements through fine-tuning or prompt engineering.
- The choice of specific LLMs (GPT-3.5, GPT-4, PaLM-2) may not represent the full range of available models.

## Confidence

- **High Confidence**: The experimental methodology and correlation analysis are sound and reproducible. The FRANK benchmark is a well-established dataset for factuality evaluation.
- **Medium Confidence**: The claims about the fundamental limitations of LLMs in factuality evaluation are supported by the results, but further investigation is needed to determine if these limitations are inherent or can be mitigated.
- **Low Confidence**: The paper does not explore alternative approaches to LLM-based factuality evaluation, such as fine-tuning or ensemble methods, which could potentially improve performance.

## Next Checks

1. **Prompt Engineering**: Test whether carefully crafted prompts or few-shot examples can improve LLM performance in factuality evaluation. This would help determine if the limitations are due to prompt quality rather than inherent LLM capabilities.
2. **Fine-Tuning**: Fine-tune the LLMs on a subset of the FRANK benchmark and evaluate their performance on the remaining data. This would assess whether supervised learning can enhance factuality evaluation capabilities.
3. **Cross-Lingual Evaluation**: Apply the same evaluation methodology to multilingual summaries to test the generalizability of the findings. This would help determine if the limitations are specific to English or more universal.