---
ver: rpa2
title: Do Multilingual Language Models Think Better in English?
arxiv_id: '2308.01223'
source_url: https://arxiv.org/abs/2308.01223
tags:
- self-translate
- direct
- language
- llama
- xglm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce self-translate, a method that leverages the
  few-shot translation capabilities of multilingual language models to improve performance
  on non-English tasks without requiring external machine translation systems. By
  prompting the model to first translate input into English and then solve the task,
  self-translate consistently outperforms direct inference across five tasks and seven
  model sizes.
---

# Do Multilingual Language Models Think Better in English?

## Quick Facts
- **arXiv ID**: 2308.01223
- **Source URL**: https://arxiv.org/abs/2308.01223
- **Reference count**: 40
- **Key outcome**: Self-translate consistently outperforms direct inference across five tasks and seven model sizes, with the gap widening at scale.

## Executive Summary
This paper introduces self-translate, a method that leverages the few-shot translation capabilities of multilingual language models to improve performance on non-English tasks without external translation systems. The approach prompts models to first translate inputs into English before solving the task, consistently outperforming direct inference across five tasks and seven model sizes. The findings suggest that multilingual language models are unable to fully leverage their multilingual potential when prompted in non-English languages, with the effectiveness gap widening at scale and being more pronounced for high-resource languages.

## Method Summary
The study evaluates multilingual language models on five tasks (XCOPA, XStoryCloze, XNLI, PAWS-X, MGSM) comparing direct inference versus self-translate methods. Direct inference feeds non-English input directly to the model, while self-translate prompts the model to first translate the input into English, then solve the task. The evaluation uses few-shot prompting without training, employing models including XGLM, LLaMA, BLOOM, OpenLLaMA, OpenLLaMA V2, Redpajama, and PolyLM. Translation quality is measured using COMET and BLEU scores on FLORES-200 data.

## Key Results
- Self-translate consistently outperforms direct inference across all tested tasks, model sizes, and languages
- The effectiveness gap between self-translate and direct inference widens with model size
- High-resource languages show larger gains from self-translate compared to low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual language models retain task-solving capabilities across languages but fail to fully activate them when prompted in non-English languages.
- Mechanism: The model's internal representations and reasoning pathways are not optimally aligned with non-English inputs, causing underutilization of its multilingual knowledge. Translating inputs into English restructures the input space into a format that better matches the model's learned representations, enabling full capability utilization.
- Core assumption: The model's task-solving mechanisms are language-agnostic but require language-aligned input representations for optimal activation.
- Evidence anchors:
  - [abstract] "language models are unable to leverage their full multilingual potential when prompted in non-English languages"
  - [section] "we find that self-translate works better than direct inference in average for all models" and "language models are more capable than immediately obvious in non-English languages, but unveiling their full potential requires performing intermediate steps"
- Break condition: If the model's internal representations are truly language-agnostic without any structural bias toward English, self-translate would show no advantage over direct inference.

### Mechanism 2
- Claim: Cross-lingual transfer of capabilities exists but is inefficient without explicit translation.
- Mechanism: The model learns task-solving capabilities that can transfer across languages, but the transfer efficiency is low when operating directly in non-English. Self-translate acts as a bridge that explicitly converts the input representation, enabling more effective cross-lingual transfer.
- Core assumption: Multilingual models can transfer capabilities across languages, but the transfer mechanism is implicit and suboptimal without explicit intervention.
- Evidence anchors:
  - [abstract] "we find that LLaMA is much better than XGLM in MGSM despite being worse in other tasks" and "LLaMA is more capable at solving math word problems, and it is able to leverage this capability even if prompted in other languages"
  - [section] "Multilingual language models do transfer capabilities across languages" and "this cross-lingual transfer is not fully effective, but our results suggest that it does happen to a large extent"
- Break condition: If the model's capabilities are learned separately for each language without any cross-lingual sharing, self-translate would not provide consistent improvements.

### Mechanism 3
- Claim: Model scale correlates with improved translation capabilities and thus better self-translate performance.
- Mechanism: Larger models have more parameters and training data, enabling them to develop better translation abilities. As translation quality improves, the gap between self-translate and direct inference narrows, approaching the performance of using external MT systems.
- Core assumption: Model size directly influences translation quality, which in turn affects the effectiveness of self-translate.
- Evidence anchors:
  - [abstract] "the gap between self-translate and direct inference widens with model size" and "it is the largest LLaMA model that obtains the biggest absolute gains over direct inference"
  - [section] "the gap between self-translate and direct inference gets larger at scale" and "the effect of scale is bigger for high-resource languages"
- Break condition: If model size doesn't correlate with improved translation capabilities, the scaling relationship would not hold.

## Foundational Learning

- Concept: Few-shot prompting and in-context learning
  - Why needed here: The self-translate approach relies on the model's ability to perform translation tasks using few-shot examples provided in the prompt, without requiring fine-tuning or additional training.
  - Quick check question: How does the model learn to translate from just a few examples in the prompt, and what limits this ability?

- Concept: Cross-lingual transfer and multilingual representation alignment
  - Why needed here: Understanding how multilingual models represent and transfer knowledge across languages is crucial for explaining why self-translate works and why direct inference underperforms.
  - Quick check question: What mechanisms allow multilingual models to transfer capabilities across languages, and what causes misalignment in non-English representations?

- Concept: Model scaling effects on capability emergence
- Why needed here: The paper demonstrates that self-translate effectiveness increases with model size, requiring understanding of how scaling affects capability emergence and translation quality.
- Quick check question: At what point does scaling lead to emergent translation capabilities, and how does this relate to task performance?

## Architecture Onboarding

- Component map:
  Input preprocessing -> Translation module (few-shot prompting) -> Task inference (English input) -> Output postprocessing -> Evaluation pipeline

- Critical path:
  1. Receive non-English input
  2. Generate translation using few-shot examples
  3. Process translated English input through task-specific prompts
  4. Extract and format final answer
  5. Evaluate accuracy

- Design tradeoffs:
  - Speed vs. accuracy: Self-translate adds translation step, increasing latency but improving results
  - Prompt complexity vs. model capability: More complex prompts may require larger models
  - Resource usage: Translation step doubles inference cost
  - Generalization: Few-shot examples may not cover all language variations

- Failure signatures:
  - Translation errors propagating to task performance
  - Inconsistent results across different few-shot examples
  - Degradation for low-resource languages
  - Scale-dependent performance variations

- First 3 experiments:
  1. Baseline comparison: Run direct inference vs. self-translate on a single task and language pair
  2. Scale analysis: Compare self-translate effectiveness across different model sizes
  3. Translation quality impact: Measure correlation between translation accuracy and task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multilingual language models perform on tasks in non-English languages when fine-tuned on parallel translation data?
- Basis in paper: [inferred] The authors show that self-translate improves performance, suggesting multilingual models don't fully leverage their capabilities when prompted in non-English languages. They speculate this might be due to lack of cross-lingual transfer or limited capacity.
- Why unresolved: The authors focus on zero-shot/few-shot prompting without fine-tuning. They don't test whether adding parallel translation data during training would eliminate the need for self-translate.
- What evidence would resolve it: Experiments comparing zero-shot/few-shot performance to fine-tuned models on the same tasks in non-English languages.

### Open Question 2
- Question: Does the effectiveness of self-translate scale with model size for tasks in low-resource languages?
- Basis in paper: [explicit] The authors find self-translate is more effective for high-resource languages and large models. They note that the gap narrows at scale, but don't specifically analyze low-resource language performance trends.
- Why unresolved: The analysis groups low- and high-resource languages together when examining translation quality improvements. The authors don't break down performance by individual low-resource languages.
- What evidence would resolve it: Detailed performance analysis of self-translate vs direct inference for each low-resource language across different model sizes.

### Open Question 3
- Question: What is the impact of translation artifacts on evaluation when using translated test data?
- Basis in paper: [explicit] The authors acknowledge that all datasets were created through human translation, which can result in evaluation artifacts for methods involving machine translation. They note this is a limitation.
- Why unresolved: The authors don't attempt to quantify or mitigate the impact of potential artifacts. They don't compare results on natively written non-English datasets.
- What evidence would resolve it: Experiments using natively written non-English datasets to validate findings from translated datasets.

### Open Question 4
- Question: How does instruction tuning affect the need for self-translate?
- Basis in paper: [explicit] The authors note that instruction-tuned models would remove the need for few-shot prompts and make self-translate more efficient, but they don't experiment with instruction-tuned models.
- Why unresolved: All experiments use base models. The authors don't test whether instruction tuning alone would eliminate the performance gap between English and non-English prompting.
- What evidence would resolve it: Direct comparison of base vs instruction-tuned models using self-translate and direct inference on the same tasks.

## Limitations

- The study focuses exclusively on encoder-decoder and decoder-only models, leaving open the question of whether findings generalize to encoder-only architectures
- Translation quality metrics are evaluated on FLORES-200, which may not reflect the actual distribution of language used in the target tasks
- The comparison assumes optimal translation quality for direct inference prompts, which is not independently verified

## Confidence

- **High confidence**: The empirical finding that self-translate consistently outperforms direct inference across all tested tasks, model sizes, and languages
- **Medium confidence**: The claim that the effectiveness gap widens with model size, as this relationship could be influenced by other factors
- **Medium confidence**: The assertion that high-resource languages show larger gains from self-translate, requiring further investigation into the underlying mechanism

## Next Checks

1. **Translation Quality Validation**: Conduct human evaluation of the self-translate outputs for a subset of languages and tasks to verify that translation quality correlates with task performance improvements.

2. **Cross-Architecture Generalization**: Test the self-translate approach on encoder-only models and smaller models (<1B parameters) to determine whether the observed benefits are specific to decoder-only architectures.

3. **External Translation Comparison**: Implement an ablation study comparing self-translate against using external machine translation APIs for the same tasks to quantify whether the self-contained nature of self-translate comes at a cost in translation quality and task performance.