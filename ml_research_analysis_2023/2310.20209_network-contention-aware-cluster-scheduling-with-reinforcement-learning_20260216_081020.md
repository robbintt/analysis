---
ver: rpa2
title: Network Contention-Aware Cluster Scheduling with Reinforcement Learning
arxiv_id: '2310.20209'
source_url: https://arxiv.org/abs/2310.20209
tags:
- scheduling
- contention
- jobs
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reinforcement learning-based GPU cluster
  scheduling approach that mitigates network contention in distributed deep learning
  training. The key idea is to formulate cluster scheduling as an RL problem where
  the agent learns to minimize average job contention sensitivity while maintaining
  high GPU utilization.
---

# Network Contention-Aware Cluster Scheduling with Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.20209
- Source URL: https://arxiv.org/abs/2310.20209
- Reference count: 40
- One-line primary result: RL-based GPU cluster scheduling reduces average job completion time by up to 18.2% and tail job completion time by up to 20.7% compared to LAS and SRTF policies.

## Executive Summary
This paper presents a reinforcement learning-based approach to GPU cluster scheduling that mitigates network contention in distributed deep learning training. The authors formulate cluster scheduling as an RL problem where the agent learns to minimize average job contention sensitivity while maintaining high GPU utilization. Their approach uses a novel state representation capturing cluster-wide job co-location and a reward function that penalizes contention while incentivizing utilization. Evaluation results show significant improvements in job completion times compared to widely used scheduling policies, with the RL-Hybrid policy achieving a preferable trade-off between performance and resource utilization.

## Method Summary
The method involves formulating GPU cluster scheduling as a reinforcement learning problem where the agent learns to minimize average job contention sensitivity. The approach uses a state representation that encodes cluster-wide job co-location as a 2D tensor, with actions representing job scheduling decisions. The reward function penalizes increased contention sensitivity while rewarding higher GPU utilization. The RL-base policy makes scheduling decisions based solely on the trained model, while RL-Hybrid combines this with a rule-based greedy policy to fill slots where RL-base would abstain, achieving better utilization without significantly increasing contention.

## Key Results
- RL-based scheduling reduces average job completion time by up to 18.2% compared to LAS and SRTF policies
- Tail job completion time is reduced by up to 20.7% using the proposed approach
- RL-Hybrid achieves a preferable trade-off between average job completion time and resource utilization
- The approach maintains high GPU utilization while effectively mitigating network contention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL-based scheduling dynamically adapts to varying contention sensitivities across diverse job co-locations.
- Mechanism: The agent learns a state representation encoding cluster-wide job co-location and uses this to predict and mitigate contention sensitivity.
- Core assumption: Contention sensitivity can be profiled online and accurately represented in the state tensor.
- Evidence anchors:
  - [abstract] "We formulate GPU cluster scheduling as a reinforcement learning problem and opt to learn a network contention-aware scheduling policy that efficiently captures contention sensitivities and dynamically adapts scheduling decisions through continuous evaluation and improvement."
  - [section III] "We observe that some jobs... show high variability with regard to the model, GPU demands, and node assignment of the co-located job."
- Break condition: If profiling contention sensitivity is inaccurate or too slow, the learned policy cannot adapt effectively.

### Mechanism 2
- Claim: Minimizing cluster-wide average contention sensitivity directly reduces average job completion time (JCT).
- Mechanism: The reward function penalizes increased contention sensitivity and rewards higher GPU utilization, leading the agent to find placements that reduce overall contention.
- Core assumption: There is a monotonic relationship between average contention sensitivity and average JCT.
- Evidence anchors:
  - [abstract] "Our approach reduces average job completion time by up to 18.2% and effectively cuts the tail job completion time by up to 20.7%."
  - [section IV] "Our reward penalizes an increase in cluster-wide average contention sensitivity of scheduled jobs and provides incentives on higher cluster-wide average GPU utilization."
- Break condition: If contention sensitivity does not correlate with JCT in certain workload mixes, the reward may not lead to optimal scheduling.

### Mechanism 3
- Claim: Hybrid policy (RL + rule-based) achieves better utilization while maintaining low JCT.
- Mechanism: RL-base avoids scheduling when ∅ is chosen, but RL-Hybrid fills those slots with a greedy rule-based policy to boost utilization.
- Core assumption: The rule-based greedy policy can safely schedule jobs without causing excessive contention when RL-base would abstain.
- Evidence anchors:
  - [abstract] "RL-Hybrid achieves a preferable trade-off between average job completion time and resource utilization."
  - [section IV] "RL-Hybrid performs decision-level multiplexing of the trained policy and a simple rule-based policy... when it faces an ∅, i.e. decision not to schedule, it follows a safety rule of trying greedy scheduling to prevent low utilization."
- Break condition: If greedy scheduling frequently causes contention spikes, the trade-off will degrade performance.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (states, actions, rewards, policy gradients)
  - Why needed here: The entire scheduling approach is built on RL, so understanding how agents learn from environment feedback is essential.
  - Quick check question: What is the difference between model-based and model-free RL, and which is used here?

- Concept: Deep learning workload characteristics (communication patterns, GPU demands)
  - Why needed here: The scheduler must understand different DL jobs' contention sensitivities to allocate resources effectively.
  - Quick check question: Why do FSDP and MoE have higher communication-to-computation ratios than traditional data-parallel training?

- Concept: Cluster scheduling metrics (JCT, resource utilization, fairness)
  - Why needed here: The scheduler's performance is evaluated on these metrics, so understanding their definitions and trade-offs is critical.
  - Quick check question: How does minimizing average contention sensitivity relate to improving average JCT?

## Architecture Onboarding

- Component map: Trainer -> Scheduler -> Controller -> NodeAgent -> Checkpoint Store

- Critical path:
  1. Job submission → waiting queue
  2. Controller calls ComputeObs() → state encoding
  3. Core loads RL policy → action decision
  4. Controller schedules jobs via NodeAgent
  5. NodeAgent profiles jobs → updates contention sensitivity
  6. If contention high → preemption → checkpointing

- Design tradeoffs:
  - Action space size vs. convergence speed (limiting to K candidates speeds training but may miss optimal placements).
  - Granularity of profiling vs. overhead (more frequent profiling improves accuracy but adds latency).
  - Reward weight tuning (W1, W2) balances JCT vs. utilization.

- Failure signatures:
  - High average contention sensitivity → policy not learning effective placements.
  - Low utilization despite jobs waiting → RL-base too conservative; consider RL-Hybrid.
  - Long preemption delays → checkpoint store throughput bottleneck.

- First 3 experiments:
  1. Run baseline LAS/SRTF on the same job traces and compare average JCT.
  2. Train RL-base with different W1/W2 ratios and plot trade-off curves.
  3. Deploy RL-Hybrid vs. RL-base on a live cluster and measure utilization improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RL-based scheduling compare to rule-based approaches in clusters with heterogeneous GPU types and varying communication patterns?
- Basis in paper: [inferred] The paper evaluates the RL-based approach on a homogeneous cluster and mentions that the design can be applied to large-scale clusters without limitation, but does not provide empirical results for heterogeneous environments.
- Why unresolved: The paper focuses on a homogeneous cluster setup and does not explore the scalability or performance implications of the RL-based approach in heterogeneous environments with diverse GPU types and communication patterns.
- What evidence would resolve it: Empirical evaluation of the RL-based scheduling policy on a heterogeneous GPU cluster with diverse communication patterns, comparing its performance to rule-based approaches in terms of average and tail job completion time, resource utilization, and contention sensitivity.

### Open Question 2
- Question: How does the performance of the RL-based scheduling policy change with different reward function weights (W1, W2) and their impact on average contention sensitivity and utilization?
- Basis in paper: [explicit] The paper presents two benchmark policies, RL-base and RL-Hybrid, and mentions that the weights in the reward function can be tailored to reflect the relative preference between average contention sensitivity and utilization.
- Why unresolved: While the paper demonstrates the trade-off relationship between average contention sensitivity and utilization, it does not provide a comprehensive analysis of how different reward function weights impact the performance of the RL-based scheduling policy.
- What evidence would resolve it: A detailed study on the impact of different reward function weights on the performance of the RL-based scheduling policy, including average and tail job completion time, resource utilization, and contention sensitivity, across various cluster workloads and communication patterns.

### Open Question 3
- Question: How does the RL-based scheduling policy perform in handling dynamic changes in job arrivals, departures, and communication patterns in real-time?
- Basis in paper: [inferred] The paper mentions that the RL-based approach can dynamically adapt scheduling decisions across diverse distributions of jobs and their contention sensitivities, but does not provide empirical evidence for its performance in handling dynamic changes in real-time.
- Why unresolved: The paper focuses on evaluating the RL-based scheduling policy using static job traces and does not explore its performance in handling dynamic changes in job arrivals, departures, and communication patterns in real-time.
- What evidence would resolve it: Empirical evaluation of the RL-based scheduling policy in a dynamic environment with varying job arrivals, departures, and communication patterns, comparing its performance to rule-based approaches in terms of average and tail job completion time, resource utilization, and contention sensitivity.

## Limitations
- The evaluation relies on a cluster simulator rather than real-world deployment data, which may not capture all runtime dynamics.
- The state representation and profiling mechanisms assume contention sensitivity can be accurately captured through job-level profiling.
- The reward function's effectiveness in reducing JCT is based on simulation results and may not generalize perfectly to real clusters.

## Confidence

- High confidence in the problem formulation and general effectiveness of RL-based scheduling for mitigating network contention.
- Medium confidence in the specific state representation and profiling mechanisms, as they rely on assumptions about contention sensitivity that may not always hold.
- Medium confidence in the reward function's ability to balance JCT reduction and utilization improvement, as this is based on simulation results and may not generalize perfectly to real clusters.

## Next Checks

1. Deploy the trained RL policies (RL-base and RL-Hybrid) on a real GPU cluster with diverse deep learning workloads to validate the simulation results and assess real-world performance.
2. Conduct stress tests with extreme load conditions and novel job types to evaluate the robustness of the state representation and profiling mechanisms in capturing contention sensitivity.
3. Perform ablation studies to analyze the impact of individual components (e.g., state representation, reward function, action space) on the scheduler's performance and identify potential bottlenecks or areas for improvement.