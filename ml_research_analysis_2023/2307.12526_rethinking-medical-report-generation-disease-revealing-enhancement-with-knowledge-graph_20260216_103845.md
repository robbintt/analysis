---
ver: rpa2
title: 'Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge
  Graph'
arxiv_id: '2307.12526'
source_url: https://arxiv.org/abs/2307.12526
tags:
- diseases
- disease
- generation
- report
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of long-tailed disease distribution
  in medical report generation (MRG), where common diseases are overrepresented while
  rare diseases are underrepresented. The authors propose a two-stage MRG approach
  that first classifies images as disease-free or disease-specific, then generates
  reports using separate transformers for each class.
---

# Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph

## Quick Facts
- arXiv ID: 2307.12526
- Source URL: https://arxiv.org/abs/2307.12526
- Authors: 
- Reference count: 16
- Primary result: Proposed method improves DS from 0.1523 to 0.1902 and DOR from 0.2911 to 0.5138 compared to R2Gen baseline

## Executive Summary
This paper addresses the long-tailed disease distribution problem in medical report generation (MRG), where common diseases are overrepresented while rare diseases are underrepresented. The authors propose a two-stage MRG approach that first classifies images as disease-free or disease-specific, then generates reports using separate transformers for each class. They also introduce a knowledge graph-based augmentation strategy to balance the disease distribution and a new evaluation metric called diverse sensitivity (DS) to assess clinical relevance. The proposed method significantly improves performance on rare disease detection and overall clinical efficacy.

## Method Summary
The proposed method consists of a two-stage generation approach: an image classifier (ResNet101) first detects the presence of abnormalities, routing the image to either a disease-free or disease-specific generator. Both generators are transformer-based models trained on their respective subsets of data. To address the long-tailed disease distribution, a knowledge graph-based augmentation strategy is employed, creating additional training samples for rare diseases by substituting sentences with different phrasings. The evaluation uses a new metric called diverse sensitivity (DS) that combines disease detection accuracy with diversity of generated diseases, along with Diagnostic Odds Ratio (DOR) for disease-free report generation.

## Key Results
- DS improves from 0.1523 to 0.1902 compared to R2Gen baseline
- DOR improves from 0.2911 to 0.5138
- The two-stage approach with separate generators outperforms single-generator models
- KG-based augmentation significantly improves rare disease representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage approach improves rare disease detection by separating disease-free from disease-specific cases.
- Mechanism: By training a classifier to first identify whether an image contains any abnormality, the system can route to specialized generators that focus on either normal findings or specific diseases. This specialization reduces the model's tendency to default to common diseases.
- Core assumption: Separate generators trained on balanced subsets of data will outperform a single generator trained on the full imbalanced dataset.
- Evidence anchors:
  - [abstract] "We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators..."
  - [section] "During the inference stage, a two-stage approach is adopted, where an input image is first fed to the image classifier to distinguish whether it contains any disease or abnormality, and then the corresponding generator is chosen to generate the diagnostic report in the second stage."
- Break condition: If the classifier has high false negative rate, disease-specific reports will be incorrectly routed to the disease-free generator, reducing accuracy.

### Mechanism 2
- Claim: Knowledge graph augmentation addresses the long-tailed disease distribution problem.
- Mechanism: The augmentation strategy creates additional training samples for rare diseases by substituting sentences with different phrasings from a key-value pool, increasing representation of tail-end diseases without requiring new data collection.
- Core assumption: Sentence-level augmentation based on disease labels can effectively balance the distribution without introducing unrealistic variations.
- Evidence anchors:
  - [abstract] "To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution."
  - [section] "we define a count interval [5, 100] by omitting sentence labels with a label count of less than 5 or more than 100. Starting from the label with the fewest unique-format sentences in this interval, we first find all diagnostic reports that contain sentences under this sentence label."
- Break condition: If augmentation creates unrealistic or inconsistent disease descriptions, it may harm model performance rather than help.

### Mechanism 3
- Claim: The DS metric aligns evaluation with clinical utility by focusing on disease detection accuracy and diversity.
- Mechanism: DS combines sensitivity (proportion of ground truth diseases captured) with diversity (variety of diseases generated), penalizing models that only generate common diseases even if they achieve high accuracy on those.
- Core assumption: Clinical relevance is better captured by disease-specific metrics than by general language metrics like BLEU.
- Evidence anchors:
  - [abstract] "To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases."
  - [section] "Lastly, DS is a harmonic mean of Sen. and Div., i.e., DS = 2 × Sen.×Div. Sen.+Div. ."
- Break condition: If the disease detection rule-based criterion is too strict or too lenient, DS may not accurately reflect clinical quality.

## Foundational Learning

- Concept: Knowledge graph construction and utilization
  - Why needed here: The KG provides the disease taxonomy and relationships needed for both augmentation and evaluation
  - Quick check question: How many disease types are included in the KG, and what are the main categories?

- Concept: Long-tailed distribution and class imbalance
  - Why needed here: Understanding how rare diseases are underrepresented in the data and why this matters for model performance
  - Quick check question: What percentage of diseases appear less than 10 times in the original dataset?

- Concept: Two-stage training with separate generators
  - Why needed here: This architecture design choice is central to the proposed solution
  - Quick check question: What are the two types of generators used, and how are they trained?

## Architecture Onboarding

- Component map:
  - Image classifier (ResNet101) → Routes to appropriate generator
  - Disease-free generator (R2Gen) → Trained on normal reports
  - Disease-specific generator (R2Gen) → Trained on abnormal reports
  - Knowledge graph → Provides disease taxonomy for augmentation and evaluation
  - Augmentation module → Creates additional training samples for rare diseases

- Critical path:
  1. Input image → Image classifier
  2. Classifier predicts disease presence/absence
  3. Route to disease-free or disease-specific generator
  4. Generator produces report
  5. Evaluation using DS metric

- Design tradeoffs:
  - Separate generators vs. single multi-task model: Specialization improves performance but adds complexity
  - Augmentation vs. collecting more data: Augmentation is cheaper but may introduce artifacts
  - Disease-specific evaluation vs. general metrics: More clinically relevant but may not correlate with other performance measures

- Failure signatures:
  - High classifier error rate → Reports routed to wrong generator
  - Overfitting on augmented data → Poor generalization to real cases
  - Low DS but high BLEU → Model generates fluent but clinically inaccurate reports

- First 3 experiments:
  1. Train and evaluate just the image classifier on disease presence/absence
  2. Train separate generators on their respective subsets and evaluate DS improvement
  3. Apply augmentation and measure changes in disease distribution and model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed knowledge graph-based augmentation strategy perform when applied to other medical imaging modalities beyond chest X-rays?
- Basis in paper: [inferred] The paper focuses on chest X-ray imaging and mentions that the KG can be extended to other applications, but does not provide experimental evidence for other modalities.
- Why unresolved: The study is limited to chest X-ray data, and the effectiveness of the KG and augmentation strategy on other imaging types remains unexplored.
- What evidence would resolve it: Applying the proposed method to other medical imaging datasets (e.g., MRI, CT scans) and comparing performance metrics like DS and DOR to baseline models would provide insights into its generalizability.

### Open Question 2
- Question: Can the two-stage generation approach be improved by incorporating uncertainty estimation in the classifier to reduce false negative predictions?
- Basis in paper: [explicit] The paper acknowledges that the classifier can produce false negative predictions, which is a limitation of the current approach.
- Why unresolved: The study does not address the issue of false negatives or explore methods to improve classifier reliability.
- What evidence would resolve it: Integrating uncertainty estimation techniques (e.g., Bayesian methods) into the classifier and evaluating the impact on DOR and DS scores would demonstrate whether this approach mitigates false negatives.

### Open Question 3
- Question: How does the proposed DS metric compare to human evaluations in terms of assessing the clinical relevance of generated reports?
- Basis in paper: [inferred] The paper introduces DS as a KG-based metric but does not compare it to human evaluations, which are mentioned as labor-intensive and subjective in the literature.
- Why unresolved: The paper does not provide a direct comparison between DS and human expert assessments.
- What evidence would resolve it: Conducting a study where human radiologists evaluate the same generated reports using DS and comparing their assessments would validate the effectiveness of the proposed metric.

## Limitations

- The augmentation strategy may introduce semantic inconsistencies or clinically implausible variations that are not captured by the DS metric
- The evaluation focuses on disease detection rather than overall report quality, potentially overlooking other clinically important aspects
- The knowledge graph construction details remain underspecified, making it difficult to assess whether the 137 disease types adequately represent clinical reality

## Confidence

- **High confidence**: The two-stage architecture design and its basic implementation are well-described and technically sound. The claim that separating disease-free from disease-specific cases improves performance is supported by clear architectural reasoning.
- **Medium confidence**: The effectiveness of the KG-based augmentation strategy is demonstrated empirically but lacks detailed validation of the augmented samples' clinical plausibility. The assumption that sentence-level substitution preserves medical accuracy needs stronger verification.
- **Medium confidence**: The DS metric represents a meaningful advancement in evaluation methodology, but its correlation with actual clinical utility remains to be established through radiologist assessment or downstream clinical task performance.

## Next Checks

1. **Clinical coherence validation**: Conduct a blinded review by radiologists to assess whether KG-augmented sentences maintain clinical plausibility and consistency with disease patterns, comparing original and augmented samples for semantic coherence.

2. **Classifier error analysis**: Systematically evaluate the image classifier's false negative rate and analyze how misclassified disease cases affect downstream report quality, measuring the impact of routing errors on final DS scores.

3. **Cross-dataset generalization**: Test the two-stage approach with KG augmentation on an independent chest X-ray dataset (e.g., PadChest or CheXpert) to assess whether improvements in DS and DOR generalize beyond the IU-Xray dataset used in training.