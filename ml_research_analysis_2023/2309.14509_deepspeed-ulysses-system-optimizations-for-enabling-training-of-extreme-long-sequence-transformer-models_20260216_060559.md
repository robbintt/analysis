---
ver: rpa2
title: 'DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long
  Sequence Transformer Models'
arxiv_id: '2309.14509'
source_url: https://arxiv.org/abs/2309.14509
tags:
- sequence
- parallelism
- attention
- deepspeed
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepSpeed-Ulysses, a system optimization method
  for enabling efficient training of Transformer models with extremely long sequence
  lengths. The key idea is to partition the input data along the sequence dimension
  and use efficient all-to-all collective communication for attention computation,
  which maintains constant communication volume when sequence length and compute devices
  are increased proportionally.
---

# DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models

## Quick Facts
- **arXiv ID**: 2309.14509
- **Source URL**: https://arxiv.org/abs/2309.14509
- **Reference count**: 7
- **Key outcome**: DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than state-of-the-art methods while supporting sequences over 1 million tokens

## Executive Summary
DeepSpeed-Ulysses introduces a novel system optimization for training Transformer models with extremely long sequence lengths. The method partitions input data along the sequence dimension and uses efficient all-to-all collective communication for attention computation, achieving constant communication volume when sequence length and compute devices scale proportionally. The system demonstrates 2.5x faster training with 4x longer sequence length compared to existing methods, while enabling training with sequences exceeding one million tokens. DeepSpeed-Ulysses is fully general, supporting dense and sparse attention mechanisms, and integrates with ZeRO-3 for memory optimization of large models.

## Method Summary
DeepSpeed-Ulysses addresses the challenge of training Transformer models with extremely long sequences by partitioning the input along the sequence dimension and employing all-to-all collective communication for attention computation. This approach ensures that communication complexity scales as O(N/P) rather than O(N), maintaining constant communication volume when sequence length (N) and compute devices (P) increase proportionally. The system integrates with ZeRO-3 to support large model sizes and works with efficient attention implementations like FlashAttention v2. The method is designed to be easy to use and portable, requiring minimal code changes to existing training frameworks while maintaining support for various attention mechanisms and model architectures.

## Key Results
- Achieves 2.5x faster training throughput compared to existing state-of-the-art methods
- Enables training with 4x longer sequence lengths (up to over 1 million tokens)
- Maintains constant communication volume when scaling sequence length and compute devices proportionally

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally
- **Mechanism**: The system partitions input data along the sequence dimension and uses all-to-all collective communication for attention computation, ensuring communication complexity scales as O(N/P) instead of O(N)
- **Core assumption**: When N and P are increased proportionally, the communication overhead remains constant
- **Evidence anchors**: Theoretical analysis shows communication volume per link is 4Nh/P (O(N/P) complexity), which remains constant when N and P scale proportionally
- **Break condition**: If sequence length increases without proportional increase in compute devices, communication volume will increase

### Mechanism 2
- **Claim**: DeepSpeed-Ulysses achieves 2.5x faster training with 4x longer sequence length than existing methods
- **Mechanism**: By reducing communication overhead through efficient all-to-all collectives and integrating with ZeRO-3 for memory optimization, the system can handle larger sequence lengths more efficiently
- **Core assumption**: The communication reduction and memory optimization translate directly to training speed improvements
- **Evidence anchors**: Communication reduction of over 10x compared to existing systems, resulting in throughput improvements of up to 2.5x
- **Break condition**: If the local attention implementation becomes a bottleneck, the performance advantage may diminish

### Mechanism 3
- **Claim**: DeepSpeed-Ulysses supports training with sequences of over a million tokens
- **Mechanism**: By partitioning the sequence dimension and using efficient communication collectives, the system can handle extremely long sequences that would otherwise exceed memory capacity
- **Core assumption**: The partitioning approach effectively distributes the memory and computation requirements across multiple devices
- **Evidence anchors**: System enables training with sequences over a million tokens while maintaining linear scalability with GPU count
- **Break condition**: If the number of GPUs is insufficient to partition the sequence adequately, memory limitations may still occur

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanism
  - **Why needed here**: Understanding how transformers work is crucial to grasping why sequence parallelism is necessary and how it improves efficiency
  - **Quick check question**: How does the attention mechanism in transformers scale with sequence length, and why does this create computational challenges?

- **Concept**: Distributed systems and collective communication
  - **Why needed here**: The paper relies heavily on concepts like all-to-all communication, which are fundamental to understanding how DeepSpeed-Ulysses achieves its efficiency gains
  - **Quick check question**: What is the difference between all-to-all and all-gather communication in terms of communication volume and complexity?

- **Concept**: Memory optimization techniques in deep learning
  - **Why needed here**: ZeRO-3 integration is a key component of DeepSpeed-Ulysses, and understanding how it partitions model states is essential to grasping the full solution
  - **Quick check question**: How does ZeRO-3 differ from traditional data parallelism in terms of memory usage and communication patterns?

## Architecture Onboarding

- **Component map**: Input sequence partitioning -> QKV projections -> All-to-all collectives -> Parallel attention computation -> All-to-all gather -> Repartition along sequence dimension -> Subsequent transformer layers

- **Critical path**: 
  1. Partition input sequences across available GPUs
  2. Perform QKV projections locally on each GPU
  3. Execute all-to-all collective communication to gather QKV tensors
  4. Compute attention per head in parallel across GPUs
  5. Perform another all-to-all to gather results and repartition along sequence dimension
  6. Continue with subsequent transformer layers (MLP, layer norm, etc.)

- **Design tradeoffs**: 
  - Communication vs. computation: The all-to-all collectives add communication overhead but enable parallel attention computation
  - Memory vs. performance: ZeRO-3 integration trades off some communication for significant memory savings
  - Flexibility vs. optimization: Supporting multiple attention mechanisms and implementations may limit some optimizations

- **Failure signatures**: 
  - High communication overhead: If the all-to-all collectives are not properly optimized, communication may become a bottleneck
  - Memory overflow: If the sequence partitioning is not sufficient for the available GPU memory, training may fail
  - Performance degradation: If the local attention implementation is inefficient, the benefits of sequence parallelism may be lost

- **First 3 experiments**:
  1. Verify basic functionality: Run a small transformer model with a moderately long sequence length to ensure the sequence parallelism works correctly
  2. Benchmark communication overhead: Measure the time spent in all-to-all collectives versus computation to identify potential bottlenecks
  3. Test ZeRO-3 integration: Verify that the model states are properly partitioned and that memory usage is optimized when using DeepSpeed-Ulysses with ZeRO-3

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the theoretical limit of sequence length scalability for DeepSpeed-Ulysses when using current GPU memory and interconnect technologies?
- **Basis in paper**: [inferred] The paper demonstrates scalability up to 1 million tokens but doesn't establish theoretical limits or discuss what would prevent further scaling
- **Why unresolved**: The paper focuses on practical implementations and comparisons rather than exploring fundamental hardware constraints that might limit future scalability
- **What evidence would resolve it**: Systematic experiments varying GPU memory configurations, interconnect bandwidth, and sequence lengths beyond 1 million tokens, coupled with analytical models of memory and communication bottlenecks

### Open Question 2
- **Question**: How does DeepSpeed-Ulysses performance scale when combining sequence parallelism with other forms of parallelism (data, tensor, pipeline) in heterogeneous GPU clusters?
- **Basis in paper**: [explicit] The paper mentions DeepSpeed-Ulysses works with ZeRO-3 and various attention mechanisms, but doesn't explore performance in heterogeneous or mixed-parallelism scenarios
- **Why unresolved**: The evaluation focuses on homogeneous setups and doesn't address the practical challenges of deploying in real-world heterogeneous clusters
- **What evidence would resolve it**: Comprehensive benchmarking across different GPU generations, mixed-precision training, and various parallelism combinations in production environments

### Open Question 3
- **Question**: What is the impact of DeepSpeed-Ulysses on training convergence for different model architectures beyond GPT and BERT?
- **Basis in paper**: [explicit] The convergence study is limited to GPT models, with no exploration of how sequence parallelism affects other architectures like Vision Transformers or multimodal models
- **Why unresolved**: The paper's evaluation is narrowly focused on text-based models, leaving questions about broader applicability unanswered
- **What evidence would resolve it**: Convergence studies across diverse model architectures, including vision, multimodal, and scientific computing models, with various sequence length configurations

## Limitations
- Paper lacks specific hardware requirements needed to achieve claimed performance improvements
- Limited information on how the method performs with different attention mechanisms beyond dense attention
- No details on how the system handles variable sequence lengths within the same training batch

## Confidence

- **High confidence**: Core mechanisms and theoretical analysis of communication complexity
- **Medium confidence**: Practical implementation details and integration with ZeRO-3
- **Low confidence**: Absolute performance claims without access to specific experimental conditions

## Next Checks
1. Reproduce the communication complexity analysis: Implement a small-scale version of DeepSpeed-Ulysses and measure the actual communication volume when scaling sequence length and GPU count proportionally to verify the O(N/P) scaling claim

2. Benchmark against state-of-the-art: Compare training throughput and memory efficiency of DeepSpeed-Ulysses against Arctic and other sequence parallelism methods using identical model configurations and hardware setups

3. Test extreme sequence lengths: Validate the claim of training with over 1 million tokens by attempting to train a small transformer model with sequences of increasing length until hitting practical limits, documenting the scaling behavior and any bottlenecks encountered