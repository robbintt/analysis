---
ver: rpa2
title: 'An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham:
  A Large Language Model Approach'
arxiv_id: '2311.04913'
source_url: https://arxiv.org/abs/2311.04913
tags:
- spam
- ipsdm
- learning
- accuracy
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the persistent problem of phishing and spam
  email detection, which causes significant financial and time losses for individuals
  and organizations. The core method involves fine-tuning transformer-based Large
  Language Models (LLMs), specifically DistilBERT and RoBERTa, to create an improved
  model called IPSDM (Improved Phishing and Spam Detection Model).
---

# An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach

## Quick Facts
- arXiv ID: 2311.04913
- Source URL: https://arxiv.org/abs/2311.04913
- Reference count: 0
- Primary result: IPSDM achieves 97.10% accuracy on balanced datasets and 67.86% on imbalanced datasets for phishing/spam/ham detection

## Executive Summary
This paper presents IPSDM, an improved transformer-based model for detecting phishing, spam, and ham emails. The authors fine-tune DistilBERT and RoBERTa on a combined dataset of phishing, spam, and ham emails, applying various optimization techniques including ADASYN resampling for class imbalance, learning rate scheduling, and early stopping. The model demonstrates significant performance improvements over baseline transformer models, achieving high accuracy rates while effectively handling imbalanced data distributions. The work addresses a critical cybersecurity challenge by providing a more accurate and efficient approach to email threat detection.

## Method Summary
The authors fine-tune DistilBERT and RoBERTa on a combined dataset containing phishing, spam, and ham emails. The original dataset shows severe imbalance with 4825 ham samples compared to 747 spam and 189 phishing samples. ADASYN synthetic sampling balances the minority classes, followed by training with AdamW optimizer using a learning rate of 2e-5, batch sizes of 32 (training) and 64 (validation), and cross-entropy loss. The model incorporates optimization techniques including learning rate scheduling, sequence length adjustment, hyperparameter tuning, and early stopping to improve performance and reduce overfitting.

## Key Results
- IPSDM achieves 97.10% accuracy on balanced datasets, significantly outperforming baseline transformer models
- On imbalanced datasets, IPSDM achieves 67.86% accuracy while maintaining better class-wise performance metrics
- The model effectively reduces overfitting and handles class imbalance through ADASYN resampling and regularization techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IPSDM improves detection performance by fine-tuning DistilBERT and RoBERTa on phishing-spam-ham data
- Mechanism: Transfer learning from general language understanding to specialized email security task via supervised fine-tuning
- Core assumption: Pre-trained transformer embeddings capture relevant linguistic patterns transferable to phishing/spam detection
- Evidence anchors:
  - [abstract] "fine-tuning transformer-based Large Language Models (LLMs), specifically DistilBERT and RoBERTa, to create an improved model called IPSDM"
  - [section] "We aim to improve the model performance and efficiency through optimization, i.e., learning rate scheduling, adjusting batch size, sequence length and loss function, hyper parameter tuning, early stopping and fine tuning."

### Mechanism 2
- Claim: IPSDM reduces overfitting by balancing datasets and using regularization techniques
- Mechanism: Adaptive Synthetic Sampling (ADASYN) balances minority classes, while AdamW with weight decay controls parameter growth
- Core assumption: Overfitting is primarily due to class imbalance rather than model complexity
- Evidence anchors:
  - [abstract] "IPSDM effectively reduces overfitting and handles imbalanced data"
  - [section] "This process reduces the bias towards the majority class making the overall predictive model more accurate and efficient."

### Mechanism 3
- Claim: IPSDM handles imbalanced data better than baseline models through careful sampling and optimization
- Mechanism: Initial imbalanced dataset is resampled using ADASYN, then fine-tuned with learning rate scheduling and batch size adjustments
- Core assumption: Minority class samples can be meaningfully augmented without introducing noise
- Evidence anchors:
  - [abstract] "IPSDM outperforms baseline pre-trained models on both balanced and imbalanced datasets"
  - [section] "Due to the heavy skewness of sample distribution, the results are biased towards 'ham' class."

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: IPSDM builds on DistilBERT/RoBERTa; understanding attention mechanisms explains why fine-tuning works
  - Quick check question: What is the role of the self-attention mechanism in capturing contextual relationships between words?

- Concept: Imbalanced classification strategies
  - Why needed here: Dataset has 747 spam, 189 phishing, 4825 ham samples; requires specialized handling
  - Quick check question: How does ADASYN differ from random oversampling in handling class imbalance?

- Concept: Fine-tuning vs. pre-training
  - Why needed here: IPSDM leverages pre-trained weights but adapts to specific task; understanding this distinction explains resource efficiency
  - Quick check question: What are the key differences between pre-training and fine-tuning in terms of data requirements and objectives?

## Architecture Onboarding

- Component map: Data pipeline: Collection → Preprocessing → Tokenization → DataLoader → Model components: Pre-trained DistilBERT/RoBERTa + Classification head + Fine-tuning layers → Training loop: Forward pass → Cross-entropy loss → AdamW optimization → Validation checkpointing

- Critical path: Tokenization → Model forward pass → Loss computation → Parameter update → Validation evaluation

- Design tradeoffs:
  - DistilBERT vs RoBERTa: Faster inference (DistilBERT) vs potentially better accuracy (RoBERTa)
  - Batch size: 32 for training vs 64 for validation balances memory and speed
  - Learning rate: 2e-5 standard for BERT-family models, but may need tuning for specific datasets

- Failure signatures:
  - High validation accuracy but low test accuracy → overfitting
  - Precision >> Recall → model biased toward majority class
  - Validation and test accuracy gap > 5% → potential overfitting or data leakage

- First 3 experiments:
  1. Run baseline DistilBERT and RoBERTa on provided balanced dataset; compare validation/test accuracy
  2. Apply ADASYN to imbalanced dataset and train both models; measure class-wise precision/recall
  3. Implement IPSDM with learning rate scheduling and early stopping; compare against baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IPSDM perform on real-world, dynamic email environments where spam and phishing techniques continuously evolve?
- Basis in paper: [inferred] The paper focuses on static datasets and controlled experimental settings, but does not address real-world deployment scenarios
- Why unresolved: The study uses pre-collected datasets without testing the model's adaptability to new, unseen phishing techniques or evolving spam patterns
- What evidence would resolve it: Long-term deployment studies with continuous retraining on new data, or A/B testing against current production systems in live environments

### Open Question 2
- Question: What is the computational cost and latency of IPSDM compared to existing heuristic-based systems in production environments?
- Basis in paper: [inferred] The paper mentions DistilBERT's efficiency but does not provide real-world performance metrics like inference time or resource usage under load
- What evidence would resolve it: Benchmarking IPSDM against production systems measuring CPU/GPU usage, latency per email, and throughput under realistic email traffic volumes

### Open Question 3
- Question: How does IPSDM handle multilingual phishing emails and domain-specific terminology?
- Basis in paper: [explicit] The paper only mentions English datasets and does not discuss multilingual capabilities or domain adaptation
- Why unresolved: The evaluation is limited to English email corpora without testing on non-English datasets or specialized terminology (e.g., financial, medical)
- What evidence would resolve it: Testing IPSDM on multilingual datasets and domain-specific corpora, measuring performance across different languages and specialized vocabulary

### Open Question 4
- Question: What are the failure modes and false positive/negative rates for IPSDM in edge cases?
- Basis in paper: [inferred] While precision and recall are reported, the paper does not provide detailed analysis of failure patterns or edge cases
- Why unresolved: The paper focuses on aggregate metrics without analyzing specific failure modes, such as false positives on legitimate business emails or false negatives on sophisticated phishing attempts
- What evidence would resolve it: Detailed error analysis with case studies of misclassified emails, identification of common failure patterns, and statistical analysis of false positive/negative distributions

## Limitations
- The exact source of phishing, spam, and ham email datasets is not specified, limiting reproducibility
- The study relies on synthetic data augmentation through ADASYN, which may not fully capture minority class distributions
- Performance on real-world, dynamic email environments with evolving phishing techniques is not evaluated

## Confidence

**High Confidence (Evidence strongly supports claims):**
- Transformer-based models can be effectively fine-tuned for phishing/spam detection
- Class imbalance negatively impacts detection performance
- IPSDM shows improved performance over baseline transformer models on reported datasets

**Medium Confidence (Evidence supports claims but with limitations):**
- IPSDM specifically reduces overfitting through the mentioned optimization techniques
- ADASYN effectively balances the dataset without introducing harmful noise
- The 97.10% and 99.00% accuracy figures represent robust performance

**Low Confidence (Evidence is limited or missing):**
- IPSDM performance generalizes to unseen phishing techniques
- The model's behavior on real-world production data matches benchmark results
- The specific hyperparameter choices are optimal across different datasets

## Next Checks
1. Train and evaluate IPSDM on at least two additional, publicly available phishing/spam datasets from different sources to verify whether the performance gains persist across varied data distributions and collection methods

2. Create a test set containing phishing emails that employ techniques specifically designed to evade transformer-based detection (e.g., excessive use of benign language, unusual formatting, or domain spoofing patterns) to evaluate whether IPSDM maintains its performance under adversarial conditions

3. Systematically disable individual optimization components (ADASYN resampling, learning rate scheduling, early stopping, etc.) to quantify their individual contributions to IPSDM's performance and determine which optimizations provide the most value