---
ver: rpa2
title: 'Late Stopping: Avoiding Confidently Learning from Mislabeled Examples'
arxiv_id: '2308.13862'
source_url: https://arxiv.org/abs/2308.13862
tags:
- examples
- training
- learning
- noisy
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning with noisy labels,
  specifically focusing on the difficulty of identifying and learning from clean hard
  examples (CHEs) that are often misclassified as noisy due to large losses. The proposed
  Late Stopping method leverages the intrinsic robust learning ability of deep neural
  networks (DNNs) by prolonging the training process.
---

# Late Stopping: Avoiding Confidently Learning from Mislabeled Examples

## Quick Facts
- arXiv ID: 2308.13862
- Source URL: https://arxiv.org/abs/2308.13862
- Authors: 
- Reference count: 40
- Primary result: Outperforms state-of-the-art methods on CIFAR-10 and CIFAR-100 with symmetric and instance-dependent label noise

## Executive Summary
This paper addresses the challenge of learning with noisy labels, specifically focusing on the difficulty of identifying and learning from clean hard examples (CHEs) that are often misclassified as noisy due to large losses. The proposed Late Stopping method leverages the intrinsic robust learning ability of deep neural networks (DNNs) by prolonging the training process. It gradually shrinks the noisy dataset by removing high-probability mislabeled examples while retaining the majority of CHEs. The method employs a novel sample selection criterion called First-time k-epoch Learning (FkL), which measures the number of epochs required for an example to be consistently and correctly classified. Experimental results on benchmark-simulated and real-world noisy datasets demonstrate that the proposed Late Stopping method outperforms state-of-the-art counterparts.

## Method Summary
The Late Stopping method prolongs training to exploit DNNs' intrinsic robust learning ability. It uses a novel sample selection criterion called First-time k-epoch Learning (FkL) that measures how many epochs an example takes to be consistently and correctly classified. The method iteratively trains classifiers on progressively cleaner datasets, removing high-probability mislabeled examples (those with high FkL values) while retaining clean hard examples. This creates a "from hard to easy" positive feedback loop that improves training set quality over iterations.

## Key Results
- Outperforms state-of-the-art methods like Me-Momentum, Co-teaching, and DMI on CIFAR-10 and CIFAR-100 with symmetric and instance-dependent noise
- Effectively retains clean hard examples while removing mislabeled ones
- Shows excellent performance when used as a pre-processing approach to reduce noise levels
- Achieves superior performance particularly on the more challenging CIFAR-100 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNNs exhibit an intrinsic robust learning ability that can be exploited by prolonging training.
- Mechanism: By continuing training past the point where generalization deteriorates, the model's learning dynamics reveal which examples are mislabeled based on how late they are consistently and correctly classified.
- Core assumption: The memorization effect causes DNNs to first learn clean patterns before overfitting noisy labels, and this effect is consistent enough across training to be used for sample selection.
- Evidence anchors:
  - [abstract] "Late Stopping method leverages the intrinsic robust learning ability of deep neural networks (DNNs) by prolonging the training process."
  - [section 3.1] "We empirically observe that mislabeled and clean examples exhibit differences in the number of epochs required for them to be consistently and correctly classified, and thus high-probability mislabeled examples can be removed."
  - [corpus] Weak. The corpus neighbors do not provide direct evidence about prolonged training exploiting robust learning ability.
- Break condition: If the memorization effect does not hold consistently across training runs, or if the gap between clean and mislabeled examples' learning times is not sufficient to enable reliable separation.

### Mechanism 2
- Claim: First-time k-epoch Learning (FkL) metric effectively distinguishes mislabeled from clean examples.
- Mechanism: Examples that are only consistently and correctly classified late in training (high FkL) are likely mislabeled, while those classified early (low FkL) are likely clean.
- Core assumption: The order in which examples are consistently classified correlates with their label correctness.
- Evidence anchors:
  - [section 3.1] "Using First-time k-epoch Learning(FkL) as a metric, we sequence the examples in the noisy dataset according to the order they meet the FkL metric during the training procedure... most mislabeled examples can only be classified into their given labels for consecutive k epochs in the later stages of training."
  - [section 4.1] "Our experimental results demonstrate that the FkL criterion outperforms the loss criterion in selecting both clean and mislabeled examples under the Late Stopping framework."
  - [corpus] Weak. The corpus does not provide specific evidence about the FkL metric's effectiveness.
- Break condition: If the correlation between FkL values and label correctness breaks down under certain noise patterns or dataset characteristics.

### Mechanism 3
- Claim: "From hard to easy" positive feedback loop progressively improves the quality of the training set while retaining clean hard examples.
- Mechanism: Instead of starting with a small clean set and expanding, Late Stopping starts with a large noisy set and iteratively removes high-probability mislabeled examples, allowing the model to learn from more clean hard examples throughout training.
- Core assumption: The robust learning ability extracted in each iteration can guide effective sample selection for the next iteration, even if the classifier's generalization performance is temporarily poor.
- Evidence anchors:
  - [section 3.2] "In each iteration, we train a new classifier on a new training set, using it solely to guide the sample selection for the current iteration... we exploit the robust learning capabilities of each new classifier... in its i iteration on Di from its learning dynamics, i.e., using the FkL criterion to select examples for the next iteration."
  - [section 4.2] "Our experimental results demonstrate that Late Stopping is an effective approach for retaining clean hard examples... on the more challenging CIFAR-100 dataset, Late Stopping significantly achieved better performance in selecting clean hard examples."
  - [corpus] Weak. The corpus does not provide specific evidence about the "from hard to easy" positive feedback loop.
- Break condition: If the sample selection guided by FkL in each iteration does not progressively improve the training set quality, or if too many clean examples are incorrectly removed in early iterations.

## Foundational Learning

- Concept: Memorization effect in DNNs
  - Why needed here: Understanding that DNNs first learn clean patterns before overfitting noisy labels is crucial for the Late Stopping framework to work.
  - Quick check question: What happens to a DNN's performance on clean vs. noisy examples as training progresses beyond the point of optimal generalization?

- Concept: Sample selection based on loss values
  - Why needed here: Late Stopping builds on and improves upon traditional small-loss sample selection methods by introducing the FkL criterion.
  - Quick check question: Why do clean hard examples often have large losses, making them difficult to distinguish from mislabeled examples using loss-based sample selection?

- Concept: Positive feedback loops in learning with noisy labels
  - Why needed here: Late Stopping employs a unique "from hard to easy" positive feedback loop that differs from conventional approaches.
  - Quick check question: How does the "from hard to easy" positive feedback loop in Late Stopping differ from the typical "from easy to hard" curriculum learning approach?

## Architecture Onboarding

- Component map: Noisy dataset -> Extended training -> FkL metric calculation -> Sample selection -> Updated training set -> Repeat

- Critical path:
  1. Initialize with noisy training set
  2. Train classifier for extended epochs
  3. Calculate FkL values for all examples
  4. Select high-probability mislabeled examples (high FkL)
  5. Remove selected examples and update training set
  6. Repeat from step 2 until stopping condition is met

- Design tradeoffs:
  - Longer training time vs. better retention of clean hard examples
  - Higher initial noise level in training set vs. more comprehensive learning from clean examples
  - Risk of overfitting to noise in late training stages vs. ability to exploit robust learning ability

- Failure signatures:
  - If FkL values do not effectively distinguish clean from mislabeled examples, the method will fail to improve training set quality
  - If too many clean examples are incorrectly removed in early iterations, the method may not converge to a good solution
  - If the prolonged training causes severe overfitting to noise, the method may not extract useful robust learning ability

- First 3 experiments:
  1. Implement FkL metric calculation and verify it distinguishes clean from mislabeled examples on a small synthetic dataset with known labels
  2. Test Late Stopping on a simple benchmark (e.g., CIFAR-10 with 20% symmetric noise) and compare performance with a baseline method
  3. Analyze the impact of the iteration rate (m%) on Late Stopping's performance and training time on a benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Late Stopping method perform on real-world noisy datasets with more complex noise patterns beyond symmetric and instance-dependent noise?
- Basis in paper: [explicit] The paper mentions that the method was tested on benchmark-simulated and real-world noisy datasets, but it does not provide results on real-world datasets with more complex noise patterns.
- Why unresolved: The paper only provides results on synthetic datasets with symmetric and instance-dependent noise, and one real-world dataset with human-annotated real-world noisy labels.
- What evidence would resolve it: Conducting experiments on real-world datasets with more complex noise patterns, such as label-dependent or feature-dependent noise, would provide insights into the method's performance in more challenging scenarios.

### Open Question 2
- Question: Can the First-time k-epoch Learning (FkL) criterion be extended to other machine learning tasks beyond classification, such as regression or anomaly detection?
- Basis in paper: [inferred] The paper introduces the FkL criterion for sample selection in the context of learning with noisy labels, but it does not explore its applicability to other machine learning tasks.
- Why unresolved: The paper focuses solely on the application of the FkL criterion in the context of learning with noisy labels and does not investigate its potential use in other machine learning tasks.
- What evidence would resolve it: Exploring the use of the FkL criterion in other machine learning tasks, such as regression or anomaly detection, and evaluating its effectiveness in those contexts would provide insights into its generalizability.

### Open Question 3
- Question: How does the performance of the Late Stopping method compare to other state-of-the-art methods in learning with noisy labels when applied to datasets with a larger number of classes or more complex feature spaces?
- Basis in paper: [inferred] The paper compares the performance of the Late Stopping method to other state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets, but it does not provide results on datasets with a larger number of classes or more complex feature spaces.
- Why unresolved: The paper focuses on the performance of the Late Stopping method on relatively simple datasets with a limited number of classes and straightforward feature spaces.
- What evidence would resolve it: Conducting experiments on datasets with a larger number of classes or more complex feature spaces, such as ImageNet or COCO, and comparing the performance of the Late Stopping method to other state-of-the-art methods would provide insights into its scalability and effectiveness in more challenging scenarios.

## Limitations

- Performance on extremely high noise rates (>50%) is not explored
- Specific hyperparameters (k value in FkL, iteration rate m%) are not explicitly defined
- Method relies on consistency of memorization effect which may vary across architectures
- Limited evaluation on real-world noisy datasets with unknown noise patterns

## Confidence

- **High confidence**: Experimental results demonstrating superior performance compared to baseline methods on benchmark datasets with controlled noise rates
- **Medium confidence**: Theoretical mechanism explaining how prolonged training exploits robust learning ability
- **Medium confidence**: Effectiveness of the FkL criterion in distinguishing clean from mislabeled examples

## Next Checks

1. Conduct hyperparameter sensitivity analysis by systematically varying the k value in the FkL criterion and iteration rate m% to determine their impact on performance and identify optimal settings for different noise rates and dataset characteristics.

2. Test Late Stopping with different DNN architectures (e.g., Vision Transformers, WideResNets) to verify the method's generalizability beyond the ResNet models used in the paper.

3. Evaluate the method on additional real-world noisy datasets with unknown noise patterns to assess its practical applicability beyond synthetic noise scenarios.