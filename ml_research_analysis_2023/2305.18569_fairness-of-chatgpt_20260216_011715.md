---
ver: rpa2
title: Fairness of ChatGPT
arxiv_id: '2305.18569'
source_url: https://arxiv.org/abs/2305.18569
tags:
- fairness
- prompts
- chatgpt
- llms
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic fairness evaluation of ChatGPT
  across four high-stakes domains: education, criminology, finance, and healthcare.
  The study assesses both group-level fairness (statistical parity, equal opportunity,
  equalized odds, overall accuracy/f1/auc equality) and individual-level fairness
  (counterfactual fairness).'
---

# Fairness of ChatGPT

## Quick Facts
- arXiv ID: 2305.18569
- Source URL: https://arxiv.org/abs/2305.18569
- Reference count: 3
- Key outcome: ChatGPT achieves comparable effectiveness to smaller baseline models while still exhibiting fairness issues across education, criminology, finance, and healthcare domains.

## Executive Summary
This paper presents a systematic fairness evaluation of ChatGPT across four high-stakes domains using both group-level and individual-level fairness metrics. The study compares ChatGPT's performance against smaller baseline models (logistic regression and MLP) while systematically varying prompt designs with biased and unbiased in-context examples. Results show that while ChatGPT achieves comparable effectiveness to smaller models, it still exhibits unfairness issues at both group and individual levels, with approximately 5-15% of individuals receiving different decisions under factual vs. counterfactual scenarios. The study finds that prompt design significantly influences model outputs, highlighting the need for careful prompt engineering.

## Method Summary
The study evaluates fairness across four datasets (PISA for education, COMPAS for criminology, German Credit for finance, and Heart Disease for healthcare) using both baseline models (logistic regression and MLP) and ChatGPT. The evaluation employs eight different prompts with varying example patterns (four biased, four unbiased) to assess how prompt design affects fairness outcomes. Both group-level fairness metrics (statistical parity, equal opportunity, equalized odds, overall accuracy/F1/AUC equality) and individual-level counterfactual fairness are measured. The method involves comparing ChatGPT's performance with baseline models across effectiveness metrics (accuracy, F1, AUC) and fairness metrics for each dataset.

## Key Results
- ChatGPT achieves comparable effectiveness to baseline models (LR and MLP) on accuracy, F1, and AUC metrics across all four domains
- Group-level unfairness exists in both small and large models, though ChatGPT generally performs better than baselines on fairness metrics
- Individual-level unfairness is present, with approximately 5-15% of individuals receiving different decisions under factual vs. counterfactual scenarios
- Prompt design significantly influences model outputs, with different prompts producing varying fairness outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering significantly influences model fairness and effectiveness
- Mechanism: The study systematically varies in-context examples (biased vs unbiased) and observes that different prompts produce different model outputs, indicating that prompt design is a key lever for controlling model behavior
- Core assumption: ChatGPT's outputs are sensitive to the examples provided in prompts
- Evidence anchors:
  - [abstract]: "The study finds that prompt design significantly influences model outputs, highlighting the need for careful prompt engineering"
  - [section 3.3]: Describes eight different prompts with varying example patterns and explicitly states "we observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts"
  - [corpus]: Weak evidence - no direct citations about prompt engineering effects on fairness in the neighboring papers

### Mechanism 2
- Claim: Large language models can achieve comparable effectiveness to smaller models while potentially improving fairness
- Mechanism: ChatGPT performs similarly to logistic regression and MLP baselines on effectiveness metrics (accuracy, F1, AUC) while showing better group and individual fairness in most cases
- Core assumption: LLMs can be as effective as smaller models for fairness-sensitive tasks when given appropriate prompts
- Evidence anchors:
  - [abstract]: "Results show that while ChatGPT achieves comparable effectiveness to smaller models (logistic regression and MLP), it still exhibits unfairness issues"
  - [section 4]: Tables show ChatGPT's effectiveness metrics are generally comparable to baselines, with explicit comparison statements
  - [corpus]: Weak evidence - neighboring papers don't provide direct comparisons between LLMs and smaller models on fairness metrics

### Mechanism 3
- Claim: Individual-level unfairness persists even when group-level fairness is addressed
- Mechanism: The study uses counterfactual fairness metrics to show that approximately 5-15% of individuals receive different decisions when sensitive features are flipped, indicating that individual fairness is a distinct concern from group fairness
- Core assumption: Counterfactual fairness captures a meaningful dimension of fairness that is not addressed by group fairness metrics
- Evidence anchors:
  - [abstract]: "Individual-level unfairness is also present, with approximately 5-15% of individuals receiving different decisions under factual vs. counterfactual scenarios"
  - [section 3.4]: Detailed explanation of counterfactual fairness metric and methodology for evaluating it
  - [corpus]: Weak evidence - no direct citations about counterfactual fairness in neighboring papers

## Foundational Learning

- Concept: Group fairness metrics (statistical parity, equal opportunity, equalized odds)
  - Why needed here: The paper evaluates multiple group fairness dimensions to comprehensively assess model fairness across different protected groups
  - Quick check question: Can you explain the difference between equal opportunity and equalized odds fairness?

- Concept: Individual fairness and counterfactual fairness
  - Why needed here: The study goes beyond group-level metrics to assess whether similar individuals are treated similarly, which is crucial for high-stakes applications
  - Quick check question: How does counterfactual fairness differ from traditional individual fairness definitions?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The paper demonstrates that prompt design significantly affects model outputs, making this a critical skill for fair LLM deployment
  - Quick check question: Why might removing sensitive features from prompts (Prompt 2) not necessarily improve fairness outcomes?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (LR, MLP) -> Prompt design -> LLM inference -> Fairness evaluation (group + individual metrics)
- Critical path: Prompt design -> LLM inference -> Fairness evaluation, as this determines whether the system meets fairness requirements
- Design tradeoffs: Between effectiveness (accuracy/F1) and fairness, and between different fairness definitions that may conflict
- Failure signatures: High disparity metrics, inconsistent outputs across similar prompts, low counterfactual fairness scores
- First 3 experiments:
  1. Test prompt variations on a small subset to observe output consistency before full-scale evaluation
  2. Compare group fairness metrics across demographic subgroups to identify systematic biases
  3. Evaluate counterfactual fairness on critical decision points to assess individual-level fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number and ordering of in-context examples in prompts affect the fairness performance of large language models like ChatGPT?
- Basis in paper: [explicit] The paper mentions this as a future research direction, stating "studying the impact of the number and order of in-context examples on fairness" as an area needing further investigation.
- Why unresolved: The current study only uses a fixed number of four examples in prompts and does not systematically vary their order or quantity to assess impact on fairness outcomes.
- What evidence would resolve it: Experiments varying the number of examples (e.g., 1, 2, 4, 8) and their positions in prompts, measuring resulting changes in fairness metrics across different sensitive attributes.

### Open Question 2
- Question: What prompt engineering strategies can be developed to simultaneously optimize both model fairness and accuracy in high-stakes domains?
- Basis in paper: [explicit] The authors call for "further efforts to better understand the influence of prompts on the fairness of LLMs" and specifically mention "how to design or learn prompts to achieve better model fairness and accuracy."
- Why unresolved: The current study only tests a limited set of hand-crafted prompts and observes mixed results without establishing clear patterns for optimal prompt design.
- What evidence would resolve it: Systematic testing of diverse prompt engineering techniques (instruction tuning, example selection strategies, template variations) with automated evaluation of fairness-accuracy trade-offs.

### Open Question 3
- Question: How transferable are fairness patterns across different high-stakes domains when applying large language models?
- Basis in paper: [inferred] The study examines four domains (education, criminology, finance, healthcare) but doesn't analyze whether fairness issues in one domain predict issues in others, suggesting this comparative analysis hasn't been done.
- Why unresolved: The paper presents domain-specific results separately without cross-domain comparative analysis to identify universal versus domain-specific fairness challenges.
- What evidence would resolve it: Meta-analysis correlating fairness metrics across domains, identifying which fairness problems persist across domains versus those unique to specific contexts.

## Limitations

- The exact prompts used in the paper are not fully specified, making exact reproduction difficult
- The study does not address whether fairness improvements are sustainable across different versions of ChatGPT or other LLMs
- The paper does not systematically analyze how varying the number and order of in-context examples affects fairness outcomes

## Confidence

- **High confidence**: ChatGPT achieving comparable effectiveness to smaller baseline models is well-supported by reported metrics and direct comparisons across multiple datasets
- **Medium confidence**: Prompt design significantly influencing fairness outcomes is supported by systematic variation, but lack of exact prompt specifications reduces reproducibility confidence
- **Medium confidence**: Individual-level unfairness persisting despite group-level fairness improvements is supported by counterfactual fairness analysis, but variability (5-15% range) suggests substantial dataset-dependent differences

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary prompt formulations (example count, example quality, prompt structure) on a subset of the data to quantify the stability of fairness metrics across prompt variations.

2. **Cross-model generalization**: Test the same fairness evaluation pipeline with other LLMs (e.g., Claude, Gemini) to determine whether the prompt sensitivity and fairness patterns observed with ChatGPT are generalizable across different language models.

3. **Longitudinal evaluation**: Evaluate the same fairness metrics across different versions of ChatGPT (if available) to assess whether fairness improvements are stable over time or subject to model updates that may alter behavior.