---
ver: rpa2
title: A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks
arxiv_id: '2307.01951'
source_url: https://arxiv.org/abs/2307.01951
tags:
- training
- graph
- plots
- gnns
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies feature evolution in Graph Neural Networks (GNNs)
  for node-wise classification, using the Neural Collapse (NC) phenomenon as a lens.
  The authors show empirically that within-class feature variability decreases during
  training, but not to the same extent as in plain DNNs for image classification.
---

# A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks

## Quick Facts
- arXiv ID: 2307.01951
- Source URL: https://arxiv.org/abs/2307.01951
- Authors: 
- Reference count: 40
- One-line primary result: GNNs exhibit partial feature collapse during training, explained by strict graph structural conditions and gradient dynamics.

## Executive Summary
This paper investigates feature evolution in Graph Neural Networks (GNNs) for node-wise classification through the lens of Neural Collapse (NC). The authors empirically demonstrate that while GNNs show decreasing within-class feature variability during training, they do not achieve the same degree of collapse as plain deep neural networks in image classification. They provide theoretical justification using a graph-based unconstrained features model (gUFM), showing that exact feature collapse requires strict structural conditions on the graph that are rarely satisfied in practice. The partial collapse observed empirically is explained through analysis of gradient dynamics.

## Method Summary
The authors study GNNs on stochastic block model (SBM) graphs with N nodes, C balanced communities, and parameters p and q controlling intra- and inter-community edge probabilities. They train GNNs using SGD with momentum and weight decay to minimize MSE loss, tracking Neural Collapse metrics (NC1, gNC1, NC2, NC3) during training and across layers during inference. The theoretical analysis introduces a gUFM that extends the unconstrained features model to include graph structure, proving conditions for exact collapse and analyzing gradient dynamics. The empirical results are compared with spectral clustering methods using power iterations on normalized Laplacian and Bethe-Hessian matrices.

## Key Results
- GNNs exhibit partial feature collapse during training, with within-class feature variability decreasing but not reaching the exact collapse seen in plain DNNs
- Exact collapse in gUFM requires a strict structural condition on the graph (condition C) that is rarely satisfied under SBM
- Gradient flow analysis explains partial collapse through decreasing within-class covariance and increasing between-class covariance
- Feature evolution across GNN layers shows decreasing NC metrics, contrasting with spectral clustering where ratios remain constant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exact neural collapse in GNNs requires graphs to satisfy a strict structural condition on the ratio of neighbors per class (condition C).
- Mechanism: The training objective can only reach exact collapse when, for each node in a class, the distribution of neighbor classes is identical across all nodes in that class. This ensures that neighborhood-aggregated features collapse to the class mean, matching the intra-class feature collapse.
- Core assumption: The graph convolution layer's aggregation step behaves like a linear combination of node features weighted by the graph's normalized adjacency matrix.
- Evidence anchors:
  - [abstract]: "the graphs obey a strict structural condition in order to possess a minimizer with exact collapse"
  - [section]: Theorem 3.1 states that collapsed minimizers require condition C; Theorem 3.2 shows such graphs are rare under SBM
  - [corpus]: Weak; corpus neighbors discuss related GNN collapse work but not the structural condition per se.
- Break condition: If the graph structure violates condition C, e.g., nodes in the same class have different neighbor class distributions, exact collapse is impossible.

### Mechanism 2
- Claim: Partial collapse occurs during training due to gradient dynamics even when exact collapse is impossible.
- Mechanism: The gradient flow of the gUFM shows that, under perturbations around the expected graph structure, within-class covariance decreases and between-class covariance increases along training, leading to partial collapse.
- Core assumption: The gradient flow analysis assumes a small perturbation of the expected adjacency matrix and smooth dynamics.
- Evidence anchors:
  - [abstract]: "by studying the gradient dynamics of the theoretical model, we provide reasoning for the partial collapse observed empirically"
  - [section]: Theorem 3.3 shows Tr(ΣW) decreases and Tr(ΣB) increases along the gradient flow for perturbed graphs
  - [corpus]: Weak; corpus does not discuss gradient dynamics explicitly.
- Break condition: If the perturbation is too large or the graph is too irregular, the gradient flow may not exhibit the same decay pattern.

### Mechanism 3
- Claim: The depthwise evolution of NC metrics in GNNs resembles but differs from spectral clustering power iterations due to the trainable weight matrices.
- Mechanism: In GNNs, each layer's feature covariance ratios decrease across depth due to the interaction between weight matrices and graph convolution, whereas in spectral methods the ratios remain constant because they are purely structural.
- Core assumption: The weight matrices W1 and W2 interact multiplicatively with the graph operator and feature covariances, modulating their evolution.
- Evidence anchors:
  - [abstract]: "study on the evolution of within- and between-class feature variability across layers of a well-trained GNN and contrast the behavior with spectral methods"
  - [section]: Theorem 4.1 bounds the ratios Tr(ΣB(X(l)))/Tr(ΣB(H(l−1))) and Tr(ΣW(X(l)))/Tr(ΣW(H(l−1))) in terms of eigenvalues of weight matrices
  - [corpus]: Weak; corpus focuses on GNN collapse but not on the spectral comparison specifically.
- Break condition: If the weight matrices are constrained or fixed (e.g., in spectral methods), the ratio behavior reverts to constancy.

## Foundational Learning

- Concept: Stochastic Block Model (SBM)
  - Why needed here: The paper uses SBM to generate controlled graph topologies with tunable homophily/heterophily for studying feature evolution.
  - Quick check question: What are the two key parameters in SBM that control edge probabilities between same-class vs different-class nodes?

- Concept: Neural Collapse (NC) metrics (NC1, NC2, NC3)
  - Why needed here: These metrics quantify the degree of feature collapse and alignment to geometric structures during training.
  - Quick check question: How does NC1 differ from gNC1 in terms of what covariance matrices they use?

- Concept: Unconstrained Features Model (UFM)
  - Why needed here: UFM treats the deepest features as freely optimizable variables, providing an optimistic baseline for studying collapse; gUFM extends this to include graph structure.
  - Quick check question: In plain UFM, what is the condition for exact collapse?

## Architecture Onboarding

- Component map: Data generator -> SBM graphs -> GNN backbone -> Feature tracker -> Theoretical model -> Evaluation
- Critical path:
  1. Generate SBM graphs → 2. Train GNN until zero training error → 3. Track NC metrics per layer during training → 4. Analyze gUFM conditions for exact collapse → 5. Study gradient flow for partial collapse → 6. Compare depthwise behavior to spectral methods
- Design tradeoffs:
  - Using MSE vs cross-entropy loss: MSE simplifies theoretical analysis but may affect empirical convergence.
  - Including identity operator I in F: Adds linear feature mixing, delays collapse compared to F'.
  - Balanced classes assumption: Simplifies metric computation but may not reflect all real datasets.
- Failure signatures:
  - NC metrics plateau above zero: Indicates lack of exact collapse due to graph structure.
  - No overlap improvement during training: Suggests GNN cannot reach TPT on given SBM.
  - High variance of NC metrics across graphs: May indicate sensitivity to graph sampling or hyperparameters.
- First 3 experiments:
  1. Train GNN on SBM(N=1000, C=2, p=0.025, q=0.0017) and plot NC1 metrics vs epoch.
  2. Modify graph to satisfy condition C (e.g., regular subgraph within classes) and observe if NC1 metrics drop more.
  3. Replace graph convolution with identity operator only (F' = {I}) and compare depthwise NC evolution to spectral clustering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the partial neural collapse observed in GNNs be theoretically quantified in terms of the graph structural properties (e.g., eigenvalues, community connectivity)?
- Basis in paper: [explicit] The paper shows that exact collapse requires a strict structural condition on the graph, which is rarely satisfied. The partial collapse is linked to gradient dynamics, but a precise quantitative relationship between graph structure and collapse degree is not provided.
- Why unresolved: The gUFM analysis identifies necessary conditions but does not offer a closed-form expression or bounds for how much collapse occurs given arbitrary graph properties.
- What evidence would resolve it: Analytical bounds or empirical correlations between graph spectral metrics (e.g., spectral gap, Cheeger constant) and observed NC1 values across diverse graph families.

### Open Question 2
- Question: Does the choice of normalization (e.g., random walk vs symmetric) in GNN operators affect the extent of neural collapse differently, and if so, why?
- Basis in paper: [explicit] The paper contrasts GNN architectures with and without identity operators and studies neighborhood-aggregated features, but does not explore the effect of different normalization schemes on collapse behavior.
- Why unresolved: While the paper provides insights into collapse for specific operator families, it does not compare the impact of alternative normalization strategies on feature variability collapse.
- What evidence would resolve it: Systematic experiments varying normalization methods across datasets and theoretical analysis of how normalization affects the gUFM minimizer properties.

### Open Question 3
- Question: Is there a principled connection between the degree of neural collapse in GNNs and their generalization performance, analogous to the role of neural collapse in plain DNNs?
- Basis in paper: [explicit] The paper contrasts feature evolution with spectral clustering and highlights the role of depth, but does not empirically or theoretically link the degree of collapse to generalization metrics like test accuracy or robustness.
- Why unresolved: While the paper suggests that partial collapse is inherent to GNNs due to graph structure, it does not investigate whether this partial collapse correlates with or predicts generalization behavior.
- What evidence would resolve it: Empirical studies correlating NC metrics with generalization gaps across graph datasets, and theoretical analysis of how collapse relates to margin-based generalization bounds in the GNN context.

## Limitations
- The theoretical analysis assumes balanced classes, which may not hold in real-world graphs
- The structural condition C for exact collapse is rarely satisfied, limiting practical applicability
- The analysis focuses on specific graph operators (I and bA) without exploring alternative normalization schemes

## Confidence
- High: Empirical observation of partial collapse during training
- Medium: Theoretical conditions for exact collapse in gUFM
- Medium: Gradient flow analysis explaining partial collapse
- Medium: Depthwise NC evolution comparison with spectral methods

## Next Checks
1. Test condition C on real-world graphs with known community structure (e.g., Cora, Citeseer) and measure deviation from exact collapse
2. Implement alternative graph operators (Chebyshev polynomials, GAT attention) and verify if the partial collapse pattern persists
3. Replace MSE loss with cross-entropy and observe changes in both training dynamics and theoretical conditions