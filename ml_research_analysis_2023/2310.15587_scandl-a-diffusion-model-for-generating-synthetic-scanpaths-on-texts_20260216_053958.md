---
ver: rpa2
title: 'ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts'
arxiv_id: '2310.15587'
source_url: https://arxiv.org/abs/2310.15587
tags:
- reading
- sentence
- scan
- human
- reader
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a diffusion model, SCAN DL, to generate synthetic
  scanpaths on texts for the first time. By leveraging pre-trained word representations
  and jointly embedding both the stimulus text and the fixation sequence, our model
  captures multi-modal interactions between the two inputs.
---

# ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts

## Quick Facts
- arXiv ID: 2310.15587
- Source URL: https://arxiv.org/abs/2310.15587
- Reference count: 40
- Key outcome: Proposes SCAN DL, a diffusion model that generates synthetic scanpaths on texts by leveraging pre-trained word representations and joint embedding of text and fixation sequences, significantly outperforming state-of-the-art methods.

## Executive Summary
This paper introduces SCAN DL, the first diffusion model for generating synthetic scanpaths on texts. The model jointly embeds both the stimulus text and fixation sequence using pre-trained BERT representations, capturing multi-modal interactions between linguistic and oculomotor information. Through extensive evaluation within and across datasets, SCAN DL demonstrates superior performance compared to existing scanpath generation methods while exhibiting human-like reading behavior patterns.

## Method Summary
SCAN DL is a discrete sequence-to-sequence diffusion model that generates synthetic scanpaths conditioned on text. The model uses a joint embedding function combining word indices, BERT embeddings, and positional information. During training, only the fixation sequence is partially noised while text embeddings remain fixed, allowing the model to learn the mapping from text to scanpath. The sqrt noise schedule with importance sampling is employed for efficient sampling, and the model is trained using a variational lower bound-based loss with three components: reconstruction loss, embedding consistency loss, and rounding loss.

## Key Results
- Significantly outperforms state-of-the-art scanpath generation methods on NLD metrics
- Demonstrates strong within-dataset performance (5-fold cross-validation on CELER)
- Shows successful cross-dataset generalization (CELER→ZuCo transfer)
- Exhibits human-like reading behavior patterns in psycholinguistic analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint embedding of text and scanpath in continuous space captures multi-modal dependencies.
- Mechanism: The embedding function EMB(x) = EMBidx(xidx) + EMBbert(xbert) + EMBpos(xpos) learns a shared representation where text semantic and positional features are combined with fixation sequence indices.
- Core assumption: The text and scanpath are sufficiently aligned in the concatenated discrete input representation for the joint embedding to capture meaningful cross-modal relationships.
- Evidence anchors: [abstract] mentions capturing multi-modal interactions; [section] details the embedding function components.

### Mechanism 2
- Claim: Partial noising of only the fixation sequence while keeping text uncorrupted allows the model to learn the mapping from text to scanpath.
- Mechanism: In forward process, latent variable splits into zw (unchanged sentence subsequence) and zf (noised fixation subsequence), with only zf corrupted by noise.
- Core assumption: The model can learn to denoise the fixation sequence conditioned on the uncorrupted text representation.
- Evidence anchors: [abstract] describes it as a discrete sequence-to-sequence diffusion model; [section] details the noising process with specific equations.

### Mechanism 3
- Claim: The sqrt noise schedule and importance sampling for T provide an effective trade-off between reconstruction accuracy and computational efficiency.
- Mechanism: Uses sqrt noise schedule βt = 1 − √(t/T̃ + s) with importance sampling to determine diffusion steps.
- Core assumption: The sqrt noise schedule and importance sampling provide a good balance for the model's learning and inference.
- Evidence anchors: [abstract] specifies the sqrt noise schedule parameters; [section] describes the importance sampling method.

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: SCAN DL is a diffusion model adapted for discrete sequence-to-sequence problems, specifically generating scanpaths conditioned on text.
  - Quick check question: What are the key components of a diffusion model (forward process, reverse process, noise schedule)?

- Concept: Joint embeddings
  - Why needed here: The model needs to learn a shared representation of the text and scanpath to capture their multi-modal interactions.
  - Quick check question: How does the embedding function EMB(x) combine the different input features (word indices, BERT embeddings, position indices)?

- Concept: Partial noising
  - Why needed here: To learn the mapping from text to scanpath, the model needs to denoise the fixation sequence conditioned on the uncorrupted text.
  - Quick check question: Why is it important that only the fixation sequence is noised, while the text remains uncorrupted?

## Architecture Onboarding

- Component map: Input → Embedding layer (EMBidx, EMBbert, EMBpos) → Diffusion model (forward process with partial noising) → Diffusion model (reverse process with denoising) → Output
- Critical path: Input → Embedding → Diffusion model (forward process) → Diffusion model (reverse process) → Output
- Design tradeoffs: Using pre-trained BERT vs. learning embeddings from scratch; partial noising of only fixation sequence vs. noising both; sqrt noise schedule vs. other schedules
- Failure signatures: Poor alignment between word indices and fixation positions; overfitting to noise schedule; ineffective denoising of fixation sequence
- First 3 experiments:
  1. Ablation study: Remove EMBbert and EMBpos to assess importance of pre-trained embeddings and positional information
  2. Noise schedule comparison: Compare sqrt with linear and cosine noise schedules
  3. Unconditional scanpath generation: Remove sentence condition to assess importance of linguistic information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SCAN DL's performance on task-specific reading datasets compare to its performance on natural reading datasets?
- Basis in paper: [explicit] The paper notes uncertainty about generalization to task-specific datasets that may provide more informative scanpaths for NLP downstream tasks.
- Why unresolved: The paper explicitly states this as an open question, indicating task-specific evaluations were not conducted.
- What evidence would resolve it: Direct comparison of SCAN DL's performance on both natural reading datasets and task-specific reading datasets, measuring NLD and other relevant metrics.

### Open Question 2
- Question: Can SCAN DL be extended to predict fixation durations in addition to fixation positions?
- Basis in paper: [explicit] The paper acknowledges this as a current limitation, stating SCAN DL is unable to predict fixation durations.
- Why unresolved: The paper identifies this as a shortcoming but does not propose a solution or test feasibility within the current framework.
- What evidence would resolve it: Experimental results showing SCAN DL's performance when modified to include duration prediction, evaluated against datasets with duration annotations.

### Open Question 3
- Question: How sensitive is SCAN DL's performance to different noise schedules, and what theoretical properties make the sqrt schedule optimal?
- Basis in paper: [inferred] The ablation study compares sqrt, linear, and cosine noise schedules, finding sqrt performs best, but no explanation of why or exploration of other schedules.
- Why unresolved: While empirical comparison is provided, there is no theoretical analysis of why sqrt schedule might be superior or exploration beyond the three tested.
- What evidence would resolve it: Theoretical analysis of noise schedule properties and systematic testing of a broader range of noise schedules with corresponding performance metrics.

## Limitations

- The psycholinguistic validity relies on correlational analysis that cannot definitively prove the model captures human-like reading strategies versus statistical artifacts
- Cross-dataset evaluation shows performance drop when transferring between datasets, suggesting potential capture of dataset-specific rather than universal reading patterns
- The diffusion model's success critically depends on the assumption that partial noising effectively separates the two modalities, with limited empirical validation of this assumption

## Confidence

High confidence: Technical implementation of the discrete diffusion model architecture and training procedure
Medium confidence: Quantitative evaluation showing SCAN DL outperforms existing methods on NLD metrics
Low confidence: Psycholinguistic interpretation of the model's behavior and its implications for human reading strategies

## Next Checks

1. **Ablation study on partial noising**: Systematically test variants where text embeddings are also noised to quantify the importance of the partial noising assumption. Compare performance when using random initialization vs. pre-trained BERT embeddings for the text component.

2. **Controlled text manipulation experiments**: Generate scanpaths on texts where known psycholinguistic properties are systematically manipulated (e.g., controlled vocabulary, sentence structure). Compare the model's sensitivity to these manipulations against human data to validate the claimed mechanisms.

3. **Cross-lingual generalization test**: Evaluate SCAN DL on eye-tracking data from a different language family (e.g., German or Chinese) to assess whether the model captures language-general reading strategies or is overfitting to English-specific patterns learned from BERT embeddings.