---
ver: rpa2
title: Principled Approaches for Learning to Defer with Multiple Experts
arxiv_id: '2310.14774'
source_url: https://arxiv.org/abs/2310.14774
tags:
- loss
- learning
- surrogate
- alt1
- h-consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning to defer to multiple
  experts, where a model can choose to either predict a label or defer to one of several
  pre-defined experts. The authors introduce a new family of surrogate losses specifically
  designed for this multiple-expert setting, where prediction and deferral functions
  are learned simultaneously.
---

# Principled Approaches for Learning to Defer with Multiple Experts

## Quick Facts
- **arXiv ID**: 2310.14774
- **Source URL**: https://arxiv.org/abs/2310.14774
- **Reference count**: 40
- **Primary result**: Introduces H-consistent surrogate losses for learning to defer to multiple experts, showing improved system accuracy on SVHN and CIFAR-10 datasets

## Executive Summary
This paper addresses the problem of learning to defer to multiple experts in classification tasks. The authors propose a new family of surrogate losses specifically designed for the multiple-expert setting, where a model can either predict a label or defer to one of several pre-defined experts. The key contribution is proving strong H-consistency bounds for these losses, which are tighter than traditional Bayes-consistency guarantees. The approach leads to new learning-to-defer algorithms that simultaneously learn prediction and deferral functions. Experimental results on SVHN and CIFAR-10 datasets demonstrate improved system accuracy as the number of experts increases.

## Method Summary
The method introduces a deferral surrogate loss constructed by combining a base multi-class classification surrogate loss with weighted deferral costs. The loss formula L(h, x, y) = ℓ(h, x, y) + ∑ⱼ(1 - cj(x, y)) ℓ(h, x, n + j) enables simultaneous learning of prediction and deferral decisions. The approach proves H-consistency bounds that relate deferral loss regret to surrogate loss regret plus a hypothesis-set-specific minimizability gap. Learning bounds are derived using Rademacher complexity of the hypothesis set. The experiments use ResNet architectures of varying depths as base models and experts, trained with Adam optimizer on SVHN and CIFAR-10 datasets.

## Key Results
- Proved H-consistency bounds for multiple-expert deferral surrogate losses, tighter than traditional Bayes-consistency
- Demonstrated explicit H-consistency bounds for exponential, logistic, generalized cross-entropy, and mean absolute error losses
- Showed improved system accuracy on SVHN and CIFAR-10 datasets as number of experts increases
- Provided learning bounds connecting finite-sample generalization to surrogate estimation error and model complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The deferral surrogate loss L is constructed by combining a base surrogate loss ℓ for multi-class classification with weighted deferral costs (1 - cj(x,y)).
- Mechanism: The formula L(h, x, y) = ℓ(h, x, y) + ∑ⱼ(1 - cj(x, y)) ℓ(h, x, n + j) ensures that deferral decisions are learned simultaneously with prediction, with deferral costs modulating the influence of each expert's surrogate loss term.
- Core assumption: The base loss ℓ is H-consistent for standard multi-class classification, and deferral costs cj(x, y) are bounded between [cj, cj].
- Evidence anchors:
  - [section]: "Since the last term ∑ⱼ(1 - cj(x, y)) does not depend on h, if ℓ is a surrogate loss for the zero-one multi-class classification loss over the augmented label set Y, then L, defined as follows for any h ∈ H and (x, y) ∈ X × Y, is a natural surrogate loss for Ldef"
  - [abstract]: "We then prove that these surrogate losses benefit from strong H-consistency bounds"
- Break condition: If the base loss ℓ is not H-consistent, or if deferral costs are unbounded, the theoretical guarantees fail.

### Mechanism 2
- Claim: H-consistency bounds provide stronger and more relevant guarantees than Bayes-consistency for finite hypothesis sets.
- Mechanism: The bound ELdef(h) - E*Ldef(H) ≤ f(EL(h) - E*L(H) + ML(H)) relates deferral loss regret to surrogate loss regret plus a hypothesis-set-specific minimizability gap ML(H).
- Core assumption: The surrogate loss ℓ admits an H-consistency bound with respect to the zero-one loss.
- Evidence anchors:
  - [section]: "H-consistency bounds, introduced by Awasthi, Mao, Mohri, and Zhong (2022b,a), are new consistency guarantees that, unlike Bayes-consistency or excess error bound, take into account the specific hypothesis set H and do not assume H to be the family of all measurable functions."
  - [abstract]: "We then prove that these surrogate losses benefit from strong H-consistency bounds"
- Break condition: If the hypothesis set H is too restrictive, the minimizability gap ML(H) becomes large, weakening the bound.

### Mechanism 3
- Claim: Learning bounds for the deferral loss can be derived from the Rademacher complexity of the hypothesis set HL.
- Mechanism: The final bound ELdef(hS) - E*Ldef(H) ≤ (ne + 1 - ∑cj)Γ(4RLm(H) + 2BL√log(2/δ)/2m + ML(H)/(ne + 1 - ∑cj)) connects finite-sample generalization to surrogate estimation error and model complexity.
- Core assumption: The surrogate loss L is bounded by BL and the Rademacher complexity RLm(H) is finite.
- Evidence anchors:
  - [section]: "Given an H-consistency bound in the form of (5), we can further use it to derive a learning bound for the deferral loss by upper bounding the surrogate estimation error EL(hS) - E*L(H) with the complexity of the family of functions associated with L and H: HL = {(x, y) ↦ L(h, x, y) : h ∈ H}"
  - [abstract]: "These loss functions readily lead to the design of new learning to defer algorithms based on their minimization"
- Break condition: If the hypothesis set is too complex, RLm(H) becomes large, leading to loose generalization bounds.

## Foundational Learning

- Concept: H-consistency bounds
  - Why needed here: Provide non-asymptotic, hypothesis-set-specific guarantees that are stronger than Bayes-consistency for finite H
  - Quick check question: What is the key difference between H-consistency and Bayes-consistency?

- Concept: Minimizability gap
  - Why needed here: Quantifies the gap between best-in-class error and the infimum over all measurable functions, providing a finer measure than approximation error
  - Quick check question: How does the minimizability gap differ from the approximation error?

- Concept: Rademacher complexity
  - Why needed here: Provides a measure of hypothesis set complexity used to derive finite-sample learning bounds
  - Quick check question: What does Rademacher complexity measure in the context of learning theory?

## Architecture Onboarding

- Component map:
  - Base model (predictor): ResNet-4 trained on original task
  - Deferral models: Multiple expert models (ResNet-10, 16, 28) pre-trained on task
  - Deferral loss: L(h, x, y) = ℓ(h, x, y) + ∑ⱼ(1 - cj(x, y)) ℓ(h, x, n + j)
  - Cost functions: cj(x, y) ∈ [cj, cj], can be misclass rate or misclass + base cost
  - Training loop: Simultaneous training of predictor and deferral models using Adam optimizer

- Critical path:
  1. Pre-train expert models on training data
  2. Initialize predictor and deferral models (ResNet-4)
  3. Compute deferral loss using formula (4)
  4. Optimize using Adam with learning rate tuned for dataset
  5. Evaluate system accuracy and deferral ratio

- Design tradeoffs:
  - Number of experts vs. system accuracy: More experts generally improve accuracy but increase complexity
  - Cost function choice: Misclass rate only vs. misclass + base cost affects deferral behavior
  - Hypothesis set complexity: More complex H reduces minimizability gap but increases Rademacher complexity

- Failure signatures:
  - Low deferral ratio: Costs too high or experts not significantly better than predictor
  - High deferral ratio: Costs too low or experts significantly better than predictor
  - Poor accuracy: Base model too weak, experts poorly calibrated, or optimization issues

- First 3 experiments:
  1. Single expert scenario: Compare ResNet-10 expert vs. no expert on SVHN/CIFAR-10
  2. Two experts scenario: Compare ResNet-10 + ResNet-16 experts vs. single expert
  3. Three experts scenario: Compare ResNet-10 + ResNet-16 + ResNet-28 experts vs. two experts

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the proposed surrogate losses perform in terms of H-consistency compared to existing methods in more complex real-world scenarios with noisy data and non-stationary distributions?
  - Basis in paper: [explicit] The paper introduces a new family of surrogate losses and proves H-consistency bounds, but experimental results are limited to SVHN and CIFAR-10 datasets.
  - Why unresolved: The theoretical guarantees are established, but the practical performance in more challenging scenarios remains unexplored.
  - What evidence would resolve it: Conducting experiments on diverse real-world datasets with varying levels of noise and distributional shifts, comparing the proposed losses to state-of-the-art methods.

- **Open Question 2**: Can the minimizability gap be further reduced by incorporating additional regularization techniques or modifying the hypothesis set H?
  - Basis in paper: [explicit] The paper highlights the importance of the minimizability gap and its potential to lead to more favorable guarantees than the approximation error.
  - Why unresolved: The analysis focuses on the theoretical properties of the proposed losses, but the impact of different regularization strategies or hypothesis sets on the minimizability gap is not explored.
  - What evidence would resolve it: Investigating the effect of various regularization techniques and hypothesis set modifications on the minimizability gap, potentially through empirical studies or theoretical analysis.

- **Open Question 3**: How does the choice of cost function impact the overall system accuracy and deferral behavior in the multiple-expert setting?
  - Basis in paper: [explicit] The paper discusses different types of cost functions but does not provide a comprehensive analysis of their impact on system performance.
  - Why unresolved: The experiments use a specific type of cost function, but the sensitivity of the proposed method to different cost function choices is not thoroughly investigated.
  - What evidence would resolve it: Conducting extensive experiments with various cost function designs, analyzing the resulting system accuracy and deferral patterns, and potentially deriving theoretical insights into the relationship between cost functions and performance.

## Limitations
- Theoretical analysis relies on strong assumptions including bounded deferral costs and H-consistency of base surrogate loss
- Experimental evaluation limited to two vision datasets with narrow range of model architectures
- Does not address practical challenges such as computational overhead of multiple experts or handling experts with varying quality distributions

## Confidence
- **High Confidence**: The theoretical framework for H-consistency bounds and the construction of surrogate losses for multiple-expert deferral
- **Medium Confidence**: The practical utility of the approach given limited experimental scope and potential sensitivity to hyperparameter choices
- **Low Confidence**: Generalization to other domains and tasks beyond image classification

## Next Checks
1. Test the framework on non-vision tasks (e.g., text classification or tabular data) with varying expert qualities and cost structures
2. Conduct ablation studies to quantify the impact of different cost function choices on deferral behavior and system accuracy
3. Evaluate the computational overhead and scalability of the approach with increasing numbers of experts and dataset sizes