---
ver: rpa2
title: Reward Collapse in Aligning Large Language Models
arxiv_id: '2305.17608'
source_url: https://arxiv.org/abs/2305.17608
tags:
- reward
- sigmoid
- distribution
- lemma
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper documents a phenomenon called "reward collapse" that
  occurs when training reward models for aligning large language models (LLMs) using
  human preference rankings. Specifically, the prevailing ranking-based approach results
  in an identical reward distribution regardless of the prompts during the terminal
  phase of training.
---

# Reward Collapse in Aligning Large Language Models

## Quick Facts
- arXiv ID: 2305.17608
- Source URL: https://arxiv.org/abs/2305.17608
- Reference count: 40
- One-line primary result: Reward collapse occurs in ranking-based reward model training, causing identical reward distributions across prompts, which is prevented by prompt-aware utility functions.

## Executive Summary
This paper identifies a critical issue called "reward collapse" in the training of reward models for aligning large language models using human preference rankings. The phenomenon occurs when the reward distribution becomes identical regardless of the prompts during the terminal phase of training, which is undesirable as different prompts should yield different reward distributions. The authors theoretically investigate this problem and show it arises from the insufficiency of ranking-based objective functions to incorporate prompt-related information during optimization. They propose a prompt-aware optimization scheme with different utility functions for different prompt types that provably admits prompt-dependent reward distributions, effectively preventing reward collapse.

## Method Summary
The method introduces a prompt-aware optimization scheme that uses distinct utility functions depending on whether prompts are open-ended or closed-ended. The approach involves training a reward model (using DeBERTaV3) with utility functions that encourage different reward distributions for different prompt types - either widely dispersed for open-ended prompts or tightly concentrated for closed-ended prompts. The optimization is designed to maximize the expected utility over the training data while maintaining prompt-specific characteristics in the resulting reward distributions. This contrasts with standard ranking-based approaches that use fixed utility functions and suffer from reward collapse.

## Key Results
- Theoretical proof that reward collapse occurs in the asymptotic regime due to the optimization problem's independence from prompt information
- Derivation of closed-form expressions for reward distributions associated with different utility functions in the large-completion limit
- Experimental demonstration that prompt-aware utility functions significantly alleviate reward collapse during reward model training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward collapse occurs because the standard ranking-based objective function fails to incorporate prompt-specific information during optimization, causing the reward distribution to become identical across all prompts in the terminal phase of training.
- Mechanism: When the reward model is trained using pairwise comparisons without considering prompt context, the optimization problem reduces to finding a reward distribution that maximizes the utility function over all possible pairs of rewards. This optimization problem has a unique solution that is independent of the prompt, leading to reward collapse.
- Core assumption: The reward neural network is sufficiently overparameterized such that the utility function can be exactly maximized for each prompt.
- Evidence anchors:
  - [abstract] "our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization."
  - [section 2.1] "the solution to this optimization program is independent of the prompt itself in the interpolating regime, thereby leading to reward collapse."
  - [corpus] Weak - the corpus neighbors don't directly address reward collapse mechanisms, though some mention related concepts like "diversity collapse" in reinforcement learning contexts.
- Break condition: If the reward model is not sufficiently overparameterized, or if the utility function is not strictly concave, the collapse may not occur or may manifest differently.

### Mechanism 2
- Claim: Prompt-aware optimization with different utility functions for different prompt types can prevent reward collapse by allowing the reward distribution to adapt to the open-endedness of each prompt.
- Mechanism: By making the utility function depend on the prompt type (open-ended vs. closed-ended), the optimization problem becomes prompt-dependent. Different utility functions encourage different reward distributions - for example, functions that encourage polarization for closed-ended prompts and uniform distribution for open-ended prompts.
- Core assumption: The utility function can be designed to reflect the desired reward distribution characteristics for each prompt type.
- Evidence anchors:
  - [abstract] "we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime."
  - [section 2.2] "we propose to use distinct utility functions depending on prompts in training the reward model, such that the resulting reward distribution can be either widely dispersed or tightly concentrated, contingent on whether the prompt is open-ended or closed-ended."
  - [corpus] Weak - corpus papers don't specifically discuss prompt-aware utility functions, though some mention related concepts like "process reward models" and "cross-lingual collapse."
- Break condition: If the prompt classification is incorrect, or if the chosen utility functions don't match the true reward distribution characteristics of the prompts, the method may not prevent collapse effectively.

### Mechanism 3
- Claim: The reward distribution in the asymptotic regime (n → ∞) can be characterized analytically using variational principles, allowing precise control over the reward distribution shape through the choice of utility function.
- Mechanism: By analyzing the optimization problem in the limit of infinite completions, the reward distribution converges to a probability measure that maximizes the expected utility. Different utility functions lead to different limiting distributions (e.g., Beta distributions or measures with point masses at 0 and 1).
- Core assumption: The utility function is bounded, strongly concave, and increasing, ensuring the existence of a unique optimal probability measure.
- Evidence anchors:
  - [abstract] "this insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime."
  - [section 2.3] "The next result gives a closed-form expression of the reward distribution in the case of a large number of completions."
  - [corpus] Weak - corpus papers don't specifically discuss asymptotic analysis of reward distributions, though some mention related concepts like "probability distribution collapse."
- Break condition: If the utility function doesn't meet the required conditions (bounded, strongly concave, increasing), the analytical characterization may not hold or may not lead to the desired reward distribution.

## Foundational Learning

- Concept: Convex optimization and strong concavity
  - Why needed here: The analysis of the optimization problem and the design of prompt-aware utility functions rely heavily on convex optimization theory, particularly the properties of strongly concave functions.
  - Quick check question: Given a strongly concave function U with parameter μ, what is the minimum distance between two points that achieve the same objective value?

- Concept: Probability theory and convergence in distribution
  - Why needed here: Understanding how the reward distribution converges to a limiting measure as the number of completions increases requires knowledge of convergence concepts in probability theory.
  - Quick check question: If a sequence of probability measures converges in distribution to a limit measure, what properties must the limit measure satisfy?

- Concept: Bradley-Terry-Luce (BTL) model and pairwise comparison models
  - Why needed here: The extension to pairwise comparisons in Section 5 uses the BTL model framework, which is fundamental to understanding how preference data is modeled and used in reward model training.
  - Quick check question: In the BTL model, how is the probability of preferring item i over item j related to their underlying scores?

## Architecture Onboarding

- Component map: Training data (prompt-completion pairs) -> Reward model (DeBERTaV3) -> Utility function (fixed or prompt-aware) -> Optimization loop -> Updated reward model weights

- Critical path:
  1. Preprocess training data to create preferred/non-preferred completion pairs.
  2. Initialize reward model and choose utility function(s).
  3. For each training step:
     - Sample a batch of prompt-completion pairs.
     - Compute rewards for all completions.
     - Calculate the utility for each preferred/non-preferred pair.
     - Update model weights using gradient ascent on the total utility.
  4. Monitor reward distribution across different prompt types to detect/prevent collapse.

- Design tradeoffs:
  - Fixed vs. prompt-aware utility functions: Fixed functions are simpler but may lead to collapse; prompt-aware functions prevent collapse but require prompt classification and multiple utility functions.
  - Utility function choice: Different functions encourage different reward distributions, requiring careful selection based on prompt characteristics.
  - Model capacity: Higher capacity models may be more prone to collapse due to better optimization of the utility function.

- Failure signatures:
  - Reward distributions becoming identical across different prompt types during training.
  - Model consistently assigning extreme rewards (0 or 1) regardless of completion quality.
  - Poor calibration of uncertainty in the aligned language model's outputs.

- First 3 experiments:
  1. Train a reward model with a fixed utility function (e.g., log sigmoid) on synthetic data with clearly distinguishable open-ended and closed-ended prompts. Monitor reward distributions to confirm collapse.
  2. Repeat experiment 1 with a prompt-aware utility function that uses different functions for open-ended and closed-ended prompts. Verify that collapse is prevented and reward distributions match prompt characteristics.
  3. Analyze the asymptotic reward distributions for different utility functions using the analytical framework. Confirm that the observed distributions match theoretical predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reward collapse phenomenon be prevented by using a more diverse range of prompts, including varying degrees of open-endedness, in the training process?
- Basis in paper: [explicit] The paper mentions that "future research can explore the use of a more diverse range of prompts, varying in terms of their open-endedness."
- Why unresolved: The paper's experiments were conducted on a limited set of prompts due to computational constraints.
- What evidence would resolve it: Experimental results demonstrating that using a diverse range of prompts in training prevents reward collapse.

### Open Question 2
- Question: How does the choice of the utility function U affect the extent of reward collapse, and can we develop a method to select the optimal utility function for a given set of prompts?
- Basis in paper: [explicit] The paper discusses various classes of utility functions and their impact on reward distribution, but does not provide a systematic method for choosing the optimal utility function.
- Why unresolved: The paper suggests that "developing a method to choose a utility function based on prompts" is an intriguing avenue for further exploration.
- What evidence would resolve it: A method for selecting the optimal utility function based on the characteristics of the prompts, along with experimental validation.

### Open Question 3
- Question: Can the prompt-aware approach be extended to other settings beyond pairwise comparisons, such as multi-way comparisons or more complex preference structures?
- Basis in paper: [explicit] The paper briefly mentions the possibility of extending the prompt-aware approach to "other settings, such as instances where only pairwise preference data is accessible."
- Why unresolved: The paper does not provide a detailed investigation of the applicability of the prompt-aware approach to other preference structures.
- What evidence would resolve it: Theoretical analysis and experimental results demonstrating the effectiveness of the prompt-aware approach in settings beyond pairwise comparisons.

## Limitations

- The theoretical analysis assumes a large number of completions (n → ∞) and relies on strong concavity conditions for the utility function, which may not hold in practical settings with finite data.
- The prompt classification scheme for distinguishing open-ended from closed-ended prompts is not explicitly defined, which could affect the effectiveness of the prompt-aware approach.
- The experiments focus on synthetic data rather than real human preference data, leaving questions about real-world applicability.

## Confidence

**High Confidence**: The observation of reward collapse in existing ranking-based approaches is well-documented and theoretically justified. The characterization of the optimal reward distribution in the asymptotic regime using variational principles is mathematically rigorous.

**Medium Confidence**: The proposed prompt-aware utility functions are theoretically sound, but their practical implementation details (e.g., specific prompt classification criteria) are not fully specified. The claim that different utility functions can effectively prevent collapse for different prompt types is supported by theory but requires empirical validation.

**Low Confidence**: The extension to pairwise comparisons using the Bradley-Terry-Luce model, while theoretically plausible, is only briefly mentioned without detailed experimental validation. The real-world effectiveness of the approach on actual human preference data remains to be demonstrated.

## Next Checks

1. **Empirical validation on real preference data**: Test the prompt-aware approach on actual human preference datasets (e.g., from InstructGPT or Anthropic's work) to verify that it prevents collapse in practical settings and improves alignment quality.

2. **Prompt classification study**: Conduct an ablation study to determine how different prompt classification criteria (e.g., keyword-based, length-based, semantic similarity) affect the performance of the prompt-aware approach, and identify robust classification methods.

3. **Utility function sensitivity analysis**: Systematically vary the parameters of the prompt-aware utility functions and measure their impact on reward distribution characteristics and alignment performance, to establish guidelines for choosing appropriate utility functions in different scenarios.