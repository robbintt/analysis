---
ver: rpa2
title: 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs'
arxiv_id: '2307.16789'
source_url: https://arxiv.org/abs/2307.16789
tags:
- apis
- name
- tool
- instructions
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToolLLM, a general framework for enabling
  large language models to use real-world APIs. The authors first create ToolBench,
  a large-scale instruction-tuning dataset of 16,000+ APIs across 49 categories, automatically
  constructed using ChatGPT.
---

# ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs

## Quick Facts
- arXiv ID: 2307.16789
- Source URL: https://arxiv.org/abs/2307.16789
- Authors: 
- Reference count: 30
- This paper introduces ToolLLM, a general framework for enabling large language models to use real-world APIs.

## Executive Summary
This paper presents ToolLLM, a framework designed to enable large language models to effectively use real-world APIs. The authors introduce ToolBench, a large-scale dataset of 16,464 APIs across 49 categories, and propose a depth-first search-based decision tree (DFSDT) algorithm to improve LLM reasoning for tool-use tasks. Experiments show that ToolLLaMA, a fine-tuned LLaMA model trained on ToolBench, demonstrates strong tool-use performance and generalization to unseen APIs, achieving results comparable to ChatGPT.

## Method Summary
The authors developed ToolLLM by first creating ToolBench, a large-scale instruction-tuning dataset with 16,464 real-world APIs across 49 categories. They used ChatGPT to generate diverse instructions and employed a depth-first search-based decision tree (DFSDT) algorithm to construct valid solution paths. A neural API retriever was devised to recommend appropriate APIs for each instruction. The fine-tuned LLaMA model, ToolLLaMA, was trained on ToolBench and integrated with the API retriever. An automatic evaluator, ToolEval, was developed to assess tool-use performance.

## Key Results
- ToolLLaMA exhibits strong tool-use performance and generalization to unseen APIs.
- The API retriever and DFSDT algorithm contribute to improved performance over prior methods.
- ToolLLaMA achieves comparable performance to ChatGPT on tool-use tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-first search-based decision tree (DFSDT) significantly improves LLM reasoning for tool-use tasks by expanding the search space and enabling strategic backtracking.
- Mechanism: DFSDT constructs a decision tree where each node represents a reasoning state. The model evaluates multiple reasoning paths and can deliberately choose to retract steps by calling "Finish by Giving Up" and expand a new node.
- Core assumption: LLMs can evaluate multiple reasoning traces and make deliberate decisions to retract steps when encountering errors.
- Evidence anchors:
  - [abstract]: "We develop a novel depth-first search-based decision tree (DFSDT) algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space."
  - [section 2.3]: "We propose to construct a decision tree to expand the search space and increase the possibility of finding a valid path... Our DFSDT allows the model to assess different reasoning paths and choose to either (1) proceed along a promising path or (2) abandon an existing node."
- Break condition: If the model cannot generate distinct nodes or the search space becomes too large to handle efficiently.

### Mechanism 2
- Claim: The neural API retriever effectively reduces manual API selection overhead by recommending relevant APIs based on instruction content.
- Mechanism: The retriever encodes instructions and API documentation into embeddings using Sentence-BERT, then computes similarity scores to retrieve the most relevant APIs. It's trained on instruction-API pairs generated during data construction.
- Core assumption: Instructions and API documentation can be meaningfully represented as embeddings that capture semantic relevance.
- Evidence anchors:
  - [abstract]: "we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection."
  - [section 3.2]: "We follow Sentence-BERT (Reimers & Gurevych, 2019) to train a dense retriever based on BERT-BASE... The model encodes the instruction and API document into two embeddings, respectively, and the relevance is determined by the similarity of these two embeddings."
- Break condition: If the embedding space fails to capture semantic relationships between instructions and APIs.

### Mechanism 3
- Claim: Instruction generation from existing APIs ensures diverse and practical tool-use scenarios that cover both single-tool and multi-tool situations.
- Mechanism: Instead of generating instructions from scratch, the system samples APIs and prompts ChatGPT to create instructions that naturally incorporate these APIs. This bottom-up approach ensures instructions are grounded in real API functionality.
- Core assumption: Starting from existing APIs and generating instructions based on their functionality will produce more diverse and practical instruction scenarios than the reverse approach.
- Evidence anchors:
  - [abstract]: "we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios"
  - [section 2.2]: "Generating high-quality instructions requires two crucial aspects: diversity, to ensure the LLMs handle a wide range of API usage scenarios... and multi-tool usage, to mirror real-world situations"
- Break condition: If the sampled API combinations don't naturally lead to meaningful instructions or if the diversity is insufficient.

## Foundational Learning

- Concept: API documentation comprehension
  - Why needed here: LLMs must understand API functionality, required parameters, and response formats to effectively use them
  - Quick check question: Can the model correctly identify required vs optional parameters from API documentation?

- Concept: Chain-of-thought reasoning
  - Why needed here: LLMs need to break down complex instructions into sequential API calls with intermediate reasoning steps
  - Quick check question: Does the model generate coherent intermediate reasoning before making API calls?

- Concept: Embeddings and similarity measures
  - Why needed here: The API retriever relies on semantic similarity between instruction embeddings and API documentation embeddings
  - Quick check question: Can the model retrieve APIs with similar functionality when given semantically related instructions?

## Architecture Onboarding

- Component map:
  - API Collection: Crawls and filters real-world RESTful APIs from RapidAPI
  - Instruction Generation: Uses ChatGPT to create diverse instructions involving sampled APIs
  - Solution Path Annotation: Uses DFSDT with ChatGPT to generate valid solution paths (chains of API calls)
  - API Retriever: Neural network that recommends relevant APIs based on instruction content
  - ToolLLaMA: Fine-tuned LLaMA model that executes instructions using recommended APIs
  - ToolEval: Automatic evaluator using ChatGPT to assess tool-use performance

- Critical path:
  1. Instruction → API Retriever → Top N APIs
  2. APIs + Instruction → ToolLLaMA → Solution Path (DFSDT)
  3. Solution Path → ToolEval → Pass Rate/Win Rate

- Design tradeoffs:
  - Using ChatGPT for data generation vs manual annotation: Cost vs quality
  - DFSDT vs simpler prompting methods: Better reasoning vs higher API call costs
  - Fine-tuning all parameters vs LoRA: Better performance vs parameter efficiency

- Failure signatures:
  - Low pass rate: Model fails to execute instructions even with ground truth APIs
  - Low win rate: Model executes instructions but produces suboptimal solutions
  - API retriever NDCG < 0.5: Retriever fails to identify relevant APIs

- First 3 experiments:
  1. Compare DFSDT vs ReACT on simple single-tool instructions (measure pass rate)
  2. Test API retriever performance on single-tool vs multi-tool instructions (measure NDCG)
  3. Evaluate ToolLLaMA generalization to unseen APIs (measure pass rate on I1-Tool and I1-Cat)

## Open Questions the Paper Calls Out
- Question: How does ToolLLaMA's performance on unseen APIs compare to ChatGPT's performance on the same APIs, and what factors contribute to any differences?
- Question: How does the size and diversity of the ToolBench dataset affect the generalization performance of ToolLLaMA on unseen APIs?
- Question: How does the proposed DFSDT algorithm compare to other state-of-the-art decision-making algorithms in terms of efficiency and effectiveness for tool use tasks?
- Question: How does the performance of ToolLLaMA vary across different categories of APIs, and what factors contribute to any variations?
- Question: How does the integration of the neural API retriever with ToolLLaMA affect the model's performance and efficiency in real-world scenarios?

## Limitations
- The scalability of the DFSDT approach is uncertain due to high data construction costs.
- ToolEval's reliance on ChatGPT's judgment introduces potential biases and limits objective validation.
- Generalization claims to unseen APIs may be overestimated as testing was done on APIs from the same domain as training data.

## Confidence
- High Confidence: The effectiveness of the neural API retriever and the overall improvement of ToolLLaMA over baseline models on the ToolBench benchmark.
- Medium Confidence: The specific contributions of DFSDT to reasoning performance, as limited ablation studies were provided.
- Medium Confidence: The quality and diversity of the ToolBench dataset, given it was entirely generated by ChatGPT without human verification.

## Next Checks
1. Conduct controlled experiments comparing DFSDT against ReACT and simple chain-of-thought prompting on identical instruction sets, measuring both pass rates and API call efficiency to isolate the specific benefits of the decision tree approach.

2. Test ToolLLaMA on APIs from completely different domains than those seen during training (e.g., train on general productivity APIs, test on specialized scientific or medical APIs) to better assess true zero-shot generalization capabilities.

3. Measure the performance degradation and computational costs when scaling DFSDT to 10x or 100x the current API volume, and explore whether alternative search strategies could achieve similar performance with lower costs.