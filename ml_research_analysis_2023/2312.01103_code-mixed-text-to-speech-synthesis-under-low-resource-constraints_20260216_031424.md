---
ver: rpa2
title: Code-Mixed Text to Speech Synthesis under Low-Resource Constraints
arxiv_id: '2312.01103'
source_url: https://arxiv.org/abs/2312.01103
tags:
- speaker
- speech
- data
- text
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of building high-quality code-mixed
  Hindi-English text-to-speech (TTS) systems for e-commerce applications. The authors
  propose a data-oriented approach that leverages monolingual datasets and a transliteration
  model to convert English text into a common Devanagari script.
---

# Code-Mixed Text to Speech Synthesis under Low-Resource Constraints

## Quick Facts
- arXiv ID: 2312.01103
- Source URL: https://arxiv.org/abs/2312.01103
- Reference count: 40
- Key outcome: Proposed transliteration-based bilingual training with transfer learning and decoder-only fine-tuning outperforms Google TTS with positive CMOS score of 0.02 for code-mixed Hindi-English TTS.

## Executive Summary
This paper addresses the challenge of building high-quality code-mixed Hindi-English text-to-speech (TTS) systems for e-commerce applications under low-resource constraints. The authors propose a data-oriented approach that leverages monolingual datasets and a transliteration model to convert English text into a common Devanagari script. They demonstrate that training a single TTS model with bilingual data without explicit code-mixing can effectively handle pure code-mixed test sets. The proposed approach, combined with transfer learning and decoder-only fine-tuning, outperforms the Google TTS system with a positive CMOS score of 0.02. Additionally, the authors show that low-resource voice adaptation is possible with just 3 hours of data, highlighting the importance of their pre-trained models in resource-constrained settings.

## Method Summary
The authors propose a transliteration-based approach for code-mixed Hindi-English TTS synthesis. They convert all text (both Hindi and English) into a common Devanagari script using a high-quality transliteration system. The TTS model is based on the Tacotron2 architecture with a two-stage pipeline consisting of a spectrogram prediction network and a Waveglow-based vocoder. The model is trained on monolingual parallel datasets for Hindi and English, without explicit code-mixing. Transfer learning is employed using pre-trained models on English LJSpeech corpus and in-house Hindi data. Low-resource voice adaptation experiments are conducted to demonstrate the effectiveness of the pre-trained models with just 3 hours of target speaker data.

## Key Results
- The proposed transliteration-based bilingual training approach outperforms Google TTS with a positive CMOS score of 0.02.
- Mix-warmstart pre-training with decoder-only fine-tuning achieves better results compared to English warmstart.
- Low-resource voice adaptation is possible with as little as 3 hours of target speaker data using the pre-trained models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single-script bilingual training approach without explicit code-mixing works for pure code-mixed test sets.
- Mechanism: By converting all text (both Hindi and English) into a common Devanagari script using high-quality transliteration, the model learns to handle code-mixed inputs without needing separate G2P models or language identification systems.
- Core assumption: High-quality transliteration preserves pronunciation accuracy and the model can generalize to mixed-language inputs even when trained on monolingual parallel data.
- Evidence anchors:
  - [abstract]: "We show that such single script bi-lingual training without any code-mixing works well for pure code-mixed test sets."
  - [section]: "Since the primary language of the end application is Hindi we convert all the data to Devanagari script. We show that independent bi-lingual data sets without pure code-mixing work well for pure code-mixed test sets."
  - [corpus]: Weak evidence; no direct mention of transliteration-based bilingual training in neighbor papers.
- Break condition: Poor transliteration quality or significant phonetic differences between source and target scripts that the model cannot resolve.

### Mechanism 2
- Claim: Transfer learning from a mixed-script pre-training setup improves performance more than English-only pre-training.
- Mechanism: Pre-training on a mix of all available speakers in Devanagari script provides a rich encoder initialization that generalizes better to the target speaker when fine-tuned with decoder-only training.
- Core assumption: The encoder benefits more from exposure to diverse Devanagari text than from English-only data, and freezing it during fine-tuning preserves this advantage.
- Evidence anchors:
  - [abstract]: "We observe that pre-training strategies are essential for building a high-quality model... mix-warmstart + decoder-only finetuning works the best."
  - [section]: "We show the advantages of transfer learning from the mix-pretraining setup... mix-warmstart configuration shows clear improvements over eng-warmstart."
  - [corpus]: Weak evidence; neighbor papers focus on few-shot adaptation but do not explicitly compare mixed-script pre-training vs. English pre-training.
- Break condition: If the pre-training data is too domain-distinct from the target, or if the encoder cannot effectively transfer to the target speaker's characteristics.

### Mechanism 3
- Claim: Low-resource voice adaptation is feasible with as little as 3 hours of target speaker data using the proposed pre-trained models.
- Mechanism: The mix-warmstart model's encoder, trained on diverse speakers, provides a strong initialization that can be adapted to a new voice with minimal data by freezing the encoder and fine-tuning only the decoder.
- Core assumption: The pre-trained model captures generalizable speech synthesis patterns that transfer well to unseen speakers, and the decoder can adapt to speaker-specific characteristics with limited data.
- Evidence anchors:
  - [abstract]: "We also perform low-resource voice adaptation experiments to show that a new voice can be onboarded with just 3 hrs of data."
  - [section]: "We observe that the mix-warmstart models can be adapted to a new speaker using just 3 hours of data... degradation in MOS scores is very less even after using just 1/5th of the original data."
  - [corpus]: No direct evidence; neighbor papers discuss few-shot adaptation but not specifically with 3-hour data limits.
- Break condition: If the target speaker's voice characteristics are too far from the pre-training speakers, or if 3 hours is insufficient to capture necessary prosodic features.

## Foundational Learning

- Concept: Text-to-Speech (TTS) pipeline architecture (spectrogram prediction + vocoder)
  - Why needed here: Understanding the two-stage Tacotron2 + Waveglow architecture is essential to grasp how the model converts text to speech and where transfer learning and fine-tuning occur.
  - Quick check question: What are the two main components of the TTS pipeline used in this work, and what does each component do?

- Concept: Speaker adaptation and embedding techniques
  - Why needed here: Knowing how speaker embeddings (x-vectors) are used to condition the model on different voices is crucial for understanding the multi-speaker setup and low-resource adaptation experiments.
  - Quick check question: How do x-vector speaker embeddings influence the output of the multi-speaker Tacotron2 model?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: The paper's performance gains rely on specific pre-training (mix-warmstart) and fine-tuning (decoder-only) approaches, so understanding these concepts is key to replicating or extending the results.
  - Quick check question: What is the difference between full fine-tuning and decoder-only fine-tuning, and why might the latter be preferred after mix-warmstart pre-training?

## Architecture Onboarding

- Component map: Text → Encoder (3 Conv + Bi-LSTM) → (Speaker Embeddings) → Attention → Decoder (2 uni-LSTM) → Spectrogram → Waveglow → Audio

- Critical path: Text → Encoder → (Speaker Embeddings) → Attention → Decoder → Spectrogram → Waveglow → Audio

- Design tradeoffs:
  - Single vs. multi-speaker: Single-speaker gives higher quality for a known voice; multi-speaker enables zero-shot adaptation but may slightly reduce quality.
  - Pre-training choice: English warmstart requires script conversion; mix-warmstart leverages more relevant data but needs careful fine-tuning.
  - Fine-tuning strategy: Full fine-tuning vs. decoder-only; the latter preserves encoder generalization but may limit adaptation.

- Failure signatures:
  - Decoder fails to generate end token: Likely issue with attention or speaker embedding conditioning.
  - Poor pronunciation of code-mixed text: Could indicate transliteration errors or insufficient bilingual training.
  - Quality drop in low-resource adaptation: May suggest encoder over-generalization or insufficient fine-tuning data.

- First 3 experiments:
  1. Train single-speaker Tacotron2 with only Hindi data (baseline) and evaluate on code-mixed test set.
  2. Train with Hindi + English (both in Devanagari) and compare MOS/CMOS to baseline.
  3. Apply mix-warmstart pre-training, then decoder-only fine-tuning on target speaker; evaluate low-resource (3h) adaptation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed transliteration-based approach perform for code-mixed TTS systems involving other language pairs beyond Hindi-English?
- Basis in paper: explicit
- Why unresolved: The paper focuses on Hindi-English code-mixing and uses Devanagari script for transliteration. It is unclear whether the approach would be equally effective for other language pairs with different scripts or linguistic structures.
- What evidence would resolve it: Empirical results comparing the transliteration-based approach for various language pairs (e.g., Tamil-English, Telugu-English) and different scripts would provide insights into its generalizability and effectiveness across diverse linguistic contexts.

### Open Question 2
- Question: What is the impact of using different pre-training strategies (e.g., pre-training on a larger corpus, using domain-specific data) on the performance of the code-mixed TTS system?
- Basis in paper: explicit
- Why unresolved: The paper explores pre-training strategies using LJSpeech English corpus and in-house Hindi data. However, the impact of using different pre-training datasets or strategies on the final system performance is not thoroughly investigated.
- What evidence would resolve it: Comparative studies evaluating the performance of the code-mixed TTS system with various pre-training strategies, including larger corpora, domain-specific data, or different languages, would provide insights into the optimal pre-training approach for achieving high-quality results.

### Open Question 3
- Question: How does the proposed approach handle code-mixing with languages that have more complex phonological or grammatical structures compared to Hindi and English?
- Basis in paper: inferred
- Why unresolved: The paper demonstrates the effectiveness of the approach for Hindi-English code-mixing, but it does not address the challenges posed by languages with more complex phonological or grammatical structures, such as tonal languages or languages with rich morphological inflections.
- What evidence would resolve it: Empirical results evaluating the performance of the code-mixed TTS system on languages with complex phonological or grammatical structures, along with a detailed analysis of the challenges encountered and potential solutions, would provide insights into the approach's adaptability to diverse linguistic contexts.

## Limitations

- The effectiveness of the transliteration-based approach heavily relies on the quality of the in-house transliteration system, which is not publicly available or thoroughly evaluated.
- The low-resource adaptation claim (3 hours of data) lacks statistical validation through multiple adaptation runs with different speakers or data subsets.
- The evaluation metrics (MOS/CMOS) are subjective and may not capture subtle mispronunciations that native speakers would notice.

## Confidence

- **High confidence**: The architectural framework (Tacotron2 + Waveglow) and general transfer learning approach are well-established in the literature. The observation that mix-warmstart + decoder-only fine-tuning outperforms English warmstart is supported by the presented results.

- **Medium confidence**: The effectiveness of transliteration-based bilingual training for code-mixed TTS. While the results show positive CMOS scores, the lack of detailed transliteration quality metrics and the subjective nature of MOS evaluation reduce confidence in this mechanism.

- **Medium confidence**: The 3-hour low-resource adaptation claim. The results show promising trends, but with only one adaptation experiment and no statistical validation, it's difficult to generalize this finding to other speakers or domains.

## Next Checks

1. **Transliteration Quality Audit**: Extract 50 English product names from the test set, manually transliterate them to Devanagari, and have native Hindi speakers rate the pronunciation accuracy. Compare this with the model's MOS on the same samples to determine if transliteration errors are masking as model performance issues.

2. **Statistical Significance of Low-Resource Adaptation**: Repeat the 3-hour adaptation experiment 5 times with different random seeds and data subsets. Calculate confidence intervals for MOS scores and perform statistical tests (e.g., paired t-test) to determine if the observed improvements over baseline are significant.

3. **Cross-Speaker Generalization Test**: Apply the mix-warmstart + decoder-only fine-tuning approach to a completely new speaker (not in the original 5-speaker pool) with only 3 hours of data. Evaluate MOS on both code-mixed and English-only test sets to verify if the approach generalizes beyond the initial adaptation scenario.