---
ver: rpa2
title: Class-Incremental Grouping Network for Continual Audio-Visual Learning
arxiv_id: '2309.05281'
source_url: https://arxiv.org/abs/2309.05281
tags:
- audio-visual
- learning
- continual
- visual
- grouping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of class-incremental audio-visual
  learning, where a model must learn to classify audio-visual events from sequential
  tasks without forgetting previously learned classes. The authors propose the Class-Incremental
  Grouping Network (CIGN), which uses learnable audio-visual class tokens and a grouping
  mechanism to aggregate category-aware features.
---

# Class-Incremental Grouping Network for Continual Audio-Visual Learning

## Quick Facts
- arXiv ID: 2309.05281
- Source URL: https://arxiv.org/abs/2309.05281
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance with average accuracy gains of 6.74%, 7.44%, and 4.43% over the best prior method on VGGSound-Instruments, VGGSound-100, and VGG-Sound Sources benchmarks respectively, while significantly reducing forgetting.

## Executive Summary
This paper introduces the Class-Incremental Grouping Network (CIGN), a novel approach for class-incremental audio-visual learning that addresses catastrophic forgetting while learning from sequential tasks. CIGN employs learnable audio-visual class tokens and a grouping mechanism to aggregate category-aware features, enabling the model to preserve knowledge from previous tasks while adapting to new ones. The method demonstrates significant improvements over existing approaches across three benchmarks, with average accuracy gains of 6.74% to 7.44% and substantial reductions in forgetting.

## Method Summary
CIGN uses ResNet18 encoders for audio and visual modalities, initialized with weights pre-trained on 2M Flickr videos. The model learns separate audio-visual class tokens for each category and employs self-attention transformers to aggregate global representations. During task transitions, KL divergence loss constrains new class tokens to match old token distributions, preserving learned representations. The model computes similarity scores between audio/visual features and class tokens, then uses weighted summation to create class-specific embeddings. Classification is performed separately for audio and visual modalities, with final predictions obtained by multiplying their output probabilities. The total objective combines class tokens distillation and continual grouping losses.

## Key Results
- Average accuracy gains of 6.74%, 7.44%, and 4.43% over the best prior method on VGGSound-Instruments, VGGSound-100, and VGG-Sound Sources benchmarks respectively
- Significant reductions in forgetting compared to existing methods
- State-of-the-art performance across all three evaluated datasets
- Effective prevention of catastrophic forgetting through class tokens distillation and continual grouping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learnable audio-visual class tokens distill knowledge from previous tasks and prevent forgetting.
- Mechanism: The model learns separate class tokens for each audio-visual category. During task transitions, KL divergence loss constrains new class tokens to match old token distributions, preserving learned representations.
- Core assumption: Audio-visual class tokens can capture sufficient semantic information to represent categories across tasks without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "leverages class tokens distillation and continual grouping to prevent forgetting parameters learned from previous tasks"
  - [section] "we apply a Kullback-Leibler (KL) divergence loss KL(ct_i||ct−1_i) on new task t and previous task t − 1"

### Mechanism 2
- Claim: Audio-visual continual grouping aggregates category-aware features by aligning features with learned class tokens.
- Mechanism: The model computes similarity scores between audio/visual features and class tokens, then uses weighted summation to create class-specific embeddings that preserve category semantics.
- Core assumption: Self-attention with class tokens can effectively disentangle individual audio-visual semantics from mixed representations.
- Evidence anchors:
  - [abstract] "leverages learnable audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features"
  - [section] "we calculate the weighted sum of audio-visual features belonged to generate the class-aware embeddings"

### Mechanism 3
- Claim: The product of audio and visual classification probabilities provides robust audio-visual category predictions.
- Mechanism: Separate classification heads process class-aware audio and visual features, then multiply their outputs to generate final audio-visual predictions.
- Core assumption: Audio and visual modalities provide complementary information that can be effectively combined through multiplication.
- Evidence anchors:
  - [section] "the product of output logits (pt_a,i, pt_v,i) from audio and visual modalities are utilized to predict the probability of audio-visual classes"

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper explicitly addresses catastrophic forgetting as the core challenge in continual audio-visual learning
  - Quick check question: What happens to a model's performance on previous tasks when trained on new tasks without any mitigation strategy?

- Concept: Self-attention mechanisms and transformer architectures
  - Why needed here: The model uses self-attention transformers to aggregate global audio and spatial visual representations with class tokens
  - Quick check question: How does self-attention help align cross-modal features with class token embeddings?

- Concept: Knowledge distillation and regularization techniques
  - Why needed here: The model employs KL divergence for class token distillation and cross-entropy loss for new category learning
  - Quick check question: What is the purpose of applying both KL divergence and cross-entropy losses during training?

## Architecture Onboarding

- Component map:
  - Audio encoder (ResNet18) → Audio features
  - Visual encoder (ResNet18) → Visual features  
  - Audio-visual class tokens → Learnable embeddings for each category
  - Audio-visual continual grouping module → Creates category-aware representations
  - Classification heads → Separate audio and visual classifiers
  - Distillation module → Preserves knowledge from previous tasks

- Critical path: Input audio/visual → Feature extraction → Class token alignment → Grouping → Classification → Distillation

- Design tradeoffs:
  - Number of class tokens vs. model complexity
  - Buffer size for rehearsal vs. computational efficiency
  - Depth of transformer layers vs. representation quality

- Failure signatures:
  - High forgetting indicates insufficient distillation
  - Poor accuracy suggests inadequate feature-token alignment
  - Overfitting may occur with insufficient regularization

- First 3 experiments:
  1. Test basic classification accuracy on single tasks to verify feature extraction
  2. Evaluate forgetting rates when transitioning between tasks
  3. Compare performance with and without class token distillation

## Open Questions the Paper Calls Out
The paper explicitly acknowledges limitations in handling open-set audio-visual classification, noting that pre-defined category tokens are required for classification and the model cannot classify samples from categories not seen during training without additional mechanisms.

## Limitations
- The method requires pre-defined category tokens and cannot handle completely unseen categories during inference without additional training or mechanisms
- The buffer size of 50 audio-visual pairs per category may be insufficient for larger class sets, potentially leading to overfitting in later tasks
- The computational overhead of maintaining and updating class tokens across multiple tasks is not explicitly quantified

## Confidence

- **High confidence**: The core architectural design (separate audio/visual encoders with class tokens, distillation via KL divergence) is well-specified and theoretically sound
- **Medium confidence**: The quantitative improvements over baselines are significant, but the limited number of competing methods and lack of statistical significance testing reduce confidence in the absolute performance claims
- **Low confidence**: The open-set classification capabilities and generalization to completely unseen categories remains unclear due to insufficient experimental validation

## Next Checks

1. Conduct ablation studies to isolate the contributions of class tokens distillation versus continual grouping mechanisms
2. Test model performance with varying buffer sizes (5, 25, 100 pairs per category) to assess memory efficiency tradeoffs
3. Evaluate forgetting rates using the standard Forgetting measure (negative value indicates performance degradation on previous tasks) across multiple random seeds to establish statistical significance