---
ver: rpa2
title: 'Energy-based learning algorithms for analog computing: a comparative study'
arxiv_id: '2312.15103'
source_url: https://arxiv.org/abs/2312.15103
tags:
- learning
- algorithms
- function
- training
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first systematic comparison of seven energy-based
  learning (EBL) algorithms: contrastive learning (CL), equilibrium propagation (EP),
  and coupled learning (CpL) in their positively-perturbed, negatively-perturbed,
  and centered variants. The comparison is conducted on deep convolutional Hopfield
  networks (DCHNs) trained on five vision tasks (MNIST, Fashion-MNIST, SVHN, CIFAR-10,
  CIFAR-100).'
---

# Energy-based learning algorithms for analog computing: a comparative study

## Quick Facts
- arXiv ID: 2312.15103
- Source URL: https://arxiv.org/abs/2312.15103
- Authors: 
- Reference count: 40
- Key outcome: This paper provides the first systematic comparison of seven energy-based learning algorithms on deep convolutional Hopfield networks trained on five vision tasks.

## Executive Summary
This paper systematically compares seven energy-based learning (EBL) algorithms—contrastive learning, equilibrium propagation, and coupled learning in their positively-perturbed, negatively-perturbed, and centered variants—on deep convolutional Hopfield networks trained on five vision tasks. The study reveals that while all algorithms perform similarly on simple tasks like MNIST, significant differences emerge on harder tasks, with centered algorithms outperforming one-sided variants and negatively-perturbed algorithms outperforming positively-perturbed ones. The centered variant of equilibrium propagation (C-EP) emerges as the best-performing algorithm, especially on CIFAR-100, achieving new state-of-the-art results for DCHN simulations on all five datasets with a 13.5x speedup through asynchronous updates and reduced precision.

## Method Summary
The study evaluates seven EBL algorithms on deep convolutional Hopfield networks (DCHNs) with architecture consisting of 4 hidden layers (128→256→512→512 units) using convolutional interactions with kernel size 3×3, padding 1, and max pooling. Energy minimization employs asynchronous updates (60 iterations at inference, 15-20 at training) with 16-bit precision. Networks are trained on MNIST, Fashion-MNIST, SVHN, CIFAR-10, and CIFAR-100 using SGD with momentum, weight decay, and cosine-annealing scheduler. The algorithms differ in their perturbation methods (positive, negative, or centered) and gradient computation approaches, with the study comparing test and training error rates across all combinations.

## Key Results
- Centered algorithms (C-EP, C-CpL) outperform one-sided algorithms on all datasets, with C-EP achieving the best overall performance
- Negatively-perturbed algorithms (N-EP, N-CpL) consistently outperform positively-perturbed variants across multiple tasks
- C-EP achieves new state-of-the-art results for DCHN simulations on all five datasets, with 13.5x speedup over previous work through asynchronous updates and reduced precision
- On CIFAR-100, C-EP achieves 50.2% test error compared to 62.5% for P-EP and 55.9% for CL

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Negative perturbations yield better optimization performance than positive perturbations in energy-based learning.
- **Mechanism:** The N-EP algorithm optimizes an upper bound of the cost function while P-EP optimizes a lower bound. Since the surrogate function in N-EP is closer to the true cost function (being an upper bound rather than a lower bound), the optimization direction is more aligned with minimizing the actual loss.
- **Core assumption:** The cost function landscape allows the upper bound to be a better surrogate for optimization than the lower bound.
- **Evidence anchors:**
  - [abstract] "our key findings reveal that negative perturbations are better than positive ones"
  - [section 2.4] "Theorem 2... LEβ is an upper bound if β < 0 (N-EP), and a lower bound if β > 0 (P-EP)"
  - [corpus] Weak - related works discuss energy-based learning but not this specific perturbation direction comparison
- **Break condition:** If the cost function landscape is such that a lower bound provides better gradient information than an upper bound, or if the perturbation strength β is too large causing the bound to become uninformative.

### Mechanism 2
- **Claim:** Centered algorithms (using perturbations of opposite signs) outperform one-sided algorithms.
- **Mechanism:** The centered variant of EP (C-EP) approximates the true cost function up to O(β²), whereas one-sided variants (P-EP and N-EP) have O(β) approximation error. This second-order accuracy means C-EP follows the true gradient more closely, leading to better convergence.
- **Core assumption:** The cost function can be well-approximated by a second-order expansion in the perturbation strength.
- **Evidence anchors:**
  - [abstract] "centered variant of EP (C-EP) emerges as the best-performing algorithm"
  - [section 2.4] "Theorem 3... the function LEP−β;+β approximates the true cost function up to O(β²) when β → 0"
  - [corpus] Weak - related works mention energy-based models but don't discuss this specific centered perturbation advantage
- **Break condition:** If the perturbation strength β is too large, making the second-order approximation invalid, or if the cost function has discontinuities that invalidate the Taylor expansion.

### Mechanism 3
- **Claim:** Asynchronous updates in energy minimization provide significant speedup without degrading performance.
- **Mechanism:** By updating even and odd layers in parallel separately (rather than all layers synchronously), the energy minimization converges faster. This reduces the number of iterations needed from 250 to 60 while maintaining or improving test accuracy.
- **Core assumption:** The energy landscape allows for faster convergence when updating layers in an alternating pattern rather than all at once.
- **Evidence anchors:**
  - [section 3] "we use a novel energy minimisation procedure for DCHNs based on asynchronous updates"
  - [section 4.2] "13.5x faster with respect to Laborieux et al. (2021)... which we achieve thanks to the use of a novel energy minimisation algorithm based on asynchronous updates"
  - [corpus] Weak - related works discuss analog training but don't specifically address asynchronous vs synchronous update strategies
- **Break condition:** If the energy landscape has strong coupling between layers that requires synchronous updates to maintain stability, or if the asynchronous updates introduce oscillations that prevent convergence.

## Foundational Learning

- **Concept: Energy-based models and contrastive learning**
  - Why needed here: The paper compares seven variants of energy-based learning algorithms, all of which rely on contrasting two states (free and perturbed) to compute gradients. Understanding how energy functions define model behavior and how contrastive learning works is fundamental to grasping the algorithmic differences.
  - Quick check question: What is the key difference between contrastive learning and equilibrium propagation in terms of how they obtain the second state for comparison?

- **Concept: Implicit differentiation and fixed-point dynamics**
  - Why needed here: EBL algorithms compute gradients through the implicit function theorem rather than explicit backpropagation. The paper discusses how different perturbation methods affect the quality of these implicit gradients.
  - Quick check question: How does equilibrium propagation compute gradients without backpropagation through the energy minimization process?

- **Concept: Surrogate loss functions and their approximation properties**
  - Why needed here: The theoretical analysis shows that different EBL algorithms optimize different surrogate functions that approximate the true cost function with varying accuracy (O(β) vs O(β²)). Understanding surrogate losses is crucial for interpreting the empirical results.
  - Quick check question: Why is it important that C-EP's surrogate function approximates the cost function to second order while P-EP and N-EP only achieve first-order approximation?

## Architecture Onboarding

- **Component map:** Input layer → 4 hidden layers (128→256→512→512 units) → Output layer → Energy function composed of quadratic terms, convolutional interactions, dense interactions, and biases → Hard-sigmoid activation functions for hidden layers, identity for output

- **Critical path:**
  1. Forward pass: Clamp input, minimize energy to find free state (h⋆, o⋆)
  2. Perturbation phase: Apply perturbation (β), minimize energy to find perturbed state
  3. Weight update: Compute gradient from energy differences, update weights
  4. Repeat for multiple epochs with learning rate scheduling

- **Design tradeoffs:**
  - Synchronous vs asynchronous updates: Synchronous is theoretically simpler but slower; asynchronous provides 13.5x speedup with comparable accuracy
  - Perturbation strength (β): Too small → weak signal; too large → poor approximation of true gradient
  - Precision: 16-bit vs 32-bit: Lower precision enables faster computation but may introduce numerical issues
  - Number of iterations: Fewer iterations → faster but risk incomplete convergence; more iterations → slower but more stable

- **Failure signatures:**
  - Training error plateaus at ~81%: Indicates poor initialization or incompatible hyperparameters for the dataset
  - Test error much higher than training error: Overfitting or insufficient regularization
  - Extremely slow convergence: May need more iterations or better initialization
  - Divergence during training: Learning rate too high or perturbation strength inappropriate

- **First 3 experiments:**
  1. **MNIST with CL baseline:** Verify basic implementation works; expect <1% test error
  2. **CIFAR-10 with C-EP:** Test best algorithm on harder task; expect ~10% test error with 100 epochs
  3. **SVHN with different weight initialization:** Reproduce the poor performance and then fix it with better initialization to understand the failure mode observed in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do negatively-perturbed algorithms consistently outperform positively-perturbed ones across multiple datasets?
- Basis in paper: [explicit] The paper states "our key findings reveal that negative perturbations are better than positive ones" and provides theoretical support through Theorem 2 showing N-EP optimizes an upper bound while P-EP optimizes a lower bound of the cost function.
- Why unresolved: While the theoretical framework explains the difference in optimization objectives, it doesn't fully explain why optimizing an upper bound performs better than optimizing a lower bound in practice. The paper notes this is "counter-intuitive" and requires further investigation.
- What evidence would resolve it: Additional theoretical analysis explaining the practical implications of upper vs lower bound optimization, or empirical studies varying the perturbation strength and sign across different model architectures.

### Open Question 2
- Question: How would these EBL algorithms compare to other non-backpropagation methods like local learning rules or biologically plausible algorithms?
- Basis in paper: [inferred] The paper compares seven EBL algorithms but doesn't benchmark against other non-gradient-based methods. The authors mention the goal of "building energy-efficient processors dedicated to model optimization" suggesting broader comparison is needed.
- Why unresolved: The paper focuses exclusively on EBL algorithms without comparing to alternative learning approaches that also avoid backpropagation. This limits understanding of EBL's relative advantages.
- What evidence would resolve it: Direct comparison of EBL algorithms with other non-backpropagation methods like local learning rules, feedback alignment, or biologically plausible algorithms on the same tasks.

### Open Question 3
- Question: Would the findings generalize to different network architectures beyond deep convolutional Hopfield networks?
- Basis in paper: [explicit] The authors state "our comparative study of EBL algorithms was conducted only on the DCHN model" and acknowledge "further studies will be required to confirm whether or not the conclusions that we have drawn extend to a broader class of models."
- Why unresolved: The study is limited to a specific architecture, and the authors explicitly note this limitation. Different architectures might have different sensitivity to perturbation methods.
- What evidence would resolve it: Replication of the comparative study using other architectures like residual networks, transformers, or analog hardware implementations of different network types.

### Open Question 4
- Question: What is the optimal perturbation strength (β) for each algorithm and dataset, and how does it interact with other hyperparameters?
- Basis in paper: [explicit] The paper uses fixed β values across experiments and notes that "the performance of the different EBL methods depends on various hyperparameters such as the perturbation strength (β)" but doesn't explore this systematically.
- Why unresolved: The study uses predetermined β values without optimization, and the authors acknowledge this limitation. Different datasets and architectures might require different perturbation strengths.
- What evidence would resolve it: Systematic hyperparameter optimization studies varying β alongside other parameters like learning rate, weight decay, and network depth to identify optimal configurations.

## Limitations

- The study focuses exclusively on Hopfield networks with hard-sigmoid activations, limiting generalizability to other architectures
- The paper does not investigate the effects of different perturbation strengths β systematically across algorithms and datasets
- The study does not explore regularization techniques that might affect algorithm performance differently

## Confidence

- Comparative algorithm performance: **High** - results are consistent across multiple datasets and tasks
- Theoretical bounds analysis: **Medium** - mathematical derivations are sound but empirical validation is limited to specific network architectures
- Asynchronous update speedup claims: **High** - quantitative comparisons with previous work are provided

## Next Checks

1. Test algorithm performance on a wider range of network architectures (e.g., ReLU activations, different layer widths) to assess generalizability beyond Hopfield networks.

2. Conduct systematic ablation studies varying perturbation strength β to identify optimal values and test the theoretical predictions about bound quality.

3. Evaluate whether the observed algorithm rankings hold when additional regularization techniques (dropout, batch normalization) are introduced.