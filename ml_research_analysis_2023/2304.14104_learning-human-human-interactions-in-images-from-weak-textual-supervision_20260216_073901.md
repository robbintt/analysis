---
ver: rpa2
title: Learning Human-Human Interactions in Images from Weak Textual Supervision
arxiv_id: '2304.14104'
source_url: https://arxiv.org/abs/2304.14104
tags:
- name
- images
- waldo
- interaction
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a new paradigm of learning human-human interactions
  (HHI) as free text from still images, allowing to model the heavy tail of possible
  interactions beyond categorical recognition. To address the absence of labeled data,
  we use knowledge distillation applied to synthetic caption data produced by a large
  language model without explicit supervision.
---

# Learning Human-Human Interactions in Images from Weak Textual Supervision

## Quick Facts
- arXiv ID: 2304.14104
- Source URL: https://arxiv.org/abs/2304.14104
- Reference count: 40
- Key outcome: A new paradigm for learning human-human interactions (HHI) as free text from still images using weak textual supervision and knowledge distillation from LLMs, outperforming state-of-the-art models on Waldo/Wenda and imSitu-HHI datasets.

## Executive Summary
This paper introduces a novel approach to understanding human-human interactions in images by modeling them as free text rather than categorical labels. The method addresses the data scarcity problem by using knowledge distillation from a large language model to generate synthetic caption-interaction pairs from noisy internet captions. The approach is evaluated on two datasets and shows superior performance compared to existing image captioning and situation recognition models, particularly in capturing the diversity of interactions through text-based metrics.

## Method Summary
The method uses knowledge distillation applied to synthetic caption data produced by a large language model (GPT-Neo-1.3B) without explicit supervision. It extracts HHI pseudo-labels from noisy Internet captions by first generating synthetic caption-interaction pairs using the LLM with in-context learning, then training an abstractive summarization model (T5-base) on this synthetic data. This summarization model is applied to Who's Waldo captions to generate pseudo-labels, which are then used to train image captioning models (CLIP encoder + GPT2 decoder) to predict HHI as free text.

## Key Results
- Outperforms state-of-the-art image captioning and situation recognition models on both Waldo/Wenda and imSitu-HHI datasets
- Achieves higher BLEURT, NLI, and verb embedding similarity scores compared to baseline methods
- Demonstrates effective modeling of the heavy tail of human-human interactions beyond categorical recognition

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation from a large language model (LLM) can generate pseudo-labels for human-human interactions that are more semantically accurate than syntactic parsing alone. The LLM is prompted with interaction-caption pairs from the Who's Waldo dataset and generates synthetic captions corresponding to interactions extracted from CC-News. These synthetic pairs are then used to fine-tune a smaller summarization model, which learns to extract HHI from noisy internet captions.

### Mechanism 2
Training an image captioning model on pseudo-labels generated from internet captions enables effective modeling of the heavy tail of human-human interactions. The pseudo-labels, which are free-text descriptions of HHI, serve as targets for training an encoder-decoder network. The model learns to generate text describing the most salient HHI in an image, going beyond categorical recognition.

### Mechanism 3
Evaluating HHI predictions using metrics that measure textual similarity, factual groundedness, and verb similarity provides a more comprehensive assessment than traditional captioning metrics. The proposed metrics (BLEURT, NLI scores, and verb embedding similarity) are designed to capture different aspects of HHI understanding, such as semantic similarity to ground truth, factual consistency with the image, and semantic similarity of predicted verbs.

## Foundational Learning

- **Knowledge Distillation**: Why needed - To transfer the knowledge of a large language model to a smaller model that can be used for inference on HHI extraction from internet captions. Quick check: What is the main advantage of using knowledge distillation in this context?
- **Encoder-Decoder Networks**: Why needed - To train a model that can generate text describing HHI in images, given pseudo-labels as targets. Quick check: What is the role of the encoder and decoder in this architecture?
- **Evaluation Metrics for Text Generation**: Why needed - To assess the quality of HHI predictions using metrics that capture textual similarity, factual groundedness, and semantic similarity. Quick check: How do the proposed metrics differ from traditional captioning metrics like BLEU or METEOR?

## Architecture Onboarding

- **Component map**: Images + Internet captions → LLM (GPT-Neo-1.3B) → Synthetic caption-interaction pairs → Summarization model (T5-base) → HHI pseudo-labels → Encoder-decoder network (CLIP encoder + GPT2 decoder) → HHI descriptions
- **Critical path**: Image → CLIP encoder → GPT2 decoder → HHI description
- **Design tradeoffs**: Using pseudo-labels from internet captions vs. manually labeled data; Evaluating HHI using proposed metrics vs. traditional captioning metrics; Training a simple encoder-decoder vs. fine-tuning a pretrained captioning model
- **Failure signatures**: Model generates irrelevant or nonsensical HHI descriptions; Evaluation metrics do not correlate with human judgment of HHI quality; Pseudo-labels are too noisy or biased, leading to poor model performance
- **First 3 experiments**:
  1. Train the encoder-decoder network on pseudo-labels and evaluate using the proposed metrics on Waldo and Wenda test set
  2. Compare the performance of the trained model to SOTA captioning and situation recognition models on the imSitu-HHI dataset
  3. Analyze the diversity of predicted HHI descriptions and compare to the diversity of pseudo-labels

## Open Questions the Paper Calls Out

- How would the performance of the HHI understanding model change if it were trained on a larger and more diverse dataset of human-human interactions?
- How would the inclusion of additional contextual information, such as body language or facial expressions, impact the model's ability to understand human-human interactions?
- How would the model's performance change if it were trained to predict multiple interactions per image, rather than just the most salient interaction?

## Limitations

- Reliance on LLM-generated synthetic data introduces uncertainty about the quality and diversity of pseudo-labels
- The approach's performance on interactions not well-represented in training data remains unclear
- The evaluation focuses on English-language data and may not generalize to other languages

## Confidence

- **Medium Confidence**: Knowledge distillation from LLMs can effectively generate pseudo-labels for HHI understanding
- **Medium Confidence**: Training on pseudo-labels enables effective modeling of the heavy tail of HHI beyond categorical recognition
- **Low Confidence**: The proposed evaluation metrics (BLEURT, NLI scores, verb embedding similarity) provide a comprehensive assessment of HHI quality

## Next Checks

1. **Synthetic Data Quality Analysis**: Generate and analyze a sample of synthetic caption-interaction pairs to assess their factual consistency, diversity, and potential biases. Compare the distribution of predicted interactions to the ground truth labels on the Waldo/Wenda test set.

2. **Cross-Dataset Generalization**: Evaluate the trained model on additional datasets or domains not seen during training to assess its ability to generalize to novel HHI scenarios. Analyze failure cases to identify common patterns or limitations.

3. **Human Evaluation Study**: Conduct a human evaluation study to assess the correlation between the proposed metrics and human judgment of HHI quality. Compare the model's predictions to human-generated HHI descriptions for a subset of test images.