---
ver: rpa2
title: 'Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented
  Large Language Models'
arxiv_id: '2309.12767'
source_url: https://arxiv.org/abs/2309.12767
tags:
- reasoning
- query
- arxiv
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel pipeline for multi-hop question answering
  (MHQA) called Furthest-Reasoning-with-Plan-Assessment (FuRePA). The key idea is
  to use a large language model (LLM) to generate reasoning paths and plans, and employ
  an information retriever to iteratively retrieve relevant knowledge.
---

# Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2309.12767
- Source URL: https://arxiv.org/abs/2309.12767
- Reference count: 40
- Outperforms state-of-the-art methods by 10-12% on answer accuracy metrics

## Executive Summary
This paper introduces FuRePA, a novel pipeline for multi-hop question answering that addresses the challenge of error propagation in iterative reasoning with retrieval-augmented large language models. The key innovation is furthest reasoning, which masks previous reasoning paths to prevent the LLM from being misled by potentially flawed earlier thoughts, combined with a Plan Assessor that selects high-quality queries from LLM-generated candidates. Evaluated on three public MHQA datasets, FuRePA demonstrates significant improvements over existing methods, achieving 10-12% better answer accuracy.

## Method Summary
FuRePA operates through an iterative pipeline where a large language model generates reasoning paths and candidate queries, while an information retriever fetches relevant knowledge. The system introduces two key components: furthest reasoning, which masks previous reasoning content to force the LLM to generate fresh chains of thought each iteration, and a Plan Assessor that uses a trained evaluator to select optimal queries from LLM proposals. The method also incorporates temperature adjustment when repetitive queries are detected, encouraging exploration of new reasoning paths.

## Key Results
- Achieves 10-12% improvement in answer accuracy compared to state-of-the-art methods
- Outperforms existing approaches on most evaluation metrics across three public MHQA datasets
- Demonstrates effective error prevention through furthest reasoning mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking previous reasoning paths prevents LLM from being misled by irrelevant evidence.
- Mechanism: At each iteration, the LLM only receives retrieved evidence and the original question, forcing it to reason from scratch rather than continuing from potentially flawed previous reasoning.
- Core assumption: Previous reasoning chains contain errors that compound over iterations and degrade performance.
- Evidence anchors:
  - [abstract] "Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration."
  - [section 3.1.2] "Unlike many other general methods, we mask the previous reasoning content and executed query because former reasoning and plans have negative effect on the multi-step reasoning process"
  - [corpus] Weak evidence - no direct citations found, but related work exists on iterative reasoning errors
- Break condition: If masking removes too much context, LLM may need more iterations to reach correct answer, increasing computational cost.

### Mechanism 2
- Claim: Plan Assessor improves query quality by selecting high-quality queries from LLM-generated candidates.
- Mechanism: Trained scorer estimates MRR quality of each query against the corpus, selecting the query most likely to retrieve relevant documents.
- Core assumption: LLM generates multiple queries with varying quality, and MRR correlates with actual retrieval performance.
- Evidence anchors:
  - [abstract] "The Plan Assessor is a trained evaluator that selects an appropriate plan from a group of candidate plans proposed by LLM."
  - [section 3.2.3] "The query scorer calculates the score representing the quality of a query in a certain context" with MRR-based training
  - [corpus] Weak evidence - no direct citations found for MRR-based query selection in multi-hop QA
- Break condition: If the scorer is poorly trained or the corpus distribution differs significantly from training data, query selection may degrade.

### Mechanism 3
- Claim: Temperature increase prevents repetitive low-quality queries when LLM gets stuck.
- Mechanism: When identical queries are detected through clustering, temperature increases to force more diverse generation.
- Core assumption: Repetitive queries indicate LLM is stuck in a local minimum and needs exploration to find better paths.
- Evidence anchors:
  - [section 3.2.2] "When identical queries are generated repetitively... Temperature raise (decoding probability threshold change) replaces the greedy decoding used in chain-of-thought prompting"
  - [abstract] "Temperature raise (decoding probability threshold change) replaces the greedy decoding used in chain-of-thought prompting"
  - [corpus] Weak evidence - temperature-based exploration is common in generation but not specifically documented for this use case
- Break condition: Excessive temperature increase may lead to completely off-topic queries that harm rather than help reasoning.

## Foundational Learning

- Concept: Chain of Thought (CoT) prompting
  - Why needed here: Enables LLM to generate intermediate reasoning steps rather than jumping directly to answers
  - Quick check question: What are the benefits and drawbacks of using CoT in multi-hop QA compared to direct answering?

- Concept: Information Retrieval (IR) quality assessment
  - Why needed here: FuRePA needs to evaluate query quality to select the best candidates for evidence retrieval
  - Quick check question: How does MRR (Mean Reciprocal Rank) differ from precision/recall metrics for query evaluation?

- Concept: Iterative reasoning and error propagation
  - Why needed here: Understanding how errors compound in multi-step reasoning is crucial for designing mitigation strategies
  - Quick check question: In a 3-hop reasoning task, if each step has 20% error rate, what's the probability of reaching the correct final answer?

## Architecture Onboarding

- Component map:
  - LLM (gpt-3.5-turbo) -> Plan Assessor -> IR (TART retriever) -> Evidence -> LLM (next iteration)

- Critical path: Question → LLM generation → Plan Assessor → IR → Evidence → LLM generation (next iteration)

- Design tradeoffs:
  - Masking vs. context: Masking prevents error propagation but may require more iterations
  - Plan Assessor complexity vs. performance: More sophisticated assessors improve quality but add latency
  - Temperature sensitivity: Finding right temperature schedule balances exploration and convergence

- Failure signatures:
  - Repeated identical queries → temperature increase needed
  - Consistently poor MRR scores → Plan Assessor may be poorly trained
  - Excessive iterations without answer → masking may be too aggressive

- First 3 experiments:
  1. Baseline comparison: Run with masking disabled to quantify error propagation cost
  2. Query scorer ablation: Compare with random query selection to measure Plan Assessor impact
  3. Temperature sensitivity: Test different temperature schedules to find optimal balance between exploration and convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of FuRePA on answer accuracy when tested on larger datasets and open-domain knowledge sources?
- Basis in paper: [inferred] from "Limitation" section discussing that experiments were done with a small proportion of datasets and the model relies heavily on the capability of LLMs.
- Why unresolved: The paper only tests FuRePA on a small subset of three datasets due to high API costs. The authors mention the need to study performance on larger datasets and open-source LLMs.
- What evidence would resolve it: Evaluating FuRePA on the full datasets and additional open-domain datasets, comparing performance to state-of-the-art methods. Testing with open-source LLMs like LLaMA or BLOOM.

### Open Question 2
- Question: How does FuRePA handle situations where the information retriever retrieves irrelevant or unhelpful documents that lead the reasoning astray?
- Basis in paper: [explicit] from "Limitation" section stating that when IR retrieves irrelevant documents, FuRePA needs modification to handle this.
- Why unresolved: The paper does not provide details on how FuRePA would be modified to handle irrelevant documents from IR. It only mentions this as a limitation.
- What evidence would resolve it: Demonstrating FuRePA's performance when IR retrieves irrelevant documents. Showing modifications made to handle such cases and their impact on reasoning accuracy.

### Open Question 3
- Question: What is the effect of changing the temperature parameter on the diversity and quality of reasoning paths and plans generated by the LLM?
- Basis in paper: [inferred] from "Implementation Details" section mentioning temperature tp0 of 0.2/1.0 and increase ∆tp of 0.8/0.5.
- Why unresolved: The paper does not provide an analysis of how different temperature settings affect the generated reasoning paths and plans. It only states the chosen values.
- What evidence would resolve it: Conducting experiments with varying temperature parameters and analyzing the impact on the diversity and quality of generated reasoning paths and plans. Comparing performance metrics across different temperature settings.

## Limitations

- The masking mechanism's optimal aggressiveness remains unclear - excessive masking could increase computational cost
- Plan Assessor's training methodology lacks detail on MRR calibration and validation against human judgment
- Temperature adjustment appears ad hoc without theoretical grounding for specific thresholds used

## Confidence

**High Confidence** in the core contribution: The masking strategy for preventing error propagation in iterative reasoning is novel and theoretically sound, supported by the observed 10-12% accuracy improvements.

**Medium Confidence** in the Plan Assessor: While the concept of query scoring is reasonable, the paper lacks ablation studies showing the specific contribution of the trained scorer versus simpler heuristics.

**Low Confidence** in temperature adjustment: The mechanism is described superficially without quantitative analysis of its impact on query diversity or reasoning quality.

## Next Checks

1. **Ablation study on masking intensity**: Systematically vary the amount of previous reasoning content retained versus masked to identify the optimal balance between error prevention and context preservation, measuring both accuracy and token efficiency.

2. **Plan Assessor robustness test**: Evaluate the trained scorer on queries generated under different temperature settings and reasoning paths to determine if it maintains performance across diverse LLM behaviors, or if it overfits to specific generation patterns.

3. **Long-horizon reasoning analysis**: Test FuRePA on questions requiring 4+ reasoning steps to determine if the masking mechanism scales effectively or if error accumulation becomes problematic despite the preventative measures.