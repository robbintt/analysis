---
ver: rpa2
title: Unlocking Accuracy and Fairness in Differentially Private Image Classification
arxiv_id: '2308.10888'
source_url: https://arxiv.org/abs/2308.10888
tags:
- private
- accuracy
- privacy
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that pre-trained foundation models fine-tuned
  with differential privacy (DP) can achieve accuracy comparable to non-private classifiers,
  even on datasets with significant distribution shifts from pre-training data. The
  authors achieve private accuracies within a few percent of the non-private state
  of the art across four datasets, including two medical imaging benchmarks (CheXpert
  and MIMIC-CXR).
---

# Unlocking Accuracy and Fairness in Differentially Private Image Classification

## Quick Facts
- **arXiv ID**: 2308.10888
- **Source URL**: https://arxiv.org/abs/2308.10888
- **Reference count**: 40
- **Primary result**: DP fine-tuned models achieve accuracy close to non-private baselines on medical imaging datasets while maintaining fairness

## Executive Summary
This paper demonstrates that pre-trained foundation models fine-tuned with differential privacy can achieve accuracy comparable to non-private classifiers, even on datasets with significant distribution shifts from pre-training data. The authors achieve private accuracies within a few percent of the non-private state of the art across four datasets, including two medical imaging benchmarks (CheXpert and MIMIC-CXR). For instance, on CheXpert, their private model achieves 89.24% AUC at ε=8, compared to 89.97% for the non-private baseline and 93.0% for the state-of-the-art. Importantly, the private medical classifiers do not exhibit larger performance disparities across demographic groups than non-private models, challenging the belief that DP training necessarily exacerbates fairness issues.

## Method Summary
The authors fine-tune pre-trained NFNet models using DP-SGD with gradient clipping (norm C=1 for ImageNet/Places-365, C=0.001 for CheXpert/MIMIC-CXR), extremely large batch sizes (up to 262,144), gradient accumulation, and EMA with rate 0.9999. Privacy accounting uses Rényi DP with δ=8e-7 (ImageNet/Places-365) or δ=1/N (CheXpert/MIMIC-CXR). Models are fine-tuned on ImageNet-1k, Places-365, CheXpert, and MIMIC-CXR with varying ε values (0.5, 1, 2, 4, 8). Fairness is evaluated by computing AUC disparities across demographic subgroups (age, sex, race, insurance).

## Key Results
- Private models achieve 89.24% AUC on CheXpert at ε=8 vs 89.97% for non-private baseline
- Private accuracies within 4-5% of non-private state-of-the-art across all datasets
- No larger fairness disparities observed in private vs non-private models
- DP training maintains accuracy even with distribution shifts from pre-training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-trained foundation models fine-tuned with DP can achieve accuracy close to non-private models even under distribution shifts.
- **Mechanism**: Transfer learning from large-scale public datasets enables effective downstream fine-tuning under differential privacy constraints, reducing the impact of noise added during DP training.
- **Core assumption**: Public pre-training data is sufficiently diverse and representative to provide useful features for downstream tasks, even when the downstream task distribution differs significantly from the pre-training distribution.
- **Evidence anchors**: Abstract shows similar accuracy to non-private classifiers under distribution shifts; section 3 shows no larger disparities in private models; corpus provides weak evidence on distribution shift mitigation.
- **Break Condition**: If the distribution shift between pre-training and fine-tuning data is too extreme, the transferred features become irrelevant and DP noise overwhelms learning.

### Mechanism 2
- **Claim**: Larger batch sizes and gradient accumulation significantly improve DP training accuracy.
- **Mechanism**: Increasing batch size reduces the relative noise contribution per example when Gaussian noise is added for privacy, while gradient accumulation maintains computational efficiency.
- **Core assumption**: The computational infrastructure can handle larger batches and gradient accumulation without memory overflow or excessive slowdown.
- **Evidence anchors**: Section 4 describes use of extremely large batch sizes; section C.8 found significant performance improvement; corpus provides moderate evidence on large batch benefits.
- **Break Condition**: If batch size becomes too large relative to dataset size, sampling becomes deterministic and privacy guarantees degrade.

### Mechanism 3
- **Claim**: Exponential Moving Average (EMA) of model parameters improves convergence under DP training.
- **Mechanism**: EMA smooths parameter updates by averaging across training steps, reducing variance from noisy DP gradients and improving generalization.
- **Core assumption**: The EMA decay rate is properly tuned to balance between recent and historical parameter values.
- **Evidence anchors**: Section C.8 found EMA consistently improved accuracy by increasing convergence rate; section 3 used EMA with parameter 0.9; corpus provides moderate evidence on EMA benefits for DP.
- **Break Condition**: If EMA decay is too aggressive, the model loses responsiveness to new gradient information.

## Foundational Learning

- **Concept**: Differential Privacy (DP) guarantees
  - **Why needed**: Understanding DP is essential to grasp why accuracy drops occur and how the mechanisms work to mitigate them.
  - **Quick check**: What does an ε=8 guarantee mean in terms of membership inference protection?

- **Concept**: Transfer learning and pre-training
  - **Why needed**: The core innovation relies on leveraging pre-trained models, so understanding feature transferability is crucial.
  - **Quick check**: Why does pre-training on ImageNet-21K help with medical image classification?

- **Concept**: Gradient clipping and noise addition in DP-SGD
  - **Why needed**: These are the fundamental operations that create the privacy-utility tradeoff being optimized.
  - **Quick check**: How does clipping norm C affect the scale of noise needed for a given ε?

## Architecture Onboarding

- **Component map**: Data loading -> Augmentation -> Per-example gradient computation with clipping -> Synchronized noise addition -> Parameter updates -> EMA tracking -> Privacy accounting
- **Critical path**: The most performance-sensitive components are per-example gradient computation and synchronized noise addition across devices. Any bottleneck here directly impacts training speed and accuracy.
- **Design tradeoffs**: Larger batches improve DP accuracy but require more memory and may reduce privacy per example. EMA improves convergence but adds memory overhead and hyperparameter tuning complexity.
- **Failure signatures**: If accuracy plateaus early, check gradient clipping norm and noise scale. If training is extremely slow, investigate per-example gradient computation efficiency. If privacy budget is exhausted prematurely, verify accounting implementation.
- **First 3 experiments**:
  1. Verify DP-SGD implementation by training on MNIST with varying ε and confirming accuracy degradation matches theoretical expectations.
  2. Test EMA implementation by comparing convergence speed with and without EMA on a small dataset.
  3. Validate privacy accounting by auditing with membership inference attacks on a known dataset.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does differentially private training consistently exacerbate fairness disparities across all datasets and tasks, or is the observed effect specific to certain data distributions and model architectures? The authors observe that their accurate private models do not cause worse group fairness outcomes than non-private baselines on medical imaging tasks, contrasting with prior work that suggested DP training could exacerbate disparities. This remains unresolved due to mixed results in prior research and dependence on factors like dataset characteristics and privacy parameters.

- **Open Question 2**: What is the precise mechanism by which differential privacy affects the learning dynamics of models in a way that could potentially impact fairness outcomes? While theoretical connections between DP and fairness have been established, the practical implications for deep learning models and their impact on specific demographic groups remain poorly understood. Detailed analysis of how DP mechanisms differentially affect gradient updates for various subgroups is needed.

- **Open Question 3**: How does the relationship between data imbalance and fairness disparities in differentially private training change as the level of privacy guarantee (epsilon) varies? The authors note that when classes are extremely underrepresented, private models may fail to predict that class at all. Prior work has shown that data imbalance can exacerbate fairness issues, but the interaction with varying privacy budgets and specific thresholds where DP training breaks down for minority groups are not well characterized.

## Limitations
- Pre-training details for NFNet models are not fully specified, particularly exact training procedures for ImageNet-21K and JFT-4B pre-training.
- Demographic attributes used for fairness analysis are limited to age, sex, race, and insurance type, potentially missing other relevant subgroups.
- Study focuses on AUC metrics but doesn't explore other fairness measures like equal opportunity or equalized odds.

## Confidence

- **High Confidence**: The core claim that DP training can achieve accuracy close to non-private models is well-supported by empirical results across multiple datasets and ε values.
- **Medium Confidence**: The fairness analysis showing no significant disparities between private and non-private models is credible but could benefit from additional fairness metrics and subgroup definitions.
- **Medium Confidence**: The mechanisms proposed (large batch sizes, EMA, gradient accumulation) are supported by evidence but would benefit from ablation studies isolating each component's contribution.

## Next Checks
1. **Reproduce DP-SGD Implementation**: Implement DP-SGD with the specified hyperparameters (batch size, clipping norm, EMA rate) on a small dataset (e.g., CIFAR-10) and verify accuracy degradation matches theoretical expectations.
2. **Validate Privacy Accounting**: Cross-check ε values computed using the provided privacy library with an independent implementation (e.g., using TensorFlow Privacy) to ensure correctness.
3. **Explore Additional Fairness Metrics**: Compute fairness metrics beyond AUC disparities (e.g., equal opportunity difference, demographic parity difference) across demographic subgroups to provide a more comprehensive fairness analysis.