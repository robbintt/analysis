---
ver: rpa2
title: On the Behavior of Audio-Visual Fusion Architectures in Identity Verification
  Tasks
arxiv_id: '2311.05071'
source_url: https://arxiv.org/abs/2311.05071
tags:
- fusion
- audio
- arxiv
- embeddings
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates modifications to audio-visual fusion architectures
  for identity verification, particularly in scenarios with missing modalities. The
  authors compare three fusion techniques: multi-view (shared embedding layer), mean
  fusion (averaging embeddings), and MLP fusion (concatenation and MLP).'
---

# On the Behavior of Audio-Visual Fusion Architectures in Identity Verification Tasks

## Quick Facts
- **arXiv ID**: 2311.05071
- **Source URL**: https://arxiv.org/abs/2311.05071
- **Reference count**: 40
- **Key outcome**: Mean fusion improves error rates in full-modality and single-modality-missing scenarios compared to multi-view and MLP fusion.

## Executive Summary
This work evaluates three audio-visual fusion architectures for identity verification: multi-view (shared embedding layer), mean fusion (averaging embeddings), and MLP fusion (concatenation and MLP). Experiments on Voxceleb1-E test set show mean fusion achieves lower equal error rates (EER) than multi-view and MLP fusion across various modality scenarios, including missing and cross-modal cases. The paper analyzes embedding distributions and finds that mean fusion exploits greater flexibility in embedding placement, allowing audio and video embeddings to occupy different regions of the embedding space while maintaining competitive performance.

## Method Summary
The authors train three fusion architectures on VoxCeleb2 using arc-margin loss with identical backbones (ResNet-50 for video, MobileNetV2 for audio). The three fusion methods compared are: multi-view fusion with shared classification layer, mean fusion averaging embeddings, and MLP fusion concatenating and processing embeddings through a neural network. Models are evaluated on Voxceleb1-E test set across five modality modes: A×A, V×V, AV×AV, AV×A, and AV×V. Random modality masking during training improves robustness to missing modalities.

## Key Results
- Mean fusion achieves lowest EER across all modality scenarios on Voxceleb1-E test set
- Mean fusion produces larger angles between audio and video embeddings compared to multi-view and MLP fusion
- MLP fusion shows tightest within-cluster separation but highest EER, suggesting potential overfitting
- Multi-view fusion underperforms in separating video embeddings due to shared classification layer bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean fusion achieves better EER than multi-view and MLP fusion by exploiting greater flexibility in embedding placement, allowing audio and video embeddings to occupy different regions of the embedding space while still achieving low loss.
- Mechanism: Mean fusion allows the network to learn embeddings that, when averaged, fall close to the target class centroid. This provides flexibility for the audio and video embeddings to be far apart in angle while still producing a correct classification, unlike multi-view fusion which constrains both modalities to map through shared weights, encouraging similarity between embeddings.
- Core assumption: The arc-margin loss permits any pair of embeddings that average to a vector close in angle to the target class to have low loss, and the network will exploit this flexibility during training.
- Evidence anchors:
  - [abstract] "mean fusion exploits greater flexibility in embedding placement, achieving higher angles between audio and video embeddings while maintaining competitive error rates."
  - [section] "For a 3-second clip from each video, we compute the absolute angle between the audio embedding and the video embedding... We can see that in general, the angle between the audio and video embeddings for the mean fusion model is larger than that of the multi-view or MLP models."
  - [corpus] Weak evidence; related papers focus on fusion strategies but do not specifically analyze angle distributions or embedding flexibility.
- Break condition: If the arc-margin loss is modified to require the individual embeddings (not just their average) to be close to the class centroid, the advantage of mean fusion would disappear.

### Mechanism 2
- Claim: Multi-view fusion underperforms mean fusion in separating video embeddings because the shared classification layer is regularized by the audio task, limiting its capacity to evenly distribute video classes in the embedding space.
- Mechanism: In multi-view fusion, both audio and video embeddings pass through the same classification layer. The layer must balance performance across both modalities, so improvements for video may degrade audio performance. This acts as a bottleneck, preventing optimal separation of video classes even if the visual backbone could support it.
- Core assumption: The shared classification layer has limited capacity, and optimizing for one modality (e.g., audio) constrains the ability to optimize for the other (video).
- Evidence anchors:
  - [section] "One possible reason that the multi-view model cannot separate classes as well is due to the limited capacity of the shared classification layer —the network would like to separate two video classes, but cannot without degrading performance in the more difficult audio task."
  - [section] "The classification layer of multi-view, from the perspective of the visual stream, is in a way regularized by the audio task."
  - [corpus] Weak evidence; no direct support found in related papers for capacity limitations of shared layers in multimodal settings.
- Break condition: If the shared classification layer is made significantly wider or if modality-specific layers are added after the shared layer, the performance gap may narrow.

### Mechanism 3
- Claim: MLP fusion, despite achieving the tightest within-cluster and most even between-cluster separation in UMAP space, does not yield the best EER because it may overfit to the training data or fail to generalize the discriminative structure to unseen examples.
- Mechanism: The MLP fusion learns a complex, non-linear combination of audio and video embeddings. While this can create well-separated clusters in training, the network may not learn robust, generalizable features, leading to higher error on the test set despite favorable embedding geometry.
- Core assumption: Good clustering in embedding space (as measured by silhouette score or UMAP visualization) does not necessarily translate to better verification performance if the network overfits or fails to learn discriminative features.
- Evidence anchors:
  - [section] "The MLP model packs embeddings within a speaker closer together... It may be that while the MLP model has made more complete use of the embedding space, it has still confused several audio examples, leading to low EER."
  - [section] "Evaluating the performance of each clustering with silhouette scores appears to confirm this visual inspection. The mean fusion clustering achieves the highest silhouette score... This apparent difficulty in separating clusters evenly may be the cause of the lower performance of the multi-view method in all of the video tasks."
  - [corpus] No direct evidence; related papers do not discuss the relationship between clustering quality and verification EER.
- Break condition: If regularization (e.g., stronger dropout or weight decay) is applied to the MLP fusion, the overfitting issue may be mitigated and EER could improve.

## Foundational Learning

- Concept: Arc-margin loss function
  - Why needed here: The arc-margin loss is central to how the network learns to cluster embeddings by class and is the key to understanding why mean fusion can exploit embedding flexibility.
  - Quick check question: What is the primary geometric effect of arc-margin loss on embeddings from the same class versus different classes?

- Concept: Embedding space geometry and angle-based similarity
  - Why needed here: The paper's core analysis revolves around the angles between audio and video embeddings and how these relate to verification performance.
  - Quick check question: How does the angle between two embeddings relate to their cosine similarity and what does a large angle imply about their relationship?

- Concept: Multi-modal fusion strategies (early, late, hybrid)
  - Why needed here: The paper compares different fusion strategies (mean, MLP, multi-view) and their impact on performance, requiring understanding of when and why to fuse modalities.
  - Quick check question: What is the key difference between mean fusion and multi-view fusion in terms of how they combine modality-specific embeddings?

## Architecture Onboarding

- Component map:
  - Audio backbone: MobileNetV2 processing log-mel spectrograms → 356-dim embedding
  - Video backbone: ResNet50 variant processing face crops → 2048-dim embedding
  - Fusion module: Three variants (mean, MLP, multi-view) combining embeddings → 256-dim fused embedding
  - Classification head: Arc-margin loss layer mapping to speaker identities
  - Data pipeline: VoxCeleb2 for training, VoxCeleb1-E for testing; random modality masking during training

- Critical path:
  1. Extract and preprocess audio/video inputs
  2. Pass through modality-specific backbones to get embeddings
  3. Apply fusion strategy (mean/MLP/multi-view)
  4. Compute cosine similarity between fused embeddings for verification
  5. Apply arc-margin loss during training for speaker classification

- Design tradeoffs:
  - Mean fusion: Simple, flexible, allows disparate embeddings; risk of losing fine-grained cross-modal correlations
  - MLP fusion: Learns complex interactions; risk of overfitting and poor generalization
  - Multi-view fusion: Enforces cross-modal consistency; risk of bottlenecking by shared layer capacity
  - Random modality masking: Improves robustness to missing modalities; adds training complexity

- Failure signatures:
  - High EER with large within-speaker angle variance → poor embedding consistency
  - High EER with small between-speaker angle variance → poor inter-class separation
  - Mean fusion underperforming → arc-margin loss not flexible enough or embeddings not well normalized
  - MLP fusion overfitting → complex fusion model memorizing training data

- First 3 experiments:
  1. Train and evaluate all three fusion variants (mean, MLP, multi-view) on full-modality VoxCeleb1-E test set; compare EER and angle distributions.
  2. Evaluate each model under missing-modality conditions (AxA, VxV, A VxA, A VxV) to confirm robustness claims.
  3. Visualize UMAP embeddings for each model/modality pair to inspect within- and between-speaker clustering quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MLP fusion architecture compare to mean fusion in terms of computational efficiency and model complexity during training and inference?
- Basis in paper: [inferred] The paper compares MLP fusion to mean fusion and multi-view fusion, but does not provide explicit details on computational efficiency or model complexity.
- Why unresolved: The paper focuses on the performance of the architectures in terms of error rates and embedding distributions, but does not delve into the computational aspects of each fusion method.
- What evidence would resolve it: Detailed analysis of the computational requirements (e.g., FLOPs, memory usage) and training/inference times for each fusion architecture would provide insights into their efficiency and complexity.

### Open Question 2
- Question: How does the performance of the fusion architectures vary with different lengths of audio and video clips?
- Basis in paper: [explicit] The paper uses 3-second clips for evaluation but does not explore the impact of varying clip lengths on the performance of the fusion architectures.
- Why unresolved: The study is limited to a fixed clip length, and the authors do not discuss the potential effects of different clip durations on the models' performance.
- What evidence would resolve it: Experiments with varying clip lengths and analysis of the corresponding performance metrics (e.g., EER) would reveal how clip duration influences the effectiveness of each fusion architecture.

### Open Question 3
- Question: What are the specific reasons for the observed differences in the within-speaker and between-speaker variance of embeddings across the fusion architectures?
- Basis in paper: [explicit] The paper discusses the differences in within-speaker and between-speaker variance of embeddings but does not provide a detailed explanation for these observations.
- Why unresolved: While the paper identifies the variance differences, it does not delve into the underlying causes or mechanisms that lead to these variations in the embedding spaces.
- What evidence would resolve it: A deeper analysis of the training dynamics, such as gradient updates and weight adjustments, for each fusion architecture could shed light on the factors contributing to the observed variance differences.

## Limitations
- Core claims about mean fusion's superiority rest on correlational analyses rather than rigorous causal proof
- Paper does not provide ablation studies on regularization or model complexity to isolate overfitting effects in MLP fusion
- Analysis focuses exclusively on Voxceleb data, limiting generalizability to other domains or tasks

## Confidence

- **High confidence**: Empirical EER comparisons across fusion methods on Voxceleb1-E test set; direct measurements of audio-video embedding angles for each fusion type
- **Medium confidence**: Interpretation of clustering quality (silhouette scores, UMAP visualizations) as explanatory for EER differences; claims about shared classification layer capacity limitations in multi-view fusion
- **Low confidence**: Mechanistic explanations for why good clustering doesn't translate to better EER in MLP fusion; causal claims about embedding flexibility without controlled ablation experiments

## Next Checks

1. Conduct ablation studies varying the capacity of the shared classification layer in multi-view fusion to test the bottleneck hypothesis
2. Apply stronger regularization (dropout, weight decay) to MLP fusion and measure changes in both clustering quality and EER to isolate overfitting effects
3. Modify the arc-margin loss to constrain individual embeddings (not just their average) to be close to class centroids, and measure impact on mean fusion's advantage