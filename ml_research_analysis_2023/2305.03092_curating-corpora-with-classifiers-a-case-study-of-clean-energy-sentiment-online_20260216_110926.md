---
ver: rpa2
title: 'Curating corpora with classifiers: A case study of clean energy sentiment
  online'
arxiv_id: '2305.03092'
source_url: https://arxiv.org/abs/2305.03092
tags:
- tweets
- sentiment
- corpus
- relevant
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using pre-trained transformer-based models
  fine-tuned for binary relevance classification as a way to curate social media corpora
  for computational social science research. The authors demonstrate this approach
  using a case study on public sentiment towards clean energy technologies.
---

# Curating corpora with classifiers: A case study of clean energy sentiment online

## Quick Facts
- arXiv ID: 2305.03092
- Source URL: https://arxiv.org/abs/2305.03092
- Reference count: 0
- Primary result: Fine-tuning pre-trained transformer models on labeled tweets achieves F1 scores up to 0.95 for relevance classification

## Executive Summary
This paper proposes using pre-trained transformer-based models fine-tuned for binary relevance classification as a way to curate social media corpora for computational social science research. The authors demonstrate this approach using a case study on public sentiment towards clean energy technologies. They show that fine-tuning models like MPNet and DistilBERT on hand-labeled tweets can achieve F1 scores up to 0.95, effectively separating relevant tweets from irrelevant ones that match broad keyword queries. By comparing sentiment time series and linguistic analyses before and after classification, they illustrate how their method provides more accurate measurements of public opinion. The low computational cost and high performance of this approach suggests it could be broadly beneficial for researchers dealing with uncertain corpus boundaries in social media datasets.

## Method Summary
The method involves collecting tweets containing specific keywords (wind, solar, nuclear) using the Twitter Decahose API, then hand-labeling a random sample of 1000 tweets per keyword as relevant or non-relevant to clean energy. Pre-trained transformer models (MPNet, DistilBERT) are fine-tuned on this labeled data for binary relevance classification. The fine-tuned models are then used to classify all collected tweets, filtering out non-relevant content before sentiment analysis using the LabMT dictionary. The approach is validated by comparing sentiment measurements and linguistic patterns before and after classification, and by visualizing tweet embeddings in semantic space using UMAP.

## Key Results
- Fine-tuned transformer models achieve F1 scores up to 0.95 for binary relevance classification
- Classification filtering significantly improves sentiment measurement accuracy by removing irrelevant content
- Semantic embeddings from fine-tuned models show clear separation between relevant and non-relevant tweets in visualization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained transformer models on small labeled datasets produces high-precision relevance classifiers for social media corpora.
- Mechanism: Pre-trained transformers like MPNet and DistilBERT have already learned rich contextual representations of language from large corpora. Fine-tuning these models on a small set of hand-labeled examples (e.g., 1,000 tweets) allows them to adapt these representations to the specific task of distinguishing relevant from irrelevant content based on semantic context rather than keyword matching alone.
- Core assumption: The semantic boundaries between relevant and non-relevant content are learnable from a small set of labeled examples, and the pre-trained model's contextual embeddings capture the necessary linguistic features.
- Evidence anchors:
  - [abstract] "We are able to achieve F1 scores of up to 0.95" and "fine-tuning models like MPNet and DistilBERT on hand-labeled tweets"
  - [section] "We hand-label a random sample of 1000 matching tweets for each keyword as either 'Relevant' or 'Non-Relevant to energy production"
- Break condition: If the relevant/non-relevant distinction requires knowledge not captured in the pre-training corpus, or if the labeled examples don't represent the full range of relevant content.

### Mechanism 2
- Claim: Sentence embeddings from fine-tuned transformers enable effective visualization and clustering of social media content in semantic space.
- Mechanism: The fine-tuned models produce embeddings where semantically similar tweets cluster together in the embedding space. This allows researchers to visually inspect and understand the distribution of relevant and non-relevant content, and to identify patterns that keyword-based approaches miss.
- Core assumption: The embedding space preserves meaningful semantic relationships between tweets, and UMAP projection maintains local structure for visualization purposes.
- Evidence anchors:
  - [section] "We compute embeddings using all-mpnet-base-v2... and project onto two dimensions for visualization using UMAP" and "Tweets classiï¬ed as relevant to clean energy technologies are clustered on the right-hand side"
  - [corpus] The visualizations in Figures 1A and 1B show clear separation between relevant and non-relevant tweets in the embedding space
- Break condition: If the embedding space becomes too high-dimensional for effective visualization, or if the semantic relationships don't align with the relevance classification task.

### Mechanism 3
- Claim: Using transformer-based classifiers as a preprocessing step dramatically improves sentiment analysis accuracy by removing irrelevant content that would otherwise skew measurements.
- Mechanism: By filtering out non-relevant tweets before sentiment analysis, the method ensures that sentiment measurements reflect actual opinions about the target topic rather than unrelated uses of the same keywords. This is particularly important for ambiguous terms that have both relevant and non-relevant meanings.
- Core assumption: The proportion of non-relevant content is significant enough to impact sentiment measurements, and the classifier can reliably distinguish between the two types of content.
- Evidence anchors:
  - [abstract] "We illustrate how their method provides more accurate measurements of public opinion" and the comparison of sentiment time series before and after classification
  - [section] "We found that this shift in language use in the NR corpus occurs without a change in query terms, and demonstrates how simple keyword queries can fail"
- Break condition: If the proportion of non-relevant content is too small to impact measurements, or if the classifier makes too many errors and removes relevant content.

## Foundational Learning

- Concept: Pre-trained transformer models and fine-tuning
  - Why needed here: The approach relies on leveraging pre-trained models like MPNet and DistilBERT rather than training from scratch, which makes the method computationally feasible and effective with limited labeled data.
  - Quick check question: What is the key advantage of using pre-trained transformer models for this task compared to training a classifier from scratch?

- Concept: Semantic textual similarity and contextual embeddings
  - Why needed here: The method uses sentence embeddings to represent tweets in a semantic space where relevant and non-relevant content can be distinguished based on meaning rather than keyword matching.
  - Quick check question: How do contextual embeddings differ from traditional bag-of-words approaches in representing the meaning of text?

- Concept: Corpus curation and relevance classification
  - Why needed here: The fundamental problem being solved is how to define the boundaries of a corpus for computational social science research when simple keyword queries are insufficient.
  - Quick check question: Why is it important to distinguish between relevant and non-relevant tweets when studying public sentiment about clean energy technologies?

## Architecture Onboarding

- Component map: Twitter API queries -> Relevance labeling -> Model fine-tuning -> Classification -> Sentiment analysis -> Visualization
- Critical path: Hand-labeled data -> Fine-tuning -> Classification -> Sentiment measurement. The quality of the labeled data directly impacts classifier performance, which in turn affects the accuracy of sentiment measurements.
- Design tradeoffs: Higher precision vs. higher recall in classification, computational cost of fine-tuning vs. accuracy gains, amount of labeled data vs. model performance, keyword-based initial queries vs. fully unsupervised approaches.
- Failure signatures: Sentiment measurements that don't match expectations, classifier that doesn't improve over baseline keyword filtering, embeddings that don't show clear separation between relevant and non-relevant content, models that are too computationally expensive for the available resources.
- First 3 experiments:
  1. Test the classification performance on a held-out set of labeled tweets to verify F1 scores around 0.95
  2. Compare sentiment measurements with and without classification filtering on a small time window
  3. Visualize the embedding space for a small subset of tweets to verify that relevant and non-relevant content separates as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can researchers determine the optimal balance between recall and precision when curating social media datasets for computational social science research?
- Basis in paper: [explicit] The paper discusses the trade-off between recall and precision when curating social media corpora, noting that expansive keyword queries increase recall but reduce precision, and suggests using transformer-based classifiers to improve precision.
- Why unresolved: While the paper demonstrates the effectiveness of fine-tuned transformer models in achieving high precision, it does not provide a systematic approach or criteria for determining the optimal balance between recall and precision based on the specific research goals and dataset characteristics.
- What evidence would resolve it: Empirical studies comparing the performance of different corpus curation methods (e.g., keyword queries, transformer classifiers) across various research domains and datasets, along with guidelines for selecting the appropriate method based on factors such as topic specificity, dataset size, and desired measurement accuracy.

### Open Question 2
- Question: How can researchers effectively handle the dynamic nature of social media language and evolving topics when using pre-trained transformer models for corpus curation?
- Basis in paper: [inferred] The paper mentions that social media language use can change over time, as evidenced by the shift in non-relevant tweets containing the keyword 'solar' after 2019. It also suggests that measuring changes in language use and the distribution of messages within semantic embeddings could help determine when more training data is needed.
- Why unresolved: While the paper acknowledges the challenge of adapting to changing language use, it does not provide a comprehensive framework or automated methods for monitoring and updating the corpus curation process to ensure continued accuracy as topics and language evolve.
- What evidence would resolve it: Development and evaluation of adaptive corpus curation techniques that automatically detect changes in language use and update the transformer models accordingly, as well as empirical studies assessing the effectiveness of these techniques in maintaining high precision and recall over time.

### Open Question 3
- Question: How can researchers effectively communicate the uncertainty and potential biases associated with corpus curation methods to readers and reviewers?
- Basis in paper: [explicit] The paper emphasizes the importance of transparently communicating the corpus curation process, stating that reviewers and skeptical readers should be empowered to make their own judgments by labeling tweets and comparing the resulting text measurements.
- Why unresolved: While the paper advocates for transparency, it does not provide specific guidelines or best practices for researchers to effectively communicate the limitations, assumptions, and potential biases of their corpus curation methods to ensure the validity and reproducibility of their findings.
- What evidence would resolve it: Development of standardized reporting guidelines for corpus curation methods, including detailed descriptions of the keyword queries, classification models, and evaluation metrics used, as well as clear statements of the limitations and potential biases associated with the chosen approach. Additionally, empirical studies assessing the impact of different levels of transparency on the interpretation and reproducibility of research findings would be valuable.

## Limitations
- Limited generalizability to other keywords, domains, and social media platforms
- Labor-intensive hand-labeling approach may not scale well for larger keyword sets or multiple languages
- Computational cost analysis focuses on training time rather than inference costs

## Confidence
- Classification performance: High
- Sentiment measurement accuracy improvements: Medium
- Generalizability to other domains: Low

## Next Checks
1. Test the classification approach on a different set of keywords from an unrelated domain (e.g., healthcare or education) to assess generalizability
2. Compare the approach against unsupervised corpus curation methods to quantify the added value of labeled data
3. Evaluate the sentiment measurement improvements on a ground truth dataset with known opinion trends