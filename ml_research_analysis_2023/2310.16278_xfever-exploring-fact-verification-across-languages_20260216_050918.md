---
ver: rpa2
title: 'XFEVER: Exploring Fact Verification across Languages'
arxiv_id: '2310.16278'
source_url: https://arxiv.org/abs/2310.16278
tags:
- languages
- language
- learning
- fact
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces XFEVER, a cross-lingual fact verification
  dataset built by translating the FEVER dataset into six languages (Spanish, French,
  Indonesian, Japanese, Chinese, and English). The training and development sets were
  machine-translated, while the test set includes both machine and professional human
  translations.
---

# XFEVER: Exploring Fact Verification across Languages

## Quick Facts
- arXiv ID: 2310.16278
- Source URL: https://arxiv.org/abs/2310.16278
- Authors: 
- Reference count: 15
- Key outcome: Multilingual language models achieve strong zero-shot cross-lingual fact verification performance, with consistency regularization improving calibration.

## Executive Summary
XFEVER is a cross-lingual fact verification dataset built by translating the English FEVER dataset into six languages: Spanish, French, Indonesian, Japanese, Chinese, and English. The training and development sets were machine-translated using DeepL, while the test set includes both machine and professional human translations. The paper evaluates two cross-lingual scenarios: zero-shot learning (training on English only, testing on all languages) and translate-train learning (training on multilingual data). Using multilingual models like mBERT and XLM-R, the study demonstrates effective cross-lingual transfer and improved calibration through consistency regularization techniques.

## Method Summary
The XFEVER dataset was created by translating the English FEVER dataset into six languages. Two cross-lingual scenarios were evaluated: zero-shot learning where models train only on English and test on all languages, and translate-train learning where models train on multilingual data. Baseline models used multilingual language models (mBERT, XLM-R) with an MLP classifier. Consistency regularization techniques were applied in the translate-train scenario to improve calibration, using prediction consistency (KL, Jensen-Shannon divergence) and representation consistency (MSE, cosine distance) between original and translated examples.

## Key Results
- Multilingual models (mBERT, XLM-R) achieved strong zero-shot performance across all target languages, significantly outperforming monolingual models
- Translate-train learning with consistency regularization improved model calibration, particularly using symmetric divergence measures like Jensen-Shannon
- Human vs. machine translation evaluation showed minimal performance differences (0.3-0.7%), indicating high translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual language models can effectively transfer knowledge from English to five other languages for fact verification.
- Mechanism: Shared multilingual pretraining creates a common semantic space where representations of claims and evidence align across languages, enabling zero-shot transfer.
- Core assumption: The semantic relationships between claims and evidence are language-agnostic and can be captured in a shared representation space.
- Evidence anchors:
  - [abstract] "Experimental results show that the multilingual language model can be used to build fact verification models in different languages efficiently."
  - [section 6.2.1] "As expected, the monolingual PLMs yield high accuracy for the source language (English) but cannot maintain reasonable accuracy for the target languages. The multilingual PLMs help alleviate this issue."
  - [corpus] Weak - related papers focus on translation bias and cross-lingual transfer but don't specifically validate this mechanism for fact verification.
- Break condition: If semantic relationships between claims and evidence are language-dependent (e.g., culturally-specific reasoning patterns), the shared representation space would fail to capture these differences.

### Mechanism 2
- Claim: Translate-train learning with parallel data and consistency regularization improves calibration compared to non-parallel training.
- Mechanism: Regularizing prediction similarity between English and translated examples reduces overconfidence by smoothing the predicted distributions, improving the alignment between confidence and accuracy.
- Core assumption: Prediction consistency across languages naturally introduces a confidence penalty that mitigates miscalibration from cross-entropy loss overfitting.
- Evidence anchors:
  - [abstract] "We also found that we can effectively mitigate model miscalibration by considering the prediction similarity between the English and target languages."
  - [section 5.1] "Our key observation is that the regularization functions of prediction consistency intrinsically introduce the confidence penalty to the loss."
  - [section 6.2.3] "the symmetric divergence measures, J and JS, significantly reduce the ECE scores because they encourage the model to output high entropy for both the original and translated examples."
- Break condition: If the machine translations introduce systematic errors that create inconsistent relationships between claims and evidence across languages, the regularization would enforce incorrect consistency.

### Mechanism 3
- Claim: Machine-translated data can be effectively used for training fact verification models with minimal performance degradation compared to human translations.
- Mechanism: DeepL's high-quality machine translations preserve the semantic relationships between claims and evidence, making the translated pairs valid training examples.
- Core assumption: Machine translation quality is sufficient to maintain the claim-evidence relationship structure across languages.
- Evidence anchors:
  - [abstract] "Human vs. machine translation evaluation showed minimal performance differences, indicating high translation quality."
  - [section 6.2.4] "the average differences are only around 0.3∼0.7%. We attribute these minor discrepancies to DeepL's accurate translations."
  - [section 3.1] "These languages cover several language families, including isolated languages such as Japanese."
- Break condition: If translation quality degrades for certain language pairs or if culturally-specific expressions lose critical meaning in translation, the claim-evidence relationships would be corrupted.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The core contribution is demonstrating that fact verification models can generalize across languages without requiring language-specific training data.
  - Quick check question: What is the key difference between zero-shot learning and translate-train learning in this context?

- Concept: Consistency regularization
  - Why needed here: The paper introduces novel regularization techniques that explicitly enforce prediction consistency across languages to improve calibration.
  - Quick check question: How does prediction consistency regularization relate to confidence penalty methods?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: The paper uses ECE to quantify and demonstrate the calibration improvements achieved through consistency regularization.
  - Quick check question: What does an ECE score represent in the context of fact verification models?

## Architecture Onboarding

- Component map: Data pipeline → Multilingual PLM (mBERT/XLM-R) → MLP classifier → Loss function (cross-entropy + consistency regularization) → Evaluation metrics (accuracy, ECE)
- Critical path: Training involves either zero-shot learning (English-only) or translate-train learning (multilingual), with the latter using parallel data for consistency regularization
- Design tradeoffs: Using machine translations enables large-scale multilingual training but introduces potential quality issues; consistency regularization helps but may not fully compensate for translation errors
- Failure signatures: Poor cross-lingual transfer (low accuracy on non-English languages), high ECE scores indicating miscalibration, minimal improvement from consistency regularization
- First 3 experiments:
  1. Compare monolingual vs. multilingual PLMs in zero-shot setting to validate cross-lingual transfer capability
  2. Test different consistency regularization functions (KL, J, JS) with parallel training to identify most effective approach
  3. Compare machine-translated vs. human-translated test sets to validate translation quality assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of machine-translated data affect the performance of cross-lingual fact verification models, and what are the specific challenges associated with low-resource languages?
- Basis in paper: [explicit] The paper mentions that the XFEVER dataset includes machine-translated data and that the performance varies by language, being somewhat inferior to the English case. It also notes that the multilingual PLMs are helpful when the training set in the target language is unavailable.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of machine-translated data impacts model performance, particularly for low-resource languages. It also does not explore the specific challenges that arise when dealing with such languages.
- What evidence would resolve it: Conducting experiments with varying quality levels of machine-translated data, including low-resource languages, and analyzing the impact on model performance. Additionally, identifying and documenting the specific challenges encountered during these experiments would provide valuable insights.

### Open Question 2
- Question: What are the long-term effects of using consistency regularization techniques, such as the Jensen-Shannon divergence, on the calibration and overall performance of cross-lingual fact verification models?
- Basis in paper: [explicit] The paper discusses the use of consistency regularization to mitigate model miscalibration and shows that symmetric divergence measures like Jensen-Shannon significantly reduce the Expected Calibration Error (ECE).
- Why unresolved: The paper does not explore the long-term effects of using consistency regularization techniques on model calibration and performance. It also does not investigate whether these techniques have any potential drawbacks or limitations.
- What evidence would resolve it: Conducting longitudinal studies to assess the impact of consistency regularization techniques on model calibration and performance over time. Additionally, investigating potential drawbacks or limitations of these techniques through extensive experimentation would provide a comprehensive understanding.

### Open Question 3
- Question: How do different cross-lingual fact verification scenarios, such as zero-shot learning and translate-train learning, compare in terms of efficiency and effectiveness for various language pairs?
- Basis in paper: [explicit] The paper defines two cross-lingual fact verification scenarios (zero-shot learning and translate-train learning) and provides baseline results for each scenario. It mentions that the multilingual PLMs are beneficial in zero-shot learning, while translate-train learning with high-quality machine-translated data can be effective.
- Why unresolved: The paper does not provide a detailed comparison of the efficiency and effectiveness of different cross-lingual fact verification scenarios for various language pairs. It also does not explore the potential trade-offs between these scenarios.
- What evidence would resolve it: Conducting comparative studies of different cross-lingual fact verification scenarios for various language pairs, focusing on efficiency and effectiveness. Additionally, analyzing the trade-offs between these scenarios through extensive experimentation would provide valuable insights.

## Limitations

- The evaluation is based on machine-translated data, and while human translation comparisons show minimal differences, systematic translation artifacts could still influence model performance in ways not captured by the reported metrics.
- The paper focuses on six languages, but the performance on truly low-resource languages (outside this sample) remains unverified, particularly for languages with different script systems or linguistic structures.
- The consistency regularization approach improves calibration but may not generalize to other cross-lingual tasks where the claim-evidence relationship structure differs significantly from fact verification.

## Confidence

- **High Confidence**: The effectiveness of multilingual language models (mBERT, XLM-R) for cross-lingual fact verification, supported by clear experimental results showing strong zero-shot performance.
- **Medium Confidence**: The calibration improvements from consistency regularization, as the mechanism is well-explained but the exact contribution of each divergence measure requires further validation.
- **Medium Confidence**: The translation quality assessment, as human vs. machine translation differences are small but the evaluation methodology and sample size are not fully detailed.

## Next Checks

1. Test the zero-shot learning approach on a truly low-resource language not included in the current evaluation (e.g., Swahili or Welsh) to verify the robustness of cross-lingual transfer.
2. Conduct ablation studies on the consistency regularization components to isolate the contribution of prediction consistency versus representation consistency to the calibration improvements.
3. Evaluate the models on out-of-domain fact verification datasets (e.g., from different domains like science or politics) to assess generalization beyond the FEVER-style claims used in XFEVER.