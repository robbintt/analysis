---
ver: rpa2
title: Decentralized Entropic Optimal Transport for Distributed Distribution Comparison
arxiv_id: '2301.12065'
source_url: https://arxiv.org/abs/2301.12065
tags:
- latexit
- sha1
- base64
- agents
- distributed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a decentralized entropic optimal transport (DEOT)
  method for privacy-preserving distributed distribution comparison. The method uses
  a mini-batch randomized block-coordinate descent (MRBCD) scheme to optimize the
  dual form of the DEOT distance, with dual variables scattered across agents and
  updated locally.
---

# Decentralized Entropic Optimal Transport for Distributed Distribution Comparison

## Quick Facts
- arXiv ID: 2301.12065
- Source URL: https://arxiv.org/abs/2301.12065
- Reference count: 35
- Primary result: Decentralized entropic optimal transport method using mini-batch randomized block-coordinate descent for privacy-preserving distributed distribution comparison

## Executive Summary
This paper introduces a decentralized approach to entropic optimal transport (EOT) for comparing distributions across multiple agents without sharing raw data. The method reformulates EOT in dual form with scattered dual variables, enabling local computation while preserving privacy. A mini-batch randomized block-coordinate descent (MRBCD) scheme optimizes the dual objective, and privacy is maintained through a kernel approximation method using random projections that avoids raw data sharing.

## Method Summary
The approach consists of two main components: privacy-preserving kernel approximation and MRBCD optimization. For kernel approximation, agents broadcast random seeds to construct binary matrices that approximate the kernel between samples without sharing raw data. The MRBCD scheme updates dual variables locally using gradients computed from a mini-batch of agents, limiting communication overhead. The method provides theoretical error bounds combining convergence, kernel approximation, and protocol mismatch errors, and demonstrates effectiveness on both synthetic and real-world distributed domain adaptation tasks.

## Key Results
- Achieves comparable performance to centralized methods in distributed domain adaptation tasks
- Provides communication efficiency with complexity independent of data dimension
- Maintains privacy by not requiring raw data sharing between agents
- Offers theoretical error bounds for the combined approximation error

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The decentralized EOT problem can be reformulated as a dual optimization problem where the dual variables are scattered across agents, enabling local computation without raw data sharing.
- **Mechanism:** The paper reformulates the EOT distance into its Fenchel dual form, where the dual objective involves a kernel matrix. By scattering the dual variables across agents and computing gradients based on partial dual variables rather than raw data, each agent can perform local updates without accessing other agents' data.
- **Core assumption:** The dual form of the EOT problem is computationally tractable and that the kernel matrix can be approximated without sharing raw data.
- **Evidence anchors:**
  - [abstract] "We design a mini-batch randomized block-coordinate descent (MRBCD) scheme to optimize the DEOT distance in its dual form. The dual variables are scattered across different agents and updated locally and iteratively with limited communications among partial agents."
  - [section 3.1] "The dual variables are scattered across different agents, and accordingly, the gradient of u(i) can be formulated as follows: ∇u(i)Fε(u, v; K, E) = ∑j eij NiMj ∇u(i)f (i,j) ε"
- **Break condition:** If the dual problem becomes ill-conditioned or the kernel approximation fails to capture the true distances, the convergence may degrade significantly.

### Mechanism 2
- **Claim:** Privacy can be preserved by approximating the kernel matrix using random projections without sharing raw data.
- **Mechanism:** The paper uses a privacy-preserving kernel approximation method based on random seed sharing. Each agent constructs binary matrices using random projections and broadcasts them to other agents. The kernel between samples is then approximated using these binary matrices and sample norms, avoiding the need to share raw data.
- **Core assumption:** The kernel function is a generalized inner product (GIP) kernel and can be approximated using random projections.
- **Evidence anchors:**
  - [abstract] "The kernel matrix involved in the gradients of the dual variables is estimated by a decentralized kernel approximation method, in which each agent only needs to approximate and store a sub-kernel matrix by one-shot communication and without sharing raw data."
  - [section 3.2] "For the GIP kernel, it is possible to approximate it without the share of raw data (Khanduri et al., 2021). Denote D as the dimension of samples. Leveraging the random seed sharing method, we can sample PD-dimensional random variables from a multivariate normal distribution, i.e., {ωℓ}Pℓ=1 ∼ N (0, ID), and broadcast them to all the agents."
- **Break condition:** If the kernel is not a GIP kernel or the random projections do not capture sufficient information about the data, the approximation error may become too large.

### Mechanism 3
- **Claim:** Communication efficiency is achieved by limiting the number of agents involved in each gradient computation step.
- **Mechanism:** The paper uses a mini-batch randomized block-coordinate descent (MRBCD) scheme where, in each iteration, only a subset of agents (L agents) are selected to compute the gradients. This reduces the communication cost compared to full-batch methods while still providing unbiased gradient estimates.
- **Core assumption:** The stochastic gradients computed using a subset of agents are unbiased estimates of the full-batch gradients.
- **Evidence anchors:**
  - [abstract] "The dual variables are updated locally and iteratively with limited communications among partial agents."
  - [section 3.2] "In the t-th iteration, the agent i receives the dual variables {v(j),t}j∈JL from L target agents, where JL ⊂ {1, ..., J} denotes the set of the L target agents."
- **Break condition:** If the subset of agents selected for gradient computation is too small or not representative, the gradient estimates may become biased, leading to poor convergence.

## Foundational Learning

- **Concept:** Fenchel duality and convex optimization
  - **Why needed here:** The paper relies on reformulating the EOT problem into its dual form to enable decentralized computation. Understanding Fenchel duality is crucial for grasping how the dual variables are scattered across agents.
  - **Quick check question:** What is the relationship between the primal and dual problems in convex optimization, and how does this apply to the EOT distance?

- **Concept:** Random projections and kernel approximation
  - **Why needed here:** The privacy-preserving kernel approximation method relies on random projections to approximate the kernel matrix without sharing raw data. Understanding how random projections can preserve distances is key to understanding this mechanism.
  - **Quick check question:** How do random projections preserve distances between high-dimensional vectors, and what is the role of the Johnson-Lindenstrauss lemma in this context?

- **Concept:** Block-coordinate descent and stochastic optimization
  - **Why needed here:** The paper uses a mini-batch randomized block-coordinate descent scheme to optimize the dual objective. Understanding how block-coordinate descent works and how mini-batching affects convergence is essential for understanding the algorithm.
  - **Quick check question:** How does block-coordinate descent differ from full gradient descent, and what are the trade-offs between convergence speed and communication efficiency?

## Architecture Onboarding

- **Component map:** Dual variables -> Kernel approximation -> MRBCD updates -> Communication protocol -> Storage protocol
- **Critical path:**
  1. Initialize dual variables and approximate kernel matrix using random projections
  2. In each iteration:
     - Select a subset of agents to compute gradients
     - Exchange dual variables with selected agents
     - Compute stochastic gradients and update local dual variables
  3. After convergence, compute the EOT distance using collected dual objectives
- **Design tradeoffs:**
  - Number of agents (L) vs. convergence speed: Larger L leads to faster convergence but higher communication cost
  - Kernel approximation accuracy (P) vs. privacy: Higher P improves accuracy but may leak more information
  - Communication protocol vs. storage protocol: Mismatch can lead to approximation errors
- **Failure signatures:**
  - Slow or no convergence: May indicate biased gradient estimates or poor kernel approximation
  - High approximation error: May indicate mismatch between storage and communication protocols or insufficient kernel approximation
  - Privacy leakage: May indicate kernel approximation revealing too much information about raw data
- **First 3 experiments:**
  1. Test convergence with different values of L (number of agents in each gradient computation) on synthetic data
  2. Evaluate privacy preservation by measuring information leakage when using different kernel approximation accuracies (P)
  3. Compare performance under different communication protocols (e.g., ideal vs. sparse vs. asymmetric) on real-world distributed domain adaptation tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section suggests areas for further investigation, particularly regarding the impact of storage-communication protocol mismatch and the extension to non-GIP kernels.

## Limitations
- Privacy guarantees are based on data not being directly shared rather than formal privacy metrics
- Theoretical bounds combine multiple error sources without clearly isolating individual contributions
- The method assumes GIP kernels and may not generalize to other kernel types

## Confidence
- High confidence in the mathematical formulation and theoretical analysis
- Medium confidence in the practical effectiveness based on experimental results
- Lower confidence in the privacy guarantees due to lack of formal privacy metrics

## Next Checks
1. Test the method with non-GIP kernels to understand the limits of the approximation technique
2. Systematically vary the storage-communication protocol mismatch to quantify its impact on convergence and approximation error
3. Implement formal privacy analysis (e.g., differential privacy metrics) to complement the data-sharing-based privacy claims