---
ver: rpa2
title: Large Language Models can Learn Rules
arxiv_id: '2310.07064'
source_url: https://arxiv.org/abs/2310.07064
tags:
- answer
- rules
- sister
- brother
- carry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Hypotheses-to-Theories (HtT) improves reasoning performance in
  large language models (LLMs) by learning explicit rules from training examples.
  HtT works in two stages: an induction stage generates and verifies rules over training
  examples, forming a rule library, and a deduction stage applies these rules to solve
  test questions.'
---

# Large Language Models can Learn Rules

## Quick Facts
- arXiv ID: 2310.07064
- Source URL: https://arxiv.org/abs/2310.07064
- Reference count: 40
- Large Language Models can Learn Rules improves reasoning performance in large language models (LLMs) by learning explicit rules from training examples

## Executive Summary
Hypotheses-to-Theories (HtT) is a framework that enhances reasoning in large language models by learning explicit rules from training examples and applying them to solve test questions. The method works in two stages: an induction stage that generates and verifies rules over training data, and a deduction stage that applies these rules to test questions. Experiments on numerical reasoning (Arithmetic dataset) and relational reasoning (CLUTRR dataset) demonstrate that HtT consistently outperforms baseline prompting methods like chain-of-thought and least-to-most, with accuracy gains of 11-27%. The learned rules are transferable to different models and problem variations, making the approach broadly applicable.

## Method Summary
HtT is a two-stage framework for improving reasoning in LLMs. In the induction stage, the model generates rules from training examples and filters them by frequency and accuracy to form a rule library. The deduction stage applies these learned rules to solve test questions. Rules are organized in a hierarchical XML structure to improve retrieval efficiency. The framework was tested on numerical reasoning tasks using non-decimal addition problems and relational reasoning tasks using the CLUTRR dataset, with experiments comparing performance across different model sizes and problem variations.

## Key Results
- HtT improves reasoning accuracy by 11-27% over baseline methods on numerical and relational reasoning tasks
- XML tagging trick doubles the performance gain by improving rule retrieval efficiency
- Learned rules successfully transfer between different models (GPT3.5 to GPT4) and problem variations (symbolic to textual CLUTRR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HtT reduces hallucination by replacing implicit knowledge with explicit, verified rules.
- Mechanism: In the induction stage, the model generates rules from training examples and filters them by frequency and accuracy. Only rules meeting coverage and confidence thresholds enter the rule library. During deduction, the model retrieves rules from this library rather than relying on its internal knowledge.
- Core assumption: Even if the LLM hallucinates rules on some examples, it can still generate correct rules on a sufficient subset, and these correct rules can be reliably identified by filtering.
- Evidence anchors:
  - [abstract]: "The learned rules are also transferable to different models and to different forms of the same problem."
  - [section 3.1]: "We collect the rules generated by the LLM over the entire training set, and only keep those that appear more than k times and pass the verification test with a probability higher than p."
  - [corpus]: Weak evidence - no direct citations discussing rule hallucination reduction in LLMs.

### Mechanism 2
- Claim: The XML tagging trick improves rule retrieval by breaking down a hard retrieval problem into multiple easy retrieval problems.
- Mechanism: Rules are organized in a hierarchical XML structure. At each level, the LLM retrieves the correct tag, narrowing the search space. This makes it easier for the LLM to find the exact rule needed for each deductive step.
- Core assumption: LLMs can reliably perform tag retrieval when the number of options at each level is limited (e.g., ≤10 rules per level).
- Evidence anchors:
  - [section 3.2]: "We manually define a hierarchy by grouping similar rules together. Inspired by the XML tags used in prompting tutorials 1, we label each level of the hierarchy with pairs of XML tags like <carry> and </carry>."
  - [section 4.4]: "Table 4 shows that the XML tagging trick significantly boosts performance, doubling the gain from 9.8% and 6.5% on Arithmetic and CLUTRR respectively."
  - [corpus]: Weak evidence - no direct citations discussing XML tagging for rule retrieval in LLMs.

### Mechanism 3
- Claim: The learned rule library is transferable to different models and problem variations.
- Mechanism: The rule library is induced from training examples and represents the core knowledge needed for the task. Since this knowledge is explicit and not tied to a specific model's parameters, it can be applied to different models (e.g., GPT3.5 vs GPT4) and different forms of the same problem (e.g., symbolic vs textual CLUTRR).
- Core assumption: The rules learned from training examples capture the essential patterns needed for the task, and these patterns are consistent across different models and problem variations.
- Evidence anchors:
  - [abstract]: "The learned rules are also transferable to different models and to different forms of the same problem."
  - [section 4.3]: "To verify this, we tackle the textual version of CLUTRR with rules learned from the symbolic version. Table 3 shows that HtT significantly improves the performance of GPT4 on the textual version."
  - [corpus]: Weak evidence - no direct citations discussing transferability of learned rules to different models or problem variations.

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: CoT is the baseline method that HtT augments. Understanding CoT is essential to grasp how HtT improves upon it by adding explicit rule learning and retrieval.
  - Quick check question: What is the main difference between CoT and HtT in terms of how they handle knowledge for reasoning tasks?

- Concept: Rule mining and filtering
  - Why needed here: HtT relies on rule mining to extract and filter rules from training examples. Understanding the principles of rule mining (e.g., coverage, confidence) is crucial to understanding how HtT builds its rule library.
  - Quick check question: What are the two main criteria used to filter rules in HtT, and why are they important?

- Concept: Knowledge graph reasoning
  - Why needed here: CLUTRR is a relational reasoning problem that is commonly studied in the knowledge graph community. Understanding the basics of knowledge graph reasoning helps contextualize the results on CLUTRR.
  - Quick check question: What is the goal of a typical relational reasoning problem like CLUTRR, and how does it relate to knowledge graph reasoning?

## Architecture Onboarding

- Component map: Training examples -> Rule generation -> Rule verification -> Rule library formation -> Rule retrieval -> Test question solving
- Critical path: Induction stage (generate and verify rules) → Deduction stage (apply learned rules with XML retrieval)
- Design tradeoffs:
  - Rule library size vs. retrieval accuracy: Larger libraries may contain more relevant rules but make retrieval harder
  - Strict vs. lenient filtering: Stricter filtering ensures high-quality rules but may result in a smaller library
  - Hierarchy depth vs. retrieval ease: Deeper hierarchies may improve retrieval but add complexity to the prompt
- Failure signatures:
  - Empty or incomplete rule library: Indicates issues with rule generation or overly strict filtering
  - Poor retrieval accuracy: Suggests problems with the XML tagging hierarchy or the LLM's retrieval ability
  - No performance gain over CoT: May indicate that the learned rules are not significantly better than the LLM's implicit knowledge
- First 3 experiments:
  1. Test HtT on a simple arithmetic problem (e.g., base-10 addition) to verify basic functionality
  2. Vary the rule coverage threshold (k) and observe its effect on rule library size and performance
  3. Test the transferability of the rule library by applying it to a different model (e.g., GPT3.5) or problem variation (e.g., textual CLUTRR)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend HtT to handle rule compositionality and longer chains of reasoning?
- Basis in paper: [inferred] The paper notes that the number of rules is limited by the LLM's context length and suggests it remains an open problem to scale up deductive reasoning when the rule library cannot fit into the LLM's input context.
- Why unresolved: The paper does not explore methods for handling very large rule libraries or techniques for efficiently composing multiple rules in longer reasoning chains.
- What evidence would resolve it: Experiments demonstrating HtT performance on problems requiring hundreds or thousands of rules, or showing successful rule composition across multiple deduction steps.

### Open Question 2
- Question: Can the rule induction process be made more robust to noise and less dependent on the quality of the base LLM?
- Basis in paper: [explicit] The paper notes that GPT3.5 struggles with inducing correct rules and performing retrieval, and that HtT's performance gain for GPT3.5 is marginal. It suggests finetuning on retrieval datasets as a potential solution.
- Why unresolved: The paper does not explore alternative rule induction methods or techniques for improving rule generation and verification in weaker models.
- What evidence would resolve it: Experiments showing HtT performance with different rule induction methods (e.g., using a stronger model for induction and a weaker model for deduction), or demonstrating improved performance after finetuning on retrieval tasks.

### Open Question 3
- Question: How can we evaluate the transferability and generalizability of learned rules to new problem domains?
- Basis in paper: [explicit] The paper shows that rules learned on the symbolic version of CLUTRR can transfer to the textual version, but notes that GPT3.5 often produces errors other than rule hallucination when processing textual input.
- Why unresolved: The paper does not explore rule transfer to more diverse problem domains or investigate methods for improving rule generalization.
- What evidence would resolve it: Experiments demonstrating HtT performance on problems from entirely different domains (e.g., visual reasoning, commonsense reasoning), or showing successful rule transfer to problems with different input formats or reasoning structures.

## Limitations
- Rule transferability claims lack systematic evaluation across different model families and problem domains
- Rule filtering mechanism's robustness is questionable with limited training data or noisy examples
- Performance gains for weaker models like GPT3.5 are marginal, suggesting dependency on base model quality

## Confidence
- **High Confidence**: XML tagging improves rule retrieval (Mechanism 2) - directly measured with 9.8-6.5% performance gains
- **Medium Confidence**: HtT reduces rule hallucination (Mechanism 1) - supported by performance improvements but lacks direct hallucination measurement
- **Low Confidence**: Rule transferability (Mechanism 3) - based on single transfer experiment without systematic evaluation

## Next Checks
1. **Cross-model Transfer Validation**: Apply rules learned from GPT4 on symbolic CLUTRR to other model families (e.g., Claude, LLaMA) on both symbolic and textual CLUTRR tasks
2. **Rule Hallucination Quantification**: Implement systematic comparison measuring hallucinated vs. retrieved rules across the same test set
3. **Robustness to Limited Training Data**: Evaluate HtT performance with progressively smaller training subsets (10%, 25%, 50%) to identify minimum viable training size