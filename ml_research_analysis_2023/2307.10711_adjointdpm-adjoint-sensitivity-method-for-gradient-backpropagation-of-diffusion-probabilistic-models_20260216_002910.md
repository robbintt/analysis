---
ver: rpa2
title: 'AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion
  Probabilistic Models'
arxiv_id: '2307.10711'
source_url: https://arxiv.org/abs/2307.10711
tags:
- diffusion
- images
- adjointdpm
- generation
- stylization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of customizing diffusion probabilistic
  models (DPMs) when only a differentiable metric is available for supervision. The
  core method, AdjointDPM, uses the adjoint sensitivity method to efficiently backpropagate
  gradients through the DPM sampling process by solving an augmented ODE, avoiding
  the need to store intermediate states and significantly reducing memory consumption.
---

# AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2307.10711
- Source URL: https://arxiv.org/abs/2307.10711
- Reference count: 40
- Key outcome: AdjointDPM reduces memory consumption for DPM customization using adjoint sensitivity method and exponential integration, achieving FID as low as 2.58 on CIFAR-10.

## Executive Summary
AdjointDPM introduces an efficient method for customizing diffusion probabilistic models (DPMs) when only a differentiable metric is available for supervision. The approach uses the adjoint sensitivity method to backpropagate gradients through the DPM sampling process by solving an augmented ODE, avoiding the need to store intermediate states and significantly reducing memory consumption. To further improve accuracy, the probability-flow ODE and augmented ODE are reparameterized using exponential integration, transforming them into non-stiff ODEs that reduce discretization errors. Experiments demonstrate the method's effectiveness in three tasks: optimizing text embeddings for visual effects, fine-tuning network weights for stylization, and generating adversarial samples to audit content moderation systems.

## Method Summary
AdjointDPM customizes diffusion models by backpropagating gradients through the sampling process using the adjoint sensitivity method. The approach reformulates DPM sampling as solving a probability-flow ODE and computes gradients by solving a reverse-time augmented ODE. To reduce numerical errors, both the forward and backward ODEs are reparameterized using exponential integration, converting them into non-stiff forms that can be accurately solved with standard ODE solvers. This enables customization based on differentiable metrics defined on generated content without requiring reference examples, while maintaining constant memory usage regardless of sampling steps.

## Key Results
- Successfully optimizes text embeddings to control Stable Diffusion for specific visual effects
- Achieves FID of 2.58 on CIFAR-10 using AdjointDPM in low function evaluation regimes
- Generates adversarial samples that bypass content filters while maintaining visual similarity to original images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdjointDPM reduces memory consumption by using the adjoint sensitivity method instead of storing all intermediate states during DPM sampling.
- Mechanism: The adjoint sensitivity method computes gradients by solving a backward ODE that only needs to store the intermediate state at the time point of function evaluation, resulting in constant memory usage regardless of the number of sampling steps.
- Core assumption: The DPM sampling process can be reformulated as solving a probability-flow ODE, and the adjoint sensitivity method can be applied to this ODE formulation.
- Evidence anchors:
  - [abstract]: "AdjointDPM uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters... by solving another augmented ODE."
  - [section 2.2]: "To efficiently obtain the gradients of the loss function L with respect to ψ, we leverage the family of adjoint sensitivity methods in the study of Neural ODEs [3] to solve Eqn. (1)."
  - [corpus]: Weak - The related papers focus on other aspects of diffusion model sampling and do not directly discuss memory-efficient gradient computation.
- Break condition: If the DPM sampling process cannot be reformulated as an ODE, or if the adjoint sensitivity method cannot be applied to this specific ODE formulation.

### Mechanism 2
- Claim: AdjointDPM reduces numerical errors in both the forward generation and gradient backpropagation processes by reparameterizing the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration.
- Mechanism: By multiplying an integrating factor exp(-∫₀ᵗ f(τ)dτ) on both sides of the ODE, the ODE is transformed into a non-stiff form that can be solved more accurately using off-the-shelf numerical ODE solvers. This avoids discretization errors for the linear part of the ODE.
- Core assumption: The diffusion ODE functions have a semi-linear structure that allows for exponential integration, and the resulting non-stiff ODEs can be accurately solved using standard numerical ODE solvers.
- Evidence anchors:
  - [section 3.1]: "We use the exponential integration to transform the ODE (6) into a simple non-stiff ODE... We also reparameterize the reverse ODE function in Algorithm 1 as follows..."
  - [section 3.2]: "We reformulate the forward and reverse ODE functions in this section and show that by using off-the-shelf numerical ODE solvers on reparameterized ODEs, the discretization error of the linear part will not be introduced by AdjointDPM."
  - [corpus]: Weak - The related papers do not discuss the specific reparameterization technique used in AdjointDPM.
- Break condition: If the diffusion ODE functions do not have the required semi-linear structure, or if the resulting non-stiff ODEs cannot be accurately solved using standard numerical ODE solvers.

### Mechanism 3
- Claim: AdjointDPM enables customization of DPMs based on a differentiable metric defined on the generated contents, without requiring access to multiple reference examples.
- Mechanism: By backpropagating the gradients of the loss computed on the generated samples to the DPM parameters (conditioning signals, network weights, and initial noises), AdjointDPM allows for optimization of these parameters to minimize the loss, effectively customizing the DPM for the target concept.
- Core assumption: The loss function can be expressed as a differentiable function of the generated samples, and the gradients of this loss with respect to the DPM parameters can be computed using the adjoint sensitivity method.
- Evidence anchors:
  - [abstract]: "This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents."
  - [section 4]: "The results demonstrate the flexibility and general applicability of AdjointDPM: 1) AdjointDPM manages to optimize a unique special text embedding # that can control a Stable Diffusion model to synthesize images with certain visual effects... 2) AdjointDPM can finetune a Stable Diffusion model for stylization defined by a Gram Matrix of a single reference image."
  - [corpus]: Weak - The related papers focus on other aspects of diffusion model sampling and do not directly discuss customization based on differentiable metrics.
- Break condition: If the loss function cannot be expressed as a differentiable function of the generated samples, or if the gradients of this loss with respect to the DPM parameters cannot be computed using the adjoint sensitivity method.

## Foundational Learning

- Concept: Diffusion Probabilistic Models (DPMs)
  - Why needed here: AdjointDPM is a method for customizing DPMs, so understanding the basic principles and sampling process of DPMs is essential.
  - Quick check question: What is the main idea behind the sampling process in DPMs, and how is it typically implemented?

- Concept: Adjoint Sensitivity Method
  - Why needed here: AdjointDPM uses the adjoint sensitivity method to compute gradients efficiently. Understanding this method is crucial for grasping how AdjointDPM works.
  - Quick check question: How does the adjoint sensitivity method differ from standard backpropagation in terms of memory usage and computational complexity?

- Concept: Ordinary Differential Equations (ODEs) and Numerical ODE Solvers
  - Why needed here: AdjointDPM involves solving ODEs (probability-flow ODEs and augmented ODEs) using numerical ODE solvers. Familiarity with ODEs and ODE solvers is necessary to understand the technical details of AdjointDPM.
  - Quick check question: What are the key differences between stiff and non-stiff ODEs, and why is it important to transform stiff ODEs into non-stiff ones for efficient and accurate solving?

## Architecture Onboarding

- Component map: DPM sampling process (forward ODE) -> Adjoint sensitivity method (backward ODE) -> Exponential integration and reparameterization -> Differentiable loss function -> Optimization of DPM parameters (conditioning signals, network weights, initial noises)

- Critical path:
  1. Generate samples using the DPM sampling process (forward ODE)
  2. Compute the loss on the generated samples
  3. Backpropagate the gradients of the loss to the DPM parameters using the adjoint sensitivity method (backward ODE)
  4. Update the DPM parameters using the computed gradients

- Design tradeoffs:
  - Memory vs. computation: AdjointDPM trades increased computation (solving the backward ODE) for reduced memory usage (constant memory regardless of the number of sampling steps).
  - Accuracy vs. efficiency: The reparameterization of ODEs using exponential integration improves accuracy but may introduce additional computational overhead.

- Failure signatures:
  - High memory usage: If the DPM sampling process is not reformulated as an ODE or if the adjoint sensitivity method cannot be applied, memory usage may remain high.
  - Inaccurate results: If the reparameterization of ODEs using exponential integration fails or if the resulting non-stiff ODEs cannot be accurately solved, the generated samples and gradients may be inaccurate.

- First 3 experiments:
  1. Verify that AdjointDPM can generate high-quality samples with reduced memory usage compared to standard backpropagation.
  2. Test the accuracy of AdjointDPM on a simple customization task (e.g., optimizing a text embedding for a specific visual effect) and compare the results to baseline methods.
  3. Evaluate the scalability of AdjointDPM on larger DPMs and more complex customization tasks (e.g., fine-tuning network weights for stylization or optimizing initial noise for adversarial sample generation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AdjointDPM scale with increasingly complex diffusion models, such as those with larger UNet architectures or higher-dimensional data like videos?
- Basis in paper: [inferred] The paper focuses on Stable Diffusion and image-based tasks, but does not explore scalability to more complex models.
- Why unresolved: The paper only provides experimental results on image generation tasks, leaving the scalability to more complex models unexplored.
- What evidence would resolve it: Experiments applying AdjointDPM to video diffusion models or other large-scale diffusion models would clarify its scalability.

### Open Question 2
- Question: What are the theoretical guarantees on the convergence of the exponential integration reparameterization when applied to non-semi-linear diffusion ODE functions?
- Basis in paper: [explicit] The paper claims exponential integration improves error control for semi-linear ODEs but does not provide theoretical analysis for general cases.
- Why unresolved: The paper lacks formal proofs or convergence bounds for the reparameterization technique beyond semi-linear cases.
- What evidence would resolve it: A rigorous mathematical analysis proving convergence rates for non-semi-linear cases would resolve this question.

### Open Question 3
- Question: How does AdjointDPM perform when the differentiable metric (loss function) is non-smooth or has discontinuous gradients?
- Basis in paper: [inferred] The paper assumes smooth, differentiable loss functions but does not test edge cases with non-smooth metrics.
- Why unresolved: The paper does not explore the robustness of AdjointDPM to non-smooth loss landscapes, which could occur in real-world applications.
- What evidence would resolve it: Experiments with non-smooth or piecewise-defined loss functions would demonstrate AdjointDPM's robustness in such scenarios.

## Limitations
- Limited empirical validation of exponential integration's superiority over standard ODE solvers
- Incomplete computational complexity analysis, particularly runtime overhead of solving augmented ODE
- Experiments focus primarily on Stable Diffusion, leaving uncertainty about generalizability to other DPM architectures
- Claims about scalability to complex customization tasks are based on limited experiments

## Confidence
- **High Confidence**: The memory efficiency claims are well-supported by the adjoint method's theoretical foundations and basic complexity analysis.
- **Medium Confidence**: The accuracy improvements from exponential integration are plausible but lack comprehensive empirical validation across different DPMs and sampling steps.
- **Low Confidence**: The scalability claims for large-scale customization tasks are based on limited experiments and require further validation.

## Next Checks
1. **Memory vs. Runtime Trade-off**: Conduct a detailed analysis comparing AdjointDPM's runtime overhead against standard backpropagation methods, particularly for different numbers of sampling steps and DPM scales.

2. **Cross-architecture Generalization**: Test AdjointDPM on multiple DPM architectures (beyond Stable Diffusion) to validate the method's general applicability and identify any architecture-specific limitations.

3. **Numerical Stability Analysis**: Systematically evaluate the numerical stability of the exponential integration reparameterization across different ODE solvers and sampling step sizes, documenting any conditions where it degrades performance.