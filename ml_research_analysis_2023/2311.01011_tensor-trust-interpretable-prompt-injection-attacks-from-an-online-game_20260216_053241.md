---
ver: rpa2
title: 'Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game'
arxiv_id: '2311.01011'
source_url: https://arxiv.org/abs/2311.01011
tags:
- access
- prompt
- attack
- attacks
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tensor Trust, a web game designed to generate
  a large dataset of human-created prompt injection attacks against instruction-following
  LLMs. The game involves players creating defenses that grant access only with a
  secret code, while attackers attempt to bypass these defenses using prompt injection.
---

# Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game

## Quick Facts
- arXiv ID: 2311.01011
- Source URL: https://arxiv.org/abs/2311.01011
- Reference count: 35
- Over 126,000 human-generated prompt injection attacks against LLMs, revealing significant vulnerabilities in state-of-the-art models

## Executive Summary
This paper introduces Tensor Trust, a web game that crowdsources human-generated prompt injection attacks against instruction-following LLMs. Players create defenses requiring secret codes while attackers attempt to bypass these defenses. The resulting dataset of over 126,000 attacks and 46,000 defenses reveals common attack strategies and exposes vulnerabilities in current LLM architectures. The study demonstrates that many models, including GPT-4, remain highly susceptible to prompt injection attacks, with hijacking robustness rates ranging from 18.4% to 84.3% and extraction robustness rates from 12.3% to 69.1%.

## Method Summary
The authors created a web game where players alternate between creating defenses (with secret codes) and attempting to bypass others' defenses through prompt injection. The game backend uses GPT 3.5 Turbo to validate attacks and defenses. From this game, they collected a large dataset of human-generated attacks and defenses. They then constructed benchmarks by filtering this dataset and evaluated multiple baseline models (GPT-3.5 Turbo, GPT-4, Claude, PaLM 2, LLaMA variants) on these benchmarks. They also analyzed attack strategies using LDA topic modeling and tested transferability to real-world applications.

## Key Results
- Current models show significant vulnerability to prompt injection, with hijacking robustness rates ranging from 18.4% to 84.3% across different models
- No current LLM APIs have sufficient mechanisms to separate trusted instructions from untrusted data
- Attack strategies can be composed to create more effective attacks, with a "viral" effect where users adopt successful strategies
- Certain rare tokens like "artisanlib" can be exploited to bypass model defenses
- Transferability of game-derived attacks to real-world applications varies significantly by application context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT 3.5 Turbo treats system and user messages with similar importance, making it vulnerable to prompt injection attacks that override system instructions.
- Mechanism: The model processes both system and user messages as input context without giving special precedence to system instructions, allowing attacker messages to override system instructions.
- Core assumption: The model's architecture does not implement a clear separation between trusted instructions and untrusted data.
- Evidence anchors:
  - [section]: "This shows that the inbuilt 'message role' functionality in GPT 3.5 Turbo is not sufficient to reject human-created prompt injection attacks."
  - [section]: "This highlights that current LLM APIs do not have a sufficient solution for separating 'instructions' from 'data'."
  - [corpus]: Weak evidence - no direct mention of message role effectiveness in the corpus papers, suggesting this is a novel finding from this study.
- Break condition: A model architecture that implements proper instruction/data separation or gives system messages priority over user messages.

### Mechanism 2
- Claim: Certain rare tokens like "artisanlib" can be exploited to bypass model defenses.
- Mechanism: The model has difficulty processing certain rare tokens, causing it to ignore preceding instructions or behave unexpectedly when these tokens are present.
- Core assumption: The model's tokenization and processing pipeline has vulnerabilities that can be triggered by specific token sequences.
- Evidence anchors:
  - [section]: "Tensor Trust users discovered that the token artisanlib can make attacks more effective. The artisanlib token was first highlighted by Fell (2023), who listed it as one of several rare 'glitch' tokens which GPT-3.5 Turbo is unable to repeat verbatim."
  - [section]: "Adding this token to Tensor Trust attacks often causes the model to ignore the pre-prompt or post-prompt, or otherwise subvert the defender's instructions in surprising and useful ways."
  - [corpus]: Weak evidence - no direct mention of token-specific vulnerabilities in the corpus papers, suggesting this is a novel finding from this study.
- Break condition: A model that properly handles all tokens or has no special behavior for rare tokens.

### Mechanism 3
- Claim: Attack strategies can be composed to increase their effectiveness against defenses.
- Mechanism: Players combine multiple attack strategies (e.g., roleplay, code execution, response prefix) to create more sophisticated attacks that are harder to defend against.
- Core assumption: The model processes concatenated attack strategies sequentially, allowing each strategy to build upon the previous ones.
- Evidence anchors:
  - [section]: "Strategies can often be combined to increase the overall strength of an attack. This had a compounding effect: users would take successful attacks against their own accounts, extract strategies from those attacks, and selectively use those strategies to strengthen their own attacks."
  - [section]: "Attacks in Tensor Trust are often compositional: for instance, a single attack might use one strategy to get the LLM to ignore the opening defense, and another strategy to make it output a particular string."
  - [corpus]: Moderate evidence - corpus papers mention "compositional" attacks but don't specifically discuss the compounding effect observed in this study.
- Break condition: A model that can identify and neutralize individual attack components or has stronger instruction adherence.

## Foundational Learning

- Concept: Message role functionality in LLMs
  - Why needed here: Understanding how system vs user messages are processed is crucial for designing defenses against prompt injection.
  - Quick check question: What happens when a model receives both system and user messages with conflicting instructions?

- Concept: Tokenization and rare token behavior
  - Why needed here: Certain tokens can trigger unexpected model behavior, which attackers can exploit.
  - Quick check question: How does a model typically handle tokens it hasn't seen frequently during training?

- Concept: Adversarial example compositionality
  - Why needed here: Attack strategies can be combined to create more effective attacks, requiring defenses to handle multiple attack vectors.
  - Quick check question: Why might combining multiple simple attacks be more effective than using a single complex attack?

## Architecture Onboarding

- Component map: Frontend web interface -> Backend LLM (GPT 3.5 Turbo) -> Database -> Validation system -> Ranking system
- Critical path: 1. Player creates defense (opening defense + access code + closing defense) 2. System validates defense by testing with correct access code 3. Player selects target account to attack 4. System evaluates attack by combining target's defense with attacker's input 5. System determines attack success and updates balances
- Design tradeoffs: Simple string comparison vs. semantic evaluation for attack success, Open-ended creativity vs. structured attack/defense formats, Real-time evaluation vs. batch processing for attacks
- Failure signatures: Attack validation fails due to LLM non-determinism, Defense creation blocked by overly strict validation rules, Ranking system exploited through repeated low-risk attacks
- First 3 experiments: 1. Test different message role schemes (System/User/System vs User/User/User) on attack success rates 2. Evaluate attack transferability by testing Tensor Trust attacks on real LLM applications 3. Measure the effectiveness of different defense strategies against various attack types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures (transformer vs. non-transformer) affect vulnerability to prompt injection attacks?
- Basis in paper: [inferred] The paper focuses on instruction-tuned LLMs but does not explore architectural differences in vulnerability.
- Why unresolved: The study primarily uses GPT-3.5 Turbo and other transformer-based models, leaving architectural effects unexplored.
- What evidence would resolve it: Comparative analysis of prompt injection vulnerability across transformer-based and non-transformer models like RNNs or CNNs.

### Open Question 2
- Question: Can prompt injection attacks be mitigated through adversarial training of instruction-following LLMs?
- Basis in paper: [explicit] The authors highlight the need for better mechanisms to differentiate trusted instructions from untrusted data, suggesting potential mitigation strategies.
- Why unresolved: The paper does not explore the effectiveness of adversarial training as a mitigation technique.
- What evidence would resolve it: Empirical results showing the impact of adversarial training on reducing vulnerability to prompt injection attacks.

### Open Question 3
- Question: How do prompt injection attacks evolve over time as models become more robust?
- Basis in paper: [explicit] The study observes the "viral" nature of attack strategies, indicating dynamic evolution.
- Why unresolved: The paper provides a snapshot of attack strategies but does not track long-term evolution.
- What evidence would resolve it: Longitudinal analysis of attack strategies and model defenses over extended periods.

### Open Question 4
- Question: Are there universal patterns in successful prompt injection attacks across different domains?
- Basis in paper: [inferred] The authors identify common attack strategies but do not generalize them across domains.
- Why unresolved: The study focuses on a specific game setting without exploring broader applicability.
- What evidence would resolve it: Analysis of prompt injection success rates across diverse applications like chatbots, code generation, and creative writing.

### Open Question 5
- Question: How does the size of the training dataset affect an LLM's susceptibility to prompt injection attacks?
- Basis in paper: [inferred] The authors use a large dataset of human-generated attacks but do not explore dataset size effects.
- Why unresolved: The study does not investigate the relationship between training data size and vulnerability.
- What evidence would resolve it: Comparative analysis of prompt injection vulnerability in models trained on varying dataset sizes.

## Limitations

- The study's defense validity metric reveals that models frequently fail to execute valid defenses, potentially underestimating defense strength and overestimating attack success rates
- The prompt extraction detection benchmark relies on semantic equivalence matching, which may not capture all successful extraction attempts
- The LDA topic modeling approach for analyzing attack strategies provides coarse-grained insights that may miss nuanced patterns

## Confidence

- **High**: Core findings about model vulnerabilities have strong empirical support from the large-scale Tensor Trust dataset
- **Medium**: Transferability of game-derived attacks to real-world applications is demonstrated but with limited sample size and manual verification
- **Medium**: Novel attack strategies (like token vulnerabilities) are empirically validated but may be model-specific rather than fundamental weaknesses

## Next Checks

1. **Cross-architectural validation**: Test the effectiveness of identified token vulnerabilities (like "artisanlib") across different model architectures (not just GPT-3.5 Turbo) to determine if these are fundamental weaknesses or model-specific quirks.

2. **Defense capability assessment**: Create a controlled evaluation where models are explicitly instructed to defend against attacks, measuring whether the observed vulnerabilities persist when models are given explicit defensive objectives.

3. **Real-world application stress testing**: Conduct systematic testing of Tensor Trust attacks against a broader range of deployed LLM applications with varying contexts, input sanitization approaches, and architectural constraints to better understand transferability patterns.