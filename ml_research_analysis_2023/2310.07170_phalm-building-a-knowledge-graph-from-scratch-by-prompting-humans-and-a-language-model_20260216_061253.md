---
ver: rpa2
title: 'PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language
  Model'
arxiv_id: '2310.07170'
source_url: https://arxiv.org/abs/2310.07170
tags:
- knowledge
- graph
- inferences
- commonsense
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose PHALM, a method of building a commonsense knowledge
  graph from scratch by prompting both crowdworkers and an LLM. First, we collect
  a small-scale knowledge graph by crowdsourcing, asking crowdworkers to describe
  events and inferences.
---

# PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model

## Quick Facts
- arXiv ID: 2310.07170
- Source URL: https://arxiv.org/abs/2310.07170
- Reference count: 15
- Key outcome: Built a Japanese event knowledge graph from scratch using crowdsourcing and LLM prompting, showing acceptable quality and cultural specificity

## Executive Summary
This paper introduces PHALM, a novel method for constructing commonsense knowledge graphs from scratch by combining human crowdsourcing with large language model (LLM) generation. The approach begins with a small-scale knowledge graph collected via crowdsourcing, which is then used as "shots" to guide an LLM in generating a large-scale graph. A filter model trained on pseudo-negative examples helps maintain quality while reducing costs. The resulting Japanese event knowledge graph demonstrates cultural specificity and acceptable quality, with experiments showing that humans and LLMs produce different types of commonsense inferences.

## Method Summary
The PHALM method involves four main stages: (1) collecting a small-scale knowledge graph through crowdsourcing, where workers describe events and associated inferences (xNeed, xEffect, xIntent, xReact); (2) generating a large-scale graph using an LLM (HyperCLOV A JP) with the small graph as prompts; (3) filtering the generated triplets using a RoBERTa-based filter model trained on pseudo-negative examples created from the small graph; and (4) fine-tuning Japanese commonsense generation models (GPT-2 and T5) on the filtered graph. The method addresses the challenge of building language-specific knowledge graphs without relying on translated datasets.

## Key Results
- Built a Japanese event knowledge graph reflecting cultural nuances (e.g., "PersonX goes to the office, xNeed, PersonX takes a train")
- Generated inferences show acceptable quality with an acceptance rate of 58.6% for events and 38.3% for inferences
- Compared human and LLM prompting, finding humans infer more probable/immediate events while LLM infers more temporally distant events
- Trained Japanese commonsense generation models achieving BLEU scores of 4.18-5.23 on test data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting humans and an LLM with the same structure enables building a commonsense knowledge graph from scratch without relying on existing datasets
- Mechanism: The method uses small-scale human-collected data as "shots" to guide the LLM in generating a larger-scale graph, mimicking the prompting process used for humans
- Core assumption: Both human and LLM prompting can be structured similarly to elicit commonsense knowledge effectively
- Evidence anchors: [abstract] "we propose PHALM, a method of building a commonsense knowledge graph from scratch by prompting both crowdworkers and an LLM"; [section 3] "we consider prompting for both humans and an LLM and gradually acquire a knowledge graph from a small scale to a large scale"
- Break condition: If the prompts are not effectively structured for both humans and the LLM, the quality of the generated knowledge graph will degrade

### Mechanism 2
- Claim: Using pseudo-negative examples created from the small graph allows for low-cost filtering of LLM-generated inferences
- Mechanism: Pseudo-negative examples are generated by reversing event order or swapping contexts within the small graph, providing training data for a filter model without human annotation
- Core assumption: Inappropriately ordered or contextually mismatched triplets can serve as effective negative examples for training a filter
- Evidence anchors: [section 3.4] "we create pseudo-negative examples from the triplets in the small graph and adopt them"; [section D.1] Details on creating negative samples by reversing time series or swapping contexts
- Break condition: If the pseudo-negative examples are not representative of actual invalid inferences, the filter model will not effectively distinguish valid from invalid triplets

### Mechanism 3
- Claim: The built knowledge graph reflects cultural nuances specific to the target language, unlike translated datasets
- Mechanism: By constructing the graph from scratch using native language crowdsourcing and an LLM trained on native data, the resulting knowledge captures culturally specific commonsense
- Core assumption: Commonsense knowledge varies by language and culture, so building from scratch is necessary for cultural relevance
- Evidence anchors: [section 4] "The generated knowledge graph in Japanese reflects the culture of Japan, such as ⟨PersonX goes to the office, xNeed, PersonX takes a train⟩"; [section 1] "there are not many non-English datasets, although the knowledge that is considered commonsense varies by languages and cultures"
- Break condition: If cultural nuances are not significant in the target language, the effort to build from scratch may not yield substantial benefits over translation

## Foundational Learning

- Concept: Prompt engineering for commonsense knowledge elicitation
  - Why needed here: Effective prompts are crucial for both human crowdsourcing and LLM generation to obtain high-quality commonsense inferences
  - Quick check question: What are the key elements to include in a prompt to elicit specific types of commonsense inferences (e.g., events, mental states)?

- Concept: Filter model training with limited labeled data
  - Why needed here: Filtering large-scale LLM-generated inferences requires a model trained on labeled data, which is expensive to obtain; pseudo-negative examples offer a low-cost alternative
  - Quick check question: How can pseudo-negative examples be generated from a small graph to train a filter model without human annotation?

- Concept: Cultural variation in commonsense knowledge
  - Why needed here: Understanding that commonsense varies by culture is essential for building language-specific knowledge graphs that capture relevant nuances
  - Quick check question: How might commonsense knowledge about daily events differ between cultures, and why is this important for building knowledge graphs?

## Architecture Onboarding

- Component map: Human crowdsourcing -> Small-scale graph collection -> LLM generation -> Filter model -> Large-scale filtered graph -> Commonsense generation models

- Critical path:
  1. Collect small-scale graph via crowdsourcing
  2. Generate large-scale graph using LLM with small-scale graph as shots
  3. Filter generated graph using filter model trained on pseudo-negatives
  4. Train commonsense generation models on filtered graph

- Design tradeoffs:
  - Using pseudo-negative examples reduces cost but may introduce bias if not representative
  - Building from scratch captures cultural nuances but is more resource-intensive than translation
  - Filtering threshold is a hyperparameter affecting quality vs. quantity of inferences

- Failure signatures:
  - Low inter-annotator agreement in crowdsourcing indicates subjective or unclear prompts
  - Filter model performs poorly if pseudo-negatives are not representative of invalid inferences
  - Commonsense generation models produce irrelevant inferences if trained on low-quality graph

- First 3 experiments:
  1. Test prompt effectiveness by comparing human-collected inferences with and without filtering
  2. Evaluate filter model performance by manually annotating a subset of LLM-generated inferences
  3. Assess commonsense generation model quality by generating inferences for unseen events and evaluating manually

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the Japanese commonsense knowledge graph compare to similar graphs in other languages when evaluated on the same metrics?
- Basis in paper: [inferred] The paper discusses building a Japanese-specific commonsense knowledge graph and mentions cultural differences, but does not provide comparative analysis with other language graphs
- Why unresolved: The paper focuses on Japanese language construction without benchmarking against English or other language graphs
- What evidence would resolve it: Direct comparison of the Japanese graph's quality metrics (AR, MP, BLEU, BERTScore) with equivalent metrics from English or other language graphs built using similar methods

### Open Question 2
- Question: What is the optimal threshold for the filtering model that balances quality and quantity of generated inferences?
- Basis in paper: [explicit] The paper mentions that "setting a high threshold increases the ratio of valid triplets" but does not specify an optimal threshold value
- Why unresolved: The paper presents filtering as a trade-off without determining the optimal balance point
- What evidence would resolve it: Empirical results showing precision-recall trade-offs at different threshold values, identifying the point of diminishing returns

### Open Question 3
- Question: How would the method perform if applied to languages with different linguistic structures than Japanese, such as agglutinative or highly inflected languages?
- Basis in paper: [inferred] The paper notes that "English and Japanese have different linguistic characteristics" but does not explore performance across diverse language types
- Why unresolved: The study is limited to Japanese without testing on languages with significantly different grammatical structures
- What evidence would resolve it: Application of PHALM to multiple language families with varying morphological complexity and comparison of outcomes

### Open Question 4
- Question: What is the long-term stability of the commonsense generation models when fine-tuned on dynamically updated knowledge graphs?
- Basis in paper: [inferred] The paper trains models on a static knowledge graph without addressing how updates to the graph would affect model performance over time
- Why unresolved: The study presents a one-time model training approach without exploring incremental learning or model updating mechanisms
- What evidence would resolve it: Longitudinal studies showing model performance degradation and adaptation when retrained on periodically updated graphs

### Open Question 5
- Question: How does the cost-effectiveness of PHALM compare to pure crowdsourcing or pure LLM approaches for building commonsense knowledge graphs?
- Basis in paper: [explicit] The paper claims to reduce cost by combining crowdsourcing and LLM, but does not provide quantitative cost comparisons
- Why unresolved: The paper discusses cost reduction conceptually without empirical cost analysis
- What evidence would resolve it: Detailed cost breakdowns (human labor, compute resources, time) for PHALM versus pure crowdsourcing versus pure LLM approaches, including quality-adjusted cost per inference

## Limitations
- The methodology is specifically designed for Japanese and may not transfer well to other languages with different linguistic structures
- The filter model's effectiveness using pseudo-negative examples is plausible but not extensively validated against human-labeled data
- The study focuses on event-centric knowledge, which may not capture the full spectrum of commonsense knowledge required for broader applications

## Confidence

- **High Confidence**: The core methodology of using human-collected data as shots for LLM generation is technically sound and well-executed. The comparison between human and LLM inference patterns is supported by clear experimental evidence.
- **Medium Confidence**: The quality assessment of the generated knowledge graph through crowdsourcing is reasonable, but the acceptance rate metrics may not fully capture the nuanced quality of commonsense inferences. The filter model's effectiveness using pseudo-negative examples is plausible but not extensively validated.
- **Low Confidence**: The claim about cultural specificity of the generated knowledge graph is based on anecdotal examples rather than systematic cultural analysis. The long-term stability and usefulness of the generated inferences for downstream tasks is not thoroughly evaluated.

## Next Checks
1. Conduct a systematic evaluation of the filter model's performance by manually annotating a diverse sample of LLM-generated inferences, comparing the results with the pseudo-negative training approach to assess false positive/negative rates.
2. Test the cross-linguistic applicability of the methodology by attempting to build knowledge graphs in other languages (e.g., English, Spanish) using the same framework, measuring both quality and cultural relevance.
3. Implement a longitudinal study tracking the quality and relevance of the generated inferences over time, assessing whether the knowledge graph remains useful for commonsense reasoning tasks as language and cultural contexts evolve.