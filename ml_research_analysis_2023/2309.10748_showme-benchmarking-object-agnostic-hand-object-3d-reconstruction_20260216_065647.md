---
ver: rpa2
title: 'SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction'
arxiv_id: '2309.10748'
source_url: https://arxiv.org/abs/2309.10748
tags:
- hand
- reconstruction
- object
- pose
- hand-object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SHOWMe, a dataset of 96 videos with high-precision
  3D textured meshes of hands holding objects, aimed at advancing object-agnostic
  hand-object 3D reconstruction. The dataset captures diverse hand-object interactions,
  including 42 unique objects and 15 subjects, with sub-millimeter accurate ground
  truth annotations.
---

# SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction

## Quick Facts
- **arXiv ID**: 2309.10748
- **Source URL**: https://arxiv.org/abs/2309.10748
- **Reference count**: 40
- **Key outcome**: Introduced SHOWMe dataset of 96 videos with high-precision 3D meshes of hands holding objects, evaluating rigid transformation estimation and multi-view reconstruction methods for object-agnostic hand-object reconstruction.

## Executive Summary
This paper introduces SHOWMe, a dataset designed to benchmark object-agnostic hand-object 3D reconstruction from monocular RGB video. The dataset contains 96 videos of hands holding 42 unique objects across 15 subjects, with sub-millimeter accurate 3D textured meshes captured using an Artec Eva 3D scanner. The authors propose a two-stage pipeline: rigid transformation estimation (using either SfM or hand pose estimation) followed by multi-view reconstruction (using VH, FDR, or HHOR). The study evaluates these methods and demonstrates that HHOR achieves the best Fscore (~82%) when ground-truth rigid transforms are provided, while estimated transforms from hand poses or SfM reduce performance. The work highlights the importance of robust hand pose estimation, especially for small or textureless objects, and leaves room for improvement in reconstruction quality.

## Method Summary
The SHOWMe dataset provides high-precision 3D scans of hand-object pairs as ground truth. The reconstruction pipeline begins with depth-based initialization using ICP registration between ground truth meshes and depth maps from an Intel RealSense L515 camera. This initial pose is refined using differentiable rendering with photometric consistency. For rigid transformation estimation, the method uses either SfM (COLMAP) or hand pose estimation (DOPE + PnP). The second stage applies multi-view reconstruction algorithms (VH, FDR, or HHOR) to aggregate observations across frames under the rigid hand-object assumption. The pipeline is evaluated using Fscore@5mm, accuracy, and completeness metrics against ground truth scans.

## Key Results
- HHOR achieves highest Fscore (~82%) with ground-truth rigid transforms
- Estimated transforms from hand poses or SfM reduce reconstruction performance
- DOPE hand pose estimation is more robust for textureless objects but suffers under heavy occlusion
- Small objects remain challenging for both rigid transformation estimation and reconstruction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sub-millimeter ground truth scans enable accurate initialization for RGB-based refinement
- **Mechanism**: High-precision 3D scans provide geometric detail to bootstrap ICP registration with depth maps, serving as strong prior for RGB refinement via differentiable rendering
- **Core assumption**: Depth maps from Intel RealSense L515 provide sufficient geometric detail
- **Evidence anchors**: [section] "we register the GT HO mesh to the depth map of each frame in the sequence", [section] "We refine the registration using a differentiable rendering pipeline"
- **Break condition**: Depth sensor bias or small objects cause ICP initialization failure

### Mechanism 2
- **Claim**: Rigid hand-object assumption enables multi-view reconstruction from single video
- **Mechanism**: Treating video frames as virtual cameras observing same rigid configuration enables MVR algorithms to aggregate observations
- **Core assumption**: Hand and object maintain constant relative pose during interaction
- **Evidence anchors**: [abstract] "we consider a rigid hand-object scenario", [section] "we simplify the problem as an intermediary step towards dynamic temporal integration by assuming that the camera is static and the hand is holding an unknown object rigidly"
- **Break condition**: Hand slips or object manipulated dynamically breaks rigidity assumption

### Mechanism 3
- **Claim**: Hand pose estimation substitutes SfM for rigid transformation when objects are textureless or occluded
- **Mechanism**: Using 2D-3D hand keypoint detection (DOPE) and PnP problem recovers rigid transformation more robustly than SfM
- **Core assumption**: Hand keypoints reliably detected even under occlusion
- **Evidence anchors**: [section] "we evaluate two methods for estimating the rigid transformations of the HO in the sequence, either using standard generic SfM toolbox, or using the hand pose as a proxy", [section] "Hand-based estimation of the rigid transformation is more robust for textureless objects but suffers in case of heavy occlusions"
- **Break condition**: Heavy occlusion or unreliable keypoints degrade transformation estimates

## Foundational Learning

- **Concept**: Rigid body transformations in SE(3)
  - Why needed here: Both SfM and hand pose estimation recover 3D rigid transformations between frames for aligning observations in multi-view reconstruction
  - Quick check question: Given two 3D point sets in correspondence, how would you compute the rotation and translation that aligns them using Procrustes analysis?

- **Concept**: Multi-view stereo and shape from silhouette
  - Why needed here: MVR stage aggregates observations from multiple views to reconstruct 3D geometry using methods like visual hulls, differentiable rendering, or implicit surfaces
  - Quick check question: What is the main limitation of visual hull reconstruction compared to volumetric methods?

- **Concept**: Differentiable rendering for pose refinement
  - Why needed here: Used to refine initial pose estimates by minimizing photometric error between rendered and observed images
  - Quick check question: What is the role of the smoothing term in the differentiable rendering loss, and why is it necessary?

## Architecture Onboarding

- **Component map**: Intel RealSense L515 RGB-D camera -> Artec Eva 3D scanner -> Depth back-projection -> Segmentation -> ICP registration -> Differentiable rendering refinement -> Rigid transformation estimation (SfM or hand pose) -> Multi-view reconstruction (VH, FDR, HHOR) -> Mesh evaluation

- **Critical path**: Depth-based ICP → RGB-based refinement → Rigid transformation estimation → MVR → Mesh evaluation

- **Design tradeoffs**:
  - Using hand pose estimation vs SfM: More robust to textureless objects but suffers under heavy occlusion
  - Using GT transforms vs estimated: Much higher accuracy but less practical
  - Using FDR vs HHOR: FDR is faster but HHOR recovers more detail

- **Failure signatures**:
  - COLMAP fails on small or textureless objects → low reconstruction accuracy
  - DOPE fails under heavy occlusion → poor rigid transformation → bad MVR
  - HHOR fails when camera poses are inaccurate → reconstruction artifacts

- **First 3 experiments**:
  1. Run COLMAP and DOPE on a subset of sequences; compare rotation/translation errors and detection rates
  2. Run FDR and HHOR with GT rigid transforms; compare F-score at 5mm threshold
  3. Run FDR with estimated transforms (both COLMAP and DOPE); measure drop in accuracy and identify failure cases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the key factors limiting the accuracy of rigid transformation estimation for small or textureless objects in the SHOWMe dataset?
- **Basis in paper**: [explicit] The paper states that COLMAP fails or performs poorly on small objects compared to larger objects, and that DOPE is better for small objects. It also mentions that COLMAP's camera pose estimation quality reduces drastically for less textured objects.
- **Why unresolved**: The paper provides empirical observations but does not perform a detailed analysis of the specific challenges and potential solutions for improving rigid transformation estimation for small or textureless objects.
- **What evidence would resolve it**: A comprehensive study analyzing the impact of object size and texture on rigid transformation estimation accuracy, along with experiments testing different feature detection and matching strategies for small or textureless objects.

### Open Question 2
- **Question**: How can the temporal aggregation of SDFs from multiple frames improve the single-image reconstruction method IHOI?
- **Basis in paper**: [explicit] The paper mentions a temporal extension of IHOI (IHOI+temp) that averages SDFs from multiple frames, but it does not provide a detailed analysis of its effectiveness or potential limitations.
- **Why unresolved**: The paper only presents a brief comparison of IHOI+temp with baseline IHOI, and does not explore the potential benefits and drawbacks of this approach in detail.
- **What evidence would resolve it**: A thorough evaluation of IHOI+temp on the SHOWMe dataset, including a comparison with other temporal aggregation methods and an analysis of the impact on reconstruction accuracy and completeness.

### Open Question 3
- **Question**: What are the limitations of the rigid hand-object assumption in the SHOWMe dataset, and how can they be addressed for more dynamic hand-object interactions?
- **Basis in paper**: [explicit] The paper acknowledges that the rigid hand-object assumption limits the reconstruction baselines to static scenarios and is an important step towards dynamic object-agnostic HO reconstruction.
- **Why unresolved**: The paper does not explore the potential challenges and solutions for extending the SHOWMe dataset and reconstruction methods to dynamic hand-object interactions.
- **What evidence would resolve it**: A study investigating the impact of hand-object motion on reconstruction accuracy, along with experiments testing different methods for handling dynamic interactions, such as non-rigid registration or motion compensation techniques.

## Limitations
- General applicability limited to rigid hand-object scenarios, not dynamic manipulation
- Reliance on high-quality depth initialization via Intel RealSense L515
- Limited evaluation of DOPE hand pose estimation robustness to occlusion

## Confidence
- **High** for dataset construction methodology and evaluation metrics
- **Medium** for relative performance of rigid transformation estimation methods
- **Low** for generalization to non-rigid or highly dynamic hand-object interactions

## Next Checks
1. Evaluate DOPE hand pose estimation accuracy on a subset of SHOWMe sequences with varying occlusion levels to quantify the robustness assumption
2. Test the reconstruction pipeline on sequences where the hand-object configuration changes dynamically (e.g., manipulation or slippage) to assess the limits of the rigidity assumption
3. Compare reconstruction results using depth sensors with different noise characteristics (e.g., structured light vs. time-of-flight) to understand the sensitivity to depth initialization quality