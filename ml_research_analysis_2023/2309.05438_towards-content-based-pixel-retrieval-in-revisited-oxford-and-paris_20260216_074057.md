---
ver: rpa2
title: Towards Content-based Pixel Retrieval in Revisited Oxford and Paris
arxiv_id: '2309.05438'
source_url: https://arxiv.org/abs/2309.05438
tags:
- retrieval
- image
- pixel
- segmentation
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROxford and PRParis, the first pixel retrieval
  benchmarks that provide pixel-level annotations for landmark images. Unlike image
  retrieval which only returns ranked lists of images, pixel retrieval segments and
  localizes the query object within database images.
---

# Towards Content-based Pixel Retrieval in Revisited Oxford and Paris

## Quick Facts
- arXiv ID: 2309.05438
- Source URL: https://arxiv.org/abs/2309.05438
- Reference count: 40
- Primary result: First pixel retrieval benchmarks (PROxford and PRParis) with 5,942 images featuring pixel-level annotations for landmark retrieval and segmentation

## Executive Summary
This paper introduces PROxford and PRParis, the first benchmarks for pixel-level object retrieval and segmentation. The datasets provide high-quality pixel-level masks for landmark images, extending traditional image retrieval by localizing query objects within database images. The authors evaluate state-of-the-art methods in image search, detection, segmentation, and dense matching, demonstrating that pixel retrieval is a challenging task requiring fine-grained recognition and variable-granularity segmentation capabilities. User studies show significant improvements in search efficiency when pixel-level annotations are provided.

## Method Summary
The PROxford and PRParis datasets are created by collecting pixel-level masks for 5,942 images from ROxford and RParis datasets. Three professional annotators perform labeling, refinement, and quality checking of segmentation masks for query-index image pairs. The pixel retrieval task is evaluated using mAP@50:5:95 and mIoU metrics, combining image ranking, reranking, localization, and segmentation performance. The authors test various baseline methods including image retrieval (SIFT+SP, DELF+SP, DELG+SP, ASMK, D2R), detection (OWL-ViT), segmentation (SSP), and dense matching (WarpC-GLUNet) approaches.

## Key Results
- PROxford and PRParis achieve 84.0% and 79.8% mAP@50:5:95 respectively, with corresponding mIoU scores of 74.2% and 70.4%
- Pixel-level annotations enable users to complete search tasks significantly faster (37.07s vs 53.71s) compared to no annotations
- Current methods show varying performance gaps between retrieval and pixel-level evaluation metrics, highlighting the difficulty of fine-grained recognition and segmentation
- Dense matching methods like WarpC-GLUNet demonstrate superior segmentation quality but require substantially more computational time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel retrieval provides fine-grained instance-level segmentation beyond standard image retrieval by localizing query objects within database images.
- Mechanism: The task extends image retrieval by adding pixel-level mask annotations, enabling machines to recognize, localize, and segment specific instances even under viewpoint and illumination changes.
- Core assumption: Query masks uniquely identify target objects, allowing removal of background and occlusions while preserving small-sized occlusion objects.
- Evidence anchors:
  - [abstract] "pixel retrieval is an extension of image retrieval and offers information about which pixels are related to the query object"
  - [section] "Pixel retrieval is defined as searching pixels that depict the query object from the database"
- Break condition: If query masks fail to uniquely identify objects, the pixel retrieval task becomes ill-defined and evaluation metrics lose meaning.

### Mechanism 2
- Claim: The PROxford and PRParis benchmarks provide challenging test cases that reflect real-world recognition difficulty by using ROxford and RParis datasets.
- Mechanism: These datasets contain positive image pairs with severe viewpoint changes, occlusions, and illumination changes, making them suitable for evaluating fine-grained recognition ability.
- Core assumption: Positive images are identifiable by humans without contextual information, ensuring the task is objectively defined.
- Evidence anchors:
  - [abstract] "These datasets are the ideal data sources for our pixel-retrieval, thanks to several properties"
  - [section] "every positive image is guaranteed to be identifiable by people without considering any contextual visual information"
- Break condition: If positive images cannot be reliably identified by humans, the benchmark loses its validity for measuring recognition performance.

### Mechanism 3
- Claim: Pixel-level annotations significantly improve user experience in web search by enabling faster identification of query objects in retrieved results.
- Mechanism: User study shows participants completed tasks faster with pixel-level annotations (mean=37.07s) compared to no annotations (mean=53.71s).
- Core assumption: Users benefit from visual guidance that highlights relevant object regions in complex search results.
- Evidence anchors:
  - [abstract] "Our user study results show pixel-level annotation can significantly improve the user experience"
  - [section] "Our results show that participants completed the task faster when the pixel-level annotations were presented"
- Break condition: If annotation quality degrades or becomes inconsistent, user experience benefits may diminish or reverse.

## Foundational Learning

- Concept: Semantic segmentation
  - Why needed here: Understanding the difference between pixel-level category information (semantic segmentation) and instance-level recognition (pixel retrieval)
  - Quick check question: What distinguishes semantic segmentation from pixel retrieval in terms of recognition granularity?

- Concept: Image retrieval metrics
  - Why needed here: Understanding mAP, mAP@50:5:95, and their application to pixel-level evaluation
  - Quick check question: How does mAP@50:5:95 differ from standard mAP in image retrieval?

- Concept: One-shot learning
  - Why needed here: Pixel retrieval requires recognizing and segmenting objects from single examples without prior training on those specific instances
  - Quick check question: What makes pixel retrieval a one-shot learning problem compared to traditional segmentation?

## Architecture Onboarding

- Component map: Data pipeline includes query mask generation → annotation refinement → metric calculation → method evaluation. Core components are annotation tools, quality assurance workflow, and evaluation metrics.
- Critical path: Professional annotators label query-index pairs → two rounds of double-checking and refinement → consensus building → final mask generation → performance evaluation using mAP@50:5:95
- Design tradeoffs: High annotation quality requires multiple annotators and refinement rounds (increases cost) versus single-annotator approach (lower quality but faster)
- Failure signatures: Inconsistent masks across annotators, ambiguous object boundaries, incorrect target object identification, poor convergence in refinement process
- First 3 experiments:
  1. Compare mAP@50:5:95 scores between different aggregation methods (VLAD vs ASMK) on PROxford dataset
  2. Evaluate impact of dense matching with WarpC versus standard GLUNet on segmentation quality
  3. Test user study performance differences between pixel-level annotations and bounding box annotations on web search task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of pixel retrieval methods be improved beyond current state-of-the-art approaches?
- Basis in paper: [explicit] The paper states that pixel retrieval is challenging and current methods show suboptimal performance, especially in fine-grained recognition and variable-granularity segmentation.
- Why unresolved: The paper demonstrates that existing methods struggle with accurate localization and segmentation of query objects in complex scenes, indicating a need for new approaches that can better handle fine-grained recognition and adaptive segmentation.
- What evidence would resolve it: Developing and evaluating novel methods that significantly outperform current approaches on the PROxford and PRParis benchmarks in terms of both detection and segmentation metrics.

### Open Question 2
- Question: How can pixel retrieval methods be scaled and optimized for speed to handle large-scale databases?
- Basis in paper: [explicit] The paper mentions that segmentation and dense matching methods are inherently slower than retrieval methods like ASMK and D2R, highlighting the need for fast methods that can handle extensive scales.
- Why unresolved: While current methods can achieve reasonable accuracy, they may not be efficient enough for real-world applications that require processing millions of images in real-time.
- What evidence would resolve it: Developing and demonstrating pixel retrieval methods that achieve high accuracy on the benchmarks while also being orders of magnitude faster than current approaches, enabling practical deployment in large-scale systems.

### Open Question 3
- Question: What role does training data play in enabling machine learning models to achieve human-like content-based segmentation abilities?
- Basis in paper: [explicit] The paper notes that humans can recognize and segment objects even without prior exposure to the specific landmarks, suggesting an innate ability that current models struggle to replicate.
- Why unresolved: The paper demonstrates that current models require extensive training data to perform well, while humans can segment objects with minimal exposure, indicating a gap in our understanding of how to enable machines to learn like humans.
- What evidence would resolve it: Conducting experiments that compare the performance of models trained on various amounts and types of data to human performance on the pixel retrieval task, identifying key factors that enable human-like recognition abilities.

## Limitations

- Annotation process requires multiple professional annotators and refinement rounds, making reproduction challenging and expensive
- Claims about human identifiability of positive images lack corpus evidence and may not hold universally
- User study methodology details are limited to supplementary materials, reducing confidence in reported results

## Confidence

**Major Claim Clusters Confidence:**
- **High confidence**: The technical definition of pixel retrieval as an extension of image retrieval with instance-level segmentation capabilities
- **Medium confidence**: The effectiveness of the three-annotator refinement process for ensuring annotation quality
- **Low confidence**: The user study results demonstrating improved user experience with pixel-level annotations, due to limited methodology details

## Next Checks

1. **Annotation Quality Validation**: Conduct a reproducibility test using independent annotators to verify consistency of pixel-level masks across the PROxford and PRParis datasets
2. **Human Identifiability Assessment**: Perform a controlled experiment to verify that all positive image pairs can indeed be identified by humans without contextual information, as claimed
3. **User Study Methodology Audit**: Request and review the detailed user study protocol and statistical analysis to validate the claimed improvements in user experience with pixel-level annotations