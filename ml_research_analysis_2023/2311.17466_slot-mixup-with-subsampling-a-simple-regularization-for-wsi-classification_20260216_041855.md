---
ver: rpa2
title: 'Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification'
arxiv_id: '2311.17466'
source_url: https://arxiv.org/abs/2311.17466
tags:
- patches
- subsampling
- attention
- classification
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of overfitting in whole slide
  image (WSI) classification, a common issue in weakly supervised multiple instance
  learning (MIL) due to limited slide-level labels. To address this, the authors propose
  Slot-MIL, a computationally efficient model that aggregates patches into a fixed
  number of slots using an attention mechanism.
---

# Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification

## Quick Facts
- arXiv ID: 2311.17466
- Source URL: https://arxiv.org/abs/2311.17466
- Reference count: 40
- One-line primary result: SubMix achieves state-of-the-art performance on WSI classification by combining attention-based aggregation with subsampling and slot-level mixup augmentation

## Executive Summary
This paper addresses overfitting in whole slide image (WSI) classification, a common challenge in weakly supervised multiple instance learning due to limited slide-level labels. The authors propose Slot-MIL, an attention-based model that aggregates patches into slots, combined with subsampling augmentation and slot-level mixup (collectively called SubMix). The approach achieves state-of-the-art performance across multiple benchmark datasets while improving both generalization and calibration of predictions.

## Method Summary
The method introduces Slot-MIL, which uses attention mechanisms to aggregate patches into a fixed number of slots, followed by PMA modules for classification. Subsampling randomly selects a subset of patches each iteration to prevent overfitting by distributing attention more evenly. Slot-Mixup applies mixup augmentation at the slot level after a late-mix parameter L epochs, avoiding ambiguity in patch selection. The combination (SubMix) is trained with Adam optimizer and evaluated on WSI datasets using accuracy, AUC, and NLL metrics.

## Key Results
- SubMix achieves state-of-the-art performance on TCGA-NSCLC, CAMELYON-16, and CAMELYON-17 datasets
- Subsampling reduces overfitting by promoting more equitable attention distribution among patches
- Slot-Mixup improves generalization through semantically meaningful convex combinations of slot representations
- The combined approach enhances both generalization and calibration compared to existing MIL methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subsampling prevents overfitting by distributing attention scores more evenly across patches
- Mechanism: Randomly selecting a subset of patches each iteration changes the set of positive patches contributing to the slide-level label, forcing the model to rely on a more diverse set of patches
- Core assumption: The slide-level label is determined by the presence of at least one positive patch
- Evidence anchors:
  - [abstract]: "We empirically demonstrate that the subsampling augmentation helps to make more informative slots by restricting the over-concentration of attention"
  - [section 4.1.2]: "empirical analysis establishes a correlation between overfitting and attention scores across patches"
- Break condition: If the subsampling rate is too high, important patches might be consistently excluded

### Mechanism 2
- Claim: Slot-Mixup improves generalization by creating smoother decision boundaries through convex combinations of slot representations
- Mechanism: Mixing slots provides semantically meaningful augmentations by combining patch summaries rather than individual patches with varying counts
- Core assumption: Slots serve as meaningful summaries of the underlying patch information
- Evidence anchors:
  - [abstract]: "we can directly adopt mixup to slots. It does not require any extra layer or knowledge distillation"
  - [section 3.3]: "the act of mixing slots is anticipated to yield more meaningful augmented data"
- Break condition: If slots don't adequately represent the underlying patch information, mixing them could create semantically meaningless combinations

### Mechanism 3
- Claim: The combination of subsampling and Slot-Mixup achieves better calibration by addressing both overfitting and improving generalization
- Mechanism: Subsampling prevents over-concentration of attention while mixup creates intermediate examples that smooth decision boundaries
- Core assumption: Both overfitting and poor calibration stem from weak supervision in MIL
- Evidence anchors:
  - [abstract]: "combining our attention-based aggregation model with subsampling and mixup... can enhance both generalization and calibration"
  - [section 4.1.3]: "Combining both techniques, which we refer to as SubMix, allows us to improve generalization performance while maintaining well-calibrated predictions"
- Break condition: If one component dominates the other, benefits might not be synergistic

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: WSI classification is a classic MIL problem where only slide-level labels are available
  - Quick check question: In MIL, if a bag contains at least one positive instance, what is the bag label?

- Concept: Attention Mechanisms
  - Why needed here: The Slot-MIL model uses attention to aggregate patches into slots
  - Quick check question: How does the attention mechanism in Slot-MIL differ from traditional self-attention in transformers?

- Concept: Data Augmentation Techniques
  - Why needed here: Due to limited labeled data in WSI classification, augmentation techniques are crucial for improving generalization
  - Quick check question: What's the key difference between patch-level mixup and slot-level mixup in the context of MIL?

## Architecture Onboarding

- Component map: Patch Encoder -> PMA Module 1 (with subsampling) -> PMA Module 2 -> Classification logits
- Critical path: Patch features → PMA Module 1 (with subsampling) → PMA Module 2 → Classification logits → Loss computation
- Design tradeoffs:
  - Number of slots vs. computational efficiency: More slots capture more information but increase computation
  - Subsampling rate vs. information preservation: Higher rates reduce overfitting but risk losing important patches
  - Mixup ratio vs. semantic preservation: Higher mixing creates smoother boundaries but may lose original semantics
- Failure signatures:
  - Overfitting: Decreasing validation performance while training performance continues to improve
  - Underfitting: Both training and validation performance remain low
  - Poor calibration: High confidence in incorrect predictions (low NLL despite good accuracy)
- First 3 experiments:
  1. Train Slot-MIL without any augmentation to establish baseline performance
  2. Add subsampling to Slot-MIL and measure changes in overfitting and attention distribution
  3. Add Slot-Mixup to Slot-MIL with subsampling to evaluate combined effects on generalization and calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SubMix vary with different numbers of inducing points (S) in the PMA module?
- Basis in paper: [explicit] The paper states "we empirically demonstrate that a small number of slots is sufficient to capture the underlying semantics of WSIs"
- Why unresolved: The paper only tests a limited range of slot numbers (4, 8, 16, 32) and finds minimal performance difference above a certain threshold
- What evidence would resolve it: A systematic study varying S across a wider range and correlating performance with dataset properties

### Open Question 2
- Question: What is the impact of subsampling on attention distribution for WSIs with varying proportions of positive patches?
- Basis in paper: [explicit] The paper shows that subsampling improves attention distribution and reduces overfitting, with different optimal rates for datasets with different positive patch proportions
- Why unresolved: While the paper demonstrates the effect of subsampling rate, it doesn't explore how the proportion of positive patches affects the attention distribution or optimal subsampling strategy
- What evidence would resolve it: Experiments varying the positive patch proportion in synthetic datasets and analyzing attention distribution with and without subsampling

### Open Question 3
- Question: How does the late-mix parameter L affect the trade-off between generalization and calibration in SubMix?
- Basis in paper: [explicit] The paper states "Starting mixup from initial epoch is not recommended especially in CAMELYON-16, and CAMELYON-17"
- Why unresolved: The paper only tests a limited range of L values (0, 0.1, 0.2, 0.3) and finds optimal values for specific datasets
- What evidence would resolve it: Experiments varying L across a wider range and analyzing its effect on both generalization metrics and calibration metrics

## Limitations
- The method assumes patch features extracted from pre-trained ResNet models, which may not generalize to other feature extractors
- Optimal hyperparameters (number of slots, subsampling rate, mixup parameters) are dataset-dependent and require extensive tuning
- Computational cost, while reduced compared to other MIL methods, still scales with the number of patches and slots

## Confidence
Our confidence in the proposed mechanisms is **Medium**. The core claims about attention distribution and slot-level mixup are supported by empirical results showing improved performance across multiple datasets, but the theoretical foundations for why these specific mechanisms work remain largely heuristic.

## Next Checks
1. **Attention Distribution Analysis**: Systematically vary the subsampling rate and measure changes in attention entropy across patches to verify the claimed relationship between subsampling and attention regularization
2. **Slot Representativeness Test**: Evaluate whether mixed slots preserve semantic content by measuring classification performance when mixing slots from different classes versus the same class
3. **Ablation on Mixup Parameters**: Conduct a grid search over the mixup ratio α and late-mix parameter L to identify the sensitivity of performance to these hyperparameters and determine if the improvements are robust across different settings