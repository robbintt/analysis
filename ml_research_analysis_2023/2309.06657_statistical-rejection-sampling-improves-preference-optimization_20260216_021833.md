---
ver: rpa2
title: Statistical Rejection Sampling Improves Preference Optimization
arxiv_id: '2309.06657'
source_url: https://arxiv.org/abs/2309.06657
tags:
- preference
- reward
- policy
- human
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach called Statistical Rejection
  Sampling Optimization (RSO) to improve the alignment of language models with human
  preferences. RSO addresses the limitation of existing offline methods like SLiC
  and DPO by using rejection sampling to generate preference pairs from the target
  optimal policy, enabling a more accurate estimation of the optimal policy.
---

# Statistical Rejection Sampling Improves Preference Optimization

## Quick Facts
- arXiv ID: 2309.06657
- Source URL: https://arxiv.org/abs/2309.06657
- Reference count: 11
- One-line primary result: RSO achieves 71.86% win rate against SFT on Reddit TL;DR task, outperforming DPO's 67.72%

## Executive Summary
This paper introduces Statistical Rejection Sampling Optimization (RSO), a novel approach that improves alignment of language models with human preferences by using rejection sampling to generate preference pairs from the optimal policy. RSO addresses limitations in existing offline methods like SLiC and DPO by unifying their loss functions under a statistical framework and proposing a more accurate estimation of the optimal policy through rejection sampling.

## Method Summary
RSO trains a pairwise reward-ranking model on human preference data, then uses statistical rejection sampling to generate preference pairs from the optimal policy. These pairs are used to fine-tune the language model using either logistic loss (similar to DPO) or hinge loss (similar to SLiC). The method introduces a hyperparameter β that controls the acceptance rate of the rejection sampling algorithm, with the goal of generating more optimal preference pairs than existing methods.

## Key Results
- RSO achieves 71.86% win rate against SFT on Reddit TL;DR task, compared to DPO's 67.72%
- On CNN/DailyMail summarization, RSO achieves 39.71% win rate versus DPO's 37.36%
- Human evaluation shows RSO is preferred over 2x as often as DPO across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rejection sampling generates preference pairs from a policy closer to the optimal policy than existing methods.
- Mechanism: By using rejection sampling, RSO generates response pairs from a policy distribution that is more aligned with the optimal policy, as opposed to SLiC which samples from the SFT policy or DPO which uses direct human preference data.
- Core assumption: The reward model accurately reflects human preferences and can guide the rejection sampling process.
- Evidence anchors:
  - [abstract] "RSO addresses the limitation of existing offline methods like SLiC and DPO by using rejection sampling to generate preference pairs from the target optimal policy"
  - [section] "We propose a statistical rejection sampling algorithm to sample pairs from the optimal policy and get them labeled by a pairwise reward-ranking model."
- Break condition: If the reward model is inaccurate or biased, the rejection sampling process will generate preference pairs that do not reflect the true optimal policy.

### Mechanism 2
- Claim: RSO unifies SLiC and DPO by analyzing their loss functions as different statistical models.
- Mechanism: RSO shows that DPO is equivalent to logistic regression on human preference data, while SLiC is equivalent to a support vector machine with hinge loss. This unification allows for a more comprehensive understanding of preference optimization methods.
- Core assumption: The preference data can be modeled using standard statistical techniques like logistic regression and SVM.
- Evidence anchors:
  - [abstract] "We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint."
  - [section] "Statistically, we unify DPO and SLiC by showing that they vary by loss functions to fit on human preference data: DPO is a logistic regression on human preference data and SLiC is almost equivalent to a support vector machine (SVM) with hinge loss."
- Break condition: If the preference data does not follow the assumed statistical models, the unification may not hold.

### Mechanism 3
- Claim: RSO's statistical rejection sampling algorithm can generate samples from the optimal policy with theoretical guarantees.
- Mechanism: The rejection sampling algorithm generates samples from the optimal policy by accepting or rejecting samples from the SFT policy based on their reward scores. The acceptance rate is theoretically bounded and depends on the parameter β.
- Core assumption: The SFT policy is a reasonable proposal distribution for the optimal policy.
- Evidence anchors:
  - [section] "Theorem 1. Let N be number of candidate responses, and rmax be the maximum rewards among the candidate responses not yet accepted. As N → ∞, Algorithm 1 can generate K distinct samples from πrψ with expected acceptance rate Ey∼πsft(y|x) [exp(1 β · (rψ(x, y) − rmax))]."
  - [section] "In practice, we treat β as a hyper-parameter and pick one according to validation metrics."
- Break condition: If the SFT policy is too dissimilar from the optimal policy, the rejection sampling algorithm may have an extremely low acceptance rate, making it impractical.

## Foundational Learning

- Concept: Rejection sampling
  - Why needed here: Rejection sampling is used to generate samples from the optimal policy, which is not directly observable.
  - Quick check question: What is the main advantage of using rejection sampling in RSO?

- Concept: Statistical estimation
  - Why needed here: RSO estimates the optimal policy by fitting a model on preference pairs generated via rejection sampling.
  - Quick check question: Why is it important to generate preference pairs from the optimal policy in RSO?

- Concept: Loss functions in preference optimization
  - Why needed here: Understanding the loss functions used in RSO (logistic regression and hinge loss) is crucial for implementing and tuning the method.
  - Quick check question: What is the difference between the loss functions used in DPO and SLiC, according to RSO?

## Architecture Onboarding

- Component map: SFT policy -> Pairwise reward-ranking model -> Rejection sampling algorithm -> Language model to be optimized
- Critical path:
  1. Train pairwise reward-ranking model on human preference data
  2. Use rejection sampling to generate preference pairs from the optimal policy
  3. Fit language model on generated preference pairs using appropriate loss function
- Design tradeoffs:
  - Rejection sampling vs. direct sampling: Rejection sampling can generate more optimal preference pairs but may have a lower acceptance rate.
  - Choice of loss function: Different loss functions (logistic regression vs. hinge loss) may lead to different performance characteristics.
- Failure signatures:
  - Low acceptance rate in rejection sampling: Indicates that the SFT policy is too dissimilar from the optimal policy.
  - Poor performance on AutoSxS or human evaluation: Suggests that the learned policy is not well-aligned with human preferences.
- First 3 experiments:
  1. Implement rejection sampling algorithm and verify that it generates samples from the optimal policy.
  2. Train pairwise reward-ranking model on human preference data and evaluate its accuracy.
  3. Implement RSO with logistic regression loss and evaluate its performance on a small-scale task.

## Open Questions the Paper Calls Out

- Open Question 1: What is the optimal value of β for statistical rejection sampling across different tasks and model sizes?
  - Basis in paper: [explicit] The paper states "In practice, we treat β as a hyper-parameter and pick one according to validation metrics." and shows experiments with β = 0.5.
  - Why unresolved: The paper only experiments with a few values of β and shows it varies by task, but does not explore the full parameter space or determine an optimal universal value.
  - What evidence would resolve it: Extensive experiments varying β across a wide range of values for multiple tasks and model sizes to determine the impact on final performance and find optimal values.

- Open Question 2: How does the quality of the pairwise reward-ranking model impact the performance of RSO?
  - Basis in paper: [inferred] The paper uses a large T5-XXL pairwise reward-ranking model but does not explore the impact of model size or quality on final RSO performance.
  - Why unresolved: The paper does not experiment with different sizes or qualities of the pairwise reward-ranking model to determine its impact on RSO.
  - What evidence would resolve it: Experiments using pairwise reward-ranking models of different sizes and qualities to determine the relationship between reward model quality and RSO performance.

- Open Question 3: Can RSO be applied to other language generation tasks beyond summarization and dialogue?
  - Basis in paper: [inferred] The paper only experiments with summarization and dialogue tasks, but the method could potentially be applied to other language generation tasks.
  - Why unresolved: The paper does not explore applying RSO to other tasks like translation, question answering, etc.
  - What evidence would resolve it: Applying RSO to a diverse set of language generation tasks and evaluating its performance to determine its generalizability.

## Limitations
- Requires training an additional pairwise reward-ranking model, increasing computational cost and complexity
- Theoretical guarantees assume access to the true optimal policy, which is not available in practice
- Does not extensively explore sensitivity to different reward models or potential for overfitting

## Confidence
- Mechanism 1: Medium - theoretical grounding is strong but empirical validation of reward model quality is limited
- Mechanism 2: High - well-supported statistical analysis with clear theoretical foundations
- Mechanism 3: Medium-High - theoretical guarantees are provided but practical limitations are not fully explored

## Next Checks
1. Implement a robustness analysis by training RSO with different pairwise reward-ranking models (e.g., different architectures or training data) and measuring the impact on final performance.
2. Conduct an ablation study to quantify the contribution of the rejection sampling component by comparing RSO with and without rejection sampling on the same tasks.
3. Investigate the acceptance rate of the rejection sampling algorithm across different values of β and tasks to understand its practical limitations and guide hyperparameter selection.