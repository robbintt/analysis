---
ver: rpa2
title: 'Assembled-OpenML: Creating Efficient Benchmarks for Ensembles in AutoML with
  OpenML'
arxiv_id: '2307.00285'
source_url: https://arxiv.org/abs/2307.00285
tags:
- ensemble
- data
- openml
- techniques
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Assembled-OpenML is a Python tool that automates the creation\
  \ of meta-datasets (metatasks) for comparing ensemble techniques in AutoML. By leveraging\
  \ prediction data from OpenML instead of retraining base models, it dramatically\
  \ reduces computational cost\u2014building 31 metatasks with 1523 base models in\
  \ ~1 hour versus ~37 minutes to train just one model on the most complex dataset."
---

# Assembled-OpenML: Creating Efficient Benchmarks for Ensembles in AutoML with OpenML

## Quick Facts
- **arXiv ID:** 2307.00285
- **Source URL:** https://arxiv.org/abs/2307.00285
- **Reference count:** 38
- **Primary result:** A Python tool that automates creation of meta-datasets for comparing ensemble techniques in AutoML by leveraging OpenML prediction data

## Executive Summary
Assembled-OpenML addresses the computational bottleneck in benchmarking ensemble techniques for AutoML by creating meta-datasets (metatasks) that store prediction data from top-performing models on OpenML tasks. Instead of retraining base models, ensemble techniques can be evaluated using these stored predictions, reducing the time to build 31 metatasks with 1523 base models from ~37 minutes to train one model down to approximately one hour. The tool fetches model configurations, extracts predictions and confidences, and packages them with metadata to enable simulation of stacking, voting, and dynamic selection ensemble methods. While initial results show promise for ensemble selection and stacking, challenges remain with small datasets, limited model diversity, and data quality issues in OpenML.

## Method Summary
Assembled-OpenML builds metatasks by fetching an OpenML task ID, retrieving the original task and dataset, collecting top-n best performing configurations, parsing and storing their prediction data, and packaging everything with metadata. The tool uses VBA-SBA-Gap filtering to identify tasks where ensemble techniques might improve performance, requiring at least a 5% gap between perfect selection and single best algorithm. Ensemble techniques are then simulated using only the stored prediction data, avoiding the computational cost of retraining base models. The framework enables efficient comparison of ensemble methods across multiple tasks while maintaining the ability to evaluate techniques like stacking, voting, and dynamic selection.

## Key Results
- Building 31 metatasks with 1523 base models takes ~1 hour using prediction data vs ~37 minutes to train just one model
- Metatasks enable simulation of ensemble techniques (stacking, voting, dynamic selection) using stored predictions instead of retraining
- Initial experiments show potential for ensemble selection and stacking to outperform single models on filtered tasks
- VBA-SBA-Gap filtering identifies tasks with theoretical room for ensemble improvement (â‰¥5% gap required)

## Why This Works (Mechanism)

### Mechanism 1
Using stored predictions instead of retraining base models dramatically reduces computational cost. The tool retrieves prediction data and confidences from OpenML for top-performing model configurations, allowing ensemble techniques to be evaluated using these predictions without retraining the base models. Core assumption: Prediction data from OpenML is sufficiently accurate and available for the desired tasks. Evidence: Building 31 metatasks with 1523 base models in ~1 hour versus ~37 minutes to train just one model. Break condition: OpenML does not have sufficient prediction data for a task, or the prediction data is corrupted or inconsistent.

### Mechanism 2
Metatasks enable simulation of ensemble techniques without training base models. A metatask contains the original task data, dataset, metadata, and prediction data from model evaluations. Ensemble techniques can be executed using only this data, leaving only the computational overhead of the ensemble technique itself. Core assumption: Ensemble techniques can be implemented to accept predictions as input instead of base models. Evidence: These metatasks enable simulation of ensemble techniques like stacking, voting, and dynamic selection by evaluating them across folds using stored predictions. Break condition: Ensemble techniques require access to training data or model internals that cannot be simulated with predictions alone.

### Mechanism 3
Post-processing metatasks with VBA-SBA-Gap filtering identifies tasks where ensemble techniques are likely to improve performance. The Virtual Best Algorithm (VBA) represents perfect selection, while the Single Best Algorithm (SBA) represents the average best. Tasks with a VBA-SBA-Gap of at least 5% indicate theoretical room for ensemble improvement. Core assumption: A significant VBA-SBA-Gap correlates with potential for ensemble techniques to outperform single models. Evidence: We required that metatasks have a VBA-SBA-Gap of 5% in performance to guarantee that there is a (theoretical) room for improvement over the SBA. Break condition: The VBA-SBA-Gap does not actually correlate with ensemble performance gains, or the threshold is set incorrectly.

## Foundational Learning

- **Concept: OpenML platform and ecosystem**
  - Why needed here: The tool builds upon OpenML to fetch task data, model configurations, and prediction data
  - Quick check question: What are the key data types (tasks, runs, flows, predictions) stored in OpenML and how do they relate to each other?

- **Concept: Ensemble learning techniques**
  - Why needed here: The tool simulates various ensemble techniques (stacking, voting, dynamic selection) using prediction data
  - Quick check question: How do stacking, voting, and dynamic selection differ in their approach to combining base model predictions?

- **Concept: Meta-learning and meta-datasets**
  - Why needed here: Metatasks are meta-datasets that enable efficient comparison of ensemble techniques
  - Quick check question: What distinguishes a metatask from a regular dataset, and why is this distinction important for ensemble benchmarking?

## Architecture Onboarding

- **Component map:** Assembled-OpenML -> Fetch OpenML data -> Parse predictions -> Package metatask -> Simulate ensembles
- **Critical path:** 1. Input: OpenML task ID 2. Fetch original task and dataset 3. Fetch top-n best performing configurations 4. Parse and collect prediction data 5. Package into metatask 6. (Optional) Post-process metatasks with filtering 7. Simulate ensemble techniques using metatask data
- **Design tradeoffs:** OpenML dependency leverages existing data but limited by OpenML's coverage and data quality; using predictions is faster but may miss techniques requiring model access; top-n selection is simple but may not ensure diverse base models for ensembles
- **Failure signatures:** Slow metatask creation indicates OpenML API issues or large/complex tasks; missing prediction data suggests insufficient runs for a task or corrupted data; inconsistent predictions may indicate algorithmic differences or numerical precision issues; poor ensemble performance could be due to lack of model diversity or insufficient training data
- **First 3 experiments:** 1. Build a metatask for a simple, well-known OpenML task (e.g., iris) and verify all components are correctly fetched 2. Implement and test a basic ensemble technique (e.g., majority voting) using the metatask's prediction data 3. Compare the performance of an ensemble technique using predictions vs. retraining the base models on a small task

## Open Questions the Paper Calls Out

- **Open Question 1:** How can Assembled-OpenML ensure diversity in base models when fetching top-performing configurations? The paper notes this as a challenge but doesn't propose a concrete solution for ensuring diversity while maintaining computational efficiency. What evidence would resolve it: Results showing that diverse base models consistently improve ensemble performance compared to non-diverse models, along with a comparison of different diversity metrics.

- **Open Question 2:** How can Assembled-OpenML handle prediction data discrepancies where predictions don't match the class with highest confidence? The paper acknowledges the problem but states that some discrepancies cannot be explained or fixed, and filters these cases rather than resolving them. What evidence would resolve it: A methodology for automatically detecting and correcting or flagging problematic prediction data, validated by comparing corrected vs. uncorrected results.

- **Open Question 3:** Would fetching best configurations from diverse algorithms (flows) rather than top overall configurations improve ensemble performance? The paper abandoned this approach due to data scarcity and duplicate algorithm problems, but doesn't provide conclusive evidence that it wouldn't improve results. What evidence would resolve it: A benchmark comparison showing performance differences between ensembles built from diverse algorithm configurations vs. top overall configurations.

## Limitations
- Effectiveness depends heavily on quality and completeness of prediction data available in OpenML, with some tasks lacking sufficient data particularly for regression tasks or newer datasets
- Reliance on top-n model configurations may limit ensemble diversity if selected models are too similar in approach or architecture
- Tool's framework may not support ensemble techniques requiring access to base model internals or training data beyond predictions

## Confidence

- **High Confidence:** The computational cost reduction mechanism (using stored predictions instead of retraining) is well-demonstrated with specific timing comparisons
- **Medium Confidence:** The framework's ability to simulate ensemble techniques using prediction data is plausible but requires validation across diverse ensemble methods
- **Low Confidence:** The correlation between VBA-SBA-Gap filtering and actual ensemble performance gains needs empirical validation, as the theoretical basis may not translate to practical improvements

## Next Checks

1. **Diversity Analysis:** Measure the diversity of base models selected by the top-n approach across multiple tasks to ensure sufficient variation for effective ensemble learning

2. **Performance Validation:** Compare ensemble technique performance using Assembled-OpenML metatasks against traditional retraining approaches on a set of benchmark tasks to verify no significant performance degradation

3. **Cross-Task Generalizability:** Test the framework across tasks with varying characteristics (classification vs. regression, different sizes, different domains) to identify limitations in task coverage