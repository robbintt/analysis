---
ver: rpa2
title: An Explainable Collaborative Dialogue System using a Theory of Mind
arxiv_id: '2302.09646'
source_url: https://arxiv.org/abs/2302.09646
tags:
- system
- user
- action
- dialogue
- pgoal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Eva is a multimodal collaborative dialogue system that assists
  users by inferring their intentions and plans, detecting obstacles, and generating
  plans to overcome them. The system employs a rich modal logic-based knowledge representation
  and a planning and reasoning architecture that obeys the principles of persistent
  goals and intentions.
---

# An Explainable Collaborative Dialogue System using a Theory of Mind

## Quick Facts
- arXiv ID: 2302.09646
- Source URL: https://arxiv.org/abs/2302.09646
- Reference count: 0
- Eva is a multimodal collaborative dialogue system that assists users by inferring their intentions and plans

## Executive Summary
Eva is a multimodal collaborative dialogue system that assists users by inferring their intentions and plans, detecting obstacles, and generating plans to overcome them. The system employs a rich modal logic-based knowledge representation and a planning and reasoning architecture that obeys the principles of persistent goals and intentions. Eva can explain its utterances because it has created a plan behind each of them, enabling it to engage in multi-party dialogues and handle complex multi-turn dialogues in a domain-independent way.

## Method Summary
The system uses a neuro-symbolic approach combining a rich modal logic-based knowledge representation with planning and reasoning subsystems. It employs a belief-desire-intention (BDI) architecture where the system maintains mental state representations of beliefs, persistent goals, and intentions using modal operators. The core mechanism involves parsing user input into logical forms, recognizing user plans through backward-chaining and hierarchical decomposition, debugging and adopting user goals, planning system actions to help the user, and executing actions including speech acts. The system treats its own speech acts as actions with preconditions, effects, and applicability conditions, enabling it to generate explanations by tracing the dependency chain from actions back to underlying persistent goals.

## Key Results
- System can infer user intentions and plans using modal logic operators and plan recognition rules
- Generates slot-filling questions by reasoning about incomplete knowledge using knowref modal operators
- Provides explanations by tracing dependency chains from actions back to motivating persistent goals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system infers user intentions by combining plan recognition with modal logic operators (bel, pgoal, intend) to attribute mental states.
- Mechanism: Plan recognition rules expand user actions into a web of interdependent logical forms representing beliefs, goals, and intentions. Backward-chaining from effects to actions and hierarchical decomposition enable this inference.
- Core assumption: The user's utterance is part of a larger plan, and the system can recognize that plan using the defined mental state operators.
- Evidence anchors:
  - [abstract]: "Eva attempts to collaborate with its users by inferring and debugging their plans, then planning to overcome obstacles to achieving their higher-level goals."
  - [section]: "Eva's planning involves the following rules... Effect-Action: If an agent Agt has pgoal to achieve a proposition P, and Agt can find an action (or actions(s)) Act that achieves P as an effect, then the planner creates a pgoal to do the action relative to the pgoal to bring about P."
- Break condition: If the user's utterance cannot be mapped to any known action in the domain knowledge base, plan recognition fails.

### Mechanism 2
- Claim: The system generates slot-filling questions by reasoning about incomplete knowledge using knowref modal operators.
- Mechanism: When an intention to perform an action is formed but arguments are unknown, the system creates persistent goals to knowref those arguments. These goals lead to planning wh-questions to acquire the missing values.
- Core assumption: The user knows the referent of the description (e.g., knows their own age), enabling the system to ask questions.
- Evidence anchors:
  - [abstract]: "The system treats its speech acts just like its other actions... This general approach enables Eva to plan a variety of speech acts including requests, informs, questions..."
  - [section]: "The system plans the Whq speech act when the effect of that speech act above matches the content of the system's pgoal, provided that the system believes the precondition holds, i.e., the system believes that the user knows the referent of 'the time User wants to make an appointment'."
- Break condition: If the system cannot find anyone who knows the referent (including the user), it cannot plan the question.

### Mechanism 3
- Claim: The system provides explanations by tracing the dependency chain from an action back to the underlying persistent goals that motivated it.
- Mechanism: When asked "why did you say that?", the system finds the lowest pgoal in the plan tree whose content the user does not already believe, and explains based on that goal.
- Core assumption: The system maintains a complete plan structure with all dependencies, and can identify what the user does and doesn't believe.
- Evidence anchors:
  - [abstract]: "Importantly, Eva can explain its utterances because it has created a plan standing behind each of them."
  - [section]: "The explanation finds the path in the plan starting from the action being referred to, and follows the chain of achievements, enablements, and relativizations to the intentions and persistent goals that led to the action to be explained."
- Break condition: If the plan structure is incomplete or corrupted, explanation generation fails.

## Foundational Learning

- Concept: Modal logic operators (bel, pgoal, intend, knowif, knowref)
  - Why needed here: These operators allow the system to represent and reason about mental states, which is essential for collaborative dialogue.
  - Quick check question: What's the difference between bel(X, P) and knowif(X, P)?

- Concept: Action descriptions with preconditions, effects, and applicability conditions
  - Why needed here: These descriptions enable planning by defining what actions achieve what effects and under what conditions.
  - Quick check question: How does the system handle an action whose applicability condition is false?

- Concept: Plan recognition and planning rules
  - Why needed here: These rules drive the system's ability to infer user plans and generate its own plans to help the user.
  - Quick check question: What happens when multiple actions can achieve the same effect?

## Architecture Onboarding

- Component map:
  - NLP parser → Logical Form generator → Plan recognition engine → Belief/goal/intention reasoner → Action planner → Executor → Explanation generator
  - Each component operates on logical forms with modal operators
  - Plan structure maintains dependencies between actions and mental states

- Critical path:
  1. Parse user input to logical form
  2. Recognize user's plan via plan recognition rules
  3. Debug and adopt user's goals
  4. Plan system actions to help user
  5. Execute actions (including speech acts)
  6. Handle user responses and repeat

- Design tradeoffs:
  - Horn clause reasoning vs. full first-order logic: Horn clauses are computationally tractable but less expressive
  - Declarative mental state representation vs. procedural state: Declarative enables explanation but is more complex
  - Backward-chaining planning vs. forward-chaining: Backward-chaining focuses on goals but may miss opportunities

- Failure signatures:
  - No plan recognized: User input doesn't match known actions
  - Blocked intentions: Applicability conditions false but system doesn't know
  - Circular dependencies: Plan structure has loops
  - Incomplete explanations: Plan structure missing dependencies

- First 3 experiments:
  1. Test plan recognition on simple command ("Book me a flight to Paris")
  2. Test slot-filling question generation ("What date do you want to travel?")
  3. Test explanation generation ("Why do you need to know my age?")

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system determine if a user is trustworthy or not?
- Basis in paper: [explicit] The paper discusses the importance of trust in dialogue systems and mentions that the system needs to be able to decide whether it trusts a given user. It also mentions that if the system does not trust a user, it can refrain from concluding that the user wants the typical utterance effect to hold.
- Why unresolved: The paper does not provide a specific method for determining user trustworthiness. It only mentions that this is a necessary feature for future dialogue systems.
- What evidence would resolve it: A concrete algorithm or set of rules that the system could use to assess user trustworthiness based on their dialogue behavior, statements, or other factors.

### Open Question 2
- Question: How can the system handle uncertainty in user beliefs and intentions?
- Basis in paper: [explicit] The paper mentions that the system operates with probabilities for mental states and that there may be uncertainty in inferring user plans and intentions. However, it does not provide a specific method for handling this uncertainty.
- Why unresolved: The paper does not provide a detailed explanation of how the system deals with uncertainty in user beliefs and intentions. It only mentions that this is an area for future research.
- What evidence would resolve it: A formal model or algorithm that the system could use to reason about and handle uncertainty in user beliefs and intentions.

### Open Question 3
- Question: How can the system generate more natural and varied responses?
- Basis in paper: [inferred] The paper mentions that the system uses a neural language model for paraphrasing, but it does not provide details on how the system generates natural and varied responses. It only mentions that this is an area for future research.
- Why unresolved: The paper does not provide a detailed explanation of how the system generates natural and varied responses. It only mentions that this is an area for future research.
- What evidence would resolve it: A concrete method or algorithm that the system could use to generate more natural and varied responses based on the user's input and the system's own mental states.

## Limitations
- No quantitative evaluation metrics or user studies demonstrating real-world effectiveness
- Theoretical framework lacks empirical validation across diverse domains
- Does not address scalability concerns or handling full complexity of natural language

## Confidence
- **High confidence**: Modal logic-based knowledge representation and integration of plan recognition with collaborative planning are well-specified and theoretically sound
- **Medium confidence**: Claims about handling complex multi-turn dialogues in a domain-independent way are supported by architecture but lack empirical validation
- **Low confidence**: No evidence addressing scalability or ability to handle full complexity of natural language

## Next Checks
1. Implement a controlled experiment comparing Eva's plan recognition accuracy against baseline methods on a standardized dialogue corpus with ground-truth plans.
2. Conduct a user study measuring how effectively human participants can understand and verify Eva's explanations of its actions during collaborative tasks.
3. Test the system's robustness by evaluating its performance when faced with incomplete or contradictory user information, and measure its ability to recover from misunderstandings.