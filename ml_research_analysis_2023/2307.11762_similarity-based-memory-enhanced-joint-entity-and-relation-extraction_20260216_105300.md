---
ver: rpa2
title: Similarity-based Memory Enhanced Joint Entity and Relation Extraction
arxiv_id: '2307.11762'
source_url: https://arxiv.org/abs/2307.11762
tags:
- relation
- memory
- entity
- extraction
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-task learning framework for joint entity
  and relation extraction. The key idea is to use a similarity-based memory module
  that provides a bidirectional feedback loop between the entity and relation classification
  tasks.
---

# Similarity-based Memory Enhanced Joint Entity and Relation Extraction

## Quick Facts
- arXiv ID: 2307.11762
- Source URL: https://arxiv.org/abs/2307.11762
- Authors: 
- Reference count: 16
- Key outcome: Achieves state-of-the-art results on BioCreative V CDR corpus for joint entity and relation extraction

## Executive Summary
This paper introduces a multi-task learning framework for joint entity and relation extraction that leverages a similarity-based memory module to create bidirectional feedback between tasks. The key innovation is using attention-based memory reading and writing operations to allow entity and relation classifiers to share information and learn complex dependencies. Experiments demonstrate that this approach outperforms existing methods on the BioCreative V CDR corpus, achieving state-of-the-art results for document-level joint entity and relation classification.

## Method Summary
The method combines four sub-tasks (mention detection, coreference resolution, entity classification, relation extraction) using a multi-task learning framework with bidirectional memory dependencies. The model uses BERT/SciBERT encoders and implements memory read/write operations based on attention mechanisms and bilinear similarity functions. Memory matrices ME and MR store representations for entity and relation categories respectively, which are updated using gradients from classifier losses. The training procedure includes memory warm-up initialization and uses a joint loss function with task-specific weights optimized via AdamW.

## Key Results
- Achieves state-of-the-art performance on BioCreative V CDR corpus
- Outperforms existing methods in document-level joint entity and relation extraction
- Introduces a novel similarity classifier module for document-level classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory reading via attention provides richer input representations by incorporating task-specific contextual information from previous stages.
- Mechanism: Attention-based memory reading computes weights between current input and memory matrices (ME for entities, MR for relations) to create extended representations.
- Core assumption: Attention can effectively identify and incorporate relevant information from memory matrices to enhance input representations.
- Evidence anchors: [abstract] memory-based extended representation module; [section 3.1] attention mechanism extends input representation; [corpus] Weak - no direct discussion of this mechanism.
- Break condition: If attention fails to identify relevant memory slots or memory contains noisy information, extended representations may degrade performance.

### Mechanism 2
- Claim: Bidirectional feedback loop between entity and relation classifiers enables more complex dependencies than sequential approaches.
- Mechanism: Memory writing stores representations in ME and MR matrices, which are then read by earlier task components, creating bidirectional information flow.
- Core assumption: Entity and relation classifiers can generate meaningful representations capturing task dependencies.
- Evidence anchors: [abstract] bidirectional memory-like dependency addresses pipeline drawbacks; [section 3.2] memory matrices store category representations; [corpus] Weak - no specific discussion of bidirectional memory feedback.
- Break condition: Poor memory writing operations could propagate errors rather than beneficial information.

### Mechanism 3
- Claim: Distance-based similarity classifiers with bilinear scoring enable more precise category representations in memory.
- Mechanism: Similarity function S uses bilinear scoring (MW^T x) to compute probabilities over entity and relation types.
- Core assumption: Bilinear similarity can effectively capture complex relationships needed for classification.
- Evidence anchors: [section 3.2] bilinear similarity definition; [section 5] similar results on DocRED, outperformed by JEREX on Re-DocRED; [corpus] Weak - no specific discussion of bilinear similarity for this application.
- Break condition: If bilinear similarity cannot capture necessary relationships or memory slot size is inadequate, classification performance suffers.

## Foundational Learning

- Concept: Multi-task learning with shared representations
  - Why needed here: Four sub-tasks need coordination requiring shared feature representations
  - Quick check question: How does sharing representations between tasks benefit joint learning, and what are potential drawbacks?

- Concept: Attention mechanisms for information fusion
  - Why needed here: Memory reading operation relies on attention to combine information from memory matrices
  - Quick check question: In attention mechanisms, what determines attention weights, and how do these weights affect final output representation?

- Concept: Bilinear similarity functions
  - Why needed here: Similarity classifier uses bilinear scoring to compute probabilities over types
  - Quick check question: How does bilinear similarity differ from dot product similarity, and what additional modeling capacity does it provide?

## Architecture Onboarding

- Component map: Input encoder (BERT/SciBERT) -> Mention extraction -> Coreference resolution -> Entity classifier (with memory) -> Relation classifier (with memory) -> Memory matrices ME/MR -> Joint loss

- Critical path: 1. Input text â†’ BERT encoding; 2. Mention extraction and coreference resolution (sequential); 3. Entity and relation classifiers with memory reading; 4. Memory writing based on classifier losses; 5. Joint optimization

- Design tradeoffs:
  - Memory slot size vs. memory matrix capacity (larger slots capture more information but increase parameters)
  - Number of memory modules (more modules capture different aspects but increase complexity)
  - Memory warm-up proportion (too little leads to poor initialization, too much leaves insufficient time for fine-tuning)

- Failure signatures:
  - Performance degradation when memory warm-up proportion is too low or too high
  - Overfitting when memory slot sizes are too large relative to training data
  - Gradient vanishing or explosion in memory reading/writing operations

- First 3 experiments:
  1. Ablation study: Remove memory reading module and compare performance to baseline JEREX
  2. Memory warm-up sweep: Test different warm-up proportions (0.2, 0.4, 0.6, 0.8) to find optimal initialization
  3. Memory size sensitivity: Vary memory slot sizes (32, 64, 128) to assess impact on entity and relation extraction performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the similarity-based memory module compare to other memory architectures, such as external memory networks or recurrent memory models, in joint entity and relation extraction?
- Basis in paper: [inferred] Paper introduces novel similarity-based memory module but doesn't compare to other memory architectures; suggests further development could involve different memory read vectors or distance-based scoring functions.
- Why unresolved: Paper focuses on demonstrating bidirectional memory dependency benefits without exploring alternative memory architectures.
- What evidence would resolve it: Direct comparison of similarity-based memory module with external memory networks or recurrent memory models on same benchmark datasets.

### Open Question 2
- Question: How does the performance of the similarity-based memory module scale with input document size and number of entity mentions and relations?
- Basis in paper: [inferred] Paper doesn't discuss scalability but mentions DocRED contains over 5000 human-annotated documents.
- Why unresolved: Paper focuses on specific dataset demonstration without exploring performance on larger or more complex datasets.
- What evidence would resolve it: Evaluating performance on larger and more complex datasets like full DocRED or other document-level relation extraction benchmarks.

### Open Question 3
- Question: How does the choice of similarity function (e.g., bilinear similarity) impact the performance of the similarity-based memory module?
- Basis in paper: [explicit] Uses bilinear similarity function; memory writing operation relies on similarity between category embeddings and instance representations.
- Why unresolved: Paper doesn't explore alternative similarity functions or their impact on module performance.
- What evidence would resolve it: Experimenting with different similarity functions (cosine similarity, dot product) and comparing performance on same benchmark datasets.

## Limitations
- Limited analysis of intermediate components like attention weights and memory representation quality
- Lack of direct ablation studies isolating memory module's specific contribution to performance gains
- Bilinear similarity choice not thoroughly evaluated against simpler alternatives

## Confidence
- **Mechanism 1 (Attention-based memory reading)**: Medium confidence - Attention mechanism is well-established but specific application lacks comprehensive evaluation
- **Mechanism 2 (Bidirectional feedback loop)**: Medium confidence - Conceptual framework is sound but empirical validation of feedback loop's contribution is limited
- **Mechanism 3 (Bilinear similarity for classification)**: Low confidence - Choice of bilinear similarity over simpler alternatives not justified through comparative analysis

## Next Checks
1. **Memory module ablation study**: Train model with and without memory reading module while keeping all other components constant, measuring specific contribution to entity and relation extraction F1-scores.

2. **Memory warm-up sensitivity analysis**: Systematically vary memory warm-up proportion (0.2, 0.4, 0.6, 0.8) and evaluate performance to determine optimal initialization and identify overfitting/underfitting patterns.

3. **Similarity function comparison**: Replace bilinear similarity with dot product and cosine similarity alternatives, comparing entity and relation extraction performance to quantify benefits of bilinear approach.