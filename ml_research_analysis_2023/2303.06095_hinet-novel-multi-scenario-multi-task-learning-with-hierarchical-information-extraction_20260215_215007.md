---
ver: rpa2
title: 'HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical Information
  Extraction'
arxiv_id: '2303.06095'
source_url: https://arxiv.org/abs/2303.06095
tags:
- scenarios
- scenario
- hinet
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes HiNet, a hierarchical information extraction
  network for multi-scenario & multi-task recommendation. The model introduces a two-layer
  framework: a scenario extraction layer that separates scenario-shared and scenario-specific
  information, and a task extraction layer that uses customized gate control to address
  negative transfer among tasks.'
---

# HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical Information Extraction

## Quick Facts
- arXiv ID: 2303.06095
- Source URL: https://arxiv.org/abs/2303.06095
- Reference count: 20
- Key outcome: Achieves 2.87% and 1.75% order quantity gains respectively in two deployed scenarios at Meituan

## Executive Summary
HiNet introduces a hierarchical information extraction network for multi-scenario & multi-task recommendation systems. The model employs a two-layer architecture: a scenario extraction layer that separates scenario-shared and scenario-specific information, followed by a task extraction layer with customized gate control to prevent negative transfer between tasks. A novel scenario-aware attentive network (SAN) explicitly models correlations between different recommendation scenarios. The system is deployed in two scenarios at Meituan, demonstrating significant improvements over state-of-the-art methods in both online and offline experiments.

## Method Summary
HiNet uses hierarchical information extraction with scenario extraction layer (scenario-shared/specific experts + SAN) and task extraction layer (CGC with task-shared/specific experts). The model is trained with cross-entropy loss weighted by dataset proportions across 6 industrial scenarios. It introduces two key innovations: scenario-aware attentive network for modeling cross-scenario correlations and customized gate control to prevent negative transfer between CTR and CTCVR tasks.

## Key Results
- Achieves 2.87% and 1.75% order quantity gains respectively in two deployed Meituan scenarios
- Significantly outperforms existing solutions across six industrial datasets in offline experiments
- Demonstrates effective hierarchical extraction based on coarse-to-fine knowledge transfer scheme

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical information extraction separates scenario-level and task-level feature learning, preventing negative transfer between tasks.
- Mechanism: Two-layer architecture (Scenario Extraction Layer → Task Extraction Layer) allows coarse-to-fine knowledge transfer. Scenario-shared and scenario-specific information is extracted first, then task-shared and task-specific information is processed separately using Customized Gate Control (CGC).
- Core assumption: Scenario-related and task-related information exist at different levels of granularity and should be processed hierarchically.
- Evidence anchors: [abstract] "achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme"; [section] "we employ the Customized Gate Control (CGC)... This module is mainly composed of two parts: task-shared expert networks and task-specific expert networks"
- Break condition: If scenario correlations are weak or task relationships are simple, the hierarchical structure may add unnecessary complexity without performance gains.

### Mechanism 2
- Claim: Scenario-aware Attentive Network (SAN) enhances representation learning by modeling cross-scenario correlations explicitly.
- Mechanism: SAN uses attention weights to measure the importance of information from other scenarios to the current scenario, allowing valuable cross-scenario information to contribute to representation learning.
- Core assumption: Different scenarios have varying degrees of contribution to each other based on user behavior patterns and item overlap.
- Evidence anchors: [abstract] "a novel scenario-aware attentive network module is proposed to model correlations between scenarios explicitly"; [section] "SAN contains two parts of input: the embedding vector Emb(si)... and S = [S1,··· ,S i−1,S i+1,··· ,S M ] corresponding to representation of scenarios"
- Break condition: If scenarios are completely independent with no overlapping users/items, the SAN module would add computation without benefit.

### Mechanism 3
- Claim: The combination of hierarchical extraction and SAN addresses both the multi-scenario transfer learning problem and the multi-task negative transfer problem simultaneously.
- Mechanism: By separating scenario-level and task-level extraction, the model can capture scenario correlations through SAN while preventing task interference through CGC, achieving improvements in both CTR and CTCVR across all scenarios.
- Core assumption: The model can effectively learn both scenario correlations and task distinctions without creating conflicting optimization objectives.
- Evidence anchors: [abstract] "significantly outperforms existing solutions" and "2.87% and 1.75% order quantity gain respectively"; [section] "HiNet can better capture and leverage the correlations among scenario and tasks with the introduction of modules such as SAN and CGC"
- Break condition: If the optimization objectives conflict significantly (e.g., CTR and CTCVR require completely different feature representations), the joint optimization may not converge effectively.

## Foundational Learning

- Concept: Multi-task learning and negative transfer
  - Why needed here: The paper addresses the seesaw phenomenon where improving one task metric hurts another, requiring understanding of how to prevent negative transfer
  - Quick check question: What is the "seesaw phenomenon" in multi-task learning and why does it occur?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: HiNet builds on MoE concepts but modifies them for hierarchical extraction, so understanding the original MoE is crucial
  - Quick check question: How does a standard MoE architecture differ from HiNet's hierarchical approach?

- Concept: Attention mechanisms and cross-domain knowledge transfer
  - Why needed here: SAN uses attention to model scenario correlations, requiring understanding of how attention weights capture inter-scenario relationships
  - Quick check question: How does the attention mechanism in SAN differ from standard self-attention in transformer models?

## Architecture Onboarding

- Component map: Input → Scenario Extraction (shared/specific experts + SAN) → Task Extraction (CGC) → Tower units → Predictions
- Critical path: Input layer (user profile, behavior features, item features, scenario indicator) → Scenario Extraction Layer (SEI modules, SAN) → Task Extraction Layer (CGC with task-shared/specific experts) → Tower units → Predictions
- Design tradeoffs:
  - Hierarchical vs. flat architecture: Hierarchical provides better separation but adds complexity
  - Number of experts: More experts increase capacity but also parameter count and training time
  - SAN vs. simple concatenation: SAN provides explicit correlation modeling but requires attention computation
- Failure signatures:
  - Poor performance on long-tail scenarios: May indicate insufficient scenario-specific expert capacity
  - Seesaw phenomenon persists: CGC may not be properly separating task-shared and task-specific information
  - Slow convergence: Too many experts or complex SAN attention may require more training data
- First 3 experiments:
  1. Baseline comparison: Implement HiNet and compare against Shared Bottom, MMoE, PLE, HMoE, and STAR on CTR/CTCVR metrics across all scenarios
  2. SAN ablation: Remove SAN module and measure performance drop to quantify its contribution
  3. Expert network sensitivity: Vary the number of sub-experts in scenario extraction and experts in task extraction to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HiNet's performance scale when applied to recommendation scenarios with significantly different data distributions or user behaviors compared to the Meituan Meishi platform?
- Basis in paper: [inferred] The paper focuses on Meituan Meishi platform data, but doesn't discuss performance in significantly different domains or platforms
- Why unresolved: The experiments are limited to a single e-commerce platform, lacking cross-domain validation
- What evidence would resolve it: Performance comparisons on diverse recommendation platforms (news, video streaming, etc.) with varying data distributions

### Open Question 2
- Question: What is the optimal number of scenarios that can be effectively modeled together in HiNet before performance degradation occurs due to excessive complexity?
- Basis in paper: [explicit] "As the number of recommendation scenarios increases..." but no upper bound or scalability limits are discussed
- Why unresolved: The paper only tests on 6 scenarios and doesn't explore scalability limits or performance degradation at scale
- What evidence would resolve it: Systematic experiments with varying numbers of scenarios (10+, 50+, 100+) measuring performance trade-offs

### Open Question 3
- Question: How does HiNet handle cold-start scenarios where new users or items appear in one scenario but not others?
- Basis in paper: [inferred] The paper doesn't discuss cold-start scenarios or how the model handles missing data across scenarios
- Why unresolved: No experiments or analysis on scenarios with sparse or missing user-item interactions across different scenarios
- What evidence would resolve it: Performance analysis on datasets with artificially introduced cold-start conditions and comparison with specialized cold-start methods

## Limitations
- Limited independent validation of hierarchical extraction effectiveness
- No scalability testing beyond 6 scenarios
- No cold-start scenario handling analysis

## Confidence
- **High confidence**: The general two-layer architecture design and the concept of separating scenario-level and task-level learning are well-grounded in multi-task learning literature
- **Medium confidence**: The specific implementation of SAN for scenario correlation modeling shows promise but lacks independent validation of its advantages over simpler approaches
- **Low confidence**: The claim that HiNet's hierarchical approach is uniquely effective for multi-scenario recommendation without negative transfer is not independently verified

## Next Checks
1. Implement an ablation study comparing HiNet with and without the SAN module across all six scenarios to quantify its specific contribution
2. Test HiNet's performance when scenarios have minimal user overlap to validate whether the SAN module adds value in truly independent scenarios
3. Compare HiNet against a simpler flat MoE architecture with equal parameter count to determine if the hierarchical structure provides measurable benefits