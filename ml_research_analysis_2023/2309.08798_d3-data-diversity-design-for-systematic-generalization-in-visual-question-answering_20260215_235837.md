---
ver: rpa2
title: 'D3: Data Diversity Design for Systematic Generalization in Visual Question
  Answering'
arxiv_id: '2309.08798'
source_url: https://arxiv.org/abs/2309.08798
tags:
- questions
- generalization
- training
- question
- systematic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how different aspects of data diversity
  in training sets affect systematic generalization in visual question answering (VQA).
  The authors introduce a data diversity design (D3) approach that incorporates diverse,
  simple questions into training to improve generalization to out-of-distribution
  (OOD) question types.
---

# D3: Data Diversity Design for Systematic Generalization in Visual Question Answering

## Quick Facts
- arXiv ID: 2309.08798
- Source URL: https://arxiv.org/abs/2309.08798
- Reference count: 40
- Key outcome: Data diversity design with diverse simple questions significantly improves systematic generalization in VQA across multiple architectures

## Executive Summary
This paper investigates how data diversity in training sets affects systematic generalization in visual question answering. The authors introduce Data Diversity Design (D3), a method that incorporates diverse, simple questions into training to improve generalization to out-of-distribution question types. Through controlled experiments on synthetic VQA datasets with compositional and length biases, the paper demonstrates that D3 significantly improves systematic generalization, with modular architectures showing particular robustness in low-data regimes.

## Method Summary
The authors generate controlled VQA datasets using the CLEVR repository with compositional and length biases. They apply D3 by replacing 30% of biased training questions with diverse simple questions (0-Hop type with single attributes). Three neural architectures are evaluated: MAC, FiLM (monolithic), and VectorNMN (modular). Models are trained on these diversified datasets and evaluated on out-of-distribution test sets measuring attribute composition and length generalization. The methodology systematically varies the diversity aspects in D3 to identify which combinations yield the best systematic generalization performance.

## Key Results
- D3 with diverse simple questions significantly improves systematic generalization across all evaluated architectures
- VectorNMN shows superior performance and robustness in low-data regimes compared to monolithic architectures
- Diversity in both question length and attribute composition simultaneously provides better generalization than diversity in only one aspect
- MAC achieves the best results with sufficient training data, while VectorNMN is more data-efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing diverse simple questions into training improves systematic generalization by exposing the model to a wider range of attribute combinations and question lengths, preventing overfitting to narrow patterns.
- Mechanism: Simple questions with single attributes and no spatial relations (0-Hop) provide broad attribute coverage without complex reasoning. This diversity allows models to learn robust representations of attributes and their relationships to question types, which transfers to novel combinations seen in OOD tests.
- Core assumption: The model can generalize compositional rules learned from simple questions to more complex scenarios involving multiple attributes or spatial relations.
- Evidence anchors:
  - [abstract] "the diversity of simple tasks (i.e. tasks formed by a few subtasks and concepts) plays a key role in achieving systematic generalization."
  - [section 3.1] "Using diverse simple questions of 0-Hop type with only a single attribute as D3 can significantly improve attribute composition generalization"
  - [corpus] Weak: corpus lacks direct evidence about simple-question diversity mechanisms; only mentions attribute diversity.
- Break condition: If the model relies heavily on co-occurrence patterns rather than compositional understanding, simple questions may not transfer effectively to complex OOD cases.

### Mechanism 2
- Claim: Modular architectures (VectorNMN) are more robust to distribution shifts in question complexity because their structure enforces compositional reasoning.
- Mechanism: VectorNMN's modular design separates reasoning steps into distinct modules (e.g., filter, relate, count). This structure helps the model generalize compositional rules learned from simple questions to more complex reasoning chains, even when attribute compositions differ from training.
- Core assumption: The modular structure enables the model to compose learned sub-tasks in novel ways, rather than memorizing fixed patterns.
- Evidence anchors:
  - [abstract] "neural module networks leverage all forms of data diversity we evaluated, while monolithic architectures require more extensive amounts of data to do so."
  - [section 3.2] "VectorNMN has superior performance and is more robust to distribution changes in low data regimes."
  - [corpus] Missing: corpus does not mention modular architectures or their advantages.
- Break condition: If the program generator fails to learn correct module compositions in the presence of biases, the modular advantage disappears.

### Mechanism 3
- Claim: Diversity in both length and attribute composition simultaneously provides better generalization than diversity in only one aspect.
- Mechanism: Questions requiring different reasoning depths (hops) and attribute combinations expose the model to a wider range of compositional patterns. This comprehensive diversity prevents the model from overfitting to specific attribute-length co-occurrences, enabling better transfer to unseen combinations.
- Core assumption: The model learns general compositional rules rather than memorizing specific attribute-length patterns.
- Evidence anchors:
  - [section 3.1] "We observe that all forms of D3 lead to improvements on the 0-Hop dataset" and "1-Hop Full outperforms the other D3 sets on the 3-Hop OOD test set, indicating superior generalization capabilities when full compositional diversity is available."
  - [corpus] Weak: corpus lacks evidence about length-attribute composition diversity interactions.
- Break condition: If the model overfits to particular attribute-length patterns, simultaneous diversity may not improve generalization.

## Foundational Learning

- Concept: Systematic generalization
  - Why needed here: The paper's central goal is to improve the model's ability to generalize to novel compositional tasks by combining known sub-tasks and concepts.
  - Quick check question: Can the model answer questions about attribute combinations it has never seen during training?

- Concept: Compositional reasoning
  - Why needed here: VQA requires reasoning about objects, attributes, and their relationships. The paper investigates how training data diversity affects the model's ability to compose these elements in novel ways.
  - Quick check question: Can the model answer questions requiring multiple reasoning steps (hops) about objects with attributes it has seen separately but not together?

- Concept: Data diversity design
  - Why needed here: The paper introduces D3 as a method for designing training data that improves systematic generalization. Understanding how different diversity aspects affect performance is crucial for applying this approach.
  - Quick check question: Does replacing 30% of training questions with simple, diverse questions improve OOD performance compared to training on biased data alone?

## Architecture Onboarding

- Component map:
  - CLEVR question generator -> D3 set selector -> Training pipeline -> Model (MAC/FiLM/VectorNMN) -> OOD evaluation

- Critical path:
  1. Generate CLEVR images and questions with controlled biases (attribute composition, length)
  2. Apply D3 by replacing 30% of biased training questions with diverse simple questions
  3. Train model on diversified dataset
  4. Evaluate on OOD test sets measuring attribute composition and length generalization
  5. Analyze results to identify which diversity aspects improve performance

- Design tradeoffs:
  - Simple vs. complex questions: Simple questions are easier to collect but may not cover complex reasoning patterns
  - Diversity proportion: 30% replacement found effective, but optimal proportion may vary by architecture and dataset
  - Module vs. monolithic: Modular architectures show better low-data performance but require program generator; monolithic need more data but are simpler to train

- Failure signatures:
  - No improvement on OOD tests despite D3 application: Model may be overfitting to attribute co-occurrences rather than learning compositional rules
  - Modular architecture performance similar to monolithic: Program generator may not be learning correct module compositions
  - Poor length generalization despite length diversity in D3: Model may be memorizing specific length-attribute patterns rather than learning general reasoning steps

- First 3 experiments:
  1. Train VectorNMN on 2-Hop A dataset (biased) and evaluate on 0-Hop test set to establish baseline performance
  2. Apply D3 with 1-Hop Full (30% replacement) and retrain VectorNMN on 2-Hop A + D3, evaluate on same test sets
  3. Repeat experiment 2 with MAC architecture to compare modular vs. monolithic performance on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of D3 vary across different VQA architectures beyond MAC and VectorNMN?
- Basis in paper: [explicit] The paper primarily evaluates D3 on MAC, FiLM, and VectorNMN architectures, noting that VectorNMN shows superior performance and robustness in low-data regimes, while MAC achieves the best results with sufficient data. The authors suggest that modular networks like VectorNMN are more data-efficient and robust to question complexity distribution variations.
- Why unresolved: The study does not explore a wide range of VQA architectures, such as transformer-based models or other neural module networks, which could provide insights into the generalizability of D3's effectiveness across different architectural designs.
- What evidence would resolve it: Comparative studies evaluating D3 on a broader spectrum of VQA architectures, including transformers and other modular networks, would clarify the generalizability of D3's effectiveness and identify which architectures benefit most from diverse training data.

### Open Question 2
- Question: What is the optimal proportion of simple to complex questions in D3 for maximizing systematic generalization?
- Basis in paper: [explicit] The paper experiments with different proportions of simple to complex questions in D3, finding that a 70% simple and 30% complex ratio yields promising results. However, the authors note that other proportions have been explored and suggest further investigation into the optimal balance.
- Why unresolved: The study provides initial findings on the effectiveness of different proportions but does not identify a definitive optimal ratio. The impact of varying proportions on systematic generalization performance remains unclear.
- What evidence would resolve it: Systematic experiments varying the proportions of simple to complex questions in D3 across different datasets and architectures would help determine the optimal balance for maximizing systematic generalization.

### Open Question 3
- Question: How does D3's impact on systematic generalization translate to real-world VQA datasets with more complex and diverse visual content?
- Basis in paper: [inferred] The paper's experiments are conducted on controlled datasets with simplified visual content, which may not fully capture the complexity and variability of real-world data. The authors acknowledge this limitation and suggest broader investigations to assess the transferability of observed effects to other domains and tasks.
- Why unresolved: The controlled nature of the datasets used in the study may not reflect the challenges posed by real-world VQA tasks, such as diverse visual content, complex relationships, and noisy data.
- What evidence would resolve it: Evaluating D3's effectiveness on real-world VQA datasets with complex and diverse visual content, such as COCO or VQA v2, would provide insights into its practical applicability and robustness to real-world challenges.

## Limitations
- Evaluation limited to synthetic CLEVR-based datasets, limiting generalizability to real-world VQA scenarios
- 30% D3 replacement ratio was empirically determined but may not be optimal across different architectures or domains
- Mechanistic explanations for why certain diversity aspects improve performance remain largely theoretical without direct experimental validation

## Confidence
- High confidence: VectorNMN architecture's superior performance in low-data regimes and the general benefit of D3 with simple questions
- Medium confidence: The specific advantages of different D3 diversity types (length-only vs. attribute-only vs. combined)
- Low confidence: The precise mechanisms by which D3 enables systematic generalization and the transferability of findings to non-synthetic datasets

## Next Checks
1. Apply D3 methodology to real-world VQA datasets (e.g., GQA, VQA-v2) to verify if synthetic dataset findings transfer to natural image-question distributions.

2. Systematically vary the percentage of simple questions in D3 (0% to 50% in 10% increments) to identify optimal diversity ratios for different architectures and dataset sizes.

3. Design targeted experiments to test whether models are learning compositional rules versus memorizing patterns, such as testing on novel attribute combinations that maintain compositional structure but differ from training distribution.