---
ver: rpa2
title: Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition
arxiv_id: '2312.03668'
source_url: https://arxiv.org/abs/2312.03668
tags:
- speech
- arxiv
- proposed
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes integrating pre-trained speech and language
  models for end-to-end automatic speech recognition (ASR). The approach uses a pre-trained
  HuBERT model to extract speech representations, which are then passed through a
  bridge network to convert them into the embedding space of a large language model
  (LLM) such as GPT.
---

# Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition

## Quick Facts
- **arXiv ID**: 2312.03668
- **Source URL**: https://arxiv.org/abs/2312.03668
- **Reference count**: 0
- **Key outcome**: Proposed model achieves comparable performance to modern E2E ASR models with 8.6% CER on Japanese test set

## Executive Summary
This paper presents a novel approach to end-to-end automatic speech recognition (ASR) by integrating pre-trained speech (HuBERT) and language models (GPT). The method uses a bridge network to convert speech representations into the LLM embedding space, enabling autoregressive text generation from speech input. Experiments demonstrate that the proposed model achieves performance comparable to modern E2E ASR systems while enabling parameter-efficient domain adaptation through LoRA fine-tuning of the LLM.

## Method Summary
The proposed method integrates a pre-trained HuBERT speech representation model with a GPT language model through a bridge network that compresses sequence lengths and aligns modalities. The HuBERT model extracts speech features from raw audio, which are then processed by the bridge network using either downsampling convolution or CTC compression to match the sequence length of text tokens. The resulting embeddings are fed into GPT, which generates text autoregressively. The model is trained using causal language modeling with CTC loss when applicable, and domain adaptation is achieved through LoRA parameter-efficient fine-tuning.

## Key Results
- Character error rates of 8.6% on Japanese test set, comparable to modern E2E ASR models
- Full parameter fine-tuning achieves lower CERs than freezing components, regardless of bridge network type
- Parameter-efficient LoRA fine-tuning enables effective domain adaptation with minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1
The bridge network successfully converts speech representations into the LLM embedding space by reducing sequence length and aligning modalities. It uses downsampling convolution or CTC compression to map HuBERT outputs (20ms shifted features) into the LLM embedding space, resolving the mismatch in sequence lengths between speech and text tokens. Core assumption: The bridge network can compress speech representations without losing critical acoustic information. Break condition: If compression is too aggressive or incorrect, critical speech information may be lost, leading to ASR errors that propagate through autoregressive generation.

### Mechanism 2
Full parameter fine-tuning of all components achieves better performance than freezing any component. Full fine-tuning allows all components to adapt jointly to the ASR task, enabling the LLM to learn to process speech information directly rather than relying on zero-shot generalization. Core assumption: Pre-trained models have sufficient capacity and initialization quality to adapt to ASR while maintaining core capabilities. Break condition: If pre-trained models are too large relative to available training data, fine-tuning may lead to overfitting or catastrophic forgetting.

### Mechanism 3
Parameter-efficient fine-tuning (LoRA) enables effective domain adaptation with minimal computational overhead. LoRA modifies self-attention parameters through low-rank updates, allowing adaptation to domain-specific speech patterns while keeping most parameters frozen. Core assumption: Low-rank adaptations are sufficient to capture domain-specific variations without full fine-tuning. Break condition: If the target domain is too different from the source domain, LoRA adaptations may be insufficient, requiring larger adaptation datasets or full fine-tuning.

## Foundational Learning

- **Concept**: Masked prediction objective in self-supervised learning
  - Why needed here: HuBERT is trained using this objective, allowing it to learn useful speech representations without labeled data
  - Quick check question: What is the primary difference between masked prediction in speech versus text, and how does this affect the quality of representations for ASR?

- **Concept**: Autoregressive generation and causal language modeling
  - Why needed here: The LLM generates text tokens sequentially based on previous tokens and speech prompts, requiring understanding of causal dependencies
  - Quick check question: How does the sequence length mismatch between speech representations and text tokens affect the autoregressive generation process, and what role does the bridge network play in resolving this?

- **Concept**: CTC compression and alignment
  - Why needed here: CTC compression is one method used in the bridge network to align speech and text sequences by removing blank predictions
  - Quick check question: What are the potential failure modes of CTC compression when applied to speech representations, and how might these manifest in the final ASR output?

## Architecture Onboarding

- **Component map**: Raw speech waveform → HuBERT → Bridge Network → LLM → Text Tokens
- **Critical path**: Waveform → HuBERT → Bridge Network → LLM → Text Tokens
- **Design tradeoffs**:
  - Full fine-tuning vs. freezing components: Full fine-tuning achieves better performance but requires more computational resources and risks overfitting
  - Bridge network type: Downsampling convolution is simpler but may lose information; CTC compression is more sophisticated but can introduce compression errors
  - Sequence length handling: Different approaches to compressing speech representations affect both computational efficiency and recognition accuracy
- **Failure signatures**:
  - High CER with specific domains (e.g., CSJ corpus with disfluencies): Indicates LLM pre-training on written text may not handle spontaneous speech well
  - Domain adaptation working in one direction but not another: Suggests bridge network or LLM may be over-specializing to adaptation domain
  - Training instability: May indicate convolutional encoder should remain frozen or that learning rates need adjustment
- **First 3 experiments**:
  1. Ablation study: Test freezing vs. fine-tuning each component (HuBERT, bridge network, GPT) to identify which components most benefit from adaptation
  2. Bridge network comparison: Compare downsampling convolution vs. CTC compression on held-out validation set to determine which method preserves more information
  3. Domain adaptation test: Apply LoRA to adapt model to new domain and measure performance on both target domain and out-of-domain data to assess robustness

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain based on the presented work:

1. How does the performance of the proposed model scale with larger speech corpora and language models?
2. How robust is the proposed model to domain shifts in speech data, such as accents or background noise?
3. What is the impact of using different pre-trained speech representation models on the proposed model's performance?
4. How does the proposed model's performance compare to state-of-the-art E2E ASR models on multilingual speech data?

## Limitations

- Lack of baseline comparisons with state-of-the-art E2E ASR models beyond ReazonSpeech model
- Absence of detailed architectural specifications for the bridge network and training hyperparameters
- Domain adaptation experiments only test one direction of adaptation (from ReazonSpeech to CSJ)

## Confidence

**High Confidence**: The mechanism of using a bridge network to compress speech representations for LLM processing is well-supported by the paper's description and follows established principles in multimodal integration. The claim that full fine-tuning outperforms freezing components is supported by empirical ablation studies presented in the paper.

**Medium Confidence**: The claim that the proposed model achieves "comparable performance to modern E2E ASR models" is supported by experimental results on multiple test sets, but the lack of comparisons to more recent state-of-the-art models and the absence of detailed training procedures reduce confidence in this claim.

**Low Confidence**: The assertion that parameter-efficient fine-tuning enables effective domain adaptation is described in the paper but lacks empirical validation or quantitative results demonstrating its effectiveness compared to other adaptation methods.

## Next Checks

1. **Bridge Network Architecture Validation**: Implement and test multiple bridge network variants (downsampling convolution vs. CTC compression) with varying compression ratios on a validation set to determine the optimal configuration that balances computational efficiency with information preservation.

2. **Cross-Domain Generalization Test**: Evaluate the adapted model's performance on both the target domain (CSJ) and multiple out-of-domain datasets to quantify domain adaptation effectiveness and measure potential performance degradation on non-adapted domains.

3. **State-of-the-Art Comparison**: Implement baseline comparisons with recent open-source E2E ASR models (e.g., Conformer, Whisper) on the same test sets to provide context for the proposed model's performance claims and identify areas for improvement.