---
ver: rpa2
title: 'Larth: Dataset and Machine Translation for Etruscan'
arxiv_id: '2310.05688'
source_url: https://arxiv.org/abs/2310.05688
tags:
- etruscan
- word
- translation
- only
- ciep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating resources for Etruscan,
  an ancient language with very limited data, by constructing a machine translation
  dataset from 2,891 translated inscriptions and proposing the Larth transformer model.
  The core method involves building a parallel Etruscan-English corpus from academic
  sources and augmenting it through name substitution and word damage, then benchmarking
  translation models including dictionary-based, n-gram, IBM, and transformer approaches.
---

# Larth: Dataset and Machine Translation for Etruscan

## Quick Facts
- arXiv ID: 2310.05688
- Source URL: https://arxiv.org/abs/2310.05688
- Reference count: 8
- Primary result: Larth achieves BLEU score of 10.1 on Etruscan-English translation

## Executive Summary
This paper tackles the challenge of machine translation for Etruscan, an ancient language with extremely limited data resources. The authors construct a parallel dataset of 2,891 translated inscriptions and propose the Larth transformer model that processes both character and word sequences aligned through token repetition. The work demonstrates that neural models can achieve meaningful translation performance even with very limited data when using hybrid character-word inputs and appropriate data augmentation strategies.

## Method Summary
The authors build a parallel Etruscan-English corpus from academic sources (ETP, CIEP, Zikh Rasna), then augment it through name substitution and word damage to increase effective training sample size. They propose Larth, a transformer model using BigBird sparse attention that takes both character and word sequences as input, aligned by repeating word tokens to match character sequence length. The model is compared against dictionary-based, n-gram, and IBM approaches using BLEU, chr-F, and TER metrics.

## Key Results
- Larth achieves BLEU score of 10.1 on the Etruscan-English translation task
- Transformer model outperforms simpler baselines (dictionary, n-gram, IBM) on both ETP and combined ETP+CIEP datasets
- Data augmentation via name substitution and word damage improves model robustness to partial inscriptions
- Hybrid character-word input representation proves more effective than single-modality approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeating word tokens aligns character and word sequence lengths, enabling the BigBird encoder to process both modalities in parallel.
- Mechanism: The transformer encoder expects fixed-length sequences for cross-attention. By repeating word tokens to match the length of the character sequence, the model creates a one-to-one correspondence between character and word embeddings, preserving positional context.
- Core assumption: The repeated word tokens do not dilute semantic meaning and that the model can learn to attend appropriately across both modalities.
- Evidence anchors:
  - [section]: "The character and word sequences are aligned so that they have the same length. To do so, we test two approaches: we either extend the word sequence by repeating the word tokens or by adding space tokens as shown in Figure 4."
  - [section]: "It takes both the characters and the words as input and concatenates their embeddings."
- Break condition: If the repeated tokens cause the model to overfit to redundancy or if the alignment masks true token boundaries, performance degrades.

### Mechanism 2
- Claim: Data augmentation via proper name substitution and word damage increases effective training sample size without altering translation validity.
- Mechanism: Proper names are replaced consistently in both source and target texts, preserving grammatical features. Word damage simulates incomplete inscriptions, forcing the model to learn robustness to partial inputs.
- Core assumption: Substitutions maintain semantic equivalence and grammatical correctness in both languages.
- Evidence anchors:
  - [section]: "The substitution is done simultaneously on the Etruscan and English texts in order to keep the translations correct."
  - [section]: "We generate more training samples by damaging more words... we assume the damage occurs at the beginning or end of the words with a set probability."
- Break condition: If substitutions introduce ambiguity or if damage exceeds recoverable token length, translation quality falls.

### Mechanism 3
- Claim: Transformer models can outperform statistical baselines even with very limited data when using both character and word sequences.
- Mechanism: The hybrid input provides richer contextual signals; character-level modeling captures morphology, while word-level modeling preserves semantic units. The BigBird sparse attention handles longer sequences efficiently.
- Core assumption: The dataset size is sufficient for the transformer to learn generalizable mappings despite scarcity.
- Evidence anchors:
  - [abstract]: "observing that it is possible to achieve a BLEU score of 10.1 with a small transformer model."
  - [section]: "Larth is able to achieve a better BLEU score than the previous models on both ETP and ETP+CIEP."
- Break condition: If the dataset is too small to prevent overfitting, BLEU scores drop below statistical baselines.

## Foundational Learning

- Concept: Sequence-to-sequence modeling with attention
  - Why needed here: The task requires mapping variable-length Etruscan inscriptions to English translations, which is inherently a sequence-to-sequence problem.
  - Quick check question: Can you describe how attention weights are computed between encoder and decoder states in a transformer?

- Concept: Tokenization and vocabulary building
  - Why needed here: Etruscan has limited resources; building a robust tokenizer that handles both characters and words is critical for model input.
  - Quick check question: What are the trade-offs between character-level, word-level, and subword tokenization in low-resource settings?

- Concept: Data augmentation for scarce datasets
  - Why needed here: With only ~2,900 parallel examples, augmentation is essential to prevent overfitting and improve generalization.
  - Quick check question: How would you design an augmentation strategy that preserves semantic and grammatical integrity across both languages?

## Architecture Onboarding

- Component map: Character embedding → Word embedding → Concatenation → BigBird encoder → Decoder with self- and cross-attention → Output projection
- Critical path: Input → Encoder (char+word) → Decoder (self-attention on translation + cross-attention to encoder) → Output logits
- Design tradeoffs: Repeating word tokens increases sequence length and memory usage but aligns modalities; using only words sacrifices morphological nuance; using only characters loses semantic clarity.
- Failure signatures: BLEU scores near zero or below random baseline; high variance across runs; model predicts only EOS token; overfitting (training BLEU >> test BLEU).
- First 3 experiments:
  1. Train with only word tokens (no character input) to confirm hybrid input advantage.
  2. Use space tokens instead of repeating word tokens to test alignment impact.
  3. Apply data augmentation with name substitution only (no damage) to isolate augmentation effects.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific bibliographic information or metadata would be most valuable to include in future versions of the Etruscan dataset to improve translation accuracy and interpretability?
  - Basis in paper: [explicit] The paper mentions that the dataset lacks bibliographic information and the reasoning behind original translations, suggesting these as future improvements.
  - Why unresolved: The paper does not specify which types of metadata (e.g., dating, provenance, scholarly annotations) would have the most impact on model performance.
  - What evidence would resolve it: Comparative experiments showing translation quality improvements when different metadata fields are added to the training data.

- **Open Question 2**: Why does data augmentation by replacing proper names decrease performance on the smaller ETP dataset but improve it on the larger ETP+CIEP dataset?
  - Basis in paper: [explicit] The paper notes this contradictory effect but does not explain the underlying cause.
  - Why unresolved: The authors observe the phenomenon but do not investigate whether it's due to noise in CIEP, vocabulary differences, or other factors.
  - What evidence would resolve it: Controlled experiments varying augmentation parameters and dataset composition to isolate the cause of the performance difference.

- **Open Question 3**: How do the transformer model's word-piece tokenization decisions affect translation quality for unknown Etruscan tokens compared to dictionary-based approaches?
  - Basis in paper: [explicit] The paper contrasts Larth's word-piece approach with dictionary models' exact-match strategy, noting differences in chr-F and TER scores.
  - Why unresolved: The paper does not analyze which types of unknown tokens (morphological variants, proper names, etc.) are most problematic for each approach.
  - What evidence would resolve it: Error analysis categorizing mistranslations by token type and comparing performance across tokenization strategies.

## Limitations
- Dataset size remains extremely small for neural machine translation, with only 2,891 parallel examples
- Quality of CIEP portion of the dataset is noted as problematic, though exact nature of noise is not quantified
- Evaluation relies solely on BLEU and chr-F metrics without human evaluation, which is concerning for an ancient language translation task

## Confidence

- **High Confidence**: The claim that Larth achieves better BLEU scores than baseline models (dictionary, n-gram, IBM) is supported by the experimental results showing Larth achieving 10.1 BLEU compared to lower scores for baselines.
- **Medium Confidence**: The assertion that this demonstrates neural models can learn from scarce data is plausible but requires caution. The improvement over baselines is modest, and the dataset remains too small to draw strong conclusions about scalability.
- **Low Confidence**: Claims about the practical utility of the model for actual Etruscan translation or scholarly work are not substantiated. The BLEU score of 10.1, while better than baselines, is quite low for production use.

## Next Checks

1. **Dataset Quality Assessment**: Conduct a systematic evaluation of the CIEP portion's quality by having classical scholars manually review a stratified sample of 100-200 translation pairs to quantify noise levels and identify systematic errors.

2. **Ablation Study on Tokenization**: Systematically test different tokenization strategies (pure character, pure word, subword with different algorithms) while keeping all other variables constant to isolate whether the claimed advantage of the character+word hybrid approach is robust.

3. **Human Evaluation of Translation Quality**: Recruit classical linguists to perform blinded evaluation of model outputs versus baseline outputs using standardized quality criteria (adequacy, fluency, faithfulness to original meaning) to validate whether BLEU improvements correspond to meaningful quality gains.