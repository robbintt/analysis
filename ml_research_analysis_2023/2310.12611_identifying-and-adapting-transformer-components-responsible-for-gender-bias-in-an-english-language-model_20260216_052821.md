---
ver: rpa2
title: Identifying and Adapting Transformer-Components Responsible for Gender Bias
  in an English Language Model
arxiv_id: '2310.12611'
source_url: https://arxiv.org/abs/2310.12611
tags:
- bias
- heads
- gender
- language
- attn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of identifying and mitigating
  gender bias in language models, specifically GPT-2 small. The authors employ three
  methods - causal mediation analysis, automated circuit discovery, and their novel
  approach called DiffMask+ - to identify components responsible for gender bias.
---

# Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model

## Quick Facts
- arXiv ID: 2310.12611
- Source URL: https://arxiv.org/abs/2310.12611
- Authors: 
- Reference count: 28
- Key outcome: Successfully identifies and fine-tunes attention heads to reduce gender bias in GPT-2 small while maintaining general language modeling performance.

## Executive Summary
This paper addresses the challenge of identifying and mitigating gender bias in language models by developing three methods to pinpoint transformer components responsible for biased predictions. The authors focus on GPT-2 small and employ causal mediation analysis, automated circuit discovery, and their novel DiffMask+ approach to identify attention heads that transfer gender information from profession tokens to pronoun predictions. They then fine-tune only these identified components on a balanced dataset, achieving significant bias reduction with less damage to general language modeling compared to full-model fine-tuning.

## Method Summary
The authors use three methods to identify gender-biased components in GPT-2 small: causal mediation analysis (CMA) measures indirect effects of components on gendered pronoun predictions; automated circuit discovery (ACDC) traces paths of gender information; and DiffMask+ learns a differentiable mask over components to find those whose ablation maximally changes gender-biased predictions. These methods are applied to a Professions dataset containing stereotypical and anti-stereotypical sentences. The identified attention heads are then fine-tuned on a balanced dataset (BUG) while keeping other parameters frozen, and the resulting models are evaluated on bias benchmarks (CrowS-Pairs, WinoBias) and general language modeling tasks (WikiText-103, BLiMP).

## Key Results
- The three methods (CMA, ACDC, DiffMask+) show significant overlap in the attention heads they identify as responsible for gender bias, suggesting robustness in component identification.
- Fine-tuning only the identified attention heads achieves better bias reduction than full-model fine-tuning while preserving general language modeling performance, as measured by perplexity on WikiText-103 and accuracy on BLiMP.
- The effectiveness of component identification methods is highly sensitive to the choice of dataset, with different datasets yielding different sets of identified components and varying levels of bias mitigation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal mediation analysis identifies attention heads that transfer gender information from profession tokens to the next-token prediction position.
- Mechanism: By swapping activations between stereotypical and anti-stereotypical inputs, the method measures how much each component's output change affects the gendered pronoun prediction, isolating the causal influence.
- Core assumption: Gender bias is introduced via a small set of components whose outputs can be intervened to change model behavior.
- Evidence anchors:
  - [abstract] "We study three methods for identifying causal relations between LM components and particular output"
  - [section 3.1.1] "Vig et al. (2020) measure how important a component i is to a model behaviour b using Natural Indirect Effect (NIE)"
  - [corpus] Weak - no direct citation of causal success, but corpus includes "Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing" which supports the premise of component-level intervention.
- Break condition: If gender bias is distributed across many components or emerges from non-linear interactions, swapping single component activations may not capture the full causal effect.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning of identified components reduces bias while preserving general language modeling performance.
- Mechanism: Fine-tuning only the subset of components found to cause bias updates their parameters to favor anti-stereotypical outputs on bias benchmarks, without affecting the rest of the model's parameters and thus its general capabilities.
- Core assumption: Gender bias is localized enough that fine-tuning only the responsible components suffices to mitigate it.
- Evidence anchors:
  - [abstract] "use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation"
  - [section 4.3] "models where only the attention heads discovered using the three methods from Section 3 were fine-tuned, achieve the best results in terms of gender bias reduction"
  - [corpus] Weak - corpus mentions "Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs" but lacks quantitative results from this paper.
- Break condition: If bias is encoded redundantly across many components, fine-tuning only a subset may leave residual bias intact.

### Mechanism 3
- Claim: DiffMask+ combines causal analysis with differentiable masking to efficiently find a minimal set of components responsible for bias.
- Mechanism: Instead of evaluating all possible component subsets combinatorially, DiffMask+ learns a differentiable mask over components, selecting those whose ablation maximally changes gender-biased predictions while minimally affecting overall output.
- Core assumption: The distribution of gender bias can be captured by a sparse, generalizable mask learned over the dataset.
- Evidence anchors:
  - [section 3.1.3] "We adapt DiffMask in two ways... We train the mask to induce the largest change in gendered pronoun prediction possible, while minimizing both the number of non-zero mask entries, and the magnitude of overall changes made to the model's output distribution"
  - [section 4.3] "DM Attention Heads model in particular significantly reduces bias... while fine-tuning all attention layers... yields high-variance results"
  - [corpus] Weak - no direct evidence from corpus about DiffMask+ efficacy, but related work on "Locating and Mitigating Gender Bias in Large Language Models" suggests component-level mitigation is plausible.
- Break condition: If the bias is not sparsely representable or the mask fails to generalize across different bias manifestations, the method will not find the right components.

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: Provides a principled way to identify which model components causally influence gender bias rather than just correlating with it.
  - Quick check question: If swapping a component's activation between two inputs changes the model's gendered pronoun prediction, what does that tell us about that component's role?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Allows targeted mitigation of bias without the computational cost and potential performance degradation of full-model fine-tuning.
  - Quick check question: Why might fine-tuning only the components found to cause bias be preferable to fine-tuning the entire model?

- Concept: Differentiable masking for subset selection
  - Why needed here: Avoids combinatorial search over all possible component subsets by learning a continuous mask that can be optimized efficiently.
  - Quick check question: How does learning a mask over components differ from evaluating each component individually for importance?

## Architecture Onboarding

- Component map: GPT-2 small has 12 layers, each with 12 attention heads and 1 MLP. The bias mitigation targets only attention heads, particularly those in later layers.
- Critical path: (1) Generate stereotypical/anti-stereotypical sentence pairs; (2) Apply causal analysis methods (CMA, ACDC, DiffMask+) to find important attention heads; (3) Fine-tune only those heads on a gender-balanced dataset; (4) Evaluate bias reduction and performance retention.
- Design tradeoffs: Targeting only attention heads (not MLPs) simplifies the problem but may miss bias encoded in MLPs; focusing on later layers assumes bias is transferred late, but earlier layers might also contribute.
- Failure signatures: If bias metrics show inconsistent improvement across datasets (e.g., good on CrowS-Pairs but not WinoBias), the component set may not generalize; if perplexity increases significantly, fine-tuning may be harming general language modeling.
- First 3 experiments:
  1. Run CMA on the Professions dataset and record the top-10 attention heads by NIE; verify they are mostly in later layers.
  2. Apply DiffMask+ to the same dataset and compare the selected heads to CMA; check overlap and sparsity.
  3. Fine-tune GPT-2 small using only the CMA-identified heads on the BUG dataset; evaluate on CrowS-Pairs and WinoBias to measure bias reduction and BLiMP to check for performance degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different operationalizations of gender bias (e.g., minimal pairs vs. linguistic phenomena like co-reference resolution) affect the identification and mitigation of bias in language models?
- Basis in paper: [explicit] The paper discusses how different bias metrics (CrowS-Pairs, WinoBias, Professions dataset) yield inconsistent results, attributing this to the implicit vs. explicit gender bias in different datasets.
- Why unresolved: The paper highlights the inconsistency but does not provide a definitive answer on which operationalization is more effective or representative of real-world gender bias.
- What evidence would resolve it: Comparative studies using a wide range of bias operationalizations on diverse datasets, measuring their effectiveness in identifying and mitigating bias in various contexts.

### Open Question 2
- Question: How generalizable are the identified components responsible for gender bias across different language models and languages?
- Basis in paper: [inferred] The paper focuses on GPT-2 small and English, but mentions the need for future work to test whether mitigation strategies generalize to larger models and other languages.
- Why unresolved: The study is limited to one model and language, so the generalizability of the findings to other contexts is unknown.
- What evidence would resolve it: Replicating the study on multiple language models (e.g., GPT-3, BERT) and languages, comparing the identified components and their effectiveness in bias mitigation.

### Open Question 3
- Question: What is the optimal balance between computational efficiency and effectiveness in identifying and mitigating bias in language models?
- Basis in paper: [explicit] The paper compares three methods (CMA, ACDC, DiffMask+) with different computational requirements and effectiveness, highlighting the trade-off between efficiency and results.
- Why unresolved: The paper does not provide a clear answer on which method is optimal for different scenarios, considering factors like model size, available resources, and desired level of bias mitigation.
- What evidence would resolve it: Systematic evaluation of different methods on various models and datasets, measuring their computational cost and effectiveness in bias identification and mitigation.

## Limitations

- The identified components may not capture all sources of gender bias, as the methods focus on attention heads and may miss bias encoded in MLPs or earlier layers.
- The effectiveness of component identification is highly dependent on the choice of dataset, with different datasets yielding different sets of identified components and varying levels of bias mitigation.
- The paper does not address potential trade-offs between reducing bias on one metric while potentially introducing it on another, as different bias benchmarks may capture different dimensions of gender bias.

## Confidence

- **High Confidence**: The finding that fine-tuning identified components reduces gender bias more effectively than full-model fine-tuning, as this is supported by direct quantitative comparisons.
- **Medium Confidence**: The claim that causal mediation analysis successfully identifies components responsible for bias, as this relies on the assumption that swapping activations captures true causal influence.
- **Low Confidence**: The assertion that DiffMask+ is a superior method for component identification compared to CMA and ACDC, due to limited comparative analysis and lack of ablation studies on DiffMask+ hyperparameters.

## Next Checks

1. **Cross-Dataset Generalization**: Apply the DiffMask+ method to a different bias dataset (e.g., StereoSet or GAP) and verify whether the same components are identified or if dataset-specific components dominate.

2. **Bias Metric Expansion**: Evaluate the fine-tuned models on additional bias benchmarks (e.g., BBQ or Winogender) not used in the original study to ensure the mitigation strategy does not introduce bias in dimensions not previously measured.

3. **Component Ablation Study**: Systematically ablate individual attention heads identified by each method (CMA, ACDC, DiffMask+) and measure the change in bias scores to determine if the identified components are individually sufficient or if their combined effect is necessary for bias reduction.