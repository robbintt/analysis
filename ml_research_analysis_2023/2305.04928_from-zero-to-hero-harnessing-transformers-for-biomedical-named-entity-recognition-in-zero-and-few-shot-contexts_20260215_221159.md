---
ver: rpa2
title: 'From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition
  in Zero- and Few-shot Contexts'
arxiv_id: '2305.04928'
source_url: https://arxiv.org/abs/2305.04928
tags:
- biomedical
- named
- entity
- class
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of biomedical named entity recognition
  (NER) when labeled training data is scarce. The proposed method transforms the multi-class
  NER problem into a binary classification task, where the model learns to identify
  whether a token contains a specific named entity or not.
---

# From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts

## Quick Facts
- arXiv ID: 2305.04928
- Source URL: https://arxiv.org/abs/2305.04928
- Authors: 
- Reference count: 18
- Key outcome: Achieves average F1 scores of 35.44% (zero-shot), 50.10% (one-shot), 69.94% (10-shot), and 79.51% (100-shot) on biomedical NER using models with 1000x fewer parameters than GPT3-based approaches

## Executive Summary
This paper addresses the challenge of biomedical named entity recognition (NER) when labeled training data is scarce. The authors propose transforming the traditional multi-class NER problem into a binary classification task, where the model learns to identify whether a token contains a specific named entity or not. By pre-training on a large corpus of biomedical data with diverse named entity classes, the model learns semantic relationships between entities, enabling it to recognize new, unseen biomedical entities with no or limited examples. The method demonstrates strong performance across zero- and few-shot scenarios, achieving results comparable to much larger transformer models.

## Method Summary
The method transforms multi-class token classification into binary token classification by adding linear and softmax layers on top of a pre-trained BioBERT or PubMedBERT model. The model classifies each token as containing or not containing a searched named entity. For training, the input consists of two segments: the first contains the entity label, and the second contains the sentence to be labeled. This two-segment format allows self-attention layers to learn dependencies between the label and sentence tokens. The model is pre-trained on a large corpus of biomedical data with diverse named entity classes, enabling it to learn semantic relationships between entities. During inference, the label is placed in the first segment, and the model identifies tokens matching that entity type in the second segment.

## Key Results
- Average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on nine evaluated biomedical entities
- Outperforms previous transformer-based methods in zero- and few-shot settings
- Achieves comparable performance to GPT3-based models while using models with over 1000 times fewer parameters
- Demonstrates effectiveness across diverse biomedical entity types including chemicals, diseases, and genes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming multi-class NER into binary classification enables the model to learn entity boundaries without being overwhelmed by class distinctions.
- Mechanism: By framing the task as "does this token contain entity X or not," the model learns to detect entity spans. During zero-shot inference, the label name is placed in the first segment of the input, and the model identifies tokens matching that entity type in the second segment.
- Core assumption: The semantic content of the entity label itself provides sufficient signal for the model to generalize to unseen entities.
- Evidence anchors:
  - [abstract] "The method is based on transforming the task of multi-class token classification into binary token classification..."
  - [section 3.4] "One linear and one softmax layer for token classification are added on top of the base model. The model classifies each token as containing or not containing the searched named entity."

### Mechanism 2
- Claim: Pre-training on a large corpus of diverse biomedical entities enables the model to learn semantic relationships between entity types, improving zero-shot generalization.
- Mechanism: The model is first trained on many biomedical entity classes. This exposes it to semantic patterns (e.g., drugs are a subset of chemicals) that allow it to infer the nature of new entities based on similarity to known ones.
- Core assumption: Semantic similarity between seen and unseen entity classes is sufficient for the model to transfer learned patterns.
- Evidence anchors:
  - [abstract] "...pre-training on a large corpus of biomedical data with diverse named entity classes, allowing it to learn semantic relationships between entities."
  - [section 3.2] "Training the model on a large number of named entity classes will enable the model to deduce semantic relationships between these classes."

### Mechanism 3
- Claim: The two-segment input format (label + sentence) enables the model to condition its predictions on the desired entity type, facilitating zero-shot inference.
- Mechanism: The label segment primes the model's attention to look for that specific entity in the sentence segment. This is similar to how cross-encoders operate in sentence similarity tasks.
- Core assumption: The model's self-attention layers can effectively learn dependencies between the label and sentence segments.
- Evidence anchors:
  - [section 3.4] "Self-attention layers are then able to learn dependencies and relations of the tokens from the first segment (label) to all the tokens from the second segment (sentence to be labeled)."
  - [section 3.4] "We hypothesize that our model will learn the desired graph connections between different segments through the zero-shot fine-tuning process."

## Foundational Learning

- Concept: Binary classification vs multi-class classification
  - Why needed here: The transformation from multi-class to binary classification simplifies the learning task and enables zero-shot generalization by focusing on entity boundaries rather than class distinctions.
  - Quick check question: Why is binary classification more suitable for zero-shot learning than multi-class classification in this context?

- Concept: Semantic similarity and transfer learning
  - Why needed here: The model relies on semantic similarity between seen and unseen entity classes to generalize. Understanding hypernymy/hyponymy relationships is crucial for predicting zero-shot performance.
  - Quick check question: How does the model's performance differ between semantically similar and unrelated unseen classes?

- Concept: Self-attention and cross-segment conditioning
  - Why needed here: The model's ability to learn dependencies between the label and sentence segments is fundamental to its zero-shot capability. Understanding how self-attention operates in this context is key to debugging and improving the architecture.
  - Quick check question: What happens to the model's performance if the label segment is removed from the input?

## Architecture Onboarding

- Component map:
  Tokenizer (PubMedBERT/BioBERT) -> Transformer encoder (pre-trained BioBERT/PubMedBERT) -> Linear layer + softmax for binary token classification -> Two-segment input: [CLS] label [SEP] sentence [SEP]

- Critical path:
  1. Tokenize input (label + sentence)
  2. Encode through transformer
  3. Apply linear layer + softmax to each token
  4. Output binary predictions (entity or not)

- Design tradeoffs:
  - Binary vs multi-class classification: Binary is simpler and enables zero-shot learning, but may be less precise for complex entity hierarchies.
  - Label encoding (0 vs 1): The choice of whether to encode the label itself as an entity token can affect model performance.
  - Pre-training corpus size: Larger, more diverse corpora improve semantic generalization but increase computational cost.

- Failure signatures:
  - Poor zero-shot performance: Likely due to insufficient semantic similarity between seen and unseen classes, or inadequate pre-training corpus diversity.
  - Unstable zero-shot results: May indicate the model is getting stuck in local minima during optimization.
  - Overfitting to seen classes: Model performs well on seen classes but poorly on unseen ones, suggesting insufficient generalization.

- First 3 experiments:
  1. Ablation study: Remove the label segment from the input and measure impact on zero-shot performance.
  2. Corpus diversity test: Train on a smaller, less diverse corpus and compare zero-shot performance on semantically distant entities.
  3. Label encoding test: Train two models, one with label encoded as 1 and one as 0, and compare zero-shot performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective validation strategy for selecting a zero-shot model when the unseen classes are unknown?
- Basis in paper: [explicit] The paper discusses several validation strategies for zero-shot model selection, including using validation sets with all classes, seen classes only, unseen classes only, a few unseen classes, or manual inspection.
- Why unresolved: The paper concludes that using an unseen class in validation may lead to better model selection for zero-shot NER, but this approach has questionable practicality since users do not know what the unseen classes will be during deployment.
- What evidence would resolve it: Empirical comparisons of different validation strategies on a wide range of biomedical NER tasks, measuring their effectiveness in selecting models that generalize well to truly unseen classes.

### Open Question 2
- Question: What is the optimal method to improve the stability of zero-shot results across different training runs?
- Basis in paper: [explicit] The paper observes that zero-shot NER results are unstable across different training runs, with F1 scores ranging from 0.96% to 44.66% for the Drug class in four runs on the same data. The authors hypothesize this is due to gradient descent getting stuck in local minima.
- Why unresolved: While the authors suggest potential solutions like adjusting learning rate, adding regularization, or using momentum, they do not provide a definitive solution to this issue.
- What evidence would resolve it: Systematic experiments testing various optimization techniques (e.g., different learning rate schedules, regularization methods, momentum) to identify the most effective approach for improving zero-shot result stability.

### Open Question 3
- Question: How can active learning approaches be integrated with the proposed method to further improve performance?
- Basis in paper: [explicit] The authors propose investigating active learning approaches to improve the method's performance further, suggesting that the model could learn to identify difficult examples and flag them for further review.
- Why unresolved: The paper does not explore or implement active learning techniques with the proposed method.
- What evidence would resolve it: Implementation and evaluation of active learning strategies (e.g., uncertainty sampling, query-by-committee) integrated with the proposed method, demonstrating improvements in performance and efficiency compared to passive learning approaches.

## Limitations
- High variance in zero-shot results (SD of 20.82 for 10-shot F1) suggests potential instability in the approach
- Only nine of 26 entity classes were evaluated in zero-shot settings, limiting generalizability
- No per-class performance breakdowns provided, making it difficult to assess which entity types the model handles well versus poorly

## Confidence
- High confidence in the core architectural contribution: The transformation of multi-class to binary classification is clearly described and technically sound. The experimental setup for comparing few-shot scenarios (1-shot, 10-shot, 100-shot) is well-defined and the results are reproducible.
- Medium confidence in the zero-shot performance claims: While the 35.44% average F1 score is reported, the high variance and lack of per-class analysis make it difficult to determine if this represents robust generalization or cherry-picked results from favorable entity classes.
- Low confidence in the semantic relationship claims: The paper asserts that pre-training on diverse entities enables learning of semantic relationships, but provides no quantitative analysis of semantic similarity between seen and unseen classes or how this correlates with performance.

## Next Checks
1. **Per-class zero-shot performance analysis**: Re-run the zero-shot experiments and report F1 scores for each of the nine evaluated entity classes individually, along with their semantic similarity to the training set entities. This would reveal whether performance is consistent across semantically related classes or if certain entity types consistently underperform.

2. **Ablation study on corpus diversity**: Train two versions of the model - one on the full diverse corpus and one on a corpus containing only a single biomedical domain (e.g., only chemicals). Compare zero-shot performance on entities outside the training domain to quantify how corpus diversity impacts generalization.

3. **Attention visualization for cross-segment dependencies**: Use attention visualization techniques to examine whether the model actually learns meaningful dependencies between the label segment and sentence segment. This would validate whether the hypothesized mechanism for zero-shot learning is actually occurring or if the model is finding alternative patterns.