---
ver: rpa2
title: How well can machine-generated texts be identified and can language models
  be trained to avoid identification?
arxiv_id: '2310.16992'
source_url: https://arxiv.org/abs/2310.16992
tags:
- texts
- learning
- detection
- language
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of machine-generated text
  detection methods and explores the potential of bypassing such detectors using reinforcement
  learning. Five different language models were fine-tuned to generate synthetic tweets,
  and their detectability was tested using shallow learning classifiers like Naive
  Bayes and advanced models like BERT.
---

# How well can machine-generated texts be identified and can language models be trained to avoid identification?

## Quick Facts
- arXiv ID: 2310.16992
- Source URL: https://arxiv.org/abs/2310.16992
- Reference count: 15
- Primary result: BERT-based detectors achieve >0.9 accuracy; RL fine-tuning can reduce this to 0.15 or less

## Executive Summary
This study investigates the reliability of machine-generated text detection methods and explores the potential of bypassing such detectors using reinforcement learning. Five different language models were fine-tuned to generate synthetic tweets, and their detectability was tested using shallow learning classifiers like Naive Bayes and advanced models like BERT. Shallow classifiers achieved moderate detection accuracy (0.6-0.8) but struggled with higher temperature values, while BERT achieved accuracy above 0.9. By employing reinforcement learning, the study successfully fine-tuned language models to evade BERT-based classifiers, reducing detection accuracy to 0.15 or less. The findings highlight the limitations of current detection methods and the need for further research to address evolving challenges in identifying machine-generated text.

## Method Summary
The authors collected 2 million tweets from 136,450 verified accounts, filtering out spam and advertisements. Five GPT variants (GPT-2, GPT-J-6B, GPT-Neo-125M/1.3B/2.7B, OPT-125M/350M/1.3B/2.7B) were fine-tuned on this tweet corpus. Synthetic tweets were generated using various temperature and sampling strategies. Detection models included Naive Bayes with bag-of-words features and BERT. To evade detection, reinforcement learning was applied to the generators, using the BERT classifier's output as a reward signal while enforcing linguistic acceptability constraints (limiting special characters, repetitions, and ensuring dictionary coverage).

## Key Results
- BERT-based classifiers achieved detection accuracy above 0.9
- Shallow classifiers (Naive Bayes) achieved 0.6-0.8 accuracy, especially with higher temperature values
- Reinforcement learning successfully reduced BERT detection accuracy to 0.15 or less
- The RL-evaded model maintained linguistic acceptability within defined constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based classifiers achieve detection accuracy above 0.9 due to self-attention layers that encode contextual information better than BoW or shallow features.
- Mechanism: BERT's self-attention transforms tokens into context-aware embeddings, allowing it to capture subtle statistical patterns and syntactic cues that distinguish human and machine-generated text.
- Core assumption: The contextual information encoded by BERT is sufficient to differentiate machine-generated text from human-written text.
- Evidence anchors:
  - [abstract]: "transformer-based classifiers have an accuracy of 0.9 and above"
  - [section 3.4]: "These layers allow the classifier to translate a token into vector space based on the surrounding tokens"
- Break condition: If generated text becomes too similar to human-written text in both statistical and contextual features, or if the classifier overfits to a narrow generation style, detection accuracy will drop.

### Mechanism 2
- Claim: Reinforcement learning with detector feedback as reward can evade BERT-based classifiers by optimizing for low detection probability while maintaining linguistic acceptability.
- Mechanism: The generator is fine-tuned using RL to maximize the reward signal from the BERT classifier (negative when detected, positive when not), subject to constraints that preserve text quality (e.g., special character limits, repetitions, acceptability thresholds).
- Core assumption: The detector can be gamed by tuning the generator to produce text that the detector misclassifies as human, while still being linguistically coherent.
- Evidence anchors:
  - [abstract]: "We found that using a reinforcement learning approach to refine our generative models can successfully evade BERT-based classifiers with a detection accuracy of 0.15 or less"
  - [section 3.5]: Describes the reward calculation and optimization process
- Break condition: If the detector is retrained on the RL-evaded outputs, or if linguistic constraints are too loose, the model may produce gibberish or be detected by other methods.

### Mechanism 3
- Claim: Shallow learning classifiers (e.g., Naive Bayes with BoW) achieve only 0.6-0.8 accuracy because they rely on simple statistical features that can be easily manipulated by changing generation parameters like temperature.
- Mechanism: BoW features capture word frequency distributions, but these are sensitive to sampling strategy and temperature settings, making it easier for generators to produce text that evades detection by shallow methods.
- Core assumption: The shallow features used by these classifiers are not robust to the variations in text generation strategies.
- Evidence anchors:
  - [abstract]: "shallow learning classification algorithms, like Naive Bayes, achieve detection accuracy between 0.6 and 0.8"
  - [section 4.1]: "Word Distributions" and "Sampling Schemes" experiments show how temperature and sampling methods affect shallow detector performance
- Break condition: If the shallow classifier is trained on a diverse set of generation parameters or uses more robust features, its accuracy may improve.

## Foundational Learning

- Concept: Reinforcement Learning with Human Feedback (RLHF)
  - Why needed here: The core of the evasion technique is using the detector's output as a reward signal to fine-tune the generator, similar to RLHF but with the detector as the "human" evaluator.
  - Quick check question: How does the reward signal guide the generator to produce text that evades the detector while maintaining quality?

- Concept: Transformer-based Text Classification
  - Why needed here: BERT's architecture and how self-attention enables better detection of machine-generated text compared to shallow methods.
  - Quick check question: What features does BERT use that shallow classifiers lack, and how do these help in detection?

- Concept: Sampling Strategies in Text Generation
  - Why needed here: Different sampling methods (greedy, top-k, nucleus, typical) produce text with different statistical properties, affecting detectability.
  - Quick check question: How do changes in temperature and sampling strategy alter the probability distribution of generated text?

## Architecture Onboarding

- Component map: Generator models (GPT variants) -> Detector models (BERT, Naive Bayes) -> Reinforcement Learning loop (reward calculation, optimization) -> Linguistic acceptability filters -> Data pipeline (tweet collection, filtering, splitting)

- Critical path:
  1. Fine-tune generator on tweet corpus
  2. Generate fake tweets with various sampling strategies
  3. Train detector models (BERT and shallow) on real vs fake
  4. Apply RL to generator using detector as reward signal
  5. Evaluate evasion success and text quality

- Design tradeoffs:
  - Model size vs. training complexity and output quality
  - Temperature/sampling strategy vs. linguistic acceptability
  - Detector accuracy vs. evasion difficulty
  - Linguistic constraints vs. generation diversity

- Failure signatures:
  - High detection accuracy (detector works too well)
  - Low linguistic acceptability (text is gibberish)
  - Overfitting to a narrow generation style
  - Poor generalization to new detectors or domains

- First 3 experiments:
  1. Train BERT and Naive Bayes detectors on tweets generated with varying temperature and sampling strategies; measure detection accuracy.
  2. Apply RL to a GPT model to evade BERT; measure detection accuracy before and after RL, and assess linguistic quality.
  3. Test the RL-evaded model against a Naive Bayes detector to see if evasion generalizes to shallow methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the detection rates of transformer-based classifiers like BERT change when fine-tuned on data sets with different linguistic properties, such as longer text passages or domain-specific jargon?
- Basis in paper: [explicit] The paper discusses the effectiveness of BERT in detecting machine-generated tweets but does not explore its performance on longer or more specialized texts.
- Why unresolved: The study focuses on short, generic tweets, leaving the performance of BERT on varied text lengths and domains untested.
- What evidence would resolve it: Experiments comparing BERT's detection accuracy on texts of varying lengths and specialized domains, such as technical articles or legal documents.

### Open Question 2
- Question: Can reinforcement learning methods for bypassing detection classifiers be adapted to work effectively with real-time text generation systems, such as chatbots, without significant latency?
- Basis in paper: [inferred] The paper demonstrates bypassing detection with reinforcement learning but does not address the computational feasibility for real-time applications.
- Why unresolved: The reinforcement learning approach is computationally intensive and may not scale efficiently for real-time systems.
- What evidence would resolve it: Benchmarking the latency and resource requirements of RL-based methods in real-time text generation environments.

### Open Question 3
- Question: What are the long-term implications of undetectable machine-generated text on trust and authenticity in digital communication platforms?
- Basis in paper: [explicit] The paper discusses the potential for machine-generated texts to evade detection but does not explore broader societal or ethical impacts.
- Why unresolved: The study focuses on technical aspects of detection and evasion, leaving ethical and social considerations unaddressed.
- What evidence would resolve it: Sociological studies or surveys assessing the impact of undetectable machine-generated text on user trust and platform integrity.

## Limitations
- The study's scope is limited to tweets, which may not generalize to other text types like news articles or scientific papers
- The effectiveness of RL evasion relies on specific linguistic constraints tuned for tweets
- Computational resources required for fine-tuning large models with RL may limit practical applicability
- Detection evasion was only demonstrated against BERT, not other transformer-based classifiers

## Confidence

**High Confidence**: The claim that transformer-based classifiers (BERT) achieve detection accuracy above 0.9 is well-supported by the experimental results and aligns with established findings about BERT's contextual encoding capabilities.

**Medium Confidence**: The claim that shallow classifiers achieve only 0.6-0.8 accuracy is supported by experiments, but the sensitivity to temperature and sampling strategies suggests this range may shift with different training approaches.

**Low Confidence**: The generalizability of the RL evasion technique across different text domains and detector types remains uncertain, as the study does not explore potential detector countermeasures.

## Next Checks
1. Test the BERT detector and RL-evaded models on non-tweet text corpora (news, reviews, academic abstracts) to assess domain transfer capabilities and required adjustments to linguistic constraints.
2. Evaluate the RL-evaded outputs against multiple transformer-based classifiers (RoBERTa, DeBERTa, GPT-based classifiers) and ensemble methods to determine if evasion generalizes beyond the specific BERT model used.
3. Implement an iterative detection-evasion cycle where the detector is periodically retrained on newly generated samples, measuring how quickly detection accuracy recovers and what generator adaptations are most effective.