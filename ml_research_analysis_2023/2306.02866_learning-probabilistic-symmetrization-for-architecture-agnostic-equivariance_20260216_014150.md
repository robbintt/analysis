---
ver: rpa2
title: Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance
arxiv_id: '2306.02866'
source_url: https://arxiv.org/abs/2306.02866
tags:
- equivariant
- group
- cited
- distribution
- symmetrization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces probabilistic symmetrization, a novel approach
  for learning group equivariant functions using arbitrary base models such as MLPs
  or transformers. Instead of relying on specialized equivariant architectures, the
  method uses a small equivariant network to parameterize a distribution of group
  transformations conditioned on the input.
---

# Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance

## Quick Facts
- arXiv ID: 2306.02866
- Source URL: https://arxiv.org/abs/2306.02866
- Reference count: 40
- One-line primary result: Introduces probabilistic symmetrization that enables arbitrary base models to learn group-equivariant functions with competitive performance against tailored equivariant architectures.

## Executive Summary
This paper introduces probabilistic symmetrization, a novel approach for learning group-equivariant functions using arbitrary base models such as MLPs or transformers. Instead of relying on specialized equivariant architectures, the method uses a small equivariant network to parameterize a distribution of group transformations conditioned on the input. This distribution is jointly trained with the base model to maximize task performance while reducing the sample complexity of symmetrization. The approach guarantees both equivariance to the given group and universal approximation capability in expectation.

## Method Summary
The method introduces a parameterized distribution pω(g|x) that learns to sample group transformations conditioned on the input. This distribution is implemented as an equivariant neural network (e.g., GIN, Vector Neurons) that takes input features and noise as input and outputs a group element g. The base function fθ (e.g., MLP, transformer) processes the input transformed by g, and the output is transformed back. During training, pω is optimized jointly with fθ to minimize task loss, allowing the distribution to learn task-specific group transformations that reduce variance compared to uniform sampling.

## Key Results
- Probabilistic symmetrization achieves competitive results against tailored equivariant architectures across various symmetry groups (permutation, Euclidean, and combinations)
- The method demonstrates enhanced learning in symmetric modalities (like graphs) when pretrained from non-symmetric modalities (like vision)
- Guarantees both equivariance to the given group and universal approximation capability in expectation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic symmetrization guarantees equivariance and universal approximation when using a G-equivariant distribution pω(g|x).
- Mechanism: By replacing the uniform distribution in group averaging with a parameterized, input-conditional distribution pω(g|x), the method preserves the mathematical guarantee of equivariance while allowing the distribution to learn task-specific group transformations that reduce variance and improve training stability.
- Core assumption: The distribution pω(g|x) is probabilistically G-equivariant, meaning pω(g|x) = pω(g′g|ρ₁(g′)x) for all x ∈ X and g, g′ ∈ G.
- Evidence anchors:
  - [abstract]: "We show that this approach ensures not only equivariance to given group but also universal approximation capability in expectation."
  - [section]: Theorem 1 states "If pω is G equivariant, then ϕθ,ω is G equivariant for arbitrary fθ" and Theorem 2 states "If pω is G equivariant and fθ is a universal approximator, then ϕθ,ω is a universal approximator of G equivariant functions."
  - [corpus]: Weak - the corpus contains related works on symmetrization but lacks direct evidence of the universal approximation claim under probabilistic distributions.
- Break condition: If the distribution pω(g|x) fails to be probabilistically G-equivariant (e.g., due to incorrect implementation of the noise-outsourced map qω), the guaranteed equivariance and universal approximation would be lost.

### Mechanism 2
- Claim: The parameterized distribution pω(g|x) learns to produce lower-variance group transformations compared to uniform sampling, improving optimization of the base function fθ.
- Mechanism: During training, pω(g|x) is optimized jointly with fθ to minimize task loss. By learning to concentrate probability mass on more task-relevant group transformations, it reduces the variance of sampled transformations, leading to more stable gradients for fθ and faster convergence.
- Core assumption: The joint optimization of pω(g|x) and fθ allows pω to adapt its probability distribution to reduce variance in a way that benefits task performance.
- Evidence anchors:
  - [abstract]: "The distribution is end-to-end trained with the base model which can maximize performance while reducing sample complexity of symmetrization."
  - [section]: "We conjecture this is since the distribution pω(g|x) can learn to provide more consistent permutations during early training" and empirical observation of entropy decreasing in early training.
  - [corpus]: Weak - while related works mention symmetrization and equivariance, direct evidence of variance reduction through parameterized distributions is not explicitly stated in the corpus.
- Break condition: If the learning rate for pω(g|x) is too high or too low, or if the base function fθ is too expressive relative to the task, the distribution may fail to learn useful variance reduction.

### Mechanism 3
- Claim: Pretraining a general-purpose base model (e.g., vision transformer) on a non-symmetric modality and then symmetrizing it enables transfer of knowledge to symmetric modalities.
- Mechanism: The base model fθ learns general features during pretraining that are not tied to any specific symmetry. When symmetrized with an appropriate pω(g|x) for a symmetric task, these general features can be leveraged, potentially providing a better initialization than training from scratch on the symmetric task.
- Core assumption: Features learned from non-symmetric modalities contain transferable information that can be beneficial when adapted to symmetric tasks through symmetrization.
- Evidence anchors:
  - [abstract]: "We further show evidence of enhanced learning in symmetric modalities, like graphs, when pretrained from non-symmetric modalities, like vision."
  - [section]: "We observe that transferring the pre-trained ViT parameters consistently improves the node classification for all symmetrization methods" and "vision pretraining allows group averaging to achieve 84.660% accuracy."
  - [corpus]: Weak - the corpus includes works on transfer learning and symmetrization but lacks direct evidence of cross-modality transfer from non-symmetric to symmetric tasks.
- Break condition: If the pretraining task is too dissimilar from the target symmetric task, or if the symmetrization distribution pω(g|x) is not well-suited to the target symmetry, the transferred knowledge may not be beneficial.

## Foundational Learning

- Concept: Group theory and group actions
  - Why needed here: Understanding how symmetry groups act on data spaces is fundamental to designing equivariant functions and implementing the distribution pω(g|x).
  - Quick check question: Can you explain the difference between a group representation and a group action, and how they relate to equivariance in neural networks?

- Concept: Universal approximation theorems
  - Why needed here: The method relies on the base function fθ being a universal approximator to guarantee that the symmetrized function ϕθ,ω can approximate any G-equivariant function.
  - Quick check question: What are the conditions under which a neural network (e.g., MLP, transformer) can be a universal approximator, and how does this relate to the input and output spaces?

- Concept: Reparameterization trick and gradient estimation through stochastic nodes
  - Why needed here: Implementing pω(g|x) as a noise-outsourced map requires differentiating through the sampling process, which is achieved using the reparameterization trick.
  - Quick check question: How does the reparameterization trick allow for gradient-based training of a distribution parameterized by a neural network, and what are its limitations?

## Architecture Onboarding

- Component map:
  Base function fθ (MLP/transformer) -> Equivariant distribution pω(g|x) (GIN/Vector Neurons) -> Postprocessing layers (argsort/Gram-Schmidt) -> Symmetrization layer

- Critical path:
  1. Prepare input data and apply centroid subtraction if needed (for Euclidean groups)
  2. Sample noise ϵ and pass through the equivariant network qω to obtain group representation ρ(g)
  3. Apply group transformation g to input and pass through base function fθ
  4. Apply inverse group transformation to output of fθ
  5. Aggregate over samples from pω(g|x) to produce final output

- Design tradeoffs:
  - Expressiveness vs. efficiency: Using a more expressive base function fθ increases the potential for universal approximation but also increases computational cost and memory usage
  - Sample size vs. variance: Increasing the number of samples from pω(g|x) during training and inference reduces variance but also increases computational cost
  - Noise scale η: Larger noise scale encourages exploration of group transformations but may lead to unstable training; smaller scale may lead to premature convergence to suboptimal solutions

- Failure signatures:
  - Training instability or divergence: May indicate issues with the reparameterization implementation or inappropriate learning rates
  - Poor performance despite correct implementation: May suggest that the base function fθ is not well-suited to the task or that the equivariant distribution pω(g|x) is not learning useful transformations
  - Memory errors: May indicate that the sample size is too large for the available hardware or that the base function fθ is too large

- First 3 experiments:
  1. Implement and test probabilistic symmetrization for the symmetric group Sn on a simple graph isomorphism task (e.g., GRAPH8c) using an MLP as the base function. Compare performance to group averaging and canonicalization.
  2. Implement and test probabilistic symmetrization for the orthogonal group O(3) on a 3D point cloud classification task using a transformer as the base function. Compare performance to existing equivariant architectures.
  3. Pretrain a vision transformer on ImageNet-21k and then symmetrize it for a graph node classification task (e.g., PATTERN). Compare performance to training from scratch and to other symmetrization methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of probabilistic symmetrization scale with increasing group size and complexity? 
- Basis in paper: [explicit] The paper tests on various groups (Sn, O(3), SO(3), E(3), SE(3), and combinations) but does not systematically analyze the effect of group size.
- Why unresolved: The experiments use specific group sizes (e.g., 5 particles in n-body problem, max 188 nodes in PATTERN) but don't explore scaling behavior. The paper mentions that group averaging struggles with large groups but doesn't quantify the performance degradation of probabilistic symmetrization.
- What evidence would resolve it: Systematic experiments varying group sizes (e.g., testing Sn with 10, 50, 100, 500 nodes) and comparing sample complexity and performance between probabilistic symmetrization and other methods would provide clarity.

### Open Question 2
- Question: What is the relationship between the expressiveness of the equivariant network qω and the overall model performance?
- Basis in paper: [explicit] The paper states that qω can be small and less expressive since "most of the reasoning is done by the base function fθ" but doesn't empirically validate this claim.
- Why unresolved: The experiments use specific architectures for qω (3-layer GIN, 2-layer Vector Neurons) but don't explore how varying the expressiveness affects performance. The paper doesn't provide ablation studies on qω's architecture.
- What evidence would resolve it: Experiments comparing different architectures and depths for qω (e.g., 1-layer vs 5-layer GIN) while keeping fθ constant would reveal the impact of qω's expressiveness on performance.

### Open Question 3
- Question: How does the choice of noise distribution p(ϵ) affect the learned pω(g|x) and downstream performance?
- Basis in paper: [explicit] The paper uses specific noise distributions (uniform Unif[0,η] for Sn, normal N(0,η²) for others) but doesn't analyze the impact of different choices.
- Why unresolved: While the paper mentions that simple noise choices "often suffice," it doesn't explore whether more sophisticated distributions could improve performance or learning dynamics.
- What evidence would resolve it: Experiments comparing different noise distributions (e.g., uniform vs normal, different scales η, structured noise) and analyzing their effect on pω(g|x) consistency and task performance would provide insights.

## Limitations

- Theoretical guarantees of universal approximation under probabilistic symmetrization rely on strong assumptions about the equivariance of the parameterized distribution pω(g|x)
- Empirical validation is limited to specific symmetry groups and datasets, leaving open questions about generalizability to more complex groups or tasks
- Transfer learning results from non-symmetric to symmetric modalities are promising but not extensively explored, with unclear mechanisms for why general-purpose features would be beneficial in symmetric contexts

## Confidence

- **High Confidence**: The implementation of probabilistic symmetrization for the symmetric group Sn on graph isomorphism tasks is well-defined and reproducible, with clear specifications for the GIN-based distribution and MLP base model
- **Medium Confidence**: The claims about variance reduction through learned distributions and the impact on optimization are supported by empirical observations but lack extensive ablation studies to isolate the effect
- **Low Confidence**: The theoretical guarantees of universal approximation under probabilistic symmetrization, while stated in theorems, are not empirically validated across a broad range of tasks and base model architectures

## Next Checks

1. **Ablation Study on Noise Scale**: Conduct experiments varying the noise scale η in the equivariant distribution pω(g|x) to empirically validate the impact on variance reduction and task performance, as suggested by the paper's discussion but not explored in depth.

2. **Generalization to Complex Groups**: Test the probabilistic symmetrization framework on more complex symmetry groups (e.g., semi-direct products or affine groups) to assess the scalability of the approach and validate the theoretical claims about universal approximation.

3. **Cross-Modality Transfer Analysis**: Perform a detailed analysis of the transfer learning results from non-symmetric (vision) to symmetric (graph) modalities, including feature visualization and similarity measures, to understand the mechanisms behind the observed performance improvements.