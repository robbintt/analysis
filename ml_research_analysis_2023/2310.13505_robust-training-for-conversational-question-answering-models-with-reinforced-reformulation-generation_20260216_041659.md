---
ver: rpa2
title: Robust Training for Conversational Question Answering Models with Reinforced
  Reformulation Generation
arxiv_id: '2310.13505'
source_url: https://arxiv.org/abs/2310.13505
tags:
- question
- answering
- conversational
- reformulations
- reformulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of robust training for conversational
  question answering (ConvQA) models over knowledge graphs (KGs), which are typically
  trained and tested on limited gold QA pairs. To address this, the authors propose
  a framework called Reign that generates reformulations of training questions using
  a taxonomy of syntactic transformations and a reformulation generator model.
---

# Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation

## Quick Facts
- arXiv ID: 2310.13505
- Source URL: https://arxiv.org/abs/2310.13505
- Reference count: 40
- One-line primary result: Reign framework achieves 44.7 P@1 and 61.3 MRR on ConvMix benchmark, outperforming standard training by 5.4 P@1 and 3.3 MRR

## Executive Summary
This paper addresses the challenge of robust training for conversational question answering (ConvQA) models over knowledge graphs by introducing Reign, a framework that generates and selectively incorporates reformulated training questions. The approach uses a taxonomy of 15 syntactic reformulation categories, a reformulation generator (BART), and a reformulation category selector (Deep Q-Network) to identify which reformulations will most improve model performance. Experiments on two benchmarks demonstrate significant improvements in both accuracy metrics (P@1, MRR) and a novel robustness metric measuring the model's ability to handle surface form variations.

## Method Summary
The Reign framework consists of three components: (1) Reformulation Category Selector (RCS) using Deep Q-Networks to select beneficial reformulation categories based on QA performance rewards, (2) Reformulation Generator (RG) using BART fine-tuned on noisy data generated from a taxonomy of 15 reformulation categories, and (3) integration with existing ConvQA models (Conqer and Explaignn) for robust training. The framework generates reformulations for training questions, selects the most beneficial ones using RL, and augments the training data with these selected reformulations paired with original gold answers. The authors also release a large set of diverse reformulations generated by ChatGPT for future model evaluation.

## Key Results
- Reign achieves 44.7 P@1 and 61.3 MRR on ConvMix benchmark, outperforming standard training by 5.4 P@1 and 3.3 MRR
- Robust metric shows 10.7 average number of reformulations correctly answerable per question (versus 7.8 for standard training)
- Significant improvements observed on ConvQuestions benchmark as well, demonstrating zero-shot transferability
- Ablation studies confirm the importance of each component, with RL-based category selection providing substantial gains over random selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Deep Q-Networks (DQN) for reformulation category selection improves ConvQA performance more than randomly selecting categories or using all reformulations.
- Mechanism: The RCS model learns to select the most beneficial reformulation categories by using QA performance metrics (e.g., reciprocal rank) as rewards. The DQN approximates the value function Q(s,a) which estimates the expected reward of taking action a in state s, guiding the model to choose categories that lead to better QA performance.
- Core assumption: QA performance metrics are reliable indicators of the usefulness of reformulation categories.
- Evidence anchors:
  - [abstract] "Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning."
  - [section] "We use Deep Q-Networks (DQN) as our RL algorithm, which is a model-free, value-based method that learns to predict so-called Q-values Q(s,a) for each state-action pair to quantify the usefulness of taking action a in state s under a policy π [49]."
  - [corpus] Weak. No direct evidence in corpus neighbors.
- Break condition: If QA performance metrics are not correlated with actual usefulness of reformulations, the DQN will learn to select suboptimal categories.

### Mechanism 2
- Claim: Fine-tuning BART on noisy data generated from the taxonomy of reformulation categories produces better reformulations than using rule-based generation alone.
- Mechanism: The RG model is fine-tuned on distantly supervised data generated by applying string edit operations (insert, delete, substitute) to conversational questions based on the reformulation taxonomy. BART's denoising ability allows it to learn to generate more natural and diverse reformulations than simple rule-based approaches.
- Core assumption: The noisy data generated from the taxonomy captures the essential patterns of valid reformulations.
- Evidence anchors:
  - [abstract] "Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning."
  - [section] "The concatenation of the conversation history ⟨Q1A1 ... Qt-1At-1⟩, the current question Qt, and a special reformulation category tag ('rc1', 'rc2', ...'rc15') constitute the input, and the category-specific reformulation is the output."
  - [corpus] Weak. No direct evidence in corpus neighbors.
- Break condition: If the noisy data is too noisy or does not capture the essence of valid reformulations, the fine-tuned BART model may generate invalid or unhelpful reformulations.

### Mechanism 3
- Claim: Using model-aware training with reformulations selected by the RCS model improves the robustness of the ConvQA model more than training on the original questions alone.
- Mechanism: The ConvQA model is trained on both the original questions and the reformulations selected by the RCS model. This exposes the model to a wider variety of surface forms for the same intent, making it more robust to variations in user queries.
- Core assumption: Exposing the ConvQA model to more diverse surface forms during training improves its ability to handle variations in user queries.
- Evidence anchors:
  - [abstract] "Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another."
  - [section] "In the original training mode, QA pairs are directly used from the benchmark train sets. This original or initial QA model is used to collect rewards for the RCS in one pass over the dev set... After the trained RCS and RG models generate the reformulations for each training question, these reformulations are paired with the corresponding gold answer of the original training question. These new ⟨reformulation, gold answer⟩ pairs are added to the benchmark, and the ConvQA model is trained again on this augmented resource."
  - [corpus] Weak. No direct evidence in corpus neighbors.
- Break condition: If the selected reformulations are not representative of the variations in user queries, the ConvQA model may not become more robust.

## Foundational Learning

- Concept: Reinforcement Learning (RL) and Deep Q-Networks (DQN)
  - Why needed here: To train the RCS model to select the most beneficial reformulation categories based on QA performance metrics.
  - Quick check question: What is the main update step in Q-Learning, and how does it differ from policy gradient methods?

- Concept: Sequence-to-Sequence Learning and Fine-tuning of Large Language Models
  - Why needed here: To train the RG model to generate reformulations based on the taxonomy and the selected categories.
  - Quick check question: What are the key components of the BART model architecture, and how does it differ from other sequence-to-sequence models like GPT?

- Concept: Question Answering (QA) Metrics and Evaluation
  - Why needed here: To assess the performance of the ConvQA model and to provide rewards for the RCS model during training.
  - Quick check question: What are the common metrics used to evaluate QA models, and how do they differ in their focus (e.g., precision, recall, ranking quality)?

## Architecture Onboarding

- Component map: Question → RCS (DQN) → RG (BART) → ConvQA → Answer
- Critical path: Conversational question flows through RCS for category selection, then to RG for reformulation generation, and finally to ConvQA model for answer retrieval
- Design tradeoffs:
  - Number of reformulation categories: More categories may lead to better coverage but also more noise
  - Number of selected reformulations per question: More reformulations may improve robustness but also increase training time
  - Use of RL vs. supervised learning for RCS: RL allows for model-aware training but may require more data and computation
- Failure signatures:
  - Poor performance on test set despite good dev set performance: RCS may be overfitting to dev set
  - Degradation in performance when using reformulations: Selected reformulations may be too noisy or not representative of real user queries
  - Slow convergence during RCS training: Reward signal may be too sparse or noisy
- First 3 experiments:
  1. Train RCS model on dev set using small number of reformulation categories (e.g., 3) and small number of selected reformulations per question (e.g., 1). Evaluate ConvQA performance on test set.
  2. Increase number of reformulation categories and selected reformulations per question, evaluate ConvQA performance again. Observe impact on performance and training time.
  3. Compare performance of ConvQA model trained with reformulations selected by RCS against baseline model trained only on original questions. Use robustness metric defined in paper to quantify improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Reign compare to other data augmentation techniques like adversarial training or back-translation for improving robustness in ConvQA models?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of Reign's reformulation-based approach but does not compare it to other data augmentation techniques commonly used in NLP.
- Why unresolved: The paper focuses on showcasing the benefits of its specific reformulation approach and does not explore other data augmentation methods for comparison.
- What evidence would resolve it: Experiments comparing Reign's performance to other data augmentation techniques like adversarial training or back-translation on the same benchmarks and metrics used in the paper.

### Open Question 2
- Question: How does the performance of Reign vary with different sizes of the training data? Is there a point of diminishing returns where adding more reformulations no longer improves the model's robustness?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of Reign but does not explore how its performance scales with different training data sizes or whether there is a point of diminishing returns.
- Why unresolved: The paper focuses on demonstrating the benefits of Reign's approach and does not investigate the relationship between training data size and model performance.
- What evidence would resolve it: Experiments evaluating Reign's performance on different subsets of the training data to determine the relationship between training data size and model robustness, as well as identifying any potential points of diminishing returns.

### Open Question 3
- Question: How does the performance of Reign generalize to other types of knowledge graphs beyond Wikidata, such as those with different schema or data quality?
- Basis in paper: [inferred] The paper demonstrates Reign's effectiveness on Wikidata but does not explore its generalizability to other types of knowledge graphs.
- Why unresolved: The paper focuses on a specific knowledge graph (Wikidata) and does not investigate the applicability of Reign to other knowledge graph types with different characteristics.
- What evidence would resolve it: Experiments evaluating Reign's performance on different knowledge graphs with varying schemas, data quality, and domain focus to assess its generalizability and robustness.

## Limitations

- Reliance on QA performance metrics as rewards for RCS model without empirical validation of their correlation with actual reformulation usefulness
- Limited generalizability analysis beyond two benchmarks, making broader applicability claims uncertain
- Quality of reformulations depends heavily on noisy supervision data, with limited analysis of how well this data captures valid reformulation patterns

## Confidence

- High confidence: Experimental results showing Reign's superiority over baseline training methods are well-supported by methodology and ablation studies
- Medium confidence: Claim that model-aware training with reformulations improves robustness is supported by results but causal mechanism requires further investigation
- Low confidence: Generalizability claim of zero-shot transfer between benchmarks is supported by results on ConvQuestions but sample size (only two benchmarks) is insufficient for strong conclusions

## Next Checks

1. **Validate reward signal correlation**: Conduct controlled experiment to measure correlation between QA performance metrics (used as rewards) and actual downstream performance when using reformulations selected by different categories. This would confirm whether RCS model is optimizing for right signal.

2. **Analyze reformulation quality**: Perform human evaluation study on sample of reformulations generated by fine-tuned BART model versus those generated by simple rule-based methods. This would quantify whether added complexity of RG component provides tangible quality improvements.

3. **Test on additional benchmarks**: Apply Reign to at least two additional ConvQA benchmarks (different domains or question types) to validate zero-shot transferability claim and assess how performance scales with dataset size and complexity.