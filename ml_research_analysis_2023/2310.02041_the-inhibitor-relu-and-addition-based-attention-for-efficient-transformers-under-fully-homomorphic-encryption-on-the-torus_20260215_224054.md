---
ver: rpa2
title: 'The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers
  under Fully Homomorphic Encryption on the Torus'
arxiv_id: '2310.02041'
source_url: https://arxiv.org/abs/2310.02041
tags:
- attention
- relu
- which
- inhibitor
- encryption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Inhibitor mechanism replaces dot-product and Softmax-based
  attention in Transformers with an alternative based on addition and ReLU activation.
  This avoids costly multiplication and Softmax operations while maintaining comparable
  performance.
---

# The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers under Fully Homomorphic Encryption on the Torus

## Quick Facts
- **arXiv ID**: 2310.02041
- **Source URL**: https://arxiv.org/abs/2310.02041
- **Reference count**: 36
- **Key outcome**: The Inhibitor mechanism replaces dot-product and Softmax-based attention in Transformers with an alternative based on addition and ReLU activation, maintaining comparable performance while reducing computational costs by 30-50% in plaintext and up to 6x under homomorphic encryption.

## Executive Summary
This paper introduces the Inhibitor, a novel attention mechanism that replaces conventional dot-product and Softmax operations with addition and ReLU activation. The mechanism computes attention scores using Manhattan distance and applies inhibition through subtraction inside a ReLU function. This approach maintains transformer functionality while significantly reducing computational complexity, particularly benefiting homomorphic encryption applications where multiplication operations are expensive. Experiments on four benchmark tasks show prediction scores similar to conventional Transformers while demonstrating substantial computational savings.

## Method Summary
The Inhibitor attention mechanism replaces standard scaled dot-product attention with equations computing pairwise Manhattan distances between queries and keys (Zij = ∑k 1/γ |Qik − Kjk|), then applies inhibition via subtraction inside a ReLU function to compute weighted sums of values (Hik = ∑j (Vjk − Zij)+). The mechanism supports both non-negative and signed values through an alternative formulation. The approach naturally enables integer arithmetic implementation and avoids multiplication operations, making it particularly suitable for homomorphic encryption environments. Experiments were conducted on four tasks: the Adding problem with 100-length sequences, MNIST digit classification, IMDB sentiment analysis, and IAM Handwriting Database text recognition.

## Key Results
- Inhibitor attention maintains prediction scores comparable to conventional Transformers across four benchmark tasks
- Computational savings of 30-50% in plaintext execution, with factor 3-6 improvements under homomorphic encryption
- The mechanism naturally supports quantization and integer arithmetic implementation
- Performance degrades when inhibition scores are too large (α parameter effects)

## Why This Works (Mechanism)

### Mechanism 1: Inhibitor Attention Replaces Multiplication with Addition
The mechanism computes attention using Manhattan distance between query-key pairs and applies inhibition through subtraction inside ReLU, avoiding expensive multiplication operations. This maintains ranking behavior similar to dot-product attention for the tested tasks. The core assumption is that Manhattan distance can approximate the relative ranking of attention scores needed for sequence modeling.

### Mechanism 2: Homomorphic Encryption Compatibility
By using only addition, subtraction, absolute value, and ReLU operations, the Inhibitor avoids multiplication of encrypted variables under TFHE homomorphic encryption. This eliminates the need for expensive programmable bootstrapping operations, enabling more efficient privacy-preserving AI applications.

### Mechanism 3: Efficient Integer Arithmetic Implementation
The inhibitor mechanism's reliance on addition, subtraction, absolute value, and ReLU operations maps directly to integer arithmetic without precision loss from floating-point conversion. This natural compatibility with quantization enables efficient execution on resource-constrained hardware.

## Foundational Learning

- **Concept: Manhattan distance as attention similarity metric**
  - Why needed here: The inhibitor mechanism uses Manhattan distance instead of dot-product to compute attention scores
  - Quick check question: How does Manhattan distance between query and key vectors differ from their dot-product in terms of what relationships it captures?

- **Concept: ReLU activation properties in attention mechanisms**
  - Why needed here: The inhibitor uses ReLU to implement inhibition/suppression of value contributions
  - Quick check question: What happens to attention values when they pass through ReLU after subtraction with inhibition scores?

- **Concept: Homomorphic encryption computational tradeoffs**
  - Why needed here: The paper claims FHE efficiency benefits require understanding the relative costs of different operations under encryption
  - Quick check question: Why are multiplication operations particularly expensive under fully homomorphic encryption compared to addition operations?

## Architecture Onboarding

- **Component map**: Input embedding layer → Inhibitor attention mechanism (Manhattan distance + ReLU inhibition) → Feed-forward network (unchanged) → Layer normalization → Output
- **Critical path**: Attention computation is now: embedding → weight matrices (Q,K,V) → pairwise Manhattan distances → inhibition via ReLU → weighted sum of values. This replaces: embedding → weight matrices → scaled dot-product → softmax → weighted sum.
- **Design tradeoffs**: 
  - Pro: Eliminates softmax computation and multiplication-heavy operations
  - Pro: Enables integer quantization and FHE compatibility
  - Con: May lose some attention score sensitivity from dot-product's angular relationship
  - Con: Manhattan distance is less sensitive to vector magnitude differences than dot-product
- **Failure signatures**: 
  - Attention scores become too uniform (α parameter too large in shifted inhibition)
  - Model fails to capture long-range dependencies (Manhattan distance less sensitive to directional relationships)
  - Performance degrades significantly on tasks requiring precise similarity ranking
- **First 3 experiments**:
  1. Implement basic inhibitor attention with α=0.5, γ=√d on adding problem (100-length sequences) and verify comparable accuracy to standard attention
  2. Benchmark plaintext execution time for sequence lengths 32, 64, 128 on CPU to verify 30-50% speedup claims
  3. Implement shifted inhibition variant and test sensitivity to α parameter on MNIST classification task

## Open Questions the Paper Calls Out
- How does the Inhibitor attention mechanism perform on large-scale pre-trained transformer models compared to conventional dot-product attention?
- What is the impact of the Inhibitor attention mechanism on the computational efficiency of transformers when using different quantization schemes or precision levels?
- How does the Inhibitor attention mechanism affect the interpretability and explainability of transformer models?

## Limitations
- The evaluation scope is narrow, focusing on simple benchmark tasks without comprehensive hyperparameter optimization
- No comparison with alternative efficient attention mechanisms like Linformer, Performer, or FlashAttention
- Limited investigation of the shifted inhibition variant's sensitivity to the α parameter
- FHE implementation details and performance metrics are not fully specified

## Confidence

**Confidence Levels:**
- **High Confidence**: The inhibitor mechanism's basic functionality and implementation are well-specified through equations 3-5. The claim that it avoids multiplication operations is mathematically sound.
- **Medium Confidence**: The reported computational savings (30-50% in plaintext, factor 3-6 under FHE) are based on scaling experiments, but these were limited to 16-bit integer arithmetic and small embedding dimensions (2).
- **Low Confidence**: The claim about maintaining "comparable performance" to conventional Transformers lacks comprehensive hyperparameter optimization and systematic ablation studies across diverse tasks.

## Next Checks

1. Implement comprehensive ablation studies varying the α parameter in shifted inhibition across all four benchmark tasks to determine optimal settings and failure modes
2. Benchmark against established efficient attention mechanisms (Linformer, Performer) on the same tasks to contextualize the claimed performance gains
3. Test the mechanism on additional sequence modeling tasks requiring long-range dependencies to evaluate whether Manhattan distance adequately captures the angular relationships that dot-product attention captures