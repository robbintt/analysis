---
ver: rpa2
title: 'BiSinger: Bilingual Singing Voice Synthesis'
arxiv_id: '2309.14089'
source_url: https://arxiv.org/abs/2309.14089
tags:
- singing
- system
- english
- speech
- voice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BiSinger, a bilingual singing voice synthesis
  system for English and Chinese Mandarin. The authors address the challenge of multilingual
  singing voice modeling by designing a shared representation between Chinese and
  English singing voices using the CMU dictionary with mapping rules.
---

# BiSinger: Bilingual Singing Voice Synthesis

## Quick Facts
- **arXiv ID:** 2309.14089
- **Source URL:** https://arxiv.org/abs/2309.14089
- **Reference count:** 0
- **Primary result:** Achieves MOS score of 3.42 for English songs and 3.24 for mixed-language songs in bilingual singing voice synthesis

## Executive Summary
This paper presents BiSinger, a bilingual singing voice synthesis system for English and Chinese Mandarin that addresses the challenge of multilingual singing voice modeling. The authors propose a shared representation between Chinese and English singing voices using the CMU dictionary with mapping rules, and explore incorporating bilingual speech data to generate bilingual singing voices. The system demonstrates that language-independent representation and dataset fusion enable a single model with enhanced performance in both English and code-switch SVS while maintaining Chinese song performance.

## Method Summary
The BiSinger system combines DiffSinger with a language-style-infused encoder to enable bilingual singing synthesis. It uses CMU phonemes as a shared representation for both languages, augmented with language ID tokens to preserve pronunciation differences. The approach fuses monolingual singing datasets (M4Singer for Chinese and NUS-48E for English) and incorporates bilingual speech data (DB-4) converted to pseudo-singing through pitch shifting. Timbre conversion using SVC expands the English dataset by converting it to match the 20 singer timbres in the larger Chinese dataset. The model is trained on this combined dataset and generates Mel-spectrograms conditioned on musical and linguistic features.

## Key Results
- Achieves MOS score of 3.42 for English songs and 3.24 for mixed-language songs
- Demonstrates successful code-switch singing synthesis within a single model
- Shows improved performance over baseline models for both Chinese and English singing synthesis
- Maintains Chinese song performance while adding English capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-independent phoneme mapping using CMU dictionary enables shared cross-lingual representation
- Mechanism: Pinyin is first mapped to CMU phonemes using a custom mapping table, then augmented with numeric language ID tokens to preserve language-specific pronunciation differences
- Core assumption: Singing pitch is primarily determined by musical score rather than tone/intonation, making CMU phonemes sufficient for cross-lingual modeling
- Evidence anchors: "design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules"

### Mechanism 2
- Claim: Timbre conversion expands English singing data using SVC techniques while preserving timbre consistency
- Mechanism: so-vits-svc5 converts English singing voices to match the 20 different timbres in the larger Chinese dataset, increasing English singing data 20x
- Core assumption: Singing voice conversion can reliably transfer timbre while preserving phoneme pronunciation quality
- Evidence anchors: "we use so-vits-svc5... to convert the singing voices in the NUS-48E dataset to the target 20 singers in the M4Singer dataset"

### Mechanism 3
- Claim: Pitch-shifted speech data provides pseudo-singing augmentation for multilingual modeling
- Mechanism: WORLD vocoder shifts speech F0 to musical chords while preserving spectral envelope, creating pseudo-singing data with rhythmic variation
- Core assumption: Speech contains sufficient linguistic information to transfer to singing when pitch is adjusted, despite lacking natural singing characteristics
- Evidence anchors: "We treat speech data as plain singing data... adjust the pitch of speech data with the WORLD vocoder"

## Foundational Learning

- Concept: CMU Pronunciation Dictionary structure and phoneme set
  - Why needed here: Understanding CMU phoneme coverage is essential for validating language-independent representation claims
  - Quick check question: How many unique phonemes are in the CMU dictionary, and does it include all English and Mandarin phonemes?

- Concept: Singing voice conversion (SVC) pipeline and quality metrics
  - Why needed here: Evaluating timbre conversion quality requires understanding SVC limitations and evaluation methods
  - Quick check question: What are the common artifacts introduced by SVC, and how do they affect singing synthesis quality?

- Concept: Pitch shifting techniques and their impact on speech perception
  - Why needed here: Pseudo-singing generation relies on pitch shifting without introducing unnatural artifacts
  - Quick check question: What are the perceptual differences between natural singing pitch contours and artificially shifted speech F0?

## Architecture Onboarding

- Component map: Annotation → CMU phoneme mapping → Language-style-infused encoder → DiffSinger acoustic model → Mel-spectrogram → Vocoder → Waveform
- Critical path: Annotation adaptation → Timbre conversion → Model training → Inference with language tokens
- Design tradeoffs:
  - Using CMU phonemes simplifies cross-lingual modeling but may lose language-specific nuances
  - SVC conversion increases data but introduces conversion artifacts
  - Speech-to-singing augmentation is cost-effective but may sound unnatural
- Failure signatures:
  - Chinese WER increases significantly → CMU phoneme mapping errors
  - English MOS drops dramatically → SVC conversion quality issues
  - Generated singing sounds too fast/smooth → Style embedding not effective
- First 3 experiments:
  1. Validate CMU phoneme coverage for both languages by testing with isolated word synthesis
  2. Evaluate SVC conversion quality on a small subset before full-scale conversion
  3. Compare pseudo-singing from pitch-shifted speech vs. real singing data on a held-out set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach compare to using multilingual TTS models for generating singing voices?
- Basis in paper: [inferred] The paper focuses on adapting existing monolingual singing datasets and bilingual speech data for multilingual singing voice synthesis, but does not compare its approach to using multilingual TTS models.
- Why unresolved: The paper does not provide a direct comparison between the proposed approach and using multilingual TTS models for generating singing voices.
- What evidence would resolve it: A comparative study between the proposed approach and using multilingual TTS models for generating singing voices, including objective and subjective evaluations.

### Open Question 2
- Question: How does the proposed approach handle code-switching within a single utterance, and what are the challenges associated with it?
- Basis in paper: [inferred] The paper mentions code-switching but does not provide details on how the proposed approach handles it within a single utterance or the associated challenges.
- Why unresolved: The paper does not provide a detailed analysis of the proposed approach's performance on code-switching within a single utterance or the challenges associated with it.
- What evidence would resolve it: A detailed analysis of the proposed approach's performance on code-switching within a single utterance, including challenges and potential solutions.

### Open Question 3
- Question: How does the proposed approach scale to more languages, and what are the potential limitations?
- Basis in paper: [inferred] The paper focuses on Chinese and English but does not discuss how the proposed approach would scale to more languages or potential limitations.
- Why unresolved: The paper does not provide information on how the proposed approach would scale to more languages or potential limitations associated with it.
- What evidence would resolve it: An analysis of the proposed approach's performance on multiple languages, including potential limitations and challenges.

### Open Question 4
- Question: How does the proposed approach handle singing styles and genres, and what are the challenges associated with it?
- Basis in paper: [inferred] The paper mentions singing styles and genres but does not provide details on how the proposed approach handles them or the associated challenges.
- Why unresolved: The paper does not provide a detailed analysis of the proposed approach's performance on different singing styles and genres or the challenges associated with it.
- What evidence would resolve it: A detailed analysis of the proposed approach's performance on different singing styles and genres, including challenges and potential solutions.

## Limitations

- CMU dictionary coverage uncertainty for Mandarin phonemes may lead to pronunciation errors
- SVC conversion quality is unknown and may introduce artifacts that degrade model performance
- Pitch-shifted speech may sound too artificial and fail to capture natural singing articulation patterns

## Confidence

**High Confidence Claims:**
- The overall architecture combining DiffSinger with language-style-infused encoder is technically sound
- MOS scores show the system produces listenable bilingual singing synthesis
- Code-switch capability within a single model is demonstrated

**Medium Confidence Claims:**
- Language-independent phoneme representation using CMU dictionary enables cross-lingual modeling
- Incorporating bilingual speech data improves synthesis quality
- Style embedding effectively prevents speech-like artifacts

**Low Confidence Claims:**
- Timbre conversion quality is sufficient for effective data augmentation
- Pitch-shifted speech provides meaningful improvement over using only real singing data
- CMU phoneme mapping preserves pronunciation quality across both languages

## Next Checks

1. **Phoneme Coverage Validation**: Conduct systematic testing of CMU dictionary coverage by synthesizing isolated words from both languages and measuring phoneme substitution errors. Create a test set of words containing potentially problematic phonemes (e.g., Mandarin retroflex sounds, English vowel distinctions) and evaluate pronunciation accuracy using forced alignment and human perceptual tests.

2. **SVC Conversion Quality Assessment**: Implement a two-stage evaluation of the timbre conversion process. First, use objective metrics like mel-cepstral distortion between converted and original samples. Second, conduct a listening test where raters compare converted English singing to original recordings and to Chinese singing data, rating naturalness and timbre similarity.

3. **Pseudo-Singing Data Impact Study**: Design an ablation study comparing model performance when trained with: (a) only real singing data, (b) real singing data plus pitch-shifted speech, and (c) real singing data plus professionally recorded pseudo-singing (ground truth). Measure differences in MOS scores, pronunciation accuracy, and naturalness across all three conditions to quantify the actual benefit of the speech-to-singing conversion approach.