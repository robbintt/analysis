---
ver: rpa2
title: Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual
  Assistants
arxiv_id: '2311.01398'
source_url: https://arxiv.org/abs/2311.01398
tags:
- rescoring
- n-best
- category
- on-device
- nnlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates strategies for improving Automatic Speech
  Recognition (ASR) accuracy for entity-rich queries on virtual assistants by using
  server-side language model rescoring. The authors propose to combine on-device ASR
  results with multiple categories of server-side language models, including N-gram
  models, neural network language models (NNLMs), and a large language model (LLM)
  baseline.
---

# Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants

## Quick Facts
- arXiv ID: 2311.01398
- Source URL: https://arxiv.org/abs/2311.01398
- Reference count: 40
- Primary result: Domain-specific server-side LM rescoring significantly improves WER for entity-rich queries

## Executive Summary
This paper investigates strategies for improving Automatic Speech Recognition (ASR) accuracy for entity-rich queries on virtual assistants through server-side language model rescoring. The authors propose combining on-device ASR results with multiple categories of server-side language models, including N-gram models, neural network language models (NNLMs), and a large language model (LLM) baseline. They train N-gram and NNLM models from scratch on domain-specific data, while using an out-of-the-box LLM as a baseline. The approach is evaluated on a synthetic dataset of media player queries, demonstrating significant WER improvements across head, torso, and tail query subpopulations compared to on-device ASR alone.

## Method Summary
The authors propose a two-stage approach combining on-device ASR with server-side rescoring. First, on-device ASR generates N-best hypotheses using a convolutional-based neural network acoustic model, 4-gram word LM, and FOFE word NNLM. Second, these hypotheses are rescored using server-side models including N-gram LMs trained with SRILM and NNLMs (LSTM, FOFE, Transformer) trained with SentencePiece tokenization and Adam optimizer on domain-specific data. The final hypothesis is selected through linear interpolation that combines on-device and server-side scores using optimized weights found via Powell's method.

## Key Results
- Server-side rescoring with N-gram and NNLM models significantly improves WER across all query subpopulations
- Combining N-gram and NNLM models through linear interpolation achieves the best performance
- Domain-specific sub-word NNLMs outperform out-of-the-box LLM baselines for N-best rescoring
- The approach is particularly effective for tail queries that are challenging for on-device models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training domain-specific server-side LMs for N-best rescoring improves ASR accuracy for entity-rich queries.
- Mechanism: The on-device ASR system is resource-constrained and uses limited knowledge sources, while server-side LMs can leverage larger models and more extensive training data specific to the domain, enabling better handling of entity-rich queries.
- Core assumption: Domain-specific training data is representative of the query distribution and captures the necessary entity knowledge.
- Evidence anchors:
  - [abstract]: "We conduct an empirical study of modeling strategies for server-side rescoring of spoken information domain queries using various categories of Language Models (LMs) (N-gram word LMs, sub-word neural LMs)."
  - [section]: "Since rescoring occurs on server, the domain-specific LMs are not subject to the same constraints as the on-device models, and therefore can be larger."
- Break condition: If the domain-specific training data is not representative or lacks sufficient entity coverage, the improvement may not materialize.

### Mechanism 2
- Claim: Combining multiple LM categories through model fusion leads to complementary strengths and further accuracy improvements.
- Mechanism: Different LM architectures capture different aspects of language structure and entity knowledge. By combining them, the rescoring system can leverage the strengths of each model and mitigate individual weaknesses.
- Core assumption: The LM categories have complementary strengths and do not introduce conflicting signals.
- Evidence anchors:
  - [abstract]: "Furthermore, we also show that model fusion of multiple server-side LMs trained from scratch most effectively combines complementary strengths of each model and integrates knowledge learned from domain-specific data to a VA ASR system."
  - [section]: "Inspired by the results obtained as part of answering (RQ1), we are motivated to investigate whether the combination of multiple LM categories can further improve recognition quality."
- Break condition: If the LM categories have overlapping strengths or conflicting signals, the fusion may not provide additional benefits.

### Mechanism 3
- Claim: Sub-word NNLMs trained on domain-specific data outperform out-of-the-box large language models (LLMs) for N-best rescoring.
- Mechanism: Sub-word NNLMs can be trained on domain-specific data to capture entity-specific patterns and relationships, while out-of-the-box LLMs may not have the same level of domain expertise or may be too general.
- Core assumption: The sub-word NNLMs can effectively learn from the domain-specific data and generalize to unseen queries.
- Evidence anchors:
  - [abstract]: "We also perform a comparison between LMs trained on domain data and a GPT-3 variant offered by OpenAI as a baseline."
  - [section]: "We find that any of our domain-specific NGram category and NNLM category rescoring model outperforms the out-of-the-box LLM category model baseline significantly."
- Break condition: If the domain-specific data is insufficient or the sub-word tokenization does not capture entity boundaries well, the advantage over LLMs may diminish.

## Foundational Learning

- Concept: N-gram language models and their limitations
  - Why needed here: Understanding the basics of N-gram models is crucial for grasping the motivation behind using more advanced neural language models for rescoring.
  - Quick check question: What is the main limitation of N-gram models when dealing with entity-rich queries?

- Concept: Neural network language models (NNLMs) and their architectures
  - Why needed here: Familiarity with NNLMs, such as LSTM and Transformer models, is essential for understanding the proposed rescoring approach and its implementation details.
  - Quick check question: What are the key differences between LSTM and Transformer architectures in the context of language modeling?

- Concept: Sub-word tokenization and its benefits
  - Why needed here: Sub-word tokenization is used in the proposed approach to handle rare words and entities effectively, so understanding its benefits and trade-offs is important.
  - Quick check question: How does sub-word tokenization help in handling rare words and entities in language modeling tasks?

## Architecture Onboarding

- Component map:
  On-device ASR -> Server-side rescoring models (N-gram, NNLM, LLM) -> Linear interpolation -> Final hypothesis

- Critical path:
  On-device ASR generates N-best hypotheses
  Server-side rescoring models compute log-likelihood scores for each hypothesis
  Linear interpolation combines scores from on-device ASR and server-side models
  Hypothesis with the highest combined score is selected as the final transcription

- Design tradeoffs:
  Model complexity vs. accuracy: Larger, more complex models may provide better accuracy but require more computational resources
  Domain-specific training vs. generalization: Training on domain-specific data may improve accuracy for entity-rich queries but may limit generalization to other domains

- Failure signatures:
  Degradation in ASR accuracy for tail queries when using only head-focused models
  Overfitting of server-side models to the training data, leading to poor performance on unseen queries
  Sub-word tokenization failing to capture entity boundaries, resulting in incorrect entity recognition

- First 3 experiments:
  1. Compare the WER of on-device ASR with and without N-gram LM rescoring on a held-out validation set
  2. Train and evaluate different NNLM architectures (LSTM, FOFE, Transformer) for rescoring and compare their performance
  3. Investigate the impact of model fusion by combining the best-performing N-gram and NNLM models and evaluating the WER on the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach perform on real-world speech data compared to synthetic data?
- Basis in paper: [inferred] The paper evaluates the approach on synthetic data generated from a neural text-to-speech system.
- Why unresolved: The paper does not provide any evaluation on real-world speech data, which could have different characteristics and challenges compared to synthetic data.
- What evidence would resolve it: Conducting experiments on real-world speech data and comparing the results with those obtained on synthetic data would provide insights into the approach's performance in practical scenarios.

### Open Question 2
- Question: How does the proposed approach scale to larger vocabularies and more diverse entity types?
- Basis in paper: [inferred] The paper uses a limited set of entities (200k) and focuses on media player queries.
- Why unresolved: The paper does not explore the approach's performance on larger vocabularies or more diverse entity types, which could impact its effectiveness in real-world applications.
- What evidence would resolve it: Evaluating the approach on datasets with larger vocabularies and more diverse entity types would provide insights into its scalability and generalization capabilities.

### Open Question 3
- Question: How does the proposed approach compare to other state-of-the-art methods for improving ASR accuracy on entity-rich queries?
- Basis in paper: [explicit] The paper mentions related work on using LMs for N-best rescoring but does not provide a direct comparison with other state-of-the-art methods.
- Why unresolved: The paper does not include a comprehensive comparison with other state-of-the-art methods, which would help in understanding the relative strengths and weaknesses of the proposed approach.
- What evidence would resolve it: Conducting experiments comparing the proposed approach with other state-of-the-art methods on the same datasets would provide insights into its effectiveness and competitiveness.

## Limitations
- The study relies heavily on synthetic data generated using a context-free grammar, raising questions about real-world generalization
- The linear interpolation approach assumes independent and additive contributions from different LM categories
- Limited comparison with out-of-box LLM baseline due to lack of details about model configuration and prompt engineering

## Confidence

**High Confidence**: The core finding that domain-specific server-side LM rescoring improves WER compared to on-device ASR alone is well-supported by experimental results across all query subpopulations (head, torso, and tail). The ablation studies clearly demonstrate the effectiveness of individual LM categories.

**Medium Confidence**: The claim that combining N-gram and NNLM models through linear interpolation provides the best performance is supported but relies on the specific synthetic dataset and evaluation setup. The relative improvements may vary with different data distributions or when applied to other domains beyond media player queries.

**Low Confidence**: The superiority of domain-specific sub-word NNLMs over the out-of-box LLM baseline needs further validation, as the comparison lacks details about the LLM's configuration and the specific challenges it faced with the synthetic dataset. The synthetic nature of the evaluation data limits generalizability.

## Next Checks
1. **Real-world Generalization Test**: Evaluate the proposed rescoring approach on a held-out dataset of real spoken entity-centric queries from actual virtual assistant usage to verify if the synthetic dataset findings hold in production scenarios.

2. **Alternative Fusion Strategy Comparison**: Implement and compare neural network-based fusion approaches (such as feed-forward networks or attention mechanisms) against the linear interpolation method to determine if more sophisticated fusion can further improve WER performance.

3. **Domain Transferability Study**: Apply the trained rescoring models to a different domain (e.g., weather queries or navigation) to assess how well the entity-specific knowledge transfers and identify domain-specific vs. generalizable components of the approach.