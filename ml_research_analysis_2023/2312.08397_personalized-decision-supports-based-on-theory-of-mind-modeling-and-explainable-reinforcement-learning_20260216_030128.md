---
ver: rpa2
title: Personalized Decision Supports based on Theory of Mind Modeling and Explainable
  Reinforcement Learning
arxiv_id: '2312.08397'
source_url: https://arxiv.org/abs/2312.08397
tags:
- learning
- human
- action
- interventions
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel decision support system that combines
  Deep Reinforcement Learning (DRL) with Theory of Mind (ToM) modeling to provide
  personalized and interpretable interventions in sequential decision-making tasks.
  The system uses DRL to recommend expert actions and Bayesian Networks to model human
  mental states, predicting when interventions are necessary.
---

# Personalized Decision Supports based on Theory of Mind Modeling and Explainable Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2312.08397
- **Source URL**: https://arxiv.org/abs/2312.08397
- **Reference count**: 36
- **Primary result**: 86.4% prediction accuracy and 48.1% long-term compliance with personalized decision support

## Executive Summary
This paper introduces a novel decision support system that integrates Deep Reinforcement Learning (DRL) with Theory of Mind (ToM) modeling to provide personalized and interpretable interventions in sequential decision-making tasks. The system uses DRL to recommend expert actions and Bayesian Networks to model human mental states, predicting when interventions are necessary. Counterfactual explanations are generated based on RL feature importance and the human's ToM model structure to provide interpretable rationale for interventions. In a simulated search and rescue task, the system outperformed control baselines, significantly improving participants' task performance.

## Method Summary
The system combines DRL for expert action recommendations with a Bayesian Network-based ToM model to predict human actions and determine intervention necessity. The ToM model infers human mental states from observed behavior and predicts the next action. Interventions are triggered when the predicted human action differs from the DRL recommendation with sufficient confidence. Counterfactual explanations are generated by identifying minimal state perturbations that would change the RL policy's action, then mapping this to salient features in the human's ToM model. The system was evaluated in a simulated search and rescue task, comparing ToM + XRL against XRL only and control conditions.

## Key Results
- The ToM + XRL approach achieved 86.4% prediction accuracy in forecasting human actions
- Long-term compliance rate of 48.1% compared to 22.7% for XRL only condition
- Participants in ToM + XRL condition showed significantly higher task performance than other groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system predicts when interventions are needed by comparing expected human actions to RL recommendations.
- Mechanism: The ToM model infers human mental states from observed behavior, predicts the next action, and triggers intervention when the prediction differs from the RL expert's recommendation.
- Core assumption: Human actions can be reliably predicted from observable state features using a Bayesian Network that updates beliefs online.
- Evidence anchors: [abstract] "Bayesian Networks to model human mental states, predicting when interventions are necessary" [section] "If the confidence level is greater than the predefined threshold and the predicted human action is different from the action recommended by the RL model, an intervention will be issued"

### Mechanism 2
- Claim: Counterfactual explanations are personalized based on the human's ToM model structure.
- Mechanism: The system identifies the minimal perturbation to the current state that would cause the RL policy to choose a different action, then maps this to the most salient feature in the human's ToM model to create an interpretable explanation.
- Core assumption: The feature importance from counterfactual explanations aligns with the causal structure of the human's mental model.
- Evidence anchors: [abstract] "counterfactual explanations based on RL's feature importance and the human's ToM model structure" [section] "we calculate the feature importance of DRL's local decisions by comparing the distance between counterfactual states"

### Mechanism 3
- Claim: The system improves long-term compliance by highlighting overlooked causal relationships in the human's ToM model.
- Mechanism: By identifying which causal relationships the human's mental model misses, the system can target interventions to those specific gaps, leading to structural changes in the human's decision-making over time.
- Core assumption: Humans will incorporate the highlighted causal relationships into their mental models when given appropriate explanations.
- Evidence anchors: [abstract] "participants in the ToM + XRL condition showed significantly higher task performance" and "long-term compliance (48.1%)" [section] "we compared participants' BN model structures before and after a certain intervention to check if the empathized causal relationship appeared"

## Foundational Learning

- Concept: Bayesian Networks and conditional probability
  - Why needed here: The ToM model is built on Bayesian Networks to represent and update beliefs about human mental states
  - Quick check question: How would you calculate P(A|B) in a Bayesian Network where A is a child node of B?

- Concept: Reinforcement Learning basics (MDPs, policies, Q-learning)
  - Why needed here: The system uses DRL to provide expert action recommendations that serve as the intervention baseline
  - Quick check question: What is the Bellman equation and how does it relate to finding optimal policies?

- Concept: Counterfactual explanations and feature importance
  - Why needed here: Explanations are generated by finding minimal state perturbations that change the RL policy's action, then identifying the most important features
  - Quick check question: How would you measure the importance of feature X in a counterfactual explanation?

## Architecture Onboarding

- Component map: ToM Model (Bayesian Network) → Intervention Manager → DRL Expert → Human Interface
- Critical path: Observation → ToM Prediction → Confidence Check → Intervention Decision → Explanation Generation → Human Delivery
- Design tradeoffs: Personalization (ToM model complexity) vs. computational efficiency, explanation quality vs. generation time
- Failure signatures: Low prediction accuracy, low intervention acceptance rate, explanations that don't improve performance
- First 3 experiments:
  1. Test ToM model prediction accuracy on held-out human behavior data
  2. Measure short-term compliance rate with interventions in a controlled environment
  3. Evaluate whether explanations improve long-term compliance by tracking mental model updates

## Open Questions the Paper Calls Out
- What advanced search algorithms beyond Hill Climbing could be used to find optimal Bayesian Network structures for ToM modeling?
- How does the proposed system perform in real-world applications with continuous and high-dimensional state spaces?
- What is the optimal frequency and timing of interventions to maximize long-term compliance without causing intervention fatigue?

## Limitations
- Limited to discrete action spaces and Markov properties in the experimental domain
- Specific implementation details for counterfactual explanation generation remain underspecified
- Effectiveness of the approach in real-world applications with continuous and high-dimensional state spaces is unknown

## Confidence
- **High Confidence**: The basic architecture combining DRL with ToM modeling for intervention prediction, supported by experimental task performance improvements
- **Medium Confidence**: The effectiveness of counterfactual explanations in improving long-term compliance, based on measured outcomes but with limited mechanistic detail
- **Low Confidence**: The specific method for generating ToM-personalized counterfactual explanations, due to lack of implementation details

## Next Checks
1. Replicate prediction accuracy: Test the ToM model's ability to predict human actions on a held-out dataset with different human subjects
2. Validate explanation effectiveness: Conduct ablation studies comparing different explanation methods (ToM-personalized vs. generic counterfactuals) on learning outcomes
3. Test generalizability: Apply the system to a different sequential decision-making task to assess whether the approach transfers beyond the search and rescue domain