---
ver: rpa2
title: Defense Against Adversarial Attacks using Convolutional Auto-Encoders
arxiv_id: '2312.03520'
source_url: https://arxiv.org/abs/2312.03520
tags:
- adversarial
- attacks
- image
- attack
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of deep learning models to
  adversarial attacks, which can cause misclassification or erroneous outputs. The
  authors propose a convolutional autoencoder-based approach to counter adversarial
  perturbations in input images.
---

# Defense Against Adversarial Attacks using Convolutional Auto-Encoders

## Quick Facts
- arXiv ID: 2312.03520
- Source URL: https://arxiv.org/abs/2312.03520
- Reference count: 35
- Primary result: Autoencoder defense increases MNIST/Fashion-MNIST accuracy by 65.61-89.88% against FGSM/PGD attacks

## Executive Summary
This paper proposes a convolutional autoencoder-based defense against adversarial attacks on image classification models. The approach uses a U-shaped autoencoder to reconstruct clean images from adversarial examples, effectively removing perturbations. The model is trained with Gaussian noise injection for robustness and evaluated on MNIST and Fashion-MNIST datasets against FGSM and PGD attacks. Results show significant accuracy improvements over baseline models and state-of-the-art defenses, with added benefits of low inference latency.

## Method Summary
The method employs a U-shaped convolutional autoencoder trained to minimize mean squared error between clean images and their adversarial counterparts. During training, Gaussian noise is added to the latent representation to improve robustness. The autoencoder reconstructs adversarial examples into clean images, which are then classified using a pre-trained VGG-16 model. The defense is evaluated against FGSM and PGD attacks on MNIST and Fashion-MNIST datasets, showing substantial accuracy improvements over baseline models and existing state-of-the-art defenses.

## Key Results
- FGSM attack accuracy increased by 65.61% on MNIST and 59.76% on Fashion-MNIST
- PGD attack accuracy increased by 89.88% on MNIST and 43.49% on Fashion-MNIST
- Outperforms Defense-GAN and PuVAE in both accuracy and inference latency
- Demonstrates effectiveness against both white-box and black-box attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations lie outside the natural data distribution, and the autoencoder learns to project perturbed images back onto this distribution.
- Mechanism: The convolutional autoencoder is trained to minimize reconstruction error between clean images and adversarial inputs. During training, Gaussian noise is added to the latent representation, encouraging the model to learn a robust manifold that maps both clean and perturbed images to a similar clean representation.
- Core assumption: Adversarial perturbations are local and continuous, so they can be mapped back to the clean data manifold by a learned reconstruction function.
- Evidence anchors:
  - [abstract]: "By generating images closely resembling the input images, the proposed methodology aims to restore the model's accuracy."
  - [section]: "The goal of the autoencoder network is to minimise the mean squared error loss [33] between the original unperturbed image and the reconstructed image, which is generated using an adversarial example."
- Break condition: If perturbations are too large (high epsilon), the clean manifold assumption fails and reconstruction error grows.

### Mechanism 2
- Claim: Adding Gaussian noise to the latent space during training increases the autoencoder's robustness to adversarial perturbations.
- Mechanism: Random Gaussian noise is injected after encoding the image. This forces the decoder to learn to reconstruct clean images from noisy latent representations, effectively learning to denoise.
- Core assumption: Adversarial perturbations behave similarly to additive noise in the latent space, so training with noise provides generalization to adversarial examples.
- Evidence anchors:
  - [section]: "While doing so, a random Gaussian noise [34] is added after encoding the image so as to make the model more robust."
- Break condition: If noise distribution during training does not match the statistical properties of adversarial perturbations, the denoising effect degrades.

### Mechanism 3
- Claim: The U-shaped autoencoder architecture with skip connections preserves spatial details lost during downsampling, improving reconstruction quality.
- Mechanism: Convolutional layers progressively downsample the image while capturing high-level features, and skip connections copy earlier spatial information to the decoder, ensuring fine details are retained in reconstruction.
- Core assumption: Skip connections allow the decoder to combine coarse semantic features with fine-grained spatial information, which is necessary to reconstruct clean images from perturbed inputs.
- Evidence anchors:
  - [section]: "An U-shaped convolutional auto-encoder as shown in Fig 5 is used to reconstruct original input from the adversarial image, effectively removing the adversarial perturbations."
- Break condition: If the skip connections fail to transmit spatial information, reconstructions become blurry and adversarial perturbations persist.

## Foundational Learning

- Concept: Adversarial attacks and their gradient-based construction
  - Why needed here: Understanding FGSM and PGD attacks is crucial to grasp why the autoencoder must learn to invert these specific perturbations
  - Quick check question: What is the main difference between FGSM and PGD in terms of how they generate perturbations?
- Concept: Convolutional autoencoders and their loss functions
  - Why needed here: The paper uses MSE loss and Gaussian noise injection; knowing how autoencoders learn reconstruction is key to understanding the defense mechanism
  - Quick check question: Why does adding Gaussian noise to the latent space during training improve robustness?
- Concept: Skip connections in U-Net architectures
  - Why needed here: The U-shape with skip connections preserves spatial information, crucial for high-quality reconstruction from adversarial inputs
  - Quick check question: How do skip connections in a U-Net help maintain spatial detail during reconstruction?

## Architecture Onboarding

- Component map: Adversarial image → Encoder → Noise injection → Decoder → Clean image → Classifier
- Critical path: Adversarial image → Encoder → Noise injection → Decoder → Clean image → Classifier
- Design tradeoffs:
  - Noise injection level vs. reconstruction fidelity
  - Depth of autoencoder vs. inference latency
  - Skip connection design vs. reconstruction detail
- Failure signatures:
  - High MSE between clean and reconstructed images
  - Classifier accuracy on reconstructed images does not improve over adversarial inputs
  - Latent space visualizations show no clear separation between clean and adversarial encodings
- First 3 experiments:
  1. Train autoencoder on clean MNIST, test reconstruction of clean images (baseline)
  2. Train on clean MNIST with Gaussian noise, test reconstruction of clean images (noise robustness)
  3. Train on clean MNIST, test reconstruction of FGSM-perturbed images (adversarial robustness)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed convolutional autoencoder-based defense compare to other state-of-the-art defenses on larger, more complex datasets like CIFAR-10 or ImageNet?
- Basis in paper: [inferred] The paper only evaluates the proposed defense on MNIST and Fashion-MNIST datasets, which are relatively simple and small
- Why unresolved: The paper does not provide any experimental results or analysis of the proposed defense on larger, more complex datasets
- What evidence would resolve it: Experiments comparing the proposed defense to other state-of-the-art defenses on larger, more complex datasets like CIFAR-10 or ImageNet, reporting metrics such as accuracy, inference latency, and robustness against various adversarial attacks

### Open Question 2
- Question: How does the proposed defense perform against black-box adversarial attacks, where the attacker has limited knowledge about the target model?
- Basis in paper: [explicit] The paper mentions that the proposed defense is evaluated against white-box attacks (FGSM and PGD) and assumes the attacker has full knowledge of the target model
- Why unresolved: The paper does not provide any experimental results or analysis of the proposed defense against black-box adversarial attacks
- What evidence would resolve it: Experiments evaluating the proposed defense against black-box adversarial attacks, such as transfer attacks or query-based attacks, and comparing its performance to other state-of-the-art defenses in this setting

### Open Question 3
- Question: How does the proposed defense handle real-world adversarial examples, which may have more complex and subtle perturbations compared to synthetic attacks like FGSM and PGD?
- Basis in paper: [inferred] The paper only evaluates the proposed defense against synthetic adversarial attacks (FGSM and PGD) on MNIST and Fashion-MNIST datasets
- Why unresolved: The paper does not provide any experimental results or analysis of the proposed defense on real-world adversarial examples
- What evidence would resolve it: Experiments evaluating the proposed defense on real-world adversarial examples, such as those found in the wild or generated using more sophisticated attack methods, and comparing its performance to other state-of-the-art defenses in this setting

## Limitations
- Missing architectural details (layer counts, filter sizes, latent dimension) necessary for reproduction
- Lack of ablation studies to quantify contribution of noise injection and architectural choices
- No testing against adaptive attacks that might circumvent the autoencoder defense
- Missing comparison context for reported latency improvements

## Confidence

- **Medium Confidence** in accuracy improvement claims: The significant accuracy gains (65.61% and 89.88% increases) are reported but depend on unspecified model architecture and hyperparameters that affect generalization
- **Low Confidence** in defense robustness: Without testing against adaptive attacks or white-box scenarios where the adversary knows about the autoencoder, the claimed robustness is uncertain
- **Medium Confidence** in mechanism claims: While the theoretical justification (mapping perturbations back to data manifold) is plausible, the lack of latent space visualizations or perturbation analysis weakens confidence in the actual mechanism

## Next Checks

1. Reconstruct clean images using the autoencoder (without adversarial examples) to establish baseline reconstruction quality and verify the network learns proper denoising before testing on adversarial examples
2. Perform ablation studies by training autoencoders with varying noise injection levels and architectural configurations to quantify the contribution of each design choice to defense performance
3. Test the defense against adaptive attacks (e.g., white-box attacks targeting both the classifier and autoencoder) to evaluate robustness against informed adversaries