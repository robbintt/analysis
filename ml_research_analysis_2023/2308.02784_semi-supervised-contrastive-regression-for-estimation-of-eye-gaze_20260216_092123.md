---
ver: rpa2
title: Semi-supervised Contrastive Regression for Estimation of Eye Gaze
arxiv_id: '2308.02784'
source_url: https://arxiv.org/abs/2308.02784
tags:
- gaze
- images
- estimation
- contrastive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a semi-supervised contrastive regression framework
  for eye gaze estimation, addressing the challenge of generalizing performance with
  limited labeled data. The method introduces a new contrastive loss that simultaneously
  maximizes similarity agreement between similar images and reduces redundancy in
  embedding representations.
---

# Semi-supervised Contrastive Regression for Estimation of Eye Gaze

## Quick Facts
- arXiv ID: 2308.02784
- Source URL: https://arxiv.org/abs/2308.02784
- Reference count: 21
- Key outcome: Proposed model achieves 3.212° mean angular error on ETH-XGaze, outperforming SimCLR (9.175°) and Barlow Twins (5.854°)

## Executive Summary
This paper introduces a semi-supervised contrastive regression framework for eye gaze estimation that leverages unlabeled data to learn robust image representations before fine-tuning on a small labeled set. The method employs dilated convolutions with varying dilation rates to capture both local and global spatial dependencies efficiently. A novel contrastive loss function combines invariance maximization with redundancy minimization, outperforming state-of-the-art contrastive learning techniques on the ETH-XGaze dataset.

## Method Summary
The framework operates in two stages: pre-training and fine-tuning. During pre-training, an encoder with dilated convolutions learns image representations from unlabeled data using a contrastive loss that maximizes similarity agreement between augmented views while minimizing redundancy in embeddings. The fine-tuning stage maps these learned embeddings to gaze directions using a small labeled dataset. The encoder uses varying dilation rates to capture multi-scale spatial features without increasing kernel size, and the loss function combines NT-Xent similarity maximization with Barlow Twins-style redundancy reduction.

## Key Results
- Achieves 3.212° mean angular error on ETH-XGaze dataset
- Outperforms SimCLR baseline (9.175°) and Barlow Twins (5.854°)
- Ablation study confirms effectiveness of proposed loss function and redundancy minimization term

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-stage training (pre-training → fine-tuning) enables robust feature learning from unlabeled data before regression
- Core assumption: Unlabeled dataset is representative of labeled dataset's distribution
- Evidence anchors: Abstract states framework finds generalized solution for unseen face images; Fig.2 shows pre-training and fine-tuning stages
- Break condition: If unlabeled data lacks diversity in head pose or illumination, learned encoder may not generalize

### Mechanism 2
- Claim: Dilated convolutions with varying dilation rates enable efficient multi-scale spatial feature extraction
- Core assumption: Gaze-relevant features exist at multiple spatial scales
- Evidence anchors: Section states larger kernel size needed for global spatial dependency, hence dilated convolutions used
- Break condition: Poorly chosen dilation rates may skip important features or cause over-smoothing

### Mechanism 3
- Claim: Proposed contrastive loss balances invariance and redundancy reduction for more discriminative features
- Core assumption: Redundancy in embedding space directly harms downstream regression performance
- Evidence anchors: Section describes using Barlow Twins' cross-correlation matrix to analyze correlation between positive and negative images
- Break condition: If redundancy coefficient γ is too high, model may lose useful shared information between positive pairs

## Foundational Learning

- Concept: Contrastive learning (SimCLR, Barlow Twins)
  - Why needed here: Framework builds on SimCLR-style pre-training but modifies loss; understanding NT-Xent and projection heads is essential
  - Quick check question: What is the role of the projection head in SimCLR, and why is it separate from the encoder?

- Concept: Semi-supervised learning pipeline
  - Why needed here: Method uses unlabeled data for pre-training and small labeled set for fine-tuning; understanding this split is key
  - Quick check question: Why is pre-training done without labels, and what advantage does that give over training only on labeled subset?

- Concept: Dilated convolutions
  - Why needed here: Encoder uses dilated convolutions with multiple dilation rates to capture multi-scale features
  - Quick check question: How does increasing dilation rate affect the receptive field and density of sampled pixels?

## Architecture Onboarding

- Component map: Data pipeline (ETH-XGaze images) → Encoder (dilated conv blocks) → Projection head (MLP) → Pre-training loss → Fine-tuning head (regression network) → Gaze direction prediction

- Critical path: 1) Pre-training: Build encoder with contrastive loss on unlabeled images 2) Fine-tuning: Freeze encoder, train regression head on labeled subset 3) Evaluate: Compute mean angular error on validation set

- Design tradeoffs: Dilated convs vs larger kernels (lower parameters, efficient multi-scale features vs risk of gridding artifacts); Global average pooling vs flatten (fewer parameters vs better performance); Redundancy term weight γ (balances pulling positives vs decorrelating features)

- Failure signatures: High MAE but stable training (check augmentation diversity, loss balance); Training loss drops but validation MAE high (overfitting, reduce capacity); Extremely low training loss but validation high (label noise, check augmentation strength)

- First 3 experiments: 1) Train baseline SimCLR on ETH-XGaze unlabeled set, then fine-tune regression head; compare MAE to 9.175° 2) Replace encoder with standard ResNet (no dilation), keep same loss; measure impact on MAE 3) Vary redundancy coefficient γ {0.001, 0.01, 0.1, 1.0}; plot MAE vs γ

## Open Questions the Paper Calls Out

- How does the proposed contrastive loss perform compared to other advanced loss functions (e.g., InfoNCE, Triplet loss) for gaze estimation?
- What is the impact of varying dilation rates in the encoder on model performance and generalization?
- How well does the model generalize to cross-dataset scenarios beyond ETH-XGaze?

## Limitations
- Lack of explicit hyperparameter values for redundancy coefficient γ and exact dilated convolution configurations
- No comprehensive ablation study on relative contribution of each component to reported performance
- Limited cross-dataset evaluation to assess real-world applicability

## Confidence
- Confidence in novelty of contrastive loss design: Medium
- Confidence in dilated convolution benefits: Low
- Confidence in dual-stage training pipeline: High

## Next Checks
1. Train same encoder architecture without dilated convolutions (replace with standard convs) and compare MAE to assess architectural impact
2. Vary redundancy coefficient γ across range (e.g., 0.001, 0.01, 0.1, 1.0) and plot MAE vs γ to identify optimal value and confirm necessity of redundancy term
3. Perform ablation on augmentation strength (weak vs strong) during pre-training to verify learned representations are robust to data diversity