---
ver: rpa2
title: 'MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models
  for X-ray Images of Multiple Body Parts'
arxiv_id: '2310.02000'
source_url: https://arxiv.org/abs/2310.02000
tags:
- learning
- muscle
- x-ray
- tasks
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUSCLE, a novel self-supervised continual
  learning pipeline that pre-trains deep neural networks using multiple X-ray image
  datasets collected from different body parts for multiple X-ray analysis tasks.
  MUSCLE proposes multi-dataset momentum contrastive learning (MD-MoCo) and multi-task
  continual learning to tackle the data heterogeneity, over-fitting, and catastrophic
  forgetting problems in pre-training.
---

# MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts

## Quick Facts
- arXiv ID: 2310.02000
- Source URL: https://arxiv.org/abs/2310.02000
- Reference count: 31
- MUSCLE outperforms ImageNet/MoCo pre-trained backbones on 9 real-world X-ray datasets for pneumonia classification, skeletal abnormality classification, lung segmentation, and tuberculosis detection tasks.

## Executive Summary
This paper introduces MUSCLE, a novel self-supervised continual learning pipeline that pre-trains deep neural networks using multiple X-ray image datasets collected from different body parts for multiple X-ray analysis tasks. MUSCLE addresses three key challenges in medical image pre-training: data heterogeneity across datasets, overfitting to specific tasks, and catastrophic forgetting during multi-task learning. The method combines multi-dataset momentum contrastive learning (MD-MoCo) with a cyclic multi-task continual learning approach that uses task-specific heads and L2-SP regularization to preserve knowledge across tasks.

## Method Summary
MUSCLE consists of three main stages: (1) Multi-Dataset Momentum Contrastive Learning (MD-MoCo) that aggregates 9 X-ray datasets and pre-trains a backbone network using MoCo-based contrastive learning, (2) Multi-Task Continual Learning that fine-tunes the pre-trained backbone across 4 medical tasks (pneumonia classification, skeletal abnormality classification, lung segmentation, TB detection) using a cyclic and reshuffled learning schedule with L2-SP regularization to prevent catastrophic forgetting, and (3) Task-specific fine-tuning where the pre-trained backbone is fine-tuned independently on each downstream task using standard optimization techniques. The method uses ResNet-18 and ResNet-50 as backbone architectures and evaluates performance across multiple metrics including accuracy, AUC, Dice Similarity Coefficient, and mAP.

## Key Results
- MUSCLE outperforms ImageNet and MoCo pre-trained backbones on all 9 X-ray datasets
- ResNet-50 backbone consistently outperforms ResNet-18 across all tasks
- MUSCLE achieves state-of-the-art results on pneumonia classification, skeletal abnormality classification, lung segmentation, and tuberculosis detection tasks
- The cyclic and reshuffled learning schedule effectively prevents overfitting to any single task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MUSCLE improves representation learning by combining multi-dataset contrastive learning with task-specific continual learning
- Mechanism: The method first uses MoCo-based contrastive learning across multiple X-ray datasets to learn general visual features, then applies continual learning with alternating task-specific heads to refine these features for specific tasks while avoiding catastrophic forgetting
- Core assumption: General features learned from diverse X-ray datasets can be effectively refined for specific medical tasks without losing previously learned knowledge
- Evidence anchors:
  - [abstract] "MUSCLE proposes multi-dataset momentum contrastive learning (MD-MoCo) and multi-task continual learning to tackle the data heterogeneity, over-fitting, and catastrophic forgetting problems"
  - [section 2.1] "To pre-train the backbone with multiple datasets, MUSCLE collects and aggregates nine X-ray image datasets... with pre-processing... then adopts a MoCo-based SSL algorithm [11] to pre-train the backbone networks"
  - [corpus] Weak evidence - the related papers focus on different aspects (Siamese autoencoders, MRI reconstruction) without directly supporting this specific mechanism
- Break condition: If the multi-dataset pre-training doesn't provide useful general features, or if the continual learning stage causes significant catastrophic forgetting

### Mechanism 2
- Claim: The cyclic and reshuffled learning schedule prevents overfitting to any single task
- Mechanism: By splitting continual learning into 10 rounds where each round iterates through the 4 learning tasks in a different order, the model cannot overfit to the first task it encounters
- Core assumption: Task ordering affects learning outcomes, and randomization prevents bias toward specific tasks
- Evidence anchors:
  - [section 2.2] "MUSCLE splits the continual learning procedure into 10 rounds of learning process, where each round of learning process iterates the 4 learning tasks one-by-one... Furthermore, to avoid overfitting to any task, MUSCLE reshuffle the order of tasks in every round"
  - [section 2.2] "adopts Cosine annealing learning rate schedule"
  - [corpus] Weak evidence - no direct support in related papers for this specific cyclic reshuffling approach
- Break condition: If the reshuffling causes insufficient training on critical tasks, or if the learning rate schedule doesn't properly balance convergence

### Mechanism 3
- Claim: Cross-task memorization with L2-SP regularization prevents catastrophic forgetting
- Mechanism: The model constrains new learning to stay close to previously learned weights using L2 distance regularization between current weights and pre-trained weights from previous iterations
- Core assumption: Weight similarity between consecutive learning iterations preserves previously learned knowledge
- Evidence anchors:
  - [section 2.2] "To solve the problem, MUSCLE leverages a knowledge transfer regularization derived from L2-SP [15]. In each iterate of learning task, given the pre-trained model obtained from previous iterates, MUSCLE sets the pre-trained weights as w0_S and trains the backbone using the following loss"
  - [section 2.2] The explicit equation showing the L2 regularization term
  - [corpus] No direct evidence - related papers don't discuss L2-SP regularization for catastrophic forgetting
- Break condition: If the regularization strength α is not properly tuned, leading to either under-learning new tasks or over-forgetting old tasks

## Foundational Learning

- Concept: Contrastive learning with momentum encoders
  - Why needed here: MoCo's momentum encoder helps maintain a consistent target distribution for contrastive learning, which is crucial for stable pre-training across diverse X-ray datasets
  - Quick check question: How does the momentum update in MoCo differ from standard contrastive learning approaches like SimCLR?

- Concept: Continual learning with task-specific heads
  - Why needed here: Different medical tasks (classification, segmentation, detection) require different output heads, and continual learning allows sharing a common backbone while adapting to each task
  - Quick check question: Why does MUSCLE use different heads (FC, DeepLab-V3, FasterRCNN) for different task types?

- Concept: Catastrophic forgetting and regularization strategies
  - Why needed here: When fine-tuning a pre-trained model on new tasks, the model tends to overwrite previously learned features; regularization helps preserve useful representations
  - Quick check question: What is the difference between L2-SP regularization and standard weight decay?

## Architecture Onboarding

- Component map: Input images → Pre-processing (resize, normalize) → MD-MoCo encoder (ResNet backbone) → Contrastive loss → Task-specific heads (FC/DeepLab-V3/FasterRCNN) → Task losses → Continual learning regularization
- Critical path: Multi-dataset pre-training → Multi-task continual learning → Task-specific fine-tuning
- Design tradeoffs: Larger backbone (ResNet-50) provides better performance but requires more computational resources; cyclic learning prevents overfitting but may slow convergence
- Failure signatures: Performance degradation on specific tasks despite good overall metrics; inconsistent results across different random seeds; validation loss increasing during continual learning phase
- First 3 experiments:
  1. Run MD-MoCo pre-training on all 9 datasets and evaluate feature quality using linear probe evaluation
  2. Implement continual learning with reshuffled task order and compare against fixed order
  3. Test different values of the regularization parameter α to find optimal balance between learning new tasks and preserving old knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different data augmentation strategies impact MUSCLE's performance on X-ray images?
- Basis in paper: [explicit] The paper mentions that MUSCLE disables random cropping, gaussian blurring, and color/gray-scale jittering to preserve semantic information for medical images, implying that these augmentations might negatively impact performance.
- Why unresolved: The paper does not provide a systematic comparison of different augmentation strategies on MUSCLE's performance. It only mentions the specific augmentations that were disabled.
- What evidence would resolve it: Experiments comparing MUSCLE's performance with different data augmentation strategies, including those that were disabled, would provide insights into their impact on X-ray image analysis.

### Open Question 2
- Question: How does MUSCLE's performance compare to task-specific models optimized for each individual medical imaging task?
- Basis in paper: [inferred] The paper acknowledges that MUSCLE is a "proof-of-concept" solution and has not been optimized for any single medical imaging task. It mentions that the overall performance of MUSCLE is still better than many recent works based on the same datasets, but this does not directly address the comparison with task-specific models.
- Why unresolved: The paper does not provide a direct comparison between MUSCLE and task-specific models optimized for each individual medical imaging task. It only compares MUSCLE to other pre-trained models and baseline algorithms.
- What evidence would resolve it: Experiments comparing MUSCLE's performance to task-specific models optimized for each individual medical imaging task, such as U-Net for segmentation, would provide insights into the trade-offs between multi-task learning and task-specific optimization.

### Open Question 3
- Question: How does the choice of backbone architecture impact MUSCLE's performance on X-ray image analysis?
- Basis in paper: [explicit] The paper evaluates MUSCLE using both ResNet-18 and ResNet-50 as backbones and reports their performance on various tasks. However, it does not explore other backbone architectures or provide insights into how the choice of backbone affects MUSCLE's performance.
- Why unresolved: The paper only evaluates MUSCLE using ResNet-18 and ResNet-50 as backbones, and does not explore other architectures or provide insights into how the choice of backbone affects MUSCLE's performance.
- What evidence would resolve it: Experiments comparing MUSCLE's performance using different backbone architectures, such as DenseNet, EfficientNet, or Vision Transformers, would provide insights into the impact of backbone choice on X-ray image analysis.

## Limitations

- Limited evaluation of backbone architecture choices beyond ResNet-18 and ResNet-50
- No systematic comparison of different data augmentation strategies for X-ray images
- Lack of direct comparison with task-specific models optimized for individual medical imaging tasks

## Confidence

- **High confidence**: The overall pipeline architecture and the need for specialized pre-training on X-ray data
- **Medium confidence**: The specific implementation of MD-MoCo and its superiority over ImageNet pre-training
- **Medium confidence**: The effectiveness of cyclic learning schedules for preventing overfitting in multi-task scenarios

## Next Checks

1. Test MD-MoCo performance sensitivity to different contrastive learning temperature values (0.1-0.5 range) and momentum update rates
2. Compare cyclic reshuffling against fixed task order and curriculum learning approaches to isolate the benefit
3. Conduct ablation studies on the L2-SP regularization weight α to find optimal values and demonstrate its necessity for catastrophic forgetting prevention