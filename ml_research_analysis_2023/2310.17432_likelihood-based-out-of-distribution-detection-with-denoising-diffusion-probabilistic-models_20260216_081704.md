---
ver: rpa2
title: Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic
  Models
arxiv_id: '2310.17432'
source_url: https://arxiv.org/abs/2310.17432
tags:
- detection
- likelihood
- sample
- ddpms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a likelihood-based OOD detection method using
  denoising diffusion probabilistic models (DDPMs). The authors identify that image
  complexity can corrupt the ELBO score as an OOD detection metric in DDPMs.
---

# Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2310.17432
- Source URL: https://arxiv.org/abs/2310.17432
- Reference count: 40
- Primary result: CCLR achieves AUROC of 1.000 on FashionMNIST/MNIST and 0.964 on CIFAR10/SVHN

## Executive Summary
This paper introduces a likelihood-based out-of-distribution (OOD) detection method using denoising diffusion probabilistic models (DDPMs). The authors identify that image complexity creates bias in ELBO scores at low noise levels, which can corrupt OOD detection performance. They propose the Complexity Corrected Likelihood Ratio (CCLR) as a solution, which compares ELBO contributions from low noise levels against the total ELBO. Experiments show CCLR achieves near-perfect AUROC scores on FashionMNIST/MNIST and strong performance on CIFAR10/SVHN, outperforming existing DDPM-based OOD detection methods.

## Method Summary
The method uses a DDPM trained with L1 loss and cosine noising schedule. For OOD detection, the CCLR score is computed by comparing the ELBO contributions from low noise levels (controlled by threshold k) against the full ELBO (up to T timesteps). The CCLR is defined as the difference between partial and full ELBOs: CCLK/T = L<k_θ − L<T_θ. The model is trained for 100 epochs with learning rate 2.0e-5 on 32x32 resized datasets. OOD detection performance is evaluated using AUROC scores on held-out test sets.

## Key Results
- CCLR achieves perfect AUROC of 1.000 on FashionMNIST/MNIST dataset pair
- CCLR achieves AUROC of 0.964 on CIFAR10/SVHN dataset pair
- Outperforms current DDPM-based OOD detection methods
- Competitive with other likelihood-based OOD detection approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complexity bias in DDPMs causes ELBO contributions from low noise levels to be disproportionately influenced by image complexity, harming OOD detection.
- Mechanism: When training DDPMs, low t-value noise predictions are easier for simple images, leading to lower losses and thus higher marginal log-likelihood estimates. This makes complex ID samples appear less likely than simple OOD samples.
- Core assumption: The ELBO loss decomposition across t-values directly reflects image complexity in a way that corrupts OOD detection performance.
- Evidence anchors:
  - [abstract] "identify that image complexity can corrupt the ELBO score as an OOD detection metric in DDPMs"
  - [section 2.2] "input sample complexity dramatically affects the ELBO contributions from low noising levels in DDPMs"
  - [corpus] Weak evidence - corpus lacks direct citations about complexity bias in DDPMs specifically

### Mechanism 2
- Claim: The CCLR score corrects for complexity bias by comparing low-t ELBO contributions against full ELBO.
- Mechanism: CCLR subtracts the full ELBO (L<T_θ) from the partial ELBO capturing low-t values (L<k_θ), creating a likelihood ratio that emphasizes high-level semantic features while de-emphasizing low-level complexity.
- Core assumption: The difference between partial and full ELBO scores provides a meaningful correction factor for complexity bias.
- Evidence anchors:
  - [abstract] "They propose a likelihood ratio called the Complexity Corrected Likelihood Ratio (CCLR)"
  - [section 2.3] "we define this ratio to be the CCLR, CCLK/T = L<k_θ − L<T_θ"
  - [corpus] Weak evidence - no corpus papers directly discussing CCLR or similar correction approaches

### Mechanism 3
- Claim: Using L1 loss instead of L2 during training improves OOD detection by reducing sample diversity and hallucinations.
- Mechanism: L1 loss leads to tighter learned distributions over training data, which is critical for distinguishing OOD samples that fall outside this distribution.
- Core assumption: Sample diversity reduction through L1 loss improves the discriminative power of the model for OOD detection.
- Evidence anchors:
  - [section 2.3] "we use an L1 rather than an L2 loss... This is important for OOD detection, as a tightly learned distribution over the training data is critical"
  - [corpus] Weak evidence - corpus lacks direct citations about L1 vs L2 loss effects on OOD detection

## Foundational Learning

- Concept: Evidence Lower Bound (ELBO) optimization in generative models
  - Why needed here: The entire CCLR method relies on decomposing and comparing ELBO scores at different noise levels
  - Quick check question: What does the ELBO represent in terms of model likelihood, and why is it called a "lower bound"?

- Concept: Diffusion model training dynamics
  - Why needed here: Understanding how noise schedules and loss functions affect the learned distribution is critical for implementing CCLR correctly
  - Quick check question: How does the noise schedule (linear vs cosine) affect the information distribution across timesteps?

- Concept: Out-of-distribution detection metrics
  - Why needed here: AUROC is used as the primary evaluation metric, requiring understanding of its interpretation and limitations
  - Quick check question: What does an AUROC score of 1.000 mean in practical terms for OOD detection performance?

## Architecture Onboarding

- Component map: DDPM with noise prediction network → ELBO computation at multiple t-levels → CCLR calculation → OOD score output
- Critical path: Sample → batch expansion → diffusion through multiple timesteps → ELBO calculation at k and T → CCLR computation → classification
- Design tradeoffs: L1 vs L2 loss (diversity vs stability), k/T threshold selection (sensitivity vs specificity), batch size for inference (coverage vs computation)
- Failure signatures: Poor AUROC despite good reconstruction quality, CCLR scores not separating ID vs OOD distributions, sensitivity to k/T threshold selection
- First 3 experiments:
  1. Verify complexity bias exists by comparing ELBO scores for complex vs simple images within ID dataset
  2. Test CCLR with different k/T values to find optimal threshold for dataset pair
  3. Compare CCLR performance against standard ELBO on held-out OOD test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance of the CCLR method be further improved by incorporating recent architectural improvements to DDPMs, such as those proposed by Hoogeboom et al. [11] or Salimans and Ho [30]?
- Basis in paper: [explicit] The authors mention that "there have been improvements that have been proposed for DDPMs that could benefit our OOD detection method" and list several potential improvements, including architecture improvements for the noise prediction network.
- Why unresolved: The authors used a vanilla DDPM implementation as a proof of concept and did not explore these potential improvements in their experiments.
- What evidence would resolve it: Conducting experiments using improved DDPM architectures and comparing the OOD detection performance with the vanilla DDPM results presented in the paper.

### Open Question 2
- Question: How does the CCLR method perform on other dataset pairs beyond FashionMNIST/MNIST and CIFAR10/SVHN, especially those with more subtle differences in complexity or domain?
- Basis in paper: [inferred] The paper only evaluates the CCLR method on two specific dataset pairs. The performance on other dataset pairs, particularly those with more challenging OOD detection scenarios, is unknown.
- Why unresolved: The authors limited their experiments to two dataset pairs and did not explore the method's performance on a wider range of datasets.
- What evidence would resolve it: Evaluating the CCLR method on a diverse set of dataset pairs with varying degrees of complexity and domain differences, and comparing the results to other OOD detection methods.

### Open Question 3
- Question: Is there an optimal choice for the noising threshold k in the CCLR method, or does it depend on the specific characteristics of the dataset pair being used?
- Basis in paper: [explicit] The authors present results for various k/T values in Table 1, showing that different values lead to different AUROC scores for the two dataset pairs. However, they do not provide a clear guideline for choosing the optimal k value.
- Why unresolved: The authors did not investigate the relationship between the choice of k and the performance of the CCLR method in detail.
- What evidence would resolve it: Conducting a systematic study to determine the impact of different k values on the CCLR method's performance across various dataset pairs, and deriving guidelines for selecting the optimal k value based on dataset characteristics.

## Limitations
- Complexity bias assumption not extensively validated across diverse datasets
- Near-perfect performance on FashionMNIST/MNIST may reflect dataset simplicity rather than general robustness
- Limited exploration of L1 vs L2 loss effects on OOD detection performance

## Confidence

- **High confidence**: The CCLR methodology is well-defined and reproducible. The empirical results on the tested dataset pairs are clearly reported and follow standard OOD detection evaluation practices.
- **Medium confidence**: The claim that image complexity specifically corrupts ELBO-based OOD detection in DDPMs is plausible but under-validated. The superiority of CCLR over baseline ELBO is demonstrated but could be dataset-specific.
- **Low confidence**: The assertion that L1 loss is critical for OOD detection performance lacks strong empirical justification. The paper doesn't adequately explore whether this is truly causal or just coincidental.

## Next Checks

1. **Cross-dataset complexity validation**: Systematically test CCLR on more diverse dataset pairs (e.g., CIFAR100 vs CIFAR10, ImageNet vs CIFAR10) to verify that complexity bias is a general phenomenon and not specific to the chosen pairs.

2. **Ablation study on loss function**: Conduct controlled experiments comparing L1 vs L2 loss effects on OOD detection performance while holding all other variables constant, including reconstruction quality metrics.

3. **Threshold sensitivity analysis**: Perform a detailed analysis of how k/T threshold selection affects CCLR performance across different dataset complexity levels, and develop principled guidance for threshold selection rather than empirical testing.