---
ver: rpa2
title: Geometric structure of shallow neural networks and constructive ${\mathcal
  L}^2$ cost minimization
arxiv_id: '2309.10370'
source_url: https://arxiv.org/abs/2309.10370
tags:
- function
- where
- cost
- which
- shallow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the geometric structure of shallow ReLU
  networks through explicit construction of upper bounds on the L2 cost function,
  avoiding gradient descent. The key method is to rotate and shift training data so
  that the activation function acts component-wise, enabling maximal rank reduction
  and least-squares fitting to outputs.
---

# Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization

## Quick Facts
- arXiv ID: 2309.10370
- Source URL: https://arxiv.org/abs/2309.10370
- Reference count: 10
- Key outcome: Constructs upper bounds on L2 cost function for shallow ReLU networks through explicit diagonalization and least-squares fitting, avoiding gradient descent

## Executive Summary
This paper investigates the geometric structure of shallow ReLU networks through explicit construction of upper bounds on the L2 cost function. The authors avoid gradient descent by rotating and shifting training data so that the activation function acts component-wise, enabling maximal rank reduction and least-squares fitting to outputs. For underparameterized networks (Q ≤ M), they prove an upper bound of O(δP) on the minimal cost, where δP measures the signal-to-noise ratio of training data.

## Method Summary
The method constructs a shallow ReLU network by first computing the Penrose inverse of training data averages Xred0, then diagonalizing the projector P onto their span using an orthogonal matrix R. The weights are set as W1=R (rotation) and W2=Y*Pen[Xred0]*P, with shifts b1 and b2 chosen to ensure non-negativity and proper centering. This construction enables maximal rank reduction through the ReLU activation and provides a transparent geometric interpretation of input-output matching.

## Key Results
- Proves upper bound O(δP) on minimal L2 cost for underparameterized networks (Q ≤ M)
- Constructs a degenerate local minimum in the special case M=Q with improvement O(δ²P)
- Shows the constructively trained network metrizes the Q-dimensional subspace spanned by training data averages
- Provides explicit weights and shifts avoiding gradient descent entirely

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The constructive upper bound O(δP) is achieved by diagonalizing the projector P onto the span of training data averages, enabling maximal rank reduction via the ReLU activation.
- **Mechanism**: Orthogonal rotation R aligns the range of P with coordinate axes, decoupling σ's action. Shifting b1=β1PRuM ensures all components are non-negative so σ acts as identity on range(P), while P⊥b1=δP⊥uM pushes insignificant information into the kernel, eliminating M-Q rows.
- **Core assumption**: The averages {x0,j} are linearly independent so that rank(X0red)=Q and the Penrose inverse exists.
- **Evidence anchors**:
  - [abstract] "We consider an L2 cost function, input space RM, output space RQ with Q≤M... We prove an upper bound on the minimum of the cost function of order O(δP)"
  - [section 3.1] "We can diagonalize it with an orthogonal matrix R∈O(M)... This is important for compatibility with the fact that σ acts component-wise"
  - [corpus] Weak - no direct neighbor papers cite this specific diagonalization construction.
- **Break condition**: If the averages are not linearly independent (rank(X0red)<Q), the Penrose inverse fails and the construction breaks.

### Mechanism 2
- **Claim**: The constructively trained network metrizes the Q-dimensional subspace range(P), enabling transparent input-output matching via nearest-neighbor in this metric.
- **Mechanism**: After rotation and truncation, the network reduces to W2RPX0 with W2=YPen[X0red]P. This creates a metric d_W2(x,y)=|W2P(x-y)| on range(P), where matching an input x reduces to finding j* minimizing d_W2(Px,x0,j).
- **Core assumption**: The matrix W2P has full rank Q so that d_W2 is a non-degenerate metric.
- **Evidence anchors**:
  - [section 3.3] "The constructively trained shallow network thus obtained matches a non-training input x∈RM with an output vector yj* by splitting x=Px+P⊥x, and by determining which of the average training input vectors x0,j is closest to Px in the d_W2 metric"
  - [abstract] "we show that it metrizes the Q-dimensional subspace in the input space RM spanned by x0,j, j=1,...,Q"
  - [corpus] Weak - neighboring papers discuss expressivity but not this specific metric interpretation.
- **Break condition**: If W2P loses rank (e.g., due to numerical instability), the metric becomes degenerate and nearest-neighbor matching fails.

### Mechanism 3
- **Claim**: In the special case M=Q, the exact degenerate local minimum differs from the upper bound by O(δ²P), providing a sharp characterization.
- **Mechanism**: With M=Q, P becomes identity and X0red is invertible. The weighted cost function CN[Wi,bi] can be minimized exactly by solving a least-squares problem with P=N−1XTT(X0N−1XT0)−1X0, yielding CN[W*i,b*i]=∥YextP⊥∥L2N with relative error O(δ²P).
- **Core assumption**: The truncation map τW1,b1(X0) is rank-preserving so that minimization in W2,b2 is well-defined.
- **Evidence anchors**:
  - [section 3.2] "we explicitly determine an exact degenerate local minimum of the cost function... the sharp value differs from the upper bound obtained for Q≤M by a relative error O(δ²P)"
  - [theorem 3.2] "CN[W*i,b*i]=∥YextP⊥∥L2N=(1−C0δ²P)∥Y(X0red)−1∆X0∥L2N"
  - [corpus] Weak - neighboring papers discuss approximation bounds but not this exact degenerate minimum.
- **Break condition**: If truncation becomes rank-reducing (τW1,b1(X0) loses rank), the least-squares solution fails and the sharp value cannot be characterized.

## Foundational Learning

- **Concept**: Orthogonal diagonalization of symmetric projectors
  - Why needed here: To align the range of P with coordinate axes so that σ's component-wise action decouples, enabling maximal rank reduction
  - Quick check question: If P has eigenvalues 1 (Q times) and 0 (M-Q times), what form does the diagonalizing matrix R take?

- **Concept**: Penrose (Moore-Penrose) inverse for rank-deficient matrices
  - Why needed here: To solve W2RPX0red=Y when rank(RPX0red)=Q<M, providing the least-squares solution
  - Quick check question: For Xred∈RM×Q with rank Q, how does Pen[Xred] relate to (XredT Xred)−1XredT?

- **Concept**: L2 Schatten class/Hilbert-Schmidt norm for matrices
  - Why needed here: The cost function uses ∥A∥L2=√(Tr(AAT)), which is the Frobenius norm, enabling the least-squares formulation
  - Quick check question: For a matrix A∈RQ×N, how does ∥A∥L2 relate to the Euclidean norms of its entries?

## Architecture Onboarding

- **Component map**: Input RM → σ(W1x+b1) with W1=R, b1 split → W2x'+b2 with W2=YPen[Xred0]PRT

- **Critical path**: (1) Compute Xred0 and its Penrose inverse, (2) Diagonalize P to get R, (3) Set W1=R, b1 as above, (4) Compute W2=YPen[Xred0]PRT, b2=−W2PRb1, (5) Evaluate upper bound ∥Y∥op δP

- **Design tradeoffs**: (1) Choosing β1≥2max|x0,j,i| ensures non-negativity but may amplify numerical errors, (2) The rotation R is not unique - any element of O(M) preserving range(P) works, (3) δP measures signal-to-noise ratio - larger δP means looser bound

- **Failure signatures**: (1) If rank(Xred0)<Q, Penrose inverse fails, (2) If RPX0+PRB1 has negative components, σ truncates prematurely, (3) If W2P loses rank, the metric interpretation fails

- **First 3 experiments**:
  1. Generate synthetic data with Q<M, compute Xred0, P, R, verify that σ(RPX0+PRB1)=RPX0+PRB1 holds
  2. Test the metric interpretation by generating random x∈RM and verifying that j*=argminj d_W2(Px,x0,j) matches the network's output
  3. For M=Q case, compute the exact degenerate minimum and verify the O(δ²P) improvement over the upper bound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the geometric structure of shallow networks generalize to deep learning networks with multiple hidden layers?
- Basis in paper: The authors explicitly state they will address an extension of their analysis to multilayer DL networks in a separate work [3].
- Why unresolved: The paper only analyzes shallow networks with one hidden layer, and the extension to deep networks is left for future work.
- What evidence would resolve it: A rigorous mathematical analysis proving similar upper bounds and geometric interpretations for deep networks with multiple hidden layers.

### Open Question 2
- Question: What is the relationship between the degenerate local minimum found in the M=Q case and the global minimum of the cost function?
- Basis in paper: The authors construct a degenerate local minimum in the M=Q case but only comment on the characterization of the global minimum without providing a definitive answer.
- Why unresolved: While the authors show their construction is a local minimum, they don't prove it's the global minimum or characterize other potential global minima.
- What evidence would resolve it: A proof that the constructed degenerate local minimum is indeed the global minimum, or a counterexample showing a different global minimum exists.

### Open Question 3
- Question: How do the results change when using different activation functions instead of the ReLU ramp function?
- Basis in paper: The authors specifically use the ReLU ramp function and note its component-wise action as crucial to their analysis.
- Why unresolved: The analysis relies heavily on the specific properties of the ReLU function, particularly its ability to act component-wise and its kernel structure.
- What evidence would resolve it: A generalization of the theorems to other activation functions, showing how the geometric structure and upper bounds change with different choices.

## Limitations
- The construction assumes training data averages are linearly independent, which may not hold in practical settings
- Numerical stability becomes critical when M≫Q due to the Penrose inverse computation
- The bound O(δP) is derived for idealized conditions and may not reflect practical performance with high noise levels

## Confidence

**High Confidence**: The geometric interpretation of the constructively trained network metrizing range(P) is mathematically sound, as it follows directly from the construction W2RPX0 with full-rank W2P. The mechanism of diagonalizing P to decouple σ's component-wise action is rigorously proven.

**Medium Confidence**: The upper bound O(δP) relies on the assumption that averages x0,j are linearly independent and that numerical computations maintain sufficient precision. While the theoretical framework is robust, practical implementation may face numerical challenges.

**Low Confidence**: The exact characterization of the degenerate local minimum for M=Q case depends on the constant C0 in Theorem 3.2, which is not explicitly computed. The O(δ²P) improvement claim needs verification for specific parameter regimes.

## Next Checks

1. **Linear Independence Verification**: For various synthetic datasets with controlled noise levels, systematically verify that rank(Xred0)=Q holds and quantify the probability of failure as a function of signal-to-noise ratio δP.

2. **Numerical Stability Analysis**: Implement the constructive algorithm with varying precision (single, double, extended) and measure the degradation of the upper bound guarantee as numerical errors accumulate in the Penrose inverse computation.

3. **Generalization Performance**: Train the constructively obtained network on synthetic data and evaluate its performance on held-out test data, comparing against gradient-based training methods to validate whether the geometric construction translates to practical accuracy improvements.