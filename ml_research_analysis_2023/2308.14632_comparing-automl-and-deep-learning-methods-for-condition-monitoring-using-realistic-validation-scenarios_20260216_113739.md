---
ver: rpa2
title: Comparing AutoML and Deep Learning Methods for Condition Monitoring using Realistic
  Validation Scenarios
arxiv_id: '2308.14632'
source_url: https://arxiv.org/abs/2308.14632
tags:
- methods
- block
- data
- number
- logo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extensively compares conventional machine learning methods
  and deep learning for condition monitoring tasks using an AutoML toolbox. The experiments
  reveal consistent high accuracy in random K-fold cross-validation scenarios across
  all tested models.
---

# Comparing AutoML and Deep Learning Methods for Condition Monitoring using Realistic Validation Scenarios

## Quick Facts
- arXiv ID: 2308.14632
- Source URL: https://arxiv.org/abs/2308.14632
- Reference count: 40
- This study extensively compares conventional machine learning methods and deep learning for condition monitoring tasks using an AutoML toolbox.

## Executive Summary
This study comprehensively evaluates conventional AutoML methods against deep learning models for condition monitoring using realistic validation scenarios. The research reveals that while random K-fold cross-validation shows high accuracy across all models, leave-one-group-out (LOGO) validation exposes domain shift challenges in real-world scenarios. The findings indicate that conventional methods offer better interpretability and are sufficient for low-complexity condition monitoring tasks, while deep learning models require specialized interpretation techniques. The study emphasizes the importance of feature selection and highlights that low-complexity models are often adequate for condition monitoring applications.

## Method Summary
The study implements an AutoML toolbox with 70 method combinations (7 feature extraction x 5 feature selection x 2 classifiers) and compares it against four deep learning architectures (MLP, CNN, ResNet, WaveNet). Seven datasets spanning different condition monitoring applications are used, including bearing, hydraulic systems, electromechanical cylinders, guided waves, and human activity recognition. Both random K-fold and leave-one-group-out cross-validation strategies are employed, with Bayesian optimization for hyperparameter tuning. The methods are implemented in MATLAB with extensive feature extraction and selection pipelines.

## Key Results
- Random K-fold cross-validation shows consistently high accuracy across all tested models, while LOGO validation reveals domain shift challenges
- No clear winner emerges between AutoML and deep learning methods when domain shift is properly accounted for
- Conventional AutoML methods provide better interpretability through their modular structure compared to neural networks
- Low-complexity models are sufficient for condition monitoring tasks due to the limited number of relevant features needed

## Why This Works (Mechanism)

### Mechanism 1
Leave-one-group-out (LOGO) cross-validation exposes domain shift in condition monitoring datasets, leading to significant accuracy degradation compared to random K-fold. LOGO validation treats each operational condition as a separate domain, forcing models to generalize across unseen domains during testing, whereas random K-fold splits data randomly within each domain, masking the true generalization challenge. The core assumption is that operational conditions are the dominant source of data distribution shift.

### Mechanism 2
Conventional AutoML methods outperform or match DNN models in condition monitoring due to their interpretability and suitability for low-complexity, low-feature tasks. These methods use modular, explainable feature extraction and selection pipelines, which align well with condition monitoring tasks that typically require only a few relevant features. The core assumption is that condition monitoring tasks have low feature complexity and require interpretable models for industrial deployment.

### Mechanism 3
Random K-fold cross-validation can artificially inflate model performance in condition monitoring by failing to account for domain shift. K-fold validation randomly partitions data within each operational condition, allowing models to memorize domain-specific patterns. This leads to overoptimistic accuracy estimates that do not generalize to unseen operational conditions. The core assumption is that the data distribution is heavily influenced by operational conditions.

## Foundational Learning

- **Domain shift and its impact on model generalization**
  - Why needed here: Understanding domain shift is critical for selecting appropriate validation strategies and interpreting model performance in condition monitoring
  - Quick check question: What is the difference between random K-fold and leave-one-group-out cross-validation in terms of domain generalization?

- **Feature extraction and selection in time-series data**
  - Why needed here: Condition monitoring often relies on extracting a small number of meaningful features from sensor signals, which is a core strength of conventional AutoML methods
  - Quick check question: Why might simple statistical features be sufficient for many condition monitoring tasks, even with complex sensor data?

- **Interpretability vs. performance trade-offs in ML models**
  - Why needed here: Industrial applications require models that can be understood and trusted by domain experts, influencing the choice between conventional and deep learning methods
  - Quick check question: How does the modular structure of conventional AutoML methods aid in identifying important features compared to neural networks?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Feature extraction (ALA, BFC, BDW, TFEx, NoFE, PCA, StatMom) -> Feature selection (Pearson, RELIEFF, RFESVM, Spearman, NoFS) -> Classification (LDAMahal, SVM, neural networks) -> Validation (Random K-fold, LOGO)

- **Critical path**: Load and preprocess dataset (segmentation, normalization) -> Apply feature extraction methods -> Perform feature selection to identify top features -> Train and validate models using both K-fold and LOGO strategies -> Compare accuracy and interpretability across models

- **Design tradeoffs**: K-fold vs. LOGO validation (optimistic performance estimates vs. realistic domain generalization assessment) -> Feature extraction methods (relevant signal characteristics vs. computational cost) -> Model complexity (low-complexity models vs. DNNs for complex, high-dimensional data)

- **Failure signatures**: High accuracy in K-fold but poor performance in LOGO (overfitting to domain-specific patterns) -> Low accuracy across all models (need for more sophisticated feature extraction or larger datasets) -> DNNs underperforming compared to conventional methods (task too simple or insufficient hyperparameter tuning)

- **First 3 experiments**: Run K-fold validation on CWRU dataset using all AutoML methods to establish baseline performance -> Repeat using LOGO validation to assess domain shift impact -> Compare top-performing conventional method with tuned DNN model on same dataset and validation strategies

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of AutoML toolbox methods compare to neural networks when the number of classes increases significantly, such as in datasets like AudioSet or ImageNet? The paper only tested on condition monitoring datasets with limited classes and does not provide evidence on how these methods would perform on datasets with hundreds or thousands of classes.

### Open Question 2
What is the impact of domain shift on the performance of AutoML methods and neural networks in real-world condition monitoring scenarios? The paper mentions domain shift is present but does not provide a detailed analysis of its impact on different model types.

### Open Question 3
How does the interpretability of AutoML methods compare to neural networks in condition monitoring tasks? While the paper mentions AutoML methods are more interpretable, it does not provide a detailed comparison of interpretability methods between the two approaches.

## Limitations

- The generalizability of results may be limited due to testing on only 7 datasets
- The assumption that operational conditions are the dominant source of domain shift needs further validation
- Claims about interpretability superiority of conventional methods lack empirical comparison with quantitative metrics

## Confidence

- High confidence: The observation that K-fold and LOGO validation produce different accuracy results across datasets
- Medium confidence: The claim that conventional methods are better suited for low-complexity, interpretable condition monitoring tasks
- Low confidence: The assertion that AutoML methods can match or outperform DNNs in all realistic condition monitoring scenarios

## Next Checks

1. Measure statistical distance (e.g., Wasserstein, MMD) between domains in each dataset to empirically verify operational conditions create meaningful distribution shifts

2. Implement and compare multiple interpretability methods (occlusion maps, SHAP values, feature importance rankings) for both conventional and deep learning models to quantify interpretability differences

3. Test AutoML vs DNN performance trade-off on additional condition monitoring datasets with varying complexity, signal types, and class distributions to assess generalizability