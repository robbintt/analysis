---
ver: rpa2
title: 'UniGen: A Unified Generative Framework for Retrieval and Question Answering
  with Large Language Models'
arxiv_id: '2312.11036'
source_url: https://arxiv.org/abs/2312.11036
tags:
- retrieval
- document
- generative
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UniGen, a unified generative framework for
  retrieval and question answering that leverages large language models to bridge
  the input-output and docid-answer gaps. The framework employs a shared encoder and
  two distinct decoders for generative retrieval and question answering, along with
  LLM-generated connectors and an iterative enhancement strategy.
---

# UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2312.11036
- Source URL: https://arxiv.org/abs/2312.11036
- Reference count: 9
- Key outcome: UniGen-Iter achieves 11.76% and 2.17% gains in R@1 on MS MARCO and NQ respectively

## Executive Summary
UniGen is a unified generative framework that jointly addresses retrieval and question answering tasks using a shared encoder and two distinct decoders. The framework leverages LLM-generated connectors to bridge semantic gaps between queries, documents, and answers, and employs an iterative enhancement strategy to improve performance. Experiments on MS MARCO and NQ datasets demonstrate significant improvements over baseline models, with UniGen-Iter achieving 11.76% and 2.17% gains in R@1 on MS MARCO and NQ respectively.

## Method Summary
UniGen employs a shared T5-base encoder and two T5-base decoders for retrieval and QA tasks. LLM-generated Q-Connectors and D-Connectors bridge semantic gaps between inputs and outputs. The model is trained in two stages: synthetic data pre-training followed by fine-tuning with labeled data. An iterative enhancement strategy refines Q-Connectors using retrieved documents and generated answers from previous iterations. The framework is evaluated on MS MARCO QnA v2.1 and NQ datasets using R@1, R@5, R@10, MRR@10, BLEU-1, ROUGE-L, EM, and F1 metrics.

## Key Results
- UniGen-Iter achieves 11.76% R@1 improvement on MS MARCO compared to baselines
- UniGen-Iter achieves 2.17% R@1 improvement on NQ compared to baselines
- UniGen demonstrates effective joint learning of retrieval and QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A shared encoder improves input understanding and overall performance.
- Mechanism: The shared encoder processes queries enriched by Q-Connectors, allowing knowledge from both retrieval and QA tasks to enhance the encoder's semantic understanding.
- Core assumption: The query semantics are sufficiently similar across retrieval and QA tasks that a single encoder can learn useful shared representations.
- Evidence anchors: Abstract and section claims; no direct evidence in neighbors.

### Mechanism 2
- Claim: LLM-generated connectors bridge semantic gaps between inputs and outputs.
- Mechanism: Q-Connectors provide contextual enrichment for queries, and D-Connectors summarize documents into a form that is easier to jointly learn with answers.
- Core assumption: LLMs can generate semantically meaningful connectors that effectively bridge the gaps between the original query/docid space and the target answer space.
- Evidence anchors: Abstract and section claims; no direct evidence in neighbors.

### Mechanism 3
- Claim: Iterative enhancement improves both retrieval and QA performance by refining inputs over iterations.
- Mechanism: Retrieved documents and generated answers from one iteration are used to create better Q-Connectors for the next iteration, providing richer context and more relevant information to guide the model.
- Core assumption: The outputs from one iteration (documents and answers) are useful for improving the next iteration's inputs (Q-Connectors).
- Evidence anchors: Abstract and section claims; no direct evidence in neighbors.

## Foundational Learning

- Concept: Transformer encoder-decoder architecture
  - Why needed here: The framework uses a shared T5-base encoder and two T5-base decoders; understanding how encoders capture context and decoders generate sequences is essential.
  - Quick check question: What is the role of the attention mechanism in a transformer encoder when processing a query with a Q-Connector?

- Concept: Generative retrieval and its difference from dense retrieval
  - Why needed here: The retrieval decoder directly generates document identifiers (docids) rather than ranking pre-indexed documents, which is a key novelty of the framework.
  - Quick check question: How does constrained beam search in the retrieval decoder ensure that generated docids are valid identifiers from the document index?

- Concept: Large language model (LLM) prompting for structured output
  - Why needed here: Connectors are generated by prompting an LLM (gpt-3.5-turbo-0613) with specific templates to produce summaries or contexts.
  - Quick check question: What are the key elements of a prompt template that ensures an LLM generates a concise, relevant summary for a D-Connector?

## Architecture Onboarding

- Component map:
  - Input: User query
  - LLM module: Generates Q-Connector (contextual query) and D-Connector (document summary)
  - Shared encoder: T5-base, processes enriched query
  - Retrieval decoder: T5-base, generates ranked docids (constrained beam search)
  - QA decoder: T5-base, generates answer (greedy search)
  - Iterative enhancement loop: Uses retrieved docs and generated answers to refine Q-Connectors for next iteration

- Critical path: Query → Q-Connector generation → Shared encoder → Both decoders (retrieval + QA) → Outputs (docids + answer)
  - The shared encoder is the bottleneck; both decoders depend on its output.

- Design tradeoffs:
  - Shared encoder vs. separate encoders: Shared encoder enables knowledge sharing but may limit task-specific optimizations.
  - LLM-generated connectors vs. learned embeddings: Connectors provide semantic bridging but add dependency on external LLM API and may introduce latency.
  - Iterative enhancement vs. single pass: Iteration improves performance but increases computation and latency.

- Failure signatures:
  - Retrieval decoder generates invalid or low-quality docids → Check constrained beam search and docid space design.
  - QA decoder produces off-topic or incorrect answers → Check Q-Connector quality and shared encoder representation.
  - Performance degrades with iterations → Check for noise accumulation in Q-Connectors and whether retrieved docs are actually helpful.

- First 3 experiments:
  1. Train UniGen-Base on synthetic data only; evaluate retrieval MRR@10 and QA BLEU-1 to confirm joint learning works.
  2. Add Q-Connector and D-Connector generation; measure impact on both tasks compared to experiment 1.
  3. Run one iteration of the enhancement loop; compare performance to UniGen-Base to confirm iterative improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UniGen scale with larger document collections beyond the MS MARCO and NQ datasets?
- Basis in paper: [inferred] The paper evaluates UniGen on MS MARCO and NQ datasets but does not discuss performance on larger collections.
- Why unresolved: The paper does not provide evidence of UniGen's scalability to larger document collections or real-world web-scale scenarios.
- What evidence would resolve it: Experiments evaluating UniGen on larger document collections (e.g., ClueWeb, Common Crawl) and web-scale retrieval scenarios would provide evidence of its scalability.

### Open Question 2
- Question: How does UniGen perform in multi-lingual retrieval and question answering scenarios?
- Basis in paper: [inferred] The paper focuses on English datasets (MS MARCO, NQ) and does not discuss multi-lingual capabilities.
- Why unresolved: The paper does not provide evidence of UniGen's effectiveness in handling queries and documents in languages other than English.
- What evidence would resolve it: Experiments evaluating UniGen on multi-lingual datasets (e.g., MLQA, XQuAD) and comparing its performance across different languages would provide evidence of its multi-lingual capabilities.

### Open Question 3
- Question: How does the choice of LLM affect the performance of UniGen, and are there diminishing returns with increasingly larger LLMs?
- Basis in paper: [explicit] The paper mentions using gpt-3.5-turbo-0613 and LLaMA-13B-Chat as LLMs for connector generation but does not explore the impact of different LLM sizes or architectures.
- Why unresolved: The paper does not provide a systematic comparison of UniGen's performance with different LLM choices or analyze the potential diminishing returns of using increasingly larger LLMs.
- What evidence would resolve it: Experiments comparing UniGen's performance using different LLM sizes (e.g., LLaMA-7B, LLaMA-33B) and architectures (e.g., GPT-3, GPT-4) would provide evidence of the impact of LLM choice on UniGen's effectiveness.

## Limitations
- Reliance on LLM-generated connectors introduces external dependencies and potential variability in performance
- Constrained beam search implementation for generative retrieval is not fully specified
- Iterative enhancement strategy's effectiveness may be dataset-dependent

## Confidence
- **High Confidence**: The core architecture of using a shared encoder with separate decoders for retrieval and QA is well-grounded in multi-task learning principles.
- **Medium Confidence**: The claimed improvements from LLM-generated connectors and iterative enhancement are based on reported results but lack ablation studies showing the individual contribution of each component.
- **Low Confidence**: The long-term stability and generalization of the iterative enhancement process across diverse domains is unclear, as the paper only evaluates on MS MARCO and NQ datasets.

## Next Checks
1. **Ablation Study**: Run UniGen-Base without Q-Connectors and D-Connectors to quantify the exact contribution of LLM-generated connectors to performance gains.
2. **Connector Quality Analysis**: Manually evaluate a sample of generated Q-Connectors and D-Connectors for semantic relevance and coherence, and correlate connector quality scores with downstream task performance.
3. **Cross-Dataset Generalization**: Test the trained UniGen model on a third QA dataset (e.g., TriviaQA or SQuAD) without fine-tuning to assess the framework's generalization capabilities and identify potential domain-specific limitations.