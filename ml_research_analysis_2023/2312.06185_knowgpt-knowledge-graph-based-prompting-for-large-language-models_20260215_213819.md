---
ver: rpa2
title: 'KnowGPT: Knowledge Graph based Prompting for Large Language Models'
arxiv_id: '2312.06185'
source_url: https://arxiv.org/abs/2312.06185
tags:
- knowledge
- llms
- knowgpt
- prompt
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KnowGPT, a black-box knowledge injection framework
  for LLMs in question answering. The framework uses deep reinforcement learning to
  extract relevant knowledge from knowledge graphs and Multi-Armed Bandit to construct
  effective prompts.
---

# KnowGPT: Knowledge Graph based Prompting for Large Language Models

## Quick Facts
- arXiv ID: 2312.06185
- Source URL: https://arxiv.org/abs/2312.06185
- Reference count: 9
- Key outcome: Achieves 92.6% accuracy on OpenbookQA, comparable to human performance

## Executive Summary
KnowGPT is a black-box knowledge injection framework that enhances large language models' question-answering capabilities by leveraging knowledge graphs. The framework uses deep reinforcement learning to extract relevant reasoning paths from knowledge graphs and Multi-Armed Bandit to construct optimal prompts for each question. Experiments on three benchmarks demonstrate significant performance improvements over state-of-the-art methods, achieving near-human accuracy on OpenbookQA.

## Method Summary
KnowGPT combines deep reinforcement learning for knowledge graph path extraction with Multi-Armed Bandit for prompt construction. The framework first extracts relevant reasoning paths between question entities using a policy network trained with policy gradient. These paths are then converted into different prompt formats (triples, sentences, graph descriptions) and the MAB selects the most effective combination for each question context. The system operates entirely through black-box LLM APIs, making it broadly applicable without requiring model access.

## Key Results
- Achieves 92.6% accuracy on OpenbookQA benchmark
- Demonstrates state-of-the-art performance across three benchmark datasets
- Reaches human-level performance on OpenbookQA (91.6%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep reinforcement learning can effectively extract informative and concise reasoning paths from knowledge graphs for question answering.
- Mechanism: The framework uses a policy network trained with policy gradient to navigate from source entities to target entities in the knowledge graph, guided by rewards that promote reachability, context-relatedness, and conciseness.
- Core assumption: The knowledge graph contains relevant paths between source and target entities that can improve LLM performance when properly extracted.
- Evidence anchors:
  - [abstract]: "KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs)"
  - [section]: "We develop a tailored path extraction method, named PRL, that employs deep reinforcement learning to sample reasoning paths"
  - [corpus]: Weak - The corpus shows related papers on KG-LLM integration but lacks specific evidence for the effectiveness of the RL approach used here.
- Break condition: If the knowledge graph lacks relevant paths between source and target entities, or if the policy network fails to learn effective navigation strategies.

### Mechanism 2
- Claim: Multi-Armed Bandit can effectively select optimal combinations of path extraction strategies and prompt templates for each question.
- Mechanism: The MAB learns to balance exploration and exploitation by estimating the potential of each (path extraction, prompt template) combination for different question contexts, using feedback from LLM responses.
- Core assumption: Different questions benefit from different combinations of path extraction methods and prompt formats, and this can be learned from LLM feedback.
- Evidence anchors:
  - [abstract]: "use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question"
  - [section]: "we introduce a prompt construction strategy based on Multi-Armed Bandit (MAB)"
  - [corpus]: Weak - The corpus shows related work on KG-LLM integration but lacks specific evidence for MAB-based prompt construction.
- Break condition: If LLM feedback is not sufficiently informative to distinguish effective prompt combinations, or if the question context embedding fails to capture relevant differences between questions.

### Mechanism 3
- Claim: Converting extracted knowledge into different prompt formats (triples, sentences, graph descriptions) allows effective integration with black-box LLMs.
- Mechanism: The framework provides multiple prompt templates that transform knowledge graph triples into formats that LLMs can process, with an MAB selecting the most effective format for each question.
- Core assumption: LLMs can effectively use structured knowledge when presented in appropriate natural language formats.
- Evidence anchors:
  - [abstract]: "convert extracted knowledge into effective prompts"
  - [section]: "a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts"
  - [corpus]: Weak - The corpus shows related work on KG-LLM integration but lacks specific evidence for the effectiveness of different prompt formats.
- Break condition: If LLMs cannot effectively process any of the provided prompt formats, or if the MAB fails to select appropriate formats for different question types.

## Foundational Learning

- Concept: Reinforcement Learning for Path Extraction
  - Why needed here: The framework needs to extract relevant knowledge paths from large knowledge graphs without access to model internals, requiring an adaptive search strategy.
  - Quick check question: How does the reward function balance between finding paths to target entities and maintaining path conciseness?

- Concept: Multi-Armed Bandit for Prompt Selection
  - Why needed here: The framework needs to automatically select optimal combinations of knowledge extraction methods and prompt formats for each question without manual tuning.
  - Quick check question: How does the MAB algorithm balance exploration of new prompt combinations against exploitation of known effective combinations?

- Concept: Knowledge Graph Embeddings
  - Why needed here: The framework needs to represent knowledge graph entities and relations in a form that can be used for path extraction and prompt construction.
  - Quick check question: How are knowledge graph embeddings obtained and used in the path extraction process?

## Architecture Onboarding

- Component map: Question context → Entity extraction → Knowledge graph subgraph retrieval → Path extraction (RL) → Prompt construction (MAB) → LLM API call → Answer selection
- Critical path: Question context → Entity extraction → Path extraction (PRL) → Prompt construction (MAB) → LLM API call → Answer selection
- Design tradeoffs: Using black-box LLMs limits control over knowledge injection but enables broader applicability; RL-based path extraction is more adaptive but potentially less stable than heuristic methods.
- Failure signatures: Poor performance may indicate: (1) RL policy fails to find relevant paths, (2) MAB selects suboptimal prompt combinations, (3) Knowledge graph lacks relevant information, (4) LLM API responses are unreliable.
- First 3 experiments:
  1. Test path extraction with simple questions where correct answers are easily found in the knowledge graph
  2. Evaluate different prompt formats with questions where the optimal format is known
  3. Test end-to-end performance on a small subset of questions from each benchmark dataset

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Performance depends heavily on the quality and coverage of the knowledge graph
- Black-box approach limits control over knowledge injection compared to white-box methods
- RL-based path extraction may struggle with scalability as KG size increases

## Confidence
- High confidence: The framework architecture (RL + MAB) is technically sound and aligns with established approaches in KG-LLM integration.
- Medium confidence: The claimed performance improvements (92.6% on OpenbookQA, comparable to human performance) are plausible given the methodology, though specific hyperparameter choices and implementation details could significantly impact results.
- Medium confidence: The generalization claim across three diverse benchmarks is supported by the experimental setup, but the selection of only three datasets limits broader generalization claims.

## Next Checks
1. Implement ablation studies removing the RL path extraction component and replacing it with heuristic path selection to quantify the specific contribution of the RL approach to overall performance.

2. Analyze the MAB's selection patterns across different question types to verify that it consistently selects optimal prompt combinations and doesn't get stuck in suboptimal strategies.

3. Evaluate performance using KGs of varying quality and coverage to establish the relationship between KG characteristics and QA accuracy, identifying potential failure points when KG coverage is incomplete.