---
ver: rpa2
title: How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?
arxiv_id: '2307.02575'
source_url: https://arxiv.org/abs/2307.02575
tags:
- maps
- land
- cover
- each
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 11 publicly available land cover maps for
  cropland classification in Sub-Saharan Africa using rigorous reference datasets
  from 8 countries. The maps span spatial resolutions from 10-1000 m/px and years
  2009-2020.
---

# How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?

## Quick Facts
- arXiv ID: 2307.02575
- Source URL: https://arxiv.org/abs/2307.02575
- Reference count: 40
- Primary result: GLAD map performs best for cropland classification in Sub-Saharan Africa, but overall consensus between maps is extremely low (<0.5% unanimous agreement)

## Executive Summary
This study evaluates 11 publicly available land cover maps for cropland classification across 8 Sub-Saharan African countries using rigorous reference datasets. The maps span spatial resolutions from 10-1000 m/px and years 2009-2020. Results show very low consensus across maps with less than 0.5% unanimous cropland agreement, and large disparities in performance between countries. The GLAD map performs best overall while GlobCover shows the poorest performance. Higher spatial resolution weakly correlates with improved accuracy and precision, but not recall. Temporal mismatch also weakly correlates with reduced performance. The study provides a benchmark reference dataset and code to facilitate future map evaluation and development.

## Method Summary
The study uses reference datasets from 8 Sub-Saharan African countries containing 3,386 cropland/non-cropland samples to evaluate 11 publicly available land cover maps. Maps are assessed using accuracy metrics including overall accuracy, F1 score, precision, and recall with standard error calculations. Consensus analysis examines unanimous cropland agreement and pairwise map agreement. The evaluation framework uses Google Earth Engine for data processing and visualization. The analysis also explores correlations between performance and spatial resolution/temporal mismatch while accounting for resampling and binarization requirements.

## Key Results
- GLAD map shows the best overall performance for cropland classification in SSA
- GlobCover demonstrates the poorest performance among evaluated maps
- Less than 0.5% unanimous agreement between maps on cropland locations
- Higher spatial resolution weakly correlates with improved accuracy and precision, but not recall
- Temporal mismatch between map year and reference data weakly correlates with reduced performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-resolution satellite imagery improves cropland classification accuracy in Africa.
- Mechanism: Finer spatial resolution (e.g., 10 m/px Sentinel-2) captures small and fragmented field sizes better than coarser resolution data, reducing misclassification.
- Core assumption: Field size fragmentation is a primary factor limiting classification accuracy in SSA.
- Evidence anchors:
  - [abstract]: "Some work has sought to resolve these inconsistencies and create improved regional maps by combining the classifications from several individual maps. Researchers have hypothesized that higher-resolution satellite datasets (e.g., 10 m/pixel Sentinel-2) would enable the development of models that can capture small-scale and heterogeneous fields not captured by earlier models based on coarser datasets, but this hypothesis has not been tested as higher-resolution map products have been published."
  - [section]: "Previous work has hypothesized that land cover products based on higher resolution satellite observations (e.g., 10 m/pixel Sentinel-2) should have improved cropland mapping performance in Sub-Saharan Africa as a result of small and fragmented field sizes being more clear in the higher-resolution images, though other work found that small field sizes were not the primary influencing factor on the disagreement between maps."
  - [corpus]: "Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps"
- Break condition: If field size is not the dominant source of error, or if classification errors are driven by spectral ambiguity rather than spatial resolution, higher resolution may not yield accuracy gains.

### Mechanism 2
- Claim: Temporal mismatch between map year and reference data reduces classification accuracy.
- Mechanism: Cropland extent and agricultural practices change over time; maps that do not align temporally with reference data will have reduced accuracy due to these changes.
- Core assumption: Agricultural land use is dynamic and changes significantly over short time periods.
- Evidence anchors:
  - [abstract]: "Temporal mismatch also weakly correlates with reduced performance."
  - [section]: "Previous work has stressed the importance of using a crop mask with the same (or as close as possible to the same) year as the downstream analysis, e.g., when assessing in-season crop conditions."
  - [corpus]: No direct corpus evidence; weak anchor.
- Break condition: If agricultural land use is stable over the time period considered, or if the analysis is insensitive to temporal shifts, the impact of temporal mismatch may be negligible.

### Mechanism 3
- Claim: Global models underperform regional models in cropland classification accuracy in SSA.
- Mechanism: Global models are trained on diverse global data, leading to high intra-class variance that makes learning to predict classes accurately across a wide range of agro-ecological conditions challenging. Regional models are optimized for specific regions, leading to better performance.
- Core assumption: The diversity of agro-ecological conditions in SSA requires region-specific optimization for accurate classification.
- Evidence anchors:
  - [abstract]: "Global models compared in this study are Esri, Dynamic World, and WorldCover. Global models are appealing because they simplify computation and enable models to learn from more diverse data, but these benefits may come at the expense of high intra-class variance that makes learning to predict classes accurately across a wide range of agro-ecological conditions challenging. In contrast, regional models are trained to optimize classification performance for a regional sub-group, such as an agro-ecological zone, a global grid tile, or a country (see Table 3). Table 1 shows that models trained to optimize regional performance tend to outperform global models."
  - [section]: "Researchers have been shifting toward training single, global models that are optimized to predict land cover anywhere in the world. Global models compared in this study are Esri, Dynamic World, and WorldCover. Global models are appealing because they simplify computation and enable models to learn from more diverse data, but these benefits may come at the expense of high intra-class variance that makes learning to predict classes accurately across a wide range of agro-ecological conditions challenging. In contrast, regional models are trained to optimize classification performance for a regional sub-group, such as an agro-ecological zone, a global grid tile, or a country (see Table 3). Table 1 shows that models trained to optimize regional performance tend to outperform global models."
  - [corpus]: "Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps"
- Break condition: If the diversity of agro-ecological conditions is not as high as assumed, or if global models can effectively learn from diverse data to achieve high accuracy across regions, regional models may not outperform global models.

## Foundational Learning

- Concept: Accuracy metrics (precision, recall, F1 score, overall accuracy)
  - Why needed here: To evaluate and compare the performance of different land cover maps for cropland classification.
  - Quick check question: What is the difference between precision and recall, and why is F1 score often preferred over overall accuracy for imbalanced datasets?

- Concept: Confusion matrix and error types (false positives, false negatives)
  - Why needed here: To understand the types of errors made by different maps and how they impact downstream applications.
  - Quick check question: How do false positives and false negatives in cropland classification affect crop condition assessments and yield estimations?

- Concept: Temporal and spatial resolution in remote sensing
  - Why needed here: To understand the impact of data characteristics on classification accuracy and to choose appropriate datasets for specific applications.
  - Quick check question: How does the choice of temporal and spatial resolution affect the ability to detect small, fragmented cropland fields in SSA?

## Architecture Onboarding

- Component map:
  - Reference datasets (8 countries, 3,386 samples) -> Land cover maps (11 products) -> Accuracy metrics (precision, recall, F1, overall) -> Consensus analysis (unanimous agreement, pairwise) -> Correlation analysis (spatial resolution, temporal mismatch)

- Critical path:
  1. Acquire and preprocess reference datasets and land cover maps
  2. Evaluate each map using the reference datasets and calculate accuracy metrics
  3. Analyze the results to identify the best-performing maps and understand the factors influencing accuracy
  4. Visualize the results and provide insights for users and future research

- Design tradeoffs:
  - Spatial resolution vs. temporal coverage: Higher resolution data may have limited temporal coverage, while lower resolution data may have better temporal coverage
  - Global vs. regional models: Global models are easier to implement but may have lower accuracy in specific regions compared to regional models
  - Ensemble methods vs. individual models: Ensemble methods can improve accuracy but may be more complex to implement and interpret

- Failure signatures:
  - Low accuracy metrics: Indicates poor performance of the maps in cropland classification
  - High disagreement between maps: Suggests inconsistencies in cropland predictions and the need for further investigation
  - Temporal or spatial mismatch between maps and reference data: Can lead to reduced accuracy and misinterpretation of results

- First 3 experiments:
  1. Evaluate the accuracy of a single land cover map using the reference dataset for one country
  2. Compare the performance of two land cover maps with different spatial resolutions for the same country
  3. Analyze the impact of temporal mismatch on the accuracy of a land cover map by comparing its performance using reference data from different years

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance disparities between countries be reduced to create more geographically fair cropland maps?
- Basis in paper: [explicit] The paper explicitly states "we encourage future work focusing on targeted improvements that boost performance for the lowest-performing sub-groups (e.g., countries or biomes) among existing methods" and "This would help to resolve inconsistencies between existing maps and reduce the disparity in performance across countries globally."
- Why unresolved: The paper identifies the problem of performance disparities but does not provide specific methods or strategies to address this issue.
- What evidence would resolve it: A study comparing different approaches to improve map accuracy in low-performing countries, showing which methods are most effective in reducing performance disparities.

### Open Question 2
- Question: What specific types of land cover are most commonly confused with cropland in these maps?
- Basis in paper: [explicit] The paper states "Future work could also help direct efforts to improve map accuracy and provide useful information to map users by identifying the types of land cover that are commonly confused with cropland in these maps."
- Why unresolved: While the paper acknowledges this as an area for future research, it does not provide any analysis or evidence on which land cover types are most frequently misclassified as cropland.
- What evidence would resolve it: An analysis of classification errors in the maps, showing the most common types of land cover that are incorrectly classified as cropland, along with the specific regions where these errors occur most frequently.

### Open Question 3
- Question: How does the performance of global models compare to regional models when evaluated on a country-by-country basis?
- Basis in paper: [explicit] The paper mentions that "regional models tend to outperform global models" and provides some examples, but does not conduct a detailed country-by-country comparison.
- Why unresolved: The paper provides a general observation about model performance but lacks a detailed breakdown of how global and regional models perform in specific countries.
- What evidence would resolve it: A comprehensive country-by-country evaluation comparing the performance of global and regional models, including specific metrics and visualizations to highlight the differences in accuracy and reliability.

## Limitations
- Low confidence in global vs regional model performance claims due to lack of access to training data/methodology
- Weak correlations (RÂ² values not reported) create high uncertainty about practical significance of resolution/temporal effects
- Small sample size (3,386 points across 8 countries) limits generalizability to full SSA region
- Exclusive focus on cropland masks neglects other land cover types affecting downstream applications

## Confidence
- **High Confidence**: Accuracy metric calculations and benchmarking methodology; consensus analysis results showing <0.5% unanimous agreement
- **Medium Confidence**: Performance differences between specific maps (GLAD best, GlobCover worst); spatial resolution correlation with accuracy
- **Low Confidence**: Claims about temporal mismatch impact; generalizability of findings beyond benchmark countries; causal mechanisms explaining performance differences

## Next Checks
1. **Temporal Stability Analysis**: Replicate the accuracy assessment using reference data from multiple years for the same locations to quantify the true impact of temporal mismatch beyond the reported weak correlation.

2. **Field Size Impact Test**: Conduct stratified analysis comparing accuracy across different field size categories to validate whether small fragmented fields actually drive the observed resolution-accuracy relationship.

3. **Ensemble Performance Evaluation**: Test whether combining predictions from multiple maps using weighted averaging or machine learning improves accuracy beyond the best individual map, addressing the practical utility of the observed low consensus.