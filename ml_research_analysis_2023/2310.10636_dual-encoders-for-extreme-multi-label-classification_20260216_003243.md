---
ver: rpa2
title: Dual-Encoders for Extreme Multi-Label Classification
arxiv_id: '2310.10636'
source_url: https://arxiv.org/abs/2310.10636
tags:
- labels
- loss
- label
- classification
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the capabilities of dual-encoder models for
  extreme multi-label classification (XMC) tasks, where the goal is to retrieve relevant
  documents for a query from a large corpus. While DE models have been successful
  in open-domain question answering, their performance in many-shot retrieval problems
  like XMC, where training data is abundant, remains under-explored.
---

# Dual-Encoders for Extreme Multi-Label Classification

## Quick Facts
- **arXiv ID**: 2310.10636
- **Source URL**: https://arxiv.org/abs/2310.10636
- **Reference count**: 40
- **Key outcome**: Standard dual-encoder models can match or outperform state-of-the-art XMC methods by up to 2% at Precision@1 on the largest XMC datasets while being 20x smaller in trainable parameters

## Executive Summary
This paper investigates the application of dual-encoder models to extreme multi-label classification (XMC) tasks, where the goal is to retrieve relevant documents for a query from a large corpus. While dual-encoders have been successful in open-domain question answering, their performance in many-shot retrieval problems like XMC remains under-explored. The authors propose modifications to contrastive learning losses to better handle the multi-label nature of XMC datasets, introducing a decoupled softmax loss and a differentiable top-k loss. With these proposed loss functions, standard dual-encoder models can match or outperform state-of-the-art XMC methods while being more parameter-efficient.

## Method Summary
The paper proposes a decoupled softmax loss that modifies the InfoNCE loss by excluding other positives from the denominator when computing the loss for a given positive. This modification allows the model to handle label imbalance in XMC datasets, where not all positive labels have equal ease of prediction. Additionally, the authors introduce a differentiable top-k loss based on a soft approximation of the top-k operation, which directly optimizes for Recall@k metrics. To enable training with extensive negatives, the paper presents a memory-efficient distributed implementation using gradient caching and multi-GPU training. The proposed methods are evaluated on several XMC datasets, including EURLex-4K, LF-AmazonTitles-131K, LF-Wikipedia-500K, and LF-AmazonTitles-1.3M.

## Key Results
- Standard dual-encoder models can match or outperform state-of-the-art XMC methods by up to 2% at Precision@1 on the largest XMC datasets
- The decoupled softmax loss improves performance by better handling label imbalance in XMC datasets
- The soft top-k loss directly optimizes for Recall@k metrics and further improves performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Standard contrastive learning losses like InfoNCE are poorly suited for multi-label classification because they force uniform scoring across all positives, penalizing confidently predicted positives.
- **Mechanism**: The decoupled softmax loss modifies InfoNCE by excluding other positives from the denominator when computing the loss for a given positive. This prevents the model from being penalized when it confidently predicts some positives over others, which is essential for handling label imbalance.
- **Core assumption**: In XMC datasets, not all positive labels have equal ease of prediction from the available data, and this imbalance should be respected during training.
- **Evidence anchors**:
  - [abstract] "We propose decoupled softmax loss - a simple modification to the InfoNCE loss - that overcomes the limitations of existing contrastive losses."
  - [section 4.1] "It is simple to see DecoupledSoftmax is optimized when we have σil = 1 if l ∈ Pi (for positives) and when the score for a negative s(qi, dl) is significantly smaller than all positive scores, i.e., s(qi, dj) for j ∈ Pi."
  - [corpus] Weak - the corpus doesn't directly address this mechanism but the paper's synthetic experiment (Section C.1) supports it.
- **Break condition**: If the assumption about label imbalance being important is false, or if all positives truly need to be predicted with equal confidence for the task, this mechanism would not provide benefit.

### Mechanism 2
- **Claim**: Using extensive negatives in training provides more consistent and unbiased loss feedback compared to using only in-batch negatives.
- **Mechanism**: By computing gradients with respect to a large pool of negatives (potentially all negatives), the model receives more informative feedback about which documents are truly irrelevant, rather than just those that happen to co-occur in a batch.
- **Core assumption**: More negatives lead to better gradient estimates and improved model performance, despite the computational cost.
- **Evidence anchors**:
  - [abstract] "The key to the improved performance is the right loss formulation for the underlying task and the use of extensive negatives to give consistent and unbiased loss feedback."
  - [section 4.2] "Since our proposed DecoupledSoftmax loss uses all negatives in its formulation, we investigate the approximations of this loss formulation by considering only a few negatives per training query in the batch."
  - [corpus] Weak - corpus doesn't provide direct evidence for this mechanism, but the ablation study (Table 3) supports it.
- **Break condition**: If the computational cost of using extensive negatives outweighs the benefits, or if the additional negatives provide diminishing returns beyond a certain point.

### Mechanism 3
- **Claim**: The soft top-k loss directly optimizes for Recall@k metrics by using a differentiable approximation of the top-k operation.
- **Mechanism**: The soft top-k operator assigns values close to 1 for the top-k scores and values close to 0 for the remaining scores, allowing gradients to flow through it during backpropagation. This loss function provides feedback that encourages all positive labels to be ranked among the top-k predictions.
- **Core assumption**: Optimizing for Recall@k requires explicitly encouraging the model to rank all positives above the k-th position, not just relative to negatives.
- **Evidence anchors**:
  - [abstract] "We further refine our loss design to this end. Based on a soft differentiable version of top-k operation, we propose a novel loss function and show it can be integrated easily with standard DE training."
  - [section 4.3] "This loss serves to provide the model with feedback on its performance in placing all positives among the top-K predictions. When all positive labels are among the top-k predictions, the loss is zero."
  - [corpus] Weak - corpus doesn't directly address this mechanism, but the results (Table 4) support its effectiveness.
- **Break condition**: If the soft approximation of top-k is not smooth enough for effective gradient flow, or if the task doesn't require optimizing for Recall@k specifically.

## Foundational Learning

- **Concept: Contrastive learning and InfoNCE loss**
  - Why needed here: The paper builds on InfoNCE loss as the foundation for dual-encoder training, then modifies it for multi-label scenarios.
  - Quick check question: What is the key difference between InfoNCE loss and the decoupled softmax loss proposed in this paper?

- **Concept: Multi-label classification and label imbalance**
  - Why needed here: Understanding that XMC datasets have imbalanced label distributions where some positives are easier to predict than others is crucial for appreciating why the decoupled softmax loss is needed.
  - Quick check question: How does label imbalance manifest in XMC datasets, and why does it pose a challenge for standard contrastive losses?

- **Concept: Differentiable approximations of non-differentiable operations**
  - Why needed here: The soft top-k operator is a differentiable approximation of the non-differentiable top-k operation, which is essential for optimizing Recall@k metrics.
  - Quick check question: How does the soft top-k operator approximate the hard top-k operation, and why is this approximation necessary for gradient-based optimization?

## Architecture Onboarding

- **Component map**: Query encoder (fϕ) -> Document encoder (hψ) -> Similarity scores -> Loss functions (DecoupledSoftmax, SoftTop-k) -> Gradient computation and backpropagation

- **Critical path**:
  1. Encode queries and documents using the dual-encoder architecture
  2. Compute similarity scores between query-document pairs
  3. Calculate the chosen loss function (decoupled softmax or soft top-k) using the similarity scores and labels
  4. Backpropagate gradients through the encoders using the memory-efficient distributed implementation
  5. Update model parameters using an optimizer (e.g., AdamW)

- **Design tradeoffs**:
  - Using all negatives vs. sampled negatives: All negatives provide more consistent feedback but are computationally expensive, while sampled negatives are more efficient but may provide biased feedback.
  - Soft top-k vs. decoupled softmax: Soft top-k directly optimizes for Recall@k but may be more complex to implement and train, while decoupled softmax is simpler but may not directly optimize for specific metrics.
  - Memory-efficient distributed implementation: Allows training with all negatives on large datasets but adds complexity to the codebase.

- **Failure signatures**:
  - Poor performance on head labels: May indicate that the model is not learning to confidently predict easy positives due to the decoupled softmax loss.
  - Overfitting to training data: May occur if the model memorizes specific query-document pairs rather than learning generalizable representations.
  - Slow convergence: May be due to the increased complexity of the loss functions or the memory-efficient distributed implementation.

- **First 3 experiments**:
  1. Train a dual-encoder model with the standard InfoNCE loss on a small XMC dataset (e.g., EURLex-4K) and evaluate its performance on P@1 and R@10 metrics.
  2. Replace the InfoNCE loss with the decoupled softmax loss and retrain the model, comparing its performance to the baseline model.
  3. Implement the soft top-k loss and train a model using this loss, evaluating its performance on Recall@k metrics and comparing it to the decoupled softmax model.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the performance of pure DE models scale when applied to extreme multi-label classification tasks with billions of labels, and is there a fundamental capacity threshold for scaling to web-scale datasets?
  - **Basis in paper**: [explicit] The paper establishes that pure DE models can match or outperform state-of-the-art methods on datasets with up to millions of labels, but acknowledges that scaling to billion-scale datasets remains an open challenge.
  - **Why unresolved**: The authors only evaluated their approach on datasets with up to millions of labels, leaving the scalability to billion-scale datasets unexplored.
  - **What evidence would resolve it**: Experiments demonstrating the performance of pure DE models on datasets with billions of labels, and identifying any capacity thresholds or limitations in scaling.

- **Open Question 2**: Can the performance of DE models on many-shot retrieval problems be improved without requiring a large pool of documents in the loss computation?
  - **Basis in paper**: [explicit] The paper highlights that DE performance is positively correlated with the availability of negatives, but designing algorithms to boost performance without a large pool of documents is identified as an interesting direction for future work.
  - **Why unresolved**: The authors have only explored the use of extensive negatives for consistent loss feedback, but have not investigated alternative approaches to improve DE performance without relying on a large pool of documents.
  - **What evidence would resolve it**: Development and evaluation of novel algorithms that can enhance DE performance on many-shot retrieval problems without requiring a large pool of documents in the loss computation.

- **Open Question 3**: Can DE models be designed with in-built fast search procedures, similar to hierarchical extreme classification methods, to eliminate the need for external approximate nearest neighbor search routines?
  - **Basis in paper**: [explicit] The paper mentions that DE approaches rely on external approximate nearest neighbor search routines for fast inference, while hierarchical extreme classification methods have in-built fast search procedures.
  - **Why unresolved**: The authors have not explored the integration of hierarchical approaches or in-built fast search procedures into DE models, leaving the potential performance gains and trade-offs unexplored.
  - **What evidence would resolve it**: Development and evaluation of DE models with in-built fast search procedures, comparing their performance and efficiency against traditional DE models with external search routines.

## Limitations
- The decoupled softmax loss requires computing scores for all positives in each batch, which can be computationally expensive for datasets with large label spaces.
- The memory-efficient distributed implementation using gradient caching is critical for scalability but is not fully detailed in the paper, making exact reproduction challenging.
- The soft top-k loss, while theoretically sound, may have stability issues during training due to the approximation of the top-k operation.

## Confidence
- **High confidence**: The core claims about InfoNCE loss being poorly suited for multi-label XMC tasks and the effectiveness of the decoupled softmax loss are well-supported by theoretical analysis and empirical results.
- **Medium confidence**: The claims about using extensive negatives providing more consistent loss feedback are supported by ablation studies but lack extensive theoretical justification.
- **Medium confidence**: The soft top-k loss directly optimizing for Recall@k metrics is theoretically sound but may have practical implementation challenges that could affect its effectiveness.

## Next Checks
1. **Gradient flow analysis for soft top-k**: Conduct a detailed analysis of gradient magnitudes and directions when using the soft top-k loss to verify that it provides meaningful gradients for optimization and doesn't suffer from vanishing or exploding gradients.

2. **Computational complexity evaluation**: Perform a systematic study comparing the computational costs (training time, memory usage) of the decoupled softmax loss with varying numbers of positives per batch, and evaluate the trade-off between performance gains and computational overhead.

3. **Generalization across tasks**: Evaluate the proposed loss functions on other multi-label classification tasks beyond XMC, such as multi-label image classification or multi-label text classification on different datasets, to assess the broader applicability of the findings.