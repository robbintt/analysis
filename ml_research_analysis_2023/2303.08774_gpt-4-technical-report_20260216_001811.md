---
ver: rpa2
title: GPT-4 Technical Report
arxiv_id: '2303.08774'
source_url: https://arxiv.org/abs/2303.08774
tags:
- gpt-4
- content
- language
- have
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GPT-4 is a large multimodal model capable of processing image and
  text inputs and producing text outputs. It exhibits human-level performance on various
  professional and academic benchmarks, including passing a simulated bar exam with
  a score around the top 10% of test takers.
---

# GPT-4 Technical Report

## Quick Facts
- arXiv ID: 2303.08774
- Source URL: https://arxiv.org/abs/2303.08774
- Authors: OpenAI
- Reference count: 40
- Primary result: GPT-4 is a large multimodal model with human-level performance on professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers.

## Executive Summary
GPT-4 is a large multimodal model that accepts image and text inputs and produces text outputs. It demonstrates human-level performance on various professional and academic benchmarks, with predictable scaling behavior that allows accurate performance predictions based on smaller models trained with up to 1,000x less compute. The model uses a Transformer-based architecture with a post-training alignment process that improves factuality and adherence to desired behavior, while significantly reducing hallucinations compared to GPT-3.5. Despite some remaining limitations like occasional reasoning errors, GPT-4 shows strong multilingual capabilities across 26 languages and represents a significant advancement in large language model capabilities.

## Method Summary
GPT-4 uses a Transformer-based architecture pre-trained to predict the next token in a document, followed by a post-training alignment process using Reinforcement Learning from Human Feedback (RLHF). The key innovation is the development of infrastructure and optimization methods that behave predictably across scales, enabling accurate performance predictions from smaller models. The model incorporates multimodal capabilities through integration of visual and textual data, and includes safety mitigations through adversarial testing and model-assisted safety pipelines. The training methodology follows established scaling laws, with the team successfully predicting final performance by fitting scaling laws from models using at most 10,000x less compute than GPT-4.

## Key Results
- Achieved human-level performance on professional benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers
- Demonstrated predictable scaling behavior, allowing accurate performance predictions based on models trained with up to 1,000x less compute
- Showed strong multilingual capabilities on MMLU benchmark across 26 languages, significantly outperforming previous models
- Reduced hallucinations compared to GPT-3.5 while maintaining strong performance on traditional NLP benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's performance can be predicted from smaller models trained with 1/1000th the compute.
- Mechanism: Scaling laws follow power-law relationships between compute and loss/capability, allowing extrapolation from smaller models.
- Core assumption: The optimization infrastructure and methodology behave predictably across scales.
- Evidence anchors:
  - [abstract]: "A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales."
  - [section 3.1]: "We predicted GPT-4's final loss on our internal codebase... by fitting a scaling law with an irreducible loss term."
  - [corpus]: Weak - no direct citations in neighbor papers, suggesting this scaling predictability is not yet widely replicated in literature.

### Mechanism 2
- Claim: GPT-4 exhibits human-level performance on professional and academic benchmarks.
- Mechanism: Large-scale pre-training on diverse datasets combined with RLHF fine-tuning aligns model behavior with human preferences and task requirements.
- Core assumption: The model's pre-training data and fine-tuning process adequately cover the knowledge domains tested in benchmarks.
- Evidence anchors:
  - [abstract]: "GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers."
  - [section 4]: "On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems."
  - [corpus]: Weak - neighbor papers focus on other multimodal models but don't directly validate GPT-4's benchmark performance claims.

### Mechanism 3
- Claim: GPT-4 can process image and text inputs to produce text outputs with similar capabilities as text-only inputs.
- Mechanism: Multimodal training incorporating both visual and textual data enables the model to understand and reason about visual information.
- Core assumption: Visual representations can be effectively integrated into the Transformer architecture alongside text tokens.
- Evidence anchors:
  - [abstract]: "GPT-4 is a large multimodal model which can accept image and text inputs and produce text outputs."
  - [section 4.1]: "GPT-4 accepts prompts consisting of both images and text... Over a range of domains—including documents with text and photographs, diagrams, or screenshots—GPT-4 exhibits similar capabilities as it does on text-only inputs."
  - [corpus]: Weak - neighbor papers describe other multimodal approaches but don't specifically validate GPT-4's visual processing capabilities.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: GPT-4 is built on Transformer architecture, which is fundamental to understanding its scaling behavior and multimodal capabilities.
  - Quick check question: How does self-attention enable the model to weigh different parts of input when making predictions?

- Concept: Scaling laws in deep learning
  - Why needed here: The paper's key contribution is demonstrating predictable scaling, which requires understanding how loss and capabilities scale with compute.
  - Quick check question: What mathematical relationship describes how model loss scales with training compute according to the paper?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: RLHF is the primary method used to align GPT-4's behavior with human preferences after pre-training.
  - Quick check question: What are the three main steps in the RLHF process described in the paper?

## Architecture Onboarding

- Component map: Pre-trained Transformer base model → RLHF fine-tuning pipeline → Safety mitigations (RBRMs, adversarial testing) → Deployment infrastructure
- Critical path: Pre-training → Capability prediction via scaling laws → RLHF fine-tuning → Safety evaluation → Deployment
- Design tradeoffs: Larger models provide better performance but increase computational cost and safety risks; multimodal capabilities add complexity but enable new applications
- Failure signatures: Performance degradation on benchmarks suggests scaling law breakdown; safety metric failures indicate RLHF misalignment; multimodal task failures suggest integration issues
- First 3 experiments:
  1. Replicate scaling law predictions using smaller models with 1/100th the compute of GPT-4
  2. Test capability predictions on HumanEval subset using the same methodology as the paper
  3. Evaluate multimodal performance on a simple image-text task to verify integration works as expected

## Open Questions the Paper Calls Out

- Question: What is the exact model size and architecture of GPT-4?
  - Basis in paper: [explicit] The paper states "This report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar."
  - Why unresolved: The authors intentionally withhold this information due to competitive and safety considerations.
  - What evidence would resolve it: Public disclosure of model specifications by OpenAI or independent analysis revealing the model's technical details.

- Question: How does GPT-4 perform on multilingual benchmarks beyond MMLU?
  - Basis in paper: [explicit] The paper mentions GPT-4's strong performance on MMLU across 26 languages but does not provide results for other multilingual benchmarks.
  - Why unresolved: The authors only report MMLU results for multilingual capabilities, leaving performance on other benchmarks unexplored.
  - What evidence would resolve it: Evaluation of GPT-4 on additional multilingual benchmarks like XTREME or XNLI, comparing performance across languages.

- Question: What are the long-term economic impacts of GPT-4 on different job sectors?
  - Basis in paper: [inferred] The paper briefly mentions potential job displacement and economic impacts but does not provide detailed analysis or projections.
  - Why unresolved: The authors acknowledge economic impacts but defer to future work, indicating this is an active area of research.
  - What evidence would resolve it: Longitudinal studies tracking GPT-4's adoption and impact on employment, wages, and industry structure across different sectors over time.

## Limitations

- The paper provides no technical details about model architecture, training compute, dataset construction, or specific training methods due to competitive and safety concerns
- Performance results, while impressive, cannot be independently verified due to the lack of methodological transparency and potential benchmark contamination
- The model still exhibits some limitations including occasional hallucinations, reasoning errors, and sensitivity to prompt phrasing

## Confidence

- **High Confidence**: Predictable scaling behavior claim supported by detailed methodology and successful performance predictions from smaller models
- **Medium Confidence**: Human-level benchmark performance claims well-documented but lack full transparency for independent verification
- **Low Confidence**: Multimodal capabilities claim demonstrated qualitatively but lacks rigorous quantitative evaluation and comparative analysis

## Next Checks

1. **Scaling Law Replication**: Attempt to reproduce the scaling law predictions by training a series of models at different scales using the same optimization infrastructure, then test whether the predicted losses match actual performance on a held-out dataset.

2. **Benchmark Independence Verification**: Conduct blind evaluations of GPT-4 on professional benchmarks using different test forms and evaluators to verify that the reported performance is not due to benchmark contamination or overfitting to specific test versions.

3. **Multimodal Capability Quantification**: Design controlled experiments comparing GPT-4's performance on tasks with and without visual inputs to quantify the actual benefit of multimodal processing, measuring whether the integration of visual information provides statistically significant improvements over text-only baselines.