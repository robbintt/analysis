---
ver: rpa2
title: Stochastic Vision Transformers with Wasserstein Distance-Aware Attention
arxiv_id: '2311.18645'
source_url: https://arxiv.org/abs/2311.18645
tags:
- learning
- stochastic
- self-supervised
- wasserstein
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty quantification
  and distance awareness in self-supervised learning models, which typically lack
  reliable confidence measures for their predictions. The authors propose a stochastic
  vision transformer that encodes image patches into elliptical Gaussian distributional
  embeddings instead of deterministic vectors.
---

# Stochastic Vision Transformers with Wasserstein Distance-Aware Attention

## Quick Facts
- arXiv ID: 2311.18645
- Source URL: https://arxiv.org/abs/2311.18645
- Reference count: 10
- Primary result: Stochastic vision transformer with Wasserstein distance-aware attention achieves CIFAR-100 top-1 accuracy of 69.42% with reduced calibration error (ECE of 0.445)

## Executive Summary
This paper addresses uncertainty quantification and distance awareness limitations in self-supervised vision transformers by introducing stochastic Gaussian distributional embeddings and Wasserstein distance-based attention. The method encodes image patches as elliptical Gaussian distributions (mean and covariance) rather than deterministic vectors, enabling the model to capture and propagate uncertainty through transformer layers. Wasserstein distance regularization terms are added to both pre-training and fine-tuning objectives to encourage distance awareness in latent representations. Extensive experiments demonstrate superior accuracy and calibration compared to self-supervised baselines across multiple tasks including in-distribution generalization, out-of-distribution detection, dataset corruption, and semi-supervised learning.

## Method Summary
The proposed method replaces deterministic vector embeddings with elliptical Gaussian distributional embeddings (mean vector and covariance matrix) for each image patch. A Wasserstein distance-based attention mechanism computes attention scores using the negative 2-Wasserstein distance between Gaussian embeddings, capturing both location and shape differences. During pre-training, a contrastive Wasserstein regularization term pulls together embeddings of masked and unmasked patches of the same image. During fine-tuning, regularization terms encourage separation between positive and negative example embeddings based on Wasserstein distance. The method is implemented using a ViT-B backbone and evaluated on CIFAR-100, CIFAR-10, and SVHN datasets with additional corruption and perturbation datasets for robustness evaluation.

## Key Results
- Achieves CIFAR-100 top-1 accuracy of 69.42% compared to baseline methods
- Reduces Expected Calibration Error (ECE) from 0.475 to 0.445
- Demonstrates improved out-of-distribution detection with higher AUROC scores
- Shows robustness to dataset corruption with improved mean Corruption Error (mCE)
- Performs well in semi-supervised learning settings with limited labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing deterministic dot-product attention with Wasserstein distance enables the model to capture distributional uncertainty between embeddings.
- Mechanism: By embedding patches as elliptical Gaussian distributions (mean and covariance), the attention mechanism computes negative 2-Wasserstein distance instead of dot products. This captures both location and shape differences between distributions, allowing the model to weigh uncertain or distant embeddings less.
- Core assumption: Gaussian distributional embeddings adequately represent uncertainty and that Wasserstein distance between Gaussians is tractable and meaningful for attention scoring.
- Evidence anchors:
  - [abstract]: "attention matrices of these stochastic representational embeddings are computed using Wasserstein distance-based attention"
  - [section 4.2]: "AQ,K = −(W 2 2 (Q, K)) = −(||µQ − µK|| 2 + Tr(ΣQ + ΣK − 2(Σ1/2 Q ΣQΣ1/2 K )1/2))"
  - [corpus]: No direct evidence; this is an inference from the described formulation.
- Break condition: If the covariance estimates are poorly calibrated or if non-Gaussian uncertainties dominate, the Wasserstein distance will not accurately reflect true embedding differences.

### Mechanism 2
- Claim: Contrastive Wasserstein regularization terms improve distance awareness and robustness by pulling together embeddings of similar items and pushing apart embeddings of dissimilar items.
- Mechanism: The regularization adds a loss term based on the Wasserstein distance between the stochastic embedding of a masked patch and its unmasked counterpart (pre-training) or between positive and negative examples (fine-tuning). This encourages the model to encode semantic similarity as proximity in the distributional embedding space.
- Core assumption: Wasserstein distance is an appropriate metric for measuring similarity in the learned embedding space and that the regularization strength λ is properly tuned.
- Evidence anchors:
  - [abstract]: "We propose a regularization term based on Wasserstein distance for both pre-training and fine-tuning processes"
  - [section 4.2]: "the cumulative loss term during pre-training ... is given as follows: Lp = Lp − λ log(σ(−W 2 2 (zout, fz(y+)))"
  - [corpus]: No direct evidence; inferred from the described regularization formulation.
- Break condition: If λ is too large, the regularization dominates training and corrupts the main learning objective; if too small, the regularization has negligible effect.

### Mechanism 3
- Claim: Stochastic Gaussian embeddings allow the model to propagate uncertainty through the transformer layers, improving calibration and robustness.
- Mechanism: Each token is embedded as a Gaussian distribution (µ, σ) with separate positional encodings. This distributional information is carried through each transformer block via the Wasserstein attention and propagated to downstream tasks, enabling the model to produce calibrated uncertainty estimates.
- Core assumption: Maintaining distributional information through multiple transformer layers is computationally tractable and that the final representation preserves meaningful uncertainty signals.
- Evidence anchors:
  - [abstract]: "encodes image patches into elliptical Gaussian distributional embeddings"
  - [section 4.1]: "We represent tokens in the stochastic Gaussian embedding space with mean µ and variance σ vectors"
  - [corpus]: No direct evidence; this is an inference from the described architecture.
- Break condition: If the distributional information is lost or distorted in later layers, the final representations will not encode meaningful uncertainty.

## Foundational Learning

- Concept: Wasserstein distance between Gaussian distributions
  - Why needed here: The method relies on closed-form Wasserstein distance for efficient attention computation and regularization.
  - Quick check question: What is the closed-form expression for 2-Wasserstein distance between two d-dimensional Gaussian distributions N(µ1, Σ1) and N(µ2, Σ2)?

- Concept: Elliptical Gaussian distributions and their parameterization
  - Why needed here: The embeddings are parameterized as elliptical Gaussians (mean vector and covariance matrix) to encode both location and uncertainty.
  - Quick check question: Why is the ELU activation used on the covariance in Equation 5?

- Concept: Contrastive learning objectives and regularization
  - Why needed here: The method adds contrastive Wasserstein regularization to encourage distance awareness in the embedding space.
  - Quick check question: How does the l1 regularization term in Equation 12 differ from the l2 term in Equation 13 in terms of what they encourage the model to learn?

## Architecture Onboarding

- Component map:
  Input: Image patches → Stochastic Gaussian Embedding Layer (mean, covariance with positional encodings) → Transformer Encoder Blocks (LayerNorm, Wasserstein Attention, projection) → Projection Head → Task output

- Critical path:
  1. Image patches → Stochastic Gaussian Embedding (µ, σ)
  2. Embedding → Transformer blocks (Wasserstein attention)
  3. Final embedding → Projection head → Task output
  4. Loss computation with Wasserstein regularization

- Design tradeoffs:
  - Using Gaussian embeddings adds computational overhead (covariance matrices) but enables uncertainty modeling
  - Wasserstein attention is more expensive than dot-product but captures distributional differences
  - Regularization strength λ must be carefully tuned to balance main task and distance awareness

- Failure signatures:
  - Poor calibration (high ECE) despite good accuracy suggests distributional embeddings are not capturing uncertainty well
  - Instability during training (exploding/vanishing gradients) may indicate λ is too large
  - Degraded accuracy suggests the distributional modeling is interfering with representation learning

- First 3 experiments:
  1. Train with deterministic embeddings (replace Gaussian with vectors) to verify that distributional modeling is necessary for improved calibration
  2. Remove Wasserstein regularization terms to verify they are necessary for distance awareness
  3. Vary λ (e.g., 1e-6, 1e-5, 1e-4) to find the optimal regularization strength for a given dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed stochastic vision transformer scale with increasing model size and dataset complexity?
- Basis in paper: [explicit] The paper mentions using ViT-B backbone and experiments on CIFAR datasets, but does not explore scaling to larger models or more complex datasets.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed method on relatively small-scale datasets and a single backbone architecture.
- What evidence would resolve it: Experiments with larger ViT models (e.g., ViT-L) on more complex datasets (e.g., ImageNet) and comparison of performance gains relative to computational costs.

### Open Question 2
- Question: What is the impact of different stochastic embedding distributions (e.g., t-distribution, mixture models) on the performance and robustness of the proposed method?
- Basis in paper: [inferred] The paper uses elliptical Gaussian distributional embeddings but does not explore alternative distribution choices.
- Why unresolved: The choice of Gaussian distributions is motivated by computational tractability, but other distributions might offer better representation or robustness properties.
- What evidence would resolve it: Comparative experiments using different distributional embeddings (e.g., t-distribution, mixture models) on the same tasks and evaluation of their impact on performance and robustness metrics.

### Open Question 3
- Question: How does the proposed method perform in real-world applications with limited labeled data and domain shifts?
- Basis in paper: [explicit] The paper evaluates the method on semi-supervised learning tasks but does not explore real-world applications or significant domain shifts.
- Why unresolved: The semi-supervised experiments use a fixed dataset split and controlled domain shift scenarios, which may not reflect the complexity of real-world applications.
- What evidence would resolve it: Application of the method to real-world datasets with significant domain shifts (e.g., medical imaging across different scanners or clinical sites) and evaluation of performance under limited labeled data conditions.

## Limitations
- The method relies on the assumption that elliptical Gaussian distributional embeddings adequately capture visual uncertainty, which may not hold for non-Gaussian or multi-modal uncertainty distributions.
- Significant computational overhead is introduced through covariance matrix operations and Wasserstein distance computations, potentially limiting scalability to larger datasets or higher-resolution images.
- Performance gains come with increased model complexity and training instability risks, particularly around the tuning of regularization parameters λ.

## Confidence
- High Confidence: The empirical improvements in calibration metrics (ECE reduction from 0.475 to 0.445) and out-of-distribution detection (AUROC improvements) are well-supported by experimental results across multiple datasets and tasks. The theoretical foundation of using Wasserstein distance for distributional embeddings is mathematically sound.
- Medium Confidence: The claim that Wasserstein-based attention is necessary for the observed performance improvements is supported by ablation studies, but alternative uncertainty-aware attention mechanisms could potentially achieve similar results. The optimal values for regularization parameters λ1 and λ2 may be dataset-dependent, suggesting the improvements might not generalize uniformly.
- Low Confidence: The assertion that the method will scale effectively to larger-scale datasets like ImageNet without architectural modifications is not validated. The computational complexity analysis is absent, making it unclear whether the benefits justify the additional costs in resource-constrained settings.

## Next Checks
1. Evaluate the method on datasets where visual uncertainty is known to be non-Gaussian (e.g., ambiguous object boundaries or multi-scale features) to verify the distributional assumption holds.
2. Measure and report wall-clock training time and memory usage compared to deterministic baselines to assess practical scalability limitations.
3. Implement a baseline using a different uncertainty-aware attention mechanism (e.g., Monte Carlo dropout or ensemble methods) to isolate whether Wasserstein distance specifically contributes to the performance gains.