---
ver: rpa2
title: Which Augmentation Should I Use? An Empirical Investigation of Augmentations
  for Self-Supervised Phonocardiogram Representation Learning
arxiv_id: '2312.00502'
source_url: https://arxiv.org/abs/2312.00502
tags:
- learning
- data
- classification
- signals
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing robust deep learning
  models for phonocardiogram (PCG) classification, particularly when faced with limited
  annotated data and out-of-distribution (OOD) samples. The authors propose using
  self-supervised contrastive learning to pretrain models on unlabeled PCG data, then
  fine-tune them for downstream classification tasks.
---

# Which Augmentation Should I Use? An Empirical Investigation of Augmentations for Self-Supervised Phonocardiogram Representation Learning

## Quick Facts
- arXiv ID: 2312.00502
- Source URL: https://arxiv.org/abs/2312.00502
- Reference count: 40
- Primary result: Self-supervised contrastive learning with specific audio augmentations (low-pass filters, uniform noise, signal inversion/reversal) significantly improves PCG model robustness to out-of-distribution data

## Executive Summary
This paper addresses the challenge of developing robust deep learning models for phonocardiogram (PCG) classification, particularly when faced with limited annotated data and out-of-distribution (OOD) samples. The authors propose using self-supervised contrastive learning to pretrain models on unlabeled PCG data, then fine-tune them for downstream classification tasks. They conduct an extensive empirical study evaluating various audio-based augmentations for improving model robustness. Key findings include that augmentations like low-pass filters, uniform noise, and signal inversion/reversal significantly enhance performance. Models trained with contrastive SSL demonstrate greater robustness to OOD data, losing only 10% effectiveness compared to 32% for fully supervised models.

## Method Summary
The method uses contrastive self-supervised learning pretraining on unlabeled PCG data using a SimCLR framework with various augmentations, followed by supervised fine-tuning on labeled data. Five public PCG datasets are used (Ephnogram, FPCGDB, Pascal, PhysioNet2016, PhysioNet2022). Data is preprocessed into 5-second overlapping windows at 2 kHz sampling rate with binary normal/abnormal labels. The SSL pretraining uses a 5-layer CNN encoder with NT-Xent contrastive loss, applying augmentation combinations including low-pass filters, noise, scaling, reversal, inversion, and random flip. The pretrained encoder is then fine-tuned on labeled data from one dataset and evaluated on both in-distribution and OOD data.

## Key Results
- Contrastive SSL pretraining improves OOD robustness, with only 10% performance drop versus 32% for fully supervised models
- Low-pass filters, uniform noise, signal inversion/reversal, and random scaling augmentations significantly enhance SSL performance
- SSL pretraining compensates for limited labeled data while maintaining adequate inference results
- The SSL paradigm shows particular promise for medical domains with scarce annotated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive SSL improves PCG model robustness to out-of-distribution (OOD) data.
- Mechanism: SSL pretraining teaches the encoder to extract invariant features by maximizing agreement between different augmentations of the same signal and minimizing agreement across different signals. This learned invariance generalizes better to unseen data distributions.
- Core assumption: The augmented views preserve meaningful signal characteristics while removing dataset-specific artifacts.
- Evidence anchors:
  - [abstract]: "Models trained with contrastive SSL demonstrate greater robustness to OOD data, losing only 10% effectiveness compared to 32% for fully supervised models."
  - [section]: "The SSL paradigm seems to fit quite well in the medical and healthcare domain... several works have proposed SSL approaches for processing biosignals in the past."
- Break condition: If augmentations destroy clinically relevant signal features or if the pretraining dataset is too small/narrow to learn general patterns.

### Mechanism 2
- Claim: Specific augmentations (low-pass filters, uniform noise, signal inversion/reversal) significantly enhance SSL performance.
- Mechanism: These augmentations simulate real-world variations in PCG recordings (e.g., sensor noise, body size effects, microphone placement) during pretraining, forcing the model to learn robust features that are invariant to such variations.
- Core assumption: The chosen augmentations reflect realistic variations encountered in clinical practice.
- Evidence anchors:
  - [abstract]: "Key findings include that augmentations like low-pass filters, uniform noise, and signal inversion/reversal significantly enhance performance."
  - [section]: "We select to implement functions that either simulate factors that can be found in real world scenarios... to at least either add artificial noise to randomly scale each audio window during the pretraining task."
- Break condition: If the augmentations introduce unrealistic distortions that don't reflect actual clinical scenarios, leading to overfitting to artificial patterns.

### Mechanism 3
- Claim: SSL pretraining compensates for limited labeled data in medical domains.
- Mechanism: By leveraging large unlabeled datasets, SSL pretrains a feature extractor that learns general PCG representations without requiring expensive manual annotation. This pretrained encoder can then be fine-tuned on smaller labeled datasets.
- Core assumption: Unlabeled PCG data contains sufficient signal diversity to learn useful general representations.
- Evidence anchors:
  - [abstract]: "The shortage of high-quality annotated data often hinders the development of robust and generalizable models... Self-Supervised Learning (SSL) contrastive learning... has shown promise in mitigating the issue of data scarcity by using unlabeled data."
  - [section]: "In healthcare, where sufficient annotated datasets are scarcely available, there proves a need to provide systems which can take advantage of the small amount of data but also yield adequate and trustworthy inference results."
- Break condition: If the unlabeled dataset lacks diversity or is biased toward a specific population, limiting the generalizability of learned features.

## Foundational Learning

- Concept: Contrastive learning loss (NT-Xent)
  - Why needed here: This loss function drives the model to learn representations where positive pairs (different augmentations of same signal) are closer than negative pairs (augmentations from different signals).
  - Quick check question: What happens to the loss if all embeddings are identical? (Answer: Loss approaches 0, but model hasn't learned useful representations)

- Concept: Data augmentation for 1D signals
  - Why needed here: Augmentation simulates realistic variations in PCG recordings, teaching the model invariance to clinically relevant factors like noise, scale, and signal direction.
  - Quick check question: Why is random scaling augmentation useful for PCG signals? (Answer: Simulates different body sizes and sensor conductances affecting signal amplitude)

- Concept: Domain generalization
  - Why needed here: The evaluation tests whether models trained on one dataset generalize to others, which is critical for medical applications where data distribution shifts are common.
  - Quick check question: What's the difference between domain adaptation and domain generalization? (Answer: DA assumes target labels are available; DG does not)

## Architecture Onboarding

- Component map: 5-layer 1D CNN encoder -> Dense projection layer (128-dim) -> Contrastive loss (NT-Xent) -> Downstream classification head (3-layer MLP) -> Augmentation pipeline (6 types, various combinations)

- Critical path: Pretraining → Frozen encoder → Classification head attachment → Fine-tuning → Evaluation

- Design tradeoffs:
  - Larger batch sizes improve contrastive learning but require more memory
  - Stronger augmentations improve robustness but risk losing signal integrity
  - Deeper encoders may learn better features but risk overfitting with limited data

- Failure signatures:
  - Poor OOD performance despite good in-distribution results suggests overfitting to training domain
  - Low SSL pretraining loss but poor downstream performance suggests learned representations aren't useful for classification
  - High variance in results across runs suggests instability in training procedure

- First 3 experiments:
  1. Train baseline fully-supervised model on PhysioNet2016 with standard augmentations
  2. Pretrain SSL encoder on all available data using low-pass filters + uniform noise, then fine-tune on PhysioNet2016
  3. Compare OOD performance of both models on PhysioNet2022 dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific combinations of augmentations perform across different PCG datasets and tasks?
- Basis in paper: [explicit] The paper mentions that low-pass filters, uniform noise, signal inversion/reversal, and random scaling are effective, but does not provide a comprehensive analysis of specific combinations across all datasets.
- Why unresolved: The paper focuses on general trends and overall effectiveness, rather than providing detailed breakdowns of individual augmentation combinations for each dataset and task.
- What evidence would resolve it: A systematic evaluation of all possible augmentation combinations for each dataset and task, reporting their performance metrics.

### Open Question 2
- Question: How does the proposed SSL method compare to other domain adaptation and generalization techniques for PCG classification?
- Basis in paper: [inferred] The paper mentions domain adaptation and generalization as related fields but does not directly compare the proposed SSL method to these approaches.
- Why unresolved: The paper focuses on the benefits of SSL and augmentations, but does not explore its relative performance against other established methods.
- What evidence would resolve it: A comparative study of the proposed SSL method against state-of-the-art domain adaptation and generalization techniques on the same datasets and tasks.

### Open Question 3
- Question: How does the proposed SSL method generalize to other biosignal classification tasks beyond PCG?
- Basis in paper: [explicit] The paper mentions the potential for applying the method to other 1D biosignals like EMG, but does not provide experimental results.
- Why unresolved: The paper focuses on PCG classification and does not explore the broader applicability of the method to other biosignal domains.
- What evidence would resolve it: Applying the proposed SSL method to other biosignal datasets and tasks, such as EEG or ECG, and reporting the results.

## Limitations
- Results may not generalize to other biosignal types without validation
- Augmentation choices lack theoretical justification for why certain transformations work better
- Evaluation focuses primarily on binary classification, limiting insights into clinical multi-class scenarios

## Confidence
- **High Confidence**: The general finding that contrastive SSL improves OOD robustness compared to fully supervised training (10% vs 32% performance drop)
- **Medium Confidence**: The specific ranking of augmentation effectiveness and their quantitative impact on performance metrics
- **Medium Confidence**: The claim that SSL pretraining effectively compensates for limited labeled data, given the mixed results across different datasets

## Next Checks
1. Test the same augmentation strategies on different biosignal types (ECG, EEG) to verify domain generality
2. Conduct ablation studies removing individual augmentation types to quantify their independent contributions
3. Evaluate model performance on multi-class PCG classification tasks to assess real-world clinical applicability