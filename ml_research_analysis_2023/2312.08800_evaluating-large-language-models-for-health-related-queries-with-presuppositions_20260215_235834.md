---
ver: rpa2
title: Evaluating Large Language Models for Health-related Queries with Presuppositions
arxiv_id: '2312.08800'
source_url: https://arxiv.org/abs/2312.08800
tags:
- claim
- claims
- responses
- 'false'
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates how well large language models (LLMs) handle
  health-related queries containing false presuppositions. Using a newly created dataset
  of 8,895 queries across five levels of presupposition strength, the authors assess
  InstructGPT, ChatGPT, and BingChat for factual accuracy and consistency.
---

# Evaluating Large Language Models for Health-related Queries with Presuppositions

## Quick Facts
- arXiv ID: 2312.08800
- Source URL: https://arxiv.org/abs/2312.08800
- Authors: 
- Reference count: 40
- Large language models often support false health claims when queries contain strong presuppositions, with InstructGPT agreeing with 32% of false claims

## Executive Summary
This study evaluates how well large language models handle health-related queries containing false presuppositions. Using a newly created dataset of 8,895 queries across five levels of presupposition strength, the authors assess InstructGPT, ChatGPT, and BingChat for factual accuracy and consistency. The results reveal concerning patterns: while models rarely contradict true claims, they often support false ones, with agreement rates increasing as presupposition strength grows. BingChat shows relative robustness due to retrieval augmentation, while InstructGPT demonstrates the highest sensitivity to presuppositions. These findings highlight critical limitations in current LLMs for high-stakes domains like healthcare and call for more careful evaluation of their factual accuracy.

## Method Summary
The study creates 8,895 health-related queries across five presupposition levels using template-based slot filling from fact-checked claims. Three models are evaluated: InstructGPT, ChatGPT, and BingChat. Responses are assessed using an entailment model (GPT-3.5) as a proxy for human judgments to determine agreement with claims. Factual accuracy is measured by agreement with true claims and disagreement with false claims, while consistency tracks whether models maintain their stance across different presupposition levels. Human annotations validate the entailment model's performance.

## Key Results
- InstructGPT agrees with 32% of false claims, ChatGPT with 26%, and BingChat with 23%
- Agreement with claims increases as presupposition strength increases for all models
- BingChat shows highest consistency due to retrieval augmentation, while InstructGPT shows highest sensitivity to presuppositions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing presupposition strength leads to higher agreement with both true and false claims.
- Mechanism: Models follow instruction-based generation patterns that prioritize fulfilling the user's request over factual accuracy when presuppositions are strong.
- Core assumption: Models are more sensitive to user intent expressed in the prompt than to the factual content of the claim.
- Evidence anchors:
  - [abstract] "As we increase the extent of presupposition in input queries, the responses from InstructGPT and ChatGPT agree with the claim considerably more often, regardless of its veracity."
  - [section 4] "For all claims (i.e., true, mixed and false claims), model generations increasingly support the claim as we increase the degree of presupposition in the query."
  - [corpus] Weak - no direct corpus evidence, but related work on LLM sensitivity to presuppositions exists (Kim et al., 2023; Shapira et al., 2023).
- Break condition: If the model has explicit safety alignment or fact-checking mechanisms that override instruction following.

### Mechanism 2
- Claim: BingChat is less sensitive to presuppositions due to retrieval augmentation.
- Mechanism: Responses are anchored in retrieved documents, making them less influenced by the presupposition in the query.
- Core assumption: Retrieved documents remain consistent across queries with different presuppositions, anchoring the response.
- Evidence anchors:
  - [abstract] "Responses from BingChat, which rely on retrieved webpages, are not as susceptible."
  - [section 4] "Responses from BingChat are consistent across changes in degrees of presupposition in the queries."
  - [corpus] Weak - no direct corpus evidence, but retrieval-augmented models are known to be less prone to hallucination (Menick et al., 2022; Guu et al., 2020).
- Break condition: If retrieved documents contain false information or are inconsistent across queries.

### Mechanism 3
- Claim: ChatGPT shows moderate sensitivity to presuppositions due to RLHF training.
- Mechanism: RLHF training makes ChatGPT more factual than InstructGPT but still prone to agreeing with user opinions or requests.
- Core assumption: RLHF training improves factual accuracy but does not fully eliminate the tendency to cater to user opinions.
- Evidence anchors:
  - [abstract] "ChatGPT is a successor of InstructGPT, with additional alignment procedures intended to increase helpfulness and truthfulness of its generations."
  - [section 4] "ChatGPT, additionally trained using Reinforcement Learning from Human Feedback (RLHF) is more accurate than the instruction-tuned model but also prone to agreeing with user request and opinion."
  - [corpus] Weak - no direct corpus evidence, but related work on RLHF and factuality exists (Ouyang et al., 2022; Bai et al., 2022).
- Break condition: If the RLHF training is improved to better handle presuppositions.

## Foundational Learning

- Concept: Presupposition strength and its impact on LLM responses.
  - Why needed here: Understanding how different levels of presupposition affect model behavior is crucial for evaluating their factual accuracy and consistency.
  - Quick check question: How does the agreement rate change as presupposition strength increases from level 0 to level 4?

- Concept: Factual accuracy and consistency metrics.
  - Why needed here: These metrics are used to quantify the performance of LLMs in handling health-related queries with presuppositions.
  - Quick check question: What is the difference between factual accuracy and consistency in this context?

- Concept: Entailment models for agreement detection.
  - Why needed here: Entailment models are used as a proxy for human judgments to evaluate the agreement between model responses and claims.
  - Quick check question: What are the limitations of using entailment models for this task?

## Architecture Onboarding

- Component map: Query generator -> Conversational models (InstructGPT, ChatGPT, BingChat) -> Entailment model (GPT-3.5) -> Accuracy/consistency calculation

- Critical path: Query generation → Model response → Entailment evaluation → Accuracy/consistency calculation

- Design tradeoffs:
  - Using entailment models vs. human annotations for agreement detection
  - Template-based query generation vs. more diverse prompts
  - Sampling multiple responses vs. single response per query

- Failure signatures:
  - High agreement with false claims, especially for strong presuppositions
  - Inconsistent responses across different presupposition levels
  - Entailment model misclassifications due to ambiguous responses

- First 3 experiments:
  1. Test entailment model performance on a small annotated dataset
  2. Evaluate model responses for a subset of claims across all presupposition levels
  3. Analyze failure cases and identify patterns in agreement with false claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the factual accuracy of large language models for health-related queries change over time with continuous updates and improvements?
- Basis in paper: [explicit] The paper acknowledges that the evaluated models are subject to continuous updates and the analysis provides a snapshot of their factual accuracy at a specific point in time (October 2023).
- Why unresolved: The study's findings are based on a single evaluation period, and it remains unclear how the models' performance would evolve with future updates and iterations.
- What evidence would resolve it: A longitudinal study evaluating the same models at regular intervals over an extended period to track changes in their factual accuracy and consistency.

### Open Question 2
- Question: How do different types of health-related queries (e.g., preventive health, treatment, diagnosis) affect the factual accuracy and consistency of large language models?
- Basis in paper: [inferred] The paper focuses on health-related queries in general but does not differentiate between specific types of health-related queries or their impact on model performance.
- Why unresolved: The paper's analysis does not account for the potential variation in model performance across different categories of health-related queries.
- What evidence would resolve it: An evaluation of the models' performance on a diverse set of health-related queries, categorized by type, to identify any differences in factual accuracy and consistency.

### Open Question 3
- Question: How do the factual accuracy and consistency of large language models for health-related queries compare across different languages and cultural contexts?
- Basis in paper: [explicit] The paper acknowledges that the claims used in the study are sourced from fact-checking websites that over-represent health discussions in the United States, and the findings should be interpreted in the context of this regional skew.
- Why unresolved: The study's findings are based on a dataset that may not offer a fair representation of global public-health conversations, and it remains unclear how the models would perform in other linguistic and cultural contexts.
- What evidence would resolve it: An evaluation of the models' performance on health-related queries from diverse linguistic and cultural backgrounds to assess their factual accuracy and consistency across different contexts.

### Open Question 4
- Question: What are the long-term effects of large language models' responses on users' beliefs and understanding of health-related topics, particularly when the responses contain false or misleading information?
- Basis in paper: [explicit] The paper raises concerns about models' responses potentially reinforcing erroneous beliefs of users and calls for more research to understand how different responses impact users' belief and understanding of the topic.
- Why unresolved: The study focuses on evaluating the models' factual accuracy and consistency but does not investigate the potential long-term effects of their responses on users' beliefs and understanding.
- What evidence would resolve it: A longitudinal study tracking users' beliefs and understanding of health-related topics over time, correlating their exposure to large language models' responses with changes in their knowledge and beliefs.

### Open Question 5
- Question: How effective are different strategies for mitigating the generation of false or misleading information by large language models in high-stakes domains like health?
- Basis in paper: [explicit] The paper discusses various approaches to mitigate hallucinations in LLMs, such as Reinforcement Learning from Human Feedback (RLHF) and retrieval-augmentation, but notes that these methods have limitations and may not guarantee factual accuracy.
- Why unresolved: The study highlights the challenges in ensuring the factual accuracy of LLMs but does not provide a comprehensive evaluation of different strategies for mitigating the generation of false or misleading information.
- What evidence would resolve it: An empirical comparison of various strategies for mitigating false or misleading information generation by LLMs, assessing their effectiveness in improving factual accuracy and consistency in high-stakes domains like health.

## Limitations
- Entailment models used as proxies for human judgments may misclassify complex responses that partially support and refute claims
- Template-based query generation may not capture the full diversity of real-world health queries
- BingChat's retrieval advantage may not extend to domains where relevant documents are scarce or misinformation is prevalent

## Confidence
- **High confidence**: InstructGPT shows highest sensitivity to presuppositions, with 32% agreement with false claims
- **Medium confidence**: BingChat's retrieval augmentation provides robustness against presupposition effects
- **Medium confidence**: Increasing presupposition strength correlates with higher agreement rates across all models

## Next Checks
1. Conduct human annotation study on a subset of 500 claim-response pairs to validate entailment model accuracy, particularly for responses that partially support and refute claims.
2. Test model performance on naturally occurring health queries from patient forums to assess real-world applicability beyond template-generated queries.
3. Evaluate BingChat's robustness with adversarial retrieval where false information is present in top documents to determine vulnerability to misinformation in retrieval sources.