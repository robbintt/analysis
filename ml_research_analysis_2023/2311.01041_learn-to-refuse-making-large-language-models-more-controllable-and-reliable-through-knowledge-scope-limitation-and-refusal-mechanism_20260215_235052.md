---
ver: rpa2
title: 'Learn to Refuse: Making Large Language Models More Controllable and Reliable
  through Knowledge Scope Limitation and Refusal Mechanism'
arxiv_id: '2311.01041'
source_url: https://arxiv.org/abs/2311.01041
tags:
- knowledge
- answer
- question
- refusal
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models often produce hallucinations that reduce\
  \ their reliability, particularly when answering factual questions. To address this,\
  \ the paper introduces Learn to Refuse (L2R), a system that limits an LLM\u2019\
  s knowledge scope using an independent, structured knowledge base and incorporates\
  \ a refusal mechanism."
---

# Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism

## Quick Facts
- arXiv ID: 2311.01041
- Source URL: https://arxiv.org/abs/2311.01041
- Reference count: 7
- Large language models often produce hallucinations that reduce their reliability, particularly when answering factual questions

## Executive Summary
Large language models (LLMs) frequently generate hallucinations when answering factual questions, reducing their reliability. Learn to Refuse (L2R) addresses this by separating the LLM's knowledge from its parametric memory into an independent, structured knowledge base. The system employs a two-layer refusal mechanism—combining hard refusal based on retrieval confidence with soft refusal guided by LLM self-knowledge—to decline answering questions beyond its knowledge scope. Experiments on the TruthfulQA dataset demonstrate significant accuracy improvements, achieving over 90% accuracy when at least 25% of the knowledge is gold, by selectively refusing uncertain questions.

## Method Summary
L2R limits an LLM's knowledge scope by using an independent structured knowledge base that contains all the LLM's understanding of the world. The system employs a two-layer refusal mechanism: hard refusal based on retrieval confidence thresholds and soft refusal guided by LLM self-knowledge about its answerability. Knowledge is automatically populated through an Automatic Knowledge Enrichment method that generates pseudo-knowledge with confidence scores by having the LLM answer self-generated questions. The LLM is instructed to rely solely on the structured knowledge base for answering questions, with no access to its internal parametric knowledge, enabling controlled and traceable responses.

## Key Results
- L2R increases MC1 accuracy from 46.6% to 65.1% on the TruthfulQA dataset
- System achieves over 90% accuracy when at least 25% of the knowledge is gold
- Significant improvement in reliability by selectively refusing uncertain questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating knowledge from the LLM into an independent structured knowledge base enables controlled and traceable responses.
- Mechanism: The LLM is instructed to rely solely on the structured knowledge base for answering questions, with no access to its internal parametric knowledge. This creates a clear knowledge boundary.
- Core assumption: LLMs contain unreliable or incomplete knowledge that can lead to hallucinations when answering factual questions.
- Evidence anchors:
  - [abstract] "We utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge."
  - [section 3.3] "Knowledge Scope Limitation means using a independent, limited, and structured knowledge base to represent the knowledge scope of an LLM."
  - [corpus] Weak evidence - the related papers focus on refusal mechanisms but don't explicitly discuss knowledge base separation as a core mechanism.

### Mechanism 2
- Claim: The refusal mechanism prevents the LLM from answering questions beyond its knowledge scope, reducing hallucinations.
- Mechanism: Two-layer refusal - soft refusal (LLM self-assessment) and hard refusal (retrieval confidence threshold) work together to decide whether to answer or refuse a question.
- Core assumption: LLMs have some self-knowledge about their knowledge boundaries and can accurately judge when they cannot answer a question.
- Evidence anchors:
  - [abstract] "We explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors."
  - [section 3.5] "Soft refusal is a mechanism where we instruct LLMs through prompts to independently judge the answerability Isoft i of a question Qi."
  - [corpus] Moderate evidence - several related papers discuss knowledge-aware refusal, supporting the concept.

### Mechanism 3
- Claim: Automatic Knowledge Enrichment (AKE) can efficiently populate the knowledge base with high-confidence pseudo-knowledge.
- Mechanism: The LLM generates questions, answers them with confidence scores, and converts QA pairs into knowledge entries that are added to the structured knowledge base.
- Core assumption: LLMs can generate factually correct knowledge with high confidence when properly prompted and validated.
- Evidence anchors:
  - [abstract] "We introduce a method for automatically and efficiently expanding the knowledge base of LLMs."
  - [section 3.3] "It enables the rapid addition of knowledge to the knowledge base, ensuring a high quality of knowledge simultaneously."
  - [corpus] Weak evidence - no direct evidence in the corpus about AKE specifically, but the concept is supported by LLM self-knowledge research.

## Foundational Learning

- Concept: Knowledge base construction and management
  - Why needed here: The system's reliability depends on having a well-structured, accurate knowledge base that the LLM can query
  - Quick check question: What are the key characteristics of an effective knowledge base for this system (e.g., size, structure, update mechanism)?

- Concept: Retrieval augmented generation (RAG)
  - Why needed here: RAG is the mechanism by which the LLM accesses relevant knowledge from the structured knowledge base to answer questions
  - Quick check question: How does the retrieval algorithm determine which knowledge entries are most relevant to a given question?

- Concept: Prompt engineering for controlled LLM behavior
  - Why needed here: Precise prompts are needed to instruct the LLM to follow the knowledge base only, refuse when appropriate, and provide step-by-step reasoning
  - Quick check question: What are the key elements that must be included in the prompt to ensure the LLM behaves as intended?

## Architecture Onboarding

- Component map: Question Generation AI -> Answer Generation AI -> QA Pair to Knowledge AI -> Knowledge Base -> Retrieval Engine -> Main QA AI -> Refusal Mechanism
- Critical path: Question → Retrieval → Refusal Decision → Answer Generation (if approved)
- Design tradeoffs:
  - Knowledge base size vs. retrieval accuracy: Larger knowledge bases may contain more relevant information but make retrieval more challenging
  - Hard refusal threshold: Higher thresholds reduce answered questions but improve accuracy; lower thresholds do the opposite
  - LLM model size: Larger models may have better self-knowledge for soft refusal but are more expensive to run
- Failure signatures:
  - High refusal rate with low accuracy: Hard refusal threshold too high or knowledge base too limited
  - Low refusal rate with poor accuracy: Hard refusal threshold too low or LLM self-knowledge inadequate
  - Slow response times: Knowledge base too large for efficient retrieval or model too large for quick inference
- First 3 experiments:
  1. Vary the hard refusal threshold α and measure the tradeoff between accuracy and number of answered questions
  2. Compare knowledge base construction methods: manual vs. automatic enrichment with different confidence thresholds
  3. Test different retrieval algorithms (e.g., semantic similarity vs. keyword matching) and measure their impact on answer accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of L2R scale with the size of the knowledge base, particularly when it reaches millions of entries?
- Basis in paper: [inferred] The paper mentions that experiments were conducted with a limited number of knowledge entries due to resource constraints, and it is uncertain how the system would perform with a much larger knowledge base.
- Why unresolved: The paper does not provide data on the system's performance with a significantly larger knowledge base, which is crucial for understanding its scalability and real-world applicability.
- What evidence would resolve it: Conducting experiments with a knowledge base that contains millions of entries and comparing the performance of L2R in terms of accuracy, response time, and resource utilization to the current performance with a smaller knowledge base.

### Open Question 2
- Question: Can the refusal mechanism in L2R be improved to handle more complex questions that require multiple pieces of knowledge or multi-step reasoning?
- Basis in paper: [inferred] The paper acknowledges that the current refusal function is simple and may not be sufficient for complex questions that need multiple knowledge or reasoning steps.
- Why unresolved: The paper does not explore advanced refusal functions or strategies to deal with complex questions, leaving a gap in the system's ability to handle a broader range of queries.
- What evidence would resolve it: Developing and testing an enhanced refusal mechanism that can effectively identify and handle complex questions, followed by evaluating its impact on the system's accuracy and refusal rate.

### Open Question 3
- Question: How can L2R be adapted for application scenarios beyond question-answering, such as text summarization or decision-making?
- Basis in paper: [inferred] The paper focuses on question-answering scenarios and notes that the system cannot be directly applied to other tasks like text summarization or decision-making without further work.
- Why unresolved: The paper does not provide insights into how the L2R framework could be modified or extended to support different types of tasks that also suffer from hallucinations in LLM outputs.
- What evidence would resolve it: Designing and implementing adaptations of the L2R system for other application scenarios, followed by empirical testing to assess improvements in reliability and controllability in those contexts.

## Limitations
- Empirical validation limited to single dataset (TruthfulQA) and one LLM model (gpt-3.5-turbo)
- Knowledge base construction methods not thoroughly evaluated against each other
- Computational costs and latency implications of two-stage refusal mechanism unexplored
- Performance on non-factual question types (reasoning, creative tasks) untested

## Confidence

- **High Confidence**: The core architecture of separating knowledge from model parameters is sound and supported by the retrieval literature. The hard refusal mechanism based on retrieval confidence is straightforward and empirically validated.
- **Medium Confidence**: The soft refusal mechanism relying on LLM self-knowledge shows promise but depends heavily on prompt engineering quality and may vary across models and domains.
- **Medium Confidence**: The accuracy improvements on TruthfulQA are demonstrated but require replication across diverse datasets and model families to confirm robustness.
- **Low Confidence**: The Automatic Knowledge Enrichment method's ability to generate high-quality pseudo-knowledge without introducing new hallucinations needs more rigorous validation.

## Next Checks

1. **Cross-dataset validation**: Test L2R on multiple factual QA datasets (e.g., Natural Questions, SQuAD) to verify the 19% average accuracy improvement generalizes beyond TruthfulQA. This will validate the approach's robustness across different question distributions and difficulty levels.

2. **Knowledge base quality analysis**: Conduct human evaluation of automatically generated knowledge entries to assess hallucination rates. Compare knowledge bases built through manual curation versus AKE to quantify the trade-off between scale and accuracy, measuring how many generated entries contain subtle errors versus outright falsehoods.

3. **Ablation studies on refusal thresholds**: Systematically vary both hard refusal thresholds (α) and soft refusal confidence cutoffs to map the full accuracy-refusal rate tradeoff space. This will reveal whether the reported 65.1% MC1 accuracy with 25% knowledge coverage represents an optimal operating point or if better configurations exist.