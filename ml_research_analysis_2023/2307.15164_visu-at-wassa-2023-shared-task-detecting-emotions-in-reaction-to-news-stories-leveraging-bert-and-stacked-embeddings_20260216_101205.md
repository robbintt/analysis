---
ver: rpa2
title: 'VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories
  Leveraging BERT and Stacked Embeddings'
arxiv_id: '2307.15164'
source_url: https://arxiv.org/abs/2307.15164
tags:
- emotion
- bert
- task
- word
- kumar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses emotion classification from essays reacting
  to news stories, focusing on small and imbalanced datasets with mixed emotion categories.
  The core method involves using deep learning models (BiLSTM and BERT) with static
  and contextual embeddings (including stacked combinations of GloVe, fastText, and
  BERT embeddings) along with tailored preprocessing strategies.
---

# VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings

## Quick Facts
- arXiv ID: 2307.15164
- Source URL: https://arxiv.org/abs/2307.15164
- Reference count: 25
- Primary result: Achieved 10th rank with Macro F1-score of 0.2717 in WASSA 2023 Track 3 emotion classification

## Executive Summary
This paper presents VISU's approach to the WASSA 2023 Shared Task Track 3: Emotion Classification, focusing on detecting emotions from essays reacting to news stories. The task involves classifying texts into 31 emotion categories (including mixed emotions) from a small, imbalanced dataset. The authors employed deep learning models using both traditional BiLSTM architectures with stacked embeddings and transformer-based BERT models. Through tailored preprocessing and strategic model selection, the system achieved a Macro F1-score of 0.2717, placing 10th in the competition. The study demonstrates that BERT-based approaches significantly outperform BiLSTM models with stacked embeddings for this challenging emotion detection task.

## Method Summary
The methodology combines advanced NLP techniques with careful preprocessing to address the challenges of emotion classification from news-related essays. The approach uses two primary model architectures: BiLSTM networks with stacked embeddings (combining GloVe, fastText, and BERT embeddings) and standalone BERT transformer models. Text preprocessing includes lowercasing, punctuation and stopword removal, special character elimination, and contraction expansion. The models are trained on a dataset of 792 training samples across 31 emotion categories, with evaluation on 208 development and 100 test samples. The system uses Adam optimizer with learning rate 2e-5, batch size 32, and 5 epochs, focusing on Macro F1-score as the primary evaluation metric.

## Key Results
- Achieved 10th rank in WASSA 2023 Track 3 with Macro F1-score of 0.2717
- BERT-based model significantly outperformed BiLSTM with stacked embeddings (GloVe + fastText + BERT)
- Demonstrated effectiveness of tailored preprocessing for small, imbalanced emotion datasets

## Why This Works (Mechanism)

### Mechanism 1
BERT's contextual embeddings capture emotion nuances better than BiLSTM models with stacked embeddings for imbalanced datasets. BERT's pre-trained representations provide richer semantic understanding for rare and mixed emotion categories, while BiLSTM models struggle to generalize from limited data.

### Mechanism 2
Stacked embeddings combine multiple embedding types (GloVe, fastText, BERT) to create richer feature representations capturing different language aspects. Different embedding types capture complementary information - semantic, morphological, and contextual - that improves classification when combined.

### Mechanism 3
Tailored preprocessing significantly improves model performance by removing noise and standardizing text representation. Clean, standardized input text reduces model confusion and improves feature extraction quality, particularly important for small, imbalanced datasets.

## Foundational Learning

- **Contextual vs static word embeddings**: Understanding why BERT outperforms traditional embeddings is crucial for architecture selection. *Quick check: What key difference allows BERT to capture word meaning based on surrounding context?*
- **Handling imbalanced datasets in classification**: The dataset has severe class imbalance which affects model training and evaluation. *Quick check: Why might Macro F1-score be a more appropriate metric than accuracy for this task?*
- **Transformer architecture fundamentals**: BERT is a transformer model, understanding its architecture is essential for effective use. *Quick check: What key architectural innovation allows transformers to handle long-range dependencies better than RNNs?*

## Architecture Onboarding

- **Component map**: Input preprocessing → Embedding layer → BiLSTM/Transformer layers → Dense layers → Output classification
- **Critical path**: Preprocessing → Embedding generation → Model training → Evaluation (Macro F1-score)
- **Design tradeoffs**: BERT provides better performance but requires more computational resources vs BiLSTM with stacked embeddings being faster to train but less accurate
- **Failure signatures**: Low Macro F1-score, poor performance on minority classes, overfitting on small datasets
- **First 3 experiments**:
  1. Baseline: Test simple BiLSTM with GloVe embeddings on preprocessed data
  2. Feature ablation: Compare BiLSTM with individual embeddings vs stacked combinations
  3. Model comparison: Evaluate BERT base model vs best performing BiLSTM configuration

## Open Questions the Paper Calls Out

### Open Question 1
How would different data augmentation techniques impact performance on the imbalanced emotion classification task? The authors mention future research aims to address data imbalance through novel augmentation techniques, but baseline models were tested without any augmentation strategies.

### Open Question 2
Would incorporating demographic information (age, gender, income) improve emotion classification accuracy? The dataset contains person-level demographic information that was not utilized in the experiments, though it might enhance emotion detection.

### Open Question 3
How does the performance of stacked embeddings compare to individual contextual embeddings on this task? While various embedding combinations were tested, controlled experiments specifically comparing stacked versus individual embeddings' efficacy were not conducted.

## Limitations

- Small dataset size (792 training samples) with severe class imbalance across 31 emotion categories
- Limited exploration of ensemble methods and hyperparameter optimization
- Preprocessing strategy may have removed potentially relevant information for emotion detection

## Confidence

- **High Confidence**: BERT-based models outperform BiLSTM models on this specific task and dataset
- **Medium Confidence**: Stacked embeddings provide benefits but are underutilized in emotion classification
- **Medium Confidence**: Tailored preprocessing improves performance on imbalanced datasets
- **Low Confidence**: Generalizability of results to larger or differently distributed emotion datasets

## Next Checks

1. **Dataset robustness test**: Evaluate model performance across multiple train/dev splits and with synthetic minority oversampling to assess sensitivity to data distribution
2. **Ablation study**: Systematically remove individual preprocessing steps to quantify their impact on final performance and identify potential information loss
3. **Model scaling experiment**: Test the same architecture on a larger, more balanced emotion dataset to determine if BERT's advantage persists under different data conditions