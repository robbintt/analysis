---
ver: rpa2
title: Data-driven Preference Learning Methods for Sorting Problems with Multiple
  Temporal Criteria
arxiv_id: '2309.12620'
source_url: https://arxiv.org/abs/2309.12620
tags:
- value
- criteria
- time
- uni00000003
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of handling temporal criteria
  in multiple criteria sorting problems, where decisions must be made based on time
  series data. The authors propose a novel approach that combines traditional multiple
  criteria decision aiding methods with deep learning techniques, specifically a monotonic
  Recurrent Neural Network (mRNN).
---

# Data-driven Preference Learning Methods for Sorting Problems with Multiple Temporal Criteria

## Quick Facts
- arXiv ID: 2309.12620
- Source URL: https://arxiv.org/abs/2309.12620
- Reference count: 15
- Primary result: mRNN achieves F1-score of 0.79 on mobile gaming user classification task

## Executive Summary
This study addresses the challenge of handling temporal criteria in multiple criteria sorting problems, where decisions must be made based on time series data. The authors propose a novel approach that combines traditional multiple criteria decision aiding methods with deep learning techniques, specifically a monotonic Recurrent Neural Network (mRNN). The mRNN is designed to capture the evolving dynamics of preferences over time while adhering to properties like monotonicity and preference independence.

## Method Summary
The paper introduces a monotonic Recurrent Neural Network (mRNN) that processes each temporal criterion independently through parallel hidden states while maintaining monotonicity constraints. The model incorporates learnable time discount factors to capture the varying importance of past information. A parallel learning algorithm is proposed to improve scalability by dividing the optimization problem into smaller subproblems. The approach is evaluated on both synthetic data and a real-world case study involving user value classification in a mobile gaming app.

## Key Results
- mRNN achieves F1-score of 0.79, outperforming baseline methods (0.77 for next best method)
- Learnable time discount factors enable personalized adaptation to individual user characteristics
- Model provides interpretable insights into learned time discount factors and marginal value functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The monotonic Recurrent Neural Network (mRNN) effectively captures evolving preferences over time by maintaining separate hidden states for each criterion.
- Mechanism: The mRNN architecture processes each temporal criterion independently through parallel hidden states, ensuring preference independence while preserving monotonicity through ReLU-activated mappings.
- Core assumption: Temporal criteria can be modeled as independent sequences while still capturing their collective impact on decision outcomes.

### Mechanism 2
- Claim: The parallel learning algorithm improves scalability by dividing the optimization problem into smaller subproblems that can be solved efficiently.
- Mechanism: The algorithm samples subsets of training data to create weaker learners, which are then aggregated to form the final model, reducing computational complexity.
- Core assumption: Multiple smaller optimization problems can be solved in parallel without significant loss of model performance.

### Mechanism 3
- Claim: The learnable time discount factors enable the model to adapt to individual user characteristics and temporal patterns.
- Mechanism: The mRNN learns personalized discount factors through a feed-forward network that adjusts the influence of past information based on current input characteristics.
- Core assumption: The importance of past information varies across users and can be learned from data.

## Foundational Learning

- Concept: Temporal data preprocessing
  - Why needed here: The input data consists of time series with varying lengths and patterns that need to be normalized and transformed into a format suitable for the mRNN.
  - Quick check question: Can you explain how you would normalize a time series with varying scales across different criteria?

- Concept: Preference independence
  - Why needed here: The model assumes criteria can be processed independently, which simplifies the architecture and interpretation but may not hold for all problems.
  - Quick check question: What would happen to the model's performance if criteria were strongly dependent but treated as independent?

- Concept: Convex optimization
  - Why needed here: The optimization problems in the paper are convex, ensuring global optima can be found efficiently.
  - Quick check question: How does convexity guarantee that we find the global optimum rather than getting stuck in local optima?

## Architecture Onboarding

- Component map: Data preprocessing layer -> mRNN core -> Time discount factor module -> Output layer -> Training loop
- Critical path: Data → mRNN processing → Time discount application → Aggregation → Classification
- Design tradeoffs:
  - Independence vs. interaction: Processing criteria independently simplifies the model but may miss important interactions
  - Complexity vs. interpretability: More complex models may perform better but are harder to interpret
  - Fixed vs. learnable discount factors: Learnable factors adapt to data but require more training data
- Failure signatures:
  - Poor performance on non-monotonic data: The model assumes monotonic relationships
  - Overfitting with many sub-intervals: Too many parameters relative to data size
  - Computational inefficiency with long time series: The model processes each timestamp sequentially
- First 3 experiments:
  1. Train the mRNN on synthetic monotonic data to verify basic functionality
  2. Compare performance with and without time discount factors on real data
  3. Test the model's robustness to non-monotonic data patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do learnable time discount factors impact the model's performance compared to fixed discount factors in real-world applications with varying temporal dynamics?
- Basis in paper: The authors introduce a novel monotonic Recurrent Neural Network (mRNN) with learnable time discount factors to capture evolving preferences over time.
- Why unresolved: The paper evaluates the mRNN's performance but does not provide a direct comparison between learnable and fixed time discount factors in real-world scenarios.
- What evidence would resolve it: Empirical studies comparing the performance of mRNN with learnable time discount factors against models with fixed discount factors in diverse real-world applications with varying temporal dynamics.

### Open Question 2
- Question: What is the optimal number of sub-intervals (γ) for the mRNN model in different types of temporal criteria and data characteristics?
- Basis in paper: The authors explore the impact of different values of γ on the mRNN's performance in simulation experiments.
- Why unresolved: The optimal value of γ may vary depending on the specific characteristics of the temporal criteria and data, which is not fully addressed in the paper.
- What evidence would resolve it: Systematic studies evaluating the performance of the mRNN model with different values of γ across various types of temporal criteria and data characteristics to identify the optimal number of sub-intervals for each scenario.

### Open Question 3
- Question: How does the mRNN model handle non-monotonic and non-independent criteria in real-world decision-making problems?
- Basis in paper: The authors design simulation experiments to assess the mRNN's performance when dealing with non-monotonic and non-independent criteria.
- Why unresolved: The paper does not provide empirical evidence of the mRNN's effectiveness in handling non-monotonic and non-independent criteria in real-world decision-making problems.
- What evidence would resolve it: Real-world case studies applying the mRNN model to decision-making problems with non-monotonic and non-independent criteria, comparing its performance against other methods and analyzing the interpretability of the learned marginal value functions.

## Limitations
- Independence assumption may not hold in all real-world scenarios where criteria interactions are important
- Parallel learning algorithm may sacrifice some model performance due to sampling-induced information loss
- Learnable time discount factors may overfit with limited temporal variation in the data

## Confidence
- **High confidence**: The monotonic RNN architecture can process temporal criteria with parallel independent streams
- **Medium confidence**: The model outperforms baseline methods on the tested dataset
- **Medium confidence**: The learnable time discount factors improve personalization

## Next Checks
1. Test the model on datasets with known criterion interactions to evaluate the independence assumption
2. Conduct ablation studies removing the time discount factors to quantify their contribution
3. Evaluate computational efficiency scaling with dataset size to verify the parallel learning algorithm's benefits