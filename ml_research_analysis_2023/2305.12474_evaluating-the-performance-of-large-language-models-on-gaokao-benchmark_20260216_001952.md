---
ver: rpa2
title: Evaluating the Performance of Large Language Models on GAOKAO Benchmark
arxiv_id: '2305.12474'
source_url: https://arxiv.org/abs/2305.12474
tags:
- questions
- language
- performance
- benchmark
- examination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GAOKAO-Bench, a new benchmark using Chinese
  college entrance exam questions to evaluate large language models (LLMs). The dataset
  includes 2,811 questions from 2010-2022 across subjects like Chinese, math, English,
  physics, chemistry, biology, politics, history, and geography, categorized as multiple-choice,
  fill-in-the-blank, and open-ended.
---

# Evaluating the Performance of Large Language Models on GAOKAO Benchmark

## Quick Facts
- arXiv ID: 2305.12474
- Source URL: https://arxiv.org/abs/2305.12474
- Reference count: 3
- Key outcome: GAOKAO-Bench benchmark reveals LLMs perform well on English objective questions (up to 88% accuracy) but struggle with math, physics, and chemistry problems requiring complex reasoning and calculation.

## Executive Summary
This paper introduces GAOKAO-Bench, a new benchmark using Chinese college entrance exam questions to evaluate large language models (LLMs). The dataset includes 2,811 questions from 2010-2022 across subjects like Chinese, math, English, physics, chemistry, biology, politics, history, and geography. The authors evaluate GPT-3.5 Turbo using zero-shot prompting and human grading for subjective questions. Results show LLMs perform well on objective questions, especially in English, but struggle with math, physics, and chemistry problems requiring complex reasoning or calculation. Subjective question scores are significantly lower in STEM subjects due to insufficient comprehension and generalization.

## Method Summary
The authors developed GAOKAO-Bench by collecting 2,811 Chinese college entrance exam questions from 2010-2022 across nine subjects. They categorized questions as multiple-choice, fill-in-the-blank, or open-ended, then converted them to JSON format with LaTeX for mathematical formulas. Using zero-shot prompting, they evaluated GPT-3.5 Turbo's performance, with human graders assessing subjective questions. The evaluation framework compared model outputs to standard answers, computing accuracy rates for objective questions and human-graded scores for subjective ones.

## Key Results
- LLMs achieved up to 88% accuracy on English objective questions
- Performance on math, physics, and chemistry problems was significantly lower, especially for subjective questions requiring complex reasoning
- Subjective question scores were notably lower than objective scores in STEM subjects due to insufficient comprehension and generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAOKAO-Bench evaluates LLM performance using zero-shot prompts that align with human examination methods
- Mechanism: The benchmark transforms exam questions into structured prompts and compares model outputs to standard answers, using human graders for subjective questions to ensure alignment with human scoring
- Core assumption: Zero-shot prompting can effectively simulate human test-taking conditions without requiring model fine-tuning
- Evidence anchors:
  - [abstract] "To align with human examination methods, we design a method based on zero-shot settings to evaluate the performance of LLMs"
  - [section] "we utilized a zero-shot prompting strategy (Ouyang et al., 2022) and created prompts tailored to different question types"
  - [corpus] Weak evidence - only one related paper (GAOKAO-Eval) that questions whether high scores reflect true capabilities
- Break condition: If zero-shot prompts fail to capture the complexity of exam questions or if human grading introduces significant variance

### Mechanism 2
- Claim: LLMs perform significantly better on objective questions than subjective questions in STEM subjects
- Mechanism: Objective questions (multiple-choice, fill-in-the-blank) can be evaluated through pattern matching, while subjective questions require reasoning and calculation that current LLMs struggle with
- Core assumption: Pattern matching is easier for LLMs than complex reasoning and calculation
- Evidence anchors:
  - [abstract] "Results show LLMs perform well on objective questions, especially in English (up to 88% accuracy), but struggle with math, physics, and chemistry problems requiring complex reasoning or calculation"
  - [section] "scores on subjective questions were significantly lower than those on objective questions in Physics, Chemistry, Biology and Mathematics"
  - [corpus] Weak evidence - no direct citations about STEM subject performance differences
- Break condition: If future LLMs develop better reasoning capabilities or if prompt engineering improves subjective question performance

### Mechanism 3
- Claim: The benchmark provides a culturally contextualized evaluation that reflects real-world capabilities
- Mechanism: Using actual Chinese college entrance exam questions creates a domain-specific benchmark that tests both knowledge and reasoning within a specific cultural and educational context
- Core assumption: Domain-specific benchmarks are more meaningful than general language tasks for evaluating real-world applicability
- Evidence anchors:
  - [abstract] "The dataset includes 2,811 questions from 2010-2022 across subjects like Chinese, math, English, physics, chemistry, biology, politics, history, and geography"
  - [section] "These questions are characterized by their level of difficulty, abstraction, and wide-ranging subject matter"
  - [corpus] Weak evidence - GAOKAO-MM and GAOKAO-Eval are related but don't provide strong supporting evidence
- Break condition: If the cultural specificity limits generalizability or if exam questions don't reflect real-world knowledge needs

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: The benchmark relies on zero-shot prompts to evaluate LLMs without fine-tuning, making it applicable to any model
  - Quick check question: What is the difference between zero-shot and few-shot prompting in the context of LLM evaluation?

- Concept: Subjectivity vs objectivity in question types
  - Why needed here: Understanding why LLMs perform differently on objective versus subjective questions is crucial for interpreting benchmark results
  - Quick check question: Why do subjective questions require human grading while objective questions can be evaluated through automated matching?

- Concept: Cultural and domain-specific benchmarking
  - Why needed here: The GAOKAO benchmark's value lies in its cultural specificity, which provides insights into LLM performance in real-world educational contexts
  - Quick check question: How does a culturally specific benchmark like GAOKAO-Bench differ from general language model evaluation benchmarks?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (exam questions → JSON format → LaTeX conversion) → Prompt generation system (zero-shot prompts for different question types) → Model inference → Evaluation framework (automated scoring for objective questions + human grading for subjective questions) → Result aggregation and analysis system
- Critical path: Data preprocessing → Prompt generation → Model inference → Scoring (automated or human) → Result aggregation → Analysis
- Design tradeoffs: Using zero-shot prompts allows evaluation of any LLM without fine-tuning but may limit performance compared to fine-tuned approaches; human grading ensures alignment with real-world scoring but introduces potential bias and cost
- Failure signatures: Low scores across all subjects may indicate prompt engineering issues; high variance in subjective question scoring may indicate grader inconsistency; poor performance on specific subjects may indicate knowledge gaps in LLMs
- First 3 experiments:
  1. Test different prompt engineering approaches for the same question type to optimize model performance
  2. Compare human grader agreement rates across different subjects and question types
  3. Analyze correlation between objective and subjective question performance to identify reasoning capability gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better handle complex mathematical equations and logical reasoning problems, as demonstrated by their poor performance on math, physics, and chemistry questions in the GAOKAO-Bench?
- Basis in paper: [explicit] The paper notes that LLMs struggle with complex equations in math problems and logical reasoning, particularly in subjects like mathematics, physics, and chemistry.
- Why unresolved: The paper does not provide specific solutions or methods to address these weaknesses in LLMs, only highlighting the need for improvement.
- What evidence would resolve it: Evidence of improved LLM performance on math, physics, and chemistry problems after implementing new training techniques or architectural changes would help resolve this question.

### Open Question 2
- Question: What are the limitations of using zero-shot prompting for evaluating LLMs on domain-specific tasks, and how can these limitations be addressed to provide more accurate assessments?
- Basis in paper: [explicit] The paper uses zero-shot prompting to evaluate LLMs on GAOKAO-Bench but acknowledges that this approach may not fully capture the models' capabilities in handling complex reasoning and calculation tasks.
- Why unresolved: The paper does not explore alternative prompting strategies or methods to overcome the limitations of zero-shot prompting for domain-specific tasks.
- What evidence would resolve it: Comparative studies showing the performance differences between zero-shot prompting and other prompting strategies (e.g., few-shot prompting, chain-of-thought prompting) on domain-specific tasks would help address this question.

### Open Question 3
- Question: How can LLMs be enhanced to improve their comprehension and generalization skills for longer reading materials, as evidenced by their lower scores on subjective questions in subjects like physics, chemistry, biology, and mathematics?
- Basis in paper: [explicit] The paper identifies insufficient comprehension and generalization skills for longer reading material as a weakness of LLMs, particularly in subjects with subjective questions requiring complex reasoning and calculation.
- Why unresolved: The paper does not propose specific methods or techniques to enhance LLMs' ability to handle longer reading materials and improve their generalization skills.
- What evidence would resolve it: Evidence of improved LLM performance on subjective questions in subjects with longer reading materials after implementing new training techniques or architectural changes would help resolve this question.

## Limitations
- Limited transparency in human evaluation methodology, including inter-rater reliability and grader qualifications
- Zero-shot prompting constraints may not fully capture LLM capabilities, with potential for prompt design limitations
- Cultural specificity to Chinese educational context may limit generalizability to other educational systems

## Confidence

**High Confidence Claims**:
- LLMs perform significantly better on objective questions than subjective questions
- English subject shows highest performance across both objective and subjective question types
- The benchmark successfully distinguishes between different subject matter difficulties

**Medium Confidence Claims**:
- LLMs struggle specifically with STEM subjects requiring complex reasoning and calculation
- Human grading aligns reasonably well with automated scoring for objective questions
- The benchmark provides meaningful differentiation between model capabilities

**Low Confidence Claims**:
- GAOKAO-Bench represents human-level performance in educational assessment
- Results generalize to non-Chinese educational contexts
- Zero-shot prompting is the optimal evaluation approach for this domain

## Next Checks
1. Conduct systematic experiments varying prompt formats, instructions, and examples to determine the impact of prompt design on model performance across different subjects
2. Implement formal inter-rater reliability testing with multiple graders across all subjects to quantify grading consistency and identify potential bias patterns
3. Apply the GAOKAO-Bench methodology to equivalent exam questions from other educational systems (e.g., SAT, A-Levels) to assess cultural specificity and generalizability of findings