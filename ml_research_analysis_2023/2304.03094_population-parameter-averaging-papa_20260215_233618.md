---
ver: rpa2
title: PopulAtion Parameter Averaging (PAPA)
arxiv_id: '2304.03094'
source_url: https://arxiv.org/abs/2304.03094
tags:
- averaging
- papa
- population
- networks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PopulAtion Parameter Averaging (PAPA), a method
  that combines the benefits of ensembling and weight averaging in neural networks.
  PAPA trains a population of diverse models with different data orders, augmentations,
  and regularizations, then occasionally replaces the weights with the population
  average.
---

# PopulAtion Parameter Averaging (PAPA)

## Quick Facts
- arXiv ID: 2304.03094
- Source URL: https://arxiv.org/abs/2304.03094
- Reference count: 40
- Primary result: PAPA achieves up to 1.9% accuracy improvement over independent training on CIFAR-10/100 and ImageNet

## Executive Summary
PAPA combines ensembling benefits with weight averaging efficiency by training multiple models with different data orderings, augmentations, and regularizations, then periodically averaging their weights. This approach addresses the high computational cost of traditional ensembling while maintaining diverse model representations. The method shows consistent performance improvements across image classification and segmentation tasks, with variants PAPA-all and PAPA-gradual generally outperforming PAPA-2.

## Method Summary
PAPA trains a population of neural networks with different initialization, data orderings, augmentations, and regularization strategies. Periodically, it replaces individual model weights with the population average, maintaining diversity while ensuring models remain similar enough to average effectively. The method uses REPAIR to prevent variance collapse after averaging and offers three variants: PAPA-all (averaging all models), PAPA-2 (averaging random pairs), and PAPA-gradual (smoothly moving weights toward the average).

## Key Results
- PAPA-all and PAPA-gradual variants show up to 1.9% accuracy improvement over independent training on CIFAR datasets
- Combining PAPA with SWA provides additional performance benefits
- PAPA maintains efficiency of single-model inference while capturing ensemble-like performance gains
- Particularly effective when inference constraints exist or single-model deployment is required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PAPA improves accuracy by mixing features learned by different networks
- Core assumption: Models are sufficiently diverse in learned features but not so dissimilar that averaging degrades performance
- Evidence anchors: Abstract states averaging is beneficial when weights are "similar enough to average well but different enough to benefit from combining them"

### Mechanism 2
- Claim: PAPA emulates training with larger batch sizes
- Core assumption: Equivalence between weight averaging and gradient accumulation holds in practice
- Evidence anchors: Section 3 claims weight averaging is equivalent to training on bigger batch size by separating computation over multiple forward passes

### Mechanism 3
- Claim: PAPA ensures models remain "well-aligned" for averaging through frequent communication
- Core assumption: Frequent enough averaging maintains sufficient similarity between models
- Evidence anchors: Section 2.1.1 states averaging not too often and too rarely ensures models are diverse and well-aligned

## Foundational Learning

- Concept: Weight averaging and its limitations
  - Why needed here: Understanding why simple weight averaging often fails is crucial to appreciating PAPA's approach of maintaining diversity while ensuring alignment
  - Quick check question: Why does averaging weights of independently trained models usually lead to worse performance than ensembling?

- Concept: Ensemble methods and their computational costs
  - Why needed here: PAPA aims to combine the benefits of ensembling (improved performance) with the efficiency of a single model
  - Quick check question: What is the main drawback of traditional ensemble methods that PAPA seeks to address?

- Concept: Permutation alignment techniques
  - Why needed here: PAPA uses REPAIR instead of permutation alignment, but understanding both approaches helps in appreciating PAPA's design choices
  - Quick check question: Why might permutation alignment be less effective than REPAIR in the PAPA setting?

## Architecture Onboarding

- Component map: Population of independently trained models -> Periodic weight averaging (PAPA-all/PAPA-2) -> REPAIR application -> Return averaged or greedy soup model
- Critical path: 1) Initialize population with different data orderings, augmentations, and regularizations. 2) Train models independently. 3) Periodically average weights. 4) Apply REPAIR. 5) Return average or greedy soup model.
- Design tradeoffs: Balances diversity (through different training conditions) and alignment (through periodic averaging). More frequent averaging improves alignment but reduces diversity.
- Failure signatures: Poor performance indicates models are either too dissimilar (averaging degrades) or too similar (no diversity benefit). REPAIR failure suggests variance collapse issues.
- First 3 experiments:
  1. Implement PAPA-all on CIFAR-10 with 2 models, comparing to baseline and traditional averaging
  2. Test different averaging frequencies (1, 5, 10 epochs) to find optimal balance between diversity and alignment
  3. Compare PAPA-all to PAPA-gradual on CIFAR-100 to evaluate the benefit of gradual averaging

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for weight averaging in PAPA for different dataset sizes and model architectures?
- Basis in paper: The authors tuned averaging frequencies of 1, 2, 5, 10 epochs and found that averaging every 5 epochs was best for small data (e.g., CIFAR-10, CIFAR-100) and every 1 epoch for large data (e.g., ImageNet), but this was not systematically tested across all settings
- Why unresolved: The paper only provides anecdotal evidence for this relationship. A more comprehensive study across various dataset sizes, model architectures, and averaging frequencies would be needed to establish a clear optimal relationship
- What evidence would resolve it: Systematic experiments testing different averaging frequencies across a range of dataset sizes and model architectures, with statistical analysis to determine the optimal relationship

### Open Question 2
- Question: How does PAPA perform on generative models and language models compared to traditional training methods?
- Basis in paper: The authors suggest that an interesting future direction would be to test PAPA on large generative and language models, but they do not provide any experimental results in this area
- Why unresolved: The paper only focuses on image classification and segmentation tasks. Generative models and language models have different training dynamics and architectures, so the performance of PAPA on these tasks is unknown
- What evidence would resolve it: Experiments applying PAPA to various generative models (e.g., GANs, VAEs) and language models (e.g., Transformers) and comparing their performance to traditional training methods

### Open Question 3
- Question: What is the effect of PAPA on feature representations learned by neural networks?
- Basis in paper: The authors hypothesize that PAPA combines features from different networks, leading to improved performance. They provide evidence of high cosine similarity between features of PAPA models compared to independent models, but do not explore the deeper implications of this feature mixing
- Why unresolved: The paper only scratches the surface of how PAPA affects feature representations. A more in-depth analysis of the feature space, including visualizations and quantitative metrics, would be needed to understand the full impact of PAPA on feature learning
- What evidence would resolve it: Detailed analysis of feature representations learned by PAPA models, including visualizations of feature spaces, quantitative metrics comparing feature similarity and diversity, and ablation studies on the impact of feature mixing on downstream tasks

## Limitations

- Theoretical foundations connecting weight averaging to gradient accumulation lack empirical validation
- Optimal frequency for population averaging remains dataset- and model-dependent with only general guidelines provided
- Benefits compared to SWA appear additive but optimal combination is not investigated

## Confidence

- High confidence: PAPA's empirical performance improvements on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet) are well-demonstrated with statistically significant results
- Medium confidence: The theoretical mechanism explaining why PAPA works (diversity + alignment) is plausible but lacks rigorous mathematical proof beyond empirical observations
- Medium confidence: The claim that PAPA provides benefits similar to ensembling while maintaining single-model efficiency is supported but could benefit from more extensive ablation studies on different architectures

## Next Checks

1. **Ablation on averaging frequency**: Systematically vary averaging intervals (1, 2, 5, 10, 20 epochs) across different datasets to establish robust guidelines for optimal PAPA configuration

2. **Gradient accumulation comparison**: Implement direct gradient accumulation experiments matching the effective batch size that PAPA achieves to verify the claimed equivalence between weight averaging and larger batch training

3. **Architecture generalization**: Test PAPA on non-vision domains (NLP, speech, tabular) and diverse architectures (Transformers, LSTMs, attention-based models) to assess whether the benefits generalize beyond convolutional networks