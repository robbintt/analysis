---
ver: rpa2
title: A Preliminary Study on a Conceptual Game Feature Generation and Recommendation
  System
arxiv_id: '2308.13538'
source_url: https://arxiv.org/abs/2308.13538
tags:
- game
- features
- games
- system
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a system that generates game feature suggestions
  from text prompts using a fine-tuned GPT-2 model trained on 60k game descriptions.
  The approach combines GloVe embeddings to find thematically similar games, then
  generates new features.
---

# A Preliminary Study on a Conceptual Game Feature Generation and Recommendation System

## Quick Facts
- arXiv ID: 2308.13538
- Source URL: https://arxiv.org/abs/2308.13538
- Reference count: 20
- Human-authored features received most votes overall (52%), but GPT-2 won in 3 of 6 prompts

## Executive Summary
This paper introduces a system for generating game feature suggestions from text prompts using a fine-tuned GPT-2 model trained on 60k game descriptions. The approach combines GloVe embeddings to find thematically similar games, then generates new features. A user study comparing GPT-2, ConceptNet, and human-authored features found human features received the most votes overall (52%), but GPT-2 won in 3 of 6 prompts, particularly for real games. ConceptNet underperformed except for one case. The system shows promise as part of a collaborative game design assistant tool, though results suggest fine-tuning and training data improvements could enhance performance.

## Method Summary
The system uses a two-stage approach: first, a recommendation system finds thematically similar games using GloVe embeddings on extracted nouns from game descriptions; second, a fine-tuned GPT-2 model generates new game features based on these recommendations. The training data consists of 60k game descriptions from Steam Web API and LaunchBox Games Database, with features extracted using part-of-speech grammar expressions. The GPT-2 model is fine-tuned for 5 epochs with specific generation parameters (temperature=0.95, top_k=100, top_p=0.8). A user study compared features generated by GPT-2, ConceptNet, and human authors across 6 prompts.

## Key Results
- Human-authored features received the most votes overall (52%)
- GPT-2-generated features won in 3 of 6 prompts, particularly for real games
- ConceptNet underperformed in all but one case
- GPT-2 showed better performance on real games compared to fictional games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-2 can generate thematically relevant game features when fine-tuned on game description data.
- Mechanism: Fine-tuning GPT-2 on 60k game descriptions with labeled features allows it to learn the mapping between textual prompts and game features, generating novel suggestions based on learned patterns.
- Core assumption: The training data contains sufficient variety and quality of game features to enable GPT-2 to learn meaningful patterns.
- Evidence anchors:
  - [abstract] "This paper introduces a system used to generate game feature suggestions based on a text prompt. Trained on the game descriptions of almost 60k games..."
  - [section III-C] "For the first generator, we fine-tuned a medium GPT-2 model [18] using the tag set, extracted entities, and description features of the training dataset."

### Mechanism 2
- Claim: The recommendation system using GloVe embeddings can identify thematically similar games to augment user prompts.
- Mechanism: By encoding game descriptions and user prompts as GloVe vectors and computing cosine similarity, the system can find games with similar themes and extract their features to enrich the user's prompt.
- Core assumption: GloVe embeddings capture semantic relationships between words and phrases in game descriptions effectively.
- Evidence anchors:
  - [section III-B] "We used SpaCy to tokenize the input prompt in the same fashion as the training data. After that, the recommendation system uses the GloVe word embedding model [16] to encode each noun entity in the input prompt and the training data."

### Mechanism 3
- Claim: Human-authored features serve as a strong baseline for evaluating the quality of AI-generated features.
- Mechanism: By comparing the preferences of users for human-authored features versus AI-generated features, the study can assess the effectiveness of the AI system in generating creative and relevant game features.
- Core assumption: Human game designers can provide high-quality, diverse, and creative game features that represent a strong baseline for comparison.
- Evidence anchors:
  - [abstract] "We perform a short user study comparing the features generated from a fine-tuned GPT-2 model, a model using the ConceptNet, and human-authored game features."
  - [section IV] "Six prompts were created for the user study experiment... For all 6 games, we hand-authored 5 game features as a third comparison group."

## Foundational Learning

- Concept: Understanding the difference between game mechanics and game features.
  - Why needed here: The paper uses Mullich's definition of "game features" as descriptive aspects of a game's design, which is distinct from Sicart's definition of "game mechanics". Clarifying this distinction is crucial for understanding the scope and goals of the system.
  - Quick check question: How do game features differ from game mechanics in the context of this paper?

- Concept: Familiarity with large language models (LLMs) and their applications in game design.
  - Why needed here: The paper utilizes GPT-2, a pre-trained LLM, for generating game features. Understanding the capabilities and limitations of LLMs is essential for interpreting the results and potential improvements of the system.
  - Quick check question: What are the key advantages and limitations of using GPT-2 for generating game features?

- Concept: Knowledge of semantic networks and their role in generating new games.
  - Why needed here: The paper mentions ConceptNet, a semantic network, as an alternative generator model. Understanding how semantic networks can be used to generate game content is important for comparing the performance of different approaches.
  - Quick check question: How does ConceptNet generate game features, and how does its approach differ from that of GPT-2?

## Architecture Onboarding

- Component map: User prompt → Recommendation System → GPT-2/ConceptNet Generator → Feature Selection → User Study
- Critical path: User prompt → Recommendation System → GPT-2/ConceptNet Generator → Feature Selection → User Study
- Design tradeoffs:
  - Fine-tuning GPT-2 vs. using a larger model like ChatGPT: Fine-tuning GPT-2 allows for more control and specificity but may limit the model's creativity compared to a larger, more general model.
  - Manual feature selection vs. automatic selection: Manual selection allows for more control but is less scalable, while automatic selection would be more efficient but may introduce noise.
- Failure signatures:
  - Irrelevant or repetitive generated features: Indicates issues with the training data or fine-tuning process.
  - Poor performance of the recommendation system: Suggests problems with the GloVe embeddings or similarity computation.
  - Low user preference for AI-generated features: May indicate a lack of creativity or relevance in the generated features.
- First 3 experiments:
  1. Evaluate the quality of GPT-2-generated features on a small subset of game prompts with expert review.
  2. Test the effectiveness of the recommendation system by comparing the similarity of recommended games to user prompts using human judgment.
  3. Conduct a small-scale user study to compare the preferences for human-authored and GPT-2-generated features on a limited set of prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system automatically select the most relevant game features from the generator model's output without manual curation?
- Basis in paper: [explicit] The paper mentions manual selection of 5 features for each prompt in the user study and states future work will look to find an automatic way to select features.
- Why unresolved: The current system requires human intervention to choose features, which doesn't scale well and doesn't provide insight into what makes features relevant or preferred.
- What evidence would resolve it: A comparison of automated feature selection methods (e.g., based on relevance scores, novelty metrics, or user preference prediction) against human curation, showing which approach better captures user preferences.

### Open Question 2
- Question: Would incorporating game genre frequency information into the training data improve the system's performance on niche or fictional game prompts?
- Basis in paper: [inferred] The paper notes that GPT-2 performed better on real games with common genres, while human features dominated for fictional games with niche themes, suggesting the training data may be biased toward common genres.
- Why unresolved: The current approach doesn't explicitly account for genre frequency or diversity in the training data, which could limit its ability to generate features for underrepresented or novel game concepts.
- What evidence would resolve it: Experiments comparing the system's performance on niche/fictional prompts before and after augmenting the training data with more diverse or genre-balanced examples, or explicitly incorporating genre weighting into the recommendation system.

### Open Question 3
- Question: How can the system provide explainability for its generated game features to enhance collaboration with human designers?
- Basis in paper: [explicit] The paper mentions that participants' suggestions often matched the grammar format of training features or provided reasoning for feature improvements, and future work will explore explainability.
- Why unresolved: While the system can generate features, it doesn't explain why certain features were chosen or how they relate to the input prompt, limiting its usefulness as a collaborative design assistant.
- What evidence would resolve it: User studies comparing the effectiveness of different explainability methods (e.g., feature reasoning, semantic connections, or comparative analysis with similar games) in helping designers understand and refine generated features.

### Open Question 4
- Question: Would fine-tuning the GPT-2 model for more epochs or with additional training data significantly improve its performance on both real and fictional game prompts?
- Basis in paper: [explicit] The paper states that training was restricted to 5 epochs due to computational constraints and mentions that more time fine-tuning and more feature data could help the transformer compete with human suggestions.
- Why unresolved: The current model was limited by computational resources and training data size, leaving open the question of whether more extensive fine-tuning would yield better results.
- What evidence would resolve it: A controlled experiment comparing the current GPT-2 model's performance against versions fine-tuned for more epochs or with larger/more diverse training datasets, measuring improvements in feature relevance and user preference.

## Limitations

- Limited training data diversity: The 60k game descriptions may not cover the full spectrum of game genres and features, potentially limiting the model's ability to generate truly novel suggestions.
- User study scale: With only 18 participants voting on 6 prompts, the user preference results may not be statistically robust enough to draw definitive conclusions.
- ConceptNet integration challenges: The paper acknowledges that ConceptNet generated fewer usable features, suggesting potential issues with the semantic relationship extraction or filtering process.

## Confidence

- High confidence: The core methodology of using fine-tuned GPT-2 for feature generation is sound and well-documented. The recommendation system using GloVe embeddings follows established NLP practices.
- Medium confidence: The user study results showing GPT-2 outperforming ConceptNet in 3 out of 6 prompts are interesting but require larger-scale validation due to the small sample size and limited prompts.
- Low confidence: The claim that GPT-2 "works better for real games" needs more systematic evaluation, as this observation is based on a single case (Mega Man) in the limited user study.

## Next Checks

1. Expand training data diversity: Test the system with additional game descriptions from underrepresented genres to assess whether the model can generate features for games outside its current training distribution.
2. Conduct larger user study: Implement a larger-scale user study with 50+ participants and 20+ prompts to obtain statistically significant comparisons between GPT-2, ConceptNet, and human-authored features.
3. Evaluate feature novelty: Develop a systematic evaluation framework to measure the novelty and creativity of generated features, comparing them against existing game features to quantify how often the system produces truly innovative suggestions.