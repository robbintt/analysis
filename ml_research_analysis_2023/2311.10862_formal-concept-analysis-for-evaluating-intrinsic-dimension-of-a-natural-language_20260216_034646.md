---
ver: rpa2
title: Formal concept analysis for evaluating intrinsic dimension of a natural language
arxiv_id: '2311.10862'
source_url: https://arxiv.org/abs/2311.10862
tags:
- dimension
- intrinsic
- concept
- language
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors apply formal concept analysis (FCA) to estimate the
  intrinsic dimension of natural languages. Using a dataset of Russian, Bengali, and
  English texts, they preprocess the corpora into keyterms, compute tf-idf-based embeddings,
  and build interval pattern structures.
---

# Formal concept analysis for evaluating intrinsic dimension of a natural language

## Quick Facts
- arXiv ID: 2311.10862
- Source URL: https://arxiv.org/abs/2311.10862
- Reference count: 25
- Primary result: Intrinsic dimension of natural languages is around 5 for both words and bigrams, regardless of embedding size

## Executive Summary
This paper applies formal concept analysis (FCA) with interval pattern structures to estimate the intrinsic dimension of natural languages. Using Russian, Bengali, and English corpora, the authors preprocess texts into keyterms, compute tf-idf embeddings, and apply a Gromov-style method over the FCA lattice to derive dimension estimates. The experiments reveal that the intrinsic dimension is consistently around 5 for both unigrams and bigrams, suggesting that current large embedding models may use unnecessarily high-dimensional spaces relative to the true dimensionality of language.

## Method Summary
The authors preprocess text corpora through stopword removal, tokenization, and lemmatization, then convert texts to tf-idf vectors. These vectors are reduced via SVD to obtain semantic embeddings, which are then represented as interval pattern structures suitable for FCA. The intrinsic dimension is estimated using a measure-based Gromov-style method that iteratively computes observed diameters for concept extents within valid measure ranges, integrating these values to obtain the final dimension estimate.

## Key Results
- Intrinsic dimension of Russian, Bengali, and English is consistently around 5 for both words and bigrams
- Estimated dimensions are significantly lower than dimensions used in popular neural network models for NLP
- Results are robust across different embedding sizes, suggesting language manifold itself is low-dimensional

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The FCA-based Gromov-style method effectively estimates intrinsic dimension by finding the largest concept extents whose attribute measures fall within a valid range
- Mechanism: The algorithm iteratively expands concept intents and computes their extents, stopping when the attribute measure exceeds the threshold, ensuring the concept reflects the true dimensionality of the data manifold
- Core assumption: The measure νM is monotonic with respect to intent size, so larger intents correspond to smaller extents and vice versa
- Evidence anchors: [abstract] "The intrinsic dimension is estimated via a measure-based Gromov-style method over the FCA lattice"; [section] "The general idea of computing the observed diameter is as follows: start with largest possible concept extents... and decrease them until the measure νM of respective concept intents falls in the interval [α; 1 − α]."
- Break condition: If the measure is not monotonic or if the data does not form a proper lattice structure, the algorithm may fail to converge or give incorrect results

### Mechanism 2
- Claim: Converting text corpora to numerical interval pattern structures preserves the semantic relationships needed for dimension estimation
- Mechanism: After preprocessing, texts are converted to tf-idf vectors, then decomposed via SVD to obtain semantic embeddings. These embeddings are represented as interval tuples, allowing FCA to compute pattern concepts
- Core assumption: The SVD components retain sufficient semantic information to represent the language manifold in a lower-dimensional space
- Evidence anchors: [abstract] "Using a dataset of Russian, Bengali, and English texts, they preprocess the corpora into keyterms, compute tf-idf-based embeddings, and build interval pattern structures"; [section] "apply SVD-decomposition of this matrix and select first high-weight components of the decomposition to obtain semantic vector-space of important features of the language"
- Break condition: If the tf-idf or SVD preprocessing discards too much information, the resulting pattern structure may not reflect the true intrinsic dimension

### Mechanism 3
- Claim: The intrinsic dimension of natural languages is low (~5) regardless of the embedding size used in practice
- Mechanism: The FCA lattice analysis consistently yields dimension estimates around 5 for words and bigrams across multiple languages, suggesting that the language manifold itself is low-dimensional
- Core assumption: The text corpora used are representative samples of the languages and capture their full semantic variety
- Evidence anchors: [abstract] "the intrinsic dimension of these languages is around 5 for both words and bigrams, regardless of embedding size"; [section] "It was found that the intrinsic dimensions of these languages are significantly less than the dimensions used in popular neural network models in natural language processing"
- Break condition: If the corpora are biased or incomplete, the estimated dimension may not generalize to the full language

## Foundational Learning

- Concept: Formal Concept Analysis (FCA)
  - Why needed here: FCA provides the lattice structure to analyze relationships between texts (objects) and their semantic features (attributes), which is essential for computing intrinsic dimension
  - Quick check question: What are the derivation operators in FCA and how do they define a formal concept?

- Concept: Interval Pattern Structures
  - Why needed here: They generalize FCA to handle numerical data (like tf-idf embeddings) by representing each object as a tuple of intervals and defining similarity via convex hulls
  - Quick check question: How is the similarity operation defined for two interval tuples in pattern structures?

- Concept: Gromov-style Dimension Estimation
  - Why needed here: This method uses measures on objects and attributes to define an observed diameter, which is then integrated to estimate intrinsic dimension, avoiding the need to compute the full concept lattice
  - Quick check question: What is the formula for the observed diameter in the Gromov-style method?

## Architecture Onboarding

- Component map: Text preprocessing → tf-idf vectorization → SVD decomposition → interval pattern structure → FCA lattice analysis → dimension estimation
- Critical path:
  1. Preprocess corpora to extract keyterms
  2. Compute tf-idf matrix and apply SVD
  3. Convert to interval pattern structure
  4. Apply FCA-based Gromov-style dimension estimation
  5. Aggregate results across languages and n-gram sizes
- Design tradeoffs:
  - Using SVD reduces dimensionality but may lose some semantic nuance
  - Interval pattern structures allow numerical data but require careful threshold selection
  - FCA-based estimation avoids full lattice computation but depends on measure monotonicity
- Failure signatures:
  - If the attribute measure is not monotonic, the diameter computation may not terminate
  - If preprocessing discards too much information, the estimated dimension may be artificially low
  - If the corpora are biased, the dimension estimate may not generalize
- First 3 experiments:
  1. Run the pipeline on a small, balanced corpus and verify that the estimated dimension is stable across different random seeds
  2. Compare the FCA-based dimension estimate with a baseline method (e.g., PCA-based intrinsic dimension) on the same data
  3. Test the sensitivity of the dimension estimate to the choice of α parameter in the Gromov-style method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the intrinsic dimension of a language change with text length and domain specificity (e.g., scientific vs. literary texts)?
- Basis in paper: [inferred] The paper analyzes general corpora but does not explore variations across text domains or lengths, leaving open the impact on dimension estimates
- Why unresolved: The current experiments use aggregated corpora without controlling for text type or length, which could affect the dimensionality of linguistic patterns
- What evidence would resolve it: Experiments comparing intrinsic dimensions across genres, text lengths, and domains using the same FCA-based method would clarify this dependency

### Open Question 2
- Question: Can alternative similarity operations or description types in pattern structures (beyond interval tuples) lead to more accurate or stable intrinsic dimension estimates?
- Basis in paper: [explicit] The authors use interval pattern structures but note that this is "an important case" and suggest analysis of "other definitions of language dimension" in future work
- Why unresolved: The choice of similarity operation (convex hull of intervals) is one of many possible generalizations; its impact on the resulting dimension estimates is not explored
- What evidence would resolve it: Systematic comparison of intrinsic dimension estimates using different pattern structure types (e.g., fuzzy, probabilistic, or multi-valued) applied to the same corpora

### Open Question 3
- Question: How does the estimated intrinsic dimension relate to the performance of downstream NLP tasks when using embeddings of matching versus higher dimensions?
- Basis in paper: [explicit] The paper notes that intrinsic dimensions (~5) are much smaller than typical embedding sizes used in models like BERT, suggesting potential inefficiency, but does not empirically test this
- Why unresolved: The theoretical argument for dimensionality reduction is not supported by experimental validation in practical NLP tasks
- What evidence would resolve it: Experiments training NLP models with embeddings constrained to the intrinsic dimension and comparing performance to standard high-dimensional embeddings on benchmark tasks

## Limitations
- The core claim that natural languages have an intrinsic dimension around 5 rests on the assumption that the chosen corpora are representative samples
- The preprocessing pipeline (tf-idf, SVD) may discard semantic information, potentially underestimating the true dimension
- The FCA-based Gromov-style method depends on the monotonicity of the measure function, which may not hold for all datasets

## Confidence
- **High Confidence**: The methodology for computing intrinsic dimension via FCA and Gromov-style measures is well-established in the literature, and the experimental results consistently show dimension estimates around 5 across languages and n-gram sizes
- **Medium Confidence**: The claim that popular embedding models use unnecessarily high dimensions is supported by the results, but the comparison is indirect and depends on the choice of embedding model and corpus
- **Low Confidence**: The assertion that the intrinsic dimension is universal across languages is not fully supported, as the study only covers three languages and may not capture cross-linguistic variation

## Next Checks
1. Reproduce on Larger, Diverse Corpora: Test the pipeline on a larger, more diverse set of languages and text types to verify the stability of the 5-dimensional estimate
2. Compare with Alternative Dimension Estimation Methods: Validate the FCA-based estimates against other intrinsic dimension estimation techniques (e.g., PCA, manifold learning) on the same datasets
3. Analyze Preprocessing Sensitivity: Investigate how the choice of preprocessing parameters (tf-idf weighting, SVD truncation) affects the estimated dimension to ensure robustness