---
ver: rpa2
title: Advancing Bayesian Optimization via Learning Correlated Latent Space
arxiv_id: '2310.20258'
source_url: https://arxiv.org/abs/2310.20258
tags:
- space
- latent
- optimization
- objective
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Correlated latent space Bayesian Optimization
  (CoBO) to address the inherent gap in latent space Bayesian optimization (LBO) between
  the latent space and the black-box objective function. CoBO introduces Lipschitz
  regularization and loss weighting to align the latent space with the objective function,
  minimizing the discrepancy between the proximity of solutions in the latent space
  and the similarity of their objective function values.
---

# Advancing Bayesian Optimization via Learning Correlated Latent Space

## Quick Facts
- arXiv ID: 2310.20258
- Source URL: https://arxiv.org/abs/2310.20258
- Authors: 
- Reference count: 40
- Key outcome: CoBO achieves state-of-the-art performance on nine tasks across three benchmark datasets for molecule design and arithmetic expression fitting, with Lipschitz regularization contributing the most significant improvement.

## Executive Summary
This paper addresses the fundamental challenge in latent space Bayesian optimization (LBO) where the latent space may not be well-aligned with the black-box objective function, leading to inefficient optimization. The proposed Correlated latent space Bayesian Optimization (CoBO) method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the gap between the latent space and the objective function. By enforcing a smoother latent space, focusing optimization on promising areas, and dynamically adjusting the search space, CoBO significantly improves optimization performance. Extensive experiments on molecule design and arithmetic expression fitting tasks demonstrate that CoBO achieves state-of-the-art results across all benchmarks.

## Method Summary
CoBO operates by jointly training a VAE with the surrogate model while applying three key innovations. First, Lipschitz regularization penalizes mappings where objective value differences are not proportional to latent space distances, creating a smoother latent space that better reflects the objective function landscape. Second, loss weighting based on objective values uses a Gaussian CDF to prioritize accurate reconstruction of high-value regions, focusing VAE learning where it matters most for optimization. Third, trust region recoordination periodically re-centers the optimization search space on the current optimal point in the updated latent space. The method is evaluated on three benchmark datasets: Guacamol (7 tasks), TDC DRD3, and arithmetic expression tasks, comparing against state-of-the-art LBO baselines.

## Key Results
- CoBO achieves state-of-the-art performance on all nine benchmark tasks across three datasets
- Lipschitz regularization contributes the most significant improvement to optimization performance
- The ablation study demonstrates the effectiveness of each proposed component, with the complete CoBO system outperforming all baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lipschitz regularization improves the correlation between latent distances and objective value distances by smoothing the latent space with respect to the objective function.
- Mechanism: The Lipschitz regularization penalizes latent space mappings where the ratio of objective value differences to latent distance exceeds a threshold L. By enforcing this constraint, the method reduces large gradients in the mapping, leading to a smoother latent space where proximity in latent space better reflects similarity in objective values.
- Core assumption: The black-box objective function is L-Lipschitz continuous, meaning small changes in the latent space should correspond to proportionally small changes in objective values.
- Evidence anchors:
  - [abstract]: "Our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas."
  - [section]: "By calculating the lower bound of the correlation, we introduce two regularizations and demonstrate their effectiveness in enhancing the correlation. Especially, one of these regularizations, called the Lipschitz regularization, encourages a smoother latent space, allowing for a more efficient optimization process"
  - [corpus]: Weak correlation - no direct mention of Lipschitz regularization in corpus neighbors
- Break condition: If the objective function is not Lipschitz continuous, the Lipschitz regularization may force unnatural smoothness that degrades optimization performance.

### Mechanism 2
- Claim: Loss weighting based on objective values focuses optimization effort on promising areas of the search space.
- Mechanism: The method assigns higher weights to reconstruction losses for input data points with higher objective values using a Gaussian CDF-based weighting function. This prioritizes learning accurate representations for high-value regions of the input space, reducing the input-to-latent space gap specifically where it matters most for optimization.
- Core assumption: High objective value regions contain more valuable optimization opportunities than low value regions.
- Evidence anchors:
  - [abstract]: "Moreover, we suggest loss weighting with respect to objective values of each input data point to particularly minimize the gap between the input space and the latent space around promising areas of high objective values."
  - [section]: "We utilize the cumulative density function of the Gaussian distribution for the weighting...The weighted reconstruction loss is as follows: Lrecon_W = λ(y)Lrecon"
  - [corpus]: Weak correlation - no direct mention of loss weighting based on objective values in corpus neighbors
- Break condition: If the objective value distribution is highly skewed or multimodal, the Gaussian-based weighting may not effectively prioritize the most promising regions.

### Mechanism 3
- Claim: Trust region recoordination adapts the search space to the updated latent space, improving optimization efficiency.
- Mechanism: After each latent space update, the method re-centers the trust region on the current optimal point mapped through the updated encoder. This ensures that subsequent Bayesian optimization searches focus on regions of the latent space that have been aligned with the objective function through the regularization process.
- Core assumption: The optimal point in the current latent space remains relevant after latent space updates.
- Evidence anchors:
  - [abstract]: "Additionally, CoBO employs trust region recoordination to adjust the search space based on the updated latent space."
  - [section]: "we propose the concept of trust region recoordination to adjust the search space in line with the updated latent space."
  - [corpus]: Weak correlation - no direct mention of trust region recoordination in corpus neighbors
- Break condition: If latent space updates cause large shifts in the optimal point location, trust region recoordination may cause optimization to jump away from promising areas.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: The method relies on VAEs to map structured/discrete input data to a continuous latent space where Bayesian optimization can be efficiently performed
  - Quick check question: How does a VAE's encoder-decoder architecture enable the transformation between input space and latent space that CoBO operates on?

- Concept: Bayesian Optimization with Gaussian Processes
  - Why needed here: The optimization is performed using a surrogate model (typically Gaussian Process) in the latent space, requiring understanding of how GP-based BO works
  - Quick check question: What role does the acquisition function play in balancing exploration and exploitation during the Bayesian optimization process?

- Concept: Lipschitz Continuity
  - Why needed here: The theoretical foundation for the Lipschitz regularization relies on understanding what it means for a function to be Lipschitz continuous
  - Quick check question: How does Lipschitz continuity mathematically constrain the relationship between input changes and output changes in a function?

## Architecture Onboarding

- Component map: VAE encoder qϕ -> Latent space -> Surrogate model (sparse GP) -> Acquisition function -> Candidate sampling -> VAE decoder pθ -> Black-box evaluation -> Update surrogate -> (periodic VAE retraining with CoBO loss)

- Critical path: Input → VAE encoder → Latent space → Surrogate model → Acquisition function → Candidate sampling → VAE decoder → Black-box evaluation → Update surrogate → (periodic VAE retraining with CoBO loss)

- Design tradeoffs: Joint training of VAE and surrogate vs. separate training, frequency of VAE retraining (computational cost vs. alignment quality), choice of Lipschitz constant L (too strict limits expressiveness, too loose reduces correlation benefits).

- Failure signatures: Poor optimization performance despite many iterations (latent space misalignment), VAE reconstruction quality degrading over time (loss weighting causing neglect of low-value regions), trust region failing to expand (recoordination causing oscillation around suboptimal points).

- First 3 experiments:
  1. Run CoBO on a simple synthetic function with known Lipschitz constant to verify the correlation improvement mechanism
  2. Compare CoBO with standard LBO on a molecule design task to demonstrate the practical performance gain
  3. Ablation study removing Lipschitz regularization to quantify its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoBO perform on optimization tasks involving non-molecular discrete structures, such as graphs or sequences?
- Basis in paper: [explicit] The paper mentions evaluating CoBO on discrete data tasks but focuses specifically on molecule design and arithmetic expression fitting.
- Why unresolved: The paper does not provide empirical evidence for the performance of CoBO on non-molecular discrete structures like graphs or sequences.
- What evidence would resolve it: Empirical results from applying CoBO to optimization tasks involving non-molecular discrete structures such as graphs or sequences.

### Open Question 2
- Question: What is the impact of the Lipschitz regularization parameter on the optimization performance and convergence rate of CoBO?
- Basis in paper: [explicit] The paper introduces Lipschitz regularization to encourage a smoother latent space but does not provide an in-depth analysis of the impact of the Lipschitz regularization parameter on optimization performance.
- Why unresolved: The paper does not explore the sensitivity of CoBO to the Lipschitz regularization parameter or its impact on convergence rate.
- What evidence would resolve it: A systematic study of the impact of the Lipschitz regularization parameter on the optimization performance and convergence rate of CoBO across various tasks.

### Open Question 3
- Question: How does CoBO scale with increasing input dimensionality and dataset size?
- Basis in paper: [inferred] The paper mentions that CoBO addresses the challenges of optimizing over high-dimensional and structured input spaces, but does not provide a detailed analysis of its scalability with increasing input dimensionality and dataset size.
- Why unresolved: The paper does not include empirical results or theoretical analysis on the scalability of CoBO with respect to input dimensionality and dataset size.
- What evidence would resolve it: Empirical results and theoretical analysis demonstrating the scalability of CoBO with increasing input dimensionality and dataset size.

## Limitations

- The method's effectiveness relies heavily on the assumption of Lipschitz continuity in the black-box objective function, which may not hold for all real-world problems.
- The Gaussian-based loss weighting scheme may not optimally handle highly skewed or multimodal objective value distributions.
- The trust region recoordination mechanism could potentially cause optimization instability if latent space updates produce large shifts in the optimal point location.

## Confidence

- **High confidence**: The overall performance improvements demonstrated on benchmark datasets and the effectiveness of the ablation study components.
- **Medium confidence**: The theoretical foundation of Lipschitz regularization and its practical implementation details, as some hyperparameters are not fully specified.
- **Medium confidence**: The robustness of the method across diverse problem types beyond the evaluated benchmarks.

## Next Checks

1. Test CoBO on synthetic functions with known Lipschitz constants and varying degrees of continuity to quantify the method's sensitivity to the Lipschitz assumption.
2. Evaluate the method's performance on problems with highly skewed or multimodal objective value distributions to assess the effectiveness of the Gaussian-based loss weighting.
3. Implement a variant of CoBO without trust region recoordination to measure its specific contribution to optimization performance and stability.