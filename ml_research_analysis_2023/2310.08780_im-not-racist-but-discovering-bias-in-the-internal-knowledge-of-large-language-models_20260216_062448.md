---
ver: rpa2
title: '"Im not Racist but...": Discovering Bias in the Internal Knowledge of Large
  Language Models'
arxiv_id: '2310.08780'
source_url: https://arxiv.org/abs/2310.08780
tags:
- knowledge
- people
- biases
- seed
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a prompt-based approach to uncover biases in
  large language models by generating knowledge graphs of stereotypes about different
  demographic groups. The method iteratively constructs triples describing associations
  between groups and attributes, using the model itself to expand the graph.
---

# "Im not Racist but...": Discovering Bias in the Internal Knowledge of Large Language Models

## Quick Facts
- arXiv ID: 2310.08780
- Source URL: https://arxiv.org/abs/2310.08780
- Reference count: 11
- Key outcome: Presents prompt-based approach to uncover biases in LLMs by generating knowledge graphs of stereotypes about demographic groups

## Executive Summary
This work introduces a novel method for identifying biases in large language models by iteratively constructing knowledge graphs of stereotypes associated with different demographic groups. The approach uses carefully crafted prompts to elicit the model's internal representations of stereotypes, then analyzes these representations using overgeneralization metrics (comparing toxicity and regard scores) and representation disparity measures (topic distribution analysis). Experiments on GPT-3 demonstrate the effectiveness of this approach in revealing group-specific stereotypes and differences in regard scores across nationalities and ethnicities.

## Method Summary
The paper proposes an iterative knowledge graph generation framework that constructs triples describing associations between demographic groups and attributes. Starting with seed entities for protected classes, the method uses LLM prompts with high temperature settings to generate initial triples, then expands these graphs through predicate and object diversity strategies. The approach analyzes outputs using identity attack scores and BERTopic topic modeling to quantify representational harms. The authors also test augmentation strategies that prepend offensive phrases to prompts to increase stereotypical content generation, revealing deeper insights into model biases.

## Key Results
- Successfully identifies group-specific stereotypes through knowledge graph generation
- Demonstrates statistically significant differences in regard scores across demographic groups
- Shows that simple prompt modifications can significantly increase generation of offensive content
- Reveals notable variations in topic distributions across different nationalities and ethnicities

## Why This Works (Mechanism)

### Mechanism 1
Prompt-based iterative knowledge graph construction uncovers hidden stereotypes in LLMs by using recursive triple generation with in-context examples to build a graph of stereotype associations. The core assumption is that LLMs' training data encodes societal stereotypes, and iterative prompting with high temperature allows exploration of these latent associations. Break condition: If the LLM lacks relevant training data for the seed entity or if iterative expansion fails to generate semantically meaningful triples.

### Mechanism 2
Augmenting prompts with offensive phrases increases generation of stereotypical content by priming the LLM's output distribution toward more toxic stereotypes. The core assumption is that token predictions are influenced by initial prompt context, and certain phrases draw from more biased distributions. Break condition: If prepended phrases are filtered out by the LLM or if safety mechanisms override the priming effect.

### Mechanism 3
Topic modeling on generated triples quantifies representation disparity across demographic groups by revealing topic distributions that differ by seed entity. The core assumption is that the LLM's knowledge about different groups is encoded in distributional semantics of generated text. Break condition: If the topic model fails to converge or if triple generation is too sparse for meaningful clustering.

## Foundational Learning

- Concept: Stereotype Content Model (warmth and competence dimensions)
  - Why needed here: The paper uses this model to categorize predicates and interpret topic distributions
  - Quick check question: What are the two primary dimensions of the Stereotype Content Model, and how do they relate to group stereotypes?

- Concept: Knowledge graph representation (subject-predicate-object triples)
  - Why needed here: The entire methodology relies on constructing and analyzing knowledge graphs
  - Quick check question: How does a knowledge graph differ from a simple list of associations, and why is it useful for bias analysis?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The methodology depends on carefully designed prompts to elicit stereotypical knowledge from the LLM
  - Quick check question: How does the temperature setting in LLM generation affect the diversity and bias of the output?

## Architecture Onboarding

- Component map: Seed entity selection -> Knowledge graph initialization -> Iterative expansion -> Augmentation -> Analysis -> Evaluation
- Critical path: Seed entity selection → Knowledge graph initialization → Iterative expansion → Augmentation → Analysis → Evaluation
- Design tradeoffs:
  - High temperature settings increase diversity but may introduce noise
  - Manual seed entity selection ensures coverage but may miss implicit biases
  - Topic modeling provides interpretable clusters but may oversimplify complex stereotypes
- Failure signatures:
  - Empty or nonsensical triple generation indicates prompt engineering issues
  - Low relative entropy across all groups suggests the model lacks diverse stereotypes
  - Inconsistent toxicity scores between runs indicate instability in generation process
- First 3 experiments:
  1. Run knowledge graph generation with default settings on a simple seed entity (e.g., "American people") to verify basic pipeline works
  2. Test effect of temperature settings on triple diversity and quality
  3. Apply augmentation strategies to a small set of seed entities and measure changes in toxicity scores

## Open Questions the Paper Calls Out

### Open Question 1
How do the biases identified in the internal knowledge of LLMs affect their performance in downstream tasks? The paper focuses on identifying and quantifying biases in internal knowledge but does not investigate how these biases influence model behavior in subsequent tasks. Conducting experiments comparing LLM performance on downstream tasks with and without bias mitigation interventions would provide insights into the impact of internal stereotypes on model behavior.

### Open Question 2
Can the proposed prompt-based approach effectively uncover intersectional biases in LLMs? The authors acknowledge that practicality of their approach in investigating intersections of demographic attributes remains unclear. Extending the knowledge graph generation framework to include multiple demographic attributes simultaneously and analyzing resulting biases would provide insights into the effectiveness of the approach in uncovering intersectional biases.

### Open Question 3
How does the proposed knowledge graph generation framework perform when applied to different LLMs? The authors note their approach is tested using GPT-3 and filtering heuristics are specifically designed for this model, acknowledging they may not be applicable to other models. Applying the knowledge graph generation framework to different LLMs and comparing results would provide insights into the robustness and generalizability of the approach across various models.

## Limitations

- Prompt engineering methodology contains significant gaps that prevent exact reproduction
- Dynamic seed entity generation process introduces variability without full entity sets or iteration specifications
- Knowledge graph expansion mechanism may have feedback loops amplifying initial stereotypes
- Heuristic-based invalid generation detection is underspecified, making it difficult to assess false positive/negative rates

## Confidence

- High confidence: The core methodology of using LLM-generated knowledge graphs to identify stereotypes is sound and overgeneralization metrics are well-established
- Medium confidence: The representation disparity analysis via topic modeling provides useful insights, though specific parameter choices may significantly affect results
- Low confidence: The augmentation strategy's effectiveness in revealing hidden biases cannot be fully evaluated without complete prompt modifications and validation across diverse test cases

## Next Checks

1. **Reproduce knowledge graph construction with controlled seeds**: Generate knowledge graphs for a small set of predefined seed entities (e.g., "American people," "Canadian people") using described methodology, then verify triple generation produces semantically coherent stereotypes and expansion strategies maintain topic diversity.

2. **Test augmentation strategy sensitivity**: Systematically vary offensive phrase prompts (e.g., different formulations of "I'm not racist but...") and measure changes in toxicity scores across multiple demographic groups to establish whether priming effect is robust or prompt-dependent.

3. **Validate topic modeling stability**: Apply BERTopic analysis to knowledge graphs with varying sizes and seed entity counts to determine if topic distributions are stable and if representation disparity metrics (relative entropy) are sensitive to these variations.