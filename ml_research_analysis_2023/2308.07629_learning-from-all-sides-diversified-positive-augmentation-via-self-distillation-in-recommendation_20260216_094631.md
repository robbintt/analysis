---
ver: rpa2
title: 'Learning from All Sides: Diversified Positive Augmentation via Self-distillation
  in Recommendation'
arxiv_id: '2308.07629'
source_url: https://arxiv.org/abs/2308.07629
tags:
- positive
- divspa
- recommendation
- item
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the positive data sparsity problem in recommendation
  systems. The proposed method, DivSPA, leverages three types of retrieval strategies
  to collect high-quality and diverse positive item candidates based on users' overall
  interests, short-term intentions, and similar users.
---

# Learning from All Sides: Diversified Positive Augmentation via Self-distillation in Recommendation

## Quick Facts
- **arXiv ID**: 2308.07629
- **Source URL**: https://arxiv.org/abs/2308.07629
- **Reference count**: 34
- **Primary result**: DivSPA outperforms state-of-the-art baselines on two large-scale industrial datasets, achieving +1.604% in APN and +3.043% in VCR in online A/B tests.

## Executive Summary
This paper addresses positive data sparsity in recommendation systems by proposing DivSPA, a framework that diversifies positive augmentation sources through three complementary retrieval strategies. The method collects high-quality and diverse positive item candidates based on users' overall interests (u2i), short-term intentions (i2i), and similar users (u2u2i), then uses self-distillation to double-check and rerank these candidates. Extensive offline and online evaluations demonstrate DivSPA's effectiveness in improving both accuracy and diversity metrics across multiple real-world recommender systems.

## Method Summary
DivSPA tackles positive data sparsity by first conducting three retrieval strategies to collect diverse positive item candidates according to users' overall interests, short-term intentions, and similar users. A self-distillation module then double-checks and re-ranks these candidates based on semantic relevance scores. Finally, a mix-up loss in the output space regularizes training by combining original and augmented positives with relevance-weighted combinations. The framework integrates with standard two-tower architectures like YoutubeDNN and has been deployed on multiple real-world recommender systems including video feeds, e-commerce platforms, and short-video applications.

## Key Results
- DivSPA outperforms several state-of-the-art baselines on two large-scale industrial datasets (Video and Vfeeds)
- Achieves significant improvements in online A/B tests: +1.604% in APN and +3.043% in VCR
- Demonstrates effectiveness across different evaluation metrics including HR@50/100/200 and NDCG@50/100/200
- Shows consistent performance gains in both accuracy and diversity metrics

## Why This Works (Mechanism)

### Mechanism 1
Three retrieval strategies (u2i, i2i, u2u2i) diversify positive augmentation sources. u2i retrieves items based on user representations, i2i based on item representations, and u2u2i based on similar users' interactions. Different retrieval strategies capture orthogonal aspects of user preferences.

### Mechanism 2
Self-distillation double-checks and re-ranks retrieved candidates to ensure accuracy. After retrieval, candidates are weighted by semantic relevance scores and filtered via self-distillation before final augmentation.

### Mechanism 3
Mix-up loss in output space regularizes training and improves generalization. Convex combinations of original and augmented positives are trained jointly, weighting by relevance scores.

## Foundational Learning

- **Concept**: User and item representation learning in two-tower architecture
  - Why needed here: Core to computing semantic relevance scores for retrieval and self-distillation
  - Quick check question: How does the base model (e.g., YoutubeDNN) encode user and item features into dense vectors?

- **Concept**: Semantic similarity scoring (cosine similarity)
  - Why needed here: Used to rank and weight retrieval candidates before augmentation
  - Quick check question: What is the formula for computing semantic relevance between user and item representations?

- **Concept**: Contrastive learning and negative sampling
  - Why needed here: Underpins the base loss function and helps distinguish positive from negative items
  - Quick check question: How are negative samples selected and used in the softmax loss?

## Architecture Onboarding

- **Component map**: Base model (e.g., YoutubeDNN) → User/item embeddings → Retrieval module (u2i, i2i, u2u2i) → Candidate sets → Self-distillation scorer → Weighted candidate selection → Mix-up loss trainer → Final augmented training
- **Critical path**: Base model → Embeddings → Retrieval → Self-distillation → Mix-up loss → Updated model
- **Design tradeoffs**: Retrieval diversity vs. computational cost; Candidate quality vs. augmentation coverage; Output-space vs. representation-space mixing
- **Failure signatures**: Accuracy drops → Self-distillation or retrieval poorly tuned; Diversity stalls → Retrieval strategies not sufficiently distinct; Training instability → Mix-up weights misconfigured
- **First 3 experiments**: 1) Replace one retrieval strategy (e.g., remove i2i) and measure accuracy/diversity impact; 2) Vary mix-up weight β to find optimal balance between original and augmented signals; 3) Test different sampling methods (uniform vs. importance vs. beta) for candidate selection

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of DivSPA vary across different recommendation domains (e.g., e-commerce, news, music)? The paper only evaluates DivSPA on video-related datasets and one online video stream scenario.

### Open Question 2
What is the optimal balance between accuracy and diversity in positive item augmentations for different user segments? The paper treats all users uniformly without investigating personalized trade-offs between accuracy and diversity.

### Open Question 3
How does DivSPA's performance degrade with different levels of initial data sparsity? Experiments were conducted on relatively large datasets without exploring edge cases of extreme sparsity.

### Open Question 4
What are the computational trade-offs of DivSPA compared to simpler augmentation methods in production environments? While mentioning deployment, the paper doesn't discuss practical considerations like inference time or resource requirements.

## Limitations
- Limited evaluation of long-tail user effects and cold-start scenarios
- Unclear robustness to concept drift or temporal shifts in user behavior
- Mix-up hyper-parameters (β) and relevance weighting not extensively explored

## Confidence
- **High confidence**: Retrieval diversity improves positive augmentation quality
- **Medium confidence**: Self-distillation effectively filters low-quality candidates
- **Medium confidence**: Output-space mix-up stabilizes training without harming accuracy

## Next Checks
1. Ablate one retrieval strategy at a time to confirm orthogonal contribution
2. Test semantic relevance scoring quality by manually inspecting top-weighted candidates
3. Measure training stability with varying mix-up weights and sampling methods