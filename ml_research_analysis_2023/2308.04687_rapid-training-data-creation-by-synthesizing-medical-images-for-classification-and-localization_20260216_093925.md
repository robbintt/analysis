---
ver: rpa2
title: Rapid Training Data Creation by Synthesizing Medical Images for Classification
  and Localization
arxiv_id: '2308.04687'
source_url: https://arxiv.org/abs/2308.04687
tags:
- images
- object
- data
- synthetic
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic data generation method for training
  deep learning models on medical images, specifically microscopic urine samples.
  The method addresses the challenge of limited annotated medical data by creating
  synthetic images from extracted object patches, which are then randomly placed on
  blank images with varying counts and classes per image.
---

# Rapid Training Data Creation by Synthesizing Medical Images for Classification and Localization

## Quick Facts
- arXiv ID: 2308.04687
- Source URL: https://arxiv.org/abs/2308.04687
- Reference count: 19
- Key outcome: Synthetic data generation method achieves competitive F1-scores for object detection and better weakly supervised localization than real image training by forcing object-level feature learning.

## Executive Summary
This paper introduces a synthetic data generation method for training deep learning models on medical images, specifically microscopic urine samples. The method addresses the challenge of limited annotated medical data by creating synthetic images from extracted object patches, which are then randomly placed on blank images with varying counts and classes per image. This approach forces models to learn object-level features rather than just discriminative features, improving localization accuracy. For weakly supervised localization, the proposed method produced class activation maps (CAMs) correctly aligned with object locations, unlike models trained on real images. For object detection, the model trained on synthetic images achieved competitive F1-scores compared to one trained on exhaustively annotated real images, with improvements seen when mixing 10% real data with synthetic data. The method reduces annotation time and eliminates the need for exhaustive real annotations while maintaining high performance.

## Method Summary
The method involves creating synthetic images by extracting object patches from real medical images and placing them randomly on blank canvases. Object patches are extracted using center annotations provided by medical experts, with patch sizes based on average biological sizes of objects. These patches are then randomly placed on blank images of specific dimensions (384x384 or 416x416 pixels), with the number of objects and their class distribution sampled from the training dataset. The synthetic images are used to train deep learning models for classification and localization tasks. For weakly supervised training, a multi-label classification approach with binary cross-entropy loss is used, while YOLO2 is employed for object detection tasks. The method also explores mixing 10% real data with synthetic data to improve performance.

## Key Results
- For weakly supervised localization, CAMs generated from models trained on synthetic images correctly aligned with object locations, while models trained on real images showed misaligned CAMs.
- For object detection, the model trained on synthetic images achieved competitive F1-scores compared to one trained on exhaustively annotated real images.
- Mixing 10% real data with synthetic data improved model performance, surpassing real image training for 2 classes and equaling it for 1 class.
- The method successfully eliminated the need for exhaustive real annotations while maintaining high performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic image generation with varied object counts per class forces models to learn object-level features rather than just discriminative features.
- Mechanism: By randomly placing extracted object patches on blank images with varying counts and classes per image, the model cannot rely on global image-level discriminative features and must instead focus on individual object characteristics.
- Core assumption: Deep learning models will naturally converge to object-level features when object distribution within images is controlled to prevent reliance on discriminative features alone.
- Evidence anchors:
  - [abstract]: "This approach forces models to learn object-level features rather than just discriminative features, improving localization accuracy."
  - [section]: "We may assume that the model will learn features that are discriminative which in this case are eyes and nose. But the model training objective is to discriminate between two images and not two objects..."
  - [corpus]: Weak evidence - related papers focus on synthetic data generation but don't directly address the object-level feature learning mechanism described here.
- Break condition: If object patches are too small or lack sufficient discriminative features (like bacteria in urine samples), the model may fail to learn meaningful object-level features regardless of distribution.

### Mechanism 2
- Claim: Using extracted object patches instead of real background images eliminates background bias and simplifies synthetic data creation.
- Mechanism: By stitching extracted object patches directly onto blank images, the model learns to focus on object features without being influenced by complex background patterns that might serve as shortcuts for classification.
- Core assumption: Background patterns in real images can serve as discriminative features that allow models to classify without learning true object-level features.
- Evidence anchors:
  - [section]: "Our approach is much simpler and different from this work. We use a simple extraction method to extract the object and stitch it onto a new image. Also, most of the synthetic data creation work involves using a real background which adds to the complexity of synthetic data creation but our method does not require any real background image thus further simplifying the process."
  - [corpus]: Weak evidence - related papers mention synthetic data generation but don't specifically discuss the advantages of using blank backgrounds versus real backgrounds.
- Break condition: If the synthetic environment is too simplistic and lacks realistic context, the model may not generalize well to real-world images with complex backgrounds.

### Mechanism 3
- Claim: Mixing 10% real data with synthetic data improves model performance by providing batch normalization statistics that better match real data distribution.
- Mechanism: The small percentage of real data provides the model with batch statistics (mean, variance) that are closer to the target distribution, helping the model generalize better during training.
- Core assumption: Batch normalization statistics from synthetic data significantly differ from real data, and this difference negatively impacts model performance.
- Evidence anchors:
  - [abstract]: "For object detection, the model trained on synthetic images achieved competitive F1-scores compared to one trained on exhaustively annotated real images, with improvements seen when mixing 10% real data with synthetic data."
  - [section]: "We also trained a model with 10% of real images mixed with created synthetic images. As can be seen from Fig. 9, model performance has improved and surpassed for 2 classes, and equaled the result of the model trained on real images for 1 class thus establishing the superiority of our method."
  - [corpus]: No direct evidence in related papers about the specific 10% mixing ratio or its effects on batch normalization.
- Break condition: If the synthetic data distribution is too different from real data, even 10% mixing may not be sufficient to bridge the gap in batch normalization statistics.

## Foundational Learning

- Concept: Cross-entropy loss and discriminative feature learning
  - Why needed here: Understanding why models might learn discriminative features instead of object-level features is crucial for grasping the motivation behind synthetic data generation
  - Quick check question: If a model only learns discriminative features, what would happen to its ability to localize objects in weakly supervised learning?

- Concept: Class Activation Maps (CAM) for weakly supervised localization
  - Why needed here: CAM is the evaluation metric used to demonstrate that synthetic data training produces better object localization than real data training
  - Quick check question: How does CAM differ from bounding box predictions in terms of what it reveals about feature learning?

- Concept: Batch normalization and its dependence on batch statistics
  - Why needed here: Understanding why mixing real data with synthetic data improves performance relates to batch normalization's reliance on batch statistics
  - Quick check question: What happens to batch normalization statistics when training data distribution changes significantly?

## Architecture Onboarding

- Component map: Data preprocessing -> Object patch extraction -> Synthetic image generation -> Model training -> CAM generation/Object detection evaluation

- Critical path: Extract object patches with accurate bounding boxes -> Generate synthetic images with random object distributions -> Train model on synthetic images -> Generate CAM for weakly supervised localization evaluation -> For object detection, train YOLO2 and evaluate F1-scores

- Design tradeoffs:
  - Using blank backgrounds vs real backgrounds: Simpler but may hurt generalization
  - 10% real data mixing: Improves performance but requires some real annotations
  - Patch size variation: Adds regularization but may occasionally cut off object features

- Failure signatures:
  - Poor CAM alignment with objects indicates insufficient object-level feature learning
  - Low F1-scores on small objects (like bacteria) suggest inadequate discriminative features
  - High precision but low recall indicates over-conservative predictions

- First 3 experiments:
  1. Train a simple CNN on synthetic images and visualize CAM to verify object-level feature learning
  2. Compare F1-scores of object detection models trained on pure synthetic vs 10% real + 90% synthetic data
  3. Test model generalization by evaluating on real test images after training on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does mixing real and synthetic data affect batch normalization statistics, and would adjusting batch normalization parameters improve model performance?
- Basis in paper: [explicit] The paper mentions that "since the batch statistics of the synthetic dataset are different from the real dataset, more experiments need to be conducted to establish the effect of mixing real data with synthetic data, and if changing the parameters of batch normalization as suggested by Li et al. would further help improve the model predictions."
- Why unresolved: The paper acknowledges this as a potential area for improvement but does not conduct experiments to test the impact of adjusting batch normalization parameters when mixing real and synthetic data.
- What evidence would resolve it: Experiments comparing model performance with and without batch normalization parameter adjustments when training on mixed real and synthetic data, measuring metrics like F1-score and localization accuracy.

### Open Question 2
- Question: What is the optimal ratio of real to synthetic data for training medical image analysis models, and does this ratio vary by object size and complexity?
- Basis in paper: [inferred] The paper shows that mixing 10% real data with synthetic data improved results for some classes, but doesn't explore other ratios or systematically test how this might vary for different object types.
- Why unresolved: The paper only tested one mixing ratio (10% real data) and didn't explore how different ratios might affect performance across different object types, particularly for challenging small objects like bacteria.
- What evidence would resolve it: Systematic experiments testing various real-to-synthetic data ratios (e.g., 1%, 5%, 10%, 25%, 50%) across different object types, measuring performance metrics to identify optimal ratios for each category.

### Open Question 3
- Question: Can the synthetic data generation method be extended to 3D medical imaging modalities like CT or MRI scans, and what modifications would be needed?
- Basis in paper: [inferred] The paper focuses on 2D microscopic images and doesn't address whether the synthetic patch extraction and placement approach could work for volumetric medical imaging data.
- Why unresolved: The paper doesn't explore applications beyond 2D microscopy images, leaving unclear whether the methodology could be adapted for other medical imaging modalities that generate 3D data.
- What evidence would resolve it: Successful implementation of the synthetic data generation method on 3D medical imaging datasets, demonstrating comparable improvements in model performance and reduced annotation requirements for volumetric data.

## Limitations

- The paper only tested the method on microscopic urine samples, leaving unclear whether the approach generalizes to other medical imaging domains or object types.
- The exact patch size extraction methodology is underspecified, making reproduction difficult and raising questions about optimal patch dimensions.
- The 10% real data mixing ratio appears to be an empirical observation rather than theoretically justified, with no exploration of sensitivity to different ratios.

## Confidence

**High Confidence**:
- Synthetic data can effectively train models for object detection tasks, achieving competitive F1-scores
- Models trained on synthetic data show better weakly supervised localization than those trained on real images
- The synthetic data generation approach is simpler than existing methods that require real background images

**Medium Confidence**:
- The mechanism by which synthetic data forces object-level feature learning (random object distribution preventing reliance on discriminative features)
- The specific claim that 10% real data mixing improves performance through batch normalization statistics
- The assertion that blank backgrounds are superior to real backgrounds for synthetic data generation

**Low Confidence**:
- Generalization of the method beyond urine sample analysis
- The robustness of the approach across different object sizes and visual complexity
- Whether the improvements in localization would translate to other weakly supervised learning tasks

## Next Checks

1. **Patch size sensitivity analysis**: Systematically vary the patch extraction size (smaller, larger, and at the claimed biological average) and measure the impact on both classification accuracy and localization quality. This would validate whether the patch size choice is critical to the method's success.

2. **Background complexity ablation study**: Compare model performance when using synthetic images with blank backgrounds versus synthetic images with simple textured backgrounds versus real backgrounds. This would test the assumption that background simplification is beneficial.

3. **Cross-domain generalization test**: Apply the same synthetic data generation methodology to a different medical imaging domain (such as histopathology slides or X-ray images) and evaluate whether similar performance improvements in localization and detection are observed. This would validate the broader applicability of the approach.