---
ver: rpa2
title: Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization
arxiv_id: '2309.07235'
source_url: https://arxiv.org/abs/2309.07235
tags:
- autotuning
- autotvm
- ytopt
- framework
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an autotuning framework for Apache TVM-based
  scientific applications using Bayesian optimization via the ytopt tool. The authors
  implement three linear algebra kernels (LU, Cholesky, and 3mm) in TVM's tensor expression
  language and compare their framework against AutoTVM with four tuners (Random, GridSearch,
  GA, and XGBoost) on a GPU cluster.
---

# Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization

## Quick Facts
- arXiv ID: 2309.07235
- Source URL: https://arxiv.org/abs/2309.07235
- Authors: 
- Reference count: 15
- Key outcome: ytopt's Bayesian optimization framework outperforms AutoTVM in most cases, completing 100 evaluations faster while achieving comparable or better runtime performance for TVM-based scientific kernels

## Executive Summary
This paper presents a novel autotuning framework for Apache TVM-based scientific applications using Bayesian optimization via the ytopt tool. The authors implement three linear algebra kernels (LU decomposition, Cholesky factorization, and 3mm matrix multiplication) in TVM's tensor expression language and compare their framework against AutoTVM with four different tuners on a GPU cluster. The results demonstrate that ytopt can efficiently explore the tuning space and find near-optimal configurations faster than AutoTVM, particularly for large problem sizes.

## Method Summary
The authors parameterize TVM tensor expression code with tunable parameters to create a code mold, which is then optimized using ytopt's Bayesian optimization framework. The tuning process involves iteratively generating new TE code for different parameter configurations, compiling it to machine code, executing it, and measuring runtime. The ytopt framework uses a Random Forest surrogate model and Lower Confidence Bound acquisition function to balance exploration and exploitation of the search space. The optimized configurations are then compared against AutoTVM's four tuners (Random, GridSearch, GA, and XGBoost) in terms of autotuning process time and runtime performance.

## Key Results
- ytopt completed 100 evaluations faster than AutoTVM for LU decomposition with tensor size 400x50 (1.659s vs 1.65s)
- For extra-large dataset LU decomposition, ytopt achieved 13.77s with tensor size 40x32
- ytopt outperformed AutoTVM in autotuning process time for Cholesky and 3mm kernels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian optimization in ytopt balances exploration and exploitation better than AutoTVM tuners for TVM-based scientific kernels.
- Mechanism: ytopt uses a dynamically updated Random Forest surrogate model and the Lower Confidence Bound (LCB) acquisition function to balance exploration of new parameter configurations with exploitation of high-performing ones. This iterative process allows more efficient search of the tuning space.
- Core assumption: The runtime of TVM kernels is a smooth function of the tuning parameters, and the surrogate model can approximate this relationship.
- Evidence anchors:
  - [abstract] "We propose a new autotuning framework for TVM-based scientific tensor applications using Bayesian Optimization"
  - [section 2.2] "ytopt ... uses Bayesian optimization ... to balance exploration and exploitation of the search space"
- Break condition: If the tuning space has many local minima or is discontinuous, the surrogate model may not accurately guide the search.

### Mechanism 2
- Claim: Replacing AutoTVM's predefined parameter space with ytopt's parameterized code mold enables more flexible exploration.
- Mechanism: The authors parameterize the TVM tensor expression code with tunable parameters, creating a code mold. ytopt then generates new TE code for each parameter configuration, which is compiled and executed. This allows testing a wider range of optimizations than AutoTVM's fixed parameter space.
- Core assumption: The parameterization captures the key performance knobs in the TVM schedule.
- Evidence anchors:
  - [section 3] "We basically replace the autotuning modules in Figure 1 with the ytopt module. Based on the TE code, we identify tunable parameters..."
  - [section 4] "we use the six tiling factors as tunable parameters to parameterize the code to generate a code mold"
- Break condition: If important performance knobs are not captured in the parameterization, the search space will be incomplete.

### Mechanism 3
- Claim: ytopt's Bayesian optimization can find near-optimal configurations faster than AutoTVM's heuristic tuners for large problem sizes.
- Mechanism: For large problem sizes, the tuning space grows exponentially. AutoTVM's heuristic tuners (Random, GridSearch, GA, XGBoost) explore this space less efficiently than ytopt's surrogate model-guided search. ytopt can quickly focus on promising regions of the space.
- Core assumption: The runtime differences between configurations are large enough to be distinguished with limited evaluations.
- Evidence anchors:
  - [section 5] "ytopt outperformed AutoTVM in most cases and took the smallest autotuning process time in most cases"
  - [section 5] "ytopt outperformed 4 AutoTVM tuners shown in Figure 10 in the smallest autotuning process time"
- Break condition: If the runtime differences between configurations are small, the Bayesian optimization may not significantly outperform random search.

## Foundational Learning

- Concept: Bayesian Optimization
  - Why needed here: ytopt uses Bayesian optimization to efficiently search the tuning space of TVM parameters.
  - Quick check question: What are the key components of a Bayesian optimization algorithm? (Answer: Surrogate model, acquisition function, iterative sampling)

- Concept: Tensor Expression (TE) Language
  - Why needed here: The authors implement the linear algebra kernels in TVM's TE language and parameterize the schedules for tuning.
  - Quick check question: What is the difference between TVM's Relay and TE representations? (Answer: Relay is a high-level graph IR, TE is a lower-level tensor computation language)

- Concept: Loop Tiling
  - Why needed here: The authors tune the loop tiling factors in the TVM schedules to optimize for different problem sizes and hardware.
  - Quick check question: How does loop tiling affect the performance of matrix multiplication? (Answer: It improves data locality and enables parallelization)

## Architecture Onboarding

- Component map: TVM framework (TE language, compiler, runtime) -> ytopt (Bayesian optimization engine, Random Forest surrogate model, LCB acquisition function) -> Linear algebra kernels (3mm, Cholesky, LU decomposition) -> Hardware (Swing GPU cluster: 2x AMD EPYC 7742, 8x NVIDIA A100)

- Critical path:
  1. Parameterize TVM TE code with tunable parameters
  2. Define parameter space using ConfigSpace
  3. ytopt selects parameter configuration using Bayesian optimization
  4. Generate new TE code and compile to machine code
  5. Execute and measure runtime
  6. Update surrogate model and repeat until convergence

- Design tradeoffs:
  - Larger parameter spaces allow more flexibility but increase search time
  - More evaluations improve confidence in the optimal configuration but increase tuning time
  - Simpler surrogate models are faster but may not capture complex relationships

- Failure signatures:
  - Surrogate model consistently predicts poor configurations
  - Runtime measurements are noisy or inconsistent
  - Optimal configuration is not found within the allocated evaluations

- First 3 experiments:
  1. Implement a simple matrix multiplication kernel in TVM TE and tune the loop tiling factors using ytopt.
  2. Compare the performance of ytopt and AutoTVM on a small problem size (e.g., 1000x1000 matrix).
  3. Scale up to a larger problem size (e.g., 4000x4000 matrix) and compare the tuning times and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Bayesian optimization framework in ytopt compare to AutoTVM in terms of convergence speed and final performance for larger problem sizes?
- Basis in paper: [explicit] The paper states that ytopt outperformed AutoTVM in most cases and took the smallest autotuning process time in most cases.
- Why unresolved: The paper only presents results for a limited number of problem sizes. It is unclear how the performance of ytopt and AutoTVM would scale for much larger problem sizes.
- What evidence would resolve it: Conducting experiments with significantly larger problem sizes and comparing the convergence speed and final performance of ytopt and AutoTVM.

### Open Question 2
- Question: How does the proposed autotuning framework handle non-linear optimization problems, and what are its limitations?
- Basis in paper: [inferred] The paper mentions that ytopt uses Bayesian optimization, which is a non-linear optimization technique. However, it does not discuss how the framework handles non-linear optimization problems or its limitations.
- Why unresolved: The paper does not provide any information on the framework's performance or limitations when dealing with non-linear optimization problems.
- What evidence would resolve it: Conducting experiments with non-linear optimization problems and analyzing the framework's performance and limitations.

### Open Question 3
- Question: How does the proposed autotuning framework handle different hardware platforms, such as CPUs and AI accelerators, and what are its limitations?
- Basis in paper: [inferred] The paper mentions that TVM is designed to optimize computations across various hardware platforms, including CPUs, GPUs, and AI accelerators. However, it does not discuss how the proposed autotuning framework handles different hardware platforms or its limitations.
- Why unresolved: The paper does not provide any information on the framework's performance or limitations when dealing with different hardware platforms.
- What evidence would resolve it: Conducting experiments with different hardware platforms and analyzing the framework's performance and limitations.

## Limitations
- Limited evaluation to three linear algebra kernels reduces generalizability
- Results specific to a particular GPU cluster configuration (AMD EPYC 7742 + NVIDIA A100)
- Significant computational resources still required for large problem sizes
- No discussion of potential overfitting to tested problem sizes

## Confidence
- **High confidence**: ytopt demonstrates faster autotuning process times compared to AutoTVM across multiple kernels and problem sizes
- **Medium confidence**: ytopt achieves comparable or better runtime performance than AutoTVM tuners for the tested kernels
- **Medium confidence**: The Bayesian optimization approach provides better exploration-exploitation balance than the heuristic tuners

## Next Checks
1. Evaluate ytopt on a broader range of scientific kernels (e.g., FFT, stencil computations, sparse matrix operations) to assess generalizability
2. Test the autotuning framework on different hardware architectures (CPUs, FPGAs, other GPUs) to verify portability
3. Analyze the sensitivity of the optimal configurations to changes in problem size to assess robustness