---
ver: rpa2
title: 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach
  to solve the Abstraction and Reasoning Corpus (ARC) Challenge'
arxiv_id: '2310.05146'
source_url: https://arxiv.org/abs/2310.05146
tags:
- grid
- object
- output
- view
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using Large Language Models (LLMs) as a system
  of multiple expert agents to solve the Abstraction and Reasoning Corpus (ARC) Challenge.
  The method involves converting input images into multiple text-based abstraction
  spaces (grid, object, and pixel views) and using LLMs to derive input-output relationships
  and map them to actions in the form of a working program.
---

# Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge

## Quick Facts
- **arXiv ID**: 2310.05146
- **Source URL**: https://arxiv.org/abs/2310.05146
- **Reference count**: 18
- **One-line primary result**: Proposes using multiple expert agents with different abstraction spaces to solve ARC tasks, achieving 45% solve rate on 111 training set problems.

## Executive Summary
This paper introduces an innovative approach to solving the Abstraction and Reasoning Corpus (ARC) challenge by leveraging Large Language Models (LLMs) as a system of multiple expert agents. The method converts input images into multiple text-based abstraction spaces (grid, object, and pixel views) and uses LLMs to derive input-output relationships, mapping them to actions in the form of working programs. Iterative environmental feedback guides the LLMs in solving tasks, achieving a 45% solve rate on 111 training set problems. The authors suggest that further improvements could be realized by incorporating additional abstraction spaces and learnable actions.

## Method Summary
The approach involves converting ARC input images into multiple abstraction spaces (grid, object, and pixel views) to help LLMs identify patterns and relationships. GPT-4 is used with primitive functions and conditional functions to generate code that transforms input grids into output grids. The method employs iterative environmental feedback, where generated code is tested against input-output pairs, and errors are used to refine solutions. The approach achieved a 45% solve rate on 111 training set problems, solving 50 out of 111 tasks.

## Key Results
- Achieved 45% solve rate on 111 training set problems (50 out of 111 tasks solved)
- Utilizes multiple abstraction spaces (grid, object, pixel views) to help LLMs identify patterns
- Iterative environmental feedback guides code refinement and improves solve rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multiple abstraction spaces help LLMs better associate input-output relationships in ARC tasks.
- **Mechanism**: Converting input images into grid, object, and pixel views allows LLMs to process tasks from different perspectives, increasing the chance of finding simple mappings from input to output.
- **Core assumption**: ARC tasks can be solved by identifying patterns in one or more predefined abstraction spaces.
- **Evidence anchors**: [abstract] "We firstly convert the input image into multiple suitable text-based abstraction spaces." [section 3] "Objects are defined as continuous sections of the grid with the same non-zero value... providing such an object view as an abstraction space using text greatly helps with the GPT-4's ability to form associations with the input-output pair."

### Mechanism 2
- **Claim**: Grounding LLMs with primitive functions and conditional functions enables program synthesis for ARC tasks.
- **Mechanism**: Providing a set of primitive functions and conditional functions allows the LLM to generate code that manipulates the grid based on identified patterns in the abstraction spaces.
- **Core assumption**: ARC tasks can be solved by composing a sequence of primitive functions and conditional statements.
- **Evidence anchors**: [abstract] "We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program." [section 3] "A key innovation in this work is to use primitive functions as action spaces, as a way to encode human priors."

### Mechanism 3
- **Claim**: Iterative environmental feedback guides the LLM to solve ARC tasks by refining generated code based on output.
- **Mechanism**: The LLM generates code based on input-output pairs and abstraction spaces. If code produces incorrect output or compilation errors, the LLM is prompted to refine the code based on feedback.
- **Core assumption**: The LLM can iteratively improve code generation by incorporating feedback from the environment.
- **Evidence anchors**: [abstract] "In addition, we use iterative environmental feedback in order to guide LLMs to solve the task." [section 3] "A recursive loop feeding in feedback from the environment (whether there is compile error, whether the code matches the intended output) can help a lot in getting the right answer."

## Foundational Learning

- **Concept: Abstraction and Reasoning Corpus (ARC)**
  - **Why needed here**: Understanding the nature of ARC tasks is crucial for designing an effective solution approach.
  - **Quick check question**: What are the key challenges in solving ARC tasks using traditional deep learning methods?

- **Concept: Large Language Models (LLMs)**
  - **Why needed here**: LLMs are the core component used to solve the ARC tasks in this approach.
  - **Quick check question**: How do LLMs differ from traditional deep learning models in terms of their ability to learn from few samples?

- **Concept: Program Synthesis**
  - **Why needed here**: The approach involves generating code to solve the ARC tasks, which is a form of program synthesis.
  - **Quick check question**: What are the key challenges in using LLMs for program synthesis, and how does this approach address them?

## Architecture Onboarding

- **Component map**: ARC task -> Multiple abstraction spaces (Grid, Object, Pixel views) -> GPT-4 with primitive functions -> Iterative feedback -> Generated code -> Output
- **Critical path**:
  1. Convert input image into multiple abstraction spaces
  2. Use LLM to identify patterns and generate code
  3. Test generated code against input-output pairs
  4. Incorporate feedback and refine code iteratively
  5. Output final code that solves the ARC task
- **Design tradeoffs**:
  - Using multiple abstraction spaces increases context length and computational cost but improves chances of finding a solution
  - Relying on primitive functions limits expressiveness of generated code but improves grounding and reduces compilation errors
  - Iterative feedback improves code quality but increases API calls and computational cost
- **Failure signatures**:
  - Generated code has compilation errors or produces incorrect output even after multiple feedback iterations
  - LLM fails to identify relevant patterns in abstraction spaces or generates code that doesn't solve the ARC task
- **First 3 experiments**:
  1. Test approach on simple ARC task with clear pattern identifiable using one abstraction space
  2. Test approach on ARC task requiring combination of multiple abstraction spaces or primitive functions
  3. Test approach on ARC task requiring iterative feedback to refine generated code

## Open Questions the Paper Calls Out

- **Open Question 1**: How does performance scale with increased number of abstraction spaces beyond the three currently used?
  - **Basis in paper**: [explicit] Authors state "we believe that with more abstraction spaces and learnable actions, we will be able to solve more" and discuss potential improvements in section 8.
  - **Why unresolved**: Current implementation only uses three abstraction spaces, and paper doesn't provide empirical evidence of performance gains from additional abstraction spaces.
  - **What evidence would resolve it**: Conducting experiments with larger set of abstraction spaces and comparing solve rates would provide concrete evidence.

- **Open Question 2**: What is the impact of iterative environmental feedback on solve rate, and can it be optimized further?
  - **Basis in paper**: [explicit] Authors mention iterative feedback helps solve 7 tasks out of 50 and discuss its importance in section 6.
  - **Why unresolved**: While paper shows iterative feedback is beneficial, it doesn't explore optimization strategies or quantify potential improvements from enhanced feedback mechanisms.
  - **What evidence would resolve it**: Implementing and testing various feedback optimization strategies would provide insights into potential for performance enhancement.

- **Open Question 3**: How does choice of primitive functions affect solve rate, and can more comprehensive set of learnable functions be developed?
  - **Basis in paper**: [explicit] Authors note coding issues account for 8 out of 61 unsolved problems and suggest better primitive functions could improve solve rate in section 6.
  - **Why unresolved**: Current set of primitive functions is hand-engineered and may not cover all possible ARC tasks, limiting approach's effectiveness.
  - **What evidence would resolve it**: Experimenting with broader and more diverse set of primitive functions and measuring their impact on solve rate would clarify their role in improving performance.

## Limitations

- The approach's reliance on GPT-4's 3,000 token context window constrains the complexity of tasks that can be addressed
- Evaluation only considers the training set rather than the private test set, making the 45% solve rate claim difficult to verify independently
- Paper lacks detailed ablation studies showing individual contributions of abstraction spaces, primitive functions, and iterative feedback to overall performance

## Confidence

**High Confidence (80-100%)**
- The basic methodology of using multiple abstraction spaces with LLMs is technically sound
- The approach can solve some ARC tasks, as evidenced by 50 out of 111 training tasks solved
- Iterative feedback helps refine generated code, consistent with established reinforcement learning principles

**Medium Confidence (50-80%)**
- Performance can be improved with more abstraction spaces, though this is plausible but not empirically validated
- The 45% solve rate is representative of general ARC task performance, given only training set was evaluated
- The approach represents significant advance over baseline methods, though comparative analysis is limited

**Low Confidence (0-50%)**
- The approach will scale effectively to more complex ARC tasks not represented in current training set
- The specific primitive functions and abstraction spaces used are optimal for ARC task solving
- The approach will maintain or improve performance on the private test set

## Next Checks

1. **Independent Reproduction on Test Set**: Validate the approach on the private ARC test set rather than the training set to establish true generalization capability, implementing the exact same methodology with the same GPT-4 prompts and abstraction space definitions.

2. **Ablation Study of Components**: Systematically remove each major component (one abstraction space at a time, the iterative feedback mechanism, or the primitive functions) to quantify their individual contributions to the 45% solve rate, clarifying which innovations are most impactful.

3. **Scaling Analysis with Additional Abstraction Spaces**: Implement and test at least two additional abstraction spaces beyond the current three (grid, object, pixel views) to empirically verify the claim that more abstraction spaces improve performance, documenting the marginal improvement per added abstraction space to establish diminishing returns.