---
ver: rpa2
title: Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural
  Language Understanding
arxiv_id: '2310.14676'
source_url: https://arxiv.org/abs/2310.14676
tags:
- gaze
- data
- language
- scanpaths
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data scarcity for gaze-augmented
  NLP models by proposing a method to generate synthetic scanpaths (sequences of eye
  fixations) during reading. The core idea is to integrate a synthetic scanpath generator
  with a scanpath-augmented language model, eliminating the need for human gaze data.
---

# Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding

## Quick Facts
- **arXiv ID**: 2310.14676
- **Source URL**: https://arxiv.org/abs/2310.14676
- **Reference count**: 23
- **Primary result**: Synthetic scanpaths can replace real gaze data in augmenting language models for NLP tasks without significant performance loss

## Executive Summary
This paper addresses the challenge of data scarcity for gaze-augmented NLP models by proposing a method to generate synthetic scanpaths (sequences of eye fixations) during reading. The core idea is to integrate a synthetic scanpath generator with a scanpath-augmented language model, eliminating the need for human gaze data. The proposed model combines the Eyettention scanpath generation model with the PLM-AS framework, allowing the synthetic scanpaths to be fine-tuned for downstream NLP tasks through error gradient propagation. Experiments on sentiment classification and the GLUE benchmark show that the proposed model outperforms the underlying language model and achieves comparable performance to models augmented with real human gaze data. The model is particularly effective in low-resource settings and across diverse NLP tasks, highlighting the potential of integrating cognitive signals from eye gaze into a wider range of NLP tasks.

## Method Summary
The proposed method integrates a pre-trained Eyettention scanpath generation model with a scanpath-augmented language model framework (PLM-AS). During training, the Eyettention model generates synthetic scanpaths that are fed into the PLM-AS framework alongside text input. The model's error gradient is propagated through the entire architecture, allowing the scanpath generator to be fine-tuned for specific NLP tasks. The Eyettention model is pre-trained on the CELER dataset and then fine-tuned on task-specific data using a Gumbel-softmax approximation. The PLM-AS framework rearranges token embeddings based on the generated scanpaths and processes them through a scanpath encoder (GRU) before making task-specific predictions.

## Key Results
- The proposed model outperforms BERT baseline across multiple GLUE tasks, especially in low-resource settings (K=200, 500, 1000 samples)
- The model achieves performance comparable to state-of-the-art ScanTextGAN augmented with real gaze data
- Ablation experiments show that both pre-training and task-specific fine-tuning of the Eyettention model are crucial for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic scanpaths can replace real gaze data in augmenting language models for NLP tasks without significant performance loss.
- Mechanism: The model uses a pre-trained Eyettention scanpath generation model to create synthetic scanpaths, which are then integrated into the PLM-AS framework. The error gradient is propagated through the entire model, allowing the scanpath generator to be fine-tuned for downstream tasks.
- Core assumption: The synthetic scanpaths generated by Eyettention are sufficiently similar to human gaze patterns to provide the same cognitive signals that benefit NLP tasks.
- Evidence anchors:
  - [abstract]: "We find that the proposed model not only outperforms the underlying language model, but achieves a performance that is comparable to a language model augmented with real human gaze data."
  - [section]: "Our model outperforms both BERT and the state-of-the-art ScanTextGAN (Khurana et al., 2023) augmented with gaze data."
  - [corpus]: Weak evidence. The corpus provides related papers on scanpath prediction but lacks direct comparison of synthetic vs. real gaze data performance.
- Break condition: If the synthetic scanpaths diverge significantly from human gaze patterns or fail to capture the relevant cognitive signals for the NLP task.

### Mechanism 2
- Claim: Fine-tuning the Eyettention model for specific NLP tasks improves the quality of synthetic scanpaths and overall model performance.
- Mechanism: During training, the error gradient from the scanpath-augmented language model is back-propagated through the Eyettention model, allowing its parameters to be adapted for the specific NLP task.
- Core assumption: The task-specific fine-tuning of the scanpath generator enhances the relevance of the synthetic scanpaths for the downstream task.
- Evidence anchors:
  - [section]: "Ablation experiments (bottom two rows) show that when the Eyettention model is frozen or not pre-trained, the performance decreases. This demonstrates the importance of both pre-training and task-specific fine-tuning of the scanpath generator."
  - [abstract]: "Since the model’s error gradient can be propagated throughout all parts of the model, the scanpath generator can be fine-tuned to downstream tasks."
  - [corpus]: Weak evidence. The corpus mentions synthesizing human gaze feedback but doesn't discuss task-specific fine-tuning of scanpath generators.
- Break condition: If the fine-tuning process overfits to the training data or if the task-specific gaze patterns are too complex to be captured by the synthetic generator.

### Mechanism 3
- Claim: Gaze augmentation is particularly beneficial in low-resource settings where limited training data is available.
- Mechanism: The synthetic scanpaths provide additional cognitive signals that help the model learn more effectively from limited training data.
- Core assumption: The cognitive signals from gaze data (synthetic or real) are especially valuable when the model has fewer examples to learn from.
- Evidence anchors:
  - [section]: "Our model consistently outperforms BERT across tasks, except for the STS-B task. In terms of average score, our model shows performance gains of 2-4% compared to BERT."
  - [abstract]: "Our approach not only outperforms the underlying language model in multiple tasks on the GLUE, especially in low-resource settings..."
  - [corpus]: No direct evidence in the corpus. The corpus discusses scanpath prediction but not low-resource NLP task performance.
- Break condition: If the synthetic gaze signals become less informative than the textual data as the amount of training data increases, or if the model relies too heavily on gaze signals in low-resource settings.

## Foundational Learning

- Concept: Scanpaths and their role in modeling human reading behavior
  - Why needed here: Understanding scanpaths is crucial for grasping how synthetic gaze data can augment language models.
  - Quick check question: What information do scanpaths capture about human reading that might be useful for NLP tasks?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The PLM-AS framework uses a scanpath encoder to rearrange token embeddings based on gaze order, which is related to attention mechanisms.
  - Quick check question: How does the scanpath encoder in PLM-AS differ from a standard attention mechanism in terms of how it processes token embeddings?

- Concept: Fine-tuning pre-trained models
  - Why needed here: The model involves fine-tuning both the language model (BERT) and the scanpath generator (Eyettention) for specific NLP tasks.
  - Quick check question: What are the potential benefits and risks of fine-tuning both the language model and the scanpath generator simultaneously?

## Architecture Onboarding

- Component map:
  Text → Eyettention (scanpath generation) → BERT (token embeddings) → Scanpath encoder (rearrangement and processing) → Output

- Critical path:
  Text → Eyettention (scanpath generation) → BERT (token embeddings) → Scanpath encoder (rearrangement and processing) → Output

- Design tradeoffs:
  - Using synthetic vs. real gaze data: Eliminates data collection needs but may introduce generation errors
  - Fine-tuning Eyettention: Improves task-specific performance but increases training complexity and risk of overfitting
  - Number of synthetic scanpaths: More paths may improve performance but increase computational cost

- Failure signatures:
  - Performance degradation when using frozen Eyettention model
  - Poor results on tasks where word order is critical (e.g., CoLA)
  - Limited improvement over BERT in high-resource settings

- First 3 experiments:
  1. Compare performance of BERT vs. BERT with synthetic scanpaths on a simple sentiment classification task
  2. Ablation study: Compare performance with frozen vs. fine-tuned Eyettention model
  3. Low-resource experiment: Evaluate performance on GLUE tasks with varying amounts of training data (K = {200, 500, 1000})

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific types of NLP tasks that benefit most from gaze-augmented models?
- Basis in paper: [explicit] The authors state that gaze signals show benefits not only for sentiment classification tasks (SST-2), as reported in previous research, but also for entailment classification tasks (MNLI, RTE) and a sentence similarity task (STS-B).
- Why unresolved: While the authors identify some tasks that benefit from gaze augmentation, they do not provide a comprehensive analysis of which types of tasks are most suited for this approach. The effectiveness of gaze augmentation may vary depending on the nature of the task, and further research is needed to identify the most promising applications.
- What evidence would resolve it: A systematic study comparing the performance of gaze-augmented models across a wide range of NLP tasks, with a focus on identifying the characteristics of tasks that benefit most from gaze signals.

### Open Question 2
- Question: How does the performance of gaze-augmented models vary across different languages and writing systems?
- Basis in paper: [inferred] The authors mention that gaze augmentation might be particularly interesting for low-resource languages, but they do not provide any empirical evidence on the performance of gaze-augmented models across different languages.
- Why unresolved: The effectiveness of gaze augmentation may depend on the specific characteristics of a language or writing system, such as the complexity of the orthography or the presence of certain linguistic features. Further research is needed to understand how gaze-augmented models perform across different languages and writing systems.
- What evidence would resolve it: Experiments comparing the performance of gaze-augmented models across multiple languages and writing systems, with a focus on identifying the factors that influence their effectiveness.

### Open Question 3
- Question: How does the quality of synthetic gaze data affect the performance of gaze-augmented models?
- Basis in paper: [explicit] The authors use Eyettention, a state-of-the-art model for scanpath generation, to generate synthetic gaze data. However, they do not investigate how the quality of the synthetic data affects the performance of the gaze-augmented model.
- Why unresolved: The quality of synthetic gaze data is crucial for the effectiveness of gaze-augmented models. If the synthetic data does not accurately reflect human gaze behavior, the model may not benefit from the gaze signals. Further research is needed to understand how the quality of synthetic gaze data impacts model performance.
- What evidence would resolve it: Experiments comparing the performance of gaze-augmented models trained on synthetic gaze data generated by different models or with different levels of realism, to identify the impact of data quality on model performance.

## Limitations

- The synthetic scanpaths may not fully capture the complexity and variability of human gaze patterns across different readers and contexts
- The model's performance gains in low-resource settings are primarily demonstrated on GLUE benchmark tasks and may not generalize to all NLP tasks
- The fine-tuning process for the Eyettention model introduces additional complexity and potential for overfitting, especially when task-specific data is limited

## Confidence

**High Confidence**: The claim that the proposed model outperforms the underlying BERT model across multiple GLUE tasks is well-supported by the experimental results. The consistent performance improvements across different tasks and data sizes (K=200, 500, 1000) provide strong evidence for this claim. The ablation studies showing the importance of both pre-training and task-specific fine-tuning of the Eyettention model also support this claim with high confidence.

**Medium Confidence**: The assertion that the proposed model achieves performance comparable to models using real human gaze data is based on comparison with the ScanTextGAN model, which is a reasonable benchmark. However, direct comparison with models trained on real gaze data for the same tasks would strengthen this claim. The effectiveness of gaze augmentation in low-resource settings is demonstrated, but the exact point at which synthetic gaze signals become less informative than textual data as training data increases is not precisely characterized.

**Low Confidence**: The claim that the synthetic scanpaths generated by Eyettention are "sufficiently similar to human gaze patterns to provide the same cognitive signals" is difficult to verify without direct comparison of synthetic and real scanpath distributions. The paper does not provide quantitative measures of scanpath similarity or ablation studies showing the impact of scanpath quality on task performance.

## Next Checks

1. **Scanpath Similarity Analysis**: Conduct a detailed comparison of synthetic scanpaths generated by Eyettention with real human gaze data on the same text samples. Use established metrics for scanpath similarity (e.g., string-edit distance, vector-based measures) to quantify the quality of synthetic gaze patterns and correlate these metrics with NLP task performance.

2. **Cross-Dataset Generalization**: Evaluate the model's performance on NLP tasks beyond the GLUE benchmark, particularly in domains with different reading patterns (e.g., scientific literature, legal documents, social media text). This will test the generalizability of synthetic scanpaths across diverse text types and reading contexts.

3. **Fine-tuning Stability Analysis**: Perform an extensive ablation study varying the amount of task-specific data used to fine-tune Eyettention. Analyze the trade-off between performance gains and the risk of overfitting as the amount of fine-tuning data increases or decreases. This will help determine the optimal data regime for balancing model complexity and performance.