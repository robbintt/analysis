---
ver: rpa2
title: Do large language models solve verbal analogies like children do?
arxiv_id: '2310.20384'
source_url: https://arxiv.org/abs/2310.20384
tags:
- children
- analogies
- llms
- analogy
- verbal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were tested on verbal analogy tasks
  designed for children aged 7-12 in Dutch. Six models (RobBERT, BERTje, XLM-V, D-GPT-2,
  M-GPT, and GPT-3) were evaluated using 622 analogies, and their performance was
  compared to children's performance on the same items.
---

# Do large language models solve verbal analogies like children do?

## Quick Facts
- arXiv ID: 2310.20384
- Source URL: https://arxiv.org/abs/2310.20384
- Authors: 
- Reference count: 20
- Key outcome: LLM performance drops by 1-2 age-equivalent years when controlling for associative processes

## Executive Summary
This study tests whether large language models (LLMs) solve verbal analogies using associative reasoning like children or relational mapping like adults. Six Dutch LLMs were evaluated on 622 analogies previously solved by 14,006 Dutch children aged 7-12. The models showed age-equivalent performance levels ranging from 7-11 years old, with the best models performing comparably to 11-year-olds. Crucially, when controlling for associative processes through specific prompting experiments, all models' performance dropped by 1-2 years in age-equivalent terms, indicating that LLMs primarily solve analogies through associative retrieval rather than relational mapping.

## Method Summary
The study evaluated six Dutch LLMs (RobBERT, BERTje, XLM-V, D-GPT-2, M-GPT, and GPT-3) on 622 verbal analogies from an adaptive learning platform. Children's performance data was collected using item response theory and ELO scoring. Models were prompted with analogies in various formats (full A:B::C:?, C:?, and transposed A:C::B:?) and their accuracy was compared to children's age-equivalent performance levels. Performance was analyzed across relation types, semantic distances, and distractor salience.

## Key Results
- Best-performing models (XLM-V, GPT-3) achieved 11-year-old level accuracy
- Worst-performing model (M-GPT) performed at 7-year-old level
- Controlling for associative processes dropped all models' performance by 1-2 years
- Model performance correlated with distractor salience in the same way as children

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs solve analogies using associative retrieval rather than relational mapping.
- Mechanism: The model predicts the most statistically likely next word based on co-occurrence patterns learned during training.
- Core assumption: LLMs treat analogies as word prediction tasks rather than reasoning tasks.
- Evidence anchors:
  - [abstract] "when we control for associative processes this picture changes and each model's performance level drops 1-2 years"
  - [section 7.1] "LLMs should still be able to solve a substantial portion of analogies purely by association with C (Ushio et al., 2021b; Poliak et al., 2018); hypothesis 3a). This was indeed the case, where GPT-3 solved 33% of the items with only the C term"
  - [corpus] Weak - corpus shows related work on LLMs and analogies but doesn't directly address associative vs relational mechanisms

### Mechanism 2
- Claim: Model performance degrades when associative shortcuts are blocked.
- Mechanism: When the model cannot rely on associative retrieval (e.g., when distractors are related to B instead of C, or when only C is presented), performance drops significantly because the model lacks the relational mapping capability to solve the analogy.
- Core assumption: Relational mapping is not an emergent capability in current LLMs.
- Evidence anchors:
  - [section 7.2] "In both cases, the models performance dropped to below that of the average child, and to around the 10 year-old-level"
  - [section 7.4] "As can be seen in Table 3, our results showed that performance dropped substantially, albeit still above chance level (20%)"
  - [corpus] Moderate - corpus contains related work on LLMs solving analogies but limited evidence on performance degradation when associative processes are blocked

### Mechanism 3
- Claim: Model performance correlates with distractor salience in the same way as children.
- Mechanism: Both LLMs and children are more likely to choose distractors that are semantically similar to C.
- Core assumption: LLMs and children share similar error patterns when solving analogies.
- Evidence anchors:
  - [section 6] "Items with lower distractor salience were easier to solve than those with high distractor salience for both children and LLMs (see Figure 5; z = 5.66, p < .001), confirming H2c"
  - [section 7.5] "GPT-3 chose the same distractor as most children 39% of the time, whereas for XLM-V the most probable incorrect response was the same as most children in 33% of cases"
  - [corpus] Moderate - corpus contains work on distractor salience in human analogy solving but limited evidence on LLM error patterns

## Foundational Learning

- Concept: Associative reasoning vs. relational mapping
  - Why needed here: The paper distinguishes between these two cognitive processes in analogy solving. Understanding this distinction is crucial for interpreting why LLMs perform similarly to children.
  - Quick check question: What is the difference between solving "horse : stable :: chicken : ?" by saying "egg" (associative) versus "chicken coop" (relational)?

- Concept: Vector arithmetic for word embeddings
  - Why needed here: The paper mentions vector arithmetic as a baseline method for solving analogies. Understanding this technique is important for contextualizing LLM performance.
  - Quick check question: How does the formula C - A + B work to find D in word embeddings?

- Concept: Item Response Theory and ELO scoring
  - Why needed here: The children's data was collected using an adaptive system that uses these concepts. Understanding them is necessary for interpreting the performance comparisons.
  - Quick check question: How does the probability of solving an item correctly change when a child's ability is higher than the item's difficulty?

## Architecture Onboarding

- Component map: RobBERT -> BERTje -> XLM-V -> D-GPT-2 -> M-GPT -> GPT-3 (all tested on same 622 Dutch analogies)
- Critical path: 1) Load analogy items and children's performance data, 2) Run each LLM on all items using appropriate prompting strategy, 3) Analyze performance by relation type, semantic distance, and distractor salience, 4) Conduct controlled experiments to test associative vs. relational mechanisms
- Design tradeoffs: Choice of prompting strategy (full analogy vs. masked language model approach) significantly affects performance. Different strategies used for different model types based on architecture.
- Failure signatures: If all models perform at ceiling or floor levels, there may be data contamination or tokenization issues. If models show no correlation with children's performance patterns, the experimental design may be flawed.
- First 3 experiments:
  1. Run each LLM on the full analogy set using optimal prompting strategy for that model type
  2. Test each LLM with only the C term to measure pure associative performance
  3. Transpose items to A:C::B:? form and rerun to test if performance changes when associative distractors are blocked

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific neural mechanisms that differentiate associative versus relational reasoning in large language models?
- Basis in paper: [explicit] The paper demonstrates that LLMs tend to solve analogies using associative processes similar to children rather than relational mapping like adults, but does not investigate the underlying neural mechanisms
- Why unresolved: The study focused on behavioral outcomes and comparisons to human performance, but did not examine the internal processing mechanisms of the LLMs
- What evidence would resolve it: Neuroimaging studies of LLMs or mechanistic analysis of attention patterns and activation pathways during analogy solving tasks

### Open Question 2
- Question: How does the size and diversity of training data affect the development of relational versus associative reasoning capabilities in large language models?
- Basis in paper: [explicit] The authors hypothesize that larger models trained on more data (like GPT-3 and XLM-V) would perform better due to more domain knowledge, but did not systematically test this relationship
- Why unresolved: The study compared a limited set of models but did not conduct controlled experiments varying training data size and diversity
- What evidence would resolve it: Controlled training experiments with identical architectures but varying dataset sizes and diversity, measuring performance on relational versus associative tasks

### Open Question 3
- Question: Would implementing explicit relational reasoning mechanisms in LLMs improve their performance on analogies compared to their current associative approaches?
- Basis in paper: [inferred] The paper shows that current LLMs rely heavily on associative processes, suggesting room for improvement through architectural modifications
- Why unresolved: The study only examined existing transformer-based models without implementing alternative reasoning architectures
- What evidence would resolve it: Comparative studies of modified LLMs with explicit relational reasoning modules versus standard models on the same analogy tasks

### Open Question 4
- Question: How does the performance gap between LLMs and humans on far analogies change as model size and training data increase?
- Basis in paper: [explicit] The paper found that humans outperformed GPT-3 on far analogies, but this was based on a limited comparison with a single model
- Why unresolved: The study only tested GPT-3 on far analogies without comparing to other model sizes or examining performance trends
- What evidence would resolve it: Systematic testing of multiple model sizes on analogies with varying conceptual distances, tracking performance improvements across model scales

### Open Question 5
- Question: What specific features of the Dutch language make it more challenging for LLMs to solve analogies compared to English?
- Basis in paper: [inferred] The authors note that their Dutch analogies may have been more challenging due to less training data in Dutch compared to English
- Why unresolved: The study did not conduct direct comparisons between Dutch and English analogies or analyze language-specific features
- What evidence would resolve it: Direct comparison of LLM performance on parallel Dutch and English analogy datasets, analyzing linguistic features that correlate with difficulty

## Limitations

- Limited generalization to other languages and domains: Study focuses exclusively on Dutch verbal analogies for children aged 7-12.
- Prompt sensitivity concerns: Different prompting strategies yield different results, but specific templates are not fully specified.
- Potential data contamination: GPT-3 showing highest performance raises questions about whether these analogies appeared in training data.

## Confidence

**High confidence**: Core finding that LLM performance drops when controlling for associative processes (supported by multiple experimental conditions showing consistent patterns).

**Medium confidence**: LLMs solve analogies like younger children rather than adults (age-equivalent comparisons are clear but interpretation requires additional validation).

**Medium confidence**: Distractor salience findings (statistically significant correlation but mechanism explaining shared error patterns needs further investigation).

## Next Checks

1. **Cross-linguistic replication**: Test the same experimental protocol with analogies from other languages (e.g., English, German) to determine if the associative reasoning pattern holds across linguistic contexts.

2. **Training data audit**: Systematically check whether the specific analogy items appear in the training corpora of the evaluated models, particularly for GPT-3, to rule out memorization as a performance driver.

3. **Relational reasoning probe**: Design a controlled experiment where associative distractors are systematically replaced with unrelated options, then measure whether models show improved performance consistent with emerging relational mapping capabilities.