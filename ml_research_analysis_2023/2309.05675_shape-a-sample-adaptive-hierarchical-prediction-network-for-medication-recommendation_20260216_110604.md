---
ver: rpa2
title: 'SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation'
arxiv_id: '2309.05675'
source_url: https://arxiv.org/abs/2309.05675
tags:
- medication
- shape
- patient
- visit
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAPE addresses the challenge of medication recommendation for
  patients with complex multimorbidity conditions using electronic health records
  (EHR). It proposes a sample-adaptive hierarchical network that captures both intra-visit
  relationships among medical events and inter-visit longitudinal dependencies.
---

# SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation

## Quick Facts
- arXiv ID: 2309.05675
- Source URL: https://arxiv.org/abs/2309.05675
- Authors: 
- Reference count: 40
- Key outcome: SHAPE achieves state-of-the-art medication recommendation performance with 55.13% Jaccard, 70.17% F1, 79.06% PRAUC, and 6.77% DDI rate on MIMIC-III dataset.

## Executive Summary
SHAPE addresses medication recommendation for patients with complex multimorbidity conditions using electronic health records (EHR). It proposes a sample-adaptive hierarchical network that captures both intra-visit relationships among medical events and inter-visit longitudinal dependencies. The method uses a compact set encoder to model medical events within a visit, followed by a longitudinal encoder with recurrent attention to capture temporal patterns. An adaptive curriculum learning module dynamically assigns learning difficulty based on visit length to improve model performance, especially for short visits.

## Method Summary
SHAPE implements a three-component architecture: an intra-visit set encoder using Induced Set Attention Blocks (ISAB) to capture relationships among medical events within visits, an inter-visit longitudinal encoder with recurrent attention to model temporal dependencies across visits, and an adaptive curriculum learning module that adjusts learning rates based on visit length. The model is trained using a combined loss function of binary cross-entropy and drug-drug interaction (DDI) loss, optimized with Adam on the MIMIC-III dataset.

## Key Results
- Achieves 55.13% Jaccard, 70.17% F1, 79.06% PRAUC on medication recommendation task
- Maintains low DDI rate of 6.77% while achieving high accuracy
- Outperforms existing baselines on all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAPE's intra-visit set encoder captures variable importance of medical events within a visit, improving representation accuracy.
- Mechanism: Uses Induced Set Attention Block (ISAB) to compress medical code embeddings into a compact space, allowing the model to learn relative importance of each event rather than treating all equally.
- Core assumption: Medical events within a visit have variable importance that affects patient health status, and this variability can be learned from data.
- Evidence anchors:
  - [abstract] "A more compact encoder for intra-relationship in the intra-visit medical event is urgent"
  - [section] "We employ the code-level embedding as the input of the set encoder to learn the code-level relationship"
  - [corpus] Weak - no direct comparison of set encoder vs. alternatives in related papers
- Break condition: If medical events within visits are truly independent and equally important, the additional complexity of set encoder provides no benefit.

### Mechanism 2
- Claim: Adaptive curriculum learning improves performance on short visit records by adjusting learning difficulty based on visit length.
- Mechanism: Dynamically assigns learning rate during training based on patient visit length (Eq. 28), giving shorter records more training attention to compensate for limited historical information.
- Core assumption: Shorter visit records are inherently harder to predict because they lack sufficient historical information for accurate medication recommendation.
- Evidence anchors:
  - [abstract] "Strategies for learning accurate representations of the variable longitudinal sequences of patients are different"
  - [section] "we found that the short and new visits samples account for most of the entire dataset"
  - [corpus] Weak - curriculum learning mentioned in related papers but not specifically for handling variable sequence lengths
- Break condition: If visit length doesn't correlate with prediction difficulty, or if the learning rate adjustment doesn't meaningfully affect model parameters.

### Mechanism 3
- Claim: Combining intra-visit and inter-visit representations through hierarchical encoding captures both local and longitudinal patterns.
- Mechanism: First encodes medical events within visits (intra-visit), then processes these visit representations over time (inter-visit), allowing the model to learn both fine-grained event relationships and broader temporal patterns.
- Core assumption: Medication recommendation benefits from both detailed intra-visit relationships and longitudinal temporal dependencies.
- Evidence anchors:
  - [abstract] "we design a compact intra-visit set encoder to encode the relationship in the medical event for obtaining visit-level representation and then develop an inter-visit longitudinal encoder"
  - [section] "By performing the intra-visit set encoder and inter-visit longitudinal encoder, collaborative information latent in longitudinal historical interactions is explicitly hierarchical encoded"
  - [corpus] Moderate - related papers use hierarchical approaches but don't explicitly compare against flat representations
- Break condition: If either intra-visit or inter-visit patterns are sufficient alone, the hierarchical approach adds unnecessary complexity.

## Foundational Learning

- Concept: Set encoding and attention mechanisms
  - Why needed here: Medical events within a visit need to be processed as an unordered set rather than a sequence, and attention mechanisms can learn relative importance
  - Quick check question: How does ISAB differ from standard self-attention in handling unordered sets?

- Concept: Curriculum learning and adaptive difficulty
  - Why needed here: EHR data contains variable-length sequences where short sequences lack historical context, requiring adaptive learning strategies
  - Quick check question: What would happen to model performance if all sequences were trained at the same learning rate regardless of length?

- Concept: Multi-label classification with DDI constraints
  - Why needed here: Medication recommendation requires predicting multiple medications simultaneously while respecting drug-drug interaction constraints
  - Quick check question: How does the combined loss function (Lbce + αLddi) balance accuracy versus safety?

## Architecture Onboarding

- Component map: Code Embedding → ISAB Set Encoder → Visit Representation → Recurrent Attention Encoder → Output
- Critical path: Embedding → Set Encoder → Visit-level Representation → Longitudinal Encoder → Output
- Design tradeoffs:
  - Set encoder vs. simple averaging: More expressive but computationally heavier
  - Masked attention vs. standard attention: Prevents information leakage but requires careful implementation
  - Curriculum learning vs. fixed learning rate: Better for short visits but adds complexity
- Failure signatures:
  - Poor performance on short visits: Likely indicates curriculum learning not working correctly
  - High DDI rates: May indicate α parameter in loss function needs adjustment
  - No improvement over baselines: Could indicate set encoder implementation issues or insufficient training data
- First 3 experiments:
  1. Compare SHAPE vs. SHAPE without set encoder on short visit performance
  2. Test different α values (0.01, 0.05, 0.1) and measure DDI rate vs. accuracy tradeoff
  3. Validate curriculum learning by comparing loss curves with and without visit-length-based learning rate adjustment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SHAPE's performance change when incorporating additional patient data modalities beyond diagnoses, procedures, and medications (e.g., vital signs, laboratory test results)?
- Basis in paper: [inferred] The paper acknowledges that it only used diagnosis and procedure information as side information, ignoring other modalities like vital signs and laboratory test records.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of incorporating additional data modalities.
- What evidence would resolve it: Comparative experiments showing SHAPE's performance with and without additional data modalities, or with different combinations of modalities.

### Open Question 2
- Question: What is the optimal penalty weight α for the DDI loss function in SHAPE, and how does it vary across different healthcare settings or patient populations?
- Basis in paper: [explicit] The paper discusses the trade-off between DDI rate and accuracy, exploring different α values, but does not provide a definitive optimal value or discuss variations across settings.
- Why unresolved: The paper only explores a limited range of α values and does not consider the impact of different healthcare settings or patient populations.
- What evidence would resolve it: A comprehensive study examining SHAPE's performance with various α values across diverse healthcare settings and patient populations.

### Open Question 3
- Question: How does SHAPE's performance generalize to datasets from different healthcare systems or countries, and what factors contribute to any performance differences?
- Basis in paper: [explicit] The paper acknowledges that evaluating SHAPE on only one public dataset limits its generalizability.
- Why unresolved: The paper only evaluates SHAPE on the MIMIC-III dataset, which may not represent all healthcare systems or patient populations.
- What evidence would resolve it: Evaluations of SHAPE on multiple datasets from diverse healthcare systems or countries, along with an analysis of factors contributing to performance differences.

## Limitations
- Limited evaluation on single dataset (MIMIC-III) may not represent all healthcare systems or patient populations
- Does not incorporate additional patient data modalities like vital signs and laboratory test results
- Does not provide definitive optimal values for key hyperparameters like the DDI loss weight α

## Confidence
- Set encoder mechanism: Medium confidence - theoretical justification present but lacks direct comparisons against simpler alternatives
- Hierarchical architecture contribution: High confidence - strong ablation studies demonstrate importance of each component
- Adaptive curriculum learning: Medium confidence - shows promise but effectiveness depends heavily on proper implementation

## Next Checks
1. **Set Encoder Validation**: Compare SHAPE's performance with and without the ISAB set encoder on datasets with varying visit lengths to isolate its contribution to capturing intra-visit relationships.

2. **Curriculum Learning Impact**: Systematically vary the α parameter in the combined loss function (Lbce + αLddi) and measure the precision-safety tradeoff across different visit lengths.

3. **Implementation Verification**: Reproduce the core SHAPE architecture using a different deep learning framework to verify that the reported performance gains are not implementation-specific artifacts.