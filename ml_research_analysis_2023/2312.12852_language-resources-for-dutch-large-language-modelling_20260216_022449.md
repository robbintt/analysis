---
ver: rpa2
title: Language Resources for Dutch Large Language Modelling
arxiv_id: '2312.12852'
source_url: https://arxiv.org/abs/2312.12852
tags:
- dutch
- language
- chat
- datasets
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces several language resources to advance Dutch
  large language modelling. The authors present two fine-tuned variants of Llama 2
  13B: one trained on Dutch web-crawled text and another further refined on synthetic
  instruction and chat datasets.'
---

# Language Resources for Dutch Large Language Modelling

## Quick Facts
- arXiv ID: 2312.12852
- Source URL: https://arxiv.org/abs/2312.12852
- Reference count: 24
- Key outcome: Two Llama 2 13B variants fine-tuned for Dutch, plus translated instruction datasets and a Dutch LLM leaderboard

## Executive Summary
This paper introduces several language resources to advance Dutch large language modelling. The authors present two fine-tuned variants of Llama 2 13B: one trained on Dutch web-crawled text and another further refined on synthetic instruction and chat datasets. They release four translated instruction datasets (Dolly, Quora, Stack Overflow, Alpaca) and provide scripts for dataset translation. A Dutch generative language modelling leaderboard is established and populated with state-of-the-art models. Despite modest benchmark results compared to base models, the authors argue their models improve conversational quality in Dutch and serve as a baseline for future development. The work highlights the need for high-quality Dutch data, open-source models, and language-specific evaluation methodologies to push forward Dutch language modelling.

## Method Summary
The authors fine-tuned Llama 2 13B on Dutch-specific data using QLoRA, first on web-crawled Dutch text and then on translated instruction and chat datasets. They created Dutch translations of four popular English instruction datasets (Dolly, Quora, Stack Overflow, Alpaca) using gpt-3.5-turbo. The models were evaluated on Dutch benchmarks (ARC, HellaSwag, MMLU, TruthfulQA) and results were submitted to a newly established Dutch generative language modelling leaderboard.

## Key Results
- Two Llama 2 13B variants released: one fine-tuned on Dutch web text, another on synthetic Dutch instruction/chat datasets
- Four translated instruction datasets (Dolly, Quora, Stack Overflow, Alpaca) released for Dutch language modeling
- Dutch generative language modelling leaderboard established and populated with state-of-the-art models
- Models show modest benchmark improvements but claim better conversational quality in Dutch compared to base Llama 2 models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning Llama 2 on Dutch-specific data improves conversational quality in Dutch.
- Mechanism: The model is initially pretrained on English data, so it lacks Dutch language patterns. By finetuning on a cleaned Dutch corpus and synthetic instruction/chat datasets, the model learns Dutch syntax, vocabulary, and conversational norms, reducing the frequency of English words and improving fluency.
- Core assumption: The Dutch data used for finetuning is representative of natural Dutch usage and covers a wide range of conversational contexts.
- Evidence anchors:
  - [abstract] "We first fine-tuned Llama 2 using Dutch-specific web-crawled data and subsequently refined this model further on multiple synthetic instruction and chat datasets."
  - [section] "Due to the computational constraints we decided to finetune on QLoRA... The goal was mostly a chat model that was capable of maintaining a conversation in Dutch, whereas we found that the original Llama 2 models have difficulties with that."
- Break condition: If the Dutch data is too small, unrepresentative, or noisy, the model may not learn proper Dutch patterns and could perform worse than the base model.

### Mechanism 2
- Claim: Releasing translated instruction datasets enables further research and model development for Dutch.
- Mechanism: By providing Dutch translations of popular English instruction datasets, the authors lower the barrier for other researchers to train or fine-tune models on Dutch instruction-following tasks. This accelerates the development of Dutch-specific language models.
- Core assumption: The translated datasets are of sufficient quality and cover a diverse range of instruction types to be useful for model training.
- Evidence anchors:
  - [abstract] "These datasets as well as the model weights are made available."
  - [section] "We release two models... trained on translated instruction datasets, which we make publicly available."
- Break condition: If the translations are poor quality, contain errors, or do not cover a wide enough range of instructions, they may not be useful for training robust Dutch models.

### Mechanism 3
- Claim: Establishing a Dutch generative language modeling leaderboard provides a focal point for evaluating and comparing Dutch models.
- Mechanism: By creating a dedicated leaderboard for Dutch generative models and populating it with results from state-of-the-art models, the authors provide a clear benchmark for measuring progress in the field. This encourages competition and collaboration among researchers working on Dutch language models.
- Core assumption: The benchmarks used on the leaderboard are appropriate for evaluating generative models and provide a fair comparison across different model architectures.
- Evidence anchors:
  - [abstract] "we provide a leaderboard to keep track of the performance of (Dutch) models on a number of generation tasks"
  - [section] "Benchmarks specifically for generative, Dutch large language models do currently not exist to the best of our knowledge. Luckily, some efforts have been made to widen the reach of existing evaluation benchmarks of English to other languages."
- Break condition: If the benchmarks are not well-suited for evaluating generative models or if they are biased towards certain model architectures, the leaderboard may not provide an accurate comparison of model performance.

## Foundational Learning

- Concept: Language model finetuning
  - Why needed here: To adapt a pre-trained English language model (Llama 2) to the Dutch language and improve its conversational abilities in Dutch.
  - Quick check question: What is the difference between pretraining and finetuning a language model, and why is finetuning necessary for adapting a model to a new language?

- Concept: Instruction following and chat dataset creation
  - Why needed here: To provide the model with examples of how to understand and respond to instructions and engage in conversational interactions in Dutch.
  - Quick check question: What are the key components of an instruction-following dataset, and how do they differ from a conversational chat dataset?

- Concept: Language model evaluation and benchmarking
  - Why needed here: To assess the performance of Dutch language models and compare them to other models, both in terms of language generation quality and task-specific capabilities.
  - Quick check question: What are the main challenges in evaluating generative language models, and how do benchmarks like ARC, HellaSwag, MMLU, and TruthfulQA address these challenges?

## Architecture Onboarding

- Component map: Pretrained Llama 2 13B model -> Dutch web-crawled text -> QLoRA fine-tuning -> Synthetic instruction/chat datasets (Dolly, Quora, Stack Overflow, Alpaca) -> Further QLoRA fine-tuning -> Evaluation benchmarks (ARC, HellaSwag, MMLU, TruthfulQA) -> Dutch generative language modelling leaderboard

- Critical path: 1) Obtain and preprocess Dutch-specific data 2) Translate English instruction and chat datasets to Dutch 3) Finetune Llama 2 on Dutch data using QLoRA 4) Evaluate finetuned models on benchmarks 5) Populate and maintain Dutch generative language modeling leaderboard

- Design tradeoffs:
  - Using QLoRA for parameter-efficient finetuning reduces computational requirements but may limit the model's ability to fully adapt to Dutch compared to full finetuning.
  - Translating English datasets to Dutch provides a quick way to create instruction-following data but may introduce translation artifacts that affect model performance.
  - Using existing English benchmarks translated to Dutch allows for quick evaluation but may not fully capture the nuances of Dutch language generation.

- Failure signatures:
  - If the finetuned model performs worse than the base Llama 2 model on Dutch benchmarks, it may indicate issues with the finetuning data or process.
  - If the model generates ungrammatical or unnatural Dutch text, it may suggest problems with the quality of the finetuning data or the translation of instruction datasets.
  - If the leaderboard shows no clear improvement in Dutch model performance over time, it may indicate that the benchmarks are not well-suited for evaluating generative models or that the field lacks sufficient resources and expertise.

- First 3 experiments:
  1. Finetune Llama 2 on the mC4 Dutch corpus using QLoRA and evaluate on a small subset of translated benchmarks to check for initial improvements in Dutch language generation.
  2. Finetune the model further on the translated Dolly dataset and evaluate on a larger set of benchmarks to assess the impact of instruction-following data on model performance.
  3. Finetune the model on a combination of all translated datasets (Dolly, Quora, Stack Overflow, Alpaca) and evaluate on the full set of benchmarks to determine the optimal mix of finetuning data for Dutch language modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training data composition would be most effective for creating high-quality Dutch language models?
- Basis in paper: [inferred] The paper discusses the challenges of creating Dutch language models and mentions the importance of training data quality, but does not specify optimal data compositions.
- Why unresolved: The paper highlights the lack of transparency in datasets used by state-of-the-art models and emphasizes the need for high-quality, publicly available Dutch data, but does not provide specific recommendations for optimal data composition.
- What evidence would resolve it: Comparative studies testing different data compositions (e.g., web-crawled text, instruction datasets, conversational data) on Dutch language model performance.

### Open Question 2
- Question: How can we develop more accurate benchmarks specifically for evaluating generative Dutch language models?
- Basis in paper: [explicit] The paper discusses the limitations of current Dutch benchmarks, which are primarily designed for classification tasks, and calls for the development of generative-specific evaluation methodologies.
- Why unresolved: The paper acknowledges the need for better evaluation methods but does not propose specific solutions or methodologies for creating generative benchmarks in Dutch.
- What evidence would resolve it: Development and validation of new benchmark datasets and evaluation metrics specifically designed for generative Dutch language models.

### Open Question 3
- Question: What are the most effective parameter-efficient fine-tuning strategies for adapting large language models to Dutch?
- Basis in paper: [explicit] The paper mentions using QLoRA for fine-tuning but suggests that full fine-tuning or including more linear layers might be beneficial for adapting models to a different target language.
- Why unresolved: The paper only explores one parameter-efficient fine-tuning method and acknowledges its limitations without comparing it to other strategies or determining the most effective approach.
- What evidence would resolve it: Comparative studies testing various parameter-efficient fine-tuning methods (e.g., LoRA, prefix tuning, prompt tuning) on Dutch language model performance and efficiency.

## Limitations
- The translated instruction datasets may contain nonfactual, inaccurate, or disfluent Dutch text that could negatively impact model performance
- Modest benchmark results raise questions about the practical significance of the improvements compared to the base model
- Limited prior work in Dutch language modeling makes it difficult to validate the methodology and establish clear baselines

## Confidence
**High Confidence**: The technical approach of using QLoRA for parameter-efficient fine-tuning and the release of translated datasets as open resources are well-established practices with clear benefits for the Dutch NLP community. The creation of a dedicated leaderboard provides a valuable infrastructure for tracking progress in Dutch language modeling.

**Medium Confidence**: The claim that finetuning improves conversational quality in Dutch is supported by qualitative observations but lacks robust quantitative validation. The benchmark results showing modest improvements create uncertainty about the practical significance of the approach.

**Low Confidence**: The assertion that this work provides a strong baseline for future Dutch language modeling is difficult to evaluate given the limited prior work in this area and the absence of direct comparisons with other Dutch-specific approaches.

## Next Checks
1. **Translation Quality Assessment**: Conduct human evaluation of the translated instruction datasets to measure translation accuracy, fluency, and factual correctness, separating the impact of translation quality from the effectiveness of the fine-tuning approach.

2. **Conversational Quality Evaluation**: Design a controlled experiment comparing the finetuned models against the base Llama 2 model in Dutch-only conversations, measuring both language switching frequency and native speaker judgments of conversational quality.

3. **Benchmark Gap Analysis**: Investigate why certain benchmarks show decreased performance after fine-tuning, particularly focusing on whether this reflects genuine degradation in task performance or artifacts introduced by the translation process or the specific fine-tuning configuration.