---
ver: rpa2
title: Convolutional Deep Kernel Machines
arxiv_id: '2309.09814'
source_url: https://arxiv.org/abs/2309.09814
tags:
- kernel
- inducing
- test
- train
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Convolutional deep kernel machines (DKMs) were introduced to extend
  the DKM framework to convolutional architectures, enabling kernel methods to learn
  representations from data rather than relying on fixed deterministic kernels. A
  key challenge was developing an efficient inter-domain inducing point scheme tailored
  for the convolutional DKM setting, as standard approaches rely on access to intermediate
  features which are unavailable in DKMs.
---

# Convolutional Deep Kernel Machines

## Quick Facts
- arXiv ID: 2309.09814
- Source URL: https://arxiv.org/abs/2309.09814
- Reference count: 40
- One-line primary result: Achieved state-of-the-art kernel method accuracy: ~99% on MNIST, ~92% on CIFAR-10, and ~71% on CIFAR-100 in ~28 GPU hours

## Executive Summary
Convolutional Deep Kernel Machines (DKMs) extend the DKM framework to convolutional architectures, enabling kernel methods to learn representations from data rather than relying on fixed deterministic kernels. The key innovation is an efficient inter-domain inducing point scheme tailored for the convolutional DKM setting, where inducing points lack spatial structure but act like fully-connected features. This allows training with O(L(P_i^3 + P_i^2 P_t)) complexity instead of the prohibitive O(P_i^3 W^3 H^3) that would arise from using image-structured inducing points.

## Method Summary
The method involves training convolutional DKMs using Gram matrices instead of explicit feature representations. At each layer, the Gram matrix is computed from the previous layer's Gram matrix and propagated through a kernel function. The inter-domain inducing point scheme approximates these Gram matrices using non-spatial inducing points, with learned parameters Cℓ that map between spatial and non-spatial representations. The model is trained by optimizing these Gram matrices directly, using batch normalization variants, regularization (ν ≈ 10^-5), and different likelihood functions. Extensive experimentation with 7 normalization schemes, two likelihoods, and two top-layer types revealed optimal configurations for each dataset.

## Key Results
- Achieved state-of-the-art kernel method accuracy: ~99% on MNIST, ~92% on CIFAR-10, and ~71% on CIFAR-100
- Trained in only ~28 GPU hours—1-2 orders of magnitude faster than full NNGP/NTK kernels
- Small regularization (ν ≈ 10^-5), batch normalization, global average pooling, categorical likelihood, and larger number of inducing points yielded best performance
- Simple 'Batch' normalization scheme was the most effective all-rounder across experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convolutional DKMs learn feature representations from data by modifying the likelihood in the infinite-width limit, unlike standard infinite-width limits where representations are fixed.
- Mechanism: The DKM objective retains representation learning by sending the number of output features to infinity in addition to intermediate features. This keeps the likelihood term from vanishing in the limit, allowing Gram matrices to adapt based on data.
- Core assumption: The Bayesian representation learning limit (Nℓ = N^νℓ for all layers including outputs) is valid and leads to a well-defined posterior.
- Evidence anchors:
  - [abstract]: "DKMs were introduced to extend the DKM framework to convolutional architectures, enabling kernel methods to learn representations from data rather than relying on fixed deterministic kernels."
  - [section]: "The DKM solves this problem by taking a slightly different infinite limit, the 'Bayesian representation learning limit'... representations remain flexible and are learned from data."
  - [corpus]: Weak. Related work mentions "Flexible Infinite-Width Graph Convolutional Neural Networks" but doesn't provide direct evidence for this specific mechanism.
- Break condition: If the likelihood term still vanishes in the infinite limit despite the modified formulation.

### Mechanism 2
- Claim: The inter-domain inducing point scheme enables efficient training by avoiding the need to work with spatially structured inducing points.
- Mechanism: Inducing points lack spatial structure but act like fully-connected features. This avoids the O(Pi^3 W^3 H^3) complexity that would arise from using image-structured inducing points, while still capturing spatial information through learned transformations.
- Core assumption: The learned parameter Cℓ can effectively map non-spatial inducing inputs to spatial-like features that correlate with actual data features.
- Evidence anchors:
  - [abstract]: "This was achieved through a novel scheme where inducing points lack spatial structure but act like fully-connected features, allowing efficient training with O(L(P_i^3 + P_i^2 P_t)) complexity."
  - [section]: "Therefore we propose a new scheme where the inducing points do not have image-like spatial structure... The parameters Cℓ are learned by optimising Eq. (7)."
  - [corpus]: Missing. No direct evidence in corpus about inter-domain inducing points for DKMs.
- Break condition: If the learned Cℓ parameters fail to create meaningful correlations with actual features.

### Mechanism 3
- Claim: Normalization and regularization techniques significantly impact model performance and prevent training collapse.
- Mechanism: Batch normalization variants normalize Gram matrices by their diagonal statistics, while regularization (ν ≈ 10^-5) prevents overfitting. Without normalization, the model fails to learn.
- Core assumption: Normalizing Gram matrices using their diagonal statistics is a valid analog to batch normalization in feature space.
- Evidence anchors:
  - [section]: "Without normalisation... the model fails to learn. As with traditional batch-normalisation, allowing the model to learn a new scale parameter after normalising (rescaling) also provides advantages."
  - [section]: "We did not observe any convincing advantages to using 'Local' normalisation / rescaling schemes over 'Batch'..."
  - [corpus]: Weak. Related work mentions "Stochastic Kernel Regularisation Improves Generalisation" but doesn't provide direct evidence for normalization schemes.
- Break condition: If normalization causes instability or degrades performance.

## Foundational Learning

- Concept: Gram matrices as kernel representations
  - Why needed here: DKMs operate entirely in kernel/Gram matrix space rather than feature space, requiring understanding of how Gram matrices represent inner products between features.
  - Quick check question: Given two feature vectors f and g, how would you compute their Gram matrix entry?

- Concept: Inter-domain inducing points
  - Why needed here: Standard inducing point methods require access to intermediate features, but DKMs only propagate Gram matrices. Understanding inter-domain approaches is crucial.
  - Quick check question: What's the key difference between standard and inter-domain inducing points in terms of the space they inhabit?

- Concept: Bayesian representation learning limit
  - Why needed here: This is the mathematical foundation that enables DKMs to learn representations, distinguishing them from standard infinite-width limits.
  - Quick check question: How does sending the number of output features to infinity differ from the standard infinite-width limit?

## Architecture Onboarding

- Component map:
  Input data -> Gram matrix computation -> Layer transformations (kernel convolutions) -> Inducing point approximation -> Objective optimization -> Prediction
  Key components: Kernel function K(G), inducing point scheme with Cℓ parameters, normalization schemes, likelihood function

- Critical path:
  1. Compute initial Gram matrix G₀ = (1/N₀)XXᵀ
  2. Propagate through layers using K(G) = Γ(Ω(G))
  3. Apply inducing point scheme at each layer
  4. Compute objective with regularization and likelihood terms
  5. Optimize Gram matrices using gradients

- Design tradeoffs:
  - Spatial vs. non-spatial inducing points: Non-spatial is computationally efficient but requires learned mapping
  - Normalization scheme: Batch vs. local affects stability and performance
  - Number of inducing points: More points improve accuracy but increase computation

- Failure signatures:
  - Model fails to train: Check normalization scheme and regularization strength
  - Poor generalization: May need stronger regularization (increase ν)
  - Memory issues: Reduce number of inducing points or use smaller batch sizes

- First 3 experiments:
  1. Run with ν = 10^-3, Batch/Batch normalization, and categorical likelihood on MNIST to verify basic functionality
  2. Compare performance with and without normalization to confirm its necessity
  3. Test different numbers of inducing points (16, 64, 256) to find the sweet spot for your hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of normalization scheme impact the performance of convolutional DKMs across different datasets and architectures?
- Basis in paper: [explicit] The paper tests 7 different normalization schemes (Appendix C) and finds that "it appears the simple 'Batch' scheme is the most effective all-rounder" but notes that "there isn't a clear winner amongst the batchnorm schemes we propose."
- Why unresolved: The experimental results show high variance between runs on different random seeds, making it difficult to definitively determine which normalization scheme performs best. The paper only tests a limited number of combinations and datasets.
- What evidence would resolve it: Systematic experiments varying normalization schemes across a wider range of architectures (different depths, filter sizes), datasets (varying sizes, domains), and hyperparameters to establish which normalization scheme provides the most robust performance improvements.

### Open Question 2
- Question: What is the impact of the number of inducing points on the generalization and calibration of convolutional DKMs?
- Basis in paper: [explicit] The paper observes that "whilst we observe good test accuracy on the largest models, the test log-likelihood actually degrades past a certain size of model" and hypothesizes this is due to overfitting on the train log-likelihood causing poor calibration.
- Why unresolved: The paper only tests a limited range of inducing point configurations (up to 2048 per block) and does not provide a theoretical explanation for why increasing inducing points degrades calibration. The relationship between inducing points, model capacity, and calibration is not well understood.
- What evidence would resolve it: Experiments systematically varying the number of inducing points while monitoring both accuracy and calibration metrics (e.g., expected calibration error) across multiple datasets. Analysis of how the learned Gram matrices change with inducing point count to understand the overfitting mechanism.

### Open Question 3
- Question: Can the efficiency of the inter-domain inducing point scheme be further improved for high-resolution image datasets?
- Basis in paper: [explicit] The paper notes as a limitation that "we have not yet developed the efficient tricks (such as lower/mixed precision, and more efficient memory utilization) to enable these larger datasets" and suggests adapting ideas from kernel literature and multi-GPU setups.
- Why unresolved: The current inducing point scheme, while efficient for CIFAR-scale datasets, becomes impractical for high-resolution images due to the large spatial dimensions. The paper does not explore optimization techniques or alternative inducing point formulations that could scale better.
- What evidence would resolve it: Implementation and evaluation of the proposed efficiency improvements (lower precision arithmetic, memory optimization techniques, multi-GPU training) on high-resolution datasets like ImageNet. Comparison against state-of-the-art kernel methods and neural networks in terms of both accuracy and training time.

## Limitations

- The inter-domain inducing point scheme lacks detailed implementation guidance, particularly for the learned parameter Cℓ
- The paper doesn't establish clear rules of thumb for optimal hyperparameter combinations across different datasets and problem scales
- Efficiency improvements for high-resolution image datasets remain unexplored, limiting applicability to larger-scale vision tasks

## Confidence

**High Confidence**: The basic DKM framework and its extension to convolutional architectures is well-established theoretically. The computational complexity analysis (O(L(Pᵢ³ + Pᵢ²Pₜ))) appears sound and the empirical results showing 1-2 orders of magnitude speedup over full NNGP/NTK kernels are reproducible.

**Medium Confidence**: The specific implementation details of the inter-domain inducing point scheme, normalization techniques, and their interactions require careful implementation to reproduce. While the paper provides theoretical justification, the practical nuances may significantly impact results.

**Low Confidence**: The exact optimal hyperparameter combinations for different datasets and problem scales are not clearly established. The paper's extensive search provides some guidance but doesn't reveal clear patterns or rules of thumb.

## Next Checks

1. **Baseline Verification**: Implement and train a simple convolutional DKM on MNIST with ν=10⁻³, Batch/Batch normalization, and categorical likelihood to verify the core framework works as expected before scaling to larger datasets.

2. **Ablation Study**: Systematically remove normalization and regularization to confirm their necessity as claimed, measuring the impact on training stability and final accuracy.

3. **Scaling Analysis**: Test the inter-domain inducing point scheme with varying numbers of inducing points (16, 64, 256, 1024) on CIFAR-10 to empirically verify the claimed O(L(Pᵢ³ + Pᵢ²Pₜ)) complexity and identify the optimal tradeoff between accuracy and computation time.