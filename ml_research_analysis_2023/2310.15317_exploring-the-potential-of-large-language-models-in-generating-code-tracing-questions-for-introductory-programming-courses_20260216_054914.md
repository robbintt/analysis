---
ver: rpa2
title: Exploring the Potential of Large Language Models in Generating Code-Tracing
  Questions for Introductory Programming Courses
arxiv_id: '2310.15317'
source_url: https://arxiv.org/abs/2310.15317
tags:
- questions
- code
- question
- tracing
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the ability of large language models (LLMs)
  to generate high-quality code-tracing questions for introductory programming courses.
  Using GPT-4 with carefully engineered prompts, the researchers generated code-tracing
  questions from 176 code snippets and descriptions.
---

# Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses

## Quick Facts
- arXiv ID: 2310.15317
- Source URL: https://arxiv.org/abs/2310.15317
- Reference count: 40
- Key outcome: GPT-4 generated code-tracing questions rated nearly as highly as human-authored ones, with experts unable to reliably distinguish authorship

## Executive Summary
This study investigates the capability of large language models (LLMs) to generate high-quality code-tracing questions for introductory programming courses. Using GPT-4 with carefully engineered prompts, the researchers generated questions from 176 code snippets and descriptions, then had human experts evaluate them across five criteria. The evaluation revealed that while GPT-4-generated questions had slightly lower mean ratings, their median scores closely matched human-authored questions. Experts could only correctly identify authorship 44% of the time, suggesting the high quality of LLM-generated questions. The study also found that zero-shot generation produced more diverse questions than few-shot generation, highlighting the potential of LLMs to assist in creating educational content for programming courses.

## Method Summary
The researchers collected 176 code snippets and descriptions from the CSAwesome Java course and YouTube videos. They used GPT-4 to generate code-tracing questions through both zero-shot and few-shot approaches, then had human experts evaluate the generated questions against human-authored ones using five criteria: relevance to learning objectives, clarity, difficulty appropriateness, relevance to code, and discernibility of authorship. The evaluation included statistical analysis of ratings and authorship identification accuracy.

## Key Results
- GPT-4-generated questions had slightly lower mean ratings but closely matched human-authored questions in median ratings
- Experts could only correctly identify authorship 44% of the time
- Zero-shot generation produced more diverse questions than few-shot generation
- Approximately 56% of GPT-4-generated questions were mistakenly identified as human-generated by experts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot generation produces more diverse questions than few-shot generation
- Mechanism: The few-shot prompt introduces template bias by constraining the model to mimic provided examples, while zero-shot allows the model to draw from its broader training experience
- Core assumption: The model's training data contains diverse examples of code-tracing questions
- Evidence anchors:
  - [section] "Contrary to our expectations, the experiment showed that the few-shot method introduced a significant bias towards the example questions, thus narrowing the diversity in the generated questions"
  - [section] "Consequently, we opted for the zero-shot generation in our tests, which fostered a broader spectrum of question types"

### Mechanism 2
- Claim: GPT-4 generates questions that are semantically similar to human-authored ones while maintaining diversity
- Mechanism: The model leverages semantic understanding from its training to create questions that align with the code's intent without copying phrasing verbatim
- Core assumption: BERTScore effectively captures semantic similarity between generated and human-authored questions
- Evidence anchors:
  - [section] "A moderate BERTScore, reflecting semantic similarity, suggests that GPT4-generated questions align with the context of human-authored ones"
  - [section] "LLM-generated questions had slightly lower mean ratings, yet their median ratings closely mirrored those of human-authored questions"

### Mechanism 3
- Claim: Experts cannot reliably distinguish between human and AI-generated questions
- Mechanism: The quality of AI-generated questions reaches a threshold where human evaluators perceive them as comparable to human work
- Core assumption: Human evaluators apply consistent criteria when rating question quality
- Evidence anchors:
  - [section] "Experts could only correctly identify authorship 44% of the time, suggesting the high quality of LLM-generated questions"
  - [section] "Approximately 56% (99 out of 176) of GPT4-generated questions were mistakenly identified by experts as human-generated"

## Foundational Learning

- Concept: Code tracing as a pedagogical technique
  - Why needed here: The paper evaluates whether AI can generate questions that support this specific learning activity
  - Quick check question: What is the primary educational benefit of code tracing exercises in introductory programming courses?

- Concept: Prompt engineering and few-shot vs zero-shot learning
  - Why needed here: The paper compares these approaches to determine optimal question generation strategy
  - Quick check question: How does providing examples in a prompt potentially bias the model's output compared to providing no examples?

- Concept: Human evaluation metrics for educational content
  - Why needed here: The study uses expert ratings to assess question quality across multiple dimensions
  - Quick check question: What are the key criteria for evaluating the pedagogical quality of programming questions?

## Architecture Onboarding

- Component map: Prompt design -> Model selection (GPT-4) -> Question generation -> Human evaluation -> Quality assessment
- Critical path: Prompt design -> Model selection -> Question generation -> Human evaluation (quality assessment depends on evaluation results)
- Design tradeoffs: Few-shot provides guidance but reduces diversity; zero-shot maximizes diversity but may lack focus
- Failure signatures: Low BERTScore similarity indicates poor semantic alignment; low expert identification accuracy suggests questions are either too similar to human ones or too dissimilar
- First 3 experiments:
  1. Compare few-shot vs zero-shot generation using the same prompt to quantify diversity differences
  2. Test different prompt formulations to optimize for specific evaluation criteria (clarity, difficulty appropriateness)
  3. Evaluate question generation across different programming languages to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable are LLM-generated code-tracing questions across different programming languages beyond Java?
- Basis in paper: Inferred from the paper's focus on Java code-tracing questions and the statement that "assessing LLM versatility across different programming domains is imperative."
- Why unresolved: The study only evaluated GPT-4's performance on Java code-tracing questions, leaving open the question of how well the model would perform on other languages like Python, C++, or specialized domain-specific languages.
- What evidence would resolve it: A comprehensive study testing GPT-4 and other LLMs on code-tracing question generation across multiple programming languages, comparing results to human-authored questions and evaluating language-specific nuances.

### Open Question 2
- Question: Can LLMs generate code-tracing questions that adapt to individual student's code submissions and skill levels?
- Basis in paper: Inferred from the paper's mention that "the study did not delve into the personalization of tracing questions based on individual student submissions" and that "personalizing questions to student needs can amplify the educational relevance."
- Why unresolved: The current study used static code snippets and descriptions, but real educational applications would require dynamic generation of questions based on students' actual code and their demonstrated understanding.
- What evidence would resolve it: An experiment where LLMs generate tracing questions based on students' actual code submissions, with evaluation of how well these personalized questions match students' skill levels and help improve their learning outcomes.

### Open Question 3
- Question: What is the optimal balance between few-shot and zero-shot learning for code-tracing question generation?
- Basis in paper: Explicit from the paper's findings that "few-shot generation biased our model towards the provided examples, largely reducing question diversity" while zero-shot "yielded more diverse questions," but also noting that "the balance between the nature of the task and the examples becomes pivotal in few-shot settings."
- Why unresolved: The paper only compared few-shot and zero-shot approaches but didn't explore intermediate approaches or determine if there's an optimal number of examples that balances diversity with quality.
- What evidence would resolve it: Systematic testing of different numbers of few-shot examples (1, 2, 3, 5, 10) to identify the sweet spot where diversity is maintained while quality is optimized, potentially developing a hybrid approach that adapts based on code complexity.

## Limitations

- The study used a relatively small dataset of 176 code snippets and 10 graduate student evaluators
- The research focused exclusively on Java programming context, limiting generalizability to other languages
- While human experts could only correctly identify authorship 44% of the time, this still leaves considerable room for improvement in generating questions that are indistinguishable from human-authored ones

## Confidence

- **High confidence**: The finding that zero-shot generation produces more diverse questions than few-shot generation is well-supported by both experimental results and the logical mechanism of template bias.
- **Medium confidence**: The claim about semantic similarity between AI-generated and human-authored questions is supported by BERTScore metrics, though this metric's pedagogical relevance could be questioned.
- **Medium confidence**: The conclusion that experts cannot reliably distinguish between human and AI-generated questions is statistically valid, but the practical implications depend on what level of indistinguishability is actually needed for educational purposes.

## Next Checks

1. **Expand language diversity**: Test the question generation approach across multiple programming languages (Python, C++, JavaScript) to assess generalizability beyond Java.

2. **Long-term effectiveness study**: Conduct a longitudinal study measuring student learning outcomes when using AI-generated versus human-authored code-tracing questions to validate pedagogical effectiveness.

3. **Cross-cultural expert evaluation**: Recruit evaluators from different educational systems and cultural backgrounds to assess whether question quality perceptions vary across contexts.