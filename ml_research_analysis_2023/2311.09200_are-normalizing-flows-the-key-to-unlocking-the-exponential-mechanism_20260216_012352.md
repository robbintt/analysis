---
ver: rpa2
title: Are Normalizing Flows the Key to Unlocking the Exponential Mechanism?
arxiv_id: '2311.09200'
source_url: https://arxiv.org/abs/2311.09200
tags:
- privacy
- expm
- dpsgd
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExpM+NF, a new approach for differentially
  private machine learning (ML) that outperforms the current state-of-the-art method,
  DPSGD, in both accuracy and privacy. The key idea is to use the Exponential Mechanism
  (ExpM) with an auxiliary Normalizing Flow (NF) to sample near-optimal model parameters
  from an intractable ExpM density.
---

# Are Normalizing Flows the Key to Unlocking the Exponential Mechanism?

## Quick Facts
- arXiv ID: 2311.09200
- Source URL: https://arxiv.org/abs/2311.09200
- Reference count: 40
- One-line primary result: ExpM+NF outperforms DPSGD in both accuracy and privacy, achieving greater than 93% of non-private training accuracy for ε ≥ 5e-4 on MIMIC-III and Adult datasets.

## Executive Summary
This paper introduces ExpM+NF, a new approach for differentially private machine learning that leverages the Exponential Mechanism (ExpM) with an auxiliary Normalizing Flow (NF) to sample near-optimal model parameters. By design, ExpM+NF offers a strong privacy guarantee (ε > 0, δ = 0) without the step-wise privacy degradation inherent in DPSGD. Experiments on MIMIC-III healthcare data and the Adult dataset demonstrate that ExpM+NF achieves greater than 93% of non-private training accuracy for ε ≥ 5e-4, representing a significant improvement in privacy-accuracy tradeoff compared to the state-of-the-art.

## Method Summary
ExpM+NF combines the Exponential Mechanism with a Normalizing Flow to enable private parameter sampling from an intractable density. The method involves defining a target model with bounded sensitivity (e.g., ℓ(2) loss with bounded targets), training an auxiliary NF to approximate the ExpM density, and sampling a single set of parameters from the trained NF. This approach circumvents the privacy degradation associated with DPSGD by pre-specifying the privacy budget ε > 0 and δ = 0. The NF is trained using Reverse KL loss to transform a Gaussian base distribution into the target ExpM density, enabling efficient sampling of near-optimal parameters.

## Key Results
- ExpM+NF achieves greater than 93% of non-private training accuracy for ε ≥ 5e-4 on MIMIC-III and Adult datasets.
- ExpM+NF offers three to four orders of magnitude better privacy than DPSGD for comparable accuracy levels.
- The method demonstrates superior privacy-accuracy tradeoff compared to DPSGD, with the ability to pre-specify a strong privacy guarantee (ε > 0, δ = 0).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ExpM+NF bypasses step-wise privacy degradation by sampling once from the Exponential Mechanism instead of adding noise at every SGD step.
- Mechanism: The privacy budget is fixed upfront (ε > 0, δ = 0), so each gradient step does not incur additional privacy loss; sampling is from a tractable surrogate NF that approximates the intractable ExpM density.
- Core assumption: The NF trained via Reverse KL loss can approximate the ExpM distribution well enough that sampled parameters are near-optimal and the KL divergence remains bounded.
- Evidence anchors:
  - [abstract] states that "by design, sampling from this distribution produces, with high likelihood, near-optimal parameters that are furnished with a strong privacy guarantee—the privacy budget ε > 0 is pre-specified and δ = 0."
  - [section 2] describes training the NF to transform a Gaussian base into the ExpM target density.
- Break condition: If the NF approximation is poor, the sampled parameters will have low utility and the privacy guarantee may not hold in practice.

### Mechanism 2
- Claim: Sensitivity bounding for ℓ(2) loss with bounded targets (0,1) enables ExpM use without clipping.
- Mechanism: Because per-data-point loss is bounded by 1 (due to squared error between bounded predictions and targets), sensitivity s ≤ 1, eliminating the need for gradient clipping.
- Core assumption: All model outputs and targets are strictly bounded in [0,1], and ℓ(2) loss is used, so the sensitivity bound holds for any dataset.
- Evidence anchors:
  - [section 2.2] provides the proof: "Since ˆy(x, θ), y ∈ [0, 1] we see (ˆy(x, θ)−y)2 ∈ [0, 1]... so |(ˆy(x, θ) − y)2 − (ˆy(x′, θ) − y′)2|< 1."
  - [abstract] notes "by constraining our training to ℓ(2) loss functions... we are able to bound sensitivity for supervised learning tasks with bounded targets."
- Break condition: If targets or outputs are not strictly bounded in [0,1], sensitivity may exceed 1, invalidating the ExpM privacy guarantee.

### Mechanism 3
- Claim: ExpM+NF achieves better privacy-accuracy trade-off because δ = 0 and ε is fixed, avoiding composition theorem degradation.
- Mechanism: Since privacy is not spent per step, ε can be kept small while maintaining accuracy; δ = 0 avoids the need for approximate DP.
- Core assumption: The privacy guarantee is strong (δ = 0) and composition theorems do not apply because only one sample is released.
- Evidence anchors:
  - [abstract] states "Circumventing these drawbacks we hypothesize, and our results empirically confirm, that ExpM+NF admits stronger privacy and accuracy than is possible with DPSGD."
  - [section 1.1.2] contrasts DPSGD's reliance on composition theorems and Gaussian noise (necessitating δ > 0) with ExpM+NF's pre-specified ε > 0, δ = 0.
- Break condition: If multiple models are released (e.g., for voting schemes), composition theorems must be re-applied, weakening the privacy guarantee.

## Foundational Learning

- Concept: Differential Privacy and the Exponential Mechanism
  - Why needed here: Understanding how ε-DP differs from (ε, δ)-DP and why ExpM permits δ = 0 is essential to grasping the privacy advantage of ExpM+NF.
  - Quick check question: If a mechanism is ε-DP, what is its δ value and why does this matter for privacy guarantees?

- Concept: Normalizing Flows and density estimation
  - Why needed here: The NF is the core innovation that makes sampling from the intractable ExpM density feasible; understanding change-of-variables and Jacobian determinants is key.
  - Quick check question: In a normalizing flow, how does the determinant of the Jacobian relate to the probability density transformation?

- Concept: Sensitivity and loss function choice
  - Why needed here: Sensitivity bounding is a prerequisite for using ExpM; knowing how loss choice (ℓ(2) vs BCE) and bounded targets affect sensitivity is crucial.
  - Quick check question: If the loss function is unbounded, what happens to the sensitivity bound and the applicability of ExpM?

## Architecture Onboarding

- Component map:
  - Data preprocessing (train/dev/test split, standardization, one-hot encoding)
  - Target model (e.g., LR, GRU-D) — defined by architecture, loss, and output bounds
  - Auxiliary NF model (Sylvester or Planar flows, Gaussian base, trained via RKL loss)
  - Sampling and release pipeline (single sample from trained NF yields model parameters)

- Critical path:
  1. Define model, loss, and sensitivity bound (s ≤ 1 for ℓ(2) with bounded targets)
  2. Train NF: sample N parameters from Gaussian base, compute Jacobian, evaluate loss, minimize RKL
  3. Sample final parameters from NF and release with ε guarantee

- Design tradeoffs:
  - Planar vs. Sylvester flows: planar is cheaper (fewer parameters) but less expressive; Sylvester increases expressiveness but parameter count scales with feature dimension
  - Number of flows vs. training time: more flows → better approximation but longer NF training
  - Choice of ϕ in utility: must separate near-optimal from far-from-optimal θ; too flat → poor discrimination

- Failure signatures:
  - High variance in NF's AUC distribution → poor density approximation or sensitivity misbound
  - Median AUC drops sharply for small ε → NF not capturing ExpM distribution well
  - Sensitivity bound too loose → poor privacy-accuracy trade-off

- First 3 experiments:
  1. LR on Adult dataset with ℓ(2) loss: baseline non-private, DPSGD, and ExpM+NF for ε = 1, 0.1, 0.01
  2. LR on MIMIC-III ICU Mortality: compare ExpM+NF vs DPSGD with PRV vs RDP accounting for ε = 1, 0.1
  3. GRU-D on MIMIC-III Length of Stay: test if deep models benefit similarly; vary hidden size, test ExpM+NF vs DPSGD for ε = 1, 0.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of ϕ affect the sensitivity bound and the overall privacy-accuracy tradeoff in ExpM+NF?
- Basis in paper: [explicit] The paper discusses the importance of choosing ϕ carefully, stating that "While any strictly decreasing ϕ theoretically ensures identical optimization, in practice ϕ must separate the near-optimal θ from those that are far from optimal." It also mentions that the choice of ϕ affects the sensitivity bound.
- Why unresolved: The paper does not provide a systematic study or theoretical analysis of how different choices of ϕ impact the sensitivity bound and the resulting privacy-accuracy tradeoff.
- What evidence would resolve it: Empirical results comparing the performance of ExpM+NF with different ϕ functions, along with a theoretical analysis of how ϕ affects the sensitivity bound and the privacy-accuracy tradeoff.

### Open Question 2
- Question: What is the impact of using different types of Normalizing Flows (NFs) on the accuracy and privacy of ExpM+NF?
- Basis in paper: [explicit] The paper mentions using Planar and Sylvester Flows, and notes that "The effect of using Sylvester flows (over planar flows) is increased expressiveness, but it does come at the cost of many more parameters in the NF model."
- Why unresolved: The paper does not provide a comprehensive comparison of different NF types in terms of their impact on the accuracy and privacy of ExpM+NF.
- What evidence would resolve it: Empirical results comparing the performance of ExpM+NF with different NF types, such as Planar Flows, Sylvester Flows, and potentially other types of NFs.

### Open Question 3
- Question: How does the privacy budget allocated for hyperparameter tuning affect the overall privacy-accuracy tradeoff of ExpM+NF?
- Basis in paper: [explicit] The paper mentions that hyperparameter tuning incurs a privacy cost and suggests that ExpM+NF's ability to achieve better privacy-accuracy tradeoffs provides the budget for this tuning.
- Why unresolved: The paper does not provide a detailed analysis of how the privacy budget for hyperparameter tuning should be allocated and its impact on the overall privacy-accuracy tradeoff.
- What evidence would resolve it: Empirical results investigating the impact of different privacy budgets for hyperparameter tuning on the final accuracy and privacy of ExpM+NF, along with theoretical analysis of the optimal allocation strategy.

### Open Question 4
- Question: How does the choice of the base distribution for the Normalizing Flow affect the accuracy and privacy of ExpM+NF?
- Basis in paper: [explicit] The paper mentions using a Gaussian base distribution and suggests that the variance of the base distribution can influence the results.
- Why unresolved: The paper does not explore different base distributions or provide a theoretical analysis of their impact on the accuracy and privacy of ExpM+NF.
- What evidence would resolve it: Empirical results comparing the performance of ExpM+NF with different base distributions, such as Gaussian, uniform, or more complex distributions, along with theoretical analysis of how the choice of base distribution affects the accuracy and privacy.

## Limitations
- The method is currently limited to loss functions with bounded sensitivity, such as ℓ(2) loss with bounded targets.
- The performance of ExpM+NF may be sensitive to hyperparameter choices and random initialization.
- The method relies on the assumption that the Normalizing Flow can accurately approximate the intractable Exponential Mechanism density.

## Confidence
- **High**: The theoretical foundation of the Exponential Mechanism and its privacy guarantees (ε > 0, δ = 0) are well-established and clearly explained in the paper.
- **Medium**: The empirical results demonstrate the effectiveness of ExpM+NF compared to DPSGD and non-private baselines, but the experiments are limited to a few datasets and model architectures.
- **Low**: The paper does not provide a thorough analysis of the computational complexity and scalability of ExpM+NF, particularly for larger datasets or more complex model architectures.

## Next Checks
1. Extend the experiments to include additional datasets and model architectures, such as deep neural networks with unbounded outputs, to assess the generality of the approach.
2. Conduct a sensitivity analysis of ExpM+NF to hyperparameter choices, such as the number of Normalizing Flow layers, base distribution variance, and learning rate, to identify optimal settings and potential failure modes.
3. Compare the computational complexity and training time of ExpM+NF to DPSGD for various model sizes and dataset scales, to evaluate the practical feasibility of the approach for real-world applications.