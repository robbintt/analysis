---
ver: rpa2
title: '(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains:
  Transparency Is Necessary but Insufficient for Comprehensibility'
arxiv_id: '2306.02312'
source_url: https://arxiv.org/abs/2306.02312
tags:
- interpretability
- ante-hoc
- which
- insights
- transparent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper unpacks the overloaded concept of ante-hoc interpretability,\
  \ distinguishing it from post-hoc explainability and showing how inherent transparency\
  \ of a model may not suffice for explainability. It introduces two sets of desiderata\u2014\
  model-based (transparency of components, information processing strategy) and explainer-based\
  \ (modularity, provenance, lineage, reasoning mechanism)\u2014to navigate distinct\
  \ realisations of ante-hoc interpretability."
---

# (Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility

## Quick Facts
- arXiv ID: 2306.02312
- Source URL: https://arxiv.org/abs/2306.02312
- Authors: 
- Reference count: 7
- Primary result: Ante-hoc interpretability is necessary but insufficient for explainability in high-stakes domains like healthcare.

## Executive Summary
This paper unpacks the concept of ante-hoc interpretability, distinguishing it from post-hoc explainability and demonstrating that model transparency alone may not suffice for explainee understanding. The authors introduce two sets of desiderata—model-based (transparency of components, information processing strategy) and explainer-based (modularity, provenance, lineage, reasoning mechanism)—to navigate distinct realisations of ante-hoc interpretability. They argue that while ante-hoc interpretability presents stronger requirements often necessary for high-stakes domains, both conceptualisations depend on an audience whose expertise remains unspecified or presupposed. The paper concludes that ante-hoc interpretability is necessary but insufficient for explainability in high-stakes domains, and calls for a principled framework to ensure safe adoption.

## Method Summary
The authors conduct a theoretical analysis of ante-hoc interpretability through conceptual argumentation and categorization of desiderata. They distinguish between model-based requirements (transparency of components and processing strategy) and explainer-based requirements (modularity, provenance, lineage, reasoning mechanism). The method involves developing illustrative examples to demonstrate how these concepts function and interact, particularly in the context of high-stakes domains like healthcare. The analysis builds on established literature in XAI while proposing a refined framework for understanding when and how ante-hoc interpretability can support explainability.

## Key Results
- Ante-hoc interpretability differs fundamentally from post-hoc explainability in its reliance on model transparency
- Transparency of model components does not guarantee explainability without proper reasoning mechanisms
- Modularity enables comprehension of complex models by presenting cognitively digestible information chunks
- Provenance and lineage of explanatory insights determine their truthfulness and alignment with model behavior
- Ante-hoc interpretability is necessary but insufficient for explainability in high-stakes domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ante-hoc interpretability relies on transparent model components but may fail without proper explainability reasoning.
- Mechanism: The model offers transparency (clear structure, parameters, processing) but human/algorithmic reasoning is required to convert that into usable explanations.
- Core assumption: Transparency of components guarantees intelligibility only when the explainee has the requisite technical and domain knowledge.
- Evidence anchors:
  - [abstract] The authors distinguish ante-hoc interpretability from post-hoc explainability and note that "inherent transparency of a model may not suffice to engender understanding in explainees."
  - [section 2] "While the functional definition of ante-hoc interpretability presents stronger, and often necessary for high-stakes domains, requirements, both conceptualisations depend on an audience, who either remains unspecified or is presupposed to have relevant technical and domain knowledge."
  - [corpus] No direct evidence from corpus; weak signal for this mechanism.
- Break condition: If the explainee lacks the background to reason about transparent components, understanding fails regardless of transparency.

### Mechanism 2
- Claim: Modularity of interpretability allows complex models to remain comprehensible by breaking them into smaller, digestible parts.
- Mechanism: Even if a model is large, extracting and presenting only relevant modules or sub-structures yields interpretable explanations without overwhelming the explainee.
- Core assumption: Human cognitive load is manageable when explanations are modular and contextually focused.
- Evidence anchors:
  - [section 3.2] "Models that process data hierarchically (e.g., classification and regression trees [Breiman et al., 1984]) or whose predictions depend on factors that can be handled independently (e.g., generalised additive models [Hastie & Tibshirani, 1986]) can with certain caveats remain interpretable regardless of their complexity."
  - [section 3.2] "Presenting parsimonious and cognitively digestible chunks of information may require reducing the scope of an explanation and abstracting away certain aspects of the model’s operational context."
  - [corpus] Weak evidence; corpus does not discuss modularity in depth.
- Break condition: If modularization is not possible due to the model's structure or if relevant context is lost, the explanation becomes misleading or incomplete.

### Mechanism 3
- Claim: Explainability depends on the provenance and lineage of explanatory insights, not just the method of generation.
- Mechanism: Endogenous insights (extracted directly from the model) maintain truthfulness even if generated post-hoc, while exogenous insights may lose fidelity.
- Core assumption: Truthfulness of explanations is preserved when the source of information aligns with the model's internal structure.
- Evidence anchors:
  - [section 3.2] "Provenance of an explanation determines the source of information it relies on. Endogenous insights are extracted directly from a (transparent) model and operate on the same concepts (features), thus are guaranteed to be truthful even if they are retrieved post-hoc by an independent algorithm."
  - [section 3.2] "Explanatory information lineage, which is similar to translucency [Robnik-ˇSikonja & Bohanec, 2018], specifies the manner in which a (transparent or explanatory) model contributes information to an explanatory insight."
  - [corpus] No evidence from corpus; weak signal for this mechanism.
- Break condition: If provenance is exogenous (e.g., from a surrogate model), the explanation may not faithfully represent the original model's behavior.

## Foundational Learning

- Concept: Transparency vs. Interpretability
  - Why needed here: The paper distinguishes between models that are transparent (structure visible) and those that are interpretable (understandable to explainees), a core distinction for applying the framework.
  - Quick check question: Does transparency of a model automatically guarantee that it will be interpretable to a non-expert explainee?

- Concept: Modular Explainability
  - Why needed here: The framework allows complex models to remain interpretable by breaking them into manageable, focused parts, which is crucial for high-stakes domains.
  - Quick check question: Can a large decision tree be made interpretable by focusing only on the subtree relevant to a specific prediction?

- Concept: Provenance and Lineage of Explanations
  - Why needed here: Understanding whether explanations come from the model itself (endogenous) or from an external proxy (exogenous) is essential for evaluating their truthfulness.
  - Quick check question: If an explanation is generated post-hoc but relies only on the model's own structure, does it still count as ante-hoc interpretability?

## Architecture Onboarding

- Component map: Model Components -> Explainability Components -> Audience Layer
- Critical path:
  1. Ensure model components are transparent and human-comprehensible.
  2. Identify the explainee audience and their background.
  3. Apply reasoning (human or algorithmic) to extract modular insights.
  4. Track provenance and lineage of explanations to maintain truthfulness.
- Design tradeoffs:
  - Transparency vs. Performance: More transparent models may sacrifice predictive accuracy.
  - Modularity vs. Completeness: Modular explanations may omit broader context.
  - Endogenous vs. Exogenous: Endogenous explanations are more truthful but may be harder to generate.
- Failure signatures:
  - Explainees cannot understand the model despite transparency.
  - Explanations are modular but lose essential context.
  - Explanations are generated post-hoc but use exogenous information, leading to misalignment with the model's behavior.
- First 3 experiments:
  1. Compare interpretability of a large decision tree vs. a shallow tree with the same performance, focusing on explainee comprehension.
  2. Generate counterfactual explanations from a decision tree using both algorithmic and human reasoning, then evaluate truthfulness and clarity.
  3. Test endogenous vs. exogenous explanations for a linear model on interpretable features, measuring explainee trust and understanding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the size/complexity of a model and its interpretability for different audiences?
- Basis in paper: [explicit] The paper discusses how transparent models may become overwhelming and lose interpretability as their size increases, but does not quantify the threshold at which this occurs for different explainee groups.
- Why unresolved: The authors acknowledge this as a key challenge but do not provide empirical data or a principled framework for determining interpretability thresholds based on model size and audience expertise.
- What evidence would resolve it: Empirical studies measuring interpretability of models of varying sizes/complexity across diverse explainee groups, with clear metrics for both model complexity and audience understanding.

### Open Question 2
- Question: How can we systematically design modular explanations that preserve truthfulness while abstracting away complex model details?
- Basis in paper: [explicit] The paper introduces the concept of modular interpretability and discusses how abstracting away parts of a model can preserve truthfulness, but does not provide a systematic framework for doing so.
- Why unresolved: While the authors identify modularity as important, they do not specify how to determine which model components can be safely abstracted without compromising the truthfulness of explanations.
- What evidence would resolve it: A principled framework and empirical validation showing how different levels of abstraction affect explanation truthfulness across various model types and explainee groups.

### Open Question 3
- Question: What is the optimal balance between human and algorithmic reasoning in the explanation process for different high-stakes domains?
- Basis in paper: [explicit] The paper discusses human, algorithmic, and shared reasoning as different approaches to extracting insights from transparent models, but does not specify how to determine the optimal balance for different contexts.
- Why unresolved: The authors recognize the importance of reasoning mechanisms but do not provide guidance on how to determine when human vs. algorithmic reasoning is more appropriate, or how to combine them effectively.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different reasoning approaches (human-only, algorithmic-only, hybrid) in various high-stakes domains, with clear metrics for explanation quality and explainee understanding.

## Limitations
- The framework relies heavily on conceptual distinctions without empirical validation in real-world high-stakes domains.
- The claim that ante-hoc interpretability is "necessary but insufficient" remains largely philosophical without quantitative evidence.
- The framework's applicability across different types of high-stakes domains (medical diagnosis, criminal justice, financial risk assessment) is not explored.

## Confidence
- **High Confidence**: The distinction between ante-hoc interpretability and post-hoc explainability is well-founded and supported by established literature. The argument that transparency alone does not guarantee explainability is robust and widely accepted in the XAI community.
- **Medium Confidence**: The proposed desiderata for ante-hoc interpretability are logically coherent but lack empirical validation. The claim that modularity enables comprehension in complex models is theoretically sound but requires experimental verification.
- **Low Confidence**: The assertion that ante-hoc interpretability is "necessary but insufficient" for high-stakes domains, while intuitively compelling, lacks concrete evidence demonstrating specific failure modes or scenarios where this insufficiency manifests.

## Next Checks
1. **Empirical Validation**: Design a user study comparing explainee comprehension of ante-hoc interpretable models versus post-hoc explanations in a healthcare diagnostic scenario, measuring both accuracy and time-to-understanding.
2. **Failure Mode Analysis**: Identify and document specific cases where ante-hoc interpretability failed to produce adequate explanations in high-stakes domains, contrasting these with successful implementations.
3. **Cross-Domain Applicability Test**: Apply the proposed framework to three distinct high-stakes domains (healthcare, criminal justice, financial risk) and assess whether the same desiderata hold or require domain-specific modifications.