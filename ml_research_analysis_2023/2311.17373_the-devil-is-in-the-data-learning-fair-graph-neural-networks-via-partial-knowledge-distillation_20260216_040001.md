---
ver: rpa2
title: 'The Devil is in the Data: Learning Fair Graph Neural Networks via Partial
  Knowledge Distillation'
arxiv_id: '2311.17373'
source_url: https://arxiv.org/abs/2311.17373
tags:
- fairness
- fairgkd
- data
- node
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies learning fair graph neural networks without
  demographic information, motivated by the observation that training GNNs on partial
  data (only node attributes or only graph topology) improves fairness at the cost
  of utility. The authors propose FairGKD, which constructs a synthetic teacher from
  two partial-data-trained fairness experts (one on attributes, one on topology) and
  distills fair yet informative knowledge to guide the student GNN.
---

# The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation

## Quick Facts
- arXiv ID: 2311.17373
- Source URL: https://arxiv.org/abs/2311.17373
- Reference count: 40
- Key outcome: FairGKD improves fairness (lower 풊DP and 풊EO) while maintaining or improving accuracy compared to strong baselines, even without sensitive attributes

## Executive Summary
This paper addresses the challenge of learning fair graph neural networks without access to sensitive demographic information. The authors propose FairGKD, a knowledge distillation framework that trains fairness experts on partial data (node attributes or topology only) to mitigate bias inheritance. A synthetic teacher is constructed from these experts, and fair yet informative knowledge is distilled to guide the student GNN. The method includes an adaptive loss-balancing scheme to trade off fairness and utility. Experiments on three real-world datasets show significant fairness improvements with minimal utility loss compared to state-of-the-art baselines.

## Method Summary
FairGKD trains two fairness experts on partial data - one on node attributes only (MLP) and one on graph topology only (GNN) - to reduce bias inheritance. A projector combines their outputs to create a synthetic teacher that distills fair knowledge to the student GNN. The student is trained using knowledge distillation with a combined loss of BCE (utility) and contrastive loss (fairness), balanced via adaptive coefficients that depend on relative loss decreases during training. The approach does not require sensitive attributes during training.

## Key Results
- FairGKD significantly improves fairness (lower 풊DP and 풊EO) on Recidivism, Pokec-z, and Pokec-n datasets
- The method maintains or improves accuracy compared to strong baselines including GCN, GIN, and GRL methods
- FairGKD achieves state-of-the-art fairness performance with minimal utility loss when trained without sensitive attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mitigating higher-level biases (in node attributes or graph topology) improves fairness without needing sensitive attributes
- Mechanism: Training on partial data (only node attributes or only topology) reduces bias inheritance compared to full data training
- Core assumption: Graph neural networks inherit biases present in their training data through message passing
- Evidence anchors:
  - [abstract] "Our work is motivated by the empirical observation that training GNNs on partial data (i.e., only node attributes or topology data) can improve their fairness"
  - [section 4] "Experimental results indicate that models trained on partial data present superior performance on fairness but sacrifice utility"
- Break condition: If the source of bias is not in the data structure itself but in external factors not captured by node attributes or topology

### Mechanism 2
- Claim: Knowledge distillation from a fair synthetic teacher preserves utility while improving fairness
- Mechanism: A synthetic teacher is constructed from two fairness experts (trained on partial data) and a projector that combines their representations to create fair yet informative knowledge for the student
- Core assumption: Fair representations from partial data can be combined to produce informative representations for downstream tasks
- Evidence anchors:
  - [abstract] "we employ a set of fairness experts (i.e., GNNs trained on different partial data) to construct the synthetic teacher, which distills fairer and informative knowledge to guide the learning of the N student"
  - [section 5.2] "To address this issue, FairGKD employs a projector to merge these two representations to generate fair and informative representations"
- Break condition: If the projector fails to effectively combine representations or if the knowledge becomes too abstract to be useful for the student

### Mechanism 3
- Claim: Adaptive loss balancing achieves trade-off between fairness and utility
- Mechanism: Dynamic coefficients for hard loss (utility) and soft loss (fairness) are calculated based on their relative decrease rates during training
- Core assumption: The loss term that decreases more slowly needs more weight to prevent it from being overshadowed
- Evidence anchors:
  - [section 5.4] "The core idea behind this algorithm is to amplify the impact of the disadvantage term (i.e., loss term with slower reduction) on the overall loss function"
  - [section 5.4] "Specifically, let 洧(洧노) denote the loss at epoch 洧노, Eq. (12) can be formulated as: 洧(洧노) = 洧띺 (洧노)洧洧녫 (洧노) + 洧띻 (洧노)洧洧녲洧녬 (洧노)"
- Break condition: If both loss terms decrease at similar rates or if the adaptive mechanism causes oscillations in training

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: FairGKD builds upon GNN architectures and needs to understand how biases propagate through message passing
  - Quick check question: How does the aggregation function in GNNs potentially amplify biases present in the training data?

- Concept: Knowledge distillation in neural networks
  - Why needed here: FairGKD uses knowledge distillation to transfer fair and informative knowledge from a synthetic teacher to a student model
  - Quick check question: What is the difference between hard loss and soft loss in the context of knowledge distillation?

- Concept: Fairness metrics in machine learning (Demographic Parity and Equal Opportunity)
  - Why needed here: FairGKD is evaluated using these metrics to measure the reduction in bias across demographic groups
  - Quick check question: How do Demographic Parity and Equal Opportunity differ in their approach to measuring fairness?

## Architecture Onboarding

- Component map:
  - Input: Graph data (nodes, edges, attributes) without sensitive attributes
  - Fairness MLP Expert: Trained on node attributes only
  - Fairness GNN Expert: Trained on graph topology only
  - Projector: Combines outputs of fairness experts
  - Synthetic Teacher: Ensemble of fairness experts and projector
  - GNN Student: Learns from synthetic teacher through knowledge distillation
  - Output: Fair node classifications

- Critical path:
  1. Train fairness experts on partial data
  2. Train projector to combine fairness expert outputs
  3. Train GNN student using knowledge distillation from synthetic teacher
  4. Apply adaptive optimization to balance fairness and utility

- Design tradeoffs:
  - Partial data training improves fairness but sacrifices utility, requiring knowledge distillation to recover utility
  - Using a synthetic teacher (ensemble) provides more robust knowledge than a single teacher
  - Adaptive optimization adds complexity but enables better balance between competing objectives

- Failure signatures:
  - If fairness doesn't improve: Check if partial data training is effectively reducing bias inheritance
  - If utility drops significantly: Verify projector is successfully combining representations and that knowledge distillation is working
  - If training becomes unstable: Examine adaptive optimization parameters and loss term behaviors

- First 3 experiments:
  1. Verify partial data training improves fairness on a simple dataset by comparing full data vs. partial data training
  2. Test the projector component in isolation by checking if it can combine fairness expert outputs effectively
  3. Evaluate the complete FairGKD pipeline on a benchmark dataset to confirm both fairness improvement and utility maintenance

## Open Questions the Paper Calls Out

- Question: How does FairGKD perform on datasets with multi-class sensitive attributes (e.g., more than two demographic groups)?
- Basis in paper: [inferred] The paper evaluates FairGKD only on binary sensitive attributes (race, region, gender, age with binary split). The authors mention FairGKD is not tailored for a specific sensitive attribute but do not demonstrate performance on multi-class attributes.
- Why unresolved: The experimental section only includes binary sensitive attributes, and the methodology section does not discuss how FairGKD would extend to multi-class cases.
- What evidence would resolve it: Experiments on datasets with multi-class sensitive attributes (e.g., three or more racial/ethnic groups, multiple regions, or age ranges) showing fairness metrics (풊DP, 풊EO) and utility metrics (accuracy, F1) compared to baselines.

- Question: What is the impact of the synthetic teacher's design (number of experts, choice of architectures) on FairGKD's performance?
- Basis in paper: [explicit] The authors state that FairGKD uses two fairness experts (MLP and GNN) and a projector, but they do not explore variations in the number of experts or alternative architectures for the experts or projector.
- Why unresolved: The ablation study removes components but does not test different configurations or numbers of experts, nor does it compare different projector architectures.
- What evidence would resolve it: Controlled experiments varying the number of experts (e.g., two vs. three), the architectures of experts (e.g., different GNN types), and the projector (e.g., different MLP depths) to measure impact on fairness and utility.

- Question: How does FairGKD scale with graph size in terms of memory and training time?
- Basis in paper: [inferred] The complexity analysis states space complexity is O(|V|) due to pairwise similarity computation, and time complexity is O(|V| + |E|), but no empirical scaling study is presented.
- Why unresolved: The paper does not include experiments on graphs of varying sizes or provide runtime/memory measurements across different graph scales.
- What evidence would resolve it: Empirical results showing training time and memory usage on graphs of increasing size (e.g., from hundreds to millions of nodes), along with comparisons to baseline methods.

## Limitations

- The paper lacks theoretical grounding for the adaptive loss balancing mechanism
- The projector component's effectiveness in combining fairness expert outputs is demonstrated empirically but not rigorously analyzed
- The method's performance on datasets with more complex graph structures and larger attribute spaces remains untested

## Confidence

Medium - The paper presents a novel approach with promising experimental results, but several limitations and uncertainties remain. The adaptive loss balancing mechanism lacks theoretical grounding and extensive ablation studies. The projector component's effectiveness in combining fairness expert outputs is demonstrated empirically but not rigorously analyzed. The paper's claims about mitigating "higher-level biases" are based on empirical observations rather than formal bias analysis.

## Next Checks

1. Conduct a comprehensive ablation study to isolate the contribution of each component (partial data training, projector, adaptive balancing) to the overall performance
2. Test FairGKD on datasets with more complex graph structures and larger attribute spaces to evaluate scalability and robustness
3. Perform a formal bias analysis to quantify how the method mitigates different types of biases in the data beyond empirical fairness metrics