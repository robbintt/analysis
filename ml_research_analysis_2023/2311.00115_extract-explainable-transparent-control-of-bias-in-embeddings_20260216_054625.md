---
ver: rpa2
title: 'EXTRACT: Explainable Transparent Control of Bias in Embeddings'
arxiv_id: '2311.00115'
source_url: https://arxiv.org/abs/2311.00115
tags:
- embeddings
- embedding
- information
- gender
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EXTRACT addresses bias in knowledge graph embeddings by detecting
  and removing unintended private information like gender, age, and occupation from
  user representations. The method uses three detection techniques: logistic classifiers
  (73% gender prediction accuracy), canonical correlation analysis (statistically
  significant correlations), and linear decomposition (user embeddings reconstructible
  as attribute sums).'
---

# EXTRACT: Explainable Transparent Control of Bias in Embeddings

## Quick Facts
- **arXiv ID**: 2311.00115
- **Source URL**: https://arxiv.org/abs/2311.00115
- **Reference count**: 30
- **Primary result**: Successfully removes unintended private information (gender, age, occupation) from knowledge graph embeddings while maintaining prediction performance (RMSE 0.88‚Üí0.93)

## Executive Summary
EXTRACT addresses bias in knowledge graph embeddings by detecting and removing unintended private information like gender, age, and occupation from user representations. The method employs three detection techniques: logistic classifiers showing 73% gender prediction accuracy, canonical correlation analysis revealing statistically significant correlations, and linear decomposition demonstrating reconstructible user embeddings from attribute sums. For bias removal, EXTRACT uses linear projection methods and first-moment loss during training, achieving gender-neutral embeddings while maintaining prediction performance. The approach successfully controls bias while preserving embedding utility, demonstrating a clear trade-off between fairness and prediction accuracy.

## Method Summary
EXTRACT operates through a two-phase approach: detection and removal. First, it detects bias using logistic classifiers to predict sensitive attributes from embeddings, canonical correlation analysis to measure correlations between embeddings and attributes, and linear decomposition to reconstruct embeddings as attribute sums. For removal, it applies linear projection methods to orthogonalize embeddings against protected attributes and first-moment loss during training to align distributions between protected groups. The method is evaluated on MovieLens-1M and KG20C citation datasets, measuring both prediction performance (RMSE, Hits@K, MRR, MR) and bias removal effectiveness through the same detection metrics.

## Key Results
- Logistic classifiers achieve 73% accuracy in detecting gender bias in embeddings
- Linear projection methods remove bias while maintaining prediction performance (RMSE changes from 0.88 to 0.93)
- First-moment loss effectively removes distributional disparities between protected groups during training
- Clear trade-off observed between prediction accuracy and bias removal strength

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear projection methods can remove sensitive attribute information from embeddings while maintaining predictive performance
- Mechanism: The method identifies a hyperplane that best separates embeddings by attribute values, then projects embeddings onto the orthogonal hyperplane to remove attribute information
- Core assumption: The sensitive attribute information can be linearly separated in the embedding space
- Evidence anchors:
  - [abstract] "linear projection methods...achieving gender-neutral embeddings while maintaining prediction performance (RMSE from 0.88 to 0.93)"
  - [section] "remove-LP: Linear Transformation...we can project ùë¢ onto the hyperplane orthogonal to bÀÜ via u‚ä• = u ‚àí (u ¬∑ bÀÜ·µÄ) ¬∑ bÀÜ"
  - [corpus] "KGIF: Optimizing Relation-Aware Recommendations with Knowledge Graph Information Fusion" (weak correlation, no direct evidence)

### Mechanism 2
- Claim: First-moment loss can remove distributional disparities between protected groups during training
- Mechanism: Penalizes the difference between first moments (means) of embedding distributions for different protected groups by adding a regularization term to the loss function
- Core assumption: The distributional difference between protected groups can be approximated by comparing their first moments
- Evidence anchors:
  - [abstract] "first-moment loss during training, achieving gender-neutral embeddings while maintaining prediction performance"
  - [section] "we aim to remove the difference in the first moments of the two classes by centering the mean coordinates of the two classes"
  - [corpus] "Iteratively Learning Representations for Unseen Entities with Inter-Rule Correlations" (weak correlation, no direct evidence)

### Mechanism 3
- Claim: Logistic classifier detection can identify whether embeddings contain sensitive attribute information
- Mechanism: Trains a classifier to predict sensitive attributes from embeddings, with high accuracy indicating presence of bias
- Core assumption: If embeddings contain sensitive attribute information, a classifier can learn to predict those attributes
- Evidence anchors:
  - [abstract] "logistic classifiers (73% gender prediction accuracy)"
  - [section] "We train a logistic classifier to predict the value of ùëéùëñ for each vertex ùë¢ ‚àà ùëà"
  - [corpus] "Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful?" (weak correlation, no direct evidence)

## Foundational Learning

- Concept: Canonical Correlation Analysis (CCA)
  - Why needed here: To measure the correlation between user embeddings and attribute vectors to detect bias
  - Quick check question: What does a high CCA correlation coefficient between embeddings and attribute vectors indicate?

- Concept: Knowledge Graph Embedding
  - Why needed here: The entire method operates on knowledge graph embeddings and their properties
  - Quick check question: What is the difference between entity prediction and relation prediction in knowledge graph embeddings?

- Concept: Logistic Regression
  - Why needed here: Used for detecting bias by predicting sensitive attributes from embeddings
  - Quick check question: What would a 73% accuracy on a balanced binary classification problem indicate?

## Architecture Onboarding

- Component map: Data layer (MovieLens-1M, KG20C) -> Detection module (detect-LC, detect-CCA, detect-LD) -> Removal module (remove-LP, remove-FM, remove-LP-multi) -> Evaluation layer (RMSE, Hits@K, MRR, MR)
- Critical path: Detection ‚Üí Bias quantification ‚Üí Removal method selection ‚Üí Post-removal evaluation
- Design tradeoffs:
  - Linear vs. non-linear bias removal methods (simplicity vs. effectiveness)
  - Detection accuracy vs. computational cost
  - Bias removal strength vs. prediction performance preservation
- Failure signatures:
  - Detection methods showing no correlation when bias exists (false negatives)
  - Removal methods showing no change in bias metrics after application
  - Prediction performance dropping significantly after bias removal
- First 3 experiments:
  1. Apply detect-LC on MovieLens embeddings to verify gender prediction accuracy of 73%
  2. Apply remove-LP on gender attribute to observe RMSE change from 0.88 to 0.93
  3. Apply detect-CCA to verify correlation between user embeddings and attribute vectors exceeds random baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of private attributes (e.g., gender vs. occupation vs. age) affect the effectiveness of bias removal techniques in knowledge graph embeddings?
- Basis in paper: [explicit] The paper shows varying accuracy in detecting and removing different attributes (gender 73%, age 68-72%, occupation 59%)
- Why unresolved: The paper demonstrates different detection accuracies but doesn't systematically compare how removal techniques perform across different attribute types
- What evidence would resolve it: Controlled experiments comparing removal technique effectiveness across multiple attribute types with consistent evaluation metrics

### Open Question 2
- Question: What is the optimal trade-off between prediction performance and bias removal strength for different application domains?
- Basis in paper: [explicit] "A trade-off between these two goals is observed" but specific optimal points aren't determined
- Why unresolved: The paper shows the trade-off exists but doesn't provide domain-specific guidance on optimal parameter settings
- What evidence would resolve it: Empirical studies mapping different application domains to optimal ùúé and ùúÉ parameter values based on domain-specific performance requirements

### Open Question 3
- Question: How does the size and quality of training data affect the detection and removal of bias in knowledge graph embeddings?
- Basis in paper: [inferred] The experiments use specific dataset sizes (MovieLens 1M, KG20C) but don't explore how results vary with different data volumes
- Why unresolved: The paper demonstrates effectiveness on existing datasets but doesn't investigate data-dependent performance characteristics
- What evidence would resolve it: Systematic experiments varying training data size and quality while measuring detection/removal effectiveness across multiple datasets

## Limitations

- The method assumes linear separability of sensitive attributes in embedding space, which may not hold for complex, non-linear relationships
- First-moment loss only addresses mean differences between groups and may miss higher-order distributional biases
- The evaluation focuses on prediction performance metrics (RMSE, Hits@K) without deeper fairness metrics or user studies
- Results are demonstrated only on MovieLens-1M and KG20C datasets, limiting generalizability to other domains

## Confidence

- **High confidence**: The linear projection mechanism for bias removal is well-established and the mathematical formulation is sound
- **Medium confidence**: The detection methods (logistic classifiers, CCA) are standard approaches, but their effectiveness depends on dataset characteristics
- **Medium confidence**: The trade-off between prediction performance and bias removal is demonstrated but the long-term effects on recommendation quality are unclear

## Next Checks

1. Test bias removal effectiveness on datasets with non-linear attribute relationships to verify mechanism 1 assumptions
2. Implement and evaluate higher-moment loss functions to determine if first-moment loss is sufficient for bias removal
3. Conduct ablation studies varying the regularization weight (œÉ and Œ∏) to understand sensitivity of first-moment loss to hyperparameter choices