---
ver: rpa2
title: Adapting Large Language Models to Domains via Reading Comprehension
arxiv_id: '2309.09530'
source_url: https://arxiv.org/abs/2309.09530
tags:
- language
- comprehension
- tasks
- text
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates continued pre-training of large language
  models on domain-specific corpora, finding that while this endows the model with
  domain knowledge, it significantly impairs prompting ability for question answering.
  To address this, the authors propose converting raw domain texts into reading comprehension
  materials enriched with various comprehension tasks and general instructions.
---

# Adapting Large Language Models to Domains via Reading Comprehension

## Quick Facts
- arXiv ID: 2309.09530
- Source URL: https://arxiv.org/abs/2309.09530
- Reference count: 37
- Key outcome: Converting raw domain corpora into reading comprehension tasks improves domain adaptation while preserving prompting ability

## Executive Summary
This paper addresses a critical challenge in domain adaptation: while continued pre-training on domain-specific corpora improves domain knowledge, it significantly impairs a model's ability to respond to diverse prompting tasks. The authors propose transforming raw domain texts into reading comprehension materials enriched with various comprehension tasks and general instructions. This approach consistently improves performance across biomedicine, finance, and law tasks, with a 7B model achieving competitive results against much larger models like BloombergGPT-50B. The method is highly scalable and can be applied to any pre-training corpora, showing potential for developing general models across more domains.

## Method Summary
The proposed method converts raw domain-specific corpora into reading comprehension texts through a three-step process: (1) mining task-relevant patterns from raw texts using regex patterns and verbalizers, (2) constructing comprehension tasks by enriching raw texts with diverse task types including summarization, natural language inference, word-to-text, commonsense reasoning, paraphrase detection, and text completion, and (3) mixing these comprehension texts with general instruction datasets at domain-specific ratios. The enriched corpus is then used for continued pre-training of LLaMA-7B, aiming to preserve domain knowledge while restoring the model's prompting ability across diverse task formats.

## Key Results
- 7B models trained with the proposed method achieve competitive performance with 50B models like BloombergGPT-50B on domain-specific tasks
- The approach consistently outperforms vanilla domain-adaptive pre-training across biomedicine, finance, and law domains
- Reading comprehension conversion successfully preserves both domain knowledge and prompting ability, addressing the degradation observed in standard DAPT approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Raw domain corpora improve domain knowledge but impair prompting ability due to lack of input-output diversity
- Mechanism: Continued pre-training on homogeneous domain texts shifts the model's learned patterns toward domain-specific syntax and semantics, which does not generalize to diverse prompting tasks
- Core assumption: The prompting performance depends on the model's ability to generalize across varied input-output patterns
- Evidence anchors: "training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability"

### Mechanism 2
- Claim: Reading comprehension tasks restore prompting ability by creating varied question-answering patterns while retaining domain knowledge
- Mechanism: By transforming raw texts into question-answer pairs across multiple task types (summarization, NLI, word-to-text, etc.), the model learns to respond to diverse queries while the context maintains domain knowledge
- Core assumption: Question-answering format preserves both domain knowledge and general prompting capability
- Evidence anchors: "transforming raw corpora into reading comprehension texts... Each raw text is enriched with a series of tasks related to its content"

### Mechanism 3
- Claim: Mixing general instructions with domain reading comprehension texts further improves prompting ability by exposing the model to diverse instruction formats
- Mechanism: General instructions provide diverse input-output patterns that complement domain-specific patterns, creating a more robust prompting capability
- Core assumption: General instructions contain sufficient diversity to enhance prompting performance across all domains
- Evidence anchors: "we propose augmenting the training data with general instructions... to cover a wider range of input-output types"

## Foundational Learning

- Concept: Domain-adaptive pre-training (DAPT)
  - Why needed here: Understanding why vanilla DAPT impairs prompting ability is crucial for developing the reading comprehension approach
  - Quick check question: What is the key difference between vanilla DAPT and the proposed reading comprehension method?

- Concept: Question-answering task formats
  - Why needed here: The paper relies on converting raw texts into question-answer pairs to restore prompting ability
  - Quick check question: How does the question-answering format help maintain both domain knowledge and prompting ability?

- Concept: Instruction tuning
  - Why needed here: Mixing general instructions with domain texts is based on principles from instruction tuning literature
  - Quick check question: What is the primary benefit of instruction tuning for large language models?

## Architecture Onboarding

- Component map: Raw text → Mining patterns → Task templates → Reading comprehension text → Continued pre-training → Improved performance
- Critical path: Raw text → Mining patterns → Task templates → Reading comprehension text → Continued pre-training → Improved performance
- Design tradeoffs:
  - Complexity vs. effectiveness: More complex task types may improve performance but increase implementation difficulty
  - Domain specificity vs. general applicability: More domain-specific tasks improve domain performance but may reduce general applicability
  - Data volume vs. training efficiency: Larger reading comprehension datasets improve performance but increase training time
- Failure signatures:
  - Poor prompting performance despite good domain knowledge: Indicates insufficient task diversity in reading comprehension texts
  - Good prompting performance but poor domain knowledge: Indicates insufficient domain content in reading comprehension texts
  - No improvement over vanilla DAPT: Indicates issues with task mining patterns or template application
- First 3 experiments:
  1. Compare performance of models trained on: raw text only, reading comprehension only, and mixed reading comprehension + general instructions
  2. Ablation study: Remove each comprehension task type and measure impact on domain-specific and general tasks
  3. Vary mixing ratios of reading comprehension and general instructions to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed method of converting raw corpora to reading comprehension texts maintain domain knowledge better than continued training on raw texts when scaling to even larger models (e.g., 100B+ parameters)?
- Basis in paper: The paper demonstrates improved performance with the 7B model compared to larger models like BloombergGPT-50B, but does not test on models significantly larger than 7B
- Why unresolved: The paper does not explore the performance of the method on models much larger than 7B parameters
- What evidence would resolve it: Experiments comparing the proposed method's performance on models of varying sizes (e.g., 7B, 30B, 65B, 100B+) across multiple domains

### Open Question 2
- Question: How does the proposed method's performance compare when applied to multilingual or cross-lingual domain-specific corpora?
- Basis in paper: The paper focuses on English-language corpora in biomedicine, finance, and law domains, with no mention of multilingual or cross-lingual capabilities
- Why unresolved: The paper does not explore the method's effectiveness on non-English or multilingual corpora
- What evidence would resolve it: Experiments applying the method to domain-specific corpora in multiple languages and comparing performance across languages

### Open Question 3
- Question: What is the impact of varying the ratio of reading comprehension texts to general instructions on model performance across different domains?
- Basis in paper: The paper explores different ratios for mixing reading comprehension texts with general instructions, finding optimal ratios for each domain (1:1 for biomedicine and law, 1:2 for finance)
- Why unresolved: While optimal ratios are found for the tested domains, the paper does not systematically explore the impact of varying these ratios across a wider range of domains or task types
- What evidence would resolve it: Experiments systematically varying the ratio of reading comprehension texts to general instructions across multiple domains and task types, and analyzing the impact on model performance

## Limitations

- The proposed approach requires substantial manual effort in designing regex patterns and task templates for each domain, limiting scalability
- Performance improvements are primarily demonstrated on three specific domains (biomedicine, finance, law), raising questions about generalizability to other domains
- The method depends on the availability of high-quality raw domain corpora with sufficient task-relevant content

## Confidence

- High confidence: The observation that vanilla domain-adaptive pre-training impairs prompting ability is well-established in prior literature and consistently observed in experiments
- Medium confidence: The specific mechanism by which reading comprehension tasks restore prompting ability is plausible but not definitively proven; alternative explanations cannot be ruled out
- Medium confidence: The claim that general instruction mixing provides additional benefits is supported by experiments but the optimal mixing ratios appear domain-dependent and not theoretically justified

## Next Checks

1. Conduct ablation studies to isolate the contribution of each comprehension task type (summarization, NLI, word-to-text, etc.) to performance improvements
2. Test the approach on additional domains beyond biomedicine, finance, and law to assess generalizability and identify domain characteristics that predict success
3. Compare the proposed method against alternative domain adaptation approaches (instruction tuning, adapter-based methods) using the same computational budget to establish relative effectiveness