---
ver: rpa2
title: Language-Driven Anchors for Zero-Shot Adversarial Robustness
arxiv_id: '2301.13096'
source_url: https://arxiv.org/abs/2301.13096
tags:
- adversarial
- anchors
- zero-shot
- text
- laat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LAAT, a language-driven, anchor-based adversarial
  training strategy for zero-shot adversarial robustness. The method uses CLIP's text
  encoder to generate normalized feature embeddings (anchors) for each category, which
  are then used for adversarial training.
---

# Language-Driven Anchors for Zero-Shot Adversarial Robustness

## Quick Facts
- arXiv ID: 2301.13096
- Source URL: https://arxiv.org/abs/2301.13096
- Authors: 
- Reference count: 40
- This paper proposes LAAT, a language-driven, anchor-based adversarial training strategy for zero-shot adversarial robustness.

## Executive Summary
This paper addresses the challenge of achieving adversarial robustness in zero-shot settings, where models must defend against adversarial examples on categories not seen during training. The authors propose LAAT (Language-driven, Anchor-based Adversarial Training), which leverages CLIP's text encoder to generate category-specific anchors for adversarial training. The method significantly improves zero-shot adversarial robustness over state-of-the-art methods, even surpassing previous one-shot methods in most attack settings when trained on large datasets like ImageNet-1K.

## Method Summary
LAAT uses CLIP's text encoder to generate normalized feature embeddings (anchors) for each category, which are then used for adversarial training. The authors identify that high cosine similarity between text anchors poses challenges for robustness. To address this, they design an expansion algorithm to enlarge distances between anchors while maintaining semantic consistency. They also introduce a relaxation of the optimization objective by reintroducing cross-entropy loss alongside the anchor-based loss. The proposed method significantly improves zero-shot adversarial robustness over state-of-the-art methods, even surpassing previous one-shot methods in most attack settings.

## Key Results
- LAAT significantly improves zero-shot adversarial robustness over state-of-the-art methods
- When trained on ImageNet-1K, LAAT models demonstrate substantial zero-shot adversarial robustness across several downstream datasets without fine-tuning
- LAAT even surpasses previous one-shot methods in most attack settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High cosine similarity between text anchors weakens adversarial robustness.
- Mechanism: When anchors are clustered closely in feature space, adversarial perturbations have a larger feasible region to push samples across decision boundaries without being detected.
- Core assumption: Adversarial attacks exploit large angular margins between classes to succeed.
- Evidence anchors:
  - "We analyzed the reason in depth and found that it is mainly caused by the large cosine similarity (CoS) between anchors (see Sec. 3.1)."
  - "Although high CoS seems to have no effect on downstream tasks on benign images [20,28,49], we found that AT would be remarkably affected because higher CoS implies that the anchors are closer to the decision boundary, which could leave more room for adversarial examples [35,56]."
  - Weak evidence: Only indirect connection to adversarial robustness in corpus; needs verification.

### Mechanism 2
- Claim: Expansion algorithm increases anchor distances while preserving semantic consistency.
- Mechanism: By mapping clustered anchors onto a hemisphere using spherical coordinate transformations, the expansion algorithm enlarges angular margins between classes.
- Core assumption: Semantic consistency can be preserved under controlled geometric transformations of the embedding space.
- Evidence anchors:
  - "Thus, we first find a center of the clustered anchors, v := ∑N i=1 ai || ∑N i=1 ai||2 , then we calculate a rotation matrix R from the center to the polar p = [1, 0,··· , 0]T."
  - "As shown, the expansion algorithm can increase the distances between different anchors while maintaining the semantic consistency of the original CLIP anchors (close anchors are still close after expansion)."
  - Weak evidence: No direct corpus evidence of expansion algorithm effectiveness.

### Mechanism 3
- Claim: Reintroducing cross-entropy loss relaxes the optimization and improves robustness.
- Mechanism: Cross-entropy loss allows visual features to be farther from all anchors as long as the correct anchor remains closest, providing more flexibility than strict cosine maximization.
- Core assumption: Relaxing optimization constraints improves robustness by avoiding overfitting to specific anchor positions.
- Evidence anchors:
  - "With CE supervision, the output feature could be optimized to be far away from all anchors, but it can still be correctly classified as long as it is closest to the GT anchor."
  - "To validate this, we computed the average CoS between GT anchors and the output visual features of the two models trained on CIFAR-FS in Tab. 5: cosθ + SP and cosθ + SP + CE. With CE loss, the visual feature's CoS with GT anchors dropped from 0.455 to 0.205 in average, while the accuracies on both benign and adversarial examples improved a lot."
  - Weak evidence: No corpus papers discussing this specific relaxation technique.

## Foundational Learning

- Concept: Adversarial training and its limitations
  - Why needed here: Understanding why standard AT struggles with zero-shot settings and how LAAT addresses these limitations is crucial for implementation.
  - Quick check question: Why does adversarial training typically require large amounts of labeled data, and how does LAAT circumvent this requirement?

- Concept: Vision-language models and semantic consistency
  - Why needed here: CLIP's text encoder provides semantically consistent embeddings that LAAT leverages for zero-shot transfer.
  - Quick check question: How does CLIP's text encoder ensure that semantically similar categories have similar embeddings?

- Concept: Spherical coordinate transformations
  - Why needed here: The expansion algorithm uses spherical coordinates to manipulate anchor positions while preserving semantic relationships.
  - Quick check question: What are the key properties of spherical coordinates that make them suitable for manipulating normalized embeddings?

## Architecture Onboarding

- Component map:
  - Text encoder (CLIP) → Anchor extraction → Expansion algorithm → Image encoder (trainable) → Adversarial training loop
  - Loss components: Cross-entropy loss (L1), smoothness loss (L2), combined loss L = L1 + αL2

- Critical path:
  1. Generate text anchors from category names using CLIP text encoder
  2. Apply expansion algorithm to increase anchor separation
  3. Initialize image encoder and set anchors as fixed targets
  4. For each training batch:
     - Generate adversarial perturbations
     - Compute features and losses
     - Backpropagate only through image encoder
  5. Evaluate zero-shot robustness on novel categories

- Design tradeoffs:
  - Expansion vs. semantic consistency: Too much expansion may break semantic relationships
  - Cross-entropy weight vs. cosine maximization: Balancing flexibility and structure
  - Smoothness loss weight: Too high may hurt benign accuracy, too low may reduce robustness

- Failure signatures:
  - Slow convergence or poor accuracy on seen categories → Check anchor quality and expansion
  - Good seen accuracy but poor zero-shot performance → Check semantic consistency of anchors
  - High clean accuracy but low robust accuracy → Increase adversarial perturbation strength or adjust smoothness loss

- First 3 experiments:
  1. Baseline: Train with original CLIP anchors (without expansion or cross-entropy) to confirm convergence issues
  2. Expansion only: Apply expansion algorithm and measure cosine similarity reduction and accuracy improvement
  3. Full LAAT: Implement complete method with expansion, cross-entropy, and smoothness loss, evaluate zero-shot robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LAAT compare to adversarially robust models trained directly on image-text pairs, similar to CLIP?
- Basis in paper: The paper states that limited by data and computation resources, they cannot perform adversarial training on large-scale image-text pairs directly like CLIP. They suggest this could be a promising way to achieve both adversarial and out-of-distribution robustness.
- Why unresolved: The authors did not have the resources to train adversarially robust models on large-scale image-text pairs. This would require significant computational power and access to large datasets.
- What evidence would resolve it: Training and evaluating adversarially robust models on large-scale image-text pairs and comparing their performance to LAAT models.

### Open Question 2
- Question: How does the choice of prompt text affect the quality of the text anchors and the subsequent zero-shot adversarial robustness?
- Basis in paper: The paper mentions that they use a fixed prompt text like "This is a photo of a{}" to generate sentences from category names. They also state that "Note that the high CoS is irrelevant to the prompt text."
- Why unresolved: While the paper claims that the high cosine similarity is irrelevant to the prompt text, it doesn't explore how different prompt texts might affect the quality of the text anchors or the model's performance.
- What evidence would resolve it: Experimenting with different prompt texts and evaluating their impact on the quality of text anchors and the model's zero-shot adversarial robustness.

### Open Question 3
- Question: How does LAAT perform on datasets with a large number of categories, especially compared to standard adversarially robust models?
- Basis in paper: The paper shows promising results on datasets like CIFAR100, AwA2, aPY, and COCO Objects. However, it doesn't explore performance on datasets with a very large number of categories.
- Why unresolved: The paper doesn't provide results on datasets with a large number of categories, so it's unclear how LAAT would perform in such scenarios.
- What evidence would resolve it: Evaluating LAAT on datasets with a large number of categories and comparing its performance to standard adversarially robust models.

## Limitations
- The effectiveness of the expansion algorithm relies heavily on the assumption that semantic consistency can be preserved through geometric transformations
- The claim that cross-entropy loss improves robustness by relaxing optimization constraints needs more rigorous ablation studies
- The paper doesn't address potential overfitting to CLIP's specific semantic space, which could limit generalizability to other vision-language models

## Confidence
- **High confidence**: The empirical results showing LAAT's effectiveness on multiple datasets and its superiority over baseline methods are well-supported by the presented data.
- **Medium confidence**: The theoretical explanation for why high cosine similarity between anchors affects adversarial robustness is plausible but could benefit from more rigorous mathematical analysis.
- **Low confidence**: The claim that LAAT "outperforms state-of-the-art few-shot adversarial robustness methods" needs clarification, as zero-shot and few-shot settings have different evaluation protocols and resource requirements.

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of the expansion algorithm, cross-entropy loss, and smoothness loss to overall performance.
2. Test LAAT with different vision-language models (e.g., BLIP, ALIGN) to verify robustness to the choice of text encoder and semantic space.
3. Evaluate the method's performance on datasets with more complex category relationships (e.g., hierarchical taxonomies) to test the limits of semantic consistency preservation during anchor expansion.