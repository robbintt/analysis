---
ver: rpa2
title: Rethinking Benchmark and Contamination for Language Models with Rephrased Samples
arxiv_id: '2311.04850'
source_url: https://arxiv.org/abs/2311.04850
tags:
- rephrased
- test
- contamination
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigate how contamination can affect language model evaluation
  and discover that existing methods for detecting contamination can be easily bypassed.
  We show that if such variation of test data is not eliminated, a 13B model can easily
  overfit the test benchmark and achieve drastically high performance, on par with
  GPT-4.
---

# Rethinking Benchmark and Contamination for Language Models with Rephrased Samples

## Quick Facts
- arXiv ID: 2311.04850
- Source URL: https://arxiv.org/abs/2311.04850
- Reference count: 22
- Key outcome: Existing contamination detection methods can be easily bypassed by rephrased samples, and a new LLM-based method is proposed to address this vulnerability

## Executive Summary
This paper investigates how contamination affects language model evaluation and demonstrates that existing detection methods based on n-gram overlap are insufficient against simple variations like paraphrasing and translation. The authors show that when these variations are not eliminated, models can achieve artificially high performance on benchmarks. To address this, they propose an LLM-based contamination detection method that uses semantic understanding rather than exact string matching, and apply it to real-world datasets revealing significant previously unknown test overlap.

## Method Summary
The authors propose an LLM-based decontamination method that combines embedding similarity search with GPT-4 judgment to detect rephrased samples. The method first uses embedding similarity to find top-k similar samples, then prompts a strong LLM to judge semantic equivalence. They apply this approach to various benchmarks (MMLU, HumanEval, GSM-8k) and training datasets (RedPajama-Data-1T, StarCoder-Data, CodeAlpaca), demonstrating its effectiveness in identifying contamination that traditional methods miss.

## Key Results
- Traditional n-gram overlap methods fail to detect rephrased samples that preserve semantic meaning
- A 13B model trained on rephrased samples can achieve performance on par with GPT-4 on contaminated benchmarks
- The proposed LLM decontaminator successfully identifies significant previously unknown test overlap in real-world datasets
- Translation-based contamination can evade both n-gram overlap and standard embedding similarity searches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rephrased samples can bypass traditional n-gram overlap contamination detection
- Mechanism: By altering word order, synonyms, and variable names while preserving semantics, rephrased samples avoid exact string matching
- Core assumption: Existing contamination detection methods rely heavily on exact string matching rather than semantic similarity
- Evidence anchors:
  - [abstract] "simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures"
  - [section 2] "N-gram overlap relies on string matching to detect contamination, widely used by leading developments such as GPT-4"
  - [corpus] Found 25 related papers, but no specific evidence on n-gram bypass mechanisms
- Break condition: If detection methods incorporate semantic understanding rather than pure string matching

### Mechanism 2
- Claim: LLM-based decontamination can detect rephrased samples through semantic similarity and contextual understanding
- Mechanism: Uses embedding similarity search to find top-k similar samples, then prompts a strong LLM to judge semantic equivalence
- Core assumption: Advanced LLMs can understand semantic equivalence even when surface forms differ significantly
- Evidence anchors:
  - [abstract] "we propose a stronger LLM-based decontamination method"
  - [section 4] "it utilizes the strong LLMs' reliable judgments"
  - [corpus] No direct evidence on LLM decontamination effectiveness
- Break condition: If LLM lacks sufficient reasoning capability or context window

### Mechanism 3
- Claim: Translation of test samples to other languages creates contamination that evades detection
- Mechanism: Different languages produce different embeddings in most language models, making translated samples appear dissimilar in embedding space
- Core assumption: Embedding models are typically language-specific and don't capture cross-linguistic semantic equivalence
- Evidence anchors:
  - [abstract] "simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures"
  - [section 3.2] "By translating test prompts into other languages, we can evade n-gram overlap detection and standard embedding similarity searches"
  - [corpus] No specific evidence on translation-based contamination
- Break condition: If multilingual embedding models become standard in contamination detection

## Foundational Learning

- Concept: String matching vs semantic similarity
  - Why needed here: Understanding the fundamental difference between exact matching and semantic understanding is crucial for grasping why traditional methods fail
  - Quick check question: Would "What is 2+2?" and "What's two plus two?" be detected as the same by n-gram overlap?

- Concept: Embedding similarity search
  - Why needed here: The LLM decontaminator relies on this technique as a first pass before LLM judgment
  - Quick check question: What threshold value would you use to balance false positives and false negatives?

- Concept: Data contamination in machine learning
  - Why needed here: Understanding the broader context of why contamination matters for benchmark reliability
  - Quick check question: How does training on test data affect the interpretation of model performance?

## Architecture Onboarding

- Component map: Rephrasing engine -> Traditional detectors -> LLM decontaminator -> Benchmark evaluation
- Critical path: Rephrase → Traditional detection → LLM decontaminator → Benchmark evaluation
- Design tradeoffs:
  - Accuracy vs computational cost in embedding similarity search
  - False positive vs false negative rates in contamination detection
  - Speed vs thoroughness in LLM-based evaluation
- Failure signatures:
  - High false negative rates in traditional methods
  - LLM decontaminator producing inconsistent judgments
  - Rephrased samples achieving near-perfect scores on benchmarks
- First 3 experiments:
  1. Test traditional n-gram overlap on rephrased samples from MMLU
  2. Implement and evaluate the LLM decontaminator on translated HumanEval samples
  3. Measure performance degradation when training on contaminated vs. clean data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we precisely define what constitutes benchmark contamination in large language models, beyond simple rephrased samples?
- Basis in paper: [inferred] The paper discusses the challenges of defining contamination, mentioning cases where only numbers are substituted in test examples (GSM-8k) and suggesting this is an open question for the community.
- Why unresolved: The boundary between contamination and legitimate generalization is unclear. Simple variations like number substitution may skew benchmark results without being detectable by current methods.
- What evidence would resolve it: Development of a formal framework that distinguishes between beneficial generalization and harmful memorization/contamination, validated through extensive experiments across diverse benchmark types.

### Open Question 2
- Question: What are the most effective methods for detecting contamination in synthetic data generated by LLMs?
- Basis in paper: [explicit] The paper identifies contamination in CodeAlpaca, a synthetic dataset generated by GPT, and suggests this is a growing concern as models are increasingly trained on LLM-generated data.
- Why unresolved: Current detection methods like n-gram overlap and embedding similarity are insufficient for identifying subtle contamination in synthetic data. The paper calls for stronger decontamination approaches.
- What evidence would resolve it: Comparative evaluation of novel contamination detection methods specifically designed for synthetic data, demonstrating superior performance to existing approaches across multiple synthetic datasets.

### Open Question 3
- Question: How can we develop fresh one-time exams for accurate LLM evaluation that minimize the risk of contamination?
- Basis in paper: [explicit] The authors advocate for the development of fresh, one-time exams similar to Codeforces and Kaggle competitions to accurately assess LLMs, as an alternative to static benchmarks.
- Why unresolved: Static benchmarks are vulnerable to contamination through repeated use. One-time exams would require new infrastructure and evaluation methodologies.
- What evidence would resolve it: Successful implementation and validation of a one-time exam framework for LLM evaluation, showing improved resistance to contamination and more accurate assessment of model capabilities.

## Limitations
- The computational cost of LLM-based contamination detection is high, requiring multiple GPT-4 API calls
- Limited evidence for generalizability across specialized domains beyond code and general knowledge tasks
- The evaluation focuses on detecting contamination rather than comprehensively quantifying its impact on model performance

## Confidence
- N-gram bypass by rephrasing: Medium-High
- LLM decontaminator effectiveness: Medium
- Translation-based contamination: Medium
- Cross-lingual detection capability: Medium-Low

## Next Checks
1. **Cross-lingual contamination test**: Evaluate the LLM decontaminator's effectiveness on translated samples across multiple language pairs (e.g., English→Chinese, English→Arabic) to verify the translation-based contamination claims and identify language-specific failure modes.

2. **Threshold sensitivity analysis**: Systematically vary the embedding similarity thresholds and top-k parameters in the LLM decontaminator to quantify their impact on false positive/negative rates across different benchmark types.

3. **Performance impact quantification**: Conduct controlled experiments training multiple model sizes (1B, 7B, 13B) on varying degrees of contaminated data to establish a clear relationship between contamination level and benchmark score inflation.