---
ver: rpa2
title: 'TinyGSM: achieving >80% on GSM8k with small language models'
arxiv_id: '2312.09241'
source_url: https://arxiv.org/abs/2312.09241
tags:
- arxiv
- verifier
- language
- data
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of achieving high accuracy on
  GSM8K, a benchmark for mathematical reasoning in language models, using small-scale
  models. The key idea is to leverage high-quality synthetic data and a verifier model
  to select the best answer from multiple candidate generations.
---

# TinyGSM: achieving >80% on GSM8k with small language models

## Quick Facts
- arXiv ID: 2312.09241
- Source URL: https://arxiv.org/abs/2312.09241
- Authors: 
- Reference count: 16
- Primary result: 1.3B model achieves 81.5% accuracy on GSM8K, setting new state-of-the-art for small language models

## Executive Summary
This paper introduces TinyGSM, a synthetic dataset of 12.3M grade-school math problems paired with Python solutions, generated by GPT-3.5-turbo. The authors finetune small language models (125M, 350M, and 1.3B parameters) on this dataset and use a verifier model to select the best answer from 48 candidate generations. The approach achieves 81.5% accuracy on GSM8K, surpassing existing small-scale models and rivaling GPT-3.5 performance. The work demonstrates that high-quality synthetic data and effective verification can enable small models to match the mathematical reasoning capabilities of much larger models.

## Method Summary
The authors generate TinyGSM by using GPT-3.5-turbo to create 12.3M synthetic grade-school math problems with Python solutions. Small language models are then finetuned on this dataset using Adam optimizer with FP16, linear warm-up, and maximum learning rate of 1e-4. A verifier model is trained to predict the correctness of generated solutions from multiple candidate generations (48 per question). The verifier selects the final answer by scoring all candidates, enabling the system to achieve high accuracy despite using small models. The approach emphasizes data diversity and verifier size as key factors for performance.

## Key Results
- 1.3B model with verifier achieves 81.5% accuracy on GSM8K
- Scaling verifier from 125M to 1.3B provides 7.2% performance boost, outperforming generator scaling benefits
- TinyGSM models significantly outperform existing small-scale models and approach GPT-3.5 performance
- Data diversity and verifier size shown to be more critical than generation model size for accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality synthetic data enables small language models to match the performance of much larger models on mathematical reasoning tasks.
- Mechanism: Synthetic data generated by GPT-3.5-turbo provides diverse, high-quality examples that capture the distribution of GSM8K problems. This allows small models to learn effective reasoning patterns without requiring massive parameter counts.
- Core assumption: The synthetic data distribution adequately represents the true problem distribution and includes sufficient diversity to generalize well.
- Evidence anchors:
  - [abstract] "TinyGSM, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated fully by GPT-3.5."
  - [section] "To enhance robustness, we also generated synthetic problems whose questions contain irrelevant information."
  - [corpus] Found 25 related papers; average neighbor FMR=0.495 indicates moderate relevance in the broader literature.
- Break condition: If the synthetic data fails to capture the full complexity of GSM8K problems, including edge cases and diverse reasoning patterns, the small models will not generalize effectively.

### Mechanism 2
- Claim: A verifier model significantly improves accuracy by selecting the best among multiple candidate generations.
- Mechanism: The verifier scores candidate solutions generated by the base model, allowing selection of the most likely correct answer. This leverages the probabilistic nature of language model generation to find better solutions than single-pass decoding.
- Core assumption: The verifier can reliably distinguish correct from incorrect solutions based on learned patterns in the training data.
- Evidence anchors:
  - [abstract] "use of a verifier, which selects the final outputs from multiple candidate generations."
  - [section] "we find that a duo of a 1.3B generation model and a 1.3B verifier model can achieve 81.5% accuracy."
  - [corpus] "Orca-Math: Unlocking the potential of SLMs in Grade School Math" suggests verifier approaches are competitive in this space.
- Break condition: If the verifier becomes overfitted to the training distribution or cannot generalize to novel problem types, its selection will not improve accuracy.

### Mechanism 3
- Claim: Data diversity and verifier size are more critical than generation model size for achieving high accuracy.
- Mechanism: Training the verifier on diverse generations from multiple checkpoints and temperatures improves its ability to select correct answers. Scaling the verifier provides more parameter-efficient improvements than scaling the generator.
- Core assumption: The verifier benefits more from parameter scaling than the generator because verification is a simpler task that can be learned effectively with more parameters.
- Evidence anchors:
  - [section] "we find that the scaling of the verifier may be more effective than scaling of the generator: while scaling up from a 125M generator to a 1.3B generator only gives a 5.1% increase in performance, scaling up the verifier from 125M to 1.3B leads to a 7.2% performance boost."
  - [section] "the verifier size seems to play a bigger role than the generation model size."
  - [corpus] Weak direct evidence; this appears to be a novel finding from the paper.
- Break condition: If verification requires the same complexity as generation, then scaling the generator would provide comparable or better returns than scaling the verifier.

## Foundational Learning

- Concept: Synthetic data generation and filtering
  - Why needed here: The quality and diversity of synthetic data directly impacts the small model's ability to learn mathematical reasoning patterns.
  - Quick check question: What criteria are used to filter synthetic data, and why are certain filtering approaches (like consistency checking) avoided?

- Concept: Verifier training and evaluation
  - Why needed here: The verifier is critical for achieving >80% accuracy by selecting the best generation from multiple candidates.
  - Quick check question: How is the verifier trained (sequence-to-sequence vs sequence classification), and what data is used for training?

- Concept: Contamination detection and prevention
  - Why needed here: Ensures that the synthetic dataset does not contain test set information, which would invalidate performance claims.
  - Quick check question: What n-gram matching approach is used to detect contamination, and what threshold is considered acceptable?

## Architecture Onboarding

- Component map: Generation model (125M/350M/1.3B) → Verifier model (125M/350M/1.3B) → Final answer selection
- Critical path: Synthetic data generation → Model training → Verification and selection → Accuracy measurement
- Design tradeoffs: Larger verifier provides better accuracy but increases computational cost; synthetic data quality vs. diversity tradeoff in filtering
- Failure signatures: Poor accuracy indicates issues with data quality, verifier training, or contamination; low verifier score correlation suggests training data issues
- First 3 experiments:
  1. Train a 125M generation model on TinyGSM and evaluate pass@1 accuracy to establish baseline
  2. Train a 125M verifier on 48 generations per question and measure its accuracy on the validation set
  3. Test different verifier sizes (125M vs 1.3B) with the same generation model to quantify the impact of verifier scaling

## Open Questions the Paper Calls Out

- The paper does not explicitly call out open questions, but raises several implicit ones:
  - How does the approach generalize to other mathematical reasoning benchmarks beyond GSM8K?
  - What is the optimal ratio between generation model and verifier model sizes?
  - How does data diversity impact performance, and is there a point of diminishing returns?

## Limitations

- The exact prompts and filtering criteria for synthetic data generation are not specified, making it difficult to assess reproducibility and data efficiency
- Contamination detection methodology (n-gram matching) is described but not detailed, leaving uncertainty about test set leakage prevention
- The verifier's performance on out-of-distribution problems is not evaluated, raising questions about generalization beyond GSM8K
- The approach relies heavily on the quality of GPT-3.5-turbo for synthetic data generation, which may not be accessible to all researchers

## Confidence

- **High Confidence**: The core finding that verifier models can significantly improve accuracy by selecting among multiple generations is well-supported by the experimental results
- **Medium Confidence**: The claim that data diversity and verifier size are more critical than generation model size is supported by ablation studies but could benefit from more extensive experiments across different model architectures
- **Medium Confidence**: The assertion that TinyGSM achieves state-of-the-art performance for small models is valid within the specified parameter range, though comparisons with larger models remain important for context

## Next Checks

1. **Contamination Verification**: Conduct an independent n-gram matching analysis between TinyGSM and GSM8K test sets using multiple threshold values to verify no test data leakage occurred
2. **Out-of-Distribution Testing**: Evaluate the 1.3B model with verifier on MATH dataset or other mathematical reasoning benchmarks to assess generalization beyond GSM8K
3. **Ablation on Generation Prompts**: Test the approach using different quality levels of synthetic data (e.g., generated by smaller models or with simplified prompts) to quantify the impact of data quality versus model architecture