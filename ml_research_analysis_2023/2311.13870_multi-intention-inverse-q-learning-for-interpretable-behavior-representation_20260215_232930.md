---
ver: rpa2
title: Multi-intention Inverse Q-learning for Interpretable Behavior Representation
arxiv_id: '2311.13870'
source_url: https://arxiv.org/abs/2311.13870
tags:
- latent
- state
- reward
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Latent (Markov) Variable Inverse Q-learning
  (L(M)V-IQL), a new framework for inverse reinforcement learning that infers discrete
  time-varying rewards from expert trajectories. L(M)V-IQL uses an Expectation-Maximization
  approach to cluster trajectories into distinct intentions and solves the IRL problem
  independently for each intention.
---

# Multi-intention Inverse Q-learning for Interpretable Behavior Representation

## Quick Facts
- arXiv ID: 2311.13870
- Source URL: https://arxiv.org/abs/2311.13870
- Reference count: 40
- Key outcome: L(M)V-IQL framework infers discrete time-varying rewards from expert trajectories, outperforming state-of-the-art methods in behavior prediction and producing interpretable reward functions for mice behavior analysis.

## Executive Summary
This paper introduces Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL), a framework for inverse reinforcement learning that infers discrete time-varying rewards from expert trajectories. The approach uses an Expectation-Maximization algorithm to cluster trajectories into distinct intentions and solves the IRL problem independently for each intention. Theoretical analysis shows L(M)V-IQL can handle both generalized Bernoulli and Markov process intention transitions. Experiments on simulated Gridworld, mice navigation, and mice reversal-learning tasks demonstrate superior behavior prediction and interpretable reward functions compared to state-of-the-art methods.

## Method Summary
The method employs an EM algorithm to cluster trajectories into latent intentions, then applies Inverse Q-learning to recover reward functions for each intention. LV-IQL handles independent latent states while LMV-IQL models Markovian transitions between intentions. The framework alternates between E-steps (computing trajectory posteriors) and M-steps (updating rewards using IQL) until convergence. It can work with either known environment models (using IA VI) or unknown models (using IQL). The approach assumes expert demonstrations follow Boltzmann policies and that intention dynamics can be modeled as either Bernoulli or Markov processes.

## Key Results
- L(M)V-IQL outperforms state-of-the-art methods in behavior prediction across simulated and real animal behavior datasets
- Learned reward functions are interpretable, revealing mice switch between 'Tired' and 'Thirsty' intentions in labyrinth navigation
- In reversal-learning tasks, mice alternate between 'Exploitation', 'Win-stay', and 'Exploration' strategies with exploration more likely after high engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LV-IQL and LMV-IQL can effectively recover discrete time-varying rewards by clustering trajectories into intentions and solving IRL independently for each.
- Mechanism: The EM algorithm iteratively assigns trajectories to latent states (intentions) based on current reward estimates, then updates rewards using only the weighted trajectories from each intention. This decouples intention clustering from reward learning.
- Core assumption: Trajectories can be meaningfully partitioned into distinct intention clusters, and the underlying intention dynamics follow either a generalized Bernoulli or Markov process.
- Evidence anchors:
  - [abstract]: "Leverag- ing an Expectation-Maximization approach, we cluster observed trajectories into distinct intentions and independently solve the IRL problem for each."
  - [section 4.1]: "We adopt the EM (Dempster et al., 1977) as a straightforward approach to attack Problem 2."
  - [corpus]: Weak evidence - most related works focus on continuous or linear reward models rather than discrete clustering.
- Break condition: If the intention dynamics are truly continuous or the trajectory clusters overlap significantly, the discrete clustering will produce mixed rewards and fail to capture the underlying patterns.

### Mechanism 2
- Claim: The Q-learning framework enables model-free learning of rewards without requiring known transition dynamics.
- Mechanism: Inverse Q-learning derives rewards from the expert's action selection probabilities via the Bellman equation, avoiding the need for explicit environment modeling. This extends to multiple intentions by applying the derivation separately to each intention cluster.
- Core assumption: Expert demonstrations follow a Boltzmann policy with respect to the optimal action-value function.
- Evidence anchors:
  - [section 3.2]: "The class of Inverse Q-learning algorithms (Kalweit et al., 2020) provides a precise yet notably time-efficient solution to Problem 1, compared to the popular Maximum Entropy IRL algorithm"
  - [section 4.1]: "Thus combining the above EM approach for trajectory clustering with IA VI or IQL algorithms leads to the class of Latent Variable Inverse Q-learning (LV-IQL) algorithms"
  - [corpus]: Strong evidence - BiCQL-ML and other Q-learning based IRL methods demonstrate similar model-free approaches.
- Break condition: If the expert policy deviates significantly from the assumed Boltzmann distribution, the reward recovery will be biased.

### Mechanism 3
- Claim: The Markov process formulation captures intention switching dynamics more accurately than independent Bernoulli processes for sequential tasks.
- Mechanism: LMV-IQL explicitly models the transition probabilities between intentions as a Markov chain, allowing it to capture temporal dependencies in intention switching.
- Core assumption: Intention transitions depend only on the current intention (Markov property) rather than the entire history.
- Evidence anchors:
  - [abstract]: "Theoretical analysis shows L(M)V-IQL can handle two types of intention transition dynamics: generalized Bernoulli and Markov processes."
  - [section 4.2]: "Under this assumption, given the agent demonstrations D consisting of a sequence of trajectories, the set of parameters to be inferred is then {Π, Λ; R1, . . . ,RK}"
  - [corpus]: Moderate evidence - some related works assume Markovian switching but don't provide empirical validation on animal behavior.
- Break condition: If intention transitions depend on longer-term history or external factors not captured by the current intention, the Markov assumption will be violated.

## Foundational Learning

- Concept: Expectation-Maximization algorithm
  - Why needed here: EM provides a principled way to alternate between assigning trajectories to intentions (E-step) and learning rewards for each intention (M-step).
  - Quick check question: What happens to the E-step if we initialize with completely random intention assignments?

- Concept: Inverse Reinforcement Learning
  - Why needed here: IRL recovers the reward function that explains observed behavior, which is essential for understanding the motivations behind animal actions.
  - Quick check question: How does IRL differ from imitation learning in terms of what it recovers from demonstrations?

- Concept: Markov Decision Processes
  - Why needed here: MDP provides the mathematical framework for formalizing the decision-making problem and defining optimal policies.
  - Quick check question: What additional complexity does a partially observable MDP introduce compared to a standard MDP?

## Architecture Onboarding

- Component map: Trajectory clustering (EM E-step) -> Reward learning (EM M-step using IQL) -> Intention transition modeling (Bernoulli or Markov) -> Repeat until convergence

- Critical path: 1) Initialize latent state parameters, 2) E-step: compute posterior probabilities for each trajectory, 3) M-step: update reward functions using IQL, 4) Update intention transition parameters, 5) Check convergence, 6) Repeat until convergence

- Design tradeoffs: Discrete intentions provide interpretability but may miss gradual transitions; model-based approaches (IA VI) are faster but require known dynamics; model-free approaches (IQL) are more flexible but need more samples

- Failure signatures: Poor clustering indicated by high overlap between learned intention trajectories; incorrect reward recovery shown by low log-likelihood on held-out data; Markov assumption violations indicated by systematic prediction errors in intention sequences

- First 3 experiments:
  1. Test LV-IQL on Gridworld with known intention switching to verify basic clustering and reward recovery
  2. Apply LMV-IQL to the real mice navigation dataset from Rosenberg et al. (2021). Compare test set log-likelihood to DIRL baseline and analyze learned reward functions and intention transition dynamics
  3. Run LMV-IQL on reversal-learning data to identify distinct behavioral strategies and their temporal dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of L(M)V-IQL change if applied to continuous rather than discrete reward functions?
- Basis in paper: [explicit] The paper focuses on discrete time-varying rewards, while Ashwood et al. (2022a) explored continuous time-varying rewards with DIRL.
- Why unresolved: The paper only demonstrates L(M)V-IQL's performance on discrete rewards, leaving open the question of its applicability to continuous reward functions.
- What evidence would resolve it: Experimental results comparing L(M)V-IQL's performance on both discrete and continuous reward functions in similar tasks.

### Open Question 2
- Question: What external factors influence the intention transition dynamics in mice behavior?
- Basis in paper: [inferred] The paper mentions extending the fixed intention transition probabilities with a generalized linear model to identify external factors influencing intention transitions.
- Why unresolved: The paper does not explore or identify specific external factors that influence intention transitions in mice behavior.
- What evidence would resolve it: Empirical data linking specific environmental or internal states to intention transition probabilities in mice.

### Open Question 3
- Question: How does the number of latent states affect the interpretability of reward functions in L(M)V-IQL?
- Basis in paper: [explicit] The paper mentions using BIC to determine the optimal number of latent states but also notes that interpretability was a consideration in choosing K=3 for the reversal-learning task.
- Why unresolved: While BIC provides a statistical measure, the trade-off between model complexity and interpretability is not fully explored.
- What evidence would resolve it: A systematic study varying the number of latent states and evaluating both model fit and interpretability across multiple tasks.

## Limitations

- Assumes discrete, well-separated intentions which may not capture gradual transitions in real animal behavior
- Requires sufficient trajectory samples for each intention to enable reliable reward recovery, limiting applicability to behaviors with sparse demonstrations
- Assumes the expert follows a Boltzmann policy, which may not hold for all animal decision-making processes

## Confidence

- **High confidence**: The theoretical foundations of EM-based trajectory clustering and inverse Q-learning for single intentions are well-established and clearly derived.
- **Medium confidence**: The extension to Markovian intention dynamics is sound, but the assumption of first-order Markov processes for intention switching in animal behavior needs empirical validation across diverse behavioral paradigms.
- **Medium confidence**: The interpretability of learned reward functions depends heavily on the quality and quantity of demonstrations, with limited guarantees when trajectory clusters overlap significantly.

## Next Checks

1. **Cross-species validation**: Apply LMV-IQL to behavioral datasets from different species (e.g., rodents, primates) to assess generalizability across different cognitive architectures.

2. **Continuous intention baseline**: Compare against a continuous reward variation model (e.g., DIRL) on the same tasks to quantify the trade-off between interpretability and model flexibility.

3. **Intervention experiment**: Test whether manipulating environmental conditions that should shift intentions (e.g., hunger state) produces corresponding shifts in the inferred latent states, providing causal validation of the model.