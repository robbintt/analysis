---
ver: rpa2
title: 'CCAE: A Corpus of Chinese-based Asian Englishes'
arxiv_id: '2310.05381'
source_url: https://arxiv.org/abs/2310.05381
tags:
- english
- language
- corpus
- ccae
- englishes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CCAE, the first open-access web corpus for
  Chinese-based Asian English varieties. The authors collect and process 340 million
  tokens from 448,000 web documents across six regions: Chinese mainland, Hong Kong,
  Macao, Taiwan, Malaysia, and Singapore.'
---

# CCAE: A Corpus of Chinese-based Asian Englishes

## Quick Facts
- arXiv ID: 2310.05381
- Source URL: https://arxiv.org/abs/2310.05381
- Reference count: 37
- Corpus: First open-access web corpus for Chinese-based Asian English varieties with 340 million tokens from 448,000 documents

## Executive Summary
This paper introduces CCAE, a novel web corpus containing 340 million tokens from Chinese-based Asian English varieties across six regions. The corpus is constructed through systematic web scraping with variety-specific queries and includes comprehensive metadata for traceability. The authors demonstrate CCAE's utility through preliminary experiments in language modeling and automatic variety identification, showing improved perplexity scores and reasonable classification performance. The corpus is made publicly available under a CC-BY-NC-ND 4.0 license.

## Method Summary
The corpus is built by collecting web documents using Google Advanced Search with region-specific trigram queries across Chinese mainland, Hong Kong, Macao, Taiwan, Malaysia, and Singapore. Documents undergo parsing with JusText to extract text bodies, followed by quality filtering and document-level deduplication. For language modeling experiments, GPT-2 models are trained using zero-shot prompting and fine-tuning on mixed and variety-specific training sets. Automatic variety identification is performed using Longformer fine-tuned as a baseline classifier.

## Key Results
- CCAE contains 340 million tokens from 448,000 web documents across six Asian English varieties
- Language models trained on CCAE show improved perplexity scores compared to zero-shot prompting
- Automatic variety identification achieves reasonable classification performance with precision, recall, and F1 scores reported for each variety

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The corpus improves language modeling performance by providing region-specific training data that captures linguistic variation in Chinese-based Asian Englishes.
- Mechanism: Language models trained on region-specific data learn patterns and features unique to each variety, reducing perplexity when evaluated on the same variety.
- Core assumption: Regional linguistic variation is sufficient to distinguish varieties and improve model performance.
- Evidence anchors:
  - [abstract] "preliminary experiments demonstrate CCAE's utility for language modeling and automatic variety identification tasks, showing improved perplexity scores and reasonable classification performance"
  - [section 5.1] "we conduct a preliminary experiment on the task of language modeling. We investigate different experimental settings on multi-variety Asian English through perplexity computations by GPT-2"
  - [corpus] Weak evidence - corpus provides variety-specific data but no direct link to perplexity improvement shown in corpus stats
- Break condition: If regional variation is too subtle for current models to capture, or if the corpus lacks sufficient domain diversity to represent real-world usage.

### Mechanism 2
- Claim: Metadata including source URLs and publication dates enables traceability and compliance with data protection regulations.
- Mechanism: Each document's origin can be verified, allowing researchers to respect withdrawal rights and exclude blacklisted websites.
- Core assumption: Maintaining origin traceability is technically feasible and valuable for researchers.
- Evidence anchors:
  - [abstract] "The corpus includes metadata like source URLs and publication dates"
  - [section 1] "It maintains the traceability of each document to its origin. This level of traceability makes it possible for researchers to apply the withdrawal right of individual website owners"
  - [corpus] Direct evidence - corpus output format includes "URL" and "Time" fields in JSON structure
- Break condition: If metadata becomes decoupled from documents during processing, or if URL structures change making original sources inaccessible.

### Mechanism 3
- Claim: Deduplication and cleaning processes improve data quality for downstream NLP tasks.
- Mechanism: Removing duplicate and low-quality documents ensures models train on representative, meaningful text rather than noise.
- Core assumption: Document-level deduplication significantly improves data quality without losing important linguistic variation.
- Evidence anchors:
  - [abstract] "The corpus includes metadata like source URLs and publication dates"
  - [section 4.2] "Data quality is key when building a corpus. In this section, we discuss the corpus' data pre-processing from three aspects including parsing, cleaning, and deduplication"
  - [corpus] Assumption - corpus description mentions deduplication but doesn't quantify quality improvement

## Foundational Learning

- Concept: Web corpus construction and data cleaning
  - Why needed here: The corpus is built from web-scraped data requiring deduplication, cleaning, and parsing to create usable NLP resources
  - Quick check question: What are the three main pre-processing steps mentioned for the corpus?

- Concept: Language modeling and perplexity metrics
  - Why needed here: The paper evaluates CCAE's utility through language modeling experiments measuring perplexity scores
  - Quick check question: What does lower perplexity indicate about a language model's performance?

- Concept: Automatic variety identification as text classification
  - Why needed here: The corpus enables experiments in distinguishing between six Asian English varieties as a classification task
  - Quick check question: Why is automatic variety identification more challenging than general language identification?

## Architecture Onboarding

- Component map: Data collection → URL gathering with Selenium → Web scraping → JusText parsing → Cleaning/deduplication → JSON output with metadata
- Critical path: The pipeline must maintain document-origin traceability from raw HTML through to final JSON output
- Design tradeoffs: Large-scale web crawling vs. data quality control; comprehensive metadata vs. storage efficiency
- Failure signatures: High perplexity scores indicate model struggles with variety-specific features; poor classification performance suggests insufficient linguistic differentiation in corpus
- First 3 experiments:
  1. Train language model on entire corpus vs. individual varieties to compare perplexity improvements
  2. Test automatic variety identification with different model architectures (Longformer vs. alternatives)
  3. Analyze perplexity variation across different time periods using the metadata to study language evolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models trained on CCAE compare to those trained on other existing corpora for Chinese-based Asian English varieties?
- Basis in paper: [explicit] The authors compare CCAE with four other corpora (GloWbE, ICE, ACE, and WikiText) in Table 1, but do not provide direct performance comparisons for language modeling tasks.
- Why unresolved: While the authors present preliminary experiments on language modeling and variety identification using CCAE, they do not benchmark these results against models trained on other relevant corpora.
- What evidence would resolve it: Direct performance comparisons of language models trained on CCAE versus other corpora for Chinese-based Asian English varieties on standard evaluation tasks and metrics.

### Open Question 2
- Question: What is the impact of corpus size and domain diversity on the performance of language models for Chinese-based Asian English varieties?
- Basis in paper: [inferred] The authors mention that CCAE is the first and largest open-access web corpus for Chinese-based Asian English varieties, and they discuss the importance of domain diversity in Section 4.1.
- Why unresolved: The authors do not conduct experiments to systematically investigate the relationship between corpus size, domain diversity, and language model performance.
- What evidence would resolve it: Experiments varying the size and domain composition of training corpora for Chinese-based Asian English varieties and measuring the impact on language model performance.

### Open Question 3
- Question: How well do the variety labels in CCAE capture the linguistic nuances and similarities between different Chinese-based Asian English varieties?
- Basis in paper: [explicit] The authors discuss the importance of variety accuracy in Section 4.1 and present preliminary results for automatic variety identification in Section 5.2.
- Why unresolved: While the authors provide some evidence for the quality of variety labels through manual inspection, they do not conduct a comprehensive linguistic analysis of the similarities and differences between the labeled varieties.
- What evidence would resolve it: A detailed linguistic analysis of the Chinese-based Asian English varieties in CCAE, comparing the labeled varieties to linguistic descriptions and investigating cases of ambiguity or misclassification.

## Limitations
- The language modeling experiments lack comparison to more modern transformer architectures beyond GPT-2
- Data collection may suffer from sampling bias due to reliance on specific Google Advanced Search queries
- Deduplication process is mentioned but not quantified in terms of redundancy removed or potential linguistic variation lost

## Confidence

**High Confidence**: The corpus construction methodology and data collection process are well-documented and technically sound. The inclusion of metadata and the choice of six representative Asian English varieties are appropriate and clearly explained.

**Medium Confidence**: The reported improvements in language modeling perplexity and variety identification performance are plausible given the corpus design, but the experimental setup lacks comprehensive baselines and statistical validation that would strengthen these claims.

**Low Confidence**: Claims about the corpus's utility for broader NLP tasks beyond the two evaluated applications are speculative. The paper doesn't demonstrate performance on downstream tasks like machine translation or information retrieval that would benefit from variety-specific language models.

## Next Checks

1. **Statistical Validation of Perplexity Improvements**: Conduct paired t-tests or bootstrap confidence intervals on the perplexity scores across different model configurations to establish whether observed improvements are statistically significant rather than due to random variation in the validation sets.

2. **Architecture Ablation Study**: Replicate the language modeling experiments using modern transformer architectures (e.g., BERT, RoBERTa, or domain-specific models) to determine whether the reported improvements are specific to GPT-2 or generalize to other model families.

3. **Domain Diversity Analysis**: Perform stratified sampling of the corpus by domain (news, blogs, academic, etc.) and evaluate whether perplexity and classification performance vary significantly across domains, indicating potential sampling biases in the data collection process.