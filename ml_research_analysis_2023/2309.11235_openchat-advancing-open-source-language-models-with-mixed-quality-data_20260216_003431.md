---
ver: rpa2
title: 'OpenChat: Advancing Open-source Language Models with Mixed-Quality Data'
arxiv_id: '2309.11235'
source_url: https://arxiv.org/abs/2309.11235
tags:
- data
- language
- arxiv
- gpt-4
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenChat introduces C-RLFT, a novel framework for fine-tuning open-source
  language models using mixed-quality data without preference labels. By treating
  different data sources as coarse-grained rewards and learning a class-conditioned
  policy, C-RLFT compensates for imperfect reward signals through lightweight, single-stage
  supervised learning.
---

# OpenChat: Advancing Open-source Language Models with Mixed-Quality Data

## Quick Facts
- arXiv ID: 2309.11235
- Source URL: https://arxiv.org/abs/2309.11235
- Reference count: 15
- OpenChat-13b achieves the highest average performance among 13B open-source models on AlpacaEval (89.5), MT-bench (57.5), and Vicuna-bench (85.0), surpassing even 70B models.

## Executive Summary
OpenChat introduces C-RLFT, a novel framework that fine-tunes open-source language models using mixed-quality data without requiring preference labels. By treating different data sources as coarse-grained rewards and learning a class-conditioned policy, C-RLFT compensates for imperfect reward signals through lightweight, single-stage supervised learning. The approach enables effective utilization of datasets with varying quality levels, achieving state-of-the-art performance among 13B models while demonstrating superior generalization capabilities.

## Method Summary
C-RLFT fine-tunes pre-trained LLMs using mixed-quality datasets by encoding different data sources as coarse-grained reward classes. The framework learns a class-conditioned policy that adapts behavior based on data quality, using class-specific prompt tokens (e.g., "GPT4 User:" vs "GPT3 User:") and KL-regularized reinforcement learning with a class-conditioned reference policy. This approach avoids the need for expensive human preference data while leveraging the quality information inherent in source differentiation.

## Key Results
- OpenChat-13b achieves highest average performance among 13B models on three standard benchmarks
- Wins AlpacaEval with 89.5% win rate, surpassing both 13B and larger models like llama-2-chat-70B
- Only 13B model to outperform its base model on AGIEval, demonstrating superior generalization without overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-RLFT leverages coarse-grained rewards without explicit preference labels.
- Mechanism: Different data sources are treated as implicit reward classes (e.g., GPT-4 vs GPT-3.5), avoiding the need for pairwise or ranking-based human preference data.
- Core assumption: Quality differences between data sources can be encoded as simple class labels and used as weak reward signals.
- Evidence anchors: [abstract] "regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information." [section 3.1] "According to the different overall quality with respect to class labels, we can naturally encode coarse-grained rewards rc(x, y) in Dc as follows: rc(xi, yi) = 1, if (xi, yi) ∈ Dexp; α, if (xi, yi) ∈ Dsub (α < 1)."
- Break condition: If data sources cannot be reliably differentiated by quality, the coarse-grained reward signal becomes meaningless.

### Mechanism 2
- Claim: Class-conditioned policy learns to differentiate expert vs sub-optimal data distributions.
- Mechanism: Conditioning the policy on source class labels allows the model to adapt its behavior based on data quality, rather than treating all data uniformly.
- Core assumption: The base LLM lacks sufficient prior knowledge to distinguish between high and low quality responses.
- Evidence anchors: [abstract] "we learn the fine-tuned LLM itself as a class-conditioned policy (i.e., conditioning data source classes with distinct prompt tokens), and regularize it with a better and more informative class-conditioned reference policy instead of the original pre-trained LLM." [section 3.2] "This can be easily implemented by conditioning each example from different data sources using distinct initial prompt tokens."
- Break condition: If prompt tokens fail to effectively condition the model, the class-conditioned advantage is lost.

### Mechanism 3
- Claim: KL-regularized RL with class-conditioned reference policy compensates for imperfect reward signals.
- Mechanism: By regularizing the fine-tuned policy with a class-conditioned behavior policy (πc) instead of the base model (π0), C-RLFT provides richer information for learning.
- Core assumption: The class-conditioned behavior policy πc contains higher quality information than the base pre-trained LLM π0.
- Evidence anchors: [section 3.2] "We adopt this design for the following reasons. First, for most existing open-source pre-trained LLMs, their performance in many cases is still inferior to API-based models. This means that even the Dsub data collected from GPT-3.5 are likely to have higher quality than π0." [section 3.2] "Second, πc contains additional data source information, which can provide extra information to help differentiate the quality of data."
- Break condition: If the class-conditioned reference policy is not significantly better than π0, the regularization benefit diminishes.

## Foundational Learning

- Concept: Supervised Fine-tuning (SFT)
  - Why needed here: C-RLFT builds on SFT as a baseline, but addresses its limitations with mixed-quality data.
  - Quick check question: What is the key limitation of SFT when applied to datasets with mixed quality?

- Concept: Reinforcement Learning Fine-tuning (RLFT)
  - Why needed here: C-RLFT modifies the KL-regularized RL framework to avoid the need for expensive human preference data.
  - Quick check question: What is the primary challenge in applying standard RLFT to open-source LLMs?

- Concept: Offline Reinforcement Learning
  - Why needed here: C-RLFT is inspired by goal-conditioned supervised learning in offline RL, where conditioning on proper information can recover optimized performance.
  - Quick check question: How does conditioning on auxiliary information (like goals or classes) help in offline RL?

## Architecture Onboarding

- Component map: Pre-trained LLM (π0) → Class-conditioned dataset (Dc) → Class-conditioned policy (πθ) → Regularization with πc → Reward-weighted regression objective
- Critical path: Data conditioning → Policy conditioning → KL-regularization → Reward weighting
- Design tradeoffs: Simpler reward encoding (coarse-grained) vs. potential loss of fine-grained quality distinctions; conditioning on source class vs. direct quality labels
- Failure signatures: If conditioning tokens are ineffective, model performance degrades to SFT baseline; if reward weighting is too strong/weak, model may overfit/underfit certain classes
- First 3 experiments:
  1. Train with uniform reward (no conditioning) to confirm SFT baseline degradation.
  2. Train with class conditioning but no reward weighting to test policy differentiation.
  3. Train with reward weighting but no class conditioning to test reward signal effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OpenChat-13b compare to larger models like llama-2-chat-70b on AGIEval, and what does this suggest about the scalability of C-RLFT?
- Basis in paper: [inferred] The paper mentions that OpenChat-13b outperforms its base model llama-2-13b on AGIEval, but doesn't compare to larger models.
- Why unresolved: The paper only compares OpenChat-13b to other 13B models on AGIEval, leaving the question of how it performs relative to larger models unanswered.
- What evidence would resolve it: Additional experiments comparing OpenChat-13b to larger models like llama-2-chat-70b on AGIEval would provide insights into the scalability and effectiveness of C-RLFT for models of different sizes.

### Open Question 2
- Question: What is the optimal value for the coarse-grained reward α in C-RLFT, and how does it impact model performance?
- Basis in paper: [explicit] The paper mentions using α < 1 for the coarse-grained reward, but doesn't explore the impact of different values.
- Why unresolved: The paper sets α to a fixed value without investigating how varying α affects the model's ability to leverage mixed-quality data.
- What evidence would resolve it: Conducting experiments with different values of α and measuring their impact on performance metrics like win rate and accuracy would help determine the optimal reward scaling factor.

### Open Question 3
- Question: How does the class-conditioned policy in C-RLFT affect the model's ability to generalize to unseen tasks and domains?
- Basis in paper: [inferred] The paper mentions using class-conditioned prompts during training and inference, but doesn't explicitly analyze the impact on generalization.
- Why unresolved: While the paper demonstrates improved performance on AGIEval, it doesn't directly attribute this to the class-conditioned policy or explore its role in generalization.
- What evidence would resolve it: Ablation studies comparing C-RLFT to variants without class-conditioned policies on diverse benchmarks like AGIEval and new task categories would reveal the impact on generalization.

### Open Question 4
- Question: How robust is OpenChat-13b to variations in the quality and size of the mixed-quality dataset, and what are the limitations of the C-RLFT approach?
- Basis in paper: [explicit] The paper conducts some analysis on data size variations but doesn't fully explore the robustness of the model.
- Why unresolved: The paper provides limited insights into how OpenChat-13b performs when the mixed-quality dataset has significant variations in quality distribution or size.
- What evidence would resolve it: Extensive experiments varying the proportion and quality of expert vs. sub-optimal data, as well as the total dataset size, would reveal the robustness and limitations of C-RLFT in handling diverse data scenarios.

## Limitations

- The coarse-grained reward system may not generalize well to datasets with more nuanced quality gradations beyond simple binary distinctions.
- The effectiveness of class-conditioning relies heavily on the ability of prompt tokens to influence model behavior, with limited discussion of potential saturation or interference effects.
- The framework's generalizability across different model architectures remains untested, as experiments are limited to a single base model (LLaMA-2-13B).

## Confidence

- **C-RLFT Framework Effectiveness (High)**: The claim that C-RLFT can achieve state-of-the-art performance on 13B models is strongly supported by empirical results across three benchmarks. The methodology is clearly described and reproducible.
- **Coarse-grained Reward Sufficiency (Medium)**: While the paper demonstrates success with binary quality classes, confidence is lower regarding performance with more granular quality distinctions or noisier data.
- **Class-conditioned Policy Advantage (Medium)**: The theoretical justification is sound, but the practical impact of class-conditioning versus simpler approaches (like weighted SFT) could benefit from more ablation studies.
- **KL-regularization Benefit (Medium)**: The paper provides reasonable arguments for why the class-conditioned reference policy is superior to the base model, but direct comparisons with alternative regularization strategies are limited.

## Next Checks

1. **Prompt Token Ablation Study**: Systematically vary the position, format, and number of class-conditioning tokens to determine optimal implementation and test sensitivity to prompt design choices.

2. **Multi-class Quality Gradient Test**: Implement C-RLFT with a dataset containing multiple quality levels (e.g., GPT-4, GPT-3.5, GPT-3.0, human) to verify the framework's effectiveness beyond binary quality distinctions.

3. **Base Model Architecture Generalization**: Apply C-RLFT to fine-tune different base models (e.g., Mistral, Vicuna) and compare performance consistency to assess the framework's generalizability across model architectures.