---
ver: rpa2
title: Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical
  Parts Explanations
arxiv_id: '2308.08162'
source_url: https://arxiv.org/abs/2308.08162
tags:
- prototypical
- image
- protopnet
- part
- parts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and quantifies a critical spatial misalignment\
  \ issue in prototypical parts-based neural networks, where the receptive field of\
  \ a prototypical part activation region can depend on image areas outside the activated\
  \ region, leading to misleading explanations. The authors introduce a comprehensive\
  \ interpretability benchmark with three metrics\u2014Prototypical part Location\
  \ Change (PLC), Activation Change (PAC), and Rank Change (PRC)\u2014to measure this\
  \ misalignment through adversarial modifications."
---

# Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations

## Quick Facts
- arXiv ID: 2308.08162
- Source URL: https://arxiv.org/abs/2308.08162
- Reference count: 40
- Key outcome: Novel interpretability benchmark identifies and quantifies spatial misalignment in prototypical parts-based networks, with compensation methodology significantly improving alignment while maintaining accuracy.

## Executive Summary
This paper addresses a critical interpretability issue in prototypical parts-based neural networks, where the receptive field of prototypical part activations can extend beyond the visible activation region, leading to misleading explanations. The authors introduce a comprehensive benchmark with three metrics (PLC, PAC, PRC) to quantify spatial misalignment through adversarial modifications. They propose a compensation methodology that enforces spatial alignment through a two-pass training approach with spatial alignment loss and masking augmentation. Experiments on CUB-200-2011 demonstrate significant improvements in alignment metrics while maintaining model accuracy.

## Method Summary
The method introduces a compensation methodology that enforces spatial alignment in prototypical parts-based networks through two-pass training. The first pass computes similarity maps normally, while the second pass computes similarity maps on a masked version where only the activated region is visible. A spatial alignment loss penalizes discrepancies between these similarity maps. Additionally, masking augmentation during training encourages the model to learn more locally-focused prototypical parts by randomly modifying regions outside the activation area. The approach is evaluated using a novel interpretability benchmark that quantifies spatial misalignment through adversarial modifications of non-activated regions.

## Key Results
- Spatial misalignment is a fundamental issue in prototypical parts networks, with receptive fields extending beyond visible activation regions
- Compensation methodology reduces PLC from 24.0 to 8.5, PRC from 13.5 to 1.6, and PAC from 23.7 to 1.7 on ProtoPNet with λalign=10 and masking augmentation
- Model accuracy is maintained while significantly improving the faithfulness of prototypical part visualizations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The spatial misalignment occurs because the similarity maps are computed in the penultimate layer, causing the receptive field of a prototypical part activation to depend on image areas outside the activated region.
- **Mechanism:** When a prototypical part's activation is computed in a deep layer, its receptive field extends beyond the localized activation region visible in the similarity map. This causes changes in non-activated areas to influence the activation and location of the prototypical part, leading to misleading explanations.
- **Core assumption:** The receptive field of a neuron in the penultimate layer can extend beyond the spatial region where it achieves maximum activation.
- **Evidence anchors:**
  - [abstract]: "However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations."
  - [section 3]: "The receptive field of the prototypical part activation region often depends on parts of the image outside this region."
  - [corpus]: Weak evidence - no direct mention of receptive field mechanics in neighboring papers.
- **Break condition:** If the network architecture changes such that similarity maps are computed in an earlier layer with smaller receptive fields, or if the model uses local attention mechanisms that constrain receptive fields.

### Mechanism 2
- **Claim:** The compensation methodology enforces spatial alignment by comparing similarity maps computed from the original image and a masked version where only the activated region is visible.
- **Mechanism:** By passing the image through the network twice—once normally and once with a mask that blocks areas outside the activation region—the method ensures that the similarity map remains consistent regardless of changes outside the activated region. The spatial alignment loss penalizes discrepancies between these two similarity maps.
- **Core assumption:** If the model is properly aligned, the similarity map should not change when the input is masked to only show the activated region.
- **Evidence anchors:**
  - [section 5]: "To formalize our approach, let us assume that the considered prototypical part model consists of a backbone convolutional network f used to calculate the similarity map si = sim(pi, f(x)). Similar to the standard prototypical parts-based approaches, the similarity maps are aggregated and classified, as presented in Fig. 5. Additionally, we interpolate bilinearly the detached si to the input resolution and binarize it so that positive values correspond to activation value above the 90th percentile, obtaining mi. This mask is used to generate a masked input ¯xi = x · mi that is used in the second pass of the model."
  - [section 5]: "For aligned explanation, ¯si should be very similar to si because the area outside the activation region should not influence the final results. Therefore, we introduce a spatial alignment loss function, which penalizes the model for not fulfilling this condition Lalign = ∥si − ¯si∥2."
  - [corpus]: No direct evidence - this is a novel compensation methodology.
- **Break condition:** If the masking operation itself introduces artifacts that the model learns to exploit, or if the model architecture makes it impossible to compute similarity maps at the required resolution.

### Mechanism 3
- **Claim:** Masking augmentation during training further improves spatial alignment by encouraging the model to learn more locally-focused prototypical parts.
- **Mechanism:** During training, random rectangular regions outside the activation area are modified (set to gray, random noise, or with added noise). This augmentation forces the model to rely more heavily on the activated region for classification, reducing dependence on contextual information from outside the region.
- **Core assumption:** Adding noise to regions outside the activation area will not change the classification decision if the model is properly aligned, but will encourage it to focus on the activated region.
- **Evidence anchors:**
  - [section 5]: "To further prevent explanation misalignment, we consider a special type of augmentation during model training (Fig. 6). For every training image, we apply masking with a given probability. For the modified samples, we randomly select a number of rectangular regions together with their widths, heights, and locations. We randomly modify each region, either by adding noise or by replacing the region with gray color or random noise."
  - [section 7]: "In terms of computations, masking augmentation has minimal impact on model training. On the other hand, computing the spatial alignment loss necessitates an additional pass, leading to an average 40% increase in training time."
  - [corpus]: No direct evidence - this is a novel augmentation technique specific to this work.
- **Break condition:** If the augmentation is too aggressive and degrades overall model performance, or if the model learns to ignore the augmentation entirely.

## Foundational Learning

- **Concept:** Receptive field in convolutional neural networks
  - Why needed here: Understanding why similarity maps computed in deep layers can have receptive fields extending beyond the activated region is crucial to grasping the spatial misalignment problem.
  - Quick check question: If a convolutional layer has a receptive field of 50x50 pixels, can it still be activated by a 10x10 patch in the input image? Why or why not?

- **Concept:** Adversarial examples and projected gradient descent
  - Why needed here: The benchmark uses adversarial modifications to quantify spatial misalignment, requiring understanding of how adversarial attacks work.
  - Quick check question: What is the key difference between standard adversarial attacks and the modifications used in this benchmark?

- **Concept:** Prototype-based learning and similarity metrics
  - Why needed here: The entire framework is built on prototypical parts networks, which rely on comparing image regions to learned prototypes using specific similarity metrics.
  - Quick check question: How does the similarity function used in ProtoPNet (log |zj - p|2 + 1 / |zj - p|2 + η) differ from a simple L2 distance?

## Architecture Onboarding

- **Component map:** Backbone CNN -> Feature extraction -> Prototypical parts layer -> Similarity map computation -> Spatial alignment module -> Masking augmentation module -> Classification

- **Critical path:**
  1. Forward pass through backbone to get feature maps
  2. Compute similarity maps between feature patches and all prototypes
  3. Find maximum activation and corresponding prototypical part
  4. Generate mask from 90th percentile of similarity map
  5. Create masked input and pass through network again
  6. Compute spatial alignment loss between original and masked similarity maps
  7. Apply masking augmentation with 50% probability
  8. Backpropagate combined loss (original + alignment + augmentation)

- **Design tradeoffs:**
  - Computational cost vs. alignment quality: The spatial alignment loss requires an additional forward pass, increasing training time by ~40%
  - Mask binarization vs. smoothness: Binarized masks are more effective than smooth masks for alignment but may introduce discontinuities
  - Augmentation strength vs. performance: Too much augmentation can hurt accuracy, too little won't improve alignment

- **Failure signatures:**
  - Metrics don't improve despite increased λalign: May indicate optimization issues or that the model has reached its alignment capacity
  - Accuracy drops significantly with masking augmentation: Augmentation may be too aggressive or masking strategy needs adjustment
  - PLC improves but PAC worsens: Location alignment improved but activation sensitivity remains, suggesting partial alignment

- **First 3 experiments:**
  1. Implement the spatial alignment loss component and verify it reduces when using a fully aligned synthetic dataset
  2. Test the adversarial modification benchmark on a baseline ProtoPNet to establish baseline misalignment metrics
  3. Add masking augmentation to training and measure its effect on alignment metrics independently of the alignment loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spatial misalignment compensation method affect the interpretability of prototypical parts in domains beyond image classification, such as medical imaging or time-series analysis?
- Basis in paper: [inferred] The paper demonstrates effectiveness on CUB-200-2011 and Stanford Cars datasets but does not explore other domains where prototypical parts are used.
- Why unresolved: The method's generalizability to other domains remains untested, particularly where prototypical parts might interact with different types of data or explainability requirements.
- What evidence would resolve it: Experimental validation showing improvements in interpretability metrics for prototypical parts-based models applied to medical imaging, time-series, or graph data.

### Open Question 2
- Question: What is the optimal balance between spatial alignment loss weight (λalign) and masking augmentation to achieve both interpretability and model accuracy?
- Basis in paper: [explicit] The paper explores various λalign values but shows conflicting results when masking augmentation is added, with some metrics improving and others not.
- Why unresolved: The paper demonstrates that higher λalign values generally improve spatial alignment, but the optimal combination with masking augmentation remains unclear, especially across different models.
- What evidence would resolve it: Systematic ablation studies across multiple prototypical parts-based models showing the sweet spot where interpretability gains are maximized without sacrificing accuracy.

### Open Question 3
- Question: Why does TesNet demonstrate superior robustness to spatial misalignment compared to other prototypical parts-based models?
- Basis in paper: [explicit] The paper notes TesNet's exceptional performance but attributes it to its prototype similarity function, which uses projection onto prototype vectors rather than L2-distance.
- Why unresolved: While the paper hypothesizes about TesNet's robustness, it does not conduct controlled experiments to isolate the specific architectural or functional differences responsible for this property.
- What evidence would resolve it: Comparative experiments where TesNet's similarity function is applied to other prototypical parts-based models, or where other models adopt TesNet's similarity function to test if the robustness transfers.

## Limitations

- Computational overhead: Spatial alignment compensation increases training time by approximately 40% due to the additional forward pass required
- Limited domain validation: The method is primarily validated on fine-grained image classification datasets (CUB-200-2011, Stanford Cars) without exploring other domains
- Adversarial benchmark limitations: The spatial misalignment benchmark relies on adversarial modifications that may not capture all real-world scenarios where misalignment manifests

## Confidence

- **High Confidence**: The identification of spatial misalignment as a fundamental issue in prototypical parts networks is well-supported by theoretical analysis and experimental evidence.
- **Medium Confidence**: The compensation methodology effectively improves alignment metrics, though the trade-off between alignment improvement and computational cost needs further evaluation.
- **Medium Confidence**: The adversarial benchmark provides a rigorous quantitative framework, but its sensitivity to hyperparameters (perturbation size, iterations) could affect results.

## Next Checks

1. **Ablation study**: Systematically vary masking augmentation parameters (region sizes, noise types, probability) to identify optimal configuration and quantify marginal benefits.

2. **Real-world validation**: Apply the benchmark to naturally occurring misclassifications rather than just adversarial examples to assess practical relevance.

3. **Cross-architecture generalization**: Test the compensation methodology on additional network architectures (e.g., Vision Transformers) and datasets to evaluate robustness beyond CNNs and fine-grained classification.