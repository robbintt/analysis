---
ver: rpa2
title: Guiding Image Captioning Models Toward More Specific Captions
arxiv_id: '2307.16686'
source_url: https://arxiv.org/abs/2307.16686
tags:
- captions
- image
- guidance
- captioning
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Classifier-free guidance (CFG) can generate more specific image
  captions by increasing the probability of image-caption associations. The guidance
  scale controls a trade-off between caption likelihood and image-caption relevance.
---

# Guiding Image Captioning Models Toward More Specific Captions

## Quick Facts
- arXiv ID: 2307.16686
- Source URL: https://arxiv.org/abs/2307.16686
- Reference count: 40
- Classifier-free guidance (CFG) improves CLIPScore from 0.775 to 0.808 and recall@1 from 26.5% to 44.6% at guidance scale 2

## Executive Summary
This paper investigates using classifier-free guidance (CFG) and language model (LM) guidance to generate more specific image captions. CFG modifies decoding logits by interpolating between conditional and unconditional distributions, with the guidance scale controlling the trade-off between maximizing caption likelihood and image-caption relevance. The approach significantly improves reference-free metrics like CLIPScore and caption-to-image retrieval but reduces reference-based metrics like CIDEr. Language model guidance can further improve captioning quality, particularly for models trained on low-quality web data, by compensating for CFG's grammaticality loss.

## Method Summary
The method fine-tunes a CoCa-Base bottleneck model with conditioning masking (0.5 proportion) on JFT-5B and ALIGN datasets, then applies classifier-free guidance during decoding by interpolating conditional and unconditional logits with a guidance scale γ. Language model guidance further reweights logits using an external LM. The model is evaluated on MS-COCO using CLIP-based metrics (CLIPScore, caption-to-image retrieval recall@1) and traditional reference-based metrics (CIDEr, BLEU-4, METEOR, ROUGE).

## Key Results
- CFG with scale 2 improves CLIPScore from 0.775 to 0.808 and recall@1 from 26.5% to 44.6%
- CFG degrades reference-based metrics: CIDEr drops from 126.1 to 78.6 at scale 2
- Language model guidance provides small improvements over CFG's Pareto frontier of reference-free vs. reference-based metrics

## Why This Works (Mechanism)

### Mechanism 1
CFG shifts caption generation toward maximizing image-caption mutual information by inflating the ratio of conditional to unconditional caption probabilities. At each decoding step, CFG modifies logits by adding a scaled difference between conditional and unconditional distributions: `scores = uncond_log_probs + gamma * (cond_log_probs - uncond_log_probs)`. This steers sampling toward captions that are more specific to the image context. The mechanism assumes conditional and unconditional distributions share similar token likelihoods for generic captions, so the difference term isolates image-specific tokens.

### Mechanism 2
CFG degrades grammaticality at high scales because it prioritizes specificity over fluency. As guidance scale increases, the relative weight of the difference term dominates, pushing sampling toward rare, image-specific tokens that may not fit natural sentence structure. This effect is amplified in greedy decoding, which lacks stochastic exploration to correct local errors. The mechanism assumes conditional model's logits for generic tokens are similar to unconditional model's, so their difference is small; however, the difference for specific nouns/attributes is large, causing CFG to over-emphasize them.

### Mechanism 3
Language model guidance compensates for CFG's grammaticality loss by injecting a prior over fluent captions. LM guidance reweights conditional logits by adding LM's token probabilities before decoding: `scores = lm_log_probs + alpha * cond_log_probs - beta * uncond_log_probs`. The LM prior suppresses unlikely or nonsensical tokens that CFG might otherwise select. This mechanism assumes the LM was trained on diverse text corpus and thus assigns higher probability to fluent, grammatical sequences, even if less image-specific.

## Foundational Learning

- **Conditional vs. unconditional probability distributions in autoregressive models**: CFG relies on the difference between `p(x|y)` and `p(x)` to isolate image-specific tokens. Quick check: In a trained captioner, what is the typical difference between conditional and unconditional probabilities for function words like "the" or "in"?

- **Autoregressive factorization and greedy decoding**: The paper uses greedy decoding with CFG, which deterministically picks the highest-scoring token at each step. Quick check: How does greedy decoding differ from beam search in terms of exploration and fluency?

- **Pointwise mutual information (PMI) and image-caption specificity**: CFG effectively increases PMI by inflating the ratio `p(x|y)/p(x)`; LM guidance uses PMI-like weighting with decoupled exponents. Quick check: If `p(x|y)/p(x)` is high for a caption, what does that imply about its specificity to the image?

## Architecture Onboarding

- **Component map**: Image → ViT-B/18 encoder → Multimodal CoCa-Base bottleneck decoder → CFG-modified logits → Greedy token selection → CLIP similarity evaluation
- **Critical path**: Image encoding → Multimodal text decoding with CFG → Greedy token selection → CLIP-based evaluation
- **Design tradeoffs**: CFG trades grammaticality for specificity; LM guidance trades specificity for fluency; bottleneck architecture simplifies inversion of image embeddings
- **Failure signatures**: At high CFG scales, captions contain repeated or nonsense tokens; LM guidance may hallucinate details not present in image; both methods may produce captions far from ground truth references
- **First 3 experiments**:
  1. Train CoCa-Base on JFT-5B + ALIGN with conditioning masking (0.5) for 20k steps; evaluate greedy decoding without guidance
  2. Apply CFG with γ ∈ {1.2, 1.5, 2.0} on same model; measure CLIPScore vs. CIDEr trade-off
  3. Add LM guidance with small prompt set (5 MS-COCO captions); compare against CFG at optimal γ for reference-free vs. reference-based metrics

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mechanism by which CFG improves specificity without substantially altering caption length at moderate guidance scales (γ ∈ [1, 2])? The paper states that at low guidance strengths, CFG improves recall by making more efficient use of words, rather than by producing more verbose captions, but does not explain the linguistic or semantic mechanisms that allow this efficiency gain.

### Open Question 2
How does the choice of bottleneck architecture versus attention pooling architecture affect performance and interpretability of CFG in image captioning? The paper reports that attention pooling architecture performs slightly better across both reference-based and reference-free evaluations, but does not explore why one might be preferable for CFG.

### Open Question 3
What are the limitations of using CLIP-based metrics as measures of caption specificity, and how do these limitations affect interpretation of CFG results? The paper acknowledges that treating image captioning as generating captions close to image in embedding space is inadequate because such captions need not be grammatical and may contain gibberish.

## Limitations

- The improvement in reference-free metrics may reflect overfitting to CLIP-based evaluation criteria rather than genuine specificity gains, as human validation beyond breed-counting is limited.
- The fundamental assumption that CFG isolates image-specific tokens by leveraging conditional-unconditional logit differences lacks robust corpus evidence and empirical validation.
- Language model guidance implementation details are insufficient, making it unclear whether improvements stem from genuine fluency enhancement or methodological artifacts.

## Confidence

**High Confidence**: The core finding that CFG with guidance scale 2 improves CLIPScore (0.808 vs. 0.775) and caption-to-image retrieval (recall@1 44.6% vs. 26.5%) while degrading reference-based metrics (CIDEr 78.6 vs. 126.1) is well-supported by systematic evaluation.

**Medium Confidence**: The mechanism explanation that CFG increases image-caption mutual information by inflating conditional-to-unconditional probability ratios is plausible but lacks direct empirical validation of underlying token-level assumptions.

**Low Confidence**: The claim that language model guidance provides consistent improvements over CFG's Pareto frontier is weakly supported due to insufficient methodological details about LM implementation.

## Next Checks

1. **Direct Specificity Validation**: Conduct human evaluation studies comparing CFG-generated captions against baseline captions using blinded annotators who rate caption specificity to image content, independent of CLIP-based metrics.

2. **Token-Level Logit Analysis**: Systematically compare conditional and unconditional logit distributions for function words versus content words across multiple guidance scales to test whether CFG selectively amplifies image-specific tokens.

3. **Ablation of LM Guidance Components**: Perform controlled experiments varying LM architecture, training data domain, and integration method to isolate which components contribute to reported improvements and whether results generalize beyond the specific LM used.