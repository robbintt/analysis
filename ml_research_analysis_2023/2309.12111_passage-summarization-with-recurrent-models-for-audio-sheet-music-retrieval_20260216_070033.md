---
ver: rpa2
title: Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval
arxiv_id: '2309.12111'
source_url: https://arxiv.org/abs/2309.12111
tags:
- music
- audio
- sheet
- retrieval
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of cross-modal audio\u2013sheet\
  \ music retrieval by proposing a recurrent neural network that learns joint embeddings\
  \ for longer passages of audio and sheet music. The core idea is to use a cross-modal\
  \ recurrent network with separate audio and sheet music pathways, each processing\
  \ variable-length snippets through convolutional and recurrent layers to produce\
  \ fixed-size embeddings."
---

# Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval

## Quick Facts
- **arXiv ID**: 2309.12111
- **Source URL**: https://arxiv.org/abs/2309.12111
- **Reference count**: 0
- **Primary result**: RNN-FT-CCA model achieves highest retrieval accuracy among tested methods, with R@1 of 0.645 for sheet-to-audio retrieval on real data, reducing performance gap between synthetic and real data.

## Executive Summary
This paper addresses cross-modal audio–sheet music retrieval by proposing a recurrent neural network that learns joint embeddings for longer passages of audio and sheet music. The approach uses separate audio and sheet music pathways, each processing variable-length snippets through convolutional and recurrent layers to produce fixed-size embeddings. By operating with longer music passages, the method only requires weakly aligned data and can handle tempo variations between audio and sheet music. Experiments on synthetic and real data show that the proposed method outperforms baseline approaches, particularly in the sheet music-to-audio direction.

## Method Summary
The proposed method uses a dual-pathway architecture with separate CNN encoders for audio (processing log-magnitude spectrograms) and sheet music (processing scaled images), followed by GRU layers and fully connected layers to produce 64-dimensional embeddings. The model is trained using a contrastive loss function that minimizes distances between embeddings from corresponding passages and maximizes distances between non-corresponding ones. The approach processes variable-length snippets to learn temporal dependencies, summarizes sequences into context vectors, and operates with longer passages to handle tempo variations. Training uses both synthetic and real data from the Multi-Modal Sheet Music Dataset (MSMD) and additional corpora.

## Key Results
- The RNN-FT-CCA model achieves R@1 of 0.645 for sheet-to-audio retrieval on real data, outperforming baseline methods
- The proposed method reduces the performance gap between synthetic and real data compared to the baseline CCA model
- Best performance is achieved with 64-dimensional embeddings, with diminishing returns for higher dimensions
- The model shows better robustness to global tempo variations compared to fixed-size snippet approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The recurrent model learns joint embeddings that can summarize longer passages of corresponding audio and sheet music
- Mechanism: Two independent pathways process variable-length snippets through convolutional and recurrent layers to produce fixed-size embeddings. The recurrent layers (GRUs) summarize sequences of encoded snippets into context vectors
- Core assumption: Temporal dependencies between snippets are important for capturing musical content and can be learned by recurrent networks
- Evidence anchors:
  - [abstract]: "designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages"
  - [section]: "each sequence is fed to a recurrent layer in order to learn the spatial and temporal relations between subsequent snippets"
  - [corpus]: Weak evidence - only 5 related papers found, average neighbor FMR=0.504 suggests moderate relevance
- Break condition: If musical passages are too long for the GRU to maintain context, or if tempo variations are too extreme for the model to handle

### Mechanism 2
- Claim: The method only requires weakly aligned audio-sheet music pairs for training
- Mechanism: By operating with longer music passages, only starting and ending positions of fragments within music documents are needed, rather than fine-grained note-precise alignments
- Core assumption: Longer passages contain enough musical context that weak alignment is sufficient for learning correspondences
- Evidence anchors:
  - [abstract]: "the benefits of our method are that it only requires weakly aligned audio – sheet music pairs"
  - [section]: "by operating with longer music passages, it is possible to rely solely on weakly-annotated data for training"
  - [corpus]: Weak evidence - related papers focus on different aspects of audio-sheet music retrieval
- Break condition: If weak alignment provides insufficient signal for the model to learn meaningful correspondences, or if passages are too short to benefit from weak alignment

### Mechanism 3
- Claim: The recurrent network handles the non-linearities caused by tempo variations between audio and sheet music
- Mechanism: By learning from longer passages rather than fixed-size snippets, the model can learn to match musical content even when tempo differences cause temporal misalignment
- Core assumption: Temporal misalignment due to tempo differences is a key challenge that can be addressed by learning from longer contexts
- Evidence anchors:
  - [abstract]: "the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music"
  - [section]: "a first limitation of this strategy relates to its super-vised nature: it requires strongly-aligned data... which means fine-grained mappings between note onsets and corresponding note positions in the score"
  - [corpus]: Weak evidence - no direct corpus support for this specific mechanism
- Break condition: If tempo variations are too extreme for the recurrent model to learn correspondences, or if the model overfits to tempo patterns in the training data

## Foundational Learning

- Concept: Convolutional neural networks for feature extraction
  - Why needed here: CNNs are used to encode individual audio and sheet music snippets into fixed-size vectors before feeding them to recurrent layers
  - Quick check question: What is the role of batch normalization in the CNN encoders?

- Concept: Recurrent neural networks for sequence modeling
  - Why needed here: GRUs are used to summarize sequences of encoded snippets into context vectors that represent entire passages
  - Quick check question: How do GRUs differ from LSTMs in terms of architecture and performance?

- Concept: Contrastive loss for metric learning
  - Why needed here: The triplet loss function is used to minimize distances between embeddings from corresponding passages and maximize distances between non-corresponding ones
  - Quick check question: What is the purpose of the margin parameter in the triplet loss function?

## Architecture Onboarding

- Component map: Sheet music/audio input → CNN encoding → GRU summarization → FC layer → Embedding vector → Cosine distance computation → Contrastive loss
- Critical path: Sheet music/audio input → CNN encoding → GRU summarization → FC layer → Embedding vector → Cosine distance computation → Contrastive loss
- Design tradeoffs: Longer passages provide more context but may be harder to align; pre-training CNN encoders can improve performance but adds complexity; different embedding dimensions affect retrieval accuracy
- Failure signatures: Poor retrieval accuracy on real data despite good performance on synthetic data; model fails to generalize to tempo variations; embeddings collapse to trivial solutions
- First 3 experiments:
  1. Evaluate the effect of embedding dimension on retrieval accuracy using synthetic data
  2. Compare the proposed recurrent model with baseline methods on both synthetic and real data
  3. Test the model's robustness to global tempo variations by re-rendering test pieces with different tempo ratios

## Open Questions the Paper Calls Out
The paper mentions transformers as a potential future direction for learning correspondences from even longer audio recordings, accommodating typical structural differences between audio and sheet music such as jumps and repetitions. The authors suggest that transformer-based architectures could be explored to handle longer sequences and more complex musical structures than the recurrent approach presented.

## Limitations
- Performance gap remains substantial between synthetic and real data even for the best model
- Evaluation relies on synthetic data with controllable conditions, raising questions about real-world generalization
- Datasets combine real and synthetic audio, which may not fully represent real-world audio-sheet music retrieval complexity

## Confidence
- **High Confidence**: The architectural design using recurrent networks to process longer passages is well-grounded in the literature on sequence modeling and cross-modal retrieval
- **Medium Confidence**: The claim that the method only requires weakly aligned data is supported by experimental results but depends on the quality and quantity of weakly aligned training examples
- **Medium Confidence**: The effectiveness of the approach in handling tempo variations is demonstrated but could benefit from more extensive testing across diverse tempo ranges

## Next Checks
1. Evaluate the model's performance on entirely real-world audio-sheet music pairs without synthetic augmentation to assess true generalization capability
2. Conduct ablation studies removing the pre-trained CNN encoders to quantify the contribution of transfer learning from synthetic data
3. Test the model's robustness across a wider range of tempo variations (beyond 90%-110%) to better understand its limitations in handling extreme tempo differences