---
ver: rpa2
title: Language models are susceptible to incorrect patient self-diagnosis in medical
  applications
arxiv_id: '2309.09362'
source_url: https://arxiv.org/abs/2309.09362
tags:
- medical
- language
- patient
- self-diagnosis
- percent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of incorrect patient self-diagnoses
  on the accuracy of large language models (LLMs) in medical applications. The authors
  modify multiple-choice questions from United States medical board exams to include
  adversarial self-diagnostic reports from patients.
---

# Language models are susceptible to incorrect patient self-diagnosis in medical applications

## Quick Facts
- arXiv ID: 2309.09362
- Source URL: https://arxiv.org/abs/2309.09362
- Reference count: 2
- Language models show significant performance degradation when presented with incorrect patient self-diagnoses

## Executive Summary
This study investigates how large language models (LLMs) perform when presented with incorrect patient self-diagnoses in medical applications. Using the MedQA dataset of multiple-choice questions from US medical board exams, researchers introduced adversarial prompts containing patient self-diagnostic reports. Four LLMs were evaluated: GPT-4, GPT-3.5, PaLM, and Llama 2. The results demonstrate that three of the four models show significant accuracy drops ranging from 27.78% to 42.2% when given incorrect self-diagnostic information, while GPT-4 remains largely robust with only a 2.6% decrease in accuracy.

## Method Summary
The study modified multiple-choice questions from the MedQA dataset by adding adversarial patient self-diagnostic reports to the prompts. Researchers tested four LLMs (GPT-4, GPT-3.5, PaLM, and Llama 2) on the same questions with both standard and adversarial prompts. The adversarial prompt represented a patient providing biased self-diagnostic information to the language model. Performance was measured by comparing the percentage of correct answers between the standard and adversarial conditions.

## Key Results
- Three of four LLMs (GPT-3.5, PaLM, Llama 2) showed accuracy drops of 27.78% to 42.2% when presented with incorrect patient self-diagnoses
- GPT-4 demonstrated unique robustness, with only a 2.6% decrease in accuracy under adversarial conditions
- All models provided definitive answers without qualification or warnings about unreliable patient input
- The study highlights the susceptibility of LLMs to patient self-diagnosis and the need for models that can recognize clinical diagnosing errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models incorporate user-provided self-diagnostic information into their reasoning, treating it as valid clinical evidence.
- Mechanism: The LLM's next-token prediction treats the self-diagnosis string as part of the prompt context, biasing subsequent token probabilities toward explanations consistent with that incorrect hypothesis.
- Core assumption: The model's inference is purely text-based and does not perform independent verification of the patient's self-diagnosis.
- Evidence anchors:
  - [abstract] "when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically"
  - [section] "The adversarial prompt represents a patient providing biased self-diagnostic information to the language model."
- Break condition: If the model includes explicit calibration or fact-checking steps against external medical knowledge bases, the bias might be mitigated.

### Mechanism 2
- Claim: Models without chain-of-thought or explicit reasoning steps are more susceptible to confirmation bias from incorrect patient input.
- Mechanism: Models like GPT-3.5, PaLM, and Llama lack explicit reasoning pathways that could question the validity of the patient's claim, so they propagate the error through to the final answer.
- Core assumption: Chain-of-thought reasoning can serve as a form of self-consistency check against the patient's self-diagnosis.
- Evidence anchors:
  - [abstract] "GPT-4 does not demonstrate significant performance decline when provided the adversarial prompt, going from 75 percent to 73 percent"
  - [section] "GPT-4 is a large-scale, multimodal LLM... technical reports demonstrate both models have significant understanding of medical and biological concepts"
- Break condition: If GPT-4's robustness is due to training on adversarial data rather than reasoning structure, other models might also benefit from similar fine-tuning.

### Mechanism 3
- Claim: The format of the prompt (direct instruction to choose an answer) removes the model's ability to hedge or qualify its response, locking in the bias.
- Mechanism: By instructing the model to "choose a response that best answers the provided medical question," the model is forced into a definitive answer mode, which prevents it from expressing uncertainty about the self-diagnosis.
- Core assumption: Models trained to provide confident answers will prioritize completing the task over questioning the input's validity.
- Evidence anchors:
  - [section] "Depending on the model, the LLM typically either responds with a single character... or the full answer"
  - [abstract] "It is worth noting that despite some of these models being trained to prevent providing information supporting risk categories (e.g. medical advice), all of the models provided answers to the prompting without any warning"
- Break condition: If the model is explicitly trained to refuse or qualify answers when patient input is unreliable, the format effect could be neutralized.

## Foundational Learning

- Concept: Confirmation bias in clinical reasoning
  - Why needed here: Understanding how clinicians are trained to recognize and counteract confirmation bias helps explain why some models might be more robust than others.
  - Quick check question: What is confirmation bias and how does it typically manifest in medical diagnosis?
- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: The difference in performance between GPT-4 and other models may be due to GPT-4's ability to perform multi-step reasoning that can identify inconsistencies.
  - Quick check question: How does chain-of-thought reasoning help LLMs avoid propagating errors from biased input?
- Concept: Prompt engineering and model behavior
  - Why needed here: The study's adversarial prompt format directly influences how models process and respond to incorrect patient information.
  - Quick check question: How can prompt structure influence whether an LLM hedges its answers or commits to a specific response?

## Architecture Onboarding

- Component map: Patient case -> Context injector -> LLM inference engine -> Output parser -> Performance evaluator
- Critical path: Patient case → Context injector → LLM inference → Output parser → Performance evaluator
- Design tradeoffs:
  - Using closed models (GPT-4, GPT-3.5) limits transparency but provides strong performance
  - Open models (PaLM, Llama) allow deeper analysis but show higher susceptibility to bias
  - Prompt-only approach is simple but may not capture all real-world complexities
- Failure signatures:
  - Accuracy drop >10% when adversarial self-diagnosis is present
  - Model provides definitive answer without qualification when input is unreliable
  - No mention of need to consult medical professional despite incorrect self-diagnosis
- First 3 experiments:
  1. Test GPT-4 with and without chain-of-thought prompt to isolate reasoning effect
  2. Compare performance when model is instructed to "qualify your answer" vs "choose the best answer"
  3. Evaluate model behavior when self-diagnosis is explicitly labeled as "potentially incorrect"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training methods or architectural features enable GPT-4 to resist incorrect patient self-diagnosis while other models fail?
- Basis in paper: [explicit] The authors note that GPT-4 is the only model that demonstrates robustness against adversarial input, with only a 2.6% decrease in accuracy compared to 27.78%-42.2% decreases in other models.
- Why unresolved: The study does not investigate the underlying reasons for GPT-4's robustness, merely observing the difference in performance.
- What evidence would resolve it: Comparative analysis of model architectures, training datasets, and fine-tuning procedures that isolate the factors contributing to GPT-4's resistance to confirmation bias.

### Open Question 2
- Question: How would the performance of medical LLMs change when presented with more complex, multi-symptom cases or sequential patient interactions rather than single, static multiple-choice questions?
- Basis in paper: [inferred] The study uses a simplified multiple-choice format that doesn't reflect the full complexity of real patient-doctor interactions, as noted in the introduction.
- Why unresolved: The current evaluation uses a constrained format that may not capture the full range of challenges in clinical reasoning.
- What evidence would resolve it: Testing LLMs on longitudinal patient cases with evolving symptoms, requiring dynamic reasoning and integration of new information over time.

### Open Question 3
- Question: Would incorporating explicit bias-detection mechanisms or uncertainty quantification improve the performance of medical LLMs when faced with patient self-diagnosis?
- Basis in paper: [inferred] The authors suggest future work should focus on developing models that can recognize and work around common clinical diagnosing errors like confirmation bias.
- Why unresolved: The study demonstrates the problem but does not explore potential mitigation strategies.
- What evidence would resolve it: Experiments comparing standard LLMs with variants that include bias-detection layers, confidence scoring, or explicit instructions to identify and account for potential patient bias.

## Limitations

- The MedQA dataset may not fully capture the complexity and variability of real-world patient presentations
- Adversarial prompts are researcher-constructed rather than collected from actual patients, potentially missing real patient communication patterns
- The study only tests four LLMs, limiting generalizability to other models or future iterations
- The mechanism behind GPT-4's robustness is not explicitly identified, leaving uncertainty about whether this will generalize

## Confidence

- **High confidence**: The observation that LLMs provide definitive answers without qualification when given unreliable patient input is well-supported by the results across all models tested.
- **Medium confidence**: The claim that GPT-4 is uniquely robust to adversarial patient input is supported by the data, but the underlying mechanism remains unclear and may not generalize to all GPT-4 use cases.
- **Low confidence**: The assertion that chain-of-thought reasoning is the primary factor differentiating GPT-4's performance from other models lacks direct evidence and requires further investigation.

## Next Checks

1. **Mechanism isolation experiment**: Test whether GPT-4's robustness persists when explicitly instructed to use chain-of-thought reasoning versus when not given such instructions, to determine if reasoning structure or other factors drive the performance difference.

2. **Real-world data validation**: Collect and test the models using actual patient self-diagnostic reports from clinical settings rather than researcher-constructed adversarial prompts to assess ecological validity.

3. **Temporal stability assessment**: Replicate the study after 3-6 months to evaluate whether model updates or training changes affect the observed susceptibility patterns, particularly for GPT-4 which may be updated more frequently than open models.