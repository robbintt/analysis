---
ver: rpa2
title: Can Large Language Models Really Improve by Self-critiquing Their Own Plans?
arxiv_id: '2310.08118'
source_url: https://arxiv.org/abs/2310.08118
tags:
- plan
- verifier
- feedback
- planning
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically investigates the ability of Large Language
  Models (LLMs) to self-critique their own plan generations in classical planning
  problems. The study employs an LLM-based planning system with both generation and
  verification components.
---

# Can Large Language Models Really Improve by Self-critiquing Their Own Plans?

## Quick Facts
- arXiv ID: 2310.08118
- Source URL: https://arxiv.org/abs/2310.08118
- Reference count: 16
- This paper critically investigates the ability of Large Language Models (LLMs) to self-critique their own plan generations in classical planning problems.

## Executive Summary
This paper investigates whether LLMs can improve their plan generation by self-critiquing through an iterative backprompting system. The study compares an LLM-based planning system with both generation and verification components against systems using external sound verifiers. Surprisingly, the results show that self-critiquing actually degrades performance compared to external verification, primarily due to high false positive rates from the LLM verifier. The research challenges the prevailing optimism about LLMs' self-critiquing abilities in iterative planning frameworks.

## Method Summary
The paper employs an LLM-based planning system using GPT-4 for both generation and verification. The system iteratively generates plans from PDDL domain and problem descriptions, with the LLM verifier providing feedback until a valid plan is found or iterations exceed a threshold. The approach is compared against systems using external sound verifiers (VAL) and no-backprompting baselines. The evaluation uses 100 randomly generated Blocksworld instances, measuring plan generation accuracy and verifier performance through true positive/negative rates.

## Key Results
- LLM verifiers produce high false positive rates (38/45 invalid plans marked as valid)
- Self-critiquing degrades plan generation performance compared to external sound verifiers
- Binary feedback is as effective as detailed feedback for plan generation
- Iterative backprompting provides only marginal improvement over single-shot generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-critiquing by LLMs degrades plan generation performance compared to external sound verifiers
- Mechanism: When an LLM verifies its own generated plans, it produces a high rate of false positives (38/45 invalid plans marked as valid), causing the system to accept invalid plans and corrupting the iterative refinement loop
- Core assumption: LLM verification capability is fundamentally weaker than ground-truth verification systems
- Evidence anchors:
  - [abstract] "LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability"
  - [section 5.1] "The subpar performance of the LLM+LLM system, especially when compared to LLM+V AL, can likely be traced back to the substantial number of type-1 errors produced by the LLM verifier"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.388" - Weak corpus evidence on this specific mechanism

### Mechanism 2
- Claim: Binary feedback is as effective as detailed feedback for plan generation
- Mechanism: The generator LLM's ability to improve plans depends primarily on accurate binary validation rather than detailed error information, since detailed feedback doesn't significantly impact performance when binary feedback is incorrect
- Core assumption: Plan generation improvement is bottlenecked by verification accuracy rather than feedback richness
- Evidence anchors:
  - [abstract] "the nature of feedback, whether binary or detailed, showed minimal impact on plan generation"
  - [section 5.3] "Interestingly, the amount of feedback provided to the LLM seems to have minimal influence on its performance improvement"
  - [corpus] No relevant corpus evidence found

### Mechanism 3
- Claim: Iterative backprompting provides marginal improvement over single-shot generation
- Mechanism: Multiple generation attempts give the LLM more opportunities to produce valid plans, but this benefit is negated when verification is unreliable
- Core assumption: LLM generation quality is variable but can occasionally produce valid plans given enough attempts
- Evidence anchors:
  - [section 5.1] "the LLM+LLM backprompting approach slightly outperforms the non-backprompting method in terms of accuracy"
  - [section 5.1] "It's worth noting that the marginal improvement over the generator-LLM-only method might not solely be attributed to the LLM verifier"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.388" - Weak corpus evidence

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: The evaluation uses classical planning problems represented in PDDL format, which defines domains, predicates, actions, and planning problems
  - Quick check question: What are the three main components of a PDDL planning problem?

- Concept: False positive vs false negative in verification
  - Why needed here: The paper distinguishes between type-1 errors (false positives) where invalid plans are marked as valid, which is particularly harmful
  - Quick check question: In the context of plan verification, why are false positives more problematic than false negatives?

- Concept: Iterative refinement in planning
  - Why needed here: The LLM+LLM system uses iterative refinement where the generator creates plans and the verifier provides feedback until a valid plan is found or iterations exceed threshold
  - Quick check question: What are the two termination conditions for the iterative planning loop in the LLM+LLM system?

## Architecture Onboarding

- Component map: PDDL files → Prompt generator → Generator LLM → Verifier LLM → Feedback → Generator LLM (repeat) → VAL validation
- Critical path: PDDL files → Prompt generator → Generator LLM → Verifier LLM → Feedback → Generator LLM (repeat) → VAL validation
- Design tradeoffs:
  - Single LLM vs separate generator/verifier models
  - Binary vs detailed feedback impact on performance
  - Iteration limit (15) vs allowing more attempts
  - Prompt structure (one-shot vs zero-shot)
- Failure signatures:
  - High false positive rate from verifier LLM
  - Low accuracy even with multiple iterations
  - Minimal performance difference between feedback types
  - System accepts invalid plans
- First 3 experiments:
  1. Compare LLM+LLM backprompting vs LLM+VAL backprompting on Blocksworld instances
  2. Measure false positive rate of LLM verifier against ground truth
  3. Test different feedback types (no feedback, binary, detailed) on plan generation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying cause of the high false positive rate in LLM verifiers, and can it be mitigated through architectural or training modifications?
- Basis in paper: [explicit] The paper explicitly identifies that the LLM verifier produces a significant number of false positives (38 out of 45 invalid plans incorrectly marked as valid), which is a critical limitation of the system.
- Why unresolved: The paper does not explore the reasons behind the false positives or propose solutions to address them.
- What evidence would resolve it: Systematic experiments testing different LLM architectures, training techniques, or fine-tuning strategies to reduce false positives, along with ablation studies to isolate the cause.

### Open Question 2
- Question: Does the performance of LLM verifiers degrade significantly on more complex or larger-scale planning domains compared to simpler domains like Blocksworld?
- Basis in paper: [inferred] The study is limited to the Blocksworld domain, which is relatively simple. The authors suggest future work could include more domains, implying uncertainty about scalability.
- Why unresolved: The paper only evaluates the system on Blocksworld, leaving open whether results generalize to more complex planning tasks.
- What evidence would resolve it: Testing the LLM verifier across diverse planning domains (e.g., logistics, scheduling, or hierarchical planning) with varying complexity and comparing performance.

### Open Question 3
- Question: Can incorporating external knowledge bases or symbolic reasoning components improve the reliability of LLM verifiers in planning tasks?
- Basis in paper: [inferred] The paper contrasts LLM-only verifiers with external sound verifiers (e.g., VAL) and highlights the superiority of the latter, suggesting that combining LLMs with external tools might be beneficial.
- Why unresolved: The paper does not explore hybrid approaches that integrate LLMs with external reasoning systems.
- What evidence would resolve it: Empirical studies comparing hybrid systems (e.g., LLM + VAL) against standalone LLM verifiers in terms of accuracy, false positive rates, and computational efficiency.

### Open Question 4
- Question: How does the iterative self-critiquing process affect the efficiency and resource consumption of planning systems, and is it scalable for real-world applications?
- Basis in paper: [explicit] The paper notes that the LLM+LLM system requires multiple iterations (average of 3.48) to generate plans, raising questions about computational overhead and scalability.
- Why unresolved: The paper does not analyze the computational cost or scalability of iterative self-critiquing in depth.
- What evidence would resolve it: Benchmarking the time, memory, and cost of iterative self-critiquing systems against non-iterative or hybrid approaches across various planning tasks and scales.

## Limitations
- Limited to a single domain (Blocksworld) with only 100 instances, raising questions about generalizability
- Results may not extend to other LLM architectures beyond GPT-4
- Does not explore the impact of different prompt engineering strategies
- Focuses on classical planning without considering temporal or stochastic planning domains

## Confidence

**High Confidence**: The finding that LLM verifiers produce high false positive rates (38/45 invalid plans marked as valid) is well-supported by empirical evidence comparing against the ground-truth VAL verifier.

**Medium Confidence**: The conclusion that binary feedback is as effective as detailed feedback has support from the experimental results, but the minimal difference observed could be due to ceiling effects or the specific nature of the Blocksworld domain.

**Low Confidence**: The assertion that iterative backprompting provides only marginal improvement is based on limited evidence from a single domain.

## Next Checks

1. **Domain Generalization Test**: Replicate the experiment across multiple classical planning domains (e.g., Logistics, Gripper, Mystery) to determine if the high false positive rate is consistent across different problem structures and complexity levels.

2. **Alternative LLM Architectures**: Test the same experimental setup using different LLM architectures (e.g., GPT-3.5, Claude, LLaMA) to determine if the verification reliability issues are specific to GPT-4 or represent a broader limitation of LLM-based verification systems.

3. **Prompt Engineering Impact**: Systematically vary prompt structures, including different instruction formats, few-shot examples, and temperature settings, to quantify how much of the verification performance can be attributed to prompt quality versus fundamental LLM limitations.