---
ver: rpa2
title: A State Representation for Diminishing Rewards
arxiv_id: '2309.03710'
source_url: https://arxiv.org/abs/2309.03710
tags:
- reward
- learning
- policy
- state
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of policy evaluation and learning\
  \ under diminishing rewards, a common phenomenon in natural sequential decision-making.\
  \ The authors introduce the \u03BB representation (\u03BBR), a novel state representation\
  \ that generalizes several existing representations and enables tractable policy\
  \ evaluation in non-stationary environments."
---

# A State Representation for Diminishing Rewards

## Quick Facts
- arXiv ID: 2309.03710
- Source URL: https://arxiv.org/abs/2309.03710
- Reference count: 40
- This paper introduces the λ representation (λR) for policy evaluation under diminishing rewards, generalizing existing representations and enabling efficient computation via dynamic programming.

## Executive Summary
This paper addresses policy evaluation in environments with diminishing rewards, a common phenomenon in natural sequential decision-making. The authors introduce the λ representation (λR), a novel state representation that generalizes several existing representations and enables tractable policy evaluation in non-stationary environments. The λR is defined as the expected discounted cumulative state occupancies under diminishing rewards and satisfies a Bellman recursion, allowing for efficient computation via dynamic programming. The authors prove the convergence of the λR and demonstrate its effectiveness in various settings, including tabular and continuous state spaces, policy composition, and foraging tasks.

## Method Summary
The method introduces the λ representation (λR) as a state representation that generalizes existing representations like the successor representation (SR) and first-occupancy representation (FR). The λR is computed using dynamic programming or function approximation, depending on the problem scale. It enables policy evaluation under diminishing rewards by decomposing the value function into the λR and the initial reward. The method is evaluated on gridworld environments with diminishing rewards and Mujoco continuous control tasks with replenishing rewards, comparing its performance to standard reinforcement learning methods.

## Key Results
- The λR satisfies a Bellman recursion, enabling efficient computation via dynamic programming
- The λR generalizes the SR and FR, interpolating between them based on the diminishing rate λ
- The λR improves policy evaluation accuracy and supports policy improvement under diminishing rewards
- The λR exhibits behavior consistent with optimal foraging theory in discrete environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The λR enables policy evaluation under diminishing rewards by generalizing existing state representations (SR, FR) to non-stationary reward environments.
- Mechanism: By defining the λR as the expected discounted cumulative state occupancies under diminishing rewards, it satisfies a Bellman recursion that allows efficient computation via dynamic programming despite the non-stationarity of rewards.
- Core assumption: The diminishing rate λ(s) is known or can be learned, and the transition dynamics p are fixed across tasks.
- Evidence anchors:
  - [abstract]: "we introduce a novel state representation, the λ representation (λR) which... generalizes the SR as well as several other state representations from the literature."
  - [section 4]: "we show that the λR still admits a Bellman recursion, allowing for efficient computation via dynamic programming (or approximate dynamic programming)"
- Break condition: If λ(s) varies unpredictably between tasks or if transition dynamics change, the λR may not generalize effectively.

### Mechanism 2
- Claim: The λR supports policy composition and transfer across tasks with different diminishing reward functions.
- Mechanism: By decomposing the value function into the λR and the initial reward, the λR enables rapid policy evaluation on new tasks without relearning, similar to how the SR enables transfer across stationary reward functions.
- Core assumption: The underlying transition dynamics remain consistent across tasks, and the reward functions only differ in their initial reward values and λ parameters.
- Evidence anchors:
  - [section 4.1]: "the λR facilitates generalization across reward functions with different ¯rs"
  - [section 5.3]: "generalized policy improvement (GPI) is defined as the identification of a new policy π′ such that Qπ′(s, a) ≥ supπ∈Π Qπ(s, a) ∀s, a ∈ S × A"
- Break condition: If tasks have fundamentally different transition dynamics or if λ values are inconsistent across tasks, the transfer benefit diminishes.

### Mechanism 3
- Claim: The λR captures behavior consistent with optimal foraging theory by encoding both the magnitude and persistence of rewards.
- Mechanism: By interpolating between the first-occupancy representation (FR) and the successor representation (SR), the λR naturally models the trade-off between exploiting immediate rewards and exploring for future rewards, consistent with the marginal value theorem.
- Core assumption: Natural foraging tasks exhibit diminishing marginal utility, and the optimal foraging strategy involves balancing immediate reward consumption against future opportunities.
- Evidence anchors:
  - [section 6]: "we provide a scheme for learning λ alongside the λO using feedback from the environment" and "the λO recovers MVT-like behavior in discrete environments"
  - [corpus]: "Foraging optimally in social neuroscience: computations and methodological considerations" supports the connection between diminishing rewards and foraging behavior
- Break condition: If the foraging environment doesn't exhibit diminishing returns or if the optimal strategy doesn't involve the exploitation-exploration trade-off modeled by the λR, the behavioral consistency may not hold.

## Foundational Learning

- Concept: Bellman equation and dynamic programming
  - Why needed here: The λR satisfies a Bellman recursion, allowing efficient computation of the representation. Understanding the Bellman equation is crucial for grasping how the λR can be computed and used for policy evaluation.
  - Quick check question: Can you explain why the Bellman equation is essential for efficient reinforcement learning, and how it enables the use of dynamic programming?

- Concept: Successor representation (SR) and first-occupancy representation (FR)
  - Why needed here: The λR generalizes both the SR and FR, interpolating between them based on the diminishing rate λ. Understanding these representations is key to understanding the properties and behavior of the λR.
  - Quick check question: What are the key differences between the SR and FR, and how does the λR unify these representations?

- Concept: Policy evaluation and policy improvement
  - Why needed here: The λR enables efficient policy evaluation under diminishing rewards, which is a prerequisite for policy improvement. Understanding these concepts is crucial for grasping the practical applications of the λR.
  - Quick check question: Can you explain the difference between policy evaluation and policy improvement, and why both are necessary for reinforcement learning?

## Architecture Onboarding

- Component map:
  - λR computation -> Policy evaluation -> Policy improvement -> Policy composition

- Critical path:
  1. Learn or acquire the λR for a given policy
  2. Use the λR to evaluate the policy on a new task with diminishing rewards
  3. Use the evaluated values to improve the policy or compose it with other policies

- Design tradeoffs:
  - Tabular vs. function approximation: Tabular representations are exact but scale poorly; function approximation is more scalable but may introduce approximation error
  - Fixed vs. learned λ: Fixed λ is simpler but may not capture task-specific diminishing rates; learned λ is more flexible but adds complexity
  - Episodic vs. continuing tasks: The λR is designed for episodic tasks with diminishing rewards; extending to continuing tasks with replenishing rewards requires additional mechanisms

- Failure signatures:
  - Poor policy evaluation accuracy: May indicate incorrect λ value or insufficient exploration
  - Slow convergence: May indicate high λ value or complex transition dynamics
  - Suboptimal behavior: May indicate incorrect λ value or mismatch between the λR and the true environment dynamics

- First 3 experiments:
  1. Policy evaluation on a simple gridworld with known λ value to verify the λR computation
  2. Policy improvement on the gridworld to verify the λR supports control under diminishing rewards
  3. Policy composition on the gridworld to verify the λR enables transfer across tasks with different diminishing rewards

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The method assumes known or learnable λ(s) values, which may not hold in all environments
- The experiments focus primarily on episodic tasks with static transition dynamics, and extension to continuing tasks or environments with changing dynamics remains unexplored
- The λR's performance relative to other state representations in more complex, high-dimensional environments is not thoroughly investigated

## Confidence
- High confidence in the theoretical foundations and Bellman recursion derivation for λR
- Medium confidence in the empirical results, given the limited scope of experiments
- Low confidence in the scalability and generalizability to real-world, non-episodic tasks

## Next Checks
1. Test λR performance on continuing tasks with replenishing rewards to verify the proposed extension beyond episodic settings
2. Evaluate λR against other state representations (SR, FR) in high-dimensional continuous control tasks to assess scalability
3. Investigate the sensitivity of λR to incorrect λ(s) estimates through ablation studies with varying levels of reward decay knowledge