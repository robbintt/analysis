---
ver: rpa2
title: 'Working Memory Capacity of ChatGPT: An Empirical Study'
arxiv_id: '2305.03731'
source_url: https://arxiv.org/abs/2305.03731
tags:
- memory
- working
- chatgpt
- n-back
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically assessed ChatGPT's working memory capacity\
  \ using verbal and spatial N-back tasks. The authors found that ChatGPT's performance\
  \ patterns closely resembled those of human participants, with significant performance\
  \ decline as N increased (p \u2264 .001), indicating a working memory capacity limit\
  \ similar to humans."
---

# Working Memory Capacity of ChatGPT: An Empirical Study

## Quick Facts
- arXiv ID: 2305.03731
- Source URL: https://arxiv.org/abs/2305.03731
- Authors: 
- Reference count: 0
- Key outcome: ChatGPT's performance on N-back tasks shows significant decline as N increases (p ≤ .001), indicating working memory capacity limits similar to humans, with one exception in spatial tasks.

## Executive Summary
This study systematically evaluated ChatGPT's working memory capacity using verbal and spatial N-back tasks, comparing its performance to human participants. The authors found that ChatGPT exhibited performance patterns closely resembling human participants, with significant decline in performance as N increased, indicating limited working memory capacity. The study also revealed an unusual pattern in spatial N-back tasks where false alarm rates decreased with increasing N, a pattern rarely reported in human studies. These findings suggest that large language models may exhibit emergent properties similar to human cognitive patterns.

## Method Summary
The study used verbal and spatial N-back tasks with 50 blocks of 24-letter sequences for verbal tasks and 50 blocks of 3x3 grid sequences for spatial tasks, for N=1,2,3. ChatGPT was prompted via OpenAI API to complete each sequence in a trial-by-trial manner, recording responses for match (m) or non-match (-) trials. Performance was analyzed using hit rate, false alarm rate, accuracy (hit rate - false alarm rate), and sensitivity (d' = Z(hit rate) - Z(false alarm rate)), with Kruskal-Wallis tests assessing statistical significance of performance differences across N values.

## Key Results
- ChatGPT's performance declined significantly as N increased (p ≤ .001) in both verbal and spatial N-back tasks
- Sensitivity (d') was comparable to human participants reported in literature
- In spatial N-back tasks, false alarm rates decreased as N increased from 1 to 3 (p = .021), a pattern rarely reported in human studies
- Performance patterns closely resembled those of human participants across both task types

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT's working memory performance declines predictably with increasing N in N-back tasks, similar to human participants. The transformer architecture's attention mechanism limits its ability to maintain and compare information across increasing temporal distances. As N increases, the model must retain and compare information further back in the input sequence, which exceeds the effective context window capacity.

### Mechanism 2
ChatGPT processes both verbal and spatial N-back tasks using textual representations rather than spatial reasoning. ChatGPT converts spatial grid information into textual descriptions, then applies its language processing capabilities to track and compare positions. The "X" symbol serves as a positional marker that the model can track through textual sequences.

### Mechanism 3
ChatGPT exhibits emergent properties in working memory that resemble human cognitive patterns. The model's training on human-generated text data has implicitly learned patterns of cognitive processing, including working memory limitations and strategies, leading to human-like performance patterns on cognitive tasks.

## Foundational Learning

- N-back task methodology:
  - Why needed here: Understanding the task structure is essential for interpreting the results and designing similar experiments
  - Quick check question: What distinguishes a 2-back task from a 3-back task in terms of cognitive demands?

- Transformer attention mechanisms:
  - Why needed here: The working memory capacity is fundamentally tied to how transformers process sequential information
  - Quick check question: How does the self-attention mechanism's computational complexity scale with sequence length?

- Working memory theory in cognitive science:
  - Why needed here: Provides the theoretical framework for comparing AI and human performance
  - Quick check question: What are the key metrics (like d') used to measure working memory performance in human studies?

## Architecture Onboarding

- Component map: Task generation → Prompt construction → API call to ChatGPT → Response parsing → Performance metric calculation → Statistical analysis
- Critical path: Task generation → Prompt construction → API call to ChatGPT → Response parsing → Performance metric calculation → Statistical analysis
- Design tradeoffs: Text-based spatial tasks vs. true spatial processing capability; standardized prompts vs. optimized task-specific instructions; computational efficiency vs. comprehensive testing coverage
- Failure signatures: Inconsistent performance across task repetitions; sensitivity to prompt wording changes; unexpected performance patterns that don't match human cognitive data
- First 3 experiments:
  1. Test with varying sequence lengths within the same N-back level to establish baseline capacity limits
  2. Implement different prompt strategies (explicit memory aids vs. standard prompts) to assess instruction impact
  3. Cross-validate with alternative N-back implementations to ensure results aren't prompt-dependent

## Open Questions the Paper Calls Out

### Open Question 1
Does ChatGPT truly process spatial information in the spatial N-back task from a "spatial" perspective, or is it merely pattern matching based on textual representations? The authors acknowledge uncertainty about whether ChatGPT truly processed inputs from a "spatial" perspective, as the "X" symbol is arbitrarily chosen to represent spatial positions textually.

### Open Question 2
How do different prompt strategies affect ChatGPT's working memory performance in N-back tasks? The authors suggest that future research should investigate the impact of various task manipulations, such as employing different stimuli or prompt strategies, on ChatGPT's working memory performance.

### Open Question 3
Is the decrease in false alarm rates with increasing N in the spatial N-back task a unique property of ChatGPT or indicative of a broader pattern in AI working memory? The authors note this pattern is "rarely reported in human studies" and occurred only in one task with one model.

### Open Question 4
How does ChatGPT's working memory capacity scale with model size and architecture in LLMs? The study focuses on ChatGPT specifically, but the discussion mentions implications for LLMs generally without testing other models.

### Open Question 5
Can N-back tasks serve as reliable benchmarks for comparing working memory capacity across different AI architectures? The authors propose N-back tasks as benchmarking tools but haven't validated this approach across different AI architectures or compared it to existing AI benchmarks.

## Limitations

- Uncertainty about whether ChatGPT truly processes spatial information "spatially" versus through textual mediation
- Study relies on a single model (ChatGPT) without exploring variations across different LLMs or architectural differences
- Weak corpus evidence for proposed mechanisms, suggesting some conclusions may be speculative

## Confidence

- High confidence: ChatGPT shows performance decline with increasing N values (p ≤ .001) is well-supported by data and consistent with transformer attention limitations
- Medium confidence: Comparable sensitivity (d') to human literature is reasonable but depends on accuracy of comparison data
- Low confidence: Assertion that ChatGPT exhibits "emergent properties akin to the human brain" is largely interpretive and not directly tested

## Next Checks

1. Test ChatGPT's performance on the same spatial N-back task using actual images rather than ASCII grid representations to determine if performance changes when spatial information is presented non-textually
2. Conduct ablation studies varying prompt formats systematically to assess how sensitive the performance patterns are to instruction wording and task presentation
3. Compare performance across multiple transformer-based models with different architectural parameters (context window sizes, attention mechanisms) to determine which aspects of the working memory limitations are fundamental versus model-specific