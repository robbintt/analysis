---
ver: rpa2
title: 'Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts
  during Language Model Training'
arxiv_id: '2310.16484'
source_url: https://arxiv.org/abs/2310.16484
tags:
- training
- learning
- information
- tasks
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how different types of linguistic information\
  \ emerge and interact during large language model pre-training. It introduces an\
  \ information-theoretic probing framework that extracts task-specific subspaces\
  \ from a language model\u2019s embedding space, allowing measurement of changes\
  \ to linguistic information over time and across tasks."
---

# Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training

## Quick Facts
- arXiv ID: 2310.16484
- Source URL: https://arxiv.org/abs/2310.16484
- Reference count: 40
- Key outcome: This paper studies how different types of linguistic information emerge and interact during large language model pre-training. It introduces an information-theoretic probing framework that extracts task-specific subspaces from a language model’s embedding space, allowing measurement of changes to linguistic information over time and across tasks. Analyzing nine diverse tasks across 2 million pre-training steps and five random seeds, the study identifies critical learning phases during which subspaces emerge, share information, and later disentangle to specialize. Syntactic knowledge is acquired rapidly in the first 0.5% of training, with continued improvements stemming from open-domain knowledge acquisition, long-range contextualization, and higher specialization. Cross-task similarity analysis reveals that linguistically related tasks share information throughout training, particularly during the critical learning phase. The findings have implications for model interpretability, multi-task learning, and learning from limited data.

## Executive Summary
This paper introduces an information-theoretic probing framework to study how linguistic information emerges, shifts, and interacts during large language model pre-training. By extracting task-specific subspaces from BERT-like models and analyzing them across 2 million training steps and five random seeds, the authors identify distinct learning phases and patterns of information sharing between linguistically related tasks. The study reveals that syntactic knowledge is acquired rapidly in early training, while continued improvements come from open-domain knowledge, contextualization, and specialization. The framework provides insights into model interpretability, multi-task learning optimization, and learning from limited data.

## Method Summary
The paper analyzes 290 BERT-based checkpoints from the MultiBERTs dataset, covering 145 checkpoints from 20k to 2M steps across 5 seeds, plus 145 custom checkpoints from 0 to 20k steps. Information-theoretic probes are trained for nine linguistic tasks (POS, DEP, SEM, NER, COREF, TOPIC, SENTI, QA, NLI) using linear transformations to extract task-specific subspaces. The framework measures codelength to capture task-relevant information, Principal Subspace Angles (SSAs) to compare task similarity, and center of gravity (COG) to analyze layer weight distribution. The study examines learning dynamics, cross-task interactions, and practical implications across the entire pre-training trajectory.

## Key Results
- Syntactic knowledge acquisition occurs rapidly in the first 0.5% of training, with continued improvements from open-domain knowledge and contextualization
- Linguistically related tasks (e.g., POS and DEP) share information throughout training, especially during the critical learning phase (0-1% steps)
- Subspaces become more specialized over time, with layer weight shifts (COG) indicating where task-relevant information concentrates
- Information-theoretic probing captures nuanced changes in linguistic subspaces that accuracy-based probing misses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information-theoretic probing with codelength captures more nuanced changes in linguistic subspaces than accuracy-based probing.
- Mechanism: The probe encodes both the quality of fit to data and the complexity of the transformation needed, revealing subtler changes even when performance plateaus.
- Core assumption: Task-relevant information can be consistently encoded even when accuracy does not improve.
- Evidence anchors:
  - [abstract]: "This paper studies how different types of linguistic information emerge and interact during large language model pre-training. It introduces an information-theoretic probing framework that extracts task-specific subspaces from a language model’s embedding space, allowing measurement of changes to linguistic information over time and across tasks."
  - [section]: "Information-theoretic probing (V oita and Titov, 2020) quantifies this intuition by incorporating the notion of probe complexity into the measure of task-relevant information. This is achieved by recasting the problem of training a probing classifier as learning to transmit all (x, y) ∈ D in as few bits as possible."
  - [corpus]: Weak - no explicit corpus evidence for this specific mechanism.
- Break condition: If the probe fails to capture consistent task-relevant information or if the model memorizes random embeddings, codelength may not reflect true learning.

### Mechanism 2
- Claim: Subspace angles (SSAs) provide a linear-invariant and dataset-agnostic measure for comparing task-specific information across time and tasks.
- Mechanism: By comparing the orthonormal bases of subspaces, SSAs measure the "energy" required to map one subspace to another, independent of individual instances.
- Core assumption: The subspace representation is linear and can be fully characterized by its orthonormal basis.
- Evidence anchors:
  - [abstract]: "By leveraging information-theoretic probes as characterizations of task-specific subspaces within an LM’s overall embedding space, we aim to answer these questions, and contribute: [...] A study of task subspace emergence, shifts and interactions across nine diverse linguistic tasks, 2M pre-training steps and five random initializations."
  - [section]: "To fulfill the above requirements, we make use of Principal Subspace Angles (SSAs; Knyazev and Argentati, 2002). The measure is closely related to Grassmann distance (Hamm and Lee, 2008) which has been used to, e.g., compare low-rank adaptation matrices (Hu et al., 2022). It allows us to compare task-specific subspaces θ in their entirety and independently of individual instances, removing the requirement of matching x across tasks."
  - [corpus]: Weak - no explicit corpus evidence for this specific mechanism.
- Break condition: If subspaces are non-linear or if the model uses non-linear transformations, SSAs may not fully capture the similarity between subspaces.

### Mechanism 3
- Claim: Layer weighting (center of gravity) reveals how task-relevant information is distributed across the model's layers and changes over time.
- Mechanism: By weighting the contribution of each layer to the probe's performance, COG summarizes the depth at which task information is most salient, showing shifts in representation during training.
- Core assumption: The probe can effectively summarize the information from different layers into a single weighted average.
- Evidence anchors:
  - [abstract]: "By leveraging information-theoretic probes as characterizations of task-specific subspaces within an LM’s overall embedding space, we aim to answer these questions, and contribute: [...] An analysis of these learning dynamics, which focuses on practical implications beyond performance to identify what can be learned given limited data, and how to effectively leverage this knowledge."
  - [section]: "Another important dimension for subspaces in Transformer-based LMs is layer depth. We plot layer weighting across time in Figure 5 using the center of gravity (COG) measure from Tenney et al. (2019), defined as Pl i=0 αii. It summarizes the depth at which the probe finds the most salient amounts of task-relevant information."
  - [corpus]: Weak - no explicit corpus evidence for this specific mechanism.
- Break condition: If the probe cannot effectively capture information from all layers or if the model's architecture changes, COG may not accurately reflect the distribution of task-relevant information.

## Foundational Learning

- Concept: Information-theoretic probing
  - Why needed here: To capture the complexity of the transformation needed to map embeddings to task labels, providing a more nuanced measure of task-relevant information than accuracy alone.
  - Quick check question: How does information-theoretic probing differ from traditional accuracy-based probing in measuring task-relevant information?

- Concept: Principal Subspace Angles (SSAs)
  - Why needed here: To provide a linear-invariant and dataset-agnostic measure for comparing task-specific information across time and tasks, enabling the study of subspace emergence, shifts, and interactions.
  - Quick check question: What is the advantage of using SSAs over other similarity measures like Singular Vector Canonical Correlation Analysis (SVCCA) for comparing task subspaces?

- Concept: Layer weighting and center of gravity (COG)
  - Why needed here: To reveal how task-relevant information is distributed across the model's layers and changes over time, providing insights into the model's learning dynamics and representation shifts.
  - Quick check question: How does the COG measure summarize the layer weighting, and what does it tell us about the distribution of task-relevant information in the model?

## Architecture Onboarding

- Component map: MultiBERTs checkpoints -> Information-theoretic probing framework -> Principal Subspace Angles (SSAs) -> Layer weighting and center of gravity (COG) -> Data preprocessing and task-specific probes
- Critical path:
  1. Load language model checkpoints
  2. Preprocess data and create task-specific probes
  3. Train information-theoretic probes on each checkpoint
  4. Measure codelength, SSAs, and COG for each probe
  5. Analyze and visualize the results
- Design tradeoffs:
  - Linear vs. non-linear probes: Linear probes enable subspace comparison but may not capture complex task-specific information.
  - Subword-level vs. token-level probing: Subword-level probing provides more fine-grained information but may introduce noise.
  - In-domain vs. out-of-domain evaluation: In-domain evaluation is more reliable but may not generalize to real-world scenarios.
- Failure signatures:
  - High variance in codelength or SSAs across random seeds: Indicates sensitivity to initial conditions or insufficient data.
  - Low codelength but high accuracy: Suggests the probe is memorizing random embeddings rather than capturing task-relevant information.
  - Inconsistent COG across tasks: May indicate that the probe is not effectively capturing task-relevant information from all layers.
- First 3 experiments:
  1. Replicate the probing results on a subset of checkpoints and tasks to verify the implementation and gain familiarity with the codebase.
  2. Analyze the layer weighting (COG) for a single task across multiple checkpoints to understand how task-relevant information shifts during training.
  3. Compare the codelength and SSAs between two related tasks (e.g., POS and DEP) to investigate their information sharing during training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do learning dynamics differ across various Transformer architectures (e.g., BERT, GPT, T5) and model scales?
- Basis in paper: [explicit] The paper mentions that MultiBERTs forms the underlying LM architecture and notes that more research is needed to verify whether the observed dynamics hold for larger, autoregressive LMs.
- Why unresolved: The study primarily focuses on BERT-like models, and it is unclear if the learning phases and subspace interactions generalize to other architectures like GPT or T5, or to models of different scales.
- What evidence would resolve it: Conducting similar subspace analysis across diverse Transformer architectures and scales would reveal whether the identified learning dynamics are universal or architecture-specific.

### Open Question 2
- Question: What is the impact of multi-task learning during the critical learning phase (1k-10k steps) on downstream task performance?
- Basis in paper: [inferred] The paper suggests that introducing linguistically motivated inductive biases during the early critical learning phase may be more beneficial than after subspaces have specialized. It also mentions that multi-task learning often involves training on related tasks after the underlying LM has converged.
- Why unresolved: The paper identifies the critical learning phase and the potential benefits of multi-task learning during this phase, but does not empirically test the impact of such an approach on downstream performance.
- What evidence would resolve it: Training LMs with multi-task learning during the critical phase and comparing their downstream performance to models trained with conventional multi-task learning would provide insights into the effectiveness of this approach.

### Open Question 3
- Question: How do the learning dynamics of linguistic subspaces change when training data is limited or imbalanced across different linguistic tasks?
- Basis in paper: [explicit] The paper analyzes learning dynamics across nine diverse linguistic tasks and discusses the implications for learning from limited data. It mentions that mid-resource languages could benefit from pre-trained LM encoders at a fraction of the full fine-tuning costs.
- Why unresolved: While the paper touches upon the implications for limited data scenarios, it does not directly investigate how the learning dynamics change under such conditions or how imbalanced data across tasks affects subspace emergence and interactions.
- What evidence would resolve it: Training LMs with limited or imbalanced data across tasks and analyzing the resulting subspace dynamics would reveal how these conditions impact the learning process and the emergence of linguistic information.

## Limitations
- MultiBERTs availability: The study relies on publicly available MultiBERTs checkpoints, but the exact training data versions are not specified, which could affect reproducibility.
- Information-theoretic probe interpretation: While codelength provides a theoretically grounded measure, it can be influenced by probe complexity and may not always align with downstream performance.
- Subspace angle interpretation: The interpretation of intermediate SSA values in terms of actual information sharing is not fully established and requires careful analysis.

## Confidence
- High Confidence: Claims about rapid syntactic knowledge acquisition in early training stages (first 0.5%) and the identification of critical learning phases (0-1% steps) are well-supported by consistent patterns across multiple seeds and tasks.
- Medium Confidence: Claims about cross-task information sharing, particularly that linguistically related tasks share information throughout training, are supported by SSAs analysis but require careful interpretation of subspace angle values.
- Medium Confidence: Claims about layer weight shifts (COG) and their relationship to task specialization are supported by the analysis but could benefit from more direct validation against downstream performance.

## Next Checks
1. **Subspace angle sensitivity analysis**: Validate that the observed SSA patterns between linguistically related tasks (e.g., POS/DEP vs POS/NER) are robust to different probe training runs and random seeds, particularly during the critical learning phase (0-1% steps).

2. **Codelength vs. performance correlation**: Systematically examine the relationship between codelength measurements and downstream task performance across all nine tasks to validate that codelength captures practically relevant information, not just probe complexity.

3. **Layer weight validation**: Test whether the observed COG shifts for different task types (syntactic vs. semantic) correspond to their actual utility in downstream applications by measuring performance when using only the top-weighted layers vs. bottom-weighted layers.