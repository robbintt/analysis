---
ver: rpa2
title: 'From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural
  Networks'
arxiv_id: '2310.11884'
source_url: https://arxiv.org/abs/2310.11884
tags:
- concept
- concepts
- neural
- https
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews recent approaches for explaining concepts in
  neural networks, which can serve as a link between learning and reasoning in neuro-symbolic
  AI. The paper categorizes concept explanation approaches into neuron-level and layer-level
  explanations.
---

# From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks

## Quick Facts
- arXiv ID: 2310.11884
- Source URL: https://arxiv.org/abs/2310.11884
- Reference count: 40
- Primary result: Survey categorizes concept explanation approaches in neural networks into neuron-level and layer-level methods, highlighting their importance for neuro-symbolic AI integration

## Executive Summary
This survey provides a comprehensive overview of recent approaches for explaining concepts in neural networks, which serve as a crucial link between neural learning and symbolic reasoning in neuro-symbolic AI. The paper systematically categorizes concept explanation methods into neuron-level approaches (analyzing individual neurons or filters) and layer-level approaches (using classifiers or vectors to explain concepts in entire layers). These methods enable the extraction of interpretable concepts from neural activations, facilitating model debugging, improvement, and integration with reasoning systems. The survey also identifies open research directions and challenges in this active field.

## Method Summary
The paper categorizes concept explanation approaches into two main paradigms: neuron-level and layer-level explanations. Neuron-level methods analyze individual neurons or filters either by comparing their activations with concepts (similarity-based approaches like network dissection) or by examining causal relationships through interventions. Layer-level methods use classifiers or vectors to explain concepts represented by entire layers, including concept activation vectors (CAVs) that train linear classifiers to distinguish concepts, and probing classifiers that evaluate linguistic features encoded in activations. The survey discusses implementation details, strengths, and limitations of each approach.

## Key Results
- Concept explanations serve as a natural bridge between neural network learning and symbolic reasoning
- Neuron-level explanations provide fine-grained insights by analyzing individual units but may be computationally expensive
- Layer-level explanations using CAVs and probing classifiers offer scalable methods for quantifying concept representation in neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept explanations bridge the gap between neural network learning and symbolic reasoning by identifying interpretable units within neural activations.
- Mechanism: The paper categorizes concept explanation methods into neuron-level (analyzing individual units or filters) and layer-level (using classifiers or vectors). By comparing activations with concepts or analyzing causal relationships, these methods extract meaningful concepts that can serve as primitives for reasoning systems.
- Core assumption: Neural networks learn representations that can be mapped to human-understandable concepts through post-hoc analysis.
- Evidence anchors:
  - [abstract] "Concepts can act as a natural link between learning and reasoning"
  - [section 2] "The smallest entity in a neural network that can represent a concept is a neuron"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: If neural activations cannot be meaningfully mapped to human-understandable concepts, or if the mapping is too noisy to be useful for reasoning.

### Mechanism 2
- Claim: Concept Activation Vectors (CAVs) provide a quantitative measure of how much a neural network layer represents specific concepts.
- Mechanism: CAVs are learned by training linear classifiers to distinguish between positive and negative examples of a concept in the activation space of a layer. The normal vector to the decision boundary serves as the CAV, allowing measurement of concept correlation with input data.
- Core assumption: Concepts in neural networks can be represented as linear boundaries in the activation space of a layer.
- Evidence anchors:
  - [section 3.1] "A concept activation vector (CAV) introduced by Kim et al. is a continuous vector that corresponds to a concept represented by a layer"
  - [section 3.1] "The vector normal vC ∈ Rn to the decision boundary of the classifier is then a CA V of concept C"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: If concepts in neural networks are not linearly separable in the activation space, or if the linear approximation is too coarse.

### Mechanism 3
- Claim: Probing classifiers can reveal linguistic features encoded in neural network activations, providing insights into what the network has learned.
- Mechanism: Probing involves training classifiers on top of neural network activations to predict linguistic features (e.g., part-of-speech tags, sentiment). The performance of these classifiers indicates how well the activations encode the corresponding features.
- Core assumption: If a probing classifier can accurately predict a linguistic feature from neural activations, then the network has learned to encode that feature.
- Evidence anchors:
  - [section 3.2] "Probing uses a classifier to explain concepts. However, instead of training a binary linear classifier for each concept C ∈ C to measure the existence of the concept in the activation of a layer, probing uses a classifier for multiclass classifications with labels that often represent linguistic features"
  - [section 3.2] "probing allows for evaluating how well the sentence embeddings of the model capture certain syntactic and semantic information"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: If the probing classifier learns features not actually used by the neural network, or if the probing task is too simple to require meaningful understanding.

## Foundational Learning

- Concept: Neural network activations and their interpretation
  - Why needed here: Understanding how neural networks represent information internally is crucial for explaining concepts in these models.
  - Quick check question: Can you explain what an activation map represents in a convolutional neural network?

- Concept: Linear classifiers and their use in machine learning
  - Why needed here: Many concept explanation methods rely on training linear classifiers to identify concept boundaries in activation space.
  - Quick check question: How does a linear classifier determine a decision boundary between two classes?

- Concept: Causal inference and its application in machine learning
  - Why needed here: Some concept explanation methods analyze causal relationships between input concepts and neural activations.
  - Quick check question: What is the difference between correlation and causation, and why is it important in machine learning?

## Architecture Onboarding

- Component map: Pre-trained neural network -> Concept explanation method (neuron-level or layer-level) -> Extracted concepts -> (Optional) Reasoning system
- Critical path: 1) Train or obtain a pre-trained neural network, 2) Apply concept explanation method to extract concepts, 3) (Optional) Integrate extracted concepts with reasoning system
- Design tradeoffs: Neuron-level explanations provide fine-grained insights but may be computationally expensive. Layer-level explanations are more scalable but may miss important details.
- Failure signatures: If concept explanations are inconsistent across different inputs, or if the extracted concepts do not align with human intuition, the method may be failing.
- First 3 experiments:
  1. Apply network dissection to a pre-trained image classification model and visualize the concepts learned by individual filters.
  2. Train a concept bottleneck model on a dataset with concept labels and evaluate its performance on the downstream task.
  3. Use probing classifiers to analyze the linguistic features encoded in a pre-trained language model and compare with human intuitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically compare different concept explanation approaches to determine which ones are most effective for different tasks?
- Basis in paper: [explicit] The paper states: "With the progress of concept extraction from neural networks, integrating the learned neural concepts with symbolic representations—also known as neuro-symbolic integration—is receiving (again) increasing attention" and mentions that "this line of research is still very active and in development, providing ample opportunities for new forms of integration in neuro-symbolic AI."
- Why unresolved: The paper does not provide a framework or methodology for empirically comparing different concept explanation approaches. It also does not discuss which approaches might be more suitable for different tasks or domains.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different concept explanation approaches across various tasks and domains, along with guidelines for selecting appropriate methods based on task characteristics.

### Open Question 2
- Question: How can we develop concept explanation methods that do not require labeled concept datasets or external resources?
- Basis in paper: [explicit] The paper mentions that "One strong assumption made by the network dissection approach is the availability of a comprehensive set C of concepts and corresponding labeled images to provide accurate explanations of neurons" and discusses methods like CLIP-Dissect and Label-Free Concept Bottleneck Models that attempt to address this limitation.
- Why unresolved: While some methods attempt to address this issue, the paper does not provide a comprehensive solution for developing concept explanation methods that do not rely on labeled concept datasets or external resources.
- What evidence would resolve it: Development of novel concept explanation methods that can effectively explain concepts without requiring labeled datasets or external resources, along with empirical validation of their effectiveness.

### Open Question 3
- Question: How can we ensure that the concepts identified by probing classifiers are actually used by the neural network, rather than being merely correlated with the activation?
- Basis in paper: [explicit] The paper states: "Since the probing classifier is trained independently from the pre-trained model, it was pointed out that the pre-trained model does not necessarily leverage the same features that the classifier uses for predicting a given concept, i.e., what the probing classifier detects can be merely a correlation between the activation and the concept."
- Why unresolved: The paper does not provide a solution for ensuring that the concepts identified by probing classifiers are actually used by the neural network, rather than being merely correlated with the activation.
- What evidence would resolve it: Development of novel probing methods or validation techniques that can ensure the identified concepts are actually used by the neural network, along with empirical validation of their effectiveness.

## Limitations
- The effectiveness of concept explanation methods across different neural network architectures and domains remains uncertain
- Many concept explanation methods are post-hoc and may not reflect true causal relationships within the network
- The robustness of concept explanations under distribution shifts and adversarial attacks is not well-established

## Confidence
- High confidence in the categorization of approaches and their general mechanisms
- Medium confidence in the empirical validation of these methods across different domains and architectures
- Low confidence in the effectiveness of these methods for complex, abstract concepts

## Next Checks
1. Cross-domain validation: Test concept explanation methods on multiple neural network architectures (CNNs, transformers, etc.) across vision, language, and multimodal tasks to assess generalizability.
2. Adversarial robustness evaluation: Assess how concept explanations degrade under adversarial attacks or distribution shifts to measure their reliability.
3. Human evaluation study: Conduct user studies to determine if the extracted concepts align with human intuitions across different concept complexity levels.