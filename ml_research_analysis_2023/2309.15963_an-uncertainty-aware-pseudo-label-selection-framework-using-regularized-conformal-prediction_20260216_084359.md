---
ver: rpa2
title: An Uncertainty-Aware Pseudo-Label Selection Framework using Regularized Conformal
  Prediction
arxiv_id: '2309.15963'
source_url: https://arxiv.org/abs/2309.15963
tags:
- data
- conformal
- uncertainty
- pseudo-labels
- raps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of noisy training data in pseudo-labeling
  based semi-supervised learning (SSL) due to poorly calibrated models producing erroneous
  high-confidence predictions. To solve this, the authors propose an uncertainty-aware
  pseudo-label selection framework that incorporates the Regularized Adaptive Prediction
  Set (RAPS) algorithm from conformal prediction to quantify model uncertainty.
---

# An Uncertainty-Aware Pseudo-Label Selection Framework using Regularized Conformal Prediction

## Quick Facts
- arXiv ID: 2309.15963
- Source URL: https://arxiv.org/abs/2309.15963
- Reference count: 1
- Key outcome: The paper proposes an uncertainty-aware pseudo-label selection framework using RAPS to improve semi-supervised learning by reducing noisy training data through uncertainty quantification.

## Executive Summary
This paper addresses the problem of noisy training data in pseudo-labeling based semi-supervised learning (SSL) due to poorly calibrated models producing erroneous high-confidence predictions. To solve this, the authors propose an uncertainty-aware pseudo-label selection framework that incorporates the Regularized Adaptive Prediction Set (RAPS) algorithm from conformal prediction to quantify model uncertainty. RAPS generates predictive sets that represent the classifier's uncertainty, which are then used instead of hard pseudo-labels for unlabeled data. The framework iteratively trains a model on labeled data, generates predictive sets for unlabeled data using RAPS, selects a subset of pseudo-labels based on confidence and uncertainty thresholds, and retrains the model on both labeled data and selected pseudo-labels. This process is repeated until convergence.

## Method Summary
The method proposes an iterative framework for semi-supervised learning that uses Regularized Adaptive Prediction Sets (RAPS) to generate uncertainty-aware predictive sets instead of hard pseudo-labels. The framework trains a model on labeled data, generates predictive sets for unlabeled data using RAPS, creates pseudo-labels from these sets, selects a subset using uncertainty-aware pseudo-label selection (UPS) criteria based on confidence and uncertainty thresholds, and retrains the model on both labeled and selected pseudo-labels. This process repeats until convergence, with a new network initialized in each iteration to prevent error propagation.

## Key Results
- The framework aims to reduce noisy training data by leveraging uncertainty quantification to improve pseudo-label selection accuracy in SSL
- Uses RAPS algorithm to generate predictive sets with coverage guarantees instead of hard pseudo-labels
- Incorporates both confidence and uncertainty thresholds in the UPS framework to filter noisy pseudo-labels
- Iteratively retrains model to improve uncertainty estimates and pseudo-label selection quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RAPS algorithm improves pseudo-label selection by replacing hard predictions with uncertainty-aware predictive sets
- Mechanism: Instead of selecting labels based solely on highest softmax score, RAPS generates predictive sets that contain the true label with user-specified probability (1-α coverage). These sets incorporate both the classifier's confidence (softmax scores) and a regularization term that accounts for the rank of the true class. The pseudo-labels are then derived from these predictive sets rather than hard argmax decisions
- Core assumption: The model's softmax outputs can be meaningfully transformed into uncertainty-aware predictive sets that better represent true label uncertainty
- Evidence anchors:
  - [abstract] "RAPS generates predictive sets that represent the classifier's uncertainty, which are then used instead of hard pseudo-labels"
  - [section] "Formally, for an image classifier with a target label Y ∈ Y = {1, . . . , K} and a feature vector X ∈ Rd, let C(x, u, τ) : Rd × [0, 1] × R → 2Y be a set function which generates a predictive set for input X"
  - [corpus] Weak evidence - only general conformal prediction papers found, none specifically validating RAPS for SSL
- Break condition: If the conformal scores are poorly calibrated or the regularization term λ is mis-specified, the predictive sets may become too large (over-conservative) or too small (under-coverage), negating the benefits

### Mechanism 2
- Claim: The uncertainty-aware pseudo-label selection (UPS) framework filters noisy pseudo-labels by combining confidence and uncertainty thresholds
- Mechanism: For each unlabeled sample and class, UPS creates binary selection masks based on both confidence thresholds (τp, τn) and uncertainty thresholds (κp, κn). A pseudo-label is selected only if it passes both the confidence threshold AND the uncertainty threshold, effectively removing high-confidence but uncertain predictions from training
- Core assumption: There exists a meaningful relationship between the model's predicted uncertainty and the actual correctness of pseudo-labels that can be thresholded effectively
- Evidence anchors:
  - [section] "UPS improves the high-confidence selection process such that: g(i)c = 1 [u(p(i)c) ≤ κp ∧ p(i)c ≥ τp] + 1 [u(p(i)c) ≤ κn ∧ p(i)c ≥ τn]"
  - [abstract] "The high-confidence selection process is based on the network's output confidence probabilities and selects a subset of pseudo-labels by confidence thresholds"
  - [corpus] Weak evidence - corpus contains uncertainty-aware papers but none specifically validating this UPS threshold combination approach
- Break condition: If uncertainty estimates are poorly calibrated or thresholds are set incorrectly, the selection process may either retain too many noisy labels (thresholds too lenient) or discard too many potentially useful labels (thresholds too strict)

### Mechanism 3
- Claim: Iterative retraining with selected pseudo-labels improves model calibration and reduces error propagation
- Mechanism: The framework iteratively trains a new model from scratch in each iteration, using labeled data plus carefully selected pseudo-labels. This prevents error accumulation from previous iterations while allowing the model to gradually improve its uncertainty estimates and pseudo-label selection quality
- Core assumption: Starting fresh each iteration prevents compounding of errors while still allowing the model to benefit from the improved pseudo-labels
- Evidence anchors:
  - [section] "A new network is initialized to prevent the error propagation issue in each iteration"
  - [abstract] "This process is repeated until convergence"
  - [corpus] No direct evidence in corpus - this is an inference from the algorithm description
- Break condition: If the model converges too quickly or the pseudo-label selection quality plateaus early, further iterations may provide diminishing returns while still incurring computational cost

## Foundational Learning

- Concept: Conformal prediction and coverage guarantees
  - Why needed here: The entire framework relies on generating predictive sets with guaranteed coverage properties, which requires understanding how conformal prediction works
  - Quick check question: What does it mean for a predictive set to have "1-α coverage" and how is this guarantee achieved through the calibration process?

- Concept: Uncertainty quantification in neural networks
  - Why needed here: The method depends on estimating prediction uncertainty (via MC-Dropout or other methods) to filter pseudo-labels
  - Quick check question: How does MC-Dropout sampling provide uncertainty estimates and what are its limitations compared to other uncertainty quantification methods?

- Concept: Semi-supervised learning and pseudo-labeling
  - Why needed here: The framework extends conventional pseudo-labeling, so understanding its limitations and typical failure modes is crucial
  - Quick check question: Why does pseudo-labeling typically underperform and how does poor calibration specifically contribute to this problem?

## Architecture Onboarding

- Component map: Labeled data -> Initial model training -> RAPS predictive set generation -> UPS pseudo-label selection -> Retraining with selected pseudo-labels -> Repeat until convergence
- Critical path: Labeled data → Initial model training → RAPS predictive set generation → UPS pseudo-label selection → Retraining with selected pseudo-labels → Repeat until convergence
- Design tradeoffs: The framework trades computational efficiency (iterative retraining from scratch) for improved pseudo-label quality and reduced error propagation. It also requires careful tuning of multiple hyperparameters (α for coverage, τp/τn for confidence, κp/κn for uncertainty)
- Failure signatures: Poor performance may manifest as (1) slow convergence or no improvement across iterations, (2) extremely small or large predictive sets indicating mis-specified λ or α, (3) high variance in selected pseudo-labels across iterations suggesting unstable uncertainty estimates
- First 3 experiments:
  1. Baseline test: Run conventional pseudo-labeling on a standard SSL benchmark (e.g., CIFAR-10 with 40 labels) to establish performance floor
  2. Ablation test: Implement RAPS-only (without UPS) to measure the isolated benefit of uncertainty-aware predictive sets versus hard labels
  3. Hyperparameter sensitivity: Systematically vary α (coverage), τp/τn (confidence thresholds), and κp/κn (uncertainty thresholds) to identify optimal settings and understand their impact on convergence and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method compare to other uncertainty quantification techniques like MC-Dropout sampling in terms of pseudo-label selection accuracy and overall SSL performance?
- Basis in paper: [explicit] The paper mentions that UPS uses MC-Dropout sampling as the uncertainty estimation method, while the proposed method uses RAPS. However, it does not provide a direct comparison between the two methods
- Why unresolved: The paper does not provide experimental results comparing the proposed method with MC-Dropout sampling or other uncertainty quantification techniques
- What evidence would resolve it: Experimental results showing the performance of the proposed method compared to other uncertainty quantification techniques in terms of pseudo-label selection accuracy and overall SSL performance

### Open Question 2
- Question: How does the proposed method handle multi-label classification tasks, and what modifications are needed to adapt it to such scenarios?
- Basis in paper: [explicit] The paper mentions that the proposed method can be applied to multi-label classification tasks, but it does not provide details on how it handles such scenarios or what modifications are needed
- Why unresolved: The paper does not provide a detailed explanation of how the proposed method handles multi-label classification tasks or what modifications are needed to adapt it to such scenarios
- What evidence would resolve it: A detailed explanation of how the proposed method handles multi-label classification tasks, including any modifications needed to adapt it to such scenarios, along with experimental results demonstrating its performance on multi-label classification tasks

### Open Question 3
- Question: How does the choice of uncertainty thresholds (κp, κn, τp, τn) affect the performance of the proposed method, and is there an optimal way to set these thresholds?
- Basis in paper: [explicit] The paper mentions that the uncertainty thresholds (κp, κn, τp, τn) are used to select a subset of pseudo-labels, but it does not provide details on how the choice of these thresholds affects the performance of the proposed method or if there is an optimal way to set them
- Why unresolved: The paper does not provide a detailed analysis of how the choice of uncertainty thresholds affects the performance of the proposed method or if there is an optimal way to set them
- What evidence would resolve it: A detailed analysis of how the choice of uncertainty thresholds affects the performance of the proposed method, along with experimental results demonstrating the impact of different threshold settings on the performance of the method

## Limitations
- The paper lacks specific hyperparameter values for the UPS framework (κp, κn, τp, τn), making exact reproduction difficult without additional tuning experiments
- Implementation details for the uncertainty estimation method u(p) are not specified, creating ambiguity about whether MC-Dropout or alternative methods are employed
- The theoretical guarantees of RAPS in the SSL context versus its original conformal prediction setting require empirical validation

## Confidence
- **High confidence**: The core mechanism of using RAPS for uncertainty-aware pseudo-labels is well-defined and theoretically grounded in conformal prediction literature
- **Medium confidence**: The iterative retraining framework with UPS selection shows promise but requires empirical validation to confirm convergence properties and practical benefits
- **Low confidence**: Without specific hyperparameter settings or uncertainty estimation implementation details, exact reproduction and optimization of the framework remains challenging

## Next Checks
1. Implement ablation studies comparing RAPS-only versus conventional hard pseudo-labeling on standard SSL benchmarks to quantify the isolated benefit
2. Conduct systematic hyperparameter sensitivity analysis for α (coverage), τp/τn (confidence thresholds), and κp/κn (uncertainty thresholds) to identify optimal settings
3. Perform convergence analysis across multiple SSL datasets to verify the iterative retraining process improves model calibration and pseudo-label quality over time