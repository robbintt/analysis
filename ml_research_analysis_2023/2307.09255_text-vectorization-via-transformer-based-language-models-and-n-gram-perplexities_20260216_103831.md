---
ver: rpa2
title: Text vectorization via transformer-based language models and n-gram perplexities
arxiv_id: '2307.09255'
source_url: https://arxiv.org/abs/2307.09255
tags:
- perplexity
- word
- text
- sentences
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for text vectorization based
  on n-gram perplexities computed using transformer-based language models. The core
  idea is to replace scalar perplexity values with vectors that capture the relative
  perplexity of each token in a text.
---

# Text vectorization via transformer-based language models and n-gram perplexities

## Quick Facts
- arXiv ID: 2307.09255
- Source URL: https://arxiv.org/abs/2307.09255
- Authors: 
- Reference count: 2
- This paper proposes a novel method for text vectorization based on n-gram perplexities computed using transformer-based language models, achieving accuracy improvements over random selection for detecting textual anomalies.

## Executive Summary
This paper introduces a method for text vectorization that transforms scalar perplexity scores into token-level vectors using n-gram perplexities from transformer-based language models. The approach slides a fixed-size window across text to compute local perplexities for each token, creating vectors that capture relative anomaly likelihood at the token level. Evaluated on Serbian literary corpora with expert translations, the method detects removed words with 10.37% accuracy, inserted words with 17.26% accuracy, and replaced words with 18.56% accuracy, significantly outperforming random selection baselines.

## Method Summary
The method computes n-gram perplexities using a transformer language model (GPT-2 for Serbian) by sliding a fixed-size window across the input text. Each n-gram is processed through the language model to obtain perplexity scores, and for each token, local perplexity is calculated as the average perplexity of all n-grams containing that token. These local perplexities form a vector representation of the text. The method was evaluated on parallel Serbian-German and Serbian-Italian corpora, where modified sentences were created by removing, inserting, or replacing words using a morphological dictionary. Anomaly detection accuracy was measured by comparing the predicted anomaly index to the actual modification index.

## Key Results
- Achieved 10.37% accuracy for detecting removed words, compared to random baseline
- Achieved 17.26% accuracy for detecting inserted words, compared to random baseline
- Achieved 18.56% accuracy for detecting replaced words, compared to random baseline
- Results demonstrate effectiveness of local perplexity vectors for anomaly detection in text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local perplexity vectors capture token-level anomalies better than global perplexity scores
- Mechanism: By sliding an n-gram window across the text and computing perplexity for each window, the method creates a vector where each element represents the local perplexity of a token. This preserves information about which specific tokens contribute most to the overall text perplexity, rather than losing it in a single scalar value.
- Core assumption: Token-level perplexity deviations correlate with textual anomalies like insertions, deletions, and replacements
- Evidence anchors:
  - [abstract] "As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input"
  - [section] "Given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation"
  - [corpus] Weak evidence - corpus neighbors discuss perplexity and n-grams but don't directly validate this specific mechanism
- Break condition: If token-level perplexity doesn't correlate with actual textual anomalies, or if the n-gram window size is inappropriate for the language structure

### Mechanism 2
- Claim: The averaging of n-gram perplexities for each token provides a smoothed, local context-aware measure
- Mechanism: Each token appears in multiple n-grams, so its local perplexity is computed as the average perplexity of all n-grams containing that token. This provides context-aware information rather than relying on single-token probabilities.
- Core assumption: Averaging multiple n-gram perplexities for each token produces a meaningful local context representation
- Evidence anchors:
  - [section] "For each token, local perplexity is calculated as an average perplexity of all n-grams that token was a part of, and an array of these, local perplexities represent the final perplexity vector"
  - [corpus] Weak evidence - no direct support in corpus neighbors
- Break condition: If the averaging process obscures important local variations or if n-gram overlap creates redundancy without added value

### Mechanism 3
- Claim: Transformer-based language models provide sufficient contextual understanding to make perplexity-based anomaly detection effective
- Mechanism: The method uses pre-trained transformer language models (GPT-2 for Serbian) to compute n-gram perplexities, leveraging the contextual understanding these models have learned during pre-training.
- Core assumption: Transformer language models capture sufficient linguistic patterns to identify anomalous tokens through perplexity
- Evidence anchors:
  - [section] "N-gram of size n are extracted one by one and passed forward to the language model used for processing"
  - [section] "The accuracy of guessing the correct index was measured, with each hit affecting an increase in the accuracy measure"
  - [corpus] Weak evidence - corpus neighbors mention transformer-based approaches but don't validate this specific application
- Break condition: If the transformer model hasn't been trained on sufficient linguistic data for the target language, or if the model's contextual understanding doesn't align with human notions of textual anomalies

## Foundational Learning

- Concept: N-gram language models and perplexity calculation
  - Why needed here: Understanding how n-gram perplexity works is fundamental to grasping why local perplexity vectors might be more informative than global perplexity scores
  - Quick check question: What is the mathematical formula for perplexity and how does it relate to probability?

- Concept: Sliding window techniques in text processing
  - Why needed here: The method relies on sliding a fixed-size window across the text to create overlapping n-grams, which is central to the vectorization approach
  - Quick check question: If you have a 10-word sentence and use a window size of 3, how many n-grams will you generate?

- Concept: Vector representation of text data
  - Why needed here: The paper's core contribution is converting scalar perplexity into vector representations that preserve token-level information
  - Quick check question: How does representing text as vectors enable different types of analysis compared to scalar representations?

## Architecture Onboarding

- Component map: Language model (transformer-based) -> Tokenization -> N-gram extraction -> Perplexity computation -> Local averaging -> Vector output
- Critical path: Text input -> Tokenization -> N-gram processing -> Perplexity vector calculation -> Anomaly detection
- Design tradeoffs: Larger n-gram windows capture more context but reduce the number of tokens with meaningful local perplexity values; smaller windows provide more tokens but less context
- Failure signatures: Poor accuracy in detecting anomalies (as shown in the evaluation), high variance in perplexity values indicating instability, or vectors that don't correlate with expected anomaly locations
- First 3 experiments:
  1. Test the method on a small synthetic dataset with known anomalies (inserted, deleted, replaced words) to verify basic functionality
  2. Compare results using different n-gram window sizes (n=3, n=5, n=7) on the same dataset to find optimal window size
  3. Validate that random baseline performance is significantly worse than the method's performance on the same test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of window size n affect the accuracy of detecting anomalies in text?
- Basis in paper: [explicit] The paper mentions using a sliding window of fixed length n and that only sentences longer than seven words were used, which is two times longer than the default window used (n=3).
- Why unresolved: The paper does not explore different window sizes or their impact on accuracy.
- What evidence would resolve it: Experiments comparing accuracy across different window sizes (e.g., n=2, n=4, n=5) on the same evaluation datasets.

### Open Question 2
- Question: How does the proposed method perform on languages other than Serbian?
- Basis in paper: [inferred] The paper evaluates the method only on Serbian text and uses a pre-trained language model for Serbian (gpt2-srlat).
- Why unresolved: The paper does not test the method on other languages or multilingual datasets.
- What evidence would resolve it: Applying the method to texts in different languages and comparing performance across languages.

### Open Question 3
- Question: How does the method handle different types of text anomalies beyond the three tested (removed, inserted, replaced words)?
- Basis in paper: [explicit] The paper evaluates the method on detecting removed, inserted, and replaced words, but does not explore other types of anomalies.
- Why unresolved: The paper does not investigate the method's effectiveness on other anomaly types like word order changes, synonyms, or grammatical errors.
- What evidence would resolve it: Creating evaluation datasets with different types of anomalies and testing the method's accuracy on each type.

## Limitations
- Evaluation conducted on a specialized and relatively small dataset of Serbian literary corpora, limiting generalizability to other languages and domains
- Modest absolute accuracy improvements over random baselines (10-18% vs. random) may not justify computational costs in production settings
- Method's effectiveness for languages with different morphological complexity remains untested, as only Serbian was evaluated

## Confidence
- High Confidence: The core mechanism of converting scalar perplexity into token-level vectors through n-gram averaging is mathematically sound and well-explained
- Medium Confidence: The empirical results showing improvement over random baselines are supported by the evaluation, but the absolute performance levels are modest
- Low Confidence: The generalizability of the approach to other languages, domains, or anomaly types remains largely untested

## Next Checks
1. Cross-linguistic validation: Test the same approach on languages with different morphological complexity (e.g., English vs. highly inflected languages) to determine if the method's effectiveness correlates with language characteristics
2. Ablation study on n-gram size: Systematically evaluate performance across different n-gram window sizes (n=2, n=3, n=5, n=7) on the same dataset to determine optimal configuration and understand the tradeoff between context window and token coverage
3. Failure case analysis: Generate synthetic datasets with controlled anomaly types (including multiple simultaneous anomalies, semantic anomalies vs. syntactic anomalies) to map the boundaries of where the method succeeds and fails