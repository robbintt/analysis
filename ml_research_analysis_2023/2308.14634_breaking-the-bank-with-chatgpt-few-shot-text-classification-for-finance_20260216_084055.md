---
ver: rpa2
title: 'Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance'
arxiv_id: '2308.14634'
source_url: https://arxiv.org/abs/2308.14634
tags:
- shot
- learning
- language
- intent
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We show how conversational GPT models like GPT-3.5 and GPT-4 can
  be used for quick and effective few-shot text classification on a challenging financial
  intent dataset. By using in-context learning with just a few examples, these models
  outperform fine-tuned transformer models, even when shown fewer examples.
---

# Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance

## Quick Facts
- arXiv ID: 2308.14634
- Source URL: https://arxiv.org/abs/2308.14634
- Reference count: 8
- Key outcome: Few-shot text classification with GPT models outperforms fine-tuned transformers on Banking77 dataset

## Executive Summary
This paper explores few-shot text classification for financial intents using conversational AI models like GPT-3.5 and GPT-4. The authors demonstrate that in-context learning with just a few examples can outperform traditional fine-tuned transformer models on the Banking77 dataset. They also achieve state-of-the-art results by fine-tuning masked language models with SetFit, a contrastive learning technique. However, the high subscription costs of GPT models may limit accessibility for smaller organizations.

## Method Summary
The authors employ two main approaches: in-context learning with GPT-3.5 and GPT-4, and fine-tuning masked language models (MLMs) with SetFit. For in-context learning, they craft prompts with few examples per class and evaluate classification performance using Micro-F1 and Macro-F1 scores. For SetFit, they fine-tune MPNet models using contrastive learning with representative human-curated samples. The Banking77 dataset, containing 77 financial intent labels, serves as the evaluation benchmark.

## Key Results
- GPT-3.5 and GPT-4 outperform fine-tuned transformer models in few-shot settings on Banking77 dataset
- Human-curated representative samples lead to better performance than random examples for both generative and non-generative models
- SetFit fine-tuning of MPNet models achieves state-of-the-art results in both full-data and few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot text classification can be achieved without fine-tuning by providing relevant examples in the prompt context.
- Mechanism: The LLM leverages its pre-trained knowledge and generalization capabilities to infer the mapping from text to label based on the few in-context examples.
- Core assumption: The LLM's pre-training has exposed it to sufficient linguistic patterns and task structures to perform the classification from a small number of examples.
- Evidence anchors:
  - [abstract] "Our findings show that querying GPT-3.5 and GPT-4 can outperform fine-tuned, non-generative models even with fewer examples."
  - [section] "The intuition behind in-context learning is that the LLM has already learned several tasks during its pre-training and the prompt tries to locate the appropriate one."
  - [corpus] Weak evidence - no direct corpus evidence, but related papers show successful few-shot applications of LLMs.
- Break condition: If the task requires fine-grained distinctions that rely on domain-specific semantics not well-represented in pre-training, or if the few examples are not representative of the task's complexity.

### Mechanism 2
- Claim: Representative, human-curated examples in the prompt context lead to better performance than random examples.
- Mechanism: Human curation ensures that the few examples presented to the LLM capture the key semantic and structural features of each class, making the task easier for the model to infer.
- Core assumption: Human experts can identify and select examples that best represent the core characteristics of each class, even in a few-shot setting.
- Evidence anchors:
  - [abstract] "Lastly, we find that generative models perform better on the given task when shown representative samples selected by a human expert rather than when shown random ones."
  - [section] "This approach provided a light curation that helped avoid overlaps and ensured that each example was highly relevant to its intended intent."
  - [corpus] Weak evidence - no direct corpus evidence, but the claim aligns with general best practices in few-shot learning.
- Break condition: If the human curation process introduces bias or fails to capture the true diversity of the class, leading to overfitting to a narrow subset of examples.

### Mechanism 3
- Claim: Fine-tuning pre-trained MLMs with contrastive learning techniques (SetFit) can achieve state-of-the-art results in few-shot settings.
- Mechanism: Contrastive learning optimizes the model to distinguish between similar and dissimilar text pairs, creating rich embeddings that capture fine-grained semantic differences relevant to the classification task.
- Core assumption: The pre-trained MLM's representations are sufficiently general to be fine-tuned effectively with contrastive learning, and the few examples are enough to learn the task-specific decision boundaries.
- Evidence anchors:
  - [abstract] "Additionally, we fine-tune other pre-trained, masked language models with SetFit, a recent contrastive learning technique, to achieve state-of-the-art results both in full-data and few-shot settings."
  - [section] "Tunstall et al. showed that using SetFit and eight training examples has comparable performance to training the model on the complete dataset."
  - [corpus] Weak evidence - no direct corpus evidence, but SetFit is a recent technique with reported success in few-shot scenarios.
- Break condition: If the pre-trained MLM's representations are too task-specific or the contrastive learning fails to generalize well from the few examples, leading to poor performance.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding how LLMs can perform tasks without fine-tuning is crucial for leveraging their capabilities in few-shot settings.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its advantages and limitations?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is a key technique used in the SetFit method to fine-tune pre-trained MLMs effectively in few-shot scenarios.
  - Quick check question: What is the intuition behind contrastive learning, and how does it help create rich text embeddings for classification tasks?

- Concept: Few-shot learning
  - Why needed here: Few-shot learning is the overarching paradigm for this work, and understanding its challenges and strategies is essential for effective implementation.
  - Quick check question: What are the main challenges in few-shot learning, and how do in-context learning and contrastive learning address these challenges?

## Architecture Onboarding

- Component map: Banking77 dataset -> GPT-3.5/GPT-4 (in-context learning) or S-MPNet-v2 (SetFit fine-tuning) -> Micro-F1/Macro-F1 scores

- Critical path:
  1. Prepare dataset and split into train/test sets
  2. For in-context learning: Craft prompts with few examples and task description
  3. For SetFit fine-tuning: Prepare text pairs, apply contrastive learning, fine-tune MLM
  4. Evaluate performance on test set using Micro-F1 and Macro-F1 scores

- Design tradeoffs:
  - In-context learning vs. fine-tuning: Quick setup vs. potential for higher performance with more data
  - Few examples vs. representative examples: Simplicity vs. better performance with human curation
  - Generative models vs. MLMs: Ease of use vs. potential cost and token limitations

- Failure signatures:
  - Low Micro-F1/Macro-F1 scores indicating poor classification performance
  - Inconsistent results across different runs, suggesting sensitivity to prompt examples
  - High costs or token limitations preventing effective use of generative models

- First 3 experiments:
  1. In-context learning with GPT-3.5: Craft a prompt with 1 representative example per class and evaluate performance
  2. In-context learning with GPT-4: Craft a prompt with 3 representative examples per class and evaluate performance
  3. SetFit fine-tuning with S-MPNet-v2: Fine-tune the model with 10 examples per class using contrastive learning and evaluate performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term cost-effectiveness of using GPT models for few-shot text classification compared to fine-tuning traditional models, considering both subscription fees and computational requirements?
- Basis in paper: [explicit] The paper discusses the subscription fees associated with GPT models, stating they can be costly for small organizations (approximately $1,600 for GPT-3.5 and GPT-4).
- Why unresolved: The paper does not provide a detailed cost analysis comparing the long-term expenses of using GPT models versus fine-tuning traditional models.
- What evidence would resolve it: A comprehensive cost analysis comparing the total cost of ownership for GPT models and traditional models over multiple projects and time periods.

### Open Question 2
- Question: How do different selection strategies for representative samples in few-shot learning affect the performance of both generative and non-generative models?
- Basis in paper: [explicit] The paper demonstrates that using human-curated representative samples leads to better performance than random samples for both GPT models and fine-tuned MPNet models.
- Why unresolved: The paper only explores human-curated samples versus random samples, leaving open the question of how other selection strategies might perform.
- What evidence would resolve it: Experiments comparing various sample selection strategies (e.g., clustering-based, uncertainty sampling) across different model types in few-shot settings.

### Open Question 3
- Question: Can the performance gap between in-context learning with GPT models and fine-tuned traditional models be closed or reversed when using larger context windows or more sophisticated prompt engineering?
- Basis in paper: [explicit] The paper shows that GPT models outperform fine-tuned models in few-shot settings, but notes that GPT-3.5 is limited to 1-shot due to token constraints.
- Why unresolved: The paper does not explore the impact of larger context windows or advanced prompt engineering techniques on model performance.
- What evidence would resolve it: Experiments using larger context windows (when available) and various prompt engineering techniques to compare the performance of GPT models and fine-tuned traditional models in few-shot settings.

## Limitations
- High subscription costs of GPT models may be prohibitive for small organizations
- Limited generalization scope to non-financial text classification tasks
- Dependency on human curation for representative sample selection introduces potential bottlenecks

## Confidence
- High confidence: GPT models outperforming fine-tuned models in few-shot settings
- Medium confidence: Superiority of human-curated examples over random selection
- Medium confidence: State-of-the-art results with SetFit fine-tuning

## Next Checks
1. Conduct a detailed cost-benefit analysis comparing GPT-based approaches against fine-tuned transformer models across different organizational sizes and usage patterns, including token consumption estimates and operational costs.

2. Test the few-shot performance of GPT models and SetFit fine-tuning on at least two non-financial text classification datasets to assess generalizability beyond the Banking77 domain.

3. Systematically vary prompt structures (example ordering, task descriptions, formatting) across multiple runs to quantify the stability and sensitivity of in-context learning performance to prompt engineering choices.