---
ver: rpa2
title: 'MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large
  Language Model'
arxiv_id: '2310.13265'
source_url: https://arxiv.org/abs/2310.13265
tags:
- answer
- question
- table
- retrieval
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoqaGPT, a zero-shot framework for multi-modal
  open-domain question answering using large language models. The method employs a
  divide-and-conquer strategy, retrieving and extracting answers from each modality
  (text, tables, images) separately, then fusing the information using LLMs to produce
  a final answer.
---

# MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model

## Quick Facts
- **arXiv ID:** 2310.13265
- **Source URL:** https://arxiv.org/abs/2310.13265
- **Reference count:** 34
- **Key outcome:** Zero-shot framework achieving +37.91 F1 and +34.07 EM on MMCoQA, +9.5 F1 and +10.1 EM on MultiModalQA

## Executive Summary
This paper introduces MoqaGPT, a zero-shot framework for multi-modal open-domain question answering using large language models. The method employs a divide-and-conquer strategy, retrieving and extracting answers from each modality (text, tables, images) separately, then fusing the information using LLMs to produce a final answer. This approach avoids complex multi-modality ranking and enables easy adaptation to new modalities and models. Experiments on MMCoQA and MultiModalQA datasets show significant improvements over supervised baselines, demonstrating the framework's effectiveness in leveraging existing models for multi-modal QA tasks.

## Method Summary
MoqaGPT processes each modality independently through retrieval and question answering before fusing the results using LLMs. For text, it uses dense retrieval with ANCE; for images, CLIP-based contrastive retrieval; and for tables, Ada-based linearization. Answer extraction is performed by BLIP2 for VQA, ChatGPT for textual QA, and GPT4 for direct QA and reasoning. A rule-based strategy filters invalid answers before LLM-based fusion selects the final answer. The framework operates without training on the target datasets, relying instead on pre-trained models and LLM reasoning capabilities.

## Key Results
- Achieves +37.91 F1 and +34.07 EM improvement on MMCoQA dataset
- Achieves +9.5 F1 and +10.1 EM improvement on MultiModalQA dataset
- Outperforms supervised baselines and direct LLM queries across both datasets
- Demonstrates flexibility by easily accommodating new modalities without retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The divide-and-conquer strategy eliminates the need for complex multi-modality ranking by processing each modality independently before fusion.
- **Mechanism:** Retrieval and question answering are performed separately for text, tables, and images. This avoids the need to map all modalities into a single joint embedding space, which is difficult and inflexible.
- **Core assumption:** Independent processing of each modality followed by LLM-based fusion is sufficient to handle cross-modal reasoning.
- **Evidence anchors:**
  - [abstract] "Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task."
  - [section 3.1.1] "Different modalities utilize pre-established, highly effective models for retrieval, eliminating the need to map all modalities into a joint embedding space."
  - [corpus] Weak - no direct corpus evidence, but related work like "Improving Zero-shot Reader by Reducing Distractions" supports the value of avoiding joint ranking complexity.
- **Break condition:** If cross-modal reasoning requires tight interaction between modalities that cannot be captured by LLM fusion, the independent processing may miss important contextual relationships.

### Mechanism 2
- **Claim:** Rule-based strategy effectively filters out invalid answers and improves final answer quality by prioritizing LLM-generated responses.
- **Mechanism:** Invalid answers containing phrases like "unknown" or "sorry" are discarded. Among valid answers, the most frequent response from top-K references is selected, reducing noise from VLMs that tend to produce answers even when information is insufficient.
- **Core assumption:** LLMs can reliably identify when to provide specific answers versus defaulting to "unknown", and frequency-based selection among top-K references improves answer accuracy.
- **Evidence anchors:**
  - [section 3.1.3] "Through empirical observation, we identified that: (i) The VLMs tend to consistently produce answers, even if relevant information is missing. (ii) The most accurate answer isn't necessarily found in the top-1 (most similar) retrieved reference."
  - [section 4.5] "Rule 1 proves to be essential, leading to a 23% reduction in GPT activations. This obviates the need for reasoning over potentially noisy answers, thereby enhancing response accuracy and curtailing inference time."
  - [corpus] Weak - no direct corpus evidence, but general LLM research supports sensitivity to input noise as mentioned in "Certified robustness for large language models with self-denoising."
- **Break condition:** If the frequency-based selection among top-K references fails to capture the correct answer due to insufficient diversity in the candidate pool.

### Mechanism 3
- **Claim:** LLM-based answer infusion can effectively integrate information across modalities and select the most plausible answer format.
- **Mechanism:** After extracting answer candidates from each modality, LLMs are used to infer the correct answer format and select the appropriate answer from the candidates. This leverages the LLM's reasoning ability to handle format-specific questions.
- **Core assumption:** LLMs can determine the correct answer format based on the question and select the appropriate answer from candidates across different modalities.
- **Evidence anchors:**
  - [section 3.2] "For the majority of queries, even though it's hard to decide which modality contains the answer, the format of the answer is usually predetermined... we leverage LLMs to infer the correct answer format and select the appropriate answer from the candidates."
  - [section 4.4] "Our zero-shot method outshines the supervised baseline because of superior retrieval, question answering, and answer infusion capabilities. It also elevates the Direct QA approach when grounded in retrieved results, showing up to a 6.0 F1 and 5.9 EM boost over ChatGPT and a 5.0 F1 and 5.1 EM enhancement over GPT4."
  - [corpus] Weak - no direct corpus evidence, but related work on "Self-prompted Chain-of-Thought" supports LLM reasoning capabilities for complex tasks.
- **Break condition:** If the LLM cannot accurately infer the answer format or if the answer candidates lack sufficient diversity to cover all possible formats.

## Foundational Learning

- **Concept: Multi-modal retrieval methods**
  - Why needed here: Different modalities require different retrieval approaches - dense retrieval for text, contrastive learning for images, and linearization for tables.
  - Quick check question: What retrieval method is used for each modality in MoqaGPT?

- **Concept: Extractive question answering**
  - Why needed here: The framework needs to extract concise answer spans from retrieved references rather than generating open-ended responses.
  - Quick check question: How does the framework ensure extracted answers are concise and relevant?

- **Concept: Chain-of-thought prompting**
  - Why needed here: While not explicitly used in the final framework, understanding CoT helps in evaluating the reasoning capabilities of LLMs for answer fusion.
  - Quick check question: What are the potential benefits and drawbacks of using CoT in the answer infusion stage?

## Architecture Onboarding

- **Component map:**
  - Retrieval module (CLIP for images, ANCE for text, Ada for tables) -> Question answering module (BLIP2 for VQA, ChatGPT for text, GPT4 for reasoning) -> Rule-based filtering -> LLM-based fusion

- **Critical path:**
  1. Retrieve relevant references for each modality
  2. Extract answer candidates from each modality
  3. Apply rule-based strategy to filter invalid answers
  4. Fuse answer candidates using LLM reasoning
  5. Generate final answer

- **Design tradeoffs:**
  - Flexibility vs. performance: Independent processing allows easy adaptation but may miss cross-modal interactions
  - Simplicity vs. accuracy: Rule-based strategy is simple but may not capture all edge cases
  - Zero-shot vs. supervised: Avoids training but relies heavily on LLM capabilities

- **Failure signatures:**
  - Low recall in VQA indicates difficulty in extracting answers from images
  - Noisy answer candidates suggest issues with the rule-based filtering strategy
  - Incorrect answer format selection indicates LLM reasoning limitations

- **First 3 experiments:**
  1. Test retrieval performance for each modality independently using Recall@5 metric
  2. Evaluate question answering quality by checking if gold answers are present in top-5 candidates
  3. Assess answer fusion by comparing F1 and EM scores against supervised baselines and direct LLM queries

## Open Questions the Paper Calls Out

- **Question:** How does MoqaGPT handle cases where the gold answer is not present in any of the top-5 retrieved references from any modality?
- **Basis in paper:** [explicit] The paper states "The primary assessment criterion is to determine if the gold answers were present among these 5 candidates" but does not discuss what happens when gold answers are missing.
- **Why unresolved:** The evaluation methodology focuses on Recall@5 but doesn't address cases where the answer is not found in the retrieved candidates.
- **What evidence would resolve it:** Analysis of cases where gold answers are missing from retrieved references and the subsequent performance of the fusion stage.

- **Question:** What is the computational efficiency and latency of MoqaGPT compared to direct QA methods when processing real-world queries?
- **Basis in paper:** [inferred] The paper mentions API costs and inference time but doesn't provide concrete latency measurements for end-to-end processing.
- **Why unresolved:** While the paper discusses the need for API calls and mentions cost considerations, it lacks quantitative analysis of processing time for real queries.
- **What evidence would resolve it:** Empirical measurements of query processing time, including retrieval, QA extraction, and fusion stages, compared to direct LLM queries.

- **Question:** How does MoqaGPT's performance scale with larger knowledge bases and more diverse modalities?
- **Basis in paper:** [explicit] The paper mentions the framework can "accommodate new modalities" and discusses dataset statistics, but doesn't explore performance at scale.
- **Why unresolved:** The experiments are conducted on datasets with limited reference numbers (top 5 per modality) and don't investigate performance degradation or improvement with larger knowledge collections.
- **What evidence would resolve it:** Experiments with varying knowledge base sizes and additional modalities to measure performance trends and identify potential bottlenecks.

## Limitations
- The framework's performance relies heavily on the availability and quality of pre-trained models for each modality
- The approach may struggle with novel question types requiring deep cross-modal reasoning that cannot be captured through separate processing and LLM fusion
- The rule-based filtering strategy may not generalize well to all answer patterns and could miss subtle invalid responses

## Confidence
- **High Confidence:** The divide-and-conquer strategy's effectiveness in avoiding complex multi-modality ranking, supported by clear empirical improvements (+37.91 F1 on MMCoQA)
- **Medium Confidence:** The rule-based filtering strategy's ability to reduce noisy answers, based on observed 23% reduction in GPT activations
- **Medium Confidence:** The LLM's ability to effectively fuse answers across modalities, though performance varies significantly between datasets

## Next Checks
1. **Retrieval Quality Analysis:** Evaluate recall@5 for each modality independently to verify that gold answers are present in retrieved candidates, as poor retrieval would invalidate downstream performance
2. **Cross-Modal Reasoning Stress Test:** Design questions requiring tight interaction between modalities to test the limits of independent processing followed by LLM fusion
3. **Rule-Based Strategy Robustness:** Test the filtering strategy on a diverse set of answer patterns to identify edge cases where it might fail to catch invalid responses or incorrectly filter valid ones