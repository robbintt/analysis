---
ver: rpa2
title: 'A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text
  Modality Conversion'
arxiv_id: '2307.11584'
source_url: https://arxiv.org/abs/2307.11584
tags:
- recognition
- speech
- emotion
- modality
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Speech Emotion Recognition
  (SER) by proposing a modality conversion approach. The authors investigate whether
  converting speech to text using Automatic Speech Recognition (ASR) can improve emotion
  recognition performance compared to traditional speech-based methods.
---

# A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion

## Quick Facts
- **arXiv ID**: 2307.11584
- **Source URL**: https://arxiv.org/abs/2307.11584
- **Reference count**: 19
- **Primary result**: Modality conversion achieves 43.1% WF1, outperforming state-of-the-art speech-based approaches on MELD

## Executive Summary
This paper explores a novel approach to Speech Emotion Recognition (SER) by converting speech to text using Automatic Speech Recognition (ASR) before applying text-based emotion classification. The authors propose two methods: Modality-Conversion, which uses real ASR output, and Modality-Conversion++, which assumes perfect ASR output. Both approaches outperform traditional speech-based methods on the MELD dataset, with Modality-Conversion++ achieving 60.4% weighted-F1 score, representing an 11.6% improvement over existing speech-based approaches. This work demonstrates the potential of modality conversion for enhancing emotion recognition tasks, particularly when text-based methods are inherently better suited to a given dataset.

## Method Summary
The authors investigate modality conversion by first converting speech to text using the Vosk ASR API, then applying text-based emotion classification using a fine-tuned RoBERTa-base model. They evaluate two approaches: Modality-Conversion uses actual ASR output, while Modality-Conversion++ assumes perfect ASR output using gold transcripts. Both methods are tested on the MELD dataset, an audio-visual emotion recognition dataset based on the TV series Friends. The primary evaluation metric is weighted-F1 score, which accounts for class imbalance in emotion categories.

## Key Results
- Modality-Conversion achieves 43.1% weighted-F1 score, outperforming state-of-the-art speech-based approaches
- Modality-Conversion++ achieves 60.4% weighted-F1 score, representing an 11.6% improvement over speech-based methods
- Text-based approaches show better performance than speech-based approaches on the MELD dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting speech to text using ASR followed by text-based emotion classification improves performance compared to direct speech-based emotion recognition.
- Mechanism: Text-based emotion recognition models have inherently better feature representations for emotion classification than acoustic models, so converting speech to text allows leveraging these superior text features.
- Core assumption: The ASR system's output text preserves sufficient emotional information from the original speech for accurate emotion classification.
- Evidence anchors:
  - [abstract] "Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset."
  - [section] "While the first method achieved significant results compared to SOTA, the second method improves emotion recognition performance even further, compared to SOTA speech-based approaches on the MELD dataset"
  - [corpus] "On the MELD dataset, our lexical-based... approach show that on the same dataset, text-based approaches achieve better results than speech-based approaches"

### Mechanism 2
- Claim: Perfect ASR output (gold transcripts) enables significantly higher emotion recognition performance than imperfect ASR.
- Mechanism: Eliminating ASR errors removes noise from the text input, allowing the text classifier to focus entirely on emotional content rather than recovering from transcription errors.
- Core assumption: Gold transcripts are accurate enough to capture all emotional nuances present in the original speech.
- Evidence anchors:
  - [abstract] "second method further improves performance with a weighted-F1 score of 60.4%, representing an 11.6% improvement over existing speech-based methods"
  - [section] "second method assumes perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++"
  - [corpus] Weak - the corpus mentions ASR but doesn't specifically address perfect vs imperfect ASR performance differences

### Mechanism 3
- Claim: Text-based emotion recognition models are inherently better suited for the MELD dataset than speech-based models.
- Mechanism: The MELD dataset's emotional content is more readily captured in textual features than in acoustic features, making text-based approaches naturally more effective.
- Core assumption: The emotional information in the MELD dataset is primarily lexical rather than prosodic.
- Evidence anchors:
  - [abstract] "This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities"
  - [section] "we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset"
  - [corpus] "In the context of emotion recognition, researchers have also explored the use of text-based features... Multi-modality approaches... have shown promising results in emotion recognition and show that on the same dataset, text-based approaches achieve better results than speech-based approaches"

## Foundational Learning

- **Concept**: Automatic Speech Recognition (ASR) systems and their error characteristics
  - Why needed here: Understanding how ASR error rates affect downstream emotion recognition performance is crucial for implementing and improving the modality conversion approach
  - Quick check question: What are the typical Word Error Rates (WER) for modern ASR systems on conversational speech like the MELD dataset?

- **Concept**: Text-based emotion recognition using transformer models (RoBERTa)
  - Why needed here: The paper uses a fine-tuned RoBERTa-base model for text classification after modality conversion, so understanding how transformers capture emotional semantics is essential
  - Quick check question: How does RoBERTa's pretraining objective differ from BERT, and why might this be beneficial for emotion recognition tasks?

- **Concept**: Weighted-F1 score calculation and interpretation
  - Why needed here: The paper uses weighted-F1 as the primary evaluation metric, so understanding how it handles class imbalance is important for correctly interpreting results
  - Quick check question: How does weighted-F1 differ from macro-F1, and why is it more appropriate for imbalanced emotion classification datasets?

## Architecture Onboarding

- **Component map**: Audio clips → Vosk ASR API → Text transcripts → RoBERTa fine-tuning → Emotion classification
- **Critical path**: Audio → ASR → Text → RoBERTa → WF1 score
  The ASR accuracy directly impacts the text quality, which determines the RoBERTa model's performance
- **Design tradeoffs**:
  - Using Vosk API vs commercial ASR services (cost vs accuracy)
  - Fine-tuning vs prompt-tuning for the RoBERTa model
  - Weighted-F1 vs other metrics (sensitivity to class imbalance)
- **Failure signatures**:
  - Low WF1 scores despite high ASR accuracy might indicate poor emotion representation in text
  - Large performance gap between Modality-Conversion and Modality-Conversion++ indicates ASR quality issues
  - Unexpected class distribution in predictions might suggest bias in the RoBERTa fine-tuning
- **First 3 experiments**:
  1. Run Modality-Conversion with Vosk ASR on a small subset of MELD and calculate WER to establish baseline ASR quality
  2. Fine-tune RoBERTa on the transcribed text from experiment 1 and measure WF1 to establish baseline text-based performance
  3. Compare results from experiments 1 and 2 with direct speech-based emotion recognition on the same subset to quantify modality conversion benefits

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of the Modality-Conversion approach compare when using different ASR systems beyond Vosk?
  - Basis in paper: [explicit] The authors use Vosk API for their first experiment and suggest exploring different ASR systems in future work.
  - Why unresolved: The paper only tests one ASR system (Vosk), so the impact of using alternative ASR systems remains unknown.
  - What evidence would resolve it: Comparative experiments using multiple ASR systems (e.g., Whisper, Google Speech-to-Text, Amazon Transcribe) with the same RoBERTa classifier to measure performance differences.

- **Open Question 2**: What is the performance ceiling for modality conversion when perfect ASR is assumed across different datasets and emotion categories?
  - Basis in paper: [explicit] The Modality-Conversion++ method assumes perfect ASR output and achieves 60.4% WF1 on MELD, representing an 11.6% improvement over speech-based methods.
  - Why unresolved: The paper only evaluates on one dataset (MELD) with a single emotion recognition task, limiting generalizability.
  - What evidence would resolve it: Systematic testing of the Modality-Conversion++ approach across multiple emotion recognition datasets (e.g., IEMOCAP, Emo-DB) with various emotion categories to establish performance limits.

- **Open Question 3**: How does incorporating visual information alongside text modality conversion affect emotion recognition performance?
  - Basis in paper: [inferred] The authors mention MELD is an audio-visual dataset and suggest incorporating visual cues in future work, while noting that multimodal approaches combining speech and text have shown promising results in prior work.
  - Why unresolved: The current study only uses audio-to-text conversion without leveraging the visual modality available in the MELD dataset.
  - What evidence would resolve it: Experiments that combine text-based emotion recognition (from ASR output) with visual feature extraction and multimodal fusion techniques, comparing performance against unimodal text-only approaches.

- **Open Question 4**: What is the impact of ASR error rates on the overall emotion recognition performance in the Modality-Conversion approach?
  - Basis in paper: [inferred] The authors demonstrate a substantial gap between Modality-Conversion (43.1% WF1) and Modality-Conversion++ (60.4% WF1), implying ASR errors negatively impact performance.
  - Why unresolved: The paper does not analyze how specific error types or rates in ASR output affect emotion classification accuracy.
  - What evidence would resolve it: Detailed error analysis correlating specific ASR transcription errors (e.g., word substitutions, deletions) with emotion classification mistakes, potentially using controlled experiments with artificially corrupted transcripts.

## Limitations
- Results depend heavily on the quality of the underlying ASR system
- The approach assumes emotional information is primarily lexical rather than prosodic
- No comparison with multimodal approaches that combine both speech and text features

## Confidence
- **High confidence**: Experimental methodology and evaluation framework
- **Medium confidence**: Mechanism explaining why text-based approaches outperform speech-based ones on MELD specifically
- **Low confidence**: Generalizability of results to other datasets or real-world ASR conditions

## Next Checks
1. Test the Modality-Conversion approach on multiple emotion recognition datasets (e.g., IEMOCAP, RAVDESS) to assess generalizability
2. Evaluate the impact of ASR Word Error Rate on final emotion recognition performance by systematically varying ASR quality
3. Compare the proposed approach against multimodal systems that fuse both speech and text features to determine if modality conversion provides additional benefits beyond simple feature concatenation