---
ver: rpa2
title: Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios
  Like a Lawyer?
arxiv_id: '2310.14880'
source_url: https://arxiv.org/abs/2310.14880
tags:
- legal
- reasoning
- irac
- answer
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs a novel dataset SIRAC with 50 legal scenarios
  pertaining to Contract Acts Malaysia and Australian Social Act for Dependent Child,
  each annotated with complete IRAC analysis. The authors evaluate ChatGPT's ability
  to perform IRAC analysis like legal professionals, finding that while ChatGPT can
  produce correct conclusions, it mostly fails to generate correct reasoning paths.
---

# Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?

## Quick Facts
- **arXiv ID**: 2310.14880
- **Source URL**: https://arxiv.org/abs/2310.14880
- **Authors**: 
- **Reference count**: 39
- **Primary result**: ChatGPT can produce correct conclusions but fails to generate correct reasoning paths in legal IRAC analysis, with performance significantly improved by providing human-authored reasoning paths and question decomposition

## Executive Summary
This paper evaluates ChatGPT's ability to perform legal reasoning using the IRAC (Issue, Rule, Application, Conclusion) method, a standard framework used by legal professionals. The authors construct a novel dataset SIRAC with 50 legal scenarios from Malaysian Contract Law and Australian Social Act for Dependent Children, each annotated with complete IRAC analysis. They find that while ChatGPT can produce correct final answers in some cases, it consistently fails to generate correct reasoning paths, often producing fluent but logically incorrect intermediate steps. The study demonstrates that providing human-authored reasoning paths dramatically improves ChatGPT's final answer accuracy, and that decomposing complex questions into simpler ones enhances legal concept identification.

## Method Summary
The authors constructed the SIRAC dataset with 50 legal scenarios from Contract Acts Malaysia and Australian Social Act for Dependent Child, each annotated with complete IRAC analysis using a semi-structured language. They evaluated ChatGPT's performance through human evaluation using law school marking rubrics, scoring outputs on factors including correctness, fluency, information relevance, concept identification, and reasoning articulation. The study tested various prompting strategies including baseline prompts, prompts with added reasoning paths, in-context learning with similar examples, and prompts with decomposed questions.

## Key Results
- ChatGPT achieves F1 score of 0.49 on average for answering legal questions, but fails to produce complete and correct reasoning paths
- Providing complete human-written reasoning paths improves average F1 score of final answers to 0.86 or higher
- Decomposing legal questions into simpler ones consistently improves accuracy of identifying legal concepts
- In-context learning with similar examples enhances performance, particularly for identifying legal concepts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ChatGPT produces correct conclusions but fails to generate correct reasoning paths.
- **Mechanism**: ChatGPT uses language patterns to mimic legal reasoning structure without understanding underlying legal logic. It can guess correct outcomes from scenario features but cannot trace the logical steps (Issue → Rule → Application → Conclusion) that legal professionals use.
- **Core assumption**: Legal reasoning is sequential and requires traceable logical steps; ChatGPT generates plausible but incorrect intermediate steps even when final answers are correct.
- **Evidence anchors**:
  - [abstract] "ChatGPT achieves an F1 of 0.49 on average for answering legal questions... However, ChatGPT fails to produce complete and correct reasoning paths toward answers for any evaluated scenario albeit some of the answers are correct."
  - [section] "Evaluation of Application and Rule... only two of them produced high quality reasoning paths... The performance is also poor on the articulation of reasoning."

### Mechanism 2
- **Claim**: Providing human-authored reasoning paths dramatically improves ChatGPT's final answer accuracy.
- **Mechanism**: When given intermediate reasoning steps as context, ChatGPT shifts from generating reasoning to pattern-matching against the provided reasoning structure, using the human reasoning as a template to guide conclusion generation.
- **Core assumption**: ChatGPT's reasoning generation is unreliable, but its ability to complete structured templates given proper context is strong.
- **Evidence anchors**:
  - [abstract] "The average F1 score of the final answers estimated by ChatGPT was improved more than 0.86, when the complete human-written reasoning paths except final answers are fed to the model."
  - [section] "Figure 4 shows that the more analysis we provided in a prompt, the higher F1 scores we gained from the final answers. The F1-score is able to reach 0.89/1.0 starting from the lowest 0.10/0.0 for CAM/ASA respectively."

### Mechanism 3
- **Claim**: Decomposing complex legal questions into simpler sub-questions improves legal concept identification accuracy.
- **Mechanism**: Breaking down complex issues reduces cognitive load and allows ChatGPT to focus on identifying specific legal concepts in isolation rather than attempting to reason about the entire problem simultaneously.
- **Core assumption**: Legal reasoning performance degrades with question complexity due to attention and context limitations.
- **Evidence anchors**:
  - [abstract] "Decomposing legal questions into simpler ones consistently improve the accuracy of identifying legal concepts, such as 'invitation to treat'."
  - [section] "From Table 4, the overall legal concept identification maintains high performance with a precision of 0.75, recall of 0.88, and an F1-score of 0.81."

## Foundational Learning

- **Concept: IRAC methodology**
  - Why needed here: Provides the structured framework for legal analysis that the paper evaluates ChatGPT against
  - Quick check question: What are the four components of IRAC and what does each represent?

- **Concept: Defeasible reasoning**
  - Why needed here: Legal reasoning allows conclusions to be overturned by new evidence or different assumptions; understanding this is crucial for evaluating legal AI
  - Quick check question: How does defeasible reasoning differ from monotonic logic in legal contexts?

- **Concept: In-context learning**
  - Why needed here: The paper evaluates how providing examples as context affects ChatGPT's legal reasoning performance
  - Quick check question: What is the difference between in-context learning and fine-tuning for LLMs?

## Architecture Onboarding

- **Component map**: Scenario input → Issue identification → Rule retrieval → Application reasoning → Conclusion generation
- **Critical path**: Scenario → ChatGPT processing → IRAC output → Human evaluation → Performance metrics
- **Design tradeoffs**: 
  - Semi-structured vs natural language annotations (tradeoff between interpretability and flexibility)
  - Complex vs simple scenarios (tradeoff between realism and tractability)
  - Automated vs manual evaluation (tradeoff between scalability and accuracy)
- **Failure signatures**:
  - High fluency but low information relevance indicates pattern matching without understanding
  - Correct conclusions with incorrect reasoning paths indicates reliance on shortcuts
  - Performance degradation with increased scenario complexity indicates context window limitations
- **First 3 experiments**:
  1. Run ChatGPT on a simple scenario without any additional context to establish baseline performance
  2. Provide human-authored reasoning paths (partial or complete) to test the template hypothesis
  3. Decompose a complex scenario's issue into simpler questions to test the cognitive load hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the semi-structured language used for IRAC annotation impact the ability of LLMs to understand and process legal reasoning paths compared to traditional natural language annotations?
- **Basis in paper**: [explicit] The paper introduces a semi-structured language that combines legal terminology with logical operators to make reasoning paths interpretable by both legal professionals and computer programs.
- **Why unresolved**: The paper demonstrates that ChatGPT struggles with producing correct reasoning paths even when given the semi-structured format, but doesn't systematically test whether this specific representation format improves LLM understanding compared to other formats.
- **What evidence would resolve it**: Comparative experiments testing ChatGPT's performance using the semi-structured language versus pure natural language annotations for the same reasoning paths.

### Open Question 2
- **Question**: What is the optimal amount of human-authored reasoning paths needed to significantly improve LLM performance without overwhelming the model or requiring excessive human effort?
- **Basis in paper**: [explicit] The paper shows that providing 20%, 40%, and 80% of human-authored reasoning paths progressively improves ChatGPT's performance, but doesn't identify the optimal threshold.
- **Why unresolved**: The paper only tests three levels (20%, 40%, 80%) and shows continued improvement with more information, but doesn't determine where diminishing returns begin or the minimum effective amount.
- **What evidence would resolve it**: Systematic testing with finer-grained percentages (e.g., 10%, 25%, 50%, 75%) to identify the point where additional human input provides negligible performance gains.

### Open Question 3
- **Question**: Does the effectiveness of question decomposition vary based on the complexity of legal concepts involved, and if so, how can decomposition strategies be optimized for different types of legal issues?
- **Basis in paper**: [explicit] The paper finds that decomposition improves identification of legal concepts but doesn't always improve reasoning path correctness, suggesting variable effectiveness.
- **Why unresolved**: The paper applies decomposition uniformly across scenarios without analyzing whether certain types of legal concepts or relationships benefit more from decomposition than others.
- **What evidence would resolve it**: Analysis categorizing scenarios by legal concept complexity and testing whether decomposition effectiveness correlates with specific characteristics of the legal issues involved.

## Limitations
- Evaluation limited to 50 scenarios from only two specific legal domains (Malaysian Contract Law and Australian Social Act for Dependent Children)
- Human evaluation process relies on subjective scoring using defined rubrics, which may introduce rater bias
- Study uses GPT-3.5 without exploring performance differences with more advanced models like GPT-4 or other LLMs

## Confidence
- **High Confidence**: ChatGPT produces correct conclusions but fails to generate correct reasoning paths; providing human-authored reasoning paths dramatically improves final answer accuracy.
- **Medium Confidence**: Decomposing complex legal questions improves legal concept identification accuracy; in-context learning shows moderate benefits.
- **Low Confidence**: The semi-structured language format for annotations is effective for evaluation; performance trends would hold with different legal domains or larger datasets.

## Next Checks
1. **Cross-domain validation**: Test the same methodology on legal scenarios from common law systems (e.g., US or UK) to assess generalizability beyond the Malaysian and Australian contexts used in this study.

2. **Model comparison**: Evaluate GPT-4 and other contemporary LLMs using identical prompts and scenarios to determine if performance improvements are model-specific or represent general LLM capabilities.

3. **Automated evaluation validation**: Compare human evaluation scores with automated metrics (BLEU, ROUGE, semantic similarity) to assess whether scalable automated evaluation could replace or supplement human scoring.