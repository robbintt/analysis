---
ver: rpa2
title: 'CKBP v2: Better Annotation and Reasoning for Commonsense Knowledge Base Population'
arxiv_id: '2304.10392'
source_url: https://arxiv.org/abs/2304.10392
tags:
- knowledge
- ckbp
- commonsense
- population
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CKBP v2 introduces a high-quality evaluation set for commonsense
  knowledge base population, addressing issues of mislabeled answers and misalignment
  with external knowledge sources in its predecessor CKBP v1. The dataset employs
  expert annotation and adversarial samples to create a more representative and challenging
  benchmark.
---

# CKBP v2: Better Annotation and Reasoning for Commonsense Knowledge Base Population

## Quick Facts
- arXiv ID: 2304.10392
- Source URL: https://arxiv.org/abs/2304.10392
- Authors: 
- Reference count: 40
- Key outcome: CKBP v2 addresses mislabeled answers and misalignment issues in CKBP v1 through expert annotation and adversarial samples, creating a more challenging benchmark for commonsense reasoning

## Executive Summary
CKBP v2 introduces a high-quality evaluation set for commonsense knowledge base population that addresses significant issues in its predecessor, CKBP v1. The dataset employs expert annotation and adversarial sampling to create a more representative and challenging benchmark for evaluating commonsense reasoning models. With 5,000 instances covering 15 relations, CKBP v2 achieves 90.55% inner-annotator agreement and serves as a rigorous test bed for future research in commonsense reasoning and knowledge base population.

## Method Summary
CKBP v2 constructs its evaluation set by combining 2,500 instances from CKBP v1 with 2,500 newly generated adversarial samples. Expert annotators with machine commonsense experience provide triple plausibility judgments using a three-point Likert scale, with final labels determined by consensus. Adversarial instances are created by identifying triples that KG-BERT confidently predicts as plausible but are actually implausible, then filtering for diversity using self-BLEU. The dataset is evaluated using AUC and F1 scores across baseline models including KG-BERT, COMET, and large language models like GPT-3.5 and ChatGPT.

## Key Results
- Expert annotators achieve 90.55% inner-annotator agreement on plausibility judgments
- CKBP v2 contains 5,000 instances (2.5k from CKBP v1, 2.5k adversarial samples) covering 15 relations
- Human performance significantly exceeds all baseline models, demonstrating the task's remaining difficulty
- Models struggle with out-of-domain instances, with performance gaps between in-domain and out-of-domain subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert annotation addresses mislabeled answers in CKBP v1
- Mechanism: Domain experts with machine commonsense experience provide more accurate triple plausibility judgments than crowdsourced annotators
- Core assumption: Experts can consistently apply nuanced relation definitions better than non-experts
- Evidence anchors: Abstract states CKBP v2 addresses mislabeled answers; section describes expert annotation process with three-point Likert scale and consensus-based labeling

### Mechanism 2
- Claim: Adversarial samples create a more representative and challenging benchmark
- Mechanism: Adding instances that KG-BERT confidently predicts as plausible (but are actually implausible) creates hard negative examples that better reflect the true distribution of commonsense knowledge
- Core assumption: Models' confident but incorrect predictions represent challenging cases that expose model weaknesses
- Evidence anchors: Abstract mentions adversarial samples make evaluation data more representative; section describes using KG-BERT predictions and self-BLEU diversity filtering

### Mechanism 3
- Claim: High inner-annotator agreement validates dataset quality
- Mechanism: 90.55% agreement between expert annotators indicates clear consensus on plausibility judgments, suggesting the dataset captures well-defined commonsense knowledge
- Core assumption: High agreement between multiple annotators indicates the dataset captures genuine semantic distinctions rather than arbitrary judgments
- Evidence anchors: Section reports 90.55% average inner-annotator agreement; abstract mentions expert annotators achieving this agreement

## Foundational Learning

- Concept: Commonsense Knowledge Bases (CSKBs)
  - Why needed here: The paper addresses CKBP v2 as an evaluation set for CSKB population - understanding what CSKBs are and how they differ from traditional knowledge bases is essential
  - Quick check question: What distinguishes commonsense knowledge bases from factual knowledge bases, and why is this distinction important for the population task?

- Concept: Knowledge Base Population vs Completion
  - Why needed here: The paper explicitly contrasts population (which deals with unseen entities/events) with completion (which uses close-world assumption) - understanding this distinction is crucial for grasping the paper's contributions
  - Quick check question: How does CSKB Population differ from CSKB Completion in terms of assumptions about the knowledge space and the entities/events involved?

- Concept: Adversarial Sampling in Dataset Construction
  - Why needed here: The paper introduces adversarial instances as a key innovation - understanding how adversarial sampling works and its purpose in dataset construction is essential
  - Quick check question: What is the purpose of adversarial sampling in dataset construction, and how does it differ from random sampling in terms of the types of examples it produces?

## Architecture Onboarding

- Component map: Data preparation pipeline (CKBP v1 sampling + adversarial instance generation + diversity filtering) -> Expert annotation system (recruitment → guidelines → annotation → conflict resolution → label aggregation) -> Model evaluation framework (zero-shot LLMs → supervised learning → semi-supervised models → human comparison)
- Critical path: Data preparation → Expert annotation → Dataset creation → Model evaluation → Analysis
- Design tradeoffs: Expert annotation provides higher quality but is more expensive and slower than crowdsourcing; adversarial sampling creates more challenging data but may introduce bias; focusing on precision/recall rather than AUC better reflects real-world usage but may disadvantage models trained on balanced data
- Failure signatures: Low agreement between annotators suggests unclear guidelines or inherently ambiguous relations; models performing well on ID but poorly on OOD/Adv suggests poor generalization; high artifact presence suggests surface-level correlations rather than genuine semantic understanding
- First 3 experiments:
  1. Run KG-BERT with BERT-base encoder on the full CKBP v2 test set to establish baseline performance
  2. Test ChatGPT (gpt-3.5-turbo) on the same test set to compare zero-shot performance against supervised methods
  3. Evaluate model performance separately on ID, OOD, and Adv subsets to identify generalization weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance gap between in-domain and out-of-domain instances in CKBP v2 be effectively reduced for future models?
- Basis in paper: [explicit] The paper highlights that models, including large language models, struggle with out-of-domain (OOD) instances, which constitute a significant portion of the dataset, and that the performance on OOD subsets is notably lower than on in-domain and adversarial subsets
- Why unresolved: The paper identifies the challenge but does not propose or evaluate specific strategies to address the generalization issue across unseen entities and events
- What evidence would resolve it: Developing and testing models that incorporate mechanisms for better generalization, such as pre-training on diverse datasets or using domain adaptation techniques, and demonstrating improved performance on OOD instances

### Open Question 2
- Question: What specific characteristics of adversarial instances in CKBP v2 make them particularly challenging for current models, and how can models be adapted to handle these cases better?
- Basis in paper: [explicit] The paper introduces adversarial instances as a new component of CKBP v2, noting that these instances are confidently predicted as plausible by baseline models but are often implausible, thus serving as hard negatives
- Why unresolved: The paper does not delve into the specific linguistic or logical features of these adversarial instances that contribute to their difficulty, nor does it suggest methods for improving model robustness against such examples
- What evidence would resolve it: Analyzing the linguistic and logical patterns in adversarial instances and designing models that can better identify and reject such hard negatives

### Open Question 3
- Question: How can the annotation process for CKBP v2 be further improved to reduce ambiguity and increase inter-annotator agreement?
- Basis in paper: [explicit] The paper reports an inter-annotator agreement (IAA) of 90.55%, indicating a high level of consistency, but also acknowledges the inherent ambiguity in some relations and the challenges in discriminating them
- Why unresolved: While the paper outlines the annotation guidelines and quality control measures, it does not explore additional strategies to further enhance agreement or reduce ambiguity
- What evidence would resolve it: Implementing more detailed annotation guidelines, using multiple rounds of annotation with feedback, or employing advanced annotation tools to track and resolve disagreements, and measuring the impact on IAA

### Open Question 4
- Question: What role do artifacts play in the evaluation of CKBP v2, and how can their impact on model performance be minimized?
- Basis in paper: [explicit] The paper conducts an artifacts analysis and identifies a small number of artifacts in CKBP v2, noting that they do not significantly affect the dataset quality due to their low frequency
- Why unresolved: The paper does not explore the potential impact of these artifacts on model evaluation or propose methods to further minimize their presence
- What evidence would resolve it: Conducting a detailed study on how artifacts might influence model performance and developing strategies to identify and mitigate their impact on evaluation

## Limitations

- Expert annotation, while higher quality than crowdsourcing, is more expensive and slower to implement
- Adversarial sampling may introduce bias if the diversity filter removes challenging but important cases
- The 90.55% agreement rate, while high, doesn't fully validate the dataset's coverage of the true commonsense knowledge distribution

## Confidence

- High confidence: The dataset construction methodology and expert annotation process are well-documented and reproducible
- Medium confidence: The claim that adversarial samples make the benchmark more representative, as effectiveness depends on the quality of KG-BERT's initial predictions
- Low confidence: The assertion that 90.55% agreement indicates clear semantic distinctions, as high agreement could also result from overly conservative annotation rules

## Next Checks

1. Conduct ablation studies removing the adversarial component to quantify its impact on model performance and benchmark difficulty
2. Perform error analysis on false positives in the Adv subset to verify they represent genuinely challenging cases rather than annotation inconsistencies
3. Test additional expert annotators on a random sample to verify the stability of the 90.55% agreement rate across different annotator pools