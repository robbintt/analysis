---
ver: rpa2
title: Manifold Learning with Sparse Regularised Optimal Transport
arxiv_id: '2307.09816'
source_url: https://arxiv.org/abs/2307.09816
tags:
- matrix
- optimal
- kernel
- manifold
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel manifold learning approach based on
  sparse regularised optimal transport. The core idea is to construct an affinity
  matrix by projecting the linear kernel matrix onto the set of hollow bistochastic
  matrices under the Frobenius norm, which yields a sparse and adaptive affinity matrix.
---

# Manifold Learning with Sparse Regularised Optimal Transport

## Quick Facts
- **arXiv ID**: 2307.09816
- **Source URL**: https://arxiv.org/abs/2307.09816
- **Reference count**: 40
- **Primary result**: Novel manifold learning approach using sparse regularised optimal transport shows superior performance to competing methods on synthetic and real data, with theoretical guarantees for noise robustness and convergence.

## Executive Summary
This paper introduces a novel manifold learning method based on sparse regularised optimal transport. The approach constructs a sparse affinity matrix by projecting the linear kernel matrix onto hollow bistochastic matrices under the Frobenius norm. This projection naturally enforces sparsity by setting some off-diagonal entries to zero, creating a graph that only connects nearby points on the manifold. The method demonstrates theoretical guarantees for robustness to heteroskedastic noise and convergence to a Laplace-type operator in the continuous limit, while also showing superior empirical performance compared to competing methods in various applications.

## Method Summary
The method constructs an affinity matrix by projecting the linear kernel matrix onto the set of hollow bistochastic matrices under the Frobenius norm. This quadratic regularisation yields a sparse and adaptive affinity matrix that preserves manifold geometry. The approach is computationally efficient, especially when combined with an active set method, and can be applied to tasks like spectral clustering and single-cell RNA sequencing data analysis.

## Key Results
- Theoretical analysis shows robustness to heteroskedastic noise with convergence rate O(p^(-1/2))
- Discrete operator converges to a Laplace-type operator in the continuous limit as N → ∞
- Superior performance compared to EOT and k-NN methods on synthetic Gaussian mixture data and single-cell RNA-seq dataset
- Computational efficiency demonstrated through active set method implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Frobenius norm projection of the linear kernel matrix yields a sparse affinity matrix that better preserves manifold geometry than dense projections.
- Mechanism: The projection onto hollow bistochastic matrices in Frobenius norm enforces sparsity by setting some off-diagonal entries to exactly zero (whenever u⋆i + u⋆j ≤ Cij), creating a sparse graph that only connects nearby points. This matches the local homeomorphism property of manifolds.
- Core assumption: The manifold is locally Euclidean, so only nearby points should be connected in the affinity graph.
- Evidence anchors:
  - [abstract]: "constructs a sparse and adaptive affinity matrix" and "leading to practical benefits for downstream tasks"
  - [section 1.2.3]: "the primal problem (4) admits a counterpart... W(f)ε(C) = 1/ε[u⋆1⊤ + 1u⋆⊤ − C]+ ⊙ (11⊤ − I)" and "some off-diagonal entries may be identically zero"
  - [corpus]: Weak evidence - only mentions sparse domain transfer, not directly related to manifold learning

### Mechanism 2
- Claim: The method is robust to heteroskedastic noise because the quadratic regularisation naturally cancels additive noise.
- Mechanism: The dual formulation shows that additive heteroskedastic noise contributes only rank-one perturbations to the cost matrix, which the Frobenius projection is invariant to (Lemma 2). The noise variance terms add to the dual potential, effectively normalizing it away.
- Core assumption: The noise is additive and has bounded variance in all directions (∥Σi∥2 ≤ κηp−1).
- Evidence anchors:
  - [abstract]: "establish robustness to heteroskedastic noise"
  - [section 2.2.1]: "W(f)ε(C) = W(f)ε(C + η ⊕ η)" and "if u⋆ is the optimal dual transport potential for the uncontaminated setting, ˜u⋆i := u⋆i + E∥ηi∥2 is an approximate optimal dual potential"
  - [section 2.2.1]: "Proposition 1. Consider the heteroskedastic noise model... ∥W(f)ε(C) − W(f)ε(˜C)∥ ≤ O(p−1/2)"
  - [corpus]: No direct evidence about noise robustness

### Mechanism 3
- Claim: The discrete operator converges to a Laplace-type operator in the continuous limit, making it theoretically sound for manifold learning.
- Mechanism: As N → ∞ with appropriate scaling ε ∝ N^(−4/(d+2)), the discrete operator ∆OT converges in L2 to a continuous Laplace-type operator. The convergence is established through bias-variance decomposition and the dual potential scaling.
- Core assumption: The data points are uniformly sampled from a smooth compact manifold with bounded curvature.
- Evidence anchors:
  - [abstract]: "proves that the resulting kernel is consistent with a Laplace-type operator in the continuous limit"
  - [section 2.3]: "Theorem 1. Consider g ∈ C2(Rp)... 2K−1ε,N ∆OT g(X0) L2 → ∆∞g"
  - [section 2.4]: "the operator resulting from quadratically regularised optimal transport falls under their framework" and discusses compatibility with general convergence results
  - [corpus]: No direct evidence about convergence to Laplace operators

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: The method is fundamentally based on optimal transport with quadratic regularisation, which requires understanding dual formulations and projections.
  - Quick check question: What is the relationship between the primal and dual problems in quadratic regularised optimal transport?

- Concept: Manifold Geometry and Differential Operators
  - Why needed here: Understanding how discrete operators converge to continuous Laplace-Beltrami operators on manifolds.
  - Quick check question: How does the Laplace-Beltrami operator relate to the graph Laplacian in the continuous limit?

- Concept: Bistochastic Matrices and Projections
  - Why needed here: The method projects onto the set of hollow bistochastic matrices, requiring knowledge of their properties and projection algorithms.
  - Quick check question: What conditions must a matrix satisfy to be bistochastic, and why is the hollow constraint important?

## Architecture Onboarding

- Component map: Data → Distance Matrix C → Quadratic Regularised OT Solver → Sparse Bistochastic Affinity W → Spectral Decomposition → Laplacian L → Downstream Analysis (clustering, embedding, etc.)
- Critical path: Distance computation → OT solver → Sparsity enforcement → Spectral decomposition
- Design tradeoffs: Sparsity vs completeness of graph connections; computational efficiency vs spectral quality; choice of regularisation parameter ε affecting both convergence and noise robustness
- Failure signatures: Dense affinity matrices (wrong projection type); poor spectral decomposition (bad ε choice); memory issues with large N (need active set method); poor performance on non-uniform data (violated sampling assumptions)
- First 3 experiments:
  1. Verify sparsity: For synthetic manifold data, check that W has expected number of non-zero entries and that sparsity pattern matches local neighborhoods
  2. Test noise robustness: Add heteroskedastic noise to clean data and compare spectral embeddings before/after projection
  3. Parameter sweep: For varying ε, measure spectral embedding quality (e.g., subspace angle to reference embedding) to find optimal range

## Open Questions the Paper Calls Out

- Question: What is the precise relationship between quadratically regularized optimal transport and the porous medium equation, and can a formal connection be established?
- Basis in paper: [explicit] The paper discusses a similarity between the solution of quadratically regularized optimal transport and the Barenblatt-Prattle solution of the porous medium equation, but does not formally establish a connection.
- Why unresolved: The paper only provides a heuristic argument and a similarity in the exponents and compact support of the solutions, but does not prove a rigorous connection.
- What evidence would resolve it: A rigorous mathematical proof establishing a formal connection between the two problems, potentially through the Wasserstein gradient flow of the Tsallis entropy.

- Question: What is the limiting operator that arises from the quadratically regularized optimal transport scheme for other settings, such as non-uniform distributions on the manifold?
- Basis in paper: [inferred] The paper proves convergence of the operator to a Laplace-type operator for the uniform distribution on a manifold, but does not explore other distributions.
- Why unresolved: The analysis in the paper is specific to the uniform distribution case, and extending it to other distributions would require a different approach.
- What evidence would resolve it: A rigorous mathematical proof establishing the limiting operator for non-uniform distributions on the manifold, potentially using different techniques than those used in the paper.

- Question: What is the finite sample behavior of the quadratically regularized optimal transport method, and under what conditions does it fail to perform well?
- Basis in paper: [explicit] The paper mentions the need for a finer description of the finite sample case and understanding when the method is not suitable, but does not provide a comprehensive analysis.
- Why unresolved: The paper focuses on the asymptotic behavior of the method, and does not provide a detailed analysis of its performance for finite sample sizes.
- What evidence would resolve it: A comprehensive analysis of the finite sample behavior of the method, including conditions under which it performs well and conditions under which it fails, potentially through numerical experiments and theoretical analysis.

## Limitations
- Theoretical claims rely on specific sampling assumptions (uniform sampling, bounded curvature) that may not hold in real-world applications
- Active set method for large datasets is mentioned but not fully specified, affecting computational efficiency claims
- Limited validation across diverse real-world datasets beyond synthetic and single-cell data

## Confidence
- Mechanism 1 (sparsity preservation): High - well-supported by theoretical analysis and empirical results
- Mechanism 2 (noise robustness): Medium - strong theoretical derivation but limited empirical validation
- Mechanism 3 (convergence to Laplace operator): High - rigorous mathematical proof provided
- Overall method efficacy: Medium - strong results but limited dataset diversity

## Next Checks
1. Test robustness to non-uniform sampling by generating data from non-uniform distributions and comparing spectral embeddings to ground truth
2. Implement and benchmark the active set method for scalability on datasets with N > 10,000 points
3. Validate noise robustness empirically by adding structured heteroskedastic noise to clean manifolds and measuring degradation in spectral clustering performance