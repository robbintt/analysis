---
ver: rpa2
title: 'FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following
  Capability of Large Language Models'
arxiv_id: '2311.09829'
source_url: https://arxiv.org/abs/2311.09829
tags:
- llms
- benchmark
- followeval
- reasoning
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FollowEval is a new benchmark for evaluating the instruction-following
  capabilities of large language models (LLMs) in both English and Chinese. It comprises
  200 manually curated test instances designed to assess LLMs across diverse dimensions
  of instruction-following, including string manipulation, commonsense reasoning,
  logical reasoning, spatial reasoning, and adherence to response constraints.
---

# FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models

## Quick Facts
- arXiv ID: 2311.09829
- Source URL: https://arxiv.org/abs/2311.09829
- Authors: Multiple
- Reference count: 3
- Key outcome: Substantial gap between LLMs and human performance on instruction-following tasks, with GPT-4 achieving 77.5% accuracy versus 100% human accuracy

## Executive Summary
FollowEval is a new benchmark designed to comprehensively evaluate the instruction-following capabilities of large language models across multiple dimensions. The benchmark consists of 200 manually curated test instances in both English and Chinese, incorporating string manipulation, commonsense reasoning, logical reasoning, spatial reasoning, and response constraints. Each test instance requires models to address multiple essential elements for a fully correct response. The evaluation uses handcrafted regex rules for automated verification, revealing a significant performance gap between current LLMs and human-level instruction-following ability.

## Method Summary
The FollowEval benchmark was constructed through a multi-stage human curation process involving six experts drafting instructions, two experts verifying them, and two specialists designing corresponding regex rules for each test instance. The benchmark contains 200 test instances covering five essential elements: string manipulation, commonsense reasoning, logical reasoning, spatial reasoning, and adherence to response constraints. LLMs are evaluated using nucleus sampling and top-k sampling decoding strategies, with responses verified against manually designed regex rules. Each model is evaluated three times to account for randomness, and accuracy is calculated as the proportion of correctly responded test instances.

## Key Results
- GPT-4 achieved the highest LLM accuracy at 77.5% on FollowEval
- Human performance reached a perfect 100% accuracy on the benchmark
- All tested LLMs showed substantial performance gaps compared to human instruction-following capability
- The benchmark successfully revealed varying strengths across different LLM models and essential elements

## Why This Works (Mechanism)

### Mechanism 1
Manual curation ensures higher quality test instances than automated construction. Human experts create test examples with multiple essential elements, increasing complexity and realism. Core assumption: Human-authored instructions better capture real-world instruction-following requirements than templated or rule-based approaches.

### Mechanism 2
Multi-dimensional essential elements increase benchmark difficulty and comprehensiveness. Each test instance incorporates multiple essential elements that must all be correctly addressed. Core assumption: Real-world instructions require simultaneous handling of multiple cognitive and linguistic dimensions.

### Mechanism 3
Handcrafted regex rules enable reliable automatic evaluation. Manual regex design provides lightweight, consistent evaluation compared to LLM-based scoring. Core assumption: Regex rules can capture correctness criteria for diverse instruction-following tasks.

## Foundational Learning

- Concept: Instruction tuning and RLHF alignment
  - Why needed here: Understanding how LLMs are trained to follow instructions provides context for evaluating their instruction-following capabilities
  - Quick check question: What is the difference between instruction tuning and reinforcement learning from human feedback (RLHF)?

- Concept: Benchmark construction methodologies
  - Why needed here: Knowing the tradeoffs between automated vs. manual benchmark creation helps understand FollowEval's design choices
  - Quick check question: What are the potential advantages and disadvantages of using automated approaches vs. human curation for benchmark construction?

- Concept: Automatic evaluation of natural language generation
  - Why needed here: Understanding the challenges of automatic evaluation contextualizes FollowEval's regex-based approach
  - Quick check question: Why is automatic evaluation of natural language generation considered an open challenge?

## Architecture Onboarding

- Component map: Instruction authoring module (human experts) → Verification pipeline (peer review) → Regex rule design (specialists) → Test instance storage (200 instances across English/Chinese) → Evaluation engine (regex matching) → Performance tracking (accuracy across dimensions)

- Critical path: Human expert drafts instruction → Peer verification → Regex rule design → Test instance storage → Model evaluation → Performance aggregation

- Design tradeoffs: Manual curation ensures quality but limits scalability; regex rules provide reliability but may miss nuanced errors

- Failure signatures: Low accuracy across multiple models suggests benchmark difficulty; inconsistent performance across runs suggests evaluation instability

- First 3 experiments:
  1. Run baseline models on FollowEval to establish performance distribution
  2. Analyze error patterns across essential elements to identify model weaknesses
  3. Test sensitivity of regex rules by introducing controlled perturbations to model responses

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLMs on FollowEval vary across different language pairs (e.g., English-Chinese vs. Chinese-English)? The paper mentions FollowEval covers both English and Chinese but does not provide detailed analysis of performance across language pairs.

### Open Question 2
What is the correlation between LLM performance on FollowEval and their performance on other established benchmarks like MMLU or BigBench? The paper mentions GPT-4 has state-of-the-art performance on diverse benchmarks but does not analyze correlation with FollowEval specifically.

### Open Question 3
How does the difficulty of FollowEval test instances scale with the number of essential elements incorporated? The paper states each test instance incorporates more than one essential element to increase complexity but does not analyze the relationship between element count and difficulty.

### Open Question 4
What specific types of errors do LLMs make most frequently on FollowEval, and how do these vary by model architecture? The paper shows LLMs perform significantly below humans but does not analyze the nature or distribution of specific errors made by different models.

## Limitations
- Heavy reliance on regex-based evaluation may miss nuanced correctness criteria and lacks empirical validation
- 200-instance size may not provide sufficient statistical power for definitive conclusions across all instruction-following dimensions
- Benchmark focuses on English and Chinese, limiting generalizability to other languages and cultural contexts

## Confidence
- **High Confidence**: Benchmark construction methodology is well-documented and results follow logically from the described evaluation procedure
- **Medium Confidence**: Substantial LLM-human performance gaps are reasonable given the results, but 100% human accuracy baseline assumes perfect human performance without variance reporting
- **Low Confidence**: Assertion that regex-based evaluation is superior to LLM-based approaches lacks empirical evidence comparing error rates or computational costs

## Next Checks
1. Conduct systematic testing of regex rules by introducing controlled perturbations to correct responses to quantify false negative rates
2. Replicate human evaluation with multiple annotators to establish confidence intervals around the 100% accuracy claim
3. Evaluate benchmark's English and Chinese test instances for cultural and linguistic biases to validate multilingual claims