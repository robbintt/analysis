---
ver: rpa2
title: On the Convergence of the Gradient Descent Method with Stochastic Fixed-point
  Rounding Errors under the Polyak-Lojasiewicz Inequality
arxiv_id: '2301.09511'
source_url: https://arxiv.org/abs/2301.09511
tags:
- rounding
- convergence
- when
- have
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes how fixed-point rounding errors affect the\
  \ convergence of the gradient descent method for problems satisfying the Polyak-Lojasiewicz\
  \ inequality. The authors identify three cases based on the magnitude of the updating\
  \ vector relative to rounding errors, and prove that stochastic rounding (SR) preserves\
  \ linear convergence rate while \u03B5-biased stochastic rounding (SR\u03B5) provides\
  \ a stricter convergence bound."
---

# On the Convergence of the Gradient Descent Method with Stochastic Fixed-point Rounding Errors under the Polyak-Lojasiewicz Inequality

## Quick Facts
- arXiv ID: 2301.09511
- Source URL: https://arxiv.org/abs/2301.09511
- Reference count: 4
- Key outcome: This paper analyzes how fixed-point rounding errors affect the convergence of the gradient descent method for problems satisfying the Polyak-Lojasiewicz inequality, identifying three cases based on gradient magnitude relative to rounding errors, and proving that stochastic rounding preserves linear convergence while ε-biased stochastic rounding provides faster convergence.

## Executive Summary
This paper investigates the convergence behavior of gradient descent when implemented with fixed-point arithmetic and stochastic rounding schemes. The authors analyze three distinct cases based on the relationship between gradient magnitudes and rounding precision, proving that stochastic rounding preserves linear convergence under the Polyak-Lojasiewicz inequality. They introduce ε-biased stochastic rounding (SRε) as an enhancement that accelerates convergence by introducing controlled rounding bias. The work demonstrates fundamental differences between fixed-point and floating-point implementations, showing that SRε exhibits adaptive step-size behavior in floating-point arithmetic.

## Method Summary
The authors analyze gradient descent with fixed-point arithmetic where rounding errors are modeled as stochastic processes. They classify convergence into three cases based on whether gradient components are larger than, comparable to, or smaller than the rounding precision. The analysis employs the Polyak-Lojasiewicz inequality to establish convergence bounds, examining both unbiased stochastic rounding (SR) and ε-biased stochastic rounding (SRε). The theoretical framework is validated through numerical experiments on quadratic functions, Rosenbrock's function, Himmelblau's function, logistic regression, and neural network training tasks.

## Key Results
- Stochastic rounding (SR) preserves linear convergence rate when gradient magnitudes exceed rounding precision
- ε-biased stochastic rounding (SRε) provides faster convergence than SR by introducing controlled rounding bias
- SRε with floating-point arithmetic exhibits adaptive step sizes proportional to iterate magnitudes, while fixed-point implementation behaves like a combination of gradient descent and stochastic sign gradient descent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic rounding (SR) preserves linear convergence rate for gradient descent under the Polyak-Lojasiewicz (PL) inequality when fixed-point rounding errors are small relative to gradients.
- Mechanism: SR introduces zero-mean random rounding errors, which on average preserve the gradient direction and magnitude. This maintains the descent property and allows the PL inequality to bound convergence similarly to exact arithmetic.
- Core assumption: The rounding error magnitude is bounded such that |σ(k)₁,i| < |∇f(˜x(k))i| for all components, ensuring non-opposite sign property (Condition 14).
- Evidence anchors:
  - [abstract] "SR preserves linear convergence rate while ε-biased stochastic rounding (SRε) provides a stricter convergence bound."
  - [section] "Lemma 6 Under Universal Assumptions, if SR is applied for evaluating σ1, then it holds E [∇f(˜x(k))T (∇f(˜x(k)) +σ(k)₁) ]≃ E [∥∇f(˜x(k))∥²]"
- Break condition: When gradients become too small relative to rounding precision (Case II/III), SR cannot maintain the descent property and convergence may slow.

### Mechanism 2
- Claim: ε-biased stochastic rounding (SRε) provides faster convergence than SR by introducing a controlled rounding bias in the descent direction.
- Mechanism: SRε biases rounding errors toward zero by a factor ε, creating a systematic downward correction that accelerates convergence beyond what unbiased SR achieves.
- Core assumption: The parameter ε ∈ (0,1) is chosen small enough to maintain convergence stability while providing acceleration benefits.
- Evidence anchors:
  - [abstract] "SRε provides a stricter convergence bound than the one achieved by unbiased stochastic rounding."
  - [section] "Lemma 10 If SRε is employed for evaluating σ2 for the kth iteration step of GD, then we have that ρk≤ 2tε"
- Break condition: If ε is too large (>1), the systematic bias may destabilize convergence or cause oscillations near the optimum.

### Mechanism 3
- Claim: The convergence behavior differs fundamentally between fixed-point and floating-point arithmetic when using SRε due to non-uniform number distribution in floating-point.
- Mechanism: In floating-point arithmetic, SRε creates adaptive step sizes proportional to the current iterate magnitude, while in fixed-point arithmetic the step size remains uniform across coordinates.
- Core assumption: IEEE floating-point numbers have non-uniform distribution allowing adaptive step size behavior.
- Evidence anchors:
  - [section] "SR ε with low-precision floating-point computation performs as a gradient descent method with automatically adapted stepsizes in each coordinate of the current iterate"
  - [section] "SR ε with low-precision fixed-point computation behaves as a combination of vanilla gradient descent and a stochastic sign gradient descent method"
- Break condition: When the number format lacks sufficient precision to represent the adaptive behavior, the advantage disappears.

## Foundational Learning

- Concept: Polyak-Lojasiewicz inequality
  - Why needed here: Provides the theoretical foundation for linear convergence of gradient descent, replacing the need for strong convexity in many machine learning problems.
  - Quick check question: What condition must a function satisfy to guarantee linear convergence of gradient descent without requiring strong convexity?

- Concept: Stochastic rounding and its zero-mean property
  - Why needed here: Explains why unbiased stochastic rounding preserves convergence properties despite introducing random errors.
  - Quick check question: How does the zero-mean property of stochastic rounding ensure that the expected gradient direction is preserved?

- Concept: Fixed-point vs floating-point number representation
  - Why needed here: Different rounding behaviors in these formats lead to different convergence characteristics, particularly for SRε.
  - Quick check question: Why does the same rounding strategy (SRε) behave differently in fixed-point versus floating-point arithmetic?

## Architecture Onboarding

- Component map: Gradient computation → Stochastic rounding (SR/SRε) → Fixed-point quantization → Update step → Objective evaluation
- Critical path: Compute gradient → Apply stochastic rounding → Update parameters → Check convergence → Repeat
- Design tradeoffs:
  - Precision vs convergence speed: Higher precision reduces rounding errors but increases computational cost
  - ε parameter in SRε: Larger ε provides faster convergence but may cause oscillations
  - Working precision vs multiplication precision: Different precisions can be used for different operations to balance accuracy and efficiency
- Failure signatures:
  - Stagnation with RN when gradients become small relative to precision
  - Oscillation with large ε in SRε near the optimum
  - Divergence if t exceeds 1/(4L) with rounding errors present
- First 3 experiments:
  1. Implement SR and SRε for fixed-point arithmetic and verify linear convergence on a quadratic function satisfying PL inequality
  2. Compare convergence rates of SR vs SRε with varying ε on Rosenbrock's function
  3. Test fixed-point vs floating-point behavior of SRε on a least squares problem with different scale components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of gradient descent with stochastic fixed-point rounding errors compare to exact arithmetic when the Polyak-Lojasiewicz inequality is not satisfied?
- Basis in paper: [inferred] The paper analyzes convergence under the PL inequality, but does not address cases where this condition is not met.
- Why unresolved: The analysis is specifically developed for problems satisfying the PL condition, and the authors do not explore the behavior outside this framework.
- What evidence would resolve it: Numerical experiments and theoretical analysis of gradient descent convergence for problems that do not satisfy the PL inequality with various rounding methods and precision levels.

### Open Question 2
- Question: What is the optimal choice of the parameter ε in ε-biased stochastic rounding (SRε) for different machine learning problems and number formats?
- Basis in paper: [explicit] The paper shows that SRε can accelerate convergence, but does not provide a systematic method for choosing ε.
- Why unresolved: The authors demonstrate the benefits of SRε but leave the selection of ε as an open problem, noting that a larger ε may lead to faster convergence but could also cause oscillations.
- What evidence would resolve it: A comprehensive study of the effect of ε on convergence rates across various problems and number formats, potentially leading to guidelines for choosing ε based on problem characteristics.

### Open Question 3
- Question: How do the convergence properties of gradient descent with stochastic fixed-point rounding errors change when using momentum-based methods or adaptive learning rates?
- Basis in paper: [inferred] The paper focuses on vanilla gradient descent and does not explore the impact of momentum or adaptive learning rates on convergence with rounding errors.
- Why unresolved: The analysis is limited to gradient descent with fixed step sizes, and the authors do not investigate how more advanced optimization techniques interact with rounding errors.
- What evidence would resolve it: Theoretical analysis and numerical experiments comparing the convergence of momentum-based and adaptive learning rate methods with different rounding strategies and precision levels.

## Limitations
- The analysis is restricted to problems satisfying the Polyak-Lojasiewicz inequality, excluding many practical optimization scenarios
- The three-case classification scheme requires precise bounds between gradient magnitude and rounding precision that may be difficult to verify in practice
- The theoretical framework assumes deterministic gradient descent rather than stochastic gradient descent used in large-scale machine learning

## Confidence
- High confidence: The zero-mean property of stochastic rounding preserving expected descent direction (Mechanism 1)
- Medium confidence: The convergence rate bounds for Cases I and II under SR (Theorem 1)
- Low confidence: The adaptive step-size behavior of SRε in floating-point arithmetic (Mechanism 3)

## Next Checks
1. **Empirical validation of Case III behavior**: Implement numerical experiments to verify the O(1/√k) convergence rate prediction for Case III, as this represents the most challenging scenario where rounding errors dominate.

2. **ε-parameter sensitivity analysis**: Systematically vary the ε parameter in SRε across multiple test problems to identify the optimal range that balances convergence acceleration with stability, particularly near optima.

3. **Comparison with alternative rounding schemes**: Benchmark SRε against other low-precision training methods (like the "tail averaging" technique mentioned in related work) on realistic deep learning tasks to assess practical advantages.