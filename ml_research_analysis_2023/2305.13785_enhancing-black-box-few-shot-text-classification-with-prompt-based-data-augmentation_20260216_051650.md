---
ver: rpa2
title: Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation
arxiv_id: '2305.13785'
source_url: https://arxiv.org/abs/2305.13785
tags:
- language
- data
- text
- bt-classifier
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BT-Classifier, a parameter-efficient approach
  for few-shot text classification using black-box large language models (LLMs). The
  key idea is to treat the black-box LLM as a feature extractor, use its hidden states
  as input to a small MLP classifier, and augment the training data via prompt-based
  finetuning on an auxiliary language model.
---

# Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation

## Quick Facts
- arXiv ID: 2305.13785
- Source URL: https://arxiv.org/abs/2305.13785
- Reference count: 0
- Primary result: BT-Classifier achieves 80.3% average accuracy across 8 text classification tasks, outperforming state-of-the-art black-box few-shot learners

## Executive Summary
This paper presents BT-Classifier, a parameter-efficient approach for few-shot text classification using black-box large language models (LLMs). The key innovation treats the black-box LLM as a feature extractor, using its hidden states as input to a small MLP classifier, while augmenting training data through prompt-based finetuning on an auxiliary language model. This approach avoids expensive gradient-based tuning of the LLM while leveraging its semantic representation capabilities. BT-Classifier significantly outperforms existing black-box few-shot learners on 8 text classification tasks and matches the performance of full-model tuning approaches, all while requiring only a single GPU for training.

## Method Summary
BT-Classifier treats a black-box LLM as a feature extractor, extracting "[MASK]" token hidden states from multiple transformer layers (L-3 to L) and applying max pooling to create vector representations. These vectors serve as input to a 2-layer MLP classifier. Data augmentation is performed using prompt-based finetuning on an auxiliary DeBERTa-Base model, which pseudo-labels unlabeled data with a 90% confidence threshold. The augmented data is balanced across classes and combined with the original training set for final model training.

## Key Results
- Achieves 80.3% average accuracy across 8 text classification tasks
- Outperforms state-of-the-art black-box few-shot learners (74.8% average accuracy)
- Matches performance of full-model tuning approaches while using only a single GPU
- Demonstrates significant improvements on SST-2 (+8.4%), MRPC (+10.8%), and SNLI (+6.2%) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The black-box LLM serves as a feature extractor, providing semantically rich representations without requiring gradient access.
- Mechanism: The approach uses the hidden states of the "[MASK]" token extracted from multiple transformer layers (L-3 to L) and applies max pooling to create a single vector representation. This vector is then fed into a small MLP classifier.
- Core assumption: The semantic information captured by the "[MASK]" token's hidden states is sufficient for downstream text classification tasks.
- Evidence anchors:
  - [abstract]: "We treat the black-box model as a feature extractor and train a classifier with the augmented text data."
  - [section]: "The hidden states w.r.t. the '[MASK]' token extracted from black-box LLM are used as input to the MLP classifier."
  - [corpus]: Found 25 related papers with average FMR=0.433, but no direct evidence for this specific mechanism.

### Mechanism 2
- Claim: Prompt-based data augmentation significantly improves few-shot learning by leveraging an auxiliary language model to generate additional labeled examples.
- Mechanism: An auxiliary DeBERTa-Base model is prompt-based finetuned using demonstrations to create a teacher model. This teacher then pseudo-labels unlabeled text data, with a confidence threshold of 90% to filter low-quality labels. The augmented data is balanced across classes.
- Core assumption: The auxiliary teacher model can generate high-quality pseudo-labels that improve the classifier's generalization ability.
- Evidence anchors:
  - [abstract]: "Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model."
  - [section]: "We propose to augment DT_train with an auxiliary language model... A filter mechanism is implemented to exclude pseudo-labeled data that the teacher model is less confident about."
  - [corpus]: No direct evidence found in corpus neighbors.

### Mechanism 3
- Claim: Using hidden states from multiple transformer layers provides richer semantic representations than using only the final layer.
- Mechanism: The approach extracts hidden states from layers L-3 to L (last three layers plus the final layer) rather than just the last layer, capturing information from different levels of abstraction.
- Core assumption: Lower transformer layers capture different semantic aspects than higher layers, and combining them provides more comprehensive representations.
- Evidence anchors:
  - [abstract]: No direct mention of multi-layer extraction.
  - [section]: "Despite its simplicity, the approach does not face the above-mentioned limitations... we perform max pooling on {h_i,l[MASK]}_l=L-3 to derive a single vector representation."
  - [corpus]: No direct evidence found in corpus neighbors.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: The approach relies on the "[MASK]" token's hidden states, which are designed to capture semantic information during MLM pretraining.
  - Quick check question: What is the purpose of the "[MASK]" token in transformer-based language models?

- Concept: Prompt-based learning and few-shot learning
  - Why needed here: The method uses prompt templates to reformulate classification tasks and leverages demonstrations for data augmentation.
  - Quick check question: How does prompt-based learning differ from standard fine-tuning in few-shot scenarios?

- Concept: Multi-layer transformer representations
  - Why needed here: The approach extracts hidden states from multiple transformer layers rather than just the final layer.
  - Quick check question: What kind of information do lower vs. higher transformer layers typically capture?

## Architecture Onboarding

- Component map:
  Black-box LLM (RoBERTa-Large) → Hidden state processor (max pooling across layers L-3 to L) → MLP classifier (2-layer with Tanh activation)

- Critical path:
  1. Generate task-specific prompts using templates
  2. Extract "[MASK]" token hidden states from black-box LLM
  3. Apply max pooling across selected layers
  4. Train MLP classifier on augmented data
  5. Evaluate on test set

- Design tradeoffs:
  - Using black-box LLM limits control but reduces computational cost
  - Multi-layer extraction increases representation richness but adds complexity
  - 90% confidence threshold for pseudo-labels balances quality vs. quantity
  - Single GPU requirement makes it accessible but may limit scalability

- Failure signatures:
  - Poor performance on tasks with highly specialized vocabulary
  - Degradation when unlabeled in-domain data is scarce
  - Sensitivity to prompt template quality and label-word mapping
  - Overfitting when augmented data quality is low

- First 3 experiments:
  1. Compare performance using CLS token vs. "[MASK]" token hidden states
  2. Test different confidence thresholds for pseudo-label filtering
  3. Evaluate using single layer vs. multi-layer hidden state extraction

## Open Questions the Paper Calls Out
1. How would BT-Classifier perform on datasets with significant domain shifts compared to the unlabeled data used for augmentation?
2. What is the impact of different template designs on BT-Classifier's performance across various classification tasks?
3. How does BT-Classifier scale when applied to larger classification tasks with more classes or longer text sequences?

## Limitations
- The paper does not provide detailed implementation specifications for key components like the "[MASK]" token pooling mechanism
- Limited ablation studies on critical design choices such as confidence thresholds and layer selection
- Experiments focus primarily on RoBERTa-Large, leaving model-agnostic claims under-validated

## Confidence
- **High Confidence**: The core methodology of using black-box LLMs as feature extractors is sound and the general approach to data augmentation through pseudo-labeling is well-established.
- **Medium Confidence**: The claim that BT-Classifier achieves 80.3% average accuracy requires careful scrutiny due to limited implementation details.
- **Low Confidence**: The specific design choices such as the 90% confidence threshold for pseudo-label filtering and the exact multi-layer pooling mechanism lack sufficient justification or sensitivity analysis.

## Next Checks
1. Reconstruct the exact "[MASK]" token hidden state extraction and pooling mechanism from layers L-3 to L and verify implementation consistency.
2. Systematically evaluate BT-Classifier performance across different pseudo-label confidence thresholds (70%, 80%, 90%, 95%) to determine threshold sensitivity.
3. Compare performance when using only the final layer (L) versus the multi-layer approach (L-3 to L) for "[MASK]" token hidden state extraction.