---
ver: rpa2
title: We're Afraid Language Models Aren't Modeling Ambiguity
arxiv_id: '2304.14399'
source_url: https://arxiv.org/abs/2304.14399
tags:
- ambiguity
- examples
- language
- ambiguous
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce AmbiEnt, a new benchmark to evaluate whether language
  models can recognize ambiguity and disentangle possible meanings. We characterize
  ambiguity through its effect on entailment relations, and collect 1,645 examples
  covering diverse types of lexical, syntactic, and pragmatic ambiguity.
---

# We're Afraid Language Models Aren't Modeling Ambiguity

## Quick Facts
- **arXiv ID**: 2304.14399
- **Source URL**: https://arxiv.org/abs/2304.14399
- **Reference count**: 23
- **Key outcome**: Language models struggle with ambiguity tasks even GPT-4, with generated disambiguations scoring only 32% in human evaluation versus 90% for human-generated ones.

## Executive Summary
This paper introduces AmbiEnt, a new benchmark to evaluate whether language models can recognize and handle ambiguity in natural language. The authors characterize ambiguity through its effect on entailment relations and collect 1,645 examples covering lexical, syntactic, and pragmatic ambiguity. Through three test suites evaluating disambiguation generation, interpretation recognition, and continuation modeling, the study finds that ambiguity handling remains extremely challenging even for recent large language models like GPT-4. The work also demonstrates the value of ambiguity-sensitive tools by using a multilabel NLI model to flag misleading political claims.

## Method Summary
The authors create AmbiEnt by annotating ambiguous sentences with all valid interpretations and corresponding disambiguations. They evaluate pretrained language models on three tasks: generating disambiguations from ambiguous sentences, recognizing valid interpretations through templated prompts, and modeling interpretation-specific continuations. Additionally, they finetune NLI models on existing datasets with label variation to create multilabel NLI systems and evaluate their performance on AmbiEnt. Finally, they apply these models to detect ambiguous political claims in the CLAIM DECOMP dataset.

## Key Results
- GPT-4's generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in the AmbiEnt dataset
- LMs show inconsistent behavior across related templates, with 76% of pairs showing contradictory responses
- Finetuned NLI models achieve below 89.7% human performance on the multilabel NLI task despite training on existing data with label variation
- The multilabel NLI model successfully flags misleading political claims that are ambiguous

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pretrained language models do not inherently learn to model ambiguity despite being trained on large amounts of text.
- **Mechanism**: The pretraining objective (next token prediction) focuses on surface-level statistical patterns rather than deeper semantic distinctions that ambiguity requires.
- **Core assumption**: Ambiguity requires understanding multiple interpretations and their entailment relationships, which is not directly incentivized by standard pretraining.
- **Evidence anchors**: 
  - [abstract] "We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset."
  - [section] "In general, we observe that LMs are not internally consistent across the questions. For instance, for 76% of pairs of disambiguations (d1,d2) for the same ambiguous sentence a, GPT-4 both acknowledges that a may mean d1 and may mean d2 (template 1), yet also asserts that a can only mean d1 and can only mean d2 (template 4)."

### Mechanism 2
- **Claim**: Ambiguity in natural language inference creates systematic label variation that can be resolved by explicit disambiguation.
- **Mechanism**: When premises or hypotheses contain ambiguity, different interpretations lead to different entailment judgments. Explicit disambiguation allows annotators to capture all valid interpretations.
- **Core assumption**: Annotators can reliably distinguish different readings of ambiguous sentences when given disambiguating rewrites.
- **Evidence anchors**:
  - [abstract] "We find that crowdworkers can reliably distinguish different readings of an ambiguous sentence and their impact on entailment choices; thus we can explicitly characterize the underlying reasons for uncertainty that would otherwise surface as 'disagreement' (ยง3)."
  - [section] "Disagreement is largely resolved on the corresponding disambiguated examples (step (iii)), with ฮบ increasing to 0.67, representing 'substantial' agreement."

### Mechanism 3
- **Claim**: Finetuning NLI models on existing data with label variation still leaves large room for improvement on the multilabel NLI task.
- **Mechanism**: While existing NLI datasets contain some examples with label variation due to ambiguity, they do not comprehensively cover the space of ambiguous examples or their interpretations.
- **Core assumption**: The distribution of ambiguous examples in existing NLI datasets does not match the true distribution of ambiguity in natural language.
- **Evidence anchors**:
  - [abstract] "We further show that finetuning NLI models on existing data with label variation still leaves large room for improvement on the multilabel NLI task."
  - [section] "While this is substantially higher than the random-guessing baseline of 1/7 = 14.3% for EM accuracy, it is considerably short of 89.7% human performance."

## Foundational Learning

- **Concept: Lexical semantics and word sense disambiguation**
  - Why needed here: Many ambiguities in AmbiEnt stem from words having multiple meanings (lexical ambiguity), which is fundamental to understanding why models struggle.
  - Quick check question: What is the difference between homonymy and polysemy, and how might each affect entailment judgments?

- **Concept: Syntactic parsing and structural ambiguity**
  - Why needed here: Syntactic ambiguities arise when sentence structure can be parsed in multiple ways, affecting how models interpret relationships between premise and hypothesis.
  - Quick check question: How does attachment ambiguity (e.g., "I saw the man with the telescope") impact the possible interpretations of an NLI example?

- **Concept: Implicature and pragmatic inference**
  - Why needed here: Pragmatic ambiguities arise from context-dependent meanings and conversational implicatures that models must learn to recognize.
  - Quick check question: How might a cancellable implicature (e.g., "Some students passed the exam" implying not all did) create ambiguity in entailment?

## Architecture Onboarding

- **Component map**: AmbiEnt dataset (1,645 examples) -> Pretrained LMs (GPT-3, GPT-4, FLAN-T5, etc.) -> Three test suites (disambiguation generation, recognition, continuation modeling) -> NLI models (roberta-large based) -> Evaluation framework (human evaluation, agreement metrics)

- **Critical path**: Load AmbiEnt dataset and identify ambiguous examples -> Construct prompts with in-context examples and generate disambiguations -> Format templates and query models for True/False judgments -> Generate continuations for each interpretation and compare likelihoods -> Train/finetune NLI models on existing data and evaluate on AmbiEnt

- **Design tradeoffs**: Using in-context learning vs. finetuning for disambiguation generation (tradeoff between flexibility and performance) vs. Including unambiguous examples in AmbiEnt (tradeoff between comprehensive evaluation and dataset size) vs. Manual curation vs. automatic generation for dataset creation (tradeoff between control and scalability)

- **Failure signatures**: Low human evaluation scores despite high automatic metrics (suggesting superficial rather than semantic understanding) -> Inconsistent answers across related templates (indicating lack of coherent interpretation) -> Poor performance on less common interpretations (suggesting bias toward frequent readings)

- **First 3 experiments**: Evaluate a pretrained LM on AmbiEnt's disambiguation generation task to establish baseline performance -> Fine-tune an NLI model on WANLI and evaluate on AmbiEnt to measure transfer learning effectiveness -> Compare human agreement on ambiguous vs. disambiguated examples to quantify the impact of explicit disambiguation

## Open Questions the Paper Calls Out

- **Question**: How would scaling up general-purpose pretraining and reinforcement learning from human feedback affect the trend of LM performance on ambiguity-related tasks?
  - **Basis in paper**: [inferred] from the discussion of GPT-4's performance and the limitations of current models.
  - **Why unresolved**: The paper hypothesizes that larger LMs may overfit to more common interpretations, but this needs to be empirically tested with scaled-up models.
  - **What evidence would resolve it**: Empirical results from experiments with larger LMs trained with reinforcement learning from human feedback, showing whether they perform better or worse on ambiguity-related tasks compared to current models.

- **Question**: How does the sensitivity of LMs to context and emphasis affect their ability to handle ambiguity?
  - **Basis in paper**: [explicit] from the conclusion suggesting future work to study the sensitivity of LMs to context and emphasis.
  - **Why unresolved**: The paper acknowledges the importance of context and emphasis but does not provide empirical results on how they affect LM performance on ambiguity.
  - **What evidence would resolve it**: Experiments varying context and emphasis in input sentences and measuring the resulting changes in LM output, showing how these factors influence ambiguity handling.

- **Question**: Are there systematic biases in interpretation across different LMs when dealing with ambiguous language?
  - **Basis in paper**: [explicit] from the conclusion suggesting future work to investigate systematic biases in interpretation.
  - **Why unresolved**: The paper does not provide a systematic analysis of biases in LM interpretations of ambiguous language.
  - **What evidence would resolve it**: A comprehensive study comparing interpretations of ambiguous sentences across multiple LMs, identifying common patterns of bias or variation in their handling of ambiguity.

## Limitations

- The study relies heavily on human evaluation which may introduce subjectivity in assessing disambiguation quality
- The analysis focuses primarily on English, potentially limiting generalizability to other languages
- Finetuning on existing NLI data with label variation leaves room for improvement, suggesting that AmbiEnt may not fully capture the complexity of ambiguity in natural language

## Confidence

- **High confidence**: The finding that pretrained LMs struggle with ambiguity tasks, as evidenced by low human evaluation scores (32% for GPT-4) and inconsistent behavior across related templates
- **Medium confidence**: The claim that finetuning NLI models on existing data with label variation leaves room for improvement, as performance is substantially below human levels (below 89.7% human performance)
- **Low confidence**: The assertion that pretraining objectives do not incentivize learning ambiguity handling, as this is inferred rather than directly tested

## Next Checks

1. Replicate the human evaluation study with a larger, more diverse annotator pool to validate the quality assessments of generated disambiguations
2. Test whether finetuning NLI models on AmbiEnt significantly improves performance on ambiguity detection compared to finetuning on existing NLI datasets alone
3. Extend the evaluation to multilingual settings by translating AmbiEnt examples to other languages and testing model performance across linguistic variations