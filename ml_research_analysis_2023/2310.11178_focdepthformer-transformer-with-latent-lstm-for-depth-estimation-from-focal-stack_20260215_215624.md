---
ver: rpa2
title: 'FocDepthFormer: Transformer with latent LSTM for Depth Estimation from Focal
  Stack'
arxiv_id: '2310.11178'
source_url: https://arxiv.org/abs/2310.11178
tags:
- depth
- stack
- transformer
- focal
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FocDepthFormer is a Transformer-based model for depth estimation
  from focal stacks. It uses a Transformer encoder with self-attention to capture
  non-local spatial features and an LSTM module to integrate stack information, enabling
  handling of arbitrary-length focal stacks.
---

# FocDepthFormer: Transformer with latent LSTM for Depth Estimation from Focal Stack

## Quick Facts
- arXiv ID: 2310.11178
- Source URL: https://arxiv.org/abs/2310.11178
- Reference count: 40
- Key outcome: FocDepthFormer achieves 78.01% accuracy (δ=1.25) on DDFF 12-Scene and 58.90% accuracy on LightField4D, outperforming state-of-the-art methods.

## Executive Summary
FocDepthFormer is a novel Transformer-based model for depth estimation from focal stacks, which are sequences of images captured at different focal distances. The model combines a Transformer encoder with self-attention for capturing non-local spatial features, an LSTM module for integrating stack information, and multi-scale convolutional kernels for extracting low-level focus/defocus features. Experiments on multiple benchmark datasets demonstrate that FocDepthFormer significantly outperforms existing methods, achieving state-of-the-art results with lower RMSE and bumpiness metrics.

## Method Summary
FocDepthFormer processes focal stacks using an early-stage multi-scale CNN to capture low-level focus/defocus features, followed by a Transformer encoder with self-attention for spatial feature learning. An LSTM module then integrates stack information across arbitrary numbers of images in latent space, and a CNN decoder with skip connections generates the final depth map. The model is pre-trained on monocular depth estimation data to improve visual pattern learning before being fine-tuned on focal stack datasets.

## Key Results
- Achieves 78.01% accuracy (δ=1.25) on DDFF 12-Scene dataset
- Achieves 58.90% accuracy on LightField4D dataset
- Outperforms state-of-the-art methods with lower RMSE and bumpiness metrics

## Why This Works (Mechanism)

### Mechanism 1
The Transformer encoder captures non-local spatial features that distinguish focus from defocus cues. Self-attention allows each patch embedding to attend to all other patches, enabling cross-referencing of sharpness and blur patterns across the image without being limited by local receptive fields. Core assumption: Sharpness/defocus information is spatially distributed and can be better captured through global attention rather than local convolutions. Break condition: If the dataset lacks sufficient texture variation, global attention may not provide additional benefit over local convolutions.

### Mechanism 2
The LSTM module enables handling of arbitrary-length focal stacks by incrementally fusing stack information in latent space. LSTM processes patch tokens sequentially across the stack dimension, caching hidden states that accumulate focus/defocus information regardless of stack length. Core assumption: Stack order (from near to far focus) provides meaningful sequential information that can be modeled with recurrent connections. Break condition: If stack images are not ordered by focus distance, the sequential modeling assumption breaks down.

### Mechanism 3
Multi-scale convolutional kernels in the early encoder capture low-level focus/defocus features at different scales. Parallel convolutions with different kernel sizes (3×3, 5×5, 7×7) followed by spatial and depth-wise convolutions create multi-scale feature maps that preserve sharp details across varying depth scales. Core assumption: Focus/defocus cues manifest at multiple spatial scales and require multi-scale feature extraction to capture effectively. Break condition: If the dataset primarily contains uniform depth scenes, multi-scale kernels may not provide significant advantage over single-scale kernels.

## Foundational Learning

- **Transformer self-attention mechanism**: Enables global spatial feature learning beyond local receptive fields, crucial for distinguishing focus from defocus patterns. Quick check: How does self-attention differ from convolutional receptive fields in terms of spatial coverage?
- **LSTM for sequential data processing**: Allows flexible handling of focal stacks with varying numbers of images by modeling stack dimension as a sequence. Quick check: What is the key difference between LSTM and simple recurrent networks that makes LSTM suitable for this task?
- **Multi-scale feature extraction**: Focus/defocus cues appear at different spatial scales, requiring features at multiple resolutions to capture all relevant information. Quick check: Why might using only 3×3 convolutions be insufficient for capturing all focus/defocus information?

## Architecture Onboarding

- **Component map**: Early-stage multi-scale CNN → Linear embedding → Transformer encoder → LSTM token grouping/fusion → CNN decoder → Depth map
- **Critical path**: Image → Multi-scale CNN → Linear embedding → Transformer → LSTM → CNN decoder → Depth map
- **Design tradeoffs**: Using Transformer increases parameter count but enables global feature learning; LSTM adds sequential processing capability but increases memory usage; multi-scale kernels improve feature capture but add computational cost
- **Failure signatures**: Poor performance on textureless regions suggests insufficient multi-scale feature capture; inconsistent results across different stack lengths indicates LSTM issues; blurry boundaries suggest Transformer attention not focusing correctly on sharp features
- **First 3 experiments**: 1) Compare performance with and without multi-scale kernels to verify their contribution; 2) Test with fixed stack length (no LSTM) to measure LSTM's impact on arbitrary-length handling; 3) Replace Transformer with CNN encoder to quantify benefit of self-attention mechanism

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed model handle focal stacks with varying image numbers during inference compared to fixed-length methods? The paper states that FocDepthFormer can handle arbitrary lengths of focal stacks with a predefined order in training and testing, unlike existing methods constrained to fixed-length stacks. This remains unresolved as the paper lacks detailed performance comparisons or specific scenarios where this flexibility proves advantageous.

### Open Question 2
What is the impact of pre-training FocDepthFormer on monocular depth estimation datasets for focal stack depth estimation tasks? The paper discusses pre-training on monocular depth estimation datasets like NYUv2 to improve visual pattern learning and reduce reliance on focal stack data, but does not provide a detailed analysis of how pre-training affects performance on focal stack tasks or the specific improvements gained.

### Open Question 3
How does the multi-scale convolutional kernel design in the early-stage encoder contribute to capturing focus/defocus features compared to single-scale kernels? The paper proposes using multi-scale convolutional kernels to directly capture low-level focus/defocus features at different scales, but lacks a thorough analysis of how multi-scale kernels specifically enhance feature extraction compared to traditional single-scale approaches.

## Limitations

- Experimental validation relies entirely on focal-stack datasets without ablation studies on individual architectural components
- Multi-scale kernel design and LSTM token grouping mechanism lack sufficient implementation details for faithful reproduction
- Limited discussion of how performance scales with stack length or image quality variations

## Confidence

- **High Confidence**: The basic premise that Transformer self-attention can capture non-local spatial features for depth estimation, supported by general Transformer literature
- **Medium Confidence**: The specific design choices (multi-scale kernels, LSTM integration) and their claimed advantages, as these are not extensively validated through ablation studies
- **Low Confidence**: The assertion that the method handles arbitrary-length focal stacks effectively, as this is primarily claimed rather than empirically demonstrated across diverse stack lengths

## Next Checks

1. **Ablation Study**: Implement and compare FocDepthFormer variants with individual components removed (no multi-scale kernels, no LSTM, no Transformer) to quantify each component's contribution to overall performance
2. **Stack Length Robustness**: Test the model on focal stacks with systematically varied numbers of images (3, 5, 7, 9, 11 images) to validate the arbitrary-length handling claim and identify performance degradation points
3. **Cross-Dataset Generalization**: Evaluate the model on datasets captured under different conditions (lighting, texture, depth range) to assess whether the multi-scale features and global attention truly generalize beyond the training distribution