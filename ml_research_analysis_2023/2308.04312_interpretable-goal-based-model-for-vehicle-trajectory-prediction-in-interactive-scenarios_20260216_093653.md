---
ver: rpa2
title: Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive
  Scenarios
arxiv_id: '2308.04312'
source_url: https://arxiv.org/abs/2308.04312
tags:
- trajectory
- prediction
- vehicle
- goal
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of interpretable trajectory prediction
  for vehicles in interactive urban environments, focusing on the challenge of explaining
  social interactions between agents. The core method combines a discrete choice model
  (DCM) with a neural network (NN) to predict vehicle trajectories while providing
  interpretable outputs.
---

# Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios

## Quick Facts
- arXiv ID: 2308.04312
- Source URL: https://arxiv.org/abs/2308.04312
- Reference count: 25
- Primary result: Combines discrete choice model with neural network for interpretable vehicle trajectory prediction in interactive urban environments

## Executive Summary
This paper addresses the challenge of interpretable trajectory prediction for vehicles in interactive urban environments by combining discrete choice models (DCM) with neural networks. The method predicts vehicle goals first using a utility-based framework, then generates trajectories conditioned on the predicted goals. The approach provides interpretable outputs explaining social interactions between agents while maintaining competitive accuracy. Experiments on the INTERACTION dataset demonstrate effectiveness in explaining predictions without compromising performance.

## Method Summary
The method combines a discrete choice model with a neural network to predict vehicle trajectories in interactive scenarios. Past trajectories of target and neighboring agents are encoded using LSTM, then processed through multi-head attention over a spatial grid to model social interactions. A Learning Multinomial Logit (L-MNL) framework fuses DCM utility functions (occupancy, direction keeping, collision avoidance) with learned neural network features to predict goals from a radial grid. Trajectories are then generated using an LSTM decoder conditioned on the predicted goal. The model is trained using a combination of regression loss and classification losses.

## Key Results
- Outperforms state-of-the-art approaches that do not use map information
- Achieves competitive results against methods that use map information
- Demonstrates interpretable outputs validated through utility function coefficient estimation and qualitative analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of discrete choice model (DCM) with neural network (NN) preserves interpretability while capturing complex social interactions.
- Mechanism: The DCM encodes interpretable behavioral rules (e.g., collision avoidance, directional preference) via utility functions, while the NN learns complex, high-dimensional social interaction patterns. The Learning Multinomial Logit (L-MNL) framework fuses these components by combining NN context representations with DCM utility scores.
- Core assumption: Social interactions can be decomposed into interpretable utility components plus residual complex patterns best modeled by NNs.
- Evidence anchors:
  - [abstract]: "we combine the interpretability of a discrete choice model with the high accuracy of a neural network-based model"
  - [section III-C]: "we use the Learning Multinomial Logit (L-MNL) framework" and "goal selection probabilities is defined as"
  - [corpus]: No direct corpus support found; weak evidence for utility function decomposition.

### Mechanism 2
- Claim: Predicting goals first, then trajectories conditioned on predicted goals, improves interpretability and accuracy.
- Mechanism: The model first uses the combined DCM-NN framework to predict the most likely goal location (from a radial grid). Then, using the predicted goal as conditioning, the LSTM decoder generates the full trajectory. This two-stage process allows human-readable reasoning at the goal selection stage.
- Core assumption: Goals are a meaningful intermediate representation that simplifies the trajectory prediction task and improves interpretability.
- Evidence anchors:
  - [abstract]: "We implement and evaluate our model using the INTERACTION dataset and demonstrate the effectiveness of our proposed architecture to explain its predictions without compromising the accuracy."
  - [section III-D]: "The loss for training the model is composed of a regression loss Lreg and two classification losses Lscore and Lcls."
  - [corpus]: No direct corpus support found; weak evidence for goal-first approach.

### Mechanism 3
- Claim: Social interaction modeling via multi-head attention over a spatial grid captures local dependencies without requiring explicit graph construction.
- Mechanism: Trajectories are encoded into a spatial tensor where each agent's hidden state is placed at their grid position. Multi-head attention processes this tensor to model pairwise and group interactions efficiently.
- Core assumption: Local spatial proximity in the grid correlates with interaction strength, allowing attention to learn meaningful social dynamics.
- Evidence anchors:
  - [section III-C]: "We use the multi-head attention mechanism [14] to model the social interactions" and "We define the interaction space of a target vehicle T as the area centered on its position at tobs and oriented in its direction of motion."
  - [section III-B]: "We use the Random Utility Maximization (RUM) theory" and the utility functions.
  - [corpus]: No direct corpus support found; weak evidence for grid-based attention.

## Foundational Learning

- Concept: Discrete Choice Models (DCM) and Random Utility Maximization (RUM)
  - Why needed here: DCM provides interpretable utility-based decision rules that can be combined with NN predictions.
  - Quick check question: What are the three utility functions used in DCM1 and DCM2 for modeling vehicle behavior?
    - Answer: occupancy, keep direction, and collision avoidance.

- Concept: Multi-Head Attention in spatiotemporal contexts
  - Why needed here: Attention allows modeling of complex pairwise and group interactions between agents without explicit graph construction.
  - Quick check question: How is the social tensor constructed from agent trajectory encoder states?
    - Answer: Each agent's trajectory encoder state is placed at its corresponding position in a 2D spatial grid, forming a tensor of shape (M, N, Ch).

- Concept: Learning Multinomial Logit (L-MNL) framework
  - Why needed here: L-MNL fuses interpretable utility functions with learned neural network features to select the most likely goal.
  - Quick check question: How is the final goal selection probability computed in the L-MNL framework?
    - Answer: π(ak|X) = esk(X) / Σj∈K esj(X), where sk(X) = uk(X) + zk(X).

## Architecture Onboarding

- Component map: Input trajectories -> FC layer -> LSTM encoder -> Social tensor -> Multi-head attention -> L-MNL fusion -> Goal prediction -> LSTM decoder -> Trajectory generation

- Critical path: 1. Encode past trajectories. 2. Build social tensor. 3. Apply multi-head attention. 4. Fuse with DCM utilities via L-MNL. 5. Select goal. 6. Decode conditioned trajectory.

- Design tradeoffs:
  - Fixed vs. dynamic radial grid: Fixed grid slightly better accuracy; dynamic grid adds velocity info but not helpful.
  - Two DCM variants: DCM2 (without real-time occupancy) yields fewer collisions despite similar ADE/FDE.
  - Interaction space size: 40m ahead, 10m behind, 25m sides; larger space increases computation.

- Failure signatures:
  - High collision rate: Likely issue with utility function or attention modeling.
  - Poor ADE/FDE but low collisions: May over-prioritize safety over accuracy.
  - Goals not interpretable: L-MNL fusion or utility coefficients may be misaligned.

- First 3 experiments:
  1. Baseline MHA-LSTM vs. G-MHA-LSTM: Check if goal-first approach improves accuracy.
  2. DCM1 vs. DCM2: Compare collision rates and overall accuracy.
  3. Fixed vs. dynamic grid: Measure impact on ADE/FDE and collisions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed interpretable model compare to state-of-the-art methods that use map information for trajectory prediction?
- Basis in paper: [explicit] The paper states that the model outperforms state-of-the-art approaches that do not use map information and achieves competitive results against methods that use map information.
- Why unresolved: The paper does not provide a detailed comparison of the model's performance against specific state-of-the-art methods that use map information.
- What evidence would resolve it: A comprehensive comparison of the model's performance against various state-of-the-art methods that use map information, including quantitative metrics and qualitative analysis.

### Open Question 2
- Question: How does the interpretability of the model's outputs impact its performance in real-world autonomous driving scenarios?
- Basis in paper: [explicit] The paper emphasizes the importance of interpretability for safety-critical applications like autonomous driving and demonstrates the effectiveness of the model in providing interpretable outputs.
- Why unresolved: The paper does not provide empirical evidence of the model's performance in real-world autonomous driving scenarios or its impact on safety.
- What evidence would resolve it: Real-world testing of the model in autonomous driving scenarios, with a focus on safety and interpretability, including quantitative metrics and qualitative analysis.

### Open Question 3
- Question: How does the model handle complex interactions between multiple agents in the scene, and how does this impact its performance?
- Basis in paper: [explicit] The paper mentions the importance of modeling interactions between agents and uses a multi-head attention mechanism to capture social interactions.
- Why unresolved: The paper does not provide a detailed analysis of how the model handles complex interactions between multiple agents or its impact on performance.
- What evidence would resolve it: A comprehensive analysis of the model's performance in scenarios with complex interactions between multiple agents, including quantitative metrics and qualitative analysis.

## Limitations

- Performance and interpretability on datasets beyond INTERACTION are unknown
- Actual interpretability of utility function coefficients needs more rigorous validation
- Computational overhead of the interpretable approach compared to non-interpretable baselines is not discussed

## Confidence

- Core architecture combining DCM and NN: High
- Interpretability claims: Medium
- Accuracy claims on INTERACTION dataset: Medium
- Goal-first prediction approach: Medium
- Utility function coefficient validation: Low

## Next Checks

1. **External dataset validation**: Test the model on at least one additional trajectory prediction dataset (e.g., nuScenes, Argoverse) to verify generalizability of both accuracy and interpretability claims

2. **Utility coefficient analysis**: Conduct a thorough analysis of the learned utility function coefficients across different scenarios to verify that they capture meaningful behavioral patterns and are consistent with human driving behavior

3. **Human evaluation of interpretability**: Perform user studies where domain experts assess whether the predicted goals and utility explanations align with their understanding of reasonable driving behavior in the scenarios presented