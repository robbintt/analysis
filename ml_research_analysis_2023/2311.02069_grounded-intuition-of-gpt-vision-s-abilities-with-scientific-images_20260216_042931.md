---
ver: rpa2
title: Grounded Intuition of GPT-Vision's Abilities with Scientific Images
arxiv_id: '2311.02069'
source_url: https://arxiv.org/abs/2311.02069
tags:
- gpt-vision
- desc
- text
- page
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a rigorous, grounded-theory-inspired method
  for qualitative evaluation of generative AI models, addressing the need for structured
  approaches to understand model capabilities and limitations. The method involves
  iterative data collection, theme exploration, and application to develop evidence-grounded
  intuition.
---

# Grounded Intuition of GPT-Vision's Abilities with Scientific Images

## Quick Facts
- arXiv ID: 2311.02069
- Source URL: https://arxiv.org/abs/2311.02069
- Reference count: 11
- One-line primary result: GPT-Vision excels at text-rich scientific images but struggles with spatial reasoning, color recognition, and numerical tasks.

## Executive Summary
This study introduces a rigorous, grounded-theory-inspired method for qualitative evaluation of generative AI models, addressing the need for structured approaches to understand model capabilities and limitations. The method involves iterative data collection, theme exploration, and application to develop evidence-grounded intuition. Applied to GPT-Vision for scientific image alt text generation, the analysis reveals key insights: GPT-Vision is highly sensitive to prompting, often relies on textual information, and struggles with spatial reasoning and color recognition. It also exhibits hallucination, incorporating general knowledge and making inferences, but sometimes produces misleading or overly confident outputs. The study highlights the importance of careful prompt design and contextual awareness, offering a framework for systematic evaluation of generative AI models.

## Method Summary
The paper presents a five-phase qualitative evaluation framework: data collection through theoretical sampling of scientific images, data review with memo-taking, theme exploration via open coding, theme development through axial coding and literature integration, and theme application for systematic documentation. The method was applied to 21 scientific images from research publications, using two prompt types ("alt" for straightforward alt text and "desc" for descriptive alt text) to analyze GPT-Vision's alt text generation capabilities and limitations.

## Key Results
- GPT-Vision demonstrates high sensitivity to prompt variations, producing significantly different outputs based on minor prompt changes
- The model shows strong reliance on textual information in images while struggling with spatial relationships and color recognition
- GPT-Vision exhibits hallucination tendencies, incorporating general knowledge and making inferences that can produce misleading or overly confident outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative theme development through grounded theory produces more accurate model capability intuitions than single-pass analysis.
- Mechanism: The method cycles through data collection → review → exploration → development → application phases, allowing themes to emerge from data rather than being imposed a priori. Each cycle refines understanding based on new evidence.
- Core assumption: Qualitative patterns in generative model outputs can be reliably identified through systematic human analysis, even without large sample sizes.
- Evidence anchors:
  - [abstract] "Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing."
  - [section] "Our method consists of five stages: (1) data collection, (2) data review, (3) theme exploration, (4) theme development, and (5) theme application."
  - [corpus] Weak - only 5 related papers, average neighbor FMR 0.298 suggests moderate relatedness but limited direct evidence for this specific mechanism.
- Break condition: If human analysts impose preconceived themes too early, or if the model's behavior is too stochastic to form consistent patterns, the iterative refinement fails to converge on reliable insights.

### Mechanism 2
- Claim: GPT-Vision's reliance on textual information in images creates vulnerability to adversarial labels while simultaneously enabling strong performance on text-rich scientific content.
- Mechanism: The model appears to process image text as primary semantic signal, often at the expense of visual context. This creates both strengths (accurate text transcription, context incorporation) and weaknesses (susceptibility to adversarial attacks, confusion when text contradicts visual content).
- Core assumption: GPT-Vision's vision-language architecture gives disproportionate weight to detected text when forming image descriptions.
- Evidence anchors:
  - [abstract] "GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships."
  - [section] "GPT-Vision was particularly prone to typographical attacks... The same dish as P1, but with adversarial labels... GPT-Vision continued to incorporate: Chicken Noodle Soup (C1) Chicken Noodle Soup, where a bowl is presented with a dark broth and a dollop of cream..."
  - [corpus] Weak - no direct corpus evidence; this is primarily derived from paper findings rather than related literature.

### Mechanism 3
- Claim: GPT-Vision exhibits inconsistent numerical and spatial reasoning abilities that vary by context and prompt type.
- Mechanism: The model shows selective competence - strong at describing element positions and spatial relationships, but weak at counting objects, interpreting axes correctly, and maintaining consistency across different views of the same image.
- Core assumption: Vision-language models may develop specialized visual reasoning capabilities that don't generalize across all quantitative tasks.
- Evidence anchors:
  - [abstract] "struggles with spatial reasoning and color recognition"
  - [section] "Counting objects was another frequent source of error, with GPT-Vision miscounting on 10 of 21 images... GPT-Vision described the x- and y-axes of each line graph moderately well, except that it consistently underestimated the bounds of the axes depending on the labels."
  - [corpus] Weak - limited related work on numerical reasoning in vision-language models found in corpus search.

## Foundational Learning

- Concept: Grounded theory methodology
  - Why needed here: Provides systematic framework for deriving insights from qualitative data without imposing researcher biases, essential for understanding novel model behaviors
  - Quick check question: What distinguishes grounded theory from traditional hypothesis-driven research approaches?

- Concept: Thematic analysis
  - Why needed here: Offers structured process for identifying, analyzing, and reporting patterns within qualitative data, enabling systematic evaluation of model outputs
  - Quick check question: How does thematic analysis differ from simple pattern recognition in unstructured data?

- Concept: Theoretical sampling
  - Why needed here: Enables targeted data collection that addresses emerging insights during analysis, improving efficiency and relevance of evaluation
  - Quick check question: Why is theoretical sampling more effective than random sampling for qualitative model evaluation?

## Architecture Onboarding

- Component map: Data collection → Data review → Theme exploration → Theme development → Theme application
- Critical path: Theme exploration and development require the most time and effort, directly determining final insight quality
- Design tradeoffs: Prioritizes depth over breadth, analyzing few images deeply rather than many superficially
- Failure signatures: Imposed rather than emergent themes, incomplete evidence collection, or premature iteration termination
- First 3 experiments:
  1. Run the complete 5-phase analysis on a small set (5-10) diverse scientific images to validate the method works as expected
  2. Test prompt sensitivity by varying prompts systematically on the same images and documenting behavioral changes
  3. Apply the method to a different model (e.g., Gemini Vision) on identical images to compare capability profiles and validate method reproducibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-Vision's performance vary when describing scientific images across different scientific domains (e.g., biology, physics, computer science)?
- Basis in paper: [inferred] The paper focuses on GPT-Vision's performance with scientific images but does not explicitly compare performance across different scientific domains.
- Why unresolved: The paper does not provide a systematic comparison of GPT-Vision's performance across different scientific domains, limiting the understanding of its domain-specific capabilities and limitations.
- What evidence would resolve it: A study evaluating GPT-Vision's performance on scientific images from various domains, with quantitative metrics and qualitative analysis of domain-specific strengths and weaknesses.

### Open Question 2
- Question: To what extent can GPT-Vision's behavior be explained by its internal mechanisms, and how can we bridge the gap between behavioral analysis and understanding its "artificial cognition"?
- Basis in paper: [explicit] The paper acknowledges the limitations of behavioral analysis in inferring the underlying processes of "artificial cognition" and the need for further investigation into the "artificial cognition" and "neuroscience" of GPT-Vision's behaviors.
- Why unresolved: The paper's behavioral analysis cannot fully explain the internal mechanisms driving GPT-Vision's behavior, highlighting the need for methods to bridge the gap between observed behavior and underlying cognitive processes.
- What evidence would resolve it: Research combining behavioral analysis with interpretability techniques, such as attention visualization or activation analysis, to gain insights into the internal representations and decision-making processes of GPT-Vision.

### Open Question 3
- Question: How can GPT-Vision be improved to generate more accurate and informative alt text for blind and low-vision readers, considering their specific needs and preferences?
- Basis in paper: [explicit] The paper discusses the challenges of generating alt text for blind and low-vision readers, highlighting the need for adaptations to meet their diverse needs and preferences, and suggests that GPT-Vision's tendency to focus on visual details may not be ideal for all readers.
- Why unresolved: The paper does not provide concrete solutions for improving GPT-Vision's alt text generation to better serve blind and low-vision readers, leaving the question of how to optimize its output for this specific audience unanswered.
- What evidence would resolve it: Studies evaluating the effectiveness of different GPT-Vision-generated alt text styles for blind and low-vision readers, comparing their preferences and comprehension, and exploring techniques to adapt the output to individual needs.

## Limitations
- Analysis based on only 21 scientific images, limiting generalizability across diverse visualization types
- Qualitative nature introduces subjectivity in theme identification despite grounded theory framework
- Evaluation focuses specifically on GPT-Vision without comparative analysis against other models or human baselines

## Confidence
- **High Confidence**: GPT-Vision's sensitivity to prompting variations and its tendency to incorporate textual information from images
- **Medium Confidence**: Claims about specific weaknesses in spatial reasoning and numerical tasks
- **Low Confidence**: Generalizability of findings to other vision-language models or scientific domains beyond those represented in the sample

## Next Checks
1. Replicate the five-phase analysis framework on a larger and more diverse set of scientific images (minimum 50 images) spanning additional domains like astronomy, geology, and social sciences to test generalizability of identified themes.
2. Apply the same evaluation methodology to a different vision-language model (e.g., Gemini Vision or Claude 3) using identical images and prompts to assess whether observed limitations are model-specific or inherent to vision-language architectures.
3. Conduct a systematic prompt sensitivity analysis by varying prompt structure, length, and specificity across a standardized image set to quantify the impact of prompt design on model performance and identify optimal prompting strategies.