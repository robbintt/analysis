---
ver: rpa2
title: Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification
arxiv_id: '2302.00368'
source_url: https://arxiv.org/abs/2302.00368
tags:
- classi
- ne-grained
- hierarchical
- coarse
- cation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing mistake severity in
  fine-grained classification by leveraging coarse-grained predictions. The proposed
  method, Hierarchical Ensembles (HiE), trains separate models for fine-grained and
  coarse-grained labels and combines their predictions using a modified decision rule
  based on joint probabilities.
---

# Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification

## Quick Facts
- arXiv ID: 2302.00368
- Source URL: https://arxiv.org/abs/2302.00368
- Reference count: 40
- One-line primary result: HiE significantly reduces average mistake severity while improving top-1 accuracy on iNaturalist-19 and tieredImageNet-H datasets

## Executive Summary
This paper addresses the problem of reducing mistake severity in fine-grained classification by leveraging coarse-grained predictions. The proposed method, Hierarchical Ensembles (HiE), trains separate models for fine-grained and coarse-grained labels and combines their predictions using a modified decision rule based on joint probabilities. HiE significantly reduces average mistake severity while improving top-1 accuracy on iNaturalist-19 and tieredImageNet-H datasets, achieving state-of-the-art performance on both benchmarks. In the semi-supervised setting, HiE brings notable gains in top-1 accuracy while decreasing mistake severity as training data decreases for fine-grained classes. The simplicity and post-hoc nature of HiE make it practical to use with any off-the-shelf trained model to improve its predictions further.

## Method Summary
Hierarchical Ensembles (HiE) improves fine-grained classification by training separate models for coarse and fine labels, then combining their predictions using joint probabilities. The method assumes conditional independence between coarse and fine-grained predictions given the input, and that coarse predictions are more reliable. At test time, HiE computes P(i, iparent|x) = P(i|x;θ)P(iparent|x;φ) and selects the prediction that maximizes this joint probability. This approach leverages the hierarchical structure of labels to reduce semantic distance of mistakes when coarse predictions are correct but fine-grained predictions are wrong.

## Key Results
- HiE achieves state-of-the-art performance on iNaturalist-19 and tieredImageNet-H datasets
- Significantly reduces average mistake severity while improving top-1 accuracy
- Brings notable gains in top-1 accuracy in semi-supervised settings as training data decreases for fine-grained classes

## Why This Works (Mechanism)

### Mechanism 1
HiE improves fine-grained classification by leveraging coarse predictions through joint probability maximization. HiE trains separate models for coarse and fine labels, then combines their predictions using a modified decision rule that maximizes joint probability P(i, iparent|x) = P(i|x;θ)P(iparent|x;φ). This leverages the assumption that coarse predictions are more reliable than fine-grained ones for structurally similar classes.

### Mechanism 2
HiE reduces mistake severity by correcting predictions when coarse predictions are correct but fine-grained predictions are wrong. When the coarse classifier correctly identifies the parent category but the fine-grained classifier misclassifies within that category, HiE shifts the prediction to the correct sub-tree by maximizing joint probability, which inherently reduces the semantic distance of mistakes.

### Mechanism 3
HiE provides consistent gains in semi-supervised settings by leveraging abundant coarse labels to improve fine-grained classification with limited fine-grained annotations. By training separate models on coarse and fine labels, HiE can utilize all available coarse annotations while only requiring a small number of fine-grained annotations, effectively transferring knowledge from the coarse to fine-grained task.

## Foundational Learning

- **Label hierarchies and taxonomic relationships**: Understanding how coarse and fine-grained labels relate through parent-child relationships is fundamental to HiE's approach of leveraging coarse predictions for fine-grained classification. Quick check: Given a fine-grained class "Junonia Genoveva" (a butterfly species), what would be its coarse label in a typical biological taxonomy?

- **Conditional independence in probabilistic models**: HiE assumes conditional independence between coarse and fine-grained predictions when computing joint probabilities. Quick check: If P(i|x) and P(iparent|x) are conditionally independent given x, what is the formula for P(i, iparent|x)?

- **Post-hoc correction methods**: HiE is a post-hoc correction method that modifies predictions at test time without retraining the base models. Quick check: What is the key difference between post-hoc correction methods and methods that modify the training objective or architecture?

## Architecture Onboarding

- **Component map**: Input image -> Fine-grained classifier -> Coarse classifier -> Joint probability computation -> Argmax decision -> Final prediction

- **Critical path**: Input image → Fine-grained classifier output → Coarse classifier output → Joint probability computation → Argmax decision → Final prediction

- **Design tradeoffs**: Training two separate models vs. joint training: HiE trades off the simplicity of separate training for the potential benefits of feature disentanglement; Post-hoc correction vs. architectural changes: HiE is more flexible but may not capture all hierarchical information that could be encoded in the architecture; Computational overhead: Requires running two models at inference time

- **Failure signatures**: No improvement or degradation in top-1 accuracy: May indicate coarse predictions are not more reliable than fine-grained ones; Increased mistake severity: May indicate the joint probability computation is not properly aligned with the semantic hierarchy; Performance highly dependent on coarse classifier accuracy: May indicate insufficient feature disentanglement between coarse and fine-grained models

- **First 3 experiments**: 1) Baseline comparison: Run HiE on a small dataset with known hierarchical structure and compare top-1 accuracy and mistake severity against cross-entropy baseline; 2) Ablation study: Test HiE with coarse predictions obtained by marginalizing fine-grained predictions (HiE-Self) vs. separate coarse classifier; 3) Semi-supervised evaluation: Test HiE in a semi-supervised setting with varying amounts of fine-grained annotations to measure knowledge transfer from coarse labels

## Open Questions the Paper Calls Out

### Open Question 1
How does the HiE method's performance scale with deeper hierarchies beyond the tested 8-13 levels? The paper tests HiE on iNaturalist-19 (8 levels) and tieredImageNet-H (13 levels) but doesn't explore deeper hierarchies.

### Open Question 2
What is the computational overhead of training separate coarse and fine-grained models compared to joint training approaches? The method requires training two separate models, but no computational cost analysis is provided.

### Open Question 3
How does HiE perform on non-biological hierarchical classification tasks where semantic relationships may be less clear? All tested datasets are biological (species classification), no evaluation on other domains.

### Open Question 4
Can the HiE approach be extended to dynamic or evolving hierarchies where relationships change over time? The method assumes static hierarchies but doesn't address scenarios where class relationships evolve.

## Limitations
- Performance relies heavily on the assumption that coarse predictions are more reliable than fine-grained ones, which isn't empirically validated across different dataset types
- Conditional independence assumption for joint probability computation may not hold in practice for structurally similar classes
- Semi-supervised evaluation lacks sufficient ablation studies to isolate the contribution of coarse label knowledge transfer

## Confidence
- **High confidence**: HiE's implementation details and mathematical formulation are clearly specified
- **Medium confidence**: Claims about mistake severity reduction are supported by experiments but may be dataset-dependent
- **Low confidence**: Semi-supervised performance claims need more rigorous ablation studies to validate the knowledge transfer mechanism

## Next Checks
1. **Conditional Independence Validation**: Conduct experiments to empirically measure the conditional independence between coarse and fine-grained predictions on a controlled synthetic dataset with known ground truth

2. **Coarse Prediction Reliability Test**: Compare error rates between coarse and fine-grained classifiers across multiple datasets to validate the core assumption that coarse predictions are more reliable

3. **Ablation Study for Semi-supervised Setting**: Isolate the contribution of coarse label knowledge transfer by comparing HiE against variants that use different combinations of fine-grained and coarse annotations during training