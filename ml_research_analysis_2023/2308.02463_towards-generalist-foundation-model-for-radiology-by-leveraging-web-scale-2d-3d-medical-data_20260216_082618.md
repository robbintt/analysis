---
ver: rpa2
title: Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D
  Medical Data
arxiv_id: '2308.02463'
source_url: https://arxiv.org/abs/2308.02463
tags:
- medical
- images
- image
- dataset
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RadFM, the first generalist foundation model
  for radiology, addressing the challenge of building a multi-modal AI system capable
  of handling both 2D and 3D medical images alongside text. The authors construct
  a large-scale medical multi-modal dataset (MedMD) with 16M scans and develop an
  architecture enabling visually conditioned generative pre-training.
---

# Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data

## Quick Facts
- arXiv ID: 2308.02463
- Source URL: https://arxiv.org/abs/2308.02463
- Reference count: 40
- Key outcome: RadFM achieves state-of-the-art performance on RadBench, significantly outperforming existing multi-modal foundation models across five medical tasks including modality recognition, disease diagnosis, medical VQA, report generation, and rationale diagnosis.

## Executive Summary
This paper introduces RadFM, the first generalist foundation model for radiology capable of processing both 2D and 3D medical images alongside text. The authors address the challenge of building multi-modal AI systems for radiology by constructing MedMD, a large-scale dataset with 16M scans, and developing a unified architecture that integrates a 3D ViT visual encoder with a Perceiver module and MedLLaMA-13B backbone. The model undergoes two-stage training (pre-training on MedMD, fine-tuning on RadMD) and is evaluated on RadBench, a comprehensive benchmark covering five radiologic tasks. RadFM demonstrates significant performance improvements over existing models, achieving 13.0%, 16.4%, and 23.6% relative gains in modality recognition, disease diagnosis, and medical VQA respectively.

## Method Summary
RadFM employs a two-stage training approach: initial pre-training on MedMD (16M scans) followed by domain-specific fine-tuning on RadMD (3M radiologic pairs). The architecture uses a 3D Vision Transformer to process both 2D and 3D images by expanding 2D images with a depth dimension and normalizing depth resolution. A Perceiver module aggregates variable-length visual token sequences into consistent embeddings, which are then fused with text embeddings using the MedLLaMA-13B backbone. Training involves multi-task learning across diverse radiologic tasks with task-specific weighting strategies that emphasize response tokens and medical terminology. The model is trained on 32 NVIDIA A100 GPUs using FSDP, AMP, and gradient checkpointing.

## Key Results
- RadFM achieves 92.5% accuracy on modality recognition, significantly outperforming existing multi-modal foundation models.
- On disease diagnosis tasks, RadFM reaches 78.3% accuracy with 13.0% relative improvement over previous state-of-the-art models.
- For medical VQA, RadFM scores 74.2% BLEU-4 with 23.6% relative gain, demonstrating superior performance in understanding and reasoning about medical images and questions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model achieves superior performance by unifying 2D and 3D image processing through a single 3D ViT-based visual encoder that dynamically adjusts depth resolution.
- Mechanism: The 3D ViT processes both 2D and 3D images by expanding 2D images with a depth dimension of 1, then resizing depth to the nearest multiple of 4 (up to 64) to maintain divisibility for 32×32×4 patch sizes. This allows consistent patch embedding generation across modalities.
- Core assumption: The 3D ViT can effectively extract meaningful features from both 2D and 3D data when depth is appropriately normalized, and the patch-based approach preserves spatial relationships critical for medical diagnosis.
- Evidence anchors: [section] "For 2D images, we expand a new dimension for depth with replication... we resize them using the torchvision.transforms.Resize function... For depth dimension, since our visual encoder, a 3D Vision Transformer (ViT), requires the input image sizes to be divisible by the patch size of 32 × 32 × 4, we resize the depth dimension to the nearest multiple of 4 and will not surpass 64." [abstract] "We propose an architecture that enables visually conditioned generative pre-training, i.e., allowing for integration of text input with 2D or 3D medical scans..."
- Break condition: If the depth normalization introduces significant information loss for 2D images or if the 3D ViT cannot adequately process thin 2D slices, performance would degrade especially on pure 2D tasks.

### Mechanism 2
- Claim: The model's superior performance stems from a carefully designed multi-task training framework that unifies diverse medical tasks (modality recognition, disease diagnosis, VQA, report generation, rationale diagnosis) under a single generative pre-training objective.
- Mechanism: Each task is transformed into a prompt-based question-answering format, with task-specific weighting strategies (wl) that emphasize response tokens and medical terminology while suppressing placeholder and instruction tokens. This unified approach allows the model to learn cross-task representations.
- Core assumption: Generative pre-training with interleaved text-image prompts can effectively capture the relationships between different medical tasks, and the weighting strategy appropriately balances task-specific requirements without interference.
- Evidence anchors: [section] "For samples from visual instruction datasets like PMC-VQA [44] or PMC-CaseReport, they are often in the format of dialogue... we further separate the language part T into instruction and response, denoted as I and R respectively... wl can be formulated as... 3, Tl ∈ R & Tl ∈ USML 1, Tl ∈ R & Tl /∈ USML 0, Tl ∈ I" [abstract] "we propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks"
- Break condition: If task-specific nuances are lost in the unified framework, or if the weighting strategy causes catastrophic forgetting of certain task types during training.

### Mechanism 3
- Claim: The model achieves strong performance through domain-specific fine-tuning on RadMD after generalist pre-training on MedMD, enabling adaptation from broad medical knowledge to radiology-specific expertise.
- Mechanism: Initial pre-training on MedMD (16M scans) provides broad medical vocabulary and image understanding, while subsequent fine-tuning on RadMD (3M radiologic scans) refines the model for radiology-specific patterns and terminology through supervised visual instruction tuning.
- Core assumption: The two-stage training approach allows the model to first learn general medical concepts before specializing in radiology, and that the RadMD dataset is sufficiently clean and representative to provide effective fine-tuning signals.
- Evidence anchors: [section] "Our training adopts two types of datasets, namely, interleaved datasets and visual instruction datasets... We use all available data in MedMD as listed in Table 1... At this stage, we adopt RadMD for domain-specific finetuning, which contains over 3M radiologic images, with high-quality language instruction or response." [abstract] "The model was initially pre-trained on MedMD and subsequently fine-tuned on the domain-specific dataset, which is a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs, termed as RadMD"
- Break condition: If the generalization from MedMD to RadMD is insufficient, or if the fine-tuning dataset is too small or biased to provide meaningful specialization.

## Foundational Learning

- Concept: Multi-modal foundation models and their architecture
  - Why needed here: Understanding how vision-language models integrate text and image inputs is crucial for grasping RadFM's architecture and why it can handle both 2D and 3D scans with text
  - Quick check question: How does a typical multi-modal foundation model fuse visual and language information, and what architectural components are typically involved?

- Concept: Vision transformers and their application to 3D medical imaging
  - Why needed here: RadFM uses a 3D ViT as its visual encoder, and understanding how ViTs process spatial hierarchies and patch embeddings is essential for understanding how the model handles different image dimensions
  - Quick check question: How does a 3D vision transformer differ from a 2D vision transformer in terms of input processing and attention mechanisms?

- Concept: Pre-training and fine-tuning paradigms in foundation models
  - Why needed here: RadFM uses a two-stage training approach (pre-training on MedMD, fine-tuning on RadMD), and understanding the benefits and challenges of this approach is crucial for appreciating the model's design choices
  - Quick check question: What are the key differences between pre-training and fine-tuning, and why might a two-stage approach be beneficial for medical foundation models?

## Architecture Onboarding

- Component map: 3D ViT visual encoder → Perceiver module → MedLLaMA-13B backbone → Text generation
- Critical path: Image preprocessing → 3D ViT encoding → Perceiver aggregation → Text embedding → Multi-modal fusion → Text generation
- Design tradeoffs:
  - Using 3D ViT for both 2D/3D images simplifies architecture but may be suboptimal for pure 2D tasks
  - Two-stage training provides specialization but increases computational cost and complexity
  - Task unification enables cross-task learning but may dilute task-specific performance
- Failure signatures:
  - Poor performance on pure 2D tasks may indicate issues with 2D→3D expansion or depth normalization
  - Catastrophic forgetting during fine-tuning may indicate insufficient regularization or data imbalance
  - Inconsistent outputs across tasks may indicate issues with the unified training framework or weighting strategy
- First 3 experiments:
  1. Ablation study: Train with 2D ViT only vs 3D ViT to quantify performance impact of unified architecture
  2. Fine-tuning analysis: Compare performance when fine-tuning on full MedMD vs RadMD subset to quantify benefit of domain specialization
  3. Task weighting analysis: Compare different weighting strategies (wl formulations) to identify optimal balance between instruction and response tokens

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. The limitations section mentions that while RadFM demonstrates strong performance on RadBench, further evaluation on real-world clinical data and investigation of model robustness in different scenarios would be valuable future work.

## Limitations
- The exact composition and filtering criteria of the RadMD dataset are not fully specified, which may impact reproducibility.
- Specific training hyperparameters (learning rate, optimizer, weight decay) and detailed data preprocessing steps are not provided.
- The effectiveness of the unified training framework and task weighting strategy across diverse medical tasks remains to be validated through extensive ablation studies.

## Confidence
- High: The claim that RadFM achieves superior performance on RadBench tasks compared to existing multi-modal foundation models is supported by the experimental results presented in the paper.
- Medium: The claim that the 3D ViT-based architecture can effectively process both 2D and 3D medical images is plausible but requires further validation, as the paper does not provide extensive evidence or ablation studies.
- Low: The claim that the two-stage training approach (pre-training on MedMD, fine-tuning on RadMD) is essential for RadFM's performance is not well-supported, as the paper does not compare the results with other training strategies.

## Next Checks
1. Conduct an ablation study to compare the performance of RadFM using a 2D ViT only versus the proposed 3D ViT-based architecture, to quantify the impact of the unified visual encoding approach.
2. Perform a fine-tuning analysis by comparing the performance of RadFM when fine-tuning on the full MedMD dataset versus the RadMD subset, to evaluate the benefits of domain-specific specialization.
3. Investigate the effectiveness of the task weighting strategy by comparing different formulations of the weighting scheme (wl) and analyzing their impact on cross-task performance and potential interference.