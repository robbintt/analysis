---
ver: rpa2
title: 'Let''s Think Outside the Box: Exploring Leap-of-Thought in Large Language
  Models with Creative Humor Generation'
arxiv_id: '2312.02439'
source_url: https://arxiv.org/abs/2312.02439
tags:
- text
- data
- clot
- creative
- oogiri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the Leap-of-Thought (LoT) ability in large
  language models (LLMs), a creative paradigm involving strong associations and knowledge
  leaps. The authors introduce the multimodal and multilingual Oogiri-GO dataset,
  containing over 130,000 samples from the Oogiri game, to investigate LoT in LLMs.
---

# Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation

## Quick Facts
- **arXiv ID**: 2312.02439
- **Source URL**: https://arxiv.org/abs/2312.02439
- **Reference count**: 40
- **Primary result**: CLoT significantly enhances LoT ability in LLMs for humor generation and related creative tasks

## Executive Summary
This paper addresses the Leap-of-Thought (LoT) ability in large language models, which involves strong associations and creative thinking beyond sequential reasoning. The authors introduce the Oogiri-GO dataset with over 130,000 multimodal samples from the Oogiri humor game and propose the Creative Leap-of-Thought (CLoT) paradigm. CLoT combines associable instruction tuning with explorative self-refinement to improve LLMs' ability to generate creative responses by exploring parallels between seemingly unrelated concepts. Experimental results demonstrate significant improvements across multiple tasks including humor generation, cloud guessing, and divergent association tasks.

## Method Summary
The CLoT paradigm consists of two main stages: associable instruction tuning and explorative self-refinement. First, the Oogiri-GO dataset is transformed into LoT-oriented instruction tuning data using carefully designed templates that encourage remote association thinking. The model is then fine-tuned using LoRA. In the self-refinement stage, the model generates new creative data under weakly-associated conditions (50% empty conditions, 50% noun conditions from the dataset), ranks the outputs, and selects high-quality responses to retrain itself. This process prevents performance collapse while enhancing LoT ability through diversity and quality control.

## Key Results
- CLoT significantly outperforms baseline models on Oogiri humor generation tasks
- The approach shows strong generalization to cloud guessing game and divergent association task
- Self-refinement stage provides substantial improvements, with one round being optimal
- Weakly-associated conditions prove more effective than strongly-associated conditions for fostering creativity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Weakly-associated conditions enable the LLM to engage in remote association thinking, bridging seemingly unrelated concepts.
- **Mechanism**: By introducing empty conditions (50% probability) and object noun conditions (50% probability) sampled from the Oogiri-GO dataset, the LLM is encouraged to explore knowledge outside traditional cognitive limitations. Empty conditions allow freedom, while noun conditions force connections between disparate concepts.
- **Core assumption**: The LLM has sufficient prior knowledge and reasoning capability to form associations when given appropriate stimuli.
- **Evidence anchors**:
  - [abstract]: "CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement."
  - [section]: "These weakly-associated conditions can either be empty, or randomly sampled from an object noun set collected from the Oogiri-GO dataset. The former empty conditions to allow LLM to operate freely, and the latter ones help the LLM to link seemingly-unrelated and weakly-related concepts, and encourage the LLM to explore knowledge outside of traditional cognitive limitations."
- **Break condition**: If the LLM lacks sufficient prior knowledge or if the noun set is too small or irrelevant, the weakly-associated conditions will not effectively stimulate remote association thinking.

### Mechanism 2
- **Claim**: Associable instruction tuning improves both generation and discrimination abilities for LoT.
- **Mechanism**: LoT-oriented instruction templates are designed to transform Oogiri-GO dataset into instruction tuning data. For generation, templates include task-specific prompts and optional conditions (image, condition) to encourage LLM to draw parallels between seemingly unrelated concepts. For discrimination, choice and ranking questions are designed to enhance selection and ranking abilities.
- **Core assumption**: The instruction templates are effective in guiding the LLM to learn the desired abilities.
- **Evidence anchors**:
  - [abstract]: "CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pretrained LLM for achieving certain LoT humor generation and discrimination abilities."
  - [section]: "For associable generation, 'USER-INPUTs' contains 'Task-specific Prompt' along with two optional conditions, 'Image' and 'Condition'... This design gives the LLM a clue to connect the game input and the correct responses while also encouraging LLM to explore and unleash its creative thinking with probability ρc."
- **Break condition**: If the instruction templates are poorly designed or if the LLM cannot effectively learn from the instruction data, the associable instruction tuning will not improve the LoT abilities.

### Mechanism 3
- **Claim**: Explorative self-refinement prevents performance collapse and enhances LoT ability through diversity and quality control.
- **Mechanism**: The LLM generates new creative data under weakly-associated conditions, then ranks and selects high-quality data to train itself. The diversity of conditions and the quality filtering process ensure that the generated data is both novel and effective.
- **Core assumption**: The LLM can generate high-quality creative data and effectively rank and select the best responses.
- **Evidence anchors**:
  - [abstract]: "CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement."
  - [section]: "This exploration strategy can help generate diverse high-quality data for self-refinement."
- **Break condition**: If the LLM cannot generate diverse and high-quality data, or if the ranking and selection process is ineffective, the explorative self-refinement will not enhance LoT ability and may even lead to performance collapse.

## Foundational Learning

- **Concept**: Leap-of-Thought (LoT) vs. Chain-of-Thought (CoT)
  - Why needed here: Understanding the difference between LoT and CoT is crucial for grasping the novelty of CLoT. LoT involves non-sequential thinking and strong associations, while CoT is sequential and step-by-step.
  - Quick check question: What is the key difference between LoT and CoT in terms of thinking process?

- **Concept**: Remote association
  - Why needed here: Remote association is the core mechanism behind LoT. It involves drawing parallels between seemingly unrelated concepts to generate novel ideas.
  - Quick check question: How does remote association contribute to creative thinking in the context of LoT?

- **Concept**: Multimodal learning
  - Why needed here: CLoT is applied to multimodal LLMs that can process both text and images. Understanding multimodal learning is essential for comprehending how CLoT handles different types of Oogiri games.
  - Quick check question: What are the advantages of using multimodal LLMs for creative tasks like humor generation?

## Architecture Onboarding

- **Component map**: Oogiri-GO dataset -> Associable instruction tuning (LoRA fine-tuning) -> Explorative self-refinement (generation + ranking + selection) -> CLoT inference
- **Critical path**: The critical path in CLoT is the sequence of stages that directly impact the LoT ability of the LLM. It starts with associable instruction tuning, which lays the foundation for LoT, followed by explorative self-refinement, which further enhances LoT through diverse and high-quality data generation.
- **Design tradeoffs**: The design of CLoT involves several tradeoffs. For example, the probability of using empty conditions (ρ) in explorative remote association balances freedom and constraint. A higher ρ allows more freedom but may reduce the effectiveness of remote association. The number of generated candidates (n) affects the difficulty of ranking and the diversity of responses.
- **Failure signatures**: Potential failure modes in CLoT include insufficient prior knowledge in the LLM, poorly designed instruction templates, ineffective ranking and selection processes, and lack of diversity in the generated data. These failures can lead to poor LoT performance and even performance collapse.
- **First 3 experiments**:
  1. Test the effectiveness of different probabilities (ρ) of using empty conditions in explorative remote association.
  2. Evaluate the impact of different numbers (n) of generated candidates on the quality and diversity of responses.
  3. Compare the performance of CLoT with and without the explorative self-refinement stage to assess its contribution to LoT enhancement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of the weakly-associated conditions affect the performance of CLoT?
- Basis in paper: [explicit] The paper states that using weakly-associated conditions is superior and more conducive to fostering the creativity of LLMs compared to strongly-associated conditions.
- Why unresolved: While the paper provides evidence that weakly-associated conditions are beneficial, it does not explore the optimal level of diversity or the specific characteristics of conditions that yield the best results.
- What evidence would resolve it: Systematic experiments varying the diversity of the weakly-associated conditions and measuring the corresponding performance of CLoT would provide insights into the optimal level of diversity.

### Open Question 2
- Question: Can CLoT be further enhanced by incorporating external knowledge bases to expand the diversity of the noun set S?
- Basis in paper: [inferred] The paper mentions that the inherent difficulty in expanding the diversity of S hinders the augmentation of S diversity, and introducing new nouns from external knowledge bases poses a challenge.
- Why unresolved: The paper does not explore the potential benefits or drawbacks of incorporating external knowledge bases to expand the diversity of S.
- What evidence would resolve it: Experiments comparing the performance of CLoT with and without the incorporation of external knowledge bases to expand the diversity of S would provide insights into the potential benefits and challenges of this approach.

### Open Question 3
- Question: How does the number of rounds of self-refinement impact the performance of CLoT?
- Basis in paper: [explicit] The paper mentions that one round of self-refinement yields promising performance, while additional rounds do not yield significant further improvements.
- Why unresolved: While the paper provides evidence that one round is sufficient, it does not explore the optimal number of rounds or the potential benefits of additional rounds in specific scenarios.
- What evidence would resolve it: Systematic experiments varying the number of rounds of self-refinement and measuring the corresponding performance of CLoT would provide insights into the optimal number of rounds and the potential benefits in specific scenarios.

## Limitations

- Dataset scope uncertainty: The multimodal and multilingual claims lack detailed statistics on language distribution and cross-modal alignment quality.
- Generalization concerns: Evaluation focuses primarily on Japanese humor generation without evidence for broader creative tasks or cultural contexts.
- Selection bias in self-refinement: The ranking-based selection process may reinforce existing model biases despite weakly-associated conditions.

## Confidence

**High confidence**: The basic mechanism of using weakly-associated conditions to stimulate remote association thinking is well-supported by experimental results. The technical implementation appears sound.

**Medium confidence**: The claim that CLoT significantly enhances LoT ability across various tasks is supported but limited by the narrow task scope and small user study sample sizes.

**Low confidence**: The assertion that most existing LLMs struggle with Oogiri due to insufficient LoT ability is based on limited baseline comparisons and would benefit from more comprehensive evaluations.

## Next Checks

1. **Cross-cultural generalization test**: Evaluate CLoT on Oogiri-style humor generation in non-Japanese languages and cultures to assess whether the LoT improvements transfer beyond the original dataset's cultural context.

2. **Ablation study on self-refinement**: Conduct experiments comparing CLoT with and without the self-refinement stage on a separate held-out test set to quantify the exact contribution of this component and test for potential performance collapse.

3. **Baseline expansion**: Test CLoT against a broader range of state-of-the-art multimodal models (e.g., GPT-4V, Gemini) on both Oogiri and non-Oogiri creative tasks to establish the true competitive advantage of the approach.