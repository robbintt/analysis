---
ver: rpa2
title: 'Amortized Reparametrization: Efficient and Scalable Variational Inference
  for Latent SDEs'
arxiv_id: '2312.10550'
source_url: https://arxiv.org/abs/2312.10550
tags:
- time
- latent
- differential
- posterior
- equations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a memory and time efficient method for inferring
  latent stochastic differential equations (SDEs) with a time and memory cost that
  scales independently with the amount of data, the length of the time series, and
  the stiffness of the approximate differential equations. The key idea is to remove
  the need to solve differential equations when approximating gradients using a novel
  amortization strategy coupled with a recently derived reparametrization of expectations
  under linear SDEs.
---

# Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs

## Quick Facts
- arXiv ID: 2312.10550
- Source URL: https://arxiv.org/abs/2312.10550
- Reference count: 40
- Primary result: Achieves constant-time and memory variational inference for latent SDEs by replacing differential equation solving with Monte Carlo integration

## Executive Summary
This paper introduces a memory and time efficient method for inferring latent stochastic differential equations (SDEs) with constant computational complexity, independent of data size, time series length, and approximate differential equation stiffness. The key innovation is removing the need to solve differential equations during gradient computation through a novel amortization strategy coupled with reparametrization of expectations under linear SDEs. This enables unbiased gradient approximations with user-chosen constant time and memory costs. The method demonstrates performance comparable to adjoint sensitivity methods while requiring over an order of magnitude fewer model evaluations during training.

## Method Summary
The method employs variational inference with a latent SDE model where the approximate posterior is assumed to be a Markov Gaussian Process. Rather than solving differential equations to compute gradients of the ELBO, the approach uses a reparametrization trick combined with Monte Carlo sampling to approximate gradients. The dataset is partitioned into non-overlapping windows, and posteriors are approximated over small windows using only local observations. Nested Monte Carlo with stratified sampling is used to approximate the time integral in the ELBO, reducing variance without increasing parallel computation cost. The overall computational complexity scales as O(M(R + S)) where M is the partition size, R is the number of posterior samples, and S is the number of time integral samples.

## Key Results
- Achieves constant time and memory complexity O(M(R + S)) independent of data size and time series length
- Requires more than an order of magnitude fewer model evaluations than adjoint sensitivity methods during training
- Demonstrates competitive performance on video data, motion capture datasets, and synthetic examples
- Successfully learns latent neural SDEs from video sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method achieves constant time and memory complexity by replacing differential equation solving with Monte Carlo integration.
- Mechanism: Instead of solving ODEs/SDEs iteratively during gradient computation, the approach uses a reparametrization trick combined with Monte Carlo sampling to approximate gradients of the evidence lower bound (ELBO). This removes the need for sequential differential equation solver steps.
- Core assumption: The approximate posterior over latent states can be modeled as a Markov Gaussian Process (MGP), allowing closed-form expectations under linear SDEs.
- Evidence anchors:
  - [abstract]: "We achieve this computational advancement by removing the need to solve differential equations when approximating gradients using a novel amortization strategy coupled with a recently derived reparametrization of expectations under linear SDEs."
  - [section 2.4]: "With this rearrangement, we can derive the main result of this work" showing the unbiased gradient estimate formula.
  - [corpus]: Weak - corpus papers discuss variational inference for SDEs but don't specifically address the constant complexity claim through this mechanism.
- Break Condition: If the latent state posterior cannot be well-approximated by an MGP, the closed-form expectations break down and differential equation solving becomes necessary again.

### Mechanism 2
- Claim: Amortization strategy allows processing of long time series by partitioning data and approximating posteriors over small windows.
- Mechanism: The dataset is split into non-overlapping partitions, and the posterior over latent states is approximated using only observations within each partition window. This reduces the problem size and memory requirements.
- Core assumption: The latent state can be approximated reasonably well using only local observations from each partition window.
- Evidence anchors:
  - [section 2.3]: "Rather than attempting to compute and store the posterior over the entire latent trajectory, we will instead construct an approximation to the posterior over a small window of observations as a function of those observations."
  - [section 2.3]: "An additional advantage of this amortization strategy is that it allows our approach to scale to multiple trajectories without an increase to the overall computational cost."
  - [corpus]: Weak - corpus papers discuss variational inference for SDEs but don't specifically address the partitioning/amortization strategy for scaling.
- Break Condition: If the latent dynamics have long-range dependencies that cannot be captured by small partition windows, the approximation quality degrades significantly.

### Mechanism 3
- Claim: Nested Monte Carlo with stratified sampling reduces variance in time integral approximation without increasing parallel computation cost.
- Mechanism: The integral over time in the ELBO is approximated using multiple samples per posterior sample (nested Monte Carlo), with stratified sampling to reduce variance. This improves gradient estimate quality while maintaining parallelizability.
- Core assumption: The variance reduction from stratified sampling outweighs the additional computational cost of more samples.
- Evidence anchors:
  - [section 2.4]: "Because the integral over time is one-dimensional we used stratified sampling to draw from U(t(j)1, t(j+1)1) in order to further reduce the variance in the integral over time."
  - [section 2.4]: "In the case that evaluations of the SDE drift term were relatively cheap compared to decoder evaluations... we found it useful to increase the number of samples used to approximate the integral over time without increasing the number of samples from the variational posterior."
  - [corpus]: Weak - corpus papers don't discuss nested Monte Carlo or stratified sampling strategies for SDE variational inference.
- Break Condition: If the cost of evaluating the drift/diffusion functions becomes comparable to decoder evaluations, the nested Monte Carlo strategy loses its advantage.

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs)
  - Why needed here: The paper builds a generative model where latent states evolve according to SDEs, so understanding SDE fundamentals is crucial for grasping the model structure and inference approach.
  - Quick check question: What is the key difference between modeling uncertainty using SDEs versus ODEs with random initial conditions?

- Concept: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: The training objective is derived from variational inference principles, and the ELBO forms the basis for the gradient-based optimization approach.
  - Quick check question: How does the ELBO relate to the Kullback-Leibler divergence between approximate and true posteriors?

- Concept: Reparametrization Trick
  - Why needed here: The method relies on reparametrization to construct unbiased gradient estimates for the ELBO, which is central to the computational efficiency claim.
  - Quick check question: What is the key insight behind the reparametrization trick that allows gradient computation through stochastic nodes?

## Architecture Onboarding

- Component map: Data → Encoder → Deep Kernel → Latent State Approximation → Drift/Decoder Evaluation → Monte Carlo Gradient Estimation → Parameter Update
- Critical path: Data → Encoder → Deep Kernel → Latent State Approximation → Drift/Decoder Evaluation → Monte Carlo Gradient Estimation → Parameter Update
- Design tradeoffs:
  - Partition size (M) vs. approximation accuracy: Larger partitions capture more temporal context but increase computation
  - Number of Monte Carlo samples (R, S) vs. gradient variance: More samples reduce variance but increase computation
  - Neural network architecture complexity vs. overfitting risk: More complex networks can capture richer dynamics but may overfit
- Failure signatures:
  - High gradient variance leading to unstable training: Likely caused by insufficient Monte Carlo samples
  - Poor reconstruction quality: May indicate encoder/decoder mismatch or insufficient latent dimension
  - Numerical instability in parameter updates: Could be caused by improper scaling of the KL-divergence term
- First 3 experiments:
  1. Implement and validate the encoder-decoder architecture on a simple synthetic dataset (e.g., noisy observations from a known SDE) to ensure basic functionality
  2. Test the amortization strategy with different partition sizes on a short time series to understand the tradeoff between approximation quality and computational cost
  3. Benchmark the method against adjoint sensitivity approaches on a medium-scale problem (e.g., motion capture data) to verify the claimed computational efficiency

## Open Questions the Paper Calls Out
- The paper acknowledges that the method only allows for time-dependent diffusion processes and should not be used when state-dependent diffusion is required, as this would violate the Markov Gaussian Process assumption.

## Limitations
- The method is limited to time-dependent diffusion processes and cannot handle state-dependent diffusion, which restricts its applicability to certain SDE models.
- The partitioning strategy may struggle with long-range dependencies in latent dynamics that cannot be captured within small partition windows.
- Theoretical analysis of approximation error bounds and variance reduction rates is lacking, with empirical validation being the primary support for claimed advantages.

## Confidence
- **High Confidence**: The constant-time complexity claim (O(M(R + S))) and the overall framework for amortized variational inference of latent SDEs are well-supported by both theoretical derivations and empirical results.
- **Medium Confidence**: The effectiveness of the nested Monte Carlo approach with stratified sampling for variance reduction is demonstrated empirically but lacks rigorous theoretical analysis.
- **Medium Confidence**: The scalability claims for long time series and multiple trajectories are supported by experimental results, though the partitioning strategy's limitations for capturing long-range dependencies are acknowledged.

## Next Checks
1. Derive explicit bounds on the approximation error introduced by the Markov Gaussian Process assumption and the partitioning strategy, quantifying the tradeoff between partition size and approximation quality.
2. Conduct a systematic study comparing the variance of gradient estimates using standard Monte Carlo versus stratified sampling across different SDE models and data regimes.
3. Test the method on datasets with significantly longer time series (100k+ time points) and multiple trajectories (1000+) to empirically validate the claimed linear scaling with data size.