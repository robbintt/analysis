---
ver: rpa2
title: Self-supervised learning of multi-omics embeddings in the low-label, high-data
  regime
arxiv_id: '2311.09962'
source_url: https://arxiv.org/abs/2311.09962
tags:
- data
- features
- pretraining
- samples
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores self-supervised learning (SSL) for multi-omics
  data integration in the low-label regime. The authors propose a contrastive SSL
  approach using a masked tabular transformer (FT-Transformer) and evaluate its performance
  on cancer subtype prediction tasks using gene expression data from TCGA.
---

# Self-supervised learning of multi-omics embeddings in the low-label, high-data regime

## Quick Facts
- arXiv ID: 2311.09962
- Source URL: https://arxiv.org/abs/2311.09962
- Authors: 
- Reference count: 23
- Key outcome: Contrastive self-supervised pretraining via MTR outperforms gradient boosting methods for cancer subtype prediction when labeled data is scarce but unlabeled data is abundant

## Executive Summary
This paper addresses the challenge of multi-omics data integration in the low-label regime, where labeled samples are scarce but unlabeled data is abundant. The authors propose a contrastive self-supervised learning approach using a masked tabular transformer (FT-Transformer) and demonstrate its effectiveness for cancer subtype prediction tasks. They show that pretraining with masked token replacement (MTR) significantly improves performance compared to traditional gradient boosting methods when labeled data is limited. Additionally, they introduce a novel multi-modal architecture (DuoFTT) that integrates data from multiple omics sources and show that it improves performance compared to single-omics models.

## Method Summary
The method uses an FT-Transformer backbone with self-supervised pretraining via Mask Token Replacement (MTR). During pretraining, a fraction of input features are masked and imputed with a learned mask token, and the model is trained using contrastive loss to align representations of original and masked versions of the same sample. For multi-omics integration, a DuoFTT architecture is proposed where each omics source passes through its own FT-Transformer, with latent representations averaged for contrastive pretraining. The model is finetuned on a small fraction (1%, 5%, or 10%) of labeled training data and evaluated on held-out test sets.

## Key Results
- MTR pretraining improves FT-Transformer performance on single-omics cancer subtype prediction compared to XGBoost and CatBoost baselines when labeled data is scarce
- DuoFTT multi-modal architecture outperforms single-omics models on combined omics datasets
- Omics-specific modules extracted from pretrained DuoFTT perform better on single-omics tasks than models pretrained on single-omics data alone
- Pretraining with unmatched data is effective when limited matched multi-omics data is available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive self-supervised pretraining via MTR learns effective representations for downstream cancer subtype prediction in low-label regimes.
- Mechanism: MTR randomly masks a fraction of features and imputes them with a learned mask token. The model is trained to align the latent representations of the original and masked versions of the same sample, while pushing apart representations of different samples. This forces the model to learn robust, generalizable features from the unmasked data.
- Core assumption: The mask token learns an effective imputation strategy, and the contrastive loss forces the model to extract meaningful information from the unmasked features.
- Evidence anchors:
  - [abstract]: "This model, a pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost, standard benchmarks for tabular data, when labelled samples are scarce but the number of unlabelled samples is high."
  - [section]: "We find that there is a significant improvement in the performance of the model when pretrained using MTR."
- Break condition: If the number of labeled samples is not significantly smaller than the number of unlabeled samples, or if the features are not structured enough to benefit from masking and imputation.

### Mechanism 2
- Claim: Multi-modal pretraining with MTR improves performance on single-omics tasks.
- Mechanism: A DuoFTT architecture is proposed, where each omics source is passed through its own FT-Transformer. The latent representations are averaged and used for contrastive pretraining. After pretraining, the omics-specific modules can be extracted and finetuned on single-omics tasks. The cross-modal information learned during pretraining improves the quality of the representations for single-omics tasks.
- Core assumption: The latent representations learned during multi-modal pretraining contain useful information for single-omics tasks, and the modularity of the DuoFTT allows for effective extraction and finetuning of the omics-specific modules.
- Evidence anchors:
  - [section]: "We also show that we can extract an omics-specific module from our pretrained multi-modal model, and find that it produces stronger predictions from a single omics than an identical model that has been pretrained and finetuned on the same data, but with features from a single omics only."
  - [corpus]: Weak or missing evidence specifically addressing cross-modal learning benefits in this paper.
- Break condition: If the omics sources are not complementary or if the task does not benefit from information from multiple omics sources.

### Mechanism 3
- Claim: Pretraining with unmatched data is effective when matched data is scarce.
- Mechanism: The DuoFTT can be pretrained by pretraining each omics-specific module individually using unmatched samples (samples with only one omics present). This allows the model to learn from a larger amount of data, even if it is not fully matched across modalities.
- Core assumption: The information learned from unmatched samples is still useful for the downstream task, even if the samples are not fully matched across modalities.
- Evidence anchors:
  - [section]: "We also show that our model can be pretrained in a joint-manner, which requires a large number of multi-modal samples, or by pretraining each omics-specific module individually. The latter means that our multi-modal model can be pretrained with large amounts of unlabelled data when there are few samples where both modalities have been measured."
  - [corpus]: Weak or missing evidence specifically addressing the effectiveness of pretraining with unmatched data in this paper.
- Break condition: If the task requires very strong correspondence between modalities, or if the unmatched data is not relevant to the task.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is used to train the model to align the latent representations of augmented versions of the same sample while pushing apart representations of different samples. This is crucial for learning effective representations from unlabeled data.
  - Quick check question: What is the difference between contrastive learning and reconstruction-based self-supervised learning?

- Concept: Self-attention
  - Why needed here: The FT-Transformer uses self-attention to process the input features. This allows the model to learn complex relationships between the features and is crucial for its effectiveness on tabular data.
  - Quick check question: How does self-attention work in the FT-Transformer?

- Concept: Masking and imputation
  - Why needed here: Masking and imputation are used as a data augmentation strategy during pretraining. This forces the model to learn to extract meaningful information from incomplete data, which is useful for handling missing features at test time.
  - Quick check question: What is the purpose of the mask token in MTR?

## Architecture Onboarding

- Component map: Input features -> FT-Transformer -> Latent representation -> Contrastive loss -> Pretrained model -> Finetuning -> Downstream task
- Critical path: Input features -> FT-Transformer -> Latent representation -> Contrastive loss -> Pretrained model -> Finetuning -> Downstream task
- Design tradeoffs:
  - Modularity vs. joint modeling: The DuoFTT is modular, which allows for easier pretraining with unmatched data and extraction of omics-specific modules. However, it may not capture all cross-modal correlations as effectively as a joint model.
  - Masking rate: The optimal masking rate for MTR is around 45%. Too little masking may not provide enough signal, while too much masking may make the task too difficult.
- Failure signatures:
  - Poor performance on downstream task: This could indicate that the pretraining was not effective or that the model architecture is not well-suited to the task.
  - Overfitting during finetuning: This could indicate that the model is too complex or that there is not enough labeled data for finetuning.
- First 3 experiments:
  1. Train an FT-Transformer on a single-omics dataset with and without pretraining via MTR. Compare the performance on the downstream task.
  2. Train a DuoFTT on a multi-omics dataset with pretraining via MTR. Compare the performance with and without pretraining.
  3. Train a DuoFTT with pretraining via MTR using unmatched data. Compare the performance with pretraining using matched data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can contrastive learning with MTR effectively handle batch effects across different public datasets by pretraining on one dataset (e.g., TCGA) and fine-tuning on another (e.g., Cancer Cell Line Encyclopedia)?
- Basis in paper: [explicit] The authors suggest this as an interesting extension of their work, noting that batch effects, such as feature dropout, can be mitigated with contrastive pretraining.
- Why unresolved: The paper only evaluates the model on data from the same dataset for pretraining and fine-tuning.
- What evidence would resolve it: Experimental results comparing the performance of a model pretrained on TCGA and fine-tuned on Cancer Cell Line Encyclopedia against a model trained on Cancer Cell Line Encyclopedia data alone, specifically measuring the model's ability to handle batch effects.

### Open Question 2
- Question: Can contrastive learning with MTR be used to build encoders suitable for modality-to-modality pipelines with limited matched samples, such as predicting RPPA expression from mRNA expression data?
- Basis in paper: [inferred] The authors hypothesize that multi-modal contrastive pretraining with MTR could be useful for modality-to-modality pipelines, which is a multi-output regression problem with high-dimensional output.
- Why unresolved: The paper focuses on multi-class classification tasks and does not explore modality-to-modality pipelines.
- What evidence would resolve it: Experimental results comparing the performance of a model trained to predict RPPA expression from mRNA expression data using contrastive learning with MTR against a model trained with other methods, such as regression or traditional machine learning algorithms.

### Open Question 3
- Question: How does the performance of the DuoFTT model change when using more than two omics modalities?
- Basis in paper: [inferred] The paper only considers models with two omics modalities, and it is unclear how the model would perform with more modalities.
- Why unresolved: The paper does not explore the use of more than two omics modalities.
- What evidence would resolve it: Experimental results comparing the performance of the DuoFTT model with two omics modalities against the performance of a model with three or more omics modalities, specifically measuring the model's ability to integrate information from multiple omics sources and improve predictive performance.

### Open Question 4
- Question: How does the performance of the DuoFTT model change when using different types of omics data, such as epigenomic or proteomic data?
- Basis in paper: [inferred] The paper only considers models with miRNA, mRNA, and RPPA data, and it is unclear how the model would perform with other types of omics data.
- Why unresolved: The paper does not explore the use of different types of omics data.
- What evidence would resolve it: Experimental results comparing the performance of the DuoFTT model with miRNA, mRNA, and RPPA data against the performance of a model with other types of omics data, such as epigenomic or proteomic data, specifically measuring the model's ability to integrate information from different omics sources and improve predictive performance.

## Limitations

- The paper lacks detailed architectural specifications for the FT-Transformer and DuoFTT fusion mechanisms, making exact reproduction difficult
- Evidence for cross-modal learning benefits is weak, with no ablation studies isolating the contribution of multi-modal pretraining
- The evaluation focuses on a single data source (TCGA Pan-Cancer Atlas) without testing generalizability across different cancer types or omics platforms

## Confidence

- **High confidence**: The superiority of MTR pretraining over no pretraining for single-omics tasks (Mechanism 1)
- **Medium confidence**: The effectiveness of DuoFTT for multi-omics integration when matched data is available
- **Low confidence**: The claimed benefits of extracting omics-specific modules from multi-modal pretraining (Mechanism 2) and pretraining with unmatched data (Mechanism 3)

## Next Checks

1. Conduct ablation studies comparing multi-modal pretraining versus single-omics pretraining for downstream single-omics tasks to validate Mechanism 2
2. Test the approach on additional cancer types and independent multi-omics datasets to assess generalizability
3. Implement and compare alternative multi-omics fusion strategies (early fusion, joint modeling) against the proposed DuoFTT architecture