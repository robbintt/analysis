---
ver: rpa2
title: Generative AI Text Classification using Ensemble LLM Approaches
arxiv_id: '2309.07755'
source_url: https://arxiv.org/abs/2309.07755
tags:
- text
- language
- arxiv
- task
- spanish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of detecting and attributing generative
  AI-generated text, addressing the growing concerns about the potential misuse of
  large language models (LLMs) in producing fake content and misinformation. The authors
  propose an ensemble approach that combines multiple pre-trained LLMs to generate
  probability scores, which are then used as features for traditional machine learning
  classifiers.
---

# Generative AI Text Classification using Ensemble LLM Approaches

## Quick Facts
- arXiv ID: 2309.07755
- Source URL: https://arxiv.org/abs/2309.07755
- Reference count: 37
- Primary result: Ensemble LLM approach achieved 5th/13th rank (macro F1: 0.733/0.649) for English/Spanish binary classification and 1st rank (macro F1: 0.625/0.653) for multi-class model attribution

## Executive Summary
This paper addresses the challenge of detecting and attributing AI-generated text through an ensemble approach that combines multiple fine-tuned LLMs with traditional machine learning classifiers. The authors propose using probability outputs from diverse LLMs as features for TML models, achieving competitive performance in the AuTexTification shared task for both binary classification (human vs AI) and multi-class model attribution. Their approach ranked in the top 5 globally for English and Spanish detection tasks while securing first place for model attribution, demonstrating the effectiveness of probability-based ensemble methods for AI text detection.

## Method Summary
The method involves fine-tuning multiple pre-trained LLMs (BERT, DeBERTa, RoBERTa, XLM-RoBERTa variants) on English and Spanish datasets, then using their probability outputs as features for traditional ML classifiers (Voting, OneVsRest, ECOC, Linear SVC). The probability vectors from each LLM are concatenated or averaged to form feature representations, which are then used to train TML models for binary classification (human/AI) and multi-class model attribution tasks. The approach leverages language-specific fine-tuning and ensemble diversity to capture complementary discriminative patterns across different linguistic features.

## Key Results
- Achieved macro F1 scores of 0.733 (English) and 0.649 (Spanish) for binary classification task
- Secured first place ranking with macro F1 scores of 0.625 (English) and 0.653 (Spanish) for multi-class model attribution
- Demonstrated competitive performance across both languages in the AuTexTification shared task
- Showed that traditional ML classifiers can effectively leverage LLM-derived probability features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble probabilities from multiple fine-tuned LLMs capture complementary discriminative patterns for AI vs human text.
- Mechanism: Each LLM encodes different linguistic and stylistic features; concatenating their probability outputs creates a richer feature space than any single model.
- Core assumption: Different pre-trained LLMs learn non-overlapping and complementary feature representations for AI/human distinction.
- Evidence anchors: Abstract mentions ensemble neural model using probabilities from pre-trained LLMs as features for TML classifiers.
- Break condition: If all LLMs converge to similar decision boundaries, ensemble gain vanishes and overfits to model-specific noise.

### Mechanism 2
- Claim: Traditional ML classifiers (e.g., Voting, OneVsRest, ECOC) outperform end-to-end fine-tuned LLMs on this detection task.
- Mechanism: TML models trained on LLM-derived probability features can learn optimal decision boundaries without the complexity of full neural fine-tuning.
- Core assumption: High-dimensional probability vectors from multiple LLMs provide sufficient discriminative information for simpler classifiers.
- Evidence anchors: Model rankings in AuTexTification shared task show competitive performance of TML classifiers using LLM probabilities.
- Break condition: If probability features are noisy or insufficiently discriminative, TML classifiers degrade faster than fine-tuned LLMs.

### Mechanism 3
- Claim: Language-specific fine-tuning of LLMs (e.g., Spanish vs English) is necessary to achieve competitive performance.
- Mechanism: Fine-tuning on monolingual corpora adapts the model's embedding space and attention patterns to language-specific stylometric cues.
- Core assumption: Stylistic cues for AI generation differ across languages and require language-specific adaptation.
- Evidence anchors: Performance differences between English (0.733) and Spanish (0.649) in binary classification task.
- Break condition: If stylometric differences between human/AI text are universal, multilingual models could match monolingual fine-tuning.

## Foundational Learning

- Concept: Probability calibration in multi-class classification
  - Why needed here: The ensemble approach relies on raw probability outputs from multiple LLMs; miscalibration can bias the concatenated feature space.
  - Quick check question: What is the difference between a model's confidence score and its true likelihood of correctness?

- Concept: Traditional ML ensemble methods (Voting, OneVsRest, ECOC)
  - Why needed here: These methods aggregate probability features from LLMs into final predictions; understanding their strengths/weaknesses guides architecture choice.
  - Quick check question: How does a OneVsRest classifier differ from a Voting classifier in handling multi-class probability inputs?

- Concept: Cross-lingual representation learning
  - Why needed here: The task involves both English and Spanish; understanding how multilingual models handle language transfer informs model selection.
  - Quick check question: What is the main advantage of XLM-R over monolingual BERT for this detection task?

## Architecture Onboarding

- Component map: Raw text documents (English/Spanish) ‚Üí Fine-tuned LLMs (DeBERTa, RoBERTa, XLM-R, etc.) ‚Üí Probability vectors ‚Üí Concatenate/average probabilities (ùëÉùê∂ or ùëÉùê¥) ‚Üí TML classifier (Voting, OneVsRest, ECOC, Linear SVC) ‚Üí Final predictions (binary or multi-class)

- Critical path: Text ‚Üí LLM probability generation (fine-tuned per language/task) ‚Üí Probability concatenation/averaging ‚Üí TML classifier training/inference

- Design tradeoffs:
  - Model diversity vs. computational cost: More LLMs improve ensemble coverage but increase latency.
  - Concatenation vs. averaging: Concatenation preserves per-model confidence but risks dimensionality explosion; averaging reduces features but may lose discriminative signal.
  - Fine-tuning depth: Deeper fine-tuning may overfit small datasets; shallow adaptation preserves generalization.

- Failure signatures:
  - All LLMs converge to similar probabilities ‚Üí ensemble provides no gain.
  - TML classifier overfits to probability calibration artifacts ‚Üí poor generalization.
  - Language-specific fine-tuning fails to capture cross-lingual stylometric cues ‚Üí performance gap between English/Spanish.

- First 3 experiments:
  1. Baseline: Single fine-tuned LLM (e.g., DeBERTa) ‚Üí end-to-end classification vs. proposed ensemble.
  2. Ablation: Probability concatenation vs. averaging ‚Üí measure impact on TML classifier performance.
  3. Cross-lingual: Fine-tune multilingual LLM (XLM-R) on combined English+Spanish data ‚Üí compare vs. language-specific fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ensemble LLM approach compare to other state-of-the-art methods for AI text detection, such as fine-tuned transformer-based models or statistical detection methods like GLTR?
- Basis in paper: The paper mentions several approaches for AI text detection but does not provide a direct comparison of their ensemble approach to these methods.
- Why unresolved: The paper focuses on the performance of their ensemble approach within the context of the AuTexTification shared task and does not explore comparisons with other methods outside of this task.
- What evidence would resolve it: A comprehensive study comparing the performance of the ensemble LLM approach to other state-of-the-art methods on various datasets and tasks would provide insights into its relative effectiveness.

### Open Question 2
- Question: How does the proposed ensemble approach perform when applied to languages other than English and Spanish?
- Basis in paper: The paper only discusses the performance of the ensemble approach on English and Spanish datasets provided by the AuTexTification shared task.
- Why unresolved: The paper does not explore the applicability of the ensemble approach to other languages, limiting the generalizability of the results.
- What evidence would resolve it: Evaluating the ensemble approach on datasets in multiple languages and comparing its performance across languages would provide insights into its cross-linguistic effectiveness.

### Open Question 3
- Question: How does the ensemble approach handle text generated by language models with varying architectures, such as encoder-only, decoder-only, or encoder-decoder models?
- Basis in paper: The paper mentions that the ensemble approach uses pre-trained large language models but does not discuss how it handles text generated by models with different architectures.
- Why unresolved: The paper focuses on the performance of the ensemble approach on text generated by specific models but does not explore its effectiveness on text generated by models with varying architectures.
- What evidence would resolve it: Evaluating the ensemble approach on text generated by models with different architectures and comparing its performance across architectures would provide insights into its versatility.

## Limitations

- Performance claims based on single shared task evaluation, limiting generalizability to other datasets or real-world scenarios.
- Ensemble approach's superiority over end-to-end fine-tuned models is asserted but not directly compared in results.
- Critical implementation details such as exact model hyperparameters, training epochs, and probability calibration methods are not specified.
- Performance gap between English (0.733) and Spanish (0.649) in binary task raises questions about cross-lingual effectiveness.

## Confidence

- **High Confidence**: The feasibility of using LLM-derived probability features as input to traditional ML classifiers has been demonstrated through competitive ranking in the shared task.
- **Medium Confidence**: The ensemble approach improves detection accuracy, but direct ablation studies comparing ensemble vs. single-model approaches are absent.
- **Low Confidence**: Claims about the superiority of TML classifiers over fine-tuned LLMs are not empirically validated within the paper.

## Next Checks

1. **Direct Comparison**: Implement and evaluate an end-to-end fine-tuned LLM baseline on the same datasets to quantify the actual performance gain from the ensemble approach.

2. **Ablation Study**: Systematically test probability concatenation vs. averaging strategies across different LLM combinations to determine the optimal feature fusion method.

3. **Cross-Lingual Generalization**: Fine-tune a multilingual LLM (e.g., XLM-R) on combined English and Spanish data, then evaluate its performance on both languages to assess whether language-specific fine-tuning is truly necessary.