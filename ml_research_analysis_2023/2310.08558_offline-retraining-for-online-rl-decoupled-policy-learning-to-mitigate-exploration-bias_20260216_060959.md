---
ver: rpa2
title: 'Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration
  Bias'
arxiv_id: '2310.08558'
source_url: https://arxiv.org/abs/2310.08558
tags:
- policy
- exploration
- offline
- learning
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of bias in policies trained with
  exploration bonuses during online reinforcement learning, which can lead to suboptimal
  performance even when sufficient data has been collected. The proposed solution,
  Offline-to-Online-to-Offline (OOO) reinforcement learning, decouples the exploration
  and exploitation policies by training a separate pessimistic exploitation policy
  on all collected data after online fine-tuning.
---

# Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias

## Quick Facts
- arXiv ID: 2310.08558
- Source URL: https://arxiv.org/abs/2310.08558
- Reference count: 29
- Key outcome: OOO improves IQL by 26% and Cal-QL by 14% on fine-tuning tasks, achieves state-of-the-art on D4RL, and improves online RL by 165% on gym environments

## Executive Summary
This paper addresses the bias problem in policies trained with exploration bonuses during online reinforcement learning, where intrinsic rewards can lead to suboptimal task performance. The proposed Offline-to-Online-to-Offline (OOO) framework decouples exploration and exploitation by training a separate pessimistic exploitation policy on all collected data after online fine-tuning. This allows the exploration policy to focus on state-covering exploration while the exploitation policy recovers optimal task performance without exploration bias. OOO significantly improves performance across multiple benchmarks, particularly in environments requiring substantial exploration.

## Method Summary
OOO is a framework that addresses exploration bias in online RL by decoupling the exploration and exploitation policies. The method involves three phases: (1) pre-train an exploration policy on offline data with exploration bonuses, (2) fine-tune online using the exploration policy to collect data, and (3) retrain an exploitation policy pessimistically on all collected data using offline RL. The exploration policy uses task rewards plus intrinsic rewards (typically RND), while the exploitation policy uses only task rewards with pessimistic value estimation. This separation allows aggressive exploration without compromising final task performance.

## Key Results
- Improves average performance of IQL by 26% and Cal-QL by 14% on fine-tuning tasks
- Achieves state-of-the-art results on several D4RL environments
- Improves online RL performance by 165% on two OpenAI gym environments
- Particularly effective on exploration-heavy tasks like relocate-binary-v0 (>165% improvement) and kitchen-complete-v0 (>45% improvement)

## Why This Works (Mechanism)

### Mechanism 1
Decoupling exploration and exploitation policies removes intrinsic reward bias from the final policy, enabling better task performance. The exploration policy is trained with both task rewards and exploration bonuses, but the exploitation policy is trained only on task rewards using pessimistic offline RL. This separation ensures the final policy is optimized solely for the task objective, not biased toward exploration.

### Mechanism 2
Training the exploitation policy pessimistically on all collected data recovers a performant policy even when exploration data is suboptimal. The exploitation policy is trained using pessimistic offline RL algorithms on the full dataset collected during exploration. This allows it to focus on maximizing task rewards from the best data, ignoring the exploration bias.

### Mechanism 3
Decoupling allows more aggressive exploration without compromising the final policy's performance. Since the final policy is trained separately on all data, the exploration policy can use larger exploration bonuses to maximize state coverage without worrying about the bias it introduces. This leads to better data for the exploitation policy.

## Foundational Learning

- **Exploration vs. Exploitation trade-off in RL**: The paper addresses the challenge of balancing exploration (discovering new states) and exploitation (maximizing rewards) in online RL, especially when initialized from offline data. *Quick check*: What is the main difference between exploration and exploitation in the context of reinforcement learning?

- **Intrinsic rewards and exploration bonuses**: The paper uses exploration bonuses (like RND) to encourage the agent to visit novel states, which is crucial for the exploration policy but can bias the final policy if not handled properly. *Quick check*: How do intrinsic rewards differ from extrinsic (task) rewards in reinforcement learning?

- **Pessimistic vs. Optimistic value estimation**: The exploitation policy is trained pessimistically to avoid overestimating values for out-of-distribution actions, while the exploration policy is optimistic to encourage exploration. *Quick check*: What is the main difference between pessimistic and optimistic value estimation in reinforcement learning?

## Architecture Onboarding

- **Component map**: Pre-trained exploration policy -> Online exploration data collection -> Exploitation policy training -> Final evaluation
- **Critical path**: 1) Pre-train exploration policy on offline data with exploration bonus, 2) Online fine-tuning: Explore with exploration policy, collect data, 3) Offline retraining: Train exploitation policy pessimistically on all data, 4) Evaluation: Use exploitation policy
- **Design tradeoffs**: Decoupling exploration and exploitation increases computational cost (training two separate policies) but allows for more aggressive exploration and better final performance. Using pessimistic algorithms for exploitation ensures safety but may limit performance if data is insufficient.
- **Failure signatures**: Exploration policy fails to collect high-reward data: Exploitation policy cannot recover good performance; Exploration bonus does not decay: Final exploration policy remains biased toward exploration; Pessimistic exploitation fails: Q-values explode or policy underperforms
- **First 3 experiments**: 1) Run OOO with a simple exploration bonus (e.g., count-based) on a sparse reward environment to verify the decoupling mechanism works, 2) Compare OOO with and without the pessimistic exploitation step to isolate the effect of pessimism, 3) Test OOO with different exploration bonuses (e.g., RND vs. count-based) to understand the impact on state coverage and final performance

## Open Questions the Paper Calls Out

### Open Question 1
Can decoupled policy learning be applied to other exploration bonuses beyond RND? The paper uses RND as the exploration bonus but mentions that "other novelty bonuses can be used in OOO". The paper only experiments with RND, leaving open the question of how other bonuses like count-based or disagreement-based bonuses would perform.

### Open Question 2
What is the optimal frequency for retraining the exploitation policy during online fine-tuning? The paper mentions that "in theory, one can repeatedly recompute Vexploit after every step t" but notes this can be expensive in practice. The paper uses a single offline retraining step at the end, but doesn't explore intermediate retraining steps or determine an optimal retraining frequency.

### Open Question 3
How does OOO perform with different offline-to-online RL algorithms beyond IQL and Cal-QL? The paper only experiments with IQL and Cal-QL as base algorithms for OOO. While OOO is presented as a general framework, the paper only validates it with two specific algorithms, leaving uncertainty about its effectiveness with other methods.

### Open Question 4
What is the impact of the offline dataset quality and size on OOO's performance gains? The paper experiments with environments having "suboptimal" offline data and shows OOO can still improve performance, but doesn't systematically vary dataset quality or size. The paper uses fixed datasets of varying quality but doesn't explore how performance scales with dataset size or how different quality levels affect the gains from OOO.

## Limitations
- The effectiveness depends on the exploration bonus not decaying to zero during online training, which isn't empirically validated
- The method requires sufficient high-reward trajectories in the collected data for the exploitation policy to learn effectively
- Training two separate policies doubles computational cost, but runtime comparisons are not provided

## Confidence
- **High confidence**: The decoupling mechanism itself (separating exploration and exploitation policies) is well-established and experimentally validated
- **Medium confidence**: The specific claim about 26% and 14% improvements over IQL and Cal-QL, as these depend on implementation details not fully specified
- **Medium confidence**: The state-of-the-art claims on D4RL environments, as performance can be sensitive to hyperparameters and implementation details

## Next Checks
1. **Exploration bonus decay analysis**: Track the magnitude of intrinsic rewards throughout the online fine-tuning phase to empirically verify whether the exploration bonus decays toward zero or maintains bias
2. **Ablation on exploration success**: Systematically vary the exploration policy's success rate at collecting high-reward trajectories and measure the corresponding impact on exploitation policy performance
3. **Computational overhead measurement**: Benchmark wall-clock training time and GPU memory usage for OOO versus standard online RL methods to quantify the practical cost of decoupling