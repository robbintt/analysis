---
ver: rpa2
title: 'Decomposer: Semi-supervised Learning of Image Restoration and Image Decomposition'
arxiv_id: '2311.16829'
source_url: https://arxiv.org/abs/2311.16829
tags:
- image
- shadow
- light
- original
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Decomposer is a semi-supervised model that decomposes sequences
  of distorted images into their original image, shadow/light masks, and occlusion
  masks. It uses a 3D Swin-Transformer encoder and three separate 3D U-Net decoders
  for each component.
---

# Decomposer: Semi-supervised Learning of Image Restoration and Image Decomposition

## Quick Facts
- arXiv ID: 2311.16829
- Source URL: https://arxiv.org/abs/2311.16829
- Authors: 
- Reference count: 4
- Primary Result: Semi-supervised model that decomposes sequences of distorted images into original image, shadow/light masks, and occlusion masks using 3D Swin-Transformer encoder and three separate 3D U-Net decoders

## Executive Summary
Decomposer is a semi-supervised model that learns to decompose sequences of distorted images into their constituent components: the original image, shadow/light masks, and occlusion masks. The model uses a 3D Swin-Transformer encoder with three separate 3D U-Net decoders, one for each component. Training occurs in stages using pseudo-labels generated from the SIDAR dataset, with mask decay regularization to reduce false positives in occlusion detection. The approach achieves strong quantitative results while using only ground truth images (not ground truth augmentations) during training.

## Method Summary
The model implements an encoder-decoder architecture where a 3D Swin-Transformer encoder produces spatio-temporal embeddings from sequences of augmented images. Three separate 3D U-Net decoders specialize in extracting different components: the original image, shadow/light masks, and occlusion masks. Training proceeds in three stages: pretraining the encoder as an autoencoder, pretraining each decoder branch on pseudo-labels generated from the SIDAR dataset, and finally fine-tuning all components together with mask decay regularization to prevent false positive occlusions.

## Key Results
- SSIM scores: 0.67 for original image reconstruction, 0.82 for shadow/light reconstruction, 0.88 for full reconstruction
- MSE values: 1102 for original image, 268 for shadow/light, 220.5 for full reconstruction
- 4.4% false positive rate for occlusion detection through mask decay regularization
- Strong performance without requiring ground truth augmentations during training

## Why This Works (Mechanism)

### Mechanism 1
Pseudo-label pretraining steers the model toward meaningful decompositions by providing weak supervision for ambiguous components. The model first learns to predict approximations of shadow/light masks and occlusion maps derived from ground truth images, constraining the loss landscape so that fine-tuning converges to physically interpretable decompositions.

### Mechanism 2
Separate decoder branches for each component prevent feature interference and improve reconstruction fidelity. The encoder produces a shared spatio-temporal embedding, but three independent 3D U-Net decoders specialize in extracting the original image, shadow/light masks, and occlusion masks respectively, allowing each branch to optimize its loss independently during pretraining.

### Mechanism 3
Mask decay regularization reduces false positives in occlusion detection by penalizing non-sparse predictions. The occlusion branch outputs a probability mask, and mask decay adds a term proportional to the mean predicted occlusion value to the loss, encouraging the model to predict zeros unless strong evidence exists.

## Foundational Learning

- **Image formation and augmentation models**: Understanding how shadows (multiplicative) and lights/occlusions (additive) alter pixel values is critical for designing correct reconstruction equations and pseudo-labels.
  - Quick check: If an image pixel value is 100 and a shadow mask value is 0.5, what is the resulting pixel after applying the shadow?

- **Semi-supervised learning and pseudo-label generation**: The paper relies on synthetic labels derived from ground truth images; grasping how to generate and validate such labels is key to reproducing the approach.
  - Quick check: Given an augmented image and its ground truth, how would you approximate the shadow mask if you know shadows are multiplicative?

- **Sequence-to-sequence modeling with transformers**: The model treats a set of augmented views as a sequence and uses Video Swin Transformer to embed temporal/spatial correlations, which differs from standard image-to-image models.
  - Quick check: Why might a 3D Swin Transformer be preferred over a 2D CNN for processing multiple augmented views of the same image?

## Architecture Onboarding

- **Component map**: Encoder (3D Swin Transformer) → OI decoder (3D U-Net) → original image, SL decoder (3D U-Net) → shadow/light masks, OCC decoder (3D U-Net) → occlusion mask
- **Critical path**: Encoder output → each decoder → compose outputs → reconstruction loss
- **Design tradeoffs**: Separate decoders increase modularity but add parameters and may reduce parameter sharing; pretraining on pseudo-labels adds training stages but improves convergence on ambiguous tasks; mask decay penalizes sparsity but too high a weight may kill true occlusion detections
- **Failure signatures**: Oversaturated OI reconstruction indicates OI sub-loss missing or too weak; occlusion masks too dense suggests mask decay weight too low or pseudo-labels inaccurate; poor SL reconstruction indicates inaccurate pseudo-labels or insufficient encoder detail retention
- **First 3 experiments**:
  1. Train only the OI branch on pseudo-labels; check if reconstructed OI matches ground truth structure
  2. Train SL branch alone; verify predicted shadow/light masks produce correct SL reconstruction when applied to OI
  3. Train OCC branch with BCE loss and mask decay; evaluate false positive rate on a validation set

## Open Questions the Paper Calls Out
- The paper discusses how very dark occlusions are often inaccurately reconstructed as shadows, identifying this as a significant challenge without providing a clear solution or evaluation of severity.
- The paper mentions using approximated targets for pseudo-labels that do not contain hard shadows and spotlights from the original data, but doesn't explore alternative pseudo-label generation methods.
- The paper justifies using separate decoders because features for different components differ, but doesn't experimentally validate this architectural choice against unified alternatives.

## Limitations
- Limited architectural details for 3D Swin-Transformer and U-Net components make exact reproduction challenging
- Training hyperparameters and loss weighting are not fully specified
- Mask decay implementation lacks clarity on how the weighted mean is computed and applied

## Confidence
- **High**: The core mechanism of using pseudo-labels for pretraining is well-supported by the paper's methodology section
- **Medium**: The separate decoder branch design is logically sound but lacks external validation in the corpus
- **Low**: The effectiveness of mask decay regularization is based solely on this paper's results with no external corroboration

## Next Checks
1. Verify pseudo-label generation by testing if subtracting augmented from original images produces reasonable shadow/light masks on a small sample
2. Test whether mask decay actually reduces false positives by training with and without this regularization on a validation set
3. Validate the independence assumption by comparing performance of separate decoders versus a single decoder with shared features