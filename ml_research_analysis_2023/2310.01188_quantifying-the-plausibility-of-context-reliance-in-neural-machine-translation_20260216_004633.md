---
ver: rpa2
title: Quantifying the Plausibility of Context Reliance in Neural Machine Translation
arxiv_id: '2310.01188'
source_url: https://arxiv.org/abs/2310.01188
tags:
- context
- association
- computational
- linguistics
- disceval-mt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PECORE, an interpretability framework for
  analyzing context reliance in language models. PECORE enables end-to-end extraction
  of context-sensitive target tokens and their influential contextual cues from model
  generations, allowing quantitative evaluation of context usage plausibility.
---

# Quantifying the Plausibility of Context Reliance in Neural Machine Translation

## Quick Facts
- arXiv ID: 2310.01188
- Source URL: https://arxiv.org/abs/2310.01188
- Reference count: 40
- Key outcome: PECORE framework enables quantitative analysis of context reliance in neural MT, showing KL-divergence and gradient norm attribution effectively identify context-sensitive tokens and cues.

## Executive Summary
This paper introduces PECORE, an interpretability framework for analyzing context reliance in language models. PECORE enables end-to-end extraction of context-sensitive target tokens and their influential contextual cues from model generations, allowing quantitative evaluation of context usage plausibility. The authors evaluate PECORE components on context-aware machine translation, showing that KL-divergence is an effective metric for detecting context-sensitive tokens and gradient norm attribution is robust for identifying contextual cues. Applying PECORE to unannotated translations reveals instances of correct and incorrect context usage, including lexical cohesion and gender agreement errors. Overall, PECORE provides a unified approach to assess when and how context influences language model generations.

## Method Summary
PECORE consists of two main components: Context-Sensitive Target Identification (CTI) and Contextual Cues Imputation (CCI). CTI uses contrastive metrics (KL-divergence, likelihood ratio) to identify tokens whose predictions change with context. CCI employs gradient-based attribution to trace these tokens back to specific contextual cues. The framework is evaluated on fine-tuned MT models using datasets with annotated context-sensitive tokens. Performance is measured using macro F1 and AUPRC scores against human annotations, with systematic comparison of different metrics and attribution methods.

## Key Results
- KL-divergence outperforms pointwise metrics for detecting context-sensitive tokens by capturing full distributional shifts
- Gradient norm attribution reliably identifies contextual cues through contrastive prediction analysis
- End-to-end PECORE successfully extracts both context-sensitive tokens and their influential cues from unannotated translations
- Analysis reveals correct context usage (lexical cohesion) and errors (gender agreement failures) in MT models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-Divergence effectively detects context-sensitive tokens because it captures distributional shifts beyond just top-1 probability changes.
- Mechanism: When context is added, the entire probability distribution over vocabulary changes. KL-Divergence measures the divergence between contextual and non-contextual distributions, capturing this holistic shift.
- Core assumption: Context affects the model's probability distribution across multiple tokens, not just the top prediction.
- Evidence anchors:
  - [abstract]: "KL-divergence is an effective metric for detecting context-sensitive tokens"
  - [section]: "KL-Divergence performs similarly and sometimes better than pointwise metrics... This suggests the distribution shift caused by context inclusion can provide useful information"
  - [corpus]: Weak evidence - no corpus studies comparing KL-Divergence to other metrics on broader datasets

### Mechanism 2
- Claim: Gradient norm attribution reliably identifies contextual cues because it measures input importance for contrastive predictions.
- Mechanism: By computing the gradient of the contrastive target (difference between contextual and non-contextual probabilities) with respect to input tokens, the method quantifies which inputs drive the model to prefer one prediction over another.
- Core assumption: Input gradients provide faithful attribution of prediction importance to input tokens.
- Evidence anchors:
  - [abstract]: "gradient norm attribution is robust for identifying contextual cues"
  - [section]: "At ctx = {∥∇c[fTGT(Pi ctx, Pi no-ctx)]|∀c∈C}" - formal definition showing gradient-based attribution
  - [corpus]: Weak evidence - corpus doesn't provide ablation studies on alternative attribution methods

### Mechanism 3
- Claim: End-to-end extraction works because CTI and CCI are complementary steps that sequentially identify what changes and why.
- Mechanism: CTI first identifies tokens whose predictions differ with context, then CCI traces these predictions back to specific context tokens using contrastive attribution. This decomposition simplifies the complex problem of context usage analysis.
- Core assumption: Context-sensitive tokens can be reliably identified first, then their causal context cues can be traced independently.
- Evidence anchors:
  - [abstract]: "PECoRe enables the end-to-end extraction of context-sensitive target tokens and their influential contextual cues"
  - [section]: "CTI first identifies tokens whose predictions differ with context, then CCI traces these predictions back to specific context tokens"
  - [corpus]: Weak evidence - corpus shows related works but lacks systematic evaluation of end-to-end vs. combined approaches

## Foundational Learning

- Concept: Contrastive conditioning
  - Why needed here: PECORE relies on comparing contextual vs. non-contextual model behavior to isolate context effects
  - Quick check question: How does contrastive conditioning help distinguish context effects from model biases?

- Concept: Gradient-based attribution
  - Why needed here: CCI uses gradient norms to quantify input importance for contrastive predictions
  - Quick check question: What's the difference between using gradient norms vs. attention weights for attribution?

- Concept: Probability distribution analysis
  - Why needed here: KL-Divergence and other metrics analyze full distributions, not just top predictions
  - Quick check question: Why might full distribution analysis be more informative than top-1 probability changes?

## Architecture Onboarding

- Component map: CTI module (metrics + selector) -> CCI module (attribution method + selector) -> Pipeline (CTI → CCI → cue-target pair extraction) -> Evaluation (macro F1, AUPRC)

- Critical path: 1. Generate contextual and non-contextual translations 2. Compute contrastive metrics for CTI 3. Select context-sensitive tokens 4. Apply attribution method for CCI 5. Select contextual cues 6. Form cue-target pairs 7. Evaluate against human annotations

- Design tradeoffs:
  - Simplicity vs. accuracy: Statistical selectors vs. learned models
  - Computational cost: Full distribution metrics vs. pointwise metrics
  - Generality vs. specificity: Task-agnostic metrics vs. task-specific ones

- Failure signatures:
  - CTI: Low precision indicates false positives in context-sensitive token detection
  - CCI: Low recall indicates failure to trace tokens back to context
  - End-to-end: Performance gap between gold and extracted tokens

- First 3 experiments:
  1. Ablation study: Compare KL-Divergence vs. likelihood ratio on same dataset
  2. Attribution comparison: Gradient norm vs. attention weights on gold context-sensitive tokens
  3. End-to-end validation: Apply PECORE on unannotated data and manually verify outputs

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but raises several important considerations: the framework's applicability to broader NLP tasks beyond translation, the impact of different model architectures on PECORE's effectiveness, and how threshold selection affects performance across different domains.

## Limitations
- Dataset dependency: Evaluation relies on gold-annotated datasets that may not represent real-world translation scenarios
- Threshold sensitivity: Heuristic threshold choices (1-2 standard deviations) may not optimize for all scenarios
- Attribution method assumptions: Gradient-based attribution may fail for saturated activations or specific attention patterns

## Confidence
- High confidence: Framework architecture and basic operational mechanism are well-established and theoretically sound
- Medium confidence: Quantitative evaluation results are based on specific datasets and may vary with different models or architectures
- Low confidence: Claims about broader applicability to other NLP tasks are largely speculative

## Next Checks
1. Apply PECORE to a different NLP task (e.g., summarization or dialogue generation) with annotated context-sensitivity data to verify framework generalizability beyond translation
2. Evaluate PECORE's performance across different model architectures (RNN, Transformer variants, smaller vs. larger models) to identify architecture-dependent effects
3. Conduct systematic parameter search for CTI and CCI selectors using cross-validation to determine optimal threshold values for different dataset characteristics