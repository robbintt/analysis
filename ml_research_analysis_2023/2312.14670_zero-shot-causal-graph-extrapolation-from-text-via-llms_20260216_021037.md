---
ver: rpa2
title: Zero-shot Causal Graph Extrapolation from Text via LLMs
arxiv_id: '2312.14670'
source_url: https://arxiv.org/abs/2312.14670
tags:
- causal
- text
- relations
- entities
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using large language models (LLMs) to infer
  causal relations from text, with a focus on biomedical literature. The authors develop
  a prompt-based approach for extracting causal graphs from abstracts by first identifying
  entities and then determining pairwise causal relationships between them.
---

# Zero-shot Causal Graph Extrapolation from Text via LLMs

## Quick Facts
- arXiv ID: 2312.14670
- Source URL: https://arxiv.org/abs/2312.14670
- Reference count: 5
- Key outcome: Using GPT-4 Turbo to extract causal graphs from biomedical text achieves 97% recall but only 74% precision

## Executive Summary
This paper investigates using large language models to infer causal relations from text without training samples, focusing on biomedical literature. The authors develop a prompt-based approach that first identifies entities from text, then determines pairwise causal relationships between them using GPT-4 Turbo. Tested on 20 biomedical abstracts with expert-validated causal graphs, their method achieves high recall (97%) but lower precision (74%), suggesting LLMs can effectively identify true causal relations but struggle to distinguish direct from indirect causal links. The study concludes that while LLMs show promise for causal graph extraction from text, additional work is needed to improve precision and handle complex graph structures like directed cycles.

## Method Summary
The approach uses GPT-4 Turbo with prompt engineering to extract entities and determine pairwise causal relations, then constructs causal graphs by iterating over all entity pairs. The method operates in two main steps: entity extraction using targeted prompts, followed by pairwise causal relation queries between all extracted entity pairs. The resulting pairwise relations are combined to form a complete causal graph, which is then evaluated against ground truth using recall and precision metrics. The approach is tested on 20 biomedical abstracts with expert-validated causal graphs as ground truth.

## Key Results
- Achieved 97% recall in extracting causal relations from biomedical text
- Obtained 74% precision, indicating significant false positive rate
- Successfully demonstrated zero-shot capability without training samples
- Identified challenges in distinguishing direct from indirect causal relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can accurately orient causal relationships in text without explicit training.
- Mechanism: LLMs leverage their pretraining on vast text corpora to understand implicit causal patterns, allowing them to infer the direction of cause-effect relationships from context.
- Core assumption: The causal knowledge embedded in LLMs during pretraining is sufficient for causal orientation tasks.
- Evidence anchors:
  - [section]: "The very accurate performance (F1-score â‰ƒ 99%) advocates the ability of GPT in properly recognising the right orientation in a causal sentence."
  - [corpus]: Weak - corpus neighbors do not directly support this mechanism, but the FMR scores suggest some topical relevance.
- Break condition: If the LLM lacks exposure to the specific causal patterns in the domain text, performance will degrade.

### Mechanism 2
- Claim: Prompt engineering improves LLM accuracy in causal graph extrapolation.
- Mechanism: Structured prompts guide the LLM to focus on the relevant text and format the output as a causal graph, reducing ambiguity.
- Core assumption: LLMs can follow structured instructions effectively.
- Evidence anchors:
  - [section]: "Among the many prompt engineering techniques, improving the clarity and precision of the prompt text by providing precise and specific instructions represents the most obvious strategy."
  - [corpus]: Weak - corpus neighbors do not directly address prompt engineering, but the FMR scores suggest some topical relevance.
- Break condition: If prompts are not sufficiently clear or the LLM fails to parse the instructions, accuracy will suffer.

### Mechanism 3
- Claim: Pairwise causal queries can be iterated to build full causal graphs.
- Mechanism: By querying the LLM for all possible pairwise causal relations and combining the results, a complete causal graph can be constructed.
- Core assumption: The LLM's pairwise causal judgments are consistent and can be combined without conflict.
- Evidence anchors:
  - [section]: "The pairwise procedure discussed in the previous section can be naturally extended to CG extrapolation by iterated applications on all the possible pairs of entities."
  - [corpus]: Weak - corpus neighbors do not directly support this mechanism, but the FMR scores suggest some topical relevance.
- Break condition: If the LLM produces inconsistent judgments or the pairwise queries miss indirect causal links, the resulting graph will be incomplete or incorrect.

## Foundational Learning

- Concept: Causal inference
  - Why needed here: Understanding the principles of causal inference is crucial for designing and evaluating the LLM-based causal graph extrapolation approach.
  - Quick check question: What is the difference between correlation and causation, and why is this distinction important for causal inference?

- Concept: Natural language processing
  - Why needed here: Familiarity with NLP techniques is necessary for understanding how LLMs process and extract causal relations from text.
  - Quick check question: What are some common NLP tasks involved in extracting causal relations from text, and how do they relate to the LLM-based approach?

- Concept: Prompt engineering
  - Why needed here: Knowledge of prompt engineering techniques is essential for designing effective prompts to guide the LLM's causal graph extrapolation.
  - Quick check question: What are some common prompt engineering strategies, and how can they be applied to improve the LLM's performance on causal graph extrapolation tasks?

## Architecture Onboarding

- Component map: Text -> Entity Extraction Module -> Pairwise Query Engine -> GPT-4 Turbo -> Causal Graph Construction Module -> Evaluation Module

- Critical path:
  1. Extract entities from the input text
  2. Generate pairwise causal queries using prompt engineering
  3. Query the LLM for causal relations between entity pairs
  4. Combine the pairwise results into a causal graph
  5. Evaluate the quality of the extracted causal graph

- Design tradeoffs:
  - Accuracy vs. computational cost: More detailed prompts and extensive pairwise queries may improve accuracy but increase computational time.
  - Direct vs. indirect causal relations: Distinguishing between direct and indirect causal links is challenging and may require additional prompt engineering or post-processing.

- Failure signatures:
  - Low precision in causal graph extraction: May indicate issues with prompt engineering or the LLM's ability to distinguish direct from indirect causal relations.
  - Inconsistency in pairwise causal judgments: May suggest the need for additional constraints or post-processing to resolve conflicts.

- First 3 experiments:
  1. Test the LLM's ability to orient causal relations in a set of biomedical sentences with known ground truth.
  2. Evaluate the impact of different prompt engineering strategies on the LLM's causal graph extrapolation performance.
  3. Assess the LLM's ability to distinguish between direct and indirect causal relations by comparing its output to expert-validated causal graphs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of large language models in distinguishing direct from indirect causal relationships be improved in causal graph extrapolation?
- Basis in paper: [explicit] The authors mention that the high number of false positives might reflect the inability of the model to distinguish between direct cause-effect relations and indirect ones, as discussed in the context of multiply connected patterns in causal graphs.
- Why unresolved: The paper identifies the issue but does not provide a solution or methodology to improve the model's ability to differentiate direct from indirect causal links.
- What evidence would resolve it: Developing and testing prompt engineering strategies or model architectures that enhance the model's capability to recognize direct causal relationships could provide evidence. Experimental results showing improved precision in distinguishing direct from indirect causal links would be conclusive.

### Open Question 2
- Question: Can large language models be effectively used as a post-processing tool to refine the output of standard causal discovery algorithms?
- Basis in paper: [explicit] The authors suggest that the good performance of GPT in recognizing the right orientation in causal sentences might make LLMs a natural post-processing tool to refine the output of standard algorithms for causal discovery when both data and text are available.
- Why unresolved: The paper does not provide empirical evidence or experiments to validate the effectiveness of using LLMs as a post-processing tool in conjunction with causal discovery algorithms.
- What evidence would resolve it: Conducting experiments that compare the performance of causal discovery algorithms with and without LLM-based post-processing on a benchmark of ground-truth causal graphs would provide evidence. Demonstrating improved accuracy or consistency in the refined outputs would be conclusive.

### Open Question 3
- Question: How can the presence of directed cycles in causal graphs, which may not reflect actual feedback loops, be addressed in large language models?
- Basis in paper: [explicit] The authors observe directed cycles in the resulting causal graphs, which are not expected in the medical domains under consideration, and suggest designing dedicated prompt engineering approaches to address this issue.
- Why unresolved: The paper identifies the occurrence of directed cycles but does not provide a methodology or prompt engineering strategy to prevent or resolve these cycles in the model's output.
- What evidence would resolve it: Developing and testing prompt engineering strategies that guide the model to avoid generating directed cycles in domains where feedback loops are not applicable would provide evidence. Experimental results showing a reduction in the occurrence of directed cycles would be conclusive.

## Limitations

- High precision (74%) but significant false positive rate suggests challenges in distinguishing direct from indirect causal relationships
- Directed cycles appear in extracted graphs despite not being expected in medical domains
- Limited generalizability beyond biomedical domain due to focus on specific dataset and GPT-4 Turbo model

## Confidence

**High Confidence**: The mechanism by which LLMs leverage pretraining to understand causal patterns is well-supported by the experimental results, particularly the high F1-score in causal orientation tasks.

**Medium Confidence**: The ability to iterate pairwise causal queries to build full causal graphs is conceptually sound but may face practical challenges in consistency and completeness.

**Low Confidence**: The generalizability of the approach beyond the biomedical domain and the robustness to different LLM architectures remain untested.

## Next Checks

1. Conduct an ablation study on prompt engineering techniques specifically targeting the reduction of false positives, particularly in distinguishing direct from indirect causal relations.

2. Apply the same LLM-based approach to a non-biomedical text corpus (e.g., news articles or legal documents) to evaluate performance consistency and identify domain-specific challenges.

3. Implement a post-processing step to detect and resolve multiply connected patterns and directed cycles in the extracted causal graphs, then compare performance with and without constraint enforcement.