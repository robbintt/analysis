---
ver: rpa2
title: Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM)
arxiv_id: '2311.08077'
source_url: https://arxiv.org/abs/2311.08077
tags:
- image
- segmentation
- prompts
- images
- bounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the application of the Segment Anything Model
  (SAM) to segment eye features in virtual reality eye-tracking images. The authors
  investigate SAM's zero-shot learning capabilities and the effectiveness of prompts
  like bounding boxes and point clicks.
---

# Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM)

## Quick Facts
- arXiv ID: 2311.08077
- Source URL: https://arxiv.org/abs/2311.08077
- Reference count: 7
- SAM achieves IoU of 93.34% for pupil segmentation using prompts in eye-tracking images

## Executive Summary
This study investigates the application of the Segment Anything Model (SAM) for zero-shot segmentation of eye features in virtual reality eye-tracking images. The authors evaluate SAM's performance using various prompting strategies including bounding boxes and point clicks to segment pupil, iris, and sclera. Results demonstrate that SAM excels at pupil segmentation (IoU 93.34%) and performs well on iris segmentation (IoU 86.63%) when using appropriate prompts, but struggles with sclera segmentation (IoU 62.19%). The findings suggest SAM has significant potential for reducing the need for specialized models and manual annotation in gaze estimation tasks, particularly for pupil and iris features.

## Method Summary
The study applies SAM to segment eye features in OpenEDS2019 and OpenEDS2020 datasets containing VR eye-tracking images. The model is evaluated in both automatic "everything" mode and manual prompting modes with strategies including point prompts, bounding boxes, and combinations of positive/negative points. Performance is measured using Intersection over Union (IoU), Dice similarity coefficient, and Hausdorff Distance metrics. The authors systematically compare different prompting strategies to determine optimal approaches for each eye feature.

## Key Results
- SAM achieves IoU of 93.34% for pupil segmentation using bounding box prompts
- Iris segmentation reaches IoU of 86.63% with combined bounding box and point prompts
- Sclera segmentation performs poorly with IoU of 62.19% regardless of prompting strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM's zero-shot learning capabilities allow it to segment eye features without task-specific training
- Mechanism: SAM combines image embeddings from a vision transformer with spatial or textual prompts to generate segmentation masks, enabling adaptation to new domains
- Core assumption: The SA-1B dataset contains sufficient visual diversity to cover eye feature appearance across different conditions
- Evidence anchors: [abstract] mentions SAM's zero-shot learning abilities; [section] describes SAM's automatic and manual modes
- Break condition: Performance degrades if eye-tracking images contain features not visually represented in SA-1B

### Mechanism 2
- Claim: Bounding box prompts significantly improve segmentation accuracy for iris and sclera compared to point-only prompts
- Mechanism: Bounding boxes provide explicit spatial constraints, reducing ambiguity in object localization compared to sparse point prompts
- Core assumption: The spatial extent of the bounding box captures the full object without excessive background noise
- Evidence anchors: [section] shows BBOX outperformed point prompts; [section] references Mattjie et al. findings
- Break condition: Performance drops if bounding boxes are too tight or too loose

### Mechanism 3
- Claim: Combining bounding box and positive/negative point prompts yields best performance for iris and sclera
- Mechanism: Positive points guide model inclusion while negative points exclude irrelevant areas, refining mask boundaries
- Core assumption: Negative point locations correctly represent true background, not occluded or ambiguous object parts
- Evidence anchors: [section] reports BBOXP strategies resulted in best performance; [section] describes negative point placement
- Break condition: Model incorrectly excludes regions if negative points are placed on partially occluded object parts

## Foundational Learning

- Concept: Zero-shot learning in computer vision
  - Why needed here: Understanding how SAM segments eye features without task-specific training is key to evaluating its applicability in gaze estimation
  - Quick check question: What is the difference between zero-shot and few-shot learning, and why is zero-shot learning particularly relevant for foundation models like SAM?

- Concept: Prompt engineering for segmentation models
  - Why needed here: Different prompting strategies significantly affect SAM's segmentation performance, and knowing how to choose and combine prompts is crucial for practical deployment
  - Quick check question: How do spatial prompts (like bounding boxes) and sparse point prompts differ in guiding a segmentation model, and in what scenarios would each be preferable?

- Concept: Evaluation metrics for image segmentation (IoU, Dice, Hausdorff Distance)
  - Why needed here: These metrics quantify overlap and boundary alignment between SAM's predicted masks and ground truth, essential for assessing segmentation quality
  - Quick check question: Why might a segmentation model have high IoU but high Hausdorff distance, and what does this tell you about the quality of the segmentation?

## Architecture Onboarding

- Component map: Input image -> Image encoder (ViT) -> Prompt encoder (points/boxes/text) -> Mask decoder -> Output mask(s)
- Critical path: Input image → Image encoder → Prompt encoder (points/boxes/text) → Mask decoder → Output mask(s)
- Design tradeoffs:
  - ViT-B vs ViT-L vs ViT-H: Larger models offer marginal improvements but increase inference time; ViT-B is often sufficient
  - Automatic mode vs manual prompting: Automatic is faster but less precise; manual yields better results but requires more input
  - Prompt types: Bounding boxes are more robust than point-only prompts for complex shapes; negative points help exclude ambiguous regions but require careful placement
- Failure signatures:
  - Low IoU but high Dice: Model captures object but misses fine details or boundaries
  - High IoU but high Hausdorff distance: Mask covers object but poorly aligns with ground truth boundary
  - Automatic mode consistently missing small or low-contrast features: Indicates need for manual prompting
- First 3 experiments:
  1. Run SAM in automatic mode on validation set and compare IoU/Dice/HD for all features; identify which need manual prompting
  2. Compare effect of bounding box prompts vs point prompts on iris and sclera segmentation accuracy
  3. Evaluate impact of positive/negative point combinations on refining mask boundaries for iris and sclera

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would SAM perform on high-resolution eye-tracking images from laboratory setups compared to VR environment performance?
- Basis in paper: [inferred] Authors note high-resolution eye tracking may demand greater precision and present more complex challenges than VR environments
- Why unresolved: Study only evaluated SAM within VR environment, leaving performance on high-resolution laboratory eye-tracking images unexplored
- What evidence would resolve it: Testing SAM on eye images from high-resolution laboratory eye trackers and comparing performance metrics to VR environment results

### Open Question 2
- Question: Can fine-tuning SAM on a small amount of eye images improve its performance compared to the standard model?
- Basis in paper: [explicit] Authors suggest fine-tuning SAM on small amount of eye images to investigate if this improves results
- Why unresolved: Study focused on zero-shot learning capabilities without addressing potential benefits of fine-tuning
- What evidence would resolve it: Experiments comparing performance of fine-tuned SAM on eye images to standard, non-fine-tuned SAM model

### Open Question 3
- Question: Would a foundation model specifically trained on eye images outperform SAM in eye image segmentation tasks?
- Basis in paper: [inferred] Authors speculate that bespoke foundation model trained on eye images could be pivotal asset for eye-tracking community
- Why unresolved: Study only evaluated SAM without exploring potential of specialized foundation model trained on eye images
- What evidence would resolve it: Developing and training foundation model on comprehensive eye image dataset and comparing performance to SAM

## Limitations

- SAM struggles with sclera segmentation (IoU 62.19%) due to challenges with shadows from eyelids and non-uniform illumination
- Evaluation limited to two datasets captured with specific VR HMDs, raising questions about generalizability to different eye-tracking systems
- Optimal spatial configuration and perturbation strategies for bounding boxes remain underspecified

## Confidence

- High confidence: SAM's excellent performance on pupil segmentation (IoU 93.34%) is well-supported and consistent with strong zero-shot capabilities
- Medium confidence: Superiority of bounding box prompts over point-only prompts is supported by experimental results but lacks extensive comparative validation
- Low confidence: Generalizability of findings to different VR headsets, lighting conditions, or real-world scenarios remains uncertain

## Next Checks

1. Test SAM's zero-shot segmentation performance on eye-tracking images from different VR headsets and real-world conditions to assess generalizability beyond OpenEDS datasets
2. Conduct ablation studies comparing various bounding box perturbation strategies (5%, 10%, 20%) and negative point placement strategies to optimize sclera segmentation performance
3. Evaluate SAM's performance on eye images with extreme variations in lighting, pupil dilation, and occlusions (eyelids, glasses) to identify failure modes and robustness limits