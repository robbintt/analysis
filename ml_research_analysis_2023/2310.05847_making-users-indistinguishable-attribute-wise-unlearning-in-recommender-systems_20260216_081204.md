---
ver: rpa2
title: 'Making Users Indistinguishable: Attribute-wise Unlearning in Recommender Systems'
arxiv_id: '2310.05847'
source_url: https://arxiv.org/abs/2310.05847
tags:
- uni00000013
- unlearning
- embedding
- loss
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a post-training attribute unlearning (PoT-AU)
  method to protect users' sensitive information in recommender systems. The method
  designs a two-component loss function that includes a distinguishability loss to
  make attributes indistinguishable and a regularization loss to maintain recommendation
  performance.
---

# Making Users Indistinguishable: Attribute-wise Unlearning in Recommender Systems

## Quick Facts
- arXiv ID: 2310.05847
- Source URL: https://arxiv.org/abs/2310.05847
- Reference count: 40
- Key outcome: D2D outperforms U2U in both unlearning and recommendation tasks, with an average 24.41% decrease in AUC and 6.28% increase in NDCG.

## Executive Summary
This paper proposes a post-training attribute unlearning (PoT-AU) method to protect users' sensitive information in recommender systems. The method uses a two-component loss function that makes attribute labels indistinguishable while maintaining recommendation performance through regularization. Two distinguishability losses are proposed: user-to-user (U2U) and distribution-to-distribution (D2D), with D2D showing superior performance in both unlearning effectiveness and recommendation quality.

## Method Summary
PoT-AU manipulates user embeddings after training completion to degrade attribute inference performance while preserving recommendation quality. The method optimizes a two-component loss function combining a distinguishability loss (U2U or D2D) with a regularization loss. U2U minimizes distance between individual user pairs with different attribute labels, while D2D uses Maximum Mean Discrepancy (MMD) to reduce distance between attribute class distributions. The optimization uses SGD to find embeddings that balance these competing objectives, making attributes indistinguishable in the embedding space.

## Key Results
- D2D loss reduces attacker AUC by 24.41% on average while improving NDCG by 6.28% compared to U2U
- D2D preserves embedding distribution shapes while reducing inter-class distance, avoiding the collapse seen with U2U
- PoT-AU successfully degrades attribute inference performance without significantly impacting recommendation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Making attribute labels indistinguishable degrades attacking performance by reducing separability in user embedding space
- Mechanism: Two-component loss function optimizes user embeddings to minimize distinguishability between attribute classes while maintaining recommendation performance through regularization
- Core assumption: User embeddings contain sufficient attribute information for attackers to infer attributes, and manipulating these embeddings can obscure this information without breaking recommendation functionality
- Evidence anchors:
  - [abstract] "To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to degrade attacking performance and make target attributes indistinguishable"
  - [section] "The core difficulty of designing a proper two-component loss function lies in defining distinguishability loss â„“ð‘¢. It is related to the primary goal of PoT-AU, i.e.,Goal #1"
- Break condition: If the regularization component becomes too strong relative to distinguishability loss, recommendation performance degrades significantly while unlearning effectiveness diminishes

### Mechanism 2
- Claim: Distribution-to-distribution loss outperforms user-to-user loss because it preserves embedding distribution shapes while reducing inter-class distance
- Mechanism: MMD (Maximum Mean Discrepancy) measures and minimizes distance between gender distributions without collapsing embeddings to mean values
- Core assumption: Embedding distributions for different attribute classes have similar shapes but different locations, and reducing distance between distributions obscures class separability more effectively than forcing individual points together
- Evidence anchors:
  - [section] "D2D loss is effective in enlarging the overlapping area of two gender distributions, which means it narrows the distance between two distributions"
  - [section] "This brings superior performance in both unlearning and recommendation"
- Break condition: If distributions become too overlapped, attacker models may learn to identify subtle remaining patterns or the regularization loss may dominate and restore separability

### Mechanism 3
- Claim: Post-training setting is more practical than in-training because it avoids retraining overhead and works with deployed models
- Mechanism: Fine-tuning existing user embeddings after training completion using only user embedding and attribute labels, without requiring access to training data or gradients
- Core assumption: User embeddings capture sufficient attribute information that can be manipulated post-hoc, and small changes to embeddings can significantly impact attribute inference while preserving recommendation functionality
- Evidence anchors:
  - [abstract] "This setting is more practical, because of i) data accessibility: we may not get access to the training data or other information after training due to regulations"
  - [section] "Compared with in-training setting (InT-AU), the post-training setting (PoT-AU) is more challenging... PoT-AU allows no interference with the training process"
- Break condition: If the model's recommendation performance is too sensitive to embedding changes, or if attribute information is encoded too deeply to be effectively removed post-training

## Foundational Learning

- Concept: Attribute Inference Attack (AIA)
  - Why needed here: Understanding how attackers extract sensitive information from user embeddings is fundamental to designing effective defenses
  - Quick check question: What three stages comprise an attribute inference attack in the context of recommender systems?

- Concept: Distribution-to-distribution distance measures (MMD, KL divergence)
  - Why needed here: D2D loss relies on measuring and minimizing distance between attribute class distributions in embedding space
  - Quick check question: How does MMD differ from simple Euclidean distance when measuring distribution separation?

- Concept: Stochastic Gradient Descent optimization
  - Why needed here: The two-component loss function is optimized using SGD to find embeddings that balance unlearning and recommendation goals
  - Quick check question: What role does the learning rate play in balancing the two components of the loss function during optimization?

## Architecture Onboarding

- Component map:
  - User embedding matrix (ð‘ Ã—ð¾) -> Two-component loss function (Distinguishability loss + Regularization loss) -> SGD optimizer -> Updated user embeddings

- Critical path:
  1. Load pre-trained recommendation model and user embeddings
  2. Initialize distinguishability and regularization losses
  3. Optimize user embeddings using SGD for specified epochs
  4. Evaluate unlearning effectiveness against attacker models
  5. Evaluate recommendation performance on held-out data

- Design tradeoffs:
  - U2U vs D2D: U2U is computationally simpler but can collapse distributions; D2D preserves distribution shapes but requires more computation
  - Regularization strength: Higher values preserve recommendation performance but reduce unlearning effectiveness
  - Epoch count: More epochs improve optimization but increase computational cost

- Failure signatures:
  - Unlearning fails: Attacker accuracy remains high (>0.7) after optimization
  - Recommendation fails: NDCG or HR drops >10% compared to original
  - Overfitting: Unlearning works on training shadow set but fails on test set
  - Numerical instability: NaN values appear during optimization

- First 3 experiments:
  1. Run U2U-R with default parameters on ML-100K dataset and verify that attacker AUC drops below 0.55
  2. Run D2D-R with default parameters on same dataset and confirm improvement over U2U-R in both unlearning and recommendation
  3. Vary the trade-off parameter Î± from 1e-6 to 1e-2 and plot the Pareto frontier between unlearning (AUC) and recommendation (NDCG@10) performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method generalize to multi-class attribute unlearning beyond binary gender classification?
- Basis in paper: [explicit] The paper states "In this work, we focus on the attribute with binary labels. In the future, we will study the multiple-labels case and further exploit different implementations of the current U2U design."
- Why unresolved: The current implementation uses binary attribute labels and simple inverse adjacency matrix for attribute divergence. Multi-class scenarios require more sophisticated handling of attribute relationships.
- What evidence would resolve it: Empirical results comparing the proposed method's performance on multi-class attribute unlearning tasks (e.g., age groups, occupation categories) versus baseline methods, along with theoretical analysis of how the loss functions scale to multiple classes.

### Open Question 2
- Question: What is the optimal trade-off parameter Î± that balances unlearning effectiveness and recommendation performance across different datasets and recommendation models?
- Basis in paper: [explicit] The paper discusses the effect of trade-off parameter Î± in Figure 2, noting that "Reducing the value of Î± results in insignificant performance change for D2D-R, i.e., enhances unlearning performance and decreases recommendation performance."
- Why unresolved: The paper shows that Î± affects performance but doesn't identify an optimal value that works across different scenarios. The sensitivity to Î± varies between methods (U2U-R vs D2D-R).
- What evidence would resolve it: A systematic study varying Î± across multiple datasets and recommendation models to identify patterns in optimal Î± values, along with theoretical justification for why certain ranges work better.

### Open Question 3
- Question: How does the proposed method perform under different attack model assumptions, such as white-box attacks or attacks with partial knowledge of the unlearning process?
- Basis in paper: [inferred] The paper assumes "grey-box attack" where attackers have access to user embeddings and attribute information but not the full model. This assumption could be relaxed to test robustness.
- Why unresolved: The evaluation only considers grey-box attacks with MLP and XGB classifiers. Real-world attackers might have more sophisticated models or more information about the system.
- What evidence would resolve it: Experiments testing the method against white-box attacks where attackers have full model information, as well as against more sophisticated attack models like neural networks with attention mechanisms or transfer learning approaches.

## Limitations

- The method's effectiveness is currently limited to binary attribute classification and may not generalize to multi-class scenarios
- The assumption that recommendation performance can be preserved while removing attribute information may not hold for all model architectures
- The paper doesn't test robustness against adaptive attackers who might learn to circumvent distribution-based obfuscation

## Confidence

- High confidence: The experimental methodology and results showing D2D superiority over U2U are well-documented and reproducible
- Medium confidence: The theoretical claims about distribution overlap and embedding space manipulation are supported by visualizations but lack formal proofs
- Medium confidence: The post-training practicality claims are reasonable but would benefit from more extensive deployment scenarios

## Next Checks

1. Test PoT-AU against adaptive attackers that incorporate knowledge of the unlearning mechanism, measuring whether attackers can learn to identify subtle patterns that survive the D2D obfuscation
2. Evaluate PoT-AU across different attribute types (age, location, interests) to determine if the distribution overlap principle generalizes beyond binary gender attributes
3. Conduct ablation studies varying the regularization strength and embedding dimensionality to map the full Pareto frontier between privacy protection and recommendation utility