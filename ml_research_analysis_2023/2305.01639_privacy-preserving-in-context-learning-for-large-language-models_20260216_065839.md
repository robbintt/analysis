---
ver: rpa2
title: Privacy-Preserving In-Context Learning for Large Language Models
arxiv_id: '2305.01639'
source_url: https://arxiv.org/abs/2305.01639
tags:
- private
- privacy
- data
- learning
- differentially
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DP-ICL, a framework for differentially private
  in-context learning with large language models (LLMs). The key idea is to generate
  private responses by establishing a noisy consensus among an ensemble of LLM responses
  based on disjoint exemplar sets, using the Report-Noisy-Max mechanism.
---

# Privacy-Preserving In-Context Learning for Large Language Models

## Quick Facts
- arXiv ID: 2305.01639
- Source URL: https://arxiv.org/abs/2305.01639
- Reference count: 11
- Outperforms state-of-the-art differentially private fine-tuning by 20% relative error reduction on SST-2

## Executive Summary
This paper introduces DP-ICL, a framework for performing in-context learning with large language models (LLMs) while providing strong differential privacy guarantees. The key innovation is using the Report-Noisy-Max mechanism to generate private responses by establishing a noisy consensus among an ensemble of LLM responses based on disjoint exemplar sets. DP-ICL achieves performance comparable to non-private ICL (<2% degradation) while providing provable privacy guarantees (ε=1-8). On SST-2, it outperforms state-of-the-art differentially private fine-tuning methods by over 20% relative error rate reduction at ε=3. The framework requires no hyperparameter tuning, is compatible with black-box LLM APIs, and enables flexible data editing.

## Method Summary
DP-ICL partitions the private dataset into disjoint subsets, queries the LLM on each exemplar-query pair, aggregates predictions into a histogram, adds Gaussian noise, and reports the top class using the Report-Noisy-Max mechanism with Gaussian noise (RNM-Gaussian). The framework uses Poisson subsampling to amplify privacy guarantees and tracks privacy loss under composition using Privacy Loss Random Variables (PRVs). Evaluation is performed on four text classification benchmarks (SST-2, Amazon, AGNews, TREC) and two language generation tasks, treating the training set as private data.

## Key Results
- Achieves performance comparable to non-private ICL (<2% degradation) while providing strong privacy guarantees (ε=1-8)
- Outperforms state-of-the-art differentially private fine-tuning methods by over 20% relative error rate reduction on SST-2 at ε=3
- Requires no hyperparameter tuning and is compatible with black-box LLM APIs
- Enables flexible data editing while maintaining privacy guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential privacy is preserved under the composition of multiple queries due to the post-processing property and composition theorems.
- Mechanism: The DP-ICL framework applies the Report-Noisy-Max mechanism to aggregate LLM responses from disjoint exemplar sets. Each query's privacy cost is tracked using Privacy Loss Random Variables (PRVs), which add up under composition. The post-processing property ensures that any further processing of the DP output remains differentially private.
- Core assumption: The privacy budget $\epsilon$ and $\delta$ parameters accurately capture the privacy loss under adaptive composition of queries.

### Mechanism 2
- Claim: Subsampling the private dataset amplifies privacy guarantees while maintaining utility.
- Mechanism: Before partitioning the private dataset into disjoint subsets, DP-ICL performs Poisson subsampling with probability $q$. This reduces the effective size of the dataset seen by each LLM query, lowering the global sensitivity and thus the required noise scale for differential privacy.
- Core assumption: The subsampling rate $q$ is chosen such that the privacy amplification effect outweighs the potential loss in utility from having fewer exemplars.

### Mechanism 3
- Claim: The ensemble of LLM responses from disjoint exemplar sets provides robustness and improves accuracy compared to using a single exemplar set.
- Mechanism: DP-ICL partitions the subsampled private dataset into $N$ disjoint subsets, queries the LLM on each exemplar-query pair, and aggregates the predictions into a histogram. The Report-Noisy-Max mechanism then selects the class with the highest (noisy) vote. This ensemble approach leverages the wisdom of crowds to improve prediction accuracy.
- Core assumption: The disjoint exemplar sets are sufficiently diverse and representative of the overall private dataset.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP is the core privacy guarantee that DP-ICL aims to provide. Understanding DP is crucial for comprehending how the framework ensures that the private data is not leaked through the LLM responses.
  - Quick check question: What is the main idea behind differential privacy, and how does it protect individual privacy in the context of DP-ICL?

- Concept: Privacy Loss Random Variables (PRVs)
  - Why needed here: PRVs are used to track the privacy cost under composition of multiple queries. Understanding PRVs is essential for grasping how DP-ICL accounts for the cumulative privacy loss across all user queries.
  - Quick check question: How do Privacy Loss Random Variables (PRVs) help in tracking the privacy cost under composition, and what is their role in DP-ICL?

- Concept: Subsampling
  - Why needed here: Subsampling is a key technique used in DP-ICL to amplify privacy guarantees while maintaining utility. Understanding subsampling is important for comprehending how DP-ICL balances privacy and performance.
  - Quick check question: How does subsampling the private dataset help in amplifying privacy guarantees in DP-ICL, and what are the potential trade-offs?

## Architecture Onboarding

- Component map: Private dataset -> Subsampling module -> Partitioning module -> LLM API -> Aggregation module -> Noise addition module -> Report-Noisy-Max module -> Final output

- Critical path:
  1. Receive a user query
  2. Subsampling module selects a subset of the private dataset
  3. Partitioning module divides the subsampled dataset into disjoint exemplar sets
  4. LLM API generates responses for each exemplar-query pair
  5. Aggregation module collects and aggregates the responses into a histogram
  6. Noise addition module adds Gaussian noise to the histogram counts
  7. Report-Noisy-Max module selects the class with the highest (noisy) vote as the final output
  8. Return the final output to the user

- Design tradeoffs:
  - Privacy vs. utility: Increasing the privacy budget $\epsilon$ improves utility but reduces privacy guarantees
  - Subsampling rate vs. performance: A higher subsampling rate improves performance but reduces privacy amplification
  - Number of exemplar sets vs. computation: Using more disjoint exemplar sets improves robustness but increases computation

- Failure signatures:
  - High privacy loss: If the actual privacy loss exceeds the theoretical bounds
  - Poor performance: If performance is significantly worse than expected
  - Incorrect outputs: If outputs are consistently incorrect

- First 3 experiments:
  1. Test DP-ICL on a small private dataset with a single query to verify basic functionality
  2. Vary the subsampling rate and privacy budget $\epsilon$ to understand their impact
  3. Test the framework with multiple queries to verify composition of privacy costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DP-ICL perform on text generation tasks compared to classification tasks?
- Basis in paper: The authors state "DP-ICL is limited to classiﬁcation tasks and requires more computation than non-private ICL" and suggest future work to extend it to text generation.
- Why unresolved: The current evaluation focuses only on classification benchmarks. Text generation involves different challenges like handling variable-length outputs and maintaining coherence.
- What evidence would resolve it: Experimental results comparing DP-ICL's performance on text generation tasks (e.g., summarization, translation) against non-private ICL and existing private text generation methods.

### Open Question 2
- Question: What is the maximum number of queries DP-ICL can handle before privacy degradation becomes significant?
- Basis in paper: The authors mention "DP-ICL cannot answer an inﬁnite number of queries from an attacker" and discuss privacy composition for up to 10,000 queries.
- Why unresolved: The experiments only evaluate up to 10,000 queries. The relationship between query volume and privacy degradation at larger scales is unknown.
- What evidence would resolve it: Empirical analysis showing DP-ICL's accuracy and privacy guarantees across varying query volumes (e.g., 10^4 to 10^6 queries) on different datasets.

### Open Question 3
- Question: How does DP-ICL's performance compare when using more advanced LLMs like GPT-4?
- Basis in paper: The authors note "all our results can be massively improved by simply replacing the GPT-3 Babbage API call with more advanced LLMs such as GPT-4" but use GPT-3 for cost reasons.
- Why unresolved: The evaluation uses GPT-3 Babbage, which may be suboptimal compared to state-of-the-art models. The potential performance gains from better models are speculative.
- What evidence would resolve it: Direct comparison of DP-ICL results using GPT-3, GPT-4, and other advanced LLMs on the same tasks, showing accuracy and privacy tradeoffs.

## Limitations

- Performance on text generation tasks remains unverified, as the framework is currently limited to classification tasks
- The relationship between query volume and privacy degradation at large scales is unknown
- The framework's compatibility with black-box LLM APIs without any fine-tuning needs more empirical support

## Confidence

- **High Confidence**: The core mechanism of using disjoint exemplar sets with the Report-Noisy-Max mechanism for DP-ICL is well-established in differential privacy literature
- **Medium Confidence**: The claim of <2% performance degradation compared to non-private ICL is based on results from four datasets
- **Low Confidence**: The assertion that the framework is fully compatible with black-box LLM APIs without any fine-tuning or adaptation needs more empirical support

## Next Checks

1. **Privacy Budget Verification**: Implement the RNM-Gaussian mechanism with various noise scales and systematically measure the actual ε achieved versus the theoretical bound, using Rényi Differential Privacy (RDP) accounting to verify composition

2. **Subsampling Sensitivity Analysis**: Conduct controlled experiments varying the subsampling rate q across multiple orders of magnitude to quantify its impact on both privacy amplification and task performance, identifying the optimal tradeoff point

3. **Cross-Domain Generalization Test**: Evaluate DP-ICL on at least two additional datasets from different domains (e.g., biomedical text, legal documents) and on a generation task (e.g., summarization) to assess the framework's robustness beyond the reported classification benchmarks