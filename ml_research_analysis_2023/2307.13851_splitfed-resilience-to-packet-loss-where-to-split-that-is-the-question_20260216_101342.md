---
ver: rpa2
title: 'SplitFed resilience to packet loss: Where to split, that is the question'
arxiv_id: '2307.13851'
source_url: https://arxiv.org/abs/2307.13851
tags:
- loss
- split
- packet
- learning
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of model split points on the loss
  resilience of SplitFed Learning (SFL). Experiments with a U-Net model for human
  embryo image segmentation show that a deeper split point produces significantly
  better results than a shallower split under various packet loss rates and numbers
  of clients experiencing packet loss.
---

# SplitFed resilience to packet loss: Where to split, that is the question

## Quick Facts
- arXiv ID: 2307.13851
- Source URL: https://arxiv.org/abs/2307.13851
- Reference count: 25
- Primary result: Deeper split points in SplitFed Learning significantly improve packet loss resilience compared to shallow splits

## Executive Summary
This study investigates how the choice of model split point affects SplitFed Learning (SFL) performance under packet loss conditions. Using a U-Net model for human embryo image segmentation, experiments compare shallow and deep split configurations across various packet loss rates and numbers of affected clients. The deep-split model consistently outperforms the shallow-split model, achieving higher mean Jaccard index values and demonstrating greater robustness to high packet loss rates. These results highlight the critical importance of split point selection in SFL system design for lossy network environments.

## Method Summary
The study evaluates two U-Net split configurations - shallow split (after first convolution layer) and deep split (after second convolution and first max-pooling) - under simulated packet loss conditions. Experiments use 781 human embryo images distributed across 5 clients with varying dataset sizes. The models are trained for 12 local and 15 global epochs using Adam optimizer with learning rate 1e-4. Five parameter aggregation methods are tested: naive averaging, FedAvg, auto-FedAvg, fed-NCL V2, and fed-NCL V4. Packet loss is simulated by randomly zeroing out rows in feature maps and gradient maps at split points, with rates ranging from 0.1 to 0.9 and varying numbers of affected clients.

## Key Results
- Deep-split models achieve significantly higher mean Jaccard index values than shallow-split models across all tested packet loss rates
- Deep splits show greater robustness to high packet loss rates (0.7-0.9), maintaining reasonable performance while shallow splits degrade sharply
- Statistical significance tests (p<0.05) confirm deep split consistently outperforms shallow split
- Shallow splits perform near-zero at 90% packet loss regardless of client count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deeper split points improve loss resilience because the client-side back-end has more layers to learn data recovery from missing packets.
- Mechanism: When packets are lost at a deeper split point, the client-side back-end can use more layers to reconstruct or compensate for missing data in feature maps before computing gradients.
- Core assumption: The client-side back-end layers after the split point have sufficient capacity to learn how to handle missing data and maintain performance.
- Evidence anchors:
  - [section] "The reason for this is twofold: (1) the deep-split model has more layers available in the client-side back-end to learn how to recover the lost data..."
  - [corpus] No direct evidence found in corpus about layer capacity for missing data recovery; this is based on paper claims.

### Mechanism 2
- Claim: Deeper splits position skip connections entirely on the client side, avoiding packet loss at those connections.
- Mechanism: In the deep-split U-Net, the first skip connection is fully located on the client side, allowing some feature maps to transfer without packet loss, preserving information that would otherwise be lost.
- Core assumption: Skip connections contain important information for model performance, and keeping them on the client side protects them from packet loss.
- Evidence anchors:
  - [section] "...(2) in our deep-split U-Net model, the first skip connection was fully located at the client and was able to transfer some features without packet loss."
  - [corpus] No corpus evidence found about skip connection placement and packet loss; based solely on paper description.

### Mechanism 3
- Claim: ReLU activations provide inherent robustness to packet loss because they produce many zero outputs, making missing data less disruptive.
- Mechanism: ReLU activations output zero for negative inputs, so when missing data is replaced with zero, it often matches the expected output, minimizing the impact on learning.
- Core assumption: The distribution of inputs to ReLU layers includes many negative values, making zero substitution a reasonable approximation for missing data.
- Evidence anchors:
  - [section] "It was reported in [12] that models with ReLU activations tend to be fairly robust to packet loss, because ReLU activations produce a lot of zeros in their output."
  - [corpus] No corpus evidence found about ReLU and packet loss; this is cited from external reference [12].

## Foundational Learning

- Concept: Federated Learning basics
  - Why needed here: Understanding how FL distributes model training across clients while keeping data local is essential for grasping why SFL was developed.
  - Quick check question: In standard FL, where are the local models trained and where is aggregation performed?

- Concept: Split Learning architecture
  - Why needed here: Knowing how models are divided between client and server in SL explains the significance of split points and packet loss locations.
  - Quick check question: In SL, which part of the model typically resides on the client device and which on the server?

- Concept: Packet loss in distributed learning
  - Why needed here: Understanding how packet loss affects feature maps and gradient maps during forward and backward passes is crucial for interpreting the results.
  - Quick check question: In the context of split learning, at what points does packet loss occur during training?

## Architecture Onboarding

- Component map:
  - U-Net model with four downsampling blocks, bottleneck, and four upsampling blocks
  - Two split configurations: shallow split (after first conv layer) and deep split (after second conv and first max-pooling)
  - Five parameter aggregation algorithms: naive averaging, FedAvg, auto-FedAvg, fed-NCL V2, fed-NCL V4
  - Client devices with varying data distributions and computational capabilities
  - Server coordinating aggregation and managing model copies

- Critical path:
  1. Initialize model copies on clients and server
  2. Clients train local models for specified epochs
  3. Clients send updated models to server
  4. Server aggregates parameters using selected algorithm
  5. Server sends aggregated model back to clients
  6. Clients validate and continue training

- Design tradeoffs:
  - Deeper split increases client computational load but improves packet loss resilience
  - Shallower split reduces client burden but is more vulnerable to packet loss
  - Choice of aggregation algorithm affects convergence speed and final accuracy
  - Model architecture (e.g., ReLU activations) impacts inherent robustness to packet loss

- Failure signatures:
  - Sharp drop in MJI when packet loss rate exceeds 70% for shallow splits
  - Near-zero performance for shallow splits at 90% packet loss regardless of client count
  - Gradual performance degradation as more clients experience packet loss
  - Statistical significance tests showing deep split consistently outperforms shallow split

- First 3 experiments:
  1. Test baseline U-Net performance without splits on centralized dataset to verify model quality
  2. Implement shallow split configuration and measure MJI under varying packet loss rates (0.1 to 0.9) with different numbers of affected clients
  3. Implement deep split configuration and repeat the same packet loss experiments for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of activation function (e.g., ReLU vs. other functions) impact SFL performance under packet loss?
- Basis in paper: [explicit] The paper notes that ReLU activations produce many zeros, making packet loss less impactful since missing values are often replaced with zeros anyway.
- Why unresolved: The study only tested ReLU; other activation functions may react differently to packet loss.
- What evidence would resolve it: Experiments comparing SFL performance under packet loss using different activation functions (ReLU, sigmoid, tanh, etc.) and analyzing the impact on model accuracy.

### Open Question 2
- Question: Can packet loss in SFL act as a regularization technique similar to dropout for higher loss rates beyond 50%?
- Basis in paper: [inferred] The paper suggests packet loss acts like dropout for low to moderate loss rates but shows lower MJI for higher rates compared to dropout.
- Why unresolved: The study only compared packet loss to dropout up to 50% loss rate; higher rates were not thoroughly explored.
- What evidence would resolve it: Experiments comparing SFL performance under high packet loss rates (e.g., 60-90%) with matching dropout rates, measuring the impact on model accuracy and convergence.

### Open Question 3
- Question: How does the optimal split point in SFL vary with different model architectures and tasks?
- Basis in paper: [explicit] The paper found that a deeper split point performed better for U-Net on embryo image segmentation, but this may not generalize to all models and tasks.
- Why unresolved: The study only tested two split points (shallow and deep) on a single model (U-Net) for one task (embryo image segmentation).
- What evidence would resolve it: Experiments testing multiple split points across various model architectures (e.g., ResNet, VGG) and tasks (e.g., object detection, natural language processing) to identify patterns in optimal split points.

## Limitations

- Results are based on a single model architecture (U-Net) and specific medical imaging task, limiting generalizability
- The study does not explore the computational resource implications of deep splits on constrained devices
- Mechanisms explaining why deep splits perform better are theoretical interpretations without direct experimental validation

## Confidence

**High Confidence**: The comparative performance results between shallow and deep splits under packet loss conditions are well-supported by statistical testing (t-tests with p<0.05 significance level) across multiple packet loss rates and client counts.

**Medium Confidence**: The mechanistic explanations for why deep splits perform better (layer capacity for recovery, skip connection protection, ReLU robustness) are plausible based on the architecture description but lack direct experimental validation.

**Low Confidence**: The applicability of these findings to other model architectures, activation functions, or application domains remains untested.

## Next Checks

1. **Architectural Generalization Test**: Implement the same packet loss experiments with different model architectures (e.g., ResNet, VGG) and activation functions (e.g., sigmoid, tanh) to verify whether the deep split advantage holds across architectures.

2. **Resource-Constrained Scenario Test**: Conduct experiments comparing shallow and deep splits under realistic device constraints, measuring both performance and computational overhead to determine practical deployment thresholds.

3. **Skip Connection Sensitivity Analysis**: Systematically vary the position of the split point while keeping the number of client-side layers constant to isolate the impact of skip connection placement from other factors affecting packet loss resilience.