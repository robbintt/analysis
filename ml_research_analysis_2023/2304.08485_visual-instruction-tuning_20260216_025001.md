---
ver: rpa2
title: Visual Instruction Tuning
arxiv_id: '2304.08485'
source_url: https://arxiv.org/abs/2304.08485
tags:
- image
- visual
- arxiv
- gpt-4
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first attempt to use language-only GPT-4
  to generate multimodal language-image instruction-following data, and instruction
  tune a large multimodal model (LLaVA) that connects a vision encoder and LLM for
  general-purpose visual and language understanding. LLaVA demonstrates impressive
  multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4
  on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4
  on a synthetic multimodal instruction-following dataset.
---

# Visual Instruction Tuning

## Quick Facts
- arXiv ID: 2304.08485
- Source URL: https://arxiv.org/abs/2304.08485
- Authors: Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, and others
- Reference count: 40
- Primary result: LLaVA achieves 85.1% relative score compared to GPT-4 on synthetic multimodal instruction-following data and sets state-of-the-art 92.53% accuracy on Science QA

## Executive Summary
This paper introduces LLaVA (Large Language and Vision Assistant), the first model to leverage language-only GPT-4 to generate multimodal instruction-following data for training a vision-language model. By connecting CLIP's vision encoder with LLaMA's language model through a simple projection matrix, LLaVA demonstrates impressive multimodal chat abilities, achieving 85.1% relative performance compared to GPT-4 on synthetic instruction-following tasks. The model shows strong zero-shot capabilities on unseen images and instructions, and when fine-tuned on Science QA, achieves a new state-of-the-art accuracy of 92.53%.

## Method Summary
LLaVA employs a two-stage training approach to connect CLIP's ViT-L/14 vision encoder with LLaMA's language model. In Stage 1, the model learns to align visual features to language embedding space by training a projection matrix while freezing both the vision encoder and LLM. In Stage 2, the full model undergoes end-to-end fine-tuning on GPT-4-generated instruction-following data, preserving knowledge while learning to follow instructions. The training data includes 158K samples across three types: conversation, detailed description, and complex reasoning, created by converting image-text pairs into instruction-following format using GPT-4.

## Key Results
- Achieves 85.1% relative score compared to GPT-4 on a synthetic multimodal instruction-following dataset
- Sets new state-of-the-art accuracy of 92.53% on Science QA when fine-tuned
- Demonstrates strong zero-shot performance on unseen images and instructions, sometimes matching multimodal GPT-4 behavior
- Shows improved performance across multiple tasks including linear probing, linear probing with CoT, and visual conversation benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 generated instruction-following data effectively teaches multimodal models to understand and respond to visual queries
- Mechanism: By converting image-text pairs into instruction-following format using GPT-4, the model learns to connect visual features with language instructions and responses through end-to-end fine-tuning
- Core assumption: GPT-4 can generate high-quality instruction-following data that captures the relationship between visual content and language queries
- Evidence anchors:
  - [abstract] "This paper presents the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data"
  - [section] "To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both accept only text as input), to create instruction-following data involving visual content"

### Mechanism 2
- Claim: Two-stage training effectively aligns visual features with LLM embeddings while preserving pre-trained knowledge
- Mechanism: Stage 1 freezes both vision encoder and LLM to align visual features to language embedding space through projection matrix training, Stage 2 fine-tunes projection and LLM weights to preserve knowledge while learning instruction-following
- Core assumption: The projection matrix can effectively bridge the visual and language embedding spaces
- Evidence anchors:
  - [section] "We apply a trainable projection matrix W to convert Zv into language embedding tokens Hv" and "We consider a two-stage instruction-tuning procedure"
  - [section] "In this way, the image features Hv can be aligned with the pre-trained LLM word embedding"

### Mechanism 3
- Claim: Combining multiple response types (conversation, detailed description, complex reasoning) creates more capable multimodal models
- Mechanism: Different response types require different reasoning capabilities - conversation for interactive Q&A, detailed description for comprehensive understanding, complex reasoning for multi-step logical deduction
- Core assumption: Diverse instruction types improve generalization across different visual reasoning tasks
- Evidence anchors:
  - [section] "We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning"
  - [section] "adding a small amount of the detailed description and complex reasoning questions contributes to a considerable improvement"

## Foundational Learning

- Concept: Multimodal feature alignment
  - Why needed here: The vision encoder and LLM operate in different embedding spaces that need to be bridged for effective multimodal understanding
  - Quick check question: What role does the projection matrix W play in connecting visual features to language embeddings?

- Concept: Instruction-following data format
  - Why needed here: The model needs to understand how to process visual inputs and generate appropriate responses based on instructions
  - Quick check question: How does the input sequence format in Table 2 help the model learn instruction-following behavior?

- Concept: Auto-regressive training objective
  - Why needed here: The model must predict sequences of tokens that form complete responses to visual instructions
  - Quick check question: What is the mathematical formulation for computing the probability of generating target answers in equation (3)?

## Architecture Onboarding

- Component map:
  CLIP vision encoder (ViT-L/14) → projection matrix W → LLaMA LLM → response generation

- Critical path:
  Image → CLIP encoder → projection matrix → LLM → response generation
  Training data → GPT-4 generation → two-stage fine-tuning → deployed model

- Design tradeoffs:
  - Simple projection vs. complex cross-attention mechanisms (faster training vs. potentially better performance)
  - Freezing vs. fine-tuning vision encoder (preserving pre-trained knowledge vs. adapting to instruction-following)
  - Different response types (improved capability vs. increased training complexity)

- Failure signatures:
  - Poor visual understanding → responses don't match image content
  - Weak instruction-following → responses don't address the specific question asked
  - Training instability → learning rate issues or projection matrix misalignment

- First 3 experiments:
  1. Test projection matrix alignment by feeding known images and checking embedding consistency
  2. Validate instruction-following by testing on held-out GPT-4 generated examples
  3. Compare performance with different response type combinations on a validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section. The main open areas mentioned include exploring more sophisticated architectures to connect image and language representations, and potentially scaling the model size and dataset further.

## Limitations
- Reliance on GPT-4-generated data may introduce distribution shifts between synthetic training examples and real-world scenarios
- Evaluation primarily uses GPT-4 scoring, which may not fully capture model limitations or biases
- Two-stage training approach requires careful hyperparameter tuning and may not generalize to all multimodal architectures

## Confidence
- **High Confidence**: The core mechanism of using GPT-4 to generate instruction-following data is well-supported by experimental results showing 85.1% relative score compared to GPT-4 on synthetic datasets
- **Medium Confidence**: The two-stage training approach and projection matrix alignment show promising results but could benefit from additional ablation studies on different architectures
- **Medium Confidence**: Claims about combining multiple response types improving capability are supported by performance improvements but lack extensive comparative analysis with alternative approaches

## Next Checks
1. Test the model's robustness to distribution shifts by evaluating on human-generated instruction-following data versus the GPT-4-generated training set
2. Conduct ablation studies comparing the two-stage training approach with end-to-end fine-tuning from scratch on various multimodal architectures
3. Evaluate the model's performance on specialized domains (medical imaging, satellite imagery, etc.) to assess generalization beyond general visual understanding