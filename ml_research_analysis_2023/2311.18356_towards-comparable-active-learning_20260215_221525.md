---
ver: rpa2
title: Towards Comparable Active Learning
arxiv_id: '2311.18356'
source_url: https://arxiv.org/abs/2311.18356
tags:
- algorithms
- learning
- oracle
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive evaluation framework for comparing
  active learning (AL) algorithms across three major domains: tabular, image, and
  text data. The authors address key challenges in AL research including poor generalization
  across domains, high variance in results, and lack of fair comparisons.'
---

# Towards Comparable Active Learning

## Quick Facts
- arXiv ID: 2311.18356
- Source URL: https://arxiv.org/abs/2311.18356
- Authors: 
- Reference count: 40
- Key outcome: Presents a comprehensive benchmark suite for comparing active learning algorithms across tabular, image, and text domains

## Executive Summary
This paper introduces a standardized evaluation framework for comparing active learning algorithms across three major domains: tabular, image, and text data. The authors address key challenges in AL research including poor generalization across domains, high variance in results, and lack of fair comparisons. The benchmark suite includes seven real-world and two synthetic datasets, six widely-used AL algorithms, and an efficient greedy oracle algorithm. Experiments are conducted with rigorous reproducibility measures including fixed seeds, controlled training protocols, and 50 restarts per experiment to reduce variance. Key findings show that no single AL algorithm consistently outperforms others across all domains, though margin sampling emerges as the most reliable approach.

## Method Summary
The benchmark employs a controlled experimental setup with fixed seeds for AL algorithms, dataset splitting, and model initialization to ensure fair comparisons. Six AL algorithms (entropy, margin, BALD, BADGE, Coreset, TypiClust) are evaluated on nine datasets (7 real-world, 2 synthetic) using a greedy oracle approximation of optimal performance. The framework uses normalized AUC as the primary metric, aggregated across 50 restarts per experiment. Classifiers are fine-tuned or trained from scratch with domain-specific architectures, and synthetic datasets (ThreeClust, DivergingSin) are designed to expose principled weaknesses in different AL algorithm categories.

## Key Results
- No single AL algorithm consistently outperforms others across all domains
- Margin sampling emerges as the most reliable approach overall
- Synthetic datasets successfully highlight principled weaknesses of different AL algorithm categories
- The greedy oracle algorithm provides a computationally feasible approximation of optimal AL performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The benchmark suite enables fair comparison across different AL algorithms by controlling for dataset splits, model initialization, and random number generators.
- **Mechanism**: By creating separate RNGs for each system (AL algorithm, dataset splitting, model initialization), the framework ensures that all algorithms start from identical conditions, preventing one algorithm from benefiting from better random seeds.
- **Core assumption**: High variance in AL results stems primarily from differences in random seeds rather than fundamental algorithmic differences.
- **Evidence anchors**:
  - [abstract] "We base our work largely on [10] and follow their guidelines for a reliable evaluation of AL algorithms"
  - [section 4.2] "To adjust for this, we aim to provide an experimental setup that is fully reproducible independent of the dataset, classification model, or AL algorithm used"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.229, average citations=0.0.
- **Break condition**: If the variance in AL performance persists even with controlled seeding, indicating deeper algorithmic issues.

### Mechanism 2
- **Claim**: The greedy oracle algorithm provides a computationally feasible approximation of the optimal AL solution.
- **Mechanism**: Instead of using computationally expensive simulated annealing, the greedy oracle evaluates a subsample of unlabeled points by training the classifier with each potential addition and selecting the one that maximizes test performance.
- **Core assumption**: A subsample of the unlabeled pool can provide a good approximation of the optimal next point without evaluating all possibilities.
- **Evidence anchors**:
  - [section 4.3] "Our oracle algorithm evaluates every data pointuk = unif(U) k ∈ [1 . . . τ] in a subsample of unlabeled points by fitting the classifier ˆy on L(i) ∪ {uk} and directly measuring the resulting test performance"
  - [abstract] "We propose a greedy oracle algorithm that constructs an approximation of the optimal set in an iterative fashion"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.229, average citations=0.0.
- **Break condition**: If the subsample size τ is too small, leading to poor oracle performance.

### Mechanism 3
- **Claim**: Synthetic datasets can expose fundamental weaknesses in AL algorithms that real-world datasets might not reveal.
- **Mechanism**: ThreeClust and DivergingSin are designed to specifically challenge uncertainty-based and geometric-based AL algorithms respectively, revealing their principled shortcomings.
- **Core assumption**: Different AL algorithm categories have distinct failure modes that can be systematically tested.
- **Evidence anchors**:
  - [section 5.2] "To test for these shortcomings, we created two synthetic datasets, ThreeClust and DivergingSin, that are hard to solve for methods focused on the classifier's decision boundary or data clustering respectively"
  - [abstract] "We introduce two novel synthetic datasets that highlight principled shortcomings of existing AL algorithms"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.229, average citations=0.0.
- **Break condition**: If algorithms can be easily modified to handle these synthetic datasets without addressing their fundamental weaknesses.

## Foundational Learning

- **Concept**: Active Learning (AL) basics
  - **Why needed here**: Understanding the core AL problem formulation is essential to grasp why different algorithms are compared and what constitutes fair evaluation.
  - **Quick check question**: What is the primary goal of pool-based active learning in terms of the labeled dataset size and classifier performance?

- **Concept**: Variance in machine learning experiments
  - **Why needed here**: The paper emphasizes that AL results have high variance, which necessitates careful experimental design and multiple restarts.
  - **Quick check question**: Why does the paper recommend 50 restarts per experiment instead of the typical 3-10 used in other ML research?

- **Concept**: Synthetic data generation for algorithm testing
  - **Why needed here**: The synthetic datasets are a key contribution for testing algorithmic weaknesses that real data might not expose.
  - **Quick check question**: How do ThreeClust and DivergingSin specifically challenge different categories of AL algorithms?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Model training with fixed hyperparameters -> AL algorithm execution with controlled seeding -> Performance evaluation -> Result aggregation across 50 restarts
- **Critical path**: Data preprocessing → Model training with fixed hyperparameters → AL algorithm execution with controlled seeding → Performance evaluation → Result aggregation across 50 restarts
- **Design tradeoffs**: Using smaller classifiers reduces computational cost but may not fully represent real-world performance; synthetic datasets provide controlled testing but may not capture all real-world complexities
- **Failure signatures**: High variance in results across restarts indicates seeding issues; oracle significantly underperforming on a dataset suggests implementation bugs; algorithms performing worse than random on certain datasets indicates fundamental algorithmic issues
- **First 3 experiments**:
  1. Run margin sampling on Splice dataset with default settings and verify it performs better than random
  2. Execute the greedy oracle algorithm on DNA dataset and confirm it outperforms all other algorithms
  3. Test ThreeClust dataset with uncertainty-based algorithms and observe their poor performance compared to geometric approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size for active learning algorithms that balances computational efficiency with acquisition function performance?
- Basis in paper: [explicit] The paper discusses batch AL in comparison to single-sample AL and notes that "Batched algorithms (and benchmarks) do not have a principled advantage over single-sample AL except for speed of computation" but doesn't empirically determine optimal batch sizes.
- Why unresolved: The paper deliberately excluded batch AL experiments to maintain focus on single-sample comparisons, leaving this fundamental trade-off unexplored.
- What evidence would resolve it: Empirical comparison of batch sizes (e.g., 10, 50, 100, 500) across the benchmark datasets showing performance vs. computational cost trade-offs, with statistical significance testing.

### Open Question 2
- Question: How do active learning algorithms perform when combined with orthogonal techniques like data augmentation, semi-supervised learning, or auxiliary tasks?
- Basis in paper: [explicit] The paper explicitly states "Our training process for the classifier does not include orthogonal techniques for handling low-data use-cases, like data augmentation, semi-supervised learning, or auxiliary tasks" and suggests these could affect performance.
- Why unresolved: The authors deliberately simplified the training protocol to control hyperparameters and focus on acquisition function comparisons, leaving these potentially important interactions unexplored.
- What evidence would resolve it: Controlled experiments comparing AL algorithms with and without augmentation/Semi-supervised techniques across multiple datasets, measuring both final accuracy and sample efficiency.

### Open Question 3
- Question: How do active learning algorithms behave when given control over classifier hyperparameters (learning rate, architecture choices, etc.) rather than having them fixed?
- Basis in paper: [explicit] The paper notes "We strongly believe that these hyperparameters should be part of the problem setting and therefore be under the control of the AL algorithm" but acknowledges "currently no AL algorithm exists that takes the classifiers hyperparameters into account or controls them."
- Why unresolved: This represents a fundamental gap in the field where acquisition functions optimize sample selection but not the learning system itself, creating a potentially suboptimal separation of concerns.
- What evidence would resolve it: Implementation of AL algorithms that can query hyperparameter configurations (e.g., learning rate schedules, dropout rates, architecture modifications) and experiments showing whether this joint optimization improves sample efficiency.

## Limitations
- High confidence in benchmark's fairness is Medium for cross-domain comparisons due to potential hyperparameter sensitivity
- Focus on single-label classification limits applicability to multi-label or structured prediction tasks
- Use of smaller classifiers and SimCLR embeddings may not fully capture real-world performance characteristics

## Confidence
- **High Confidence**: The experimental reproducibility framework with controlled seeding and 50 restarts per experiment is robust and well-implemented
- **Medium Confidence**: Domain-specific algorithm rankings are reliable within each domain but may not generalize to all possible datasets in those domains
- **Low Confidence**: The synthetic datasets' ability to reveal fundamental algorithmic weaknesses across all possible real-world scenarios remains to be fully validated

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary key hyperparameters (e.g., classifier architecture, embedding dimensions) across all datasets to assess robustness of the benchmark rankings
2. **Real-World Performance Validation**: Apply the benchmarked algorithms to a separate set of real-world datasets not included in the original study to verify domain-specific findings
3. **Scalability Assessment**: Evaluate the computational feasibility of the greedy oracle algorithm on larger datasets (100K+ points) and its impact on benchmark completeness