---
ver: rpa2
title: Adapter-based Selective Knowledge Distillation for Federated Multi-domain Meeting
  Summarization
arxiv_id: '2308.03275'
source_url: https://arxiv.org/abs/2308.03275
tags:
- federated
- meeting
- learning
- data
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adapter-based federated selective knowledge
  distillation method for training performant client models in the multi-domain meeting
  summarization task. Specifically, we develop an adapter-based summarization model
  where two adapters cooperatively facilitate learning using fewer parameters to reduce
  communication costs.
---

# Adapter-based Selective Knowledge Distillation for Federated Multi-domain Meeting Summarization

## Quick Facts
- arXiv ID: 2308.03275
- Source URL: https://arxiv.org/abs/2308.03275
- Reference count: 40
- Primary result: Proposed method achieves comparable performance with centralized training while reducing communication costs

## Executive Summary
This paper introduces an adapter-based federated learning framework for multi-domain meeting summarization that addresses the challenge of non-IID data distribution across clients. The method employs two cooperative adapters - a global adapter for server-side aggregation and a local adapter for client-specific adaptation - along with a selective knowledge distillation strategy that uses entropy-based filtering to transfer only reliable global knowledge. Extensive experiments on the QMSum benchmark demonstrate that the approach achieves performance comparable to centralized training while significantly reducing communication overhead.

## Method Summary
The method develops an adapter-based meeting summarizer using a frozen backbone model (BART-large or LED-large) with two lightweight adapter modules. The global adapter is maintained by the server and updated via federated averaging, while each client maintains a local adapter trained on their private domain-specific data. A selective knowledge distillation strategy transfers knowledge from the global to local adapter using entropy-based filtering, where only low-entropy global knowledge distributions are distilled to the local adapter. This architecture reduces communication costs by transmitting only adapter parameters rather than full model weights, and improves robustness to non-IID data through selective knowledge transfer.

## Key Results
- Achieves comparable ROUGE scores to centralized training methods on QMSum benchmark
- Demonstrates superior performance compared to standard federated averaging and other federated distillation methods
- Shows robustness across different domain distributions and client participation rates
- Reduces communication costs significantly by transmitting only adapter parameters

## Why This Works (Mechanism)

### Mechanism 1
- Adapter-based models reduce communication costs in federated learning by transmitting only lightweight adapter parameters instead of full model weights, dramatically reducing bandwidth usage while maintaining model capacity through the frozen backbone.

### Mechanism 2
- Selective knowledge distillation improves robustness to non-IID data by using entropy-based filtering to transfer only reliable global knowledge, preventing the propagation of uncertain or domain-inappropriate information from the server to clients.

### Mechanism 3
- Two-adapter architecture (global + local) balances domain specificity with global generalization by separating the optimization paths, allowing local adapters to specialize while still benefiting from global knowledge distillation.

## Foundational Learning

- Concept: Federated Averaging (FedAvg)
  - Why needed here: Core server aggregation algorithm that determines how client updates are combined into global model
  - Quick check question: What happens to FedAvg performance when client data distributions are highly non-IID?

- Concept: Knowledge Distillation
  - Why needed here: Enables student (local adapter) to learn from teacher (global adapter) without direct parameter sharing, preserving privacy
  - Quick check question: How does KL divergence loss differ from cross-entropy in the distillation objective?

- Concept: Entropy-based Uncertainty
  - Why needed here: Provides metric to filter unreliable global knowledge during distillation
  - Quick check question: Why would high-entropy output distributions indicate less trustworthy knowledge?

## Architecture Onboarding

- Component map: Clients (Local adapter + frozen backbone) -> Server (Global adapter storage + FedAvg aggregation) -> Clients (global adapter updates)
- Critical path: 1) Clients train local adapter on private data 2) Clients distill from global adapter using entropy threshold 3) Clients send updated local adapter to server 4) Server aggregates via FedAvg 5) Server broadcasts new global adapter
- Design tradeoffs: Adapter bottleneck dimension vs. model capacity; entropy threshold vs. knowledge transfer completeness; participation rate vs. convergence stability
- Failure signatures: Low ROUGE scores across domains; large parameter updates indicating backbone fine-tuning; high variance across clients suggesting non-IID handling failure
- First 3 experiments: 1) Compare FedAvg-only vs. FedAvg+distillation on balanced non-IID data 2) Sweep entropy threshold values to find optimal selective transfer 3) Measure communication cost reduction vs. full-model federation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform in a real-world federated learning environment with client dropout and asynchronous updates?
- Basis in paper: [inferred] The paper mentions that future work will strive to apply the method to real scenarios and collaborate with organizations to implement the federated learning framework
- Why unresolved: The current research is conducted using a simulated federated learning environment, which may not fully capture the complexities of real-world scenarios
- What evidence would resolve it: Implementing the method in a real-world federated learning environment with client dropout and asynchronous updates, and evaluating its performance and robustness under these conditions

### Open Question 2
- Question: How effective would it be to use knowledge distillation by absolute means only when learning target nouns and verbs?
- Basis in paper: [inferred] The paper discusses the part-of-speech tag distribution of target summary words learned with and without knowledge distillation, and suggests that nouns and verbs are essential for articulating the core and domain-specific ideas of the meetings
- Why unresolved: The paper does not explicitly test the effectiveness of using knowledge distillation only for nouns and verbs
- What evidence would resolve it: Conducting experiments using knowledge distillation only for nouns and verbs, and comparing the results with the current method to determine its effectiveness

### Open Question 3
- Question: How does the proposed method handle noisy, imperfect textual data generated by automatic speech recognition (ASR) systems?
- Basis in paper: [inferred] The paper mentions that meeting transcripts are typically generated via ASR systems, resulting in noisy, imperfect textual data
- Why unresolved: The current research does not address the issue of handling noisy, imperfect textual data
- What evidence would resolve it: Evaluating the method's performance on meeting transcripts with varying levels of noise and imperfection, and comparing it with other methods to determine its effectiveness in handling such data

## Limitations

- Evaluation relies on a single dataset (QMSum) and only three client domains, limiting generalizability to other summarization tasks
- Entropy threshold of τ=5 appears empirically chosen without systematic sensitivity analysis
- Adapter architecture specifics (layer placement, dimension tuning) are not fully detailed, making exact reproduction challenging

## Confidence

- High confidence: Adapter-based federated learning reduces communication costs compared to full-model federation
- Medium confidence: Selective knowledge distillation with entropy filtering improves robustness to non-IID data
- Medium confidence: Two-adapter architecture balances domain specificity with global generalization

## Next Checks

1. Test entropy threshold sensitivity by evaluating performance across τ ∈ [1, 10] to determine optimal selective transfer rate
2. Conduct cross-dataset validation using different summarization benchmarks (e.g., CNN/DailyMail) to assess generalizability beyond QMSum
3. Implement ablation study comparing: full-model FedAvg, adapter-only FedAvg, adapter+selective distillation, and adapter+random knowledge transfer to isolate the impact of selective filtering