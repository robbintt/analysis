---
ver: rpa2
title: Detection and prediction of clopidogrel treatment failures using longitudinal
  structured electronic health records
arxiv_id: '2310.08757'
source_url: https://arxiv.org/abs/2310.08757
tags:
- data
- tasks
- codes
- treatment
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied machine learning algorithms to detect and predict
  clopidogrel treatment failures using longitudinal structured electronic health records
  (EHR). The authors organized diagnoses, prescriptions, and procedures into visits
  per patient and applied various time series models, including BERT, LSTM, and GRU.
---

# Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records

## Quick Facts
- arXiv ID: 2310.08757
- Source URL: https://arxiv.org/abs/2310.08757
- Reference count: 21
- Key outcome: BERT achieved AUC of 0.928 for detection and 0.729 for prediction of clopidogrel treatment failures, outperforming other models when trained on limited data

## Executive Summary
This study applied machine learning algorithms to detect and predict clopidogrel treatment failures using longitudinal structured electronic health records (EHR). The authors organized diagnoses, prescriptions, and procedures into visits per patient and applied various time series models, including BERT, LSTM, and GRU. Models were evaluated on two tasks: detecting and predicting treatment failures within one year after the first clopidogrel prescription. BERT achieved an AUC of 0.928 for detection and 0.729 for prediction, while GRU showed the best overall performance (AUC = 0.938 for detection).

## Method Summary
The study used UK Biobank data (502,527 patients) including prescriptions, diagnoses, procedures, and dates. Treatment failure cases (1,824) and controls (6,859) were identified within one year of first clopidogrel prescription. Data was processed into visits per patient, flattened and concatenated into sequences. Models included BERT, LSTM, GRU, RF, and LR. BERT used pre-training on unlabeled data and fine-tuning on labeled data. Sequential models used one hidden layer with 768 units. Data was split 80/20 for train/test evaluation.

## Key Results
- BERT achieved AUC of 0.928 for treatment failure detection and 0.729 for prediction
- GRU showed the best overall performance with AUC = 0.938 for detection
- Time series models outperformed bag-of-words approaches in both detection and prediction tasks
- Using all modalities (diagnoses, procedures, prescriptions) together improved predictive power compared to using single data types

## Why This Works (Mechanism)

### Mechanism 1
Time series modeling strategies (GRU, LSTM, BERT) outperform bag-of-words models for treatment failure detection and prediction in EHR because EHR data has inherent temporal order, and time series models can capture the dynamics of how diagnoses, prescriptions, and procedures evolve over time, which is critical for detecting treatment failures.

### Mechanism 2
BERT's pre-training on large unlabeled EHR data improves performance when labeled training data is limited because BERT learns general representations of clinical codes from a large corpus of unlabeled data, which can be fine-tuned for specific tasks like treatment failure detection, allowing it to generalize better with less task-specific labeled data.

### Mechanism 3
Combining multiple types of EHR data (diagnoses, procedures, prescriptions) improves predictive power for treatment failure detection and prediction because different types of medical records provide complementary information about a patient's condition and treatment, and modeling their interactions can provide a more comprehensive view for predicting treatment failures.

## Foundational Learning

- **Understanding of Electronic Health Records (EHR) and their structure**
  - Why needed here: The study uses longitudinal structured EHR data, which consists of diagnoses, prescriptions, and procedures organized into visits per patient.
  - Quick check question: What are the main types of data included in the EHR used in this study, and how are they organized?

- **Familiarity with machine learning models used in natural language processing (NLP)**
  - Why needed here: The study draws analogies between natural language and structured EHR, and applies various NLP algorithms (e.g., BERT, LSTM, GRU) to model the longitudinal EHR data.
  - Quick check question: How do the NLP models used in this study (BERT, LSTM, GRU) differ in their approach to modeling sequential data?

- **Knowledge of evaluation metrics for classification tasks**
  - Why needed here: The study evaluates the performance of the models using receiver operating characteristic (ROC) curves and areas under the curve (AUC).
  - Quick check question: What do the AUC values reported in the study indicate about the performance of the models for treatment failure detection and prediction?

## Architecture Onboarding

- **Component map**: Data preprocessing (organizing codes into visits and patients) -> Model training and fine-tuning (using different models and modalities) -> Evaluation (using ROC curves and AUC)
- **Critical path**: Data preprocessing -> Model training and fine-tuning -> Evaluation
- **Design tradeoffs**: Time series models vs. bag-of-words models in terms of performance and interpretability; using all modalities vs. individual modalities in terms of predictive power and model complexity; pre-training BERT vs. training from scratch in terms of performance with limited data
- **Failure signatures**: Poor performance of time series models compared to bag-of-words models may indicate that the temporal information is not informative for the task; low AUC values may indicate that the models are not able to effectively distinguish between treatment failure cases and controls; poor performance with limited data may indicate that the models are overfitting or that the pre-training is not effective
- **First 3 experiments**:
  1. Compare the performance of time series models (BERT, LSTM, GRU) vs. bag-of-words models (RF, LR) for treatment failure detection using all modalities
  2. Evaluate the impact of using different combinations of modalities (diagnoses, procedures, prescriptions) on the performance of the best time series model for treatment failure prediction
  3. Investigate the effect of pre-training BERT on a large unlabeled dataset vs. training from scratch on the labeled data for treatment failure detection with limited training samples

## Open Questions the Paper Calls Out

### Open Question 1
How would the performance of treatment failure prediction change if longitudinal data beyond one year after the first clopidogrel prescription were included? The study specifically constrained its timeframe to one year post-prescription, leaving the potential impact of longer-term data unexplored.

### Open Question 2
Would incorporating additional clinical data types such as laboratory results, imaging findings, or genetic information significantly improve the detection and prediction of clopidogrel treatment failures? The current study only used structured EHR data (diagnoses, procedures, prescriptions) and explicitly states this as a limitation.

### Open Question 3
How would different pre-training strategies or larger unlabeled datasets affect the performance of BERT and other transformer models in this clinical prediction task? The study used a specific pre-training approach and dataset size, but did not explore variations in pre-training methodology or dataset scale.

## Limitations

- The study relies on UK Biobank data with specific inclusion criteria for treatment failure cases and controls, raising questions about generalizability to other populations
- While treatment failures are defined within one year of first clopidogrel prescription, the specific ICD codes and clinical criteria used to identify these events are not fully specified
- The performance advantage of BERT with limited data is demonstrated, but the exact size of training sets tested and transfer learning effectiveness across different clinical scenarios requires further validation

## Confidence

- **Time series models outperform bag-of-words approaches**: High confidence
- **BERT's pre-training improves performance with limited data**: Medium confidence
- **Multi-modal data improves predictive power**: High confidence

## Next Checks

1. Validate model performance on an independent EHR dataset from a different healthcare system to assess generalizability of the findings

2. Conduct ablation studies to quantify the individual contribution of each data modality (diagnoses, procedures, prescriptions) to model performance

3. Test model performance across varying amounts of training data to confirm the claimed advantage of BERT's pre-training approach and identify the threshold where this advantage diminishes