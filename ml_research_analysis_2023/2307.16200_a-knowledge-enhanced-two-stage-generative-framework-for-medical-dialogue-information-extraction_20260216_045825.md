---
ver: rpa2
title: A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information
  Extraction
arxiv_id: '2307.16200'
source_url: https://arxiv.org/abs/2307.16200
tags:
- status
- term
- ktgf
- terms
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of extracting term-status pairs
  from medical dialogues, which is crucial for automatic electronic medical record
  generation and diagnosis dialogue systems. Existing generative methods for this
  task output a sequence of term-status pairs in one stage and neglect to integrate
  prior knowledge, which requires a deeper understanding to model the relationship
  between terms and infer the status of each term.
---

# A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction

## Quick Facts
- arXiv ID: 2307.16200
- Source URL: https://arxiv.org/abs/2307.16200
- Reference count: 40
- Key outcome: Proposes KTGF, achieving 5.55 F1-score improvement on Chunyu and 3.13 F1-score improvement on CMDD datasets

## Executive Summary
This paper addresses the challenge of extracting term-status pairs from medical dialogues for automatic electronic medical record generation. The authors propose a knowledge-enhanced two-stage generative framework (KTGF) that first generates all medical terms from the dialogue, then generates their statuses using structured knowledge prompts. The framework demonstrates superior performance over state-of-the-art models on two medical dialogue datasets, particularly in low-resource settings.

## Method Summary
KTGF uses T5 as backbone with a two-stage approach: first generating comma-separated terms from medical dialogue text, then generating corresponding statuses using knowledge-enhanced prompts that incorporate term categories and predefined status candidates. The framework handles both full-data and low-resource settings through multi-task learning with different prompting strategies. A special "not mentioned" status enables training with partially annotated data, expanding effective training examples in low-resource scenarios.

## Key Results
- KTGF achieves 5.55 and 5.28 F1-score improvements on Chunyu dataset for term and full evaluations respectively
- On CMDD dataset, KTGF shows 3.13 and 4.48 F1-score improvements for term and full evaluations respectively
- The framework demonstrates strong performance in low-resource settings by utilizing data with only term annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling term generation from status generation allows the model to focus on modeling term-term relationships without the interference of status labels.
- Mechanism: By generating all terms first in stage one, the model builds contextualized term representations that capture inter-term dependencies. In stage two, it uses these representations to generate accurate statuses.
- Core assumption: The relationship between terms is independent of their status and can be modeled effectively in a status-free sequence.
- Evidence anchors:
  - [abstract] "In this way, the relationship between terms can be learned more effectively from the sequence containing only terms in the first phase"
  - [section] "The sequence contains only terms in the first phase, which enables the model to learn the relationship between terms more effectively without considering the status."
- Break condition: If term-term relationships are actually mediated through their statuses (e.g., certain term combinations only make sense with specific status patterns), then decoupling may harm performance.

### Mechanism 2
- Claim: Incorporating category and status candidate information into the prompt provides task-aware contextualization that guides more accurate status generation.
- Mechanism: The prompt includes the category of each generated term and the predefined status candidates for that category. This acts as a structured schema guide during decoding.
- Core assumption: The model can effectively use discrete category/status candidate information provided in the prompt to improve generation quality.
- Evidence anchors:
  - [abstract] "our designed knowledge-enhanced prompt in the second phase can leverage the category and status candidates of the term for status generation"
  - [section] "We integrate the status candidates and the category of generated terms defined in the schema into the prompt of the second phase and name it the knowledge-enhanced prompt."
- Break condition: If the model overfits to the prompt format or fails to generalize when status candidates change, or if the prompt overwhelms the model's capacity.

### Mechanism 3
- Claim: The "not mentioned" status expands training data in low-resource settings by allowing terms not present in the dialogue to still be included in training pairs.
- Mechanism: By adding "not mentioned" as a valid status, every term in the schema can be paired with at least one status, increasing the diversity and volume of training examples.
- Core assumption: Including negative examples (terms not mentioned) is beneficial for training a robust status generator.
- Evidence anchors:
  - [abstract] "Furthermore, our proposed special status 'not mentioned' makes more terms available and enriches the training data in the second phase, which is critical in the low-resource setting."
  - [section] "In the low-resource setting, we improve the knowledge-enhanced prompt by introducing a special status as described in Section 3.4."
- Break condition: If the model becomes biased toward predicting "not mentioned" or if the additional negative examples introduce noise that outweighs the benefits.

## Foundational Learning

- Concept: Prompt engineering with structured schema information
  - Why needed here: The model needs to generate statuses from a predefined set; providing this structure directly in the prompt reduces ambiguity and guides decoding.
  - Quick check question: How does the knowledge-enhanced prompt differ from a plain text prompt in this task?

- Concept: Two-stage generative frameworks for structured prediction
  - Why needed here: Term-status extraction is a structured prediction task where modeling dependencies between components (terms) separately from their attributes (statuses) can improve accuracy.
  - Quick check question: What is the advantage of generating terms first and then statuses, compared to generating term-status pairs in one pass?

- Concept: Low-resource data augmentation via task reformulation
  - Why needed here: Medical dialogue datasets are often small and expensive to annotate fully; reformulating the task to use partially annotated data expands the effective training set.
  - Quick check question: How does the "not mentioned" status help in low-resource scenarios?

## Architecture Onboarding

- Component map: Dialogue text → Term generation → Knowledge-enhanced prompt construction → Status generation → Term-status pairs

- Critical path: Dialogue → Term generation → Knowledge-enhanced prompt construction → Status generation

- Design tradeoffs:
  - Using two stages adds latency but improves accuracy; single-stage is faster but less accurate.
  - Knowledge-enhanced prompts increase model capacity requirements but improve performance.
  - The "not mentioned" status increases training data but may bias predictions.

- Failure signatures:
  - Terms not generated → Stage 1 failure → No statuses produced
  - Incorrect statuses → Stage 2 prompt not effective or model misunderstanding
  - Over-generation of terms → Prompt or decoding parameters need tuning

- First 3 experiments:
  1. Run KTGF with T5-small on Chunyu dataset, compare to T5-small baseline
  2. Remove knowledge-enhanced prompt (w/o knowledge) and measure drop in performance
  3. Remove "not mentioned" status and evaluate low-resource performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KTGF vary when using different backbone models, such as BERT or RoBERTa, instead of T5?
- Basis in paper: [inferred] The paper uses T5 as the backbone model for KTGF, but it does not explore the use of other pre-trained language models.
- Why unresolved: The paper does not provide any comparison or analysis of the performance of KTGF when using different backbone models.
- What evidence would resolve it: Experiments comparing the performance of KTGF using different backbone models, such as BERT or RoBERTa, on the same datasets and evaluation metrics used in the paper.

### Open Question 2
- Question: How does the performance of KTGF change when the two-stage generation is replaced with a single-stage generation approach?
- Basis in paper: [inferred] The paper proposes a two-stage generation approach for KTGF, but it does not compare its performance with a single-stage generation approach.
- Why unresolved: The paper does not provide any comparison or analysis of the performance of KTGF when using a single-stage generation approach.
- What evidence would resolve it: Experiments comparing the performance of KTGF using a two-stage generation approach with a single-stage generation approach on the same datasets and evaluation metrics used in the paper.

### Open Question 3
- Question: How does the performance of KTGF vary when using different prompt designs, such as different task-specific prompts or knowledge-enhanced prompts?
- Basis in paper: [explicit] The paper discusses the design of task-specific prompts and knowledge-enhanced prompts for KTGF, but it does not explore the impact of different prompt designs on the model's performance.
- Why unresolved: The paper does not provide any comparison or analysis of the performance of KTGF when using different prompt designs.
- What evidence would resolve it: Experiments comparing the performance of KTGF using different prompt designs, such as different task-specific prompts or knowledge-enhanced prompts, on the same datasets and evaluation metrics used in the paper.

## Limitations
- The paper's performance claims rely heavily on the specific architecture of knowledge-enhanced prompts, but the exact prompt templates are not fully specified, making exact reproduction challenging.
- While demonstrating superior performance on two medical dialogue datasets, the generalization to other medical domains or languages remains untested.
- The "not mentioned" status may introduce bias toward negative predictions that isn't fully characterized.

## Confidence
- **High confidence**: The two-stage generation approach provides a meaningful architectural improvement over single-stage methods for this task
- **Medium confidence**: The knowledge-enhanced prompt mechanism delivers the claimed performance benefits, though exact implementation details affect replicability
- **Medium confidence**: The low-resource setting improvements are significant, but the "not mentioned" status may introduce bias that isn't fully characterized

## Next Checks
1. **Reproduce the knowledge-enhanced prompt template**: Implement and test multiple variations of the prompt structure to identify which components are essential for performance gains

2. **Ablation study on status candidates**: Remove the category and status candidate information from prompts to quantify their specific contribution to overall performance

3. **Cross-domain validation**: Test KTGF on medical dialogue datasets from different specialties (e.g., neurology, orthopedics) to assess domain generalizability beyond cardiovascular and pediatric cases