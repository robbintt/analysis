---
ver: rpa2
title: 'LLM in a flash: Efficient Large Language Model Inference with Limited Memory'
arxiv_id: '2312.11514'
source_url: https://arxiv.org/abs/2312.11514
tags:
- memory
- flash
- data
- dram
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of running large language models
  (LLMs) on devices with limited memory by storing the model parameters in flash memory
  and selectively loading them on demand to DRAM. The key idea is to exploit the sparsity
  in LLM FeedForward Network (FFN) layers, loading only the non-zero parameters from
  flash memory.
---

# LLM in a flash: Efficient Large Language Model Inference with Limited Memory

## Quick Facts
- arXiv ID: 2312.11514
- Source URL: https://arxiv.org/abs/2312.11514
- Reference count: 14
- This paper enables running LLMs up to twice the size of available DRAM with 4-5x and 20-25x speedup on CPU and GPU respectively

## Executive Summary
This paper addresses the challenge of running large language models on memory-constrained devices by storing model parameters in flash memory and selectively loading them to DRAM on demand. The key innovation exploits sparsity in LLM FeedForward Network layers, loading only non-zero parameters during inference. Two complementary techniques - windowing (reusing recently activated neurons) and row-column bundling (reading larger contiguous chunks from flash) - enable models up to twice the size of available DRAM while achieving significant speedups compared to naive approaches.

## Method Summary
The method stores the full LLM model in flash memory while keeping only active portions in DRAM. It introduces windowing to reuse recently activated neurons and reduce data transfer, and row-column bundling to increase flash memory throughput by reading larger contiguous chunks. The approach combines sparsity awareness with context-adaptive loading, using a predictor to identify likely active neurons and a static memory preallocation strategy to minimize DRAM transfers. The system is optimized for the sequential read strengths of flash memory while minimizing unnecessary data loading.

## Key Results
- Enables running LLMs up to twice the size of available DRAM
- Achieves 4-5x speedup in inference compared to naive CPU approaches
- Achieves 20-25x speedup compared to naive GPU approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flash memory can store the full LLM model while DRAM holds only active portions, reducing memory requirements
- Mechanism: The model parameters are stored on flash memory, which has much higher capacity than DRAM. During inference, only the necessary parameters are selectively loaded from flash into DRAM as needed
- Core assumption: LLMs have inherent sparsity in their FeedForward Network (FFN) layers, allowing selective loading of only non-zero parameters
- Evidence anchors: [abstract] "exploit the sparsity in LLM FeedForward Network (FFN) layers, loading only the non-zero parameters from flash memory"

### Mechanism 2
- Claim: Windowing technique reduces data transfer by reusing recently activated neurons
- Mechanism: The windowing technique maintains neuron data for only the most recent input tokens. When processing a new token, only the neuron data that differs from the immediate predecessors is loaded from flash memory
- Core assumption: Aggregated neuron usage decreases over time, allowing a sliding window approach to reduce data loading
- Evidence anchors: [abstract] "First, 'windowing' strategically reduces data transfer by reusing previously activated neurons"

### Mechanism 3
- Claim: Row-column bundling increases flash memory throughput by reading larger contiguous chunks
- Mechanism: The columns of the upward projection and rows of the downward projection for each neuron are stored together in flash memory. This allows reading larger contiguous chunks, which increases throughput
- Core assumption: Flash memory performs optimally with large sequential reads, and the bundling doesn't significantly increase the data load
- Evidence anchors: [abstract] "second, 'row-column bundling', tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory"

## Foundational Learning

- Concept: Sparsity in LLMs
  - Why needed here: The paper relies on the assumption that LLMs exhibit high sparsity in FFN layers to selectively load only non-zero parameters from flash memory
  - Quick check question: What percentage of sparsity is typically observed in FFN layers of LLMs like OPT and Falcon?

- Concept: Flash memory characteristics
  - Why needed here: Understanding flash memory's sequential read strengths and bandwidth limitations is crucial for optimizing data transfer and chunk sizes
  - Quick check question: How does flash memory's throughput for random reads compare to sequential reads?

- Concept: Memory management in constrained environments
  - Why needed here: Efficient memory management is essential when working with limited DRAM capacity and large model sizes stored on flash memory
  - Quick check question: What are the key challenges in managing memory when the model size exceeds available DRAM?

## Architecture Onboarding

- Component map:
  Flash memory -> DRAM -> CPU/GPU -> Predictor -> Windowing system -> Bundling system

- Critical path:
  1. Load predictor results from DRAM
  2. Use predictor to identify active neurons
  3. Load corresponding model parameters from flash to DRAM using windowing and bundling
  4. Perform inference using loaded parameters
  5. Update window and manage memory for next token

- Design tradeoffs:
  - Window size vs. memory usage: Larger windows reduce data transfer but require more DRAM
  - Chunk size vs. throughput: Larger chunks increase throughput but may load unnecessary data
  - Predictor accuracy vs. computation overhead: More accurate predictors may require more computation

- Failure signatures:
  - High latency in token generation
  - Memory overflow errors
  - Inefficient data transfer (loading too much or too little data)
  - Incorrect predictions leading to loading wrong parameters

- First 3 experiments:
  1. Measure sparsity levels in FFN layers of a sample LLM to validate the core assumption
  2. Test windowing technique with different window sizes to find optimal balance between memory usage and data transfer
  3. Experiment with various chunk sizes for row-column bundling to maximize flash memory throughput

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed "row-column bundling" technique vary with different neural network architectures and sizes beyond the tested OPT and Falcon models?
- Basis in paper: [explicit] The paper mentions that "our analysis and experiment show this increases the throughput of the model" but does not provide a comprehensive evaluation across diverse model architectures and sizes
- Why unresolved: The paper focuses on specific models (OPT 6.7B and Falcon 7B) without exploring the broader applicability of the bundling technique to other architectures or larger models
- What evidence would resolve it: Conducting experiments on a wider range of models, including different architectures (e.g., BERT, RoBERTa) and varying sizes, to assess the effectiveness and scalability of the bundling technique

### Open Question 2
- Question: What is the impact of the "sliding window" technique on the accuracy of language models, particularly in tasks requiring long-term context or memory?
- Basis in paper: [inferred] The paper discusses the sliding window technique for managing neuron data but does not explicitly address its impact on model accuracy, especially for tasks needing extended context
- Why unresolved: While the technique aims to reduce data transfer, its effect on the model's ability to maintain context over longer sequences is not explored
- What evidence would resolve it: Evaluating the model's performance on tasks that require long-term context (e.g., question answering, summarization) with and without the sliding window technique to determine any accuracy trade-offs

### Open Question 3
- Question: How does the proposed method handle the dynamic nature of LLM usage patterns, such as varying batch sizes or sequence lengths, in real-world applications?
- Basis in paper: [explicit] The paper mentions that the method is tested with sequences processed individually and focuses on optimizing for specific memory constraints
- Why unresolved: Real-world applications often involve dynamic workloads with varying batch sizes and sequence lengths, which may affect the efficiency of the proposed method
- What evidence would resolve it: Testing the method under different operational scenarios with varying batch sizes and sequence lengths to evaluate its adaptability and efficiency in dynamic environments

## Limitations

- The approach relies heavily on sparsity in FFN layers, which may vary across different LLM architectures and tasks
- Predictor accuracy for identifying active neurons is critical and may degrade under certain input distributions
- Static memory preallocation may not adapt well to varying inference patterns across different workloads

## Confidence

**High Confidence**: The core architectural approach of storing models in flash memory with selective loading to DRAM is technically sound and aligns with established memory hierarchy principles.

**Medium Confidence**: The specific performance improvements depend heavily on the sparsity characteristics of different LLMs and the effectiveness of the predictor.

**Low Confidence**: The generalization of results to real-world deployment scenarios, particularly regarding predictor robustness and adaptability to diverse workloads, remains unproven.

## Next Checks

1. **Sparsity Characterization**: Measure and characterize FFN layer sparsity across multiple LLM architectures (GPT, Llama, Mistral) under different inference scenarios to validate the foundational assumption.

2. **Predictor Robustness**: Test the predictor's accuracy and performance across diverse input distributions, including non-standard text, code, and multilingual inputs, to assess generalizability.

3. **Hardware Portability**: Implement and benchmark the system across different flash memory implementations and CPU/GPU configurations to validate hardware independence claims.