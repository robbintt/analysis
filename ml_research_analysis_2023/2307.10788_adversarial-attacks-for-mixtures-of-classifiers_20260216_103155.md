---
ver: rpa2
title: Adversarial attacks for mixtures of classifiers
arxiv_id: '2307.10788'
source_url: https://arxiv.org/abs/2307.10788
tags:
- classifiers
- attack
- linear
- attacks
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes adversarial attacks against mixtures of classifiers,
  where predictions are made by randomly selecting one classifier from an ensemble.
  The authors argue that existing attacks are ineffective because they try to attack
  all classifiers simultaneously, which often fails when classifiers have non-overlapping
  vulnerability regions.
---

# Adversarial attacks for mixtures of classifiers

## Quick Facts
- arXiv ID: 2307.10788
- Source URL: https://arxiv.org/abs/2307.10788
- Authors:
- Reference count: 40
- Key outcome: This paper analyzes adversarial attacks against mixtures of classifiers, where predictions are made by randomly selecting one classifier from an ensemble. The authors propose a new attack called Lattice Climber Attack (LCA) that navigates the vulnerability regions in a principled way by climbing the adversarial lattice one level at a time. LCA is shown to be optimal in the binary linear setting and performs better than existing attacks on both synthetic and real datasets (CIFAR-10 and CIFAR-100). On CIFAR-10, LCA achieves 0% accuracy against baseline mixtures, while ARC achieves 1.7% accuracy with 12 models. Against DVERGE models, LCA achieves 0.1% accuracy with 3 models compared to ARC's 14.4%.

## Executive Summary
This paper addresses the challenge of attacking mixtures of classifiers, where predictions are made by randomly selecting one classifier from an ensemble. The authors identify a fundamental flaw in existing attacks like ARC and APGD - they attempt to attack all classifiers simultaneously, which often fails when classifiers have non-overlapping vulnerability regions. To address this, they propose the Lattice Climber Attack (LCA) that navigates vulnerability regions in a principled way by climbing the adversarial lattice one level at a time. The attack maintains a pool of successfully attacked classifiers and attempts to add new classifiers to this pool, ensuring it remains within a maximal vulnerability region.

## Method Summary
The paper proposes the Lattice Climber Attack (LCA) for adversarial attacks against mixtures of classifiers. LCA navigates vulnerability regions by climbing the adversarial lattice one level at a time, maintaining a pool of successfully attacked classifiers and attempting to add new classifiers to this pool. When adding a classifier fails, it is discarded, ensuring the attack remains within a maximal vulnerability region. The method uses projected gradient descent on the sum of reverse hinge losses to find perturbations that simultaneously fool multiple classifiers. LCA is shown to be optimal in the binary linear setting and performs better than existing attacks (APGD and ARC) on both synthetic and real datasets (CIFAR-10 and CIFAR-100).

## Key Results
- LCA achieves 0% accuracy against baseline mixtures on CIFAR-10, while ARC achieves 1.7% accuracy with 12 models
- Against DVERGE models, LCA achieves 0.1% accuracy with 3 models compared to ARC's 14.4%
- LCA is shown to be optimal in the binary linear setting through perfect intersection finding using sum of reverse hinge losses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Lattice Climber Attack (LCA) outperforms existing attacks by navigating vulnerability regions in a principled way rather than attacking all classifiers simultaneously.
- Mechanism: LCA climbs the adversarial lattice one level at a time by maintaining a pool of successfully attacked classifiers and attempting to add new classifiers to this pool. When adding a classifier fails, it is discarded, ensuring the attack remains within a maximal vulnerability region.
- Core assumption: Vulnerability regions form a lower semi-lattice structure where I ⪯ adv J if V(J) ≠ ∅ and I ⊆ J, meaning attacking in V(J) is always preferable to attacking in V(I).
- Evidence anchors:
  - [abstract] The authors propose LCA that navigates vulnerability regions by climbing the adversarial lattice one level at a time and is shown to be optimal in the binary linear setting.
  - [section 3.2] "The idea that in configuration(d), the region V(h) is preferable than any other vulnerability region can be formalized with the concept of maximal vulnerability region."
  - [corpus] Weak evidence - related papers discuss adversarial attacks and mixtures but don't specifically validate the lattice navigation mechanism.
- Break condition: If vulnerability regions don't form a proper lattice structure or if the intersection finder fails to converge, LCA would degrade to effectiveness-only guarantees similar to existing attacks.

### Mechanism 2
- Claim: LCA guarantees maximality in the binary linear setting through perfect intersection finding using sum of reverse hinge losses.
- Mechanism: When all classifiers are linear, minimizing the sum of reverse hinge losses SRH(I,h,x,y) = 1/|I| Σ_{i∈I} ℓrev(y·fi(x)) with projected gradient descent finds perturbations that simultaneously fool all classifiers in I if such perturbations exist.
- Core assumption: For linear classifiers, SRH is a convex function and if V(I) ≠ ∅, PGD will converge to a global minimum of 0, indicating successful simultaneous attack.
- Evidence anchors:
  - [section 4.1] "In this setting, running projected gradient descent with the correct parameters on the sum of reverse hinge will converge to some x′′ that fools all the hi at the same time, see [21, Theorem 3]."
  - [section 4.1] "By [21, Theorem 3], [5, Theorem 3.2] with δ < 1/m, running PGD for T > ϵ²·m² steps with step size η = ϵ/√T returns a solution x′′ such that SRH(h,x′′,y) < 1/m."
  - [corpus] No direct evidence found in corpus for this specific convex optimization property.
- Break condition: If classifiers are non-linear or the assumption about convexity fails, the intersection finder becomes approximate rather than perfect, weakening maximality guarantees.

### Mechanism 3
- Claim: LCA's ordering heuristic of considering classifiers in decreasing order of their associated probabilities qi yields good performance.
- Mechanism: The algorithm processes classifiers in order of their mixture weights, which in the binary case with two classifiers ensures optimality by targeting the highest-weight classifier first.
- Core assumption: When vulnerability regions have different sizes, starting with higher-weight classifiers increases the likelihood of finding larger vulnerability regions that fool multiple classifiers.
- Evidence anchors:
  - [section 4.1] "Similar to [11], we find that the heuristic of considering the classifiers in decreasing order of their associated probability qi yields a good performance. For example, in the case m = 2, it ensures that LCA is optimal."
  - [section 3.1] "When considering a set of classifiers h, we define its vulnerability region as the intersection of the individual vulnerability regions, i.e. V(h) = ⋂_{h∈h} V(h)."
  - [corpus] Weak evidence - related papers discuss attack strategies but don't validate this specific ordering heuristic.
- Break condition: If the vulnerability regions don't correlate with classifier weights, or if the optimal order depends on spatial arrangement rather than weights, this heuristic may perform poorly.

## Foundational Learning

- Concept: Adversarial attacks and vulnerability regions
  - Why needed here: The entire attack framework depends on understanding how small perturbations can change classifier predictions and how these perturbations interact across multiple classifiers.
  - Quick check question: What is the vulnerability region V(h) for a classifier h and how does it relate to adversarial examples?

- Concept: Lattice theory and partial orders
  - Why needed here: LCA's navigation mechanism relies on the mathematical structure of vulnerability regions forming a lower semi-lattice under the ⪯adv ordering.
  - Quick check question: How does the ⪯adv ordering define preferences between vulnerability regions and what does maximality mean in this context?

- Concept: Convex optimization and gradient descent
  - Why needed here: The intersection finder component uses projected gradient descent on the sum of reverse hinge losses, which requires understanding convergence properties.
  - Quick check question: Under what conditions does PGD converge to a global minimum when minimizing the sum of reverse hinge losses for linear classifiers?

## Architecture Onboarding

- Component map:
  - Main attack loop (Algorithm 2) -> Intersection finder (PGD on SRH) -> Navigation mechanism -> Vulnerability region checker

- Critical path:
  1. Initialize empty pool I and zero perturbation δ
  2. For each classifier in order, add to pool and run intersection finder
  3. If intersection finder succeeds (SRH = 0), update δ and keep classifier in pool
  4. If intersection finder fails, remove classifier from pool
  5. Return final perturbation x + δ

- Design tradeoffs:
  - Perfect intersection finding (binary linear case) vs. approximate finding (multi-class case) - guarantees maximality vs. practicality
  - Fixed classifier order vs. adaptive ordering - simplicity vs. potential performance gains
  - Single random climb vs. exhaustive lattice exploration - speed vs. completeness

- Failure signatures:
  - Attack fails to find any adversarial example even when one exists (intersection finder convergence issues)
  - Attack finds non-maximal vulnerability regions (navigation mechanism flaws)
  - Performance degrades with increasing number of classifiers (scalability issues)
  - Results vary significantly with classifier ordering (heuristic sensitivity)

- First 3 experiments:
  1. Implement LCA for binary linear classifiers and verify maximality guarantees on synthetic data with known vulnerability regions
  2. Compare LCA against ARC on mixtures with varying angles between linear classifiers to demonstrate superiority in small common vulnerability regions
  3. Test LCA on CIFAR-10 with DVERGE models under ℓ∞ threat model and measure accuracy degradation compared to APGD and ARC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LCA be adapted to handle non-differentiable classifiers like decision trees or k-nearest neighbors in mixtures?
- Basis in paper: [explicit] The paper discusses adapting LCA for multi-class differentiable classifiers but doesn't address non-differentiable models.
- Why unresolved: The intersection finder mechanism relies on gradient-based optimization which requires differentiability, making the approach inapplicable to non-differentiable models.
- What evidence would resolve it: Demonstrating a modified LCA variant that can effectively attack mixtures containing non-differentiable classifiers, or proving fundamental limitations preventing such an extension.

### Open Question 2
- Question: How does LCA's performance scale with extremely large mixtures (e.g., 100+ classifiers) compared to other attacks?
- Basis in paper: [inferred] The experiments show LCA performs well for mixtures up to 12 classifiers, but don't test scalability to much larger mixtures.
- Why unresolved: The paper mentions LCA has O(m³) complexity due to m PGD steps each taking O(m²) iterations, but doesn't empirically validate performance at scale.
- What evidence would resolve it: Benchmarking LCA against existing attacks on mixtures with 50+ classifiers, measuring both attack success rate and computational efficiency.

### Open Question 3
- Question: What is the theoretical relationship between the maximality guarantee of LCA and achieving optimal attack performance?
- Basis in paper: [explicit] The paper states maximality is weaker than optimality and proves LCA achieves maximality in the binary linear setting.
- Why unresolved: The paper doesn't quantify how often maximal vulnerability regions correspond to globally optimal attacks, or characterize the performance gap.
- What evidence would resolve it: Theoretical analysis or empirical measurements showing the expected loss difference between LCA's maximal attacks and the optimal attack across various mixture configurations.

## Limitations

- The paper's claims about LCA's optimality and superiority rely heavily on theoretical guarantees in the binary linear setting, but the empirical validation is limited to specific model architectures (DVERGE and GAL ensembles).
- The 0.1% accuracy achieved against DVERGE models is impressive but was tested on a relatively small ensemble (3 models), raising questions about scalability to larger mixtures.
- The ordering heuristic's optimality claim for m=2 classifiers extends to general cases without rigorous justification, and the scalability to larger ensembles remains unproven.

## Confidence

- **High confidence**: The core mechanism of lattice navigation is mathematically sound and the binary linear setting guarantees are well-established through existing convex optimization theory.
- **Medium confidence**: The empirical superiority over ARC and APGD on CIFAR datasets, though promising, lacks statistical significance testing and ablation studies to isolate the contribution of each component.
- **Low confidence**: The ordering heuristic's optimality claim for m=2 classifiers extends to general cases without rigorous justification, and the scalability to larger ensembles remains unproven.

## Next Checks

1. Implement LCA on synthetic mixtures with varying numbers of classifiers (5-10 models) to test scalability and measure how performance degrades with ensemble size.
2. Conduct ablation studies comparing LCA with and without the probability-based ordering heuristic to quantify its contribution to attack success.
3. Perform statistical significance tests comparing LCA against ARC across multiple random seeds and dataset splits to validate the robustness of the reported accuracy improvements.