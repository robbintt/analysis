---
ver: rpa2
title: 'Neural Networks with Sparse Activation Induced by Large Bias: Tighter Analysis
  with Bias-Generalized NTK'
arxiv_id: '2301.00327'
source_url: https://arxiv.org/abs/2301.00327
tags:
- bound
- neural
- network
- training
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies training overparameterized ReLU networks in
  the NTK regime with trainable biases initialized to a constant. The key idea is
  that large bias initialization induces sparsity in the network's activation, reducing
  the number of neurons that need to be updated during training and thereby lowering
  computational cost.
---

# Neural Networks with Sparse Activation Induced by Large Bias: Tighter Analysis with Bias-Generalized NTK

## Quick Facts
- arXiv ID: 2301.00327
- Source URL: https://arxiv.org/abs/2301.00327
- Authors: 
- Reference count: 40
- Primary result: Large bias initialization induces sparsity in ReLU networks operating in the NTK regime, reducing computational cost while maintaining convergence and generalization guarantees.

## Executive Summary
This paper analyzes training overparameterized ReLU networks with trainable biases initialized to large constants, inducing sparsity in activation patterns. The authors develop a bias-generalized NTK framework that accounts for this sparsity, providing tighter convergence and generalization analyses compared to standard NTK. They show that sparse networks can achieve comparable convergence rates to dense networks with improved width requirements, and derive data-dependent eigenvalue bounds that lead to non-vacuous generalization guarantees. Experimental results on MNIST demonstrate stable sparsity patterns across training iterations.

## Method Summary
The method involves training one-hidden-layer ReLU networks with trainable biases initialized to a large constant B, inducing sparse activation patterns. The network operates in the NTK regime with gradient descent optimization. The analysis uses a bias-generalized NTK framework that accounts for the sparsity, deriving convergence rates and generalization bounds. Symmetric initialization is employed to enable zero initialization while maintaining theoretical guarantees. The experimental evaluation uses a 6-layer MLP on MNIST with Kaiming initialization for weights and BatchNorm after ReLU to prevent vanishing gradients.

## Key Results
- Large bias initialization (B) induces sparsity in ReLU activations that persists throughout training in the NTK regime
- Sparse networks achieve convergence rates comparable to dense networks with improved width requirements
- Bias-generalized NTK provides tighter eigenvalue bounds leading to non-vacuous generalization guarantees
- Experimental results show stable sparsity patterns across training iterations with different bias initializations on MNIST

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large bias initialization induces sparsity in neural network activation, reducing computational cost during training.
- Mechanism: When biases are initialized to a large constant, ReLU activations become sparse because the threshold for activation is higher. This sparsity persists throughout training in the NTK regime because weights change minimally.
- Core assumption: The network operates in the NTK regime where weights change only microscopically during training.
- Evidence anchors:
  - [abstract]: "large bias initialization induces sparsity in the network's activation, reducing the number of neurons that need to be updated during training and thereby lowering computational cost"
  - [section]: "The tantalizing benefit of such initialization is that the neural network will provably have sparse activation pattern before, during and after training"
  - [corpus]: "Sparse Neural Networks in Practice" section shows sparsity can reduce computational cost
- Break condition: If the network leaves the NTK regime or if weights change significantly during training, sparsity may not be preserved.

### Mechanism 2
- Claim: The bias-generalized NTK provides a tighter analysis of convergence and generalization compared to standard NTK.
- Mechanism: The new kernel accounts for the sparsity induced by large bias initialization, leading to improved width requirements and sharper eigenvalue bounds.
- Core assumption: The limiting kernel with trainable biases (bias-generalized NTK) accurately captures the network's behavior during training.
- Evidence anchors:
  - [abstract]: "We show that under such initialization, the neural network will have sparse activation throughout the entire training process, which enables fast training procedures... With such initialization, we show that the neural networks possess a different limiting kernel which we call bias-generalized NTK"
  - [section]: "we show that the network after sparsification can achieve as fast convergence as the original network... Our result improves the previous required width to ensure convergence"
  - [corpus]: No direct evidence found in corpus neighbors
- Break condition: If the data distribution doesn't satisfy the non-degeneracy assumptions or if the sparsity pattern changes significantly.

### Mechanism 3
- Claim: Symmetric initialization enables zero initialization while maintaining convergence guarantees.
- Mechanism: By pairing neurons with opposite weights and signs, the network output is zero at initialization, eliminating the need for scaling factors while preserving theoretical guarantees.
- Core assumption: The symmetric initialization doesn't affect the fundamental convergence properties of the network.
- Evidence anchors:
  - [section]: "we use symmetric initialization... which yields no effect (up to constant factors) on previously established convergence results"
  - [section]: "Symmetric initialization allows us to organically combine the results derived for convergence to be reused for generalization"
  - [corpus]: No direct evidence found in corpus neighbors
- Break condition: If the symmetric initialization introduces correlations that violate independence assumptions in the analysis.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) regime
  - Why needed here: The analysis relies on the network staying close to its linearization, which only holds in the NTK regime where weights change minimally
  - Quick check question: What condition must be satisfied for a network to operate in the NTK regime?

- Concept: Gaussian anti-concentration inequality
  - Why needed here: Used to bound the probability of activation flipping when weights and biases change during training
  - Quick check question: How does the Gaussian anti-concentration bound change when the strip is centered at a non-zero value?

- Concept: Localized Rademacher complexity
  - Why needed here: Used to derive generalization bounds that depend on the network's effective complexity after training
  - Quick check question: Why is localized Rademacher complexity preferred over standard Rademacher complexity for this analysis?

## Architecture Onboarding

- Component map:
  - One-hidden-layer ReLU network with trainable biases
  - Gradient descent optimization
  - Neural Tangent Kernel framework
  - Symmetric initialization scheme
  - Data-dependent eigenvalue analysis

- Critical path:
  1. Initialize network with large biases (B) and symmetric weights
  2. Train using gradient descent with small learning rate
  3. Monitor sparsity pattern throughout training
  4. Analyze convergence using bias-generalized NTK
  5. Compute generalization bounds using localized Rademacher complexity

- Design tradeoffs:
  - Larger B → more sparsity but requires wider networks for convergence
  - Symmetric initialization → eliminates output at initialization but doubles neuron count
  - Smaller learning rate → better NTK approximation but slower training

- Failure signatures:
  - Loss plateaus before convergence → network may have left NTK regime
  - Sparsity decreases during training → bias initialization may be too small
  - Generalization gap remains large → width may be insufficient for the chosen B

- First 3 experiments:
  1. Train a shallow ReLU network with B=0, B=1, B=2 and measure sparsity throughout training
  2. Vary network width and bias initialization to find minimum width for convergence at different B values
  3. Test symmetric vs standard initialization on MNIST classification with bias initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal bias initialization magnitude B that maximizes sparsity while maintaining good convergence and generalization?
- Basis in paper: The paper discusses bias initialization B and its effect on sparsity, convergence rates, and generalization bounds, noting that B can be chosen within [0,√0.5 logm] and that the convergence rate depends on λ(B) = λ0 exp(-B²/2).
- Why unresolved: The paper establishes bounds on B for theoretical guarantees but doesn't identify the optimal value that balances sparsity benefits with performance.
- What evidence would resolve it: Empirical studies varying B on different datasets to find the sweet spot where sparsity-induced computational savings are maximized without degrading accuracy.

### Open Question 2
- Question: Can the data-dependent region R be efficiently computed or approximated for large-scale datasets?
- Basis in paper: Theorem 3.11 relies on a data-dependent region R for sharper eigenvalue bounds, but the paper notes that computing this region may be challenging in practice.
- Why unresolved: The paper provides theoretical characterization of R but doesn't address computational tractability for real-world applications.
- What evidence would resolve it: Algorithms for efficiently estimating or approximating R, or empirical validation showing how R performs on standard benchmarks.

### Open Question 3
- Question: How does the sparsity-inducing initialization generalize to deeper networks beyond the one-hidden-layer case?
- Basis in paper: The paper focuses on one-hidden-layer networks and only briefly mentions extending to multi-layer settings in the experimental section.
- Why unresolved: The analysis relies heavily on properties specific to shallow networks, and the experimental section only shows preliminary results on MLPs without theoretical guarantees.
- What evidence would resolve it: Extension of the convergence, generalization, and eigenvalue analysis to multi-layer networks, or empirical studies demonstrating consistent sparsity benefits across different depths.

### Open Question 4
- Question: Is it possible to achieve similar sparsity benefits without requiring trainable biases?
- Basis in paper: The paper shows that trainable biases help identify data-dependent regions for better eigenvalue bounds, but notes that it's not proven whether trainable biases are necessary for sparsity benefits.
- Why unresolved: The paper demonstrates benefits with trainable biases but doesn't explore whether fixed non-zero biases could achieve similar computational savings.
- What evidence would resolve it: Comparative studies of networks with fixed vs. trainable non-zero biases, or theoretical analysis showing conditions under which fixed biases suffice.

### Open Question 5
- Question: What is the relationship between the width requirements for convergence and generalization in sparsely activated networks?
- Basis in paper: The paper shows different width requirements - (n/λ(B))⁴ for convergence vs (n/λ(B))⁶ for generalization - but doesn't explore the fundamental reasons for this gap.
- Why unresolved: The paper presents these requirements separately without explaining the theoretical connection or whether this gap is inherent to the problem.
- What evidence would resolve it: A unified analysis that explains the width gap, or empirical studies showing whether this gap persists across different architectures and datasets.

## Limitations

- The analysis relies heavily on the NTK regime assumption, which may break down with larger learning rates or complex decision boundaries
- The sparsity benefits depend on data-dependent patterns that may vary significantly across different distributions
- The theoretical width requirements for generalization are significantly higher than for convergence, suggesting a gap between theory and practice

## Confidence

- **High Confidence**: The mechanism by which large bias initialization induces sparsity in ReLU activations is well-established and experimentally verified
- **Medium Confidence**: The convergence and generalization results under the bias-generalized NTK framework are theoretically sound but rely on several technical assumptions
- **Low Confidence**: The claim that symmetric initialization allows zero initialization while maintaining convergence guarantees lacks extensive empirical validation

## Next Checks

1. **Empirical NTK regime validation**: Test whether the network maintains the NTK regime assumption across different learning rates and network widths by measuring weight changes during training and comparing with the theoretical bounds.

2. **Data distribution sensitivity**: Evaluate the sparsity patterns and generalization performance across different datasets (not just MNIST) to assess how data-dependent factors affect the theoretical guarantees.

3. **Finite-width effects**: Conduct experiments with varying network widths to quantify the gap between theoretical bounds (which assume infinite width) and practical performance, particularly focusing on the width requirements for convergence at different bias initialization values.