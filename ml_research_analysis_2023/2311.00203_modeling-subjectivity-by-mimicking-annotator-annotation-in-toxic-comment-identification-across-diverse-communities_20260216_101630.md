---
ver: rpa2
title: Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment
  identification across diverse communities
arxiv_id: '2311.00203'
source_url: https://arxiv.org/abs/2311.00203
tags:
- annotators
- dataset
- toxicity
- expert
- toxic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the subjectivity of toxicity annotations
  in online content moderation, focusing on disagreement among annotators from different
  backgrounds and timelines. The authors introduce a new dataset annotated by expert
  annotators and analyze three datasets using inter-rater reliability (IRR) and cross-replication
  reliability (xRR) metrics.
---

# Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities

## Quick Facts
- **arXiv ID**: 2311.00203
- **Source URL**: https://arxiv.org/abs/2311.00203
- **Reference count**: 40
- **Primary result**: Subjectivity is inherent in toxicity annotation, and subjective annotations should be used as ground truth labels for training models in diverse communities.

## Executive Summary
This paper investigates the subjectivity of toxicity annotations in online content moderation, focusing on disagreement among annotators from different backgrounds and timelines. The authors introduce a new dataset annotated by expert annotators and analyze three datasets using inter-rater reliability (IRR) and cross-replication reliability (xRR) metrics. They find high levels of disagreement among annotators, even experts, across all datasets and annotator groups. To model subjectivity, the authors conduct experiments using a large language model (LLM), varying training data size and test sets. Results show that increasing training data size and using the same annotators as the test set improves the model's ability to mimic annotator perspectives. The study concludes that subjectivity is inherent in toxicity annotation, and subjective annotations should be used as ground truth labels for training models in diverse communities.

## Method Summary
The authors introduce a new dataset annotated by expert annotators and analyze three datasets using inter-rater reliability (IRR) and cross-replication reliability (xRR) metrics. To model subjectivity, they conduct experiments using a large language model (LLM), varying training data size and test sets. The LLM is trained using soft prompt tuning on a 62B FLAN-cont-PaLM model with cross-entropy loss for binary toxicity classification. The authors evaluate the model's performance using IRR, xRR, and normalized xRR, and compare soft-tuning to few-shot prompting using delta IRR.

## Key Results
- High levels of disagreement among annotators, even experts, across all datasets and annotator groups
- Increasing training data size and using the same annotators as the test set improves the model's ability to mimic annotator perspectives
- Soft-tuning large language models on toxicity annotations is more effective than few-shot prompting for capturing annotator perspectives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Increasing training data size improves the model's ability to mimic annotator perspectives on toxicity.
- **Mechanism**: Larger training datasets expose the model to more diverse annotations, allowing it to better learn the individual annotator's viewpoint on what constitutes toxic content.
- **Core assumption**: Each annotator has a consistent perspective on toxicity that can be learned from sufficient examples.
- **Evidence anchors**:
  - [abstract]: "Results show that increasing training data size and using the same set of annotators as the test set improves the model's ability to mimic annotator perspectives."
  - [section]: "Our result (Figure 5 (a)) shows that the model trained with toxicity annotations performs better with more training examples."
  - [corpus]: Weak - corpus contains related work on annotator representations but no direct evidence for this specific mechanism.
- **Break condition**: If annotators' perspectives are inconsistent or context-dependent rather than stable patterns.

### Mechanism 2
- **Claim**: Using the same annotators in training and testing improves model performance compared to using different annotators.
- **Mechanism**: The model learns the specific annotation patterns and reasoning of the annotators it was trained on, allowing it to reproduce similar judgments when encountering similar content.
- **Core assumption**: Annotators have identifiable annotation patterns that can be learned and reproduced.
- **Evidence anchors**:
  - [abstract]: "utilizing the same set of annotators as the test set used during model training and a separate set of annotators as the test set."
  - [section]: "Figure 5 (b) indicates that for the 2023 dataset (expert annotators), the smaller the training size, the better the delta IRR which is the opposite when the 2017 test split is used as a test set."
  - [corpus]: Weak - corpus mentions annotator disagreement but not this specific mechanism of matching annotators across training/testing.
- **Break condition**: If annotators' judgments are highly context-dependent or if the same annotator would judge differently under different conditions.

### Mechanism 3
- **Claim**: Soft-tuning large language models on toxicity annotations is more effective than few-shot prompting for capturing annotator perspectives.
- **Mechanism**: Soft-tuning allows the model to adapt its internal representations to the specific annotation patterns, while few-shot prompting relies on the model's pre-existing knowledge which may not align with the annotators' perspectives.
- **Core assumption**: LLMs can be effectively adapted to specific annotation patterns through soft-tuning.
- **Evidence anchors**:
  - [section]: "We evaluate the model's performance using the formula Î”IRR = IRR with soft-tuning - IRR with few shots"
  - [section]: "We first tested the trained model (trained with the 2017 train set from 10 balanced annotators) using the 2017 dataset test split and then with the 2023 dataset (expert annotators)."
  - [corpus]: Weak - corpus contains related work on soft prompt tuning but not specifically in the context of toxicity annotation.
- **Break condition**: If the LLM's pre-existing knowledge is already well-aligned with the annotators' perspectives, making soft-tuning unnecessary.

## Foundational Learning

- **Concept**: Inter-rater reliability (IRR) and cross-replication reliability (xRR)
  - **Why needed here**: These metrics are essential for quantifying disagreement among annotators and understanding the subjectivity in toxicity annotations.
  - **Quick check question**: If two annotators disagree on 70% of cases, what would their IRR be? (Answer: IRR = 0.3, since IRR measures agreement, not disagreement)

- **Concept**: Large Language Models (LLMs) and soft-tuning
  - **Why needed here**: The paper uses LLMs as the modeling framework, and soft-tuning is the specific method employed to adapt the model to toxicity annotation tasks.
  - **Quick check question**: What's the difference between soft-tuning and few-shot prompting? (Answer: Soft-tuning adapts the model's internal representations, while few-shot prompting relies on the model's pre-existing knowledge)

- **Concept**: UMAP (Uniform Manifold Approximation and Projection)
  - **Why needed here**: UMAP is used in the simulated experiment to visualize and separate annotators and items in the embedding space.
  - **Quick check question**: What's the purpose of using UMAP in the simulated experiment? (Answer: To visualize the separation between annotators and items in the embedding space, helping to understand how well the model can distinguish between them)

## Architecture Onboarding

- **Component map**: Data preprocessing -> LLM training (soft-tuning) -> Evaluation (IRR, xRR) -> Analysis
- **Critical path**: 1. Data preparation and transformation 2. LLM training with soft-tuning 3. Evaluation using IRR and xRR metrics 4. Analysis of results and comparison across conditions
- **Design tradeoffs**:
  - Training size vs. model generalization: Larger training sizes improve mimicry but may reduce generalization to new annotators
  - Binary vs. multi-point scales: Simplifying to binary scale for consistency but losing nuance
  - Soft-tuning vs. few-shot: Soft-tuning is more effective but requires more computational resources
- **Failure signatures**:
  - Low delta IRR between soft-tuning and few-shot: Indicates the model's pre-existing knowledge is already well-aligned with annotator perspectives
  - High xRR between different annotator groups: Suggests annotators have similar perspectives, reducing the need for personalization
  - Low separation in UMAP projections: Indicates the model struggles to distinguish between annotators and items
- **First 3 experiments**:
  1. Train the LLM with varying training sizes (50, 100, 250, 400 examples) and evaluate IRR on the same annotators
  2. Compare IRR when using the same annotators vs. different annotators as the test set
  3. Compare soft-tuning vs. few-shot prompting on the same dataset and annotators

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific properties of the comment, annotator skill levels, and biases contribute to the high levels of disagreement observed in toxicity annotations across different datasets and annotator groups?
- **Basis in paper**: [inferred] The paper discusses the high levels of disagreement among annotators, even experts, across all datasets and annotator groups. It also mentions the influence of annotator background and the need to understand subjectivity. However, it does not explicitly identify the specific properties or factors contributing to the disagreement.
- **Why unresolved**: The paper focuses on modeling subjectivity and evaluating the model's ability to mimic annotator perspectives, but it does not delve into the specific properties or factors that lead to disagreement in toxicity annotations.
- **What evidence would resolve it**: Further analysis of the datasets and annotator backgrounds, including factors such as comment properties, annotator skill levels, and biases, could provide insights into the specific properties contributing to disagreement. Additionally, conducting experiments with different annotator groups and comment types could help identify the key factors influencing disagreement.

### Open Question 2
- **Question**: How does the model's performance in mimicking annotator perspectives on toxicity annotations change when trained on larger datasets with more diverse annotator groups and comment types?
- **Basis in paper**: [explicit] The paper discusses the model's ability to mimic annotator perspectives by varying the size of the training data and using the same set of annotators as the test set. It also mentions the need to understand subjectivity and the potential benefits of using subjective annotations as ground truth labels.
- **Why unresolved**: The paper presents experiments with varying training data sizes and test sets, but it does not explore the model's performance with larger datasets and more diverse annotator groups and comment types.
- **What evidence would resolve it**: Conducting experiments with larger datasets that include a wider range of annotator groups and comment types would provide insights into how the model's performance in mimicking annotator perspectives changes. Comparing the model's performance across different dataset sizes and annotator group compositions would help determine the impact of dataset diversity on the model's ability to capture subjective perspectives.

### Open Question 3
- **Question**: What are the potential risks and ethical considerations associated with using subjective annotations as ground truth labels for training models in diverse communities?
- **Basis in paper**: [explicit] The paper mentions the importance of using subjective annotations as ground truth labels and discusses the potential risks of amplifying biases present in the dataset annotations. It also emphasizes the need for responsible dataset use and anonymization of participants.
- **Why unresolved**: The paper briefly touches upon the potential risks and ethical considerations but does not provide a comprehensive analysis of the specific risks and ethical implications associated with using subjective annotations as ground truth labels.
- **What evidence would resolve it**: Conducting a thorough analysis of the potential risks and ethical considerations, including the impact of biases in annotations, the implications for content moderation, and the need for responsible dataset use, would provide a more comprehensive understanding of the ethical implications. Additionally, seeking input from experts in ethics and fairness in machine learning could help identify and address potential risks and ethical concerns.

## Limitations
- The 2023 expert annotator dataset contains only 50 comments, which is a relatively small sample size for drawing broad conclusions about annotator subjectivity.
- The transformation of all datasets to binary toxicity labels (toxic vs. non-toxic) discards potentially valuable information from multi-point scales used in the original annotations.
- The study focuses on English-language content, limiting generalizability to multilingual contexts.

## Confidence
- **High Confidence**: The finding that annotator disagreement exists across all datasets and annotator groups, including expert annotators.
- **Medium Confidence**: The claim that increasing training data size improves model mimicry of annotator perspectives.
- **Medium Confidence**: The assertion that soft-tuning outperforms few-shot prompting for capturing annotator perspectives.

## Next Checks
1. **Generalization Test**: Evaluate the trained models on an independent dataset annotated by the same annotators to verify that the mimicry generalizes beyond the training examples.
2. **Annotation Stability Test**: Re-annotate a subset of the 2023 dataset after a time interval to measure intra-annotator reliability and determine whether the observed subjectivity is stable or context-dependent.
3. **Fairness Impact Assessment**: Test the trained models on comments from different demographic groups to assess whether mimicking annotator perspectives introduces or amplifies biases in toxicity detection.