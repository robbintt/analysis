---
ver: rpa2
title: 'CaT: Balanced Continual Graph Learning with Graph Condensation'
arxiv_id: '2309.09455'
source_url: https://arxiv.org/abs/2309.09455
tags:
- graph
- learning
- tasks
- memory
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the catastrophic forgetting problem in continual
  graph learning (CGL), where models struggle to retain knowledge from historical
  data when trained on new incoming graphs. Existing replay-based methods store sampled
  graphs in a memory bank but suffer from two key issues: (1) difficulty capturing
  historical distribution with limited storage, and (2) imbalanced training due to
  size differences between large incoming graphs and small memory bank graphs.'
---

# CaT: Balanced Continual Graph Learning with Graph Condensation

## Quick Facts
- arXiv ID: 2309.09455
- Source URL: https://arxiv.org/abs/2309.09455
- Reference count: 40
- One-line primary result: State-of-the-art performance in continual graph learning using only 1% storage budget

## Executive Summary
This paper addresses catastrophic forgetting in continual graph learning (CGL), where models struggle to retain knowledge from historical data when trained on new incoming graphs. The proposed Condense and Train (CaT) framework introduces two innovations: Condensed Graph Memory (CGM) that uses graph condensation to create small yet informative synthetic replayed graphs, and Training in Memory (TiM) that updates the model using only the memory bank instead of the full incoming graph. Experiments on four benchmark datasets show that CaT achieves state-of-the-art performance while using only 1% of the storage budget, matching ideal joint training performance.

## Method Summary
The CaT framework consists of two key components: CGM generates synthetic replayed graphs using distribution matching in embedding space via MMD between random GNN encoders, creating small graphs that better capture historical distributions than sampling-based memory banks. TiM is a training scheme that updates the model exclusively with condensed graphs in the memory bank, eliminating the data imbalance problem between large incoming graphs and small memory bank graphs. The framework uses a 2-layer GCN backbone for node classification and processes streaming graph data where each task contains nodes from new classes.

## Key Results
- Achieves state-of-the-art performance in both class-incremental and task-incremental CGL settings
- Matches ideal joint training performance while using only 1% storage budget
- Outperforms baseline replay methods (ER-GNN, SSM) and regularization methods (EWC, MAS) across CoraFull, Arxiv, Reddit, and Products datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph condensation generates small but informative synthetic graphs that better approximate historical data distribution than sampling-based memory banks.
- Mechanism: Distribution matching via MMD in the embedding space of random GNN encoders preserves essential graph characteristics in much smaller synthetic graphs.
- Core assumption: MMD can effectively measure and preserve the statistical properties of original graphs in condensed form.
- Evidence anchors: Abstract states condensation creates "small yet informative synthesised replayed graphs" and section describes "distance between two graphs is measured in the embedding space."
- Break condition: If condensed graphs fail to preserve critical structural relationships, historical task performance degrades.

### Mechanism 2
- Claim: Training only on memory bank eliminates data imbalance between large incoming graphs and small memory bank graphs.
- Mechanism: TiM updates the model exclusively with condensed graphs rather than combining them with full incoming graphs, ensuring balanced training scales.
- Core assumption: Condensed graphs contain sufficient information for effective model updates without requiring full incoming graphs.
- Evidence anchors: Abstract explicitly states "Training in Memory (TiM), which updates the model using only the memory bank" and section confirms "model only updates with the memory bank."
- Break condition: If condensed graphs are too small or information-poor, model underfits on new tasks.

### Mechanism 3
- Claim: CGM and TiM combination achieves SOTA performance at 1% storage budget.
- Mechanism: CGM provides high-quality synthetic graphs while TiM ensures balanced training, enabling effective learning with minimal storage.
- Core assumption: Both CGM's distribution capture and TiM's balanced training are simultaneously necessary for optimal performance.
- Evidence anchors: Abstract claims "CaT achieves state-of-the-art performance... matching the ideal joint training performance while using only 1% of the storage budget."
- Break condition: Failure of either component requires more storage or causes imbalanced training, reducing overall performance.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why models lose performance on historical tasks when learning new ones is fundamental to grasping the problem CaT solves
  - Quick check question: Why does a model trained on task 1 typically perform worse on task 1 after being trained on task 2?

- Concept: Graph neural networks and message passing
  - Why needed here: The framework builds on GNN fundamentals, and understanding how node representations are computed is crucial for grasping condensation and training mechanisms
  - Quick check question: How does a 2-layer GCN compute node embeddings from the adjacency matrix and feature matrix?

- Concept: Distribution matching and Maximum Mean Discrepancy (MMD)
  - Why needed here: CGM relies on distribution matching via MMD to create synthetic graphs that preserve the original graph's statistical properties
  - Quick check question: What does MMD measure between two distributions, and why is it suitable for comparing graph embeddings?

## Architecture Onboarding

- Component map: CGM -> TiM -> Memory Bank -> GNN Backbone
- Critical path:
  1. Incoming graph arrives
  2. CGM condenses graph into synthetic replayed graph
  3. Synthetic graph added to memory bank
  4. Model updated using TiM on memory bank only
  5. Process repeats for next incoming graph
- Design tradeoffs:
  - Storage vs. Performance: Smaller condensed graphs save space but may lose information
  - Computation vs. Accuracy: TiM reduces computation but relies entirely on memory bank quality
  - Encoder choice: Random GNN encoders for condensation vs. trained encoders for classification
- Failure signatures:
  - Performance degradation on historical tasks: Memory bank not capturing distributions well
  - Poor learning on new tasks: Condensed graphs too small or information-poor
  - Unstable training: Imbalance between memory bank and incoming graph scales
- First 3 experiments:
  1. Implement CGM with random GNN encoders and test on CoraFull with budget ratio 0.01
  2. Add TiM scheme to CGM and compare performance against baseline replay methods
  3. Vary budget ratios (0.005, 0.01, 0.05) and measure performance/quality tradeoffs

## Open Questions the Paper Calls Out

- Question: How does CaT perform with even smaller budget ratios (e.g., 0.001 or 0.0005) on CoraFull and other datasets?
- Basis in paper: The paper notes that CoraFull with a 0.01 budget ratio (4-node replayed graph) performs slightly worse than with 0.005 (2-node replayed graph), suggesting further exploration of extremely small budgets could be valuable.
- Why unresolved: The paper only experiments with budget ratios down to 0.005, leaving the performance at even smaller ratios unknown.
- What evidence would resolve it: Experiments with budget ratios of 0.001 and 0.0005 on CoraFull and other datasets, comparing AP and BWT to higher budget ratios.

## Limitations

- Limited to datasets without inter-task edges, which is unrealistic for many real-world applications
- Graph condensation method (distribution matching) may not preserve all critical structural properties
- Implementation details for condensation algorithm are sparse, making exact reproduction difficult

## Confidence

- High confidence: Problem formulation and general framework design are well-established concepts
- Medium confidence: Experimental results showing SOTA performance are plausible given framework design
- Low confidence: Specific claim that distribution matching via random GNN encoders can create synthetic graphs preserving all critical properties requires empirical validation

## Next Checks

1. Implement and validate the MMD-based graph condensation with ablation studies on synthetic graphs to measure information preservation
2. Test the TiM training scheme with varying memory bank sizes to identify minimum effective storage budget for different graph datasets
3. Conduct stress tests by creating scenarios where condensed graphs deliberately fail to capture distributions, observing performance degradation patterns