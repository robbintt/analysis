---
ver: rpa2
title: The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable
  Metrics
arxiv_id: '2310.19792'
source_url: https://arxiv.org/abs/2310.19792
tags:
- metrics
- evaluation
- task
- shared
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Eval4NLP 2023 explores prompting large language models as explainable
  evaluation metrics for machine translation and summarization. The shared task restricts
  models to a fixed list and disallows fine-tuning, focusing on prompting strategies
  and score extraction.
---

# The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics

## Quick Facts
- arXiv ID: 2310.19792
- Source URL: https://arxiv.org/abs/2310.19792
- Reference count: 34
- One-line primary result: Eval4NLP 2023 explores prompting LLMs as explainable evaluation metrics for MT/summarization, achieving results on par with or surpassing recent reference-free metrics despite restrictions.

## Executive Summary
Eval4NLP 2023 introduced a shared task that restricts participants to a fixed list of LLMs without fine-tuning, focusing on prompting strategies and score extraction for explainable evaluation metrics in machine translation and summarization. Nine teams participated, achieving results comparable to or better than recent reference-free metrics like GEMBA and Comet-Kiwi-XXL. The competition fostered inclusivity by allowing small models, with the best small model submissions outperforming large models. The task also explored the explainability of LLM-based metrics, though results in this area were inconclusive due to limited human evaluation.

## Method Summary
The shared task provided participants with a list of allowed LLMs and prohibited fine-tuning to ensure focus on prompting strategies and score extraction methods. Participants could only vary how models were prompted, how scores were extracted, and how models were used in combination. The task included two main tracks: machine translation (MT) evaluation and reference-free summarization evaluation, with a separate track for evaluating the plausibility of explanations generated by LLMs. A new reference-free test set was collected from Wikipedia articles to minimize pre-training overlap. Evaluation metrics included Kendall's tau, Spearman, and Pearson correlations between system-generated scores and human judgments.

## Key Results
- Nine teams participated, achieving results on par with or surpassing recent reference-free metrics like GEMBA and Comet-Kiwi-XXL.
- Best-performing approaches used small models, demonstrating strong performance and efficiency.
- Probability-based score extraction methods showed promise for multilingual tasks due to LLM translation capabilities.
- Explainability track results were inconclusive due to limited human evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricting LLMs and prohibiting fine-tuning shifts focus to prompting strategy and score extraction.
- **Mechanism:** By removing fine-tuning, participants optimize prompting and score extraction to elicit desired metric behaviors from pre-trained models.
- **Core assumption:** Base LLMs have sufficient capability for evaluation tasks, and improvements come primarily through prompt engineering.
- **Evidence anchors:** Abstract states novel competition setting with fixed LLMs and no fine-tuning; section specifies participants may only vary prompting, score extraction, and model combination.
- **Break Condition:** If base LLMs lack sufficient understanding of the evaluation task, prompting alone cannot compensate.

### Mechanism 2
- **Claim:** Smaller models foster inclusivity and can yield competitive results, challenging assumptions about model size superiority.
- **Mechanism:** Smaller models require less computational resources, enabling broader participation and faster experimentation. Results show best small model submissions outperform large models.
- **Core assumption:** Model size is not the sole determinant of evaluation quality; prompt quality and score extraction methods are equally important.
- **Evidence anchors:** Abstract notes best-performing approaches use small models; section highlights how smaller models foster inclusiveness.
- **Break Condition:** If evaluation tasks require deep reasoning or extensive world knowledge that smaller models lack, prompting cannot bridge the gap.

### Mechanism 3
- **Claim:** Probability-based score extraction outperforms output-based methods for multilingual tasks due to LLM translation capabilities.
- **Mechanism:** Probability-based approaches calculate likelihood of paraphrases/translations generated by LLMs, leveraging strong translation capabilities for better evaluation scores.
- **Core assumption:** Translation probability output is a more reliable signal for translation quality than direct assessment in multilingual contexts.
- **Evidence anchors:** Section discusses HIT-MI&T Lab's competitive performance using probability-based approach with smaller model.
- **Break Condition:** If LLM's translation capabilities are not significantly better than evaluation capabilities, additional computational cost may not be justified.

## Foundational Learning

- **Concept:** Prompt engineering and score extraction techniques for LLMs
  - Why needed here: Shared task restricts model choice and prohibits fine-tuning, making prompt engineering and score extraction primary levers for improving evaluation metric performance.
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting, and when might each be most effective?

- **Concept:** Evaluation metrics for natural language generation (NLG) tasks
  - Why needed here: Understanding existing evaluation metrics (BLEU, ROUGE, BERTScore) provides context for innovations and challenges addressed by shared task.
  - Quick check question: What are the key differences between reference-based and reference-free evaluation metrics, and what are the trade-offs of each approach?

- **Concept:** Machine translation (MT) and summarization evaluation methodologies
  - Why needed here: Shared task focuses on these two NLG tasks, requiring familiarity with their unique evaluation challenges and existing metrics.
  - Quick check question: What are the main challenges in evaluating machine translation quality, and how do metrics like COMET and GEMBA address these challenges?

## Architecture Onboarding

- **Component Map:** Input text -> Prompt Generation -> LLM Inference -> Score Extraction -> (Optional) Aggregation
- **Critical Path:** 1. Input text preparation 2. Prompt generation based on task and desired evaluation criteria 3. LLM inference with generated prompt 4. Score extraction from LLM output 5. (Optional) Aggregation of scores
- **Design Tradeoffs:**
  - Prompt complexity vs. computational efficiency: More complex prompts may yield better results but require more tokens and computation
  - Output-based vs. probability-based score extraction: Output-based methods are generally faster, while probability-based methods may be more accurate for certain tasks
  - Single model vs. ensemble approaches: Ensembling can improve robustness but increases computational cost
- **Failure Signatures:**
  - Low correlation between metric scores and human judgments
  - Inconsistent scores across different prompts or examples
  - Excessive computational cost for real-time evaluation
  - Inability to handle specific linguistic phenomena or error types
- **First 3 Experiments:**
  1. Implement a simple zero-shot prompt for the target task and evaluate its correlation with human judgments
  2. Experiment with adding demonstrations to the prompt (few-shot) and measure the impact on performance
  3. Compare output-based and probability-based score extraction methods for the target task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do probability-based prompting strategies for evaluation metrics compare to output-based strategies in terms of computational efficiency and correlation to human judgments?
- **Basis in paper:** [explicit] The paper discusses probability-based approaches (e.g., HIT-MI&T Lab) and output-based approaches (e.g., DSBA, iML) and their performance on the test set, noting that the best-performing systems use different strategies for MT and summarization.
- **Why unresolved:** The paper does not provide a detailed quantitative comparison of the computational costs (e.g., inference time, memory usage) of probability-based versus output-based approaches. It only mentions that probability-based approaches are more computationally intensive.
- **What evidence would resolve it:** A systematic comparison of the computational costs and correlation scores of probability-based and output-based approaches across multiple datasets and model sizes would resolve this question.

### Open Question 2
- **Question:** To what extent does the performance of LLM-based evaluation metrics depend on the specific dataset used for evaluation, and how can this dependency be mitigated?
- **Basis in paper:** [inferred] The paper notes that HIT-MI&T Lab's strong performance on the test set does not translate to the dev set, suggesting that dataset characteristics influence performance. It also mentions the use of Wikipedia articles created after 15.07.2023 to minimize pre-training overlap, but acknowledges limitations of this approach.
- **Why unresolved:** The paper does not conduct a systematic analysis of how different dataset characteristics (e.g., domain, language pair, annotation scheme) affect the performance of LLM-based evaluation metrics. It also does not explore strategies to mitigate this dependency.
- **What evidence would resolve it:** Experiments evaluating LLM-based metrics on multiple datasets with varying characteristics and analyzing the correlation between dataset features and performance would resolve this question.

### Open Question 3
- **Question:** How can the explainability of LLM-based evaluation metrics be improved to provide more meaningful and actionable feedback to users?
- **Basis in paper:** [explicit] The paper includes a track on evaluating the plausibility of explanations generated by LLMs, but the results are inconclusive due to the small scale of the human evaluation. It notes that many explanations tend to be vague and do not provide specific insights into the evaluation process.
- **Why unresolved:** The paper does not explore alternative approaches to generating explanations, such as using different prompting strategies or incorporating external knowledge bases. It also does not investigate how to evaluate the quality of explanations beyond plausibility.
- **What evidence would resolve it:** Experiments comparing different explanation generation approaches and evaluating their quality using metrics beyond plausibility (e.g., faithfulness, specificity) would resolve this question.

## Limitations

- Key specifics of top-performing team approaches remain unknown, limiting detailed understanding of successful strategies.
- Direct evidence for certain mechanism claims (e.g., probability-based method superiority) is weak in the corpus.
- Computational constraints and resource requirements for reproducing results may be prohibitive for some researchers.

## Confidence

- **High confidence:** The shared task's focus on prompting strategies and score extraction as primary optimization levers is clearly supported by the abstract and section excerpts.
- **Medium confidence:** The claim about small models achieving competitive results is supported by results mentions but lacks direct comparative evidence in the corpus.
- **Low confidence:** The superiority of probability-based methods for multilingual tasks is mentioned but not strongly supported by corpus evidence.

## Next Checks

1. Examine the specific prompts and score extraction methods used by top-performing teams to understand the key differentiators in their approaches.
2. Conduct controlled experiments comparing output-based and probability-based score extraction methods on multilingual tasks to validate the claimed performance differences.
3. Analyze the computational efficiency and resource requirements of different model sizes and prompting strategies to assess the practical implications of the "little giants" finding.