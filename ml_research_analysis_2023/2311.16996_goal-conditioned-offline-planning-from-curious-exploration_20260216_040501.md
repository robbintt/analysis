---
ver: rpa2
title: Goal-conditioned Offline Planning from Curious Exploration
arxiv_id: '2311.16996'
source_url: https://arxiv.org/abs/2311.16996
tags:
- value
- learning
- exploration
- planning
- model-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies the problem of learning goal-conditioned behavior\
  \ from unsupervised exploration, without further environment interaction. The key\
  \ insight is that learned value functions contain estimation artifacts\u2014specifically,\
  \ states that appear as local optima despite not being optimal globally\u2014which\
  \ prevent standard RL methods from achieving goals."
---

# Goal-conditioned Offline Planning from Curious Exploration

## Quick Facts
- arXiv ID: 2311.16996
- Source URL: https://arxiv.org/abs/2311.16996
- Authors: 
- Reference count: 40
- Primary result: First fully offline method for achieving goals after curious exploration phase

## Executive Summary
This paper addresses the challenge of learning goal-conditioned behavior from unsupervised exploration without further environment interaction. The authors identify that value functions learned from curiosity-driven exploration contain estimation artifacts—local optima that prevent standard RL methods from achieving goals. They propose a novel approach combining model-based planning with graph-based value aggregation to correct both local and global artifacts, achieving significant improvements in zero-shot goal-reaching performance across four diverse simulated environments.

## Method Summary
The method consists of two phases: an intrinsic phase where curiosity-driven exploration collects 200k transitions using an ensemble dynamics model with intrinsic rewards from model disagreement, and an extrinsic phase where a goal-conditioned value function is trained with TD3 on relabeled exploration data. At inference, model-based planning with zero-order trajectory optimization (iCEM) is performed using the learned dynamics model and value function, optionally enhanced with graph-based value aggregation to correct global estimation artifacts by replacing long-horizon estimates with aggregated short-horizon estimates computed via shortest path search.

## Key Results
- First fully offline method achieving goals after curious exploration phase
- Success rates ranging from 30-80% across four diverse simulated environments
- Significant improvements over conventional approaches in zero-shot goal-reaching
- Combines model-based planning with graph-based value aggregation to address both local and global estimation artifacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-based planning with zero-order trajectory optimization is more robust to local estimation artifacts than actor networks.
- Mechanism: Actor networks optimize one-step lookaheads of the value function, making them vulnerable to T-local optima. Model-based planning with longer horizons can escape these local optima by considering multiple steps ahead.
- Core assumption: The dynamics model is sufficiently accurate to enable effective planning.
- Evidence anchors:
  - [abstract]: "we propose to combine model-based planning over learned value landscapes with a graph-based value aggregation scheme"
  - [section]: "model-based planning is robust to T −local optima of small depth" (Section 6.1)
  - [corpus]: "Planning with learned models" (corpus paper 244776) - weak direct evidence for robustness claim

### Mechanism 2
- Claim: Graph-based value aggregation corrects global estimation artifacts by replacing long-horizon value estimates with aggregated short-horizon estimates.
- Mechanism: A graph is constructed from the exploration dataset, connecting states with edges weighted by learned value estimates. Edges encoding long-horizon estimates below a threshold are pruned, and value estimates are computed by minimizing products along paths.
- Core assumption: Short-horizon value estimates are more accurate than long-horizon ones.
- Evidence anchors:
  - [abstract]: "combine model-based planning over learned value landscapes with a graph-based value aggregation scheme"
  - [section]: "grounding the value function in states observed during exploration, such that long-horizon value estimates are computed by aggregating short-horizon (i.e., local) estimates" (Section 6.2)
  - [corpus]: "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning" (corpus paper 64745) - supports graph-based planning concept

### Mechanism 3
- Claim: The combination of model-based planning and graph-based aggregation addresses both local and global estimation artifacts.
- Mechanism: Model-based planning handles local artifacts through multi-step lookahead, while graph aggregation corrects global artifacts by replacing unreliable long-horizon estimates. Together they provide robust performance across diverse environments.
- Core assumption: The two methods complement each other in addressing different scales of estimation errors.
- Evidence anchors:
  - [abstract]: "This combination can correct both local and global artifacts, obtaining significant improvements in zero-shot goal-reaching performance across diverse simulated environments."
  - [section]: "Combining these two components can address estimation artifacts both locally and globally, and consistently improves goal-reaching performance" (Section 6.3)

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and goal-conditioned RL
  - Why needed here: The entire framework operates within the MDP formalism, and goal-conditioned RL provides the task specification mechanism.
  - Quick check question: What distinguishes goal-conditioned RL from standard RL in terms of reward function formulation?

- Concept: Value function estimation and Bellman optimality
  - Why needed here: The method relies on learning and optimizing value functions, and understanding T-local optima requires knowledge of Bellman optimality conditions.
  - Quick check question: How does Bellman optimality ensure that optimal value functions have no T-local optima?

- Concept: Model-based planning and trajectory optimization
  - Why needed here: The method uses model-based planning with zero-order trajectory optimization as a core component for action selection.
  - Quick check question: What is the key difference between model-based planning and actor-critic methods in terms of optimization horizon?

## Architecture Onboarding

- Component map:
  Dynamics model -> Value function -> Graph construction -> Planning module -> Aggregation module

- Critical path:
  1. Collect exploration data with curiosity-driven exploration
  2. Train dynamics model on exploration data
  3. Train value function with TD3 on relabeled exploration data
  4. At inference: build graph from value function and dataset
  5. Plan using model-based planning with aggregated values

- Design tradeoffs:
  - Model-based planning vs actor networks: Planning is more robust to local artifacts but computationally expensive
  - Graph construction: More vertices improve coverage but increase computational cost
  - Value aggregation: More aggressive pruning removes more artifacts but risks disconnecting the graph

- Failure signatures:
  - Model exploitation: Planning fails due to inaccurate dynamics model
  - Graph disconnection: Pruning threshold too high, no path exists between states
  - Local trapping: Value function still contains T-local optima that planning cannot escape

- First 3 experiments:
  1. Verify that TD3 trained on exploration data can learn reasonable value estimates by testing actor network performance
  2. Test model-based planning with horizon H=1 vs H=10 to observe robustness to local artifacts
  3. Implement graph-based aggregation with varying vertex counts to find optimal balance between accuracy and computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of graph-based value aggregation scale with graph size and vertex sampling strategy?
- Basis in paper: [explicit] The paper mentions that graph size is limited by shortest path search complexity and uses default value of 1000 vertices, but notes that 100 vertices achieved sufficient coverage in pinpad environment.
- Why unresolved: The paper doesn't provide systematic ablation studies varying graph size or vertex sampling strategies to understand the tradeoff between computational cost and performance.
- What evidence would resolve it: Empirical results comparing success rates across different graph sizes (e.g., 100, 500, 1000, 2000 vertices) and different vertex sampling strategies (uniform vs. density-aware) across all environments.

### Open Question 2
- Question: Can the value correction be performed directly during training rather than at inference time?
- Basis in paper: [inferred] The discussion section mentions that finetuning the neural value estimator with CQL-like penalties on T-local optima values was attempted but unsuccessful, suggesting this remains an open challenge.
- Why unresolved: The paper tried TD finetuning with penalties on T-local optima values but found it sensitive to hyperparameters and prone to overly pessimistic estimates.
- What evidence would resolve it: Successful implementation of a value correction method that updates the value function during training, showing improved performance without the computational overhead of post-hoc correction.

### Open Question 3
- Question: How robust is the method to varying amounts of exploration data and different data distributions?
- Basis in paper: [explicit] The paper evaluates performance on datasets of different sizes (50k, 100k, 200k samples) and shows that mixing curious exploration data with expert or random uniform data yields different results, with long-horizon environments being more sensitive to data quality.
- Why unresolved: While the paper shows some sensitivity to data size and distribution, it doesn't systematically explore the minimum amount of data required or the impact of different data quality levels on performance.
- What evidence would resolve it: Systematic evaluation of performance across a wider range of data sizes and distributions, including the minimum data threshold for each environment and the impact of data quality on long-horizon vs. short-horizon tasks.

## Limitations

- Empirical validation limited to four simulated environments, raising questions about generalization to more complex tasks
- Implementation-specific parameters for graph construction (vertex sampling density, pruning thresholds) are underspecified
- Claim about TD3's overparameterized architecture learning fewer T-local optima is supported by limited empirical evidence

## Confidence

- **High Confidence**: The existence of estimation artifacts in value functions learned from exploration data is well-established. The observation that actor networks are vulnerable to local optima due to their optimization horizon is theoretically sound.
- **Medium Confidence**: The specific claim that model-based planning with zero-order trajectory optimization is more robust to local artifacts than actor networks is supported by reasoning but would benefit from more systematic ablation studies across different planning horizons and environment complexities.
- **Medium Confidence**: The graph-based aggregation approach for correcting global artifacts is conceptually sound, but the practical implementation details (particularly vertex sampling and pruning criteria) are underspecified, making exact reproduction challenging.

## Next Checks

1. **Systematic Ablation of Planning Horizons**: Conduct experiments varying the planning horizon H in the iCEM planner (e.g., H=1, H=5, H=10) across all environments to quantify the relationship between planning depth and robustness to local artifacts.

2. **Graph Connectivity Analysis**: Implement a systematic study of how graph construction parameters (vertex count, density estimation method, pruning threshold) affect both the success rate and computational efficiency. Measure the percentage of disconnected graphs and the average shortest path lengths across different parameter settings.

3. **Cross-Environment Generalization Test**: Apply the method to a significantly different environment class (e.g., sparse-reward tasks, continuous control with contacts) to evaluate whether the combined approach maintains its advantage over baselines.