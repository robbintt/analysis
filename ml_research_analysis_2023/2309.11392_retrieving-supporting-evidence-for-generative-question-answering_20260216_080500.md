---
ver: rpa2
title: Retrieving Supporting Evidence for Generative Question Answering
arxiv_id: '2309.11392'
source_url: https://arxiv.org/abs/2309.11392
tags:
- answer
- question
- retrieved
- generated
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can generate plausible but incorrect answers
  (hallucinations), requiring verification against external sources. This paper investigates
  whether LLMs can automatically detect and correct their own hallucinations using
  retrieved evidence.
---

# Retrieving Supporting Evidence for Generative Question Answering

## Quick Facts
- arXiv ID: 2309.11392
- Source URL: https://arxiv.org/abs/2309.11392
- Reference count: 40
- Key outcome: LLMs can verify their own generated answers with over 80% accuracy when provided with supporting evidence from retrieved passages

## Executive Summary
This paper addresses the critical problem of hallucinations in large language models by proposing a verification system that automatically detects and corrects fabricated content. The authors investigate whether LLMs can validate their own generated answers by retrieving supporting evidence from a corpus and checking for consistency. Through two experiments - validating entire answers and decomposing them into factual statements - they demonstrate that LLMs achieve over 80% accuracy in verification when provided with relevant supporting material. While the approach significantly reduces hallucinations, it cannot completely eliminate them due to the LLM's occasional confusion between generated content and retrieved evidence, particularly in cases of partial support or temporal mismatches.

## Method Summary
The method involves two experiments using MS MARCO corpus and gpt-3.5-turbo. In Experiment 1, the LLM generates an answer to a question, then retrieves passages and classifies whether they support, contradict, or are unrelated to the answer. In Experiment 2, the LLM decomposes the answer into individual factual statements and validates each statement separately against retrieved evidence. The retrieval pipeline uses both BM25 and neural methods (SPLADE+ANCE+MonoT5+DuoT5) to obtain supporting passages. The LLM then evaluates the relationship between its generated content and retrieved evidence using a classification system.

## Key Results
- LLMs achieve over 80% accuracy in verifying their generated answers against supporting evidence
- When excluding "Not Related" cases, LLMs assert that retrieved material supports their answers for about 93% of questions
- After excluding "Neither" cases, LLMs believe retrieved material supports about 85% of their factual claims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can detect hallucinations when provided with supporting evidence retrieved from a corpus
- Mechanism: The LLM compares its generated answer against retrieved passages and identifies whether the evidence supports or contradicts the answer
- Core assumption: The LLM can accurately judge the relationship between its generated answer and retrieved evidence
- Evidence anchors:
  - [abstract] "With an accuracy of over 80%, we find that an LLM is capable of verifying its generated answer when a corpus of supporting material is provided."
  - [section 3.2.2] "Overall, after excluding the 'Not Related', the LLM asserts that the retrieved material supports its own answer for about 93% of questions."
- Break condition: The LLM confuses its own generated answer with the retrieved evidence, leading to false positives where it incorrectly validates hallucinated content

### Mechanism 2
- Claim: Breaking the generated answer into factual statements improves verification accuracy
- Mechanism: The LLM decomposes the answer into individual claims, retrieves evidence for each claim separately, and validates them independently
- Core assumption: Individual factual claims can be more accurately validated than the answer as a whole
- Evidence anchors:
  - [section 4.2] "Overall, after excluding the 'Neither' cases where the retrieved passage cannot be used to determine whether the factual claim hallucinates or not, the LLM believes the retrieved material supports about 85% of its claims."
  - [abstract] "In the second experiment, we consider the generated answer at a more granular level, prompting the LLM to extract a list of factual statements from the answer and verifying each statement separately."
- Break condition: The LLM extracts overly detailed or repetitive factual statements that don't add meaningful verification value

### Mechanism 3
- Claim: Retrieval quality affects verification accuracy
- Mechanism: Different retrieval methods (BM25, neural, qrel) produce evidence of varying relevance, impacting the LLM's ability to verify its answer
- Core assumption: Higher relevance of retrieved evidence leads to better verification accuracy
- Evidence anchors:
  - [section 3.1] "The second retrieval method we adopt for our experiments is a more modern neural retrieval method that emphasizes the quality of the retrieved passages over retrieval efficiency."
  - [section 3.2.2] "We observed an average accuracy of about 80% for samples that the LLM classified as 'Yes' and 90% for samples that the LLM classified as 'No'."
- Break condition: The retrieval method produces irrelevant or contradictory evidence, leading to incorrect verification decisions

## Foundational Learning

- Concept: Retrieval-augmented generation
  - Why needed here: Understanding how retrieval complements LLM generation is crucial for implementing the verification process
  - Quick check question: What is the difference between retrieval-augmented generation and post-generation verification?

- Concept: Factual decomposition
  - Why needed here: Breaking down complex answers into verifiable factual statements is key to the second experiment's approach
  - Quick check question: How would you extract factual statements from a multi-sentence answer?

- Concept: Prompt engineering
  - Why needed here: Crafting effective prompts is essential for getting the LLM to perform verification tasks accurately
  - Quick check question: What are the key elements of a good verification prompt?

## Architecture Onboarding

- Component map:
  Question input -> LLM generation -> Answer retrieval -> LLM verification -> Output
  Optional: Factual decomposition -> Individual claim retrieval -> Individual claim verification -> Output composition

- Critical path:
  1. Generate answer with LLM
  2. Retrieve supporting evidence
  3. Verify answer against evidence
  4. Output verification result

- Design tradeoffs:
  - Simple verification vs. granular factual decomposition
  - Retrieval method choice (BM25 vs. neural vs. qrel)
  - Prompt complexity vs. verification accuracy

- Failure signatures:
  - LLM confuses generated answer with retrieved evidence
  - Retrieval produces irrelevant or contradictory passages
  - Factual decomposition is too detailed or too coarse

- First 3 experiments:
  1. Verify entire generated answer against single retrieved passage
  2. Verify factual statements extracted from answer against individual retrieved passages
  3. Compare verification accuracy across different retrieval methods (BM25, neural, qrel)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based hallucination detection compare to traditional fact-checking methods when applied to open-domain question answering?
- Basis in paper: [explicit] The paper discusses using LLMs to self-detect hallucinations with retrieval-augmented evidence, achieving 80% accuracy, but notes this is insufficient to eliminate hallucinations entirely
- Why unresolved: The paper does not compare LLM-based detection to traditional fact-checking approaches or benchmarks
- What evidence would resolve it: Head-to-head comparison studies between LLM-based hallucination detection and traditional fact-checking methods on the same datasets, measuring precision, recall, and false positive/negative rates

### Open Question 2
- Question: What specific prompt engineering techniques or fine-tuning approaches could improve the LLM's ability to distinguish between supported and unsupported claims when evidence partially addresses the answer?
- Basis in paper: [explicit] The paper notes that LLMs often misclassify partially supported answers and suggests better prompt engineering or fine-tuning might help, but does not explore these solutions
- Why unresolved: The paper identifies the problem but does not test or evaluate potential solutions through prompt engineering or model fine-tuning
- What evidence would resolve it: Experiments testing different prompt formulations, few-shot examples, or fine-tuned models specifically for the task of evaluating evidence support for claims

### Open Question 3
- Question: How does the temporal relationship between generated answers and retrieved evidence affect the LLM's ability to correctly validate claims, and can this be systematically improved?
- Basis in paper: [explicit] The paper identifies temporal mismatches as a significant source of error, with LLMs struggling to infer relationships across different time points
- Why unresolved: While the problem is identified, the paper does not explore solutions or quantify the impact of temporal mismatches on detection accuracy
- What evidence would resolve it: Analysis of detection accuracy across different temporal relationships, and experiments testing approaches like temporal reasoning modules or explicit temporal context in prompts

## Limitations
- Verification accuracy drops significantly when evidence only partially supports the answer or when temporal mismatches exist between answer and evidence
- The LLM occasionally fabricates evidence to support statements or misses obvious connections in retrieved passages
- The verification process cannot completely eliminate hallucinations, achieving only 80%+ accuracy

## Confidence

- **High Confidence**: The LLM can detect hallucinations in most cases when provided with relevant supporting material, as evidenced by the 80%+ accuracy rates across experiments
- **Medium Confidence**: Breaking answers into factual statements improves verification granularity, though the paper acknowledges this approach still has limitations in handling complex or nuanced claims
- **Low Confidence**: The verification process can completely eliminate hallucinations, as the paper explicitly states that the LLM still makes errors and cannot fully prevent fabrication of evidence

## Next Checks

1. **Prompt Structure Validation**: Test whether the exact prompt structure for factual statement extraction significantly impacts verification accuracy by running controlled experiments with variations in prompt phrasing and format

2. **Temporal Consistency Testing**: Evaluate the verification system's performance on questions with temporal dependencies by creating a test set where answers and evidence have explicit time-based relationships that should affect verification outcomes

3. **Evidence Fabrication Detection**: Implement a benchmark test to measure how often the LLM fabricates supporting evidence when verification fails, using human annotation to identify cases where the LLM invents connections between answer and retrieved passages