---
ver: rpa2
title: Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding
  for MER 2023
arxiv_id: '2309.07925'
source_url: https://arxiv.org/abs/2309.07925
tags:
- emotion
- feature
- fusion
- features
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hierarchical audio-visual information fusion
  framework for recognizing both discrete and dimensional emotions. The method extracts
  deep features from pre-trained foundation models, including HUBERT for audio and
  ResNet/FER and MANet for visual modalities.
---

# Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023

## Quick Facts
- arXiv ID: 2309.07925
- Source URL: https://arxiv.org/abs/2309.07925
- Reference count: 40
- Third place on MER-MULTI sub-challenge with combined metric score of 0.6846

## Executive Summary
This paper presents a hierarchical audio-visual information fusion framework for recognizing both discrete and dimensional emotions in the MER 2023 challenge. The method extracts deep features from pre-trained foundation models including HUBERT for audio and ResNet/FER and MANet for visual modalities, then fuses them using attention-guided mechanisms. A joint decoding module leverages the correlation between discrete emotion categories and valence values through a multi-task learning approach with uncertainty weighting. The framework achieves state-of-the-art performance, ranking third on the MER-MULTI sub-challenge with a 16.6% improvement over unimodal systems.

## Method Summary
The framework extracts deep features from multiple layers of pre-trained models (HUBERT for audio, ResNet-FER and MANet for visual) and fuses them using three different attention-guided feature gathering structures. A joint decoding module simultaneously handles emotion classification and valence regression, leveraging their correlation through a dedicated mapping branch. The model employs a multi-task loss based on uncertainty to balance the two tasks during optimization. Decision-level fusion combines predictions from all three fusion structures to produce final outputs. The approach uses frozen pre-trained models for feature extraction rather than fine-tuning, making it computationally efficient while maintaining high performance.

## Key Results
- Ranked third on MER-MULTI sub-challenge with combined metric score of 0.6846
- Achieved 16.6% improvement over unimodal systems
- Demonstrated effectiveness of hierarchical feature fusion from multiple pre-trained model layers
- Validated the correlation between discrete emotions and valence through joint decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint decoding of discrete and dimensional emotions leverages their correlation to improve both tasks simultaneously.
- Mechanism: The model uses the predicted discrete emotion to condition the valence prediction through a dedicated branch that maps discrete emotions to dimensional values. This creates a feedback loop where the discrete emotion prediction informs the valence regression, and vice versa.
- Core assumption: There exists a stable and learnable mapping between discrete emotion categories and valence values.
- Evidence anchors:
  - [abstract] "A joint decoding structure for emotion classification and valence regression in the decoding stage."
  - [section 2.2] "We found there exists a relatively stable distribution between discrete and dimensional emotions... the discrete emotions can determine the dimensional emotions according to the following formula"
- Break condition: If the correlation between discrete and dimensional emotions is weak or non-linear in ways the model cannot capture, joint decoding may not provide benefits and could even introduce noise.

### Mechanism 2
- Claim: Multi-task loss with uncertainty weighting allows the model to balance classification and regression tasks based on task-specific noise levels.
- Mechanism: The model uses learnable uncertainty weights (δ1, δ2) to scale the cross-entropy and MSE losses dynamically. Tasks with higher uncertainty are downweighted automatically during training.
- Core assumption: Different tasks have different noise levels and the model can learn optimal weighting through the uncertainty parameters.
- Evidence anchors:
  - [abstract] "A multi-task loss based on uncertainty is also designed to optimize the whole process."
  - [section 2.2] "Inspired by the Uncertainty loss [6], we introduce uncertainty loss weighting to L_e and L_v"
- Break condition: If uncertainty estimation fails to converge or if one task dominates regardless of uncertainty, the multi-task learning may not provide balanced improvements.

### Mechanism 3
- Claim: Hierarchical feature fusion from different layers of pre-trained models captures complementary information at multiple abstraction levels.
- Mechanism: The framework extracts features from multiple layers of HUBERT (audio) and ResNet/MANet (visual), then fuses them using attention-guided mechanisms. Lower layers capture acoustic details while higher layers capture semantic information.
- Core assumption: Different layers of pre-trained models encode complementary information relevant to emotion recognition.
- Evidence anchors:
  - [section 2.1] "Previous research has indicated that different layers in pre-trained speech model HUBERT... capture audio hidden states with distinctive characteristics"
  - [section 3.3] "Among various acoustic features, HL(18), HL(19), HL(20) outperform others, confirming that the mid-level features from HUBERT-large model is more suitable for emotion recognition"
- Break condition: If feature complementarity is minimal or if the attention mechanism cannot effectively weigh the features, hierarchical fusion may not provide significant gains over single-layer features.

## Foundational Learning

- Concept: Attention mechanisms and their role in feature fusion
  - Why needed here: The framework uses attention-guided feature gathering (AFG) to weigh and combine features from different sources and layers.
  - Quick check question: How does the attention mechanism in AFG differ from standard self-attention?

- Concept: Multi-task learning and uncertainty weighting
  - Why needed here: The model jointly optimizes emotion classification and valence regression using uncertainty-based loss weighting.
  - Quick check question: What advantage does uncertainty weighting provide over fixed weighting schemes in multi-task learning?

- Concept: Foundation models and layer-wise feature extraction
  - Why needed here: The framework leverages pre-trained models (HUBERT, ResNet, MANet) and extracts features from multiple layers for emotion recognition.
  - Quick check question: Why might mid-level features from HUBERT be more effective for emotion recognition than very low or very high-level features?

## Architecture Onboarding

- Component map: HUBERT layers (HL(18), HL(19), HL(20)) -> AFG modules -> Joint decoding -> Multi-task loss -> Decision fusion
- Critical path: Feature extraction → AFG fusion → Joint decoding → Multi-task optimization → Decision fusion
- Design tradeoffs:
  - Using multiple pre-trained models vs. fine-tuning them: The framework uses frozen pre-trained models for feature extraction, which is computationally efficient but may miss task-specific adaptations.
  - Three different fusion strategies vs. one optimal strategy: Multiple strategies provide robustness but increase complexity and computational cost.
- Failure signatures:
  - Performance degradation when removing joint decoding suggests the discrete-valence correlation is crucial.
  - Performance degradation when removing uncertainty weighting suggests multi-task balance is important.
  - Performance degradation when using single-layer features suggests hierarchical fusion is beneficial.
- First 3 experiments:
  1. Train unimodal systems (audio-only, visual-only) to establish baseline performance and identify which modality contributes more.
  2. Train the three different fusion strategies independently to compare their effectiveness and understand their complementary strengths.
  3. Test the impact of joint decoding by comparing with separate emotion and valence branches to validate the correlation hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed framework change when incorporating text modality features, given that previous research has shown these features to be underperforming compared to audio and visual features?
- Basis in paper: [explicit] The paper mentions that text features are "proved to be underperforming comparing with acoustic and visual features in this task" but does not explore whether incorporating them with different strategies could improve performance.
- Why unresolved: The paper deliberately excludes text features without exploring potential strategies for making them more effective in the multimodal fusion framework.
- What evidence would resolve it: Comparative experiments showing performance with and without text features using various fusion strategies and feature engineering techniques.

### Open Question 2
- Question: What is the optimal layer combination for feature extraction from pre-trained models across different modalities, and how does this vary with different emotion recognition tasks?
- Basis in paper: [explicit] The paper notes that "different layers in pre-trained speech model HUBURT capture audio hidden states with distinctive characteristics" and that "HL(18), HL(19), HL(20) outperform others" for audio, but does not systematically explore optimal combinations for different tasks.
- Why unresolved: The paper only explores a limited set of layers for each modality and does not investigate how optimal layer selection might vary across different emotion recognition datasets or tasks.
- What evidence would resolve it: Systematic ablation studies testing various layer combinations across multiple emotion recognition datasets to identify optimal configurations.

### Open Question 3
- Question: How does the proposed hierarchical fusion approach compare to other fusion strategies (e.g., early, late, or hybrid fusion) in terms of computational efficiency and performance trade-offs?
- Basis in paper: [inferred] The paper introduces three specific fusion strategies based on attention-guided feature gathering but does not compare them to alternative fusion approaches or analyze their computational complexity.
- Why unresolved: The paper focuses on the proposed fusion strategies without benchmarking against other established fusion methodologies or providing computational complexity analysis.
- What evidence would resolve it: Comparative experiments measuring both performance metrics and computational resources (e.g., inference time, memory usage) across different fusion strategies on the same dataset.

## Limitations
- Proprietary MER 2023 dataset limits independent verification of results
- Unclear implementation details for attention-guided feature gathering modules
- Only tested on a single dataset without cross-dataset validation
- No computational complexity analysis provided

## Confidence

- High confidence in the multi-task uncertainty weighting mechanism (well-established approach with clear implementation details)
- Medium confidence in the joint decoding correlation hypothesis (theoretically sound but dataset-specific validation needed)
- Medium confidence in hierarchical feature fusion benefits (supported by ablation studies but architectural details unclear)
- Low confidence in exact reproducibility due to unspecified implementation details

## Next Checks

1. **Ablation on correlation**: Systematically test the joint decoding module by training separate emotion and valence branches to quantify the actual performance gain from leveraging discrete-valence correlation.

2. **Attention mechanism analysis**: Visualize and analyze the attention weights learned in the AFG modules to verify they are capturing meaningful cross-modal relationships rather than random patterns.

3. **Layer-wise feature importance**: Conduct controlled experiments varying which HUBERT layers are used (HL(18), HL(19), HL(20)) to confirm that mid-level features provide optimal performance and understand the impact of each layer's contribution.