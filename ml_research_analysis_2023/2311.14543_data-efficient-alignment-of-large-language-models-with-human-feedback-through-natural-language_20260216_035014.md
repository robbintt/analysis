---
ver: rpa2
title: Data-Efficient Alignment of Large Language Models with Human Feedback Through
  Natural Language
arxiv_id: '2311.14543'
source_url: https://arxiv.org/abs/2311.14543
tags:
- response
- responses
- critique
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a data-efficient approach called Critique
  and Revise (CnR) for aligning large language models (LLMs) with human preferences
  using natural language feedback. The core idea is to fine-tune an open-source LLM,
  such as Falcon-40B-Instruct, on a small dataset of human-written critiques and revisions
  of model responses.
---

# Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language

## Quick Facts
- arXiv ID: 2311.14543
- Source URL: https://arxiv.org/abs/2311.14543
- Reference count: 23
- Win rate over original responses: 56.6% after one iteration, 65.9% after five iterations

## Executive Summary
This paper introduces Critique and Revise (CnR), a data-efficient approach for aligning large language models with human preferences using natural language feedback. The method fine-tunes open-source LLMs on small datasets of human-written critiques and revisions of model responses, enabling the model to generate critiques identifying positive and negative aspects of responses and then revise them accordingly. Experiments demonstrate that CnR can significantly improve responses from strong models like ChatGPT with only 1000 or fewer training samples, achieving 56.6% win rate after one iteration and 65.9% after five iterations. The approach offers an alternative to reinforcement learning from human feedback (RLHF) while avoiding its complexity and instability.

## Method Summary
CnR works by fine-tuning an open-source LLM on a small dataset of human-written critiques and revisions. The model learns to critique responses by identifying positive and negative aspects, then revise them accordingly. Three different fine-tuning formulations were explored: p→i→c→r (full CnR), p→i→r (instruction + revision), and p→r (revision only). The approach achieves strong data efficiency, requiring fewer than 1000 samples for effective training. The fine-tuned CnR model can then be used to improve responses from other models through iterative critique and revision cycles, with improvements persisting across multiple iterations.

## Key Results
- 56.6% win rate over original ChatGPT responses after one iteration of revision
- 65.9% win rate after five iterations of revision
- Strong data efficiency with performance gains plateauing after ~800 training samples
- Larger base models and higher quality instruction-tuning data lead to better revision quality

## Why This Works (Mechanism)

### Mechanism 1
CnR model can improve responses even from strong models like ChatGPT through iterative revision cycles. By training on natural language critiques and revisions, the model learns to identify and correct specific flaws in responses, enabling continuous improvement over multiple iterations. Critiques provide sufficient granularity to guide meaningful revisions that improve response quality.

### Mechanism 2
CnR achieves high data efficiency with fewer than 1000 samples. Natural language feedback provides richer, more specific information than pairwise ranking, allowing the model to learn alignment patterns more efficiently from limited examples. Natural language critiques contain more actionable information per sample than simple preference labels.

### Mechanism 3
Larger base models and higher quality instruction-tuning data improve revision quality. Larger pre-trained models have more capacity to learn complex critique-generation and revision patterns, while better instruction-tuning provides stronger foundational understanding of tasks. Model capacity and instruction-following ability are key determinants of revision quality.

## Foundational Learning

- **Concept: Supervised fine-tuning (SFT) on human demonstrations**
  - Why needed here: CnR builds upon SFT by fine-tuning already instruction-tuned models on the critique and revision dataset
  - Quick check question: What is the key difference between SFT for instruction tuning versus SFT for CnR?

- **Concept: Reinforcement learning from human feedback (RLHF)**
  - Why needed here: CnR provides an alternative to RLHF that avoids the complexity and instability of RL while achieving similar alignment goals
  - Quick check question: How does CnR's approach to human feedback differ fundamentally from RLHF's approach?

- **Concept: Critique and revision framework**
  - Why needed here: This is the core methodology that enables the iterative improvement process
  - Quick check question: What are the three main steps in the CnR process, and why is each necessary?

## Architecture Onboarding

- **Component map:** Base LLM (e.g., Falcon-40B-Instruct) -> CnR training dataset (prompts, responses, critiques, revisions) -> Training pipeline (fine-tuning with different formulations) -> Evaluation pipeline (GPT4 and human evaluation) -> Diagnostic dataset (for analyzing critique and revision quality)

- **Critical path:** 1) Prepare CnR training data (1000 samples or fewer) 2) Fine-tune base model on CnR data using best formulation 3) Use fine-tuned model to revise target responses 4) Evaluate revised responses against originals

- **Design tradeoffs:** Data quantity vs. quality (CnR works with <1000 samples, but higher quality data yields better results), Model size vs. efficiency (larger models perform better but require more resources), Iteration count vs. diminishing returns (multiple iterations improve quality but with decreasing marginal gains)

- **Failure signatures:** Revised responses become shorter but less informative (loss of detail), Critiques become generic and fail to address specific issues, Revision quality plateaus early despite multiple iterations, Model generates responses that are structurally different but not substantively better

- **First 3 experiments:** 1) Compare three CnR formulations (p→i→c→r vs p→i→r vs p→r) on a small validation set 2) Test revision quality on simple prompts with known good answers before scaling to complex ones 3) Measure win rate improvement after 1, 3, and 5 iterations on a subset of the test set

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CnR models vary when applied to prompts requiring advanced reasoning capabilities like coding, math, and physics? The paper provides an error analysis showing that CnR performs poorly on coding, math, and writing tasks, but doesn't explore the underlying reasons or potential solutions.

### Open Question 2
How does the quality of the initial response affect the effectiveness of CnR revisions? The paper mentions that CnR can improve even strong models like ChatGPT, but doesn't explore how initial response quality impacts revision effectiveness.

### Open Question 3
What is the impact of CnR training on the model's ability to handle multi-turn interactions? The paper notes that the current prompt set contains only single-turn prompts and needs expansion to include multi-turn interactions.

### Open Question 4
How does the specificity of critiques affect the quality of revisions? While the paper measures adherence to critiques, it doesn't explore how critique specificity impacts revision quality or user satisfaction.

### Open Question 5
What is the optimal balance between positive and negative feedback in critiques for effective revisions? The paper discusses collecting critiques with positive and negative aspects but doesn't analyze the optimal balance.

## Limitations

- The method requires high-quality natural language critiques, and the paper doesn't fully characterize the annotation effort or expertise needed to produce these critiques at scale
- While improvements persist through multiple iterations, the marginal gains decrease substantially (from 56.6% to 65.9%), suggesting potential diminishing returns or ceiling effects
- The approach shows better performance on longer responses but may sacrifice response structure and detail in the revision process

## Confidence

- **High Confidence**: Data efficiency claim (fewer than 1000 samples needed) is well-supported by ablation studies
- **Medium Confidence**: Win rates against strong models like ChatGPT are based on GPT4 evaluations, which may have different preferences than human evaluators
- **Medium Confidence**: Persistence of improvements across multiple revision cycles is demonstrated but may be sensitive to prompt quality and critique specificity

## Next Checks

1. Conduct human evaluation studies comparing GPT4 evaluations to human preferences to validate the claimed win rates
2. Test the method on a broader range of base models with varying instruction-tuning quality to better understand the relationship between base model quality and revision effectiveness
3. Analyze the trade-off between response length improvements and loss of structural detail through systematic comparison of original versus revised responses on structured tasks