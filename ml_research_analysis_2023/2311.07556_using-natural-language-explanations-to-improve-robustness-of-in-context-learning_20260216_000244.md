---
ver: rpa2
title: Using Natural Language Explanations to Improve Robustness of In-context Learning
arxiv_id: '2311.07556'
source_url: https://arxiv.org/abs/2311.07556
tags:
- nles
- x-icl
- language
- llms
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using natural language explanations (NLEs)
  to improve the robustness of large language models (LLMs) on adversarial datasets
  for natural language inference. The authors propose prompting LLMs with human-generated
  NLEs to produce further NLEs, yielding more accurate results than both a zero-shot
  in-context learning setting and using only human-generated NLEs.
---

# Using Natural Language Explanations to Improve Robustness of In-context Learning

## Quick Facts
- arXiv ID: 2311.07556
- Source URL: https://arxiv.org/abs/2311.07556
- Reference count: 18
- Primary result: Using few-shot ChatGPT-generated natural language explanations (NLEs) improves LLM robustness on adversarial NLI datasets by over 6% vs baseline approaches

## Executive Summary
This paper investigates using natural language explanations (NLEs) to improve the robustness of large language models (LLMs) on adversarial datasets for natural language inference. The authors propose prompting LLMs with human-generated NLEs to produce further NLEs, yielding more accurate results than both a zero-shot in-context learning setting and using only human-generated NLEs. Their results on five popular LLMs show that their approach yields over 6% improvement over baseline approaches for eight adversarial datasets.

## Method Summary
The authors evaluate five LLMs (GPT3.5-turbo, Llama2, Vicuna, Zephyr, Mistral) on seven adversarial NLI datasets. They construct demonstration sets by randomly selecting 40 unique instances from e-SNLI, with 3 additional instances for few-shot NLE generation. ChatGPT generates NLEs for each instance in zero-shot and few-shot settings. The demonstration set with generated NLEs is used to prompt each LLM, and accuracy is measured on adversarial datasets. They compare their approach (X-ICL with ChatGPT-generated NLEs) against baseline ICL without NLEs and X-ICL with human-generated NLEs.

## Key Results
- X-ICL (ChatGPTfew) leads to more than 6% gains over ICL for the majority of the LLMs and datasets
- Data selection strategies that work well on in-distribution test sets are less effective on adversarial datasets compared to their approach
- Using few-shot ChatGPT-generated NLEs leads to more accurate results than both zero-shot and human-generated NLEs alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT-generated NLEs improve LLM robustness by providing reasoning chains that help models avoid adversarial shortcuts
- Mechanism: When models see NLEs, they shift from pattern matching to reasoning-based prediction, making them less susceptible to dataset artifacts
- Core assumption: NLEs provide semantically grounded justifications that override superficial statistical cues
- Evidence anchors: [abstract] "X-ICL (ChatGPTfew) leads to more than 6% gains over ICL for the majority of the LLMs and datasets"
- Break condition: If NLEs contain hallucinations or are misaligned with instance semantics, performance degrades

### Mechanism 2
- Claim: Few-shot prompting produces better NLEs than zero-shot because it constrains the generation space with exemplars
- Mechanism: Demonstration examples guide the model to generate contextually relevant explanations rather than generic ones
- Core assumption: Small set of high-quality examples provides sufficient signal for generating task-appropriate NLEs
- Evidence anchors: [abstract] "using few-shot ChatGPT-generated NLEs leads to more accurate results than both zero-shot and human-generated NLEs alone"
- Break condition: If demonstration examples are poor quality or not representative, few-shot approach degrades

### Mechanism 3
- Claim: NLEs work better than data selection because they provide reasoning context rather than just similarity matching
- Mechanism: Instead of retrieving similar examples, NLEs teach the model to reason about the relationship between premise and hypothesis
- Core assumption: Semantic understanding is more valuable than surface-level similarity for adversarial robustness
- Evidence anchors: [abstract] "data selection strategies that work well on in-distribution test sets are less effective on adversarial datasets compared to our approach"
- Break condition: If reasoning context is misaligned with test distribution, approach fails

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: Core task being evaluated across multiple adversarial datasets
  - Quick check question: What are the three possible labels in NLI and what do they represent?

- Concept: In-Context Learning (ICL)
  - Why needed here: Baseline method being compared against X-ICL approaches
  - Quick check question: How does ICL differ from traditional fine-tuning in terms of parameter updates?

- Concept: Adversarial datasets and distribution shift
  - Why needed here: The study focuses on robustness to adversarial examples that exploit dataset biases
  - Quick check question: Why might a model that performs well on SNLI fail on HANS or ISCS?

## Architecture Onboarding

- Component map: Demonstration set (8-shot from e-SNLI) → NLE generation pipeline (zero-shot/few-shot ChatGPT) → ICL prompt construction → LLM inference → evaluation

- Critical path: Demonstration examples → NLE generation → ICL prompt construction → LLM inference → evaluation

- Design tradeoffs:
  - Few-shot vs zero-shot NLE generation: better quality vs broader applicability
  - Human-written vs synthetic NLEs: higher quality vs scalability
  - Data selection vs reasoning: surface similarity vs semantic understanding

- Failure signatures:
  - Performance drops when NLEs are swapped or randomized (X-ICL swap/rand cases)
  - Zero-shot NLEs underperform few-shot despite human preference
  - Data selection methods overfit to in-distribution and fail on adversarial

- First 3 experiments:
  1. Compare ICL vs X-ICL (Human) on SNLI to establish baseline improvement
  2. Test X-ICL (ChatGPTzero) vs X-ICL (ChatGPTfew) to validate few-shot benefit
  3. Evaluate data selection methods (COSINE, BM25, SET-BSR) vs X-ICL on reNLI

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the demonstration set impact the performance of X-ICL (ChatGPTfew) compared to other baselines?
- Basis in paper: [explicit] The paper mentions using an 8-shot setting and drawing 40 unique instances from the training-associated datasets, but does not explore the impact of varying the size of the demonstration set.
- Why unresolved: The paper does not provide an analysis of how different sizes of demonstration sets might affect the performance of X-ICL (ChatGPTfew) compared to other approaches like ICL, X-ICL (Human), and X-ICL (ChatGPTzero).
- What evidence would resolve it: Conducting experiments with varying sizes of demonstration sets and comparing the performance of X-ICL (ChatGPTfew) against other baselines across different NLI datasets would provide insights into the impact of demonstration set size on the effectiveness of the approach.

### Open Question 2
- Question: Can the faithfulness of NLEs generated by ChatGPT be improved to ensure they provide reliable explanations for model predictions?
- Basis in paper: [explicit] The paper acknowledges that the NLEs generated by ChatGPT might include unfaithful or hallucinated information, which can lead to severe implications if relied upon for model trust.
- Why unresolved: While the paper demonstrates the effectiveness of X-ICL (ChatGPTfew) in improving model performance, it does not address the issue of enhancing the faithfulness of the generated NLEs to ensure they provide accurate and reliable explanations for the model's predictions.
- What evidence would resolve it: Developing and evaluating methods to improve the faithfulness of NLEs generated by ChatGPT, such as incorporating additional constraints or fine-tuning the model on high-quality explanation data, would help ensure the reliability of the explanations provided by X-ICL (ChatGPTfew).

### Open Question 3
- Question: How well does X-ICL (ChatGPTfew) generalize to other NLP tasks beyond natural language inference?
- Basis in paper: [inferred] The paper focuses on evaluating X-ICL (ChatGPTfew) on natural language inference tasks, but does not explore its applicability to other NLP tasks such as sentiment analysis, question answering, or text summarization.
- Why unresolved: While the paper demonstrates the effectiveness of X-ICL (ChatGPTfew) in improving model performance on NLI tasks, it remains unclear whether the approach can be successfully applied to other NLP tasks and yield similar improvements in robustness and accuracy.
- What evidence would resolve it: Conducting experiments to evaluate X-ICL (ChatGPTfew) on a diverse range of NLP tasks and comparing its performance against other baselines would provide insights into the generalizability of the approach across different domains and tasks.

## Limitations

- The meta-prompts used for ChatGPT NLE generation are not disclosed, making reproduction difficult
- Evaluation is limited to NLI tasks, leaving generalizability to other domains untested
- Fixed 8-shot demonstration set size without exploring optimal shot counts for different tasks
- Human evaluation of NLE quality is subjective and may not fully capture semantic alignment

## Confidence

**High Confidence**: The claim that X-ICL (ChatGPTfew) improves accuracy over baseline ICL and human-generated NLEs is well-supported by quantitative results across multiple LLMs and adversarial datasets.

**Medium Confidence**: The assertion that data selection strategies underperform compared to NLEs on adversarial datasets is supported, but the comparison is indirect and relies on relative performance differences.

**Low Confidence**: The claim that zero-shot ChatGPT-generated NLEs underperform few-shot ones assumes demonstration examples are always high quality and representative.

## Next Checks

1. **Prompt Ablation**: Systematically vary the meta-prompts used for ChatGPT NLE generation (e.g., different reasoning constraints, example formats) and measure the impact on LLM robustness across adversarial datasets.

2. **Shot Count Sweep**: Evaluate the effect of varying the number of demonstration examples (e.g., 2-shot, 4-shot, 8-shot, 16-shot) on NLE quality and downstream LLM accuracy to identify the optimal trade-off between quality and efficiency.

3. **Cross-Domain Generalization**: Test the X-ICL approach on non-NLI tasks (e.g., commonsense reasoning, fact verification) to assess whether the robustness gains generalize beyond the NLI domain and identify any domain-specific failure modes.