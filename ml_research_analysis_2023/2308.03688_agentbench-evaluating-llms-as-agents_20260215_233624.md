---
ver: rpa2
title: 'AgentBench: Evaluating LLMs as Agents'
arxiv_id: '2308.03688'
source_url: https://arxiv.org/abs/2308.03688
tags:
- llms
- evaluation
- action
- agent
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentBench introduces a systematic benchmark to evaluate large
  language models as autonomous agents across eight diverse interactive environments,
  including operating systems, databases, knowledge graphs, digital card games, lateral
  thinking puzzles, house-holding, web shopping, and web browsing. It reveals that
  while top commercial models like GPT-4 demonstrate strong agent capabilities with
  task success rates up to 78% in some scenarios, there is a significant performance
  gap compared to open-source models under 70B parameters.
---

# AgentBench: Evaluating LLMs as Agents

## Quick Facts
- arXiv ID: 2308.03688
- Source URL: https://arxiv.org/abs/2308.03688
- Authors: Various
- Reference count: 40
- Key outcome: AgentBench reveals significant performance gaps between commercial and open-source LLMs in multi-turn interactive environments, with top models achieving up to 78% success rates while highlighting challenges in long-term reasoning and decision-making.

## Executive Summary
AgentBench introduces a systematic benchmark to evaluate large language models as autonomous agents across eight diverse interactive environments. The study reveals that while top commercial models like GPT-4 demonstrate strong agent capabilities with task success rates up to 78% in some scenarios, there is a significant performance gap compared to open-source models under 70B parameters. The main challenges identified are poor long-term reasoning, decision-making, and instruction-following abilities. The study also finds that code training has mixed effects on different agent tasks. AgentBench provides an integrated evaluation toolkit and serves as a foundation for developing more capable and practical LLM agents.

## Method Summary
AgentBench evaluates LLMs across eight interactive environments (Operating System, Database, Knowledge Graph, Digital Card Game, Lateral Thinking Puzzles, House-holding, Web Shopping, and Web Browsing) using Chain-of-Thought prompting with 1-shot examples and greedy decoding. The evaluation employs Docker containers for environment isolation and API interfaces for LLM interaction, measuring success rates and task-specific metrics across 25 models including both commercial APIs and open-sourced models up to 30B parameters.

## Key Results
- Top commercial models achieve up to 78% task success rates while open-source models under 70B parameters perform significantly worse
- Code training improves performance on coding-related tasks but harms logical reasoning tasks
- Models struggle with long-term reasoning, decision-making, and instruction-following across all environments
- Action validity requirements expose limitations in models' ability to follow instructions and generate executable commands

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AgentBench exposes a significant performance gap between top commercial and open-source LLMs in multi-turn interactive environments.
- Mechanism: The benchmark evaluates models across diverse environments that require long-term reasoning, decision-making, and instruction-following, which are areas where smaller open-source models struggle.
- Core assumption: Open-source models under 70B parameters lack the reasoning depth and instruction-following capabilities needed for complex interactive tasks.
- Evidence anchors:
  - [abstract] "While top commercial models like GPT-4 demonstrate strong agent capabilities with task success rates up to 78% in some scenarios, there is a significant performance gap compared to open-source models under 70B parameters."
  - [section 4.2] "Generally, most open-sourced LLMs also perform far poorer than API-based LLMs in AgentBench (Avg. 0.42 v.s. 2.24), and even the most capable open-sourced model openchat-13b-v3.2 presents a clear performance gap to gpt-3.5-turbo."
  - [corpus] Weak - no direct citations, only related work titles suggesting similar benchmarks exist but without specific performance comparisons.
- Break condition: If open-source models improve long-term reasoning or instruction-following, or if commercial models' advantages are domain-specific rather than general.

### Mechanism 2
- Claim: The multi-turn interaction format reveals limitations in LLMs' action validity and consistency.
- Mechanism: By requiring models to generate valid actions in each turn (e.g., executable bash commands, valid SQL queries, appropriate game moves), the benchmark identifies where models fail to follow instructions or produce invalid actions.
- Core assumption: The action validity requirement significantly increases the threshold for LLMs to act as agents, as models may generate incomplete commands or fail to understand task instructions.
- Evidence anchors:
  - [section 4.3] "We observe several common errors in model outputs... the model outputs an action, but it is incorrect or incomplete (e.g., missing parameters), so it is rejected by the environment."
  - [section 2] "Definition: Action Validity. Compared to previous evaluation of language agents... the model may not output a valid action or the action is not in the action space."
  - [corpus] Weak - related works mention action generation but don't specifically address validity in multi-turn contexts.
- Break condition: If models improve instruction-following or if environments relax action validity requirements.

### Mechanism 3
- Claim: Code training has mixed effects on different agent tasks, contradicting assumptions about its universal benefit.
- Mechanism: Models trained on code (like codegeex2-6b-chat) perform better on coding-related tasks (OS, DB, KG) but worse on tasks requiring logical reasoning (LTP), while wizardcoder underperforms despite code training.
- Core assumption: Single-turn format of code training data hinders multi-turn aptitude, creating tradeoffs between coding ability and reasoning ability.
- Evidence anchors:
  - [section 4.3] "Surprisingly, we find codegeex2-6b-chat significantly outperforms chatglm2-6b on OS, DB, and KG, which are related to coding. However, performance on LTP, which requires logical reasoning, decreases."
  - [section 4.3] "Meanwhile, wizardcoder underperforms despite code training. We speculate that the single-turn format of its training data hinders its multi-turn aptitude."
  - [corpus] Weak - related works don't specifically address the impact of code training on multi-turn agent tasks.
- Break condition: If code training data incorporates multi-turn examples or if models develop better reasoning abilities regardless of training data.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: AgentBench formalizes LLM-as-Agent evaluation as a POMDP with state space S, action space A, transition function T, reward function R, task instruction space U, and observation space O.
  - Quick check question: What are the key differences between a standard MDP and a POMDP, and why is POMDP more appropriate for LLM agent evaluation?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is the primary reasoning strategy adopted in AgentBench to evaluate LLMs' ability to reason through multi-step problems, reflecting practical user experience.
  - Quick check question: How does CoT differ from other reasoning strategies like reflection or search, and why might primitive CoT be more representative of real-world usage?

- Concept: Action space design and validity
  - Why needed here: AgentBench requires models to generate valid actions in open-ended environments, making action space design and validity checking critical components of the evaluation.
  - Quick check question: What are the key challenges in defining action spaces for LLM agents compared to traditional reinforcement learning agents?

## Architecture Onboarding

- Component map: Dataset loaders -> Environment wrappers (Docker) -> LLM API interfaces -> Prompt templates -> Checking pipelines -> Result aggregation
- Critical path: For each task, the system follows: (1) Load dataset and initialize environment, (2) Construct prompt with CoT example, (3) Send to LLM API, (4) Parse response for action or answer, (5) Execute action in environment or validate answer, (6) Repeat until task completion or failure, (7) Record results and move to next task.
- Design tradeoffs: Docker isolation ensures environment consistency but adds startup overhead; API-based LLM interaction simplifies integration but introduces network latency and rate limiting; CoT prompts improve reasoning but increase token usage; greedy decoding ensures reproducibility but may miss better solutions.
- Failure signatures: Common failures include invalid action generation (syntax errors, missing parameters), context overflow in long tasks, inconsistent behavior across turns, and premature termination. Environment-specific failures might include Docker startup issues, API authentication errors, or prompt formatting problems.
- First 3 experiments:
  1. Test the evaluation pipeline with a simple, deterministic task (e.g., a basic OS file count) to verify environment setup, API integration, and result parsing.
  2. Run a medium-complexity task (e.g., a database query) to test multi-turn interaction and checking pipeline functionality.
  3. Evaluate a challenging task (e.g., a knowledge graph question) to stress-test the model's reasoning capabilities and identify performance bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does training on code impact different agent tasks, and what are the tradeoffs involved?
- Basis in paper: Explicit - The paper states "surprisingly, we find codegeex2-6b-chat significantly outperforms chatglm2-6b on OS, DB, and KG, which are related to coding. However, performance on LTP, which requires logical reasoning, decreases."
- Why unresolved: The paper only provides a single example of the impact of code training, and does not explore the broader implications or underlying reasons for this phenomenon.
- What evidence would resolve it: A more comprehensive study comparing the performance of code-trained models across a wider range of agent tasks, including both coding and non-coding tasks, would help understand the full impact of code training.

### Open Question 2
- Question: How can the long-term reasoning and decision-making abilities of LLMs be improved for agent tasks?
- Basis in paper: Explicit - The paper identifies "poor long-term reasoning, decision-making, and instruction following abilities" as the main obstacles for developing usable LLM agents.
- Why unresolved: The paper does not provide specific solutions or strategies for addressing these challenges.
- What evidence would resolve it: Research demonstrating effective techniques for improving the long-term reasoning and decision-making abilities of LLMs in agent tasks, such as new training methods or architectural modifications.

### Open Question 3
- Question: What is the impact of expanding the context length of LLMs on their performance in multi-turn agent tasks?
- Basis in paper: Inferred - The paper mentions that "the context length of some open-sourced models is only 2k tokens, which impacts their performance on these interaction tasks" and suggests that "expanding the context length could improve multi-turn performance."
- Why unresolved: The paper does not provide empirical evidence or a detailed analysis of the relationship between context length and performance in multi-turn agent tasks.
- What evidence would resolve it: Experiments comparing the performance of LLMs with different context lengths on multi-turn agent tasks would provide insights into the impact of context length on their capabilities.

## Limitations

- The evaluation relies on API-based access which may not fully represent true model capabilities
- Limited sample size of code-trained models makes generalization of code training effects uncertain
- Docker-based environments may not fully capture real-world agent deployment scenarios

## Confidence

- **High confidence**: The benchmark design and evaluation methodology are clearly specified and reproducible
- **Medium confidence**: Performance gap between commercial and open-source models
- **Low confidence**: The impact of code training on agent performance

## Next Checks

1. Replicate the performance gap findings using self-hosted open-source models to verify the API access doesn't artificially limit their performance
2. Test the code training hypothesis with a larger sample of models specifically trained on multi-turn code data to isolate the effect of training format
3. Evaluate the same models on a subset of tasks using a different reasoning strategy (e.g., reflection vs chain-of-thought) to assess the impact of prompting strategy on performance metrics