---
ver: rpa2
title: Multimodal Chain-of-Thought Reasoning in Language Models
arxiv_id: '2302.00923'
source_url: https://arxiv.org/abs/2302.00923
tags:
- vision
- reasoning
- language
- answer
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Multimodal-CoT, a method that uses large language
  models to perform reasoning over both text and images by generating intermediate
  rationales. The key idea is to separate the process into two stages: generating
  a rationale using both modalities, and then using that rationale to infer the answer.'
---

# Multimodal Chain-of-Thought Reasoning in Language Models

## Quick Facts
- **arXiv ID:** 2302.00923
- **Source URL:** https://arxiv.org/abs/2302.00923
- **Authors:** [Not specified in input]
- **Reference count:** 23
- **Primary result:** 1B-parameter model outperforms GPT-3.5 by 16% accuracy (91.68% vs 75.17%) on ScienceQA

## Executive Summary
This paper introduces Multimodal-CoT, a framework that extends chain-of-thought reasoning to handle both text and image inputs in language models. By separating the reasoning process into two stages—generating a rationale using multimodal features, then using that rationale to infer the answer—the approach significantly improves performance on the ScienceQA benchmark. The method achieves 91.68% accuracy, surpassing both GPT-3.5 and human performance while addressing hallucination issues common in multimodal reasoning.

## Method Summary
The method fine-tunes a text-to-text language model (T5) in a two-stage framework for multimodal question answering. First, the model generates an intermediate rationale by incorporating vision features (extracted using DETR) alongside language inputs. Second, it uses this rationale to predict the final answer. Vision-language interaction is achieved through attention mechanisms, with gated fusion dynamically combining language and vision features at each position. The model is trained on the ScienceQA benchmark, which contains 21k multimodal multiple-choice questions with images.

## Key Results
- 1B-parameter Multimodal-CoT outperforms GPT-3.5 by 16% accuracy (91.68% vs 75.17%) on ScienceQA
- Surpasses human performance on the benchmark
- Two-stage framework reduces hallucination compared to direct answer generation
- Vision feature incorporation boosts rationale quality (RougeL score from 96.97%) and answer accuracy (84.91%)

## Why This Works (Mechanism)

### Mechanism 1
Decoupling rationale generation from answer inference reduces hallucination in multimodal reasoning. By separating the reasoning process into two stages, the model can focus on producing accurate intermediate reasoning steps before committing to a final answer. This separation allows the model to leverage richer vision-language interaction during rationale generation without the pressure of simultaneously producing the correct answer. If the rationale generation stage produces poor-quality or irrelevant rationales, the answer inference stage will be misled regardless of the decoupling.

### Mechanism 2
Incorporating vision features into both stages of reasoning significantly improves rationale quality and answer accuracy. By fusing vision features with language representations at both the rationale generation and answer inference stages, the model can maintain a rich multimodal context throughout the reasoning process. This allows the model to generate rationales that are grounded in visual information and use those rationales effectively for final answer prediction. If the vision features are not properly aligned with the language context or if the vision-language interaction mechanism is flawed, the benefits of multimodal information may not be realized.

### Mechanism 3
Using a gated fusion mechanism to combine language and vision features improves the quality of multimodal reasoning. The gated fusion mechanism allows the model to dynamically weight the contribution of language and vision features at each position in the sequence. This adaptive fusion ensures that the model can emphasize visual information when relevant and rely more on language when visual information is less important. If the learned weights in the gated fusion mechanism consistently favor one modality over the other inappropriately, the model may fail to leverage the full potential of multimodal information.

## Foundational Learning

- **Concept:** Chain-of-thought (CoT) reasoning
  - Why needed here: Multimodal-CoT builds upon the foundation of CoT reasoning in language models, extending it to handle both text and image inputs.
  - Quick check question: What is the primary benefit of using chain-of-thought prompting in language models?

- **Concept:** Vision-language interaction
  - Why needed here: The paper's success relies on effectively fusing vision and language representations to enable multimodal reasoning.
  - Quick check question: What are the key challenges in designing vision-language interaction mechanisms?

- **Concept:** Model architecture for multimodal tasks
  - Why needed here: Understanding the architecture (encoder-decoder with vision-language fusion) is crucial for implementing and extending Multimodal-CoT.
  - Quick check question: What are the main architectural components of multimodal transformer models?

## Architecture Onboarding

- **Component map:** Question text -> Language Encoder -> Language Embeddings; Image -> Vision Extractor -> Vision Features -> Interaction Layer -> Fused Features -> Decoder -> Rationale/Answer

- **Critical path:**
  1. Encode language input using transformer encoder
  2. Extract vision features using DETR or similar model
  3. Apply attention to correlate language and vision features
  4. Fuse features using gated mechanism
  5. Generate output using transformer decoder

- **Design tradeoffs:**
  - Single-stage vs. two-stage: Two-stage (decoupled) training allows better rationale quality but adds complexity
  - Caption-based vs. feature-based vision: Feature-based approach (using DETR) provides richer visual information than captions
  - Vision feature choice: DETR features perform better than CLIP or ResNet in this context

- **Failure signatures:**
  - Poor rationale quality leading to incorrect answers
  - Hallucination in generated rationales not grounded in visual context
  - Vision features not properly aligned with language context
  - Gated fusion mechanism learning inappropriate weights

- **First 3 experiments:**
  1. **Baseline comparison:** Implement language-only baseline (UnifiedQABase) and compare performance on ScienceQA
  2. **Vision feature ablation:** Test different vision feature extractors (DETR, CLIP, ResNet) and measure impact on performance
  3. **Decoupling ablation:** Compare single-stage (generate answer directly) vs. two-stage (generate rationale then answer) approaches

## Open Questions the Paper Calls Out

### Open Question 1
How can we improve Multimodal-CoT's performance on map understanding and counting tasks in images? Most factual mistakes are due to the failures of understanding maps and counting numbers in the images. Current vision features may not provide sufficient detail for complex spatial reasoning required for maps and counting tasks. Comparative experiments using advanced vision models like Mask R-CNN or fine-grained counting models, showing improved accuracy on map and counting questions, would resolve this question.

### Open Question 2
What is the impact of injecting commonsense knowledge into the Multimodal-CoT framework? It is possible to improve Multimodal-CoT by injecting commonsense knowledge. The paper suggests this as a potential improvement but does not provide empirical evidence or implementation details. Experiments comparing Multimodal-CoT with and without a commonsense knowledge injection module, demonstrating accuracy improvements on questions requiring commonsense reasoning, would resolve this question.

### Open Question 3
How effective would a filtering mechanism be in selecting only the effective chain-of-thought (CoT) for answer inference? Apply a filtering mechanism, e.g., using only the effective CoT to infer the answer and get rid of irrelevant CoT. The paper mentions this as a future direction but does not explore or validate its effectiveness. Ablation studies comparing Multimodal-CoT with and without a CoT filtering mechanism, showing changes in accuracy and robustness to incorrect rationales, would resolve this question.

## Limitations
- Evaluation limited to a single benchmark (ScienceQA), which may not represent diverse multimodal reasoning tasks
- Focus on multiple-choice questions with images, potentially missing other important multimodal reasoning scenarios
- Does not extensively explore computational costs of the two-stage approach compared to direct answer generation methods

## Confidence

- **High Confidence:** The claim that Multimodal-CoT outperforms GPT-3.5 on ScienceQA (91.68% vs 75.17%) is well-supported by experimental results and represents a concrete, measurable outcome.
- **Medium Confidence:** The assertion that the two-stage framework reduces hallucination is supported by the experimental comparison between decoupled and non-decoupled training, but the analysis of hallucination is somewhat limited in scope.
- **Medium Confidence:** The claim that incorporating vision features improves rationale quality is supported by the comparison with caption-only approaches, but the analysis could benefit from more detailed qualitative examination of generated rationales.

## Next Checks

1. **Generalization Test:** Evaluate Multimodal-CoT on additional multimodal reasoning benchmarks beyond ScienceQA to assess generalization across different domains and question types.

2. **Ablation Study:** Conduct a more comprehensive ablation study removing individual components (vision features, two-stage framework, gated fusion) to quantify their individual contributions to performance.

3. **Efficiency Analysis:** Compare the computational efficiency and inference time of the two-stage Multimodal-CoT approach against direct answer generation methods to assess practical deployment implications.