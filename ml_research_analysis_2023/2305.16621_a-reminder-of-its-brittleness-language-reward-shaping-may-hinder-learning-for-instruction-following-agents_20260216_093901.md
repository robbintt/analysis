---
ver: rpa2
title: 'A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning
  for Instruction Following Agents'
arxiv_id: '2305.16621'
source_url: https://arxiv.org/abs/2305.16621
tags:
- reward
- language
- learning
- arxiv
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study identifies a key weakness in Language Reward Shaping
  (LRS) for instruction-following agents: LRS models can reward partially matched
  trajectories, leading to suboptimal learning. This occurs due to the compression
  of high-dimensional language information into scalar rewards and the limitations
  of gradient-based learning.'
---

# A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents

## Quick Facts
- arXiv ID: 2305.16621
- Source URL: https://arxiv.org/abs/2305.16621
- Authors: 
- Reference count: 26
- Key outcome: Language Reward Shaping (LRS) can reward partially matched trajectories, leading to suboptimal learning due to information compression and gradient-based smoothing.

## Executive Summary
This paper identifies a fundamental weakness in Language Reward Shaping (LRS) for instruction-following agents: the compression of high-dimensional language information into scalar rewards can lead to false positive rewards for partially matched trajectories. Through theoretical analysis and experiments on Montezuma's Revenge, the authors demonstrate that agents trained with LRS rewards converge more slowly than those trained with pure reinforcement learning. The study proposes a novel reward perturbation method based on loosening task constraints to address this issue, highlighting the brittleness of existing LRS methods and suggesting improvements like vectorised multi-label rewards.

## Method Summary
The study investigates LRS in sparse reward environments using Montezuma's Revenge. Agents are trained using PPO and PPO+RND algorithms with both non-simulated and simulated LRS models implementing different reward rules (fully matched, partially matched, relaxed ordering). The simulated LRS models use predefined target trajectories for each instruction sentence. Performance is evaluated using area under the curve (AUC) of wins over training episodes and success rate of reaching the goal state.

## Key Results
- Agents trained with LRS rewards converge more slowly than pure RL agents due to false positive rewards for partially matched trajectories
- Temporal constraints are crucial for LRS effectiveness; neglecting them leads to learning failure and local optima
- The proposed reward perturbation method based on loosening task constraints shows promise in addressing LRS brittleness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRS compresses high-dimensional language information into scalar rewards, introducing ambiguity and imprecision in what should be rewarded.
- Mechanism: The LRS model uses gradient-based learning to assign non-zero rewards even when inputs are not perfectly matched, leading to false positive rewards for partially matched trajectories.
- Core assumption: Transformer models used in LRS cannot adequately capture chronological ordering of events.
- Evidence anchors:
  - [abstract] "The core mechanism of LRS involves compressing high-dimensional semantic vectors of natural language instructions from pretrained large models into one-dimensional numerical rewards."
  - [section] "The nature of gradient-based learning allows for non-zero rewards to be assigned even when inputs are not perfectly matched."
  - [corpus] Weak corpus evidence for this specific mechanism - only 5 related papers found with average FMR=0.49.
- Break condition: When the LRS model can perfectly distinguish between fully matched and partially matched trajectories without relying on gradient-based smoothing.

### Mechanism 2
- Claim: Rewarding partially matched trajectories reduces the convergence rate of reinforcement learning algorithms.
- Mechanism: False positive rewards create a gradient that pushes the policy in the wrong direction, slowing down convergence to optimal policies.
- Core assumption: The RL algorithm uses gradient ascent on Q-value functions to update policies.
- Evidence anchors:
  - [abstract] "We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents."
  - [section] "Corollary 4.1 (Convergence rate reduction) . In Actor-Critic algorithm, gradient ascent on Q(s, a)πθi(a|s) pushes the next updated policy πθi+1 in the direction provided by the Q value function."
  - [corpus] Weak corpus evidence - no direct citations found supporting this specific convergence rate claim.
- Break condition: When the expectation of cumulative false positive rewards approaches zero or becomes negative.

### Mechanism 3
- Claim: Temporal constraints are crucial for LRS effectiveness, and neglecting them leads to learning failure.
- Mechanism: Without proper temporal ordering, agents get trapped in local minima close to the initial state, pursuing rewards from incomplete instruction sequences.
- Core assumption: Language instructions contain temporal ordering information that must be preserved in reward computation.
- Evidence anchors:
  - [abstract] "We characterised a novel type of reward perturbation that addresses the issue based on the concept of loosening task constraints."
  - [section] "The algorithm completely failed to converge when LRS models neglected the temporal ordering."
  - [corpus] Weak corpus evidence - only 5 related papers found with average FMR=0.49.
- Break condition: When the task environment naturally enforces temporal ordering through state transitions or when alternative temporal reasoning mechanisms are implemented.

## Foundational Learning

- Concept: Sparse reward Markov decision processes
  - Why needed here: The paper specifically analyzes LRS in sparse reward environments where environmental rewards are only given upon reaching the goal state.
  - Quick check question: In a sparse reward MDP, what is the primary challenge for reinforcement learning agents?

- Concept: Linear Temporal Logic (LTL)
  - Why needed here: LTL is used to formally express temporal constraints in language instructions and determine if trajectories satisfy these constraints.
  - Quick check question: How does LTL help distinguish between trajectories that execute instructions in correct vs. incorrect temporal order?

- Concept: Potential-based reward shaping
  - Why needed here: The paper proves that LRS can be formulated as a potential-based shaping function, which guarantees policy invariance.
  - Quick check question: What mathematical property must a reward shaping function satisfy to preserve optimal policies?

## Architecture Onboarding

- Component map: Multimodal reasoning model (pretrained) → Reward function computation → Scalar reward output; Policy model (RL-based) ← Scalar rewards ← Trajectory embedding
- Critical path: Trajectory encoding → Language instruction matching → Reward computation → Policy update
- The reward computation stage is where the brittleness manifests due to information compression
- Design tradeoffs:
  - Granularity vs. generalization: Finer instruction partitioning improves discrimination but may overfit to specific scenarios
  - Temporal sensitivity vs. robustness: Strict temporal ordering prevents some false positives but may miss valid alternative solutions
  - Model complexity vs. training efficiency: More sophisticated LRS models may capture better distinctions but require more data and computation
- Failure signatures:
  - Agent gets stuck in suboptimal loops despite receiving environmental rewards
  - Training curves show slow initial progress followed by plateaus
  - Movement heatmaps show repetitive patterns in specific areas
  - Success rate drops significantly when instruction complexity increases
- First 3 experiments:
  1. Compare PPO+RND baseline against PPO+LRS with perfect reward matching (Rule 1) on simple vs. complex tasks
  2. Test LRS with different temporal constraint strictness levels (Rules 1-3) on Montezuma's Revenge rooms A1, A2, B3
  3. Evaluate the impact of instruction granularity by systematically removing intermediate steps vs. entire constraint dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Language Reward Shaping (LRS) models vary across different levels of task complexity?
- Basis in paper: [explicit] The paper discusses the brittleness of LRS in Montezuma's Revenge, a challenging sparse reward environment, and notes that LRS agents learned faster compared to PPO+RND agents in the simpler task of Montezuma room A1.
- Why unresolved: The paper provides limited analysis on how task complexity affects LRS performance, focusing mainly on two rooms in Montezuma's Revenge.
- What evidence would resolve it: Comparative studies of LRS performance across a wider range of task complexities, including both simpler and more complex environments than Montezuma's Revenge, would provide clearer insights into how task complexity impacts LRS effectiveness.

### Open Question 2
- Question: Can vectorised multi-label rewards or trajectory optimisation formulations improve the effectiveness of Language Reward Shaping (LRS) models?
- Basis in paper: [explicit] The paper suggests using vectorised multi-label rewards or formulating the learning problem as a trajectory optimisation task as potential improvements to address the brittleness of LRS models.
- Why unresolved: The paper does not provide experimental results or theoretical analysis to support the effectiveness of these proposed improvements.
- What evidence would resolve it: Experimental studies comparing the performance of LRS models using vectorised multi-label rewards or trajectory optimisation formulations against traditional scalar reward models would demonstrate the potential benefits of these approaches.

### Open Question 3
- Question: How does the choice of language model architecture impact the brittleness of Language Reward Shaping (LRS) models?
- Basis in paper: [explicit] The paper discusses the challenges of compressing high-dimensional semantic vectors into scalar rewards and mentions that both cosine similarity prediction and binary classification prediction are commonly used in LRS models.
- Why unresolved: The paper does not explore how different language model architectures, such as those using attention mechanisms or different encoding strategies, affect the brittleness of LRS models.
- What evidence would resolve it: Comparative studies of LRS models using different language model architectures, evaluating their performance in terms of convergence rate and reward accuracy, would clarify the impact of architecture choice on LRS brittleness.

## Limitations
- The use of simulated LRS models based on predefined target trajectories may not fully capture real LRS system behavior
- Empirical results are primarily based on Montezuma's Revenge, limiting generalization to other domains
- Theoretical analysis assumes specific conditions about gradient-based learning that may not hold in all RL implementations

## Confidence
- **High Confidence**: The core finding that LRS can reward partially matched trajectories due to information compression and gradient-based smoothing is well-supported by both theory and controlled experiments.
- **Medium Confidence**: The claim that this leads to slower convergence rates in RL algorithms is demonstrated empirically but requires more diverse environments for robust generalization.
- **Medium Confidence**: The proposed reward perturbation method shows promise in controlled settings, but its effectiveness in real-world LRS applications remains to be validated.

## Next Checks
1. **Generalization Testing**: Replicate the experiments across multiple instruction-following environments beyond Montezuma's Revenge, including text-based games and robotic manipulation tasks, to assess the robustness of the findings.

2. **Real LRS Implementation**: Implement and evaluate the proposed reward perturbation method using actual pretrained language models (rather than simulated LRS) to verify if the theoretical improvements translate to practical performance gains.

3. **Alternative Temporal Reasoning**: Test whether incorporating explicit temporal reasoning mechanisms (such as temporal attention or sequence models) in the LRS pipeline can mitigate the identified brittleness without requiring reward perturbation.