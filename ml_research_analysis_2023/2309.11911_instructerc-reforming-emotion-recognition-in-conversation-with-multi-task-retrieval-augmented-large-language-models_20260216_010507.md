---
ver: rpa2
title: 'InstructERC: Reforming Emotion Recognition in Conversation with Multi-task
  Retrieval-Augmented Large Language Models'
arxiv_id: '2309.11911'
source_url: https://arxiv.org/abs/2309.11911
tags:
- task
- emotion
- emotional
- speaker
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces InstructERC, a method that reformulates\
  \ emotion recognition in conversation (ERC) as a generative task using large language\
  \ models (LLMs). The core idea is to leverage a retrieval template module and two\
  \ auxiliary tasks\u2014speaker identification and emotion impact prediction\u2014\
  to enhance the model's ability to capture dialogue dynamics."
---

# InstructERC: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models

## Quick Facts
- **arXiv ID**: 2309.11911
- **Source URL**: https://arxiv.org/abs/2309.11911
- **Reference count**: 34
- **Primary result**: Achieves state-of-the-art performance on three benchmark datasets with up to 2.70% improvement in weighted F1 score using parameter-efficient fine-tuning

## Executive Summary
This paper introduces InstructERC, a novel approach that reformulates emotion recognition in conversation (ERC) as a generative task using large language models (LLMs). The method combines a retrieval template module with two auxiliary tasks—speaker identification and emotion prediction—to enhance the model's ability to capture dialogue dynamics and emotional flow. By leveraging parameter-efficient fine-tuning techniques like LoRA, InstructERC achieves superior performance while preventing overfitting, particularly in low-resource scenarios. The framework demonstrates strong generalization across different data distributions and outperforms existing models on standard ERC benchmarks.

## Method Summary
InstructERC reformulates ERC as a generative task by constructing structured prompts that integrate historical context, label statements, and retrieved demonstrations. The method employs two auxiliary tasks: speaker identification to capture speaker-specific emotional patterns, and emotion prediction to model the impact of current utterances on future emotional states. The entire framework is fine-tuned using LoRA parameter-efficient techniques to prevent overfitting while maintaining model capacity. The approach is evaluated on three benchmark datasets with unified emotion labels mapped to The Feeling Wheel standard.

## Key Results
- Achieves state-of-the-art weighted F1 scores on IEMOCAP, MELD, and EmoryNLP datasets
- Improves performance by up to 2.70% compared to existing models
- Demonstrates robustness in low-resource scenarios and cross-dataset generalization
- Shows parameter-efficient fine-tuning prevents overfitting while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval template module explicitly integrates multi-granularity dialogue supervision by concatenating historical utterances, label statements, and domain-relevant demonstrations
- Mechanism: Constructs structured prompt with four parts: instruction, historical context limited to preceding utterances, label statement to constrain output, and retrieved demonstration from domain base filtered by semantic similarity
- Core assumption: Historical context must be limited to preceding utterances only to preserve causal consistency
- Evidence anchors: Abstract states retrieval template helps "explicitly integrate multi-granularity dialogue supervision information"; section confirms historical content limited to preceding utterances
- Break condition: Including future utterances violates temporal causality and degrades performance

### Mechanism 2
- Claim: Two auxiliary tasks—speaker identification and emotion prediction—implicitly model dialogue role relationships and future emotional tendencies
- Mechanism: Speaker identification task predicts utterance speaker to capture speaker-specific emotional patterns; emotion prediction task models current utterance impact on subsequent emotions
- Core assumption: Different speakers exhibit distinct emotional expression patterns, and current emotional states influence future states
- Evidence anchors: Abstract mentions "two additional emotion alignment tasks... to implicitly model the dialogue role relationships and future emotional tendencies"; section describes joint training of these tasks
- Break condition: Omitting auxiliary tasks loses speaker-specific emotional modeling and emotional causality

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (LoRA) prevents overfitting on ERC task and improves generalization, especially in low-resource scenarios
- Mechanism: LoRA inserts low-rank adapters after self-attention layers with dimension 16, learning rate 2e-4, while freezing most LLM parameters
- Core assumption: Overfitting is significant when fine-tuning large LLMs on small or imbalanced ERC datasets
- Evidence anchors: Section states "Replacing LoRA with full-parameter fine-tuning results in a significant drop in performance"; indicates parameter-efficient approach prevents overfitting
- Break condition: Full-parameter fine-tuning on small datasets causes overfitting to specific dialogue patterns

## Foundational Learning

- **Concept**: Large Language Models (LLMs) and their generative capabilities
  - Why needed here: InstructERC reformulates ERC as a generative task using LLMs, requiring understanding of how LLMs process prompts and generate text
  - Quick check question: What is the difference between discriminative and generative frameworks in NLP, and why is ERC reformulated as a generative task in InstructERC?

- **Concept**: Parameter-efficient fine-tuning techniques (e.g., LoRA)
  - Why needed here: InstructERC uses LoRA to fine-tune LLMs efficiently, preventing overfitting and enabling deployment on limited resources
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning, and what are the benefits for small or imbalanced datasets?

- **Concept**: Retrieval-based prompting and semantic similarity
  - Why needed here: InstructERC retrieves domain demonstrations using SBERT and cosine similarity to provide relevant emotional supervision signals
  - Quick check question: How does SBERT generate embeddings for retrieval, and why is semantic similarity important for selecting demonstrations?

## Architecture Onboarding

- **Component map**: Input utterance → Retrieval Template construction → Auxiliary tasks pre-training → Joint fine-tuning on main task + emotion impact prediction → Inference with demonstration retrieval
- **Critical path**: Input utterance flows through retrieval template construction, auxiliary task integration, LoRA fine-tuning, and final inference with semantic demonstration retrieval
- **Design tradeoffs**: LoRA reduces computational cost and overfitting risk but may limit model capacity; limiting historical context preserves causality but may lose context
- **Failure signatures**: Performance degradation if future utterances are included in historical context, if auxiliary tasks are omitted, or if full fine-tuning is used on small datasets
- **First 3 experiments**:
  1. Verify retrieval template construction by checking if historical context is limited to preceding utterances and demonstrations are semantically similar
  2. Test speaker identification and emotion prediction tasks independently to ensure they capture speaker dynamics and emotional causality
  3. Compare LoRA vs full fine-tuning on a small dataset to confirm parameter-efficient fine-tuning prevents overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InstructERC vary when using different large language models (LLMs) as backbones, and what specific characteristics of these models contribute to the observed differences?
- Basis in paper: Paper mentions using ChatGLM-6B, ChatGLM2-6B, Llama-7B, and Llama2-7B as backbones and notes differences in their performance
- Why unresolved: While the paper reports performance differences, it does not delve into the specific model characteristics that might explain these variations
- What evidence would resolve it: A detailed analysis comparing the architectural and training differences of the LLMs used, alongside their performance on ERC tasks

### Open Question 2
- Question: What is the impact of different data mixing strategies (Total Mix vs. Ratio Mix) on the model's ability to generalize across diverse emotional paradigms?
- Basis in paper: Paper explores Total Mix and Ratio Mix strategies and observes performance biases due to varying training data quantities
- Why unresolved: The paper does not provide a comprehensive explanation of how these strategies affect the model's generalization capabilities across different datasets
- What evidence would resolve it: Conducting experiments that systematically vary the mixing strategies and analyzing the resulting model performance on unseen datasets

### Open Question 3
- Question: How does the retrieval template module in InstructERC adapt to different conversation lengths and domains, and what are the limitations of this adaptation?
- Basis in paper: Paper describes the retrieval template module's role in integrating multi-granularity dialogue supervision but does not detail its adaptability to varying conversation lengths and domains
- Why unresolved: The paper lacks a thorough examination of the module's performance across diverse conversation scenarios and its limitations
- What evidence would resolve it: Testing the retrieval template module across a wide range of conversation lengths and domains, and analyzing its performance

## Limitations

- **Uncertainty in domain base construction**: The paper lacks detailed specifications of domain base construction process and SBERT model selection, creating reproducibility challenges across different domains or languages
- **Incomplete ablation analysis**: While ablation studies show auxiliary tasks improve performance, the paper doesn't clearly separate individual contributions of speaker identification versus emotion prediction tasks
- **Limited generalizability claims**: The framework is only evaluated on three English-language datasets without systematic testing across different conversation domains, languages, or low-resource scenarios

## Confidence

- **High confidence**: Core methodology and experimental results are well-documented with clear implementation details for retrieval template construction, LoRA fine-tuning procedure, and evaluation protocol across three benchmark datasets
- **Medium confidence**: Mechanism explanations for auxiliary tasks are theoretically sound but empirically validated only through overall performance gains rather than individual task contributions
- **Low confidence**: Cross-domain generalizability claims lack systematic testing beyond stated experiments, as the framework is only evaluated on three English-language datasets

## Next Checks

1. **Ablation analysis extension**: Conduct detailed ablation studies separating individual contributions of speaker identification and emotion prediction tasks, including performance with only one auxiliary task active versus both combined

2. **Domain generalization testing**: Evaluate InstructERC on conversations from different domains (customer service, healthcare, social media) and languages to assess whether the retrieval template and auxiliary tasks maintain effectiveness across varied contexts

3. **Overfitting measurement**: Compare train-test performance gaps between LoRA fine-tuning and full fine-tuning on small datasets, and measure generalization to out-of-distribution samples to quantitatively verify the overfitting prevention claim