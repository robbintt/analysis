---
ver: rpa2
title: Bayesian Pseudo-Coresets via Contrastive Divergence
arxiv_id: '2303.11278'
source_url: https://arxiv.org/abs/2303.11278
tags:
- dataset
- methods
- synthetic
- arxiv
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach for constructing Bayesian Pseudo-Coresets
  (BPCs) by utilizing contrastive divergence. The method aims to create a small synthetic
  dataset that approximates the posterior inference achieved with the original dataset.
---

# Bayesian Pseudo-Coresets via Contrastive Divergence

## Quick Facts
- arXiv ID: 2303.11278
- Source URL: https://arxiv.org/abs/2303.11278
- Reference count: 40
- Key outcome: A novel approach for constructing Bayesian Pseudo-Coresets (BPCs) using contrastive divergence that eliminates distributional assumptions and achieves performance comparable to dataset condensation methods.

## Executive Summary
This paper introduces a novel approach for constructing Bayesian Pseudo-Coresets (BPCs) by utilizing contrastive divergence. The method creates a small synthetic dataset that approximates the posterior inference achieved with the original dataset, eliminating the need for distributional assumptions on the original posterior. By modeling the posterior associated with synthetic data using an energy-based distribution and deriving a contrastive-divergence-like loss function, the authors demonstrate that their method outperforms existing BPC techniques while achieving performance comparable to dataset condensation methods.

## Method Summary
The method constructs Bayesian Pseudo-Coresets by minimizing forward KL divergence between the real posterior p(θ|D) and an energy-based synthetic posterior q(θ|~D). The energy function E(θ,~D) is chosen as a classification loss (e.g., cross-entropy), and the contrastive divergence loss is computed as the difference between energies of parameters sampled from real trajectories and those generated via finite-step Langevin dynamics from the synthetic posterior. The synthetic dataset parameters are updated via gradient descent on this loss, avoiding the need for approximations in pseudo-coreset construction.

## Key Results
- The proposed method outperforms existing BPC techniques on multiple datasets including CIFAR10, SVHN, MNIST, FashionMNIST, CIFAR100, and Tiny Imagenet.
- Performance is comparable to state-of-the-art dataset condensation methods while maintaining Bayesian inference properties.
- The method demonstrates strong cross-architecture generalization, with synthetic datasets trained on one architecture performing well on others.
- Using the same loss function for training real parameters and as the energy function yields optimal performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive divergence loss enables learning synthetic data without distributional assumptions on the original posterior.
- **Mechanism:** The method constructs an energy-based distribution q(θ|~D) = exp(-E(θ,~D))/Z(~D) for the synthetic dataset posterior, then minimizes forward KL divergence DKL(p(θ|D)||q(θ|~D)) by gradient descent on ~D. This avoids needing to approximate p(θ|D) with a parametric form.
- **Core assumption:** The energy function E(θ,~D) can be chosen as a classification loss (e.g., cross-entropy), so that sampling from q(θ|~D) corresponds to training a network on the synthetic data.
- **Evidence anchors:**
  - [abstract] "We introduce a novel approach for constructing pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing contrastive divergence eliminates the need for approximations in the pseudo-coreset construction process."
  - [section 3.2] "Taking the monte-carlo approximation of the above loss function, the final loss function comes out be: L = E_θ+∼p(θ|D)[E(θ+,~D)] - E_θ-∼q(θ|~D)[E(θ-,~D)]"
  - [corpus] No direct evidence; the method is novel and not covered in related papers.
- **Break condition:** If the energy function is not differentiable w.r.t θ or ~D, gradient-based optimization fails.

### Mechanism 2
- **Claim:** Langevin dynamics can efficiently sample from the synthetic posterior without requiring MCMC to reach stationarity.
- **Mechanism:** Finite-step Langevin dynamics (Eq. 12) approximates sampling from q(θ|~D) by running noisy gradient descent on the energy function for T steps, starting from a parameter θk+ from the real dataset trajectory.
- **Core assumption:** A finite number of Langevin steps provides sufficient mixing to approximate the target distribution for the contrastive divergence loss.
- **Evidence anchors:**
  - [abstract] "it enables the use of finite-step MCMC methods, alleviating the requirement for extensive mixing to reach a stationary distribution."
  - [section 3.2] "To sample θ−, we resort to langevin dynamics to sample θ from q(θ|~D). Langevin dynamics is an iterative gradient-based MCMC sampling methods given by the following update rule: θ(t+1) = θ(t) - α/2∇θ(t)E(θ,~D) + √αη, η∼N(0,I)"
  - [corpus] No direct evidence; finite-step MCMC is a common approximation in EBM literature.
- **Break condition:** If T is too small, the Langevin trajectory may not explore the energy landscape enough, leading to poor approximation of the second expectation.

### Mechanism 3
- **Claim:** Using the same loss function for training real parameters and as energy function aligns the posteriors and improves performance.
- **Mechanism:** By choosing ℓD = ℓ~D (e.g., both cross-entropy), the method ensures that the real trajectory parameters and synthetic posterior samples are optimized under the same criterion, making the contrastive divergence loss meaningful.
- **Core assumption:** The posterior distributions p(θ|D) and q(θ|~D) are similar enough in support when the same loss is used.
- **Evidence anchors:**
  - [section 3.3] "To this end, we propose to use the categorical cross-entropy as the energy function... One can also choose other classification losses such as Focal loss and Multi-Margin Loss."
  - [section 5.5] "We observe that our method performs best for a certain loss function when ℓD = ℓ~D. This can be attributed to the fact that the posterior estimates are closer when same loss function is used to obtain the trained parameters."
  - [corpus] No direct evidence; this is an empirical observation from the paper.
- **Break condition:** If ℓD ≠ ℓ~D, the two posteriors may have very different supports, causing the contrastive divergence to be ill-posed or numerically unstable.

## Foundational Learning

- **Concept:** Energy-based models (EBMs) and contrastive divergence training.
  - **Why needed here:** The synthetic posterior is modeled as an EBM, and the loss function is derived from contrastive divergence between real and synthetic posteriors.
  - **Quick check question:** What is the form of the contrastive divergence loss used in standard EBM training, and how does it differ from the loss in this method?

- **Concept:** Langevin dynamics for MCMC sampling.
  - **Why needed here:** Finite-step Langevin dynamics is used to approximate sampling from the synthetic posterior q(θ|~D) without requiring full mixing to stationarity.
  - **Quick check question:** What is the update rule for Langevin dynamics, and how does the finite-step approximation affect the quality of the samples?

- **Concept:** Forward KL divergence and its behavior with respect to posterior approximation.
  - **Why needed here:** The method minimizes forward KL divergence, which encourages the synthetic posterior to cover the support of the real posterior.
  - **Quick check question:** How does forward KL divergence differ from reverse KL divergence in terms of the resulting approximation, and why is forward KL preferable here?

## Architecture Onboarding

- **Component map:** Real dataset D -> Trajectory buffer -> θ+ sampling -> Langevin dynamics -> θ- sampling -> Energy function E(θ,~D) -> Contrastive divergence loss L -> Synthetic dataset ~D update
- **Critical path:** 1) Train real network on D to collect trajectories. 2) Sample θ+ from trajectory buffer. 3) Run T-step Langevin dynamics from θ+ to obtain θ−. 4) Compute contrastive divergence loss. 5) Backpropagate loss to update ~D. 6) Repeat until convergence.
- **Design tradeoffs:** Choice of energy function (loss type) affects performance and generalization. Number of Langevin steps T balances computational cost vs. mixing quality. Size of trajectory buffer impacts variance in loss estimation. Using forward KL vs. reverse KL changes the nature of the synthetic data.
- **Failure signatures:** Loss not decreasing: Check energy function differentiability and Langevin step size. Synthetic data collapsing: Increase T or adjust Langevin noise scale. Poor cross-architecture performance: Real posteriors may be too network-specific; consider ensemble or more diverse trajectories.
- **First 3 experiments:** 1) **Sanity check:** Run the method on MNIST with ipc=1 and compare accuracy to random synthetic data. 2) **Energy function ablation:** Train with cross-entropy vs. focal loss as energy function and measure impact on CIFAR10 ipc=10. 3) **Langevin step study:** Vary T from 1 to 10 and plot validation accuracy vs. T for CIFAR10 ipc=50.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of energy function affect the performance of Bayesian Pseudo-Coresets?
- **Basis in paper:** [explicit] The paper mentions that they propose to use categorical cross-entropy as the energy function, but also suggests that other classification losses like Focal loss and Multi-Margin Loss could be used. They perform a 'cross-loss' analysis to observe the effect of using different choices of loss functions.
- **Why unresolved:** While the paper presents some initial findings on the effect of different energy functions, a comprehensive understanding of how various energy functions impact the performance of Bayesian Pseudo-Coresets is still lacking. The paper does not provide a definitive answer on which energy function is optimal for different scenarios.
- **What evidence would resolve it:** Systematic experiments comparing the performance of Bayesian Pseudo-Coresets using various energy functions across different datasets and model architectures would help resolve this question. Additionally, theoretical analysis of the properties of different energy functions in the context of Bayesian Pseudo-Coresets could provide insights.

### Open Question 2
- **Question:** Can Bayesian Pseudo-Coresets be extended to generative models and other resource-intensive tasks?
- **Basis in paper:** [explicit] The paper mentions that a better understanding of theoretically grounded works such as BPC can not only improve the performance of standard classification tasks but also pave the way to extend these methods to other resource-intensive tasks. They express intent to investigate BPC and dataset condensation techniques for large generative models in the future.
- **Why unresolved:** The current work focuses on Bayesian Pseudo-Coresets for classification tasks. Extending this approach to generative models and other complex tasks involves additional challenges and considerations that are not addressed in the current paper.
- **What evidence would resolve it:** Successful application of Bayesian Pseudo-Coresets to generative models and other resource-intensive tasks, along with comparative analysis of performance and efficiency gains, would provide evidence to resolve this question.

### Open Question 3
- **Question:** How does the performance of Bayesian Pseudo-Coresets scale with dataset size and complexity?
- **Basis in paper:** [inferred] The paper presents experiments on various datasets with different numbers of images per class (ipc), showing that performance generally improves with increasing ipc. However, the paper does not extensively explore how performance scales with overall dataset size or complexity.
- **Why unresolved:** While the paper demonstrates the effectiveness of Bayesian Pseudo-Coresets on several benchmark datasets, it does not provide a comprehensive analysis of how performance scales with dataset size or complexity. This information is crucial for understanding the practical limitations and potential applications of the method.
- **What evidence would resolve it:** Extensive experiments on datasets of varying sizes and complexities, coupled with theoretical analysis of the computational complexity and convergence properties of the method, would help resolve this question. Additionally, real-world applications of Bayesian Pseudo-Coresets to large-scale problems would provide valuable insights.

## Limitations
- The method relies on finite-step Langevin dynamics without theoretical guarantees on mixing time or convergence properties.
- Performance is empirically shown to be optimal when using the same loss function for real and synthetic data, but the underlying reason is not fully explained.
- Cross-architecture generalization results are based on a limited set of experiments and may not generalize across all network architectures or datasets.

## Confidence
- **High confidence:** The overall framework of using contrastive divergence for Bayesian pseudo-coreset construction is well-justified and supported by experimental results.
- **Medium confidence:** The claim that finite-step Langevin dynamics provides sufficient approximation for the synthetic posterior sampling is plausible but lacks rigorous theoretical backing.
- **Medium confidence:** The empirical observation that using the same loss function for real and synthetic data yields better performance is convincing but may be dataset-dependent.

## Next Checks
1. **Theoretical analysis:** Derive bounds on the approximation error introduced by finite-step Langevin dynamics in the contrastive divergence loss, and relate it to the number of steps T and the mixing time of the synthetic posterior.
2. **Robustness to hyperparameters:** Systematically vary the Langevin step size α and the number of steps T, and measure the impact on the quality of the synthetic dataset (e.g., using a metric like FID or MMD) and downstream classification accuracy.
3. **Cross-architecture generalization study:** Test the method on a wider range of network architectures (e.g., ResNet, DenseNet) and datasets, and quantify the drop in performance when using synthetic data across architectures. Investigate whether ensembling trajectories from multiple architectures improves generalization.