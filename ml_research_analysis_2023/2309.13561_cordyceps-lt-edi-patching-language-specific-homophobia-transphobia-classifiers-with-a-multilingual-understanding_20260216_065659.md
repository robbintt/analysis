---
ver: rpa2
title: 'Cordyceps@LT-EDI: Patching Language-Specific Homophobia/Transphobia Classifiers
  with a Multilingual Understanding'
arxiv_id: '2309.13561'
source_url: https://arxiv.org/abs/2309.13561
tags:
- language
- hate
- speech
- languages
- homophobia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Language-PAINT, a joint multilingual and language-specific
  approach to detecting homophobia and transphobia in social media. The method combines
  a multilingual model with language-specific models using weight interpolation to
  leverage knowledge from both approaches.
---

# Cordyceps@LT-EDI: Patching Language-Specific Homophobia/Transphobia Classifiers with a Multilingual Understanding

## Quick Facts
- arXiv ID: 2309.13561
- Source URL: https://arxiv.org/abs/2309.13561
- Authors: 
- Reference count: 7
- One-line primary result: Achieved 0.997 macro average F1-score on Malayalam texts using weight interpolation between multilingual and language-specific models

## Executive Summary
This paper introduces Language-PAINT, a novel approach for detecting homophobia and transphobia in social media across five languages. The method combines multilingual understanding with language-specific cultural context through weight interpolation between a multilingual Bernice model and five language-specific models. Experiments demonstrate that this joint approach outperforms both pure multilingual and pure language-specific approaches, achieving the best results in three languages and showing particular strength in Malayalam with a 0.997 macro average F1-score. The approach is also robust to label distribution shifts, maintaining strong performance even under challenging data conditions.

## Method Summary
Language-PAINT works by training both a multilingual model and language-specific models on homophobia/transphobia detection data, then interpolating their weights using a parameter α that is optimized per language based on validation F1 scores. The framework uses the multilingual Bernice model as a foundation, fine-tunes separate language-specific models for each target language, and then creates interpolated models that combine the strengths of both approaches. The interpolation parameter is selected from a discrete set {0, 0.1, 0.2, ..., 1} based on validation performance for each language.

## Key Results
- Language-PAINT achieved the best results in three of the five tested languages
- The method attained a 0.997 macro average F1-score on Malayalam texts, demonstrating exceptional performance on low-resource languages
- The approach showed robustness to label distribution shifts, maintaining strong performance when training and test distributions differed significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interpolating between multilingual and language-specific model weights allows each to compensate for the other's weaknesses
- Mechanism: Language-specific models capture cultural and linguistic context better, while multilingual models capture cross-linguistic patterns and rare terms. Weight interpolation combines these complementary strengths
- Core assumption: The weaknesses of L-S models (missing cross-linguistic patterns) and M-L models (weaker cultural context) are complementary and can be balanced
- Evidence anchors:
  - [abstract] "M-L models are needed to catch words, phrases, and concepts that are less common or missing in a particular language and subsequently overlooked by L-S models. Nonetheless, L-S models are better situated to understand the cultural and linguistic context"
  - [section] "Knowledge of dogwhistles in their current form will make content moderation systems more robust to these signals as they appear in different languages in new online spaces"

### Mechanism 2
- Claim: The interpolation parameter α can be optimized per language to maximize performance
- Mechanism: By selecting α based on validation F1 scores for each language, the model can dynamically balance the contributions of L-S and M-L components
- Core assumption: The optimal interpolation ratio varies by language and can be determined empirically
- Evidence anchors:
  - [section] "Where θi is used to create predictions for the respective language i = 1, .., k in the test set. In practice, we select alpha from a discrete set α ∈ {0, 0.1, 0.2, ..., 1} and select based on the resulting model's F1 performance on a held-out validation set"
  - [section] "We can see for most languages, the L-S approach tends to perform best, with the exception of the Malayalam language"

### Mechanism 3
- Claim: The interpolation approach is robust to label distribution shifts
- Mechanism: By leveraging both multilingual and language-specific knowledge, the model maintains performance even when training and test label distributions differ
- Core assumption: The complementary knowledge from both approaches provides redundancy that helps maintain performance under distribution shifts
- Evidence anchors:
  - [section] "For our second experiment, our results (see Table 2) are much more in favor of our method. Perhaps the considerably worse performance of the L-S and M-S models is due to the high label-distribution shift between the re-sampled train and test splits. Nonetheless, LangPAINT appears to be robust to this shift"

## Foundational Learning

- Concept: Weight interpolation in neural networks
  - Why needed here: The entire approach depends on understanding how to combine weights from different models
  - Quick check question: If θz has weights [0.5, 0.3, 0.2] and θf has weights [0.2, 0.4, 0.4], what are the interpolated weights when α = 0.7?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding why multilingual models can help language-specific tasks requires knowledge of cross-lingual transfer mechanisms
  - Quick check question: What are the main challenges in cross-lingual transfer that might affect this approach?

- Concept: Hate speech detection nuances
  - Why needed here: The task involves detecting subtle forms of hate speech that vary culturally and linguistically
  - Quick check question: How might cultural context affect the interpretation of potentially hateful phrases in different languages?

## Architecture Onboarding

- Component map: Bernice (multilingual Twitter model) -> Language-specific fine-tuning -> Weight interpolation layer -> Ensemble layer
- Critical path: Bernice → Language-specific fine-tuning → Weight interpolation → Ensemble voting
- Design tradeoffs:
  - Simpler than training entirely new models for each language combination
  - Less data-intensive than full multilingual training
  - Potentially less optimal than task-specific architectures
  - Interpolation parameter adds complexity but enables adaptation
- Failure signatures:
  - Flat performance curves when varying α (indicating no benefit from interpolation)
  - Large performance gaps between languages (suggesting imbalanced contributions)
  - Degradation when label distributions shift (suggesting overfitting to specific patterns)
- First 3 experiments:
  1. Compare single α across all languages vs. language-specific α optimization
  2. Test performance with extreme α values (0 and 1) to understand individual contributions
  3. Evaluate robustness by intentionally creating label distribution shifts in validation data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Language-PAINT vary when applied to languages with significantly different linguistic structures or script systems (e.g., Chinese, Arabic, or Russian)?
- Basis in paper: [inferred] The paper primarily focuses on Indo-European languages (English, Spanish) and Dravidian languages (Tamil, Malayalam), with Hindi as a case study. There is no exploration of languages with significantly different linguistic structures or script systems.
- Why unresolved: The paper does not provide data or analysis on languages outside the studied set, leaving the generalizability of the approach to other language families unknown.
- What evidence would resolve it: Conducting experiments with Language-PAINT on languages with different linguistic structures and script systems, and comparing the results to those obtained for the languages studied in the paper.

### Open Question 2
- Question: What is the impact of varying the interpolation parameter α on the performance of Language-PAINT for languages with different levels of resource availability?
- Basis in paper: [explicit] The paper mentions selecting α from a discrete set and choosing based on validation performance, but does not explore how this parameter affects languages with different resource levels.
- Why unresolved: The paper does not provide a detailed analysis of how the interpolation parameter α influences the model's performance across languages with varying resource availability.
- What evidence would resolve it: Analyzing the effect of different α values on the performance of Language-PAINT for both high-resource and low-resource languages, and determining if there is an optimal α range for each category.

### Open Question 3
- Question: How does Language-PAINT perform in detecting more subtle forms of homophobia and transphobia, such as microaggressions or coded language, compared to explicit hate speech?
- Basis in paper: [inferred] The paper focuses on detecting homophobia and transphobia but does not distinguish between explicit hate speech and more subtle forms of discrimination.
- Why unresolved: The paper does not provide a detailed breakdown of the types of homophobia and transphobia detected, making it unclear how well the model performs on subtle forms of discrimination.
- What evidence would resolve it: Evaluating Language-PAINT on a dataset that includes both explicit and subtle forms of homophobia and transphobia, and comparing its performance on each category.

## Limitations
- Reliance on a single dataset from LT-EDI 2022 constrains generalizability
- Interpolation approach assumes complementary strengths that may not hold for all language pairs
- Grid search for α parameter is computationally expensive and may not scale well

## Confidence

- **High confidence** in the core mechanism of weight interpolation working as described, based on clear mathematical formulation and empirical results
- **Medium confidence** in the robustness claims, as the label distribution shift experiments were limited to a single artificial scenario
- **Low confidence** in the generalizability across different hate speech detection tasks, given the specialized nature of homophobia/transphobia detection

## Next Checks

1. **Cross-task validation**: Apply the Language-PAINT approach to a different hate speech detection dataset (e.g., general toxic language) to assess whether the interpolation mechanism generalizes beyond the specific domain of LGBTQ+ hate speech.

2. **Computational efficiency analysis**: Measure the wall-clock time and memory requirements for training the five language-specific models versus training a single multilingual model, to quantify the practical tradeoffs of the interpolation approach.

3. **Extreme distribution shift testing**: Create more severe label distribution shifts (e.g., 90/10 vs 10/90 class ratios) to rigorously test the robustness claims and identify failure thresholds where neither component model has relevant knowledge.