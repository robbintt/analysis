---
ver: rpa2
title: Multi-level Multiple Instance Learning with Transformer for Whole Slide Image
  Classification
arxiv_id: '2306.05029'
source_url: https://arxiv.org/abs/2306.05029
tags:
- grouping
- sub-bags
- learning
- transformer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Multi-level Multiple Instance Learning with
  Transformer (MMIL-Transformer) approach to address the challenge of Whole Slide
  Image (WSI) classification. The proposed method introduces a hierarchical structure
  to Multiple Instance Learning (MIL) and employs a messenger-based self-attention
  mechanism to efficiently handle large-scale MIL tasks.
---

# Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification

## Quick Facts
- arXiv ID: 2306.05029
- Source URL: https://arxiv.org/abs/2306.05029
- Reference count: 40
- Key outcome: Achieves 94.74% AUC and 93.41% accuracy on CAMELYON16, and 99.04% AUC and 94.37% accuracy on TCGA-NSCLC for WSI classification

## Executive Summary
This paper addresses the challenge of Whole Slide Image (WSI) classification by proposing a Multi-level Multiple Instance Learning with Transformer (MMIL-Transformer) approach. The method introduces a hierarchical structure to Multiple Instance Learning (MIL) and employs a messenger-based self-attention mechanism to efficiently handle large-scale MIL tasks. By partitioning instance sequences into sub-bags and using messenger tokens to capture intra-sub-bag information, the approach enables exact self-attention without the quadratic cost of full-sequence attention. The method is validated on CAMELYON16 and TCGA-NSCLC datasets, demonstrating significant improvements over state-of-the-art methods.

## Method Summary
The MMIL-Transformer framework processes WSIs by first extracting feature embeddings using ResNet, then grouping patches into sub-bags using random assignment to mitigate class imbalance. Messenger tokens are introduced to capture information within each sub-bag through self-attention, and these messenger tokens are then merged to form higher-level bags. This hierarchical structure is repeated for multiple levels, with optional masking to improve computational efficiency. The final classification is performed using an MLP on the CLS token. The approach achieves exact self-attention with reduced computational complexity by limiting attention to sub-bags rather than the full instance sequence.

## Key Results
- Achieves 94.74% AUC and 93.41% accuracy on CAMELYON16 dataset
- Achieves 99.04% AUC and 94.37% accuracy on TCGA-NSCLC dataset
- Outperforms existing state-of-the-art methods including DeepMIL and WSI-Transformer
- Demonstrates effectiveness of hierarchical MIL framework in WSI-based digital diagnosis

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical MIL with messenger tokens enables non-approximate self-attention on long WSI sequences by partitioning the long instance sequence into sub-bags, applying self-attention within each sub-bag using messenger tokens, and then merging the messenger tokens to form higher-level bags. This allows the model to perform exact self-attention without the quadratic cost of full-sequence attention. The core assumption is that messenger tokens can effectively capture and transmit intra-sub-bag information to the next level without losing critical instance relationships.

### Mechanism 2
Random grouping of instances reduces the impact of class imbalance in MIL tasks by ensuring each sub-bag contains a mix of positive and negative instances. This prevents the model from focusing too heavily on highly imbalanced sub-bags and enables the attention mechanism to focus on positive instances within each sub-bag. The core assumption is that random grouping ensures a more uniform distribution of instance labels across sub-bags, improving the robustness of the attention mechanism.

### Mechanism 3
Multi-level bags reduce the computational complexity of self-attention from O(p²) to O(p²/g) per layer by dividing the instance sequence into g sub-bags, where the self-attention complexity within each sub-bag becomes O((p/g)²). The total complexity across all sub-bags is O(p²/g), which scales linearly with the inverse of the number of sub-bags. The core assumption is that the reduction in computational cost does not significantly impair the model's ability to capture global relationships between instances.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: WSIs lack pixel-level annotations, so MIL provides a framework to learn from bag-level labels while inferring instance-level information.
  - Quick check question: What is the key difference between instance-based and embedding-based MIL, and why does this paper use embedding-based MIL?

- Concept: Self-attention and Transformers
  - Why needed here: Standard CNNs struggle with the long-range dependencies in gigapixel WSIs, while Transformers can model complex relationships between instances.
  - Quick check question: How does the complexity of self-attention scale with sequence length, and why is this problematic for WSIs?

- Concept: Hierarchical representation learning
  - Why needed here: Building multi-level bags allows the model to progressively aggregate information from fine-grained patches to whole-slide representations.
  - Quick check question: What is the role of messenger tokens in the hierarchical structure, and how do they facilitate information flow between levels?

## Architecture Onboarding

- Component map:
  - WSI patches -> ResNet feature embeddings -> Random grouping into sub-bags -> Messenger-based MSA within each sub-bag -> Merging of messenger tokens -> Higher-level bags -> MLP classifier

- Critical path:
  1. Preprocess WSI into patches and extract features
  2. Group patches into sub-bags
  3. Apply messenger-based MSA to each sub-bag
  4. Merge messenger tokens to build higher-level bags
  5. Repeat for desired number of levels
  6. Classify using CLS token

- Design tradeoffs:
  - Number of sub-bags vs. memory/computation: More sub-bags reduce complexity but may impair communication
  - Grouping strategy: Random grouping helps with imbalance but loses spatial information; coordinate grouping preserves spatial structure but may worsen imbalance
  - Masking ratio: Higher masking improves efficiency but risks losing important information

- Failure signatures:
  - Performance degrades significantly when using coordinate or sequential grouping (loss of randomness)
  - Accuracy drops sharply if the number of sub-bags is too large (insufficient instances per sub-bag)
  - Model underperforms without masking on large datasets (excessive computation)

- First 3 experiments:
  1. Validate that random grouping improves accuracy over coordinate/sequential grouping on CAMELYON16
  2. Test the effect of masking ratio on both accuracy and training speed
  3. Compare performance with and without the hierarchical multi-level framework using Nyström attention as baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of grouping operator affect the performance of MMIL-Transformer on highly imbalanced WSI datasets? While the paper provides experimental results comparing different grouping methods, it does not offer a comprehensive theoretical analysis of why certain operators perform better on imbalanced data. The optimal grouping strategy for different types of imbalance is not fully explored.

### Open Question 2
What is the optimal number of hierarchical levels in MMIL-Transformer for different WSI classification tasks? The paper only tested up to two levels and did not explore scenarios where deeper hierarchies might be beneficial. The trade-off between model complexity and performance gains from additional levels is not fully characterized.

### Open Question 3
How does the messenger-based self-attention mechanism in MMIL-Transformer compare to other attention approximation methods in terms of interpretability and performance? While the paper shows improved performance, it does not provide a detailed comparison of interpretability aspects or benchmark against a wider range of attention approximation methods under varying conditions.

## Limitations

- The novel messenger mechanism lacks explicit validation through ablation studies in the paper
- The claim that random grouping specifically addresses class imbalance requires more rigorous validation
- The complexity analysis assumes uniform sub-bag sizes, but real-world WSI data may violate this assumption
- The performance improvements on TCGA-NSCLC (99.04% AUC) may indicate potential overfitting or an easier dataset

## Confidence

- **High Confidence**: The general framework of hierarchical MIL with messenger tokens is technically sound and the complexity reduction claims are mathematically correct
- **Medium Confidence**: The empirical results showing performance improvements over baseline methods are promising but would benefit from more extensive ablation studies
- **Low Confidence**: The claim that random grouping specifically addresses class imbalance requires more rigorous validation, as the paper only provides observational evidence

## Next Checks

1. **Ablation Study on Grouping Strategies**: Systematically compare random, coordinate, embedding, and sequential grouping across multiple datasets with varying class imbalance ratios to quantify the impact on performance and validate the class imbalance hypothesis.

2. **Messenger Token Sensitivity Analysis**: Evaluate how the number of messenger tokens, their initialization strategy, and the merging operation affect both performance and computational efficiency across different WSI datasets.

3. **Cross-Dataset Generalization Test**: Train the MMIL-Transformer on CAMELYON16 and evaluate on an independent breast cancer WSI dataset (or vice versa) to assess whether the model learns generalizable representations rather than dataset-specific patterns.