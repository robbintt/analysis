---
ver: rpa2
title: 'SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification'
arxiv_id: '2311.00048'
source_url: https://arxiv.org/abs/2311.00048
tags:
- learning
- instance
- sparse
- feature
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of whole slide image (WSI) classification
  using weakly supervised multiple instance learning (MIL). The authors propose a
  novel SC-MIL framework that integrates unrolled sparse dictionary learning into
  MIL.
---

# SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification

## Quick Facts
- arXiv ID: 2311.00048
- Source URL: https://arxiv.org/abs/2311.00048
- Authors: 
- Reference count: 40
- Primary result: Proposes SC-MIL framework integrating unrolled sparse dictionary learning into MIL for WSI classification, achieving significant performance improvements on multiple datasets

## Executive Summary
This paper addresses the challenge of whole slide image (WSI) classification using weakly supervised multiple instance learning (MIL). The authors propose SC-MIL, a novel framework that integrates unrolled sparse dictionary learning into MIL to enhance instance feature embeddings. By expressing instances as sparse linear combinations of atoms in an over-complete dictionary, SC-MIL captures global instance similarities and improves interpretability. The method is evaluated on multiple datasets, including five classical MIL benchmarks, MNIST-bags, CAMELYON16, and TCGA-NSCLC, demonstrating significant performance improvements over state-of-the-art MIL methods.

## Method Summary
SC-MIL integrates unrolled sparse dictionary learning into existing MIL frameworks through a plug-and-play SC module. The method first extracts 224×224 patches from WSIs using a ResNet-18 feature extractor. These patches are then processed through the SC module, which learns a sparse representation by expressing each instance as a linear combination of atoms in an over-complete dictionary. The dictionary and sparsity parameters are optimized through unrolled iterative soft thresholding algorithm (ISTA) layers. A feed-forward network learns the optimal sparsity regularization strength λ for each bag. The enhanced instance features are then aggregated using various MIL methods (attention-based, non-local, transformer, or knowledge distillation) and classified at the bag level using cross-entropy loss.

## Key Results
- SC-MIL achieves an average AUC gain of 4.01% and 2.60% on CAMELYON16 using ImageNet and SimCLR pre-training, respectively
- On TCGA-NSCLC, SC-MIL improves average accuracy by 6.63% and 4.01% using features from ImageNet and SimCLR pre-trained ResNet-18
- The SC module can be easily incorporated into existing MIL frameworks in a plug-and-play manner
- SC-MIL consistently outperforms state-of-the-art MIL methods across five classical MIL benchmarks and MNIST-bags dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SC-MIL improves instance feature embeddings by expressing them as sparse linear combinations of atoms in an over-complete dictionary
- Mechanism: Sparse dictionary learning models similarities across instances by representing each instance as a sparse linear combination of atoms in a learned dictionary. The ℓ1 sparsity penalty suppresses irrelevant instances while retaining the most relevant ones
- Core assumption: The initial instance embeddings lie in a low-dimensional manifold that can be effectively captured by sparse linear combinations of dictionary atoms
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the assumption that instance embeddings lie in a low-dimensional manifold is violated, the sparse representation becomes less effective or unnecessary

### Mechanism 2
- Claim: The unrolled sparse dictionary learning is compatible with deep learning and can be optimized through end-to-end training
- Mechanism: By unrolling the Iterative Soft Thresholding Algorithm (ISTA) into a series of network layers, the sparse dictionary learning becomes differentiable and trainable through backpropagation
- Core assumption: The unrolled ISTA maintains the convergence properties of the original iterative algorithm while being differentiable
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the unrolled network fails to converge to good solutions or the unrolled layers cannot capture the necessary sparse structure

### Mechanism 3
- Claim: The sparsity regularization strength λ can be learned per bag through a feed-forward network
- Mechanism: Instead of manually tuning λ, it is parameterized as a feed-forward network that takes the instance embeddings as input and outputs a scalar λi for each bag
- Core assumption: The optimal sparsity level for a bag can be predicted from its instance embeddings
- Evidence anchors: [section], [abstract], [corpus]
- Break condition: If the feed-forward network cannot accurately predict the optimal λ for different bags, leading to suboptimal sparsity levels

## Foundational Learning

- Concept: Multiple Instance Learning (MIL) framework
  - Why needed here: The entire paper builds upon MIL concepts, where bags contain instances and labels are assigned at the bag level
  - Quick check question: In MIL, if a bag contains at least one positive instance, how is the bag labeled?

- Concept: Sparse Dictionary Learning
  - Why needed here: SC-MIL's core innovation relies on expressing instance embeddings as sparse linear combinations of dictionary atoms
  - Quick check question: What is the role of the ℓ1 penalty in sparse dictionary learning?

- Concept: Algorithm Unrolling
  - Why needed here: The paper converts the iterative sparse coding algorithm into a series of network layers to make it compatible with deep learning
  - Quick check question: What is the main advantage of unrolling an iterative algorithm into a neural network layer?

## Architecture Onboarding

- Component map:
  Feature Extractor (ResNet-18) → SC Module (dictionary learning) → MIL Aggregator → Bag Classifier

- Critical path:
  Feature Extractor → SC Module (dictionary learning) → MIL Aggregator → Bag Classifier

- Design tradeoffs:
  - Number of dictionary atoms (m) vs. computational cost: More atoms capture more patterns but increase parameters and FLOPs
  - Number of unrolled layers (L) vs. performance: More layers may improve convergence but increase computation
  - Fixed vs. learned λ: Learned λ adapts to each bag but adds a small network overhead

- Failure signatures:
  - Poor convergence during training: Check initialization of dictionary and stepsize µ
  - No performance improvement over baseline: Verify that SC module is properly integrated and dictionary learning is effective
  - High computational cost: Reduce m or L, or consider more efficient implementations

- First 3 experiments:
  1. Integrate SC module into ABMIL on MNIST-bags with m=16, L=6, verify attention distribution improves
  2. Test SC module with different m values (4, 8, 16, 32) on CAMELYON16 to find optimal tradeoff
  3. Compare SC-MIL performance with and without SimCLR pre-training on TCGA-NSCLC to assess feature quality dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of atoms (m) and number of unrolled layers (L) for the SC module in different MIL tasks and datasets?
- Basis in paper: [explicit] The paper discusses the impact of hyperparameter selection (number of atoms in the dictionary m and number of unrolled layers L) on performance and presents ablation studies
- Why unresolved: The optimal values of m and L may vary depending on the specific MIL task and dataset characteristics, requiring further investigation
- What evidence would resolve it: Systematic experiments across diverse MIL tasks and datasets to determine the optimal values of m and L that maximize performance

### Open Question 2
- Question: How does the performance of SC-MIL compare to other state-of-the-art methods when applied to real-world WSI datasets with limited annotations?
- Basis in paper: [inferred] The paper mentions the potential of SC-MIL in real-world applications, but does not provide direct comparisons on real-world WSI datasets with limited annotations
- Why unresolved: Real-world WSI datasets often have limited annotations, making it challenging to evaluate the effectiveness of MIL methods
- What evidence would resolve it: Comparative experiments on real-world WSI datasets with limited annotations to assess the performance of SC-MIL against other state-of-the-art methods

### Open Question 3
- Question: Can the SC module be extended to handle more complex MIL scenarios, such as multi-label classification or instance-level predictions?
- Basis in paper: [explicit] The paper focuses on bag-level binary MIL classification and does not explore extensions to more complex MIL scenarios
- Why unresolved: The proposed SC module may need modifications to handle more complex MIL scenarios beyond binary classification
- What evidence would resolve it: Extensions of the SC module to handle multi-label classification or instance-level predictions, along with experimental results demonstrating its effectiveness in these scenarios

## Limitations
- The method relies heavily on pre-trained feature extractors, potentially limiting generalization to untrained networks
- Computational overhead from unrolled sparse dictionary learning with multiple dictionary atoms and unrolled layers may limit scalability to very large WSIs
- The adaptive λ learning mechanism's effectiveness depends on the feed-forward network's ability to accurately predict sparsity levels across diverse tissue types

## Confidence
- High confidence in the SC module's implementation and empirical validation on benchmark datasets
- Medium confidence in the scalability claims due to computational complexity considerations
- Low confidence in the universality of the method across different feature extraction backbones without extensive validation

## Next Checks
1. Validate SC-MIL performance with randomly initialized ResNet-18 (no pre-training) to assess dependency on pre-trained features
2. Measure computational overhead empirically on WSIs of varying sizes to quantify scalability limits
3. Test the method's sensitivity to SC module hyperparameters (m, L, λ range) through ablation studies on CAMELYON16