---
ver: rpa2
title: Machine Learning Technique Based Fake News Detection
arxiv_id: '2309.13069'
source_url: https://arxiv.org/abs/2309.13069
tags:
- news
- used
- fake
- have
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates fake news detection using both traditional\
  \ machine learning and deep learning models. The authors train and evaluate five\
  \ models\u2014Naive Bayes, Logistic Regression, SGD, LSTM, and AWD-LSTM\u2014on\
  \ a dataset of 1,876 news articles."
---

# Machine Learning Technique Based Fake News Detection

## Quick Facts
- arXiv ID: 2309.13069
- Source URL: https://arxiv.org/abs/2309.13069
- Reference count: 28
- Best-performing model: Naive Bayes with 56% accuracy and 32% F1-macro

## Executive Summary
This paper investigates fake news detection using both traditional machine learning and deep learning models. The authors train and evaluate five models—Naive Bayes, Logistic Regression, SGD, LSTM, and AWD-LSTM—on a dataset of 1,876 news articles. They preprocess the text using NLP techniques and test various feature extraction methods (TF-IDF, CountVectorizer, Word2vec, FastAI tokenization). The Naive Bayes classifier achieved the highest performance with 56% accuracy and an F1-macro score of 32%, outperforming the other models. Logistic Regression and SGD scored lower, while LSTM and AWD-LSTM performed worst, likely due to limited data. The authors conclude that Naive Bayes is best suited for this task and suggest further exploration of models for future work.

## Method Summary
The authors investigate fake news detection by training five models on a dataset of 1,876 news articles. They preprocess text by removing URLs, emails, non-ASCII characters, and stop words, then lemmatize the remaining words. Features are extracted using TF-IDF, CountVectorizer, Word2Vec, and FastAI tokenization. The models evaluated include Naive Bayes (MultinomialNB), Logistic Regression, SGDClassifier, LSTM (Sequential with word embeddings), and AWD-LSTM (FastAI wrapper). Performance is measured using accuracy and F1-macro score on a held-out test set.

## Key Results
- Naive Bayes achieved the highest performance with 56% accuracy and 32% F1-macro score
- LSTM and AWD-LSTM performed worst, likely due to limited data (1,876 articles)
- Logistic Regression and SGD scored lower than Naive Bayes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Naive Bayes classifier outperforms other models due to its assumption of feature independence, which works well when the dataset has sparse, high-dimensional textual features like TF-IDF or CountVectorizer.
- Mechanism: By treating each word as an independent signal, Naive Bayes avoids overfitting to feature interactions that small datasets cannot support reliably. This reduces model variance and helps maintain stability in low-data regimes.
- Core assumption: The features (words) are conditionally independent given the class label, which may not hold in natural language but still yields robust performance for this task.
- Evidence anchors:
  - [abstract] "Our research conducts 3 popular Machine Learning... algorithms. After we have found our best Naive Bayes classifier with 56% accuracy and an F1-macro score of an average of 32%."
  - [section] "Naive bayes incorporate strong independence assumptions in datasets so in this task, this algorithm works well among other 4 algorithms..."
  - [corpus] Weak; no related works explicitly discuss independence assumptions for this specific model setup.
- Break condition: If the dataset grows significantly in size and diversity, or if word co-occurrence patterns become predictive, the independence assumption may degrade performance relative to models that capture interactions.

### Mechanism 2
- Claim: Deep learning models (LSTM, AWD-LSTM) underperform because the dataset is too small (1,876 articles) for them to learn meaningful sequential dependencies.
- Mechanism: Deep models require large volumes of data to converge and generalize; with only ~1,800 samples, they overfit or fail to capture long-range dependencies, leading to lower accuracy.
- Core assumption: The models' capacity exceeds the information content of the training set, causing poor generalization.
- Evidence anchors:
  - [abstract] "LSTM and AWD-LSTM performed worst, likely due to limited data."
  - [section] "our deep learning model doesn't do well for this fake news detection problem as deep learning models need more data to predict more accurately."
  - [corpus] Weak; corpus neighbors focus on other fake news detection papers but do not analyze model-data size mismatches.
- Break condition: Adding more labeled examples or using pre-trained embeddings fine-tuned on similar text could improve deep model performance.

### Mechanism 3
- Claim: TF-IDF and CountVectorizer feature extraction methods preserve discriminative word-level signals that are directly useful for Naive Bayes classification.
- Mechanism: These sparse, interpretable feature spaces align with Naive Bayes' probabilistic framework, allowing the model to weight words by their informativeness relative to the corpus.
- Core assumption: The bag-of-words representation captures sufficient signal for the classification task without needing complex embeddings.
- Evidence anchors:
  - [section] "We have mentioned step by step by process... We have to transform that raw data into some features extracted from. In our work we have used TF-IDF, CountVectorizer..."
  - [section] "For TF-IDF, we have used this tool from the scikit-learn v1.0.2 Python library feature_extraction.text module."
  - [corpus] Weak; corpus papers do not detail feature extraction choices or compare their effects.
- Break condition: If the dataset contained more nuanced semantic relationships or longer-range context, richer embeddings (e.g., contextual word vectors) might yield better results.

## Foundational Learning

- Concept: Text preprocessing (lowercasing, stopword removal, lemmatization)
  - Why needed here: Raw text contains noise (URLs, punctuation, case variation) that can mislead models; cleaning standardizes input and reduces dimensionality.
  - Quick check question: What preprocessing step removes common words like "the," "is," and "at" to reduce noise?

- Concept: Feature extraction (TF-IDF, CountVectorizer)
  - Why needed here: Machine learning models require numerical input; these methods convert text into sparse numeric vectors that preserve word importance.
  - Quick check question: Which feature extraction method weights words by their rarity across documents?

- Concept: Class imbalance handling
  - Why needed here: The dataset is imbalanced with more "False" labeled examples, which can bias the model toward predicting the majority class.
  - Quick check question: What problem arises if one class dominates the training set in terms of model predictions?

## Architecture Onboarding

- Component map: CSV ingestion -> Preprocessing (cleaning, lemmatization) -> Feature extraction (TF-IDF, CountVectorizer, Word2Vec) -> Train-test split -> Model training (Naive Bayes, Logistic Regression, SGD, LSTM, AWD-LSTM) -> Evaluation (accuracy, F1-macro)
- Critical path: Preprocessing -> Feature extraction -> Model training -> Evaluation. Each step must complete before the next.
- Design tradeoffs: Naive Bayes is fast and robust but ignores word order; deep models can capture sequence but require more data; sparse features are interpretable but lose semantic nuance.
- Failure signatures: Low accuracy and F1-macro indicate model underfitting; high variance between training and test scores indicates overfitting; class-wise confusion shows bias toward majority class.
- First 3 experiments:
  1. Train Naive Bayes with CountVectorizer on clean text; record accuracy/F1.
  2. Train Logistic Regression with TF-IDF; compare performance to Naive Bayes.
  3. Train LSTM with Word2Vec embeddings; observe if data size limits performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the performance differences between traditional ML and DL models when trained on larger datasets for fake news detection?
- Basis in paper: [explicit] The authors note that LSTM and AWD-LSTM performed worst, likely due to limited data, and suggest further exploration of models for future work.
- Why unresolved: The current study used a relatively small dataset (1,876 articles), and the authors acknowledge that DL models typically require more data to perform well.
- What evidence would resolve it: Testing the same models on a significantly larger dataset (e.g., 10,000+ articles) and comparing performance metrics like accuracy and F1 scores.

### Open Question 2
- Question: How does incorporating multimodal features (e.g., images, videos) impact fake news detection accuracy compared to text-only approaches?
- Basis in paper: [inferred] The literature review mentions that some authors used visual features alongside textual features, and the authors suggest exploring new research models in the future.
- Why unresolved: The current study only used textual features, and there's no comparison with multimodal approaches.
- What evidence would resolve it: Training and evaluating models that incorporate both text and visual features, then comparing their performance to text-only models on the same dataset.

### Open Question 3
- Question: What is the optimal feature extraction method for fake news detection across different model types?
- Basis in paper: [explicit] The authors tested TF-IDF, CountVectorizer, Word2vec, and FastAI tokenization, with Naive Bayes performing best using CountVectorizer.
- Why unresolved: While Naive Bayes performed best with CountVectorizer, it's unclear if this holds true for other models or if different feature extraction methods would yield better results with more data.
- What evidence would resolve it: Systematically testing all combinations of feature extraction methods and models on various dataset sizes to identify optimal pairings.

## Limitations
- Dataset details are not fully specified, making it difficult to reproduce the exact conditions and results
- Performance metrics (56% accuracy, 32% F1-macro) are relatively low, suggesting the task may be inherently challenging or the data limited
- The paper does not explore ensemble methods or hybrid approaches that could potentially improve performance

## Confidence
- **Medium confidence** in the claim that Naive Bayes outperforms other models due to its independence assumptions and robustness to sparse, high-dimensional features
- **Medium confidence** in the assertion that deep learning models underperform due to limited data, though this could be mitigated with more examples or transfer learning
- **Low confidence** in the overall conclusions, given the modest performance metrics and lack of comparison with state-of-the-art baselines

## Next Checks
1. **Reproduce results**: Obtain or recreate the dataset and retrain the five models (Naive Bayes, Logistic Regression, SGD, LSTM, AWD-LSTM) using the specified preprocessing and feature extraction methods. Compare the accuracy and F1-macro scores to the reported values.
2. **Test data size sensitivity**: Vary the training set size and observe how each model's performance changes. This will help confirm whether deep models truly underperform due to data limitations.
3. **Explore class imbalance**: Analyze the class distribution in the dataset and apply techniques like class weighting or oversampling to mitigate potential bias toward the majority class. Retrain models and compare results.