---
ver: rpa2
title: Adapting Text-based Dialogue State Tracker for Spoken Dialogues
arxiv_id: '2308.15053'
source_url: https://arxiv.org/abs/2308.15053
tags:
- dialogue
- system
- error
- spoken
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of adapting text-based dialogue
  state tracking (DST) models for spoken dialogue systems. The authors propose a three-module
  approach: (1) ASR error correction using a T5 model to align ASR hypotheses with
  ground truth text, (2) a description-driven DST model (D3ST) that incorporates slot
  descriptions and uses random ordering of target states during training, and (3)
  post-processing to recover errors in proper nouns using a Levenshtein distance ratio-based
  similarity metric.'
---

# Adapting Text-based Dialogue State Tracker for Spoken Dialogues

## Quick Facts
- **arXiv ID**: 2308.15053
- **Source URL**: https://arxiv.org/abs/2308.15053
- **Reference count**: 5
- **Primary result**: Three-module approach achieves 40.2% JGA on TTS-verbatim test set in DSTC11 challenge

## Executive Summary
This paper presents a novel approach for adapting text-based dialogue state tracking (DST) models to handle spoken dialogue systems by addressing automatic speech recognition (ASR) errors. The authors propose a three-module pipeline consisting of ASR error correction, a description-driven DST model with random ordering, and post-processing for proper noun recovery. The system ranks third in the DSTC11 challenge, demonstrating the effectiveness of explicit ASR error correction and proper noun handling in improving dialogue state tracking performance on spoken data.

## Method Summary
The approach consists of three modules: (1) ASR error correction using a T5-base model fine-tuned to map ASR hypotheses to ground truth text, (2) a description-driven DST (D3ST) model incorporating slot descriptions with random ordering of target states during training, and (3) post-processing using Levenshtein distance ratio-based similarity matching to recover proper noun errors. The T5-large model is used for D3ST fine-tuned on MultiWOZ, while T5-base handles both ASR correction and D3ST. Random ordering during training prevents slot-order bias, and the proper noun database is derived from Wikipedia for post-processing.

## Key Results
- Achieves 40.2% joint goal accuracy (JGA) on TTS-verbatim test set
- Ranks third among participants in DSTC11 challenge
- Demonstrates importance of explicit ASR error correction and post-processing for proper nouns
- Shows data augmentation through random ordering improves robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR error correction improves JGA by reducing the propagation of transcription errors to the dialogue state tracker.
- Mechanism: A T5-based seq2seq model is fine-tuned to map ASR hypotheses to ground truth text, effectively recovering formatting, special characters, and time formats before state tracking.
- Core assumption: The ASR hypothesis and ground truth share enough semantic overlap for the model to learn a reliable correction mapping.
- Evidence anchors: Abstract mentions "explicit automatic speech recognition error correction module" and experiments show it's important; corpus supports but is sparse on quantitative backing.

### Mechanism 2
- Claim: Post-processing with Levenshtein distance ratio recovers proper noun errors that seq2seq correction cannot handle.
- Mechanism: After DST prediction, a similarity metric compares misrecognized slot values against a Wikipedia-derived proper noun database and replaces low-similarity matches with closest candidates.
- Core assumption: Proper noun spelling errors are small edit-distance perturbations and the database covers all domain-relevant names.
- Evidence anchors: Abstract mentions "post-processing for recovering the error of the estimated slot value"; corpus supports the algorithmic choice but not the proper noun coverage claim.

### Mechanism 3
- Claim: Random ordering of target dialogue states during training prevents slot-order bias and improves generalization.
- Mechanism: D3ST input is augmented with randomly permuted slot-value pairs, forcing the model to predict each slot independently of position.
- Core assumption: The model's generation head can resolve each slot without relying on fixed sequence cues.
- Evidence anchors: Abstract mentions "D3ST that incorporates slot descriptions and uses random ordering of target states during training"; corpus confirms intent but lacks ablation results.

## Foundational Learning

- Concept: Seq2seq fine-tuning for domain-specific text correction
  - Why needed here: Standard ASR output contains task-specific formatting errors that generic correction models cannot handle.
  - Quick check question: Can a T5 model trained on ASR hypothesis→ground truth pairs generalize to unseen formatting errors?

- Concept: Edit-distance-based string similarity for proper noun recovery
  - Why needed here: Named entities are often misrecognized and cannot be recovered by semantic models alone.
  - Quick check question: Does the Levenshtein ratio threshold chosen maximize precision without excessive false positives?

- Concept: Random permutation data augmentation for sequence prediction
  - Why needed here: Prevents the DST model from learning spurious positional dependencies in slot prediction.
  - Quick check question: How does random ordering affect convergence speed and final accuracy compared to fixed order?

## Architecture Onboarding

- Component map: Audio → ASR → T5 ASR Correction → D3ST with Random Ordering → Levenshtein Post-Processing → JGA/SER Metrics
- Critical path: ASR Correction → D3ST Prediction (any break here dominates final error rate)
- Design tradeoffs: Larger T5 models increase accuracy but slow inference; chosen T5-base balances speed and quality. Post-processing adds small latency overhead but recovers ~15% of proper noun errors. Random ordering doubles training time but improves robustness.
- Failure signatures: High SER but stable JGA → ASR correction failing on formatting but DST model compensating. Low JGA, high variance across slots → post-processing missing many proper nouns or random ordering too aggressive.
- First 3 experiments:
  1. Measure sentence error rate drop after ASR correction on a held-out ASR set.
  2. Ablation: run D3ST with and without random ordering on validation to quantify slot-order bias.
  3. Post-processing precision-recall on a curated proper noun test set to tune Levenshtein threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ASR error correction models vary with different sizes of pre-trained language models?
- Basis in paper: Authors mention using T5-base and T5-large and observed performance differences but could not experiment with larger models due to constraints.
- Why unresolved: Only experimented with T5-base and T5-large; authors mention experimental constraints prevented testing larger models.
- What evidence would resolve it: Conducting experiments with various sizes of pre-trained language models (e.g., T5-small, T5-XL, T5-XXL) for ASR error correction and comparing their performance.

### Open Question 2
- Question: How can proper noun recognition and correction be improved in spoken dialogue systems?
- Basis in paper: Authors mention proper nouns like hotel and restaurant names pose challenges for ASR error correction and post-processing modules.
- Why unresolved: The paper's post-processing module using Levenshtein distance ratio-based similarity metric did not fully address proper noun recognition and correction.
- What evidence would resolve it: Developing and evaluating new techniques for proper noun recognition and correction, such as using named entity recognition (NER) models or creating a dedicated proper noun database.

### Open Question 3
- Question: What is the impact of data augmentation techniques on the performance of spoken dialogue systems?
- Basis in paper: Authors mention data augmentation was used and contributed to higher performance.
- Why unresolved: Paper does not provide detailed analysis of specific data augmentation techniques or their individual contributions.
- What evidence would resolve it: Conducting ablation studies to evaluate the impact of different data augmentation techniques (e.g., adding noise, paraphrasing, back-translation) on performance.

## Limitations
- Performance is benchmarked exclusively on DSTC11 challenge dataset (travel booking domain), limiting generalizability to other domains or languages.
- Proper noun recovery relies heavily on Wikipedia-derived database without coverage metrics or domain relevance filtering, creating uncertainty about effectiveness for domain-specific terminology.
- Random ordering technique lacks ablation studies demonstrating marginal benefit over fixed ordering, leaving unclear whether added training complexity is justified.

## Confidence

**High Confidence**: ASR error correction module using T5 successfully reduces transcription errors in TTS-verbatim test set, as evidenced by measurable JGA improvements. Levenshtein distance ratio post-processing effectively recovers proper noun errors when database coverage is sufficient.

**Medium Confidence**: Random ordering of target dialogue states during training prevents slot-order bias and improves generalization, though direct ablation evidence is limited. Overall three-module pipeline achieves state-of-the-art performance on DSTC11 dataset, ranking third among participants.

**Low Confidence**: Proposed system will generalize effectively to other spoken dialogue domains beyond DSTC11 challenge. Computational overhead of all three modules is justified by accuracy gains in practical deployment scenarios.

## Next Checks

1. **Cross-Domain Transfer Validation**: Evaluate complete pipeline on at least two additional spoken dialogue datasets from different domains (e.g., restaurant reservations, technical support) to quantify performance degradation and identify domain-specific failure modes.

2. **Ablation Study for Random Ordering**: Conduct controlled experiments comparing D3ST performance with fixed ordering, random ordering, and no ordering augmentation to isolate marginal benefit of random ordering technique and determine optimal training strategies.

3. **Proper Noun Database Coverage Analysis**: Measure precision and recall of Levenshtein-based post-processing module by creating curated test set of domain-relevant proper nouns, including both Wikipedia-covered and domain-specific entities, to identify coverage gaps and optimize similarity thresholds.