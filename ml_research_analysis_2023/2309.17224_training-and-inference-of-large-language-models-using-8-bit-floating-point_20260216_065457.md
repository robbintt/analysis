---
ver: rpa2
title: Training and inference of large language models using 8-bit floating point
arxiv_id: '2309.17224'
source_url: https://arxiv.org/abs/2309.17224
tags:
- scaling
- bias
- fp16
- weights
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a methodology for selecting per-tensor scaling
  biases in FP8 linear layers, enabling accurate training and inference of large language
  models like GPT and Llama 2. The approach dynamically updates scaling biases for
  weights, activations, and gradients during training, while using post-training quantization
  for inference.
---

# Training and inference of large language models using 8-bit floating point

## Quick Facts
- **arXiv ID:** 2309.17224
- **Source URL:** https://arxiv.org/abs/2309.17224
- **Reference count:** 40
- **Key outcome:** FP8 achieves comparable accuracy to FP16 across model sizes from 111M to 70B parameters on tasks like MNLI, QQP, and SST-2, with dynamic per-tensor scaling biases preventing underflow/overflow.

## Executive Summary
This paper presents a methodology for training and inference of large language models using 8-bit floating point (FP8) formats. The approach introduces per-tensor scaling biases that dynamically update during training for weights, activations, and gradients, while using post-training quantization for inference. Experiments demonstrate that FP8 achieves comparable accuracy to FP16 across model sizes from 111M to 70B parameters on various benchmarks, enabling significant memory and compute savings with minimal accuracy loss.

## Method Summary
The methodology employs dynamic per-tensor scaling biases for weights, activations, and gradients during FP8 training, while using post-training quantization for inference. Different FP8 formats are utilized: E4 for weights and activations, and E5 for gradients to maintain numerical accuracy. The scaling biases shift FP16 distributions into the representable range of FP8 before casting, preventing underflow and overflow. Matrix multiplications use higher precision accumulation to avoid overflow, and the approach is validated across GPT and Llama 2 architectures on various benchmarks.

## Key Results
- FP8 inference matches FP16 accuracy across all tested model sizes (111M to 70B parameters)
- FP8-AMAX training maintains comparable accuracy to FP16 baselines on GLUE tasks
- FP8-CSCALE shows limitations for larger models during fine-tuning, indicating scaling bias range constraints
- The methodology enables significant memory and compute savings while preserving model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-tensor scaling biases prevent underflow and overflow in FP8 matrix multiplications.
- Mechanism: By dynamically computing and applying scaling biases to weights, activations, and gradients before casting to FP8, the values are shifted into the representable range of FP8 E4/E5 formats.
- Core assumption: The maximum absolute value of each tensor determines an optimal scaling factor.
- Evidence anchors:
  - [abstract] "dynamically updating per-tensor scales for the weights, gradients and activations during training"
  - [section 2.1] "we introduce per-tensor scalings that shift the FP16 distributions before casting to FP8 E4"
  - [corpus] Weak evidence: related works discuss FP8 scaling but lack detailed methodology for large models.
- Break condition: If the maximum value changes too rapidly or exceeds FP8 dynamic range despite scaling, underflow/overflow can still occur.

### Mechanism 2
- Claim: Using different FP8 formats (E4 for weights/activations, E5 for gradients) maintains numerical accuracy.
- Mechanism: E5 provides wider dynamic range for gradients to handle their larger magnitude variations during training.
- Core assumption: Gradients require higher range than activations due to backpropagation dynamics.
- Evidence anchors:
  - [abstract] "using FP8 E5 the gradient format"
  - [section 2.2] "gradients need to be cast to FP8 E5 to preserve a wider dynamic range"
  - [corpus] Weak evidence: FP8 format proposals exist but do not explicitly validate this split for large models.
- Break condition: If gradient scaling is not updated frequently enough, training may diverge due to accumulated errors.

### Mechanism 3
- Claim: Post-training quantization (PTQ) without fine-tuning achieves full accuracy in FP8 inference.
- Mechanism: Scaling biases are computed once per tensor from a FP16 checkpoint, then applied during inference without further updates.
- Core assumption: The FP16 checkpoint distribution remains stable for inference tasks.
- Evidence anchors:
  - [abstract] "using post-training quantization for inference"
  - [section 3.1] "a scaling bias to each tensor and subsequently casting to FP8"
  - [corpus] Weak evidence: PTQ is common for INT8 but FP8 PTQ details are scarce.
- Break condition: If the inference data distribution differs significantly from the checkpoint, accuracy may degrade.

## Foundational Learning

- Concept: Floating-point number representation (sign, exponent, mantissa).
  - Why needed here: Understanding FP8 format limitations and scaling mechanisms.
  - Quick check question: What is the maximum representable value in FP8 E4 and how does it compare to FP16?

- Concept: Matrix multiplication accumulation precision.
  - Why needed here: FP8 matrix multiplication outputs are accumulated in higher precision to avoid overflow.
  - Quick check question: Why must the accumulation for FP8 matmul be done in FP16/FP32 instead of FP8?

- Concept: Dynamic range and underflow/overflow in low-precision formats.
  - Why needed here: The primary challenge FP8 addresses is maintaining values within representable ranges.
  - Quick check question: How does the scaling bias method prevent underflow/overflow when casting from FP16 to FP8?

## Architecture Onboarding

- Component map: Linear layers (attention QKV, attention out, feed-forward intermediate, feed-forward output) are FP8-quantized; other components remain in higher precision.
- Critical path: Forward pass → scale and cast inputs → FP8 matmul → unscale output; Backward pass → scale gradients → FP8 matmul → unscale → weight update.
- Design tradeoffs: FP8 offers memory and compute savings but requires careful scaling; using mixed precision (FP8 matmul, higher precision accumulation) mitigates range issues but adds complexity.
- Failure signatures: Training divergence, validation accuracy drop, or NaNs indicate scaling bias misconfiguration or overflow/underflow.
- First 3 experiments:
  1. Implement FP8-AMAX scaling on a small GPT model and verify inference accuracy matches FP16.
  2. Test FP8-CSCALE sweeps on the same model to identify converging scaling bias intervals.
  3. Validate that FP8-AMAX maintains accuracy during fine-tuning across multiple model sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for updating per-tensor scaling biases during FP8 training to balance computational efficiency and accuracy?
- Basis in paper: [explicit] The paper discusses the dynamic update of per-tensor scaling biases during training and mentions that the gradient scaling bias changes more frequently than weights and activations, but does not specify the optimal update frequency.
- Why unresolved: The paper provides insights into the behavior of scaling biases but does not determine the exact frequency at which updates should occur to achieve the best trade-off between efficiency and accuracy.
- What evidence would resolve it: Empirical studies comparing training outcomes with different update frequencies, analyzing both accuracy and computational overhead, would provide clarity on the optimal update frequency.

### Open Question 2
- Question: How does the FP8 training methodology perform with transformer architectures beyond GPT and Llama, such as those used in computer vision or graph neural networks?
- Basis in paper: [inferred] The paper focuses on FP8 training for GPT and Llama models and mentions the potential for applying FP8 to other transformer architectures, but does not provide experimental results for models outside the language domain.
- Why unresolved: The methodology is demonstrated on language models, and its effectiveness on other types of models remains untested, leaving a gap in understanding its broader applicability.
- What evidence would resolve it: Experimental results showing the performance of FP8 training on a variety of model architectures, including those used in computer vision and graph neural networks, would demonstrate its versatility and effectiveness across different domains.

### Open Question 3
- Question: What are the long-term effects of using FP8 on model robustness and generalization across diverse datasets and tasks?
- Basis in paper: [inferred] The paper shows that FP8 achieves comparable accuracy to FP16 on specific tasks but does not explore the long-term effects on model robustness and generalization across varied datasets and tasks.
- Why unresolved: The experiments focus on specific benchmarks and tasks, leaving open questions about how FP8 training affects model behavior in more diverse or unseen scenarios.
- What evidence would resolve it: Longitudinal studies and extensive testing across a wide range of datasets and tasks would provide insights into the long-term impacts of FP8 training on model robustness and generalization.

## Limitations
- The methodology relies heavily on specific hardware configurations (IPU machines) that may not be universally accessible.
- Results are primarily validated on GPT and Llama 2 architectures, with limited testing across diverse model families.
- The FP8-CSCALE method shows instability for larger models during fine-tuning, suggesting scaling limitations.
- Claims about general applicability across diverse tasks and datasets remain weakly supported.

## Confidence
- **High Confidence**: The basic FP8-AMAX methodology for inference works as described, with stable scaling bias computation and matching FP16 accuracy on tested benchmarks.
- **Medium Confidence**: The FP8-AMAX training methodology shows promise but has demonstrated limitations with the FP8-CSCALE variant for larger models.
- **Low Confidence**: Claims about general applicability across diverse model architectures and tasks remain weakly supported.

## Next Checks
1. **Scaling Bias Stability Analysis**: Implement real-time monitoring of scaling bias evolution during training across multiple epochs and model sizes to identify potential instability patterns before divergence occurs.
2. **Cross-Architecture Generalization**: Test the FP8-AMAX methodology on transformer variants beyond GPT/Llama 2 (such as OPT or BLOOM architectures) to validate broader applicability claims.
3. **Distribution Shift Robustness**: Evaluate inference accuracy when applying FP8-quantized models to out-of-distribution data, measuring degradation compared to FP16 baselines to assess the limitations of static post-training quantization.