---
ver: rpa2
title: Downstream Trade-offs of a Family of Text Watermarks
arxiv_id: '2311.09816'
source_url: https://arxiv.org/abs/2311.09816
tags:
- tasks
- watermarking
- generation
- which
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Watermarking large language models (LLMs) involves embedding an
  imperceptible signal into generated text to enable later detection. A prominent
  watermarking strategy boosts the probabilities of a pseudorandomly selected subset
  of tokens at each generation step.
---

# Downstream Trade-offs of a Family of Text Watermarks

## Quick Facts
- arXiv ID: 2311.09816
- Source URL: https://arxiv.org/abs/2311.09816
- Reference count: 40
- One-line primary result: Watermarking degrades downstream performance when class tokens are split across partitions, with long-form generation tasks experiencing 5-20% drops in performance metrics.

## Executive Summary
This work evaluates the performance of LLMs watermarked using three different strategies across diverse tasks including classification, multiple-choice question answering, short-form generation, and long-form generation. The study finds that classification accuracy is largely unaffected in the average case, but can drop dramatically with non-negligible probability if class tokens are separated into different partitions. Multiple-choice question answering and short-form generation tasks see minimal impact, while long-form generation tasks experience 5-20% drops in performance metrics. These results highlight trade-offs between watermark detectability and downstream performance, providing insights for users and developers of watermarked LLMs.

## Method Summary
The evaluation uses the watermarking strategy by Kirchenbauer et al. (2023a) with parameters γ, δ, and k, applied to LLaMA 7B models. The study tests three watermarking intensities (light, moderate, heavy) across multiple task categories: classification tasks (MNLI, SST-2, BoolQ, CB, RTE), multiple-choice question answering (HellaSwag, RACE, PIQA), short-form generation (CoQA, DROP, SQuAD2.0), long-form generation (WMT translation tasks, summarization tasks), and language modeling tasks. Performance is measured using task-specific metrics including accuracy, F1 scores, BLEU scores, ROUGE-L scores, and perplexity.

## Key Results
- Classification accuracy can drop dramatically with non-negligible probability if class tokens are separated into different partitions
- Multiple-choice question answering and short-form generation tasks see minimal impact from watermarking
- Long-form generation tasks experience 5-20% drops in performance metrics, with heavy watermarking causing repetitive degeneration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Watermarking degrades downstream performance when class tokens are split across partitions
- Mechanism: The watermarking scheme boosts green token logits. If the final decision token (e.g., label token in classification) is boosted while others are not, the model systematically favors that class, causing accuracy drops
- Core assumption: The template always ends with the same token, making the partition deterministic per prompt
- Evidence anchors:
  - [abstract]: "classification accuracy can drop dramatically with non-negligible probability if class tokens are separated into different partitions"
  - [section]: "Modifying the logits using a deterministic hash of the prompt's terminal token could therefore introduce a systematic bias in the probabilities that the model assigns to the classes"
  - [corpus]: No direct corpus support; assumed from experimental setup
- Break condition: If prompt templates vary per example or do not end with a consistent token, the partition bias disappears

### Mechanism 2
- Claim: Multiple-choice QA tasks are largely unaffected because options are long enough that each sees nearly the same green-token fraction
- Mechanism: Each answer choice contains enough tokens that the fraction of green tokens ≈ γ for all, so logit boosts are uniform across options, preserving relative ordering
- Core assumption: Options are sufficiently long relative to vocabulary size
- Evidence anchors:
  - [abstract]: "multiple-choice question answering and short-form generation tasks see minimal impact"
  - [section]: "As these options are sufficiently long, the fraction of green tokens that occur in each of them is close to γ"
  - [corpus]: No direct corpus support; inferred from dataset properties
- Break condition: Very short options or heavy watermarking (large δ) could skew logits enough to change the top choice

### Mechanism 3
- Claim: Long-form generation tasks degrade because salient tokens (proper nouns, key terms) get boosted, altering semantic coherence
- Mechanism: Watermarking intervenes at every generation step, boosting green tokens across all contexts. Salient tokens—often green—receive higher logits, introducing factual inaccuracies or repetitive degeneration
- Core assumption: Salient tokens are randomly distributed across green/red lists, but the boost is strong enough to alter meaning
- Evidence anchors:
  - [abstract]: "long-form generation tasks experience 5-20% drops in performance metrics"
  - [section]: "We observe qualitatively that while moderate watermarks can lead to occasional factual errors in the generations, heavy watermarks can cause them to degenerate into repeats of a short sequence of tokens"
  - [corpus]: No direct corpus support; supported by example outputs in supplementary
- Break condition: If green/red partitioning happens to avoid critical content tokens, or if δ is small, degradation is minimal

## Foundational Learning

- Concept: Conditional probability modeling in autoregressive decoders
  - Why needed here: Watermarking modifies logits at each step; understanding how next-token probabilities are derived is essential to reason about downstream effects
  - Quick check question: In an autoregressive LM, what distribution is produced by applying softmax to the logits at generation step n?

- Concept: Binomial partitioning of vocabulary under pseudorandom function
  - Why needed here: The watermarking scheme relies on pseudorandomly assigning tokens to green/red lists; grasping the distribution of class-token splits is key to estimating worst-case impacts
  - Quick check question: If a label token is a single token, what is the probability it lands in the green list under γ = 0.1?

- Concept: Statistical hypothesis testing via z-score for watermark detection
  - Why needed here: The watermark is verified by comparing observed green-token frequency to the expected γ; understanding this lets you gauge detectability vs. degradation trade-offs
  - Quick check question: If a 100-token sequence has 15 green tokens under γ = 0.1, what is its z-score?

## Architecture Onboarding

- Component map: LLM backbone (e.g., LLaMA 7B) -> Watermarking wrapper (partitioning function F, boosting δ) -> Task formatter (prompt template) -> Evaluation harness (metrics per task category)

- Critical path:
  1. Format input via task template
  2. Run generation with watermarking applied at each step
  3. Collect outputs and compute task-specific metrics
  4. Aggregate results over multiple hash keys

- Design tradeoffs:
  - Higher δ improves detection confidence but worsens generation quality
  - Larger γ increases signal strength but may make green tokens too common, reducing detectability
  - Prompt template choice affects whether classification accuracy is at risk

- Failure signatures:
  - Classification accuracy near random when class tokens are split
  - BLEU/ROUGE drops in translation/summarization
  - Repetitive token sequences in long-form generation under heavy watermarking
  - Multiple-choice accuracy drops only under very heavy watermarking or short options

- First 3 experiments:
  1. Run a simple classification task (e.g., SST-2) with and without watermarking, using a template ending in a consistent token; verify accuracy drop probability
  2. Test a multiple-choice QA task (e.g., RACE) under light, moderate, and heavy watermarking; confirm minimal accuracy loss
  3. Evaluate a long-form generation task (e.g., translation) under moderate and heavy watermarking; observe BLEU degradation and qualitative repetition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between watermark strength and downstream task performance?
- Basis in paper: [explicit] The paper discusses the trade-off between watermark detectability and downstream performance, but does not provide a definitive optimal point
- Why unresolved: The paper only explores a limited range of watermark strengths (γ, δ) and does not perform a comprehensive analysis to find the optimal balance
- What evidence would resolve it: A systematic study varying both γ and δ across a wider range, measuring performance on all downstream tasks, and identifying the point where the watermark is still detectable but performance is maximized

### Open Question 2
- Question: How do different watermarking schemes affect downstream performance?
- Basis in paper: [inferred] The paper only evaluates one specific watermarking scheme, but mentions that other schemes exist and could have different trade-offs
- Why unresolved: The paper does not compare the evaluated scheme to other watermarking methods, leaving uncertainty about whether better trade-offs exist
- What evidence would resolve it: Empirical evaluation of multiple watermarking schemes on the same downstream tasks, comparing their impact on performance and detectability

### Open Question 3
- Question: Can decoding strategies be adapted to mitigate the negative impact of watermarking on long-form generation tasks?
- Basis in paper: [explicit] The paper mentions that watermarking can cause long-form generations to degenerate into repetitions, but does not explore potential solutions
- Why unresolved: The paper identifies the problem but does not investigate whether adjusting decoding parameters (e.g., temperature, top-k sampling) or using different decoding algorithms could reduce the negative effects
- What evidence would resolve it: Experiments testing various decoding strategies on watermarked models for long-form generation, measuring performance metrics like BLEU and ROUGE-L to identify strategies that minimize degradation

## Limitations

- The evaluation assumes binary classification with templates ending in a consistent token, limiting generalizability to real-world multi-class scenarios
- The study focuses on LLaMA 7B specifically, and performance trade-offs may differ for larger or smaller models
- The analysis does not account for potential adversarial mitigation strategies that could exploit the watermarking mechanism

## Confidence

- High confidence: Long-form generation performance degradation (BLEU/ROUGE drops, repetitive degeneration) is consistently observed across tasks and watermarking intensities
- Medium confidence: Multiple-choice QA tasks are largely unaffected, though this depends on answer option length and watermarking parameters
- Medium confidence: Classification accuracy degradation mechanism is theoretically sound but highly dependent on prompt template design and dataset characteristics

## Next Checks

1. Test watermarking impact on datasets with 3+ classes (e.g., AG News or DBpedia) using diverse prompt templates to verify whether classification accuracy degradation extends beyond binary scenarios

2. Analyze the partitioning of domain-specific or named entity tokens across green/red lists for specialized vocabularies (e.g., biomedical or legal text) to quantify how non-random token distributions affect performance

3. Evaluate watermarking trade-offs across different model scales (e.g., LLaMA 13B, 33B) to determine whether performance degradation patterns scale linearly or exhibit non-monotonic behavior with model capacity