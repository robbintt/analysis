---
ver: rpa2
title: Causal Adversarial Perturbations for Individual Fairness and Robustness in
  Heterogeneous Data Spaces
arxiv_id: '2308.08938'
source_url: https://arxiv.org/abs/2308.08938
tags:
- fairness
- causal
- adversarial
- capify
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating individual fairness,
  adversarial robustness, and causal awareness in machine learning. The authors propose
  a novel framework that introduces a causal adversarial perturbation (CAP) set based
  on a causal fair metric, which accounts for sensitive attributes in heterogeneous
  data spaces.
---

# Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces

## Quick Facts
- arXiv ID: 2308.08938
- Source URL: https://arxiv.org/abs/2308.08938
- Reference count: 40
- Key outcome: Proposed CAPIFY method achieves significant reductions in unfair area (UAI) metrics while maintaining competitive accuracy on real-world and synthetic datasets

## Executive Summary
This paper addresses the challenge of integrating individual fairness, adversarial robustness, and causal awareness in machine learning. The authors propose a novel framework that introduces a causal adversarial perturbation (CAP) set based on a causal fair metric, which accounts for sensitive attributes in heterogeneous data spaces. By incorporating CAP into adversarial training, they develop a new regularizer that ensures individual fairness, causality, and robustness in classifiers. The method, called CAPIFY, is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving accurate classifiers that simultaneously exhibit fairness, adversarial robustness, and causal awareness.

## Method Summary
The method introduces a causal adversarial perturbation (CAP) set that transforms adversarial perturbations through a semi-latent space constructed from structural causal models (SCMs) and sensitive attributes. The framework uses a causal fair metric to measure semantic similarity among individuals, then generates CAPs that maintain this similarity while being adversarial. CAPIFY integrates this into adversarial training as a regularizer, optimizing for accuracy, robustness, and fairness simultaneously. The approach is evaluated on real-world datasets (Adult, COMPAS) and synthetic datasets generated from various SCM structures, with performance measured using accuracy, CAPI fairness (UAI), and adversarial robustness metrics.

## Key Results
- CAPIFY achieved UAI values as low as 0.07 and 0.03 on Adult and COMPAS datasets respectively
- Method significantly outperformed other approaches in maintaining CAPI fairness while preserving accuracy
- Demonstrated effectiveness across heterogeneous data spaces with both real-world and synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal adversarial perturbation (CAP) unifies individual fairness, robustness, and causal awareness by transforming adversarial perturbations through the semi-latent space
- Mechanism: The method uses a causal fair metric defined in a semi-latent space that combines observed sensitive attributes with latent variables. Adversarial perturbations are then transformed through the inverse mapping T⁻¹, ensuring that similar individuals (by causal metric) receive similar outcomes
- Core assumption: The semi-latent space construction properly captures causal relationships and sensitive attribute effects
- Evidence anchors:
  - [abstract] "By introducing a novel causal adversarial perturbation and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier"
  - [section] "Building upon the foundation laid by our proposed causal fair metric, we introduce the concept of the causal adversarial perturbation"
- Break condition: If the structural causal model assumptions don't hold or the sensitive attribute relationships are misspecified, the semi-latent space transformation would fail to capture true similarity

### Mechanism 2
- Claim: The unfair area indicator (UAI) provides a quantifiable measure of fairness violations that guides training optimization
- Mechanism: UAI measures the probability of instances where the CAPI fairness property fails, allowing the training process to minimize this metric through adversarial learning
- Core assumption: UAI accurately captures the extent of fairness violations in heterogeneous data spaces
- Evidence anchors:
  - [abstract] "Key results include significant reductions in unfair area (UAI) metrics, with CAPIFY outperforming other methods"
  - [section] "Definition 5 (Unfair Area Indicator (UAI)) Let M be the SCM with parameters denoted by ⟨G, V, U, F, PU⟩, and ˆh be the trained binary classifier. The probabilityPV(A̸= ∆), referred to as the Unfair Area Indicator, quantifies the likelihood of the CAPI unfairness for ˆh"
- Break condition: If the UAI metric doesn't properly account for the complex interactions between sensitive attributes and other features, it may provide misleading optimization signals

### Mechanism 3
- Claim: The causal fair metric construction ensures that similarity is evaluated while being robust to sensitive attribute variations
- Mechanism: The metric uses a pseudometric for sensitive attributes that treats certain attribute levels as having zero distance, effectively protecting them from causing unfairness
- Core assumption: The pseudometric approach properly captures the intended fairness properties of sensitive attributes
- Evidence anchors:
  - [abstract] "We use causal structural models and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals"
  - [section] "Definition 2 (Pseudometric Protected) In SCM M, suppose the sensitive feature S endowed with a pseudometric space (S, dS). S is partially protected if there are two levels with zero distance"
- Break condition: If the pseudometric assumptions don't match the real-world sensitivity of attributes, the fairness guarantees may not hold

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: The entire framework builds on SCMs to capture causal relationships between variables, which is essential for defining the causal fair metric and CAP
  - Quick check question: What are the key components of an SCM and how do they relate to each other?

- Concept: Individual Fairness
  - Why needed here: The paper aims to achieve individual fairness that accounts for causal relationships and sensitive attributes, going beyond standard formulations
  - Quick check question: How does individual fairness differ from group fairness and why is this distinction important?

- Concept: Adversarial Robustness
  - Why needed here: Adversarial robustness techniques are adapted through CAP to ensure both robustness and fairness simultaneously
  - Quick check question: What is the relationship between adversarial perturbations and fairness violations?

## Architecture Onboarding

- Component map:
  - SCM parser -> Semi-latent space transformer -> Causal fair metric calculator -> CAP generator -> CAPIFY trainer -> UAI evaluator

- Critical path:
  1. Parse SCM and identify sensitive attributes
  2. Transform instances to semi-latent space
  3. Compute causal fair metric
  4. Generate CAP
  5. Train classifier with CAPIFY
  6. Evaluate UAI and adjust training

- Design tradeoffs:
  - Complexity vs. interpretability: The semi-latent space adds complexity but enables causal fairness
  - Computation cost: CAP generation requires additional computation compared to standard adversarial training
  - Flexibility: The method requires a known SCM, limiting applicability to domains where causal structure is uncertain

- Failure signatures:
  - High UAI despite training: Indicates the CAPIFY regularizer isn't effectively reducing fairness violations
  - Decreased accuracy without fairness improvement: Suggests the method isn't balancing the tradeoff properly
  - Unstable training: May indicate issues with CAP generation or metric computation

- First 3 experiments:
  1. Test CAPIFY on a simple synthetic dataset with known causal structure and sensitive attributes
  2. Compare UAI reduction against baseline methods on the same dataset
  3. Evaluate the impact on accuracy and robustness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAPIFY compare to other methods in terms of accuracy when considering the trade-off between accuracy and fairness?
- Basis in paper: [explicit] The paper mentions that CAPIFY shows slightly lower accuracy compared to ERM, but real-world data indicates a greater reduction in unfairness than in accuracy
- Why unresolved: The paper does not provide a clear quantitative comparison of accuracy between CAPIFY and other methods, making it difficult to assess the trade-off between accuracy and fairness
- What evidence would resolve it: A comprehensive analysis of accuracy scores for all methods across different datasets would help in understanding the trade-off between accuracy and fairness

### Open Question 2
- Question: How does the choice of the fair metric impact the performance of CAPIFY in terms of CAPI fairness?
- Basis in paper: [explicit] The paper introduces a causal fair metric based on the structural causal model and sensitive attributes, but does not explore the impact of different fair metrics on the performance of CAPIFY
- Why unresolved: The paper focuses on the proposed causal fair metric and does not compare it with other fair metrics or discuss the potential impact of different fair metrics on the performance of CAPIFY
- What evidence would resolve it: An empirical study comparing the performance of CAPIFY using different fair metrics would provide insights into the impact of the fair metric on CAPI fairness

### Open Question 3
- Question: How does the performance of CAPIFY vary across different types of sensitive attributes (e.g., binary, continuous)?
- Basis in paper: [explicit] The paper mentions that the proposed approach can handle continuous sensitive attributes, but the experiments primarily focus on binary sensitive attributes
- Why unresolved: The paper does not provide a detailed analysis of the performance of CAPIFY across different types of sensitive attributes, making it unclear how well the method generalizes to continuous sensitive attributes
- What evidence would resolve it: Conducting experiments with continuous sensitive attributes and comparing the performance of CAPIFY with other methods would provide insights into the generalizability of the approach

## Limitations
- Effectiveness heavily dependent on accurate specification of structural causal models
- Method complexity may limit practical implementation in domains with uncertain causal structures
- Computational overhead from CAP generation and semi-latent space transformations

## Confidence
- Claims about UAI reduction: High
- Claims about theoretical framework construction: High
- Claims about real-world applicability: Medium
- Claims about scalability to complex domains: Low

## Next Checks
1. Test CAPIFY on datasets with known ground-truth causal relationships and systematically vary the accuracy of the SCM specification to measure performance degradation
2. Conduct ablation studies isolating the contributions of CAP vs. standard adversarial training to quantify the marginal benefit of the causal component
3. Evaluate robustness to different pseudometric specifications for sensitive attributes to understand sensitivity to this design choice