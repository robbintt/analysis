---
ver: rpa2
title: Conversation Understanding using Relational Temporal Graph Neural Networks
  with Auxiliary Cross-Modality Interaction
arxiv_id: '2311.04507'
source_url: https://arxiv.org/abs/2311.04507
tags:
- graph
- multimodal
- corect
- iemocap
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CORECT, a novel framework for multimodal emotion
  recognition in conversations. It leverages local context from utterance-level interactions
  via a Relational Temporal Graph Convolutional Network (RT-GCN) and global context
  from cross-modal interactions via a Pairwise Cross-modal Feature Interaction (P-CM)
  module.
---

# Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction

## Quick Facts
- arXiv ID: 2311.04507
- Source URL: https://arxiv.org/abs/2311.04507
- Reference count: 28
- State-of-the-art results on IEMOCAP and CMU-MOSEI datasets

## Executive Summary
This paper introduces CORECT, a novel framework for multimodal emotion recognition in conversations that captures both local context through a Relational Temporal Graph Convolutional Network (RT-GCN) and global context via a Pairwise Cross-modal Feature Interaction (P-CM) module. The approach addresses key limitations in existing methods by modeling utterance-level interactions and cross-modal dependencies, achieving state-of-the-art performance on benchmark datasets. CORECT outperforms previous models by 2.75% and 2.49% on weighted F1 score for IEMOCAP and CMU-MOSEI respectively.

## Method Summary
CORECT processes multimodal conversation data through dedicated unimodal encoders (Transformer for text, fully connected layers for audio/visual) to extract utterance-level features. A multimodal graph is constructed where each utterance generates three nodes (audio, visual, textual) connected by 15 edge types capturing intra- and inter-utterance relationships. The RT-GCN processes this graph to learn local context representations, while the P-CM module uses pairwise cross-modal transformers to exchange information between modalities at the conversation level. These local and global context representations are concatenated and passed through a classification layer for emotion prediction.

## Key Results
- Achieves state-of-the-art weighted F1 score of 74.1% on IEMOCAP (2.75% improvement over previous best)
- Achieves state-of-the-art weighted F1 score of 65.9% on CMU-MOSEI (2.49% improvement over previous best)
- Ablation studies confirm the effectiveness of both RT-GCN and P-CM modules
- Multimodal combinations consistently outperform single modalities across both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed RT-GCN captures local context by leveraging multimodal graph interactions at the utterance level, enabling modality-specific representations.
- Mechanism: RT-GCN constructs a multimodal graph where each utterance generates three nodes (audio, visual, textual) and models intra- and inter-utterance relationships using RGCN and Graph Transformer layers.
- Core assumption: Modality-specific node representations combined with relational modeling improve local context learning compared to fused inputs.
- Evidence anchors:
  - [abstract]: "the local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level"
  - [section 3.2.1]: "We consider two groups of relations: Rmulti and Rtemp... There are 15 edge types created"
  - [corpus]: Weak. Related works mention "Relational Temporal Graph Reasoning" but lack direct comparison to this specific graph construction.
- Break condition: If the edge types fail to capture relevant modality or temporal interactions, the local context representation degrades.

### Mechanism 2
- Claim: The P-CM module captures global context by modeling cross-modal interactions at the conversation level, addressing modality heterogeneity.
- Mechanism: P-CM uses pairwise cross-modal transformers to exchange information between modalities, computing enriched representations for each modality pair.
- Core assumption: Cross-modal attention can align misaligned sequences and capture long-term dependencies across modalities.
- Evidence anchors:
  - [abstract]: "the global representation could be captured via modeling of cross-modal interactions at the conversation level"
  - [section 3.3]: "To model the cross-modal interactions on unaligned multimodal sequences... we utilize D cross-modal transformer layers"
  - [corpus]: Weak. Related works discuss "Relational Graph Transformer" but do not validate cross-modal interaction effectiveness for emotion recognition.
- Break condition: If cross-modal attention fails to align sequences, global context representation becomes noisy.

### Mechanism 3
- Claim: Combining local and global context representations via concatenation improves utterance-level emotion prediction.
- Mechanism: Final representation H = Fusion([G, Z]) concatenates RT-GCN outputs G (local) and P-CM outputs Z (global) before classification.
- Core assumption: Local and global context features are complementary and their combination enhances prediction accuracy.
- Evidence anchors:
  - [abstract]: "These features are aggregated to enhance the performance of the utterance-level emotional recognition"
  - [section 3.4]: "The local- and global context representation resulted in by the RT-GCN and P-CM modules are fused together"
  - [corpus]: Weak. No direct corpus evidence on concatenation effectiveness for this task.
- Break condition: If either local or global representation is noisy, concatenation may propagate errors.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs model relational dependencies between utterances and modalities, capturing complex interactions missed by sequential models.
  - Quick check question: How does a Graph Convolutional Network differ from a standard Convolutional Neural Network?

- Concept: Cross-modal attention mechanisms
  - Why needed here: Cross-modal attention aligns misaligned multimodal sequences and captures long-term dependencies across modalities.
  - Quick check question: What is the role of the scaling factor 1/âˆšdk in the cross-modal attention computation?

- Concept: Temporal modeling in conversations
  - Why needed here: Temporal relationships (past/future utterances) significantly influence emotional labels in dialogues.
  - Quick check question: How does the window size [P, F] affect the temporal context captured by RT-GCN?

## Architecture Onboarding

- Component map: Unimodal encoders -> Speaker embedding -> RT-GCN (RGCN + Graph Transformer) -> P-CM (cross-modal transformers) -> Concatenation -> Classification
- Critical path: Unimodal features -> RT-GCN -> P-CM -> Concatenation -> Classification
- Design tradeoffs:
  - Modality-specific vs. fused representations: Specific captures nuances but increases complexity
  - Window size [P, F]: Larger windows capture more context but increase computation
  - Number of cross-modal transformer layers: More layers improve interaction modeling but risk overfitting
- Failure signatures:
  - Degraded performance when RT-GCN or P-CM is ablated (as shown in ablation studies)
  - Poor results with single modalities vs. multimodal combinations
- First 3 experiments:
  1. Test unimodal vs. multimodal performance to validate cross-modal benefit
  2. Vary window size [P, F] to find optimal temporal context
  3. Ablate RT-GCN or P-CM to measure individual contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the temporal aspect of conversations influence emotion recognition in multimodal ERC tasks?
- Basis in paper: [explicit] The paper states that the temporal aspect of conversations is crucial and past and future utterances can significantly influence the query utterance.
- Why unresolved: The paper mentions that few methods take into account the temporal aspect, and it explores various combinations of past and future nodes to determine their effects, but does not provide a definitive answer on the influence of temporal aspects.
- What evidence would resolve it: A comprehensive study comparing the performance of models with and without temporal context, and an analysis of the specific contributions of past and future utterances to emotion recognition.

### Open Question 2
- Question: What is the optimal combination of modalities for emotion recognition in conversations?
- Basis in paper: [explicit] The paper explores various modality combinations (A, V, T, A+T, T+V, V+A, A+V+T) and reports their performance on the IEMOCAP and CMU-MOSEI datasets.
- Why unresolved: The paper shows that different modality combinations perform differently, but does not provide a clear answer on the optimal combination for emotion recognition.
- What evidence would resolve it: A study that systematically evaluates the performance of all possible modality combinations on multiple datasets and identifies the combination that consistently performs best.

### Open Question 3
- Question: How does the number of attention heads in the Graph Transformer and P-CM modules affect the performance of CORECT?
- Basis in paper: [explicit] The paper mentions that the numbers of multi-head attentions used in Graph Transformer and P-CM are selected as 7 and 2, respectively, but does not explore the impact of varying these numbers.
- Why unresolved: The paper does not provide an analysis of how different numbers of attention heads affect the performance of CORECT.
- What evidence would resolve it: A sensitivity analysis that varies the number of attention heads in both modules and evaluates the impact on the performance of CORECT.

## Limitations

- The cross-modal interaction module (P-CM) lacks direct empirical validation of its individual contribution to performance gains
- The specific edge types and relational structures in RT-GCN are predefined without sensitivity analysis or exploration of alternatives
- Computational complexity and scalability to longer conversations is not discussed or evaluated

## Confidence

- **High confidence**: The architectural framework combining RT-GCN and P-CM is clearly specified and ablation studies demonstrate their individual contributions to performance gains.
- **Medium confidence**: The claim of state-of-the-art results is supported by F1 score improvements, but the absence of ablation studies isolating the cross-modal interaction mechanism reduces confidence in the P-CM module's specific contribution.
- **Low confidence**: The effectiveness of the specific edge types and relational structures in RT-GCN lacks empirical validation beyond their inclusion in the full model.

## Next Checks

1. Isolate cross-modal interaction contribution: Run controlled experiments comparing P-CM against simpler fusion mechanisms (concatenation, attention without cross-modal alignment) to quantify the specific benefit of pairwise cross-modal transformers for emotion recognition.

2. Test relational structure sensitivity: Systematically vary the edge types in RT-GCN (remove temporal edges, remove cross-modal edges, modify relation definitions) to identify which relational structures are essential versus redundant.

3. Analyze computational efficiency: Measure training/inference time and memory usage across different conversation lengths to evaluate scalability, and test performance degradation with reduced graph window sizes to understand the temporal context requirements.