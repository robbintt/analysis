---
ver: rpa2
title: 'Composer''s Assistant: An Interactive Transformer for Multi-Track MIDI Infilling'
arxiv_id: '2301.12525'
source_url: https://arxiv.org/abs/2301.12525
tags:
- measure
- note
- midi
- track
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Composer's Assistant, a system for interactive
  human-computer composition in the REAPER digital audio workstation. The system uses
  a T5-like transformer model trained to perform multi-track MIDI infilling, filling
  in arbitrary (track, measure) pairs that have been deleted from a contiguous slice
  of measures in a MIDI file.
---

# Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling

## Quick Facts
- arXiv ID: 2301.12525
- Source URL: https://arxiv.org/abs/2301.12525
- Reference count: 40
- The basic vocabulary model outperforms the word-level model on 5 out of 9 multi-track MIDI infilling tasks, including all 3 8-measure tasks

## Executive Summary
This paper introduces Composer's Assistant, a system for interactive human-computer composition in REAPER that uses a T5-like transformer model for multi-track MIDI infilling. The model fills arbitrary (track, measure) pairs deleted from a contiguous slice of measures in MIDI files, using either a basic MIDI-like event vocabulary or a joined word-like version of this vocabulary. The system is trained and evaluated on a new test set created from the Lakh MIDI dataset, with performance measured by note onset F1 scores across 9 multi-track infilling tasks.

## Method Summary
The approach uses a T5-like transformer with relative position attention to fill missing (track, measure) pairs in MIDI files. Two vocabulary designs are compared: a basic MIDI-like event vocabulary and a SentencePiece-joined word-level vocabulary. The model is trained on corrupted-span sequence-to-sequence objective using the Lakh MIDI dataset, with evaluation on 9 multi-track infilling tasks using note onset F1 as the primary metric.

## Key Results
- Basic vocabulary model outperforms word-level model on 5/9 tasks, including all 3 eight-measure tasks
- Word-level model performs better on 2/3 larger tasks requiring chunking
- The joined vocabulary reduces token count by approximately half, enabling processing of longer musical spans
- Complete system including REAPER integration, pretrained models, and source code is publicly released

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformer model can fill arbitrary (track, measure) pairs because it uses relative position attention and a track-aware token structure.
- Mechanism: Relative position attention allows the model to focus on musical relationships independent of absolute location, while explicit instrument tokens ensure track-specific generation without mixing content across tracks.
- Core assumption: The musical context needed for filling missing measures is preserved within the available surrounding measures and track assignments.
- Evidence anchors: [abstract] "T5-like model trained to accomplish this task"; [section] "We use a track-and-measure-based infilling approach"; [corpus] Weak evidence; no direct citations found
- Break condition: If the surrounding musical context is too sparse or the missing segment spans too many measures, the model's context window cannot capture enough information for coherent generation.

### Mechanism 2
- Claim: Joined vocabulary improves efficiency for longer prompts by reducing token count, allowing more measures to fit in memory.
- Mechanism: SentencePiece tokenization joins common chord/phrase tokens, reducing sequence length and enabling the model to process longer musical spans without chunking.
- Core assumption: Joined tokens capture meaningful musical phrases without losing semantic content.
- Evidence anchors: [section] "Our SentencePiece model typically tokenizes songs... into about half of the number of tokens"; [corpus] Weak evidence; no direct citations found
- Break condition: If the joined vocabulary overgeneralizes and merges distinct musical ideas, it may reduce the model's ability to differentiate subtle musical contexts.

### Mechanism 3
- Claim: The <mono> and <poly> tokens improve performance by guiding the model to generate the correct note density for masked sections.
- Mechanism: These tokens act as soft constraints, signaling whether the model should output monophonic or polyphonic content, reducing ambiguity during generation.
- Core assumption: The masked region's note density is predictable from the surrounding context.
- Evidence anchors: [section] "We add a <poly> or <mono> token corresponding to the nature of the masked tokens"; [corpus] Weak evidence; no direct citations found
- Break condition: If the model learns to ignore these tokens or if the context does not provide enough signal for the correct choice, performance gains may disappear.

## Foundational Learning

- **Concept: Transformer architecture with relative attention**
  - Why needed here: Enables modeling of musical sequences where relationships matter more than absolute positions
  - Quick check question: Why might relative attention be more suitable than absolute for music generation?

- **Concept: Tokenization and vocabulary design**
  - Why needed here: Determines how musical information is encoded and influences model expressiveness
  - Quick check question: How does joining tokens into phrases affect the model's ability to learn musical structure?

- **Concept: Masked language modeling for infilling**
  - Why needed here: Allows the model to learn to predict missing musical content from context
  - Quick check question: What's the difference between infilling and continuation tasks in terms of model conditioning?

## Architecture Onboarding

- **Component map:** MIDI files -> Tokenizer (basic/SentencePiece) -> T5 encoder-decoder with relative attention -> Masked span corruption -> Training loop -> REAPER integration scripts

- **Critical path:** 1. Preprocess MIDI files → tokenize → create infilling examples; 2. Train T5 model on corrupted span objective; 3. Evaluate using note onset F1 on held-out tasks; 4. Deploy with REAPER scripts for interactive use

- **Design tradeoffs:** Basic vs. joined vocabulary: expressiveness vs. efficiency; Full attention vs. memory-efficient attention: accuracy vs. scalability; Greedy vs. nucleus sampling: determinism vs. creativity

- **Failure signatures:** Low F1 scores indicate inability to reconstruct missing content; Chunking artifacts suggest prompt length exceeds model capacity; Overfitting to training data shows poor generalization

- **First 3 experiments:** 1. Train basic model on 8-measure infilling tasks and evaluate F1; 2. Switch to joined vocabulary and compare performance on same tasks; 3. Test effect of including/excluding <mono>/<poly> tokens on output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the basic model compare to the word-level model on tasks involving larger inputs that require chunking, and what are the implications for training in other small-vocabulary domains?
- Basis in paper: [explicit] The authors note that the basic model outperforms the word-level model on 5 out of 9 tasks, including all 8-measure tasks, but the word-level model outperforms on 2 out of 3 larger tasks.
- Why unresolved: The paper does not provide a detailed analysis of why the word-level model performs better on larger tasks, and the implications for training in other small-vocabulary domains are not fully explored.
- What evidence would resolve it: Additional experiments comparing the performance of the two models on tasks of varying sizes, and a detailed analysis of the reasons for the observed differences in performance.

### Open Question 2
- Question: What is the impact of using relative position instead of absolute position in the language model for musical information, and how does it affect the model's ability to capture musical patterns?
- Basis in paper: [explicit] The authors mention that they use relative position instead of absolute position in their language model, and cite a reference that suggests relative positions are more important for musical information.
- Why unresolved: The paper does not provide a detailed analysis of the impact of using relative position on the model's performance or its ability to capture musical patterns.
- What evidence would resolve it: Experiments comparing the performance of models using relative and absolute positions, and a detailed analysis of the musical patterns captured by each approach.

### Open Question 3
- Question: How does the choice of vocabulary size and model dimension affect the performance of the transformer model in small-vocabulary domains, and what are the optimal choices for different tasks?
- Basis in paper: [inferred] The authors note that their basic vocabulary is large enough to fully utilize their model dimension, and suggest that the choice of vocabulary size and model dimension may depend on the task at hand.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between vocabulary size, model dimension, and performance in small-vocabulary domains, or guidelines for choosing optimal values for different tasks.
- What evidence would resolve it: Experiments comparing the performance of models with different vocabulary sizes and model dimensions, and a detailed analysis of the factors that influence the choice of optimal values for different tasks.

## Limitations

- The evaluation is limited to 9 multi-track infilling tasks from a filtered subset of the Lakh MIDI dataset, potentially biasing toward certain musical styles
- Performance differences between vocabulary types lack statistical significance testing, making it unclear if observed gaps are meaningful
- The F1 metric focuses only on note onset detection without capturing rhythmic accuracy, harmonic coherence, or stylistic appropriateness

## Confidence

- **High Confidence**: The basic technical approach (T5-like transformer with relative attention for multi-track MIDI infilling) is sound and well-implemented
- **Medium Confidence**: The relative performance comparison between vocabulary types shows clear trends but lacks statistical validation
- **Low Confidence**: The generalization of results beyond the specific test set is uncertain, with no demonstration on diverse musical genres

## Next Checks

1. Conduct statistical significance testing (e.g., paired t-tests) on the F1 score differences between basic and joined vocabulary models across all 9 tasks to determine if performance differences are meaningful.

2. Expand evaluation to include musical quality metrics beyond note onset F1, such as rhythmic consistency, harmonic plausibility, and stylistic coherence, potentially through expert listener studies.

3. Test model performance on a more diverse corpus including non-classical genres, off-grid rhythms, and varying track counts to assess generalizability beyond the filtered Lakh MIDI subset used in evaluation.