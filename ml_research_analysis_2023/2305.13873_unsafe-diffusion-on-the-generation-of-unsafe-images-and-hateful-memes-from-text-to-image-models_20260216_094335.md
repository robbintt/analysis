---
ver: rpa2
title: 'Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From
  Text-To-Image Models'
arxiv_id: '2305.13873'
source_url: https://arxiv.org/abs/2305.13873
tags:
- images
- unsafe
- image
- variants
- hateful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates the safety of four popular text-to-image
  models, finding that they collectively generate unsafe content in 14.56% of cases,
  with Stable Diffusion being the highest-risk model at 18.92%. The research also
  demonstrates that these models can automatically generate realistic hateful meme
  variants, with DreamBooth achieving a 24% success rate in creating variants that
  combine original meme features with targeted entities.
---

# Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models

## Quick Facts
- arXiv ID: 2305.13873
- Source URL: https://arxiv.org/abs/2305.13873
- Reference count: 40
- Key outcome: Four popular text-to-image models generate unsafe content in 14.56% of cases, with Stable Diffusion being highest-risk at 18.92%

## Executive Summary
This study systematically evaluates the safety of four popular text-to-image models (Stable Diffusion, Latent Diffusion, DALL·E 2-demo, and DALL·E mini) and demonstrates their potential for generating unsafe images and hateful meme variants. The research reveals that 3.46%-5.80% of training images are unsafe, resulting in models collectively generating unsafe content in 14.56% of cases. The study also shows that image editing methods can automatically generate realistic hateful meme variants, with DreamBooth achieving a 24% success rate in creating variants that combine original meme features with targeted entities.

## Method Summary
The study employs a multi-headed safety classifier based on CLIP embeddings to categorize generated images into safe, unsafe, and hateful meme categories. Researchers sampled 1,000 images from each model's training data to estimate unsafe content prevalence. For hateful meme generation, they used three image editing methods (DreamBooth, Textual Inversion, and SDEdit) to create variants from a dataset of 2,029 real-world hateful memes, targeting 15 entities. Generated images were evaluated for safety classification, image fidelity, and text alignment using CLIP-based similarity metrics.

## Key Results
- Four text-to-image models generate unsafe content in 14.56% of cases collectively
- DreamBooth achieves 24% success rate in generating hateful meme variants that combine original meme features with targeted entities
- Safety classifier flags 44.19% of generated hateful memes while maintaining 96.5% accuracy on unsafe image classification
- Training data contains 3.46%-5.80% unsafe images across models, directly correlating with unsafe generation rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-image models can generate unsafe content because unsafe images are present in their training data
- Mechanism: The models learn representations from training images, including unsafe ones. When prompted with harmful content, they can reproduce unsafe images by applying learned representations
- Core assumption: The proportion of unsafe images in training data correlates with unsafe generation in models
- Evidence anchors:
  - [section]: "We estimate that there are 3.46%-5.80% unsafe images in the training datasets... resulting in models generating unsafe content"
  - [abstract]: "The unsafe generation is traced back to 3.46%-5.80% of unsafe training images across models"
  - [corpus]: Weak - no direct corpus evidence, but aligns with related work on training data safety
- Break condition: If training data is thoroughly filtered and cleaned, unsafe generation should decrease proportionally

### Mechanism 2
- Claim: Image editing methods enable adversaries to generate hateful meme variants by fine-tuning models on specific subjects
- Mechanism: DreamBooth and Textual Inversion allow fine-tuning on target meme images, creating a "subject embedding" that can be combined with prompts about target entities to generate variants
- Core assumption: The fine-tuned model retains enough of the original meme's features while adapting to new prompts
- Evidence anchors:
  - [section]: "With DreamBooth, its generated variants have the highest percentage to be considered successful... exceeding SDEdit (10%), and Textual Inversion (9%)"
  - [abstract]: "DreamBooth achieving a 24% success rate in creating variants that combine original meme features with targeted entities"
  - [corpus]: Weak - no direct corpus evidence, but aligns with related work on image editing
- Break condition: If fine-tuning is prevented or restricted, this attack vector would be blocked

### Mechanism 3
- Claim: ChatGPT can improve hateful meme generation by creating more descriptive prompts
- Mechanism: ChatGPT rephrases basic prompts (entity + caption) into more descriptive variations, improving text alignment between prompts and generated images
- Core assumption: More descriptive prompts lead to better image-text alignment in the generated variants
- Evidence anchors:
  - [section]: "The result shows that adding ChatGPT in the loop leads to higher text alignment values"
  - [abstract]: "Our evaluation result shows that 24% of the generated images using DreamBooth are hateful meme variants"
  - [corpus]: Weak - no direct corpus evidence, but aligns with work on prompt engineering
- Break condition: If language models like ChatGPT are restricted from generating prompts for this purpose, the attack would be less effective

## Foundational Learning

- Concept: CLIP model architecture
  - Why needed here: CLIP is used for both image safety classification and measuring image-text similarity
  - Quick check question: What are the two components of CLIP and how do they work together?

- Concept: K-means clustering for thematic analysis
  - Why needed here: Used to group generated unsafe images into clusters for thematic coding
  - Quick check question: What metric is used to determine the optimal number of clusters in the elbow method?

- Concept: Diffusion model fundamentals
  - Why needed here: All four evaluated models are based on diffusion architectures
  - Quick check question: What is the role of the noise vector in diffusion models?

## Architecture Onboarding

- Component map:
  - Training data → Model training → Prompt input → Image generation → Safety classification → Output
  - For hateful memes: Target meme + Image editing method → Fine-tuning → Prompt generation → Variant generation → Evaluation

- Critical path: Prompt → Model → Image generation → Safety filter
  - The safety filter is the last defense before unsafe content reaches users

- Design tradeoffs:
  - Open-source models offer flexibility but lack centralized safety controls
  - Safety filters can be bypassed but provide a basic defense layer
  - Fine-tuning capabilities enable personalization but can be misused

- Failure signatures:
  - High unsafe generation rates indicate training data contamination
  - Successful hateful meme generation indicates insufficient fine-tuning controls
  - Low safety classifier accuracy suggests need for better detection methods

- First 3 experiments:
  1. Test safety classifier on known unsafe images to establish baseline accuracy
  2. Generate images with both safe and unsafe prompts to measure model behavior
  3. Apply different image editing methods to target memes with various entities to compare effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different cultural contexts affect the definition and detection of unsafe content in text-to-image models?
- Basis in paper: [explicit] The paper acknowledges that "what is considered inappropriate can be subjective based on one's cultural and social predisposition" and that "some of our qualitative evaluations rely on manual annotation, which may introduce bias."
- Why unresolved: The study primarily uses Western-centric datasets (4chan, Lexica) and definitions. Different cultures may have varying thresholds for what constitutes unsafe content, and the safety classifier may not generalize well across diverse cultural contexts.
- What evidence would resolve it: Cross-cultural studies using diverse datasets from multiple regions, testing the safety classifier's performance across different cultural contexts, and conducting user studies with participants from various cultural backgrounds to validate the definition of unsafe content.

### Open Question 2
- Question: What are the long-term societal impacts of widespread access to text-to-image models capable of generating unsafe content?
- Basis in paper: [explicit] The paper discusses real-world impacts, mentioning that "AI-generated memes like Pepe the Frog have already spread on websites like Know Your Meme and Reddit" and that generated content could be used for "large-scale hate campaigns."
- Why unresolved: The paper provides a snapshot of current risks but does not explore longitudinal effects, psychological impacts on different demographic groups, or how societal norms might shift with increased exposure to AI-generated unsafe content.
- What evidence would resolve it: Longitudinal studies tracking the spread and impact of AI-generated unsafe content over time, psychological research on exposure effects, and sociological studies examining how AI-generated content influences online discourse and behavior.

### Open Question 3
- Question: How effective are current mitigation strategies in preventing the generation of unsafe content across different text-to-image models?
- Basis in paper: [explicit] The paper discusses several mitigation measures including "curating training data, regulating prompts, and implementing post-processing safety classifiers" but notes limitations, such as their safety classifier only flagging 44.19% of hateful memes.
- Why unresolved: The evaluation of mitigation strategies is limited to one safety classifier and does not comprehensively test all proposed measures across different models or assess their real-world effectiveness in preventing misuse.
- What evidence would resolve it: Comparative studies testing multiple mitigation strategies (data curation, prompt regulation, safety filters) across all four models, red teaming exercises to assess how easily adversaries can bypass safety measures, and real-world deployment studies measuring actual misuse rates.

## Limitations

- Safety classifier may have blind spots for culturally-specific or emerging unsafe content categories
- Hateful meme generation evaluation uses limited set of 15 target entities, potentially not capturing full misuse spectrum
- CLIP-based automated classification may struggle with nuanced or context-dependent unsafe content

## Confidence

**High Confidence**: The finding that 3.46%-5.80% of training images are unsafe and that this contamination correlates with unsafe generation rates (14.56% across models).

**Medium Confidence**: The effectiveness of image editing methods for hateful meme generation, particularly DreamBooth's 24% success rate.

**Low Confidence**: The assertion that ChatGPT significantly improves hateful meme generation through better prompt engineering.

## Next Checks

1. **Cross-Validation of Safety Classifier**: Test the CLIP-based safety classifier on an independent dataset of known unsafe images from multiple cultural contexts to verify its generalizability and identify potential blind spots in unsafe content detection.

2. **Temporal Stability Analysis**: Generate images from the same prompts across multiple versions of each model (if available) to assess how safety profiles evolve with model updates and training data changes over time.

3. **Adversarial Prompt Testing**: Systematically test the safety filters with deliberately crafted adversarial prompts that attempt to bypass safety measures, measuring the false negative rate and identifying common patterns in successful bypasses.