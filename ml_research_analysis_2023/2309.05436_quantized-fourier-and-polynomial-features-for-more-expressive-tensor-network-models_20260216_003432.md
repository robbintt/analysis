---
ver: rpa2
title: Quantized Fourier and Polynomial Features for more Expressive Tensor Network
  Models
arxiv_id: '2309.05436'
source_url: https://arxiv.org/abs/2309.05436
tags:
- tensor
- features
- fourier
- quantized
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning large-scale kernel
  machines with tensor-product features, which suffer from an exponential growth in
  model parameters. The core method idea is to quantize (further tensorize) polynomial
  and Fourier features, enabling the quantization of model weights.
---

# Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models

## Quick Facts
- arXiv ID: 2309.05436
- Source URL: https://arxiv.org/abs/2309.05436
- Reference count: 22
- Key outcome: Quantized tensor network models achieve state-of-the-art performance on large regression tasks while requiring fewer parameters than non-quantized counterparts

## Executive Summary
This paper addresses the problem of learning large-scale kernel machines with tensor-product features, which suffer from exponential growth in model parameters. The core method idea is to quantize (further tensorize) polynomial and Fourier features, enabling the quantization of model weights. This quantization imposes additional tensor network structure between features, yielding more expressive models for the same number of model parameters. The primary results show that quantized models have higher bounds on the VC-dimension, prioritize learning the most salient features, and provide state-of-the-art performance on large regression tasks.

## Method Summary
The method quantizes polynomial and Fourier features by decomposing them into smaller Kronecker factors, then constrains the model weights to follow the same tensor network structure. This allows model responses and gradients to be computed in O(P) time where P is the number of parameters. The approach uses alternating least squares (ALS) or Riemannian optimization to train low-rank tensor network representations (CPD, TT, or TR) of the quantized weights. The method is evaluated on 8 small UCI datasets and a large airline dataset with 5.9 million samples.

## Key Results
- Quantized models attain higher upper bounds on VC-dimension compared to non-quantized models
- Quantization prioritizes learning the most salient features by concentrating optimization energy on high-magnitude coefficients
- Achieved state-of-the-art results on the airline dataset (MSE of 0.748) using only 3840 parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantizing tensor-product features into smaller Kronecker factors reduces the effective dimensionality of the feature space, enabling model weights to be further tensorized without increasing parameter count.
- Mechanism: Original polynomial/Fourier features of size M per dimension are factorized into Q smaller blocks of size Q^K = M. This structure allows model weights to be indexed over the same smaller blocks, yielding a tensor network with more vertices (higher VC dimension) for the same number of parameters.
- Core assumption: The original feature map can be exactly represented as a Kronecker product of quantized sub-features without approximation error.
- Evidence anchors:
  - [abstract] "We quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights..."
  - [section] "Theorem 3.4 (Quantized pure-power-(Md −1) polynomial feature map). Each Vandermonde vector v(d)(xd) can be expressed as a Kronecker product of Kd factors..."
- Break Condition: If M cannot be factored into integer powers Q^K, the quantization step fails and the model reverts to the non-quantized representation.

### Mechanism 2
- Claim: Quantized model weights regularize the learning problem by prioritizing high-magnitude Fourier coefficients, which correspond to the most salient signal features.
- Mechanism: Under a low-rank tensor network constraint on the quantized weights, optimization energy is concentrated on capturing the largest peaks in the frequency spectrum first. As rank increases, additional degrees of freedom allow modeling finer details.
- Core assumption: Squared-loss optimization naturally seeks to maximize energy reconstruction in the frequency domain, so higher peaks are learned before lower peaks under a fixed parameter budget.
- Evidence anchors:
  - [abstract] "we show that quantized models attain higher upper bounds on the VC-dimension... We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features..."
  - [section] "we observe that the coefficients which are recovered for lower ranks, e.g. in case of R = 10, are the peaks with the highest magnitude."
- Break Condition: If the loss function is changed to one that does not correspond to energy maximization (e.g., L1 loss), the prioritization of peaks may not occur.

### Mechanism 3
- Claim: Quantized tensor network models achieve state-of-the-art performance on large regression tasks while requiring fewer parameters than non-quantized counterparts.
- Mechanism: By exploiting the Kronecker structure of quantized features and weights, model responses and gradients can be computed in O(P) time, where P is the number of parameters. This allows scaling to large datasets without sacrificing expressive power.
- Core assumption: The computational complexity reduction from O(M^D) to O(P) is realizable in practice and does not introduce numerical instability.
- Evidence anchors:
  - [abstract] "We finally benchmark our approach on large regression task, achieving state-of-the-art results on a laptop computer."
  - [section] "Training QTKM on the Intel Core i7-10610U CPU... took (6613 ± 40) s for R = 20..."
- Break Condition: If the dataset is too small or the feature space too low-dimensional, the quantization overhead may outweigh the benefits.

## Foundational Learning

- Concept: Tensor product and Kronecker product algebra
  - Why needed here: The paper's entire approach relies on decomposing high-dimensional feature maps into Kronecker products of lower-dimensional sub-features, then reconstructing them exactly.
  - Quick check question: Given two vectors u ∈ R^a and v ∈ R^b, what is the length of their Kronecker product u ⊗ v?

- Concept: Tensor network decompositions (CPD, TT, TR)
  - Why needed here: The model weights are constrained to lie in a low-rank tensor network format to reduce parameters and enable efficient computation.
  - Quick check question: In a rank-R CP decomposition, how many parameters are required to represent a D-way tensor of size M^D?

- Concept: VC-dimension and generalization bounds
  - Why needed here: The paper uses VC-dimension bounds to theoretically justify that quantized models are more expressive for the same parameter count.
  - Quick check question: If a binary classifier has VC-dimension h, what is the maximum number of points it can shatter?

## Architecture Onboarding

- Component map: Input features -> Quantized Vandermonde/Fourier vectors -> Kronecker product feature map -> Low-rank tensor network weights -> Model output
- Critical path: Feature quantization → Weight tensorization → ALS optimization → Prediction
- Design tradeoffs:
  - Higher quantization factor Q → more vertices in TN → higher VC bound, but potentially more numerical instability
  - Lower TN rank R → fewer parameters, faster training, but risk of underfitting
  - Choice of TN format (CPD vs TT vs TR) → different computational complexity and expressiveness
- Failure signatures:
  - Training divergence → check quantization factor Q and TN rank R
  - Poor generalization → verify that feature quantization is lossless for given M and Q
  - Slow inference → ensure tensor contractions are implemented efficiently
- First 3 experiments:
  1. Verify lossless feature quantization on a small synthetic dataset with known M and Q
  2. Compare test MSE of quantized vs non-quantized models on a mid-sized UCI dataset
  3. Benchmark training time and memory usage on the airline dataset with varying R and Q

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quantization interact with other tensor network decompositions (e.g., hierarchical Tucker, block-term) in terms of VC-dimension bounds and generalization?
- Basis in paper: [inferred] The paper discusses CPD, TT, and TR networks explicitly but notes that hierarchical Tucker and block-term decompositions can have t ≥ 2 singleton edges. The VC-dimension bound is derived for the networks studied, but the effect of quantization on other decompositions is not explored.
- Why unresolved: The paper only provides theoretical and experimental results for specific tensor network types. The generalization behavior of quantized weights under other decompositions remains unexplored.
- What evidence would resolve it: Theoretical analysis of VC-dimension bounds for quantized weights under hierarchical Tucker or block-term decompositions, paired with experimental comparisons on datasets.

### Open Question 2
- Question: What is the optimal quantization factor Q for different types of data distributions (e.g., sparse vs. dense, periodic vs. non-periodic)?
- Basis in paper: [explicit] The paper suggests Q = 2 as optimal based on maximizing the VC-dimension bound but does not explore how this choice varies with data characteristics.
- Why unresolved: The paper fixes Q = 2 for all experiments without investigating its impact on different data types or whether adaptive quantization could improve performance.
- What evidence would resolve it: Systematic experiments varying Q across diverse datasets with different structures, measuring test error and model expressiveness.

### Open Question 3
- Question: How does the quantization-induced regularization affect feature selection in high-dimensional, noisy datasets where signal-to-noise ratio is low?
- Basis in paper: [explicit] The paper shows that quantization prioritizes high-magnitude frequency peaks in the sound dataset, suggesting a form of implicit feature selection. However, the behavior in noisy, high-dimensional settings is not examined.
- Why unresolved: The experiments focus on low-dimensional or structured datasets. The interaction between quantization, noise, and feature selection in complex, high-dimensional spaces is not studied.
- What evidence would resolve it: Experiments on high-dimensional, noisy datasets (e.g., genomics, text) comparing quantized vs. non-quantized models in terms of feature recovery and robustness to noise.

## Limitations
- The theoretical VC-dimension bounds rely on exact lossless quantization, which may not hold for arbitrary polynomial orders M and quantization factors Q
- Empirical validation focuses primarily on regression tasks with relatively low-dimensional inputs (D ≤ 8)
- The paper only provides theoretical and experimental results for specific tensor network types (CPD, TT, TR)

## Confidence
- High confidence: The core mechanism of feature quantization and its implementation details - the Kronecker product decomposition is mathematically well-defined and the computational complexity claims are verifiable
- Medium confidence: The regularization effect of quantized weights prioritizing salient features - while theoretically plausible, the empirical evidence is limited to a single synthetic example
- Medium confidence: State-of-the-art performance claims - the results are compelling but only compared against a limited set of baselines on regression tasks

## Next Checks
1. **Theoretical validation**: Derive explicit conditions on M and Q under which the feature quantization remains lossless, and quantify the approximation error when these conditions are violated
2. **Empirical validation**: Test the quantized tensor kernel machine on high-dimensional classification datasets (e.g., CIFAR-10) to assess scalability beyond the low-dimensional regression problems studied
3. **Ablation study**: Systematically vary the quantization factor Q and tensor network rank R to quantify their independent effects on model expressiveness, computational complexity, and generalization performance