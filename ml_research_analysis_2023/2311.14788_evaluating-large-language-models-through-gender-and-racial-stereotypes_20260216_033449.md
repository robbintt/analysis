---
ver: rpa2
title: Evaluating Large Language Models through Gender and Racial Stereotypes
arxiv_id: '2311.14788'
source_url: https://arxiv.org/abs/2311.14788
tags:
- race
- gender
- bias
- language
- professions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates gender and racial biases in large language
  models (LLMs) by analyzing their outputs for 99 professions across five racial groups.
  The methodology involves prompting models to classify professions by gender and
  generate descriptive sentences for different racial groups.
---

# Evaluating Large Language Models through Gender and Racial Stereotypes

## Quick Facts
- arXiv ID: 2311.14788
- Source URL: https://arxiv.org/abs/2311.14788
- Reference count: 18
- Primary result: GPT-3.5 significantly reduces gender bias but persistent racial bias remains across LLMs

## Executive Summary
This study evaluates gender and racial biases in large language models by analyzing outputs for 99 professions across five racial groups. The research finds that while newer models like GPT-3.5 show substantial improvement in gender bias (achieving a 0.07 bias score with 91 correct predictions out of 99 professions), racial bias remains a significant concern. The study reveals that GPT-3.5 generates stereotypical descriptions across all racial groups, with notable differences in emotional expression, social behavior, and work attitudes. These findings highlight the need for continued focus on bias mitigation in LLM development, particularly for racial biases that could impact workplace dynamics and individual well-being.

## Method Summary
The methodology involves prompting models to classify professions by gender and generate descriptive sentences for different racial groups. Gender bias is measured using few-shot prompting with masked models to force gender-specific pronoun generation, which is then classified into Male/Female/Neutral categories. Racial bias analysis uses sentence embeddings (SentenceBERT) to compare semantic similarity across racial groups for the same profession, supplemented by LIWC linguistic feature analysis. The study evaluates multiple LLMs including GPT-3.5, GPT-3, Bard, Claude, and Flan-T5 using a dataset of 99 professions with human-annotated gender labels.

## Key Results
- GPT-3.5 achieved a low gender bias score of 0.07 with 91 correct predictions out of 99 professions
- Racial bias persists across all tested models, with significant differences in linguistic features across racial groups
- Descriptive sentences generated for different races show similarity scores below the 0.8 threshold, indicating stereotypical language use
- LIWC analysis reveals disparities in emotional expression, social behavior, and work attitudes across racial descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked models complete the blank based on context, enabling controlled bias measurement by forcing the model to generate a gender-specific pronoun.
- Mechanism: The prompt structure "<mask> is a <profession>" constrains the model to output a single token (pronoun), which is then mapped to Male/Female/Neutral classes.
- Core assumption: The model's next-token prediction for the masked pronoun directly reflects its internal gender association with the profession.
- Evidence anchors:
  - [abstract] shows use of masked prompts to evaluate gender bias across 99 professions.
  - [section] describes masked model format and iterative prediction to derive gender probabilities.
  - [corpus] neighbors include similar masked/contextual bias studies.
- Break condition: If the model frequently outputs non-pronoun tokens or generates multiple tokens, the classification step fails.

### Mechanism 2
- Claim: Few-shot prompting with explicit class definitions steers model outputs toward desired categories, reducing random or ambiguous responses.
- Mechanism: Providing examples like "Actor: Neutral" and "Actress: Female" primes the model to classify new professions into Male/Female/Neutral only.
- Core assumption: The model's instruction-following capability is sufficient to override its base associations and adopt the given classification schema.
- Evidence anchors:
  - [abstract] and [section] describe the few-shot prompt structure and its role in controlling model behavior.
  - [section] notes repetition and majority voting to ensure stable classification.
  - [corpus] neighbors reference reinforcement learning and fine-tuning for bias mitigation.
- Break condition: If the model ignores examples and hallucinates categories, the classification will be unreliable.

### Mechanism 3
- Claim: SentenceBERT embeddings capture semantic similarity between model-generated descriptions, revealing bias via low similarity across races for the same profession.
- Mechanism: Comparing embeddings of descriptions for identical professions across different races shows whether the model varies its output based on race.
- Core assumption: SentenceBERT embeddings preserve semantic content differences tied to stereotypical language use.
- Evidence anchors:
  - [section] describes using SentenceBERT to compute similarity scores and find that similarity across races is below the expected threshold.
  - [section] shows analysis of LIWC scores to detect linguistic disparities.
  - [corpus] neighbors discuss unsupervised concept extraction and bias evaluation in LLMs.
- Break condition: If SentenceBERT embeddings are too coarse or the model generates near-identical text, similarity metrics will not reflect underlying bias.

## Foundational Learning

- Concept: Inter-annotator agreement and Kappa statistic
  - Why needed here: Ensures human-labeled ground truth is reliable and reduces introduction of human bias during dataset creation.
  - Quick check question: What Kappa value indicates strong agreement between annotators, and what value was achieved here?

- Concept: Few-shot prompting and instruction tuning
  - Why needed here: Controls model behavior to fit the experimental classification schema rather than relying on default associations.
  - Quick check question: How does providing explicit examples in the prompt influence model output distribution?

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: Quantifies linguistic variation in model outputs across races, revealing bias in descriptive language.
  - Quick check question: What threshold similarity score would indicate bias-free output across races for the same profession?

## Architecture Onboarding

- Component map:
  Dataset (99 professions with human-annotated gender labels) -> Prompt templates (masked/unmasked for gender, structured examples for few-shot race analysis) -> Models (multiple LLMs: GPT-3.5, Bard, Claude, Flan-T5, GPT-3) -> Classifiers (gender subject classifier for unmasked models; manual majority voting) -> Analysis tools (SentenceBERT for embedding similarity; LIWC for linguistic feature extraction)

- Critical path:
  1. Prepare prompts and iterate over professions and races
  2. Collect model outputs and classify/govern them
  3. Compute similarity metrics and LIWC scores
  4. Compare against ground truth or expected similarity thresholds

- Design tradeoffs:
  - Masked prompts give precise control but may not reflect natural generation
  - Few-shot examples constrain output but may not eliminate subtle bias
  - Embedding-based similarity is scalable but may miss nuanced bias patterns

- Failure signatures:
  - High variance in similarity scores despite identical professions
  - Majority voting fails to converge across runs
  - LIWC scores show consistent disparities across all races

- First 3 experiments:
  1. Run masked prompt gender classification on a small subset of professions and verify correct pronoun extraction
  2. Test few-shot prompting on a held-out profession to ensure consistent classification without hallucination
  3. Compute SentenceBERT similarity for a few identical profession prompts across two races and check for threshold breach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features in LLMs' outputs most strongly correlate with racial stereotypes, and how do these features differ across racial groups?
- Basis in paper: [explicit] The paper analyzes LIWC scores and finds discrepancies in how different racial groups are described, with variations in emotional expression, social behavior, and work attitudes.
- Why unresolved: The paper identifies differences but does not establish which specific linguistic features have the strongest correlation with stereotypes or quantify their impact.
- What evidence would resolve it: Detailed statistical analysis correlating specific LIWC features (e.g., emotional tone, social references) with stereotype strength across racial groups, including effect sizes and significance testing.

### Open Question 2
- Question: How do newer LLMs compare to GPT-3.5 in terms of racial bias mitigation, and what architectural or training differences contribute to these outcomes?
- Basis in paper: [inferred] The paper evaluates GPT-3.5's performance but does not compare it to other newer models like Claude or PaLM-2 in the context of racial bias.
- Why unresolved: The study focuses on GPT-3.5 and pre-2021 models, leaving a gap in understanding how other state-of-the-art models perform on racial bias tasks.
- What evidence would resolve it: Comparative analysis of multiple newer LLMs (e.g., Claude, PaLM-2) on the same racial bias framework, with detailed breakdowns of their architectural or training differences.

### Open Question 3
- Question: What is the long-term impact of biased LLM outputs on workplace dynamics and individual well-being, and how can these effects be measured in real-world settings?
- Basis in paper: [explicit] The authors suggest conducting a field study to understand the direct impact of biases on human behavior, acknowledging the need for further research.
- Why unresolved: The paper proposes this as future work but does not provide empirical data on the real-world consequences of biased LLM outputs.
- What evidence would resolve it: Longitudinal studies tracking workplace outcomes (e.g., hiring, promotions) and individual well-being metrics in environments where biased LLM outputs are used, compared to control groups.

## Limitations
- Analysis relies on descriptive prompts rather than task-oriented scenarios, which may not fully capture real-world bias manifestations
- SentenceBERT similarity threshold of 0.8 for identifying bias-free outputs appears somewhat arbitrary
- The study does not provide a complete dataset of professions and human annotations, limiting reproducibility

## Confidence
- High Confidence: GPT-3.5's reduced gender bias performance (0.07 score, 91/99 correct predictions) is well-supported by quantitative results and clear methodology
- Medium Confidence: The persistence of racial bias across all models is convincingly demonstrated, though the severity assessment relies on similarity metrics that could be refined
- Medium Confidence: The LIWC-based linguistic analysis reveals consistent disparities, but interpretation of these differences as "stereotypical" requires additional validation

## Next Checks
1. Test the SentenceBERT similarity threshold of 0.8 across multiple domains to determine if this is an appropriate benchmark for bias-free outputs, or if profession-specific thresholds are needed

2. Evaluate whether changing prompt structure (e.g., from descriptive to task-oriented scenarios) alters the observed racial bias patterns, testing the sensitivity of results to prompt engineering

3. Conduct human evaluation of model-generated descriptions to verify whether the LIWC-detected linguistic differences align with human perceptions of stereotypical language use across racial groups