---
ver: rpa2
title: 'To Tell The Truth: Language of Deception and Language Models'
arxiv_id: '2311.07092'
source_url: https://arxiv.org/abs/2311.07092
tags:
- deception
- judges
- number
- cues
- bottleneck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel dataset derived from the TV game
  show "To Tell The Truth" for deception detection. The dataset includes conversations
  where contestants with conflicting objectives engage in high-stakes dialogues, providing
  objective truth for verification.
---

# To Tell The Truth: Language of Deception and Language Models

## Quick Facts
- **arXiv ID**: 2311.07092
- **Source URL**: https://arxiv.org/abs/2311.07092
- **Reference count**: 18
- **Primary result**: A novel dataset and LLM-based bottleneck framework achieves comparable deception detection performance to human judges, with models identifying deception cases where humans fail.

## Executive Summary
This paper introduces a novel dataset derived from the TV game show "To Tell The Truth" for deception detection research. The dataset contains conversations where contestants with conflicting objectives engage in high-stakes dialogues, providing objective truth for verification. The authors develop a computational model based on a large language model with a bottleneck framework that analyzes language cues to detect deception. The model performs comparably to human judges and can identify deception in cases where humans fail, uncovering new reasoning chains. This work opens possibilities for human-algorithm collaboration in improving deception detection capabilities.

## Method Summary
The method employs a zero-shot LLM-based bottleneck framework that processes conversation transcripts through sequential intermediate steps: entailment, ambiguity, overconfidence, and half-truths. For each conversation snippet, the model uses separate LLM calls to evaluate these deception cues, then aggregates them to predict the real contestant. The approach relies on detailed prompt engineering to bias the discriminator without task-specific training on the T5 dataset. The framework is evaluated on a dataset of 150 episodes transcribed from the TV show, using accuracy and accuracy@2 metrics, plus human evaluation of generated explanations.

## Key Results
- The model achieves deception detection accuracy comparable to human judges using only textual cues
- Models identify deception cases where humans fail by discovering novel language-based reasoning chains
- Sequential bottleneck processing preserves conversational context better than independent derivation
- Zero-shot LLM reasoning with detailed prompts effectively detects deception cues without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The bottleneck framework progressively derives deception cues in a sequential manner that mirrors human judgment.
- **Mechanism**: For each conversation snippet, the model evaluates entailment, ambiguity, overconfidence, and half-truths using separate LLM calls, then aggregates them to predict the real contestant.
- **Core assumption**: Sequential cue derivation preserves conversational context better than independent derivation.
- **Evidence anchors**:
  - [abstract] "Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth"
  - [section] "Our bottleneck models are employed through a set of bottleneck controls... The success of the final prediction depends on the success of the intermediate functions"
  - [corpus] Weak - no direct mention of sequential vs. independent derivation
- **Break condition**: If conversation snippets are too long or noisy, intermediate cue accuracy may degrade and propagate errors to the final prediction.

### Mechanism 2
- **Claim**: Zero-shot LLM reasoning with a detailed prompt can detect deception cues without task-specific training.
- **Mechanism**: A single prompt describing game rules, affidavit structure, and conversation format primes the LLM to apply reasoning chains for truth detection.
- **Core assumption**: LLMs have sufficient world knowledge and reasoning ability to identify deception cues from text alone.
- **Evidence anchors**:
  - [abstract] "Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth"
  - [section] "We resort to a zero-shot approach while biasing the discriminator using a detailed prompt"
  - [corpus] Weak - no explicit mention of prompt effectiveness
- **Break condition**: If deception cues are subtle or heavily rely on multimodal signals (visual/audio), text-only reasoning may fail.

### Mechanism 3
- **Claim**: Models can identify deception in cases where humans fail by discovering novel language-based reasoning chains.
- **Mechanism**: The LLM analyzes linguistic features like ambiguity, overconfidence, and half-truths without human cognitive biases or sensory distractions.
- **Core assumption**: Language cues contain sufficient information to detect deception when multimodal cues are absent.
- **Evidence anchors**:
  - [abstract] "Our model detects novel but accurate language cues in many cases where humans failed to detect deception"
  - [section] "In what follows, we explore if textual cues may increase the likelihood of fraud detection even in the presence of more overt visual or aural indicators"
  - [corpus] Moderate - dataset shows judges using visual cues, models relying on text
- **Break condition**: If deception relies heavily on visual or audio signals, language-only models may not detect it.

## Foundational Learning

- **Concept: Large Language Models (LLMs)**
  - Why needed here: LLMs provide zero-shot reasoning capability to identify deception cues without task-specific training
  - Quick check question: Can an LLM identify deception cues if given only the conversation transcript and affidavit?

- **Concept: Zero-shot Learning**
  - Why needed here: The model must generalize to deception detection without seeing labeled examples from the T5 dataset
  - Quick check question: Does the model require training examples from T5 to perform well?

- **Concept: Bottleneck Framework**
  - Why needed here: The framework breaks down deception detection into interpretable intermediate steps (cue identification)
  - Quick check question: How does sequential cue derivation differ from processing all cues simultaneously?

## Architecture Onboarding

- **Component map**: Name, affidavit, conversation transcript → Base LLM with task prompt → Bottleneck controls (entailment, ambiguity, overconfidence, half-truths) → Discriminator → Final prediction
- **Critical path**: Conversation → Sequential snippet processing → Bottleneck controls → Discriminator → Final prediction
- **Design tradeoffs**:
  - Zero-shot vs. supervised: Zero-shot generalizes better to unseen contexts but may be less precise
  - Sequential vs. independent bottleneck derivation: Sequential preserves context but is slower
  - GPT-4 vs. GPT-3.5 vs. GPT-3: Higher capability models perform better but cost more
- **Failure signatures**:
  - Random or inconsistent predictions: Bottleneck controls not working correctly
  - Low accuracy on sessions with strong visual cues: Model over-relying on text
  - Poor explanations: LLM not reasoning through intermediate steps
- **First 3 experiments**:
  1. Compare sequential vs. independent bottleneck derivation on accuracy
  2. Test different LLM sizes (GPT-3, GPT-3.5, GPT-4) for bottleneck controls
  3. Evaluate zero-shot vs. few-shot performance with demonstration examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the linguistic cues identified by the model compare in effectiveness to other multimodal cues (e.g., visual, audio) in deception detection?
- Basis in paper: [explicit] The paper mentions that humans have access to multimodal cues (language and audio-visual) while the model only accesses language cues, yet achieves similar performance.
- Why unresolved: The paper does not provide a direct comparison of the effectiveness of linguistic cues alone versus multimodal cues in deception detection.
- What evidence would resolve it: A study comparing the deception detection accuracy of models using only linguistic cues to those using multimodal cues, or a direct comparison of human performance with and without access to audio-visual cues.

### Open Question 2
- Question: Can the zero-shot bottleneck models be further improved with few-shot approaches, and if so, what are the optimal methods for selecting demonstration examples?
- Basis in paper: [explicit] The paper mentions experimenting with 2-shot bottleneck models but found performance was either similar or worse depending on the choice of demonstration examples.
- Why unresolved: The paper does not explore the potential for improvement with few-shot approaches or the optimal methods for selecting demonstration examples.
- What evidence would resolve it: Further experiments with different few-shot approaches and methods for selecting demonstration examples, comparing their performance to the zero-shot models.

### Open Question 3
- Question: How does the performance of the model vary across different types of deception (e.g., half-truths, overconfidence, ambiguity) and what are the implications for real-world applications?
- Basis in paper: [explicit] The paper discusses the model's ability to detect different types of deception, such as half-truths, overconfidence, and ambiguity, but does not provide a detailed analysis of performance across these types.
- Why unresolved: The paper does not provide a detailed breakdown of the model's performance across different types of deception.
- What evidence would resolve it: A detailed analysis of the model's performance across different types of deception, including accuracy rates and error patterns for each type, to understand its strengths and weaknesses in real-world applications.

## Limitations

- The paper lacks explicit prompt templates for both the base LLM and bottleneck controls, making exact reproduction difficult.
- No specification of LLM parameters (temperature, max tokens) for different model variants, which could affect performance.
- Human evaluation of explanations may introduce subjectivity and inconsistency in assessing explanation quality.

## Confidence

**High Confidence:**
- The bottleneck framework concept and its four controls (entailment, ambiguity, overconfidence, half-truths) are clearly defined and implementable.
- The dataset creation process from TV show transcripts is reproducible using available tools like Whisper.
- Zero-shot learning approach with detailed prompts is technically sound and has clear implementation path.

**Medium Confidence:**
- The claim that models detect deception in cases where humans fail is supported but would benefit from more detailed analysis of specific failure cases.
- The sequential vs. independent bottleneck derivation comparison lacks empirical validation in the paper.
- The explanation quality evaluation methodology is reasonable but relies on subjective human judgment.

**Low Confidence:**
- Performance claims across different LLM sizes (GPT-3, GPT-3.5, GPT-4) are not empirically validated in the paper.
- The effectiveness of zero-shot learning without any demonstration examples is not thoroughly tested.

## Next Checks

1. **Prompt Template Validation**: Create and test multiple prompt variations for the base LLM and bottleneck controls to assess sensitivity to prompt engineering. Measure how prompt changes affect accuracy and explanation quality.

2. **Human vs. Model Comparison Analysis**: Conduct detailed case studies of specific episodes where humans failed but models succeeded (and vice versa). Analyze whether failures stem from text limitations or model reasoning gaps.

3. **Sequential vs. Independent Derivation Test**: Implement both sequential and independent bottleneck control processing. Compare accuracy, processing time, and error propagation patterns to validate the claimed advantages of sequential processing.