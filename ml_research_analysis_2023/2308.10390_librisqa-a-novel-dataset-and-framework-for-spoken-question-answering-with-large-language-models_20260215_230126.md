---
ver: rpa2
title: 'LibriSQA: A Novel Dataset and Framework for Spoken Question Answering with
  Large Language Models'
arxiv_id: '2308.10390'
source_url: https://arxiv.org/abs/2308.10390
tags:
- speech
- part
- text
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LibriSQA, a novel dataset and framework
  for spoken question answering (SQA) with large language models (LLMs). The dataset,
  curated from Librispeech, consists of 107k free-form and open-ended SQA pairs across
  two parts: Part I with natural conversational formats and Part II with multiple-choice
  questions.'
---

# LibriSQA: A Novel Dataset and Framework for Spoken Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2308.10390
- Source URL: https://arxiv.org/abs/2308.10390
- Reference count: 32
- Key outcome: Introduces LibriSQA dataset (107k SQA pairs) and end-to-end framework achieving WER of 3.95 on ASR and high scores on SQA tasks using LLMs

## Executive Summary
This paper presents LibriSQA, a novel dataset and framework for spoken question answering (SQA) with large language models (LLMs). The dataset consists of 107k free-form and open-ended SQA pairs derived from Librispeech, divided into two parts: natural conversational formats and multiple-choice questions. The proposed lightweight, end-to-end framework reformulates automatic speech recognition (ASR) into the SQA format, leveraging pre-trained speech feature extractors and fine-tuning LLaMA-7B using NLL loss. Experiments demonstrate the framework's effectiveness across both ASR and SQA tasks, achieving state-of-the-art results while enabling direct speech-to-text answer generation without intermediate ASR steps.

## Method Summary
The framework reformulates ASR data into SQA format and trains an end-to-end model that maps speech features directly to text answers. It uses frozen pre-trained speech feature extractors (wav2vec 2.0, HuBERT, or WavLM) to convert speech into representations, which are then concatenated with text features and passed to an LLM decoder (LLaMA-7B with LLaMA-adapter). The model is trained using negative log-likelihood loss to generate free-form answers directly from speech. ASR pre-training on LibriSpeech improves SQA performance. The approach achieves lightweight, efficient training by freezing speech feature extractors while fine-tuning only the LLM parameters.

## Key Results
- Achieves WER of 3.95 on ASR reform task using the proposed framework
- Demonstrates high accuracy, BLEU, ROUGE, and BERT similarity scores on both LibriSQA Part I (free-form) and Part II (multiple-choice)
- Shows that wav2LM speech feature extractor provides superior universal representations compared to wav2vec 2.0 and HuBERT
- ASR pre-training on LibriSpeech improves SQA task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NLL loss enables direct free-form answer generation without intermediate ASR step
- Mechanism: Reformulating ASR data into SQA format with NLL training teaches the model to map speech features to text answers end-to-end
- Core assumption: LLM can learn cross-modal alignment between speech and text representations through joint training
- Evidence: "By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks" and "The implementation of this particular loss function is intended to capacitate the model to anticipate the subsequent word by leveraging both the speech and the current sentence context"
- Break condition: If speech features are insufficiently discriminative, the model cannot generate coherent answers

### Mechanism 2
- Claim: Freezing speech feature extractors creates lightweight, efficient training
- Mechanism: Pre-trained speech models provide frozen feature extraction while only LLM parameters are updated
- Core assumption: Pre-trained speech feature extractors contain sufficient information for SQA tasks
- Evidence: "Throughout the training phase, the parameters of the pre-trained speech models remain frozen, with only the linear layer engaging in the training procedure"
- Break condition: If speech feature extractors don't capture task-relevant information, freezing prevents learning task-specific alignments

### Mechanism 3
- Claim: ASR pre-training improves SQA performance through transferable speech-text alignment
- Mechanism: Training on LibriSpeech in SQA format first helps the model learn speech-text alignment that transfers to better LibriSQA performance
- Core assumption: Learning speech-text alignment on ASR-formatted data provides transferable knowledge for SQA tasks
- Evidence: "We postulate that this might be due to WavLM's capacity for universal representations, whereas wav2vec 2.0 and HuBERT appear more tailored towards the ASR task"
- Break condition: If alignment learned from ASR data doesn't transfer to SQA task, pre-training may not provide benefits

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR)
  - Why needed here: Understanding ASR is crucial because the framework reformulates ASR data into SQA format and uses pre-trained ASR speech feature extractors
  - Quick check question: What is the primary difference between traditional ASR and the SQA approach used in this framework?

- Concept: Negative Log-Likelihood (NLL) Loss
  - Why needed here: NLL loss is used instead of CTC loss to train the model to generate answers directly, fundamental to understanding the training methodology
  - Quick check question: How does NLL loss differ from CTC loss in the context of speech processing?

- Concept: Speech Feature Extraction
  - Why needed here: The framework relies on pre-trained speech feature extractors (wav2vec 2.0, HuBERT, WavLM) to convert speech into representations processed by LLMs
  - Quick check question: What are the key differences between wav2vec 2.0, HuBERT, and WavLM in terms of their approach to speech representation learning?

## Architecture Onboarding

- Component map: Speech → Feature Extractor → Linear Layer → Concatenation → LLM Decoder → Answer Generation
- Critical path: Speech features extracted by frozen wav2vec 2.0/HuBERT/WavLM + linear layer, concatenated with text features, passed to LLaMA-7B decoder with LLaMA-adapter
- Design tradeoffs: Freezing speech features reduces computational cost but may limit task-specific adaptation; NLL loss enables free-form answers but requires more complex training than span prediction
- Failure signatures: Poor WER indicates speech feature extractors not capturing sufficient information; low BLEU/ROUGE scores indicate LLM decoder not generating coherent answers; inconsistent accuracy indicates model not learning to choose correct options
- First 3 experiments: 1) Train on ASR reform task only to verify speech-text alignment capability, 2) Train on LibriSQA Part I only to test free-form answer generation, 3) Train on LibriSQA Part II only to validate multiple-choice reasoning capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of speech feature extractor (wav2vec 2.0, HuBERT, WavLM) affect the framework's performance on the ASR task?
- Basis in paper: [explicit] The paper states that the choice of speech feature extractor markedly influences the results on the ASR task
- Why unresolved: The paper does not provide a detailed comparison or analysis of performance differences between the three feature extractors on the ASR task
- What evidence would resolve it: A comprehensive comparison of ASR performance using each feature extractor with detailed metrics and analysis of strengths and weaknesses

### Open Question 2
- Question: How does the proposed framework compare to existing methods that use ASR modules and textual QA for the SQA task?
- Basis in paper: [inferred] The paper mentions that current SQA methodologies predominantly focus on predicting time spans and that the proposed framework reformulates ASR into the SQA format
- Why unresolved: The paper does not provide a direct comparison between the proposed end-to-end framework and existing methods that use ASR modules and textual QA for the SQA task
- What evidence would resolve it: A detailed comparison of the proposed framework with existing methods including metrics such as accuracy, BLEU, ROUGE, and BERT similarity scores

### Open Question 3
- Question: How does the proposed framework perform on the LibriSQA Part I subset containing free-form and open-ended questions and answers?
- Basis in paper: [explicit] The paper mentions that the model's performance on LibriSQA Part I will be assessed to showcase its aptitude in natural question-answering scenarios
- Why unresolved: The paper does not provide the results or analysis of the model's performance on LibriSQA Part I
- What evidence would resolve it: The experimental results and analysis of the model's performance on LibriSQA Part I including metrics such as BLEU, ROUGE, and BERT similarity scores

## Limitations

- Evaluation is primarily conducted on LibriSpeech-derived data representing clean read speech rather than natural conversational speech
- Comparison with baseline approaches is limited, making it difficult to assess relative improvement over existing SQA methods
- Computational efficiency claims are based on theoretical considerations but actual training/inference times are not reported

## Confidence

**High Confidence**: The framework's core architectural design (frozen speech features + LLM decoder with NLL loss) is technically sound and the experimental methodology is rigorous. The reported results on LibriSpeech and LibriSQA are reproducible based on provided implementation details.

**Medium Confidence**: The claim that wav2LM provides universal representations superior to wav2vec 2.0 and HuBERT is supported by limited ablation studies. The ASR pre-training benefits for SQA are observed but the mechanism behind this transfer is not fully explored.

**Low Confidence**: The assertion that this work "paves the way for the development of universal multimodal LLMs" is speculative. While the framework demonstrates SQA capability, it hasn't been tested on broader multimodal tasks or scaled to larger datasets.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the framework on spontaneous speech datasets (e.g., TED talks, podcasts) to assess performance degradation when moving beyond read speech

2. **Ablation study on speech feature extractors**: Conduct detailed analysis comparing wav2vec 2.0, HuBERT, and WavLM performance across different speech characteristics (speaker variability, background noise, speaking rate)

3. **Efficiency benchmarking**: Measure actual training time, memory usage, and inference latency for the framework versus traditional ASR+SQA pipeline to validate computational efficiency claims