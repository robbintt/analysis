---
ver: rpa2
title: 'HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large
  Object Count'
arxiv_id: '2310.00723'
source_url: https://arxiv.org/abs/2310.00723
tags:
- object
- data
- dataset
- giver
- handover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HOH is a markerless multimodal dataset for human-object-human handover
  with 136 objects, 40 participants, and 2,720 interactions. It captures natural handover
  interactions using 4 Kinect RGB-D and 4 Point Grey cameras, recording multi-view
  RGB, depth, skeletons, fused point clouds, grasp types, segmentations, comfort ratings,
  and 6DOF object poses.
---

# HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count

## Quick Facts
- **arXiv ID**: 2310.00723
- **Source URL**: https://arxiv.org/abs/2310.00723
- **Reference count**: 40
- **Key outcome**: HOH is a markerless multimodal dataset for human-object-human handover with 136 objects, 40 participants, and 2,720 interactions.

## Executive Summary
HOH is a markerless multimodal dataset capturing human-object-human handover interactions. It includes 136 objects, 40 participants, and 2,720 interactions recorded using 4 Kinect RGB-D and 4 Point Grey cameras. The dataset provides multi-view RGB, depth, skeletons, fused point clouds, grasp types, segmentations, comfort ratings, and 6DOF object poses. HOH enables AI research on human-robot handover by providing diverse, high-resolution data for learning grasp, orientation, and trajectory prediction from 2D/3D data. It is the largest handover dataset in object count, participants, and role-reversal pairs, and the only fully markerless capture.

## Method Summary
HOH captures natural handover interactions using 4 Azure Kinect RGB-D and 4 FLIR Point Grey RGB cameras, providing 360° coverage. The dataset records multi-view RGB and depth data, synchronized at 30Hz (Kinect) and 60Hz (Point Grey). Ground truth annotations include hand and object segmentations, 28-class grasp types, 6DOF object poses obtained via ICP alignment, and manual key event annotations (G, T, R). The dataset spans 136 objects across 8 everyday use classes, with interactions involving 40 participants performing 68 role-reversal pairs. Data undergoes preprocessing including OpenPose skeleton extraction, SAM segmentation, multi-view fusion, and denoising before model training.

## Key Results
- HOH is the largest handover dataset with 136 objects, 40 participants, and 2,720 interactions
- Neural networks trained on HOH successfully predict grasp, orientation, and trajectories
- The dataset is the only fully markerless capture, enabling natural clothing and motion capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Markerless capture enables natural human-human interaction without restricting clothing or requiring markers, leading to more realistic handover dynamics.
- Mechanism: By avoiding markers and suits, participants move freely in natural attire, allowing capture of authentic posture, grasp, and transfer behaviors that would be constrained in marker-based setups.
- Core assumption: Natural clothing and unrestricted movement are essential for capturing realistic handover dynamics relevant to robotics.
- Evidence anchors:
  - [abstract] states it is the "only fully markerless handover capture dataset" and overcomes challenges with "markered datasets that require specific suiting for body tracking."
  - [section] discusses the dataset's markerless approach to capture "natural real-world motions and clothing."
- Break condition: If markerless tracking cannot reliably estimate hand/object pose or segmentation, realism gains are lost.

### Mechanism 2
- Claim: Large object diversity (136 objects) enables robust learning of handover parameters across varied object properties.
- Mechanism: With 136 objects spanning 8 everyday use classes and 17 categories based on aspect ratio and functionality, the dataset provides sufficient variability in size, shape, mass, and affordances to train models that generalize beyond small object sets.
- Core assumption: Generalizable handover parameter estimation requires training data covering a wide range of object geometries and physical properties.
- Evidence anchors:
  - [abstract] highlights it is "the largest handover dataset in object count" with 136 objects.
  - [section] details objects spanning "toys, mugs, food/drink items, cooking utensils, tools, office items, household items, and 3D printed items."
- Break condition: If object count or diversity is insufficient, models may overfit to specific object types.

### Mechanism 3
- Claim: Multi-view 360° capture enables full geometric understanding of hand-object interactions, critical for robotic manipulation planning.
- Mechanism: Using 4 Kinect RGB-D and 4 Point Grey cameras provides full 360° coverage, ensuring that occluded surfaces during handover are visible from at least one viewpoint, enabling accurate 3D reconstruction and pose estimation.
- Core assumption: Complete 3D geometric data from all angles is necessary for robots to plan safe grasps and transfers.
- Evidence anchors:
  - [abstract] mentions "multi-view RGB and depth data" and "360° coverage."
  - [section] explains the use of "4 30FPS Kinect RGB-D sensors and 4 60FPS FLIR Point Grey cameras to perform 360° allocentric capture."
- Break condition: If multi-view fusion fails due to calibration or synchronization errors, 3D reconstruction quality degrades.

## Foundational Learning

- Concept: Handover phases (reach and grasp, transport, transfer, end)
  - Why needed here: Understanding these phases is critical for interpreting key event annotations (G, T, R) and modeling the temporal dynamics of handovers.
  - Quick check question: What are the four phases of a handover according to Kopnarski et al. [33]?

- Concept: Grasp taxonomy (Cini et al. [18])
  - Why needed here: The dataset labels grasps using a 28-class taxonomy; knowing this taxonomy is essential for analyzing grasp type distributions and training grasp prediction models.
  - Quick check question: What are the three main grasp classifications in the Cini et al. taxonomy?

- Concept: 6DOF object pose estimation via ICP alignment
  - Why needed here: The dataset provides GT 6DOF object poses by aligning 3D models to point clouds; understanding this process is key for using pose data in experiments.
  - Quick check question: How is the 6DOF object pose obtained in HOH?

## Architecture Onboarding

- Component map: Capture (4 Azure Kinect + 4 FLIR Point Grey) -> Synchronization -> Multi-view fusion -> Segmentation (OpenPose, SAM) -> Annotation (manual G,T,R) -> Tracking (Track Anything) -> Model training
- Critical path: Capture → Synchronization → Multi-view fusion → Segmentation → Annotation → Tracking → Model training
- Design tradeoffs: Markerless vs. marker-based (naturalness vs. tracking precision), high object count vs. annotation effort, 360° vs. single-view (completeness vs. computational load)
- Failure signatures: Missing frames → data gaps; poor calibration → misalignment; segmentation errors → wrong masks; ICP failure → wrong pose
- First 3 experiments:
  1. Train o2gg on complete data: predict giver hand point cloud from object cloud at Gpre
  2. Train g2rt on trajectory centroids: predict receiver trajectory from giver trajectory
  3. Train o2or: predict object orientation at T from object cloud at Gpre

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of grip force data impact the accuracy of robotic handover parameter prediction?
- Basis in paper: [inferred] The paper mentions grip force as a limitation due to the use of instrumentation-free setup and suggests future work on instrumenting everyday objects with grip force units.
- Why unresolved: The current dataset lacks grip force data, preventing analysis of its impact on robotic handover parameter prediction.
- What evidence would resolve it: A follow-up study that collects grip force data using minimally invasive grip force units on everyday objects and evaluates its impact on the accuracy of robotic handover parameter prediction models trained on HOH data.

### Open Question 2
- Question: How do handover trajectories vary across different participant demographics, such as age and ability?
- Basis in paper: [explicit] The paper acknowledges the current dataset's narrow range over age and ability and suggests future collections to include children, older adults, and individuals with different abilities.
- Why unresolved: The current dataset does not include a diverse range of participants, limiting the generalizability of findings to different demographic groups.
- What evidence would resolve it: A study that collects handover data from a diverse range of participants across different age groups, abilities, and demographics and analyzes how handover trajectories vary across these groups.

### Open Question 3
- Question: How does the alignment between giver and receiver comfort ratings impact the success of handover interactions?
- Basis in paper: [explicit] The paper mentions a high overall giver/receiver Pearson correlation of 0.38 and suggests future studies to analyze the alignment between giver and receiver comfort ratings.
- Why unresolved: While the paper provides an initial analysis of comfort ratings, it does not delve into how the alignment between giver and receiver comfort impacts the success of handover interactions.
- What evidence would resolve it: A study that collects detailed data on the alignment between giver and receiver comfort ratings and analyzes how this alignment correlates with the success of handover interactions, measured by factors such as task completion time, errors, and subjective ratings of interaction quality.

## Limitations
- No quantitative evaluation of markerless pose estimation accuracy during close proximity handover phases
- Reliance on manual annotation for key events introduces potential human labeling bias
- Experimental results lack comparison against baseline methods or ablation studies

## Confidence

- **High Confidence**: The dataset construction methodology and technical specifications (object count, participant number, camera setup, annotation types) are well-documented and verifiable from the provided information.
- **Medium Confidence**: The claim that HOH is "the largest handover dataset" is supported by comparison to cited datasets, but the absolute magnitude of improvement in object count and participant diversity could not be independently verified.
- **Medium Confidence**: The experimental results showing neural network performance on HOH are internally consistent but lack external validation or comparison to alternative datasets or methods.

## Next Checks

1. **Tracking Accuracy Validation**: Conduct a quantitative evaluation of hand and object pose estimation accuracy during handover phases by comparing markerless estimates against a small subset of marker-based ground truth data.
2. **Annotation Consistency Analysis**: Perform inter-rater reliability testing on key event annotations (G, T, R) by having multiple annotators label the same subset of interactions and calculating agreement metrics.
3. **Baseline Comparison Study**: Implement and compare at least two baseline methods (e.g., heuristic-based approaches or models trained on smaller handover datasets) against the HOH-trained networks to establish performance gains attributable to dataset size and diversity.