---
ver: rpa2
title: Model of models -- Part 1
arxiv_id: '2308.04600'
source_url: https://arxiv.org/abs/2308.04600
tags:
- will
- also
- more
- learning
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MOM, a cognitive model for AGI agents that
  extends prior models by incorporating operational models, will, and dual learning
  approaches. MOM represents knowledge as a dynamic semantic network of models and
  instances, emphasizing cognitive operations like generalization, specialization,
  and consolidation.
---

# Model of models -- Part 1

## Quick Facts
- arXiv ID: 2308.04600
- Source URL: https://arxiv.org/abs/2308.04600
- Reference count: 29
- One-line primary result: Proposes MOM, a cognitive model for AGI agents integrating symbolic and connectionist paradigms through dual learning and will-driven decision-making

## Executive Summary
This paper introduces MOM (Model of Models), a comprehensive cognitive architecture for AGI agents that extends prior models by incorporating operational models, will, and dual learning approaches. The model represents knowledge as a dynamic semantic network of models and instances, emphasizing cognitive operations like generalization, specialization, and consolidation. MOM aims to achieve mature intelligence by aligning will with actions and organizing knowledge efficiently through a combination of empirical and rule-based learning.

## Method Summary
MOM proposes a hybrid cognitive architecture combining symbolic and connectionist paradigms through program-search-based learning. The method involves implementing a dynamic semantic network to represent knowledge as models and instances, integrating attention mechanisms for model isolation, and developing dual learning approaches (empirical and rule-based) with consolidation principles. The architecture emphasizes will-driven decision-making and reusability of cognitive operations to achieve scalable, interpretable knowledge organization.

## Key Results
- Presents a theoretical framework for AGI cognition integrating operational models, will, and dual learning approaches
- Introduces consolidation as a mechanism to transform chaotic knowledge into stable symbolic representations
- Proposes attention-based model separation for scalable knowledge organization
- Emphasizes explainability through program-search-based learning combining symbolic and connectionist paradigms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual learning approaches (empirical and rule-based) produce a stable cognitive model through consolidation.
- Mechanism: Empirical learning builds bottom-up models from examples, while rule-based learning provides top-down constraints. Consolidation merges these into a consistent symbolic representation.
- Core assumption: Symbolic models are more stable and interpretable than purely connectionist ones.
- Evidence anchors:
  - [abstract] "The model integrates symbolic and connectionist paradigms through program-search-based learning, enabling explainability and flexibility."
  - [section 7.2] "Here, model learning mechanism is proposed, where two contrary but completing learning approaches in AI are combined [Day, 2022]: empirical, i.e. from examples (induction), and expertise (rule-based)."
  - [corpus] Weak - no explicit consolidation evidence in corpus.
- Break condition: If consolidation cannot resolve conflicts between empirical and rule-based models, the system may oscillate or produce inconsistent behavior.

### Mechanism 2
- Claim: Will acts as a field that aligns knowledge models and guides action selection.
- Mechanism: Will is represented as a vector in state space, influencing which actions are relevant and creating a directional field for problem-solving.
- Core assumption: Will can be learned and aligned with actions through experience.
- Evidence anchors:
  - [abstract] "MOM represents knowledge as a dynamic semantic network of models and instances, emphasizing cognitive operations like generalization, specialization, and consolidation."
  - [section 3.5] "As been mentioned previously, if actions are vectors, then will is also a vector. This fact may imply that the direction property of the action is will..."
  - [corpus] Moderate - corpus contains similar papers discussing purpose-driven models.
- Break condition: If will alignment fails, the agent may produce irrelevant or inefficient actions.

### Mechanism 3
- Claim: Model separation and attention enable scalable, interpretable knowledge organization.
- Mechanism: Models are separated into distinct components, and attention mechanisms focus on relevant subsets during processing.
- Core assumption: Limited working memory capacity requires selective attention to manage complexity.
- Evidence anchors:
  - [abstract] "The model is introduced in its mature intelligence state...incorporating operational models, will, and dual learning approaches."
  - [section 8.2] "After assuming a set of unlearned models...will or more probably consciousness, acting like a flashlight or a beacon, produces consistent attention..."
  - [corpus] Moderate - corpus includes papers on attention-based AGI architectures.
- Break condition: If attention mechanisms are too rigid, the system may miss important cross-model interactions.

## Foundational Learning

- Concept: Dual learning (empirical + rule-based)
  - Why needed here: Provides both flexibility to learn from data and structure to maintain consistency
  - Quick check question: Can you explain how empirical learning builds models bottom-up while rule-based learning provides top-down constraints?

- Concept: Consolidation
  - Why needed here: Transforms chaotic, uninterpretable knowledge into stable, symbolic representations
  - Quick check question: How does consolidation reduce the hypothesis space from many possibilities to a small set of consistent patterns?

- Concept: Model separation with attention
  - Why needed here: Manages complexity by focusing on relevant model subsets while maintaining overall system coherence
  - Quick check question: What happens when attention shifts between different model components during problem-solving?

## Architecture Onboarding

- Component map: Perception → Attention selection → Model activation → Will alignment → Action selection → Learning consolidation
- Critical path: Perception → Attention selection → Model activation → Will alignment → Action selection → Learning consolidation
- Design tradeoffs: Flexibility vs. interpretability, computational efficiency vs. model complexity, generalization vs. specialization
- Failure signatures: Inconsistent model behavior, attention fixation, consolidation collapse, will-action misalignment
- First 3 experiments:
  1. Implement dual learning on simple symbolic tasks to verify consolidation works
  2. Test will alignment by having agent solve problems with explicit goal states
  3. Validate attention mechanism by measuring model selection accuracy in multi-task scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can will be learned in AGI systems during the evolution phase, and what are the mechanisms for aligning will with actions?
- Basis in paper: [explicit] The paper discusses the importance of will in AGI and proposes that actions are learned to align with will during the evolution phase. It also mentions inverse reinforcement learning and imitation learning as potential approaches for learning will.
- Why unresolved: The paper does not provide a detailed implementation of how will is learned or how it aligns with actions. It only proposes the idea and mentions some general approaches.
- What evidence would resolve it: A concrete implementation of will learning and alignment in an AGI system, demonstrating how will influences action selection and how it is updated through interaction with the environment.

### Open Question 2
- Question: How can the dynamic operational memory of models and instances be implemented in practice, and what are the challenges in learning and maintaining this memory?
- Basis in paper: [explicit] The paper proposes a cognitive model with a dynamic operational memory, but does not provide a detailed implementation or discuss the challenges in learning and maintaining this memory.
- Why unresolved: The paper only presents the concept of dynamic operational memory and its advantages, but does not address the practical challenges in implementing and maintaining it.
- What evidence would resolve it: A working implementation of the dynamic operational memory in an AGI system, along with an analysis of the challenges and solutions in learning and maintaining this memory.

### Open Question 3
- Question: How can the consolidation principle be effectively utilized in the learning process to achieve a mature intelligence state, and what are the trade-offs involved?
- Basis in paper: [explicit] The paper discusses the consolidation principle as a key aspect of the cognitive evolution process, but does not provide a detailed explanation of how it is utilized or the trade-offs involved.
- Why unresolved: The paper only mentions the consolidation principle and its importance, but does not explain how it is applied in practice or the potential trade-offs in using this approach.
- What evidence would resolve it: A detailed analysis of the consolidation process in an AGI system, including the methods used, the trade-offs involved, and the impact on the overall learning process and performance.

## Limitations
- Lacks empirical validation and concrete implementation details
- Key mechanisms like will alignment and consolidation are described conceptually without operational specifications
- The integration of symbolic and connectionist components remains theoretical
- Computational complexity and scalability concerns are not addressed

## Confidence
- Dual learning mechanism: Medium
- Will alignment: Low
- Model consolidation: Medium
- Attention mechanisms: Medium
- Overall architecture: Medium

## Next Checks
1. Implement a minimal prototype demonstrating dual learning consolidation on a simple symbolic task
2. Design experiments to measure will alignment effectiveness in goal-directed behavior
3. Test attention mechanism scalability with increasing model complexity