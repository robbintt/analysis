---
ver: rpa2
title: Successfully Applying Lottery Ticket Hypothesis to Diffusion Model
arxiv_id: '2310.18823'
source_url: https://arxiv.org/abs/2310.18823
tags:
- winning
- diffusion
- conference
- ticket
- tickets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper for the first time applies the Lottery Ticket Hypothesis
  to diffusion models, specifically Denoising Diffusion Probabilistic Models (DDPM).
  The authors identify sparse subnetworks (winning tickets) at 90%-99% sparsity across
  three benchmark datasets (CIFAR-10, CIFAR-100, MNIST) without compromising performance.
---

# Successfully Applying Lottery Ticket Hypothesis to Diffusion Model

## Quick Facts
- arXiv ID: 2310.18823
- Source URL: https://arxiv.org/abs/2310.18823
- Reference count: 40
- This paper for the first time applies the Lottery Ticket Hypothesis to diffusion models, specifically Denoising Diffusion Probabilistic Models (DDPM)

## Executive Summary
This paper introduces a novel application of the Lottery Ticket Hypothesis to Denoising Diffusion Probabilistic Models (DDPM), identifying sparse subnetworks (winning tickets) at 90%-99% sparsity across three benchmark datasets (CIFAR-10, CIFAR-100, MNIST) without compromising performance. The authors propose a variable pruning ratio strategy across different layers based on empirical observations of layerwise similarity in winning tickets, achieving up to 90% reduction in FLOPs while maintaining or even improving sample quality compared to the original dense model.

## Method Summary
The method involves iterative magnitude-based pruning of a DDPM model, with varying sparsity ratios across layers determined by centered kernel alignment (CKA) similarity measurements. After training and pruning, weights are rewound to early-stage initialization (5% of current iteration count) before the next iteration. The approach leverages the observation that upstream layers show higher similarity between winning tickets than downstream layers, justifying less aggressive pruning for upstream layers where parameters are more critical for learning meaningful representations from noisy input data.

## Key Results
- Successfully identified winning tickets at 90%-99% sparsity across CIFAR-10, CIFAR-100, and MNIST datasets
- Achieved up to 90% reduction in FLOPs while maintaining or improving sample quality
- Variable pruning ratio strategy based on layerwise similarity improved efficiency over uniform sparsity approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layerwise similarity in winning tickets drives variable pruning efficiency
- Mechanism: The paper observes that upstream layers of winning tickets show higher CKA similarity than downstream layers. By configuring lower pruning ratios for upstream layers (where parameters are more critical), the method preserves essential information while achieving higher overall sparsity.
- Core assumption: The similarity measured by CKA between winning ticket layers correlates with their importance for model performance
- Evidence anchors:
  - [abstract] "We observe that the similarity between two winning tickets of a model varies from block to block. Specifically, the upstream layers from two winning tickets for a model tend to be more similar than the downstream layers."
  - [section] "We introduce centered kernel alignment (CKA) as an index to measure the similarity between the sparsified modules from two winning tickets."
- Break condition: If CKA similarity does not correlate with actual layer importance for denoising performance, the variable pruning strategy may fail

### Mechanism 2
- Claim: Rewinding to early initialization preserves lottery ticket properties in diffusion models
- Mechanism: The iterative pruning process involves training, pruning, then rewinding parameters to an early stage (5% of current iteration count). This maintains the original initialization structure needed for the winning ticket hypothesis to hold.
- Core assumption: Early-stage parameters retain lottery ticket properties that enable sparse subnetworks to train effectively
- Evidence anchors:
  - [section] "After each iteration of training and pruning, we rewind the model with the parameters at τ epoch."
  - [abstract] "The winning tickets can be identified by training a network and pruning its parameters with the smallest magnitude in an iterative way or one-shot way. Before it is trained in each iteration, the weights will be reset to the original initialization."
- Break condition: If diffusion models require different initialization strategies than standard neural networks, rewinding may not preserve winning ticket properties

### Mechanism 3
- Claim: Noise levels in diffusion process justify layer-specific sparsity
- Mechanism: Early denoising stages involve highly noisy data requiring more parameters to learn meaningful representations. This justifies keeping upstream layers less sparse than downstream layers where noise has been reduced.
- Core assumption: The amount of noise in input data correlates with the number of parameters needed for effective denoising
- Evidence anchors:
  - [abstract] "Intuitively, we need to make sure there are enough parameters to be trained so that meaningful full hidden states can be learned from noisy input data."
  - [section] "Intuitively, the input data is highly noisy in the denoising process and we need more parameters to learn a meaningful full hidden state."
- Break condition: If diffusion model architecture changes to handle noise differently, the correlation between noise levels and required parameters may not hold

## Foundational Learning

- Concept: Lottery Ticket Hypothesis (LTH)
  - Why needed here: Understanding LTH is fundamental to grasping why sparse subnetworks can match or exceed dense model performance
  - Quick check question: What are the three key components required to identify a winning ticket according to LTH?

- Concept: Denoising Diffusion Probabilistic Models (DDPM)
  - Why needed here: The paper applies LTH to diffusion models, requiring understanding of how these models work
  - Quick check question: How does the reverse process in DDPM differ from the forward diffusion process in terms of noise levels?

- Concept: Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA)
  - Why needed here: These metrics are used to measure similarity between winning tickets across different layers
  - Quick check question: What does a higher CKA score between two layers indicate about their representational similarity?

## Architecture Onboarding

- Component map: U-Net backbone for diffusion model → Iterative training and pruning loop → Layer-specific pruning ratio scheduler → Early-stage parameter rewinding mechanism → Performance evaluation pipeline (FID score)
- Critical path: Train → Prune → Rewind → Repeat until target sparsity → Evaluate performance
- Design tradeoffs:
  - Higher upstream layer retention vs. overall sparsity goals
  - Rewinding timing (early vs. later stages) affects lottery ticket quality
  - Computational cost of iterative pruning vs. one-shot pruning
  - Layer grouping strategy for pruning ratio assignment
- Failure signatures:
  - Performance degradation when upstream layers are pruned too aggressively
  - Lottery ticket properties lost if rewinding happens too late in training
  - Inconsistent results across different random seeds indicating instability
  - Subnetworks that cannot train to convergence when isolated
- First 3 experiments:
  1. Verify baseline LTH on dense DDPM with unified sparsity across all layers
  2. Implement layerwise similarity measurement using CKA to confirm upstream-downstream difference
  3. Test variable pruning ratios with gradual increase from upstream to downstream layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we use sub-networks with different sparsity at different stages of the denoising process to improve efficiency?
- Basis in paper: [explicit] "We raise a new question regarding improving the efficiency of reversing: can we use sub-networks with different sparsity in the reverse process?"
- Why unresolved: The authors acknowledge this as an open question and note challenges including how to optimize a dense model and a winning ticket while guaranteeing convergence, and deciding at which step to use the winning ticket.
- What evidence would resolve it: Experimental results showing improved efficiency (reduced FLOPs/time) when using different sparsity levels at different denoising stages, while maintaining or improving sample quality compared to using uniform sparsity throughout.

### Open Question 2
- Question: What are the theoretical implications of winning tickets for understanding optimization and generalization in diffusion models?
- Basis in paper: [explicit] "Since the combination of sparse architectures and initializations in a winning ticket can reveal the potential implications for theoretical study of optimization and generalization in diffusion models..."
- Why unresolved: The authors only mention this as a potential area for theoretical study without providing specific theoretical results or analysis.
- What evidence would resolve it: Theoretical proofs or analysis showing how winning tickets relate to optimization dynamics, generalization bounds, or other theoretical properties of diffusion models.

### Open Question 3
- Question: How does the varying sparsity approach compare to other efficient sampling methods like DDIM, DPM-Solver, or EDM-Sampling?
- Basis in paper: [inferred] The authors mention existing efficient sampling methods but focus on parameter pruning as their approach, without comparing the two paradigms.
- Why unresolved: The paper doesn't provide empirical comparisons between pruning-based efficiency gains and sampling-based efficiency gains.
- What evidence would resolve it: Head-to-head experimental comparisons showing the relative performance (in terms of quality vs. efficiency trade-offs) between pruning-based methods and sampling-based acceleration techniques.

## Limitations

- The approach depends heavily on the assumption that upstream layer similarity correlates with parameter importance, which may not generalize across different diffusion model architectures or datasets
- The CKA-based similarity measurement may not perfectly capture the true importance of parameters for denoising performance
- The rewinding strategy (resetting to 5% of current iteration count) may be suboptimal for diffusion models specifically and was not extensively validated

## Confidence

- **High confidence**: The identification of sparse winning tickets at 90-99% sparsity maintaining performance (supported by empirical results across three datasets)
- **Medium confidence**: The variable pruning ratio strategy based on layerwise similarity (CKA measurements provide reasonable evidence but may not capture all relevant factors)
- **Medium confidence**: The claim of up to 90% FLOP reduction (computational analysis appears sound but depends on specific hardware implementation)

## Next Checks

1. **Architecture Generalization Test**: Apply the method to different diffusion model variants (e.g., improved DDPM, score-based models) to verify if upstream-downstream similarity patterns hold across architectures

2. **Alternative Similarity Metrics**: Compare CKA-based layer similarity with other metrics (e.g., mutual information, reconstruction error) to validate that CKA effectively identifies critical parameters

3. **Dynamic Sparsity Validation**: Implement the proposed open question about using different sparsity levels at different denoising stages and measure impact on both performance and computational efficiency