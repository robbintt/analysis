---
ver: rpa2
title: Group-blind optimal transport to group parity and its constrained variants
arxiv_id: '2310.11407'
source_url: https://arxiv.org/abs/2310.11407
tags:
- supp
- data
- repair
- fairness
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in machine learning when sensitive
  attributes are unavailable or illegal to collect. The authors propose a group-blind
  optimal transport approach that projects feature distributions of privileged and
  unprivileged groups to achieve demographic parity without requiring individual sensitive
  attribute values.
---

# Group-blind optimal transport to group parity and its constrained variants

## Quick Facts
- arXiv ID: 2310.11407
- Source URL: https://arxiv.org/abs/2310.11407
- Reference count: 27
- Primary result: Achieves demographic parity without individual sensitive attributes using population-level distributions and optimal transport

## Executive Summary
This paper addresses fairness in machine learning when sensitive attributes are unavailable or illegal to collect. The authors propose a group-blind optimal transport approach that projects feature distributions of privileged and unprivileged groups to achieve demographic parity without requiring individual sensitive attribute values. The method uses population-level distribution information and assumes unbiased sampling from the population. Experiments on synthetic and real data (Adult census dataset) demonstrate the effectiveness of the method, achieving similar or better fairness metrics while maintaining good classification performance.

## Method Summary
The approach extends entropic regularized optimal transport by adding a constraint that bounds the total variation distance between projected group distributions. The algorithm uses Dykstra's method with specialized KL projections to solve the constrained optimization problem. By setting the target distribution equal to the training data distribution, the method preserves classification performance while achieving group parity in the projected features. The repair threshold parameter allows for controllable trade-offs between bias repair and data distortion.

## Key Results
- Achieves disparate impact closer to 1 and S-wise TV distance close to 0 on Adult census dataset
- Maintains f1 scores comparable to or better than baseline methods
- Provides controllable balance between bias repair and data distortion through repair threshold parameter
- Demonstrates effectiveness on both synthetic and real data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group-blind optimal transport achieves demographic parity without requiring individual sensitive attribute values by using population-level distributions and unbiased sampling assumptions.
- Mechanism: The algorithm projects feature distributions of privileged and unprivileged groups to a common target distribution using a coupling matrix γ that satisfies specific constraints, while the projection map T is constructed to be independent of the sensitive attribute.
- Core assumption: Source data represents unbiased samples from the broader population, allowing population-level distribution information to be used without individual sensitive attribute values.
- Evidence anchors:
  - [abstract] "Our approach utilises the feature distributions of the privileged and unprivileged groups in a boarder population and the essential assumption that the source data are unbiased representation of the population."
  - [section 1.2] "The challenge of the availability of protected attributes has been partially addressed by..."
- Break condition: If source data is biased or not representative of the population, the population-level distributions will not accurately reflect group characteristics, breaking the assumption.

### Mechanism 2
- Claim: The constrained optimal transport formulation ensures that projected distributions achieve either total repair (exact parity) or partial repair (within a threshold) by adding a constraint on the total variation distance between group distributions.
- Mechanism: The algorithm extends entropic regularized optimal transport by adding a constraint that bounds the total variation distance between projected group distributions, which is enforced through Dykstra's method with specialized KL projections.
- Core assumption: The vector V, which captures the difference between group distributions, can be computed from population-level information even when individual sensitive attributes are unavailable.
- Evidence anchors:
  - [section 3.2] "if one wishes to achieve total repair, the coupling should satisfy... γtrV = 0"
  - [section 3.3] "We say that Θ-repair is satisfied if... TV(PXs0, PXs1) ≤ ∥Θ∥1/2"
- Break condition: If the computation of V is inaccurate due to violations of the unbiased sampling assumption, the constraint on total variation distance may not be properly enforced.

### Mechanism 3
- Claim: The algorithm maintains classification performance while achieving fairness by allowing the target distribution to be chosen as the training data distribution rather than requiring a barycenter between group distributions.
- Mechanism: By setting the target distribution P~X equal to the training data distribution, the algorithm preserves the classification or prediction performance of pre-trained models while achieving group parity in the projected features.
- Core assumption: The pre-trained model's performance is tied to the distribution of the training data, so preserving this distribution in the target maintains performance.
- Evidence anchors:
  - [section 3.6] "the distribution of training data used to learn such a pre-trained model can be our target distribution, to preserve the classification or prediction performance"
  - [section 4.1] "The target distribution is simply PX to avoid unnecessary data distortion"
- Break condition: If the relationship between model performance and training data distribution is not as assumed, or if the training data distribution itself is biased, maintaining this distribution may not preserve or improve performance.

## Foundational Learning

- Optimal transport basics
  - Why needed here: The core technique uses optimal transport to find a coupling between source and target distributions that minimizes cost while satisfying fairness constraints
  - Quick check question: What is the role of the entropic regularization parameter ϵ in the optimal transport formulation?

- KL divergence and Bregman projections
  - Why needed here: The algorithm uses KL projections within Dykstra's method to enforce the fairness constraints on the coupling matrix
  - Quick check question: How does the KL projection PKL_C(ξ) differ from a standard Euclidean projection onto set C?

- Total variation distance and fairness metrics
  - Why needed here: The algorithm measures and constrains the total variation distance between group distributions to achieve the desired level of fairness repair
  - Quick check question: What is the relationship between total variation distance and demographic parity?

## Architecture Onboarding

- Component map:
  - Input layer: Source data distributions (PX, PXs0, PXs1), target distribution (P~X), cost matrix C, entropic parameter ϵ, repair threshold Θ
  - Core algorithm: Dykstra's method with KL projections for constrained optimal transport
  - Output layer: Coupling matrix γ, projection map T, projected data distributions
  - Evaluation: Fairness metrics (disparate impact, TV distance) and performance metrics (f1 scores)

- Critical path:
  1. Compute vector V from population-level distributions
  2. Initialize coupling matrix γ with exp(-C/ϵ)
  3. Iterate Dykstra's algorithm with KL projections onto C1, C2, and C3
  4. Compute projection map T from optimal coupling
  5. Apply T to source data to obtain projected data
  6. Evaluate fairness and performance metrics

- Design tradeoffs:
  - Entropic regularization vs. exact transport: Balances computational efficiency with solution accuracy
  - Repair threshold Θ vs. data distortion: Controls the trade-off between fairness and utility
  - Target distribution choice: Affects both fairness outcomes and model performance preservation

- Failure signatures:
  - Infeasible coupling: No solution exists that satisfies all constraints (check V computation and unbiased sampling assumption)
  - Poor convergence: Dykstra's algorithm fails to converge within iteration limit (check cost matrix scaling and regularization parameter)
  - Unintended bias: Projected distributions show unexpected disparities (verify population-level distribution calculations)

- First 3 experiments:
  1. Verify basic functionality with synthetic data where ground truth distributions are known
  2. Test sensitivity to the repair threshold Θ parameter by varying it and observing fairness-performance trade-offs
  3. Validate unbiased sampling assumption by comparing V computed from source data vs. known population distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical convergence rate of the Dykstra algorithm with KL projections for the constrained optimal transport formulation, and how does it compare to other projection-based methods?
- Basis in paper: [inferred] The paper mentions that Dykstra's algorithm is used to solve the constrained optimal transport problem and refers to convergence results in Bauschke et al. (2020) and Bauschke and Lewis (2000), but does not provide a specific convergence rate analysis for their particular formulation.
- Why unresolved: The authors only state that Dykstra's algorithm converges without quantifying the speed of convergence or comparing it to alternative methods.
- What evidence would resolve it: A rigorous proof of the convergence rate for Dykstra's algorithm applied to this specific formulation, along with experimental comparisons to other optimization methods.

### Open Question 2
- Question: How does the choice of target distribution (e.g., barycenter vs. training data distribution) impact the trade-off between fairness and classification performance in real-world applications?
- Basis in paper: [explicit] The paper discusses different choices of target distributions in Section 3.6 and notes that the choice depends on whether one prioritizes data distortion or preserving classification performance, but does not provide a comprehensive analysis of this trade-off.
- Why unresolved: The authors only briefly mention the trade-off without conducting an in-depth study of how different target distributions affect the balance between fairness and accuracy.
- What evidence would resolve it: Extensive experiments comparing the performance of different target distributions on various real-world datasets, measuring both fairness metrics and classification accuracy.

### Open Question 3
- Question: How can the proposed framework be extended to handle multiple sensitive attributes or continuous sensitive attributes?
- Basis in paper: [explicit] The paper states that the limitation is that only one binary attribute is allowed and suggests extending the schemes to cases where the sensitive attribute has multiple classes, but does not provide a concrete solution.
- Why unresolved: The authors acknowledge the limitation but do not propose a specific method for handling more complex sensitive attribute scenarios.
- What evidence would resolve it: A mathematical formulation and algorithmic extension of the framework to handle multiple or continuous sensitive attributes, along with experimental validation on datasets with such attributes.

## Limitations

- The method relies on the assumption that source data is an unbiased representation of the population, which may not hold in practice
- The KL projection implementation for constraint set C3 involves numerical root-finding that could introduce computational instability
- The framework is limited to handling only one binary sensitive attribute, with no concrete extension proposed for multiple or continuous attributes

## Confidence

- High: The core mechanism of using optimal transport for distribution alignment is well-established and mathematically sound
- Medium: The extension to group-blind settings and the specific constraint formulations are novel but depend on the validity of population-level distribution assumptions
- Medium: Empirical results show promising performance, but the evaluation is limited to specific datasets and may not generalize to all domains

## Next Checks

1. Test the algorithm's sensitivity to violations of the unbiased sampling assumption by comparing V computed from biased vs. unbiased source data
2. Evaluate the impact of different cost matrix constructions on both fairness outcomes and computational efficiency
3. Validate the numerical stability of the KL projection implementation across different parameter regimes and data distributions