---
ver: rpa2
title: Using Artificial Populations to Study Psychological Phenomena in Neural Models
arxiv_id: '2308.08032'
source_url: https://arxiv.org/abs/2308.08032
tags:
- population
- typicality
- language
- arxiv
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the methodological challenges in studying cognitive
  behavior in language models, particularly the limitations of single-model experiments.
  It introduces PopulationLM, a novel tool that leverages Monte Carlo dropout to create
  efficient populations of neural models for robust statistical analysis.
---

# Using Artificial Populations to Study Psychological Phenomena in Neural Models

## Quick Facts
- arXiv ID: 2308.08032
- Source URL: https://arxiv.org/abs/2308.08032
- Reference count: 7
- Single-model experiments overestimate cognitive behaviors in language models

## Executive Summary
This work introduces PopulationLM, a methodological framework for studying cognitive phenomena in language models using artificial populations created via Monte Carlo dropout. The approach addresses the reproducibility crisis in single-model experiments by generating diverse model populations that approximate deep Gaussian processes. Applied to typicality effects and structural priming, the method reveals that while typicality effects are consistently present (10-25% variance explained), structural priming is largely absent across model families. The population approach demonstrates that single-model studies tend to overestimate behavioral phenomena, highlighting the importance of population-based methodology in computational psychology research.

## Method Summary
PopulationLM creates artificial populations of language models using Monte Carlo dropout with 0.1 rate to generate 50-member stratified populations per model. The method treats each dropout-sampled model as an independent "subject" for statistical analysis, enabling standard psychological testing methods like Pearson correlation and Wilcoxon signed-rank tests. The framework was applied to two behavioral studies: typicality effects using Rosch (1975) categories and structural priming using Sinclair et al. (2022) stimuli. Population uncertainty was measured as normalized standard deviation of predictions, and Kolmogorov-Smirnov tests validated population diversity against base models.

## Key Results
- Typicality effects present in all model families with probability-typicality correlations explaining 10-25% of variance in well-represented categories
- Structural priming effects largely absent, with only marginal evidence in RoBERTa models (p=0.015)
- Population uncertainty scales with prediction mean and correlates with typicality rankings
- Single-model experiments overestimate cognitive behaviors compared to population-based analysis

## Why This Works (Mechanism)

### Mechanism 1
MC dropout populations approximate deep Gaussian process distributions, providing robust uncertainty estimates. Each dropout mask samples different network configurations, and the ensemble approximates Bayesian posterior distribution without multiple trainings. This breaks down if models are under-parameterized or dropout masks aren't independent.

### Mechanism 2
Treating dropout samples as independent subjects enables direct application of standard statistical tests. The population predictions act as repeated measures from distinct subjects, allowing within-group paired-sample tests that account for population variability. Statistical power degrades if samples are too correlated.

### Mechanism 3
Population uncertainty correlates with typicality through decreased agreement on atypical items. As items become less typical, standard deviation of predicted probabilities increases. This relationship breaks down if uncertainty doesn't scale meaningfully with mean predictions.

## Foundational Learning

- **Monte Carlo dropout as Bayesian approximation**: Provides computationally cheap model populations without training multiple models. Quick check: How does MC dropout approximate Bayesian posterior distribution in neural networks?

- **Statistical significance in population studies**: Ensures detected phenomena aren't single-model artifacts. Quick check: Why report both p-values and effect sizes when comparing model populations?

- **Normalization of uncertainty measures**: Enables meaningful comparison across different prediction magnitudes. Quick check: What's the purpose of mean-normalizing standard deviation of model predictions?

## Architecture Onboarding

- **Component map**: PyTorch model → Dropout layers → PopulationLM wrapper → Inference loop (multiple dropout samples) → Aggregation/statistics module
- **Critical path**: Model loading → Dropout rate configuration → Population sampling → Inference on stimuli → Statistical analysis
- **Design tradeoffs**: Higher dropout rate → more variation but signal loss; larger population → more robust stats but higher compute cost
- **Failure signatures**: Low KS test effect size → population too similar to base; unstable correlations → insufficient population size or too high dropout rate
- **First 3 experiments**:
  1. Run single model inference with varying dropout rates (0.1, 0.5, 0.8) and observe effect on prediction stability
  2. Generate 10-member population and compare KS test results against base model to validate diversity
  3. Apply typicality experiment to 50-member population and compute within-category Pearson correlations to verify typicality effects

## Open Questions the Paper Calls Out

### Open Question 1
How does the presence of typicality effects relate to underlying training data distribution and category representation? The paper shows correlation between training data frequency and typicality effects but doesn't establish causation or identify the mechanism.

### Open Question 2
What's the nature of the relationship between model certainty and typicality in causal versus masked language models? The paper finds opposite patterns but doesn't explain why the fundamental relationship would reverse between model types.

### Open Question 3
What's the practical limit of dropout rates before behavioral signals become indistinguishable from noise? The paper empirically determines 0.1 as optimal but doesn't systematically investigate the relationship between dropout rate, signal preservation, and population size.

### Open Question 4
Can population-based methods be effectively applied to closed-source models through test-time augmentation? The paper mentions this as future work but doesn't experimentally test this approach.

## Limitations
- Population independence assumption may artificially inflate statistical power if dropout samples aren't sufficiently diverse
- Gaussian process approximation validity may vary across different model architectures and task domains
- Several novel behavioral findings lack extensive prior work for proper contextualization

## Confidence
- **High confidence**: MC dropout methodology for population creation is well-established in uncertainty estimation literature
- **Medium confidence**: Typicality effects findings are robust across model families, but structural priming absence needs further validation
- **Low confidence**: Population uncertainty scaling with prediction mean and its relationship to typicality is a novel claim requiring additional validation

## Next Checks
1. **Population size sensitivity analysis**: Systematically vary population sizes (10, 25, 50, 100 members) and measure how KS test statistics and behavioral effect sizes change
2. **Cross-paradigm behavioral validation**: Apply population methodology to additional cognitive phenomena beyond typicality and structural priming
3. **Correlation structure analysis**: Measure pairwise correlations between dropout samples within populations to quantify effective independence of "subjects"