---
ver: rpa2
title: Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression
  by Adversarial Training
arxiv_id: '2307.04042'
source_url: https://arxiv.org/abs/2307.04042
tags:
- neural
- adversarial
- deep
- risk
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sup-norm convergence of deep neural network
  estimators for nonparametric regression. The key challenge is that ordinary adversarial
  training does not yield consistent estimators due to bias from input perturbations.
---

# Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training

## Quick Facts
- arXiv ID: 2307.04042
- Source URL: https://arxiv.org/abs/2307.04042
- Reference count: 18
- Primary result: Novel adversarial training with preprocessing achieves minimax optimal sup-norm convergence rates for nonparametric regression

## Executive Summary
This paper addresses the fundamental challenge of achieving sup-norm convergence in nonparametric regression using deep neural networks trained with adversarial methods. The key insight is that standard adversarial training introduces bias when applied to regression tasks, preventing consistent estimation in the sup-norm sense. The authors develop a novel framework that combines preprocessing of output variables with adversarial training, demonstrating that this approach can achieve the minimax optimal convergence rate under appropriate conditions on function smoothness and network architecture.

## Method Summary
The proposed method involves three key steps: (1) preprocessing the output variable using techniques like k-nearest neighbors to create surrogate outputs that account for input perturbations, (2) defining an adversarial risk that minimizes the maximum squared error over perturbation neighborhoods, and (3) training a deep neural network to minimize this adversarial risk. The approach corrects the bias introduced by standard adversarial training by ensuring the model learns to predict the correct output even when inputs are perturbed, enabling consistent sup-norm estimation.

## Key Results
- The proposed estimator achieves the minimax optimal convergence rate of n^(-2β/(2β+d)) in the sup-norm sense
- Theoretical analysis proves consistency and optimality for functions in Hölder and Besov spaces
- Experiments validate the theoretical findings on synthetic regression problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training alone introduces bias that prevents consistency in sup-norm estimation
- Mechanism: Input perturbations during adversarial training cause the model to fit shifted outputs rather than the true function, creating a systematic bias
- Core assumption: The regression output variable cannot accommodate perturbations in the same way as classification outputs
- Evidence anchors:
  - [abstract] "ordinary adversarial training makes neural estimators inconsistent"
  - [section] "We show the inconsistency of the ordinary adversarial training without preprocessing"
- Break condition: When the true function is constant or when perturbations are negligible relative to the function's variation

### Mechanism 2
- Claim: Preprocessing corrects the bias by aligning perturbed inputs with appropriate output adjustments
- Mechanism: Preprocessing creates a surrogate output variable that corresponds to each perturbed input, allowing the adversarial training to minimize the correct loss
- Core assumption: The preprocessing error converges to zero at a rate faster than the estimation error
- Evidence anchors:
  - [abstract] "novel adversarial training scheme with preprocessing to correct the bias"
  - [section] "We develop a random map bY: [0,1]^d → R for surrogate outputs, which referred to a preprocessed output"
- Break condition: When the preprocessing method introduces more error than it corrects, or when the preprocessing rate is slower than n^(-2β/(2β+d))

### Mechanism 3
- Claim: The adversarial loss combined with preprocessing enables uniform convergence by controlling the sup-norm through volume arguments
- Mechanism: The adversarial norm bounds the sup-norm via a relationship involving the volume of the perturbation neighborhood, enabling uniform convergence analysis
- Core assumption: The input distribution has a uniformly lower bounded density on the domain
- Evidence anchors:
  - [section] "We develop an empirical process technique for the evaluation of preprocessing"
  - [section] "the adversarial norm bounds the sup-norm via a relationship involving the volume of the perturbation neighborhood"
- Break condition: When the input density has regions of arbitrarily low probability, breaking the volume-based bound

## Foundational Learning

- Concept: Hölder smoothness and Besov spaces
  - Why needed here: The paper assumes the true function belongs to these function classes to establish convergence rates and minimax optimality
  - Quick check question: What is the difference between Hölder and Besov smoothness, and why does the paper consider both?

- Concept: Empirical process theory and covering numbers
- Concept: Uniform convergence and sup-norm risk
  - Why needed here: The paper needs to control the maximum error over the entire domain, not just average error
  - Quick check question: How does sup-norm convergence differ from L2 convergence in terms of what it guarantees about estimator performance?

- Concept: Adversarial training and robust optimization
  - Why needed here: The core method relies on adversarial training, but with modifications to handle regression
  - Quick check question: What is the key difference between adversarial training for classification versus regression?

## Architecture Onboarding

- Component map: Data generation -> Preprocessing (k-NN, kernel, or Bayesian) -> Adversarial training loop with perturbation budget h -> Deep neural network with ReLU activations -> Risk evaluation using adversarial norm and sup-norm bounds

- Critical path:
  1. Generate training data from regression model
  2. Apply preprocessing to create surrogate outputs
  3. Train neural network using adversarial loss with perturbations
  4. Evaluate sup-norm risk using theoretical bounds

- Design tradeoffs:
  - Preprocessing method vs. convergence rate: simpler methods may be faster but converge slower
  - Perturbation budget h vs. bias-variance tradeoff: larger h may reduce bias but increase variance
  - Network architecture (depth/width) vs. approximation error: deeper/wider networks reduce approximation error but increase complexity

- Failure signatures:
  - Estimator shows no improvement with more data: likely preprocessing error dominates
  - Estimator overfits to training data: adversarial training may be too aggressive or preprocessing insufficient
  - Convergence rate slower than theoretical bounds: may indicate suboptimal network architecture or preprocessing choice

- First 3 experiments:
  1. Verify consistency: Run with small n and simple function, check if sup-norm error decreases
  2. Test preprocessing impact: Compare with/without preprocessing on same data and architecture
  3. Validate minimax optimality: Compare convergence rate against theoretical lower bound for different β values

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the convergence rate of adversarial training estimators be improved for functions with non-smooth singularities, such as discontinuities or cusp points?
  - Basis in paper: [inferred] The paper explicitly states that the adversarial training estimator converges to smooth true functions with minimax optimal rate, but acknowledges that there are "significant obstacles to the sup-norm convergence of estimators for the non-smooth functions."
  - Why unresolved: The paper does not provide any analysis or discussion of the convergence rate for non-smooth functions. The minimax optimality is only proven for smooth functions in the Hölder or Besov space.
  - What evidence would resolve it: Theoretical analysis of the convergence rate for adversarial training estimators when the true function has non-smooth singularities. Empirical experiments demonstrating the convergence rate for different types of singularities.

- **Open Question 2**: How does the choice of preprocessing method impact the convergence rate of adversarial training estimators?
  - Basis in paper: [explicit] The paper discusses several examples of preprocessing methods, such as k-nearest neighbor and Bayesian methods, and provides theoretical analysis of their convergence rates. However, it does not provide a comprehensive comparison of different preprocessing methods.
  - Why unresolved: The paper does not provide a systematic comparison of the convergence rates achieved by different preprocessing methods. The choice of preprocessing method is left as a design choice for the user.
  - What evidence would resolve it: Empirical experiments comparing the convergence rates of adversarial training estimators with different preprocessing methods on a range of functions and noise levels. Theoretical analysis of the optimal preprocessing method for different function classes.

- **Open Question 3**: Can the adversarial training framework be extended to other types of neural networks, such as convolutional neural networks or recurrent neural networks?
  - Basis in paper: [explicit] The paper focuses on fully-connected neural networks with ReLU activation, but mentions that the results can be extended to other architectures. However, it does not provide any specific analysis or experiments for other architectures.
  - Why unresolved: The paper does not provide any theoretical or empirical analysis of the performance of adversarial training for other types of neural networks. The convergence rate and robustness properties may differ for different architectures.
  - What evidence would resolve it: Theoretical analysis of the convergence rate and robustness properties of adversarial training for different types of neural networks. Empirical experiments comparing the performance of adversarial training with different architectures on a range of tasks.

## Limitations

- The preprocessing error rate requirement (faster than n^(-2β/(2β+d))) is not fully characterized for all possible preprocessing methods
- The relationship between perturbation budget h and function smoothness β requires careful calibration that may be difficult in practice
- Theoretical guarantees are limited to smooth functions in Hölder and Besov spaces, with no analysis for non-smooth functions

## Confidence

- **High confidence** in theoretical framework and main theorem statements, following established empirical process techniques
- **Medium confidence** in practical implementation details, particularly around preprocessing method selection and optimization hyperparameters
- **Medium confidence** in experimental validation, which is limited to small-scale simulations

## Next Checks

1. **Preprocessing sensitivity analysis**: Systematically compare different preprocessing methods (k-NN, kernel, Bayesian) to identify which achieve the required error decay rate across varying sample sizes and function smoothness levels.

2. **Perturbation budget calibration**: Conduct experiments to determine optimal h values as a function of β and n, testing whether the theoretical relationship h ∝ n^(-1/(2β+d)) holds in practice.

3. **Scalability verification**: Evaluate the method on higher-dimensional problems (d > 2) and with larger network architectures to assess whether the theoretical convergence guarantees extend beyond the tested regime.