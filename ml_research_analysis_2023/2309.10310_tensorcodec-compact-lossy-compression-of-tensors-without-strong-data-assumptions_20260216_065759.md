---
ver: rpa2
title: 'TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions'
arxiv_id: '2309.10310'
source_url: https://arxiv.org/abs/2309.10310
tags:
- tensor
- codec
- mode
- ensor
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TensorCodec, a lossy tensor compression algorithm
  designed for general tensors without requiring strong assumptions about data properties.
  The key idea is to enhance Tensor-Train Decomposition (TTD) with a neural network
  (Neural Tensor-Train Decomposition or NTTD), allowing it to model high-rank tensors
  while maintaining a small number of parameters.
---

# TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions

## Quick Facts
- arXiv ID: 2309.10310
- Source URL: https://arxiv.org/abs/2309.10310
- Reference count: 35
- One-line primary result: TensorCodec achieves up to 7.38x more compact compression with similar reconstruction error and up to 3.33x more accurate reconstruction given the same compressed size budget compared to state-of-the-art methods.

## Executive Summary
This paper introduces TensorCodec, a lossy tensor compression algorithm designed for general tensors without requiring strong assumptions about data properties such as order, sparsity, rank, or smoothness. The key innovation is the Neural Tensor-Train Decomposition (NTTD), which integrates a recurrent neural network (LSTM) into the Tensor-Train Decomposition to enhance its expressive power. By conditioning TT cores on preceding mode indices, NTTD can model high-rank tensors with a small number of parameters. The method also incorporates folding to reduce the number of parameters and reordering of mode indices to improve approximation accuracy.

## Method Summary
TensorCodec uses three key ideas: Neural Tensor-Train Decomposition (NTTD) with LSTM to enhance expressiveness, folding input tensors into higher-order tensors to reduce parameters, and reordering mode indices to improve approximation accuracy. The model is trained using mini-batch gradient descent to minimize reconstruction error. The compressed representation includes NTTD parameters and a reordering map. Evaluation is performed on 8 real-world tensor datasets, comparing TensorCodec against state-of-the-art baselines in terms of compressed size and fitness (reconstruction accuracy).

## Key Results
- TensorCodec achieves up to 7.38x more compact compression with similar reconstruction error compared to state-of-the-art methods.
- Up to 3.33x more accurate reconstruction is achieved given the same compressed size budget.
- The algorithm scales well, with compression time linear in the number of tensor entries and reconstruction time logarithmic in the largest mode size.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NTTD improves expressiveness by making TT cores dependent on all preceding mode indices via LSTM.
- Mechanism: Each TT core is generated by a neural network conditioned on the sequence of mode indices, allowing non-linear, contextual transformations.
- Core assumption: Target tensor entries can be approximated more accurately if each TT core varies with context, not just the current mode index.
- Evidence anchors:
  - [abstract]: "Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power"
  - [section]: "each TT core varies depending not only on the current mode index but also on all preceding mode indices"
- Break condition: If the added non-linearity and context dependency do not improve reconstruction error, or if the model overfits to noise.

### Mechanism 2
- Claim: Folding reduces the number of parameters needed by converting a high-order tensor into a higher-order one with smaller mode lengths.
- Mechanism: The tensor is reshaped into a higher-order format (TT-tensor), which decreases the number of entries per mode and thus the total parameter count of NTTD.
- Core assumption: The number of parameters in NTTD scales with the sum of mode lengths; reducing mode lengths reduces parameters.
- Evidence anchors:
  - [abstract]: "Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD"
  - [section]: "We fold the input tensor into a higher-order tensor with smaller mode lengths... reducing the number of parameters and thus the size of the compressed output"
- Break condition: If the folding changes the tensor's structure in a way that degrades reconstruction quality, or if mode lengths cannot be reduced without increasing the order too much.

### Mechanism 3
- Claim: Reordering mode indices clusters similar entries together, enabling more efficient sharing of TT cores.
- Mechanism: Mode indices are reordered so that adjacent slices are similar; this causes similar entries to share more mode indices in the folded tensor, allowing NTTD to use shared TT cores more effectively.
- Core assumption: If two entries share k mode indices, they will use the same first k TT cores; thus, grouping similar entries increases sharing.
- Evidence anchors:
  - [abstract]: "the mode indices of the input tensor are reordered to reveal patterns that can be exploited by NTTD for improved approximation"
  - [section]: "we reorder the mode indices in the input tensor before folding so that entries with similar values are placed close to each other"
- Break condition: If reordering does not lead to more similar adjacent slices, or if the cost of reordering outweighs the gains.

## Foundational Learning

- Concept: Tensor-Train Decomposition (TTD)
  - Why needed here: Provides the base compression format that NTTD extends; understanding TTD is essential to grasp how NTTD generalizes it.
  - Quick check question: In TTD, how is each tensor entry approximated in terms of TT cores?

- Concept: Recurrent Neural Networks (RNNs), specifically LSTMs
  - Why needed here: NTTD uses an LSTM to generate TT cores conditioned on mode indices; knowing how LSTMs process sequences is critical.
  - Quick check question: What is the role of the hidden state in an LSTM when processing a sequence of embeddings?

- Concept: Mode-index reordering and the Metric TSP approximation
  - Why needed here: Reordering is done by solving a TSP-like problem on slice similarities; understanding this optimization is key to the algorithm.
  - Quick check question: Why does minimizing the sum of Frobenius norms between adjacent slices correspond to solving a TSP?

## Architecture Onboarding

- Component map:
  Input tensor → (optional reorder) → fold → NTTD model (embedding + LSTM + linear layers) → compressed output (parameters + reordering map)

- Critical path:
  1. Reorder mode indices (if enabled)
  2. Fold tensor into higher-order form
  3. Initialize NTTD model parameters
  4. Train NTTD via mini-batch gradient descent on reconstruction loss
  5. Output compressed representation (parameters + reordering functions)

- Design tradeoffs:
  - Expressive power vs. parameter count: NTTD increases expressiveness but adds parameters; folding mitigates this.
  - Reordering accuracy vs. computation: Better reordering can improve compression but costs extra optimization.
  - Tensor order vs. mode lengths: Folding increases order but reduces lengths; must balance to avoid excessive order.

- Failure signatures:
  - High reconstruction error: Likely due to poor reordering, insufficient model capacity, or bad folding choice.
  - Slow compression: Caused by large tensor size, high-order folding, or many reordering iterations.
  - Memory blowup: Folding increases order; if not controlled, may exceed memory limits.

- First 3 experiments:
  1. Compress a small synthetic tensor with known low-rank structure using NTTD without reordering or folding; measure reconstruction error.
  2. Apply folding only to the same tensor; compare parameter count and error.
  3. Add reordering; evaluate impact on error and compression time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of different neural network architectures (e.g., GRU, Scaled Dot-product Attention) on the performance of NTTD compared to LSTM?
- Basis in paper: [explicit] The paper mentions that alternatives to LSTM, such as GRU and Scaled Dot-product Attention, can be used instead in NTTD.
- Why unresolved: The paper only examines the performance of TensorCodec equipped with LSTM in Section VIII of the supplementary materials. A comprehensive comparison with other architectures is not provided.
- What evidence would resolve it: Experimental results comparing the performance of TensorCodec using different neural network architectures (LSTM, GRU, Scaled Dot-product Attention) on the same datasets and with the same hyperparameters.

### Open Question 2
- Question: How does the performance of TensorCodec change when applied to tensors with different properties, such as higher order or different density and smoothness characteristics?
- Basis in paper: [inferred] The paper evaluates TensorCodec on 8 real-world datasets with varying properties, but does not systematically explore the impact of these properties on performance.
- Why unresolved: The paper does not provide a systematic analysis of how different tensor properties affect the performance of TensorCodec.
- What evidence would resolve it: Experimental results showing the performance of TensorCodec on tensors with varying properties, such as different orders, densities, and smoothness levels.

### Open Question 3
- Question: What is the theoretical upper bound on the approximation accuracy of TensorCodec, and how does it compare to other tensor compression methods?
- Basis in paper: [inferred] The paper provides empirical results showing the performance of TensorCodec, but does not provide a theoretical analysis of its approximation accuracy.
- Why unresolved: The paper does not derive a theoretical upper bound on the approximation accuracy of TensorCodec or compare it to other methods.
- What evidence would resolve it: A theoretical analysis of the approximation accuracy of TensorCodec, including a derivation of an upper bound and a comparison to other tensor compression methods.

## Limitations
- The three key mechanisms (NTTD expressiveness, folding, and reordering) lack external validation from the broader ML or tensor compression literature.
- Claims about folding and reordering reducing parameters and improving accuracy are theoretical; empirical evidence is only provided within the paper's own experiments.
- Exact hyperparameters and preprocessing steps are not specified, which are critical for faithful reproduction.

## Confidence
- **High**: The overall approach of combining NTTD with folding and reordering is novel and logically consistent. The experimental setup and comparison methodology are clearly described.
- **Medium**: The claim that TensorCodec outperforms state-of-the-art methods is supported by experiments, but the lack of hyperparameter details and preprocessing specifics introduces uncertainty.
- **Low**: The effectiveness of the reordering mechanism (Metric TSP approximation) is theoretically sound but not empirically validated outside the paper's context.

## Next Checks
1. **Parameter and preprocessing audit**: Reconstruct the exact hyperparameter settings and preprocessing steps for each dataset by contacting authors or reverse-engineering from the code.
2. **Ablation study**: Run TensorCodec with and without each of the three key mechanisms (NTTD, folding, reordering) on a small synthetic dataset to isolate their individual contributions.
3. **Robustness test**: Evaluate TensorCodec on additional real-world tensors not used in the paper to verify generalization across diverse data properties.