---
ver: rpa2
title: Exact identification of nonlinear dynamical systems by Trimmed Lasso
arxiv_id: '2308.01891'
source_url: https://arxiv.org/abs/2308.01891
tags:
- trim
- data
- sparse
- nonlinear
- lasso
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes the Trimmed Lasso for robust identification
  of nonlinear dynamical systems, addressing the limitations of existing sparse regression
  methods in handling noisy, finite data and multicollinearity. The method employs
  a non-convex penalty that enforces exact sparsity, allowing for efficient computation
  and intuitive hyperparameter tuning.
---

# Exact identification of nonlinear dynamical systems by Trimmed Lasso

## Quick Facts
- arXiv ID: 2308.01891
- Source URL: https://arxiv.org/abs/2308.01891
- Authors: 
- Reference count: 40
- One-line primary result: TRIM outperforms STLS, E-SINDy, and IRL1 in exact support recovery and coefficient error for nonlinear systems under noise and multicollinearity.

## Executive Summary
This paper proposes the Trimmed Lasso (TRIM) for robust identification of nonlinear dynamical systems from noisy, finite data. TRIM employs a non-convex penalty that enforces exact sparsity, addressing limitations of existing sparse regression methods in handling multicollinearity and providing exact coefficient recovery. The method bridges the gap between ℓ0 and ℓ1 formulations, allowing efficient computation and intuitive hyperparameter tuning. The authors demonstrate TRIM's superior performance compared to STLS, E-SINDy, and IRL1 across various challenging nonlinear systems including Lorenz 63, Bouc Wen oscillator, and time delay systems.

## Method Summary
The TRIM method uses a Trimmed Lasso penalty that enforces exact sparsity K while penalizing the solution, implemented via an alternating minimization scheme that approximates the non-convex problem using convex solvers. The algorithm constructs a library matrix from candidate functions, solves for sparse coefficients using the Trimmed Lasso, and tunes hyperparameters via the L-curve criterion. Uncertainty quantification is performed post-model-selection using wild bootstraps, offering computational advantages over methods requiring ensembling before selection. The approach is validated against STLS, E-SINDy, and IRL1 on benchmark nonlinear systems with varying noise levels and multicollinearity structures.

## Key Results
- TRIM achieves exact support recovery under more severe noise, finite data, and multicollinearity compared to E-SINDy and other sparse regression methods
- Uncertainty quantification for TRIM is computationally cheaper than E-SINDy because it is performed after model selection
- TRIM demonstrates superior performance in terms of exact support recovery rate and coefficient error across Lorenz 63, Bouc Wen oscillator, and self-excited vibration systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Trimmed Lasso enforces exact sparsity (K) while penalizing the solution, bridging the gap between ℓ0 and ℓ1 formulations.
- Mechanism: The Trimmed Lasso's non-convex penalty T(ξj, k) penalizes the k+1 smallest coefficients in magnitude, allowing exact control over sparsity K. When λ reaches a threshold λ̂, the smallest P-k entries are forced to zero, making the problem equivalent to the best subset selection.
- Core assumption: The optimization of the Trimmed Lasso can be efficiently approximated using convex solvers via alternating minimization.
- Evidence anchors:
  - [abstract]: "The method employs a non-convex penalty that enforces exact sparsity, allowing for efficient computation and intuitive hyperparameter tuning."
  - [section]: "An ideal penalty would be able to be penalized by λ and enforce sparsity K simultaneously... One such penalty, the Trimmed Lasso, has a non-convex penalty that satisfies both control of the sparsity and can be penalized."
  - [corpus]: Weak – corpus does not discuss Trimmed Lasso specifically.
- Break condition: If the convex approximation fails to converge or if the sparsity parameter k is mis-specified, the exact recovery property may be lost.

### Mechanism 2
- Claim: TRIM achieves exact support recovery under more severe noise, finite data, and multicollinearity compared to E-SINDy and other sparse regression methods.
- Mechanism: TRIM uses the Trimmed Lasso penalty, which has oracle properties under less strict conditions than the Lasso. The sparsity parameter k directly controls the number of non-zero coefficients, avoiding the bias against small coefficients that occurs in ℓ1-based methods like STLS and IRL1.
- Core assumption: The L-curve criterion for hyperparameter tuning in TRIM reliably identifies the correct sparsity level.
- Evidence anchors:
  - [abstract]: "The authors demonstrate that TRIM outperforms other methods... in terms of exact support recovery and coefficient error across various challenging nonlinear systems."
  - [section]: "TRIM is postulated to have lenient conditions to satisfy its oracle properties, and thus better performance."
  - [corpus]: Weak – corpus neighbors do not discuss exact support recovery or oracle properties.
- Break condition: If the L-curve does not exhibit a clear corner point, or if the forward step selection tolerance is too loose, TRIM may select an incorrect sparsity level.

### Mechanism 3
- Claim: Uncertainty quantification for TRIM is computationally cheaper than E-SINDy because it is performed after model selection.
- Mechanism: TRIM uses wild bootstraps for uncertainty quantification, but only after a single set of hyperparameters is chosen via model selection. In contrast, E-SINDy requires bootstrapping and ensembling for each thresholding parameter before model selection.
- Core assumption: Wild bootstraps preserve dependence structure and heteroscedasticity in the data, providing robust confidence intervals.
- Evidence anchors:
  - [abstract]: "uncertainty quantification comes after model selection for TRIM, meaning that resampling techniques are performed for only one set of hyperparameters. This is a computational advantage over E-SINDy."
  - [section]: "These bootstrap samples are then used to estimate the sampling variability of an estimator and construct robust confidence intervals and their forecasts."
  - [corpus]: Weak – corpus neighbors do not discuss uncertainty quantification methods.
- Break condition: If the wild bootstrap assumptions (e.g., preservation of dependence structure) are violated, the confidence intervals may be inaccurate.

## Foundational Learning

- Concept: Sparse regression and ℓ1/ℓ0 penalties
  - Why needed here: Understanding the difference between convex (ℓ1) and non-convex (ℓ0, Trimmed Lasso) penalties is crucial for grasping why TRIM can achieve exact sparsity.
  - Quick check question: What is the main advantage of using a non-convex penalty like the Trimmed Lasso over the convex ℓ1 penalty in sparse regression?

- Concept: Oracle properties and irrepresentable condition
  - Why needed here: Oracle properties determine whether a sparse regression method can recover the true support of the model. The irrepresentable condition is a key requirement for the Lasso to have oracle properties.
  - Quick check question: Why is the irrepresentable condition hard to meet in practice, and how does the Trimmed Lasso address this issue?

- Concept: Model selection criteria (AIC, BIC, RICc, L-curve)
  - Why needed here: Choosing the right model selection criterion is essential for hyperparameter tuning in sparse regression methods like TRIM.
  - Quick check question: What is the main difference between AIC and BIC, and why is RICc preferred for TRIM in this study?

## Architecture Onboarding

- Component map: Library matrix Θ -> Trimmed Lasso penalty -> Alternating minimization -> L-curve criterion -> Wild bootstrap uncertainty quantification

- Critical path:
  1. Construct library matrix Θ from candidate functions
  2. Solve the Trimmed Lasso problem using alternating minimization to obtain the sparse coefficient vector
  3. Tune the sparsity parameter k using the L-curve criterion or RICc
  4. Perform uncertainty quantification using wild bootstraps on the identified model

- Design tradeoffs:
  - Exact sparsity vs. computational efficiency: The Trimmed Lasso enforces exact sparsity but requires non-convex optimization, which is approximated using convex solvers
  - Robustness vs. bias: TRIM is more robust to noise and multicollinearity but may have higher bias for large coefficients compared to ℓ1-based methods

- Failure signatures:
  - If TRIM fails to recover the true support, it may be due to a mis-specified sparsity parameter k or a poorly constructed library matrix
  - If uncertainty quantification is inaccurate, it may be due to violations of the wild bootstrap assumptions or a small number of bootstrap iterations

- First 3 experiments:
  1. Simulate the Lorenz 63 system with varying noise levels and compare TRIM's exact support recovery rate to STLS, E-SINDy, and IRL1
  2. Construct a library matrix with high multicollinearity and evaluate TRIM's performance on the Bouc Wen oscillator benchmark
  3. Apply TRIM to the self-excited vibration system and propagate coefficient uncertainties to the stability lobe diagram using wild bootstraps

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. The authors focus on demonstrating the effectiveness of TRIM through empirical comparisons and theoretical discussions of its oracle properties.

## Limitations
- The theoretical guarantees of the Trimmed Lasso's oracle properties under noisy, finite data conditions require further validation
- The computational complexity of the alternating minimization scheme for TRIM, particularly for large-scale problems, is not explicitly analyzed
- The robustness of the L-curve criterion for hyperparameter tuning under varying noise structures needs systematic evaluation

## Confidence
- **High Confidence**: The empirical performance comparisons between TRIM and other methods (STLS, E-SINDy) on benchmark systems. The implementation of the TRIM algorithm using alternating minimization is well-defined.
- **Medium Confidence**: The theoretical claims about TRIM's oracle properties and its ability to handle multicollinearity better than ℓ1-based methods. The computational advantages of post-model-selection uncertainty quantification.
- **Low Confidence**: The generalizability of TRIM to highly complex, high-dimensional systems with unknown nonlinear structures. The sensitivity of TRIM to the choice of library functions and combinatorial terms.

## Next Checks
1. **Robustness Analysis**: Systematically vary noise levels, data lengths, and multicollinearity structures in the Lorenz 63 system to quantify TRIM's exact support recovery rate and compare it against theoretical predictions
2. **Computational Scaling**: Evaluate the runtime and memory requirements of TRIM's alternating minimization scheme for increasing numbers of candidate functions and system dimensions. Compare against ℓ1-based methods to verify computational advantages
3. **Uncertainty Quantification Validation**: Perform extensive simulations to assess the coverage probability of TRIM's wild bootstrap confidence intervals under different noise structures and dependence patterns. Compare against analytical error bounds when available