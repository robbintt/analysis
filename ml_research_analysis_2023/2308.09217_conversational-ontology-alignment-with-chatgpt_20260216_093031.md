---
ver: rpa2
title: Conversational Ontology Alignment with ChatGPT
arxiv_id: '2308.09217'
source_url: https://arxiv.org/abs/2308.09217
tags:
- ontology
- chatgpt
- prompt
- alignment
- ontologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using ChatGPT for ontology alignment in
  a zero-shot manner. The authors evaluate different prompt strategies on the OAEI
  2022 conference track ontologies.
---

# Conversational Ontology Alignment with ChatGPT

## Quick Facts
- arXiv ID: 2308.09217
- Source URL: https://arxiv.org/abs/2308.09217
- Reference count: 11
- Primary result: ChatGPT achieves up to 92% recall but only 37% precision in zero-shot ontology alignment

## Executive Summary
This paper investigates using ChatGPT (v4.0) for ontology alignment tasks in a zero-shot manner, evaluating seven different prompt strategies on conference track ontologies from OAEI 2022. The authors find that while ChatGPT can achieve high recall (up to 92%) in identifying potential matches, precision remains low (as low as 37%), resulting in an average F1-score of 0.52. The study identifies key challenges including ChatGPT's context length limitations, handling of inverse functional properties, and matching with subclasses. Despite these limitations, the authors conclude that ChatGPT shows promise for ontology alignment, particularly in identifying potential matches that can be filtered by domain experts.

## Method Summary
The authors evaluate ChatGPT-4's performance on zero-shot ontology alignment using seven different prompt designs formatted as structured triples (Predicate(Subject, Object)). They test the approach on seven conference ontologies from OAEI 2022, generating 21 ontology pairs for evaluation. The prompts are compared against reference alignments (ra1-M3) using precision, recall, and F1-score metrics. Due to ChatGPT's context length limitations, prompts are divided into smaller parts when necessary. The study focuses on identifying which prompt strategies work best and analyzing the factors contributing to low precision in the generated alignments.

## Key Results
- ChatGPT achieves high recall (up to 92%) in identifying potential ontology matches
- Precision remains low (37%) despite high recall, resulting in F1-score of 0.52
- Structured predicate-subject-object formatting outperforms natural language conversion
- Context length limitations require prompt splitting, potentially affecting performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can identify potential ontology matches with high recall by leveraging its knowledge of semantic relationships from pre-training.
- Mechanism: The model processes triples in structured format (Predicate(Subject, Object)) and uses its understanding of natural language semantics to infer correspondences between ontology elements.
- Core assumption: ChatGPT's pre-training corpus contains sufficient semantic information to recognize ontological relationships.
- Evidence anchors:
  - [abstract] "ChatGPT can achieve high recall (up to 92%)"
  - [section] "ChatGPT (v4.0) has improved contextual understanding and better adaptability to long inputs"
  - [corpus] "Exploring Large Language Models for Ontology Alignment" - similar approach found comparable results
- Break condition: When ontologies contain domain-specific terminology not present in ChatGPT's training data, recall will drop significantly.

### Mechanism 2
- Claim: Structured prompt formatting improves ChatGPT's ability to process ontology triples compared to natural language conversion.
- Mechanism: The Predicate(Subject, Object) format aligns with ChatGPT's understanding of function-like relationships, making it easier to parse and match concepts.
- Core assumption: The model's training included sufficient examples of function-style notation to develop reliable parsing patterns.
- Evidence anchors:
  - [section] "we choose to adopt the formatted text approach for our prompts, which aligns well with suggestions from OpenAI"
  - [section] "An original triple such as 'track subclassOf conference part' can be represented as 'Is-a (track, conference part)'"
  - [corpus] "Reward-free Policy Imitation Learning for Conversational Search" - shows structured formatting improves LLM performance
- Break condition: When ontology relationships are too complex for simple predicate-subject-object representation, the formatting advantage disappears.

### Mechanism 3
- Claim: Zero-shot prompting allows ChatGPT to perform ontology alignment without requiring task-specific fine-tuning.
- Mechanism: The model applies general semantic understanding and reasoning capabilities acquired during pre-training to the alignment task based solely on the prompt instructions.
- Core assumption: The pre-training data covered sufficient ontology-like structures and semantic relationships for the model to generalize.
- Evidence anchors:
  - [abstract] "we conduct a comparative analysis of ChatGPT's performance in ontology alignment when prompted with different strategies"
  - [section] "For using some LLMs in downstream tasks, fine-tuning would be helpful since it would make the LLM adapt its knowledge (from the pre-training process) to the specific task"
  - [corpus] "Large Language Models as Oracles for Ontology Alignment" - confirms zero-shot approach is viable
- Break condition: When ontologies require specialized domain knowledge not present in pre-training, zero-shot performance becomes inadequate.

## Foundational Learning

- Concept: Ontology alignment fundamentals
  - Why needed here: Understanding the task of finding semantic correspondences between ontologies is essential for interpreting results and limitations
  - Quick check question: What is the difference between class alignment and property alignment in ontology matching?

- Concept: Prompt engineering principles
  - Why needed here: Different prompt strategies significantly impact ChatGPT's performance, as shown by the varying F1-scores across prompts
  - Quick check question: How does zero-shot prompting differ from few-shot prompting in terms of information provided to the model?

- Concept: Evaluation metrics for ontology alignment
  - Why needed here: The paper uses precision, recall, and F1-score to evaluate performance, requiring understanding of what these metrics measure
  - Quick check question: If a system has high recall but low precision, what does this indicate about its matching behavior?

## Architecture Onboarding

- Component map: ChatGPT (LLM) as the core matching engine → structured prompt formatting → evaluation against reference alignments → analysis of precision/recall trade-offs
- Critical path: Ontology triples → prompt formatting → ChatGPT processing → match generation → evaluation → analysis
- Design tradeoffs: High recall comes at the cost of low precision; context length limitations require splitting inputs; structured formatting improves parsing but may not capture complex relationships
- Failure signatures: Low precision with high recall indicates the model is finding many true positives but also many false positives; context loss when prompts are split; inverse property handling errors
- First 3 experiments:
  1. Test single comprehensive prompt vs. split prompts on a simple ontology pair to measure context retention impact
  2. Compare structured formatting vs. natural language conversion on a known ontology pair to validate formatting benefits
  3. Evaluate inverse property handling by creating test cases with explicit inverse relationships to measure error rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance on ontology alignment tasks compare to other state-of-the-art ontology matching systems?
- Basis in paper: [explicit] The authors compare ChatGPT's results to OAEI 2022 results and find that ChatGPT achieves an average F1-score of 0.52, which is lower than some of the other systems mentioned (e.g., ALIN with 0.61, GraphMatcher with 0.67, LogMap with 0.68).
- Why unresolved: While the paper provides a comparison with OAEI 2022 results, it does not conduct a comprehensive evaluation against all the mentioned systems or other recent state-of-the-art ontology matching systems.
- What evidence would resolve it: A detailed comparison of ChatGPT's performance with other state-of-the-art ontology matching systems on the same benchmark datasets would provide a clearer picture of its relative performance.

### Open Question 2
- Question: How can the precision issues in ChatGPT's ontology alignment output be effectively addressed?
- Basis in paper: [explicit] The authors identify several factors contributing to low precision, including ChatGPT's context length limit, handling of inverse functional properties, matching with subclasses, unseen/ambiguous alignments, and uncertain matchings. They suggest that domain experts can filter out irrelevant generated statements and propose potential solutions such as revising reference datasets or developing methods to help LLMs detect implausible alignments.
- Why unresolved: While the authors identify the issues and propose potential solutions, they do not provide a concrete implementation or evaluation of these solutions in the paper.
- What evidence would resolve it: Implementing and evaluating the proposed solutions or other methods to improve ChatGPT's precision in ontology alignment tasks would provide insights into their effectiveness.

### Open Question 3
- Question: How can ChatGPT be effectively used to expand reference ontologies?
- Basis in paper: [explicit] The authors mention that ChatGPT's ability to generate new entities suggests that it could be used to expand reference ontologies, but they do not provide specific details or examples of how this could be achieved.
- Why unresolved: The paper does not explore or evaluate the potential of using ChatGPT to generate new entities or expand reference ontologies.
- What evidence would resolve it: Conducting experiments to evaluate ChatGPT's ability to generate relevant and accurate new entities for reference ontologies would provide evidence of its potential for ontology expansion.

## Limitations

- The study is limited to conference track ontologies from OAEI 2022, which may not generalize to other domains
- Zero-shot approach appears insufficient for achieving high precision, with F1-score of only 0.52 despite 92% recall
- The paper doesn't explore fine-tuning ChatGPT on ontology alignment tasks, which could potentially improve precision
- Context length limitations require prompt splitting, but the impact on alignment quality remains unclear

## Confidence

- **High Confidence**: ChatGPT can achieve high recall (up to 92%) in identifying potential ontology matches when properly prompted with structured formatting.
- **Medium Confidence**: Structured predicate-subject-object formatting improves ChatGPT's ability to process ontology triples compared to natural language conversion, though the magnitude of improvement needs further validation.
- **Low Confidence**: The zero-shot prompting approach is optimal for ontology alignment with ChatGPT, as the paper doesn't explore fine-tuning or few-shot alternatives that might yield better precision-recall trade-offs.

## Next Checks

1. **Fine-tuning experiment**: Compare zero-shot performance against a fine-tuned version of ChatGPT on the same ontology alignment task to quantify the potential precision gains from task-specific adaptation.

2. **Domain generalization test**: Evaluate ChatGPT's alignment performance on ontologies from different domains (e.g., biomedical, geographic) to assess whether the high recall observed in conference ontologies generalizes to other knowledge areas.

3. **Context retention analysis**: Systematically measure the impact of prompt splitting on alignment quality by comparing performance across different split strategies and quantifying context loss when processing large ontology pairs.