---
ver: rpa2
title: '4M: Massively Multimodal Masked Modeling'
arxiv_id: '2312.06647'
source_url: https://arxiv.org/abs/2312.06647
tags:
- input
- modalities
- tokens
- training
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 4M is a multimodal training framework that unifies diverse vision
  modalities (RGB, depth, normals, segmentation, captions, bounding boxes, CLIP features)
  into discrete tokens and trains a single Transformer encoder-decoder via masked
  modeling. It scales by masking only a small random subset of input and target tokens,
  enabling efficient training across many modalities.
---

# 4M: Massively Multimodal Masked Modeling

## Quick Facts
- arXiv ID: 2312.06647
- Source URL: https://arxiv.org/abs/2312.06647
- Reference count: 40
- One-line primary result: 4M is a multimodal training framework that unifies diverse vision modalities into discrete tokens and trains a single Transformer encoder-decoder via masked modeling.

## Executive Summary
4M is a multimodal training framework that unifies diverse vision modalities (RGB, depth, normals, segmentation, captions, bounding boxes, CLIP features) into discrete tokens and trains a single Transformer encoder-decoder via masked modeling. It scales by masking only a small random subset of input and target tokens, enabling efficient training across many modalities. The method achieves strong out-of-the-box performance on key vision tasks, excels when fine-tuned on unseen downstream tasks, and supports flexible multimodal conditional generation for editing and content creation.

## Method Summary
4M tokenizes all input modalities into discrete tokens using modality-specific tokenizers (VQ-VAE for dense modalities, WordPiece for text/bounding boxes). It then trains a single Transformer encoder-decoder using multimodal masked modeling, randomly sampling a small subset of input and target tokens from each modality. The model is trained on the CC12M dataset with pseudo-labeled modalities, using a symmetric Dirichlet distribution for token sampling. During fine-tuning, the model can be adapted to various downstream tasks by masking and predicting the relevant target modality.

## Key Results
- 4M achieves strong out-of-the-box performance on key vision tasks including ImageNet-1K classification, COCO detection/segmentation, ADE20K segmentation, and NYUv2 depth estimation.
- When fine-tuned on downstream tasks, 4M excels compared to baselines and shows competitive transfer learning results.
- The method supports flexible multimodal conditional generation for editing and content creation, enabling text-to-image tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenizing all modalities into discrete tokens enables a single Transformer encoder-decoder to handle diverse vision modalities.
- Mechanism: By converting images, text, bounding boxes, depth maps, and other modalities into sequences or sets of discrete tokens, the model treats all modalities uniformly as token prediction problems. This removes the need for task-specific encoders or heads, enabling full parameter sharing and compatibility across modalities.
- Core assumption: Tokenization preserves sufficient information from each modality for the model to learn useful representations and make accurate predictions.
- Evidence anchors:
  - [abstract]: "4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens"
  - [section]: "To enable training a single Transformer on modalities with different formats, like text, bounding boxes, images, or neural network feature maps, we choose to unify their representational spaces by mapping them into sets or sequences of discrete tokens"
- Break condition: If tokenization fails to preserve critical information (e.g., high quantization loss for depth maps), the model's ability to transfer to downstream tasks or generate high-fidelity outputs would degrade.

### Mechanism 2
- Claim: Multimodal masked modeling with random input and target token subsets enables efficient training on many modalities.
- Mechanism: Instead of encoding and decoding all tokens from all modalities, the model randomly samples a small subset of input tokens to encode and another small subset of target tokens to decode. This decouples the computational cost from the number of modalities, allowing scalable training.
- Core assumption: Randomly sampling tokens from all modalities during each training step provides sufficient cross-modal learning signal while keeping computational costs manageable.
- Evidence anchors:
  - [abstract]: "4M achieves scalability by... performing multimodal masked modeling on a small randomized subset of tokens"
  - [section]: "Decoupling the number of input and target tokens from the number of modalities prevents the computational cost from rapidly escalating with increasing modalities"
- Break condition: If the random sampling is too aggressive (e.g., too few tokens per modality), the model may not learn meaningful cross-modal relationships, leading to poor generalization.

### Mechanism 3
- Claim: Training on pseudo-labeled multimodal data enables learning rich representations without requiring large-scale aligned datasets.
- Mechanism: By using off-the-shelf models to generate aligned pseudo labels for depth, normals, segmentation, etc., 4M can train on large-scale datasets like CC12M without needing expensive manual annotations. This allows the model to benefit from diverse data while learning cross-modal predictive coding.
- Core assumption: The pseudo-labeled data is sufficiently accurate and diverse to enable the model to learn transferable representations.
- Evidence anchors:
  - [abstract]: "Leveraging the availability of single-modal or text-image pair datasets, such as CC12M, we employ strong pseudo labeling networks to generate aligned binding data across various modalities"
  - [section]: "Because this approach only requires access to a dataset of RGB images, it may scale to even larger web-scale image datasets"
- Break condition: If pseudo labels are noisy or biased, the model may learn incorrect associations, leading to poor performance on downstream tasks or generation.

## Foundational Learning

- Concept: Discrete tokenization of continuous signals
  - Why needed here: Vision modalities like RGB images, depth maps, and surface normals are continuous signals. Converting them into discrete tokens allows the model to treat them uniformly alongside text and other modalities, enabling a single architecture to handle all tasks.
  - Quick check question: How does the vector-quantized autoencoder (VQ-VAE) ensure that the discrete tokens preserve meaningful information from the original continuous signals?

- Concept: Masked modeling objective
  - Why needed here: Masked modeling forces the model to learn to predict missing information from context, which is crucial for both representation learning and generative capabilities. In a multimodal setting, this encourages cross-modal predictive coding.
  - Quick check question: Why does masking only a small random subset of tokens (rather than all tokens) improve training efficiency and scalability?

- Concept: Cross-modal predictive coding
  - Why needed here: By training to predict one modality from another (e.g., depth from RGB), the model learns rich, shared representations that are useful for both understanding (e.g., depth estimation) and generation (e.g., text-to-image) tasks.
  - Quick check question: How does the model ensure that the learned representations are useful for both understanding and generation tasks?

## Architecture Onboarding

- Component map:
  Input modalities (RGB, depth, normals, segmentation, captions, bounding boxes, CLIP features) -> Tokenization (VQ-VAE, WordPiece) -> Transformer encoder-decoder -> Output projection (cross-entropy loss per modality)

- Critical path:
  1. Tokenize all input modalities into discrete tokens.
  2. Randomly sample a small subset of input tokens and encode them.
  3. Randomly sample a small subset of target tokens from the remaining modalities.
  4. Decode the target tokens conditioned on the encoded inputs.
  5. Compute cross-entropy loss for each target modality and backpropagate.

- Design tradeoffs:
  - Tokenization vs. raw features: Tokenization enables a unified architecture but may lose fine-grained information; raw features preserve detail but require modality-specific processing.
  - Masking ratio: Higher masking ratios improve efficiency but may make the task too hard; lower ratios improve learning but increase computational cost.
  - Input vs. target token budget: Balancing these affects both training efficiency and the model's ability to learn cross-modal relationships.

- Failure signatures:
  - Poor reconstruction quality: Indicates tokenization or masking strategy issues.
  - Degraded transfer learning: Suggests insufficient cross-modal learning or poor pseudo label quality.
  - Mode collapse in generation: Points to imbalanced guidance or insufficient diversity in training data.

- First 3 experiments:
  1. Train a baseline 4M model on RGB→RGB only (like MAE) to verify tokenization and masking work.
  2. Add a second modality (e.g., depth) and train on RGB→depth to test cross-modal learning.
  3. Train on all modalities with full masking strategy and evaluate on a simple transfer task (e.g., RGB→segmentation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of 4M scale with even larger datasets and longer training schedules?
- Basis in paper: [explicit] The paper states "We anticipate 4M to conveniently extend to such new modalities" and discusses the potential benefits of training on larger datasets and longer schedules. It also shows that 4M scales with dataset size and training length in the ablation study.
- Why unresolved: The paper only trained 4M models on CC12M and for a fixed number of training tokens. It's unclear how much further performance gains could be achieved with significantly larger datasets and longer training schedules.
- What evidence would resolve it: Training 4M models on datasets an order of magnitude larger than CC12M (e.g. LAION-5B) for training schedules 10x longer than the current longest schedule. Comparing the performance of these models to the current 4M models on downstream tasks.

### Open Question 2
- Question: How would the performance of 4M be affected by using different types of tokenizers for different modalities?
- Basis in paper: [explicit] The paper discusses the use of modality-specific tokenizers and states "We found that to avoid the joint VQ-VAE and diffusion training to collapse to degenerate solution, it is crucial to restart dead / unused codebook entries". It also shows that different tokenizers are used for different modalities (e.g. CLIP features, semantic segmentation).
- Why unresolved: The paper only uses a limited set of tokenizers for the modalities it trains on. It's unclear how the choice of tokenizer would affect the performance of 4M for other modalities or for the same modalities with different tokenizers.
- What evidence would resolve it: Training 4M models with different tokenizers for the same modalities and comparing their performance on downstream tasks. Also, training 4M models with tokenizers for modalities not currently used and evaluating their performance.

### Open Question 3
- Question: How would the performance of 4M be affected by using different masking strategies during pre-training?
- Basis in paper: [explicit] The paper extensively ablates different masking strategies and states "We ablate various choices of Dirichlet parameter α, both for the input and target sampling" and "We ablate the following two loss weighting strategies". It also discusses the impact of masking ratio and spatial overlap on performance.
- Why unresolved: The paper only explores a limited set of masking strategies. It's unclear how other masking strategies (e.g. different ways of sampling tokens, different ways of computing the loss) would affect the performance of 4M.
- What evidence would resolve it: Training 4M models with different masking strategies and comparing their performance on downstream tasks. Also, exploring masking strategies that are not currently used (e.g. masking based on the content of the image, masking based on the task).

## Limitations

- The quality and compatibility of discrete tokenization across diverse modalities (RGB, depth, normals, segmentation, captions, bounding boxes, CLIP features) is not fully validated, which could impact the model's ability to learn useful representations and generate high-quality outputs.
- The reliance on pseudo-labeled data generated by off-the-shelf models may introduce noise or bias, leading to poor generalization, especially on tasks where ground truth is unavailable for validation.
- The optimal masking ratios, token budgets, and Dirichlet sampling parameters for different task sets or modalities are not extensively explored, and aggressive masking may lead to insufficient cross-modal learning.

## Confidence

**High Confidence**:
- The feasibility of unifying diverse vision modalities into discrete tokens and training a single Transformer encoder-decoder via multimodal masked modeling.
- The scalability benefits of masking only a small random subset of input and target tokens, enabling efficient training across many modalities.
- The strong out-of-the-box performance on key vision tasks (ImageNet-1K, COCO, ADE20K, NYUv2) and competitive transfer learning results.

**Medium Confidence**:
- The benefits of multimodal masking, model scaling, and dataset size for improving performance, based on ablation studies.
- The effectiveness of pseudo-labeled multimodal data for learning rich representations without requiring large-scale aligned datasets.
- The flexibility of the model for multimodal conditional generation and editing tasks.

**Low Confidence**:
- The robustness and generalization of the learned representations to novel modalities or out-of-distribution inputs.
- The quality and diversity of generated outputs for complex or diverse text prompts.
- The optimal masking ratios, token budgets, and Dirichlet sampling parameters for different task sets or modalities.

## Next Checks

1. **Tokenization Quality Analysis**: Conduct a detailed quantitative and qualitative analysis of tokenization quality across all modalities (RGB, depth, normals, segmentation, captions, bounding boxes, CLIP features). Measure reconstruction fidelity, information preservation, and modality compatibility. Visualize pseudo labels and compare to ground truth where available. Assess the impact of tokenization quality on downstream task performance.

2. **Robustness and Generalization Study**: Evaluate the model's robustness and generalization to out-of-distribution inputs, rare modalities, and novel tasks. Test on datasets with different data distributions, image styles, or domain shifts (e.g., medical images, satellite imagery). Analyze failure modes and identify scenarios where the model's performance degrades. Investigate the model's adaptability to new modalities or tasks with minimal fine-tuning.

3. **Generation Quality and Diversity Evaluation**: Conduct a comprehensive evaluation of the model's generation quality and diversity for text-to-image tasks. Use both quantitative metrics (FID, CLIP score, diversity metrics) and qualitative human evaluations. Test on diverse and complex prompts, including rare objects, abstract concepts, and varied styles. Analyze the generated outputs for realism, coherence, and creativity. Investigate the model's ability to handle long or multi-modal prompts and generate diverse outputs.