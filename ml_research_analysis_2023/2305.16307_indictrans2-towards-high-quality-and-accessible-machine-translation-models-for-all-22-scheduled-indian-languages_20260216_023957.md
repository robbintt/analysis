---
ver: rpa2
title: 'IndicTrans2: Towards High-Quality and Accessible Machine Translation Models
  for all 22 Scheduled Indian Languages'
arxiv_id: '2305.16307'
source_url: https://arxiv.org/abs/2305.16307
tags:
- languages
- translation
- language
- data
- deva
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the lack of high-quality and accessible machine
  translation systems for all 22 scheduled Indian languages. The authors identify
  four key areas of improvement: curating and creating larger training datasets, creating
  diverse and high-quality benchmarks, training multilingual models, and releasing
  models with open access.'
---

# IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages

## Quick Facts
- **arXiv ID**: 2305.16307
- **Source URL**: https://arxiv.org/abs/2305.16307
- **Reference count**: 40
- **Primary result**: First translation model supporting all 22 scheduled Indian languages, outperforming existing models on multiple benchmarks including human evaluation

## Executive Summary
This paper addresses the critical gap in high-quality machine translation for all 22 scheduled Indian languages. The authors present four key contributions: the Bharat Parallel Corpus Collection (BPCC), the first n-way parallel benchmark covering all 22 Indian languages (IN22), IndicTrans2 (the first translation model supporting all 22 languages), and open access to all models and data. The work demonstrates significant improvements in translation quality through larger training datasets, diverse benchmarks, and multilingual model training. The results show that IndicTrans2 surpasses existing models on multiple benchmarks, validated through both automatic metrics and human evaluation, with all resources released under permissive licenses to promote accessibility and collaboration.

## Method Summary
The authors developed IndicTrans2 through a multi-stage approach: (1) curating the Bharat Parallel Corpus Collection by mining web-scale monolingual data and filtering through LaBSE embeddings and FAISS indexing to create ~230M bitext pairs, supplemented with 644K manually translated pairs; (2) creating the IN22 benchmark with three diverse subsets covering Wikipedia, web content, and conversations; (3) training a 1.1B parameter multilingual Transformer model using sequential training (first on combined data, then fine-tuning on high-quality seed data) with back-translation augmentation; and (4) evaluating performance across multiple metrics including chrF++, BLEU, COMET, and human evaluation via XSTS methodology.

## Key Results
- IndicTrans2 achieves state-of-the-art performance on multiple benchmarks for all 22 Indian languages
- The model demonstrates strong cross-lingual transfer, particularly benefiting low-resource languages
- Human evaluation confirms automatic metric results, with IndicTrans2 outperforming existing models in adequacy and fluency
- All training data and models are released under permissive licenses for accessibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger parallel corpora directly improve translation quality, especially for low-resource languages
- Mechanism: Increasing the volume of parallel sentences provides more examples for the model to learn mappings between languages, which is critical for languages with sparse training data
- Core assumption: Quality of the mined data is sufficiently high; filtering reduces noise
- Evidence anchors:
  - [abstract]: "Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages... BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added..."
  - [section]: "Our mining efforts focus on 12 Indic languages... In our mining efforts, a total of ~126 million sentence pairs were mined in addition to existing corpora, resulting in an aggregated collection of ~230.5 million sentence pairs..."
  - [corpus]: "Weak. Corpus lacks explicit quality scores or human evaluation of mined pairs; relies on similarity thresholds (0.80) but no error rate data"
- Break condition: If mined pairs are mostly misaligned or low quality, performance gains plateau or degrade

### Mechanism 2
- Claim: Human-annotated seed data improves model performance more than mined data alone
- Mechanism: High-quality, manually translated sentences provide accurate, domain-diverse examples that help the model learn nuanced mappings and reduce translationese effects
- Core assumption: Translators follow strict guidelines and undergo quality checks, ensuring high fidelity
- Evidence anchors:
  - [abstract]: "Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets"
  - [section]: "The parallel corpus thus created has additional benefits for extremely low-resource languages... Some languages have minimal monolingual sources, so the translated data can serve as high-quality, multi-domain seed monolingual corpora"
  - [corpus]: "Weak. Corpus does not quantify annotation accuracy or inter-annotator agreement; quality control relies on post-hoc checks rather than continuous measurement"
- Break condition: If translators deviate from guidelines or quality control is inconsistent, seed data introduces noise

### Mechanism 3
- Claim: Multilingual models benefit from cross-lingual transfer, especially among related Indic languages
- Mechanism: Shared linguistic features (e.g., cognates, syntax) allow knowledge to transfer across languages, improving low-resource language performance
- Core assumption: Language embeddings capture these similarities; training data is sufficient for transfer
- Evidence anchors:
  - [abstract]: "Next, we present IndicTrans2, the first translation model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks..."
  - [section]: "To visualize how Indian languages are related, we plot the average embedding representations... While major linguistic similarities are reflected in the plot, similarity in writing script also seems to play a role..."
  - [corpus]: "Weak. No quantitative transfer learning results (e.g., zero-shot scores per language pair) shown; similarity plot is qualitative"
- Break condition: If embeddings do not capture linguistic relatedness, transfer learning fails

## Foundational Learning

- Concept: Parallel corpora and their role in training machine translation models
  - Why needed here: Understanding how bitext pairs are collected, filtered, and used is central to grasping why BPCC is a major contribution
  - Quick check question: What is the difference between mined parallel corpora and human-annotated seed data, and why are both needed?

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: The paper claims IndicTrans2 benefits from similarities among Indian languages; understanding transfer learning explains this mechanism
  - Quick check question: How does a model trained on multiple related languages improve performance on a low-resource language it has seen little data for?

- Concept: Evaluation metrics for machine translation (chrF++, BLEU, COMET)
  - Why needed here: The paper uses these metrics to compare models; knowing their strengths/weaknesses is key to interpreting results
  - Quick check question: Why might chrF++ be preferred over BLEU for morphologically rich languages like those in India?

## Architecture Onboarding

- Component map:
  - Monolingual corpora -> LaBSE embeddings -> FAISS indexing -> similarity search -> mined pairs
  - Tokenization -> Transformer encoder-decoder (1.1B params) -> Stage 1 training -> Stage 2 fine-tuning -> Back-translation augmentation
  - Benchmarks (IN22, FLORES) -> Metrics (chrF++, BLEU, COMET) -> Human evaluation (XSTS)

- Critical path:
  1. Curate and filter parallel corpora (BPCC creation)
  2. Train base model on full corpus (Stage 1)
  3. Fine-tune on high-quality seed data (Stage 2)
  4. Augment with back-translated data
  5. Evaluate on multiple benchmarks

- Design tradeoffs:
  - Large models (1.1B) vs. inference latency: Larger models may perform better but are slower and costlier to deploy
  - Monolingual mining vs. document-aligned mining: Mining from web-scale monolingual data yields more pairs but risks noise; document-aligned mining is cleaner but scarcer
  - English-centric training vs. true multilingual: Pivoting through English simplifies training but adds latency for Indic-Indic translation

- Failure signatures:
  - Poor chrF++ scores on low-resource languages: Indicates insufficient quality or quantity of seed data
  - High variance in translation quality across domains: Suggests domain mismatch between training and test data
  - Low correlation between automatic metrics and human evaluation: Implies metrics do not capture adequacy/fluency well for Indic languages

- First 3 experiments:
  1. Ablation study: Train with and without human-annotated seed data to quantify its impact on low-resource languages
  2. Cross-lingual transfer test: Evaluate zero-shot translation between closely related Indic languages (e.g., Hindi-Marathi) to measure transfer effectiveness
  3. Back-translation impact: Compare model performance with and without back-translation augmentation to assess data diversity benefits

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results, several implicit questions remain regarding the scalability to even lower-resource languages, the effectiveness of cross-lingual transfer mechanisms, and the long-term sustainability of maintaining such large multilingual models for 22 languages with varying resource levels.

## Limitations

- The quality control process for mined parallel corpora is not quantitatively validated, relying on similarity thresholds without reporting error rates
- The paper does not report zero-shot translation performance between Indic languages, leaving questions about cross-lingual transfer effectiveness
- Inference latency and computational costs for the 1.1B parameter models are not discussed, which is critical for real-world deployment

## Confidence

- **High Confidence**: Claims about dataset sizes (230M total pairs, 126M newly mined) and benchmark creation (IN22 with three subsets) are well-supported by specific counts and clear methodology descriptions
- **Medium Confidence**: Claims about IndicTrans2 outperforming existing models on chrF++ metrics are credible but limited by the lack of zero-shot evaluation and potential benchmark overfitting
- **Low Confidence**: Claims about human evaluation superiority lack sufficient detail on evaluator selection, sample size, and methodology to be fully verified

## Next Checks

1. **Ablation Study on Seed Data**: Train models with and without the manually translated seed data to quantify its specific contribution to low-resource language performance, particularly for languages with minimal monolingual sources

2. **Zero-Shot Indic-Indic Translation**: Evaluate translation quality between closely related Indic languages (e.g., Hindi-Marathi, Tamil-Telugu) without English as an intermediary to assess true multilingual capability and cross-lingual transfer

3. **Mined Data Quality Analysis**: Conduct a quantitative assessment of mined parallel corpus quality by sampling and manually evaluating a subset of pairs, reporting alignment accuracy and error rates to validate the mining methodology's effectiveness