---
ver: rpa2
title: The geometry of financial institutions -- Wasserstein clustering of financial
  data
arxiv_id: '2305.03565'
source_url: https://arxiv.org/abs/2305.03565
tags:
- data
- missing
- points
- which
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for clustering probability
  distributions with missing data, motivated by financial regulation needs. The method
  extends Lloyd's algorithm to probability distributions by using generalized Wasserstein
  barycenters and incorporates a soft imputation approach that avoids the bias of
  traditional imputation methods.
---

# The geometry of financial institutions -- Wasserstein clustering of financial data

## Quick Facts
- arXiv ID: 2305.03565
- Source URL: https://arxiv.org/abs/2305.03565
- Reference count: 40
- Key outcome: Novel method for clustering probability distributions with missing data using soft imputation and generalized Wasserstein barycenters, outperforming classical imputation techniques

## Executive Summary
This paper addresses the challenge of clustering probability distributions with missing data, motivated by financial regulation needs. The authors propose a variant of Lloyd's algorithm that operates on probability distributions using generalized Wasserstein barycenters. The method incorporates a soft imputation approach that avoids the bias of traditional deterministic imputation methods by using a weighted combination of observed values from the same cluster. The approach successfully clusters financial institutions based on granular loan data with varying levels of missing information and provides visual representations of banking landscapes.

## Method Summary
The method extends Lloyd's algorithm to probability distributions by using generalized Wasserstein barycenters and incorporates a soft imputation approach that avoids bias from traditional imputation methods. The algorithm iteratively assigns each observed distribution to the nearest cluster barycenter (based on Wasserstein distance in observed coordinates) and updates the barycenters using generalized Wasserstein barycenters that account for missing data. The soft imputation strategy fills missing values using a weighted combination of observed values from the same cluster, with weights inversely proportional to the distance between distributions in observed coordinates.

## Key Results
- Outperforms classical imputation techniques in both Euclidean and distributional settings when evaluated using Gromov-Wasserstein distance
- Successfully clusters financial institutions based on granular loan data with varying levels of missing information
- Provides visual representation of banking landscapes and identifies potential outliers
- Particularly effective when data is missing systematically, achieving higher Rand scores compared to standard imputation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method avoids bias from deterministic imputation by using a soft, probabilistic imputation strategy.
- Mechanism: Instead of filling missing values deterministically (e.g., with mean or median), the algorithm imputes missing coordinates using a weighted combination of observed values from the same cluster. The weights are inversely proportional to the distance between distributions in the observed coordinates, ensuring that values from similar distributions contribute more to the imputation.
- Core assumption: The similarity of distributions in the observed coordinates is a good proxy for similarity in the missing coordinates.
- Evidence anchors:
  - [abstract]: "We devise a particular way of soft imputation which accounts for a random element in filling up missing values, and in particular avoids the above mentioned bias."
  - [section 3.2]: "We introduce the shorthand Di ℓ := d(yi ℓ,xℓ) = d(ϕ−1 i (˜xi),xℓ). In order to determine the weights of a probability measure, we set pi ℓ := f(Di ℓ) with f : [0 ,∞) → [0 ,∞) being some positive decreasing function fixed in advance, e.g. f(x) := exp(−x2)."
  - [corpus]: Weak. Corpus mentions "Wasserstein-based Kernel Principal Component Analysis for Clustering Applications" but doesn't directly address imputation bias avoidance.

### Mechanism 2
- Claim: The algorithm successfully clusters probability distributions with missing data by generalizing Lloyd's algorithm to use Wasserstein barycenters.
- Mechanism: The algorithm iteratively assigns each observed distribution to the nearest cluster barycenter (based on the Wasserstein distance in the observed coordinates) and then updates the barycenters using a generalized Wasserstein barycenter that accounts for the missing data. This process is repeated until convergence.
- Core assumption: The generalized Wasserstein barycenter can effectively represent a cluster of distributions with missing data.
- Evidence anchors:
  - [abstract]: "To address these challenges, we propose a variant of Lloyd's algorithm that applies to probability distributions and uses generalized Wasserstein barycenters to construct a metric space which represents given data on various objects in condensed form."
  - [section 5]: "Algorithm 1: NA Wasserstein k-means algorithm... We can tackle problem (NA-W-k-means) by the suggested iterations in Section 3.1, which read as i) update cluster assignments given barycenters ν1,...,ν k, ii) update cluster barycenters given an assignment a..."
  - [corpus]: Weak. Corpus neighbors don't directly address generalized Wasserstein barycenters for clustering with missing data.

### Mechanism 3
- Claim: The method provides a visual representation of complex financial data landscapes and identifies potential outliers.
- Mechanism: After clustering, the algorithm imputes missing values to obtain a complete representation of each financial institution as a probability measure. It then calculates pairwise distances between these measures using a generalized metric based on product couplings. These distances are embedded into a low-dimensional space (e.g., using multidimensional scaling) for visualization.
- Core assumption: The generalized metric based on product couplings accurately captures the dissimilarity between imputed distributions.
- Evidence anchors:
  - [abstract]: "The approach successfully clusters financial institutions based on granular loan data with varying levels of missing information, providing a visual representation of banking landscapes and identifying potential outliers."
  - [section 7.2]: "Using the distance ρ on P2(P2(R7)), defined in (15), we calculate pairwise distances between the financial institutions. In order to get a visual representation of the landscape of financial institutions, and the clustering results, we use multidimensional scaling..."
  - [corpus]: Weak. Corpus neighbors don't directly address visualization of financial data landscapes using Wasserstein clustering.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: The algorithm relies on Wasserstein distance to measure dissimilarity between probability distributions, and optimal transport theory to compute generalized Wasserstein barycenters.
  - Quick check question: What is the Kantorovich formulation of the Wasserstein distance, and how does it relate to the Monge formulation?

- Concept: Lloyd's Algorithm and K-Means Clustering
  - Why needed here: The algorithm is a generalization of Lloyd's algorithm to the space of probability distributions, using Wasserstein barycenters instead of Euclidean means.
  - Quick check question: How does Lloyd's algorithm guarantee convergence to a local minimum, and what are the limitations of this guarantee?

- Concept: Metric Spaces and Generalized Metrics
  - Why needed here: The algorithm operates in metric spaces (Euclidean space and Wasserstein space) and uses generalized metrics to compare imputed distributions.
  - Quick check question: What are the axioms of a metric space, and how does the generalized metric defined in (8) satisfy these axioms?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Handling missing data and converting observations to probability measures
  - Distance computation: Calculating Wasserstein distances between distributions in observed coordinates
  - Cluster assignment: Assigning each distribution to the nearest cluster barycenter
  - Barycenter update: Computing generalized Wasserstein barycenters for each cluster
  - Imputation: Filling missing values using a weighted combination of observed values from the same cluster
  - Visualization: Embedding pairwise distances into a low-dimensional space for visualization

- Critical path:
  1. Preprocess data and convert to probability measures
  2. Initialize cluster barycenters
  3. Iterate: assign distributions to clusters, update barycenters
  4. Impute missing values using soft imputation strategy
  5. Calculate pairwise distances between imputed distributions
  6. Embed distances for visualization

- Design tradeoffs:
  - Soft imputation vs. deterministic imputation: Soft imputation reduces bias but increases computational complexity
  - Number of clusters (k): Higher k may lead to overfitting, lower k may miss important patterns
  - Distance metric: Wasserstein distance captures distributional differences but is computationally expensive

- Failure signatures:
  - Algorithm fails to converge: Check initialization, barycenter computation, and distance calculations
  - Clusters are not meaningful: Check data preprocessing, distance metric choice, and imputation strategy
  - Visualization is misleading: Check embedding method, distance calculations, and imputation quality

- First 3 experiments:
  1. Synthetic data with known cluster structure and varying levels of missingness: Evaluate clustering performance and imputation quality
  2. Real financial data with different types of missingness: Compare clustering results with domain expertise and identify potential outliers
  3. Sensitivity analysis on algorithm parameters (k, imputation weights, etc.): Understand the impact of parameter choices on clustering and visualization results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when the data is missing systematically with varying degrees of correlation between the missingness and the underlying cluster structure?
- Basis in paper: [explicit] The paper mentions that the method is particularly effective when data is missing systematically, but does not provide extensive experimental evidence on how it performs under varying degrees of correlation between missingness and cluster structure.
- Why unresolved: The paper provides some simulation results with different settings of missing values, but does not systematically vary the correlation between missingness and cluster structure to evaluate the method's robustness.
- What evidence would resolve it: Additional simulation studies that vary the correlation between missingness and cluster structure, along with corresponding evaluation metrics like the Rand index or Gromov-Wasserstein distance, would provide insights into the method's performance under different scenarios.

### Open Question 2
- Question: How does the proposed method compare to other state-of-the-art methods for clustering distributions with missing data, such as the k-pod method or the Wasserstein k-means++ method?
- Basis in paper: [explicit] The paper mentions that clustering distributions with missing coordinates has not been investigated before and introduces the NA Wasserstein k-means algorithm. However, it does not provide a comprehensive comparison with other existing methods for clustering distributions with missing data.
- Why unresolved: The paper focuses on introducing the new method and demonstrating its effectiveness compared to classical imputation techniques, but does not explore its performance relative to other methods specifically designed for clustering distributions with missing data.
- What evidence would resolve it: A thorough comparison of the proposed method with other state-of-the-art methods for clustering distributions with missing data, using various evaluation metrics and datasets, would provide insights into its relative performance and advantages.

### Open Question 3
- Question: How does the proposed method scale to high-dimensional data with a large number of missing values?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed method on low-dimensional data with missing values, but does not provide evidence of its scalability to high-dimensional data with a large number of missing values.
- Why unresolved: The paper focuses on the theoretical development and experimental validation of the proposed method, but does not investigate its performance and scalability in high-dimensional settings with a large number of missing values.
- What evidence would resolve it: Experimental results on high-dimensional datasets with varying numbers of missing values, along with computational complexity analysis and comparisons with other methods, would provide insights into the scalability and efficiency of the proposed method.

## Limitations

- Reliance on systematic missingness patterns: The method performs best when data is missing systematically, and may introduce noise when missingness is random
- Computational complexity: Generalized Wasserstein barycenters are computationally expensive, limiting scalability to large datasets
- Limited validation on diverse real-world datasets: Most empirical results are based on simulated data or a single financial dataset

## Confidence

- High Confidence: The mathematical framework for extending Lloyd's algorithm to probability distributions using Wasserstein barycenters is rigorously defined and internally consistent
- Medium Confidence: The empirical results showing improved clustering performance over traditional imputation methods are promising but based on limited simulations
- Low Confidence: The visualization component's ability to reliably identify outliers in complex financial data landscapes requires further testing across diverse datasets

## Next Checks

1. Test the algorithm on synthetic data with varying missingness mechanisms (MCAR, MAR, MNAR) to quantify performance degradation when missingness is not systematic
2. Conduct a sensitivity analysis on the weighting function f(x) used in soft imputation to determine its impact on clustering stability and bias reduction
3. Apply the method to multiple real-world datasets with known ground truth clustering to validate the Rand score improvements reported in simulations