---
ver: rpa2
title: Action valuation of on- and off-ball soccer players based on multi-agent deep
  reinforcement learning
arxiv_id: '2305.17886'
source_url: https://arxiv.org/abs/2305.17886
tags:
- players
- learning
- actions
- action
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to value on- and off-ball soccer
  players using multi-agent deep reinforcement learning. The key idea is to consider
  a discrete action space in a continuous state space that mimics Google Research
  Football, and leverage supervised learning for actions in reinforcement learning.
---

# Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning

## Quick Facts
- arXiv ID: 2305.17886
- Source URL: https://arxiv.org/abs/2305.17886
- Reference count: 40
- Primary result: Proposed method effectively values players' actions and provides important information for understanding player characteristics

## Executive Summary
This paper introduces a novel method for valuing on- and off-ball soccer players using multi-agent deep reinforcement learning. The approach discretizes continuous player movements into 8 directions and uses action supervision to improve Q-value learning. The method can assess player contributions during attacks, including actions without explicit event labels, which is vital for teamwork analysis, scouting, and fan engagement.

## Method Summary
The method uses a deep reinforcement learning framework with SARSA algorithm, discrete action space, and action supervision loss. It processes 92-dimensional state vectors (positions and velocities of 22 players and the ball) through a GRU-based neural network to output Q-values for 14 possible actions per player. The training combines TD loss for reinforcement learning, cross-entropy action supervision loss to bias learning toward actual player actions, and L1 regularization to prevent overfitting.

## Key Results
- The model successfully values both on-ball and off-ball player actions simultaneously
- Action supervision improves learning efficiency by reinforcing actual player actions
- The method provides meaningful player valuations that correlate with conventional performance indicators

## Why This Works (Mechanism)

### Mechanism 1
The model can value off-ball players by treating their continuous movements as discrete actions. It defines 8 discrete movement directions for off-ball players in a continuous state space, enabling the RL model to assign Q-values to actions like positioning and supporting runs that lack explicit event labels. This assumes continuous positional movements can be meaningfully discretized into a small set of discrete actions without losing important tactical information. The approach is novel with limited corpus evidence.

### Mechanism 2
Action supervision improves Q-value learning by biasing the model toward actual player actions. The supervised loss term cross-entropy of softmax values of the Q-function encourages the model to assign higher Q-values to actions that players actually took, providing inductive bias that improves learning efficiency. This assumes actions taken by players in real games are better than random actions and should be reinforced. No direct evidence exists in corpus papers about action supervision for multi-agent sports RL.

### Mechanism 3
The model can value multiple players simultaneously by treating each as an independent agent with shared state representation. The framework uses a single holistic network that takes in the state of all players and outputs Q-values for each player's possible actions, allowing simultaneous valuation of on-ball and off-ball players. This assumes players' actions can be modeled as independent decisions based on shared game state, even though they are actually interdependent in real gameplay. Limited corpus evidence exists for multi-agent approaches in sports RL.

## Foundational Learning

- Concept: Reinforcement Learning with Q-learning
  - Why needed here: The core valuation mechanism relies on estimating state-action values (Q-values) to assess player contributions
  - Quick check question: What is the Bellman equation for optimal Q-values, and how does SARSA differ from Q-learning?

- Concept: Supervised learning for action classification
  - Why needed here: Action supervision helps the model learn to assign appropriate Q-values to observed actions, improving learning efficiency
  - Quick check question: How does the cross-entropy loss for action supervision work, and why is it important to keep λ1 small?

- Concept: Multi-agent reinforcement learning
  - Why needed here: The framework needs to handle multiple players simultaneously, each with their own set of possible actions
  - Quick check question: What are the key differences between independent Q-learning and joint action learning in multi-agent settings?

## Architecture Onboarding

- Component map: Data preprocessing -> State/action sequence construction -> GRU forward pass -> Q-value output -> TD and supervised loss computation -> Parameter updates via Adam optimizer

- Critical path: Data preprocessing → Model input construction (state and action sequences) → GRU forward pass → Q-value output → TD and supervised loss computation → Parameter updates via Adam optimizer

- Design tradeoffs: The discrete action space simplifies learning but may miss nuanced movements; the single holistic network is efficient but may not capture complex interdependencies between players; the action supervision improves learning but risks overfitting if λ1 is too large.

- Failure signatures: If the model fails to learn, you'll see high TD loss that doesn't decrease; if it overfits to training data, you'll see low training loss but high test loss; if action supervision is too strong, Q-values for non-chosen actions will be artificially deflated.

- First 3 experiments:
  1. Train the model without action supervision (λ1=0) and compare TD loss convergence to the full model
  2. Vary the action supervision weight (λ1=0.001, 0.01, 0.1) and evaluate impact on Q-value distributions
  3. Test the model on a held-out game to see if it can generalize to players not in the training set

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the proposed multi-agent deep RL model's performance to the weighting of the action supervision loss (λ1) and L1 regularization loss (λ2)? The authors mention that λ1 = 0.01 and λ2 = 0.1 were used, and note that "much less supervision (e.g., λ1 ≪ 0.01) would lead to insufficient learning of counterfactual action values, whereas much more supervision (e.g., λ1 ≫ 0.01) may overfit to the actual actions." This remains unresolved as the paper does not provide a systematic analysis of how varying these hyperparameters affects model performance or player valuation accuracy.

### Open Question 2
How does the proposed method compare to other state-of-the-art deep RL approaches for player valuation in soccer, such as inverse reinforcement learning or trajectory prediction models? The authors mention that their approach "considers the RL model overall rather than as a sub-problem" and can value "on- and off-ball players even when no event occurs," but do not directly compare to other RL-based methods. This remains unresolved as the paper focuses on comparing the proposed method to conventional statistical indicators rather than other deep RL approaches.

### Open Question 3
Can the proposed method be extended to value players in other team sports beyond soccer, such as basketball or hockey? The authors mention that "Analysis of invasive sports such as soccer is challenging" and discuss how their method can "assess how multiple players move continuously throughout the game," but do not explore applications to other sports. This remains unresolved as the paper focuses on soccer-specific actions and rewards without discussing generalizability to other sports with different dynamics and rules.

## Limitations
- The discretization of continuous player movements into 8 discrete actions may lose tactical nuance and important movement patterns
- The model treats player actions as independent decisions despite their interdependence in real gameplay, potentially missing team coordination
- The reliance on event data for action supervision means the model cannot value actions that occur between labeled events

## Confidence
- **High Confidence**: The framework for combining RL with action supervision for Q-value learning is well-grounded in established RL principles
- **Medium Confidence**: The specific implementation details (8 discrete actions, λ1=0.01) appear reasonable but lack extensive ablation study support
- **Low Confidence**: Claims about the model's ability to capture complex team dynamics and provide actionable insights for scouting require more validation

## Next Checks
1. Conduct an ablation study varying the number of discrete actions (4, 8, 16) to quantify the impact on valuation accuracy for off-ball movements
2. Test the model on held-out games to evaluate generalization to new players and teams not in the training data
3. Compare the model's valuations against expert tactical analysts' assessments of the same player actions to validate real-world relevance