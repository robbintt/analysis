---
ver: rpa2
title: Learning Unseen Modality Interaction
arxiv_id: '2306.12795'
source_url: https://arxiv.org/abs/2306.12795
tags:
- modality
- modalities
- training
- learning
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unseen modality interaction,
  where multimodal models must generalize to modality combinations not seen during
  training. The core method idea involves projecting modality-specific features into
  a common space using a feature projection module, enabling flexible accumulation
  of information from any available modalities via simple summation.
---

# Learning Unseen Modality Interaction

## Quick Facts
- arXiv ID: 2306.12795
- Source URL: https://arxiv.org/abs/2306.12795
- Reference count: 40
- Primary result: Achieves 23.7% top-1 accuracy on unseen modality combinations for EPIC-Kitchens, outperforming late fusion baseline of 18.1%

## Executive Summary
This paper addresses the challenge of unseen modality interaction, where multimodal models must generalize to novel combinations of modalities not seen during training. The proposed method uses a feature projection module to transform modality-specific features into a common space, enabling flexible accumulation of information through simple summation. A dual-branch prediction approach with pseudo-supervision reduces overfitting to unreliable training modality combinations. Evaluated across video classification, robot state regression, and multimedia retrieval tasks, the method demonstrates strong performance on unseen modality combinations while maintaining robustness to noisy modalities.

## Method Summary
The method projects modality-specific features into a shared semantic space using an attention-based feature projection module, allowing flexible accumulation of information from any available modalities via summation. To reduce overfitting to less reliable training modality combinations, a dual-branch prediction approach is employed where one branch uses groundtruth supervision and the other uses pseudo-labels derived from averaged unimodal predictions. A feature alignment loss ensures projected features from different modalities occupy a common semantic space by encouraging proximity to learnable class tokens.

## Key Results
- On EPIC-Kitchens, achieves 23.7% top-1 accuracy on unseen modality combinations vs 18.1% for late fusion baseline
- Maintains strong performance across video classification, robot state regression, and multimedia retrieval tasks
- Demonstrates robustness to noisy modalities and scales effectively to more than two training modality subsets

## Why This Works (Mechanism)

### Mechanism 1
The feature projection module enables unseen modality interaction by transforming modality-specific features into a shared feature space, allowing accumulation through simple summation. The module uses attention-based feature reorganization to map different-length modality tokens into fixed-length, fixed-dimension vectors in a common space, preserving rich distinguishing information.

### Mechanism 2
Dual branch prediction with pseudo-supervision reduces overfitting to unreliable training modality combinations. One branch is supervised by groundtruth labels while the other uses pseudo-labels derived from averaging unimodal predictions over training epochs, with KL divergence loss encouraging consistency.

### Mechanism 3
Feature alignment loss ensures that projected features from different modalities occupy a common semantic space. Learnable tokens are introduced for each class/task, and the average projected feature is encouraged to be close to its corresponding token using Euclidean distance.

## Foundational Learning

- Concept: Multimodal feature fusion
  - Why needed here: The paper needs to combine features from different modalities that have different dimensionalities and semantic spaces.
  - Quick check question: What are the key differences between concatenation-based fusion and projection-based fusion?

- Concept: Attention mechanisms
  - Why needed here: The feature projection module uses attention to reorganize modality-specific tokens into a common space.
  - Quick check question: How does the attention matrix Om transform features of different lengths into a fixed length k*?

- Concept: Pseudo-label generation and usage
  - Why needed here: The dual branch approach uses pseudo-labels to indicate modality reliability and prevent overfitting.
  - Quick check question: Why does averaging unimodal predictions over training epochs provide a good estimate of modality reliability?

## Architecture Onboarding

- Component map: Unimodal encoders → Feature projection module → Dual branch transformer → Predictions → Feature alignment loss → Pseudo-supervision generation → Training objective

- Critical path: 1. Extract modality-specific features 2. Project features into common space 3. Apply feature alignment loss 4. Generate pseudo-labels 5. Train dual branches with different supervision 6. Combine predictions at inference

- Design tradeoffs:
  - Feature projection vs. concatenation: Projection preserves information but adds complexity; concatenation is simpler but loses information when modalities are missing
  - Dual branches vs. single branch: Dual branches reduce overfitting but increase parameters; single branch is simpler but more prone to overfitting
  - Learnable tokens vs. fixed vocabulary: Learnable tokens adapt to data but require careful initialization; fixed vocabulary is stable but may not capture task nuances

- Failure signatures:
  - Poor performance on unseen combinations but good on seen combinations: Likely overfitting to training modality combinations
  - Performance worse than late fusion: Feature projection may not be preserving discriminative information
  - Training instability: Loss balancing (α, λ) may need adjustment

- First 3 experiments:
  1. Verify feature projection preserves information: Compare classification accuracy using projected features vs. original features on seen combinations
  2. Test pseudo-supervision effectiveness: Train with and without dual branches on modality-incomplete data and compare generalization to unseen combinations
  3. Validate feature alignment: Visualize t-SNE of projected features with and without alignment loss to check if they occupy a common space

## Open Questions the Paper Calls Out

- How does the proposed method perform when the number of samples or modalities is imbalanced across the training subsets?
- Can the proposed method be extended to a self-supervised learning framework?
- How does the proposed method compare to other state-of-the-art multimodal learning methods in terms of computational efficiency?

## Limitations
- Effectiveness of pseudo-supervision mechanism depends heavily on stability of unimodal predictions across training epochs
- Feature projection module details are underspecified, with architectural choices potentially impacting performance significantly
- No ablation studies on loss weights to demonstrate robustness to hyperparameter choices

## Confidence
- **High confidence**: The core idea of feature projection into common space enabling flexible modality combination works mechanistically
- **Medium confidence**: Dual branch approach with pseudo-supervision reduces overfitting, but effectiveness depends heavily on pseudo-label quality
- **Medium confidence**: Generalization to unseen modality combinations is demonstrated but could be task-specific

## Next Checks
1. Conduct ablation studies on pseudo-supervision - compare dual branch performance against single branch with strong regularization to isolate the benefit
2. Test feature projection module on a simple synthetic multimodal dataset where groundtruth common space structure is known
3. Evaluate robustness to noisy modalities by systematically degrading each modality and measuring performance impact on unseen combinations