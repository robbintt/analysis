---
ver: rpa2
title: 'KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal
  Projection and Prototype-based One-Versus-All'
arxiv_id: '2311.15414'
source_url: https://arxiv.org/abs/2311.15414
tags:
- task
- learning
- tasks
- koppa
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KOPPA improves prompt-based continual learning by addressing feature
  shift and classification head misalignment. It introduces a key-query orthogonal
  projection inspired by MAML to ensure new task keys are perpendicular to past task
  queries, preventing prompt representation mismatch between training and inference.
---

# KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All

## Quick Facts
- **arXiv ID**: 2311.15414
- **Source URL**: https://arxiv.org/abs/2311.15414
- **Reference count**: 29
- **Key outcome**: KOPPA achieves up to 20% accuracy improvement over state-of-the-art methods in class-incremental continual learning benchmarks.

## Executive Summary
KOPPA addresses the challenge of feature shift and classification head misalignment in prompt-based continual learning. It introduces a key-query orthogonal projection mechanism inspired by MAML to ensure new task keys remain perpendicular to past task queries, preventing prompt representation mismatch. Additionally, KOPPA employs a prototype-based One-Versus-All (OVA) component to strengthen task-specific classification head distinctions. The method demonstrates superior performance on benchmark datasets like CIFAR-100 and ImageNet-R splits, achieving state-of-the-art results without storing raw data.

## Method Summary
KOPPA is a continual learning method that improves prompt-based learning by preventing feature shift and enhancing classification head distinction. It uses key-query orthogonal projection to constrain new task keys to be perpendicular to past task queries, ensuring stable prompt representations across tasks. The method also incorporates a prototype-based OVA component that strengthens the separation between classification heads for different tasks. During training, KOPPA learns prompts and keys while storing prototypes for each task, and during inference, it synthesizes prompts using attention weights computed with all keys and employs the OVA component to boost correct classification head activation.

## Key Results
- Achieves up to 20% accuracy improvement over state-of-the-art methods on benchmark datasets
- Effectively reduces catastrophic forgetting in class-incremental learning scenarios
- Outperforms methods like CODA, DualPrompt, and ER without requiring raw data storage
- Demonstrates superior results in maintaining model stability and performance across multiple tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KOPPA prevents prompt representation mismatch between training and inference by enforcing orthogonality between new task keys and past task queries.
- Mechanism: The method uses a key-query orthogonal projection inspired by MAML. When learning task t, new keys Kt are constrained to be perpendicular to the subspace Qt-1 spanned by query vectors from tasks 1 to t-1. This ensures that prompts for past tasks remain unchanged during inference.
- Core assumption: Feature shift occurs because prompts used during training differ from those used during inference when new tasks are learned.
- Evidence anchors:
  - [abstract]: "introduce a novel key-query learning strategy based on orthogonal projection... to enhance prompt matching efficiency and address the challenge of shifting features"
  - [section 3.3]: "We explicitly enforce the constraints: q(x) ⊥ Kt 1:M or γ(q(x), Kt 1:M) = 0 for every x from every past task"
- Break condition: If the orthogonality constraint is not properly enforced or the subspace Qt-1 is not accurately computed, feature shift could still occur.

### Mechanism 2
- Claim: KOPPA strengthens task classification head distinction using a prototype-based One-Versus-All (OVA) component.
- Mechanism: For each task, N prototypes represent feature vectors. An OVA loss is computed where for each class, the model predicts both in-distribution and out-distribution probabilities. This helps ensure that test samples activate the correct task's classification head.
- Core assumption: Feature shift can cause test samples to trigger wrong classification heads, leading to poor performance.
- Evidence anchors:
  - [abstract]: "we introduce a One-Versus-All (OVA) prototype-based component that enhances the classification head distinction"
  - [section 3.4]: "Using this strategy, we obtain a compact set of prototypes across the tasks with the total size N × T × d"
- Break condition: If prototypes are not representative or the OVA loss is not properly integrated, the classification head distinction may not improve.

### Mechanism 3
- Claim: KOPPA achieves state-of-the-art performance by combining orthogonal projection and OVA components.
- Mechanism: The two components are complementary - orthogonal projection prevents feature shift while OVA ensures correct classification head activation. Together they address both problems that cause catastrophic forgetting in prompt-based continual learning.
- Core assumption: The two mechanisms address different aspects of the continual learning problem and their combination provides superior results.
- Evidence anchors:
  - [abstract]: "Experimental results on benchmark datasets demonstrate that our method empowers the model to achieve results surpassing those of current state-of-the-art approaches by a large margin of up to 20%"
  - [section 4.2]: "KOPPA surpasses other methods based on prompt tuning across all considered cases. Our method achieves the most impressive results when exhibiting a substantial performance advantage exceeding 20%"
- Break condition: If either component fails to work as intended, the overall performance gain may be reduced.

## Foundational Learning

- Concept: Continual Learning (CL)
  - Why needed here: KOPPA is specifically designed to address the catastrophic forgetting problem in continual learning scenarios
  - Quick check question: What is the fundamental challenge that continual learning methods aim to solve?

- Concept: Prompt Tuning
  - Why needed here: KOPPA builds upon prompt tuning techniques applied to large pre-trained vision transformers
  - Quick check question: How does prompt tuning differ from traditional fine-tuning in terms of parameter efficiency?

- Concept: Orthogonal Projection
  - Why needed here: The key-query orthogonal projection mechanism relies on understanding how to project vectors onto subspaces
  - Quick check question: What mathematical operation ensures that two vectors are perpendicular to each other?

## Architecture Onboarding

- Component map: ViT backbone -> Classification head -> Prompts and Keys -> Orthogonal projection and OVA components
- Critical path: During training of task t, compute query vectors, enforce orthogonality constraint on new keys, optimize prompts and keys, store prototypes for OVA. During inference, compute attention weights with all keys, synthesize prompts, and use OVA to boost correct classification head.
- Design tradeoffs: The method trades additional memory for prototypes and basis vectors against improved performance and reduced forgetting. The orthogonal constraint may limit the model's ability to adapt positively to new tasks.
- Failure signatures: If prototypes are not representative, OVA may not help. If subspace computation is inaccurate, orthogonality may not be properly enforced. If attention weights are not computed correctly, prompt synthesis will be wrong.
- First 3 experiments:
  1. Implement the orthogonal projection mechanism on a simple continual learning benchmark and verify that feature shift is reduced compared to baseline
  2. Add the OVA component and verify that classification head triggering accuracy improves
  3. Combine both components and measure performance on standard continual learning benchmarks (CIFAR-100, ImageNet-R splits)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of prototypes per task affect the performance of the OVA component in KOPPA?
- Basis in paper: [explicit] The paper mentions that increasing the number of prototypes generally improves performance, but the impact varies across datasets and the exact relationship is not quantified.
- Why unresolved: The paper provides some insights but lacks a detailed analysis of how the number of prototypes affects different aspects of model performance.
- What evidence would resolve it: A comprehensive study varying the number of prototypes and analyzing the resulting impact on accuracy, forgetting, and feature separation across different datasets.

### Open Question 2
- Question: Can the key-query orthogonal projection strategy be extended to other types of neural networks beyond ViT?
- Basis in paper: [inferred] The paper focuses on ViT but mentions that prompt tuning has been applied to other models, suggesting potential applicability.
- Why unresolved: The paper does not explore the application of this strategy to other architectures, leaving its generalizability unclear.
- What evidence would resolve it: Experiments applying the key-query orthogonal projection to different types of neural networks (e.g., CNNs, RNNs) and comparing the results with baseline methods.

### Open Question 3
- Question: What is the impact of the orthogonal projection strategy on positive backward transfer?
- Basis in paper: [explicit] The paper mentions that the orthogonal projection strategy could hinder positive backward transfer but does not provide a detailed analysis.
- Why unresolved: The paper only briefly mentions this potential issue without exploring its implications or providing mitigation strategies.
- What evidence would resolve it: Experiments designed to specifically measure positive backward transfer with and without the orthogonal projection strategy, along with analysis of when and why it might occur.

## Limitations

- The orthogonal projection mechanism's effectiveness depends on accurate subspace computation, which may become unstable with a large number of tasks
- The method requires storing basis vectors, prototypes, and classification heads for all past tasks, potentially leading to significant memory usage
- The orthogonal constraint may limit the model's ability to adapt positively to new tasks, potentially hindering positive backward transfer

## Confidence

**High Confidence**: The core mechanism of key-query orthogonal projection is mathematically sound and well-explained. The basic principle that enforcing orthogonality between new task keys and past task queries should prevent feature shift is clearly articulated and supported by the mathematical formulation.

**Medium Confidence**: The experimental results showing superior performance compared to baselines like CODA, DualPrompt, and ER are compelling, but the paper doesn't provide extensive ablation studies to isolate the individual contributions of the orthogonal projection versus the OVA component. The 20% improvement figure is impressive but may be dataset-dependent.

**Low Confidence**: The scalability claims regarding memory efficiency are somewhat vague. While the method avoids storing raw data, it requires storing basis vectors, prototypes, and classification heads for all past tasks, and the paper doesn't provide a detailed analysis of memory growth as the number of tasks increases.

## Next Checks

1. **Ablation Study**: Conduct experiments isolating the orthogonal projection mechanism from the OVA component to quantify their individual contributions to performance gains. This would involve testing variants with only orthogonal projection, only OVA, and comparing against the full KOPPA method.

2. **Scalability Analysis**: Systematically vary the number of tasks and measure both performance degradation and memory requirements. This should include stress-testing the method with 50+ tasks to understand when the orthogonal projection mechanism becomes unstable or the OVA component becomes ineffective.

3. **Prototype Sensitivity**: Experiment with different prototype selection strategies (random sampling vs. k-means clustering vs. representative sampling) and analyze how prototype quality affects the OVA component's performance. This would help understand the robustness of the method to prototype selection and whether there's a minimum prototype quality threshold for the OVA mechanism to be effective.