---
ver: rpa2
title: Improving Differentiable Architecture Search via Self-Distillation
arxiv_id: '2302.05629'
source_url: https://arxiv.org/abs/2302.05629
tags:
- supernet
- architecture
- search
- darts
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Distillation DARTS (SD-DARTS), a method
  to improve differentiable architecture search by reducing the sharpness of the supernet
  loss landscape. The core idea is to use self-distillation, where the supernet at
  the previous time step guides the training of the current supernet, minimizing the
  loss difference between consecutive iterations.
---

# Improving Differentiable Architecture Search via Self-Distillation

## Quick Facts
- arXiv ID: 2302.05629
- Source URL: https://arxiv.org/abs/2302.05629
- Authors: 
- Reference count: 40
- One-line primary result: SD-DARTS achieves 2.58% test error on CIFAR-10 and 25.0% on ImageNet

## Executive Summary
This paper introduces Self-Distillation DARTS (SD-DARTS), a method to improve differentiable architecture search by reducing the sharpness of the supernet loss landscape. The core idea is to use self-distillation, where the supernet at the previous time step guides the training of the current supernet, minimizing the loss difference between consecutive iterations. Additionally, the paper proposes voted teachers, which aggregate predictions from multiple previous supernets to provide more informative guidance. Experimental results show that SD-DARTS achieves state-of-the-art performance on CIFAR-10 and ImageNet, with a test error of 2.58% on CIFAR-10 and 25.0% on ImageNet. The method effectively bridges the performance gap between the supernet and the optimal architecture, demonstrating the advantages of the novel self-distillation-based NAS approach.

## Method Summary
SD-DARTS improves differentiable architecture search by introducing self-distillation to reduce the sharpness of the supernet loss landscape. The method uses the supernet at the previous time step as a teacher to guide the training of the current supernet, minimizing the KL divergence between their predictions. Additionally, voted teachers aggregate predictions from multiple previous supernets to provide more informative guidance. The approach is evaluated on CIFAR-10 and ImageNet, showing state-of-the-art performance with a test error of 2.58% on CIFAR-10 and 25.0% on ImageNet.

## Key Results
- SD-DARTS achieves 2.58% test error on CIFAR-10, surpassing previous state-of-the-art methods
- On ImageNet, SD-DARTS achieves 25.0% test error, demonstrating strong generalization capabilities
- The method effectively reduces the sharpness of the supernet loss landscape, bridging the performance gap between the supernet and the optimal architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation reduces the sharpness of the supernet loss landscape.
- Mechanism: By using the supernet at the previous time step as a teacher, the current supernet is trained to minimize the difference between consecutive iterations, which is equivalent to minimizing the sharpness of the loss landscape.
- Core assumption: The change in the supernet's validation loss between consecutive iterations is proportional to the square of the sharpness loss.
- Evidence anchors:
  - [abstract]: "utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet's loss"
  - [section]: "minimizing the loss difference for the two consecutive iterations, i.e., the sharpness of the supernet's loss landscape"
  - [corpus]: Weak evidence. The corpus papers discuss related DARTS improvements but do not specifically mention self-distillation for sharpness reduction.
- Break condition: If the teacher supernet is of poor quality due to insufficient warm-up or over-optimization, it may misguide the training of the current supernet, leading to increased sharpness.

### Mechanism 2
- Claim: Voted teachers provide more abundant information than a single teacher.
- Mechanism: Multiple previous supernets are selected as teachers, and their output probabilities are aggregated through voting to generate the final teacher prediction, providing richer information for guiding the current supernet.
- Core assumption: The ensemble predictions of several models are more accurate than the prediction of a single model.
- Evidence anchors:
  - [abstract]: "Furthermore, we introduce the concept of voting teachers, which aggregate predictions from multiple previous supernets to provide more informative guidance"
  - [section]: "Ensemble learning proposes that the ensemble predictions of several models are more accurate than the prediction of a single model"
  - [corpus]: Weak evidence. The corpus papers discuss various DARTS improvements but do not specifically mention the use of voted teachers.
- Break condition: If the time window K is too large, the supernets in earlier time steps may be too far from the current supernet, leading to misalignment and potential misguidance.

### Mechanism 3
- Claim: Self-distillation with KL divergence as a regularization term improves the smoothness of the supernet loss landscape.
- Mechanism: The KL divergence between the student and teacher output probabilities is used as a regularization term, encouraging the current supernet to produce similar predictions to the previous supernet, thus smoothing the loss landscape.
- Core assumption: Minimizing the KL divergence between consecutive supernet predictions is equivalent to minimizing the sharpness of the loss landscape.
- Evidence anchors:
  - [abstract]: "SD-DARTS can minimize the loss difference for the two consecutive iterations so that minimize the sharpness of the supernet's loss"
  - [section]: "we utilize the correlation metric H to calculate the correlation between teacher output probability and student output probability. There are many correlation metrics can be used... In the paper, our fundament task is image classification, and then we use KL as the metric H"
  - [corpus]: Weak evidence. The corpus papers discuss various DARTS improvements but do not specifically mention the use of KL divergence for self-distillation.
- Break condition: If the regularization coefficient λ is not properly tuned, it may either have no effect or overly constrain the supernet, leading to suboptimal performance.

## Foundational Learning

- Concept: Bilevel optimization in DARTS
  - Why needed here: Understanding the bilevel optimization problem is crucial for grasping how SD-DARTS modifies the training process.
  - Quick check question: What are the two levels of optimization in DARTS, and how does SD-DARTS modify them?

- Concept: Knowledge distillation
  - Why needed here: Self-distillation is a key component of SD-DARTS, and understanding the general concept of knowledge distillation is essential.
  - Quick check question: How does self-distillation differ from traditional knowledge distillation, and why is it used in SD-DARTS?

- Concept: Sharpness-aware minimization (SAM)
  - Why needed here: SAM is related to the concept of minimizing sharpness in the loss landscape, which is the goal of SD-DARTS.
  - Quick check question: How does SAM aim to improve generalization, and how is this concept applied in SD-DARTS?

## Architecture Onboarding

- Component map:
  - Supernet -> Architecture parameters -> Network parameters -> Teacher supernet -> Voted teachers

- Critical path:
  1. Warm up the supernet for a sufficient number of epochs.
  2. For each training epoch:
     a. Use the previous supernet as the teacher.
     b. Calculate the KL divergence between student and teacher predictions.
     c. Update architecture and network parameters using the combined loss.
     d. If using voted teachers, aggregate predictions from multiple previous supernets.

- Design tradeoffs:
  - Warm-up epochs: Insufficient warm-up leads to poor teacher quality; excessive warm-up may over-optimize the supernet.
  - Time window K: Too small provides limited information; too large introduces misalignment.
  - Regularization coefficient λ: Too low has no effect; too high overly constrains the supernet.

- Failure signatures:
  - Poor teacher quality due to insufficient warm-up.
  - Misalignment between current and previous supernets when using large time windows.
  - Over-constraining of the supernet due to high regularization coefficient.

- First 3 experiments:
  1. Run SD-DARTS with a single teacher (K=1) and vary the warm-up epochs to find the optimal value.
  2. Compare the performance of SD-DARTS with and without voted teachers (K=1 vs K=2).
  3. Analyze the effect of different regularization coefficients λ on the smoothness of the loss landscape and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SD-DARTS vary with different values of the regularization coefficient λ, and what is the optimal range for this hyperparameter?
- Basis in paper: [explicit] The paper mentions that the regularization coefficient λ is empirically set as 1.0 to balance classification loss and correlation loss, but does not explore its impact on performance.
- Why unresolved: The paper does not provide a systematic study of how different values of λ affect the performance of SD-DARTS.
- What evidence would resolve it: A series of experiments varying λ and reporting the corresponding performance metrics would clarify its impact.

### Open Question 2
- Question: Can the voted teachers approach be extended to include supernets from earlier time steps with a decaying weight to incorporate more historical information?
- Basis in paper: [inferred] The paper discusses the limitations of using supernets from earlier time steps due to their weaker correlation with the current supernet, but does not explore methods to mitigate this issue.
- Why unresolved: The paper does not investigate methods to incorporate information from earlier supernets in a way that accounts for their weaker correlation.
- What evidence would resolve it: Experiments comparing the performance of SD-DARTS with and without a decaying weight for earlier supernets would demonstrate the effectiveness of this approach.

### Open Question 3
- Question: How does the performance of SD-DARTS compare to other regularization techniques, such as Sharpness-Aware Minimization (SAM), when applied to DARTS?
- Basis in paper: [explicit] The paper discusses the use of self-distillation as a regularization technique to improve the smoothness of the supernet's loss landscape, but does not compare it to other regularization methods.
- Why unresolved: The paper does not provide a direct comparison between SD-DARTS and other regularization techniques like SAM.
- What evidence would resolve it: Experiments comparing the performance of SD-DARTS with DARTS augmented with other regularization techniques would provide insights into their relative effectiveness.

## Limitations

- The effectiveness of voted teachers is highly dependent on the choice of time window K and the quality of stored teacher supernets.
- The direct relationship between KL divergence minimization and sharpness reduction requires further empirical validation.
- The method relies on hyperparameter tuning for optimal performance, which may limit its practical applicability.

## Confidence

- Claim: SD-DARTS effectively reduces the sharpness of the supernet loss landscape
  - Evidence: Moderate
  - Confidence: Medium
- Claim: Voted teachers provide more informative guidance than a single teacher
  - Evidence: Moderate
  - Confidence: Medium
- Claim: SD-DARTS achieves state-of-the-art performance on CIFAR-10 and ImageNet
  - Evidence: Strong
  - Confidence: High

## Next Checks

1. Conduct ablation studies to determine the optimal warm-up epochs and regularization coefficient λ for SD-DARTS on CIFAR-10 and ImageNet.
2. Analyze the effect of different time window sizes K on the performance of voted teachers and the quality of the aggregated predictions.
3. Compare the sharpness of the loss landscape (measured by Hessian norm) between standard DARTS and SD-DARTS to empirically validate the claim of reduced sharpness.