---
ver: rpa2
title: 'ChatGPT4PCG Competition: Character-like Level Generation for Science Birds'
arxiv_id: '2303.15662'
source_url: https://arxiv.org/abs/2303.15662
tags:
- prompt
- level
- chatgpt
- levels
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ChatGPT4PCG Competition at the 2023 IEEE Conference on Games
  tasked participants with designing effective prompts to generate Science Birds levels
  resembling English capital letters. The competition focused on stability and similarity
  as key evaluation metrics.
---

# ChatGPT4PCG Competition: Character-like Level Generation for Science Birds

## Quick Facts
- arXiv ID: 2303.15662
- Source URL: https://arxiv.org/abs/2303.15662
- Reference count: 38
- Primary result: Detailed stability explanations in prompts yielded best performance with 30.40 average normalized score

## Executive Summary
The ChatGPT4PCG Competition challenged participants to design prompts that generate stable and character-like levels for the Science Birds game platform. The competition focused on English capital letters as target shapes, using stability (block retention after 10 seconds) and similarity (classification accuracy) as key evaluation metrics. An initial experiment with modified versions of a sample prompt revealed that prompts with detailed stability explanations performed best, highlighting the importance of carefully crafted prompts that balance both stability and similarity. The competition aims to inspire creativity in prompt engineering for procedural content generation and demonstrate the potential of LLMs in game design.

## Method Summary
The competition used the Science Birds platform where participants designed prompts (≤900 words) to generate levels resembling English capital letters. Prompts must follow specific syntax rules using the `ab_drop()` function to place blocks vertically. Evaluation used 10 trials per letter, measuring stability (proportion of blocks remaining after 10 seconds) and similarity (fine-tuned vision transformer classification). Participants interacted with ChatGPT only once per letter. The competition provided baseline scripts for evaluation and encouraged submissions focusing on prompt engineering techniques.

## Key Results
- Variant with detailed stability explanations achieved highest performance (30.40 average normalized score)
- Stability and similarity metrics successfully differentiated prompt quality
- More challenging characters showed greater variation in performance across prompt variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt structure directly influences ChatGPT's ability to generate stable and character-like levels.
- Mechanism: The sample prompt is organized into four main sections: instructions, definitions, environment, and tools. Each section provides ChatGPT with specific information needed to generate the desired output. The instructions section tells ChatGPT what to do, the definitions section clarifies the construction space, the environment section defines the available block types, and the tools section specifies how to use the `ab_drop()` function.
- Core assumption: ChatGPT can understand and follow the instructions provided in the prompt, and that the prompt structure is sufficient to guide the generation of stable and character-like levels.
- Evidence anchors:
  - [abstract] "The experiment using modified versions of a sample prompt showed that the variant with detailed stability explanations performed best..."
  - [section] "The sample prompt, shown in Fig. 3, contains four main sections: (1) instructions, (2) definitions, (3) environments, and (4) tools."
  - [corpus] "ChatGPT4PCG Competition: Character-like Level Generation for Science Birds" - This paper provides evidence that prompt engineering can be used to generate game levels with specific characteristics.
- Break condition: If ChatGPT cannot understand the instructions or if the prompt structure is insufficient to guide the generation of stable and character-like levels.

### Mechanism 2
- Claim: The `ab_drop()` function is a critical component for generating stable levels in Science Birds.
- Mechanism: The `ab_drop()` function is used to vertically drop a block from the top and center it at a specific slot, denoted by the x position. This function shares similarities with the popular game Tetris. By defining the function in this manner, the competition organizers aim to reduce the chance of having overlapping blocks in the level.
- Core assumption: The `ab_drop()` function is sufficient to generate stable levels in Science Birds.
- Evidence anchors:
  - [abstract] "Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability."
  - [section] "The ab drop() function is designed to vertically drop a block from the top and center it at a specific slot, denoted by x position."
  - [corpus] "ChatGPT4PCG Competition: Character-like Level Generation for Science Birds" - This paper provides evidence that the `ab_drop()` function can be used to generate game levels with specific characteristics.
- Break condition: If the `ab_drop()` function is not sufficient to generate stable levels in Science Birds, or if the function is not implemented correctly.

### Mechanism 3
- Claim: The evaluation metrics used in the competition encourage participants to design prompts that work well on all characters for both stability and similarity.
- Mechanism: The competition uses two evaluation metrics: stability and similarity. Stability measures how well the blocks in a level can withstand the in-game gravity, while similarity measures how similar a generated level is to a target character. The metrics are designed to give more weight to difficult characters, those that a majority of prompts do not perform well in terms of both stability and similarity.
- Core assumption: The evaluation metrics are effective in encouraging participants to design prompts that work well on all characters for both stability and similarity.
- Evidence anchors:
  - [abstract] "The competition focuses on stability and similarity as key evaluation metrics."
  - [section] "The prompts will be scored using the criteria outlined in the scoring policy, which is provided below."
  - [corpus] "ChatGPT4PCG Competition: Character-like Level Generation for Science Birds" - This paper provides evidence that the evaluation metrics used in the competition are effective in encouraging participants to design prompts that work well on all characters for both stability and similarity.
- Break condition: If the evaluation metrics are not effective in encouraging participants to design prompts that work well on all characters for both stability and similarity.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the foundation of ChatGPT and are used to generate the game levels. Understanding how LLMs work is crucial for understanding how to design effective prompts.
  - Quick check question: What is the main difference between GPT-3.5 and other LLMs?

- Concept: Procedural Content Generation (PCG)
  - Why needed here: PCG is the process of using algorithms to generate game content, such as levels, characters, and stories. Understanding PCG is crucial for understanding how to design effective prompts for generating game levels.
  - Quick check question: What are the two critical elements of PCG for level generation?

- Concept: Prompt Engineering
  - Why needed here: Prompt engineering is the process of designing prompts to control the behavior of LLMs. Understanding prompt engineering is crucial for understanding how to design effective prompts for generating game levels.
  - Quick check question: What are the six categories of prompt patterns identified by White et al.?

## Architecture Onboarding

- Component map: ChatGPT model -> Science Birds platform -> Evaluation scripts (stability and similarity metrics) -> Prompt engineering process
- Critical path: Design prompt → Send to ChatGPT → Extract ab_drop() functions → Convert to XML → Evaluate stability and similarity → Score
- Design tradeoffs: Complexity vs. quality tradeoff (more complex prompts may yield higher quality levels but are harder to design and evaluate); Stability vs. similarity tradeoff (prompts optimized for stability may sacrifice similarity and vice versa)
- Failure signatures: Unstable levels (blocks collapse due to poor prompt design or ab_drop() implementation); Levels not similar to target characters (poor prompt design or inadequate evaluation metrics)
- First 3 experiments:
  1. Test the sample prompt to see if it generates stable and character-like levels.
  2. Modify the sample prompt to improve its performance on stability or similarity.
  3. Test the modified prompt to see if it generates more stable or more character-like levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are different prompt engineering techniques (e.g., few-shot prompting, chain-of-thought prompting, self-consistency) for improving the stability and similarity of ChatGPT-generated Science Birds levels?
- Basis in paper: [explicit] The paper discusses various prompt engineering techniques like few-shot prompting, chain-of-thought prompting, and self-consistency, and encourages participants to experiment with these techniques.
- Why unresolved: The paper presents an initial experiment with modified versions of a sample prompt, but does not comprehensively evaluate the impact of different prompt engineering techniques on the final results.
- What evidence would resolve it: A systematic study comparing the performance of different prompt engineering techniques on generating stable and character-like Science Birds levels, using a consistent evaluation framework.

### Open Question 2
- Question: What are the limitations of using ChatGPT for procedural content generation in more complex games beyond Science Birds?
- Basis in paper: [inferred] The paper focuses on generating simple character-like levels in Science Birds, but mentions the potential for ChatGPT to generate more complex content like word-like, object-like, and image-like levels.
- Why unresolved: The paper does not explore the challenges and limitations of applying ChatGPT to more complex game genres and content types.
- What evidence would resolve it: Case studies and experiments applying ChatGPT to procedural content generation in different game genres, analyzing the strengths and weaknesses of the approach.

### Open Question 3
- Question: How can the evaluation metrics for stability and similarity be further refined to better capture the quality and playability of generated game levels?
- Basis in paper: [explicit] The paper describes the evaluation metrics used in the competition, but acknowledges that they may need to be adjusted based on the number of participating teams and the specific characteristics of the generated levels.
- Why unresolved: The current metrics focus on stability and similarity, but may not fully capture other important aspects of level quality, such as difficulty, diversity, and player engagement.
- What evidence would resolve it: User studies and expert evaluations of generated levels, comparing the results with the automated evaluation metrics to identify areas for improvement.

## Limitations
- Results specific to Science Birds platform and English capital letters
- Limited generalizability to other game engines or content types
- Single case study in prompt engineering for procedural content generation

## Confidence
- Prompt structure effectiveness: Medium - supported by experimental results but platform-specific
- ab_drop() function sufficiency: Medium - functional for Science Birds but not tested elsewhere
- Evaluation metric effectiveness: High - clear quantitative differentiation between prompt variants

## Next Checks
1. Replicate the competition evaluation pipeline with different target shapes beyond English capital letters to test generalizability.
2. Test the winning prompt variants on alternative game engines with different physics systems to assess platform dependency.
3. Conduct ablation studies removing specific prompt sections to quantify their individual contributions to stability and similarity outcomes.