---
ver: rpa2
title: 'Edit at your own risk: evaluating the robustness of edited models to distribution
  shifts'
arxiv_id: '2303.00046'
source_url: https://arxiv.org/abs/2303.00046
tags:
- editing
- task
- edited
- ne-tuning
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of edited neural network
  models, addressing the problem that while model editing methods enable computationally
  inexpensive, interpretable, post-hoc model modifications, little is known about
  how editing affects model robustness to distribution shifts. The authors employ
  techniques from deep learning robustness research to examine both the general robustness
  of edited models and the robustness of specific behaviors targeted by the edits.
---

# Edit at your own risk: evaluating the robustness of edited models to distribution shifts

## Quick Facts
- arXiv ID: 2303.00046
- Source URL: https://arxiv.org/abs/2303.00046
- Reference count: 31
- Key outcome: Model editing methods generally reduce robustness to distribution shifts, but more constrained techniques that modify fewer parameters preserve more robustness.

## Executive Summary
This paper investigates how model editing techniques affect the robustness of neural networks to distribution shifts. While model editing offers computationally inexpensive, interpretable ways to modify pre-trained models post-hoc, the authors find that these edits typically reduce model robustness. The degree of degradation depends strongly on the editing algorithm and which layers are modified, with more constrained techniques that update fewer parameters generally preserving more robustness. The paper introduces two new model editing algorithms—direct low-rank model editing and 1-layer interpolation (1-LI)—that exhibit strong generalization performance while maintaining better robustness than previous methods.

## Method Summary
The paper evaluates model editing robustness by applying various editing techniques (rewriting, direct low-rank editing, local/global fine-tuning, and 1-layer interpolation) to pre-trained models (VGG16, ResNet50, ViT, CLIP) on different editing tasks. They measure editing task accuracy, original task accuracy on distribution-shifted data (ImageNet-C, ImageNet-R, ImageNet-A), and editing task accuracy on distribution-shifted data. Weight-space interpolation is used to navigate the trade-off between editing task performance and original task robustness. The authors introduce direct low-rank editing as a simpler alternative to rewriting and 1-layer interpolation as a technique combining single-layer fine-tuning with weight interpolation.

## Key Results
- Edits generally reduce model robustness to distribution shifts, but the degree of degradation depends on the editing algorithm and layers chosen
- More constrained techniques that modify fewer model parameters preserve more robustness
- Direct low-rank editing and 1-layer interpolation (1-LI) exhibit strong generalization performance with less robustness degradation than previous methods
- Single-layer fine-tuning can outperform global fine-tuning on OOD tasks for certain editing layers and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model editing degrades general robustness, but more constrained techniques that modify fewer model parameters preserve more robustness.
- Mechanism: When editing updates a large fraction of model parameters, it disrupts the learned representations that provide robustness to distribution shifts. Techniques that restrict updates to fewer parameters preserve these representations better.
- Core assumption: The original model's robustness stems from distributed representations across many parameters, and preserving these representations is necessary for maintaining robustness after editing.
- Evidence anchors:
  - [abstract]: "We find that edits tend to reduce general robustness, but that the degree of degradation depends on the editing algorithm and layers chosen. In particular, robustness is best preserved by more constrained techniques that modify less of the model."
  - [section]: "Our first basic finding is that all editing methods result in negative OOD penalties, meaning the edited models tend to be less robust."
  - [corpus]: Weak - The related papers focus on knowledge retention and fine-tuning interactions rather than distribution shift robustness specifically.

### Mechanism 2
- Claim: Weight-space interpolation between original and edited weights allows navigation of the trade-off between editing task accuracy and original task robustness.
- Mechanism: Linearly interpolating between original and edited weights creates a continuum of models with varying degrees of editing task performance and original task robustness. This allows finding a balance point that preserves more robustness while still achieving editing goals.
- Core assumption: The interpolation path between original and edited weights maintains a smooth trade-off between the two objectives, allowing meaningful interpolation.
- Evidence anchors:
  - [abstract]: "Motivated by these observations, we introduce two new model editing algorithms, direct low-rank model editing and 1-layer interpolation (1-LI), which each exhibit strong generalization performance."
  - [section]: "We believe this is new, with the following exception: one can prove that if only the final (pre-logit) weights are updated and the pairwise loss function is convex (for example, cross entropy loss) then MLI holds."
  - [corpus]: Weak - The related papers do not discuss weight-space interpolation as a mechanism for balancing editing and robustness.

### Mechanism 3
- Claim: Single-layer fine-tuning can preserve more original task robustness than global fine-tuning for certain editing tasks.
- Mechanism: When editing only a single layer, the majority of the model's parameters remain unchanged, preserving the learned representations that contribute to robustness. Global fine-tuning modifies all parameters, potentially disrupting these representations more.
- Core assumption: The original model's robustness is largely encoded in the parameters not being edited, and preserving these parameters is sufficient to maintain most of the original robustness.
- Evidence anchors:
  - [abstract]: "Motivated by these observations, we introduce two new model editing algorithms, direct low-rank model editing and 1-LI, which each exhibit strong generalization performance."
  - [section]: "For all three tasks there are layers where single-layer fine-tuning provides higher ImageNet-R (and ImageNet-C/A in Figures 6 and 7) accuracy than global fine-tuning along some portion of the interpolation path."
  - [corpus]: Weak - The related papers focus on knowledge retention and fine-tuning interactions rather than comparing single-layer versus global fine-tuning for robustness.

## Foundational Learning

- Concept: Distribution shift and out-of-distribution (OOD) robustness
  - Why needed here: The paper's central question is how model editing affects robustness to distribution shifts. Understanding what distribution shift is and how it's measured is fundamental to interpreting the results.
  - Quick check question: What is the difference between in-distribution and out-of-distribution data, and why is OOD robustness important for deployed models?

- Concept: Model editing techniques and their parameter modification scope
  - Why needed here: The paper compares different editing techniques and finds that more constrained methods preserve more robustness. Understanding what these techniques are and how they differ in parameter modification scope is crucial.
  - Quick check question: How do rewriting, direct low-rank editing, and single-layer fine-tuning differ in terms of which model parameters they modify?

- Concept: Weight-space interpolation
  - Why needed here: The paper introduces 1-layer interpolation and shows that interpolating between original and edited weights can balance editing task performance and original task robustness. Understanding this technique is key to interpreting the results.
  - Quick check question: What is weight-space interpolation, and how does it allow balancing between original and edited model performance?

## Architecture Onboarding

- Component map:
  Data preparation -> Model editing -> Evaluation -> Analysis

- Critical path:
  1. Prepare editing task datasets and distribution-shifted validation sets
  2. Apply editing techniques to models
  3. Evaluate edited models on original and editing tasks, including distribution-shifted versions
  4. Analyze results to determine robustness degradation and effectiveness of different techniques

- Design tradeoffs:
  - Editing technique selection: More constrained techniques preserve more robustness but may be less effective at the editing task
  - Interpolation method: Weight-space interpolation allows balancing editing and original task performance but adds complexity
  - Distribution shift modeling: Using synthetic corruptions (ImageNet-C) versus natural distribution shifts (ImageNet-R, ImageNet-A) provides different insights but may not capture all real-world scenarios

- Failure signatures:
  - Editing task accuracy much lower than expected despite successful optimization
  - Original task accuracy on distribution-shifted data degrades more than expected
  - Interpolation curves not monotonic or not smooth
  - Computational resources exhausted due to large number of editing experiments

- First 3 experiments:
  1. Apply rewriting to a VGG16 model on the vehicles-on-snow task, evaluate on original and distribution-shifted validation sets
  2. Apply direct low-rank editing to a ViT model on the synthetic concept-style task, evaluate on original and distribution-shifted validation sets
  3. Apply 1-layer interpolation to a CLIP model on the MNIST task, evaluate interpolation curves for balancing original and editing task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed linear correlation between OOD robustness and in-distribution accuracy hold consistently across different editing algorithms, or are there systematic exceptions that could inform better editing strategies?
- Basis in paper: [inferred] The paper notes that while accuracy on OOD variants generally correlates with ImageNet accuracy, there are notable exceptions where single-layer edited models show better OOD performance than expected from this correlation alone.
- Why unresolved: The paper only observes this deviation in specific cases and doesn't systematically investigate when and why the correlation breaks down or what this means for selecting editing algorithms.
- What evidence would resolve it: A comprehensive study comparing OOD robustness across multiple editing methods while controlling for in-distribution accuracy, identifying patterns in when single-layer editing outperforms global fine-tuning on OOD tasks.

### Open Question 2
- Question: How does the choice of editing layer affect the trade-off between editing task performance and OOD robustness, and can this be predicted from the model architecture or task characteristics?
- Basis in paper: [explicit] The paper shows that different editing layers yield different performance curves in Figures 2 and 5, with some single-layer edits outperforming global fine-tuning on specific tasks, but doesn't provide a theoretical framework for predicting optimal layer selection.
- Why unresolved: The paper observes layer-dependent performance but doesn't investigate the underlying reasons or develop predictive models for layer selection based on task properties or network architecture.
- What evidence would resolve it: Systematic experiments varying both the editing layer and task characteristics across multiple model architectures, combined with analysis of feature representation changes at different layers during editing.

### Open Question 3
- Question: Does the mean-centering issue in the rewriting implementation significantly affect the observed robustness results, and how sensitive are different editing algorithms to implementation details?
- Basis in paper: [explicit] The paper acknowledges a discrepancy in their rewriting implementation regarding mean-centering of features, noting this could affect results but hasn't fully assessed the impact.
- Why unresolved: The authors haven't completed sensitivity analysis to determine if this implementation detail materially affects their conclusions about rewriting's relative performance.
- What evidence would resolve it: Re-running the key experiments with corrected mean-centering in rewriting and comparing results to other methods, particularly focusing on the robustness metrics where rewriting showed competitive performance.

## Limitations
- Robustness degradation may be task-specific rather than a general phenomenon across all editing scenarios
- Evaluation relies on synthetic and curated distribution shifts that may not fully represent real-world deployment scenarios
- The theoretical explanation for weight-space interpolation's effectiveness is limited

## Confidence
- **High Confidence**: The observation that edits generally reduce model robustness and that more constrained techniques preserve more robustness
- **Medium Confidence**: The effectiveness of the newly introduced techniques (direct low-rank editing and 1-layer interpolation)
- **Low Confidence**: The theoretical explanation for why weight-space interpolation allows balancing between editing and original task performance

## Next Checks
1. **Generalization Across Tasks**: Validate whether the robustness degradation pattern holds for additional editing tasks beyond concept-style modification and vehicles-on-snow, particularly tasks that require more substantial semantic changes to model behavior.
2. **Real-World Distribution Shifts**: Evaluate the edited models on real-world distribution shifts encountered in deployment (e.g., different camera sensors, lighting conditions, geographic locations) rather than synthetic corruptions to assess practical robustness implications.
3. **Mechanism Validation**: Conduct ablation studies to quantify the contribution of different parameter sets to original task robustness, directly testing whether the hypothesis about distributed representations is correct by systematically measuring robustness as different parameter subsets are modified.