---
ver: rpa2
title: 'ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning'
arxiv_id: '2307.01053'
source_url: https://arxiv.org/abs/2307.01053
tags:
- graph
- learning
- information
- contrastive
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ENGAGE introduces explanation-guided data augmentation for contrastive
  graph learning. It uses a smoothed activation map method to identify important graph
  components and then applies two augmentation strategies that preserve important
  information while perturbing unimportant parts.
---

# ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning

## Quick Facts
- arXiv ID: 2307.01053
- Source URL: https://arxiv.org/abs/2307.01053
- Reference count: 40
- Key outcome: Achieves 2.9% average accuracy improvement for graph classification and 2.7% for node classification over random perturbation baselines

## Executive Summary
ENGAGE introduces explanation-guided data augmentation for contrastive graph learning, using a Smoothed Activation Map (SAM) method to identify important graph components. The framework applies two augmentation strategies that preserve important information while perturbing unimportant parts, theoretically justified through information theory. Experiments show consistent performance improvements across multiple datasets and tasks, with average accuracy gains of 2.9% for graph classification and 2.7% for node classification. The method also demonstrates improved stability in representation learning compared to random perturbation baselines.

## Method Summary
ENGAGE combines explanation-guided augmentation with contrastive learning for graph representation learning. It uses SAM to identify node importance scores based on local representation distributions, then applies edge and feature perturbation strategies that preserve high-importance nodes/edges while perturbing low-importance ones. The method operates with two augmentation strategies: edge masking (preserving important connections while perturbing others) and feature perturbation (modifying node features based on importance scores). ENGAGE is compatible with SimCLR and SimSiam contrastive learning frameworks and can be applied to both graph-level and node-level tasks using appropriate GNN encoders.

## Key Results
- Average accuracy improvement of 2.9% for graph classification tasks
- Average accuracy improvement of 2.7% for node classification tasks
- Consistent outperformance of random perturbation baselines across multiple datasets
- Demonstrated improved stability in representation learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation-guided augmentation preserves task-relevant information while removing superfluous information in graph representations.
- Mechanism: SAM identifies node importance scores based on local representation distributions, using these scores to guide augmentation by preserving high-importance nodes/edges and perturbing low-importance ones.
- Core assumption: Nodes highlighted as important by SAM explanations in the latent space are also important for downstream tasks.
- Evidence anchors: Abstract states representations should "preserve key characteristics in data and abandon superfluous information"; section explains information recommended by explanation is given higher priority to be preserved.

### Mechanism 2
- Claim: Contrastive learning with explanation-guided views reduces mutual information between augmented views while preserving task-relevant information.
- Mechanism: By preserving important nodes/edges in both views and perturbing unimportant ones differently, views share task-relevant information but differ in superfluous details.
- Core assumption: Views that preserve shared task-relevant information while differing in other aspects create better contrastive learning signals than random perturbation.
- Evidence anchors: Abstract mentions "explanation guided graph augmentation method by perturbing edges and node features"; section discusses maintaining important connections intact while perturbing unimportant parts.

### Mechanism 3
- Claim: Local smoothing of explanation scores improves reliability by incorporating information from nearby nodes/graphs in the embedding space.
- Mechanism: SAM gathers importance scores from neighboring nodes/graphs in the latent space to create smoothed importance scores, reducing noise from individual instances.
- Core assumption: Important graph components show consistency across similar instances in the embedding space.
- Evidence anchors: Abstract introduces "Smoothed Activation Map (SAM)"; section explains explanation information is smoothed by considering nearby nodes and graphs.

## Foundational Learning

- Concept: Contrastive learning objectives and information bottleneck principle
  - Why needed here: Understanding how contrastive learning reduces superfluous information and why preserving task-relevant information matters
  - Quick check question: What is the difference between the mutual information I(X;z) and I(z;y) in terms of what information is preserved?

- Concept: Graph neural network architectures (GCN, GAT, GIN)
  - Why needed here: The paper applies ENGAGE to multiple GNN architectures, understanding their differences is crucial for implementation
  - Quick check question: How does the message passing mechanism differ between GCN and GAT, and how might this affect explanation methods?

- Concept: Graph augmentation strategies and their effects on graph structure
  - Why needed here: Understanding how different augmentation methods (edge/node dropping, feature masking) affect graph properties is key to designing explanation-guided augmentation
  - Quick check question: What graph properties are most vulnerable to edge perturbation, and which are more robust?

## Architecture Onboarding

- Component map: Input graph → GNN encoder → SAM explanation module → Augmentation mask generation → Two augmented views → Contrastive learning head (SimCLR/SimSiam) → Loss computation
- Critical path: Graph → GNN → SAM → Augmentation masks → View generation → Contrastive loss
- Design tradeoffs: More aggressive augmentation (higher λ values) may better remove superfluous information but risks breaking important structures; local smoothing radius affects SAM quality; choice between SimCLR and SimSiam affects stability and need for negative samples
- Failure signatures: Performance worse than random augmentation baseline suggests SAM isn't identifying useful importance signals; high variance in results indicates unstable explanation scores or augmentation parameters; degraded performance on specific graph types suggests domain mismatch
- First 3 experiments: 1) Run ENGAGE with λe=λf=0 (no perturbation) vs random augmentation baseline to verify baseline performance; 2) Sweep λe, λf values to find optimal thresholds for a target dataset, measuring both performance and explanation sparsity; 3) Compare SAM-based augmentation vs augmentation using alternative explanation methods (e.g., GNNExplainer) on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the threshold parameters θe and θf in ENGAGE's edge and feature perturbation strategies affect the quality of learned representations across different graph datasets?
- Basis in paper: [explicit] The paper discusses that "the optimal data augmentation threshold varies for different datasets" and explores how λe and λf combinations influence model performance.
- Why unresolved: The paper shows sensitivity varies across datasets but doesn't provide a systematic method for determining optimal thresholds for new, unseen datasets.
- What evidence would resolve it: A method for automatically determining optimal threshold values based on dataset characteristics, validated across diverse graph types and tasks.

### Open Question 2
- Question: Can the Smoothed Activation Map (SAM) explanation method be extended to handle dynamic or temporal graphs where node importance may change over time?
- Basis in paper: [inferred] SAM relies on the distribution of representations in the latent space and local smoothing, which could potentially adapt to temporal changes, but this extension is not explored.
- Why unresolved: The current SAM method is designed for static graphs, and extending it to temporal graphs would require addressing how to capture and smooth node importance over time.
- What evidence would resolve it: A modified SAM approach that incorporates temporal information and demonstrates improved performance on temporal graph datasets.

### Open Question 3
- Question: How does the performance of ENGAGE compare to explanation-guided augmentation methods in other domains, such as computer vision or natural language processing?
- Basis in paper: [explicit] The paper mentions that "more intelligent data augmentation methods have been proposed by leveraging the semantic information [22,35] or domain knowledge [4]" in other domains, but doesn't compare ENGAGE to such methods.
- Why unresolved: The paper focuses on graph data and doesn't explore how ENGAGE's explanation-guided approach compares to similar methods in other data modalities.
- What evidence would resolve it: Comparative experiments between ENGAGE and explanation-guided augmentation methods from other domains on a common set of tasks or datasets.

## Limitations
- The core assumption that unsupervised SAM explanations align with task-relevant importance remains largely unverified
- Local smoothing mechanism assumes meaningful structure in the embedding space neighborhood, which may not hold for all graph types
- Theoretical justification through information theory is presented but not rigorously proven in the context of graph data

## Confidence

- Mechanism 1 (Explanation-guided preservation): Medium - Performance gains are demonstrated but the alignment between unsupervised and task-relevant importance is assumed rather than verified
- Mechanism 2 (Contrastive learning with preserved/preserved views): High - The information-theoretic framework is well-established and the approach is consistent with contrastive learning principles
- Mechanism 3 (Local smoothing): Medium - The concept is sound but the effectiveness depends heavily on embedding space quality and neighborhood structure

## Next Checks

1. Conduct ablation studies to verify that SAM-identified important nodes correspond to truly task-relevant components by comparing with supervised importance scores
2. Test the method on graph types where the embedding space neighborhood assumption might fail (e.g., heterophilic graphs, graphs with community structure)
3. Analyze the mutual information between original and augmented views across different λ values to identify optimal perturbation levels that balance preservation and perturbation