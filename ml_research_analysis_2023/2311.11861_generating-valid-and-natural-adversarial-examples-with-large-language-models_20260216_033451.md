---
ver: rpa2
title: Generating Valid and Natural Adversarial Examples with Large Language Models
arxiv_id: '2311.11861'
source_url: https://arxiv.org/abs/2311.11861
tags:
- adversarial
- examples
- word
- attack
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the challenge of generating valid and natural
  adversarial examples for text classification models, which often fail to preserve
  semantic meaning, grammaticality, and human imperceptibility. The proposed method,
  LLM-Attack, leverages large language models (LLMs) to create adversarial examples
  through a two-stage process: word importance ranking and word synonym replacement.'
---

# Generating Valid and Natural Adversarial Examples with Large Language Models

## Quick Facts
- **arXiv ID**: 2311.11861
- **Source URL**: https://arxiv.org/abs/2311.11861
- **Reference count**: 32
- **Primary result**: LLM-Attack generates adversarial examples that outperform baselines on validity, naturalness, and attack success rate while preserving semantic meaning and grammaticality

## Executive Summary
This paper addresses the challenge of generating valid and natural adversarial examples for text classification models that often fail to preserve semantic meaning, grammaticality, and human imperceptibility. The proposed LLM-Attack method leverages large language models to create adversarial examples through a two-stage process: word importance ranking and word synonym replacement. Experimental results on Movie Review, IMDB, and Yelp Review Polarity datasets demonstrate that LLM-Attack outperforms baseline models in both automatic and human evaluations, achieving higher validity and naturalness in the generated adversarial examples.

## Method Summary
The method employs a two-stage approach to generate adversarial examples. First, it identifies the most vulnerable words using a masked language modeling strategy that calculates word importance scores. Second, it uses large language models to generate synonyms for these words while preserving semantic meaning, syntax, and grammar. The method applies additional constraints including named entity preservation, pronoun preservation, and universal sentence encoder threshold to prevent common failure modes in adversarial example generation.

## Key Results
- LLM-Attack achieves higher attack success rates compared to baseline models on all three datasets
- Human evaluations show significant improvements in validity, naturalness, and grammaticality of generated adversarial examples
- GPT-4 evaluations confirm the effectiveness of LLM-Attack in creating imperceptive adversarial examples
- The method maintains semantic similarity between original and modified sentences while successfully fooling target models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based synonym replacement produces adversarial examples that preserve both semantic meaning and grammatical correctness better than traditional word embedding or BERT-based approaches.
- Mechanism: Large language models (LLMs) like ChatGPT and ChatGLM are used to generate synonyms for vulnerable words, leveraging their advanced language understanding and generation capabilities.
- Core assumption: LLMs have superior capabilities in understanding context and generating grammatically correct text compared to static word embedding models or masked language models like BERT.
- Evidence anchors: [abstract] "Based on the exceptional capacity of language understanding and generation of large language models (LLMs), we propose LLM-Attack..." [section III-B] "After determining the vulnerable words to be perturbed, we iteratively obtain the synonyms of the words with LLMs..."

### Mechanism 2
- Claim: The two-stage process (word importance ranking followed by LLM-based synonym replacement) effectively identifies and modifies the most vulnerable words while maintaining overall sentence integrity.
- Mechanism: First, a masked language modeling strategy calculates word importance scores to identify the most vulnerable words. Then, LLMs generate synonyms for these words while preserving semantic meaning, syntax, and grammar.
- Core assumption: The masked language modeling approach can effectively identify words whose modification would most impact the model's prediction.
- Evidence anchors: [section III-A] "Under the black-box scenario, only the logits output by the target models... can be regarded as the supervision signal. To balance effectiveness and efficiency of word ranking, we adopt the masked language modeling (MLM) strategy..." [section III-B] "After determining the vulnerable words to be perturbed, we iteratively obtain the synonyms of the words with LLMs..."

### Mechanism 3
- Claim: Additional constraints (named entity preservation, pronoun preservation, and universal sentence encoder threshold) prevent common failure modes in adversarial example generation.
- Mechanism: The method applies spaCy NER to skip named entities during substitution, preserves pronouns by adding them to stop words, and uses USE to ensure semantic similarity between original and modified sentences.
- Core assumption: Named entities and pronouns are particularly sensitive to modification, and semantic similarity thresholds can prevent invalid substitutions.
- Evidence anchors: [section III-B.1] "As shown in Fig. 1, named entities (such as people, locations, and companies) are usually disregarded in prior adversarial attacks..." [section III-B.1] "Additionally, considering the fact that substitution of pronouns tends to cause grammatical errors..." [section III-B.1] "Although previous work concludes that the universal sentence encoder (USE) is insensitive to invalid word substitutions..."

## Foundational Learning

- Concept: Masked Language Modeling (MLM) for word importance ranking
  - Why needed here: To identify which words in a sentence are most vulnerable to modification without access to model gradients (black-box setting)
  - Quick check question: How does replacing a word with [UNK] token and measuring prediction change help identify word importance?

- Concept: Prompt engineering for LLM-based synonym generation
  - Why needed here: To guide LLMs to generate synonyms that preserve semantic meaning, syntax, and grammar rather than just any synonym
  - Quick check question: What key elements should be included in the prompt to ensure the LLM generates appropriate synonyms?

- Concept: Universal Sentence Encoder (USE) for semantic similarity
  - Why needed here: To verify that LLM-generated synonyms maintain semantic similarity between original and modified sentences
  - Quick check question: What threshold value for USE similarity is typically considered acceptable for maintaining semantic meaning?

## Architecture Onboarding

- Component map: Input sentence -> MLM word importance ranking -> LLM synonym generation -> Constraint application (NER, pronoun, USE) -> Adversarial example output
- Critical path: Input → MLM ranking → LLM synonym generation → Constraint application → Output
- Design tradeoffs:
  - LLM choice vs. performance: ChatGLM used, but other LLMs could be substituted
  - Synonym count (k=15) vs. efficiency: More synonyms provide better options but increase computational cost
  - Constraint strictness vs. attack success: Tighter constraints preserve quality but may reduce attack effectiveness
- Failure signatures:
  - Semantic drift: USE similarity falls below threshold
  - Grammatical errors: Human evaluation detects unnatural phrasing
  - Named entity corruption: NER detects modified entities
  - Pronoun issues: Sentence becomes grammatically incorrect
- First 3 experiments:
  1. Verify MLM word importance ranking correctly identifies vulnerable words by comparing with gradient-based methods on a small dataset
  2. Test LLM synonym generation with different prompt variations to find optimal configuration
  3. Validate constraint effectiveness by generating adversarial examples with and without each constraint and comparing quality metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LLM-Attack vary across different types of text classification tasks beyond sentiment analysis, such as named entity recognition or text summarization?
- Basis in paper: [explicit] The paper evaluates LLM-Attack on sentiment analysis tasks using the Movie Review (MR), IMDB, and Yelp Review Polarity datasets.
- Why unresolved: The study focuses solely on sentiment analysis, leaving the generalizability of LLM-Attack to other NLP tasks unexplored.
- What evidence would resolve it: Conducting experiments on diverse text classification tasks like named entity recognition, text summarization, and question answering to assess the performance of LLM-Attack.

### Open Question 2
- Question: What is the impact of incorporating structured knowledge from quality knowledge bases on the generation of adversarial examples at both word and sentence levels?
- Basis in paper: [explicit] The paper mentions future work on investigating the possibility of incorporating structured knowledge from quality knowledge bases in generating adversarial examples.
- Why unresolved: The paper does not explore this direction, leaving the potential benefits and challenges of using structured knowledge for adversarial example generation unclear.
- What evidence would resolve it: Conducting experiments that integrate structured knowledge into LLM-Attack and comparing the results with the current approach to evaluate improvements in validity and naturalness.

### Open Question 3
- Question: How does the performance of LLM-Attack change when evaluated against more advanced or larger language models as victims, such as GPT-4 or other state-of-the-art models?
- Basis in paper: [inferred] The paper evaluates LLM-Attack against BERT, a pre-trained language model, and mentions the potential of GPT-4 as an evaluator.
- Why unresolved: The study does not test LLM-Attack against more advanced or larger language models, which could reveal limitations or strengths of the approach.
- What evidence would resolve it: Performing adversarial attacks on models like GPT-4, RoBERTa, or other advanced PLMs to assess the robustness and adaptability of LLM-Attack.

## Limitations
- Limited ablation study of LLM dependency without comprehensive comparison across different LLM models, prompt variations, or synonym counts
- Constraint implementation ambiguity regarding exact implementation details and interaction during generation process
- Black-box assumption limitations that may not scale well to larger models or different architectures beyond BERT

## Confidence
- **High Confidence**: The LLM-Attack method successfully generates adversarial examples that outperform baseline methods on the tested datasets according to automatic and human evaluation metrics
- **Medium Confidence**: The claim that LLM-based synonym generation produces more valid and natural adversarial examples than traditional methods is supported by experimental results but needs more rigorous comparison
- **Low Confidence**: The specific threshold values (k=15 synonyms, USE similarity ≥0.9, word embedding distance ≤0.5) and their impact on attack effectiveness are not empirically justified

## Next Checks
1. Conduct constraint sensitivity analysis by systematically varying USE similarity and word embedding distance thresholds to determine optimal values
2. Replace ChatGLM with alternative LLMs (GPT-3.5, GPT-4, Claude) while keeping all other components constant to isolate LLM impact
3. Evaluate LLM-Attack against different victim models beyond BERT (RoBERTa, XLNet, GPT-based classifiers) to assess cross-model generalization