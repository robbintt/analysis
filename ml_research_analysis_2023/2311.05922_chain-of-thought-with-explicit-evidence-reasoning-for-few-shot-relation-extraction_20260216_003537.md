---
ver: rpa2
title: Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction
arxiv_id: '2311.05922'
source_url: https://arxiv.org/abs/2311.05922
tags:
- entity
- context
- relation
- cot-er
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses few-shot relation extraction (FSRE), which
  aims to identify the type of relationship between two entities in text using limited
  annotated data. The proposed method, CoT-ER (Chain-of-Thought with Explicit Evidence
  Reasoning), leverages large language models (LLMs) with in-context learning to generate
  explicit evidence for relation reasoning.
---

# Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction

## Quick Facts
- arXiv ID: 2311.05922
- Source URL: https://arxiv.org/abs/2311.05922
- Reference count: 40
- Few-shot relation extraction method using 3-step reasoning that achieves competitive performance with 0% training data compared to fully-supervised approaches

## Executive Summary
This paper addresses few-shot relation extraction (FSRE) by proposing CoT-ER, a Chain-of-Thought approach with Explicit Evidence Reasoning. The method leverages large language models with in-context learning to generate explicit evidence for relation reasoning, employing a 3-step process: inferring concept-level entities, extracting contextual evidence, and constructing coherent expressions. Experimental results demonstrate that CoT-ER achieves competitive performance compared to fully-supervised state-of-the-art approaches on FewRel1.0 and FewRel2.0 datasets, surpassing all training-free baselines.

## Method Summary
CoT-ER uses a 3-step reasoning framework: (1) infer concept-level entities corresponding to head and tail entities, (2) extract relevant contextual spans as evidence, and (3) construct a coherent expression combining entities and relation label. The method employs manual seed examples (approximately 100 for FewRel1.0) for human-Instructed reasoning and uses a similarity-based KNN retrieval module to select demonstrations. The approach is implemented using OpenAI's "text-davinci-003" model with temperature=0, and experiments are conducted on FewRel1.0 and FewRel2.0 datasets across various few-shot settings (5-way 1-shot, 5-way 5-shot, 10-way 1-shot, 10-way 5-shot).

## Key Results
- CoT-ER (0% training data) achieves competitive performance compared to fully-supervised state-of-the-art approaches (100% training data) on FewRel1.0 and FewRel2.0 datasets
- The method surpasses all training-free baselines in few-shot relation extraction tasks
- CoT-ER demonstrates greater stability and effectiveness in few-shot settings compared to previous in-context learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- LLMs possess pre-training knowledge that can be leveraged for relation extraction when properly activated
- The 3-step reasoning process activates relevant knowledge by requiring concept-level abstractions before final predictions
- Core assumption: LLM's pre-training corpus contains sufficient concept-level knowledge about entities and relationships
- Evidence: Abstract states LLM has considerable knowledge base; section 3.3 introduces novel 3-step reasoning framework

### Mechanism 2
- Semantic ambiguity in relation labels can be resolved through explicit evidence-based reasoning
- Requiring model to generate evidence spans and construct coherent expressions disambiguates between similar relation types
- Core assumption: Ambiguity stems from label semantics rather than contextual understanding
- Evidence: Abstract notes "sport" barely contains enough relation information; section 1 enforces label selection from provided options

### Mechanism 3
- In-context learning with carefully designed demonstrations is more effective than traditional few-shot approaches
- Similarity-based KNN retrieval selects demonstrations most relevant to query instance, maximizing information density within token limits
- Core assumption: Similarity in semantic embeddings correlates with relevance for relation extraction
- Evidence: Section 3.4 states selecting demonstrations based on similarity yields strong improvements

## Foundational Learning

- **Chain-of-thought prompting**: Enables step-by-step reasoning that mimics human-like inference for complex relation extraction. Why needed: Allows complex multi-step reasoning for relation extraction. Quick check: Can you explain why a 3-step reasoning process might be more effective than a single-step approach?

- **In-context learning**: Allows model to perform few-shot relation extraction without parameter updates, making it training-free. Why needed: Enables few-shot learning without model fine-tuning. Quick check: What's the key difference between in-context learning and traditional fine-tuning approaches?

- **Semantic embeddings and similarity**: Enables effective demonstration retrieval by measuring semantic similarity between support and query instances. Why needed: Allows selection of most relevant demonstrations. Quick check: Why might Euclidean distance in embedding space be a reasonable proxy for demonstration relevance?

## Architecture Onboarding

- **Component map**: Seed examples → CoT-ER generation → KNN retrieval → Prompt construction → LLM inference
- **Critical path**: Manual seed examples → CoT-ER reasoning generation → Instance retrieval using similarity → Prompt construction → LLM inference for relation prediction
- **Design tradeoffs**: Token limits vs. demonstration quality vs. retrieval accuracy
- **Failure signatures**: Poor performance on domain-specific relations, high variance across random seeds, sensitivity to demonstration selection
- **First 3 experiments**: 1) Compare Auto-CoT-ER vs Manual-CoT-ER performance on FewRel 1.0 validation set, 2) Test ablation of first two reasoning steps to quantify concept-level knowledge contribution, 3) Evaluate stability across different random seeds and demonstration counts

## Open Questions the Paper Calls Out

### Open Question 1
- How does CoT-ER performance scale with larger language models that have longer context windows and larger parameter counts?
- Basis: Authors mention limited budget restricted evaluation on superior LLMs with longer maximum input tokens
- Why unresolved: API costs prevented evaluation on larger language models
- What evidence would resolve it: Experimental results comparing CoT-ER performance across different sizes of language models (GPT-3.5, GPT-4) with varying context window lengths

### Open Question 2
- Can the seed examples in CoT-ER be optimized to further improve performance, and what would be the optimal design for these examples?
- Basis: Authors acknowledge seed examples haven't been optimized
- Why unresolved: Lack of resources to conduct extensive experiments on optimizing seed examples
- What evidence would resolve it: Systematic experiments varying quality, quantity, and design of seed examples to identify optimal configuration

### Open Question 3
- How does CoT-ER perform on datasets with relation descriptions (FewRel 1.0) versus datasets without descriptions (FewRel 2.0)?
- Basis: FewRel 1.0 provides relation descriptions while FewRel 2.0 does not
- Why unresolved: Authors did not conduct experiments testing incorporation of relation descriptions
- What evidence would resolve it: Experiments comparing CoT-ER performance with and without relation descriptions on both datasets

## Limitations
- Manual demonstration quality: The 100 hand-annotated seed examples are critical but not publicly available, making faithful reproduction difficult
- LLM dependency: Results tied to specific OpenAI model ("text-davinci-003") and API, limiting reproducibility and introducing cost barriers
- Semantic ambiguity scope: Paper claims CoT-ER resolves semantic ambiguity but doesn't fully quantify which relation types benefit most or when ambiguity remains unresolved

## Confidence

- **High**: The core claim that CoT-ER achieves competitive results with 0% training data compared to fully-supervised methods is well-supported by experimental results
- **Medium**: The claim that semantic ambiguity is the primary bottleneck for in-context learning in RE has reasonable theoretical grounding but limited empirical dissection
- **Low**: The assertion that concept-level knowledge universally exists in pre-trained LLMs for all relation types is not fully validated, especially for domain-specific relations

## Next Checks

1. **Ablation of demonstration types**: Run experiments comparing manual demonstrations, auto-generated demonstrations, and random demonstrations to quantify the contribution of high-quality reasoning examples

2. **Domain-specific performance analysis**: Test CoT-ER on relations from highly specialized domains (legal, medical) to evaluate limits of pre-training knowledge activation

3. **Prompt engineering sensitivity**: Systematically vary prompt templates and human instructions to measure stability of performance and identify critical prompt components