---
ver: rpa2
title: Submodular Reinforcement Learning
arxiv_id: '2307.13372'
source_url: https://arxiv.org/abs/2307.13372
tags:
- policy
- submodular
- state
- function
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces submodular reinforcement learning (SubRL),
  a framework for optimizing non-additive rewards that exhibit diminishing returns,
  modeled via submodular set functions. The authors show that SubRL is generally NP-hard
  to approximate, but propose a practical algorithm, SubPO, based on policy gradients
  that greedily maximizes marginal gains.
---

# Submodular Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.13372
- Source URL: https://arxiv.org/abs/2307.13372
- Reference count: 40
- Key outcome: Submodular reinforcement learning (SubRL) framework with constant-factor approximation guarantees under specific MDP conditions, demonstrated on diverse tasks including biodiversity monitoring and robotics.

## Executive Summary
This paper introduces SubRL, a framework for optimizing non-additive rewards exhibiting diminishing returns through submodular set functions. The authors show that standard RL approaches fail to exploit the diminishing returns property, while their proposed SubPO algorithm achieves constant-factor approximations under specific conditions. The method is demonstrated to be sample-efficient and scalable to high-dimensional state-action spaces across multiple task domains.

## Method Summary
The method introduces SubPO, a policy gradient-based algorithm that handles non-additive rewards by maximizing marginal gains rather than immediate rewards. The algorithm computes unbiased gradient estimates using a score function estimator with variance reduction through baseline subtraction. Under ϵ-Bandit SMDP conditions, the expected submodular reward becomes DR-submodular, enabling constant-factor approximation guarantees. The approach scales to high-dimensional continuous control tasks through neural network policy parameterizations.

## Key Results
- SubPO outperforms standard policy gradient methods on tasks with submodular rewards by exploiting diminishing returns
- Under ϵ-Bandit SMDP conditions, SubPO achieves constant-factor approximation guarantees (1-1/e)
- The method scales to high-dimensional state-action spaces in environments like car racing and Mujoco robotics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Submodular rewards can be optimized via policy gradient methods by decomposing trajectory return into marginal gains, which reduces variance and enables effective learning.
- Mechanism: The key innovation is replacing immediate reward with marginal gain at each step (F(sj+1|τ0:j)), which captures diminishing returns while maintaining an unbiased gradient estimate. This allows standard policy gradient techniques to work despite non-additivity.
- Core assumption: The marginal gain decomposition maintains unbiasedness and provides variance reduction compared to sparse trajectory rewards.
- Evidence anchors:
  - [abstract]: "SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains"
  - [section 4]: Theorem 2 proves the gradient estimator using marginal gains is unbiased and can incorporate baselines
  - [corpus]: No direct corpus evidence for this specific mechanism; relies on paper's theoretical derivation
- Break condition: If the baseline cannot adequately estimate cumulative marginal gains, variance reduction benefits disappear.

### Mechanism 2
- Claim: Under ϵ-Bandit SMDP conditions, the optimization landscape becomes DR-submodular, enabling constant-factor approximations.
- Mechanism: When transition probabilities are nearly deterministic (ϵ small), the expected submodular reward becomes DR-submodular in policy space, allowing application of continuous DR-submodular optimization theory.
- Core assumption: The ϵ-Bandit condition creates sufficient structure for DR-submodularity to emerge in the expected reward function.
- Evidence anchors:
  - [section 5]: Theorem 3 establishes that for ϵ-Bandit SMDP with horizon-dependent policies, J(π) is monotone DR-submodular
  - [section 5]: References Bian et al. [20] showing (1-1/e) approximation guarantees for DR-submodular maximization
  - [corpus]: No direct corpus evidence; theoretical construction based on specific MDP assumptions
- Break condition: If transitions have significant stochasticity (large ϵ), the DR-submodularity property breaks down.

### Mechanism 3
- Claim: Bounded curvature of submodular functions guarantees performance bounds relative to optimal non-Markovian policies.
- Mechanism: Curvature c quantifies deviation from modularity; when bounded away from 1, submodular rewards are "close enough" to modular that policy gradient methods achieve constant-factor approximations.
- Core assumption: The submodular function has bounded total curvature c ∈ (0,1), meaning marginal gains don't decrease too rapidly.
- Evidence anchors:
  - [section 5]: Proposition 3 states SUBPO guarantees J(π) ≥ (1-c)J(π⋆) for bounded curvature c
  - [section 2]: Defines curvature as c = 1 - minA,j∉A ∆(j|A)/F(j), with c=0 for modular functions
  - [corpus]: No direct corpus evidence; theoretical result based on curvature definition
- Break condition: If curvature approaches 1, the approximation guarantee degrades to zero.

## Foundational Learning

- Concept: Submodular set functions and diminishing returns
  - Why needed here: The entire framework relies on modeling rewards that exhibit diminishing returns, which is the defining property of submodular functions
  - Quick check question: If visiting state v provides marginal gain ∆(v|A) that decreases as A grows, is this submodular behavior?

- Concept: Policy gradient methods and score function estimators
  - Why needed here: SubPO is fundamentally a policy gradient algorithm that requires computing unbiased gradient estimates for non-differentiable reward functions
  - Quick check question: How does the score function estimator ∇θ log πθ(ai|si) relate to the REINFORCE algorithm?

- Concept: DR-submodularity in continuous domains
  - Why needed here: Under specific MDP assumptions, the expected reward becomes DR-submodular, enabling application of continuous optimization theory
  - Quick check question: What is the key difference between discrete submodularity and DR-submodularity in continuous domains?

## Architecture Onboarding

- Component map:
  Environment interface -> Policy network -> Marginal gain calculator -> Baseline estimator -> Optimizer

- Critical path:
  1. Sample trajectories using current policy
  2. Compute marginal gains for each state in trajectories
  3. Estimate policy gradient using Theorem 2 formula
  4. Update policy parameters via gradient ascent
  5. Repeat until convergence

- Design tradeoffs:
  - Tabular vs neural network policies: Tabular provides theoretical guarantees but doesn't scale; neural networks scale but lose some theoretical properties
  - Baseline choice: Different baselines affect variance and bias; cumulative marginal gains heuristic works well empirically
  - Horizon dependence: State-independent vs state-dependent policies affect DR-submodularity conditions

- Failure signatures:
  - Policy collapse to local optima: MODPO gets stuck in high-density regions while SubPO continues exploring
  - High variance gradients: If marginal gain estimates are noisy, learning becomes unstable
  - Slow convergence: DR-submodularity approximation degrades with increased stochasticity

- First 3 experiments:
  1. Compare SubPO-M vs MODPO on simple grid world with synthetic coverage rewards
  2. Test SubPO performance on deterministic vs stochastic versions of same environment
  3. Vary curvature parameter in synthetic submodular functions and measure approximation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SubRL algorithms scale with the dimensionality of the state and action spaces, especially in high-dimensional continuous control tasks?
- Basis in paper: [inferred] The paper mentions that SubPO scales well to high-dimensional state-action spaces, but does not provide a detailed analysis of the scaling behavior with increasing dimensionality.
- Why unresolved: The paper does not present a systematic study of the algorithm's performance as the dimensionality of the state and action spaces increases, leaving the scalability properties unclear.
- What evidence would resolve it: Experimental results showing the performance of SubPO on tasks with varying state and action space dimensions, along with a detailed analysis of the scaling behavior.

### Open Question 2
- Question: How do different policy parameterizations (e.g., RNNs, transformers) impact the performance of SubRL algorithms in tasks that require history-dependent policies?
- Basis in paper: [inferred] The paper mentions that autoregressive policies like RNNs or transformers can be used to capture history-dependence, but does not provide a detailed comparison of different policy parameterizations.
- Why unresolved: The paper does not present a comprehensive study of how different policy parameterizations affect the performance of SubRL algorithms in tasks that require history-dependent policies.
- What evidence would resolve it: Experimental results comparing the performance of SubPO with different policy parameterizations (e.g., RNNs, transformers) in tasks that require history-dependent policies.

### Open Question 3
- Question: How does the choice of baseline function in the policy gradient estimator impact the variance reduction and overall performance of SubRL algorithms?
- Basis in paper: [explicit] The paper mentions that different choices of baseline functions can be used in the policy gradient estimator, but does not provide a detailed analysis of their impact on variance reduction and performance.
- Why unresolved: The paper does not present a systematic study of how different baseline functions affect the variance reduction and overall performance of SubRL algorithms.
- What evidence would resolve it: Experimental results comparing the performance of SubPO with different baseline functions in terms of variance reduction and overall performance.

## Limitations
- Theoretical guarantees rely heavily on specific MDP structure assumptions (ϵ-Bandit condition) that may not hold in practice
- Empirical validation is primarily focused on relatively simple environments, scalability to truly high-dimensional tasks remains to be demonstrated
- The paper does not provide a systematic study of how algorithm performance scales with state and action space dimensionality

## Confidence

### High confidence
- The core algorithmic framework of SubPO and its relationship to policy gradient methods is well-established and theoretically sound

### Medium confidence
- The variance reduction benefits of using marginal gains in policy gradients are supported by the paper's analysis, though empirical validation is limited
- The ϵ-Bandit SMDP assumption and its implications for DR-submodularity are theoretically justified but may be restrictive in practice

## Next Checks

1. **Stress-test the DR-submodularity assumption**: Systematically vary the ϵ parameter in the ϵ-Bandit condition and measure how quickly the approximation guarantees degrade as stochasticity increases.

2. **Validate curvature bounds empirically**: For each experiment, measure the actual curvature of the reward function and verify whether it remains bounded away from 1, as required for the theoretical guarantees.

3. **Benchmark against non-submodular baselines**: Compare SubPO against standard RL methods (e.g., PPO, SAC) on environments with submodular vs non-submodular reward structures to quantify the benefit of the submodular approach.