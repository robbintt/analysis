---
ver: rpa2
title: Skill-Based Few-Shot Selection for In-Context Learning
arxiv_id: '2305.14210'
source_url: https://arxiv.org/abs/2305.14210
tags:
- examples
- in-context
- skill
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skill-KNN is a skill-based few-shot selection method for in-context
  learning that generates skill-based representations of input queries and examples
  using a pre-processing few-shot prompting, eliminating unimportant surface features.
  This approach addresses the problem of existing methods based on pre-trained embeddings
  being easily biased by surface natural language features that are not important
  for the target task.
---

# Skill-Based Few-Shot Selection for In-Context Learning

## Quick Facts
- arXiv ID: 2305.14210
- Source URL: https://arxiv.org/abs/2305.14210
- Reference count: 23
- Key outcome: Skill-KNN achieves up to 76.6% execution accuracy on Spider semantic parsing with text-davinci-003, significantly outperforming existing few-shot selection methods.

## Executive Summary
Skill-KNN is a skill-based few-shot selection method for in-context learning that generates skill-based representations of input queries and examples using pre-processing few-shot prompting. This approach eliminates unimportant surface features that bias traditional embedding-based selection methods. By focusing on task-specific skills rather than raw input similarity, Skill-KNN addresses the problem of existing methods being easily biased by surface natural language features not important for the target task. The method does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks.

## Method Summary
Skill-KNN generates skill-based representations by rewriting input queries and examples through few-shot prompting, which are then embedded using off-the-shelf models for similarity-based selection. The method includes two variants: consistency-based (selecting central embeddings across multiple prompt orderings) and distinctiveness-based (selecting maximum similarity representations). This approach optimizes the inputs fed into embedding models rather than tuning the models themselves, addressing the limitation of existing methods that are easily biased by surface features.

## Key Results
- Achieves up to 76.6% execution accuracy on Spider semantic parsing task with text-davinci-003
- Significantly outperforms existing few-shot selection methods across five cross-domain semantic parsing datasets
- Demonstrates improved cross-domain generalization through better-matched embedding space distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skill-based representations reduce the influence of surface-level features by focusing on task-specific skills rather than raw input similarity.
- Mechanism: By rewriting input queries into skill-based representations through prompting, the embedding model focuses on underlying task structures rather than surface entities, leading to more relevant example selection.
- Core assumption: The LLM can reliably extract and represent task-specific skills from raw inputs through prompting, and these representations are more informative for selecting relevant in-context examples.
- Evidence anchors:
  - [abstract] "Skill-KNN generates skill-based representations of input queries and examples using a pre-processing few-shot prompting, eliminating unimportant surface features."
  - [section] "The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself."
  - [corpus] Weak - no direct corpus evidence supporting the skill extraction capability of prompting.
- Break condition: If the LLM fails to extract meaningful task skills or if the embedding model cannot effectively distinguish between different skill representations, the method will not outperform raw-input-based selection.

### Mechanism 2
- Claim: Consistency-based and distinctiveness-based variants address the sensitivity to demonstration order in prompting, improving robustness.
- Mechanism: By generating multiple candidate representations through shuffling demonstration orders and selecting based on either central similarity (consistency) or maximum similarity (distinctiveness), the method becomes less sensitive to prompt order variations.
- Core assumption: The order of annotated demonstrations affects the quality of generated skill-based representations, and averaging or selecting the most distinctive representation can mitigate this effect.
- Evidence anchors:
  - [section] "We notice that the generated skill-based representations sometimes could be sensitive to the order of annotated demonstrations."
  - [section] "Consistency-Based Variant... takes the central embedding of all candidate representations during selecting examples."
  - [section] "Distinctiveness-Based Variant... considers the maximum similarity score between two sets for selection."
  - [corpus] Weak - no corpus evidence showing the magnitude of order sensitivity or the effectiveness of the proposed variants.
- Break condition: If the sensitivity to demonstration order is negligible or if the averaging/maximum selection strategies do not effectively address the sensitivity, the variants will not provide meaningful improvements over the base version.

### Mechanism 3
- Claim: Skill-based embedding space better matches the distribution of test cases and example bank, facilitating cross-domain generalization.
- Mechanism: The skill-based representations create a more aligned embedding space where test cases and example bank examples are distributed similarly, allowing the KNN selection to find more relevant examples across domain shifts.
- Core assumption: The skill-based representations capture the essential task characteristics that are shared across domains, while raw-input representations are dominated by domain-specific surface features.
- Evidence anchors:
  - [section] "The embedding space of skill-based representations can better alleviate the distribution shift between the example bank and test instances."
  - [section] "Under the skill-based embedding space (shown in Figure 4b), the distributions of test cases and the example bank are better matched."
  - [corpus] Weak - no corpus evidence demonstrating that skill-based representations consistently capture domain-agnostic task characteristics.
- Break condition: If the skill-based representations do not effectively capture cross-domain task characteristics or if the embedding space alignment does not translate to better example selection, the method will not improve cross-domain generalization.

## Foundational Learning

- Concept: Prompt engineering and few-shot prompting
  - Why needed here: The method relies on prompting an LLM to generate skill-based representations, requiring understanding of how to effectively prompt for task-specific skill extraction.
  - Quick check question: How would you design a prompt to extract SQL operation skills from a natural language question and database schema?

- Concept: Embedding similarity and KNN selection
  - Why needed here: The method uses embedding similarity to select in-context examples, requiring understanding of how embedding models capture semantic similarity and how KNN works.
  - Quick check question: What factors influence the effectiveness of KNN selection in high-dimensional embedding spaces?

- Concept: Cross-domain generalization and distribution shift
  - Why needed here: The method aims to improve cross-domain semantic parsing by addressing distribution shift between example bank and test instances through skill-based representations.
  - Quick check question: How can you measure and visualize distribution shift between training and test data in semantic parsing tasks?

## Architecture Onboarding

- Component map: Input query preprocessing -> Skill-based representation generation -> Embedding generation -> Similarity calculation -> Example selection -> In-context learning

- Critical path:
  1. Generate skill-based representations for input query and all example bank entries
  2. Calculate similarity scores between input query and all examples
  3. Select top-k most similar examples
  4. Concatenate selected examples with input query
  5. Generate output using backbone LLM

- Design tradeoffs:
  - Prompt complexity vs. generation cost: More detailed prompts may generate better skill representations but increase generation time and cost
  - Embedding model choice: Different embedding models may capture different aspects of skill representations, affecting selection quality
  - Number of demonstrations: More annotated demonstrations may improve skill extraction but increase prompt engineering effort

- Failure signatures:
  - Poor selection quality: If skill-based representations do not capture relevant task characteristics, KNN selection will not find appropriate examples
  - Sensitivity to demonstration order: If the base version is highly sensitive to demonstration order, the variants may not consistently improve performance
  - Domain mismatch: If skill-based representations are too domain-specific, the method may not improve cross-domain generalization

- First 3 experiments:
  1. Compare skill-based selection vs. raw-input selection on a simple semantic parsing task with clear skill differences
  2. Test sensitivity to demonstration order by varying the order of annotated demonstrations and measuring performance variance
  3. Evaluate the effect of different embedding models on skill-based selection quality by comparing results with Sentence-BERT vs. OpenAI Babbage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Skill-KNN compare to other few-shot selection methods on non-semantic parsing tasks?
- Basis in paper: [inferred] The paper primarily focuses on semantic parsing tasks and mentions that Skill-KNN could be beneficial for other challenging tasks where task-specific skills are needed, but does not provide experimental results on other task types.
- Why unresolved: The paper only evaluates Skill-KNN on semantic parsing tasks, leaving the question of its effectiveness on other task types unanswered.
- What evidence would resolve it: Conducting experiments on a variety of non-semantic parsing tasks (e.g., text classification, machine translation) and comparing Skill-KNN's performance to other few-shot selection methods would provide insights into its generalizability and effectiveness across different task domains.

### Open Question 2
- Question: How does the choice of backbone model influence the preference for consistency-based or distinctiveness-based variants of Skill-KNN?
- Basis in paper: [explicit] The paper mentions that different backbone models prefer different variants of Skill-KNN, but does not provide a detailed analysis of the factors influencing this preference.
- Why unresolved: While the paper identifies that different models have preferences, it does not explore the underlying reasons for these preferences or provide a systematic way to determine which variant is best suited for a given backbone model.
- What evidence would resolve it: Conducting a more in-depth analysis of the relationship between backbone model characteristics and the effectiveness of each Skill-KNN variant, possibly through ablation studies or correlation analysis, would help understand the factors influencing the preference and guide the selection of the most appropriate variant for a given model.

### Open Question 3
- Question: Can Skill-KNN be further improved by incorporating additional information beyond the input query and database schema, such as example metadata or user feedback?
- Basis in paper: [inferred] The paper focuses on generating skill-based representations from the input query and database schema, but does not explore the potential benefits of incorporating additional information that may be available during the few-shot selection process.
- Why unresolved: The paper does not investigate whether incorporating additional information, such as example metadata (e.g., example difficulty, example popularity) or user feedback (e.g., example relevance ratings), could further enhance the performance of Skill-KNN.
- What evidence would resolve it: Conducting experiments that incorporate additional information into the skill-based representation generation process and evaluating the impact on Skill-KNN's performance would provide insights into the potential benefits and limitations of leveraging such information.

## Limitations
- The method's reliance on prompt engineering introduces uncertainty about the reliability of skill extraction, with limited validation of prompt effectiveness
- The paper does not adequately quantify the computational overhead of generating skill-based representations for all example bank entries
- Cross-domain generalization benefits are primarily supported by qualitative observations rather than comprehensive quantitative analysis

## Confidence
- High confidence: The general framework of using skill-based representations for example selection is sound and well-motivated. The experimental results showing improvements over baseline methods are credible.
- Medium confidence: The effectiveness of the consistency-based and distinctiveness-based variants, as well as the claimed improvements in cross-domain generalization, are less certain due to limited quantitative evidence and lack of ablation studies.
- Low confidence: The specific prompt engineering details and their impact on skill extraction quality are not well-established, making it difficult to assess the robustness of the method to prompt design choices.

## Next Checks
1. **Prompt ablation study**: Systematically vary the prompt format, examples, and demonstration order to quantify their impact on skill-based representation quality and selection performance. This would validate the robustness of the method to prompt engineering choices.

2. **Order sensitivity quantification**: Measure the performance variance across different demonstration orders and compare it with the improvement from consistency/distinctiveness variants to determine if the sensitivity is significant enough to warrant the added complexity.

3. **Embedding space analysis**: Conduct quantitative analysis of embedding space alignment between domains using skill-based vs. raw-input representations, such as measuring the maximum mean discrepancy (MMD) or other domain adaptation metrics to verify the claimed benefits for cross-domain generalization.