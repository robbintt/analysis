---
ver: rpa2
title: 'From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic
  Regression'
arxiv_id: '2310.01687'
source_url: https://arxiv.org/abs/2310.01687
tags:
- loss
- training
- ergodic
- esting
- sharpness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a comprehensive analysis of gradient descent
  dynamics in quadratic regression models, revealing five distinct training phases:
  monotonic, catapult, periodic, chaotic, and divergent. The authors develop a cubic-map-based
  dynamical system that captures these dynamics, parameterized by the step-size.'
---

# From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression

## Quick Facts
- arXiv ID: 2310.01687
- Source URL: https://arxiv.org/abs/2310.01687
- Authors: 
- Reference count: 40
- Primary result: Reveals five distinct training phases in gradient descent dynamics for quadratic regression models through cubic-map-based analysis

## Executive Summary
This paper presents a comprehensive analysis of gradient descent dynamics in quadratic regression models, revealing five distinct training phases: monotonic, catapult, periodic, chaotic, and divergent. The authors develop a cubic-map-based dynamical system that captures these dynamics, parameterized by the step-size. Through rigorous bifurcation analysis, they precisely characterize the boundaries between these phases. Their framework applies to both orthogonal and non-orthogonal data, with practical implications for stabilizing test error through ergodic trajectory averaging in non-monotonic phases.

## Method Summary
The authors analyze gradient descent dynamics for quadratic regression models, specifically two-layer neural networks with quadratic activation functions and constant outer layers. They generate orthogonal training data by taking rows from random orthogonal matrices and create labels using a specific quadratic form with added noise. The method involves implementing gradient descent with varying step-sizes to target the five distinct phases (monotonic, catapult, periodic, chaotic, divergent), then plotting training loss curves, sharpness, and test loss with/without ergodic trajectory averaging for each phase.

## Key Results
- Five distinct training phases identified: monotonic, catapult, periodic, chaotic, and divergent
- Cubic-map-based dynamical system captures gradient descent dynamics, parameterized by step-size
- Bifurcation analysis precisely characterizes phase boundaries
- Ergodic trajectory averaging stabilizes test error in non-monotonic (and non-divergent) phases
- Similar phase structures emerge across orthogonal and non-orthogonal data conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient descent dynamics in quadratic regression models can be fully captured by a cubic map parameterized by the step-size.
- Mechanism: The authors derive that for certain quadratic regression problems, the evolution of the training error follows a discrete dynamical system of the form $z_{t+1} = f_a(z_t)$ where $f_a(z) = z(z+a)(z-2)+1-2a$. This cubic map naturally parameterizes the training dynamics through the step-size.
- Core assumption: The data matrix $X$ is orthogonal (i.e., $XX^T = \text{diag}(\|X_1\|^2, \ldots, \|X_n\|^2)$), which decouples the loss function across samples and allows the dynamics to be non-interacting.
- Evidence anchors:
  - [abstract]: "we reveal that the dynamics can be encapsulated by a specific cubic map, naturally parameterized by the step-size"
  - [section]: "We show in Theorem 3.2 and 3.3, that the dynamics of gradient descent for two non-convex statistical problems, namely phase retrieval and two-layer neural networks with constant outer layers and quadratic activation functions, with orthogonal training data is captured by the cubic-map-based dynamical system."
- Break condition: If the orthogonality assumption is violated (non-orthogonal data), the dynamics become an interacting system that cannot be captured by the simple cubic map.

### Mechanism 2
- Claim: Different step-size regimes lead to five distinct training phases: monotonic, catapult, periodic, chaotic, and divergent.
- Mechanism: Through bifurcation analysis of the cubic map $f_a$, the authors precisely characterize the boundaries between these phases. For small step-sizes ($a \leq 2\sqrt{2}-2$), the dynamics are monotonic and converge to zero. As the step-size increases, the system transitions through catapult, periodic, and chaotic phases before eventually diverging.
- Core assumption: The cubic map exhibits the full range of dynamical behaviors (fixed points, periodic orbits, chaos) as the parameter $a$ varies.
- Evidence anchors:
  - [abstract]: "Through a fine-grained bifurcation analysis concerning the step-size parameter, we delineate five distinct training phases"
  - [section]: "We perform a fine-grained, global theoretical analysis of a cubic-map-based dynamical system (see Equation 2.1), and identify the precise boundaries of the following five phases"
- Break condition: If the loss function or model structure deviates significantly from the quadratic form assumed, the cubic map approximation may break down.

### Mechanism 3
- Claim: Ergodic trajectory averaging stabilizes test error in non-monotonic (and non-divergent) phases.
- Mechanism: When training in periodic or chaotic phases, the gradient descent trajectory converges to an invariant distribution. Averaging predictions over this trajectory approximates expectation with respect to this distribution, reducing fluctuations and stabilizing test error.
- Core assumption: The gradient descent trajectory converges to an invariant measure in non-monotonic phases, making ergodic averaging meaningful.
- Evidence anchors:
  - [abstract]: "We also empirically investigate the generalization performance when training in the various non-monotonic (and non-divergent) phases. In particular, we observe that performing an ergodic trajectory averaging stabilizes the test error in non-monotonic (and non-divergent) phases."
  - [section]: "Hence, a natural approach is to do perform ergodic trajectory averaging to reduce the fluctuations"
- Break condition: If the trajectory does not converge to an invariant measure (e.g., in the divergent phase), ergodic averaging will not stabilize predictions.

## Foundational Learning

- Concept: Bifurcation theory in dynamical systems
  - Why needed here: The paper uses bifurcation analysis to characterize the boundaries between different training phases as the step-size parameter varies.
  - Quick check question: What is a bifurcation point in the context of dynamical systems?

- Concept: Lyapunov exponents and chaos
  - Why needed here: The authors use Lyapunov exponents to characterize chaotic behavior in the gradient descent dynamics and distinguish between periodic and chaotic phases.
  - Quick check question: How does a positive Lyapunov exponent relate to chaotic behavior in dynamical systems?

- Concept: Ergodic theory and invariant measures
  - Why needed here: The paper proposes ergodic trajectory averaging as a method to stabilize predictions when training in non-monotonic phases, which requires understanding invariant measures and ergodic averages.
  - Quick check question: What is the relationship between ergodic averages and expectations with respect to invariant measures?

## Architecture Onboarding

- Component map: Cubic map $f_a$ -> Bifurcation analysis -> Phase characterization -> Ergodic averaging technique
- Critical path: 1) Define the cubic map from the gradient descent updates, 2) Perform bifurcation analysis to identify phase boundaries, 3) Validate phases experimentally, 4) Test ergodic averaging for prediction stabilization.
- Design tradeoffs: The orthogonal data assumption simplifies the analysis but limits applicability to non-orthogonal data. The ergodic averaging technique stabilizes predictions but increases computational cost at test time.
- Failure signatures: If training diverges (divergent phase), if predictions become unstable in non-monotonic phases, or if ergodic averaging fails to improve test error.
- First 3 experiments:
  1. Implement the cubic map $f_a$ and verify the five phases by plotting orbits for different values of $a$.
  2. Apply the phase retrieval model from Section 3.1 and verify that the training dynamics follow the predicted cubic map behavior.
  3. Test the ergodic averaging technique on the two-layer neural network model from Section 3.2 and measure its impact on test error stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact value of a* in Theorem 2.1 where the transition from periodic to chaotic phase occurs?
- Basis in paper: [explicit] The paper states that a* exists in (1, 2) but does not provide its exact value
- Why unresolved: The authors conjecture that a* is the smallest number in (1, 2) such that (1-2a)/3 is a period-3 point, but this requires further analysis
- What evidence would resolve it: A rigorous mathematical proof showing that (1-2a*)/3 is indeed a period-3 point and that this is the smallest such value in (1, 2)

### Open Question 2
- Question: How do the five phases (monotonic, catapult, periodic, chaotic, divergent) manifest in gradient descent training with non-orthogonal data?
- Basis in paper: [explicit] The paper provides empirical evidence for non-orthogonal data but lacks theoretical analysis
- Why unresolved: The theoretical framework developed in the paper relies on orthogonal data assumptions to decouple the loss function across samples
- What evidence would resolve it: A rigorous bifurcation analysis of interacting dynamical systems for non-orthogonal data that characterizes the phase boundaries

### Open Question 3
- Question: What are the provable generalization benefits of training in the periodic and chaotic phases using ergodic trajectory averaging?
- Basis in paper: [explicit] The paper empirically observes that ergodic trajectory averaging stabilizes test error in non-monotonic phases but lacks theoretical guarantees
- Why unresolved: The paper only provides empirical observations without establishing theoretical bounds on generalization performance
- What evidence would resolve it: Rigorous generalization bounds that account for the invariant measure properties of the training trajectory in non-monotonic phases

## Limitations

- Theoretical framework relies heavily on orthogonal data assumption, limiting applicability to real-world non-orthogonal datasets
- Cubic-map-based dynamical system may not fully represent gradient descent dynamics in more complex models or non-orthogonal settings
- Empirical validation focuses on specific model architectures (two-layer networks with quadratic activations) and may not generalize to other architectures

## Confidence

*High Confidence:* The characterization of five distinct training phases through bifurcation analysis of the cubic map is well-supported by both theoretical and empirical evidence.

*Medium Confidence:* The claim that ergodic trajectory averaging stabilizes test error in non-monotonic phases is empirically validated but lacks rigorous theoretical justification.

*Low Confidence:* The generalizability of the orthogonal data assumption and its implications for non-orthogonal datasets remains unclear.

## Next Checks

1. **Non-orthogonal data validation:** Systematically test the phase characterization and ergodic averaging technique on datasets with varying degrees of orthogonality to quantify the impact of the orthogonal assumption on the theoretical predictions.

2. **Alternative architectures:** Validate the five-phase characterization and prediction stabilization techniques on different neural network architectures (e.g., ReLU networks, deeper networks) to assess the generalizability of the findings.

3. **Rigorous ergodic theory analysis:** Develop a formal theoretical framework connecting the gradient descent trajectories in non-monotonic phases to invariant measures and ergodic averages, providing rigorous justification for the prediction stabilization technique.