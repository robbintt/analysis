---
ver: rpa2
title: 'Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target
  Data'
arxiv_id: '2311.01420'
source_url: https://arxiv.org/abs/2311.01420
tags:
- target
- classes
- source
- learning
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a practical learning problem of adapting a
  pre-trained source model to a target domain for classifying all source classes,
  using target data that covers only a partial label space. This is motivated by the
  difficulty of collecting complete labeled data in real-world target environments.
---

# Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data

## Quick Facts
- arXiv ID: 2311.01420
- Source URL: https://arxiv.org/abs/2311.01420
- Reference count: 40
- Key outcome: Proposes methods for adapting pre-trained models to target domains with partial label coverage, improving performance on both seen and unseen classes.

## Executive Summary
This paper addresses the practical problem of adapting a pre-trained source model to a target domain when target data covers only a partial label space. The proposed Holistic Transfer framework introduces novel techniques including local SGD with class subsampling (LOLSGD) and feature rank regularization to disentangle domain gradients from classification gradients while preserving class relationships. Extensive experiments across multiple benchmark datasets demonstrate that these methods can improve overall performance without hurting accuracy on unseen classes, establishing solid baselines for this practical learning paradigm.

## Method Summary
The paper proposes adapting pre-trained models to target domains with partial label coverage by fine-tuning with techniques that prevent catastrophic forgetting of unseen classes. The core approach uses local SGD with class subsampling (LOLSGD) to average gradients from multiple class-subsampled datasets, effectively canceling out class-specific biases while preserving domain style shifts. Feature rank regularization prevents dimensional collapse of feature spaces, and selective distillation preserves class relationships from the source model. The framework also includes updating normalization layers and ensembling with the source model as complementary strategies.

## Key Results
- LOLSGD with class subsampling successfully cancels disruptive class biases while preserving domain style shifts
- Feature rank regularization prevents feature space collapse and maintains generalization to unseen classes
- The proposed methods improve overall accuracy without degrading performance on unseen classes compared to naive fine-tuning
- Extensive experiments on Office-Home, FEMNIST, iWildCam, VTAB, and iNaturalist Fungi datasets validate the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leave-Out Local SGD (LOLSGD) cancels disruptive class biases by averaging gradients from multiple class-subsampled datasets.
- Mechanism: By subsampling target data into M subsets, each missing some classes, and averaging their gradients, the method cancels out class-specific biases while preserving domain style shifts. This exploits the fact that all subsets share the same target domain style but have different class biases.
- Core assumption: Local gradients from class-subsampled datasets will have opposite biases for different classes, allowing cancellation when averaged.
- Evidence anchors:
  - [abstract]: "We propose a novel approach based on local SGD... averaging them could 'cancel out' the gradients biased to certain classes."
  - [section 3.2]: "Averaging these models can potentially cancel out the disruptive concept shifts and strengthen the shared covariate shift."
- Break condition: If local gradients become too correlated or the class distribution is highly imbalanced, cancellation may fail.

### Mechanism 2
- Claim: Feature rank regularization prevents dimensional collapse that degrades generalization to unseen classes.
- Mechanism: Regularizing the covariance matrix of features to maintain higher rank prevents the feature space from collapsing to a low-dimensional subspace dominated by seen classes.
- Core assumption: Fine-tuning on partial target data causes features to collapse to a low-dimensional space that cannot generalize to unseen classes.
- Evidence anchors:
  - [abstract]: "We observe in practice it can still be sensitive and lean towards {ySeen}. We, therefore, hypothesize that it is necessary to have some regularization to preserve the nice class relationships encoded in the source model already during target training."
  - [section 3.3]: "we found the features tend to collapse to a low-dimensional space... We consider a regularizer to avoid too many singular values of the features becoming zeros."
- Break condition: If regularization is too strong, it may prevent necessary adaptation to the target domain.

### Mechanism 3
- Claim: Freezing the linear classifier preserves source model's class relationships while adapting feature extractor to target domain style.
- Mechanism: By keeping the classification layer fixed during adaptation, the method maintains the source model's semantic understanding of classes while only adapting the feature extractor to match target domain styles.
- Core assumption: The source model's classifier contains valuable class relationship information that should be preserved during adaptation.
- Evidence anchors:
  - [section 3.3]: "we found it crucial to freeze the linear classifier w, which corresponds to the common practice in source-free DA."
  - [section 4]: "We include another baseline LP-FT [42] proposed for the OOD generalization of fine-tuning but not for missing classes in HT."
- Break condition: If target domain styles are too different from source, frozen classifier may become suboptimal.

## Foundational Learning

- Concept: Domain adaptation and covariate shift
  - Why needed here: HT is fundamentally a domain adaptation problem where the target test distribution differs from training distribution
  - Quick check question: What is the difference between covariate shift and concept shift in domain adaptation?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: HT must balance adaptation to target domain while preserving source model's knowledge of unseen classes
  - Quick check question: How does fine-tuning on partial target data lead to forgetting of unseen classes?

- Concept: Local SGD and distributed optimization
  - Why needed here: LOLSGD extends local SGD principles to handle class imbalance in target data
  - Quick check question: What is the key difference between standard SGD and local SGD?

## Architecture Onboarding

- Component map: Feature extractor (h(x; Ï•)) -> Linear classifier (g(z; w)) -> Normalization layers -> Regularization modules
- Critical path:
  1. Initialize with pre-trained source model
  2. Apply LOLSGD optimization with class subsampling
  3. Add regularization (Lrank, Ldistill)
  4. Fine-tune normalization layers
  5. Ensemble with source model if needed
- Design tradeoffs:
  - Freezing classifier vs. full fine-tuning
  - Regularization strength vs. adaptation capability
  - Computational cost of LOLSGD vs. standard fine-tuning
- Failure signatures:
  - Unseen class accuracy drops significantly below source model
  - Feature visualization shows low-rank structure
  - Seen class accuracy exceeds oracle upper bound
- First 3 experiments:
  1. Baseline: Fine-tune on partial target data, measure unseen class performance
  2. LOLSGD only: Compare gradient averaging vs. standard fine-tuning
  3. Full method: Add regularization and normalization updates, measure overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively generalize domain shifts learned on target seen classes to target unseen classes in holistic transfer?
- Basis in paper: [explicit] The paper identifies this as a key challenge and proposes LOLSGD to address it, but notes that the effectiveness of generalization to unseen classes remains an open question.
- Why unresolved: While LOLSGD shows promise in experiments, the paper acknowledges that further research is needed to fully understand and improve the generalization of domain shifts to unseen classes.
- What evidence would resolve it: Rigorous experiments comparing LOLSGD and other methods on diverse datasets, along with theoretical analysis of the generalization ability, would help resolve this question.

### Open Question 2
- Question: What are the optimal strategies for combining normalization layer updates and feature rank regularization in holistic transfer?
- Basis in paper: [explicit] The paper explores updating batchnorm statistics and instance normalization layers, as well as feature rank regularization, but does not provide definitive conclusions on the best combination of these techniques.
- Why unresolved: The paper presents various approaches and their effects, but does not determine the optimal combination or configuration of these strategies for holistic transfer.
- What evidence would resolve it: Systematic ablation studies and hyperparameter tuning experiments comparing different combinations of normalization updates and regularization techniques would provide insights into the optimal strategies.

### Open Question 3
- Question: How can holistic transfer be extended to non-classification tasks such as image segmentation or object detection?
- Basis in paper: [explicit] The paper acknowledges this as a limitation and suggests it as future work.
- Why unresolved: The current focus of the paper is on classification tasks, and the authors recognize the need to explore holistic transfer in other domains.
- What evidence would resolve it: Successful application and thorough evaluation of holistic transfer techniques on non-classification tasks would demonstrate the feasibility and effectiveness of extending the approach to these domains.

## Limitations

- The approach assumes local gradients from class-subsampled datasets will have opposite biases that cancel out, which may fail in highly imbalanced or correlated class distributions
- Feature rank regularization assumes feature collapse is the primary cause of poor unseen class performance, though other factors like domain shift magnitude could play larger roles
- The frozen classifier assumption may not hold for large domain gaps where classifier adaptation becomes necessary

## Confidence

- **High Confidence**: The core problem formulation (Holistic Transfer) is well-defined and addresses a real practical need. The experimental setup with partial target data is clearly specified.
- **Medium Confidence**: The LOLSGD mechanism and feature rank regularization show promise based on ablation studies, but the theoretical guarantees for gradient cancellation need further validation across diverse class distributions.
- **Low Confidence**: The assumption that frozen classifiers preserve source knowledge across all domain shifts may not hold for large domain gaps where classifier adaptation becomes necessary.

## Next Checks

1. Test LOLSGD performance on highly imbalanced target datasets where certain classes dominate multiple subsampled partitions
2. Compare feature rank regularization against alternative approaches like spectral normalization or contrastive learning objectives
3. Evaluate model performance when gradually unfreezing classifier layers during fine-tuning to assess the tradeoff between preserving source knowledge and adapting to target domain