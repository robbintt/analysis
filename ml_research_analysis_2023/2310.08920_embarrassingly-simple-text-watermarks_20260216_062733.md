---
ver: rpa2
title: Embarrassingly Simple Text Watermarks
arxiv_id: '2310.08920'
source_url: https://arxiv.org/abs/2310.08920
tags:
- text
- watermark
- texts
- detect
- watermarking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Easymark is a simple text watermarking method that exploits Unicode's
  multiple codepoints with the same visual appearance. It replaces standard whitespaces
  with alternative Unicode whitespace characters that are visually identical but detectable
  through character analysis.
---

# Embarrassingly Simple Text Watermarks

## Quick Facts
- arXiv ID: 2310.08920
- Source URL: https://arxiv.org/abs/2310.08920
- Reference count: 14
- Key outcome: Achieves 100% detection accuracy with zero quality degradation through Unicode whitespace substitution

## Executive Summary
Easymark is a simple text watermarking method that exploits Unicode's multiple codepoints with the same visual appearance. It replaces standard whitespaces with alternative Unicode whitespace characters that are visually identical but detectable through character analysis. This approach requires only a few lines of code and can be implemented on the user side without access to language models. Experiments with NLLB-200-3.3B and LLaMA-7B show Easymark achieves 100% detection accuracy while maintaining BLEU scores and perplexity equal to non-watermarked text. The authors also prove an impossibility theorem showing any reliable watermark can be removed without significant quality degradation, justifying the use of simple watermarking approaches.

## Method Summary
Easymark works by replacing standard whitespace characters (U+0020) with visually identical but distinct Unicode whitespace codepoints like U+2004. The detection mechanism simply counts occurrences of the substitute character. The method exploits the fact that Unicode has many codepoints for whitespace that render identically to humans but are distinguishable programmatically. This enables watermarking that preserves text quality while providing reliable detection. The approach is embarrassingly simple, requiring minimal implementation effort while achieving perfect detection accuracy on tested models.

## Key Results
- Achieves 100% detection accuracy on NLLB-200-3.3B and LLaMA-7B models
- Maintains identical BLEU scores and perplexity compared to non-watermarked text
- Proves impossibility theorem showing any watermark can be removed without quality degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unicode whitespace substitution enables invisible watermarking
- Mechanism: Replaces standard whitespaces (U+0020) with visually identical but distinct Unicode whitespace characters (e.g., U+2004). Detection works by counting occurrences of the substitute character.
- Core assumption: Human perception cannot distinguish between different Unicode whitespace characters
- Evidence anchors:
  - [abstract] "replaces standard whitespaces with alternative Unicode whitespace characters that are visually identical but detectable through character analysis"
  - [section] "Whitemark is the simplest method that exploits the fact that Unicode has many codepoints for whitespace and replaces a whitespace (U+0020) with another codepoint of a whitespace, e.g., U+2004"
  - [corpus] Weak - corpus doesn't contain specific evidence about Unicode whitespace substitution effectiveness
- Break condition: If text undergoes normalization that converts all Unicode whitespaces to standard U+0020, or if all whitespaces are manually replaced with standard spaces

### Mechanism 2
- Claim: High detection accuracy with zero quality degradation
- Mechanism: Since only whitespace characters are modified and not printable text, BLEU scores and perplexity remain identical to original text. Detection works by checking for presence of substitute whitespace character.
- Core assumption: Watermark detection accuracy depends solely on presence of substitute character, not on text content
- Evidence anchors:
  - [abstract] "Easymark achieves 100% detection accuracy while maintaining BLEU scores and perplexity equal to non-watermarked text"
  - [section] "Proposition 3.1. The BLEU scores and perplexity of the raw text and the text with Whitemark are the same"
  - [corpus] Weak - corpus neighbors discuss watermarking but don't specifically validate zero quality degradation claims
- Break condition: If substitute whitespace character appears naturally in text (δFP > 0) or if all original whitespaces are removed (δFN > 0)

### Mechanism 3
- Claim: Impossibility theorem proves watermark vulnerability
- Mechanism: Any watermark can be erased without quality degradation by finding text in the same metric space that satisfies detection function but avoids watermark pattern.
- Core assumption: Watermark must not change meaning significantly to maintain text quality
- Evidence anchors:
  - [abstract] "We also prove an impossibility theorem showing any reliable watermark can be removed without significant quality degradation"
  - [section] "Theorem 3.4 (Impossibility Theorem, Formal)" with formal proof showing universal erasing function exists
  - [corpus] Weak - corpus neighbors discuss watermarking but don't specifically validate impossibility theorem claims
- Break condition: If watermark changes meaning significantly (violates metric assumption) or if detection function has false positive requirement

## Foundational Learning

- Concept: Unicode character encoding and whitespace variants
  - Why needed here: Easymark relies on exploiting multiple Unicode codepoints that render identically but are distinguishable programmatically
  - Quick check question: How many different Unicode codepoints represent whitespace characters, and which ones are visually indistinguishable from U+0020?

- Concept: Statistical watermark detection and false positive/negative rates
  - Why needed here: Understanding detection accuracy requires grasping how watermark patterns create statistical signatures that can be measured
  - Quick check question: What conditions ensure δFP = 0 and δFN = 0 in watermark detection, and how do these relate to natural text statistics?

- Concept: Metric space theory and Lipschitz continuity
  - Why needed here: The impossibility theorem relies on text being embedded in a metric space where small perturbations preserve meaning
  - Quick check question: Why does requiring the loss function to be 1-Lipschitz continuous enable the proof of universal watermark erasure?

## Architecture Onboarding

- Component map:
  - add_watermark(text) -> Function that replaces target characters with substitute characters
  - detect_watermark(text) -> Function that checks for presence of substitute characters
  - Unicode codepoint mapping -> Table of visually identical characters and their codepoints
  - Detection threshold -> Criteria for determining watermark presence

- Critical path:
  1. Input text processing
  2. Character substitution (add_watermark)
  3. Output watermarked text
  4. Input text analysis (detect_watermark)
  5. Substitute character counting
  6. Boolean detection result

- Design tradeoffs:
  - Whitespace vs. printable character modification: Whitespace preserves meaning but has lower capacity
  - Single vs. multiple substitute characters: More substitutes increase capacity but risk false positives
  - Streaming vs. batch processing: Streaming enables real-time watermarking but limits algorithm complexity

- Failure signatures:
  - False negatives: Substitute character not present due to text editing
  - False positives: Substitute character appears naturally in text
  - Quality degradation: Incorrect character mapping or normalization issues
  - Detection failure: Detection function logic errors

- First 3 experiments:
  1. Basic functionality test: Apply add_watermark to text with spaces, verify detect_watermark returns True
  2. Quality preservation test: Compare BLEU scores of original vs. watermarked text using translation task
  3. False positive rate measurement: Test detect_watermark on large corpus of natural text to measure δFP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the impossibility theorem be extended to watermarking methods that deliberately change the meaning of the text to create unbreakable watermarks?
- Basis in paper: [explicit] The paper presents a counterexample showing that the impossibility theorem does not hold if a watermark does not care about the metric, i.e., it does not change the meaning of the text much. This suggests that watermarking methods that deliberately change the meaning of the text could potentially create unbreakable watermarks.
- Why unresolved: The paper does not explore this direction in detail and leaves it as a future research direction.
- What evidence would resolve it: Experiments showing that watermarking methods that deliberately change the meaning of the text are more resistant to removal attempts than methods that preserve meaning.

### Open Question 2
- Question: How can the trade-off between detection accuracy and erasing difficulty be optimized in watermarking methods?
- Basis in paper: [explicit] The paper suggests that watermarking methods can be designed to be difficult to erase if we make a trade-off between the detection accuracy and the erasing difficulty. However, the paper does not provide a detailed analysis of how to optimize this trade-off.
- Why unresolved: The paper does not provide a framework for quantifying or optimizing the trade-off between detection accuracy and erasing difficulty.
- What evidence would resolve it: A theoretical framework or experimental results showing how to optimize the trade-off between detection accuracy and erasing difficulty in watermarking methods.

### Open Question 3
- Question: Can the impossibility theorem be extended to other types of watermarking, such as image or audio watermarking?
- Basis in paper: [explicit] The paper presents a theorem that shows the impossibility of perfect watermarking in text. However, the theorem is based on specific assumptions about the text space and the watermarking function, which may not hold for other types of data.
- Why unresolved: The paper does not explore the applicability of the theorem to other types of data.
- What evidence would resolve it: Experiments or theoretical analysis showing whether the impossibility theorem holds for image or audio watermarking.

## Limitations
- Impossibility theorem proof assumes perfect knowledge of watermarking mechanism
- Evaluation focuses on translation tasks, leaving questions about cross-task robustness
- Method's security against sophisticated attacks (like character-level adversarial examples) remains untested

## Confidence
- Mechanism 1 (Unicode substitution): High - Direct implementation and basic testing confirm visual indistinguishability
- Mechanism 2 (Zero quality degradation): High - BLEU and perplexity metrics show no statistical difference
- Mechanism 3 (Impossibility theorem): Medium - Formal proof appears sound but requires independent verification of the erasure function construction

## Next Checks
1. Test Easymark robustness against character-level adversarial attacks (e.g., homoglyph substitution attacks) to validate security claims beyond basic watermark removal
2. Evaluate cross-task generalization by testing watermark detection accuracy on diverse NLP benchmarks beyond translation (summarization, question answering, etc.)
3. Measure real-world false positive rates on large-scale natural text corpora to determine practical deployment limits and required detection thresholds