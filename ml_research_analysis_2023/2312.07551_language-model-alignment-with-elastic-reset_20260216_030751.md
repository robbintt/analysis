---
ver: rpa2
title: Language Model Alignment with Elastic Reset
arxiv_id: '2312.07551'
source_url: https://arxiv.org/abs/2312.07551
tags:
- reset
- reward
- drift
- elastic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of language model alignment drift
  that occurs during reinforcement learning from human feedback (RLHF) training. When
  optimizing language models for a specific reward, they often degrade in performance
  on other tasks and drift from natural language syntax - a phenomenon known as reward
  hacking, alignment tax, or language drift.
---

# Language Model Alignment with Elastic Reset

## Quick Facts
- arXiv ID: 2312.07551
- Source URL: https://arxiv.org/abs/2312.07551
- Reference count: 40
- Key outcome: Achieves better reward-drift tradeoffs than existing methods including KL penalties and masking approaches for language model alignment.

## Executive Summary
This paper addresses language model alignment drift during reinforcement learning from human feedback (RLHF) training, where optimizing for reward degrades performance on other tasks and drifts from natural language syntax. The authors propose Elastic Reset, a method that periodically resets the online model to an exponentially moving average (EMA) of itself, then resets the EMA model to the initial model. This approach achieves higher reward with less drift without explicitly modifying the training objective, outperforming baselines on pivot translation, IMDB sentiment, and LLaMA-7B chatbot tasks.

## Method Summary
Elastic Reset maintains an EMA of the online policy during RL training, periodically resetting the online model to this EMA and then resetting the EMA back to the initial pretrained model. The method works by providing intermediate checkpoints between the online and initial states, preventing the model from drifting too far while retaining more performance than a reset to the initial model alone. The approach is robust to hyperparameter choices and works well with LoRA adapters for efficient training.

## Key Results
- Achieves state-of-the-art performance on small-scale pivot-translation benchmark
- Outperforms all baselines in medium-scale RLHF-like IMDB mock sentiment task
- Leads to more performant and aligned technical QA chatbot with LLaMA-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Resetting to an EMA smooths optimization and mitigates catastrophic drift by providing intermediate checkpoints between online and initial states.
- Mechanism: Elastic Reset maintains an EMA of the online policy, which acts as a smoothed trajectory of training. Periodic resets to this EMA prevent the model from drifting too far while retaining more performance than a reset to the initial model.
- Core assumption: The EMA represents a model state that balances reward optimization with retention of useful capabilities from pretraining.
- Evidence anchors: Abstract mentions periodic resets to EMA, section describes maintaining EMA and resetting online model to it, but corpus evidence for EMA-based smoothing in RLHF context is weak.
- Break condition: If the EMA decay is too slow, the EMA will drift significantly and no longer serve as a stable intermediate checkpoint.

### Mechanism 2
- Claim: Elastic Reset improves the reward-drift tradeoff by reducing overfitting to the reward model without explicit KL regularization.
- Mechanism: By periodically resetting the policy, Elastic Reset interrupts the overfitting process, forcing the model to recover capabilities that may have degraded during reward optimization. This creates a better balance between reward and drift.
- Core assumption: The model overfits to the reward model over time, degrading other capabilities. Resets interrupt this process.
- Evidence anchors: Abstract states achievement of higher reward with less drift without explicitly modifying training objective, section discusses minimizing drift during RLHF, but corpus evidence for overfitting interruption in RLHF context is weak.
- Break condition: If resets are too frequent, the model may not have enough time to optimize the reward before being reset.

### Mechanism 3
- Claim: Maintaining the value function across resets provides stable gradients and prevents large performance drops.
- Mechanism: Elastic Reset resets the policy but keeps the value function, which means the model can recover quickly after a reset without losing the learned value estimates that guide policy improvement.
- Core assumption: The value function generalizes across nearby policy states, so it remains useful even after a policy reset.
- Evidence anchors: Section mentions resetting policy but maintaining value function, Appendix E.1 shows resets iteratively improve value function, but corpus evidence for value function maintenance across resets in RLHF context is weak.
- Break condition: If the value function becomes too specific to the drifted policy, it may provide poor guidance after a reset.

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: EMA provides a smoothed trajectory of the policy during training, serving as an intermediate checkpoint between the online and initial models.
  - Quick check question: If η=0.99, what fraction of the current online model is retained in the EMA after each update?

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: KL divergence measures the difference between probability distributions, used here to quantify drift from the initial model.
  - Quick check question: In the reward formula R(x,y) = r(x,y) - β log πθ(y|x) / πθ0(y|x), what does the KL term represent?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the training paradigm where a reward model learned from human preferences is used to fine-tune a language model.
  - Quick check question: What problem does RLHF specifically address that supervised learning cannot?

## Architecture Onboarding

- Component map: Online policy (θ) -> EMA policy (¯θ) -> Initial policy (θ0) -> Value function -> Reward model
- Critical path: Online policy → EMA update → Reset to EMA → Reset EMA to initial → Repeat
- Design tradeoffs:
  - EMA decay rate η: Higher values make EMA track online policy more closely (less smoothing), lower values provide more smoothing but slower recovery
  - Reset frequency: More frequent resets reduce drift but may slow reward optimization
  - KL coefficient: Higher values prevent drift but may limit reward optimization
- Failure signatures:
  - High drift with low reward: Reset frequency too low or EMA decay too slow
  - Low reward with high drift: KL coefficient too low or reset frequency too high
  - Unstable training: EMA decay too high or value function not properly maintained
- First 3 experiments:
  1. Run baseline PPO with KL penalty on IMDB mock sentiment task, measure reward-drift tradeoff
  2. Implement Elastic Reset with EMA decay 0.995 and reset every 17 epochs, compare to baseline
  3. Vary EMA decay rate (0.95, 0.99, 0.995, 0.999) and reset frequency to find optimal hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal reset frequency for Elastic Reset in RLHF training?
- Basis in paper: [explicit] The paper mentions that Elastic Reset resets every 260 epochs for the StackLlama experiment and every 17 epochs for the IMDB experiment, but states that the reset rate is chosen heuristically.
- Why unresolved: The paper does not provide a systematic method for determining the optimal reset frequency and only mentions that results are robust to choice of this hyperparameter.
- What evidence would resolve it: Empirical studies comparing different reset frequencies on various RLHF tasks to determine the optimal reset frequency that balances performance and drift.

### Open Question 2
- Question: How does Elastic Reset perform compared to other methods on large-scale RLHF tasks with real human feedback?
- Basis in paper: [inferred] The paper mentions that the IMDB and StackLlama experiments use reward models trained on human preferences, but does not compare Elastic Reset to other methods on tasks with real human feedback.
- Why unresolved: The paper does not provide direct comparisons of Elastic Reset to other methods on large-scale RLHF tasks with real human feedback, only on tasks with simulated or proxy feedback.
- What evidence would resolve it: Direct comparisons of Elastic Reset to other methods on large-scale RLHF tasks with real human feedback, such as the Anthropic Helpful and Harmless (H&H) benchmark or the OpenAI API.

### Open Question 3
- Question: Can Elastic Reset be extended to other types of language model finetuning tasks beyond RLHF?
- Basis in paper: [explicit] The paper focuses on applying Elastic Reset to RLHF tasks, but mentions that it is inspired by previous work on image classification and sample-efficient RL.
- Why unresolved: The paper does not explore the application of Elastic Reset to other types of language model finetuning tasks beyond RLHF, such as supervised finetuning or unsupervised pretraining.
- What evidence would resolve it: Empirical studies applying Elastic Reset to other types of language model finetuning tasks and comparing its performance to other methods.

## Limitations
- The paper lacks ablation studies isolating the contribution of individual components like EMA and value function maintenance.
- Scalability to larger models and more complex alignment tasks remains unproven beyond the LLaMA-7B chatbot experiment.
- The characterization that Elastic Reset works "without explicitly modifying the training objective" is somewhat misleading since it still uses KL penalties.

## Confidence

- **High confidence**: The empirical results demonstrating improved reward-drift tradeoffs on the three benchmark tasks (pivot translation, IMDB sentiment, and LLaMA-7B chatbot). The quantitative comparisons with baselines are well-documented.
- **Medium confidence**: The proposed mechanism that EMA smoothing provides intermediate checkpoints between online and initial states. While intuitively sound, the paper lacks ablation studies isolating the EMA component's contribution.
- **Low confidence**: The claim that Elastic Reset works without explicitly modifying the training objective. The method still uses KL penalties (though at lower coefficients), making this characterization somewhat misleading.

## Next Checks

1. **Ablation study on EMA decay rate**: Systematically vary η from 0.95 to 0.999 across all three task scales to identify the optimal decay rate and understand its impact on the reward-drift tradeoff. This would clarify whether the EMA component is essential or if simpler averaging methods would suffice.

2. **Comparison with pure KL regularization**: Implement a baseline with high KL penalty (β=0.01) and no resets, then compare the reward-drift curves directly with Elastic Reset. This would validate the claim that Elastic Reset achieves better tradeoffs without explicitly modifying the objective.

3. **Value function ablation**: Run experiments where the value function is also reset alongside the policy to determine whether maintaining the value function across resets is actually contributing to the improved performance, or if the policy resets alone are responsible for the benefits.