---
ver: rpa2
title: Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks
arxiv_id: '2304.05727'
source_url: https://arxiv.org/abs/2304.05727
tags:
- data
- accuracy
- nement
- original
- carton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hidden Clever Hans strategies
  in pretrained ML models that may go undetected during user validation. It proposes
  Explanation-Guided Exposure Minimization (EGEM), a method that refines models to
  minimize exposure to input variations while preserving validated explanations.
---

# Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks

## Quick Facts
- **arXiv ID**: 2304.05727
- **Source URL**: https://arxiv.org/abs/2304.05727
- **Reference count**: 40
- **Primary result**: EGEM significantly improves robustness to spurious features, achieving higher accuracy on poisoned test data while maintaining performance on clean data

## Executive Summary
This paper addresses the problem of hidden Clever Hans strategies in pretrained ML models that may go undetected during user validation. It proposes Explanation-Guided Exposure Minimization (EGEM), a method that refines models to minimize exposure to input variations while preserving validated explanations. The core idea is soft-pruning model weights based on feature importance and activation patterns. Experiments show EGEM significantly improves robustness to spurious features, achieving higher accuracy on poisoned test data while maintaining performance on clean data. PCA-EGEM, an extension using PCA space, further enhances results by better separating valid and spurious features.

## Method Summary
EGEM is a model refinement approach that uses explanation-guided soft pruning to remove Clever Hans strategies from pretrained models. It works by minimizing the model's exposure to input variations (feature attribution importance) on a clean validation set while maintaining explanation consistency. The method only requires data points whose predictions and explanations have been approved by the user - no prior knowledge about the spurious feature or data containing it is needed. PCA-EGEM extends this by projecting activations onto PCA space to better disentangle observed and unobserved strategies, allowing more precise pruning of spurious features.

## Key Results
- EGEM achieves up to 80% accuracy on 100%-poisoned data compared to near-random performance of baseline models
- PCA-EGEM consistently outperforms EGEM by better separating spurious features from valid features in PCA space
- The method maintains clean data accuracy within 5% of baseline while significantly improving poisoned data performance

## Why This Works (Mechanism)

### Mechanism 1
EGEM can remove hidden Clever Hans strategies without access to data containing those features by pruning weights in a layer such that feature exposure is minimized while maintaining explanation consistency on the clean validation set. This forces the model to rely less on features not validated by the user. The core assumption is that Clever Hans strategies are represented by features not needed for accurate predictions on the clean validation set.

### Mechanism 2
PCA-EGEM improves disentanglement of Clever Hans features from valid features by pruning in PCA space. Mapping activations to PCA space allows separating directions that support observed strategies (top principal components) from unobserved ones. Pruning in this space targets spurious features more precisely, based on the assumption that Clever Hans features are not well represented in the top PCA components derived from clean data.

### Mechanism 3
The pruning strength increases with layer index to prioritize removing features in later layers. By defining thresholds τl that increase with layer index and setting λl accordingly, later layers (closer to output) are pruned more aggressively. This assumes that Clever Hans features are more likely to be represented in later layers where high-level concepts are formed.

## Foundational Learning

- **Concept: Clever Hans effect**
  - Why needed here: Understanding what Clever Hans strategies are and why they are problematic is essential to grasp the motivation for EGEM.
  - Quick check question: What is a Clever Hans strategy and why can it lead to poor generalization?

- **Concept: Explainable AI (XAI) and feature attribution**
  - Why needed here: EGEM relies on explanation techniques to identify which features the model uses and to constrain pruning.
  - Quick check question: How do feature attribution methods like LRP or Gradient×Input help identify Clever Hans strategies?

- **Concept: Principal Component Analysis (PCA)**
  - Why needed here: PCA-EGEM uses PCA to disentangle observed and unobserved strategies in activation space.
  - Quick check question: How does projecting activations onto PCA space help separate different types of features?

## Architecture Onboarding

- **Component map**: Input layer → Convolutional/fully connected layers → Activation layers → Output layer → EGEM inserts pruning layers after specified activation layers → PCA-EGEM adds PCA transformation before pruning

- **Critical path**: 
  1. Load pretrained model
  2. Run inference on clean validation set
  3. Compute explanations (e.g., LRP)
  4. Apply EGEM or PCA-EGEM to obtain refined model
  5. Evaluate on poisoned test set

- **Design tradeoffs**: 
  - Pruning strength vs. clean accuracy: Stronger pruning removes more Clever Hans features but risks losing valid features
  - Layer selection: Pruning earlier layers affects more features but may harm performance; later layers are more targeted
  - Slack parameter: Controls how much clean accuracy can be sacrificed for robustness

- **Failure signatures**: 
  - Clean accuracy drops significantly after refinement
  - No improvement on poisoned data despite refinement
  - Pruning removes too many weights, causing training instability

- **First 3 experiments**: 
  1. Apply EGEM to a small CNN on modified MNIST with 3-pixel corner artifact on digit '8'; measure clean vs. poisoned accuracy
  2. Apply PCA-EGEM to VGG-16 on ImageNet carton task; compare with EGEM and baselines
  3. Vary slack parameter on CelebA blond hair task; analyze recall changes across subgroups

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed methods completely eliminate the model's response to spurious (Clever Hans) features? The paper states that "a complete removal of the model's response to the spurious (CH) feature is usually not achieved" due to entanglement with generalizing features.

- **Open Question 2**: How does the number of user-verified examples affect the effectiveness of model refinement? The paper mentions that "the number of examples for which one collects explanatory feedback is limited" and suggests that not all generalizing features may be present in the refinement data.

- **Open Question 3**: Can the proposed methods be extended to handle more complex data distributions and model architectures? The paper focuses on image classification tasks and CNNs but does not discuss applicability to other domains or model types.

## Limitations
- Complete removal of Clever Hans strategies is usually not achieved due to entanglement with generalizing features
- Computational overhead is significant, requiring training 24 models per task for hyperparameter tuning
- Relies heavily on quality of explanation validation - incorrect user validation cannot be detected by the method

## Confidence
- **High confidence**: EGEM effectively removes known artifacts when properly validated explanations are available (MNIST experiments)
- **Medium confidence**: PCA-EGEM provides consistent improvements across tasks by better separating features in PCA space
- **Low confidence**: The approach generalizes to arbitrary real-world Clever Hans strategies without prior knowledge of what constitutes a spurious feature

## Next Checks
1. Test EGEM on naturally occurring dataset biases (e.g., gender prediction from facial hair in CelebA) where spurious correlations are not artificially introduced
2. Evaluate the robustness of explanation validation itself - introduce subtle artifacts and measure how often users incorrectly validate explanations that depend on these features
3. Compare computational efficiency against alternative approaches like adversarial training or data augmentation for achieving similar robustness gains