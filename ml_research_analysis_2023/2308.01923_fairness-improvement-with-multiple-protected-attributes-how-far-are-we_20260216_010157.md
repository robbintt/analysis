---
ver: rpa2
title: 'Fairness Improvement with Multiple Protected Attributes: How Far Are We?'
arxiv_id: '2308.01923'
source_url: https://arxiv.org/abs/2308.01923
tags:
- fairness
- protected
- methods
- attributes
- improvement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of state-of-the-art fairness
  improvement methods when dealing with multiple protected attributes simultaneously.
  The study evaluates 11 methods on five datasets across financial, social, and medical
  domains, using four ML models and three fairness metrics.
---

# Fairness Improvement with Multiple Protected Attributes: How Far Are We?

## Quick Facts
- arXiv ID: 2308.01923
- Source URL: https://arxiv.org/abs/2308.01923
- Reference count: 40
- Primary result: State-of-the-art fairness improvement methods often decrease fairness for unconsidered protected attributes when dealing with multiple attributes simultaneously.

## Executive Summary
This paper investigates the effectiveness of fairness improvement methods when applied to multiple protected attributes simultaneously, revealing that methods optimized for one attribute can significantly harm fairness for other attributes. The study evaluates 11 state-of-the-art methods across five datasets, four ML models, and three fairness metrics, finding that while accuracy loss remains stable when considering multiple attributes, precision and recall are severely impacted (5-8 times greater than single-attribute scenarios). The research identifies MAAT, FairMask, and RW as the most effective methods for improving intersectional fairness and achieving favorable fairness-performance trade-offs.

## Method Summary
The study evaluates 11 fairness improvement methods on five datasets (Adult, Compas, Default, Mep15, Mep16) with two protected attributes each, using four ML models (LR, SVM, RF, DNN). Each method is applied to original models with 20 repeated runs using 70/30 train/test splits. Fairness is measured using SPD, AOD, and EOD metrics, with intersectional fairness calculated as the maximum disparity between subgroups formed by attribute combinations. The Fairea benchmarking tool classifies trade-off effectiveness across five levels, and statistical significance is assessed using Mann-Whitney U tests.

## Key Results
- Improving fairness for one protected attribute decreases fairness for unconsidered attributes in up to 88.3% of scenarios (57.5% on average)
- Large fairness decreases occur in up to 69.2% of scenarios (29.1% on average)
- Accuracy loss is similar between single and multiple attribute scenarios, but precision and recall are affected 5-8 times more severely
- MAAT, FairMask, and RW achieve the best fairness-performance trade-offs, surpassing baseline in 71.3%, 69.9%, and 68.3% of cases respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-attribute fairness improvement can unintentionally degrade fairness on unconsidered protected attributes.
- Mechanism: Methods optimized for one protected attribute alter model behavior in ways that can disproportionately harm fairness for other attributes, especially when attribute values are correlated.
- Core assumption: The correlation structure between protected attributes influences how improvements for one affect the other.
- Evidence anchors:
  - [abstract] states "improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes."
  - [section] reports "existing methods decrease fairness regarding unconsidered protected attributes in up to 88.3% of scenarios."
  - [corpus] has no direct evidence for the correlation claim, but supports general fairness trade-off literature.
- Break condition: If attribute correlation is zero or near-zero, the negative spillover effect may be minimal.

### Mechanism 2
- Claim: Fairness-performance trade-off is relatively stable when moving from single to multiple protected attributes.
- Mechanism: Accuracy loss does not increase substantially when considering multiple attributes, but precision and recall degrade more, due to balancing multiple fairness constraints.
- Core assumption: The underlying model capacity and data structure allow maintaining accuracy while fairness constraints create precision/recall tension.
- Evidence anchors:
  - [abstract] notes "little difference in accuracy loss when considering single and multiple protected attributes."
  - [section] shows "precision and recall are greatly affected, with the impact...being about 5 times and 8 times that of a single attribute."
  - [corpus] has no direct evidence for this stability claim.
- Break condition: If data is highly imbalanced or precision-critical, the trade-off may become unfavorable.

### Mechanism 3
- Claim: Methods that require access to training data (pre- and in-processing) are more effective at improving intersectional fairness than post-processing methods.
- Mechanism: Access to training data allows more thorough bias mitigation at the source, whereas post-processing can only adjust outputs without changing underlying biases.
- Core assumption: Training data access enables methods to reweight or transform data to balance multiple protected attributes simultaneously.
- Evidence anchors:
  - [abstract] highlights MAAT, FairMask, and RW as top performers in intersectional fairness.
  - [section] states "MAAT, FairMask, and RW...improve intersectional fairness in the most scenarios."
  - [corpus] has no direct evidence linking training data access to effectiveness.
- Break condition: If training data is unavailable or privacy constraints prevent access, post-processing methods must be used despite lower effectiveness.

## Foundational Learning

- Concept: Group fairness metrics (SPD, AOD, EOD)
  - Why needed here: The paper evaluates fairness using these standard metrics for both single and multiple protected attributes.
  - Quick check question: What does a SPD value of 0.2 indicate about model behavior between privileged and unprivileged groups?

- Concept: Intersectional fairness
  - Why needed here: The study measures fairness across subgroups formed by combinations of multiple protected attributes.
  - Quick check question: How many subgroups are formed when considering sex (2 values) and race (2 values) together?

- Concept: Fairness-performance trade-off
  - Why needed here: The paper benchmarks methods using Fairea to evaluate whether improvements in fairness come at an acceptable cost to performance.
  - Quick check question: What are the five trade-off effectiveness levels defined by Fairea?

## Architecture Onboarding

- Component map: Datasets → ML Models → Fairness Improvement Methods → Evaluation Metrics → Statistical Analysis
- Critical path: For each dataset-model pair, apply each fairness method, measure fairness and performance, classify trade-off effectiveness.
- Design tradeoffs: Pre-processing methods (RW) need training data but are effective; post-processing methods (EOP) preserve privacy but are less effective.
- Failure signatures: Large drops in precision/recall when considering multiple attributes; methods classified as "lose-lose" by Fairea.
- First 3 experiments:
  1. Apply RW to Adult dataset with LR model for sex and race attributes; measure SPD, AOD, EOD, and accuracy.
  2. Apply FairMask to Compas dataset with DNN model for sex and race; evaluate intersectional fairness and Fairea trade-off.
  3. Apply EOP to Default dataset with SVM model; compare precision and recall impact vs. pre-processing methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of fairness improvement methods change when considering three or more protected attributes simultaneously?
- Basis in paper: [explicit] The paper states that existing research mostly focuses on single protected attributes and that this study covers 11 methods with two protected attributes, leaving open the question of scalability to multiple attributes.
- Why unresolved: The paper only evaluates two protected attributes per dataset and does not extend the analysis to three or more attributes.
- What evidence would resolve it: Experimental results showing performance, fairness, and trade-off outcomes for three or more protected attributes using the same 11 methods.

### Open Question 2
- Question: Can we develop post-processing fairness improvement methods that match the effectiveness of pre- and in-processing methods for multiple protected attributes?
- Basis in paper: [explicit] The paper identifies that MAAT, FairMask, and RW are the most effective methods but all require training data access, while post-processing methods like EOP show inferior performance.
- Why unresolved: The paper compares existing methods but does not propose or evaluate new post-processing methods designed specifically for multiple protected attributes.
- What evidence would resolve it: Development and evaluation of novel post-processing methods that achieve similar effectiveness to pre- and in-processing methods for intersectional fairness and trade-offs.

### Open Question 3
- Question: What are the underlying mechanisms that cause fairness improvement for one protected attribute to decrease fairness for another attribute?
- Basis in paper: [inferred] The paper observes that improving fairness for one attribute can significantly decrease fairness for unconsidered attributes in up to 88.3% of scenarios, but does not deeply investigate the causal mechanisms.
- Why unresolved: The study identifies the phenomenon but does not conduct detailed analysis of the data distributions, model behaviors, or algorithmic mechanisms that drive this trade-off.
- What evidence would resolve it: Detailed analysis of data characteristics, model behavior patterns, and algorithmic transformations that explain when and why fairness gains for one attribute cause losses for others.

## Limitations
- The study assumes correlation between protected attributes drives fairness degradation without directly testing this relationship
- The 88.3% degradation statistic represents a worst-case scenario that may overstate typical behavior
- The Fairea benchmarking tool's five-tier classification introduces subjective thresholds that may not generalize

## Confidence

- High confidence: General finding that single-attribute fairness improvements can harm unconsidered attributes (supported by multiple datasets and methods)
- Medium confidence: Quantitative claims about degradation rates (88.3% and 57.5% statistics may be sensitive to method selection and dataset characteristics)
- Low confidence: Claims about the mechanism (attribute correlation) driving degradation, as this is asserted but not empirically validated in the paper

## Next Checks

1. Test the correlation hypothesis directly by measuring fairness degradation against statistical dependence measures between protected attributes across datasets
2. Replicate the study with additional datasets containing attributes with varying correlation structures to assess generalizability
3. Compare Fairea's trade-off classifications with alternative benchmarking approaches to validate the 71.3%, 69.9%, and 68.3% effectiveness claims across different domains