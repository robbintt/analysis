---
ver: rpa2
title: Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language
  Models
arxiv_id: '2311.08472'
source_url: https://arxiv.org/abs/2311.08472
tags:
- fairness
- demographic
- performance
- language
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fairness of large language models (LLMs)
  in few-shot learning settings, specifically focusing on how demonstration selection
  strategies impact prediction fairness across different demographic groups. The authors
  evaluate seven popular LLMs on three text classification datasets, comparing performance
  and fairness under various shot selection methods, including semantic similarity,
  diversity, and demographic-based strategies.
---

# Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models

## Quick Facts
- **arXiv ID**: 2311.08472
- **Source URL**: https://arxiv.org/abs/2311.08472
- **Reference count**: 32
- **Key outcome**: This paper investigates the fairness of large language models (LLMs) in few-shot learning settings, specifically focusing on how demonstration selection strategies impact prediction fairness across different demographic groups.

## Executive Summary
This paper investigates how demonstration selection strategies in few-shot learning affect the fairness of large language models (LLMs) across demographic groups. The authors evaluate seven popular LLMs on three text classification datasets, comparing performance and fairness under various shot selection methods including semantic similarity, diversity, and demographic-based strategies. Their findings reveal that model selection for fairness cannot be generalized across datasets, and that adding demonstrations does not consistently yield fairer models. The study demonstrates a complex relationship between performance and fairness, with some models exhibiting positive, negative, or no correlation between these metrics.

## Method Summary
The authors evaluate seven LLM families (LLaMA, LLaMA2, Alpaca, UL2, Flan-UL2, davinci-003, gpt-3.5-turbo) using in-context learning on three text classification datasets with demographic annotations. They compare six demonstration selection strategies: zero-shot, random, similarity, diversity, within, and stratified. For each model-dataset combination, they measure macro-averaged F1 score for performance and 1-GAP (modified GAP metric measuring largest recall difference across demographic groups) for fairness. The study systematically examines how different shot selection methods impact both metrics across various model sizes and training types.

## Key Results
- Model selection for fairness cannot be generalized across datasets
- Adding demonstrations does not consistently yield fairer models
- Fairness and performance are not necessarily correlated in few-shot learning
- Zero-shot settings can be fairer than few-shot settings, even when few-shot settings achieve higher performance
- Similarity selection was the most helpful in both performance and fairness across models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demonstration selection strategy impacts model fairness more than overall performance in few-shot learning with LLMs.
- Mechanism: Different shot selection methods (semantic similarity, diversity, demographic-aware) alter the representation of demographic groups in the prompt context, affecting model predictions and thus fairness metrics.
- Core assumption: LLMs can generalize from in-context examples to unseen data in a way that reflects the composition of those examples.
- Evidence anchors:
  - [abstract] "we explore the effect of shots, which directly affect the performance of models, on the fairness of LLMs as NLP classification systems."
  - [section 3.3] Describes demographic-based selection strategies like "sampling only within the same demographic group" and "using a representative sample."
  - [corpus] Neighbor paper "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification" supports investigation into few-shot fairness with LLMs.
- Break condition: If the model does not learn from in-context examples or the examples are too few to influence predictions.

### Mechanism 2
- Claim: Fairness and performance are not necessarily correlated in few-shot learning with LLMs; improvements in one do not guarantee improvements in the other.
- Mechanism: The relationship between performance (F1) and fairness (1-GAP) is inconsistent across datasets and models, suggesting different underlying factors drive each metric.
- Core assumption: Fairness and performance are independent dimensions of model quality in few-shot learning.
- Evidence anchors:
  - [section 4.2] "Our experiments (perhaps distressingly) exhibit both positive and negative correlations for certain models across datasets."
  - [section 4.4] "While similarity selection was the most helpful in both performance and fairness, we would hope that there exists a single demonstration selection strategy that consistently improves performance and fairness."
  - [corpus] Neighbor paper "Improving LLM Group Fairness on Tabular Data via In-Context Learning" suggests that group fairness can be improved independently of overall performance.
- Break condition: If fairness and performance are found to be inherently linked in a specific dataset or model architecture.

### Mechanism 3
- Claim: Zero-shot settings can be fairer than few-shot settings, even when few-shot settings achieve higher performance.
- Mechanism: Adding demonstrations to prompts may introduce biases or representational imbalances that are not present in the zero-shot baseline, affecting fairness metrics.
- Core assumption: The zero-shot model, without any examples, may have a more balanced prior over demographic groups.
- Evidence anchors:
  - [section 4.3] "The relationship between demonstrations and fairness is more varied. In general, when both fairness and performance in zeroshot settings are high, adding demonstrations does not help and can even harm fairness."
  - [section 4.4] "In contrast, Diversity selection has less consistent behavior, where it helps LLaMA-65B and Flan-UL2, but hurts every other model."
  - [corpus] Neighbor paper "Sociocultural knowledge is needed for selection of shots in hate speech detection tasks" implies that shot selection can introduce sociocultural biases.
- Break condition: If all models consistently show improved fairness with added demonstrations.

## Foundational Learning

- Concept: Few-shot learning and in-context learning
  - Why needed here: The paper investigates how the choice of demonstration examples (shots) in few-shot learning affects the fairness of LLM predictions.
  - Quick check question: What is the difference between few-shot learning and traditional supervised learning?

- Concept: Fairness metrics and bias evaluation
  - Why needed here: The paper evaluates the fairness of LLMs by measuring performance disparities across demographic groups using metrics like 1-GAP and Kruskal-Wallis tests.
  - Quick check question: How is the 1-GAP metric different from standard accuracy or F1 score?

- Concept: Semantic similarity and diversity in example selection
  - Why needed here: The paper compares different demonstration selection strategies, including semantic similarity and diversity, to understand their impact on fairness.
  - Quick check question: Why might selecting semantically diverse examples be beneficial for model performance?

## Architecture Onboarding

- Component map:
  - Data (Bias in Bios, Twitter Sentiment, HateXplain) -> Preprocessing (demographic labels extraction) -> Model (7 LLM families) -> Prompting (in-context learning) -> Demonstration Selection (6 strategies) -> Evaluation (F1, 1-GAP metrics)

- Critical path:
  1. Load dataset and split into training, validation, and test sets with demographic labels
  2. Select LLM model and configure prompt with demonstration selection strategy
  3. Generate predictions for test set using in-context learning
  4. Calculate F1 and 1-GAP metrics
  5. Perform statistical tests to determine fairness significance

- Design tradeoffs:
  - Model choice: Open-source vs. closed-source, size vs. cost, pretrained vs. finetuned
  - Demonstration selection: Semantic relevance vs. diversity, demographic balance vs. representativeness
  - Prompting: Information content vs. brevity, demographic awareness vs. potential bias

- Failure signatures:
  - Performance below random baseline: Indicates model is not learning from demonstrations
  - Perfect fairness but low performance: Suggests model is overly cautious or not utilizing context
  - Inconsistent results across datasets: Implies fairness is highly dataset-dependent

- First 3 experiments:
  1. Compare zero-shot vs. 10-shot performance and fairness for a single model on one dataset
  2. Evaluate all six demonstration selection strategies for a single model on one dataset
  3. Compare fairness across all models for a single dataset using the best-performing demonstration selection strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine when a LLM is being unfair in its predictions without access to demographically labeled data?
- Basis in paper: [explicit] The paper states "There is now a large focus on fine-tuning LLMs... The goal of these methods has been better instruction following and improved accuracy on prediction tasks, but our results suggest they do not always make models fairer. How can we include fairness objectives in this training process?"
- Why unresolved: Current methods for evaluating LLM fairness require demographically labeled data, which is often unavailable or expensive to collect. The paper highlights this limitation and suggests developing automatic metrics for fairness.
- What evidence would resolve it: Development of a robust proxy metric or statistical test that can reliably detect fairness issues in LLM predictions without requiring demographic labels, validated across multiple datasets and model architectures.

### Open Question 2
- Question: What is the relationship between model size and fairness in few-shot learning with LLMs?
- Basis in paper: [inferred] The paper compares models of different sizes (e.g., LLaMA 7B vs 65B, Llama2 13B vs 70B) and observes varying fairness results, but doesn't explicitly analyze the relationship between size and fairness.
- Why unresolved: While the paper mentions that larger models tend to perform better, it doesn't systematically investigate whether model size correlates with fairness improvements or degradation.
- What evidence would resolve it: A controlled study varying model size while holding other factors constant, measuring both performance and fairness metrics across multiple tasks and datasets.

### Open Question 3
- Question: Can we develop a demonstration selection strategy that consistently improves both performance and fairness across different datasets and model architectures?
- Basis in paper: [explicit] The paper concludes "While we cannot say that a single selection method performs the best across all datasets and models... our experiments suggest that, on average, similarity is the better option."
- Why unresolved: The paper finds that different selection methods work better for different models and datasets, and that no single strategy consistently improves both metrics.
- What evidence would resolve it: Development and validation of a meta-learning approach that learns to select optimal demonstrations based on model architecture, dataset characteristics, and task type, demonstrating consistent improvements across diverse settings.

## Limitations

- Evaluation is restricted to three text classification datasets, limiting the breadth of demographic attributes and task types examined
- The 1-GAP metric may not capture all aspects of algorithmic bias, particularly subtle forms of discrimination
- The study focuses on in-context learning without fine-tuning, which may not represent all practical deployment scenarios for LLMs

## Confidence

- **High confidence**: The observation that adding demonstrations does not consistently improve fairness, supported by consistent patterns across multiple datasets and models
- **Medium confidence**: The finding that model selection for fairness cannot be generalized across datasets, as this is based on a limited set of three datasets
- **Low confidence**: The claim about zero-shot being fairer than few-shot in certain cases, as this appears to be highly dataset and model dependent

## Next Checks

1. **Dataset expansion validation**: Replicate the study using additional text classification datasets with diverse demographic attributes to verify if the observed patterns of fairness inconsistency hold across a broader range of domains and demographic groups.

2. **Metric sensitivity analysis**: Compare the 1-GAP metric results with alternative fairness metrics (e.g., equal opportunity difference, demographic parity) to determine if the observed fairness patterns are consistent across different measurement approaches.

3. **Cross-task generalization**: Test the same demonstration selection strategies on non-classification tasks (e.g., generation or summarization) to determine if the relationship between shots, fairness, and performance generalizes beyond the classification setting studied here.