---
ver: rpa2
title: RJHMC-Tree for Exploration of the Bayesian Decision Tree Posterior
arxiv_id: '2312.01577'
source_url: https://arxiv.org/abs/2312.01577
tags:
- tree
- decision
- split
- each
- leaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new MCMC method for Bayesian decision tree
  learning, called RJHMC-Tree, which uses Hamiltonian Monte Carlo (HMC) to improve
  exploration of the posterior. Standard decision trees have a piecewise constant
  likelihood, making HMC incompatible.
---

# RJHMC-Tree for Exploration of the Bayesian Decision Tree Posterior

## Quick Facts
- arXiv ID: 2312.01577
- Source URL: https://arxiv.org/abs/2312.01577
- Authors: 
- Reference count: 40
- Key outcome: RJHMC-Tree improves Bayesian decision tree learning using HMC with soft approximations, achieving higher acceptance rates and better predictive accuracy than existing methods.

## Executive Summary
This paper introduces RJHMC-Tree, a novel MCMC method for Bayesian decision tree learning that leverages Hamiltonian Monte Carlo (HMC) to improve exploration of the posterior distribution. The key innovation is overcoming HMC's incompatibility with piecewise constant decision tree likelihoods by introducing two softened tree formulations: HMC-DF (soft decision function) and HMC-DFI (soft input selection). The method uses a three-step update scheme combining HMC for parameter updates with reversible-jump MCMC for topology changes. Experiments demonstrate superior performance compared to existing methods on both synthetic and real-world datasets, often producing simpler trees with better predictive accuracy.

## Method Summary
RJHMC-Tree is a three-step MCMC algorithm for Bayesian decision tree learning. First, tree parameters are updated using Hamiltonian Monte Carlo (HMC) with either HMC-DF (soft decision function) or HMC-DFI (soft input selection) formulations. Second, the tree topology is modified using standard MCMC moves (grow, prune, stay) within a reversible-jump framework. Third, parameters are updated again with HMC. The reversible-jump framework handles the transdimensional nature of the problem when tree structure changes. A biased acceptance probability is used during burn-in to accelerate convergence to high-likelihood regions. The method assumes independence between tree topology and parameters, simplifying the transition kernel.

## Key Results
- RJHMC-Tree achieves higher acceptance rates than existing methods (CGM, WU, SMC) on tested datasets
- The algorithm produces trees with better predictive accuracy (lower misclassification/MSE) across multiple real-world datasets
- RJHMC-Tree often generates simpler trees with fewer leaf nodes while maintaining or improving performance
- Both HMC-DF and HMC-DFI implementations show consistent improvements, with HMC-DFI performing particularly well on datasets with continuous inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HMC improves acceptance rates by using gradient information from the posterior.
- Mechanism: Standard decision tree likelihoods are piecewise constant due to hard splits, making derivatives undefined. Softening split functions using HMC-DF or HMC-DFI creates smooth, differentiable likelihoods that allow HMC to leverage gradient information for more efficient proposals.
- Core assumption: Soft approximations sufficiently approximate the true posterior while enabling HMC to work.
- Evidence anchors:
  - [abstract] "These modifications allow the derivatives of the likelihood to be computed and enable HMC to be applied."
  - [section 3.1] "To overcome this pathology, we introduce two new parameterisations such that HMC can be applied..."
  - [corpus] Weak - corpus does not discuss HMC or soft splits in decision trees.
- Break condition: If soft approximations introduce too much bias or if gradients are still too noisy to be useful for HMC.

### Mechanism 2
- Claim: RJHMC framework handles the transdimensional nature of Bayesian decision trees.
- Mechanism: When tree topology changes, the dimension of the parameter vector changes. RJHMC handles this using dimension-matching transformations and reversible jumps, allowing HMC to work across different tree sizes while maintaining detailed balance.
- Core assumption: RJHMC framework properly handles changes in dimensionality while maintaining detailed balance.
- Evidence anchors:
  - [section 3.4.1] "The assumed independence between tree topology and parameters implies that this transition kernel can be simplified... The RJHMC-Tree algorithm is based on the assumption that the dimension of the parameter vector varies throughout the sampling routine."
  - [section 3.4.2] "Exploration of the Bayesian decision tree posterior via RJHMC-Tree can be thought of as generating samples from a transdimensional model..."
  - [corpus] Weak - corpus does not mention RJHMC or reversible-jump methods in decision trees.
- Break condition: If dimension-matching transformations introduce significant bias or if acceptance probabilities for topology changes are too low.

### Mechanism 3
- Claim: Biased acceptance probability during burn-in accelerates convergence to high-likelihood regions.
- Mechanism: During burn-in, a simplified acceptance probability omits ratios of intermediate distributions, which can slow down movement. This allows faster convergence to areas of high likelihood before switching to full acceptance probability for sampling.
- Core assumption: Biased acceptance probability does not significantly distort the posterior distribution during burn-in.
- Evidence anchors:
  - [section 3.4.2] "The other is a biased version which is used during the burn-in phase, allowing the chain to converge rapidly to areas of high probability while targeting areas similar to the true distribution."
  - [section 5] "One interesting aspect of the method is that in some cases, the biased acceptance probability leads to trees of higher likelihood, which appears to, on average, find trees with better performance metrics."
  - [corpus] Weak - corpus does not discuss biased acceptance probabilities or burn-in strategies.
- Break condition: If biased acceptance probability causes the chain to get stuck in local optima or if final samples are significantly biased.

## Foundational Learning

- Concept: Bayesian inference and posterior sampling
  - Why needed here: The paper aims to learn decision trees from data using a Bayesian approach, which requires sampling from the posterior distribution over tree structures and parameters.
  - Quick check question: What is the fundamental equation in Bayesian inference that relates the posterior distribution to the likelihood and prior?

- Concept: Markov chain Monte Carlo (MCMC) methods
  - Why needed here: MCMC methods are used to approximate the posterior distribution by generating a Markov chain of samples. The quality of these samples is crucial for accurate inference.
  - Quick check question: What is the main challenge with using MCMC for Bayesian decision trees, and how does RJHMC-Tree address it?

- Concept: Hamiltonian Monte Carlo (HMC)
  - Why needed here: HMC is an MCMC method that uses gradient information to propose new samples, which can be more efficient than random-walk proposals. However, it requires a smooth, differentiable likelihood.
  - Quick check question: Why is HMC normally incompatible with standard decision tree likelihoods, and how do the soft approximations in RJHMC-Tree solve this?

## Architecture Onboarding

- Component map: Initialize tree -> Update parameters with HMC (HMC-DF or HMC-DFI) -> Propose topology change (grow, prune, stay) -> Update parameters with HMC -> Accept/reject with RJHMC acceptance probability

- Critical path:
  1. Initialize a decision tree
  2. Update tree parameters using HMC (HMC-DF or HMC-DFI)
  3. Propose a new tree topology using MCMC moves (grow, prune, stay)
  4. Update parameters again using HMC
  5. Accept or reject the proposed tree using appropriate acceptance probability (biased during burn-in, full RJHMC during sampling)
  6. Repeat steps 2-5 for desired number of iterations

- Design tradeoffs:
  - HMC-DF vs HMC-DFI: HMC-DF is more interpretable but requires mixed discrete/continuous updates, while HMC-DFI is fully continuous but less interpretable
  - Biased acceptance during burn-in: Improves convergence speed but may introduce bias
  - Softness parameter (h): Affects trade-off between approximation accuracy and HMC efficiency

- Failure signatures:
  - Low acceptance rates: May indicate poor mixing or inappropriate proposal distributions
  - Overfitting: Trees with too many leaves may indicate model is too complex for data
  - Slow convergence: May indicate chain is getting stuck in local optima or proposals are ineffective

- First 3 experiments:
  1. Run RJHMC-Tree on simple synthetic dataset with known tree structure to verify it can recover true tree
  2. Compare acceptance rates and predictive accuracy of RJHMC-Tree to standard MCMC method on real-world dataset
  3. Investigate effect of softness parameter (h) on performance by running with different h values on range of datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for varying the split sharpness parameter (h) during adaptation phase for different input data types (discrete vs continuous)?
- Basis in paper: [inferred] Authors mention optimal final value of h is linked to spread in data, with larger values better for discrete inputs and smaller values for real attributes. They suggest investigating way to define variation of h independently for each input dimension.
- Why unresolved: Paper does not provide specific method for varying h based on input type or dimension, only suggests this could be area for further investigation.
- What evidence would resolve it: Empirical studies comparing performance of different h adaptation schemes for various input types and dimensions, showing impact on acceptance rates, predictive accuracy, and tree complexity.

### Open Question 2
- Question: How can tree topology update scheme be improved to move beyond current "random walk" proposal mechanism used in all Bayesian decision tree methods?
- Basis in paper: [explicit] Authors acknowledge tree topology update scheme still relies on "random walk" proposal mechanism, which is limitation of their method and all Bayesian decision tree methods.
- Why unresolved: Paper does not propose specific alternative to random walk proposal mechanism, only identifies this as area for future work.
- What evidence would resolve it: Development and testing of new tree topology proposal mechanism that outperforms random walk approach in terms of acceptance rates, predictive accuracy, and tree complexity, validated on multiple datasets.

### Open Question 3
- Question: What is impact of number of HMC samples taken through intermediate distributions (k) on overall performance of RJHMC-Tree algorithm?
- Basis in paper: [explicit] Authors mention number of HMC samples to take through intermediate distributions (k) is input parameter to RJHMC-Tree algorithm, but don't discuss its impact on performance or provide guidance on selecting appropriate value.
- Why unresolved: Paper doesn't provide empirical results or analysis on how choice of k affects algorithm's performance metrics.
- What evidence would resolve it: Empirical studies comparing performance of RJHMC-Tree with different values of k on multiple datasets, showing impact on acceptance rates, predictive accuracy, tree complexity, and computational efficiency.

## Limitations
- Approximation quality of soft formulations may introduce bias into posterior distribution
- Computational complexity is increased due to multiple HMC updates and reversible-jump transitions
- Hyperparameter tuning (especially softness parameter h) lacks clear guidelines for different problem types

## Confidence
**High Confidence**:
- Using soft approximations to enable HMC in decision trees is sound and well-motivated
- Experimental results show RJHMC-Tree achieves higher acceptance rates and better predictive accuracy than existing methods

**Medium Confidence**:
- RJHMC framework for handling transdimensional models is well-established but specific implementation performance in decision trees may vary
- Biased acceptance probability during burn-in accelerates convergence but long-term impact on final samples is uncertain

**Low Confidence**:
- No thorough theoretical analysis of approximation error or convergence properties
- Generalizability to other datasets or domains is uncertain due to limited experimental scope

## Next Checks
1. Investigate impact of softness parameter (h) on approximation quality by running RJHMC-Tree with different h values on various datasets and comparing results to true posterior distribution or other approximation methods
2. Measure computational time and memory requirements of RJHMC-Tree compared to standard MCMC methods on datasets of varying sizes to identify bottlenecks and potential optimizations
3. Conduct systematic sensitivity analysis of RJHMC-Tree to hyperparameters (h, burn-in iterations, NUTS adaptation parameters) by running with different settings and analyzing impact on results across multiple datasets