---
ver: rpa2
title: 'MCC-KD: Multi-CoT Consistent Knowledge Distillation'
arxiv_id: '2310.14747'
source_url: https://arxiv.org/abs/2310.14747
tags:
- rationales
- reasoning
- mcc-kd
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring complex reasoning
  capabilities from large language models (LLMs) to smaller models through knowledge
  distillation. The authors propose Multi-CoT Consistent Knowledge Distillation (MCC-KD),
  which improves both the diversity and consistency of rationales generated by LLMs.
---

# MCC-KD: Multi-CoT Consistent Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2310.14747
- **Source URL**: https://arxiv.org/abs/2310.14747
- **Authors**: [Not specified in source]
- **Reference count**: 13
- **Primary result**: Improves reasoning transfer from LLMs to smaller models via diverse, consistent rationales with up to 3.57% accuracy gains on GSM8K

## Executive Summary
MCC-KD addresses the challenge of transferring complex reasoning capabilities from large language models to smaller models through knowledge distillation. The method improves both the diversity and consistency of rationales generated by LLMs by employing N-gram similarity filtering and bidirectional KL-divergence consistency loss. This enables smaller student models to learn robust reasoning patterns without overfitting to specific rationale styles.

The approach is evaluated across different model architectures (LLaMA/FlanT5) and scales (3B/7B/11B/13B) on mathematical and commonsense reasoning benchmarks. MCC-KD achieves superior performance on in-distribution datasets with accuracy improvements of up to 3.57% on GSM8K and demonstrates robust generalization on out-of-distribution datasets with gains up to 1.83% on ASDiv.

## Method Summary
MCC-KD employs a multi-step pipeline to extract and distill reasoning capabilities from large language models. First, it generates multiple rationales using GPT-3.5-Turbo with temperature τ=1.3 and zero-shot CoT prompting. Second, it filters rationales using 3-gram Jaccard similarity to retain K=5 diverse rationales per question. Third, it trains student models (LLaMA/FlanT5) with LoRA adapters using cross-entropy loss plus KL-divergence consistency loss between rationale predictions. The method uses random pairwise rationale sampling during training and applies different consistency loss weights (α=0.01 for math, α=0.1 for commonsense) to balance consistency enforcement and flexibility.

## Key Results
- **In-distribution performance**: Up to 3.57% accuracy improvement on GSM8K and 1.04% on ASDiv
- **Out-of-distribution generalization**: Up to 1.83% accuracy gain on ASDiv and 0.59% on CommonsenseQA
- **Architecture scalability**: Effective across LLaMA (3B-13B) and FlanT5 (XL-XXL) model scales

## Why This Works (Mechanism)

### Mechanism 1: Diverse Rationale Generation
- **Claim**: Diverse rationales reduce student model overfitting by exposing it to multiple reasoning paths
- **Mechanism**: N-gram similarity filtering ensures selected rationales cover distinct reasoning steps rather than repetitive phrasing
- **Core assumption**: High similarity between rationales correlates with redundancy in reasoning paths
- **Evidence anchors**: "The diversity of reasoning rationales plays a crucial role in transferring reasoning capabilities"
- **Break condition**: If teacher model generates very similar rationales even with varied temperatures, diversity filtering becomes ineffective

### Mechanism 2: Consistency Loss via KL-divergence
- **Claim**: Bidirectional KL-divergence enforces alignment among answer distributions from different rationales
- **Mechanism**: Penalizes divergence between answer distributions from different rationales, ensuring the student treats them as equivalently valid paths
- **Core assumption**: Different rationales leading to the same answer produce similar final answer distributions
- **Evidence anchors**: "enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence"
- **Break condition**: If teacher rationales produce conflicting answers or KL-divergence constraint is too strong

### Mechanism 3: Random Pairwise Sampling
- **Claim**: Random sampling prevents student model from memorizing specific rationale pairs
- **Mechanism**: Randomly selects two distinct rationales per training epoch to compute consistency loss
- **Core assumption**: Random sampling introduces sufficient variability to prevent memorization
- **Evidence anchors**: "randomly select two distinct ones from the set of rationales in each training epoch"
- **Break condition**: If number of rationales K is too small, random sampling may not provide sufficient diversity

## Foundational Learning

- **Concept**: Knowledge Distillation
  - Why needed here: Transfer reasoning capabilities from large LLMs (teachers) to smaller models (students) that cannot directly generate complex rationales
  - Quick check question: What is the key difference between standard knowledge distillation and CoT-based distillation?

- **Concept**: Chain of Thought (CoT) Prompting
  - Why needed here: Enables LLMs to generate intermediate reasoning steps that can be used as training signals for smaller models
  - Quick check question: Why do LLMs require >100B parameters to effectively generate CoTs?

- **Concept**: Bidirectional KL-divergence
  - Why needed here: Measures and minimizes the difference between probability distributions of answers from different rationales
  - Quick check question: How does bidirectional KL-divergence differ from standard KL-divergence in this context?

## Architecture Onboarding

- **Component map**: Teacher LLM (GPT-3.5-Turbo) → Rationale Generator → Diversity Filter (N-gram Jaccard) → Student Model (LLaMA/FlanT5) with LoRA adapters → Training loop with consistency loss
- **Critical path**: Teacher rationale generation → Diversity filtering → Pairwise KL-divergence consistency loss → Student parameter updates via LoRA
- **Design tradeoffs**: High teacher temperature increases rationale diversity but may reduce quality; low temperature ensures quality but reduces diversity; KL-divergence strength (α) must balance consistency enforcement vs flexibility
- **Failure signatures**: Poor student performance on in-distribution data (overfitting to training rationales), degraded OOD performance (lack of generalization), training instability (KL-divergence conflicts), or high similarity scores between filtered rationales (ineffective diversity enforcement)
- **First 3 experiments**:
  1. Verify teacher model generates diverse rationales at temperature τ=1.3 by calculating Jaccard similarity across generated rationales for same question
  2. Test KL-divergence consistency loss with α=0.01 on GSM8K using LLaMA-7B to confirm training stability and performance improvement
  3. Compare student performance with and without diversity filtering on ASDiv to quantify benefit of N-gram-based rationale selection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of MCC-KD scale with different teacher LLM sizes (e.g., GPT-4, PaLM)?
- **Basis in paper**: The paper only evaluates MCC-KD with GPT-3.5-Turbo as the teacher model
- **Why unresolved**: The impact of teacher size on distillation quality remains untested
- **What evidence would resolve it**: Comparative experiments using larger teacher models (e.g., GPT-4, PaLM) with MCC-KD to measure performance gains

### Open Question 2
- **Question**: What is the impact of varying the number of rationales (K) beyond 5 on both performance and computational cost?
- **Basis in paper**: The paper explores K=2 to 10 rationales but settles on K=5, noting diminishing returns beyond this point
- **Why unresolved**: The optimal K value may depend on task complexity, dataset size, or model scale
- **What evidence would resolve it**: Extensive ablation studies varying K across different tasks, datasets, and model scales

### Open Question 3
- **Question**: How does MCC-KD perform when applied to non-reasoning tasks (e.g., summarization, classification)?
- **Basis in paper**: MCC-KD is evaluated only on reasoning tasks (mathematical and commonsense reasoning)
- **Why unresolved**: The method's reliance on diverse rationales and consistency constraints may not translate directly to tasks without explicit reasoning steps
- **What evidence would resolve it**: Experiments applying MCC-KD to diverse NLP tasks and comparing results with standard distillation methods

## Limitations

- **Teacher quality dependence**: The method's effectiveness heavily relies on GPT-3.5-Turbo generating high-quality, diverse rationales, with no quantitative measures of rationale quality reported
- **Component contribution unclear**: The paper lacks ablation studies isolating the contributions of diversity filtering, consistency loss, and random sampling to overall performance gains
- **Hyperparameter sensitivity**: Fixed hyperparameters are used without exploring sensitivity or providing guidance for dataset-specific tuning

## Confidence

- **High confidence**: Claims about MCC-KD's effectiveness on in-distribution datasets (GSM8K, ASDiv) are well-supported by reported accuracy improvements
- **Medium confidence**: Claims about out-of-distribution generalization are supported but lack deeper analysis of failure cases or comparison to alternative methods
- **Low confidence**: Claims about specific contributions of diversity filtering, consistency loss, and random sampling are weakly supported due to absence of ablation studies

## Next Checks

1. **Rationale quality audit**: Generate 100 rationales using MCC-KD's pipeline on GSM8K and compute answer correctness rate, pairwise Jaccard similarity distribution, and human evaluation of reasoning quality

2. **Ablation study on consistency loss**: Train student models with MCC-KD pipeline but vary α (consistency loss weight) from 0 to 0.1 in increments of 0.02, measuring both in-distribution accuracy and OOD generalization

3. **Teacher model comparison**: Repeat MCC-KD training using alternative teacher models (e.g., LLaMA-7B, GPT-3.5 with different temperatures) on the same GSM8K dataset to test method robustness and identify whether improvements are primarily due to teacher quality or the MCC-KD pipeline itself