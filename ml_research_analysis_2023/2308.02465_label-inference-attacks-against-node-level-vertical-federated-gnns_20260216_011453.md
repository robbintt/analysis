---
ver: rpa2
title: Label Inference Attacks against Node-level Vertical Federated GNNs
arxiv_id: '2308.02465'
source_url: https://arxiv.org/abs/2308.02465
tags:
- attack
- accuracy
- gradients
- learning
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BlindSage, the first label inference attack
  on node-level vertical federated graph neural networks (VFGNNs). The attack exploits
  gradient leakage from the server to infer private labels held by the server, requiring
  no background knowledge about label relationships.
---

# Label Inference Attacks against Node-level Vertical Federated GNNs

## Quick Facts
- **arXiv ID**: 2308.02465
- **Source URL**: https://arxiv.org/abs/2308.02465
- **Reference count**: 40
- **Primary result**: First label inference attack on node-level vertical federated GNNs (VFGNNs) achieving near 100% accuracy in full knowledge scenarios and above 85% without any knowledge of model architecture or number of classes

## Executive Summary
This paper introduces BlindSage, the first label inference attack against node-level vertical federated graph neural networks (VFGNNs). The attack exploits gradient leakage from the server to infer private labels held by the server, requiring no background knowledge about label relationships. BlindSage uses synthetic labels and a server model approximation to generate adversarial gradients, which are matched against real gradients from the server through a loss function. The attack is evaluated across three GNN architectures (GCN, GAT, GraphSAGE) and four datasets, demonstrating high effectiveness even under limited attacker knowledge.

## Method Summary
BlindSage operates by generating synthetic labels and approximating the server model to create adversarial gradients. These gradients are compared with real gradients received from the server using an L2 norm-based matching loss. The synthetic labels are iteratively updated to minimize the difference between adversarial and real gradients, ultimately revealing the true labels. The attack is designed to work in three scenarios: full knowledge (attacker knows model architecture and number of classes), partial knowledge (attacker knows architecture but not number of classes), and no knowledge (attacker knows neither). When the number of classes is unknown, the attacker estimates it by clustering node embeddings using HDBSCAN.

## Key Results
- Achieves near 100% attack accuracy in full knowledge scenarios across all tested datasets and architectures
- Maintains above 85% accuracy even without knowledge of model architecture or number of classes
- Common defenses like differential privacy and gradient compression fail to mitigate the attack without harming main task performance
- Attack remains effective with approximate server model architectures by iteratively updating both synthetic labels and model approximation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: BlindSage can infer labels by matching adversarial gradients with real gradients from the server.
- **Mechanism**: The attacker generates synthetic labels and a model approximation to create adversarial gradients. These are compared with the real gradients received from the server using a matching loss function. The synthetic labels are iteratively updated to minimize the difference between adversarial and real gradients.
- **Core assumption**: Gradients from the server inherently contain information about the true labels due to the cross-entropy loss calculation.
- **Evidence anchors**:
  - [abstract]: "BlindSage uses synthetic labels and a server model approximation to generate adversarial gradients, matching them against real gradients through a loss function."
  - [section]: "Since the server calculates the gradients using the real labels, RealLabels, in the loss, we argue that the adversary can exploit the received gradients to conduct an attack during the training phase and extract useful information about them."
- **Break condition**: If the gradients are sufficiently obfuscated (e.g., by adding large amounts of noise or using aggressive gradient compression), the matching loss becomes ineffective, preventing accurate label inference.

### Mechanism 2
- **Claim**: The attacker can estimate the number of classes even without prior knowledge by clustering node embeddings.
- **Mechanism**: When the number of classes is unknown, the attacker uses HDBSCAN (or a similar clustering algorithm) on the node embeddings produced by their local model. The optimal number of clusters corresponds to the number of classes.
- **Core assumption**: Node embeddings learned during federated learning contain discriminative features that reflect class membership, making them suitable for clustering.
- **Evidence anchors**:
  - [abstract]: "Even when the attacker has no information about the used architecture or the number of classes, the accuracy remains above 85% in most instances."
  - [section]: "In this setting, the attacker must approximate the server model as in the previous case and estimate the number of classes n to perform the attack. Our intuition is that node embeddings learned during the federated learning include crucial information about the classes the nodes belong to."
- **Break condition**: If the local model has not been sufficiently trained to produce meaningful embeddings, clustering will fail to accurately estimate the number of classes, reducing attack effectiveness.

### Mechanism 3
- **Claim**: The attack remains effective even with approximate server model architectures.
- **Mechanism**: The attacker uses a server model approximation with a different architecture than the true server model. Despite the mismatch, the attack can still achieve high accuracy by iteratively updating both the synthetic labels and the model approximation to minimize the matching loss.
- **Core assumption**: Even an imperfect approximation of the server model can capture enough information to enable successful label inference through gradient matching.
- **Evidence anchors**:
  - [abstract]: "Experiments across three GNN architectures (GCN, GAT, GraphSAGE) and four datasets show attack accuracy of nearly 100% in full knowledge scenarios, and above 85% even without any knowledge of the model architecture or the number of classes."
  - [section]: "In this case, the attacker can hypothesize a classification model, possibly with a different architecture than the server one. Then, the same strategy of the previous case can be adopted to estimate the association between the n labels and the training data."
- **Break condition**: If the architecture mismatch is too large (e.g., significantly different number of layers or incompatible activation functions), the adversarial gradients may diverge too much from the real gradients, making the matching loss ineffective.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their node classification task.
  - **Why needed here**: The attack targets VFL systems using GNNs for node classification. Understanding GNN architectures (GCN, GAT, GraphSAGE) and their training process is essential to grasp how the attack exploits gradient leakage.
  - **Quick check question**: What is the key difference between GCN and GAT in terms of neighbor aggregation?

- **Concept**: Vertical Federated Learning (VFL) and gradient sharing.
  - **Why needed here**: The attack exploits the fact that in VFL, clients receive gradients from the server without knowing the true labels. Understanding the VFL framework and how gradients are computed and shared is crucial.
  - **Quick check question**: In VFL, which party holds the labels, and which parties only receive gradients?

- **Concept**: Adversarial machine learning and gradient-based attacks.
  - **Why needed here**: The attack is an example of a gradient-based inference attack. Knowledge of how gradients can leak information and how adversarial examples are crafted is necessary to understand the attack mechanism.
  - **Quick check question**: How does adding noise to gradients affect the success of gradient-based inference attacks?

## Architecture Onboarding

- **Component map**: Server -> Active Client (Attacker) -> Passive Clients
- **Critical path**: Server computes gradients → Attacker receives gradients → Attacker generates adversarial gradients → Matching loss computed → Synthetic labels and model approximation updated → Labels inferred
- **Design tradeoffs**:
  - Attack accuracy vs. computational cost: Higher learning rates and more attack iterations improve accuracy but increase computation.
  - Model approximation accuracy vs. attack stealth: Closer approximation to the true server model improves accuracy but may be harder to achieve without detection.
  - Defense effectiveness vs. main task performance: Strong defenses (e.g., high noise, aggressive compression) protect privacy but degrade model accuracy.
- **Failure signatures**:
  - Low attack accuracy: Indicates ineffective gradient matching, possibly due to strong defenses or poor model approximation.
  - High variance in attack accuracy across epochs: Suggests instability in gradient matching, possibly due to fluctuating gradient magnitudes.
  - Rapid decline in attack accuracy after initial epochs: Indicates that the attacker started too late, missing the most informative gradients.
- **First 3 experiments**:
  1. **Baseline attack**: Implement BlindSage with full knowledge (attacker knows model architecture and number of classes) on a simple dataset (e.g., Cora) using GCN. Measure attack accuracy and observe gradient matching behavior.
  2. **Model approximation impact**: Test BlindSage with partial knowledge by using a server model approximation with one additional layer. Compare attack accuracy with the baseline.
  3. **Defense evaluation**: Apply gradient clipping and noise addition to the gradients. Measure the impact on both attack accuracy and main task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would alternative clustering algorithms be compared to HDBSCAN for estimating the number of classes in the no-knowledge scenario?
- Basis in paper: [explicit] The authors state that HDBSCAN was chosen for its ability to automatically estimate the optimal number of clusters, but note that any clustering algorithm with a heuristic for estimating the number of clusters could be used.
- Why unresolved: The paper only tested HDBSCAN for class estimation. Performance of other clustering algorithms like k-means with silhouette method or DBSCAN is not evaluated.
- What evidence would resolve it: Experiments comparing BlindSage's accuracy using different clustering algorithms (k-means, DBSCAN, etc.) for class estimation against the same datasets and scenarios.

### Open Question 2
- Question: Would modifying the VFL protocol to include differential gradients (gradients with opposite signs) mitigate BlindSage attacks without significantly impacting main task performance?
- Basis in paper: [inferred] The authors mention in the defense section that flipping the sign of gradients during FL training may prevent successful label inference, but this was only mentioned as future work, not tested.
- Why unresolved: This potential defense strategy was only speculated about but not implemented or evaluated experimentally.
- What evidence would resolve it: Experiments implementing and testing differential gradients in VFL training, measuring both attack accuracy and main task performance across various datasets.

### Open Question 3
- Question: How would BlindSage perform against more complex GNN architectures like Graph Attention Networks with multiple attention heads or deeper Graph Convolutional Networks?
- Basis in paper: [explicit] The authors tested BlindSage against GCN, GAT, and GraphSAGE architectures but used relatively standard configurations. The Reddit dataset results showed decreased performance with GCN.
- Why unresolved: The paper focused on standard GNN architectures and didn't explore how BlindSage scales with more complex or deeper models that might have more intricate gradient patterns.
- What evidence would resolve it: Experiments testing BlindSage against deeper GCN variants, GAT with more attention heads, or more complex architectures like Graph Isomorphism Networks (GINs) across the same datasets.

## Limitations

- Attack effectiveness may decrease as models converge and gradients diminish in magnitude
- Success depends on attacker's ability to accurately approximate server model architecture
- The attack is limited to node-level vertical federated GNNs and may not generalize to other federated learning settings

## Confidence

- **High confidence**: The core gradient matching mechanism and its theoretical foundation
- **Medium confidence**: Attack effectiveness with approximate architectures and unknown number of classes
- **Low confidence**: Long-term attack stability and performance across different learning rate schedules

## Next Checks

1. **Gradient magnitude decay analysis**: Track how attack accuracy correlates with gradient magnitude across training epochs to identify when attacks become ineffective
2. **Architecture mismatch sensitivity**: Systematically vary the difference between server and attacker model architectures to quantify the relationship between architectural similarity and attack success
3. **Early vs. late training phase comparison**: Compare attack accuracy when applied in early training epochs versus later epochs to understand temporal effectiveness patterns