---
ver: rpa2
title: 'Less than One-shot: Named Entity Recognition via Extremely Weak Supervision'
arxiv_id: '2311.02861'
source_url: https://arxiv.org/abs/2311.02861
tags:
- entity
- x-ner
- entities
- span
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces X-NER, a method for named entity recognition
  under extremely weak supervision (XWS), where only one example entity per type is
  provided. X-NER outperforms state-of-the-art few-shot NER methods by mining high-quality
  pseudo labels using the pre-trained language model's intrinsic understanding of
  entities.
---

# Less than One-shot: Named Entity Recognition via Extremely Weak Supervision

## Quick Facts
- arXiv ID: 2311.02861
- Source URL: https://arxiv.org/abs/2311.02861
- Reference count: 20
- One-line primary result: X-NER achieves 87.3 F1 on CoNLL03 with just one example per entity type, outperforming 1-shot methods and ChatGPT

## Executive Summary
This paper introduces X-NER, a method for named entity recognition under extremely weak supervision (XWS) where only one example entity per type is provided. X-NER mines high-quality pseudo labels by comparing context distributions before and after replacing a span with the example entity, using KL divergence as a similarity metric. Extensive experiments on 4 NER datasets show that X-NER significantly outperforms other methods, including those with 1-shot supervision and ChatGPT annotations. The method also exhibits favorable properties such as inheriting cross-lingual abilities of the underlying language models.

## Method Summary
X-NER addresses named entity recognition under extremely weak supervision by mining pseudo-labeled data from unlabeled text using a span test approach. The method tests each possible span in the corpus by replacing it with an example entity and measuring the KL divergence between context token prediction distributions before and after replacement. Top-ranked spans are selected as pseudo-labels to train an NER tagger. The approach avoids direct span representation comparison, instead leveraging the pre-trained language model's intrinsic understanding of entity-context relationships.

## Key Results
- X-NER achieves 87.3 F1 on CoNLL03 with just one example per entity type
- Outperforms 1-shot methods (ETAL, SEE-Few, TemplateNER) and ChatGPT by significant margins
- Shows remarkably high precision on specific entity types, particularly PER and MISC entities
- Entity mining ability improves commensurately with backbone model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context distribution comparison via KL divergence captures entity similarity more effectively than direct span representation comparison.
- Mechanism: When a span is replaced with an example entity, the change in context token prediction distributions reflects semantic similarity. Lower KL divergence indicates the span context is compatible with the entity, making it a good pseudo-label candidate.
- Core assumption: Pre-trained language models encode sufficient entity semantics in their context prediction distributions, and these distributions change predictably when entity-compatible spans are replaced with example entities.
- Evidence anchors:
  - [abstract]: "Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example, using KL divergence as a similarity metric."
  - [section 4.2]: "We propose a divergence distance evaluation inspired by Peng et al. (2023) that accounts the language modeling objective more. The core insight is that instead of comparing the spans Z and xi:j, which may have different lengths, directly, we compare their impact on a shared context."
  - [corpus]: Weak - no explicit corpus evidence for this mechanism, though experimental results support it.
- Break condition: If language models do not encode entity semantics in context predictions, or if entity contexts are too diverse to produce meaningful distribution comparisons.

### Mechanism 2
- Claim: The span test method with context-aware ranking produces high-precision entity mining compared to representation-based methods.
- Mechanism: By testing each possible span against the seed entity and ranking by similarity score, the method selects spans most contextually compatible with the entity type. The two-way testing with annotation-free context further improves precision by comparing both directions.
- Core assumption: Context compatibility is a stronger signal for entity mining than surface token similarity, and ranking spans by this compatibility produces high-precision results.
- Evidence anchors:
  - [section 4.2]: "While cosine similarity is a straightforward method, it is not necessarily consistent with the primary objective for which the PLM encoder Enc(Â·) is optimized for. We propose a divergence distance evaluation..."
  - [section 5.4]: "The results reveal that X-NER significantly outperforms other methods across all datasets and shows remarkably high precision on specific entity types, particularly PER and MISC entities."
  - [corpus]: Weak - no explicit corpus evidence, though Table 4 shows high precision@1000 scores.
- Break condition: If context compatibility does not correlate with entity type membership, or if ranking by similarity produces too many false positives.

### Mechanism 3
- Claim: The method scales effectively with language model size and type, with bidirectional models outperforming unidirectional ones.
- Mechanism: Larger language models capture more nuanced context patterns, improving entity mining precision. Bidirectional context provides better understanding of entity relationships than unidirectional context.
- Core assumption: Model scale and architecture directly impact the quality of context understanding and distribution predictions used for entity mining.
- Evidence anchors:
  - [section 6.3]: "The figure highlights that the entity mining ability of our X-NER commensurately improves with the backbone model size."
  - [section 6.3]: "For the unidirectional, CLM-based GPT2, we adapted our label context... The results showed the MLM-based roberta-large (355M) to considerably outperformed the CLM-based gpt2-medium (345M)."
  - [corpus]: Weak - no explicit corpus evidence, though experimental results support the claim.
- Break condition: If model scale does not improve context understanding, or if unidirectional models can achieve similar performance with architectural adjustments.

## Foundational Learning

- Concept: KL divergence as a measure of distribution difference
  - Why needed here: Used to quantify how much context changes when a span is replaced with an example entity, providing a similarity metric
  - Quick check question: If distribution A assigns probability 0.8 to token X and distribution B assigns 0.2, what is the KL divergence from A to B?

- Concept: Masked language modeling vs causal language modeling
  - Why needed here: Different model types provide different context for span testing, with MLM providing bidirectional context that improves performance
  - Quick check question: In BERT's MLM, what tokens can the model see when predicting a masked token at position i?

- Concept: Pseudo-labeling for training data augmentation
  - Why needed here: The method generates synthetic training data by mining entities from unlabeled text, then trains a standard NER model on this data
  - Quick check question: What is the risk of training on pseudo-labels, and how does the high-precision mining help mitigate this?

## Architecture Onboarding

- Component map: Input corpus -> Span testing with KL divergence -> Similarity ranking -> Top-K entity selection -> Pseudo-labeled dataset -> Standard NER model training -> Final NER model
- Critical path: The span testing and ranking step is critical, as it directly determines the quality of pseudo-labels and thus final model performance
- Design tradeoffs: Bidirectional vs unidirectional models (better context vs simpler architecture), one-way vs two-way testing (faster vs more precise), cosine similarity vs KL divergence (simpler vs more effective)
- Failure signatures: Low precision in entity mining indicates problems with similarity metric or language model understanding; poor final NER performance suggests low-quality pseudo-labels or insufficient training data
- First 3 experiments:
  1. Implement span testing with cosine similarity on a small dataset to verify basic functionality
  2. Replace cosine similarity with KL divergence and measure precision improvement
  3. Test different language model sizes and types to find optimal configuration for the target domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between model scale and entity mining performance in X-NER, and how does this relationship vary across different entity types?
- Basis in paper: [explicit] The paper states that "the entity mining ability of our X-NER commensurately improves with the backbone model size" and provides a figure (Figure 6) showing this relationship for PER and LOC entities.
- Why unresolved: The paper only presents results for two entity types (PER and LOC) and does not explore how the relationship between model scale and mining performance varies across different entity types or in other datasets.
- What evidence would resolve it: Additional experiments comparing model scale performance across all entity types in all datasets used in the study, as well as potentially other datasets with different entity types.

### Open Question 2
- Question: How does the performance of X-NER compare to other few-shot NER methods when using the same language model backbone, and what factors contribute to any performance differences?
- Basis in paper: [explicit] The paper compares X-NER to other few-shot NER methods (ETAL, SEE-Few, TemplateNER, EntLM, SpanNER, FFF-NER, SDNET) but uses different language models for each method.
- Why unresolved: The paper does not provide a direct comparison between X-NER and other few-shot NER methods using the same language model backbone, making it difficult to isolate the impact of the X-NER approach itself from the impact of the language model.
- What evidence would resolve it: Additional experiments comparing X-NER to other few-shot NER methods using the same language model backbone, such as RoBERTa-large.

### Open Question 3
- Question: How does the choice of similarity metric (divergence distance vs. cosine similarity) affect the performance of X-NER, and what are the underlying reasons for any differences observed?
- Basis in paper: [explicit] The paper presents a variant of X-NER using cosine similarity (X-NER w/ Cosine) and shows that it performs significantly worse than the default divergence distance-based X-NER.
- Why unresolved: The paper does not provide a detailed analysis of why the divergence distance metric outperforms cosine similarity, such as examining the distribution of similarity scores or the impact on different entity types.
- What evidence would resolve it: A detailed analysis of the similarity score distributions for both metrics, as well as an examination of how each metric performs across different entity types and datasets.

## Limitations
- Performance is bounded by the quality of pre-trained language models and their understanding of entities
- Computationally expensive span testing across entire corpus limits scalability
- Method may struggle with polysemous entities or those with highly diverse contexts

## Confidence

**High Confidence**: The experimental results showing X-NER outperforming 1-shot and zero-shot baselines are well-supported by the data across multiple datasets. The finding that bidirectional models outperform unidirectional ones is also robust.

**Medium Confidence**: The claim that context distribution comparison via KL divergence is superior to span representation comparison is supported by results but could benefit from more ablation studies comparing different similarity metrics directly.

**Low Confidence**: The assertion that X-NER inherits cross-lingual abilities of underlying language models is based on limited evidence (one example with mBERT) and requires more extensive multilingual validation.

## Next Checks

1. **Ablation Study on Similarity Metrics**: Systematically compare KL divergence against other distribution comparison methods (Jensen-Shannon divergence, Wasserstein distance) and span representation approaches on the same datasets to isolate the contribution of the KL divergence choice.

2. **Scalability Benchmark**: Measure wall-clock time and memory usage of the span testing process across different corpus sizes and model scales, establishing clear computational complexity bounds and identifying practical limits.

3. **Cross-Lingual Stress Test**: Evaluate X-NER on a broader set of low-resource languages using mBERT, testing both the zero-shot cross-lingual transfer capability and the method's robustness when dealing with languages that have limited pre-training data.