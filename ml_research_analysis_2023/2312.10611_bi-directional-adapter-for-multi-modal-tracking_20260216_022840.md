---
ver: rpa2
title: Bi-directional Adapter for Multi-modal Tracking
arxiv_id: '2312.10611'
source_url: https://arxiv.org/abs/2312.10611
tags:
- tracking
- adapter
- multi-modal
- modality
- bi-directional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dynamic multi-modal tracking
  where the dominant imaging modality (RGB vs. infrared) changes across frames in
  complex environments.
---

# Bi-directional Adapter for Multi-modal Tracking

## Quick Facts
- **arXiv ID**: 2312.10611
- **Source URL**: https://arxiv.org/abs/2312.10611
- **Reference count**: 10
- **Primary result**: Proposes BAT, a bi-directional adapter for dynamic multi-modal tracking that achieves state-of-the-art performance with only 0.32M trainable parameters

## Executive Summary
This paper addresses the challenge of dynamic multi-modal tracking where the dominant imaging modality (RGB vs. infrared) changes across frames in complex environments. The authors propose a Bi-directional Adapter for Multi-modal Tracking (BAT) that uses a frozen pre-trained foundation model and lightweight learnable adapters to transfer complementary information between modalities dynamically. The adapter structure enables cross-prompting between RGB and infrared features without pre-defining a fixed dominant modality. BAT adds only 0.32M trainable parameters and achieves state-of-the-art performance on RGBT234 and LasHeR datasets, outperforming existing methods by significant margins in precision and success rates.

## Method Summary
BAT uses a frozen pre-trained transformer foundation model with modality-specific branches for RGB and thermal processing. Lightweight bi-directional adapters are embedded in each transformer layer to enable cross-modal feature transfer. The model trains only the adapter parameters (0.32M total) while keeping the foundation model frozen. The architecture processes aligned RGB-T frame pairs through separate modality branches, exchanges information bidirectionally via the adapters, and produces tracking predictions through a shared prediction head. The approach dynamically extracts effective information from the auxiliary modality based on environmental conditions without requiring pre-defined modality dominance.

## Key Results
- BAT achieves state-of-the-art performance on RGBT234 and LasHeR datasets
- Outperforms existing methods by significant margins in precision and success rates
- Adds only 0.32M trainable parameters while maintaining strong performance
- Demonstrates robust tracking across diverse environmental conditions and extreme attributes

## Why This Works (Mechanism)

### Mechanism 1
The bi-directional adapter enables dynamic feature transfer between modalities without pre-defining which modality is dominant. The adapter uses a lightweight hourglass structure with down-projection, linear projection, and up-projection layers to extract and transfer feature prompts between RGB and thermal infrared tokens within each transformer layer. The frozen pre-trained foundation model provides a shared feature space where cross-modal information can be meaningfully transferred through learned adapter parameters. Break condition: If the pre-trained foundation model's feature representations are too modality-specific, the adapter cannot find meaningful cross-modal transfer signals.

### Mechanism 2
Parameter-efficient tuning via adapter freezing allows strong performance with minimal trainable parameters (0.32M). The foundation model parameters remain frozen while only the adapter weights are trained, enabling effective multi-modal tracking without full fine-tuning. The pre-trained foundation model contains sufficient general visual representation knowledge that can be adapted to multi-modal tracking through lightweight prompt mechanisms. Break condition: If the pre-trained model's initialization is poor or domain-mismatched, freezing parameters will prevent necessary adaptation.

### Mechanism 3
Dual-stream architecture with shared parameters and bi-directional prompting captures complementary information better than single-stream or fixed-modality approaches. Separate modality-specific branches process RGB and thermal features independently, then exchange information bidirectionally through the adapter, allowing both modalities to contribute dynamically. The shared foundation model parameters provide a compatible feature space for both modalities, and bi-directional exchange captures more complementarity than unidirectional prompting. Break condition: If modality-specific feature distributions are too divergent, shared parameters may create incompatible representations.

## Foundational Learning

- **Transformer-based visual foundation models and self-attention mechanisms**
  - Why needed here: BAT relies on a pre-trained transformer backbone to extract features from both modalities before adapter-based prompting
  - Quick check question: How does self-attention enable the model to weigh different regions of the template and search frames when computing the final tracking prediction?

- **Parameter-efficient fine-tuning and adapter-based transfer learning**
  - Why needed here: BAT freezes the foundation model and only trains lightweight adapters, requiring understanding of how adapter layers modify feature representations
  - Quick check question: What is the computational difference between full fine-tuning and training only adapter parameters in terms of parameter count and GPU memory usage?

- **Multi-modal data alignment and synchronization**
  - Why needed here: BAT assumes RGB and thermal frames are temporally synchronized and spatially aligned for effective cross-modal feature transfer
  - Quick check question: How would tracking performance degrade if the RGB and thermal frames had even small temporal offsets or spatial misalignments?

## Architecture Onboarding

- **Component map**: Input (RGB template, RGB search, thermal template, thermal search) → Embedding (patch and position embedding) → Transformer Encoder (with adapter) → Feature Addition → Prediction Head → Output (classification, offset, bounding box)

- **Critical path**: Input → Embedding → Transformer Encoder (with adapter) → Feature Addition → Prediction Head → Output

- **Design tradeoffs**:
  - Shared vs. separate foundation models: Shared parameters reduce memory but require compatible feature spaces
  - Adapter depth: More adapter layers capture more cross-modal information but increase parameters
  - Bidirectional vs. unidirectional prompting: Bidirectional captures more complementarity but doubles adapter parameters

- **Failure signatures**:
  - Poor tracking when one modality is degraded: Adapter fails to extract useful information from the weak modality
  - Performance plateau despite training: Frozen foundation model parameters limit necessary adaptation
  - Memory issues: Too many adapter layers or large adapter dimensions exceed GPU capacity

- **First 3 experiments**:
  1. Ablation study removing the adapter entirely to measure baseline performance of dual-stream without cross-modal prompting
  2. Test with adapter only in middle layers (e.g., layers 4-8) to find optimal trade-off between performance and efficiency
  3. Compare bidirectional adapter vs. two separate unidirectional adapters to quantify benefit of mutual information exchange

## Open Questions the Paper Calls Out

- **Open Question 1**: How does BAT's performance scale when extended to more than two modalities (e.g., RGB, thermal, depth, and LiDAR)?
  - Basis: Authors mention BAT is "more flexible to handle more modalities in a parameter-efficient manner" and express interest in exploring general models for diverse modalities
  - Unresolved: Current validation only on RGB-T tasks; effectiveness with three or more modalities remains untested
  - Evidence needed: Empirical results demonstrating performance and parameter efficiency with three or more modalities compared to existing multi-modal fusion methods

- **Open Question 2**: What is the impact of adapter depth on tracking performance and computational efficiency?
  - Basis: Authors show BAT-4 achieves comparable performance to BAT-12 while saving parameters
  - Unresolved: Optimal number of adapter layers for balancing performance and efficiency across different tracking scenarios is not determined
  - Evidence needed: Comprehensive study varying adapter layers across diverse tracking datasets, analyzing trade-offs between accuracy, computational cost, and parameter efficiency

- **Open Question 3**: How does BAT's performance compare to single-modal trackers when one modality is completely unavailable or severely degraded?
  - Basis: Authors claim BAT "outperforms single-stream prompt-learning approaches in complex scenarios when RGB images are distorted or even unavailable"
  - Unresolved: Quantitative comparisons against single-modal trackers under varying degrees of modality degradation are not presented
  - Evidence needed: Quantitative experiments measuring BAT's accuracy, robustness, and parameter efficiency against single-modal trackers across datasets with controlled modality degradation

## Limitations

- Limited evaluation to only two datasets (RGBT234 and LasHeR) may not capture full diversity of real-world scenarios
- Lack of detailed architectural specifications makes exact reproduction challenging
- No ablation studies on different adapter configurations or comparisons with other parameter-efficient tuning methods

## Confidence

- **High confidence**: Parameter efficiency claim (0.32M trainable parameters) is well-supported by the adapter-based approach
- **Medium confidence**: Performance improvements over existing methods are credible but lack detailed architectural specifications
- **Low confidence**: Dynamic modality dominance detection claims lack empirical validation showing how the model determines dominant modality

## Next Checks

1. **Architecture Verification**: Implement the bi-directional adapter with multiple configurations and measure performance scaling with parameter count to verify 0.32M parameter efficiency claim

2. **Modality Dominance Analysis**: Conduct experiments to visualize and quantify which modality the model relies on more heavily under different environmental conditions

3. **Cross-dataset Generalization**: Test the trained model on a third multi-modal tracking dataset not seen during training to evaluate true generalization beyond the two datasets used