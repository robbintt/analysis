---
ver: rpa2
title: 'HouYi: An open-source large language model specially designed for renewable
  energy and carbon neutrality field'
arxiv_id: '2308.01414'
source_url: https://arxiv.org/abs/2308.01414
tags:
- power
- energy
- wind
- assistant
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HouYi, the first open-source large language
  model (LLM) specifically designed for renewable energy and carbon neutrality research.
  The model is built by fine-tuning a general LLM using a newly created dataset of
  1.1 million academic papers from the renewable energy field.
---

# HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field

## Quick Facts
- arXiv ID: 2308.01414
- Source URL: https://arxiv.org/abs/2308.01414
- Reference count: 40
- Primary result: HouYi outperforms general LLMs and matches or slightly exceeds ChatGPT in renewable energy academic content generation

## Executive Summary
HouYi is the first open-source large language model specifically designed for renewable energy and carbon neutrality research. Developed by fine-tuning ChatGLM-6B on a dataset of 1.1 million academic paper abstracts from the renewable energy field, HouYi demonstrates superior performance in generating domain-specific academic content compared to popular general LLMs like LLaMA-13B. The model achieves performance comparable to or slightly better than commercial models such as ChatGPT, Claude, ERNIE Bot, and SparkDesk in generating academic content related to wind and solar energy.

## Method Summary
The HouYi model was developed by fine-tuning ChatGLM-6B using the REAP dataset, which contains 1,168,970 academic paper titles and abstracts from Web of Science. The fine-tuning process used a learning rate of 0.01 for 30 epochs on eight RTX 3090 GPUs. The model was evaluated using a combination of ChatGPT-assisted scoring and human expert evaluation with the Analytical Hierarchy Process (AHP) across six metrics: helpfulness, relevance, accuracy, academic standard, detail, and novelty.

## Key Results
- HouYi outperforms LLaMA-13B on renewable energy academic content generation tasks
- Human expert evaluation using AHP confirms HouYi's strong performance across multiple metrics
- HouYi achieves performance comparable to or slightly better than ChatGPT in generating academic papers on renewable energy

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a general LLM with domain-specific academic text creates a specialized model with superior domain task performance. The REAP dataset provides 1.1 million renewable energy abstracts as paired input-output training examples, allowing ChatGLM-6B to internalize field-specific terminology, paper structure, and reasoning patterns. Core assumption: Academic abstracts contain sufficient domain knowledge to teach an LLM to generate domain-compliant content.

### Mechanism 2
Analytical Hierarchy Process (AHP) provides a robust, bias-mitigated multi-criteria evaluation framework for LLM outputs. Human experts score LLMs on six dimensions, and AHP weights each dimension according to pairwise importance judgments, producing a composite score that balances qualitative and quantitative criteria. Core assumption: Experts can consistently judge relative importance of metrics like accuracy, relevance, and academic standard.

### Mechanism 3
ChatGPT-assisted evaluation correlates with human expert judgments and serves as a cost-effective proxy for model comparison. ChatGPT uses structured prompts to score six LLMs on helpfulness, relevance, accuracy, and detail, averaging over multiple trials to reduce variance. Core assumption: ChatGPT can reliably evaluate and rank LLM outputs in the same domain.

## Foundational Learning

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: The paper explicitly fine-tunes a base model rather than relying solely on prompting; understanding the distinction explains performance gains.
  - Quick check question: What is the difference between fine-tuning a model on domain data and using zero-shot prompting with a general model?

- Concept: Analytical Hierarchy Process (AHP)
  - Why needed here: AHP is used to combine multiple evaluation criteria into a single composite score; knowing how it works is key to interpreting the results.
  - Quick check question: How does AHP convert pairwise expert judgments into weights for each evaluation metric?

- Concept: Transformer attention mechanism
  - Why needed here: The LLM backbone is a Transformer; understanding attention helps explain why fine-tuning can adapt the model to new domains.
  - Quick check question: What role does multi-head self-attention play in a Transformer's ability to model long-range dependencies in text?

## Architecture Onboarding

- Component map: REAP dataset builder (Web of Science query → title/abstract pairs) → ChatGLM-6B base model → Fine-tuning pipeline (PyTorch + 8x RTX 3090 GPUs) → Evaluation modules (ChatGPT assistant scoring + AHP-weighted human expert scoring) → Gradio web demo

- Critical path: 1) Collect and clean REAP dataset, 2) Fine-tune ChatGLM-6B on REAP, 3) Deploy model and generate sample papers, 4) Run ChatGPT and human evaluations, 5) Compute AHP weights and final scores

- Design tradeoffs:
  - Dataset size vs. compute cost: Larger REAP yields better fine-tuning but increases GPU time
  - Evaluation automation vs. cost: ChatGPT is cheaper than human evaluation but may be less reliable
  - Open-source vs. proprietary base: ChatGLM-6B is freely available but less performant than GPT-4

- Failure signatures:
  - Low BLEU/ROUGE on generated abstracts → dataset quality or fine-tuning issues
  - ChatGPT and human scores diverge significantly → evaluation inconsistency
  - Gradio demo latency > 5s → GPU/serving bottleneck

- First 3 experiments:
  1. Fine-tune ChatGLM-6B on a small subset of REAP (e.g., 10k samples) and compare output quality to base model
  2. Perform AHP weight sensitivity analysis: perturb pairwise comparisons and observe score changes
  3. Benchmark HouYi vs. ChatGPT on a held-out set of renewable energy abstracts using automated metrics (e.g., perplexity, ROUGE)

## Open Questions the Paper Calls Out

### Open Question 1
How does the HouYi model's performance scale with different dataset sizes and compositions in the renewable energy domain? The paper uses a dataset of 1.1 million academic papers but does not explore the effects of dataset size variations on model performance. This remains unresolved because the paper does not investigate how changes in dataset size or composition affect the model's ability to generate academic content in renewable energy.

### Open Question 2
What are the limitations of using human expert-based analytical hierarchy process (AHP) for evaluating LLMs in the renewable energy field? The paper introduces AHP for evaluating LLMs but does not discuss its limitations or potential biases. This remains unresolved because the paper does not address the potential biases or limitations of using human experts for LLM evaluation, nor does it compare AHP with other evaluation methods.

### Open Question 3
How does HouYi perform on tasks beyond academic paper generation, such as answering technical questions or summarizing research findings in renewable energy? The paper focuses on academic paper generation but does not explore other potential applications of the model. This remains unresolved because the paper does not test HouYi's capabilities on other language tasks relevant to renewable energy research and development.

## Limitations
- Evaluation relies on ChatGPT itself for initial scoring, creating potential circularity in the validation process
- Human expert evaluation involves only five experts, which may not provide sufficient statistical power
- Limited exploration of model capabilities beyond academic paper generation

## Confidence

- **High confidence**: The technical approach of fine-tuning ChatGLM-6B on the REAP dataset is clearly specified and reproducible
- **Medium confidence**: The claim that HouYi outperforms LLaMA-13B is supported by presented metrics, but comparisons with commercial models rely on indirect evaluation methods
- **Low confidence**: The assertion that HouYi is "comparable to or slightly better than" ChatGPT is based on ChatGPT-assisted evaluation, creating a self-referential validation scenario

## Next Checks

1. **Independent benchmark test**: Have domain experts generate blind evaluations comparing HouYi outputs with ChatGPT outputs on identical prompts, without ChatGPT participating in the scoring process.

2. **Cross-dataset generalization**: Test HouYi's performance on renewable energy abstracts from sources outside the REAP dataset (e.g., IEEE Xplore, ScienceDirect) to verify it hasn't simply memorized training data.

3. **Ablation study on dataset size**: Train intermediate models using subsets of REAP (100K, 500K, 800K samples) to quantify how dataset scale affects performance and determine if the full 1.1M samples are necessary for claimed capabilities.