---
ver: rpa2
title: Tractable Control for Autoregressive Language Generation
arxiv_id: '2304.07438'
source_url: https://arxiv.org/abs/2304.07438
tags:
- generation
- language
- constraint
- lexical
- gelato
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GeLaTo, a framework for controlling autoregressive
  language models to satisfy complex lexical constraints. The key idea is to use tractable
  probabilistic models (TPMs), specifically distilled hidden Markov models, to efficiently
  compute the conditional distribution of the next token given the prefix and the
  constraint.
---

# Tractable Control for Autoregressive Language Generation

## Quick Facts
- arXiv ID: 2304.07438
- Source URL: https://arxiv.org/abs/2304.07438
- Reference count: 10
- Key outcome: GeLaTo achieves state-of-the-art performance on CommonGen with 100% constraint satisfaction

## Executive Summary
This paper introduces GeLaTo, a framework for controlling autoregressive language models to satisfy complex lexical constraints while maintaining generation quality. The key innovation is using distilled hidden Markov models (HMMs) as tractable probabilistic models to efficiently compute conditional distributions for constraint satisfaction. By training HMMs to approximate base language models like GPT-2, GeLaTo can guide generation toward constraint-satisfying outputs without requiring retraining for different constraints. The framework combines TPM guidance with the base LM through weighted geometric means or direct multiplication, achieving superior performance on the CommonGen benchmark compared to strong baselines while guaranteeing 100% constraint satisfaction.

## Method Summary
GeLaTo works by first fine-tuning a base language model (e.g., GPT-2) on task data, then training a tractable probabilistic model (TPM) - specifically a distilled HMM - to approximate the base LM via maximum likelihood estimation on samples from the base model. The TPM is trained to efficiently compute conditional probabilities for lexical constraints encoded as conjunctive normal form (CNF). At generation time, GeLaTo combines the TPM's guidance with the base language model using weighted geometric mean (supervised setting) or direct multiplication (unsupervised setting) to approximate the desired conditional distribution. A dynamic programming algorithm enables efficient probabilistic reasoning with HMMs, and beam search is used to generate high-quality, constraint-satisfying text.

## Key Results
- Achieves state-of-the-art BLEU and ROUGE scores on CommonGen benchmark
- Guarantees 100% constraint satisfaction rate
- Outperforms various strong baselines in both supervised and unsupervised settings
- Demonstrates effectiveness of HMMs as tractable probabilistic models for language generation control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The HMM is trained to approximate the base LM, allowing the TPM to guide generation toward satisfying lexical constraints.
- Mechanism: Maximum likelihood estimation (MLE) on samples from the base LM minimizes KL divergence between the TPM and the LM, creating a close approximation.
- Core assumption: The distilled HMM can approximate the base LM well enough for practical generation quality.
- Evidence anchors:
  - [abstract] "we use distilled hidden Markov models, where we can efficiently compute Pr(text | α), to guide autoregressive generation from GPT2."
  - [section] "we train a TPM via maximum likelihood estimation (MLE) on samples drawn from PrLM, which is effectively minimizes the KL-divergence between PrTPM and PrLM."
  - [corpus] Weak - no direct corpus evidence found for this specific approximation claim.

### Mechanism 2
- Claim: The TPM efficiently computes conditional probabilities for lexical constraints, enabling tractable generation control.
- Mechanism: A dynamic programming algorithm computes Pr(x1:t, α) for HMMs, breaking down the constraint satisfaction problem into manageable sub-problems.
- Core assumption: The constraint can be encoded as a conjunctive normal form (CNF) and the non-overlapping assumption holds for keystrings.
- Evidence anchors:
  - [abstract] "we propose a dynamic programming algorithm for efficient probabilistic reasoning with HMMs."
  - [section] "We propose a dynamic programming algorithm for hidden Markov models (HMMs) that computes Pr(x1:t, α), where α is some lexical constraint encoded as a conjunctive normal form (CNF)."
  - [corpus] Weak - no direct corpus evidence found for this specific CNF encoding claim.

### Mechanism 3
- Claim: Combining TPM and base LM distributions through weighted geometric mean or direct multiplication produces high-quality, constraint-satisfying text.
- Mechanism: For supervised settings, GeLaTo uses a weighted geometric mean of TPM and LM distributions; for unsupervised, it multiplies them directly.
- Core assumption: The independence assumption holds for the unsupervised setting, and the geometric mean formula provides a good approximation for the supervised setting.
- Evidence anchors:
  - [abstract] "At generation time, GeLaTo combines the TPM's guidance with the base language model to approximate the desired conditional distribution."
  - [section] "we generate autoregressively following the next-token distribution defined as their weighted geometric mean."
  - [corpus] Weak - no direct corpus evidence found for this specific combination claim.

## Foundational Learning

- Concept: Autoregressive language models
  - Why needed here: GeLaTo works by controlling autoregressive language models to satisfy constraints.
  - Quick check question: What is the key limitation of autoregressive language models when it comes to satisfying lexical constraints?

- Concept: Tractable probabilistic models (TPMs)
  - Why needed here: TPMs allow efficient computation of conditional probabilities for constraints, enabling tractable generation control.
  - Quick check question: What makes TPMs "tractable" compared to other probabilistic models?

- Concept: Hidden Markov models (HMMs)
  - Why needed here: HMMs are used as a running example TPM to demonstrate GeLaTo's effectiveness.
  - Quick check question: What are the key components of an HMM and how do they relate to language modeling?

## Architecture Onboarding

- Component map:
  Base LM (e.g., GPT-2) -> TPM (e.g., distilled HMM) -> Dynamic programming algorithm -> Combination mechanism -> Beam search

- Critical path:
  1. Fine-tune base LM on task data (unsupervised or supervised)
  2. Train TPM on samples from base LM via MLE
  3. Encode lexical constraints as CNF
  4. Use dynamic programming to compute conditional probabilities
  5. Combine TPM and LM distributions for generation
  6. Use beam search to generate high-quality, constraint-satisfying text

- Design tradeoffs:
  - HMM vs. other TPMs: HMMs are simpler but may not capture complex dependencies as well as more expressive TPMs
  - Unsupervised vs. supervised setting: Unsupervised is more challenging but doesn't require task-specific training data
  - Beam size: Larger beams improve quality but increase computation time

- Failure signatures:
  - Poor approximation of base LM by TPM: Generated text quality suffers
  - Inefficient constraint encoding: Dynamic programming becomes intractable
  - Violation of independence assumption: Generated text may not satisfy constraints

- First 3 experiments:
  1. Verify that HMM can approximate base LM by comparing log-likelihoods on held-out data
  2. Test dynamic programming algorithm on simple constraints to ensure correctness
  3. Evaluate generation quality and constraint satisfaction on a small dataset with known constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can other tractable probabilistic models (TPMs) beyond HMMs achieve better performance in controlling autoregressive language models?
- Basis in paper: [explicit] The paper mentions that "our work motivates the development of more expressive tractable probabilistic models" and that HMMs were used as an example TPM.
- Why unresolved: The paper only demonstrates the effectiveness of GeLaTo using HMMs. Other TPMs like sum-product networks or probabilistic sentential decision diagrams have not been explored in this context.
- What evidence would resolve it: Comparative experiments using different TPMs (e.g., sum-product networks, probabilistic sentential decision diagrams) in the GeLaTo framework on the same benchmarks, measuring generation quality and constraint satisfaction rates.

### Open Question 2
- Question: How does the performance of GeLaTo scale with increasingly complex lexical constraints beyond those tested in the CommonGen benchmark?
- Basis in paper: [inferred] The paper demonstrates GeLaTo's effectiveness on CommonGen with relatively simple keyword constraints, but mentions that the framework can handle "complex lexical constraints" in general.
- Why unresolved: The experiments focus on CommonGen's keyword-based constraints, which are relatively straightforward compared to more complex linguistic constraints that might involve syntactic or semantic relationships between words.
- What evidence would resolve it: Experiments applying GeLaTo to tasks with more complex constraints, such as generating text that must satisfy specific syntactic patterns, semantic relationships, or logical conditions between multiple elements.

### Open Question 3
- Question: What is the computational overhead of using GeLaTo compared to standard autoregressive generation, and how does it scale with sequence length and constraint complexity?
- Basis in paper: [explicit] The paper mentions that GeLaTo uses dynamic programming for efficient probabilistic reasoning, and provides time complexity analysis (O(2|α|nm) for autoregressive generation).
- Why unresolved: While the paper provides theoretical complexity analysis, it does not report empirical runtime comparisons between GeLaTo and standard autoregressive generation methods across different sequence lengths and constraint complexities.
- What evidence would resolve it: Empirical runtime measurements comparing GeLaTo against standard autoregressive generation methods across varying sequence lengths and constraint complexities, including memory usage and wall-clock time per generated token.

## Limitations
- Approximation quality between HMM and complex base LMs like GPT-2 may be limited for all types of language patterns
- Constraint encoding relies on CNF format and non-overlapping keystrings assumption, which may not hold for complex real-world constraints
- Computational overhead of dynamic programming for constraint satisfaction, though tractable, adds complexity compared to standard generation

## Confidence
**High confidence** (Strong evidence, well-supported claims):
- The overall framework design and motivation for using TPMs to control autoregressive generation
- The theoretical possibility of training HMMs to approximate base LMs via MLE
- The dynamic programming approach for computing conditional probabilities with HMMs

**Medium confidence** (Reasonable evidence but some gaps):
- The effectiveness of the weighted geometric mean for combining TPM and LM distributions
- The 100% constraint satisfaction claim, as this depends on proper constraint encoding and doesn't address semantic constraint satisfaction
- The scalability of the approach to more complex constraint types beyond simple lexical constraints

**Low confidence** (Limited evidence or significant assumptions):
- The approximation quality between HMM and complex base LMs like GPT-2 for all types of language patterns
- Performance on constraints involving semantic relationships rather than just lexical requirements
- Runtime efficiency claims without specific timing measurements across different constraint complexities

## Next Checks
1. **Approximation quality validation**: Measure the KL divergence between the HMM and base LM on held-out data across different HMM sizes (e.g., 1024, 2048, 4096, 8192 states) to quantify the approximation quality and identify the point of diminishing returns.

2. **Constraint complexity stress test**: Design a systematic evaluation using constraints of increasing complexity (simple lexical, overlapping patterns, nested constraints) to identify the breaking point where the dynamic programming algorithm becomes intractable or constraint satisfaction fails.

3. **Base LM sensitivity analysis**: Evaluate GeLaTo's performance across different base LMs (GPT-2 small, GPT-2 large, GPT-3) to determine how sensitive the framework is to the choice of base model and whether the HMM approximation quality scales appropriately.