---
ver: rpa2
title: Investigating Large Language Models' Perception of Emotion Using Appraisal
  Theory
arxiv_id: '2310.04450'
source_url: https://arxiv.org/abs/2310.04450
tags:
- coping
- scenarios
- appraisal
- human
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates Large Language Models' (LLMs) understanding
  of human emotion through appraisal and coping theory using the Stress and Coping
  Process Questionnaire (SCPQ). The researchers applied SCPQ to three recent OpenAI
  LLMs (text-davinci-003, ChatGPT, and GPT-4) and compared the results with predictions
  from appraisal theory and human data.
---

# Investigating Large Language Models' Perception of Emotion Using Appraisal Theory

## Quick Facts
- arXiv ID: 2310.04450
- Source URL: https://arxiv.org/abs/2310.04450
- Reference count: 24
- Primary result: LLMs show partial understanding of emotional appraisal dynamics but fail to differentiate along key appraisal dimensions as predicted by theory and human data

## Executive Summary
This study investigates how Large Language Models (LLMs) understand human emotion through appraisal and coping theory using the Stress and Coping Process Questionnaire (SCPQ). The researchers applied SCPQ to three recent OpenAI models (text-davinci-003, ChatGPT, and GPT-4) and compared results with theoretical predictions and human data. While LLMs demonstrate some alignment with human trends in emotional dynamics, they fail to differentiate along key appraisal dimensions and show sensitivity to instruction formatting. The findings contribute to understanding current LLM capabilities in psychological reasoning.

## Method Summary
The study applied the SCPQ instrument, a validated clinical tool using evolving hypothetical scenarios divided into three phases, to three OpenAI LLMs. Researchers compared model responses with human data and theoretical predictions across multiple variables including controllability, changeability, and emotional intensity. The study also tested how LLMs respond when instructed to act as depressed individuals and investigated sensitivity to different prompt formats (instruction placement, batch vs individual questioning, output format).

## Key Results
- LLMs show similar dynamics of appraisal and coping to humans but fail to differentiate along key appraisal dimensions
- Model responses differ in magnitude from human data across several variables
- LLMs demonstrate significant sensitivity to instruction phrasing and question formatting
- When instructed to act as depressed persons, ChatGPT and GPT-4 respond consistently with theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
LLMs demonstrate partial understanding of emotional appraisal dynamics similar to humans through exposure to structured scenario stimuli that evolve over three phases, allowing models to track changes in appraisal variables like controllability and changeability.

### Mechanism 2
LLMs are highly sensitive to instruction and prompt formatting, with variations in phrasing (before/after scenario), output format (choice vs number), and batch vs individual questioning leading to measurable differences in model outputs.

### Mechanism 3
LLMs can simulate stereotypical behaviors of psychological states when explicitly instructed, adjusting their appraisal and coping responses in ways consistent with theoretical predictions without requiring fine-tuning on clinical data.

## Foundational Learning

- **Appraisal Theory of Emotion**
  - Why needed: Provides theoretical framework for interpreting how people evaluate situations and generate emotional responses
  - Quick check: What are the two main dimensions of appraisal in Lazarus's theory?

- **Likert Scale Measurement**
  - Why needed: SCPQ uses Likert scales to quantify emotional and appraisal responses
  - Quick check: What does a Likert scale range from in SCPQ measurements?

- **Scenario-Based Testing**
  - Why needed: SCPQ uses hypothetical evolving scenarios to elicit appraisal and coping responses
  - Quick check: How many phases do SCPQ scenarios have, and why is this important?

## Architecture Onboarding

- **Component map**: SCPQ scenarios (3 phases) -> Instruction variants -> LLM inference (temperature=0) -> Likert scale outputs -> Comparison with human/theory trends

- **Critical path**: 1. Load SCPQ scenario set 2. Apply instruction variant 3. Query LLM (batch or individual) 4. Parse and normalize outputs 5. Aggregate results 6. Compare against human/theory trends

- **Design tradeoffs**: Batch querying saves time but may reduce output quality due to context length limits; temperature=0 ensures reproducibility but may reduce creative variability; individual querying increases cost and runtime but yields more reliable outputs

- **Failure signatures**: Inconsistent responses across similar scenarios; failure to follow instruction format; missing responses in batch mode; divergence from expected trends in key variables

- **First 3 experiments**: 1. Run SCPQ scenarios with default instruction, batch mode, temperature=0; verify output format compliance 2. Compare individual vs batch outputs for controllability and changeability; document any discrepancies 3. Test depression instruction variant on GPT-4; check alignment with theoretical predictions for controllability and self-blame

## Open Questions the Paper Calls Out

### Open Question 1
Can Large Language Models accurately differentiate between aversive and loss scenarios based on appraisal dimensions like controllability and changeability? While the study shows LLMs struggle to differentiate between these scenario types as predicted by theory, further research with more nuanced scenarios or enhanced model training could provide clearer insights.

### Open Question 2
How does the sensitivity of LLMs to instruction and prompt variations affect their perception of emotional scenarios? The study identified sensitivity to instruction but did not fully explore the extent or implications of this sensitivity on emotional perception, warranting systematic testing of various instruction and prompt variations.

### Open Question 3
How do real-world emotional experiences compare to hypothetical scenarios in evaluating LLMs' understanding of human emotions? Since human data is from hypothetical scenarios, future research comparing LLMs' responses to human reactions in real-life emotional situations could provide valuable insights.

## Limitations

- Lack of detailed documentation on the exact SCPQ instrument used, including specific scenarios and questions, affects reproducibility
- Limited exploration of how training data biases might influence LLM responses to emotional and appraisal-related prompts
- Insufficient analysis of the implications of instruction sensitivity on the consistency and reliability of model outputs

## Confidence

- **High Confidence**: LLMs demonstrate partial understanding of emotional appraisal dynamics similar to humans, evidenced by alignment with human trends in dynamics of appraisal and coping
- **Medium Confidence**: LLMs are sensitive to instruction and prompt formatting, given observable differences in outputs based on prompt structure variations
- **Low Confidence**: LLMs can simulate stereotypical behaviors of psychological states when explicitly instructed, due to limited exploration and potential influence of training data biases

## Next Checks

1. Reproduce SCPQ scenarios using the exact instrument on the same three OpenAI LLMs to verify consistency of findings
2. Conduct detailed analysis of how different instruction phrasings and prompt formats affect LLM responses, focusing on magnitude and direction of changes in key appraisal dimensions
3. Investigate potential biases in training data of LLMs that could influence responses to emotional and appraisal-related prompts, particularly regarding simulation of psychological states