---
ver: rpa2
title: De-novo Chemical Reaction Generation by Means of Temporal Convolutional Neural
  Networks
arxiv_id: '2310.17341'
source_url: https://arxiv.org/abs/2310.17341
tags:
- reaction
- dataset
- reactions
- used
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work combines Recurrent Neural Networks (RNN) and Temporal
  Convolutional Neural Networks (TCN) for de novo chemical reaction generation. The
  method uses a novel Reaction Smiles-like representation (CGRSmiles) that incorporates
  atom mapping directly.
---

# De-novo Chemical Reaction Generation by Means of Temporal Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2310.17341
- Source URL: https://arxiv.org/abs/2310.17341
- Reference count: 13
- One-line primary result: RNN-TCN combination outperforms RNN alone for de novo chemical reaction generation

## Executive Summary
This paper presents a novel approach for de novo chemical reaction generation using a combination of Recurrent Neural Networks (RNN) and Temporal Convolutional Neural Networks (TCN). The method introduces a Reaction Smiles-like representation (CGRSmiles) that incorporates atom mapping directly. The authors demonstrate that combining RNN and TCN latent representations results in better performance compared to RNN alone, with higher numbers of unique and valid generated reactions. Additionally, different fine-tuning protocols significantly impact generative scope, with a weight-freezing approach enabling few-shot learning on smaller datasets. The model successfully generates reactions with novel reaction centers not seen in training data, showcasing its potential for discovering new chemical reactions.

## Method Summary
The method combines RNN-LSTM with TCN to generate chemical reactions in CGRSmiles format. The model is pre-trained on the USPTO atom-mapped dataset (216,308 reactions) and fine-tuned on a smaller H2O dataset (166 reactions) using weight-freezing protocols. The CGRSmiles representation encodes atom mappings directly into the SMILES-like string, allowing the model to learn reaction patterns while preserving atomic correspondences. The TCN component uses dilated convolutions to capture long-range dependencies, while the RNN captures sequential information. The latent representations from both architectures are combined to generate reactions one token at a time.

## Key Results
- RNN-TCN combination generates higher numbers of unique and valid reactions compared to RNN alone
- Weight-freezing fine-tuning enables effective few-shot learning on smaller datasets
- Model successfully generates reactions with novel reaction centers not present in training data
- Transfer learning through fine-tuning protocols significantly impacts generative scope

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining RNN and TCN latent representations results in better performance than RNN alone for de novo chemical reaction generation.
- Mechanism: RNNs excel at capturing sequential dependencies in the forward direction, while TCNs provide a wide receptive field with dilated convolutions that can capture long-range dependencies in both directions. The combination leverages both strengths, leading to improved generative performance.
- Core assumption: The latent representations learned by RNN and TCN are complementary and can be effectively combined.
- Evidence anchors:
  - [abstract] "The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone."
  - [section] "We show that a combination of RNN-LSTM with TCN results in a better generative model compared to pure RNN models expressed as baseline models."
- Break condition: If the latent representations learned by RNN and TCN are not complementary or if the combination method is not effective, the performance improvement may not be observed.

### Mechanism 2
- Claim: Different fine-tuning protocols significantly impact the generative scope of the model when applied on a dataset of interest via transfer learning.
- Mechanism: The choice of fine-tuning protocol determines how much of the pre-trained model's knowledge is retained versus adapted to the new dataset. Freezing certain layers allows the model to leverage previously learned patterns while adapting to the new data, enabling few-shot learning.
- Core assumption: The pre-trained model has learned generalizable patterns that can be effectively transferred to new datasets.
- Evidence anchors:
  - [abstract] "Different fine-tuning protocols significantly impact generative scope, with a weight-freezing approach enabling few-shot learning on smaller datasets."
  - [section] "With a variant of the weight freezing, we show that our fine tuned model, trained on our own dataset, significantly outperforms models that learned from an all-weights optimization transfer learning approach."
- Break condition: If the pre-trained model has not learned generalizable patterns or if the new dataset is too dissimilar, the fine-tuning protocol may not have a significant impact.

### Mechanism 3
- Claim: The model can generate reactions with novel reaction centers not seen in the training data, demonstrating its potential for discovering new chemical reactions.
- Mechanism: The generative model learns the underlying grammar and patterns of chemical reactions from the training data. This learned knowledge allows it to generate novel reactions that follow similar patterns but involve unseen reaction centers.
- Core assumption: The training data contains sufficient diversity and the model can effectively learn the underlying patterns.
- Evidence anchors:
  - [abstract] "The model successfully generates reactions with novel reaction centers not seen in training data, demonstrating its potential for discovering new chemical reactions."
  - [section] "Figure 5 shows examples of generated reactions with novel reaction centers that were not present in the dataset."
- Break condition: If the training data lacks diversity or if the model fails to learn the underlying patterns effectively, it may not be able to generate novel reactions with unseen reaction centers.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks
  - Why needed here: RNNs are used as the baseline model for sequence generation, and LSTMs are a variant of RNNs that address the vanishing gradient problem, making them suitable for learning long-term dependencies in chemical reaction sequences.
  - Quick check question: What is the main advantage of using LSTMs over traditional RNNs in this context?

- Concept: Temporal Convolutional