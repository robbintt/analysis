---
ver: rpa2
title: Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context
  Learning
arxiv_id: '2310.08923'
source_url: https://arxiv.org/abs/2310.08923
tags:
- learning
- examples
- random
- sampling
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of instability in In-Context Learning
  (ICL) due to the variance introduced by randomly selecting examples, even when input
  distribution, ordering, and prompt formats are held constant. To improve ICL performance,
  the authors propose quantifying the Information Gain (IG) of each example and selecting
  those with maximum IG.
---

# Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning

## Quick Facts
- **arXiv ID:** 2310.08923
- **Source URL:** https://arxiv.org/abs/2310.08923
- **Reference count:** 40
- **Primary result:** MaxIG selection achieves 14.3% average relative improvement in one-shot learning over random baseline

## Executive Summary
This paper addresses the instability problem in In-Context Learning (ICL) where random example selection leads to high variance in model performance. The authors propose quantifying the Information Gain (IG) of each example and selecting those with maximum IG to improve ICL effectiveness. They identify Template Bias in zero-shot prompts and introduce Calibration Before Sampling to mitigate this bias. Experiments across six classification tasks using three LLMs demonstrate that their MaxIG-based approach consistently outperforms random and entropy-based baselines.

## Method Summary
The method computes Information Gain for each candidate example by measuring the reduction in entropy of the LLM's output distribution after observing that example. To address template bias, the approach first calibrates the LLM's zero-shot predictions using content-free strings before computing IG. The calibrated probabilities are used to rank examples, and the top-K examples with highest IG are selected as demonstrations. The framework is model-agnostic and can be applied to any LLM with a zero-shot prompt interface.

## Key Results
- MaxIG selection achieves 14.3% average relative improvement in one-shot learning accuracy over random baseline
- Calibration Before Sampling effectively mitigates template bias, leading to fairer IG evaluation
- MaxIG consistently outperforms both Random and MaxEntropy baselines across all six classification tasks tested

## Why This Works (Mechanism)

### Mechanism 1
Random example selection in ICL leads to high variance because different examples contribute differently to model performance. Information Gain (IG) quantifies how much each example improves prediction accuracy when included as a demonstration. The core assumption is that the LLM's output distribution change after observing an example correlates with that example's contribution to downstream task performance. Break condition: If IG estimation is inaccurate (e.g., due to template bias or poor calibration), the selected examples may not improve performance.

### Mechanism 2
Template Bias causes unfair evaluation of IG because LLMs have inherent preferences even with content-free inputs. Calibration Before Sampling adjusts the LLM's output distribution to remove template-induced bias before computing IG. The core assumption is that the LLM's predictions on content-free strings represent the baseline bias that needs correction. Break condition: If the calibration model is poorly fitted (e.g., insufficient content-free samples), the correction may be inadequate.

### Mechanism 3
MaxIG selection outperforms Random and MaxEntropy baselines because it directly targets informative examples rather than uncertain ones. By selecting examples with maximum IG, the method ensures each demonstration provides maximal information gain for the task. The core assumption is that IG is a better proxy for demonstration quality than entropy-based uncertainty in ICL. Break condition: If IG and entropy are highly correlated for a given task, the advantage of MaxIG may diminish.

## Foundational Learning

- **Concept:** Information Gain (IG) from information theory
  - Why needed here: IG provides a principled way to measure how much each example improves prediction accuracy
  - Quick check question: If an example's inclusion doesn't change the LLM's output distribution, what is its IG value?
    Answer: Zero, because there is no information gain.

- **Concept:** Template Bias in zero-shot prompts
  - Why needed here: Unaddressed template bias leads to unfair IG evaluation and suboptimal example selection
  - Quick check question: If a template consistently predicts "positive" for content-free inputs, what kind of bias does this represent?
    Answer: Positive bias, which needs to be calibrated out.

- **Concept:** Calibration using Platt scaling
  - Why needed here: Adjusts the LLM's raw probabilities to correct for template-induced bias before IG computation
  - Quick check question: What does the diagonal weight matrix W in Platt scaling represent?
    Answer: The inverse of the average output probabilities on content-free inputs.

## Architecture Onboarding

- **Component map:** LLM inference engine → Zero-shot prompt construction → Calibration module → IG computation → Example ranking
- **Critical path:** 1. Construct zero-shot prompts for each candidate example 2. Compute raw probabilities from LLM 3. Calibrate probabilities using content-free strings 4. Calculate conditional entropy (IG) 5. Rank examples and select top-K
- **Design tradeoffs:** Accuracy vs. speed (computing IG for all candidates is expensive but improves selection quality); Model-agnostic vs. model-specific (IG is model-aware, requiring recomputation for new LLMs)
- **Failure signatures:** High variance in performance across runs (calibration may be insufficient); No improvement over Random (IG estimation may be inaccurate or IG and entropy are correlated)
- **First 3 experiments:** 1. Run MaxIG vs. Random on SST-2 with GPT-2 XL to verify performance improvement 2. Test calibration effectiveness by comparing IG scores before and after calibration 3. Validate Template Bias exists by checking LLM predictions on content-free strings

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can information gain be defined and computed for generation tasks where the output distribution contains open words and has variable lengths?
**Basis in paper:** [explicit] The paper mentions that extending experiments to generation tasks is challenging because it requires a tractable definition of information gain for output distributions with open words and variable lengths.
**Why unresolved:** The paper does not provide a solution or methodology for defining information gain in generation tasks, focusing instead on text classification where information gain is well-defined.
**What evidence would resolve it:** A proposed method or framework for computing information gain in generation tasks, along with experimental results showing its effectiveness compared to random selection.

### Open Question 2
**Question:** How can diversity be incorporated into the sampling process for in-context learning, especially when selecting many examples?
**Basis in paper:** [explicit] The paper notes that their sampling process does not explicitly consider the diversity of examples, prioritizing informative ability instead. They mention that exploring methods to incorporate diversity is important for future work.
**Why unresolved:** The paper does not explore or propose methods for incorporating diversity into the sampling process, focusing on selecting the most informative examples regardless of diversity.
**What evidence would resolve it:** A study comparing the performance of in-context learning with and without diversity consideration in the sampling process, ideally showing improved results when diversity is incorporated.

### Open Question 3
**Question:** How does the choice of sampling strategy differ between in-context learning (ICL) and traditional active learning settings, and what factors influence this choice?
**Basis in paper:** [explicit] The paper suggests that the MaxEntropy method from conventional active learning, which relies on parameter updates, may not be suitable for ICL where parameters remain static. They propose that examples with high information gain contribute more to ICL than uncertain examples with high entropy.
**Why unresolved:** The paper does not provide a comprehensive comparison between ICL and traditional active learning settings, nor does it explore the factors that influence the choice of sampling strategy in each case.
**What evidence would resolve it:** A comparative study of different sampling strategies in ICL and traditional active learning settings, along with an analysis of the factors that influence the effectiveness of each strategy in the respective settings.

## Limitations
- The method requires computing IG for all candidate examples, which can be computationally expensive for large datasets
- Performance depends on the quality of calibration using content-free strings, which may not adequately represent template bias for all tasks
- The approach is currently limited to classification tasks and doesn't address generation tasks with open-ended outputs

## Confidence

**High Confidence:** The identification of high variance in ICL performance due to random example selection is well-supported by empirical observations across multiple tasks and models. The mathematical formulation of Information Gain using conditional entropy is standard and correctly applied.

**Medium Confidence:** The effectiveness of Calibration Before Sampling is demonstrated empirically but relies on assumptions about the representativeness of content-free strings for template bias. The choice of Platt scaling for calibration is reasonable but not the only possible approach.

**Low Confidence:** The claim that MaxIG is universally superior to MaxEntropy across all ICL scenarios is based on limited empirical evidence across six tasks. The paper doesn't explore edge cases where entropy-based methods might perform better, particularly for tasks requiring diverse demonstrations rather than maximally informative ones.

## Next Checks

1. **Calibration Robustness Test:** Run the MaxIG selection on SST-2 with varying numbers of content-free strings (5, 10, 20, 50) to verify that calibration quality directly impacts selection performance. Compare IG scores and final ICL accuracy across these settings.

2. **Template Bias Validation:** For each task, measure the LLM's zero-shot prediction distribution on content-free strings and compare it to the calibrated distribution. Quantify the reduction in bias and verify that calibration consistently improves IG estimation across all six tasks.

3. **Generalization Test:** Apply MaxIG selection to a new task (e.g., IMDB sentiment) using a different LLM (e.g., LLaMA) not included in the original experiments. Compare performance against Random and MaxEntropy baselines to assess method transferability.