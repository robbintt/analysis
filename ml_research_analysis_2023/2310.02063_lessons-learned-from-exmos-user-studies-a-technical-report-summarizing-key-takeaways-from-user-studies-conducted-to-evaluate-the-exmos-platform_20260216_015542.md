---
ver: rpa2
title: 'Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key
  Takeaways from User Studies Conducted to Evaluate The EXMOS Platform'
arxiv_id: '2310.02063'
source_url: https://arxiv.org/abs/2310.02063
tags:
- explanations
- data
- systems
- learning
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A technical report summarizing the key takeaways from two user
  studies conducted to evaluate the EXMOS platform, an interactive machine learning
  system for healthcare experts to optimize predictive models. The studies investigated
  the impact of data-centric and model-centric global explanations on trust, understandability,
  and model improvement.
---

# Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform

## Quick Facts
- arXiv ID: 2310.02063
- Source URL: https://arxiv.org/abs/2310.02063
- Reference count: 24
- Key outcome: Hybrid explanations combining data-centric and model-centric approaches yield the best performance in model improvement and user understanding

## Executive Summary
This technical report summarizes key findings from two user studies evaluating the EXMOS platform, an interactive machine learning system designed for healthcare experts to optimize predictive models. The studies compared the effectiveness of data-centric explanations, model-centric explanations, and hybrid approaches in improving trust, understandability, and model performance. Results demonstrated that combining both explanation types was most effective, with hybrid approaches leading to significantly better performance in data configuration for model improvement. Data-centric explanations were particularly effective in improving prediction accuracy compared to model-centric explanations alone.

## Method Summary
The research employed a prototype XIL system using a Random Forest algorithm on a diabetes prediction dataset. Two user studies were conducted: a quantitative study with 70 healthcare experts and a qualitative study with 30 healthcare experts. Participants used three different explanation dashboard versions - Data-Centric, Model-Centric, and Hybrid - to configure data and improve model performance. The studies measured trust, understandability, and model improvement through various metrics including prediction accuracy and user task performance.

## Key Results
- Hybrid explanations combining data-centric and model-centric approaches yielded the best performance in model improvement and user understanding
- Data-centric explanations were more effective than model-centric explanations in improving prediction accuracy
- Users using hybrid explanations demonstrated significantly better performance in data configuration despite perceiving higher task load

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid explanations combining data-centric and model-centric approaches yield the best performance in model improvement and user understanding.
- Mechanism: By providing both holistic data quality insights (data-centric) and detailed model decision logic (model-centric), users gain a complete mental model that enables effective debugging and configuration.
- Core assumption: Users need both types of information simultaneously rather than sequentially to make informed decisions.
- Evidence anchors:
  - [abstract] "Results show that global model-centric explanations alone are insufficient for effectively guiding users during the intricate process of data configuration. In contrast, data-centric explanations exhibited their potential by enhancing the understanding of system changes that occur post-configuration. However, a combination of both showed the highest level of efficacy"
  - [section] "Data-centric explanations were more effective in improving prediction accuracy compared to model-centric explanations, with the hybrid (HYB) approach being the most effective"
- Break Condition: If users become overwhelmed by information density, the hybrid approach may degrade rather than improve performance.

### Mechanism 2
- Claim: Increased task load from exploring comprehensive explanations leads to better manual configurations.
- Mechanism: More time spent exploring explanations (higher hover-time) facilitates deeper understanding, resulting in more effective data configurations despite perceived cognitive load.
- Core assumption: Time invested in explanation exploration directly translates to better configuration decisions.
- Evidence anchors:
  - [section] "HYB users performed better manual data configurations despite a higher perceived task load and longer average hover-time, as more time spent exploring HYB explanations facilitated faster and more effective data configurations"
  - [abstract] "participants using the Hybrid explanation dashboard demonstrated significantly better performance in data configuration for model improvement, even though they perceived a higher task load"
- Break Condition: If task load becomes excessive, users may abandon the system or make rushed decisions.

### Mechanism 3
- Claim: Lack of data quality information in model-centric explanations reduces effectiveness of automated data corrections.
- Mechanism: Users spend less time understanding automated corrections when they lack context about data quality issues, limiting their ability to verify or learn from system actions.
- Core assumption: Data quality context is essential for user trust and understanding of automated system behaviors.
- Evidence anchors:
  - [section] "Lack of data quality information in MCE resulted in less time spent by users to understand automated data corrections"
- Break Condition: If automated corrections are simple and obvious, users may not need extensive data quality context.

## Foundational Learning

- Concept: Global vs. Local Explanations
  - Why needed here: The paper focuses on global explanations but mentions local explanations in design recommendations, requiring understanding of both types
  - Quick check question: What's the key difference between global and local explanations in terms of scope and user utility?

- Concept: Interactive Machine Learning (IML) and Explanatory Interactive Learning (XIL)
  - Why needed here: The EXMOS platform is an XIL system, and understanding this paradigm is crucial for grasping the research context
  - Quick check question: How does XIL differ from traditional IML in terms of user interaction and feedback mechanisms?

- Concept: Data-Centric vs. Model-Centric Explanations
  - Why needed here: These are the two primary explanation types evaluated in the studies, and their distinction drives the research findings
  - Quick check question: What types of insights does each explanation type provide, and why might combining them be more effective?

## Architecture Onboarding

- Component map:
  - Explanation Dashboard (three versions: Data-Centric, Model-Centric, Hybrid)
  - Random Forest Prediction Model
  - Diabetes Prediction Dataset
  - Configuration Interface (manual and automated)
  - User Study Interface

- Critical path:
  1. User accesses explanation dashboard
  2. User explores explanations to understand model/data
  3. User makes configuration decisions (manual or automated)
  4. System updates model based on configuration
  5. User evaluates model improvement

- Design tradeoffs:
  - Comprehensive explanations vs. cognitive load
  - Automation vs. user control
  - Real-time updates vs. system performance

- Failure signatures:
  - Users spending excessive time without making configurations
  - Users ignoring explanations and making random changes
  - Users losing trust due to lack of data quality information

- First 3 experiments:
  1. Implement single-user prototype with only Data-Centric explanations
  2. Add Model-Centric explanations to create Hybrid version
  3. Conduct A/B testing between Data-Centric, Model-Centric, and Hybrid versions with target user group

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of global explanations (data-centric, model-centric, and hybrid) affect trust and understanding across various healthcare domains beyond diabetes prediction?
- Basis in paper: [explicit] The paper states that the study focused on diabetes prediction and recommends combining data-centric and model-centric explanations for healthcare XIL systems, but does not explore other healthcare domains.
- Why unresolved: The study's scope was limited to a single healthcare domain, and the paper does not provide evidence of the generalizability of the findings to other healthcare contexts.
- What evidence would resolve it: Conducting user studies in multiple healthcare domains with diverse prediction tasks and comparing the effectiveness of different explanation types across these domains.

### Open Question 2
- Question: What is the optimal balance between manual and automated data configuration approaches in XIL systems for healthcare professionals with varying levels of technical expertise?
- Basis in paper: [explicit] The paper mentions that users have different preferences for control levels and suggests offering both manual and automated configurations, but does not provide guidance on the optimal balance.
- Why unresolved: The study did not investigate how technical expertise influences the preference for manual versus automated configurations or the optimal balance between these approaches.
- What evidence would resolve it: User studies involving healthcare professionals with varying technical expertise, comparing the effectiveness and efficiency of different ratios of manual to automated configurations.

### Open Question 3
- Question: How does the inclusion of local explanations (e.g., counterfactual and what-if explanations) impact the effectiveness of global explanations in XIL systems for healthcare professionals?
- Basis in paper: [explicit] The paper recommends incorporating local explanations to enhance the usefulness, understandability, and actionability of global explanations, but does not provide empirical evidence of their impact.
- Why unresolved: The study focused on global explanations and did not investigate the role of local explanations in improving the effectiveness of XIL systems for healthcare professionals.
- What evidence would resolve it: Conducting user studies comparing the performance of XIL systems with and without local explanations, measuring the impact on trust, understanding, and model improvement.

### Open Question 4
- Question: How does the presentation of data quality information and data collection process details in the initial view of XIL systems affect the trust and understanding of healthcare professionals?
- Basis in paper: [explicit] The paper suggests providing comprehensive data quality information and disclosing the data collection process in the initial view to improve transparency and trust, but does not provide empirical evidence of their impact.
- Why unresolved: The study did not investigate the specific placement and presentation of data quality and collection process information in the XIL system's user interface.
- What evidence would resolve it: User studies comparing the effectiveness of different placements and presentations of data quality and collection process information in the initial view of XIL systems, measuring the impact on trust, understanding, and system adoption.

## Limitations

- Findings are based on prototype evaluations rather than production deployment, limiting generalizability to real-world healthcare settings
- Study participants were healthcare experts but not necessarily the end-users who would deploy these systems in clinical practice
- Diabetes prediction dataset may not represent the full complexity of real healthcare data scenarios
- Focus on global explanations may overlook important local explanation needs for specific patient cases

## Confidence

- High confidence in the core finding that hybrid explanations outperform single-type explanations for model improvement and user understanding
- Medium confidence in the specific performance metrics and task load measurements due to potential prototype limitations
- Medium confidence in the generalizability of findings across different healthcare domains and ML applications
- Low confidence in long-term sustainability and adoption factors beyond initial user experience

## Next Checks

1. Conduct longitudinal studies tracking user behavior over extended periods (minimum 3 months) to assess sustained effectiveness and learning retention
2. Test the hybrid explanation approach with actual clinical end-users who would deploy these systems in practice, not just healthcare experts
3. Validate findings across multiple disease prediction tasks and different ML algorithm types to assess generalizability beyond Random Forest on diabetes data