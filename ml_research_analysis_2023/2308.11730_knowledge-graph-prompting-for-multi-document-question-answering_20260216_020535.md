---
ver: rpa2
title: Knowledge Graph Prompting for Multi-Document Question Answering
arxiv_id: '2308.11730'
source_url: https://arxiv.org/abs/2308.11730
tags:
- question
- graph
- passages
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge graph prompting (KGP) method to
  enhance large language models (LLMs) for multi-document question answering (MD-QA).
  The core idea is to construct a knowledge graph (KG) over the given documents with
  nodes representing passages or document structures and edges denoting their lexical/semantic
  similarity or structural relations.
---

# Knowledge Graph Prompting for Multi-Document Question Answering

## Quick Facts
- arXiv ID: 2308.11730
- Source URL: https://arxiv.org/abs/2308.11730
- Reference count: 40
- Key outcome: KGP-T5 achieves 46.51% EM and 66.77% F1 on HotpotQA

## Executive Summary
This paper proposes Knowledge Graph Prompting (KGP), a novel method for multi-document question answering (MD-QA) that leverages large language models (LLMs) to navigate a knowledge graph (KG) constructed over the document collection. The KG serves as a global ruler to regulate the transitional space among passages and reduce retrieval latency, while an LM-guided graph traverser acts as a local navigator to gather pertinent context. The method demonstrates significant improvements over baseline retrieval strategies on the HotpotQA dataset, with KGP-T5 achieving the best performance among various baselines.

## Method Summary
KGP constructs a knowledge graph over the document collection, with nodes representing passages or document structures (pages/tables) and edges denoting their lexical/semantic similarity or structural relations. An LM-guided graph traverser is then used to adaptively navigate the KG and retrieve supporting passages for the LLM. The traverser is fine-tuned to predict the next supporting fact given the current evidence and the question, enhancing its reasoning capabilities. The retrieved context is then used to prompt the LLM for answer generation.

## Key Results
- KGP-T5 achieves 46.51% EM and 66.77% F1 on HotpotQA, outperforming baseline retrieval methods.
- The LM-guided graph traversal significantly improves the quality of retrieved context compared to traditional retrieval strategies.
- KGP effectively handles questions related to document structures, such as pages and tables.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KGP improves MD-QA by structuring document content into a navigable graph, enabling LLM-guided traversal toward relevant supporting passages.
- **Core assumption**: The logical associations needed to answer MD-QA questions are encoded in the lexical/semantic similarity or structural relations between passages, and these can be effectively captured in a KG structure.
- **Evidence anchors**:
  - [abstract] "The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency."
  - [section 3] "This motivates us to construct the graph by modeling passages as nodes and their lexical/semantic similarity as edges."
  - [corpus] Weak - no direct corpus evidence comparing graph traversal vs. flat retrieval in this specific task.
- **Break condition**: If the constructed KG fails to encode the necessary logical associations between passages, or if the LM-guided traverser cannot effectively rank neighbors, the method will not improve over baseline retrieval strategies.

### Mechanism 2
- **Claim**: Fine-tuning the LM used for graph traversal on the task of predicting the next supporting fact enhances its ability to generate contextually relevant evidence for navigating the KG.
- **Core assumption**: Pre-training an LM on a diverse corpus provides general world knowledge, but fine-tuning on task-specific data (predicting supporting facts) is necessary to develop the reasoning skills required for effective KG traversal in MD-QA.
- **Evidence anchors**:
  - [section 4] "To mitigate the hallucination issue and enhance the reasoning capability of LMs, we further apply instruction fine-tuning to f..."
  - [section 5.4] "This aligns with our previous observation on the generally low precision in Figure 5 and further demonstrates the necessity of using LMs to guide the graph traversal."
  - [corpus] Weak - no direct evidence showing the impact of instruction fine-tuning vs. other fine-tuning strategies.
- **Break condition**: If the instruction fine-tuning data is not representative of the MD-QA task, or if the LM overfits to the training data, the traversal quality will degrade.

### Mechanism 3
- **Claim**: Incorporating document structure (pages/tables) as nodes in the KG allows the system to handle questions that specifically target structured information, which traditional retrieval methods struggle with.
- **Core assumption**: Questions targeting document structures can be effectively answered by retrieving the corresponding structured content, and that LLMs can understand and reason over markdown-formatted tables.
- **Evidence anchors**:
  - [abstract] "The feature of table nodes is the markdown since LLMs can understand this as demonstrated in Figure 8 in Supplementary."
  - [section 3] "If pages include passages and tables, we add a directed edge to denote the belonging relations."
  - [corpus] Weak - no direct evidence comparing performance on structure-specific questions with and without structural nodes.
- **Break condition**: If the document structure extraction is inaccurate, or if the LLM cannot effectively understand the markdown-formatted table content, the system will fail to answer structure-specific questions.

## Foundational Learning

- **Concept**: Knowledge Graph Construction Methods
  - **Why needed here**: Understanding how to construct a KG over documents with nodes representing passages/structures and edges encoding relationships is fundamental to the KGP method.
  - **Quick check question**: What are the different ways to construct a KG over documents for MD-QA, and how do they differ in terms of the relationships they encode?
- **Concept**: Language Model Fine-tuning for Task-Specific Reasoning
  - **Why needed here**: The LM-guided traverser relies on a fine-tuned language model to rank candidate neighbors based on their relevance to the question. Understanding how to fine-tune LMs for task-specific reasoning is crucial.
  - **Quick check question**: How does instruction fine-tuning differ from other fine-tuning strategies, and why is it particularly effective for enhancing reasoning capabilities in the context of KG traversal?
- **Concept**: Graph Traversal Algorithms
  - **Why needed here**: The LM-guided traverser navigates the KG to find relevant supporting passages. Understanding graph traversal algorithms, such as breadth-first search, is essential for implementing and optimizing the traversal process.
  - **Quick check question**: How does the LM-guided graph traversal algorithm differ from traditional graph traversal algorithms, and what are the key considerations for balancing exploration and exploitation?

## Architecture Onboarding

- **Component map**: Document Processing -> Knowledge Graph Construction -> LM-guided Graph Traverser -> Context Retrieval -> LLM-based Answer Generation
- **Critical path**: Document Processing → Knowledge Graph Construction → LM-guided Graph Traverser → Context Retrieval → LLM-based Answer Generation
- **Design tradeoffs**:
  - KG Sparsity vs. Retrieval Quality: A denser KG increases the likelihood of including relevant passages but also increases retrieval latency and the risk of including irrelevant information.
  - LM Fine-tuning Data vs. Generalization: Fine-tuning the LM on a large dataset of supporting facts enhances its reasoning capabilities but may reduce its ability to generalize to out-of-distribution questions.
  - Structural Node Inclusion vs. Complexity: Including structural nodes (pages/tables) enables handling structure-specific questions but increases the complexity of the KG construction and traversal.
- **Failure signatures**:
  - Low retrieval quality: The retrieved context does not include the necessary supporting facts to answer the question.
  - High retrieval latency: The KG traversal takes too long to find relevant passages, making the system impractical for real-time use.
  - LLM hallucination: The LLM generates an answer that is not supported by the retrieved context.
- **First 3 experiments**:
  1. Compare the MD-QA performance of KGP using different KG construction methods (TF-IDF, KNN-ST, KNN-MDR, TAGME) on a small dataset.
  2. Evaluate the impact of instruction fine-tuning on the LM-guided traverser's ability to rank relevant neighbors by comparing its performance with and without fine-tuning.
  3. Test the system's ability to handle structure-specific questions by evaluating its performance on a dataset with questions targeting pages and tables.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the constructed knowledge graph (KG) impact the performance of multi-document question answering (MD-QA)?
- Basis in paper: [explicit] The paper states that the quality of the KG, in terms of density and the trade-off between precision and supporting fact exact matching (SF-EM), impacts the MD-QA performance.
- Why unresolved: The paper mentions the trade-off between precision and SF-EM as the KG density increases, but it does not provide a clear threshold or optimal density for the KG to maximize MD-QA performance. Additionally, the paper does not explore the impact of different KG construction methods on the MD-QA performance in depth.
- What evidence would resolve it: A comprehensive study comparing the MD-QA performance using KGs constructed with different methods (TF-IDF, KNN-ST, KNN-MDR, and TAGME) at varying densities, along with an analysis of the optimal density for each method, would help resolve this question.

### Open Question 2
- Question: How does the LM-guided graph traversal agent's performance vary when using different language models (LMs) for guidance?
- Basis in paper: [explicit] The paper compares the performance of KGP-T5, KGP-LLaMA, and KGP-MDR, which use T5, LLaMA, and MDR as the LM-guided graph traversal agent, respectively.
- Why unresolved: The paper provides a comparison of the performance of these three LMs, but it does not explore the impact of other factors, such as the size of the LM, the pre-training data, or the fine-tuning strategy, on the MD-QA performance. Additionally, the paper does not investigate the potential of using ensemble methods or combining multiple LMs for guidance.
- What evidence would resolve it: A study comparing the MD-QA performance using different LMs with varying sizes, pre-training data, and fine-tuning strategies, as well as exploring the potential of ensemble methods, would help resolve this question.

### Open Question 3
- Question: How can the proposed knowledge graph prompting (KGP) method be extended to handle more complex document structures, such as tables and figures, in MD-QA?
- Basis in paper: [inferred] The paper mentions that the KGP method can handle questions related to document structures, such as pages and tables, but it does not provide a detailed analysis of how to handle more complex structures or how to incorporate them into the KG construction and traversal process.
- Why unresolved: The paper only briefly mentions the possibility of handling more complex document structures, but it does not provide a concrete solution or experimental results to demonstrate the effectiveness of the proposed method in handling such structures.
- What evidence would resolve it: A study extending the KGP method to handle more complex document structures, such as tables and figures, and evaluating its performance on MD-QA tasks involving these structures would help resolve this question.

## Limitations

- The evaluation is limited to a single MD-QA dataset (HotpotQA), leaving the method's generalizability to other domains and question types uncertain.
- The LM-guided traversal algorithm relies on fine-tuning an LLM to predict supporting facts, but the paper provides limited evidence on how this fine-tuning data was curated or whether the model generalizes beyond the training distribution.
- The knowledge graph construction methods (TF-IDF, KNN-ST, KNN-MDR, TAGME) are not directly compared against each other, making it unclear which approach is most effective for different types of questions or document collections.

## Confidence

- High: The overall KGP framework improves MD-QA performance compared to baseline retrieval methods (supported by HotpotQA results)
- Medium: The LM-guided traversal algorithm effectively navigates the knowledge graph to find relevant passages (supported by qualitative examples but limited quantitative ablation)
- Low: The specific KG construction methods (TF-IDF, KNN-ST, KNN-MDR, TAGME) are optimal for MD-QA (no direct comparison between methods)

## Next Checks

1. Evaluate KGP on additional MD-QA datasets (IIRC, 2WikiMultiHopQA, MuSiQue) to test generalizability beyond HotpotQA
2. Conduct ablation studies comparing different KG construction methods (TF-IDF, KNN-ST, KNN-MDR, TAGME) to identify optimal approaches for different question types
3. Test the robustness of the LM-guided traversal by evaluating performance when the knowledge graph contains noisy or irrelevant passages