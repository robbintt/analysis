---
ver: rpa2
title: Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on
  Low-Dimensional Manifolds
arxiv_id: '2309.13915'
source_url: https://arxiv.org/abs/2309.13915
tags:
- policy
- function
- lemma
- lipschitz
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the sample complexity of neural policy mirror
  descent (NPMD) with convolutional neural networks (CNN) for policy optimization
  on low-dimensional manifolds. The key contributions are: (1) Under Lipschitz MDP
  conditions, CNN can well approximate both value functions and policies, with approximation
  errors controlled by network size and inheriting smoothness from previous networks.'
---

# Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds

## Quick Facts
- arXiv ID: 2309.13915
- Source URL: https://arxiv.org/abs/2309.13915
- Reference count: 40
- One-line primary result: NPMD with CNNs finds ε-optimal policy with O(ε^{-d/α-2}) samples on low-dimensional manifolds

## Executive Summary
This paper analyzes the sample complexity of neural policy mirror descent (NPMD) for policy optimization when the state space has low intrinsic dimension d ≪ D. Under Lipschitz MDP conditions, the authors show that CNNs can approximate both value functions and policies with errors controlled by network size while inheriting smoothness from previous networks. The key insight is that NPMD can leverage the low-dimensional manifold structure to escape the curse of dimensionality, achieving sample complexity that depends on the intrinsic dimension d rather than the ambient dimension D.

## Method Summary
The method uses NPMD with two neural networks: a critic Q_w that approximates the value function and an actor f_θ that approximates the policy. Both networks are implemented as CNNs from class F(M,L,J,I,R1,R2). The critic is updated via empirical risk minimization to approximate Q_π_k, while the actor is updated to approximate π*_{k+1} using samples from the current visitation distribution. A temperature parameter λ_k is scheduled to make the stochastic policy increasingly deterministic, and regularization parameters τ_k and η_k control the bias-variance tradeoff. The algorithm iterates these updates for K = O(ε^{-2}) steps with decreasing temperature and regularization.

## Key Results
- NPMD with CNNs achieves O(ε^{-d/α-2}) sample complexity on low-dimensional manifolds, escaping curse of dimensionality
- The sample complexity depends on the intrinsic dimension d rather than ambient dimension D when d ≪ D
- The approximately Lipschitz condition allows handling approximation errors when critic network is not exact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN approximation does not suffer from the curse of dimensionality when the state space has low intrinsic dimension d ≪ D
- Mechanism: The sample complexity depends on d instead of D because the manifold structure allows CNN to exploit the low-dimensional geometry. The number of parameters needed scales as O(D^3 I ϵ^{-d/α}), where the exponent over ϵ is the intrinsic dimension d rather than the data dimension D.
- Core assumption: The state space S is a d-dimensional compact Riemannian manifold embedded in RD with d ≪ D
- Evidence anchors:
  - [abstract]: "our result exhibits that NPMD can leverage the low-dimensional structure of state space to escape from the curse of dimensionality"
  - [section 4.2]: "Note that the exponent over ϵ is the intrinsic dimension d rather than the data dimension D, and the hidden terms in eO(·) also have no exponential dependence on D"
- Break condition: If the manifold assumption fails or d is not much smaller than D, the complexity reverts to O(D^{d/α} ϵ^{-d/α})

### Mechanism 2
- Claim: The approximately Lipschitz condition allows actor network approximation even when the target function is not exactly Lipschitz
- Mechanism: When approximating the policy π⋆_{k+1}, the target function is a weighted sum of the current critic and actor networks. Since the critic is only approximately Lipschitz (not exactly), we introduce the approximately Lipschitz condition with proximity constant ϵ to characterize the smoothness inherited from approximating a Lipschitz function.
- Core assumption: The critic network Qwk is a uniform approximation of the Lipschitz function Qπk with error bounded by ϵQ
- Evidence anchors:
  - [section 4.2]: "Definition 5 (Approximate Lipschitzness)... When ϵ = 0, f is (L, α)-Lipschitz as defined in Definition 4"
  - [section 4.2]: "Lemma 4.If f 0 : S → R is (L, α)-Lipschitz, f : S → R satisfies ∣f − f 0∣∞ ≤ ϵ for some ϵ > 0, then f is (L, α, ϵ)-approximately Lipschitz"
- Break condition: If the approximation error exceeds the proximity constant ϵQ, the Lipschitz restriction on the actor network may not be satisfied

### Mechanism 3
- Claim: The temperature parameter λk allows the stochastic policy πk to approximate the deterministic optimal policy π⋆
- Mechanism: As λk approaches zero, the stochastic policy πk becomes increasingly peaked around the action with maximal value of fθk(s, a). By using sufficiently small temperature, πk can approximate the deterministic optimal policy π⋆ that chooses the action a ∈ A with the maximal value of fθk(s, a).
- Core assumption: The actor network fθk(s, ·) admits a maximizer a ∈ supp(π⋆(·|s)) for any state s ∈ S
- Evidence anchors:
  - [section 3.2]: "Remark 1. The temperature parameter λk is introduced mainly for technical reasons... As λk approaches zero, πk is prone to the action with the maximal value of fθk"
  - [section 4.2]: "Although πk is a stochastic policy by construction, it can approximate the deterministic policy that chooses the action a ∈ A with the maximal value of fθk(s, a) by using a sufficiently small temperature λk > 0"
- Break condition: If the actor network is not sufficiently accurate or the temperature is not small enough, the approximation of the deterministic policy will fail

## Foundational Learning

- Concept: Riemannian manifold and geodesic distance
  - Why needed here: The paper defines Lipschitz continuity with respect to geodesic distance dS(x,y) on the manifold, which is a global distance rather than local Euclidean distance. This is crucial for the approximation theory.
  - Quick check question: What is the difference between geodesic distance and Euclidean distance on a manifold, and why does the paper prefer geodesic distance for defining Lipschitzness?

- Concept: Approximate Lipschitzness and its properties
  - Why needed here: The actor update target is not exactly Lipschitz because the critic network is only an approximation. The paper introduces the concept of approximately Lipschitz functions to handle this case.
  - Quick check question: How does the approximately Lipschitz condition with proximity constant ϵ generalize the standard Lipschitz condition, and why is this generalization necessary for the actor approximation?

- Concept: Total variation distance and χ²-divergence for distribution mismatch
  - Why needed here: The paper measures the distance between visitation distributions using total variation distance for the transition kernel and χ²-divergence for the concentrability assumption.
  - Quick check question: What are the key differences between total variation distance and χ²-divergence, and why does the paper use each for different purposes?

## Architecture Onboarding

- Component map:
  - Critic update → ERM with samples from νπk_ρ → Approximates Qπk
  - Actor update → ERM with samples from νπk_ρ → Approximates π⋆_{k+1}
  - Temperature scheduling → λk = Cγ^k / (1-γρ) → Makes policy increasingly deterministic
  - Regularization → τk = Cγ^{k+1}ρ and ηk = (1-γρ)/τk → Controls bias-variance tradeoff
  - Manifold structure → d-dimensional manifold embedded in RD → d ≪ D

- Critical path:
  1. Sample states from νπk_ρ
  2. Update critic network to approximate Qπk
  3. Update actor network to approximate π⋆_{k+1}
  4. Iterate with decreasing temperature and regularization
  5. After K iterations, output πK with expected value function error ≤ ε

- Design tradeoffs:
  - Manifold dimension d vs data dimension D: Lower d reduces sample complexity but requires more assumptions
  - Temperature λk: Smaller λk makes policy more deterministic but requires more accurate approximation
  - Regularization τk: Larger τk reduces variance but increases bias
  - Sample size N: Larger N improves accuracy but increases computational cost

- Failure signatures:
  - If d ≈ D, sample complexity becomes O(ε^{-D/α-2}) and curse of dimensionality returns
  - If manifold assumption fails, Lipschitz approximation theory doesn't apply
  - If temperature schedule is incorrect, policy may not converge to optimal
  - If sample size is insufficient, approximation errors accumulate

- First 3 experiments:
  1. Test on simple 2D manifold (circle or torus) with known optimal policy to verify sample complexity scaling
  2. Compare performance on low-dimensional manifold vs high-dimensional ambient space with same intrinsic dimension
  3. Vary temperature schedule to find optimal tradeoff between exploration and exploitation for different environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the overall sample complexity of eO(ϵ^{-d/α - 2}) tight, or can it be improved?
- Basis in paper: Explicit - The paper derives this complexity but notes it is "yet to be examined whether the overall complexity is tight in ϵ".
- Why unresolved: The paper provides a theoretical upper bound but does not establish a lower bound to prove tightness. It also notes that the eO(ϵ^{-2}) term from iterations is optimal up to log terms, but the eO(ϵ^{-d/α}) term from function approximation may not be.
- What evidence would resolve it: A matching lower bound on sample complexity that matches the eO(ϵ^{-d/α - 2}) upper bound, or an improved upper bound with a better dependence on ϵ.

### Open Question 2
- Question: How does the curse of dimensionality manifest in practice when the intrinsic dimension d is not much smaller than the data dimension D?
- Basis in paper: Explicit - The paper shows the sample complexity has no exponential dependence on D when d << D, but notes that the sample complexity depends on log LQ = log(Lc + γC/(1-γ)LP), which can be large when the cost function scale C is large.
- Why unresolved: The paper focuses on the theoretical case where d << D, but does not provide empirical evidence or theoretical analysis of the case where d is closer to D. The dependence on the cost function scale C also suggests potential issues when C is large.
- What evidence would resolve it: Empirical results on environments where d is closer to D, showing the degradation in performance. Theoretical analysis of the sample complexity in the regime where d is closer to D.

### Open Question 3
- Question: Can the assumptions of full support (Assumption 2) and concentrability (Assumption 3) be relaxed or removed?
- Basis in paper: Explicit - The paper notes that these assumptions are unavoidable for the current analysis and provides a discussion on potential workarounds, but acknowledges that removing them remains an open problem.
- Why unresolved: The current analysis relies heavily on these assumptions, and removing them would require new theoretical techniques to handle the resulting issues with distribution mismatch and concentrability.
- What evidence would resolve it: A theoretical analysis of NPMD without Assumptions 2 and 3, either by providing alternative assumptions or by developing new techniques to handle the resulting issues.

## Limitations

- The manifold assumption (d ≪ D) may not hold for many real-world applications
- The approximately Lipschitz condition introduces complexity in verifying appropriate proximity constant
- The paper assumes access to accurate sampling from visitation distributions, which may be challenging in practice

## Confidence

- High confidence: The mechanism by which CNN approximation avoids curse of dimensionality (Mechanism 1) is well-supported by the theoretical analysis and empirical evidence in the paper.
- Medium confidence: The approximately Lipschitz condition (Mechanism 2) is a novel contribution but requires careful verification of the proximity constant in practical implementations.
- Medium confidence: The temperature scheduling mechanism (Mechanism 3) is theoretically sound but may require extensive tuning in practice to balance exploration and exploitation effectively.

## Next Checks

1. Verify the manifold assumption: Test the algorithm on environments with varying intrinsic dimensions d and ambient dimensions D to empirically confirm the sample complexity scaling with d rather than D.
2. Validate approximately Lipschitz approximation: Implement the actor update with different approximation error bounds to verify the impact of the proximity constant ϵ on the convergence and sample complexity.
3. Tune temperature scheduling: Experiment with different temperature schedules λk to find the optimal tradeoff between making the policy increasingly deterministic and maintaining sufficient exploration for convergence.