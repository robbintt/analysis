---
ver: rpa2
title: Scaling Laws for Adversarial Attacks on Language Model Activations
arxiv_id: '2312.02780'
source_url: https://arxiv.org/abs/2312.02780
tags:
- attack
- tokens
- token
- tmax
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new class of adversarial attacks on language\
  \ model activations, demonstrating that by manipulating a small subset of activation\
  \ vectors, an attacker can control the exact prediction of a significant number\
  \ of subsequent tokens. The authors empirically verify a scaling law where the maximum\
  \ number of target tokens predicted depends linearly on the number of attack tokens,\
  \ with an attack multiplier (\u03BA) that scales linearly with the activation dimension\
  \ (d) across different model sizes."
---

# Scaling Laws for Adversarial Attacks on Language Model Activations

## Quick Facts
- arXiv ID: 2312.02780
- Source URL: https://arxiv.org/abs/2312.02780
- Reference count: 25
- Key outcome: By manipulating a small subset of activation vectors, an attacker can control the exact prediction of many subsequent tokens, with a scaling law where target length depends linearly on attack tokens.

## Executive Summary
This paper introduces a new class of adversarial attacks targeting language model activations rather than tokens. The authors demonstrate that by optimizing perturbations to activation vectors immediately after the embedding layer, an attacker can control the exact prediction of a significant number of subsequent tokens. They empirically verify a scaling law where the maximum number of target tokens predicted depends linearly on the number of attack tokens, with an attack multiplier that scales linearly with activation dimension across different model sizes. The attack resistance (input bits needed to control output bits) remains constant at around 16-25 across the studied models, revealing a fundamental vulnerability in language model architectures.

## Method Summary
The attack method involves optimizing perturbations to activation vectors immediately after the embedding layer. Given context tokens S and target tokens T, the attacker computes the activations v = fbefore(S + T) and adds a perturbation P to the first a tokens of v. The perturbed activations are then passed through the remaining model layers, and the cross-entropy loss between the predicted and target tokens is minimized using gradient-based optimization (Adam optimizer). The maximum target length tmax is measured as a function of attack tokens a to establish the scaling law tmax = κa.

## Key Results
- A scaling law exists where maximum target length tmax depends linearly on attack tokens a as tmax = κa
- The attack multiplier κ scales linearly with activation dimension d across different model sizes
- Attack resistance χ (input bits needed per output bit) remains constant at approximately 16-25 across 2 orders of magnitude of model sizes
- Activation attacks are significantly stronger than token substitution attacks, but one bit of input control exerts similar control over output bits regardless of method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlling a small number of activation vectors can deterministically set many subsequent tokens.
- Mechanism: Activation vectors form a continuous representation space where small perturbations can shift the model's prediction trajectory. By optimizing these perturbations, the attacker steers the model toward a desired output sequence.
- Core assumption: The model's prediction for future tokens depends heavily on the immediate activation context, and the relationship between activation perturbations and output tokens is locally smooth enough for gradient-based optimization to work.
- Evidence anchors:
  - [abstract] "By manipulating a relatively small subset of model activations, a, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens t."
  - [section 2.2] "The goal of the attacker is to come up with a perturbation P to the first a token activations... such that the argmax autoregressive sampling from the model would yield the target sequence T as the continuation."

### Mechanism 2
- Claim: There is a linear scaling relationship between attack strength and target length.
- Mechanism: Each activation dimension provides a certain amount of control over output dimensions. The total control scales linearly with the number of activation dimensions controlled, leading to a proportional increase in the number of tokens that can be predicted.
- Core assumption: The mapping from activation space to output space preserves dimensional relationships, and control is additive across dimensions.
- Evidence anchors:
  - [abstract] "We empirically verify a scaling law where the maximum number of target tokens tmax predicted depends linearly on the number of tokens a whose activations the attacker controls as tmax = κa."
  - [section 4.1] "Fitting our scaling law from Eq. 2... we estimate the attack multiplier for EleutherAI/pythia-1.4b-v0 to be κ = 119.2 ±2.9."

### Mechanism 3
- Claim: The attack resistance χ remains roughly constant across model sizes.
- Mechanism: The ratio of input to output dimensionality, when measured in bits, stays similar across different model scales, suggesting a fundamental architectural constraint on how information flows from activations to predictions.
- Core assumption: The effective information capacity of the activation-to-output mapping doesn't scale proportionally with model size, maintaining a roughly constant ratio.
- Evidence anchors:
  - [abstract] "the number of bits of control in the input space needed to control a single bit in the output space (that we call attack resistance χ) is remarkably constant between ≈ 16 and ≈ 25 over 2 orders of magnitude of model sizes."
  - [section 4.3] "An interesting observation is that while the model trainable parameter counts span two orders of magnitude... the resulting relative attack multipliers κ/d, and the attack resistances χ stay surprisingly constant."

## Foundational Learning

- Concept: Gradient-based optimization in continuous spaces
  - Why needed here: The attack method relies on computing gradients of the loss with respect to activation perturbations to find effective adversarial examples.
  - Quick check question: If you have a loss function L(P) where P is a continuous vector, how would you find the direction that most decreases L?

- Concept: Dimensionality and information theory
  - Why needed here: Understanding how the number of controllable dimensions (activations) relates to the number of controllable output bits is central to the scaling law and attack resistance concepts.
  - Quick check question: If you can control d dimensions each with p bits of precision, how many total bits of control do you have?

- Concept: Autoregressive language modeling
  - Why needed here: The attack targets the autoregressive generation process, where each predicted token becomes part of the context for the next prediction.
  - Quick check question: In an autoregressive model, if you want to predict the 5th token, what input does the model actually receive?

## Architecture Onboarding

- Component map:
  Tokenization layer → Embedding layer → Transformer layers → Output logits
  Attack surface: Activation vectors immediately after embedding layer
  Optimization target: Perturbations to these activation vectors

- Critical path:
  1. Sample random context S and target T
  2. Compute activations v = fbefore(S + T)
  3. Add perturbation P to first a tokens of v
  4. Pass through remaining model layers
  5. Compute loss between predicted and target tokens
  6. Backpropagate to update P
  7. Repeat until target is predicted or max steps reached

- Design tradeoffs:
  - Attack strength vs. stealth: Larger perturbations are more effective but more detectable
  - Target length vs. success rate: Longer targets are harder to control
  - Context separation: Attack tokens can be separated from targets, but effectiveness decreases

- Failure signatures:
  - Optimization stalls (loss stops decreasing)
  - Perturbations grow very large without success
  - Success rate drops sharply with target length
  - Multi-attack setup fails to converge

- First 3 experiments:
  1. Single token attack (a=1) on a small model (70M params) with target length t=1, measure success rate
  2. Vary attack length a from 1 to 8 on same setup, plot success rate vs. target length
  3. Implement multi-attack with n=2, compare to single attack results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the attack multiplier κ and the numerical precision of model activations (e.g., 16-bit float vs 32-bit float)?
- Basis in paper: [explicit] The paper mentions that κ scales linearly with activation dimension d and defines attack resistance χ in terms of precision p, but does not experimentally vary precision to test this relationship.
- Why unresolved: The paper uses only 16-bit activations for all experiments and does not systematically vary the precision to observe its effect on κ or χ.
- What evidence would resolve it: Experiments measuring κ for the same model with different activation precisions (e.g., 16-bit vs 32-bit float) to determine if κ scales linearly with precision.

### Open Question 2
- Question: How does the attack resistance χ change with different training methods, such as adversarial training or other robustness-enhancing techniques?
- Basis in paper: [explicit] The paper assumes χ is constant across models and mentions that adversarial training probably changes it, but does not test this hypothesis.
- Why unresolved: The paper does not experiment with adversarially trained models or other robustness methods to measure their effect on χ.
- What evidence would resolve it: Experiments comparing χ values between standard and adversarially trained models to determine if and how training methods affect attack resistance.

### Open Question 3
- Question: What is the maximum achievable attack multiplier κ for extremely large models with activation dimensions much larger than 2560?
- Basis in paper: [explicit] The paper observes κ scaling linearly with d up to 2560 dimensions but does not explore models with significantly larger activation spaces.
- Why unresolved: The largest model studied has d=2560, leaving open whether the linear scaling continues indefinitely or saturates at some point.
- What evidence would resolve it: Experiments measuring κ for models with activation dimensions significantly larger than 2560 (e.g., frontier models with d > 8192) to test if the linear relationship holds.

## Limitations
- The study focuses only on decoder-only transformer models from the Pythia family, limiting generalizability to other architectures
- The attacks assume full access to model activations and multiple optimization steps, which may not be feasible in deployed systems
- The effectiveness may vary significantly depending on semantic content and relationship between context and target sequences

## Confidence
- High Confidence: The existence of activation-based adversarial attacks and their basic mechanism
- Medium Confidence: The scaling law tmax = κa with linear κ across all model sizes
- Low Confidence: The claim that activation attacks are "significantly stronger" than token substitution attacks

## Next Checks
1. Apply the same attack methodology to BERT, GPT-2, and LLaMA models to verify if the scaling laws and attack resistance values hold across different transformer architectures
2. Implement activation quantization (8-bit, 4-bit, 2-bit) and measure how it affects attack success rates and the scaling law parameters
3. Design experiments testing coordinated attacks where multiple perturbation vectors are optimized simultaneously with shared or competing objectives