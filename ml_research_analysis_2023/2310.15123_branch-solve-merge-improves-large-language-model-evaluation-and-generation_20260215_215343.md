---
ver: rpa2
title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
arxiv_id: '2310.15123'
source_url: https://arxiv.org/abs/2310.15123
tags:
- arxiv
- evaluation
- branch
- response
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BRANCH-SOLVE-MERGE (BSM) is a Large Language Model program that
  improves the performance of LLMs on multi-faceted natural language tasks by decomposing
  problems into parallel sub-tasks. The method consists of three modules: a branch
  module that generates a solution plan by decomposing the task into multiple parallel
  sub-tasks, a solve module that independently solves each sub-task, and a merge module
  that fuses the solutions to generate the overall solution.'
---

# Branch-Solve-Merge Improves Large Language Model Evaluation and Generation

## Quick Facts
- arXiv ID: 2310.15123
- Source URL: https://arxiv.org/abs/2310.15123
- Reference count: 26
- One-line primary result: BSM framework improves LLM evaluation accuracy and reduces biases while enhancing constrained story generation

## Executive Summary
Branch-Solve-Merge (BSM) is a novel framework that enhances LLM performance on multi-faceted tasks through systematic decomposition into parallel sub-tasks. The framework consists of three modules: branch (decomposes tasks into sub-tasks), solve (independently addresses each sub-task), and merge (combines sub-solutions). Applied to LLM evaluation and constrained story generation, BSM demonstrates significant improvements in evaluation accuracy, bias reduction, and constraint satisfaction while maintaining or exceeding baseline performance.

## Method Summary
BSM implements a three-stage process for complex NLP tasks. The branch module analyzes the problem and generates a decomposition plan, either as evaluation criteria for response assessment or concept subsets for story generation. The solve module independently processes each branch using the LLM, either scoring responses against specific criteria or generating content for concept subsets. The merge module aggregates or combines these sub-solutions, either by scoring aggregation for evaluation or story fusion for generation. For evaluation tasks, BSM runs twice with swapped response order to detect and mitigate position bias, accepting verdicts only when both runs agree.

## Key Results
- BSM improves human-LLM agreement by up to 26% on MT-Bench evaluation
- Reduces position and length biases by up to 50% compared to zero-shot baselines
- Enhances constraint satisfaction by 12% while improving story coherence in constrained generation tasks
- Enables LLaMA-2-chat to match or outperform GPT-4 on most evaluation domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BSM improves LLM-human agreement by decomposing multi-faceted evaluation tasks into parallel sub-tasks, each handled independently.
- Mechanism: The branch module generates a task-specific evaluation plan (criteria like Relevance, Clarity, Accuracy), the solve module evaluates responses on each criterion separately, and the merge module aggregates these criterion-wise scores. This decomposition reduces the cognitive load on the LLM by breaking complex holistic judgments into simpler, focused assessments.
- Core assumption: Parallel decomposition of evaluation criteria reduces interference and bias compared to holistic evaluation, and the LLM can generate meaningful sub-tasks for the given problem.
- Evidence anchors:
  - [abstract] "The 'branch' module generates a solution plan by decomposing the task into multiple parallel sub-tasks, where each sub-task is represented by a unique branch, representing different components required to solve the overall problem."
  - [section 3.2] "Given an open-ended question...the task requires producing a preference judgement...Evaluating LLM responses is challenging for multiple reasons...LLM evaluators are prone to biases."
  - [corpus] Weak: no direct mention of BSM decomposition benefits in neighbor papers; most are about reinforcement learning or biological sequences.
- Break condition: If the LLM cannot generate meaningful or relevant evaluation criteria, the decomposition fails and the aggregated judgment may be inconsistent or meaningless.

### Mechanism 2
- Claim: BSM reduces position and length biases by executing independent runs with swapped response order and using consistent aggregation.
- Mechanism: The solve module is not symmetric—order matters due to auto-regressive nature. BSM runs twice with swapped input order; only if both runs agree on the verdict is a decision made, otherwise it's a tie. This forces consistency and penalizes biased responses.
- Core assumption: Position bias manifests as inconsistent judgments based on encoding order, and symmetric consistency checks can detect and mitigate this.
- Evidence anchors:
  - [abstract] "BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%..."
  - [section 3.2] "To account for position bias, the program executes two independent runs of BSM by swapping the encoding order of the responses in the 'solve' module."
  - [corpus] Weak: neighbor papers do not discuss bias mitigation in LLM evaluation.
- Break condition: If the LLM consistently produces the same verdict regardless of order (no bias), the double-run check adds unnecessary computation without benefit.

### Mechanism 3
- Claim: BSM improves coherence and constraint satisfaction in constrained generation by dividing concepts into subsets and generating sub-stories before merging.
- Mechanism: The branch module splits the concept set into two groups and proposes a story topic. Each solve module generates a coherent sub-story for one group. The merge module fuses the two sub-stories into one, ensuring all concepts are included while maintaining narrative flow.
- Core assumption: Generating stories from smaller concept sets is easier and more coherent, and merging two topically aligned sub-stories yields a coherent whole.
- Evidence anchors:
  - [abstract] "On a constraint story generation task, BSM improves the coherence of the stories while also improving constraint satisfaction by 12%."
  - [section 3.3] "The branch module branch(l) → (l1, l2, t) proposes a story generation plan, consisting of (1) two subsets of concepts l1 and l2 and (2) a story topic t...The merge module merge(y1, y2) → y conditions on two intermediate stories...and fuses them together to generate the final story y."
  - [corpus] Weak: neighbor papers focus on biological sequences or fairness audits, not constrained generation.
- Break condition: If concept splitting leads to incoherent sub-stories or if merging produces repetition or logical gaps, coherence and constraint satisfaction degrade.

## Foundational Learning

- Concept: Task decomposition and parallel sub-problem solving
  - Why needed here: Multi-faceted tasks like evaluation and constrained generation involve many criteria or constraints that are hard to handle holistically; decomposition simplifies each sub-task and reduces interference.
  - Quick check question: What is the benefit of generating multiple evaluation criteria in parallel rather than evaluating all criteria together?

- Concept: Bias mitigation through consistency checks
  - Why needed here: LLMs are prone to position and length biases; enforcing consistent judgments across input orders can detect and reduce such biases.
  - Quick check question: How does running the solve module twice with swapped input order help reduce position bias?

- Concept: Story coherence and constraint satisfaction
  - Why needed here: Constrained generation tasks require including many concepts without sacrificing narrative flow; splitting concepts and merging sub-stories balances both goals.
  - Quick check question: Why might splitting a concept set into two smaller groups improve the coherence of generated stories?

## Architecture Onboarding

- Component map: Branch -> Solve -> Merge
- Critical path:
  1. Branch: Plan generation (single forward pass)
  2. Solve: Per-branch evaluation/generation (one pass per branch)
  3. Merge: Aggregation/fusion (single pass)
  4. For evaluation: Run twice with swapped order → compare verdicts

- Design tradeoffs:
  - Branching factor vs. computational cost: More branches = more LLM calls but finer-grained decomposition
  - Greedy decoding vs. sampling: Greedy is deterministic and faster; sampling may improve diversity but adds variance
  - Non-neural vs. neural merge: Non-neural (summing scores) is simpler and works well; neural merge could learn better aggregation but needs training

- Failure signatures:
  - Low agreement improvement: Branch module generates irrelevant or redundant criteria
  - High position bias persists: Solve module's asymmetry not properly handled or LLM is too biased
  - Poor constraint satisfaction: Branch module splits concepts poorly, or merge loses concepts

- First 3 experiments:
  1. Vary branching factor (2-5) on a small subset of evaluation data and measure agreement and bias reduction
  2. Compare greedy vs. sampled solve outputs for each branch on constrained generation
  3. Swap merge strategy (sum vs. neural) on evaluation task and measure effect on agreement and consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BSM scale with increasingly larger branching factors beyond 5, and what is the optimal branching factor for different types of tasks?
- Basis in paper: Inferred from the analysis of branching factor effects on 100 samples of the 'writing' category.
- Why unresolved: The paper only explores branching factors up to 5 and suggests that the optimal branching factor may depend on the specific question, but does not provide a comprehensive analysis for larger branching factors or different task types.
- What evidence would resolve it: Conducting experiments with branching factors larger than 5 across various task types and analyzing the performance metrics (e.g., LLM-human agreement, position bias) to determine the optimal branching factor for each task type.

### Open Question 2
- Question: Can BSM be effectively combined with other advanced prompting methods, such as tree-of-thought or graph-of-thought, to further improve performance on complex reasoning tasks?
- Basis in paper: Inferred from the discussion of BSM as an instance of Graph-of-Thoughts (GoT) prompting and its application to multi-faceted natural language tasks.
- Why unresolved: The paper focuses on the application of BSM to specific tasks (LLM evaluation and constrained text generation) and does not explore its potential combination with other prompting methods for complex reasoning tasks.
- What evidence would resolve it: Implementing BSM in conjunction with tree-of-thought or graph-of-thought prompting methods and evaluating their performance on complex reasoning tasks, comparing the results with BSM alone and other baseline methods.

### Open Question 3
- Question: How does BSM perform when applied to tasks beyond LLM evaluation and constrained text generation, such as open-ended question answering or multi-modal tasks involving images and text?
- Basis in paper: Inferred from the general description of BSM as a framework for planning and task decomposition for addressing challenging multi-faceted natural language generation and evaluation tasks.
- Why unresolved: The paper primarily focuses on the application of BSM to LLM evaluation and constrained text generation, without exploring its potential for other types of tasks or multi-modal scenarios.
- What evidence would resolve it: Applying BSM to a diverse set of tasks, including open-ended question answering and multi-modal tasks, and evaluating its performance using appropriate metrics for each task type. Comparing the results with baseline methods and analyzing the strengths and limitations of BSM in these scenarios.

## Limitations

- Performance gains are demonstrated primarily on two specific tasks (LLM evaluation and constrained generation) without extensive testing on other multi-faceted tasks
- The framework's computational overhead increases linearly with branching factor, potentially limiting scalability for very complex tasks
- Evaluation relies heavily on GPT-4 as an oracle judge, raising questions about the true ground truth for human preferences

## Confidence

- LLM evaluation improvements: Medium - Results are strong but rely on a single benchmark and GPT-4 as an oracle judge
- Bias reduction claims: Medium - Position bias mitigation is demonstrated but the mechanism's effectiveness across diverse scenarios is unclear
- Story generation enhancements: Low-Medium - Improvements are shown but the dataset is small and evaluation relies heavily on GPT-4 judgments

## Next Checks

1. Test BSM on additional multi-faceted tasks beyond evaluation and constrained generation to assess generalizability
2. Conduct ablation studies removing each module (branch, solve, merge) to quantify individual contributions
3. Implement and evaluate BSM across multiple LLM families (not just LLaMA-2 and Vicuna) to verify consistent performance gains