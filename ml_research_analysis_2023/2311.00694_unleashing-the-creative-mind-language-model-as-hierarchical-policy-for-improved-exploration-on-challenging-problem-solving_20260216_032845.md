---
ver: rpa2
title: 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved
  Exploration on Challenging Problem Solving'
arxiv_id: '2311.00694'
source_url: https://arxiv.org/abs/2311.00694
tags:
- reasoning
- answer
- question
- policy
- hints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a hierarchical language model policy for
  solving complex mathematical problems. The approach frames an LLM as a two-level
  decision-making process: a high-level leader proposes diverse problem-solving tactics
  and hints, and a low-level follower executes detailed reasoning chains based on
  these hints.'
---

# Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving

## Quick Facts
- arXiv ID: 2311.00694
- Source URL: https://arxiv.org/abs/2311.00694
- Reference count: 30
- This paper introduces a hierarchical language model policy for solving complex mathematical problems, improving final answer accuracy by 5-7% compared to standard chain-of-thought sampling.

## Executive Summary
This paper introduces a hierarchical language model policy for solving complex mathematical problems. The approach frames an LLM as a two-level decision-making process: a high-level leader proposes diverse problem-solving tactics and hints, and a low-level follower executes detailed reasoning chains based on these hints. The leader can either generate concise techniques or retrieve relevant problems from the training set. A tournament-based selection method efficiently chooses the best reasoning chains from multiple groups. Experiments on the MATH dataset show that this hierarchical exploration strategy improves the discovery and visibility of correct solutions.

## Method Summary
The hierarchical policy framework consists of a high-level leader that generates diverse problem-solving tactics and hints, and a low-level follower that executes detailed reasoning chains based on these hints. The leader can either generate techniques using GPT-4 prompting or retrieve similar problems using SentenceBERT. For each problem, n diverse hints are generated, and for each hint, m reasoning chains are produced. Grouped-majority voting determines the majority answer within each group, followed by tournament selection where chains reaching the group-majority answer are compared using GPT-4 to select the final answer.

## Key Results
- Final answer accuracy improved by 5-7% compared to standard chain-of-thought sampling
- Hierarchical approach particularly effective when using GPT-4 as the follower model
- Grouped-Majority Recall metric demonstrates improved visibility of correct answers through hierarchical exploration
- Tournament-based selection efficiently identifies best reasoning chains with only n×k model calls

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The high-level leader generates diverse, relevant tactics that guide the low-level follower toward correct solutions.
- Mechanism: The leader connects the target problem to its knowledge base, producing tactics like "Angle Bisector Theorem" or "Vieta's Formula" that are concise and easily interpretable. These tactics steer the follower's exploration in promising directions.
- Core assumption: The leader's knowledge base contains relevant techniques that, when presented as hints, can effectively guide the follower to generate correct reasoning chains.
- Evidence anchors:
  - [abstract] "This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction."
  - [section] "We would like to encourage the leader πhigh to effectively establish the connection between Q and relevant language model knowledge, from which it proposes high-level hints and directions that holds significant potential for reaching the goal."
  - [corpus] Weak - related work mentions hierarchical reasoning but lacks direct experimental evidence on hint generation quality.
- Break condition: If the leader generates irrelevant or misleading tactics, the follower's exploration becomes ineffective, and correct solutions become less discoverable.

### Mechanism 2
- Claim: The hierarchical structure improves exploration by partitioning the solution space into tactic-based groups, increasing the visibility of correct answers.
- Mechanism: By sampling from multiple high-level tactics (h1, h2, ..., hn), the approach ensures exploration of different reasoning paths. Even rare correct tactics can dominate their group, making correct answers more visible during majority voting.
- Core assumption: Correct solutions are more likely to emerge when multiple high-level strategies are explored, even if some are infrequently sampled.
- Evidence anchors:
  - [section] "Our strategy samples all the different hints returned by πhigh with equal probabilities... Our strategy, therefore, aligns with the spirit of common practice in reinforcement learning to encourage the exploration behavior by making the density of actions more uniform."
  - [abstract] "Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset."
  - [corpus] Moderate - hierarchical policies in RL show improved exploration, but specific evidence for LLMs is limited.
- Break condition: If the tactic groups are too small or the follower cannot effectively use the hints, the partitioning provides little benefit over flat exploration.

### Mechanism 3
- Claim: The tournament-based selection process efficiently identifies the best reasoning chains from multiple groups without requiring excessive computation.
- Mechanism: Within each group, majority voting determines a group-majority answer. Then, reasoning chains from each group that reach this answer are compared in a tournament, using GPT-4 to select the best one. This requires only n×k model calls instead of evaluating all chains.
- Core assumption: The group-majority answer is a good proxy for quality, and GPT-4 can reliably compare reasoning chains to select the best.
- Evidence anchors:
  - [section] "Within each group of reasoning chains Ti, we conduct majority voting to establish the group-majority answer Ai. Then, for every group, we randomly select a single reasoning chain from those that reach the group-majority answer..."
  - [abstract] "Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer."
  - [corpus] Strong - tournament selection is a well-established method in RL and optimization.
- Break condition: If GPT-4 cannot reliably distinguish good from bad reasoning chains, or if the group-majority answer is frequently incorrect, the selection process fails.

## Foundational Learning

- Concept: Metacognition in problem-solving
  - Why needed here: The paper frames LLM reasoning as hierarchical cognition, where high-level tactics guide low-level execution. Understanding metacognition explains why this separation matters.
  - Quick check question: Why might a human solver first think of "use Vieta's formulas" before working out the detailed algebra?
- Concept: Markov Decision Process (MDP) and marginal probabilities
  - Why needed here: The paper uses MDP concepts to explain how the hierarchical policy improves exploration by sampling tactics with equal probability rather than by their marginal likelihood.
  - Quick check question: If Pr(h|Q) is the probability of tactic h given question Q, why does sampling Unif(h ∈ H) · Pr(H|Q) improve exploration compared to sampling by Pr(h|Q)?
- Concept: Chain-of-Thought (CoT) prompting and majority voting
  - Why needed here: The paper compares its hierarchical approach to CoT sampling + majority voting, so understanding these baselines is essential.
  - Quick check question: How does majority voting across 64 CoT samples differ from majority voting within 4 groups of 16 samples each?

## Architecture Onboarding

- Component map:
  - High-level leader (πhigh) → Low-level follower (πlow) → Grouped-majority voting → Tournament selection → Final answer
- Critical path: Leader → Follower (n groups × m chains) → Grouped-majority → Tournament → Final answer
- Design tradeoffs:
  - More hints (larger n) increases diversity but also computational cost
  - Stronger follower (GPT-4 vs GPT-3.5) better utilizes hints but is more expensive
  - Retrieval-based hints provide concrete examples but may be less flexible than generated tactics
- Failure signatures:
  - Low grouped-majority recall despite high raw recall indicates hints are not effectively guiding exploration
  - Tournament selection fails when GPT-4 cannot distinguish reasoning quality
  - Follower ignores hints entirely, treating them as irrelevant context
- First 3 experiments:
  1. Compare grouped-majority recall between hierarchical approach and flat CoT sampling with same total number of chains
  2. Ablation study: Vary n (number of hints) and m (chains per hint) to find optimal configuration
  3. Test different follower models (GPT-3.5 vs GPT-4) to quantify hint utilization effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of high-level hints (n) and reasoning chains per group (m) for maximizing performance across different problem types?
- Basis in paper: [explicit] The paper tests n=4 and m=16, but doesn't explore a comprehensive parameter sweep
- Why unresolved: The authors only experiment with one configuration of n=4 hints and m=16 reasoning chains per group, leaving the optimal parameter values unknown
- What evidence would resolve it: Systematic experiments varying n and m across different MATH problem categories to identify optimal configurations

### Open Question 2
- Question: How does the quality of high-level hints generated by the leader policy impact the follower's performance, and can this relationship be quantified?
- Basis in paper: [explicit] The paper mentions that 94.3% of generated hints match ground-truth hints, but doesn't measure how hint quality affects follower performance
- Why unresolved: The paper establishes that most hints are relevant but doesn't empirically measure how hint quality variations affect the follower's success rate
- What evidence would resolve it: Experiments correlating hint quality scores with follower performance metrics across multiple problems

### Open Question 3
- Question: Can the tournament-based selection method be improved by incorporating additional criteria beyond pairwise comparison?
- Basis in paper: [inferred] The tournament method only considers pairwise comparisons, suggesting potential for enhancement
- Why unresolved: The paper presents a basic tournament selection but doesn't explore alternative selection strategies or additional evaluation criteria
- What evidence would resolve it: Comparative studies of tournament selection against other methods (e.g., diversity-aware selection, confidence scoring) on final answer accuracy

## Limitations

- The method relies heavily on GPT-4's capabilities for both hint generation and evaluation, raising questions about scalability and cost-effectiveness
- Experimental validation is limited to the MATH dataset Level-5 subset, constraining generalizability to other problem domains
- The paper doesn't thoroughly address cases where high-level hints might mislead the follower, potentially creating false confidence in incorrect solutions

## Confidence

- Medium confidence in final answer accuracy improvement claims (requires replication with consistent base models)
- Low confidence in scalability and cost-effectiveness claims (no ablation on follower model strength)
- Medium confidence in the hierarchical exploration mechanism (strong theoretical grounding but limited empirical validation)

## Next Checks

1. Replicate experiments using the same base model (e.g., GPT-3.5) for both hierarchical and flat approaches to isolate the effect of the hierarchical structure
2. Conduct a cost-benefit analysis comparing accuracy gains against the increased computational expense of generating multiple hints and using GPT-4 for evaluation
3. Test the approach on a broader range of mathematical problem difficulty levels to assess generalizability beyond the hardest MATH problems