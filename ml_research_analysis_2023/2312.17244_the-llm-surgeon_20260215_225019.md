---
ver: rpa2
title: The LLM Surgeon
arxiv_id: '2312.17244'
source_url: https://arxiv.org/abs/2312.17244
tags:
- pruning
- structured
- weights
- performance
- surgeon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents LLM Surgeon, a data-driven method for compressing
  large language models (LLMs) by pruning weights, rows, and columns. The core idea
  is to use Kronecker-factored curvature approximations of the loss landscape to compute
  both which structures can be removed and how to update the remaining weights.
---

# The LLM Surgeon

## Quick Facts
- arXiv ID: 2312.17244
- Source URL: https://arxiv.org/abs/2312.17244
- Authors: 
- Reference count: 40
- Key outcome: Achieves state-of-the-art results, pruning OPT and Llama-v2 models by 20%-30% with minimal performance degradation.

## Executive Summary
The LLM Surgeon presents a data-driven method for compressing large language models (LLMs) through pruning of weights, rows, and columns. The core innovation uses Kronecker-factored curvature approximations of the loss landscape to compute both which structures can be removed and how to update the remaining weights. This approach enables unstructured, semi-structured, and structured pruning within a single framework, improving upon prior work by capturing more correlations between weights and using multi-shot pruning with global thresholding. Experimentally, the method achieves state-of-the-art results, pruning OPT and Llama-v2 models by 20%-30% with minimal performance degradation.

## Method Summary
LLM Surgeon uses Kronecker-factored curvature approximations of the loss landscape to determine which weights, rows, or columns to prune from LLMs. The method computes pruning costs using empirical Fisher matrices approximated through Kronecker products of activation and gradient factors. Unlike prior work that treats weight updates independently, LLM Surgeon considers correlations between weights to derive joint weight updates for pruning multiple weights or structured sets simultaneously. The algorithm uses global thresholding to dynamically allocate sparsity across layers and employs multi-shot pruning with curvature re-estimation to improve final compression accuracy. The approach supports unstructured, semi-structured, and structured pruning within a unified framework.

## Key Results
- Achieves state-of-the-art results pruning OPT and Llama-v2 models by 20%-30% with minimal performance degradation
- Provides first practically usable results for structured pruning of LLMs
- Outperforms prior methods on both unstructured and structured pruning tasks across multiple model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Kronecker-factored curvature approximations captures correlations between weights more accurately than diagonal approximations.
- Mechanism: The Fisher matrix is approximated using Kronecker products of activation and gradient factors (Gl ⊗ Al), enabling inversion of much smaller matrices while retaining off-diagonal correlations. This improves weight update accuracy during pruning.
- Core assumption: The independence assumption between activations and gradients (IAD) holds sufficiently well for practical curvature estimation.
- Evidence anchors:
  - [abstract] "We further expand upon the work by deriving OBS-like weight pruning costs and updates for structured pruning of multiple rows and columns, and provide a general framework that also incorporates semi-structured and unstructured pruning. Instead of treating individual weight updates independently, we strive to consider as many correlations between weights as practically possible and derive joint weight updates for pruning multiple weights (or multiple sets of structured weights) at once."
  - [section 3.4] "Unlike most prior work, we present correlated weight updates by taking into account off-diagonal elements of the Fisher approximation in section 3.4."
- Break condition: If the Kronecker product approximation fails to capture significant curvature structure (e.g., strong cross-layer dependencies), the method degrades toward diagonal approximations with lower accuracy.

### Mechanism 2
- Claim: Global thresholding enables dynamic allocation of sparsity across layers, improving compression efficiency.
- Mechanism: Instead of fixing sparsity per layer, a global threshold τ is applied to pruning costs (Lr, Lc, Lk), allowing more sensitive layers to retain more weights while aggressively pruning robust layers.
- Core assumption: Loss sensitivity to weight removal varies sufficiently across layers to justify non-uniform sparsity.
- Evidence anchors:
  - [section 3.3] "Unlike prior works that compress layer-by-layer (Frantar & Alistarh, 2023), we use a global threshold τ enabling a dynamic allocation of sparsity levels across layers, pruning most where it hurts the least."
  - [section 4.5] "In Fig. 4.5, we compare total allocated sparsity levels per layer depth and per layer type after compressing a pretrained OPT-125m model. We find that the LLM Surgeon prunes relatively more in the first layer and less in middle layers."
- Break condition: If all layers have similar loss sensitivity to weight removal, global thresholding offers no advantage over uniform per-layer sparsity.

### Mechanism 3
- Claim: Multi-shot pruning with curvature re-estimation improves final compression accuracy compared to single-shot pruning.
- Mechanism: After each pruning shot, remaining weights are updated and curvature is re-estimated, allowing smaller parameter space jumps and better local quadratic approximations.
- Core assumption: The quadratic surrogate loss landscape remains locally valid between pruning shots.
- Evidence anchors:
  - [section 3.5] "To improve the performance-to-sparsity ratio, we propose pruning in multiple shots... We theoretically justify this multi-shot approach by noting that the surrogate loss landscape q relies on a Taylor expansion (eq. (3)) that only holds locally and thus becomes unreliable for larger jumps ∆θ in parameter space."
  - [section 4] "We use 40 shots at α=0.5 sparsity and report intermediate compression rates, effectively using T =8 shots for α=0.9, T =16 for α=0.8, T =24 for α=0.7, and T =32 for α=0.6."
- Break condition: If curvature changes dramatically between shots, the local approximation becomes invalid and multi-shot pruning degrades to noise.

## Foundational Learning

- Concept: Kronecker product algebra and its use in matrix approximations.
  - Why needed here: The core curvature approximation relies on factoring large Fisher blocks into Kronecker products for efficient computation.
  - Quick check question: Given G ∈ Rr×r and A ∈ Rc×c, what is the dimension of G ⊗ A and how many parameters does it represent?

- Concept: Second-order Taylor expansion and its use in loss approximation.
  - Why needed here: Pruning decisions are based on a quadratic expansion of the loss around pretrained weights.
  - Quick check question: Write the second-order Taylor expansion of the log-likelihood loss around pretrained weights θ*.

- Concept: Fisher information matrix and its empirical approximation.
  - Why needed here: The Hessian of the loss for classification is the Fisher matrix, which is approximated empirically using forward activations and backward gradients.
  - Quick check question: How is the empirical Fisher matrix constructed from forward passes and backward gradients?

## Architecture Onboarding

- Component map: Data pipeline -> Curvature estimator -> Pruning selector -> Weight updater -> Repeat (multi-shot)
- Critical path: Data → Curvature → Costs → Threshold → Removal → Update → Repeat (multi-shot)
- Design tradeoffs:
  - More Kronecker factors (RK>1) → better curvature approximation but higher memory/compute
  - More pruning shots → better accuracy but longer runtime
  - Correlation level (full vs within-row/col) → better updates vs computational cost
- Failure signatures:
  - High perplexity on test data after pruning → curvature approximation too crude or insufficient shots
  - Memory errors during curvature computation → too many Kronecker factors or insufficient GPU memory
  - Very slow pruning → excessive correlation level or too many shots for target sparsity
- First 3 experiments:
  1. Implement unstructured pruning on OPT-125m with RK=1, T=5 shots, verify perplexity degradation
  2. Add global thresholding and compare layer-wise vs global sparsity allocation
  3. Extend to structured pruning on OPT-1.3b, measure row/column removal efficiency

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several unresolved issues emerge from the analysis:

- How does the number of Kronecker factors (RK) in the curvature approximation impact the quality of pruning results for LLMs?
- How does the choice of sparsity schedule (e.g., linear, exponential) affect the final performance of LLM Surgeon?
- How does the choice of damping parameter (λG, λA) affect the quality of the curvature approximation and the final pruning results?

## Limitations

- The method shows diminishing returns beyond 30% sparsity, with performance degradation becoming more pronounced
- The empirical Fisher approximation requires substantial calibration data (128 sequences of 2048 tokens), which may be problematic for domains with limited data
- Scalability claims to production-scale models (e.g., 175B parameters) are based on extrapolation rather than direct testing

## Confidence

- **High confidence**: The core mechanism of using Kronecker-factored curvature for correlated weight updates is theoretically sound and well-supported by empirical evidence
- **Medium confidence**: The multi-shot pruning strategy and global thresholding approach are validated within tested parameter ranges but may have undiscovered failure modes at extreme settings
- **Low confidence**: The scalability claims to production-scale models are based on extrapolation rather than direct testing, and memory constraints during curvature computation are not fully characterized

## Next Checks

1. Test LLM Surgeon on at least two additional LLM architectures (e.g., GPT-2, Bloom) to verify that the Kronecker-factored approximation performs consistently across different activation patterns and layer organizations.

2. Systematically evaluate performance degradation beyond 40% sparsity with varying numbers of pruning shots to identify the breaking point where multi-shot pruning fails to compensate for curvature approximation errors.

3. Measure performance sensitivity to calibration dataset size by varying from 32 to 512 sequences while keeping other hyperparameters constant, to establish minimum data requirements for reliable curvature estimation.