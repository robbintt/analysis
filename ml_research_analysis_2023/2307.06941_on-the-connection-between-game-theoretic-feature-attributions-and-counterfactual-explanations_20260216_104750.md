---
ver: rpa2
title: On the Connection between Game-Theoretic Feature Attributions and Counterfactual
  Explanations
arxiv_id: '2307.06941'
source_url: https://arxiv.org/abs/2307.06941
tags:
- counterfactual
- explanations
- feature
- counterfactuals
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a theoretical connection between Shapley\
  \ value-based feature attributions (e.g., SHAP) and counterfactual explanations.\
  \ It proves that under specific conditions\u2014using the binary decision function,\
  \ normalizing frequency-based importance, and enforcing maximal sparsity on counterfactuals\u2014\
  the two methods yield equivalent explanations."
---

# On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations

## Quick Facts
- arXiv ID: 2307.06941
- Source URL: https://arxiv.org/abs/2307.06941
- Reference count: 40
- Key outcome: Proves theoretical equivalence between Shapley value-based feature attributions and counterfactual explanations under specific conditions, validated empirically on three datasets

## Executive Summary
This paper establishes a theoretical connection between Shapley value-based feature attributions (e.g., SHAP) and counterfactual explanations. The authors prove that when using the binary decision function, normalizing frequency-based importance, and enforcing maximal sparsity on counterfactuals, these two seemingly different approaches yield equivalent explanations. Experiments on three datasets demonstrate that normalization has minimal impact while maximal sparsity significantly alters explanations. The findings highlight limitations of naive counterfactual frequency-based importance in accurately capturing feature importance.

## Method Summary
The paper modifies both SHAP and counterfactual frequency-based importance methods to prove their equivalence. SHAP is modified to use the binary decision function instead of model output, while counterfactual explanations are normalized and made maximally sparse using a depth-first search algorithm. The theoretical results are validated on three datasets (HELOC, Lending Club, Adult) with pre-trained XGBoost models. Counterfactuals are generated using K-NN with Manhattan distance over quantile space. The study evaluates explanation differences using metrics like Kendall-Tau correlation, necessity, sufficiency, counterfactual-ability, and plausibility.

## Key Results
- Under specific conditions (binary decision function, normalization, maximal sparsity), Shapley values and counterfactual explanations yield identical feature attributions
- Normalization of counterfactual frequency importance has minimal impact on explanations
- Enforcing maximal sparsity on counterfactuals significantly alters the resulting explanations
- Theoretical findings are empirically supported across three datasets using multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
The equivalence between Shapley