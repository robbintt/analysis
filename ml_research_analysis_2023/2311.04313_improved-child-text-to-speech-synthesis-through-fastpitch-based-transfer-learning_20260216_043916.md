---
ver: rpa2
title: Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning
arxiv_id: '2311.04313'
source_url: https://arxiv.org/abs/2311.04313
tags:
- speech
- child
- dataset
- synthetic
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating synthetic child
  speech, which is difficult due to the scarcity of child voice datasets and the unique
  vocal characteristics of children. The authors propose a transfer learning approach
  that leverages Fastpitch, a state-of-the-art TTS model, to synthesize child speech.
---

# Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning

## Quick Facts
- arXiv ID: 2311.04313
- Source URL: https://arxiv.org/abs/2311.04313
- Authors: 
- Reference count: 40
- This paper proposes a transfer learning approach using Fastpitch to synthesize child speech by pretraining on adult speech data (LibriTTS) and finetuning on child speech (MyST).

## Executive Summary
This paper addresses the challenge of synthesizing high-quality child speech, which is difficult due to the scarcity of child voice datasets and the unique acoustic characteristics of children's voices. The authors propose a transfer learning approach using Fastpitch, a state-of-the-art TTS model, where the model is first pretrained on a large adult speech dataset (LibriTTS, 585 hours) and then finetuned on a cleaned child speech dataset (MyST, 55 hours). Objective evaluations using MOSNet, wav2vec2 ASR, and a speaker encoder demonstrate that the synthetic child speech generated by this approach has high correlation with real child speech in terms of naturalness, intelligibility, and speaker similarity. The method achieves a WER of 17.61 for synthetic child speech compared to 15.27 for real child speech, with an average cosine similarity of 77% between synthetic and real child voices.

## Method Summary
The approach uses Fastpitch, a sequence-to-sequence TTS model, pretrained on the LibriTTS adult speech dataset for approximately 250k iterations. The pretrained model is then finetuned on the MyST child speech dataset for an additional 270k iterations. WaveGlow, a neural vocoder trained on LibriTTS, is used to convert the generated mel-spectrograms into waveforms. The model is conditioned on speaker identity using global speaker embeddings to enable synthesis of distinct child voices. Two synthetic child speech datasets are created using Harvard Sentences (CS_HS) and LJ Speech sentences (CS_LJ) for evaluation purposes.

## Key Results
- WER of 17.61 for synthetic child speech versus 15.27 for real child speech, indicating high intelligibility
- Average cosine similarity of 77% between synthetic and real child voices, demonstrating good speaker similarity
- Significant correlation between synthetic and real child speech quality according to MOSNet naturalness scores

## Why This Works (Mechanism)

### Mechanism 1
Transfer learning from adult to child speech improves naturalness and intelligibility in synthetic child speech. Fastpitch is first pretrained on LibriTTS (adult speech, 585 hours) to learn robust linguistic and acoustic features, then finetuned on MyST (child speech, 55 hours) to adapt to child-specific pitch, duration, and prosody patterns. The core assumption is that the phonetic and prosodic characteristics of adult speech are sufficiently transferable to child speech after finetuning, reducing the need for large child speech datasets.

### Mechanism 2
Using pretrained MOSNet and wav2vec2 ASR enables objective correlation between synthetic and real child speech without expensive subjective testing. MOSNet, pretrained on adult speech, evaluates naturalness of synthetic child speech. wav2vec2, also pretrained on Librispeech, measures intelligibility via WER comparison. The core assumption is that models pretrained on adult speech generalize sufficiently to child speech for meaningful objective evaluation.

### Mechanism 3
Multispeaker conditioning with speaker embeddings allows synthesis of distinct child voices while preserving speaker identity. Fastpitch uses global speaker embeddings to condition the model on individual child speakers, enabling diverse and consistent voice generation. The core assumption is that speaker embeddings capture enough speaker-specific information to produce distinguishable synthetic voices.

## Foundational Learning

- **Concept:** Phonetic and prosodic differences between adult and child speech
  - **Why needed here:** Understanding why child speech (higher F0, longer phoneme durations, variable prosody) requires special modeling helps justify transfer learning and speaker conditioning.
  - **Quick check question:** What is the typical fundamental frequency range for child speech compared to adult speech?

- **Concept:** Transfer learning in neural TTS
  - **Why needed here:** Knowing how pretraining on large adult datasets can bootstrap child speech synthesis is key to grasping the model design.
  - **Quick check question:** What is the main benefit of pretraining on a large dataset before finetuning on a small child dataset?

- **Concept:** Evaluation metrics for speech quality
  - **Why needed here:** Understanding MOSNet naturalness scores and WER intelligibility scores is essential for interpreting results without human listening tests.
  - **Quick check question:** What does a lower WER indicate about ASR performance on synthetic speech?

## Architecture Onboarding

- **Component map:** Fastpitch (acoustic model) -> WaveGlow (vocoder) -> MOSNet (naturalness) / wav2vec2 (intelligibility) / Resemblyzer (speaker similarity)
- **Critical path:** Pretrain Fastpitch on LibriTTS → Finetune on MyST → Generate mel-spectrograms → WaveGlow synthesizes audio → Evaluate with MOSNet, wav2vec2, Resemblyzer
- **Design tradeoffs:**
  - Using LibriTTS for pretraining reduces data needs but may introduce adult speech artifacts
  - WaveGlow trained on adult speech may not perfectly match child speech acoustics
  - Objective evaluation avoids subjective testing but may miss perceptual nuances
- **Failure signatures:**
  - High MOSNet scores but poor human ratings indicate model overfits objective metrics
  - WER close to real child speech but unintelligible to humans suggests ASR bias
  - Low speaker similarity scores mean embeddings fail to capture child voice identity
- **First 3 experiments:**
  1. Train Fastpitch on LibriTTS only, generate speech, evaluate naturalness with MOSNet
  2. Finetune on MyST for 50k, 100k, 150k steps, compare MOSNet scores to baseline
  3. Generate synthetic child datasets (CS_HS, CS_LJ), evaluate WER and speaker similarity

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed Fastpitch-based transfer learning approach compare to other TTS models like Tacotron 2 and FastSpeech in terms of naturalness and intelligibility of synthetic child speech? The paper mentions a comparative analysis with Tacotron 2 but does not provide detailed comparisons with other TTS models, leaving a gap in understanding how it performs relative to other TTS models.

### Open Question 2
Can the proposed approach be extended to synthesize child speech in languages other than English, and how would the performance be affected? The paper mentions that Fastpitch has multilingual support, but does not explore its application to child speech synthesis in other languages, limiting understanding of the model's cross-lingual capabilities.

### Open Question 3
How does the proposed approach handle the synthesis of child speech with different accents and dialects, and can it be further optimized to improve the naturalness and intelligibility of such speech? The paper mentions that the model can be optimized to accommodate individual characteristics like regional accents, but does not provide specific details or results, leaving uncertainty about its effectiveness in handling accent and dialect variations.

## Limitations
- The study relies solely on objective evaluation metrics without direct human perceptual validation, which may not accurately reflect human-perceived quality
- The approach uses only two text corpora (Harvard Sentences and LJ Speech) which may not represent the full diversity of child speech contexts
- The WaveGlow vocoder trained on adult speech may not perfectly match child speech acoustics, potentially limiting synthesis quality

## Confidence
- **High confidence:** The transfer learning methodology (pretraining on LibriTTS, finetuning on MyST) is technically sound and well-documented
- **Medium confidence:** The objective evaluation approach using MOSNet, wav2vec2, and speaker encoder provides meaningful comparisons, though correlation with human judgment is assumed rather than proven
- **Low confidence:** Claims about the naturalness and intelligibility of synthetic child speech relative to real child speech are not fully supported without human listening tests

## Next Checks
1. Conduct human listening tests comparing synthetic child speech samples with real child speech to validate the correlation between objective metrics and perceptual quality
2. Test the synthetic datasets (CS_HS, CS_LJ) with multiple ASR models trained on child speech to verify intelligibility claims across different acoustic models
3. Perform ablation studies removing the pretraining step to quantify the actual benefit of transfer learning versus training from scratch on MyST