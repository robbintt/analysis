---
ver: rpa2
title: Symmetry Defense Against XGBoost Adversarial Perturbation Attacks
arxiv_id: '2308.05575'
source_url: https://arxiv.org/abs/2308.05575
tags:
- adversarial
- samples
- symmetry
- defense
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines whether symmetry can be used to defend tree-based
  ensemble classifiers such as gradient-boosting decision trees (GBDTs) against adversarial
  perturbation attacks. The paper shows that GBDTs also lack invariance with respect
  to symmetries, similarly to convolutional neural networks (CNNs).
---

# Symmetry Defense Against XGBoost Adversarial Perturbation Attacks

## Quick Facts
- arXiv ID: 2308.05575
- Source URL: https://arxiv.org/abs/2308.05575
- Reference count: 40
- Key outcome: Symmetry defense achieves up to 100% accuracy improvement against zero-knowledge adversaries and over 95% against perfect-knowledge adversaries by exploiting XGBoost's lack of invariance to feature inversion symmetry

## Executive Summary
This paper introduces a symmetry-based defense mechanism for XGBoost classifiers against adversarial perturbation attacks. The key insight is that tree-based ensemble classifiers like XGBoost lack invariance with respect to feature inversion symmetry, similar to CNNs. By training classifiers with both original and inverted samples, and applying symmetry transformations during inference, the defense significantly improves adversarial robustness. The approach works against both zero-knowledge and perfect-knowledge adversaries, with the latter requiring two symmetry transformations to be applied before classification.

## Method Summary
The symmetry defense trains XGBoost classifiers on datasets augmented with symmetric samples (inverted and/or flipped versions). During inference, adversarial samples are transformed using symmetry operations before classification. For zero-knowledge adversaries, samples are inverted (1-a for each feature value). For perfect-knowledge adversaries who know the defense, both inverted and flipped samples are used, and the majority vote determines the final classification. The method exploits XGBoost's greedy splitting algorithm which creates asymmetric decision boundaries even when trained on symmetric data.

## Key Results
- Symmetry defense exceeds default classifier accuracy by up to 100 percentage points against zero-knowledge adversaries
- Achieves over 95% accuracy improvement against perfect-knowledge adversaries on most datasets
- MILP, LT-Attack, and Cube attacks fail to generate adversarial samples against symmetry defense classifiers for zero-knowledge adversaries
- Training with symmetric samples increases the number of splitting conditions by roughly 50-100% for most datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XGBoost classifiers lack invariance with respect to feature inversion symmetry, classifying original and inverted samples differently even when trained on both
- Mechanism: The greedy splitting algorithm and fixed equality conditions create asymmetric split values in symmetric training settings
- Core assumption: XGBoost's split finding algorithm produces different splits for symmetric data due to its greedy nature
- Evidence anchors: Empirical results showing different classifications for original vs inverted samples across multiple datasets
- Break condition: If XGBoost's split finding were modified to ensure symmetric splits when trained on symmetric data

### Mechanism 2
- Claim: Training with symmetric samples increases adversarial robustness by fragmenting the input space into smaller regions with more splitting conditions
- Mechanism: Data augmentation with symmetric samples creates more decision boundaries, making it harder for attacks to find successful perturbations
- Core assumption: Input space fragmentation directly correlates with increased adversarial robustness
- Evidence anchors: Table 8 shows 50-100% increase in splitting conditions when training with symmetric samples
- Break condition: If fragmentation does not lead to meaningful robustness improvements

### Mechanism 3
- Claim: Certain attacks (MILP, LT-Attack, Cube) fail against symmetry defense classifiers because they cannot find perturbations in the more fragmented space
- Mechanism: These attacks rely on finding minimal perturbations or small Hamming distances, which become infeasible with increased splitting conditions
- Core assumption: Attack algorithms are not designed to handle the complexity introduced by symmetry defense
- Evidence anchors: MILP, LT-Attack, and Cube attacks fail to generate misclassifying adversarial samples against symmetry defense
- Break condition: If attacks are modified to handle the increased complexity or if symmetry defense does not impact attack success

## Foundational Learning

- Concept: XGBoost and GBDT classifiers
  - Why needed here: Understanding XGBoost's greedy splitting and tree structure is essential to comprehend why it lacks symmetry invariance
  - Quick check question: What is the difference between a continuous and non-continuous classifier, and why does this matter for adversarial attacks?

- Concept: Symmetry, equivariance, and invariance
  - Why needed here: These concepts are fundamental to understanding how the symmetry defense exploits classifier properties
  - Quick check question: What is the difference between equivariance and invariance, and how does this relate to classifier behavior with symmetric samples?

- Concept: Adversarial attacks and defenses
  - Why needed here: Knowledge of attack types and defense mechanisms is necessary to understand the threat model and defense effectiveness
  - Quick check question: What is the difference between a zero-knowledge and a perfect-knowledge adversary, and how does this impact the design of the symmetry defense?

## Architecture Onboarding

- Component map: Original dataset → Symmetric dataset (inverted, flipped) → Combined dataset → XGBoost classifier training → Input sample → Apply symmetry transformations → Classify with XGBoost → Determine label based on majority vote or agreement

- Critical path:
  1. Train XGBoost classifier with original and symmetric samples
  2. During inference, apply symmetry transformations to input sample
  3. Classify transformed samples and determine final label
  4. Evaluate against adversarial attacks

- Design tradeoffs:
  - Training time: Doubled or quadrupled due to symmetric data augmentation
  - Inference time: Slightly increased due to symmetry transformations and majority voting
  - Memory usage: Increased due to larger training dataset
  - Robustness: Significantly improved against certain adversarial attacks

- Failure signatures:
  - If the classifier does not achieve higher accuracy against adversarial samples compared to default/robust classifiers
  - If the symmetry transformations do not revert the classification of adversarial samples to the original correct classification
  - If the attacks are able to generate adversarial samples despite the symmetry defense

- First 3 experiments:
  1. Train XGBoost classifier on breast-cancer dataset with original and inverted samples separately, then test with original and inverted samples to verify lack of invariance
  2. Apply symmetry defense against Cube attack on breast-cancer dataset and measure accuracy improvement
  3. Compare splitting conditions in classifiers trained with and without symmetric samples to verify input space fragmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which training with symmetric samples leads to decreased adversarial perturbation values for some datasets but not others?
- Basis in paper: The paper hypothesizes that training with additional symmetry samples causes the classifier to fragment the input space into smaller areas with samples of the same classification due to more splitting conditions
- Why unresolved: The paper provides a hypothesis but does not provide definitive evidence to support it
- What evidence would resolve it: Experiments that directly measure the fragmentation of the input space and the size of areas with samples of the same classification in classifiers trained with and without symmetric samples

### Open Question 2
- Question: Why are the MILP, LT-Attack, and Cube attacks unable to generate adversarial samples against the symmetry defense classifier for zero-knowledge adversaries?
- Basis in paper: The paper states these attacks do not succeed against symmetry defense classifiers
- Why unresolved: The paper provides hypotheses for why each attack might fail but does not provide definitive evidence
- What evidence would resolve it: Experiments that directly test the hypotheses for why each attack fails

### Open Question 3
- Question: Can the symmetry defense be applied to datasets that lack inherent symmetries, such as non-image datasets?
- Basis in paper: The paper states that the lack of inherent symmetries in non-image datasets impedes the application of the symmetry defense against perfect-knowledge attacks
- Why unresolved: The paper does not provide any experiments or results for applying the symmetry defense to non-image datasets
- What evidence would resolve it: Experiments that apply the symmetry defense to non-image datasets and measure the effectiveness of the defense against adversarial attacks

## Limitations
- Effectiveness varies significantly across datasets, with some showing reduced improvements
- The defense requires training with symmetric samples, which doubles or quadruples training data requirements
- Perfect-knowledge adversaries require more complex handling with two symmetry transformations
- The exact mechanism for why certain attacks fail is hypothesized rather than rigorously proven

## Confidence
- **High Confidence**: XGBoost classifiers lack invariance with respect to feature inversion symmetry, supported by empirical evidence across multiple datasets
- **Medium Confidence**: Training with symmetric samples increases adversarial robustness through input space fragmentation, though the direct causal relationship is not fully established
- **Low Confidence**: Specific adversarial attacks cannot generate adversarial samples against symmetry defense classifiers, as this relies on indirect evidence and hypothesized mechanisms

## Next Checks
1. Replicate the symmetry invariance test: Train XGBoost classifiers on breast-cancer dataset with original and inverted samples separately, then test classification consistency on symmetric pairs to verify the lack of invariance claim
2. Validate attack failure mechanisms: Implement MILP, LT-Attack, and Cube attacks against symmetry defense classifiers and systematically analyze why they fail
3. Test dataset dependency: Apply symmetry defense to additional datasets beyond the nine studied, particularly focusing on datasets where the defense showed reduced effectiveness (like HIGGS) to understand boundary conditions