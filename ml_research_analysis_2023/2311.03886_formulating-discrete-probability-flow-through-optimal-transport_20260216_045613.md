---
ver: rpa2
title: Formulating Discrete Probability Flow Through Optimal Transport
arxiv_id: '2311.03886'
source_url: https://arxiv.org/abs/2311.03886
tags:
- discrete
- probability
- diffusion
- flow
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of defining a deterministic
  probability flow for discrete diffusion models, which is crucial for improving sampling
  certainty. The authors establish the theoretical foundation by proving that the
  continuous probability flow coincides with the Monge optimal transport map under
  certain conditions.
---

# Formulating Discrete Probability Flow Through Optimal Transport

## Quick Facts
- **arXiv ID**: 2311.03886
- **Source URL**: https://arxiv.org/abs/2311.03886
- **Reference count**: 40
- **Primary result**: Discrete probability flow significantly reduces sampling uncertainty in discrete diffusion models while maintaining generation quality

## Executive Summary
This paper addresses a fundamental limitation in discrete diffusion models: the lack of a deterministic probability flow analogous to continuous diffusion models. The authors establish theoretical foundations by proving that continuous probability flow coincides with Monge optimal transport maps under specific conditions, then develop a discrete analogue. By modifying the forward process through optimal transport principles, they create a discrete probability flow that significantly reduces sampling uncertainty. Experiments demonstrate substantial reductions in conditional standard deviation while maintaining comparable generation quality on both synthetic and real-world datasets.

## Method Summary
The authors propose a discrete probability flow framework based on optimal transport theory. They first prove that under certain conditions (point masses on a line, finite time intervals), continuous probability flow coincides with the Monge optimal transport map. Building on this foundation, they modify the standard discrete diffusion model's generator by introducing a rectification term that eliminates mutual flows between states, creating an optimal transport plan. The resulting discrete probability flow is implemented through a score-based training approach similar to SDDM, with sampling performed via Euler integration of the modified transition rates.

## Key Results
- Discrete probability flow reduces conditional standard deviation (CSD) by up to 61.4% on synthetic datasets
- MMD scores remain comparable to baseline methods while achieving significant certainty improvements
- CIFAR-10 experiments show 49.6% reduction in class-std and 61.4% reduction in class-entropy metrics
- The method maintains generation quality while dramatically improving sampling certainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The discrete probability flow is derived from optimal transport principles by modifying the forward process to create an optimal transport map.
- Mechanism: The authors modify the standard discrete diffusion model's generator (QD) by introducing a rectification term based on the difference between source and destination state probabilities. This creates a new generator (Q) where transitions only occur from lower probability states to higher probability states, ensuring mass flows optimally.
- Core assumption: The process defined by the modified generator Q maintains the same single-time marginal distribution as the original process while eliminating mutual flows between states.
- Evidence anchors:
  - [abstract]: "we propose a discrete analogue of the probability flow under the framework of optimal transport, which we have defined as the discrete probability flow"
  - [section 4.1]: "we propose a modified version that will be proved to be a solution to the Kantorovich problem, namely, an optimal transport plan"
  - [corpus]: Weak - no direct corpus evidence found for this specific construction method
- Break condition: If the rectification term doesn't preserve the marginal distribution or if mutual flows cannot be eliminated for certain state configurations.

### Mechanism 2
- Claim: The continuous probability flow coincides with the Monge optimal transport map under specific conditions.
- Mechanism: When the initial distribution consists of point masses on a line and the process evolves according to the Fokker-Planck equation, the probability flow ODE generates a diffeomorphism that transports mass optimally between time points.
- Core assumption: The initial distribution follows a specific form (18) with all points on the same line, and the time interval is finite (not reaching 0 or +∞).
- Evidence anchors:
  - [abstract]: "we first prove that the continuous probability flow is the Monge optimal transport map under certain conditions"
  - [section 3.2]: "Under some conditions, the conjecture is correct" and the proof relies on the convexity of the solution to the transport equation
  - [corpus]: Weak - no direct corpus evidence found for this specific equivalence proof
- Break condition: If the initial distribution doesn't satisfy the point mass on a line condition, or if the time interval includes t=0 or t=+∞.

### Mechanism 3
- Claim: The discrete probability flow significantly reduces sampling uncertainty compared to standard discrete diffusion models.
- Mechanism: By ensuring transitions only occur from lower probability states to higher probability states (as enforced by the rectification term), the reverse process becomes more deterministic, concentrating samples around high-probability regions.
- Core assumption: The reduction in conditional standard deviation (CSD) directly translates to improved sampling certainty in practical applications.
- Evidence anchors:
  - [abstract]: "surpasses previous discrete diffusion models in its ability to generate more certain outcomes"
  - [section 4.2]: "Our discrete probability flow significantly reduces CSD s,t(X)" and provides numerical evidence in Table 2
  - [section D.5]: "Our DPF method results in a significant reduction in the CSD score" with specific percentage improvements reported
- Break condition: If the CSD metric doesn't accurately capture sampling uncertainty, or if the reduction in CSD doesn't translate to practical improvements in sample diversity or quality.

## Foundational Learning

- **Concept**: Optimal Transport Theory
  - Why needed here: The entire framework of discrete probability flow is built on optimal transport principles, requiring understanding of Monge and Kantorovich formulations
  - Quick check question: What is the key difference between Monge and Kantorovich optimal transport problems when dealing with discrete distributions?

- **Concept**: Diffusion Models and Score-Based Generative Modeling
  - Why needed here: The paper builds on existing diffusion model frameworks, requiring understanding of forward/reverse processes and score matching
  - Quick check question: How does the score-based approach in discrete diffusion models differ from the ELBO approach in terms of what neural networks are trained to approximate?

- **Concept**: Stochastic Differential Equations and Itô Calculus
  - Why needed here: The continuous probability flow is derived from the Fokker-Planck equation, which requires understanding of stochastic processes
  - Quick check question: What is the relationship between the generator of an Itô diffusion and the Fokker-Planck equation?

## Architecture Onboarding

- **Component map**: Forward Process -> Modified Generator Q -> Neural Network Score Model -> Reverse Process (Euler integration) -> Sampling Evaluation (CSD, MMD)
- **Critical path**:
  1. Train score model using standard SDDM training procedure with modified forward process
  2. Implement reverse process using Algorithm 1 with rectification logic
  3. Generate samples and evaluate certainty via CSD and quality via MMD
  4. Visualize sampling behavior and compare with baseline
- **Design tradeoffs**:
  - Precision vs. Efficiency: The rectification process adds computational overhead but improves sampling certainty
  - Complexity vs. Performance: The modified generator requires more complex implementation but provides theoretical guarantees
  - Generality vs. Specificity: The method is tailored to specific discrete diffusion frameworks but may not generalize to all discrete generative models
- **Failure signatures**:
  - High CSD values despite using DPF (indicates rectification not working as intended)
  - Degraded MMD scores compared to baseline (indicates quality sacrificed for certainty)
  - Numerical instability in Euler integration (indicates step size issues)
  - Non-convergence during training (indicates learning rate or architecture issues)
- **First 3 experiments**:
  1. Implement the basic DPF sampling on 2spirals dataset with S=2 and verify CSD reduction compared to SDDM
  2. Test the method on higher state size (S=5, S=10) datasets to verify generalization
  3. Apply DPF to the CIFAR-10 dataset using the pre-trained τLDR-0 model and evaluate certainty improvements via class-std and class-entropy metrics

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the discrete probability flow be extended to a broader range of discrete diffusion models beyond the specific type studied in this paper?
  - Basis in paper: [explicit] The paper mentions that "the definition of the probability flow has been limited to a specific type of discrete diffusion model, which also could be extended to a broader range of models."
  - Why unresolved: The paper does not provide specific methods or approaches for extending the discrete probability flow to other types of discrete diffusion models.
  - What evidence would resolve it: A proposed method or framework for extending the discrete probability flow to other types of discrete diffusion models, along with experimental validation.

- **Open Question 2**: What are the potential benefits and challenges of separating the drift term and the stochastic term in discrete diffusion models, as opposed to assimilating them into a single term?
  - Basis in paper: [inferred] The paper discusses the importance of the drift term in continuous diffusion models and mentions that in discrete diffusion models, the drift term can be assimilated into the stochastic term.
  - Why unresolved: The paper does not explore the potential benefits and challenges of treating the drift term and stochastic term separately in discrete diffusion models.
  - What evidence would resolve it: A comparative study of discrete diffusion models with separate drift and stochastic terms versus models with a single combined term, evaluating their performance and properties.

- **Open Question 3**: How can the discrete probability flow be applied to improve controllable generation and data editing in discrete diffusion models?
  - Basis in paper: [inferred] The paper suggests that reducing uncertainty in discrete diffusion models can bring benefits such as controllable generation and data editing.
  - Why unresolved: The paper does not provide specific examples or methods for applying the discrete probability flow to improve controllable generation and data editing.
  - What evidence would resolve it: Demonstrations of using the discrete probability flow to achieve controllable generation and data editing tasks, along with quantitative evaluations of the improvements.

## Limitations
- The theoretical equivalence between continuous probability flow and Monge optimal transport holds only under restrictive conditions (point masses on a line, finite time intervals)
- The empirical validation is primarily based on synthetic datasets and a single real-world dataset (CIFAR-10)
- The generalizability to other discrete diffusion frameworks and different state space structures remains uncertain

## Confidence
- **High Confidence**: The empirical results showing CSD reduction and MMD scores are directly measurable and reproducible. The algorithmic implementation details for discrete probability flow are clearly specified.
- **Medium Confidence**: The theoretical claims about the relationship between continuous probability flow and optimal transport maps are well-founded but rely on restrictive conditions that may not hold in practice.
- **Low Confidence**: The generalizability of the discrete probability flow approach to other discrete diffusion frameworks and its robustness to different state space structures remains uncertain.

## Next Checks
1. Test the discrete probability flow construction on a broader class of discrete distributions to verify that the rectification process consistently eliminates mutual flows while preserving marginal distributions across different state space configurations.
2. Evaluate the discrete probability flow approach on additional real-world datasets with discrete structures (e.g., text, graphs, or multi-modal data) to assess its effectiveness beyond image data and synthetic distributions.
3. Investigate whether the CSD metric accurately captures practical sampling uncertainty by conducting user studies or analyzing downstream task performance to verify that reduced CSD translates to meaningful improvements in real applications.