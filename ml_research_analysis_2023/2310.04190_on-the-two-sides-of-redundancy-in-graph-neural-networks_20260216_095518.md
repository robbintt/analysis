---
ver: rpa2
title: On the Two Sides of Redundancy in Graph Neural Networks
arxiv_id: '2310.04190'
source_url: https://arxiv.org/abs/2310.04190
tags:
- trees
- graph
- node
- tree
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses information and computational redundancy in
  message passing graph neural networks (MPNNs) that causes oversquashing. The authors
  propose using k-redundant neighborhood trees (k-NTs) which prune redundant branches
  from unfolding trees, combined with a neural tree canonization technique and compact
  DAG representations.
---

# On the Two Sides of Redundancy in Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.04190
- Source URL: https://arxiv.org/abs/2310.04190
- Reference count: 40
- Key outcome: k-redundant neighborhood trees reduce information redundancy in message passing while maintaining competitive accuracy on graph classification tasks

## Executive Summary
This paper addresses information and computational redundancy in message passing graph neural networks (MPNNs) that causes oversquashing. The authors propose using k-redundant neighborhood trees (k-NTs) which prune redundant branches from unfolding trees, combined with a neural tree canonization technique and compact DAG representations. Their DAG-MLP architecture processes k-NTs systematically, avoiding redundant computations. Experiments show k-NTs are more expressive than MPNNs and achieve competitive accuracy on synthetic and real-world graph classification tasks, with 0- and 1-NTs performing best. The approach reduces oversquashing while maintaining high performance.

## Method Summary
The method uses k-redundant neighborhood trees (k-NTs) that allow nodes to appear at most k times within distance k from the root. These trees are merged into a directed acyclic graph (DAG) using a neural tree canonization approach that identifies isomorphic subtrees. A DAG-MLP architecture then processes the compact DAG representation using level-wise message passing. The approach combines k-NTs of different heights for readout, and a final MLP produces class predictions. The method is evaluated on synthetic and real-world graph classification datasets.

## Key Results
- 0- and 1-NTs perform best experimentally, with accuracy decreasing as k increases
- 1-NTs are strictly more expressive than unfolding trees and can distinguish graphs that the Weisfeiler-Leman test cannot
- The approach achieves competitive accuracy on real-world datasets (MUTAG, IMDB-B, IMDB-M, ENZYMES, PROTEINS) while reducing oversquashing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing redundant branches from unfolding trees reduces information redundancy in message passing
- Mechanism: By pruning redundant branches where nodes appear multiple times within k levels, the method prevents repeated encoding of identical information that would otherwise be passed back and forth between connected nodes
- Core assumption: Information redundancy in message passing amplifies oversquashing effects
- Evidence anchors:
  - [abstract]: "Information redundancy in message passing, i.e., the repetitive exchange and encoding of identical information amplifies oversquashing"
  - [section 4.1]: "We argue that oversquashing can be alleviated by removing the encoding of repeated information"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If k is set too high (k â‰¥ i for height i), the neighborhood trees become equivalent to unfolding trees and redundancy is not reduced

### Mechanism 2
- Claim: Merging trees into a DAG allows computational redundancy to be exploited through reuse of isomorphic subtrees
- Mechanism: The merge DAG represents multiple trees by identifying isomorphic substructures and representing them only once, allowing embeddings computed for one subtree to be reused for all isomorphic occurrences
- Core assumption: Isomorphic subtrees can share computed embeddings without loss of expressiveness
- Evidence anchors:
  - [section 4.2]: "Results computed for one tree can be reused for others by identifying isomorphic substructures causing computational redundancy"
  - [section 4.2]: "The neural tree canonization approach applied to the merge DAG produces the same result for the nodes in the DAG as for the nodes in the original trees"
  - [corpus]: Weak - no direct corpus evidence found for this specific computational redundancy mechanism
- Break condition: If labeling function does not properly distinguish non-isomorphic subtrees, incorrect sharing of embeddings may occur

### Mechanism 3
- Claim: 1-redundant neighborhood trees (1-NTs) are strictly more expressive than unfolding trees
- Mechanism: By allowing a node to appear at most once within 1 level of the root in the neighborhood tree, 1-NTs capture more structural information than unfolding trees while still avoiding some redundancy
- Core assumption: The expressivity gain from 1-NTs over unfolding trees is sufficient to distinguish graphs that WL cannot distinguish
- Evidence anchors:
  - [section 4.4]: "The 1-NT isomorphism test is more powerful than the Weisfeiler-Leman isomorphism test"
  - [section 4.4]: "âˆƒðº, ð» : ðº â‰ ð» âˆ§ ð‘¤ð‘™ âˆž(ðº) = ð‘¤ð‘™ âˆž(ð» ) âˆ§ ð‘›ð‘¡âˆž,1(ðº) â‰  ð‘›ð‘¡âˆž,1(ð»)"
  - [corpus]: Weak - no direct corpus evidence found for this specific expressiveness claim
- Break condition: If the structural difference captured by 1-NTs is not relevant to the target task, the expressivity gain may not translate to improved performance

## Foundational Learning

- Concept: Weisfeiler-Leman (WL) algorithm and unfolding trees
  - Why needed here: The paper builds on the WL algorithm as a baseline for expressivity comparison, and unfolding trees are the foundation for both MPNNs and the proposed neighborhood trees
  - Quick check question: What is the relationship between WL colors and isomorphism of unfolding trees?

- Concept: Graph isomorphism and tree isomorphism testing
  - Why needed here: The paper relies on tree isomorphism concepts for both the theoretical analysis of expressivity and the practical implementation of merging trees into DAGs
  - Quick check question: How does the AHU algorithm for tree isomorphism work, and why is it relevant to this work?

- Concept: Computational redundancy and information redundancy
  - Why needed here: The paper distinguishes between these two types of redundancy and proposes different solutions for each, making it crucial to understand the distinction
  - Quick check question: What is the difference between information redundancy in message passing and computational redundancy in the implementation?

## Architecture Onboarding

- Component map: Graph â†’ k-NTs generation â†’ Merge DAG construction â†’ DAG-MLP layers â†’ Readout pooling â†’ Final prediction
- Critical path: Graph â†’ k-NTs generation â†’ Merge DAG construction â†’ DAG-MLP layers â†’ Readout pooling â†’ Final prediction
- Design tradeoffs:
  - Higher k values reduce information redundancy but increase computational cost and may reintroduce redundancy
  - Combining heights captures multi-scale information but increases model complexity
  - Fixed single-height is simpler but may miss important structural details at other scales
- Failure signatures:
  - Accuracy plateaus or degrades with increasing k (indicates oversquashing from information redundancy)
  - Memory usage grows exponentially with graph size (indicates failure to properly merge isomorphic subtrees)
  - Training instability with high layer counts (indicates gradient issues from deep DAG processing)
- First 3 experiments:
  1. Verify k-NT generation: Test that 0-NTs and 1-NTs are correctly pruned from unfolding trees for simple graphs
  2. Test DAG merging: Verify that isomorphic subtrees are correctly identified and merged in the DAG for graphs with symmetries
  3. Validate expressivity: Test that 1-NTs can distinguish the hexagon vs two triangles example from the paper

## Open Questions the Paper Calls Out

- Open Question 1: What is the exact theoretical relationship between the expressive power of 0-NTs and unfolding trees as node invariants?
  - Basis in paper: [inferred] The paper states that 0-NTs can distinguish nodes that unfolding trees cannot (Figure 9), but also that they are not more expressive than unfolding trees in every case
  - Why unresolved: The paper acknowledges this is an open theoretical question without providing a complete characterization of the expressive relationship
  - What evidence would resolve it: A formal proof establishing the exact conditions under which 0-NTs and unfolding trees have equal vs. different expressive power as node invariants

- Open Question 2: How does the choice of k parameter affect the trade-off between expressivity and computational efficiency across different graph classes?
  - Basis in paper: [explicit] The paper shows 0- and 1-NTs perform best experimentally, with accuracy decreasing as k increases, suggesting redundancy leads to oversquashing
  - Why unresolved: The paper doesn't provide a theoretical framework for understanding how k should be selected based on graph characteristics like diameter, density, or symmetry
  - What evidence would resolve it: A comprehensive study characterizing optimal k values across diverse graph families, potentially with theoretical bounds on the expressivity-computational efficiency trade-off

- Open Question 3: Can the DAG-MLP architecture be extended to incorporate edge attributes while maintaining the theoretical properties?
  - Basis in paper: [explicit] The paper notes in a footnote that methods can be extended to incorporate edge attributes, but doesn't explore this extension
  - Why unresolved: The paper focuses on node-only attributes and doesn't investigate how edge attributes would affect the merge DAG construction, tree canonization, or theoretical expressiveness
  - What evidence would resolve it: An extension of the DAG-MLP framework that incorporates edge attributes, with analysis of how this affects the theoretical properties and experimental performance

- Open Question 4: What is the impact of combining multiple tree heights (as in "Combine Heights") on generalization capabilities across different graph classification tasks?
  - Basis in paper: [explicit] The paper notes that using multiple different tree heights doesn't improve generalization capabilities, with Table 5 showing similar results to fixed single-height
  - Why unresolved: The paper doesn't provide theoretical analysis of why combining heights doesn't help, or explore whether this might be beneficial for specific graph types
  - What evidence would resolve it: A theoretical analysis of why combining heights doesn't improve performance, or identification of specific graph characteristics where combining heights would be beneficial

## Limitations
- The theoretical expressivity claims for 1-NTs rely on abstract constructions without comprehensive empirical validation across diverse graph classes
- The computational complexity analysis is incomplete, with no discussion of how merging efficiency scales with graph size or density
- The paper lacks comprehensive ablation studies to isolate the effects of individual components (k-NT pruning, DAG merging, and canonization)

## Confidence
- Theoretical expressivity claims (1-NTs > WL): Medium - supported by formal proofs but limited empirical validation
- Practical performance improvements: Medium - competitive results on standard benchmarks but narrow dataset coverage
- Computational efficiency claims: Low - complexity analysis is incomplete and implementation details are sparse

## Next Checks
1. **Expressivity validation**: Test 1-NTs on graph pairs that WL cannot distinguish but should be distinguishable by structural features captured in the 1-NT representation
2. **Scalability analysis**: Measure memory usage and runtime as graph size increases to validate computational efficiency claims for the DAG merging approach
3. **Ablation study**: Compare performance with and without each key component (pruning, merging, canonization) to quantify individual contributions to the observed improvements