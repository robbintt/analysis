---
ver: rpa2
title: End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear
  Model Predictive Control
arxiv_id: '2308.01674'
source_url: https://arxiv.org/abs/2308.01674
tags:
- control
- koopman
- learning
- training
- controllers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an end-to-end reinforcement learning approach
  for training Koopman surrogate models to improve economic nonlinear model predictive
  control (eNMPC) performance. The method uses Koopman theory to construct convex
  optimal control problems and automatic differentiation to train task-optimal models.
---

# End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear Model Predictive Control

## Quick Facts
- arXiv ID: 2308.01674
- Source URL: https://arxiv.org/abs/2308.01674
- Reference count: 0
- Key outcome: This paper proposes an end-to-end reinforcement learning approach for training Koopman surrogate models to improve economic nonlinear model predictive control (eNMPC) performance. The method uses Koopman theory to construct convex optimal control problems and automatic differentiation to train task-optimal models. Evaluated on two CSTR-based applications, the learned models outperform system identification-based counterparts in both NMPC and eNMPC settings. Additionally, the MPC controllers can adapt to modified constraints without retraining, unlike model-free policies. This demonstrates the potential of combining Koopman models with end-to-end RL for real-time, adaptive predictive control.

## Executive Summary
This paper introduces an end-to-end reinforcement learning framework for training Koopman surrogate models to enhance economic nonlinear model predictive control (eNMPC) performance. By leveraging Koopman theory to construct convex optimal control problems and using automatic differentiation to train task-optimal models, the proposed method aims to outperform traditional system identification (SI) approaches. The authors demonstrate the effectiveness of their approach on two CSTR-based applications, showing improved performance in both NMPC and eNMPC settings. Additionally, the MPC controllers can adapt to modified constraints without retraining, unlike model-free policies, highlighting the potential of combining Koopman models with end-to-end RL for real-time, adaptive predictive control.

## Method Summary
The proposed method involves training Koopman surrogate models end-to-end via reinforcement learning (RL) to improve economic nonlinear model predictive control (eNMPC) performance. The process begins with system identification (SI) pretraining using a mechanistic CSTR model to generate a dataset. Koopman models are then trained using an autoencoder loss, prediction loss, and combined loss. The end-to-end refinement stage employs proximal policy optimization (PPO) with differentiable Koopman-MPC policies as actors, trained on NMPC and eNMPC applications. The Koopman models have an 8-dimensional latent space and a 2-layer encoder MLP with tanh activation. The method uses Adam optimizer with various learning rates and hyperparameters specified in supplementary tables.

## Key Results
- Learned Koopman models outperform system identification-based counterparts in both NMPC and eNMPC settings
- MPC controllers can adapt to modified constraints without retraining, unlike model-free policies
- End-to-end RL training of Koopman models is more sample-efficient than learning model-free policies from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Koopman surrogate models trained end-to-end via RL outperform system identification (SI) models in (e)NMPC.
- Mechanism: RL directly optimizes the Koopman model parameters for control performance rather than prediction accuracy, using differentiable MPC layers to backpropagate task-specific rewards.
- Core assumption: The Koopman model must produce convex OCPs so that differentiable MPC solvers can be applied reliably.
- Evidence anchors:
  - [abstract] "The learned models outperform system identification-based counterparts in both NMPC and eNMPC settings."
  - [section 2.4] "Using the cvxpylayers package... we construct automatically differentiable stochastic Koopman-MPC policies..."
- Break condition: If the Koopman model fails to produce convex OCPs, the differentiable MPC assumption breaks, preventing effective RL training.

### Mechanism 2
- Claim: End-to-end trained Koopman-MPC controllers can adapt to changes in constraints without retraining.
- Mechanism: The MPC formulation explicitly encodes constraints, so adjusting bounds in the solver allows adaptation; model-free policies cannot do this since constraints are learned implicitly through rewards.
- Core assumption: The underlying system dynamics remain unchanged when constraints are modified.
- Evidence anchors:
  - [abstract] "Additionally, the MPC controllers can adapt to modified constraints without retraining, unlike model-free policies."
  - [section 3.5] "We shift the bounds of the product concentration variable c and rerun the test without retraining."
- Break condition: If dynamics change along with constraints, adaptation via bound shifts will fail.

### Mechanism 3
- Claim: Using RL to refine Koopman models is more sample-efficient than learning a model-free policy from scratch.
- Mechanism: Pretraining via SI gives a reasonable initial Koopman model, then RL fine-tunes only for the control task; model-free policies must learn both dynamics and policy.
- Core assumption: The mechanistic model is available to generate a good SI baseline.
- Evidence anchors:
  - [abstract] "learning the system dynamics may be more sample-efficient than learning a model-free control policy (Amos et al. (2018); Chen et al. (2019))."
  - [section 3.2] "We perform SI by minimizing the sum of three loss functions..."
- Break condition: If no mechanistic model is available, SI pretraining is impossible, eliminating the sample efficiency advantage.

## Foundational Learning

- Concept: Koopman operator theory
  - Why needed here: Provides a linear embedding of nonlinear dynamics, enabling convex OCPs for MPC.
  - Quick check question: What property of Koopman embeddings allows linear MPC to be applied to nonlinear systems?

- Concept: Reinforcement learning with continuous action spaces
  - Why needed here: Used to train the Koopman model parameters via policy gradient methods (PPO) for task-specific control.
  - Quick check question: Why is PPO preferred over pure policy gradient methods in this setting?

- Concept: Post-optimal sensitivity analysis
  - Why needed here: Enables differentiation of the MPC solution with respect to model parameters for end-to-end training.
  - Quick check question: What assumptions must hold for the implicit function theorem to apply to parametric convex programs?

## Architecture Onboarding

- Component map: Data generator -> SI pretraining -> End-to-end RL -> Test controller
- Key objects: Koopman encoder/decoder, Aθ/Bθ/Cθ matrices, MPC solver, PPO actor/critic
- Critical path: Generate data -> SI train Koopman model -> Initialize PPO with Koopman-MPC policy -> RL refine -> Deploy without exploration
- Design tradeoffs:
  - Larger latent space N improves prediction but increases OCP solve time.
  - More training episodes improve performance but increase compute cost.
  - Exploration noise helps exploration but degrades test-time performance.
- Failure signatures:
  - Poor initial SI -> RL cannot recover; check data quality and model capacity.
  - Non-convex OCPs -> Differentiable solver fails; verify Koopman model structure.
  - High constraint violations -> Reward scaling β too low or slack penalty M too small.
- First 3 experiments:
  1. Verify SI training reproduces dynamics on validation set.
  2. Run RL for 100 episodes and check reward trend; confirm gradient flow in Koopman parameters.
  3. Test trained controller on unseen data; measure constraint violation rate and score.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed end-to-end Koopman model learning approach scale to larger, more complex control systems beyond the CSTR benchmark?
- Basis in paper: [explicit] The paper acknowledges that the case studies used are relatively small and less complex than most real-world control systems, suggesting this as a direction for future work.
- Why unresolved: The current validation is limited to a small CSTR model, and the authors do not provide empirical data or theoretical analysis on scalability to larger systems.
- What evidence would resolve it: Testing the approach on progressively larger and more complex systems, demonstrating that the computational tractability and performance benefits are maintained as system size increases.

### Open Question 2
- Question: What is the impact of different Koopman model architectures (e.g., different latent space dimensionalities or encoder network structures) on the performance of the end-to-end trained models in MPC?
- Basis in paper: [inferred] The paper uses a specific Koopman model architecture (8-dimensional latent space, 2 hidden layers in encoder) but does not explore the sensitivity of results to architectural choices.
- Why unresolved: The authors do not conduct an ablation study or systematic comparison of different architectures, leaving the optimal model structure unclear.
- What evidence would resolve it: A comprehensive study comparing MPC performance across a range of Koopman model architectures, identifying the most effective design choices.

### Open Question 3
- Question: How does the proposed method perform in scenarios with significant distribution shift, beyond the simple bound adaptation tested in the paper?
- Basis in paper: [explicit] The paper tests adaptation to shifted bounds in eNMPC but notes that this is just one example of distribution shift and suggests it as a challenge for future work.
- Why unresolved: The evaluation of adaptability is limited to a single type of distribution shift (bound changes), and the method's robustness to other forms of shift (e.g., changes in system dynamics) is not explored.
- What evidence would resolve it: Testing the trained controllers on a variety of distribution shift scenarios, including changes in system dynamics, operating conditions, or objective functions, and quantifying the degradation in performance.

## Limitations

- Evaluation limited to two CSTR-based applications with relatively simple dynamics, raising questions about performance on more complex, higher-dimensional systems.
- Method requires a mechanistic model for system identification pretraining, which may not be available in many real-world applications.
- Computational overhead of differentiable MPC solvers and increased OCP solve time for larger latent spaces could limit real-time deployment in resource-constrained settings.

## Confidence

- Core claims: Medium
  - Performance improvements over system identification models are well-supported by experimental results.
  - Constraint adaptation capability is convincingly demonstrated.
  - Sample efficiency claims lack direct quantitative comparison with model-free policies.
  - Long-term stability of trained controllers under varying operating conditions is not explored.

## Next Checks

1. Test the approach on a higher-dimensional nonlinear system (e.g., multiple CSTRs in series or a chemical plant benchmark) to assess scalability and robustness to increased model complexity.

2. Conduct a direct comparison of sample efficiency between the proposed Koopman-RL approach and a pure model-free RL method (e.g., PPO with a neural network policy) on the same control tasks, measuring convergence speed and final performance.

3. Evaluate the long-term stability and adaptability of the trained controllers by running extended simulations with time-varying parameters or unmodeled disturbances, measuring performance degradation and constraint violation rates over time.