---
ver: rpa2
title: Contrastive Learning-based Sentence Encoders Implicitly Weight Informative
  Words
arxiv_id: '2310.15921'
source_url: https://arxiv.org/abs/2310.15921
tags:
- sentence
- word
- mean
- words
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically and experimentally demonstrates that contrastive-based
  sentence encoders, such as SBERT and SimCSE, implicitly weight words according to
  information-theoretic quantities during fine-tuning. Specifically, the norm of word
  embeddings in these models reflects the information gain associated with the distribution
  of surrounding words, meaning more informative words receive greater weight.
---

# Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words

## Quick Facts
- arXiv ID: 2310.15921
- Source URL: https://arxiv.org/abs/2310.15921
- Reference count: 40
- This paper theoretically and experimentally demonstrates that contrastive-based sentence encoders implicitly weight words according to information-theoretic quantities during fine-tuning.

## Executive Summary
This paper presents a theoretical and experimental analysis showing that contrastive-based sentence encoders like SBERT and SimCSE implicitly weight words according to their information gain during fine-tuning. The theory establishes that the norm of word embeddings encodes the KL divergence between word distributions, meaning more informative words receive greater weight. Experiments using 12 models and 4 datasets with Integrated Gradients and SHAP confirm strong empirical correlations between the encoders' word weightings and information-theoretic quantities like KL divergence and self-information.

## Method Summary
The method involves fine-tuning pre-trained MLMs (BERT, RoBERTa, MPNet) with contrastive objectives to create sentence encoders, then analyzing the implicit word weighting through XAI methods (IG/SHAP) and correlating these with information-theoretic quantities (KL divergence and self-information). The theoretical analysis derives that under specific conditions (mean pooling, inner product similarity), the norm of word embeddings reflects information gain. The experimental validation measures word contributions using IG/SHAP, aggregates them across sentence positions, and performs regression analysis against information-theoretic quantities to compute R² values.

## Key Results
- The norm of word embeddings in contrastive fine-tuned models reflects information gain (KL divergence) between word distributions
- R² values increased by up to 40 percentage points after contrastive fine-tuning compared to pre-trained models
- Strong empirical correlations (R² up to 0.4) between word weightings and information-theoretic quantities across 12 models and 4 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning objectives implicitly weight words based on their information gain (KL divergence)
- Mechanism: During contrastive fine-tuning, the model learns to discriminate between positive and negative sentence pairs. The norm of word embeddings encodes the KL divergence between the word distribution in sentences containing that word versus the general corpus distribution.
- Core assumption: The similarity function between sentence embeddings is bilinear (inner product), and sentence embeddings are constructed via mean pooling without normalization.
- Evidence anchors:
  - [abstract] "the norm of word embedding reflects the information gain associated with the distribution of surrounding words"
  - [section] "1/2 ||w||^2 ≈ KL(w)" from Theorem 1
  - [corpus] No direct corpus evidence found - this is a theoretical derivation
- Break condition: If cosine similarity is used instead of inner product, or if sentence embeddings are normalized, the mechanism may not hold.

### Mechanism 2
- Claim: Integrated Gradients and SHAP methods accurately capture implicit word weighting within sentence encoders
- Mechanism: These XAI techniques decompose the sentence embedding into additive contributions from each input word, allowing measurement of implicit weighting learned during contrastive fine-tuning.
- Core assumption: The word contributions calculated by IG/SHAP align with the norm of word embeddings used in the theoretical analysis.
- Evidence anchors:
  - [abstract] "two methods to measure the implicit weighting of models (Integrated Gradients and SHAP)"
  - [section] "IG and SHAP also additively decomposes the contribution of each input feature"
  - [corpus] No direct corpus evidence found - this is an experimental methodology claim
- Break condition: If the baseline selection for IG/SHAP is inappropriate, or if the aggregation method doesn't preserve additive properties, the measurements may be inaccurate.

### Mechanism 3
- Claim: Information-theoretic quantities (KL divergence and self-information) effectively measure word informativeness for sentence representation
- Mechanism: Words with higher information gain or self-information contribute more to distinguishing sentence meaning, so weighting by these quantities improves sentence embeddings.
- Core assumption: The information-theoretic quantities are good proxies for word informativeness in sentence context.
- Evidence anchors:
  - [abstract] "two information-theoretic quantities (information gain and self-information)"
  - [section] "KL(w) represents the extent to which the topic of a sentence is determined by observing a word w"
  - [corpus] Weak evidence - only mentions related papers without specific findings about these quantities
- Break condition: If other information-theoretic measures (e.g., mutual information) better capture word informativeness, the conclusions may not generalize.

## Foundational Learning

- Concept: Information gain (KL divergence)
  - Why needed here: KL divergence measures how much observing a word changes the distribution of words in a sentence, which directly relates to word informativeness
  - Quick check question: What does KL(Psent(·|w) || P(·)) represent in terms of word information?

- Concept: Self-information (-log P(w))
  - Why needed here: Self-information measures word rarity, which correlates with how much information a word conveys
  - Quick check question: How is self-information related to word frequency in the corpus?

- Concept: Integrated Gradients and SHAP
  - Why needed here: These XAI techniques provide quantitative measures of how much each word contributes to the final sentence embedding
  - Quick check question: What property must IG/SHAP satisfy to accurately measure implicit word weighting?

## Architecture Onboarding

- Component map: Pre-trained MLM (BERT/RoBERTa/MPNet) -> Contrastive fine-tuning layer (loss function and positive/negative sampling) -> Sentence encoder output (pooled embeddings) -> Analysis layer (IG/SHAP for word attribution, information-theoretic quantity calculation)

- Critical path: Pre-trained MLM → Contrastive fine-tuning → Sentence encoder → Word attribution → Correlation analysis

- Design tradeoffs:
  - Pooling method: MEAN pooling vs CLS token - MEAN preserves additive properties needed for theoretical analysis
  - Loss function: Inner product vs cosine similarity - inner product enables bilinear form needed for proof
  - Baseline selection: PAD tokens vs MASK tokens - affects IG/SHAP interpretation

- Failure signatures:
  - Low R² values between word weighting and information-theoretic quantities
  - Inconsistent results across different pooling methods
  - No improvement after contrastive fine-tuning

- First 3 experiments:
  1. Compare R² between pre-trained and contrastive fine-tuned models for word weighting vs KL divergence
  2. Test different baseline inputs for IG/SHAP to check sensitivity
  3. Verify additive property of IG/SHAP contributions by checking if sum equals sentence embedding norm

## Open Questions the Paper Calls Out

- Question: How does the use of cosine similarity instead of inner product in sentence encoders affect the theoretical connection between contrastive learning and information gain?
- Question: How does the word weighting behavior of contrastive-based sentence encoders differ when applied to retrieval models like DPR or Contriever?
- Question: What is the effect of different POS tags on word weighting in contrastive-based sentence encoders, given that KL(w) and -log P(w) don't consider POS?

## Limitations
- The theoretical connection relies on specific architectural choices (mean pooling, inner product similarity) that may not generalize
- Empirical validation depends on XAI methods with known limitations and baseline sensitivities
- Information-theoretic quantities are computed from limited datasets that may not capture full domain complexity

## Confidence
- High confidence: The theoretical derivation linking contrastive learning objectives to information gain through embedding norms (Theorem 1)
- Medium confidence: The empirical correlation results between word weightings and information-theoretic quantities (R² values up to 0.4 improvement)
- Low confidence: The generalizability of these findings to other sentence encoders using cosine similarity or different pooling strategies

## Next Checks
1. Test the correlation between word weightings and information-theoretic quantities across additional diverse datasets (beyond STS-B and Wikipedia) to assess domain robustness
2. Implement the same analysis pipeline with cosine similarity instead of inner product to determine if the mechanism depends on specific similarity functions
3. Compare the results using different XAI methods (e.g., SHAP vs Integrated Gradients) with various baseline configurations to establish sensitivity to methodological choices