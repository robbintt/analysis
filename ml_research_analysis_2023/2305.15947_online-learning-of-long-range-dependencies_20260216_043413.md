---
ver: rpa2
title: Online learning of long-range dependencies
arxiv_id: '2305.15947'
source_url: https://arxiv.org/abs/2305.15947
tags:
- learning
- recurrent
- online
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an online learning algorithm for recurrent
  neural networks with long-range dependencies, focusing on a specific architecture
  of independent recurrent modules (IRMs). The key idea is to leverage the independence
  of modules within a layer to compute exact gradients online with minimal memory
  overhead, using forward-mode differentiation.
---

# Online learning of long-range dependencies

## Quick Facts
- arXiv ID: 2305.15947
- Source URL: https://arxiv.org/abs/2305.15947
- Reference count: 40
- Primary result: Competitive online learning performance on long-range dependency tasks using independent recurrent modules

## Executive Summary
This paper presents an online learning algorithm for recurrent neural networks that can handle long-range dependencies without the computational overhead of backpropagation through time. The key innovation is leveraging independent recurrent modules within each layer to compute exact gradients online with minimal memory overhead. The algorithm approximates backpropagation across layers while maintaining exact gradients within each layer, enabling causal learning of temporal dependencies.

## Method Summary
The method uses a multi-layer network of independent recurrent modules (IRMs), specifically linear recurrent units (LRUs). Each LRU layer contains independent complex-valued recurrent units with diagonal recurrence matrices. The online learning algorithm combines exact differentiation within a layer of IRMs with spatial backpropagation across layers. Parameters are updated using sensitivity terms computed during the forward pass, avoiding the need to store intermediate states for backpropagation. The model is trained using the AdamW optimizer with a one-cycle cosine learning rate schedule.

## Key Results
- Achieves 100% accuracy on 20-bit copy task, matching BPTT performance
- Outperforms spatial backpropagation, truncated backpropagation, and independent recurrent modules on long-range arena benchmarks
- Demonstrates significant advantage over competing online algorithms on memory tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent recurrent modules enable exact online gradient computation with minimal memory overhead
- Mechanism: Within each layer, modules operate independently so their parameters affect only their own hidden states, allowing storage of only |θ| additional states
- Core assumption: Module independence holds throughout training
- Evidence anchors: Abstract mentions leveraging independent recurrent modules; section 2.2 defines module independence
- Break condition: If modules become coupled through learned weights

### Mechanism 2
- Claim: Diagonal recurrence matrices provide stability and enable parallel processing
- Mechanism: Diagonal structure ensures eigenvalues are directly controlled, preventing instability and allowing parallel processing across time steps
- Core assumption: Diagonal parametrization remains sufficient for task requirements
- Evidence anchors: Section 2.2 mentions benefits of diagonal structure; section 4.2 shows linearization improves performance
- Break condition: If tasks require more complex temporal dynamics

### Mechanism 3
- Claim: Spatial backpropagation across layers approximates temporal credit assignment while maintaining online learning
- Mechanism: Exact gradients computed within each layer, then spatial backpropagation propagates error signals between layers
- Core assumption: Approximation error from ignoring future temporal dependencies is acceptable
- Evidence anchors: Section 3.2 acknowledges approximation; section 4.1 claims stronger theoretical guarantees
- Break condition: If approximation error becomes too large for task complexity

## Foundational Learning

- Complex differentiation:
  - Why needed here: LRU uses complex-valued states, requiring Wirtinger derivatives
  - Quick check question: How does the chain rule differ for complex-valued functions compared to real-valued functions?

- Real-time recurrent learning:
  - Why needed here: Algorithm builds on RTRL principles but exploits module independence
  - Quick check question: What is the memory complexity of standard RTRL versus the proposed method?

- Backpropagation through time:
  - Why needed here: Method approximates BPTT across layers while maintaining exact gradients within layers
  - Quick check question: What information does standard BPTT store that this method avoids storing?

## Architecture Onboarding

- Component map:
  Input encoder → Layer norm → Gated Linear Units → Independent recurrent modules (LRU) → Layer norm → GLU → ... → Decoder

- Critical path:
  1. Forward pass through all layers to compute outputs
  2. Compute output errors at final time step
  3. Backward pass through layers to compute δ signals
  4. Update eλ, eγ, eB sensitivities within each layer
  5. Compute parameter updates using δ and sensitivity terms

- Design tradeoffs:
  - Diagonal vs dense recurrence: Diagonal enables online learning but may limit expressiveness
  - Complex vs real values: Complex enables richer dynamics but requires complex differentiation
  - Layer depth: More layers improve approximation quality but increase computational cost

- Failure signatures:
  - Unstable training: Check λ initialization and normalization
  - Poor performance: Verify module independence assumption holds, check approximation quality
  - Memory issues: Ensure e-updates are implemented efficiently, verify no unnecessary state storage

- First 3 experiments:
  1. Implement single-layer LRU with online learning on copy task (length 20) to verify basic functionality
  2. Compare spatial backpropagation vs truncated BPTT on same task to measure approximation quality
  3. Test multi-layer architecture on longer copy task (length 50) to verify scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed online learning algorithm scale with increasing sequence lengths beyond 1000 time steps?
- Basis in paper: [inferred] Paper demonstrates competitive performance on tasks up to 1000 steps but doesn't explore longer sequences
- Why unresolved: Paper doesn't provide empirical results for sequence lengths exceeding 1000 steps
- What evidence would resolve it: Experimental results on tasks with sequence lengths of 2000 steps or more

### Open Question 2
- Question: What is the impact of using different types of recurrent independent modules (e.g., LSTM cells) on the performance of the online learning algorithm?
- Basis in paper: [explicit] Paper mentions design principles apply to networks of independent recurrent modules with low-dimensional state vectors
- Why unresolved: Paper focuses on linear recurrent units (LRUs) and doesn't explore other types
- What evidence would resolve it: Experimental results comparing different types of independent recurrent modules

### Open Question 3
- Question: How does the proposed online learning algorithm perform on tasks with non-stationary data distributions?
- Basis in paper: [inferred] Paper doesn't discuss algorithm's performance on tasks with changing data distributions
- Why unresolved: Experiments focus on tasks with fixed data distributions
- What evidence would resolve it: Experimental results on tasks with non-stationary data distributions

## Limitations

- Complex number requirement may limit practical adoption since most deep learning frameworks are optimized for real-valued operations
- Computational complexity of maintaining sensitivity terms may become prohibitive for very deep networks
- Initialization sensitivity is not systematically analyzed, particularly for complex-valued parameters

## Confidence

### Mechanism 1: Medium
- Exact gradient computation within independent modules is well-established
- Paper doesn't fully address module independence breakdown during training

### Mechanism 2: Medium
- Diagonal recurrence enables efficient online learning
- Paper lacks systematic analysis of expressiveness limitations compared to dense recurrence

### Mechanism 3: Low-Medium
- Approximation quality of spatial backpropagation is not thoroughly evaluated
- Paper claims competitive performance but doesn't quantify approximation error compared to full BPTT

## Next Checks

1. **Module Independence Robustness**: Test algorithm on tasks requiring coordination between modules to assess how violations of independence assumption affect performance

2. **Approximation Error Analysis**: Implement exact BPTT version and compare gradient alignment and performance to quantify approximation quality of spatial backpropagation approach

3. **Real-valued Alternative Evaluation**: Implement real-valued version of LRU to understand practical tradeoffs of complex number requirement and assess whether benefits justify additional complexity