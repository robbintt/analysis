---
ver: rpa2
title: A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual
  3D Reconstruction
arxiv_id: '2308.08812'
source_url: https://arxiv.org/abs/2308.08812
tags:
- learning
- saliency
- session
- reconstruction
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a continual learning approach for single-image
  3D reconstruction that addresses catastrophic forgetting. The method combines variational
  distribution priors and saliency map replay to retain knowledge of previously seen
  objects while learning new ones.
---

# A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction

## Quick Facts
- **arXiv ID:** 2308.08812
- **Source URL:** https://arxiv.org/abs/2308.08812
- **Reference count:** 40
- **Primary result:** Achieves average IOU of 0.555 across 5 incremental sessions on ShapeNet-13 dataset

## Executive Summary
This paper introduces a continual learning approach for single-image 3D reconstruction that addresses catastrophic forgetting. The method combines variational distribution priors and saliency map replay to retain knowledge of previously seen objects while learning new ones. Variational priors represent abstract shapes to combat forgetting, while saliency maps preserve object attributes with lower memory usage. Experiments on ShapeNet-13 demonstrate competitive performance compared to established baselines.

## Method Summary
The approach combines variational distribution priors with saliency map-based experience replay for continual 3D reconstruction. A 2D encoder (ResNet-18 based) extracts image features, which are combined with variational latent codes to generate 3D voxel reconstructions. The method uses a memory buffer to store saliency maps that capture distinctive object