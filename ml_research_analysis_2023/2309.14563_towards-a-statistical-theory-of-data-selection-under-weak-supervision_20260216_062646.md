---
ver: rpa2
title: Towards a statistical theory of data selection under weak supervision
arxiv_id: '2309.14563'
source_url: https://arxiv.org/abs/2309.14563
tags:
- data
- selection
- surrogate
- subsampling
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a theoretical framework for data selection
  under weak supervision. The authors study the problem of selecting a subset of samples
  from a large unlabeled dataset, using a surrogate model to predict labels.
---

# Towards a statistical theory of data selection under weak supervision

## Quick Facts
- arXiv ID: 2309.14563
- Source URL: https://arxiv.org/abs/2309.14563
- Authors: 
- Reference count: 40
- Key outcome: This paper proposes a theoretical framework for data selection under weak supervision, showing that data selection can outperform training on the full sample, but popular methods like unbiased reweighted subsampling can be substantially suboptimal.

## Executive Summary
This paper develops a statistical theory for data selection under weak supervision, where a surrogate model predicts labels for an unlabeled dataset and a subset is selected for training. The authors theoretically and empirically demonstrate that optimal data selection depends non-trivially on the subsampling fraction, and that biased selection schemes can outperform unbiased methods by increasing the curvature of the risk landscape. Through low- and high-dimensional asymptotic analyses, they provide conditions under which data selection can actually reduce test error below what is achievable with full-sample training.

## Method Summary
The paper studies data selection by first using a surrogate model to predict labels for an unlabeled dataset. A selection probability is assigned to each sample based on properties of the surrogate model's predictions (such as label uncertainty), then n samples are drawn according to these probabilities. The selected subset is then used for training via regularized empirical risk minimization, optionally with importance reweighting. The framework analyzes both low-dimensional asymptotics (where the number of parameters p is fixed as n, N → ∞) and high-dimensional asymptotics (where p/n → c > 0), deriving conditions for optimal selection schemes in each regime.

## Key Results
- Data selection can outperform training on the full sample, with optimal schemes depending non-trivially on the subsampling fraction n/N
- Unbiased reweighted subsampling can be substantially suboptimal compared to biased schemes that increase curvature
- The optimal selection scheme under low-dimensional asymptotics involves selecting samples proportional to the variance of the surrogate model's predictions raised to a power that depends on the subsampling fraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal data selection depends on the subsampling fraction in a non-trivial way
- Mechanism: The selection probability that minimizes estimation error changes with the target sample size because the "optimal" datapoints to select depend on the composition of the selected subset itself
- Core assumption: The statistical properties of the selected subset differ from the full sample, and this difference scales with the subsampling fraction
- Evidence anchors:
  - [abstract] "optimal data selection schemes depend in a non-trivial way on n/N"
  - [section] "We will see that optimal schemes depend in a non-trivial way on n/N"
  - [corpus] No direct corpus evidence found for this specific claim
- Break condition: If the subsampling fraction approaches 1 (selecting nearly all data), then the distinction between optimal and suboptimal schemes may disappear

### Mechanism 2
- Claim: Unbiased subsampling can be substantially suboptimal compared to biased schemes
- Mechanism: Unbiased schemes maintain the same curvature of the risk landscape as full-sample training, while biased schemes can increase curvature, leading to faster convergence rates
- Core assumption: The estimation error is inversely proportional to the curvature of the expected risk
- Evidence anchors:
  - [abstract] "Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling... can be substantially suboptimal"
  - [section] "Unbiased methods do not change the curvature, while biased methods can increase the curvature"
  - [corpus] No direct corpus evidence found for this specific claim
- Break condition: If the data distribution is such that curvature is already maximized or if the loss function is not twice differentiable

### Mechanism 3
- Claim: Data selection can improve generalization even when using a weaker surrogate model
- Mechanism: Selecting a diverse, representative subset can reduce overfitting compared to training on the full dataset, especially in high-dimensional settings where n/p is small
- Core assumption: The full dataset contains redundant or noisy samples that harm generalization, and careful selection can mitigate this
- Evidence anchors:
  - [abstract] "Data selection can be very effective, in particular beating training on the full sample in some cases"
  - [section] "we discover that good data selection... can actually reduce the test error below the one obtained from the full sample"
  - [corpus] No direct corpus evidence found for this specific claim
- Break condition: If the surrogate model is too weak to provide meaningful signal for selection, or if the full dataset is already well-regularized

## Foundational Learning

- Concept: Low-dimensional vs high-dimensional asymptotics
  - Why needed here: The paper uses two different asymptotic regimes to capture complementary aspects of data selection performance
  - Quick check question: In low-dimensional asymptotics, what happens to the parameter dimension p as n, N → ∞?

- Concept: Influence functions and their role in data selection
  - Why needed here: Many existing data selection methods are based on influence functions, which the paper shows can be suboptimal
  - Quick check question: What is the relationship between influence functions and the gradient of the loss with respect to parameters?

- Concept: Bias-variance tradeoff in subsampling
  - Why needed here: The paper demonstrates that introducing bias through non-reweighting schemes can sometimes reduce overall error
  - Quick check question: How does changing the subsampling scheme affect the bias and variance of the resulting estimator?

## Architecture Onboarding

- Component map: Surrogate model -> Selection probabilities -> Sample selection -> Training on selected data -> Evaluation
- Critical path: Surrogate model → Selection probabilities → Sample selection → Training on selected data → Evaluation
- Design tradeoffs: Unbiased vs biased selection (unbiased is simpler but can be suboptimal), reweighting vs non-reweighting (reweighting requires modifying training but can improve performance)
- Failure signatures: Poor surrogate model quality leading to bad selection, selection probabilities that don't sum to target n, or selection that produces too homogeneous a subset
- First 3 experiments:
  1. Compare unbiased vs biased subsampling on a synthetic dataset with known ground truth
  2. Vary the subsampling fraction γ and observe how optimal selection changes
  3. Test data selection with a deliberately weakened surrogate model to see if performance degrades gracefully

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the subsampling fraction n/N and the optimal data selection scheme in low-dimensional asymptotics?
- Basis in paper: [explicit] The paper mentions that "optimal schemes depend in a non-trivial way on n/N" and provides theoretical results on optimal schemes for different subsampling fractions.
- Why unresolved: While the paper provides theoretical insights, the exact relationship and how to determine the optimal scheme for a given n/N remains unclear.
- What evidence would resolve it: Numerical experiments or theoretical results showing the optimal data selection scheme for various subsampling fractions would clarify this relationship.

### Open Question 2
- Question: How does the choice of surrogate model impact the effectiveness of data selection schemes?
- Basis in paper: [explicit] The paper states that "certain popular choices in data selection methods can be substantially suboptimal" and that "unbiased methods are suboptimal under a broad set of conditions."
- Why unresolved: While the paper suggests that the choice of surrogate model is important, the exact impact and how to choose the best surrogate model for a given problem is not clear.
- What evidence would resolve it: Experiments comparing different surrogate models and their impact on data selection effectiveness would provide insights into this question.

### Open Question 3
- Question: What is the role of biased subsampling in high-dimensional settings, and when is it beneficial?
- Basis in paper: [explicit] The paper mentions that "biasing data selection towards hard samples can be suboptimal" and "even when biasing towards hard samples is effective, selecting the top hardest ones can lead to poor behavior at small γ."
- Why unresolved: The paper provides some insights into biased subsampling in high-dimensional settings, but the exact role and when it is beneficial is not fully understood.
- What evidence would resolve it: Further theoretical analysis or numerical experiments exploring the behavior of biased subsampling in high-dimensional settings would help clarify this question.

### Open Question 4
- Question: How does data selection affect the generalization error, and can it outperform training on the full sample?
- Basis in paper: [explicit] The paper states that "data selection can improve generalization" and "learning after data selection can outperform learning on the full sample."
- Why unresolved: While the paper provides some evidence for this phenomenon, the exact mechanisms and conditions under which it occurs are not fully understood.
- What evidence would resolve it: More extensive numerical experiments or theoretical analysis exploring the generalization error of models trained on selected subsets would help clarify this question.

## Limitations
- Theoretical claims rely heavily on quadratic loss approximations that may not hold for practical classification tasks
- Empirical evaluation uses a single dataset (KITTI-360) with specific preprocessing, limiting generalizability
- Analysis assumes the surrogate model is reasonably accurate but doesn't extensively explore robustness to surrogate model quality degradation

## Confidence
- High Confidence: Claims about unbiased subsampling being suboptimal compared to biased schemes (supported by both theory and experiments)
- Medium Confidence: Claims about optimal selection depending on subsampling fraction n/N (strong theoretical backing but limited empirical validation across different regimes)
- Low Confidence: Claims that data selection can outperform full-sample training (based on single experimental result without ablation studies)

## Next Checks
1. **Cross-dataset validation**: Test the optimal data selection scheme on diverse datasets (CIFAR, ImageNet) with different surrogate models to assess robustness and generalizability of the theoretical findings.

2. **Surrogate model quality ablation**: Systematically vary the quality of the surrogate model and measure how this affects the performance gap between optimal and suboptimal selection schemes, particularly examining the proposed benefit of biased subsampling.

3. **Dimensionality regime analysis**: Conduct experiments comparing low-dimensional and high-dimensional asymptotic predictions by varying both n/N and p/n ratios, explicitly testing when theoretical predictions break down in practical settings.