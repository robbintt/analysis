---
ver: rpa2
title: Self-Supervised Learning with Lie Symmetries for Partial Differential Equations
arxiv_id: '2307.05432'
source_url: https://arxiv.org/abs/2307.05432
tags:
- equation
- learning
- group
- symmetry
- burgers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised learning framework for learning
  representations of partial differential equations (PDEs) using Lie point symmetries.
  The key idea is to leverage the mathematical structure of PDEs to design augmentations
  that preserve the underlying dynamics, allowing for the training of encoders that
  produce useful representations.
---

# Self-Supervised Learning with Lie Symmetries for Partial Differential Equations

## Quick Facts
- arXiv ID: 2307.05432
- Source URL: https://arxiv.org/abs/2307.05432
- Reference count: 40
- Primary result: Self-supervised learning framework using Lie symmetries outperforms supervised baselines on PDE tasks like coefficient regression and time-stepping

## Executive Summary
This paper introduces a self-supervised learning framework that leverages Lie point symmetries of partial differential equations to learn meaningful representations. The approach uses theoretically grounded augmentations derived from PDE symmetry groups, combined with VICReg loss, to train encoders without labeled data. The learned representations demonstrate broad utility across multiple downstream tasks including parameter regression and time-stepping for various PDEs such as Burgers', KdV, KS, and Navier-Stokes equations.

## Method Summary
The method pretrains a ResNet18 encoder using VICReg self-supervised loss with Lie symmetry augmentations (translations, Galilean boosts, scaling) and spatial/temporal cropping. The pretrained encoder is then applied to downstream tasks through either a linear regression head or by conditioning a neural operator for time-stepping. The approach is evaluated on Burgers', KdV, KS, and Navier-Stokes equations using datasets with varied parameters and initial conditions.

## Key Results
- Self-supervised pretraining with Lie symmetries outperforms supervised learning baselines on both regression and time-stepping tasks
- Representations learned through this framework are particularly effective when conditioning neural operators for time-stepping
- The approach demonstrates broad applicability across multiple types of PDEs with different symmetry structures

## Why This Works (Mechanism)

### Mechanism 1
Lie point symmetries provide meaningful augmentations for self-supervised learning of PDE representations. PDE symmetry groups preserve the underlying dynamics of the equation, so representations invariant under these symmetries capture the essential structure needed for downstream tasks like parameter regression and time-stepping. The core assumption is that the Lie point symmetries derived from Lie theory are valid and complete for the PDEs considered.

### Mechanism 2
VICReg loss with Lie symmetry augmentations prevents representation collapse while encouraging invariance. The variance-invariance-covariance regularization (VICReg) loss enforces that representations have sufficient variance, are invariant to augmentations, and have uncorrelated dimensions, preventing collapse to trivial solutions. The core assumption is that the VICReg hyperparameters are appropriate for the PDE data distribution.

### Mechanism 3
Pretraining on heterogeneous unlabeled data improves downstream task performance compared to supervised learning. The SSL framework learns representations that capture the underlying dynamics from diverse data, which generalizes better to unseen parameter settings than representations trained only on labeled data for specific tasks. The core assumption is that the unlabeled dataset contains sufficient diversity in parameters and initial conditions to learn meaningful representations.

## Foundational Learning

- **Lie group theory and symmetry groups of differential equations**: Why needed here - Lie point symmetries provide the mathematical foundation for designing augmentations that preserve the underlying PDE dynamics. Quick check - What is the infinitesimal generator of a Lie point symmetry and how is it used to derive the symmetry group operations?

- **Self-supervised learning frameworks (VICReg)**: Why needed here - VICReg provides the loss function that enforces invariance to augmentations while preventing representation collapse. Quick check - How do the variance, invariance, and covariance terms in VICReg work together to create useful representations?

- **Neural operators for time-stepping**: Why needed here - Neural operators are used as the baseline time-stepping models that are improved by conditioning on the learned representations. Quick check - What is the key architectural difference between a standard CNN and a neural operator for solving PDEs?

## Architecture Onboarding

- **Component map**: Encoder (ResNet18) → Projector (smaller MLP) → VICReg loss → frozen encoder → downstream task (linear head or conditioned neural operator)
- **Critical path**: Data augmentation with Lie symmetries → encoder forward pass → projector forward pass → VICReg loss calculation → encoder weight update
- **Design tradeoffs**: Using Lie symmetries as augmentations provides theoretically grounded transformations but requires careful tuning of augmentation strength; cropping provides more robust performance but is less theoretically motivated
- **Failure signatures**: Poor downstream task performance, collapsed representations (low variance), or representations that don't improve with more unlabeled data
- **First 3 experiments**:
  1. Verify Lie symmetry augmentations preserve PDE solutions by applying to known analytical solutions
  2. Train with only cropping augmentations to establish baseline performance
  3. Add one Lie symmetry augmentation at a time to determine which provides the most improvement

## Open Questions the Paper Calls Out

1. What is the theoretical basis for the superior performance of self-supervised learning over supervised learning for PDEs, and can this be formalized? The paper suggests that enforcing similarity between two different views of the same solution forces the network to learn the underlying dynamics, but doesn't provide a rigorous theoretical explanation.

2. How do different types of augmentations beyond Lie point symmetries (e.g., nonlocal symmetries or approximate symmetries like Lie-Backlund symmetries) affect the performance of self-supervised learning for PDEs? The paper mentions these as potential augmentations but doesn't explore their impact experimentally.

3. How can the scope of symmetries available for learning tasks be expanded by finding ways to preserve boundary conditions during augmentation, even approximately? The paper discusses the limitation of implementing group operations with small strengths due to boundary conditions violating symmetries.

## Limitations

- The paper assumes Lie point symmetries are complete and valid for all tested PDEs without extensive validation across different boundary conditions
- The effectiveness of Lie symmetry augmentations versus simpler approaches (like cropping) is not rigorously quantified
- The VICReg hyperparameters are not systematically tuned or analyzed for sensitivity

## Confidence

- **High confidence**: The basic framework architecture (ResNet18 + VICReg + Lie symmetries) is sound and well-established
- **Medium confidence**: The specific Lie symmetry augmentations chosen are optimal for all tested PDEs
- **Medium confidence**: The pretraining datasets are sufficiently diverse to learn generalizable representations

## Next Checks

1. Perform ablation studies systematically removing each Lie symmetry augmentation to quantify their individual contributions to downstream performance
2. Test representation quality on PDEs with different boundary conditions or domain restrictions not seen during pretraining
3. Analyze the learned representations using techniques like PCA or t-SNE to verify they capture meaningful dynamical structure rather than just memorizing training patterns