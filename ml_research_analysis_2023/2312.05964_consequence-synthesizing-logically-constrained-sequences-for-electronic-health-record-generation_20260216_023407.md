---
ver: rpa2
title: 'ConSequence: Synthesizing Logically Constrained Sequences for Electronic Health
  Record Generation'
arxiv_id: '2312.05964'
source_url: https://arxiv.org/abs/2312.05964
tags:
- rule
- time
- generation
- rules
- consequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConSequence is a method for generating logically constrained sequential
  data, such as electronic health records, that ensures full temporal and spatial
  constraint satisfaction. It introduces a rule-based formulation with temporal aggregation
  and antecedent evaluation modules, efficiently enforced through matrix multiplication
  to achieve high computational efficiency and scalability.
---

# ConSequence: Synthesizing Logically Constrained Sequences for Electronic Health Record Generation

## Quick Facts
- arXiv ID: 2312.05964
- Source URL: https://arxiv.org/abs/2312.05964
- Reference count: 17
- Primary result: Achieves zero rule violations while improving model quality with minimal performance overhead

## Executive Summary
ConSequence is a novel method for generating logically constrained sequential data, specifically designed for electronic health records (EHRs). It addresses the challenge of ensuring full temporal and spatial constraint satisfaction during generation by introducing a rule-based formulation with temporal aggregation and antecedent evaluation modules. The method achieves computational efficiency through an innovative matrix multiplication approach that processes multiple rules in parallel. Experimental results demonstrate that ConSequence outperforms existing constraint enforcement approaches by achieving complete logical consistency without compromising runtime performance or generative quality.

## Method Summary
ConSequence integrates temporal aggregation and antecedent evaluation modules into neural network outputs using matrix multiplication for parallel constraint application. The method processes patient record sequences through a temporal aggregator that creates fixed representations for each rule, then uses rule neurons to evaluate constraints deterministically. Rules are grouped by temporal components and processed simultaneously across time steps and batches using efficient matrix operations. This design enables seamless incorporation into existing generative models while ensuring hard and soft logical constraints are satisfied across all time steps.

## Key Results
- Achieves zero rule violations while improving model quality with 5% reduction in test perplexity
- Incurs less than 13% slowdown in generation speed compared to unconstrained models
- Outperforms existing constraint enforcement approaches in achieving complete logical consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ConSequence achieves full logical consistency by using a rule neuron that evaluates antecedents and fires only when all required conditions are met.
- Mechanism: The rule neuron uses weighted inputs where positive literals have weight +1 and negative literals have weight -1. Activation occurs when the sum of weighted inputs meets or exceeds the threshold (number of positive literals).
- Core assumption: Logical constraints can be expressed as conjunctive implicative rules and evaluated deterministically through neuron activation.
- Evidence anchors:
  - [abstract] "Our rule-based formulation includes temporal aggregation and antecedent evaluation modules, ensured by an efficient matrix multiplication formulation"
  - [section] "The rule neuron accepts inputs from the variables in x(r)t and is constructed as follows: we denote the weight of the connection between the neuron and the i-th variable in x(r)t as w(r)i and set it to 1 if the variable is in L and -1 if its negation is in L"

### Mechanism 2
- Claim: ConSequence efficiently handles temporal constraints through attentive history aggregation that compiles past visit information.
- Mechanism: Temporal aggregation uses a binary mask vector to identify relevant past time steps, then combines those visits using boolean OR operations to create a fixed representation for each rule at each time step.
- Core assumption: Past visit information can be compressed into fixed representations without losing critical information needed for constraint evaluation.
- Evidence anchors:
  - [abstract] "Our rule-based formulation includes temporal aggregation and antecedent evaluation modules"
  - [section] "We employ an attention-based temporal aggregator for effectively consolidating historical data at each time step, supporting both absolute and relative aggregation while accommodating sequence variations"

### Mechanism 3
- Claim: ConSequence achieves computational efficiency through parallel constraint application using matrix multiplication.
- Mechanism: Rules are grouped by temporal components, then processed simultaneously across time steps and batches using matrix operations (H(g) = M(g)P and S(g) = X(g)W(g)).
- Core assumption: Matrix multiplication can effectively represent rule evaluation and constraint enforcement without sacrificing accuracy.
- Evidence anchors:
  - [abstract] "ensured by an efficient matrix multiplication formulation, to satisfy hard and soft logical constraints across time steps"
  - [section] "We represent the mentioned components as weight matrices. These are then seamlessly incorporated as a constraint head in neural network models, processing many rules and records in parallel"

## Foundational Learning

- Concept: Temporal aggregation and history representation
  - Why needed here: EHR data spans multiple visits over time, requiring historical context for meaningful constraint evaluation
  - Quick check question: How would you modify the aggregation if you needed to weight recent visits more heavily than older ones?

- Concept: Conjunctive Implicative Form for logical constraints
  - Why needed here: Provides a concise, computationally efficient way to express complex domain knowledge without losing representational power
  - Quick check question: Can you convert a constraint expressed as (A AND B) OR C into Conjunctive Implicative Form?

- Concept: Matrix multiplication for parallel rule evaluation
  - Why needed here: Enables efficient processing of multiple rules across many records simultaneously, crucial for real-world scalability
  - Quick check question: What would be the memory complexity of storing the weight matrices for all rules if you had 1000 rules and 10000 possible codes?

## Architecture Onboarding

- Component map: Input (Patient record sequences P matrix) -> Temporal Aggregation (Creates H(g) matrices) -> Antecedent Evaluation (Rule neurons W(g) matrices) -> Output (Modified patient records)
- Critical path: Temporal aggregation → Rule neuron construction → Matrix multiplication → Output modification
- Design tradeoffs:
  - Memory vs. Speed: Larger batch sizes improve speed but require more memory
  - Constraint Expressiveness vs. Efficiency: More complex rules may require more computational resources
  - Static vs. Temporal rules: Temporal rules need history representation, adding complexity
- Failure signatures:
  - Rules not being enforced: Check temporal aggregation mask and rule neuron construction
  - Slow performance: Verify matrix multiplication efficiency and batch size appropriateness
  - Distribution shift in generated data: Examine how constraint enforcement modifies the underlying model outputs
- First 3 experiments:
  1. Test temporal aggregation with a simple rule requiring a code to appear in consecutive visits
  2. Verify rule neuron activation by creating a rule with known truth values and checking if it fires correctly
  3. Benchmark matrix multiplication formulation against sequential rule evaluation on a small dataset

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Relies heavily on the assumption that logical constraints can be effectively expressed in Conjunctive Implicative Form, which may not capture all real-world domain knowledge
- Performance gains were tested primarily on MIMIC-III and claims datasets, limiting generalizability to other domains
- Rule generation process and threshold selection for constraint identification were not fully detailed, making it difficult to assess robustness across different datasets

## Confidence

**High Confidence:** The computational efficiency of the matrix multiplication formulation (claim about <13% slowdown) is well-supported by the described architecture and aligns with standard deep learning optimization principles.

**Medium Confidence:** The 5% reduction in test perplexity and improved model quality are supported by experimental results, but the evaluation setup lacks detail about baseline model tuning and hyperparameter optimization.

**Low Confidence:** The generalizability of the approach to domains beyond healthcare remains uncertain, as the evaluation focused exclusively on EHR data.

## Next Checks

1. **Temporal Aggregation Robustness Test:** Evaluate how the method handles varying lengths of historical context by systematically testing on datasets with different temporal patterns and measuring constraint satisfaction rates as history length increases.

2. **Cross-Domain Generalization:** Apply ConSequence to a non-healthcare sequential generation task (such as financial transactions or user behavior sequences) and compare constraint satisfaction and computational efficiency against domain-specific baselines.

3. **Rule Complexity Scaling Analysis:** Generate increasingly complex rule sets (varying the number of conjuncts, nested temporal dependencies, and inter-rule conflicts) and measure how computational time, memory usage, and constraint satisfaction rates scale with rule complexity.