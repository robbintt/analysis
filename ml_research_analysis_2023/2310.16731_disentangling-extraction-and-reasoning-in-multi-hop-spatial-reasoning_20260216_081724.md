---
ver: rpa2
title: Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning
arxiv_id: '2310.16731'
source_url: https://arxiv.org/abs/2310.16731
tags:
- spatial
- reasoning
- extraction
- triangle
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies disentangling extraction and reasoning in multi-hop
  spatial reasoning tasks, such as answering questions about spatial relations in
  text. The authors propose a pipeline model that first extracts spatial information
  from text using neural modules, then applies symbolic reasoning rules to infer implicit
  spatial relations.
---

# Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning

## Quick Facts
- arXiv ID: 2310.16731
- Source URL: https://arxiv.org/abs/2310.16731
- Authors: 
- Reference count: 22
- Key outcome: Disentangling extraction and reasoning improves performance in spatial question answering tasks, with a pipeline model excelling in controlled environments and an end-to-end neural model performing better in realistic domains.

## Executive Summary
This paper addresses the challenge of multi-hop spatial reasoning in question answering tasks. The authors propose two main approaches: a pipeline model (PISTAQ) that separates spatial information extraction from symbolic reasoning, and an end-to-end neural model (SREQA) with explicit extraction and reasoning layers. The study demonstrates that disentangling extraction and reasoning can significantly improve performance in controlled environments, while the end-to-end approach shows advantages in more realistic, ambiguous domains. The research highlights the importance of leveraging spatial information supervision effectively and explores the use of large language models for information extraction tasks.

## Method Summary
The paper presents two main approaches for spatial question answering. PISTAQ is a pipeline model that first extracts spatial roles and relations using neural modules (SPRL and coreference resolution), then applies a symbolic reasoner to infer implicit spatial relations. SREQA is an end-to-end neural model with separate layers for entity extraction, relation extraction, and reasoning using binary classifiers for each relation type. The authors also explore using large language models (LLMs) like GPT3.5 for information extraction in PISTAQ. Experiments are conducted on various spatial QA datasets, including controlled synthetic data and more realistic human-generated data, to evaluate the performance of these approaches in different environments.

## Key Results
- PISTAQ outperforms existing state-of-the-art models on controlled environment benchmarks (SPARTQA-TOY and SPARTUN).
- SREQA surpasses PLMs trained on QA and QA+SPRL annotation, showcasing the advantage of explicit extraction layers in utilizing QA and SPRL data.
- Using LLMs as extraction modules in PISTAQ can solve complex questions that specialized extraction modules struggle with, particularly in human-generated datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling extraction and reasoning improves deterministic spatial reasoning in controlled environments.
- Mechanism: Separate neural modules extract spatial roles and relations, then a symbolic reasoner applies deterministic rules to infer implicit relations.
- Core assumption: Spatial relations can be accurately extracted and mapped to a symbolic representation amenable to rule-based reasoning.
- Evidence anchors:
  - [abstract]: "Results highlight the benefits of disentangling extraction and reasoning for deterministic spatial reasoning"
  - [section]: "PISTAQ outperforms existing SOTA models by a significant margin on benchmarks with a controlled environment (toy tasks)"
  - [corpus]: Weak - corpus neighbors focus on spatial reasoning but do not directly support disentangling extraction and reasoning as a mechanism.
- Break condition: Extraction errors or ambiguity in natural language prevent accurate symbolic representation.

### Mechanism 2
- Claim: Explicit neural layers for extraction and reasoning improve generalization in realistic domains.
- Mechanism: End-to-end neural model with separate layers for entity extraction, relation extraction, and reasoning using binary classifiers for each relation type.
- Core assumption: The model can learn to extract relevant spatial information and apply learned reasoning rules even with ambiguous natural language.
- Evidence anchors:
  - [abstract]: "Results highlight the benefits of disentangling extraction and reasoning for deterministic spatial reasoning and leveraging spatial information supervision effectively"
  - [section]: "SREQA * surpasses the PLMs trained on QA and QA+SPRL annotation, showcasing the advantage of the design of this model in utilizing QA and SPRL data within explicit extraction layers"
  - [corpus]: Weak - corpus neighbors focus on spatial reasoning but do not directly support explicit neural layers for extraction and reasoning.
- Break condition: Insufficient supervision or model capacity to learn complex reasoning rules.

### Mechanism 3
- Claim: Using LLMs for information extraction mitigates errors from specialized extraction modules.
- Mechanism: LLMs extract entities, relations, and coreferences from text, which are then used by a symbolic reasoner to answer questions.
- Core assumption: LLMs can accurately extract spatial information from natural language, even when specialized modules struggle.
- Evidence anchors:
  - [abstract]: "leveraging language models for information extraction tasks and emphasize the importance of explicit reasoning modules"
  - [section]: "PISTAQ using LLM as extraction modules can solve all of these 7 questions"
  - [corpus]: Weak - corpus neighbors focus on spatial reasoning but do not directly support using LLMs for information extraction.
- Break condition: LLMs fail to extract relevant information or introduce errors that propagate to the reasoning module.

## Foundational Learning

- Concept: Spatial Role Labeling (SPRL)
  - Why needed here: Accurately extracting spatial roles (Trajector, Landmark, Spatial Indicator) is crucial for representing spatial relations.
  - Quick check question: Can you identify the Trajector, Landmark, and Spatial Indicator in the sentence "The book is on the table"?

- Concept: Coreference Resolution
  - Why needed here: Linking different mentions of the same entity is essential for connecting extracted spatial relations.
  - Quick check question: In the text "John walked his dog. He was happy.", what does "He" refer to?

- Concept: Logic Programming for Spatial Reasoning
  - Why needed here: Applying deterministic rules to infer implicit spatial relations requires a formal representation and reasoning system.
  - Quick check question: Given the facts "In front of the house is a tree" and "Behind the tree is a car", what is the implicit relation between the house and the car?

## Architecture Onboarding

- Component map: PISTAQ (Pipeline) - SPRL extraction, Coreference Resolution, Symbolic Reasoner. SREQA - BERT for entity/relation extraction, Binary classifiers for reasoning. LLM-based extraction - GPT3.5-Turbo for extraction, Symbolic Reasoner for reasoning.
- Critical path: Text -> SPRL/Coref extraction -> Symbolic Reasoning (PISTAQ) OR Text -> BERT extraction -> Binary classifiers (SREQA) OR Text -> LLM extraction -> Symbolic Reasoning (LLM-based).
- Design tradeoffs: PISTAQ offers high accuracy in controlled environments but struggles with ambiguity. SREQA generalizes better but may be less accurate. LLM-based extraction is flexible but requires careful prompt engineering.
- Failure signatures: Incorrect answers due to extraction errors, reasoning errors, or both. Low performance on datasets with high ambiguity or missing commonsense knowledge.
- First 3 experiments:
  1. Evaluate PISTAQ on a synthetic dataset with ground truth SPRL annotations to isolate the impact of the reasoning module.
  2. Train SREQA on a subset of SPARTUN and evaluate on SPARTQA-HUMAN to assess generalization.
  3. Compare LLM-based extraction with specialized modules on a dataset with high natural language ambiguity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the disentangled extraction and reasoning models compare to end-to-end models when spatial commonsense knowledge is required?
- Basis in paper: [explicit] The paper mentions that RESQ questions often require spatial commonsense information that cannot be answered solely based on the given relations in the stories. The low performance of PISTAQ on RESQ is attributed to the absence of integrating commonsense information in this model.
- Why unresolved: The paper does not provide experimental results comparing the disentangled models with end-to-end models on questions that require spatial commonsense knowledge. It only mentions the limitations of PISTAQ on such questions.
- What evidence would resolve it: Experimental results comparing the performance of disentangled extraction and reasoning models with end-to-end models on questions that require spatial commonsense knowledge, using datasets like RESQ where such knowledge is needed.

### Open Question 2
- Question: How does the performance of using LLMs as extraction modules in PISTAQ compare to using trained BERT-based SPRL modules?
- Basis in paper: [explicit] The paper mentions using GPT3.5-Turbo with few-shot prompting to extract information from SPARTQA-HUMAN and RESQ examples in PISTAQ. It states that GPT3.5 extracts more accurate information, leading to correct answers compared to the trained BERT-based SPRL extraction modules.
- Why unresolved: The paper does not provide detailed experimental results comparing the performance of PISTAQ using LLMs as extraction modules versus trained BERT-based SPRL modules. It only mentions the potential benefits based on limited analysis.
- What evidence would resolve it: Detailed experimental results comparing the performance of PISTAQ using LLMs as extraction modules versus trained BERT-based SPRL modules on various SQA datasets, including human-generated and synthetic datasets.

### Open Question 3
- Question: How does the performance of the disentangled extraction and reasoning models scale with increasing complexity of spatial reasoning tasks?
- Basis in paper: [inferred] The paper discusses the benefits of disentangling extraction and reasoning in multi-hop spatial reasoning tasks. It shows that disentangling extraction and symbolic reasoning enhances models' reasoning capabilities compared to end-to-end models in controlled experimental conditions.
- Why unresolved: The paper does not provide experimental results demonstrating how the performance of disentangled extraction and reasoning models scales with increasing complexity of spatial reasoning tasks. It only shows results on controlled and realistic environments.
- What evidence would resolve it: Experimental results demonstrating the performance of disentangled extraction and reasoning models on increasingly complex spatial reasoning tasks, including tasks with more entities, relations, and reasoning steps, to show how well the models scale.

## Limitations

- The controlled experiments may not fully represent the complexity of real-world spatial reasoning tasks.
- The symbolic reasoning rules may not capture the full range of spatial commonsense knowledge required for more nuanced questions.
- The reliance on high-quality SPRL and coreference annotations could limit the applicability of these methods to domains where such annotations are unavailable or difficult to obtain.

## Confidence

- **High Confidence:** The disentanglement of extraction and reasoning improves performance in controlled environments (Mechanism 1). The paper provides strong experimental evidence from SPARTQA-TOY and SPARTUN datasets.
- **Medium Confidence:** Explicit neural layers for extraction and reasoning improve generalization in realistic domains (Mechanism 2). While the results on SPARTQA-HUMAN are promising, the performance gap with PISTAQ is smaller than in controlled settings.
- **Low Confidence:** Using LLMs for information extraction mitigates errors from specialized extraction modules (Mechanism 3). The paper only provides preliminary results using GPT3.5, and further experiments with different LLMs and datasets are needed to validate this claim.

## Next Checks

1. Evaluate the models on a diverse set of real-world spatial reasoning tasks, such as navigation instructions or geographic information system (GIS) queries, to assess their performance beyond the controlled benchmarks.
2. Conduct an ablation study by removing individual reasoning rules from the symbolic reasoner and measuring the impact on performance. This will help identify the most critical rules and guide future rule development.
3. Investigate the models' performance when trained on noisy or incomplete SPRL and coreference annotations. This will provide insights into the importance of annotation quality and the models' ability to handle imperfect supervision.