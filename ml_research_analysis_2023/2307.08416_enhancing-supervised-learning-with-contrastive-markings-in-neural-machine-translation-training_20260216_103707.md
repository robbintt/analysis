---
ver: rpa2
title: Enhancing Supervised Learning with Contrastive Markings in Neural Machine Translation
  Training
arxiv_id: '2307.08416'
source_url: https://arxiv.org/abs/2307.08416
tags:
- learning
- markings
- system
- postedits
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the exposure bias problem in neural machine
  translation (NMT) caused by teacher forcing, where models are trained on reference
  tokens rather than their own predictions. The proposed solution extends maximum
  likelihood estimation with a contrastive marking objective that provides fine-grained
  token-level feedback by comparing model hypotheses against references or postedits.
---

# Enhancing Supervised Learning with Contrastive Markings in Neural Machine Translation Training

## Quick Facts
- **arXiv ID**: 2307.08416
- **Source URL**: https://arxiv.org/abs/2307.08416
- **Reference count**: 19
- **Primary result**: TER improvements of up to 3.5 points on IWSLT14 En-De when combining teacher forcing with online contrastive markings

## Executive Summary
This paper addresses the exposure bias problem in neural machine translation caused by teacher forcing, where models are trained on reference tokens rather than their own predictions. The authors propose extending maximum likelihood estimation with a contrastive marking objective that provides fine-grained token-level feedback by comparing model hypotheses against references or postedits. The method requires one additional translation pass per epoch to generate error markings via longest common subsequence calculations. Experiments show TER improvements of up to 3.5 points on IWSLT14 En-De and up to 1 TER point on WMT21 APE when learning from postedits via knowledge distillation of legacy MT outputs.

## Method Summary
The method extends standard supervised learning by computing token-level contrastive markings through longest common subsequence (LCS) comparison between model hypotheses and references/postedits. After each epoch, the model generates new hypotheses for the training data, and markings are extracted by comparing these hypotheses to references (for independent translations) or postedits (for human corrections to MT outputs). These markings are integrated into the loss function as weights, allowing the model to reinforce correct predictions and down-weight incorrect ones at the token level. The approach uses an interpolated objective combining postedit likelihood with marking likelihood, with the interpolation weight α tuned per dataset.

## Key Results
- TER improvements of up to 3.5 points on IWSLT14 En-De when using contrastive markings with teacher forcing
- Up to 1 TER point improvement on WMT21 APE dataset when learning from postedits
- Larger gains observed when learning from postedits compared to independent references
- Best results achieved with interpolation weight α=0.5 balancing postedit likelihood and marking likelihood

## Why This Works (Mechanism)

### Mechanism 1
Token-level contrastive markings provide more granular feedback than sequence-level rewards, enabling targeted weight updates for correct and incorrect subword tokens. By marking tokens as correct (1) or incorrect (0) based on their presence in the longest common subsequence between hypothesis and reference, the model receives fine-grained supervision signals. These markings are integrated into the loss function as weights, allowing the model to reinforce correct predictions and down-weight incorrect ones at the token level. Core assumption: Token-level feedback is more informative than sequence-level feedback for correcting specific translation errors, particularly for insertion and substitution errors that contribute to TER.

### Mechanism 2
Online computation of error markings after each epoch prevents divergence and maintains relevance of feedback as model predictions improve. Instead of using static markings computed once from initial translations, the system generates new hypotheses and computes fresh markings after each epoch. This ensures that the feedback remains aligned with the model's current capabilities and prevents the model from overfitting to outdated error signals. Core assumption: Static error markings become less relevant as the model improves, potentially causing the model to learn from incorrect feedback signals that no longer match its current performance level.

### Mechanism 3
Learning from postedits provides more relevant feedback than learning from independent reference translations because postedits indicate actual human corrections to machine translation errors rather than differences between independent translations. When the model learns from postedits (human corrections to MT outputs), the contrastive markings highlight where the model's predictions deviate from human-verified correct translations. This is more targeted than learning from references created independently, where differences might reflect style or preference rather than errors. Core assumption: Postedits are more closely aligned with the model's error patterns than independent references, making the contrastive markings more informative for correcting specific types of errors.

## Foundational Learning

- **Concept: Teacher forcing vs. scheduled sampling**
  - Why needed here: Understanding the exposure bias problem requires knowing that teacher forcing uses reference tokens as context during training, while scheduled sampling gradually introduces the model's own predictions. This paper addresses exposure bias through a different mechanism (contrastive markings) rather than scheduled sampling.
  - Quick check question: What is the key difference between teacher forcing and scheduled sampling in terms of the conditioning context used during training?

- **Concept: Longest common subsequence (LCS) algorithm**
  - Why needed here: The error markings are computed by finding the LCS between the hypothesis and reference, then marking tokens in the LCS as correct and others as incorrect. Understanding LCS is crucial for implementing the marking extraction process.
  - Quick check question: How does the LCS algorithm determine which tokens to mark as correct when comparing a hypothesis to a reference?

- **Concept: Knowledge distillation**
  - Why needed here: The paper uses knowledge distillation to emulate a legacy MT system whose outputs were postedited, since the original system is unavailable. This allows the model to learn from postedits as feedback on its own outputs rather than independent references.
  - Quick check question: What is the purpose of using knowledge distillation in this work, and how does it relate to the postedit learning setup?

## Architecture Onboarding

- **Component map**: Pre-trained baseline Transformer model -> Data loader with combined batches -> LCS-based marking extraction module -> Weighted cross-entropy loss function -> Training loop with online marking updates

- **Critical path**:
  1. Pre-train baseline model on WMT17 data
  2. Fine-tune on either IWSLT14 references or APE postedits
  3. After each epoch, generate new hypotheses for training data
  4. Compute LCS-based markings between hypotheses and references/postedits
  5. Train with weighted combination of postedit likelihood and marking likelihood
  6. Evaluate TER on test sets

- **Design tradeoffs**:
  - Online vs. static markings: Online markings prevent feedback staleness but add computational overhead of one additional translation pass per epoch
  - Weight values for markings: The paper uses -0.5 for incorrect tokens and 0.5 for correct tokens, but this could be tuned
  - Interpolation weight α: Balancing between postedit likelihood and marking likelihood requires tuning per dataset
  - Knowledge distillation fidelity: The effectiveness depends on how well the distilled model approximates the legacy system

- **Failure signatures**:
  - TER scores plateau or degrade after initial improvements (suggests markings becoming noisy or counterproductive)
  - Training loss diverges when using online markings (suggests weighting or marking extraction issues)
  - No improvement over baseline on postedit data (suggests postedits not well-aligned with model errors)
  - Significant computational overhead without corresponding gains (suggests online marking cost outweighs benefits)

- **First 3 experiments**:
  1. Baseline fine-tuning on IWSLT14 references only, compare TER to pre-trained model
  2. Fine-tuning on IWSLT14 with contrastive markings (α=0.5), compare TER improvement over baseline
  3. Fine-tuning on APE postedits with and without contrastive markings, compare TER and analyze which error types are corrected

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed contrastive marking approach compare to other exposure bias mitigation techniques like scheduled sampling, minimum risk training, or reinforcement learning in terms of final translation quality?
- **Basis in paper**: The paper discusses various exposure bias mitigation techniques in the related work section and mentions that most approaches use sequence-level rewards from automatic metrics, while their method provides token-level feedback through contrastive markings.
- **Why unresolved**: The paper only compares their method to standard supervised learning and doesn't benchmark against other exposure bias mitigation techniques using the same datasets and evaluation metrics.
- **What evidence would resolve it**: Direct comparison experiments between contrastive markings and scheduled sampling, minimum risk training, and reinforcement learning on the same datasets (IWSLT14 En-De and WMT21 APE) using identical evaluation metrics.

### Open Question 2
- **Question**: What is the optimal interpolation weight α between the postedits loss and contrastive markings loss, and how does it vary across different domains and datasets?
- **Basis in paper**: The paper experiments with different interpolation weights (α) on the IWSLT14 dataset and finds that increasing the weight given to online markings improves TER scores up to a threshold, but doesn't systematically explore the optimal values across different datasets.
- **Why unresolved**: The paper only tests a limited range of interpolation weights on two datasets and doesn't provide a principled method for determining optimal weights or analyze how they might vary with domain, dataset size, or quality of postedits.
- **What evidence would resolve it**: Systematic experiments varying α across a wider range of values and datasets, along with analysis of how optimal weights correlate with dataset characteristics like domain, size, and quality of postedits.

### Open Question 3
- **Question**: How does the computational overhead of the online contrastive marking approach scale with dataset size, and what are the practical limitations for very large-scale training scenarios?
- **Basis in paper**: The paper states that the approach requires one additional translation pass over the training set per epoch, but doesn't analyze how this overhead scales with dataset size or discuss practical limitations for large-scale scenarios.
- **Why unresolved**: While the paper mentions the computational cost, it doesn't provide detailed analysis of scaling behavior, memory requirements, or practical limitations when applying this approach to very large datasets typical in industrial settings.
- **What evidence would resolve it**: Empirical analysis showing computational overhead as a function of dataset size, memory requirements for storing intermediate results, and practical experiments demonstrating performance on datasets of varying scales (from small to very large).

## Limitations

- **Token-Level Marking Sensitivity**: The effectiveness depends critically on accurate token-level error identification through LCS, but the paper doesn't explore whether more nuanced marking strategies could yield better results.
- **Dataset Specificity**: The reported TER improvements are based on specific datasets (IWSLT14 En-De and WMT21 APE) without establishing whether the method generalizes to other language pairs or domain-specific translation tasks.
- **Computational Overhead**: The approach requires one additional translation pass per epoch, but the paper doesn't quantify the computational cost or compare the cost-benefit ratio against alternative approaches.

## Confidence

- **High Confidence**: The basic mechanism of using token-level contrastive markings to provide fine-grained feedback is sound and well-supported by the TER improvements on two different datasets.
- **Medium Confidence**: The specific claims about TER improvements (up to 3.5 points on IWSLT14, up to 1 point on APE) are based on experimental results, but lack statistical significance tests or ablation studies.
- **Low Confidence**: The claim that learning from postedits provides significantly better feedback than learning from independent references is based on a single dataset comparison without establishing whether this advantage holds across different error types or language pairs.

## Next Checks

1. **Ablation Study on Marking Strategies**: Systematically test different marking schemes (varying weight values, using confidence scores instead of binary markings, different marking algorithms) to determine whether the specific -0.5/+0.5 scheme is optimal.

2. **Statistical Significance Testing**: Conduct paired significance tests (e.g., bootstrap resampling) on TER scores across multiple runs to establish whether the reported improvements are statistically significant.

3. **Cross-Dataset Generalization**: Apply the method to additional datasets with different characteristics (different language pairs, domains, or quality levels) to verify whether the TER improvements generalize beyond the IWSLT14 and WMT21 APE datasets used in the experiments.