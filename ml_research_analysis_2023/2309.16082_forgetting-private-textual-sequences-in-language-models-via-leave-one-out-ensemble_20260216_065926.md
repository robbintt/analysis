---
ver: rpa2
title: Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble
arxiv_id: '2309.16082'
source_url: https://arxiv.org/abs/2309.16082
tags:
- training
- data
- canaries
- corpus
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a leave-one-out ensemble method to forget
  targeted private token sequences in language models without retraining from scratch.
  The approach trains multiple teacher models on disjoint user data subsets, then
  fine-tunes the base model using aggregated predictions from teachers excluding the
  one trained on data containing the target sequence.
---

# Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble

## Quick Facts
- **arXiv ID**: 2309.16082
- **Source URL**: https://arxiv.org/abs/2309.16082
- **Authors**: [Not specified in input]
- **Reference count**: 0
- **Primary result**: Achieves better privacy-utility trade-offs than baseline approaches, successfully forgetting canaries while maintaining perplexity and ASR performance at less than 0.1% of retraining cost

## Executive Summary
This paper introduces a leave-one-out ensemble method to forget targeted private token sequences in language models without retraining from scratch. The approach trains multiple teacher models on disjoint user data subsets, then fine-tunes the base model using aggregated predictions from teachers excluding the one trained on data containing the target sequence. Experiments on LibriSpeech and WikiText-103 datasets show this method achieves better privacy-utility trade-offs than baseline approaches, successfully forgetting canaries while maintaining perplexity and ASR performance. The method is computationally efficient compared to full retraining, requiring less than 0.1% of the cost for fine-tuning.

## Method Summary
The method works by first training a base language model on the full dataset containing sensitive sequences (canaries). Multiple teacher models are then trained on disjoint subsets of the user data. When forgetting a specific targeted sequence, the teacher that was trained on the data containing that sequence is excluded from the ensemble. The remaining teachers' predictions are aggregated to provide soft supervision during fine-tuning of the base model. This fine-tuning uses KL divergence loss to align the base model's predictions with the aggregated teacher outputs, but only for the targeted sequences that need to be forgotten. Optionally, Gaussian noise can be added to the aggregated teacher predictions to further mask sensitive information.

## Key Results
- Achieves better privacy-utility trade-offs than baseline approaches (re-training, PATE, GA)
- Successfully forgets synthetic canaries while maintaining perplexity and ASR performance
- Computationally efficient at less than 0.1% of the cost compared to full retraining
- Effectively reduces canary detection rates using beam search and random sampling methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The leave-one-out ensemble method masks sensitive sequences by excluding the teacher trained on the user's data and averaging predictions from remaining teachers.
- Mechanism: For each targeted sequence, the model excludes the teacher that was trained on data containing the sequence, then aggregates predictions from other teachers to provide supervision during fine-tuning. This creates a soft label that doesn't reflect the sensitive information.
- Core assumption: The remaining teachers' aggregated predictions approximate what a fully retrained model would produce without the sensitive data.
- Evidence anchors:
  - [abstract] "for each targeted sequence to be removed, we exclude the teacher trained on the set containing this sequence and aggregate the predictions from remaining teachers to provide supervision during fine-tuning"
  - [section] "for each targeted sequence, we leave out the teacher that was trained on the set including this specific sequence, and then aggregate the predictions of remaining teachers to provide supervision during fine-tuning"
- Break condition: If the excluded teacher's predictions are substantially different from the ensemble, the approximation breaks down, leading to incomplete forgetting.

### Mechanism 2
- Claim: The teacher-student framework allows fine-tuning without full retraining by using aggregated teacher predictions as soft labels.
- Mechanism: The base model is fine-tuned using Kullback-Leibler divergence between the aggregated teacher predictions and the student model's predictions, but only for the targeted sequences that need to be forgotten.
- Core assumption: Fine-tuning on the loss function specified in equations 3-4 effectively re-calibrates the model's probabilities for the sensitive sequences.
- Evidence anchors:
  - [abstract] "aggregate the predictions from teachers to provide supervision during fine-tuning"
  - [section] "Given the corpus Dforget to be forgotten, we fine-tune the original LM θbase on this corpus using the loss above"
- Break condition: If the fine-tuning process overfits to the sensitive sequences or if the learning rate is too high/low, the forgetting may be incomplete or cause excessive degradation in general performance.

### Mechanism 3
- Claim: Adding Gaussian noise to teacher predictions further masks sensitive sequences while maintaining utility.
- Mechanism: Random noise sampled from N(0, σ²) is added to each coordinate of the predicted probabilities from teacher models, followed by renormalization over the vocabulary space.
- Core assumption: The added noise sufficiently masks the presence of sensitive sequences without degrading the model's ability to handle general language tasks.
- Evidence anchors:
  - [abstract] "random noises can be added to the aggregated outputs from teacher LMs so that they can further mask the presence of sensitive sequences"
  - [section] "the teacher supervision LKLt(θ) in (4) can be adjusted as DKL(s(gLOO-E−k(·|w1:t−1) + N(0, σ2)) ||pθ(·|w1:t−1))"
- Break condition: If the noise magnitude (σ) is too large, general language modeling performance degrades; if too small, sensitive sequences may still be recoverable.

## Foundational Learning

- Concept: Teacher-Student Knowledge Distillation
  - Why needed here: The method relies on transferring knowledge from multiple teacher models to a student model through soft labels, which is essential for the leave-one-out approach to work.
  - Quick check question: Can you explain how the KL divergence loss in equation 4 ensures the student model learns from teacher predictions while only focusing on targeted sequences?

- Concept: Ensemble Methods and Aggregation
  - Why needed here: The core innovation depends on properly aggregating predictions from multiple models while excluding one specific teacher's contribution.
  - Quick check question: What would happen to the aggregated predictions if all teachers were included instead of using leave-one-out?

- Concept: Differential Privacy and Noise Addition
  - Why needed here: The optional Gaussian noise addition is based on differential privacy principles to further protect sensitive information.
  - Quick check question: How does the choice of σ in the Gaussian noise addition affect the privacy-utility tradeoff?

## Architecture Onboarding

- Component map:
  Base Language Model (θbase) -> Teacher Models (θ1...θM) -> Aggregation Mechanism -> Student Model -> Loss Function (KL divergence)

- Critical path:
  1. Train base LM on full dataset
  2. Partition data by user and train M teacher models on disjoint subsets
  3. When forgetting request received, identify which teacher contains the sensitive data
  4. Aggregate predictions from all teachers except the one containing sensitive data
  5. Fine-tune base model using KL divergence loss only on targeted sequences
  6. Deploy updated student model

- Design tradeoffs:
  - Computational cost vs privacy: More teachers provide better privacy but increase training overhead
  - Granularity vs complexity: Finer partitioning of user data improves privacy but requires more teachers
  - Noise magnitude vs utility: Larger noise values improve privacy but may degrade performance

- Failure signatures:
  - High perplexity increase on general text indicates over-forgetting or poor fine-tuning
  - Detection of canaries by beam search or random sampling indicates incomplete forgetting
  - Large discrepancy between base and student model performance suggests improper fine-tuning

- First 3 experiments:
  1. Verify that baseline model can detect canaries using beam search and random sampling
  2. Test leave-one-out ensemble with 2-3 teachers on a small dataset to observe the aggregation effect
  3. Compare perplexity and canary detection rates across different fine-tuning epochs and learning rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the leave-one-out ensemble method perform with larger language models (e.g., models with billions of parameters)?
- Basis in paper: [explicit] The authors mention that future work might include experiments with large LMs and parameter-efficient fine-tuning techniques during the post-training unlearning process.
- Why unresolved: The current experiments are conducted on relatively smaller models (LSTM-based with 300-dimensional embeddings and 2 layers of 1,500 hidden units). The scalability and effectiveness of the method for much larger models remain untested.
- What evidence would resolve it: Experimental results comparing the performance of the leave-one-out ensemble method on large-scale models (e.g., GPT-3, LLaMA) versus smaller models, including metrics on privacy-utility trade-offs and computational efficiency.

### Open Question 2
- Question: What is the impact of using different knowledge distillation mechanisms on the effectiveness of the right to be forgotten (RTBF) in language models?
- Basis in paper: [explicit] The authors state that they investigated the effect of several different knowledge distillation mechanisms for enhancing RTBF, but do not provide detailed comparisons or results.
- Why unresolved: The paper does not specify which knowledge distillation mechanisms were tested or how they compare in terms of privacy-utility trade-offs.
- What evidence would resolve it: A comparative analysis of various knowledge distillation mechanisms (e.g., different temperature settings, distillation losses) and their impact on the ability to forget targeted sequences while maintaining model performance.

### Open Question 3
- Question: How does the addition of Gaussian noise to the aggregated outputs from teacher LMs affect the balance between privacy and utility?
- Basis in paper: [explicit] The authors discuss the option of adding Gaussian noise to the aggregated outputs from teacher LMs to further mask the presence of sensitive sequences, but do not provide a comprehensive study on its effectiveness.
- Why unresolved: The paper only briefly mentions the addition of Gaussian noise and its potential benefits, without exploring different noise magnitudes or their impact on privacy and utility.
- What evidence would resolve it: Experiments varying the magnitude of Gaussian noise (σ) and analyzing its effects on the model's ability to forget targeted sequences and maintain perplexity, along with a trade-off analysis.

### Open Question 4
- Question: How does the leave-one-out ensemble method compare to other unlearning techniques in terms of computational efficiency and effectiveness?
- Basis in paper: [explicit] The authors compare their method to baseline approaches (e.g., re-training, PATE, GA) and claim superior privacy-utility trade-offs, but do not provide a comprehensive comparison with other unlearning techniques.
- Why unresolved: The paper focuses on a few specific methods and does not explore a broader range of unlearning techniques or their computational costs.
- What evidence would resolve it: A detailed comparison of the leave-one-out ensemble method with other unlearning techniques (e.g., influence functions, data augmentation) in terms of computational efficiency, effectiveness in forgetting targeted sequences, and impact on model performance.

## Limitations

- The evaluation uses synthetic canaries rather than naturally occurring private information, limiting real-world applicability
- The method has only been validated on LSTM-based language models, with unknown effectiveness on transformer architectures
- The teacher ensemble size and data partitioning strategy were not systematically explored, potentially limiting scalability

## Confidence

**Medium-High**: The paper provides clear experimental evidence showing the leave-one-out ensemble method outperforms baseline approaches in forgetting canaries while maintaining perplexity. However, confidence is tempered by the synthetic nature of the evaluation and the lack of testing on more complex, real-world scenarios involving naturally occurring private information.

## Next Checks

1. **Real-world privacy evaluation**: Apply the method to naturally occurring private information in datasets (e.g., personal identifiers, sensitive topics) rather than synthetic canaries, and evaluate using black-box extraction attacks rather than simple detection methods.

2. **Architecture generalization test**: Implement and validate the approach on transformer-based language models (BERT, GPT-style) to assess whether the teacher-student ensemble framework transfers effectively beyond LSTMs.

3. **Scalability and robustness analysis**: Systematically vary the number of teachers, data partitioning granularity, and noise parameters to identify optimal configurations and determine the method's limits in terms of dataset size and model complexity.