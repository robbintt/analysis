---
ver: rpa2
title: Emergent representations in networks trained with the Forward-Forward algorithm
arxiv_id: '2305.18353'
source_url: https://arxiv.org/abs/2305.18353
tags:
- ensembles
- data
- neural
- neurons
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the Forward-Forward algorithm as a biologically
  plausible alternative to backpropagation for training neural networks. The key contribution
  is demonstrating that the Forward-Forward algorithm produces sparse neural representations
  similar to those observed in biological sensory cortices.
---

# Emergent representations in networks trained with the Forward-Forward algorithm

## Quick Facts
- arXiv ID: 2305.18353
- Source URL: https://arxiv.org/abs/2305.18353
- Reference count: 40
- Primary result: Forward-Forward algorithm produces sparse neural ensembles similar to biological sensory cortices

## Executive Summary
This paper investigates the Forward-Forward algorithm as a biologically plausible alternative to backpropagation for training neural networks. The authors demonstrate that Forward-Forward produces sparse neural representations with category-specific ensembles that share neurons across semantically related classes, resembling biological sensory cortices. They train models on MNIST and Fashion-MNIST datasets, finding that Forward-Forward creates highly sparse representations dominated by negative weights, in contrast to backpropagation's denser representations. The results suggest Forward-Forward may offer advantages for biological plausibility while maintaining competitive classification performance.

## Method Summary
The authors compare Forward-Forward (FF) against backpropagation (BP) and a hybrid BP/FF approach using 784-unit fully-connected 3-layer networks with sigmoid activations on MNIST and Fashion-MNIST datasets. FF uses layer-wise training with ℓ∞ norm as goodness function and ℓ∞ normalization between layers, while BP uses cross-entropy loss. Labels are encoded as one-hot vectors at image borders to distinguish positive from negative examples. The study analyzes internal representations by identifying category-specific neural ensembles using median activation thresholds, measuring sparsity, shared neurons across categories, and weight polarity distributions to assess excitatory-inhibitory balance.

## Key Results
- Forward-Forward produces highly sparse neural ensembles with consistent activation patterns for specific classes
- Semantically related categories share neurons across their respective ensembles
- Forward-Forward models show dominance of negative weights, resembling biological excitatory-inhibitory balance
- Sparse representations enable generalization to unseen classes through formation of new neural ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forward-Forward training produces sparse, category-specific neural ensembles that resemble biological sensory cortices.
- Mechanism: By optimizing a layer-wise goodness function (e.g., ℓ∞ norm) separately for each layer, Forward-Forward encourages neurons to specialize in discriminating between positive and negative data, leading to high activation sparsity within ensembles.
- Core assumption: Local optimization of goodness functions at each layer, rather than global gradient propagation, naturally leads to sparse, modular representations.
- Evidence anchors:
  - [abstract] "internal representations obtained by the Forward-Forward algorithm can organise into category-specific ensembles exhibiting high sparsity"
  - [section] "optimizing a goodness function of the neural activations (e.g. the ℓp norm) on positive data and minimize it on negative data"
  - [corpus] No direct corpus evidence for sparsity; related works discuss general Forward-Forward properties but not biological plausibility claims.
- Break condition: If goodness function optimization is performed globally (as in BP/FF) rather than locally, sparsity decreases and the biological resemblance weakens.

### Mechanism 2
- Claim: Semantically related categories tend to share neurons across their respective ensembles.
- Mechanism: The shared structure in input data leads to overlapping activation patterns, causing certain neurons to respond to multiple related categories.
- Core assumption: Input similarity translates to shared internal representations when trained with a discriminative goodness function.
- Evidence anchors:
  - [section] "semantically related categories can be expected to share units of their ensembles, and this is indeed what we observe"
  - [section] "when image categories are characterized by a certain degree of visual similarity the corresponding ensembles often share one or more units"
  - [corpus] No corpus evidence for category overlap; related works focus on architecture and not representation overlap.
- Break condition: If input categories are highly dissimilar or the goodness function is too coarse, shared neurons may not emerge.

### Mechanism 3
- Claim: The dominance of negative weights in Forward-Forward models mimics the excitatory-inhibitory balance found in biological neural networks.
- Mechanism: Training with Forward-Forward implicitly encourages inhibitory connections to sharpen selectivity and reduce cross-talk between unrelated ensembles.
- Core assumption: Local layer-wise optimization without gradient backpropagation leads to weight distributions favoring inhibition for functional isolation.
- Evidence anchors:
  - [section] "the presence of a small number of positive weights connecting the first and second hidden layers highlights the selectivity of neurons"
  - [section] "the Excitatory/Inhibitory (E/I) balance is known to play a key role in the stability of the network and in general in brain dynamics"
  - [corpus] No corpus evidence for weight polarity patterns; related works do not discuss E/I balance in Forward-Forward.
- Break condition: If trained with global backpropagation or a different loss function, the E/I balance may resemble standard networks without strong inhibitory dominance.

## Foundational Learning

- Concept: Layer-wise goodness function optimization
  - Why needed here: Forward-Forward trains each layer separately by maximizing/minimizing a goodness function, unlike backpropagation which propagates errors globally.
  - Quick check question: How does Forward-Forward decide whether to maximize or minimize the goodness function for a given layer?

- Concept: One-hot encoding of labels into image borders
  - Why needed here: Defines positive vs. negative data by embedding class labels directly in the image, enabling the network to learn class-specific activation patterns.
  - Quick check question: What distinguishes a positive from a negative example in the Forward-Forward training setup?

- Concept: Activation sparsity and ensemble detection
  - Why needed here: The core biological claim rests on identifying sparse groups of consistently active neurons; without proper detection, the sparsity claim cannot be validated.
  - Quick check question: What threshold or criterion is used to decide if a neuron is part of an ensemble?

## Architecture Onboarding

- Component map:
  Input layer (784 units) → Hidden layer 1 (784 units, sigmoid) → Normalization layer (ℓ∞) → Hidden layer 2 (784 units, sigmoid) → Normalization layer (ℓ∞) → Hidden layer 3 (784 units, sigmoid) → Output layer (10 units, softmax for BP only)

- Critical path:
  For FF: Forward pass → compute goodness per layer → update layer parameters with Adam → repeat for each layer sequentially → repeat over epochs.
  For BP/FF: Forward pass → compute goodness function globally → backward pass with Adam → update all parameters.

- Design tradeoffs:
  FF: More biologically plausible, higher sparsity, slower convergence, lower accuracy.
  BP/FF: Retains FF architecture benefits, moderate sparsity, faster convergence, still lower accuracy than standard BP.
  BP: Highest accuracy, denser representations, biologically implausible.

- Failure signatures:
  Low sparsity → likely using global loss instead of layer-wise goodness.
  Poor accuracy → insufficient training epochs or learning rate misconfiguration.
  Imbalanced E/I ratio → normalization or initialization issues.

- First 3 experiments:
  1. Train FF on MNIST and verify that Layer 1 activations form sparse ensembles using Method 1.
  2. Compare ensemble sizes across FF, BP/FF, and BP models to confirm sparsity differences.
  3. Measure weight polarity distributions in each model to confirm E/I balance differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the activation patterns and ensemble structures evolve during the training process of Forward-Forward models?
- Basis in paper: [inferred] The paper discusses the emergence of sparse neural ensembles in Forward-Forward models but does not investigate how these ensembles develop over time during training.
- Why unresolved: The authors do not provide a temporal analysis of the training process, focusing instead on the final state of the networks.
- What evidence would resolve it: Detailed analysis of the activation patterns and ensemble structures at various stages of training, potentially using techniques like tracking the evolution of active units and their connections over epochs.

### Open Question 2
- Question: Can the Forward-Forward algorithm be effectively scaled to larger and more complex datasets, such as CIFAR-10, and what architectural modifications would be necessary?
- Basis in paper: [explicit] The authors mention the intention to extend experiments to larger models and datasets like CIFAR-10 in the future.
- Why unresolved: The current study is limited to MNIST and Fashion-MNIST datasets, which are relatively simple. Scaling to more complex datasets remains untested.
- What evidence would resolve it: Successful application of the Forward-Forward algorithm to larger datasets with appropriate architectural adjustments, demonstrating comparable or improved performance and maintaining sparse representations.

### Open Question 3
- Question: What is the relationship between the intrinsic dimension of data representation in hidden layers and the size of the neural ensembles in Forward-Forward models?
- Basis in paper: [explicit] The authors suggest investigating the relationship between the intrinsic dimension of data representation and ensemble size as a potential future direction.
- Why unresolved: The current study does not explore the intrinsic dimensionality of the representations or its correlation with ensemble sizes.
- What evidence would resolve it: Analysis of the intrinsic dimensionality of the representations in each layer and its correlation with the number and size of the neural ensembles, potentially using techniques like principal component analysis or other dimensionality reduction methods.

## Limitations

- The biological plausibility claims lack rigorous comparison against alternative layer-wise training methods to isolate Forward-Forward's unique contributions
- Excitatory-inhibitory balance claims are based on weight polarity observations rather than functional validation of inhibitory effects
- Generalization to unseen classes is demonstrated but not explained mechanistically

## Confidence

**High confidence**: The sparsity measurements and ensemble detection methods are clearly described and reproducible. The weight polarity observations are straightforward statistical analyses.

**Medium confidence**: The biological plausibility arguments connecting sparse representations to cortical function are reasonable but not definitively proven. The comparison between FF and BP/FF architectures is valid but limited to specific network configurations.

**Low confidence**: Claims about Forward-Forward being more "biologically plausible" overall are speculative without broader neurobiological validation beyond sparse representations and weight distributions.

## Next Checks

1. Compare Forward-Forward against layer-wise training methods using standard backpropagation to isolate whether sparsity emerges specifically from FF's goodness function or from the layer-wise architecture itself.

2. Test the robustness of sparse ensemble formation across different goodness functions (ℓ₁, ℓ₂ norms) and activation functions to determine which aspects of FF are essential for the observed representations.

3. Validate the functional role of negative weights by ablating or reversing weight polarities in trained models to measure impact on sparsity and classification performance.