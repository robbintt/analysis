---
ver: rpa2
title: 'Evaluation of GPT-4 for chest X-ray impression generation: A reader study
  on performance and perception'
arxiv_id: '2311.06815'
source_url: https://arxiv.org/abs/2311.06815
tags:
- radiological
- text
- impressions
- image
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluated GPT-4''s ability to generate radiological
  impressions for chest X-rays using different inputs: image, text, text and image.
  Four radiologists assessed the generated impressions based on coherence, factual
  consistency, comprehensiveness, and medical harmfulness.'
---

# Evaluation of GPT-4 for chest X-ray impression generation: A reader study on performance and perception

## Quick Facts
- arXiv ID: 2311.06815
- Source URL: https://arxiv.org/abs/2311.06815
- Authors: 
- Reference count: 26
- Key outcome: Human-written radiological impressions were rated highest, but text-based impressions generated by GPT-4 were not significantly different in quality, highlighting both the model's capabilities and evaluation challenges.

## Executive Summary
This study evaluated GPT-4's ability to generate radiological impressions for chest X-rays using different inputs: image, text, or both. Four radiologists assessed the generated impressions based on coherence, factual consistency, comprehensiveness, and medical harmfulness. The results showed that human-written impressions were rated highest, but text-based impressions were not significantly different. Automated evaluation metrics showed moderate correlations with the radiological score for image impressions, but varied for other inputs. The study highlights discrepancies between radiological assessments and automated metrics, emphasizing the need for improved evaluation methods and further investigation into radiological quality aspects.

## Method Summary
The study used 25 cases from the NIH Chest X-ray dataset, each with a written report containing findings and impression sections. GPT-4 generated impressions based on three input types (image, text, or both) with a fixed prompt structure. Four radiologists evaluated the generated impressions and written impressions on four dimensions using a 5-point Likert scale, classified their origin, and provided justification. Correlations between automated metrics and radiological scores were calculated to assess alignment.

## Key Results
- Text-based impressions generated by GPT-4 were rated similarly to human-written impressions by radiologists
- Image-based impressions showed higher factual consistency errors, making them easier to detect as AI-generated
- Automated evaluation metrics showed only moderate to substantial correlations with radiological assessments, varying by input type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based inputs allow GPT-4 to generate impressions that radiologists rate similarly to human-written impressions because the model leverages contextual understanding from radiological findings.
- Mechanism: GPT-4 processes the findings section to produce coherent impressions that align with radiologist expectations when trained on medical language patterns.
- Core assumption: The model's internal representation of medical vocabulary and report structure is sufficient to mimic human quality.
- Evidence anchors:
  - [abstract] "text-based impressions were not significantly different [to human-written]"
  - [section] "For the automatic evaluation metrics the impression based on the combined text and image achieved the highest score"
- Break condition: If the findings text lacks critical clinical detail, the generated impression may miss essential information.

### Mechanism 2
- Claim: Radiologists can detect AI-generated impressions based on input type, with highest accuracy for image inputs due to factual inconsistencies.
- Mechanism: Image-based inputs challenge GPT-4's ability to accurately interpret visual data, leading to errors that radiologists can identify.
- Core assumption: The model's zero-shot visual reasoning capabilities are insufficient for nuanced radiological interpretation.
- Evidence anchors:
  - [abstract] "detection of AI-generated impressions varied by input and was 61% for text-based impressions"
  - [section] "Image -based impressions were classified as AI-generated with 85% due to Factual consistency/Error"
- Break condition: If the model's visual reasoning improves, the discrepancy between human and AI-generated impressions may diminish.

### Mechanism 3
- Claim: Automated evaluation metrics correlate moderately with radiological scores for image inputs but poorly for text inputs, indicating insufficient representation of radiological quality.
- Mechanism: Metrics like BLEU and BERTScore measure text similarity and semantics but fail to capture clinical accuracy and comprehensiveness.
- Core assumption: Current automated metrics do not encode domain-specific knowledge required for medical report evaluation.
- Evidence anchors:
  - [abstract] "automated evaluation metrics showed moderate to substantial correlations to the radiological score for the image impressions"
  - [section] "the significance of the radiological scores needs to be discussed. Evaluation metrics can only be as good as the human assessment"
- Break condition: If domain-specific metrics are developed, correlation with radiological assessments may improve.

## Foundational Learning

- Concept: Multimodal foundation models
  - Why needed here: Understanding how GPT-4 processes both text and image inputs to generate radiological impressions.
  - Quick check question: What is the difference between zero-shot learning and in-context learning in foundation models?

- Concept: Radiological report structure
  - Why needed here: Knowing the components of a radiological report (findings, impressions) is crucial for interpreting the study's results.
  - Quick check question: What are the key elements that differentiate a good radiological impression from a poor one?

- Concept: Evaluation metrics for text generation
  - Why needed here: Familiarity with BLEU, BERTScore, and RadGraph is necessary to understand the study's automated evaluation approach.
  - Quick check question: How do BLEU and BERTScore differ in their approach to evaluating text similarity?

## Architecture Onboarding

- Component map:
  - GPT-4 model with multimodal input processing -> NIH Chest X-ray dataset for cases -> Blinded radiological report generation -> Four radiologists for quality assessment -> Automated metrics (BLEU, BERT, CheXbert, RadGraph, RadCliQ) -> Statistical analysis pipeline

- Critical path:
  1. Select chest X-ray cases from NIH dataset
  2. Generate blinded radiological reports
  3. Input image, text, or both into GPT-4
  4. Collect generated impressions
  5. Radiologist evaluation of impressions
  6. Calculate automated metrics
  7. Perform statistical analysis

- Design tradeoffs:
  - Using GPT-4's zero-shot capabilities vs. fine-tuning on medical data
  - Blinded evaluation to reduce bias vs. potential loss of context
  - Small sample size for manageability vs. limited generalizability

- Failure signatures:
  - Low correlation between automated metrics and radiological scores
  - High detection rate of AI-generated impressions by radiologists
  - Significant differences in radiological scores between input types

- First 3 experiments:
  1. Vary the complexity of findings text input to test GPT-4's performance
  2. Introduce radiologist feedback into GPT-4's prompt to improve outputs
  3. Test different automated metrics combinations to find the best correlation with radiological scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated evaluation metrics be improved to better capture radiological quality aspects of generated impressions?
- Basis in paper: [explicit] The study found significant discrepancies between radiological assessments and automated metrics, indicating insufficient representation of radiological quality.
- Why unresolved: Current metrics like BLEU, BERT, and RadCliQ show varying correlations with radiological scores depending on input type, suggesting they do not fully capture the nuances of radiological reporting.
- What evidence would resolve it: Development and validation of new metrics specifically designed to evaluate radiological quality, tested against expert radiological assessments across diverse cases.

### Open Question 2
- Question: What is the impact of reader bias on the evaluation of AI-generated radiological impressions?
- Basis in paper: [explicit] The study found that impressions classified as human-written received higher ratings, indicating potential bias.
- Why unresolved: The study did not explore the extent of bias or its sources, such as prior beliefs about AI capabilities or familiarity with AI-generated text.
- What evidence would resolve it: Experiments comparing evaluations of AI-generated impressions under blinded and non-blinded conditions, with measures to quantify bias.

### Open Question 3
- Question: How does the input modality (image, text, or both) affect the quality and perception of AI-generated radiological impressions?
- Basis in paper: [explicit] The study showed varying performance and perception of impressions based on different inputs, with text-based impressions performing comparably to human-written ones.
- Why unresolved: The study did not explore the underlying reasons for these differences or how they might be mitigated.
- What evidence would resolve it: Detailed analysis of the strengths and weaknesses of each input modality, potentially through controlled experiments with varied case complexities.

## Limitations

- Sample size of 25 cases limits generalizability to broader chest X-ray populations
- Selection bias from using only NIH Chest X-ray dataset cases
- Automated metrics show poor correlation with radiological assessments, questioning their validity

## Confidence

- **High confidence**: The overall methodology and experimental design are sound and well-documented
- **Medium confidence**: The radiological assessment results, given the small sample size and single institution involvement
- **Low confidence**: The generalizability of findings to different clinical settings or patient populations

## Next Checks

1. **External validation**: Test the same methodology with chest X-ray cases from multiple institutions to assess generalizability across different clinical contexts and patient demographics
2. **Extended radiological assessment**: Expand the evaluation to include additional radiological quality dimensions such as clinical utility and diagnostic impact on patient care
3. **Comparative analysis**: Compare GPT-4's performance with specialized medical language models fine-tuned on radiological data to determine if general-purpose models are sufficient for this task