---
ver: rpa2
title: Active Learning Principles for In-Context Learning with Large Language Models
arxiv_id: '2305.14264'
source_url: https://arxiv.org/abs/2305.14264
tags:
- learning
- in-context
- language
- active
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of selecting the most informative
  demonstrations for in-context learning with large language models (LLMs). The authors
  formulate the demonstration selection process as a single-iteration pool-based active
  learning problem and evaluate four standard active learning approaches: uncertainty,
  diversity, similarity, and random sampling.'
---

# Active Learning Principles for In-Context Learning with Large Language Models

## Quick Facts
- arXiv ID: 2305.14264
- Source URL: https://arxiv.org/abs/2305.14264
- Reference count: 34
- Primary result: Similarity-based demonstration selection outperforms all other methods for in-context learning

## Executive Summary
This work addresses the problem of selecting optimal demonstrations for in-context learning with large language models. The authors formulate demonstration selection as a single-iteration pool-based active learning problem and evaluate four approaches: uncertainty, diversity, similarity, and random sampling. Using 15 models from GPT and OPT families across 24 tasks, they find that selecting in-context examples most similar to test examples consistently outperforms all other methods, while uncertainty sampling—typically effective in supervised learning—performs poorly in this paradigm.

## Method Summary
The paper frames in-context learning demonstration selection as a pool-based active learning problem. For each test example, the method selects k demonstrations from a pool of labeled examples using one of four approaches: random sampling, diversity sampling (k-means clustering), uncertainty sampling (perplexity-based), or similarity sampling (k-nearest neighbors using Sentence-BERT embeddings). The selected demonstrations are combined with the test example in a prompt and evaluated using 15 different LLM models (125M to 30B parameters) across 24 classification and multi-choice tasks from the Crossfit benchmark.

## Key Results
- Similarity-based selection consistently outperforms random, diversity, and uncertainty methods across all model families and sizes
- Uncertainty sampling performs poorly in in-context learning despite being effective in supervised active learning
- Ground truth labels in demonstrations are crucial for robust performance
- Performance improvements from similarity selection are consistent across both GPT and OPT model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting in-context examples similar to the test example improves few-shot performance more than random or diversity-based sampling
- Mechanism: Semantic similarity ensures demonstrations are contextually aligned with the test instance, providing more relevant patterns for the LLM to generalize from
- Core assumption: LLM representations capture semantic similarity in a way that transfers to better task performance when examples are contextually close
- Evidence anchors: [abstract] "choosing in-context examples that are similar to the test example outperforms consistently all methods by a large margin"; [section 3.2] "the most similar demonstrations to the test example will best help the model answer the query"

### Mechanism 2
- Claim: Uncertainty sampling performs poorly in in-context learning compared to supervised learning
- Mechanism: Perplexity-based uncertainty scores do not align with the few-shot paradigm's needs; LLMs may ignore or misinterpret uncertain demonstrations in context
- Core assumption: The loss signals useful for model training do not translate to useful signals for demonstration selection without weight updates
- Evidence anchors: [abstract] "uncertainty sampling—typically effective in supervised active learning—performs poorly in the in-context learning paradigm"; [section 5] "uncertainty sampling... exhibits the poorest performance"

### Mechanism 3
- Claim: Ground truth labels in demonstrations are crucial for robust in-context learning performance
- Mechanism: Correct labels provide accurate supervision signals within the prompt, preventing the model from learning incorrect mappings
- Core assumption: LLMs rely on correct label demonstrations to infer task structure, and random labels introduce noise that degrades performance
- Evidence anchors: [section 6.2] "we find that ground truth demonstrations are crucial for high performing, robust in-context learning"; [section 5] "ground truth labels... significantly drops" when replaced with random

## Foundational Learning

- Concept: Pool-based active learning
  - Why needed here: The paper frames demonstration selection as a single-iteration pool-based AL problem
  - Quick check question: What is the difference between stream-based and pool-based active learning?

- Concept: Semantic similarity embeddings (e.g., Sentence-BERT)
  - Why needed here: Similarity sampling relies on encoding examples and test instances to find nearest neighbors
  - Quick check question: How does Sentence-BERT differ from standard BERT embeddings for sentence similarity tasks?

- Concept: Perplexity as uncertainty measure
  - Why needed here: Uncertainty sampling uses LLM perplexity scores instead of probability distributions
  - Quick check question: Why can't we use standard entropy or least confidence measures in this in-context setting?

## Architecture Onboarding

- Component map: Data pool -> Embedding model -> Selection algorithm -> Prompt construction -> LLM inference -> Performance evaluation
- Critical path: Data pool → Embedding/Scoring → Demonstration selection → Prompt construction → LLM inference → Performance evaluation
- Design tradeoffs:
  - Similarity vs. diversity: Similarity is more effective but requires test-time embedding; diversity ensures coverage but may lack task relevance
  - Single iteration vs. iterative AL: Simpler but potentially less optimal than adaptive selection
  - Perplexity-based uncertainty vs. probability-based: More general but less precise uncertainty signal
- Failure signatures:
  - Poor performance despite high similarity: Embeddings may not capture task-relevant features
  - Random sampling outperforms uncertainty: Uncertainty signal misaligned with in-context needs
  - Performance drop with random labels: Model relies heavily on demonstration correctness
- First 3 experiments:
  1. Compare similarity vs. random sampling on a small classification task
  2. Test uncertainty sampling on a simple task to confirm underperformance
  3. Evaluate the effect of k (number of demonstrations) on similarity method performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance trends of uncertainty sampling in in-context learning change as model scale increases beyond the current study's range?
- Basis in paper: [inferred] The authors note that uncertainty sampling performs poorly in in-context learning with current models, but also observe that larger models may start to benefit from high uncertainty prompts
- Why unresolved: The study only evaluates models up to 30B parameters, leaving open the question of whether and how uncertainty sampling's effectiveness changes with even larger models
- What evidence would resolve it: Experiments comparing uncertainty sampling performance across a broader range of model sizes, particularly focusing on models with hundreds of billions or trillions of parameters

### Open Question 2
- Question: What specific characteristics of in-context examples make them effective for large language models beyond semantic similarity and low uncertainty?
- Basis in paper: [explicit] The authors identify semantic similarity and low uncertainty as key factors but acknowledge that other properties like ordering and calibration also influence performance
- Why unresolved: While the paper identifies some properties, it doesn't comprehensively characterize all factors that contribute to effective in-context examples
- What evidence would resolve it: A systematic study examining multiple properties of in-context examples (e.g., diversity, difficulty, linguistic features) and their relationship to model performance

### Open Question 3
- Question: How does the effectiveness of similarity-based in-context example selection vary across different task domains and complexity levels?
- Basis in paper: [inferred] The authors find similarity sampling consistently outperforms other methods across various tasks, but don't deeply analyze domain-specific variations
- Why unresolved: The study uses a fixed set of tasks and doesn't explore how the effectiveness of similarity-based selection might change for different types of tasks or varying complexity levels
- What evidence would resolve it: Experiments testing similarity-based selection across a more diverse set of task domains and complexity levels, potentially including tasks with varying semantic distances between examples and test inputs

## Limitations

- The study focuses exclusively on classification and multi-choice tasks, limiting generalizability to other task types
- The experimental setup uses a fixed number of demonstrations (k=16) across all tasks, which may not be optimal for all problem types
- The findings assume access to embedding models (Sentence-BERT) and may not transfer to settings where such resources are unavailable

## Confidence

**High Confidence Claims:**
- Similarity-based selection consistently outperforms random, diversity, and uncertainty methods across all tested models and tasks
- Ground truth labels are essential for robust in-context learning performance
- Uncertainty sampling performs poorly in the in-context learning paradigm compared to supervised settings

**Medium Confidence Claims:**
- The mechanism by which semantic similarity improves performance (contextual alignment) is the primary driver
- The specific k=16 demonstration count is optimal for all task types
- Sentence-BERT embeddings are the best choice for similarity measurement

**Low Confidence Claims:**
- These findings will generalize to non-classification tasks without modification
- The observed patterns will hold for models outside the GPT/OPT families
- No alternative uncertainty measures could make uncertainty sampling competitive

## Next Checks

1. **Task Diversity Validation**: Test similarity-based selection on generation tasks (summarization, translation) and reasoning tasks to verify if the performance advantage holds beyond classification and multi-choice formats

2. **Embedding Space Validation**: Evaluate alternative embedding methods (e.g., task-specific embeddings, model-based embeddings from the LLMs themselves) to determine if the superiority of similarity sampling depends on Sentence-BERT specifically or generalizes to other semantic representations

3. **Iterative Selection Validation**: Implement an iterative demonstration selection process where selected examples inform subsequent selections, comparing performance against the single-iteration approach to assess potential gains from adaptive selection strategies