---
ver: rpa2
title: Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep
  Learning Systems
arxiv_id: '2307.06496'
source_url: https://arxiv.org/abs/2307.06496
tags:
- attack
- adversarial
- rate
- queries
- against
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of black-box adversarial attacks
  against interpretable deep learning systems (IDLSes), where attackers aim to generate
  adversarial examples that fool both the target DNN model and its coupled interpreter
  with minimal queries. The core method, QuScore, combines transfer-based and score-based
  techniques using a microbial genetic algorithm (MGA) to efficiently generate adversarial
  examples from seed samples produced by a white-box attack.
---

# Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems

## Quick Facts
- arXiv ID: 2307.06496
- Source URL: https://arxiv.org/abs/2307.06496
- Reference count: 34
- Key outcome: QuScore achieves 95%-100% attack success rates with ~150 average queries against IDLSes, maintaining high attribution map similarity and transferability across models and defenses.

## Executive Summary
This paper presents QuScore, a black-box adversarial attack framework targeting interpretable deep learning systems (IDLSes) that combine DNN classifiers with interpretation models. The attack leverages transfer-based and score-based methods using a microbial genetic algorithm to generate adversarial examples that fool both the classifier and interpreter with minimal queries. Experiments demonstrate high attack success rates (95%-100%) across multiple datasets and models while maintaining low query counts and high attribution map similarity. The attack remains effective against common preprocessing defenses and shows strong transferability between different DNN architectures.

## Method Summary
QuScore combines transfer-based and score-based black-box attack strategies using a microbial genetic algorithm (MGA). The method first generates initial adversarial seeds using a white-box attack (AdvEdge) on a source model, then refines these seeds using MGA to optimize against the target black-box model and its interpreter. The MGA uses a small population size (5) and fitness evaluation based on cross-entropy loss, with selection favoring both high- and low-fitness individuals. The attack is designed to minimize query count while maintaining effectiveness against both the classifier and interpreter components of IDLSes.

## Key Results
- Attack success rates of 95%-100% across ImageNet and CIFAR datasets with average 150 queries
- High attribution map similarity (IoU scores above 0.9) between benign and adversarial samples
- Effective against multiple preprocessing defenses including bit depth compression, median smoothing, JPEG compression, and random resizing/padding
- Strong transferability with average 69% success rate across different DNN models
- Resilience demonstrated against common defense mechanisms while maintaining query efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QuScore generates adversarial examples that fool both the target DNN model and its coupled interpreter in black-box settings.
- Mechanism: The attack uses transfer-based techniques to generate initial adversarial samples from a white-box source model, then refines these samples using a microbial genetic algorithm (MGA) guided by score-based feedback from the target black-box model.
- Core assumption: Adversarial examples generated against one DNN model can transfer to other DNN models, and the MGA can effectively navigate the search space to find perturbations that fool both the classifier and interpreter.
- Evidence anchors:
  - [abstract] "Our method is designed to reduce the number of queries necessary to carry out successful attacks, resulting in a more efficient process."
  - [section] "QuScore is based on transfer-based and score-based methods by employing an effective microbial genetic algorithm."
  - [corpus] Weak evidence - the corpus does not directly discuss the specific mechanisms of QuScore, but related papers discuss black-box attacks and genetic algorithms.
- Break condition: If the initial adversarial samples from the source model do not transfer to the target model, or if the MGA cannot find effective perturbations within the query limit.

### Mechanism 2
- Claim: The attack is query-efficient, requiring on average only 150 queries to generate successful adversarial examples.
- Mechanism: The MGA uses a small population size (5) and employs a selection strategy that balances exploration and exploitation, focusing on samples that have high fitness scores and are less detectable by the interpreter.
- Core assumption: A smaller population size with an effective selection strategy can converge to a good solution faster than a larger population.
- Evidence anchors:
  - [abstract] "Our results show that the proposed approach is query-efficient with a high attack success rate that can reach between 95% and 100% with only an average of 150 queries."
  - [section] "In summary, AdvEdge generates adversarial samples against a source DNN model f' and its coupled interpreter g in a white-box setting. Then we provide those generated samples as the seed for the initial population."
  - [corpus] Weak evidence - the corpus does not directly discuss the query efficiency of QuScore, but related papers discuss query-efficient black-box attacks.
- Break condition: If the target model requires a large number of queries to generate successful adversarial examples, or if the attack fails to converge within the query limit.

### Mechanism 3
- Claim: The attack is robust against various preprocessing defense techniques.
- Mechanism: The attack can generate adversarial examples that are effective even when the input is preprocessed using techniques such as bit depth compression, median smoothing, JPEG compression, and random resizing/padding.
- Core assumption: The perturbations added to the adversarial examples are small enough to evade detection by preprocessing defenses, yet large enough to fool the target model and interpreter.
- Evidence anchors:
  - [abstract] "We have also demonstrated that our attack is resilient against various preprocessing defense techniques."
  - [section] "We present the resilience of the proposed attack when using four defense mechanisms: i.e., bit depth compression, median smoothing, JPEG compression, and random resizing/padding."
  - [corpus] Weak evidence - the corpus does not directly discuss the robustness of QuScore against preprocessing defenses, but related papers discuss adversarial attacks against defended models.
- Break condition: If the preprocessing defenses are effective in removing the adversarial perturbations, or if the attack fails to generate effective adversarial examples when defenses are applied.

## Foundational Learning

- Concept: Transferability of adversarial examples
  - Why needed here: The attack relies on the ability to generate adversarial examples against a source model that can fool a different target model.
  - Quick check question: What factors influence the transferability of adversarial examples between different DNN models?

- Concept: Genetic algorithms and their application to optimization problems
  - Why needed here: The attack uses a microbial genetic algorithm to refine the initial adversarial samples and find effective perturbations.
  - Quick check question: How does a genetic algorithm work, and what are the key components of a microbial genetic algorithm?

- Concept: Interpretable deep learning systems and their vulnerability to adversarial attacks
  - Why needed here: The attack targets IDLSes, which are designed to provide interpretable explanations for the decisions made by DNN models.
  - Quick check question: What are the different types of interpretation models used in IDLSes, and how can they be fooled by adversarial examples?

## Architecture Onboarding

- Component map: Source model (white-box) -> MGA algorithm -> Target model (black-box) -> Interpreter -> Fitness function

- Critical path:
  1. Generate initial adversarial samples using AdvEdge attack on the source model
  2. Use the initial samples as the seed population for the MGA
  3. Evaluate the fitness of each sample in the population by querying the target model
  4. Select samples for crossover and mutation based on their fitness scores
  5. Generate new offspring samples and update the population
  6. Repeat steps 3-5 until a successful adversarial example is found or the query limit is reached

- Design tradeoffs:
  - Population size: A larger population may lead to better solutions but requires more queries and computational resources
  - Selection strategy: Balancing exploration (trying new solutions) and exploitation (refining good solutions) is crucial for the success of the attack
  - Perturbation threshold: The maximum allowed perturbation size affects the quality and detectability of the adversarial examples

- Failure signatures:
  - Low attack success rate: Indicates that the initial adversarial samples do not transfer well to the target model or that the MGA cannot find effective perturbations
  - High number of queries: Suggests that the attack is struggling to find a successful adversarial example within the query limit
  - Low similarity in attribution maps: Indicates that the adversarial examples are not fooling the interpreter effectively

- First 3 experiments:
  1