---
ver: rpa2
title: New metrics for analyzing continual learners
arxiv_id: '2309.00462'
source_url: https://arxiv.org/abs/2309.00462
tags:
- learning
- task
- metrics
- continual
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces new metrics for analyzing continual learning
  (CL) models that address limitations of traditional metrics. The authors argue that
  existing metrics like Average Accuracy (AA) and Average Forgetting (AF) do not account
  for the increasing difficulty of classification tasks as more classes are added
  sequentially.
---

# New metrics for analyzing continual learners

## Quick Facts
- arXiv ID: 2309.00462
- Source URL: https://arxiv.org/abs/2309.00462
- Reference count: 21
- One-line primary result: Proposes new metrics (RAA and RAF) that better assess continual learning model performance by accounting for increasing task difficulty

## Executive Summary
This paper introduces Rescaled Average Accuracy (RAA) and Rescaled Average Forgetting (RAF) metrics to address limitations in traditional continual learning evaluation metrics. The authors argue that as classification tasks become more difficult with more classes, standard metrics like Average Accuracy (AA) and Average Forgetting (AF) become misleading. RAA and RAF normalize these metrics using a random classifier baseline to provide a more accurate assessment of model learning behavior throughout the continual learning process.

## Method Summary
The proposed method involves creating two new metrics that rescale traditional AA and AF by the performance of a random classifier. These metrics incorporate task difficulty coefficients (γk and βk) that increase with the number of classes, effectively normalizing for the growing complexity of classification tasks. The approach is evaluated using experience replay, fine-tuning, and GDumb methods on split-CIFAR100 (10 tasks, 10 classes each) and split-Tiny ImageNet (100 tasks, 2 classes each).

## Key Results
- RAA and RAF metrics successfully detect when models reach their maximum learning capabilities
- RAF better reflects true forgetting behavior by accounting for task difficulty
- The proposed metrics reveal distinct training regimes that traditional metrics cannot distinguish
- Experiments show clear differences in model behavior when evaluated with the new metrics versus traditional ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed RAA and RAF metrics account for the increasing difficulty of classification tasks by normalizing with respect to the performance of a random classifier.
- Mechanism: By dividing the traditional AA and AF by the expected accuracy and forgetting of a random classifier, the metrics isolate the model's actual learning behavior from the effect of increasing task complexity.
- Core assumption: The random classifier's performance is a reliable baseline for the difficulty of the classification task at any given step.
- Evidence anchors:
  - [abstract] "They normalize AA and AF by the performance of a random classifier."
  - [section] "We propose new metrics that attempt to dissociate this setup-induced forgetting from the overall performance by rescaling the original AA and AF using the performances of a random classifier to account for task difficulty."
- Break condition: If the random classifier baseline is not representative of the true difficulty, e.g., due to class imbalance or non-i.i.d. class distributions, the normalization will be misleading.

### Mechanism 2
- Claim: RAA and RAF reveal distinct training regimes by decoupling the effect of task difficulty from the stability-plasticity trade-off.
- Mechanism: By normalizing for task difficulty, these metrics can distinguish between a model that is simply struggling with harder tasks and one that has failed to balance learning new information with retaining old information.
- Core assumption: The traditional metrics conflate the effects of increasing task difficulty with actual forgetting or poor plasticity.
- Evidence anchors:
  - [section] "Decoupling the task difficulty allows us to detect if the model’s performances change due to the increasing number of classes or due to a failed stability-plasticity trade-off."
- Break condition: If the model's performance is not primarily driven by task difficulty or forgetting, but by other factors like domain shift or data quality, the metrics may not accurately reflect the learning behavior.

### Mechanism 3
- Claim: The coefficients γk and βk in the RAA and RAF formulas act as task difficulty coefficients, adjusting the metrics based on the relative difficulty of each task.
- Mechanism: These coefficients increase as the number of classes increases, reflecting the growing difficulty of the classification task. This allows the metrics to provide a more accurate assessment of the model's performance at each stage of learning.
- Core assumption: The number of classes is a good proxy for task difficulty in continual learning scenarios.
- Evidence anchors:
  - [section] "The coefficients βk and γk act as task difficulty coefficients. Figure 2 shows different values for these coefficients along training. The harder the task, the higher the coefficient."
- Break condition: If the number of classes is not a good proxy for task difficulty, e.g., if the new classes are very similar to the old ones, the coefficients may not accurately reflect the true difficulty.

## Foundational Learning

- Concept: Continual Learning (CL)
  - Why needed here: Understanding the unique challenges of CL, such as catastrophic forgetting and the stability-plasticity dilemma, is crucial for interpreting the proposed metrics.
  - Quick check question: What is the main challenge in CL that motivates the development of new metrics?

- Concept: Catastrophic Forgetting (CF)
  - Why needed here: CF is a key phenomenon that the new metrics aim to measure more accurately by accounting for task difficulty.
  - Quick check question: How does CF typically manifest in CL, and why is it problematic?

- Concept: Stability-Plasticity Dilemma
  - Why needed here: The metrics are designed to provide insights into how well a model balances stability (retaining old knowledge) with plasticity (learning new information).
  - Quick check question: What is the stability-plasticity dilemma, and why is it central to CL?

## Architecture Onboarding

- Component map:
  - Datasets (CIFAR100, Tiny ImageNet) -> Task split into sequential classes -> CL methods (ER, Finetune, GDumb) -> Evaluation metrics (AA, AF, RAA, RAF)

- Critical path:
  1. Split the datasets into sequential tasks
  2. Train the baselines on each task, updating the model and memory (if applicable)
  3. Evaluate the model's performance on all previously seen tasks after each training step
  4. Calculate the traditional and proposed metrics to assess the model's learning behavior

- Design tradeoffs:
  - The proposed metrics trade off simplicity for accuracy in measuring learning behavior. While they require more computation (calculating the random classifier's performance), they provide a more nuanced understanding of the model's performance.
  - The choice of baseline methods (ER, Finetune, GDumb) reflects a tradeoff between computational efficiency and performance in mitigating forgetting.

- Failure signatures:
  - If the RAA and RAF metrics do not reveal distinct training regimes or do not align with the expected behavior of the models, it may indicate issues with the metrics or the experimental setup.
  - If the random classifier's performance is not a good baseline for task difficulty, the metrics may be misleading.

- First 3 experiments:
  1. Train and evaluate the Finetune baseline on CIFAR100, measuring performance using AA, AF, RAA, and RAF
  2. Train and evaluate the ER baseline on Tiny ImageNet, comparing the traditional and proposed metrics to assess learning behavior
  3. Analyze the RAA and RAF values for the GDumb baseline on both datasets to identify any plateaus or other interesting patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of continual learning models change when using RAA and RAF metrics compared to traditional metrics across different datasets and task complexities?
- Basis in paper: [explicit] The paper proposes new metrics (RAA and RAF) to address limitations of traditional metrics (AA and AF) and demonstrates their effectiveness through experiments on benchmark datasets (split-CIFAR100 and split-Tiny ImageNet).
- Why unresolved: The experiments conducted in the paper focus on specific datasets and task configurations. The generalizability of the results to other datasets and task complexities remains untested.
- What evidence would resolve it: Conducting experiments with a wider variety of datasets and task complexities to compare the performance of models using RAA and RAF metrics against traditional metrics.

### Open Question 2
- Question: How do the proposed RAA and RAF metrics correlate with the actual forgetting behavior of models in continual learning scenarios?
- Basis in paper: [inferred] The paper argues that traditional AF metrics do not fairly represent forgetting behavior due to the increasing difficulty of tasks. The proposed RAF metric aims to dissociate setup-induced forgetting from overall performance.
- Why unresolved: The paper does not provide a detailed analysis of how RAF correlates with actual forgetting behavior observed in models. The relationship between RAF values and the true forgetting mechanisms remains unclear.
- What evidence would resolve it: Analyzing the correlation between RAF values and detailed forgetting analyses, such as tracking the change in model performance on specific tasks over time.

### Open Question 3
- Question: Can the proposed RAA and RAF metrics be extended to evaluate continual learning models in scenarios with non-uniform task difficulties or varying numbers of classes per task?
- Basis in paper: [explicit] The paper proposes RAA and RAF metrics based on the assumption of uniform task difficulties and a constant number of classes per task. The metrics are derived under these assumptions.
- Why unresolved: The paper does not explore the applicability of the metrics to scenarios with non-uniform task difficulties or varying numbers of classes per task. The robustness of the metrics in such scenarios is untested.
- What evidence would resolve it: Testing the RAA and RAF metrics on datasets with non-uniform task difficulties or varying numbers of classes per task to evaluate their effectiveness and robustness in these scenarios.

## Limitations
- The metrics assume that random classifier performance is a reliable baseline for task difficulty, which may not hold in all scenarios
- The effectiveness of the metrics in scenarios with imbalanced class distributions has not been thoroughly validated
- The metrics' generalizability to other continual learning paradigms (e.g., domain-incremental learning) remains untested

## Confidence
- High: The mathematical formulation of RAA and RAF is sound, and the metrics correctly normalize for task difficulty based on the random classifier baseline
- Medium: The empirical results on CIFAR100 and Tiny ImageNet support the claims about the metrics' ability to reveal distinct training regimes and provide more accurate assessments compared to traditional metrics
- Low: The generalizability of the metrics to other continual learning scenarios (e.g., domain-incremental learning, class-imbalanced streams) has not been demonstrated

## Next Checks
1. Test the RAA and RAF metrics on additional datasets with varying task difficulty profiles (e.g., gradual domain shifts, imbalanced class distributions) to assess their robustness and generalizability
2. Conduct ablation studies to quantify the impact of the random classifier baseline on the metrics' performance, comparing it to alternative baselines (e.g., random guessing per class, heuristic baselines)
3. Implement and evaluate the metrics on a wider range of continual learning methods (e.g., regularization-based methods, generative replay) to determine their applicability across different CL paradigms