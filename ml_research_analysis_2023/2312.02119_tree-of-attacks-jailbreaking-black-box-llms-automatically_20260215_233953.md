---
ver: rpa2
title: 'Tree of Attacks: Jailbreaking Black-Box LLMs Automatically'
arxiv_id: '2312.02119'
source_url: https://arxiv.org/abs/2312.02119
tags:
- prompts
- prompt
- language
- jailbreaks
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents TAP, an automated black-box method for jailbreaking
  LLMs that generates interpretable prompts to bypass safety filters. TAP uses tree-of-thought
  reasoning and pruning to efficiently generate jailbreak prompts with significantly
  higher success rates than previous methods.
---

# Tree of Attacks: Jailbreaking Black-Box LLMs Automatically

## Quick Facts
- arXiv ID: 2312.02119
- Source URL: https://arxiv.org/abs/2312.02119
- Reference count: 40
- Primary result: Automated black-box jailbreak method achieving 80%+ success rate on GPT4 and GPT4-Turbo with <30 queries

## Executive Summary
TAP (Tree of Attacks with Pruning) is an automated method for jailbreaking black-box LLMs that generates interpretable prompts to bypass safety filters. The method uses tree-of-thought reasoning with pruning to efficiently explore attack strategies while minimizing queries to the target model. TAP achieves significantly higher success rates than previous black-box approaches while using fewer queries on average, representing a substantial improvement in automated jailbreaking capabilities.

## Method Summary
TAP employs three LLMs: an attacker (Vicuna-13B-v1.5) to generate and refine prompts, an evaluator (GPT4) to assess prompt quality and prune off-topic candidates, and a target LLM to be jailbroken. The attacker uses tree-of-thought reasoning with breadth-first search to iteratively refine prompts, while the evaluator prunes prompts unlikely to succeed before sending them to the target. The method explores diverse attack strategies through a branching factor while maintaining efficiency through early termination when a successful jailbreak is found.

## Key Results
- Achieves over 80% success rate on state-of-the-art models like GPT4 and GPT4-Turbo
- Uses fewer than 30 queries on average to successfully jailbreak target models
- Significantly outperforms prior black-box methods in both success rate and query efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TAP's tree-of-thought reasoning with pruning reduces off-topic prompts, leading to higher success rates.
- Mechanism: The attacker LLM iteratively refines prompts using tree-of-thought reasoning. At each iteration, prompts are evaluated and pruned if they are off-topic. This focused search space increases the likelihood of generating jailbreak prompts.
- Core assumption: The evaluator LLM can accurately distinguish on-topic from off-topic prompts for the given goal.
- Evidence anchors:
  - [abstract]: "TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM."
  - [section]: "TAP maintains a tree where each node stores one prompt P generated by A along with some metadata about it... Since it works layer-by-layer, the conversation history at a node is a subset of the conversation histories of any of its children... This allows TAP to explore disjoint attack strategies, while still prioritizing the more promising strategies/prompts by pruning prompts P that are off-topic and/or have a low score from Judge(P, G)."
- Break condition: If the evaluator LLM cannot accurately assess on-topic prompts, pruning may remove promising prompts, reducing success rate.

### Mechanism 2
- Claim: TAP's branching factor allows exploration of diverse attack strategies.
- Mechanism: TAP uses a branching factor (b) to generate multiple refined prompts at each iteration. This exploration of diverse strategies increases the chances of finding a successful jailbreak.
- Core assumption: A diverse set of attack strategies is more likely to succeed than a single strategy.
- Evidence anchors:
  - [abstract]: "TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning with breadth-first search [Yao+23] until a prompt P is found which jailbreaks the target LLM T, or the tree-of-thought reaches a maximum specified depth."
  - [section]: "For each leaf â„“ of the tree, its prompt P is refined by A using one step of chain-of-thought repeated b times to construct refined prompts P1, P2, . . ., Pb."
- Break condition: If the branching factor is too low, exploration is limited. If too high, the search space becomes too large, increasing query cost without proportional benefit.

### Mechanism 3
- Claim: TAP's pruning reduces the number of queries to the target LLM.
- Mechanism: TAP prunes off-topic prompts before sending them to the target LLM. This reduces the total number of queries, making the attack more efficient.
- Core assumption: Off-topic prompts are unlikely to result in successful jailbreaks.
- Evidence anchors:
  - [abstract]: "Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks."
  - [section]: "Since the method prunes off-topic prompts and stops as soon as one of the generated prompts jailbreaks T, the number of queries to T can be much smaller."
- Break condition: If the evaluator LLM incorrectly identifies on-topic prompts as off-topic, promising prompts may be pruned, reducing success rate.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their vulnerabilities to adversarial attacks.
  - Why needed here: Understanding LLMs is crucial for comprehending how TAP exploits their vulnerabilities to jailbreak them.
  - Quick check question: What are some common types of adversarial attacks against LLMs?

- Concept: Tree-of-thought reasoning and its application in prompt engineering.
  - Why needed here: TAP uses tree-of-thought reasoning to iteratively refine prompts and explore diverse attack strategies.
  - Quick check question: How does tree-of-thought reasoning differ from chain-of-thought reasoning in prompt engineering?

- Concept: Evaluation metrics for assessing the success of jailbreak attempts.
  - Why needed here: TAP relies on an evaluator LLM to assess the quality of generated prompts and responses, using metrics like the GPT4-Metric and Human-Judgement.
  - Quick check question: What are the strengths and limitations of using automated metrics like the GPT4-Metric to evaluate jailbreak success?

## Architecture Onboarding

- Component map: Attacker LLM (Vicuna-13B-v1.5) -> Evaluator LLM (GPT4) -> Target LLM
- Critical path: Goal -> Attacker LLM generates prompts -> Evaluator LLM prunes off-topic prompts -> Target LLM is queried -> Evaluator LLM assesses response -> Repeat until jailbreak or max depth reached
- Design tradeoffs:
  - Branching factor (b): Higher values explore more diverse strategies but increase query cost
  - Maximum depth (d): Higher values allow more iterations but increase query cost and risk degradation
  - Maximum width (w): Higher values retain more prompts for exploration but increase query cost
- Failure signatures:
  - Low success rate: May indicate issues with the attacker or evaluator LLM, or suboptimal hyperparameters
  - High query count: May indicate overly conservative pruning or a branching factor that's too high
  - Off-topic prompts: May indicate issues with the evaluator LLM's ability to assess prompt relevance
- First 3 experiments:
  1. Test TAP with different branching factors (b) to find the optimal balance between exploration and query cost
  2. Evaluate the impact of using different evaluator LLMs on TAP's performance
  3. Compare TAP's performance against different target LLMs to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would TAP perform with different combinations of attacker and evaluator models beyond Vicuna-13B and GPT4?
- Basis in paper: [explicit] The paper mentions that TAP uses Vicuna-13B as attacker and GPT4 as evaluator, and notes that "Since the method only sends black-box queries to A, E, and T, they can be instantiated with any LLMs that have public query access."
- Why unresolved: The paper only evaluates TAP with this specific combination and mentions it as a potential area for future work without conducting experiments.

### Open Question 2
- Question: What is the theoretical limit of TAP's success rate against increasingly sophisticated safety training methods?
- Basis in paper: [inferred] The paper shows TAP achieves 80%+ success rates against current state-of-the-art models including GPT4 and GPT4-Turbo, but doesn't explore how this might change as safety training evolves.
- Why unresolved: The paper only tests against current models and doesn't provide analysis of how TAP might scale against future safety improvements.

### Open Question 3
- Question: How does TAP's performance scale with different tree depth, width, and branching factor configurations?
- Basis in paper: [explicit] The paper states "We have not attempted to optimize the choices of these parameters and optimizing them likely would lead to further performance improvements."
- Why unresolved: The paper uses fixed parameters (depth=10, width=10, branching factor=4) without exploring the parameter space or providing sensitivity analysis.

### Open Question 4
- Question: Can TAP be extended to successfully jailbreak models that currently show high resistance (like Llama-2-Chat-7B)?
- Basis in paper: [explicit] The paper notes that "Llama-2-Chat-7B model seems to be much more robust to these types of black-box attacks" and that it "refuses all requests for harmful information."
- Why unresolved: The paper only observes this resistance without attempting to modify TAP or develop new strategies to overcome it.

### Open Question 5
- Question: How would TAP perform in multi-prompt jailbreak scenarios where a sequence of prompts is required?
- Basis in paper: [explicit] The paper states in its conclusion that "it is also important to rigorously evaluate LLM's vulnerability to multi-prompt jailbreaks, where a small sequence of adaptively constructed prompts P1, P2, ..., Pm together jailbreak an LLM."
- Why unresolved: The paper only tests single-prompt jailbreaks and explicitly identifies multi-prompt scenarios as important future work.

## Limitations

- The evaluation uses a curated AdvBench Subset of 50 prompts rather than the full dataset, potentially inflating success rates
- Performance varies significantly between target models, with the 80% success rate being an average that may overstate typical effectiveness
- The method's reliance on GPT-4 for evaluation creates a circular dependency that could bias results toward strategies GPT-4 finds persuasive

## Confidence

**High Confidence** (well-supported by evidence):
- TAP generates interpretable jailbreak prompts (explicitly demonstrated)
- TAP achieves higher success rates than prior black-box methods (statistically significant results provided)
- TAP uses fewer queries than prior methods (quantitative comparison with published baselines)

**Medium Confidence** (plausible but with caveats):
- TAP is effective against state-of-the-art models like GPT4 (supported by results but with performance variation across targets)
- Tree-of-thought reasoning with pruning is the key mechanism for success (mechanism described but not isolated experimentally)
- The branching factor and pruning strategy optimize the exploration-exploitation tradeoff (theoretically sound but hyperparameters not thoroughly explored)

**Low Confidence** (major uncertainties remain):
- TAP generalizes to arbitrary black-box LLMs beyond the tested models (limited model diversity in evaluation)
- The 80% success rate represents typical real-world performance (based on curated dataset, not comprehensive testing)
- TAP's prompts are genuinely novel versus memorized from training data (training data overlap not investigated)

## Next Checks

1. Test TAP on the full AdvBench dataset rather than the curated subset to verify that performance gains hold across all categories and aren't inflated by selective sampling.

2. Evaluate TAP with different attacker LLMs (including open-source models not fine-tuned on jailbreak data) to determine whether performance depends on Vicuna's specific training or generalizes to other attackers.

3. Conduct a human evaluation of prompt originality to assess whether TAP's jailbreak prompts are genuinely novel strategies or simply memorized examples from the attacker LLM's training data.