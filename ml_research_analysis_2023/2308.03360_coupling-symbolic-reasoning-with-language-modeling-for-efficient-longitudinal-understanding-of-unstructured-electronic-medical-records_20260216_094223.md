---
ver: rpa2
title: Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal
  Understanding of Unstructured Electronic Medical Records
arxiv_id: '2308.03360'
source_url: https://arxiv.org/abs/2308.03360
tags:
- llms
- medical
- cancer
- easoning
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines the use of coupling symbolic reasoning with
  language modeling to improve the understanding of unstructured electronic medical
  records. The authors propose a clinical abstraction pipeline that leverages both
  Natural Language Processing and symbolic reasoning, and compare it to three setups
  that utilize Large Language Models (LLMs) for retrieval and generation.
---

# Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records

## Quick Facts
- arXiv ID: 2308.03360
- Source URL: https://arxiv.org/abs/2308.03360
- Reference count: 38
- Key outcome: Coupling symbolic reasoning with language modeling improves extraction of cancer-related medical variables from unstructured EMRs

## Executive Summary
This paper addresses the challenge of extracting medical variables from unstructured electronic medical records by combining symbolic reasoning with language modeling. The authors propose a clinical abstraction pipeline that leverages both Natural Language Processing and symbolic reasoning, comparing it against LLM-based retrieval and generation setups. Through evaluation on 100 patient records containing 13 cancer-related medical variables, they demonstrate that symbolic reasoning acts as an effective steering mechanism for LLMs, with retrieval-augmented approaches showing consistent improvements over generation-only methods.

## Method Summary
The authors compare four setups: NLP_REASONING (baseline using NLP and symbolic reasoning), RET_NLP_REASONING (LLM retrieval + symbolic reasoning), GEN_NLP_REASONING (LLM generation + symbolic reasoning), and RET_GEN_NLP_REASONING (LLM retrieval+generation + symbolic reasoning). They preprocess 100 patient records (42 colorectal, 28 breast, 30 lung cancer) by applying OCR, de-identification, document segmentation, entity extraction, relation extraction, and symbolic reasoning consolidation using an in-house medical ontology. The pipeline then integrates retrieval and/or generative LLMs to produce answers for 31 predefined questions, which are evaluated against ground truth for 13 medical variables.

## Key Results
- Symbolic reasoning combined with LLM retrieval improves extraction of several medical variables from unstructured records
- Commercially-free retrieval LLMs perform comparably to commercial counterparts
- Exclusive use of generative LLMs considerably drops overall performance, highlighting the need for symbolic reasoning as an LLM steering mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic reasoning improves LLM performance by structuring reasoning over ontologies
- Mechanism: The symbolic reasoning layer consolidates document-level object graphs into patient-level heterogeneous graphs using logical rules traced transitively across a medical ontology
- Core assumption: The ontology comprehensively captures relevant medical concepts and relations
- Evidence anchors:
  - [abstract] "we show that such a combination improves the extraction of several medical variables from unstructured records"
  - [section 3.2.3] "Our symbolic reasoning operates on document-level and patient-level bases...combining those objects that are compatible with one another through the application of a large set of logical rules that traverse the ontology"
  - [corpus] Weak - no direct evidence in neighbors about symbolic reasoning + ontology consolidation

### Mechanism 2
- Claim: Retrieval LLMs improve performance by selecting relevant text chunks
- Mechanism: Retrieval LLMs identify semantically related chunks from preprocessed documents for each question
- Core assumption: Retrieval LLMs accurately identify semantically relevant text chunks
- Evidence anchors:
  - [abstract] "the understanding of several medical variables benefits from the retrieval and generation capabilities of LLMs"
  - [section 4.3.2] "retrieval models accurately select text chunks with relevant information to the asked questions, and thus improve the abstraction of several variables"
  - [corpus] Weak - neighbors discuss LLMs for cohort retrieval but not chunk selection for variable extraction

### Mechanism 3
- Claim: Generative LLMs improve performance for subjective variables but may introduce noise
- Mechanism: Generative LLMs synthesize responses from retrieved chunks, particularly beneficial for subjective variables
- Core assumption: Generative LLMs can accurately interpret context and synthesize meaningful responses
- Evidence anchors:
  - [abstract] "the understanding of several medical variables benefits from the retrieval and generation capabilities of LLMs"
  - [section 4.3.2] "Generated answers, on the other hand, are less beneficial to the core reasoning system except in the abstraction of Response, Surgeries and Cancer Diagnosis Date"
  - [corpus] Weak - neighbors don't discuss generative LLMs for variable extraction

## Foundational Learning

- Concept: Medical ontologies and graph representations
  - Why needed here: The system uses a hierarchical medical ontology to represent medical concepts and relations, forming object graphs that the reasoning system processes
  - Quick check question: What is the difference between a tag graph and an object graph in this system?

- Concept: Transformer-based sequence tagging and classification
  - Why needed here: The system uses transformer-based architectures for entity extraction and relation classification between medical entities
  - Quick check question: What types of medical entities does the system extract from clinical texts?

- Concept: Retrieval-augmented generation pipelines
  - Why needed here: The system integrates retrieval and generative LLMs with symbolic reasoning, requiring understanding of how these components interact
  - Quick check question: How does the system combine retrieved chunks with generated answers?

## Architecture Onboarding

- Component map: OCR -> De-identification -> Document Segmentation -> Entity/Relation Extraction -> Symbolic Reasoning -> LLM Integration -> Core Abstraction
- Critical path: OCR → De-identification → Document Segmentation → Entity/Relation Extraction → Symbolic Reasoning → LLM Integration → Final Output
- Design tradeoffs: Using smaller chunks (vs full records) reduces compute cost but may miss cross-chunk context; generative LLMs can synthesize responses but risk hallucination
- Failure signatures: Low precision for subjective variables (Response) suggests LLM generation issues; inconsistent variable extraction across setups suggests ontology/rule gaps
- First 3 experiments:
  1. Compare NLP_REASONING performance on full records vs chunked records
  2. Test retrieval-only setup (no generation) to isolate retrieval contribution
  3. Evaluate standalone generative LLM vs symbolic reasoning to quantify steering benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the symbolic reasoning component vary across different types of unstructured medical records, such as pathology reports versus physician notes?
- Basis in paper: [inferred] The paper mentions that clinical language is convoluted and that patient records often contain discrepancies that require human expertise to decode
- Why unresolved: The paper does not provide a detailed breakdown of how the symbolic reasoning component performs on different types of medical records
- What evidence would resolve it: A comparative analysis of the symbolic reasoning component's performance on different types of unstructured medical records

### Open Question 2
- Question: What are the specific logical rules used by the symbolic reasoning component to consolidate document-level object graphs into a patient-level graph?
- Basis in paper: [explicit] The paper mentions that the symbolic reasoning component applies a large set of logical rules that traverse the ontology to consolidate document-level object graphs into a patient-level graph
- Why unresolved: The paper does not provide details on the specific logical rules used by the symbolic reasoning component
- What evidence would resolve it: A detailed description of the logical rules used by the symbolic reasoning component

### Open Question 3
- Question: How does the performance of the retrieval and generation capabilities of different commercially-free LLMs compare when applied to other medical domains beyond cancer?
- Basis in paper: [explicit] The paper shows that commercially-free retrieval LLMs perform comparably to commercial ones in the context of cancer-related medical variables
- Why unresolved: The paper does not explore the performance of these LLMs in other medical domains
- What evidence would resolve it: A comparative study of the retrieval and generation capabilities of different commercially-free LLMs across various medical domains

## Limitations
- Lack of transparency regarding in-house medical ontology and rule-based approaches used for symbolic reasoning
- Specific transformer-based architectures for entity extraction, relation extraction, and document segmentation not specified
- Evaluation focuses only on 13 cancer-related variables without broader clinical validation

## Confidence
- **High confidence**: Core finding that combining retrieval with symbolic reasoning outperforms standalone LLM approaches is well-supported by ablation results
- **Medium confidence**: Claim that symbolic reasoning acts as effective LLM steering mechanism is supported but limited by lack of transparency in ontology and rules
- **Low confidence**: Claim that commercially-free retrieval LLMs perform comparably to commercial counterparts based on limited comparison without statistical significance testing

## Next Checks
1. Request access to in-house medical ontology and rule definitions to assess whether symbolic reasoning consolidation logic is domain-specific or generalizable
2. Run pipeline with symbolic reasoning disabled to quantify exact contribution of ontology-based consolidation versus NLP components alone
3. Test RET_NLP_REASONING setup on non-cancer medical domain (e.g., cardiovascular or diabetes records) using same retrieval LLMs to assess generalization beyond cancer-specific ontologies