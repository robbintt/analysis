---
ver: rpa2
title: A Non-monotonic Smooth Activation Function
arxiv_id: '2310.10126'
source_url: https://arxiv.org/abs/2310.10126
tags:
- activation
- function
- relu
- sqish
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sqish, a novel smooth activation function
  based on an approximation of the maximum function. The proposed function is non-monotonic
  and smooth, capable of approximating ReLU or its variants.
---

# A Non-monotonic Smooth Activation Function

## Quick Facts
- arXiv ID: 2310.10126
- Source URL: https://arxiv.org/abs/2310.10126
- Reference count: 6
- Primary result: Sqish activation function achieves 8.21% improvement over ReLU on CIFAR100 with ShuffleNet V2 in FGSM adversarial attack

## Executive Summary
This paper introduces Sqish, a novel smooth activation function that approximates the maximum function. The proposed function is non-monotonic and smooth, capable of approximating ReLU or its variants while providing better gradient propagation. Extensive experiments demonstrate that Sqish outperforms baseline activations across diverse tasks including image classification, object detection, 3D medical imaging, and adversarial attack scenarios, making it a promising alternative to existing activation functions.

## Method Summary
Sqish is a smooth activation function based on an approximation of the maximum function, formulated as f(ax, x; β, γ) = ax + (1-α)x / (1 + βe^(-2γ(1-α)x)). The function includes three parameters (a, β, γ) that can be used as hyperparameters or trainable parameters. It aims to eliminate the vanishing gradient problem and reduce dead neurons by providing a smooth approximation to ReLU while maintaining computational efficiency. The function is implemented as a layer that replaces standard activation functions in neural network architectures.

## Key Results
- Achieved 8.21% improvement over ReLU on CIFAR100 with ShuffleNet V2 in FGSM adversarial attack
- Demonstrated 5.87% improvement on image classification on CIFAR100 with ShuffleNet V2
- Showed superior performance across image classification, object detection, 3D medical imaging, and adversarial attack tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sqish approximates the ReLU function smoothly, eliminating the vanishing gradient problem and reducing dead neurons.
- Mechanism: The function uses a smooth approximation of the maximum function that converges to max(0, x) as gamma approaches infinity, allowing gradients to flow more consistently than ReLU.
- Core assumption: The smooth approximation maintains computational efficiency while providing better gradient propagation.
- Evidence anchors: Abstract mentions superior performance across multiple tasks; section provides the mathematical formulation.

### Mechanism 2
- Claim: The non-monotonic nature of Sqish allows it to capture more complex patterns in data compared to monotonic activation functions.
- Mechanism: Non-monotonicity through smooth approximation enables modeling of complex relationships that monotonic functions cannot capture.
- Core assumption: Non-monotonic activation functions are better suited for modeling highly non-linear distributions in real-world data.
- Evidence anchors: Abstract confirms non-monotonic property; limited corpus evidence supporting non-monotonicity benefits.

### Mechanism 3
- Claim: The learnable parameters (a, β, γ) in Sqish allow the network to adapt the activation function to specific tasks and data distributions.
- Mechanism: These parameters can be updated via backpropagation, allowing optimization for specific problems rather than using fixed activation functions.
- Core assumption: Allowing the activation function to adapt during training provides performance benefits over fixed activation functions.
- Evidence anchors: Section describes the parameters as trainable; weak corpus evidence for learnable activation parameters.

## Foundational Learning

- Concept: Smooth approximation of non-differentiable functions
  - Why needed here: The core innovation relies on approximating the non-differentiable maximum function with a smooth function.
  - Quick check question: Why is it important that the proposed function is smooth in the real line?

- Concept: Role of activation functions in neural networks
  - Why needed here: Understanding how activation functions introduce non-linearity and affect gradient flow is crucial for appreciating Sqish's benefits.
  - Quick check question: How do activation functions like ReLU and their variants address the vanishing gradient problem?

- Concept: Adversarial attacks and their impact on model performance
  - Why needed here: Sqish shows significant improvements in adversarial robustness, which is a key aspect of the paper's contributions.
  - Quick check question: What is the FGSM attack method mentioned in the abstract, and why is adversarial robustness important?

## Architecture Onboarding

- Component map: Input -> Sqish activation layer -> Output
- Critical path: Forward pass applies Sqish formula; backward pass computes gradients with respect to input and learnable parameters (a, β, γ)
- Design tradeoffs: Improved performance vs increased computational complexity compared to ReLU; learnable parameters add flexibility but increase overfitting risk
- Failure signatures: Poor performance or instability may indicate parameters are not well-suited for the task or non-monotonicity is causing training difficulties
- First 3 experiments:
  1. Replace ReLU with Sqish in a simple CNN on CIFAR-10 and compare training curves and final accuracy
  2. Test Sqish with different values of gamma (γ) to observe its effect on approximating ReLU
  3. Implement Sqish as a fixed activation function (with predetermined parameters) and as a trainable activation function to compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of hyperparameters (a, β, γ) in Sqish affect its performance across different architectures and datasets?
- Basis in paper: The paper mentions that Sqish has parameters a, β, and γ which can be used as hyperparameters or trainable parameters.
- Why unresolved: The paper does not provide a detailed analysis of how different hyperparameter choices impact performance.
- What evidence would resolve it: Systematic experiments varying these hyperparameters across multiple architectures and datasets.

### Open Question 2
- Question: What is the theoretical justification for the proposed approximation formula in equation (8)?
- Basis in paper: The paper derives the approximation formula but does not provide rigorous mathematical proof of its properties.
- Why unresolved: While the paper shows empirical success, it lacks theoretical guarantees about the approximation quality.
- What evidence would resolve it: Mathematical proofs showing bounds on approximation error and convergence properties.

### Open Question 3
- Question: How does Sqish compare to other smooth activation functions in terms of computational efficiency and memory usage?
- Basis in paper: The paper provides runtime comparisons with other activation functions but doesn't discuss memory usage.
- Why unresolved: The paper only focuses on runtime comparisons, not memory efficiency.
- What evidence would resolve it: Detailed analysis of memory requirements and computational complexity across different hardware platforms.

## Limitations

- The paper lacks specific hyperparameter values for the learnable parameters (a, β, γ) used in experiments, making exact reproduction difficult
- The non-monotonic nature of Sqish introduces potential instability risks that weren't thoroughly explored
- Claims of superiority across diverse tasks seem extensive for a single activation function without addressing potential task-specific limitations

## Confidence

- **High confidence** in the mathematical formulation and smoothness properties of Sqish
- **Medium confidence** in the claimed performance improvements given extensive experimental results but limited hyperparameter specification
- **Low confidence** in the generalizability claims across all tested domains without more detailed ablation studies

## Next Checks

1. Implement an ablation study varying parameters a, β, and γ to determine their optimal ranges and sensitivity to initialization
2. Test Sqish on simpler architectures (e.g., ResNet-18) on CIFAR-10 to establish baseline performance before scaling to complex models
3. Conduct stability analysis during training to quantify the risk of optimization difficulties introduced by the non-monotonic formulation