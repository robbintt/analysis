---
ver: rpa2
title: Learning Diverse Features in Vision Transformers for Improved Generalization
arxiv_id: '2308.16274'
source_url: https://arxiv.org/abs/2308.16274
tags:
- features
- heads
- arxiv
- spurious
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiverseViT, a method to improve out-of-distribution
  (OOD) generalization in vision transformers (ViTs) by increasing the diversity of
  learned features. The authors find that ViTs already have some modularity in their
  attention heads, with some relying on spurious features and others on robust features.
---

# Learning Diverse Features in Vision Transformers for Improved Generalization

## Quick Facts
- arXiv ID: 2308.16274
- Source URL: https://arxiv.org/abs/2308.16274
- Reference count: 11
- Primary result: DiverseViT improves OOD accuracy by encouraging diverse features in ViT attention heads

## Executive Summary
This paper introduces DiverseViT, a method to improve out-of-distribution (OOD) generalization in vision transformers (ViTs) by increasing the diversity of learned features. The authors find that ViTs already have some modularity in their attention heads, with some relying on spurious features and others on robust features. To enhance this modularity, they propose a regularizer that encourages orthogonality of input gradients across attention heads, leading to more diverse and complementary features. Experiments on MNIST-CIFAR and Waterbirds benchmarks show that DiverseViT improves OOD accuracy, and pruning attention heads associated with spurious features further enhances performance.

## Method Summary
DiverseViT adds a diversity regularizer to standard ViT training that encourages orthogonality of input gradients across attention heads. The method computes the gradient of the top prediction with respect to each head's output separately, then penalizes similarity between these gradients across heads. This forces attention heads to learn different aspects of the data. At test time, heads can be pruned based on validation performance to remove those relying on spurious features. The approach is evaluated on MNIST-CIFAR and Waterbirds diagnostic datasets.

## Key Results
- DiverseViT improves OOD accuracy on MNIST-CIFAR and Waterbirds benchmarks
- ViTs naturally exhibit modularity with some heads capturing robust features and others spurious features
- Pruning spurious feature heads at test time further improves OOD performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision transformers inherently learn modular representations where distinct attention heads capture different features, some robust and some spurious.
- Mechanism: The attention mechanism in ViTs allows each head to focus on different aspects of the input, creating a natural separation of features. When trained with standard ERM, this modularity already exists to some degree, enabling head pruning to improve OOD performance.
- Core assumption: The attention heads in ViTs develop distinct feature representations during training.
- Evidence anchors:
  - [abstract] "we first examine vision transformers (ViTs) and find that they tend to extract robust and spurious features with distinct attention heads."
  - [section 4] "We observe that the ERM baseline already exhibits a degree of separation between spurious and robust features among the self-attention heads"
- Break condition: If attention heads learn highly overlapping features or if the model collapses to using only a single dominant head.

### Mechanism 2
- Claim: The input gradient orthogonality regularizer increases diversity between attention heads by encouraging them to learn complementary features.
- Mechanism: By computing the gradient of the top prediction with respect to each attention head's output separately, we can measure how each head contributes to the prediction. The orthogonality regularizer then penalizes similarity between these gradients across heads, forcing them to learn different aspects of the data.
- Core assumption: Input gradients provide a meaningful measure of what each attention head is learning.
- Evidence anchors:
  - [section 3.2] "To promote diversity across the heads, we define an orthogonality regularizer over the input gradients"
  - [section 4] "With the proposed diversification method, we observe that most heads predict either, but not both of the robust and spurious features"
- Break condition: If the gradient-based regularizer causes the model to learn adversarial solutions that don't generalize, or if the gradients become too noisy to be useful.

### Mechanism 3
- Claim: Pruning attention heads at test time based on validation performance can selectively remove spurious features and improve OOD accuracy.
- Mechanism: Since different heads capture different features, some heads will be more affected by distribution shifts than others. By selecting heads that perform well on OOD validation data, we can retain the robust heads and discard those relying on spurious correlations.
- Core assumption: We have access to labeled validation data from the target distribution to identify which heads are robust.
- Evidence anchors:
  - [abstract] "As a result of this modularity, their performance under distribution shifts can be significantly improved at test time by pruning heads corresponding to spurious features"
  - [section 3.3] "To prune a subset of the heads, we multiply their output hi by zero in Equation 2"
  - [section 4] "In all experiments, we select hyperparameters for highest OOD validation accuracy"
- Break condition: If the validation data is not representative of the test distribution, or if all heads are equally affected by the distribution shift.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: The entire method relies on understanding how multi-head self-attention works and how different heads can capture different features
  - Quick check question: What is the purpose of having multiple attention heads in a transformer?

- Concept: Input gradients and their interpretation
  - Why needed here: The diversification method uses input gradients to measure the importance of each head's output to the final prediction
  - Quick check question: How does the input gradient ∇x = ∂p*/∂x relate to feature importance?

- Concept: Regularization techniques in deep learning
  - Why needed here: The method adds a custom regularizer to the training objective to encourage diversity between attention heads
  - Quick check question: What is the difference between L1, L2, and orthogonality regularization?

## Architecture Onboarding

- Component map: Input image patches -> token sequence with positional embeddings -> Multi-head self-attention layer (H heads) -> Output projection layer -> Classification head -> Loss function (cross-entropy + diversity regularizer)

- Critical path:
  1. Forward pass through ViT
  2. Compute input gradients for each head
  3. Calculate diversity regularizer
  4. Backpropagate combined loss
  5. At test time: prune heads based on validation performance

- Design tradeoffs:
  - Regularization strength λ: Higher values increase diversity but may hurt in-distribution performance
  - Number of attention heads H: More heads allow for greater diversity but increase computational cost
  - Head pruning strategy: Selecting heads based on OOD validation data vs. other heuristics

- Failure signatures:
  - If in-distribution accuracy drops significantly with diversification: λ may be too high
  - If head pruning doesn't improve OOD accuracy: The heads may not be as modular as expected
  - If training becomes unstable: The gradient-based regularizer may be causing optimization issues

- First 3 experiments:
  1. Train baseline ViT with ERM on MNIST-CIFAR and analyze per-head performance on robust vs spurious attributes
  2. Add input gradient diversity regularizer with different λ values and measure impact on feature diversity and OOD performance
  3. Implement head pruning at test time and evaluate on OOD validation set to identify which heads capture spurious vs robust features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiverseViT scale with larger, real-world datasets compared to controlled diagnostic benchmarks?
- Basis in paper: [inferred] The paper states "Our methods should be evaluated on larger-scale real-world datasets in vision, but also language and reinforcement learning."
- Why unresolved: The experiments were conducted on small-scale diagnostic benchmarks (MNIST-CIFAR and Waterbirds), which may not capture the complexity and nuances of real-world data distributions.
- What evidence would resolve it: Evaluating DiverseViT on large-scale datasets like ImageNet, COCO, or real-world domain adaptation tasks would provide insights into its practical effectiveness and limitations.

### Open Question 2
- Question: Can the head pruning procedure be effectively performed without access to labeled OOD data?
- Basis in paper: [explicit] The paper mentions that head selection "requires access to labeled OOD examples" and should be "evaluated with the recent heuristics proposed for OOD model selection."
- Why unresolved: The current head pruning method relies on labeled OOD validation data, which may not be available in real-world scenarios. The paper suggests exploring unsupervised objectives and human feedback as alternatives.
- What evidence would resolve it: Developing and evaluating unsupervised or weakly-supervised methods for identifying spurious attention heads would demonstrate the practical applicability of the pruning approach.

### Open Question 3
- Question: How does the diversity regularizer affect the interpretability and modularity of the learned features across different ViT architectures?
- Basis in paper: [explicit] The paper finds that "ViTs already have an inherent property for modularity: their attention heads rely each on different features" and that diversification leads to "a higher level of specialization of the heads."
- Why unresolved: The experiments were conducted on a specific ViT architecture, and it's unclear how the regularizer's effects generalize to other ViT variants or different model sizes.
- What evidence would resolve it: Conducting ablation studies across various ViT architectures (e.g., different depths, widths, or attention mechanisms) and analyzing the resulting feature modularity and interpretability would clarify the regularizer's broader impact.

## Limitations
- Performance claims based on synthetic benchmarks may not generalize to real-world distribution shifts
- Method requires labeled OOD validation data for head pruning, limiting practical applicability
- Computational overhead of computing input gradients for each attention head could be prohibitive for large-scale models

## Confidence
- High confidence in the mechanism of attention head modularity: The empirical evidence from per-head ablation studies on synthetic benchmarks is strong.
- Medium confidence in the effectiveness of input gradient orthogonality: While the method shows improvements, the theoretical justification for why gradient orthogonality leads to better generalization is limited.
- Medium confidence in practical applicability: The need for OOD validation data and computational overhead are significant practical constraints not fully addressed.

## Next Checks
1. Test DiverseViT on a broader range of OOD scenarios including natural distribution shifts (e.g., different image domains, lighting conditions) beyond the synthetic benchmarks used in the paper.
2. Conduct ablation studies on the importance of having OOD validation data for head pruning - compare against alternative pruning strategies that don't require distribution-specific validation.
3. Measure the computational overhead of the diversity regularizer and head pruning on larger ViT models to assess scalability to real-world applications.