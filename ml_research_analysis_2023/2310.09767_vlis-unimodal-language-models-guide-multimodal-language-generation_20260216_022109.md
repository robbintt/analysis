---
ver: rpa2
title: 'VLIS: Unimodal Language Models Guide Multimodal Language Generation'
arxiv_id: '2310.09767'
source_url: https://arxiv.org/abs/2310.09767
tags:
- vlis
- image
- language
- text-only
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLIS is a novel plug-and-play framework that improves multimodal
  language generation by combining the visual conditioning capability of vision-language
  models with the linguistic understanding of unimodal text-only language models,
  without further training. It extracts pointwise mutual information of image and
  text from a visual-language model and uses the value as an importance sampling weight
  to adjust the token likelihood from a text-only model.
---

# VLIS: Unimodal Language Models Guide Multimodal Language Generation

## Quick Facts
- arXiv ID: 2310.09767
- Source URL: https://arxiv.org/abs/2310.09767
- Reference count: 27
- Primary result: VLIS is a plug-and-play framework that improves multimodal language generation by combining visual conditioning from VLMs with linguistic fluency from text-only LMs using PMI-weighted importance sampling.

## Executive Summary
VLIS is a novel plug-and-play framework that enhances multimodal language generation by decoupling visual conditioning from linguistic fluency. It extracts pointwise mutual information (PMI) between image and text from a visual-language model (VLM) and uses this as an importance sampling weight to adjust token likelihood from a text-only language model. This approach improves vision-language models on diverse tasks including commonsense understanding and complex text generation while maintaining visual conditioning and improving linguistic capabilities like prompt responsiveness.

## Method Summary
VLIS combines a visual-language model (VLM) and a text-only language model (LM) without additional training. It computes PMI between each candidate token and the input image using the VLM, then multiplies this by the smoothed token likelihood from the text-only LM. A fluency mask filters out low-probability tokens before selection. For efficiency, the marginal likelihood is approximated using only two blank images (black and white) rather than integrating over all possible images. The token with the highest resulting score is selected for generation.

## Key Results
- VLIS improves vision-language models on commonsense understanding tasks (WHOOPS, OK-VQA, ScienceQA)
- VLIS enhances complex text generation tasks (Concadia, Image Paragraph Captioning, ROCStories)
- VLIS improves linguistic capabilities such as responsiveness to prompts while maintaining visual conditioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLIS improves multimodal generation by decoupling visual conditioning from linguistic fluency using PMI-weighted importance sampling.
- Mechanism: VLIS multiplies the token likelihood from a text-only model by the exponentiated pointwise mutual information (PMI) between the current token and the image, as computed by a VLM. This selectively boosts tokens that are both linguistically fluent and visually relevant.
- Core assumption: The VLM's PMI score isolates visual relevance from linguistic preference, and the text-only model's likelihood reflects fluent language generation.
- Evidence anchors: [abstract] "It extracts pointwise mutual information of image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model."

### Mechanism 2
- Claim: Using a minimal visual set (black/white images) to approximate the marginal likelihood avoids the computational burden of full marginalization while preserving the PMI signal.
- Mechanism: Instead of integrating over all possible images, VLIS samples from a tiny set of blank images to estimate p_vl(x_t | x_{<t}).
- Core assumption: The marginal likelihood for a token is similar across blank images, so averaging over two suffices as a proxy for the full expectation.
- Evidence anchors: [section] "We use the last method with the least computational overhead... we use two images: a black-filled image c_b and a white-filled image c_w."

### Mechanism 3
- Claim: The fluency mask prevents degenerate text generation by filtering out low-probability tokens before applying PMI weights.
- Mechanism: Tokens with text-only likelihood below threshold α are assigned -inf score, effectively removing them from consideration.
- Core assumption: The text-only model reliably identifies fluent tokens, and extreme PMI values won't compensate for poor fluency.
- Evidence anchors: [section] "To prevent such text degeneracy, we apply a fluency mask to our importance sampling score... only the tokens with text-only likelihood larger than the threshold α are allowed to be selected."

## Foundational Learning

- Concept: Pointwise Mutual Information (PMI)
  - Why needed here: PMI quantifies the association between a token and an image, allowing VLIS to isolate visual relevance from linguistic fluency.
  - Quick check question: What does PMI measure between two random variables, and how does exponentiating it turn it into a weight?

- Concept: Importance Sampling
  - Why needed here: VLIS reweights samples from a proposal distribution (text-only model) using the target distribution (VLM), making the text-only model's tokens more visually aligned.
  - Quick check question: In importance sampling, what is the relationship between nominal, importance, and target distributions?

- Concept: Marginalization in Language Models
  - Why needed here: Computing the marginal likelihood p(x_t) requires integrating over all possible image contexts; VLIS approximates this efficiently.
  - Quick check question: Why is exact marginalization over images computationally infeasible, and how does sampling from a minimal set approximate it?

## Architecture Onboarding

- Component map: Text-only LM -> VLM -> Fluency Mask -> VLIS Scorer
- Critical path: 1. Text-only model generates token scores 2. VLM computes image-conditional likelihood and marginal likelihood 3. PMI is calculated and exponentiated 4. Fluency mask filters tokens 5. VLIS score = smoothed text-only score × exponentiated PMI 6. Token with highest score is selected
- Design tradeoffs: Computational cost vs. accuracy (two blank images vs. more images), fluency threshold (higher α reduces degeneration but may discard useful tokens), language temperature (controls confidence in text-only model)
- Failure signatures: Degenerate text (low fluency threshold or extreme PMI values overpower text-only model), loss of visual alignment (high fluency threshold or low language temperature suppresses visual signals), hallucinations (PMI weights introduce incorrect associations)
- First 3 experiments: 1. Replicate OK-VQA results with different fluency thresholds 2. Compare PMI-based weighting vs. simple product of token likelihoods 3. Vary the number and type of images used for marginal approximation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of using different sets of images for approximating the marginal likelihood in VLIS?
- Basis in paper: Inferred from the "Marginal Approximation Experiment" section in the appendix
- Why unresolved: The paper provides some experimental results but does not fully explore the impact of different image sets
- What evidence would resolve it: A comprehensive study comparing various image sets and their impact on VLIS performance across multiple tasks

### Open Question 2
- Question: How sensitive is VLIS to the choice of text-only language model and VLM pair?
- Basis in paper: Inferred from the "Backbone Scale Experiment" section in the appendix
- Why unresolved: While the paper shows that VLIS generally improves performance, it does not provide a systematic analysis of the importance of choosing the right pair
- What evidence would resolve it: A large-scale study evaluating VLIS with various combinations of text-only LMs and VLMs

### Open Question 3
- Question: Can VLIS be extended to incorporate other modalities beyond images, such as audio or text documents?
- Basis in paper: The "Conclusion" section mentions the possibility of extending VLIS to other modalities
- Why unresolved: The paper focuses on image-to-text generation and does not investigate the applicability of VLIS to other multimodal tasks
- What evidence would resolve it: Experiments applying VLIS to tasks involving other modalities and analyzing the challenges and potential adaptations needed

## Limitations

- The framework's effectiveness depends critically on the VLM's ability to accurately compute PMI between tokens and images
- Using only two blank images for marginal likelihood approximation may be insufficient for accurate estimation
- The optimal fluency threshold α varies across tasks without clear guidance for new applications
- VLIS inherits potential biases from both the VLM and text-only model, particularly the risk of hallucinations

## Confidence

**High Confidence**: VLIS can be applied as a plug-and-play enhancement without additional training (well-supported by experimental results)

**Medium Confidence**: PMI-weighted importance sampling effectively combines visual relevance and linguistic fluency (theoretical support but limited empirical validation)

**Low Confidence**: Using two blank images is sufficient for accurate marginal likelihood approximation (lacks rigorous validation)

## Next Checks

1. **PMI Sensitivity Analysis**: Conduct controlled experiments varying the exponentiated PMI scaling factor to quantify its impact on visual alignment versus linguistic fluency

2. **Marginal Approximation Ablation**: Systematically test different strategies for estimating the marginal likelihood, including using more blank images, random images, or learned approximations

3. **Failure Mode Investigation**: Design experiments that specifically target known failure modes, such as when the VLM hallucinates visual information or when the text-only model's fluency assessment is incorrect