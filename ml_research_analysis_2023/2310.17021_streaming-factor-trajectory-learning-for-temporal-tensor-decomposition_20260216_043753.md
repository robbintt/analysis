---
ver: rpa2
title: Streaming Factor Trajectory Learning for Temporal Tensor Decomposition
arxiv_id: '2310.17021'
source_url: https://arxiv.org/abs/2310.17021
tags:
- data
- factor
- decomposition
- tensor
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a streaming factor trajectory learning method
  for temporal tensor decomposition that models object representations as time-varying
  functions using Gaussian processes, then converts them to state-space models for
  efficient streaming inference. The key innovation is an online filtering algorithm
  based on conditional expectation propagation that maintains decoupled Gaussian approximations
  of factor state posteriors, enabling parallel Rauch-Tung-Striebel smoothing without
  revisiting past data.
---

# Streaming Factor Trajectory Learning for Temporal Tensor Decomposition

## Quick Facts
- arXiv ID: 2310.17021
- Source URL: https://arxiv.org/abs/2310.17021
- Reference count: 40
- Primary result: Superior online and final prediction accuracy compared to state-of-the-art streaming and static decomposition approaches on synthetic and real-world temporal tensor datasets

## Executive Summary
This paper introduces a streaming factor trajectory learning method for temporal tensor decomposition that models object representations as time-varying functions using Gaussian processes. The key innovation is converting these GPs into state-space models via stochastic differential equations, enabling efficient online inference through a conditional expectation propagation framework. This approach maintains decoupled Gaussian approximations of factor state posteriors, allowing parallel Rauch-Tung-Striebel smoothing without revisiting past data. Experiments demonstrate the method achieves superior prediction accuracy compared to existing streaming and static decomposition techniques while maintaining worst-case linear scalability in the number of timestamps.

## Method Summary
The method learns time-varying factor trajectories in tensor decomposition by placing Gaussian process priors on factor representations and converting them to state-space models through spectral analysis. The conditional expectation propagation framework enables efficient online filtering by approximating the coupled likelihood with decoupled product of Gaussians, allowing parallel updates of factor state chains. This architecture supports both online prediction during data streaming and final prediction after all data is processed, with the decoupled structure enabling standard Rauch-Tung-Striebel smoothing without revisiting historical data.

## Key Results
- Consistently outperforms state-of-the-art streaming CP and Tucker decomposition algorithms on online and final predictive performance
- Achieves worst-case linear scalability in the number of timestamps
- Learned factor trajectories exhibit interpretable temporal evolution patterns on real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
The state-space prior via SDE conversion enables efficient streaming inference by avoiding full covariance matrix computation. Converting the GP prior into an LTI-SDE allows the use of Kalman filtering and RTS smoothing, which operate on Gauss-Markov chains with linear complexity in the number of timestamps. The core assumption is that the spectral analysis of the Matérn kernel can be accurately approximated by an LTI-SDE with a polynomial expansion of the frequency response. Break condition: If the polynomial approximation of the spectral density becomes inaccurate for the chosen kernel parameters, the LTI-SDE may not faithfully represent the GP prior.

### Mechanism 2
The conditional expectation propagation (CEP) framework enables online filtering by approximating the coupled likelihood with a decoupled product of Gaussians. CEP uses conditional moment matching to update approximations of the running posterior in parallel, allowing decoupling of factor state chains for independent RTS smoothing. The core assumption is that the factorized structure of the CP/Tucker likelihood allows efficient conditional moment matching within the CEP framework. Break condition: If the conditional moments become too complex to compute accurately, the CEP approximation may degrade, affecting the quality of the factor trajectory estimates.

### Mechanism 3
Learning time-varying factor trajectories improves prediction accuracy compared to static decomposition methods. The GP prior on factor trajectories captures temporal evolution patterns that static methods miss, leading to better generalization in both online and final prediction tasks. The core assumption is that the temporal evolution of factor representations contains valuable predictive information that is not captured by static factor models. Break condition: If the temporal dynamics are weak or non-existent in the data, the added complexity of trajectory learning may not provide significant benefits over static decomposition.

## Foundational Learning

- **Concept: Gaussian Process (GP) priors and their spectral representation**
  - Why needed here: GPs provide flexible, nonparametric priors for modeling time-varying factor trajectories, and spectral analysis enables efficient conversion to state-space models.
  - Quick check question: Can you explain how the spectral density of a Matérn kernel relates to the coefficients of the equivalent LTI-SDE?

- **Concept: State-space models and Kalman filtering**
  - Why needed here: The SDE conversion creates a state-space model that enables efficient streaming inference through Kalman filtering and RTS smoothing.
  - Quick check question: What are the computational advantages of using a Gauss-Markov chain over a full Gaussian process in streaming scenarios?

- **Concept: Expectation Propagation (EP) and conditional EP (CEP)**
  - Why needed here: EP and CEP provide frameworks for approximate Bayesian inference in models with complex, coupled likelihoods like tensor decomposition.
  - Quick check question: How does conditional moment matching in CEP differ from standard moment matching in EP, and why is it useful for our model?

## Architecture Onboarding

- **Component map**: Data stream processor -> SDE converter -> Online filter -> RTS smoother -> Predictor
- **Critical path**: 1) Receive batch of entries at new timestamp, 2) Update running posteriors using CEP-based online filtering, 3) (Optional) Perform RTS smoothing on factor state chains, 4) Generate predictions for test set using current trajectory estimates
- **Design tradeoffs**: Matérn kernel parameters (smoothness, length-scale) vs. computational efficiency and representation flexibility; Rank of decomposition (R) vs. model complexity and prediction accuracy; Frequency of RTS smoothing vs. real-time processing requirements
- **Failure signatures**: Degraded prediction accuracy with increasing stream length (potential issues with SDE approximation or CEP convergence); Unstable online predictions (possible numerical issues in CEP updates or Kalman filtering); Excessive memory usage (incorrect handling of running posterior storage)
- **First 3 experiments**: 1) Test SDE conversion accuracy by comparing learned trajectories with ground truth on synthetic data, 2) Evaluate CEP convergence and approximation quality on a small, controlled tensor stream, 3) Benchmark prediction accuracy and running time against static decomposition methods on a real-world dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of Matérn kernel smoothness parameter (ν) affect the accuracy of learned factor trajectories? The paper notes that "the choice of the length-scale is critical" when using Matérn-3/2 kernel and mentions sensitivity analysis in Table 3, but does not explore how different ν values affect trajectory quality. This remains unresolved because the paper only tests Matérn-1/2 and Matérn-3/2 kernels but doesn't systematically vary ν to understand its impact on trajectory estimation quality. What evidence would resolve it: A comprehensive study varying ν across different smoothness levels while keeping other parameters constant, measuring trajectory reconstruction accuracy on synthetic data.

### Open Question 2
What is the impact of the decoupling approximation in the conditional EP framework on the quality of factor trajectory estimates? The paper uses conditional EP to approximate the posterior, which relies on factorizing approximations, but doesn't provide error bounds or theoretical guarantees for this approximation. This remains unresolved because while the paper shows empirical success, it doesn't quantify how much information is lost through the decoupling approximation or under what conditions it might fail. What evidence would resolve it: Theoretical analysis of the approximation error bounds or empirical studies comparing full posterior estimates with EP approximations on controlled synthetic data.

### Open Question 3
How does the proposed method scale with increasing tensor order (number of modes) beyond the 3-4 mode tensors tested? The paper mentions "worst-case linear scalability" but only tests on tensors with up to 4 modes, leaving scalability to higher-order tensors unexplored. This remains unresolved because the computational complexity analysis focuses on scalability with respect to number of timestamps, not tensor order, and empirical validation is limited to relatively low-order tensors. What evidence would resolve it: Systematic experiments on higher-order tensors (5+ modes) measuring both accuracy and computational efficiency, along with complexity analysis for the multi-mode case.

## Limitations

- The paper relies on spectral approximation methods for SDE conversion, which may introduce errors that affect trajectory quality
- Empirical validation of CEP convergence properties in the tensor decomposition context lacks theoretical guarantees
- Interpretability of learned factor trajectories is claimed but not quantitatively evaluated in experiments

## Confidence

- **High Confidence**: The overall methodology of converting GP priors to state-space models for streaming tensor decomposition is well-established and the computational complexity analysis appears sound.
- **Medium Confidence**: The specific implementation details of the conditional expectation propagation algorithm and its convergence properties in the tensor decomposition context.
- **Low Confidence**: The claim of interpretability of learned factor trajectories, as this is not quantitatively evaluated in the experiments.

## Next Checks

1. **Ablation Study**: Conduct experiments isolating the impact of SDE conversion quality, CEP approximation accuracy, and trajectory learning on prediction performance by comparing against baseline methods that use only one or two of these components.
2. **Scalability Benchmark**: Evaluate the method on high-dimensional tensors (e.g., 100+ modes) with varying ranks to verify the claimed worst-case linear scalability in the number of timestamps holds in practice.
3. **Interpretability Analysis**: Develop quantitative metrics to assess the interpretability of learned factor trajectories, such as correlation with known temporal patterns or consistency across multiple runs, and apply these to the real-world datasets.