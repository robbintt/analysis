---
ver: rpa2
title: 'Contrastive Preference Learning: Learning from Human Feedback without RL'
arxiv_id: '2310.13639'
source_url: https://arxiv.org/abs/2310.13639
tags:
- learning
- policy
- function
- reward
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Contrastive Preference Learning (CPL) is a new method for Reinforcement
  Learning from Human Feedback (RLHF) that avoids the need for reinforcement learning.
  Instead of learning a reward function from human preferences and then optimizing
  it via RL, CPL directly learns the optimal policy using a contrastive objective.
---

# Contrastive Preference Learning: Learning from Human Feedback without RL

## Quick Facts
- arXiv ID: 2310.13639
- Source URL: https://arxiv.org/abs/2310.13639
- Reference count: 40
- Primary result: CPL achieves better performance than RL-based methods while being 1.6× faster and 4× more parameter efficient

## Executive Summary
Contrastive Preference Learning (CPL) introduces a novel approach to Reinforcement Learning from Human Feedback (RLHF) that bypasses the traditional two-phase process of reward learning followed by RL optimization. Instead, CPL directly learns the optimal policy using a contrastive objective based on human preferences. By leveraging the principle of maximum entropy, CPL establishes a bijection between the optimal advantage function and the optimal policy, enabling policy optimization through supervised learning rather than reinforcement learning. This approach demonstrates superior performance on MetaWorld robotics tasks while offering significant computational and parameter efficiency advantages.

## Method Summary
CPL directly optimizes a policy from human preferences without learning an intermediate reward function or using reinforcement learning. The method generates preference datasets from suboptimal rollouts, where human preferences follow a regret-based model under the expert's optimal policy. CPL uses a contrastive objective that maximizes the likelihood of preferred segments over unpreferred ones, leveraging the maximum entropy principle to establish a bijective relationship between advantage functions and policies. A conservative regularizer keeps the policy within the distribution of the preference data. The approach is fully off-policy and can be applied to arbitrary Markov Decision Processes, demonstrating both theoretical elegance and practical efficiency advantages over traditional RLHF methods.

## Key Results
- Outperforms prior RL-based methods in 5 out of 6 MetaWorld environments
- Trains 1.6× faster than RL-based approaches
- Achieves 4× parameter efficiency compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Human preferences follow the regret under the user's optimal policy rather than the discounted sum of rewards. This means preferences directly indicate how much worse one action is compared to the optimal action, which is exactly what the advantage function measures. The regret preference model provides direct information about the optimal advantage function, enabling policy learning without intermediate reward modeling.

### Mechanism 2
Under maximum entropy RL, the optimal policy is proportional to the exponential of the optimal advantage function (π*(a|s) = e^A*(s,a)/α). This bijective relationship means learning the optimal advantage function is equivalent to learning the optimal policy. This theoretical foundation allows CPL to transform preference learning into a supervised learning problem.

### Mechanism 3
CPL substitutes the advantage function with the log-probability of the policy in the regret preference model, transforming preference learning into a supervised contrastive learning problem. The policy is optimized to maximize the likelihood of preferred segments over unpreferred ones using a Noise Contrastive Estimation objective. This contrastive approach directly optimizes the policy without requiring reinforcement learning.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: CPL is a new method within the RLHF paradigm that avoids traditional reward learning and RL optimization
  - Quick check question: What are the two traditional phases of RLHF algorithms, and how does CPL eliminate the need for one of them?

- Concept: Maximum Entropy Reinforcement Learning
  - Why needed here: The principle of maximum entropy provides the theoretical foundation for the bijection between advantage functions and policies
  - Quick check question: What is the relationship between the optimal policy and optimal advantage function under maximum entropy RL?

- Concept: Contrastive Learning
  - Why needed here: CPL uses a contrastive objective similar to Noise Contrastive Estimation to learn from preferences
  - Quick check question: How does CPL's objective relate to standard contrastive learning frameworks like InfoNCE?

## Architecture Onboarding

- Component map: Preference dataset → Policy network (πθ) → Evaluation
- Critical path: Preference dataset → Policy network training → Evaluation
  - Data collection: Generate suboptimal rollouts, create pairwise comparisons
  - Model training: Optimize policy using CPL objective with conservative regularization
  - Evaluation: Test policy performance on the target task
- Design tradeoffs:
  - Direct policy learning vs. reward learning + RL: CPL is simpler and faster but requires accurate preference labels
  - Conservative regularization strength (λ): Higher values keep policy closer to data but may limit exploration
  - Segment size: Larger segments provide more context but require more memory and computation
- Failure signatures:
  - Poor performance despite convergence: Preference labels may be inaccurate or insufficient
  - Policy overfits to training data: Conservative regularization λ may be too low
  - High variance in results: Insufficient number of preference comparisons or poor quality data
- First 3 experiments:
  1. Implement CPL on a simple gridworld environment with synthetic preferences to verify basic functionality
  2. Compare CPL performance against reward learning + RL on a continuous control task with state-based observations
  3. Test CPL with different conservative regularization strengths (λ) on the same task to find optimal value

## Open Questions the Paper Calls Out

### Open Question 1
How does CPL's performance scale with increasingly larger and more complex models, such as those used in large language models or more sophisticated robotics tasks? While the paper mentions that CPL's supervised objective could scale better than RLHF methods that use traditional RL algorithms, and that it would be interesting to apply CPL to LLMs for fine-tuning on multi-turn dialogue, the paper does not provide empirical evidence for this claim. The paper only evaluates CPL on MetaWorld robotics tasks with relatively small models. Scaling to larger models and more complex tasks would require significant computational resources and experimental validation.

### Open Question 2
How sensitive is CPL to the choice of the temperature parameter α and the bias regularizer λ? Are there principled ways to select these hyperparameters, or do they require extensive tuning for each task? The paper includes ablation studies on the impact of α and λ on CPL's performance, showing that the choice of these hyperparameters can affect the results. However, it does not provide a systematic approach for selecting them. Developing a principled method for selecting α and λ, such as a learning-based approach or a set of heuristics based on the characteristics of the task and dataset, would help address this open question.

### Open Question 3
How does CPL perform when the human preferences are not perfectly aligned with the regret-based model? In real-world scenarios, human preferences might be noisy, inconsistent, or based on factors beyond regret. The paper assumes that human preferences follow the regret-based model and does not explore the impact of deviations from this assumption. Understanding CPL's robustness to different types of preference noise would be valuable for real-world applications. Conducting experiments with synthetic preference data that deviates from the regret-based model in various ways would help assess CPL's performance under more realistic conditions.

## Limitations
- Requires suboptimal rollouts to generate preference data, which may not be available in real human feedback scenarios
- Conservative regularization is critical but its optimal strength is task-dependent and not systematically explored
- Performance gains come at the cost of requiring pairwise preference labels, which can be expensive to collect

## Confidence
- **High confidence**: CPL successfully eliminates the need for intermediate reward learning and achieves faster training than RL-based methods
- **Medium confidence**: The claim that CPL outperforms prior methods in 5/6 environments, as results depend heavily on preference data quality and labeling methodology
- **Medium confidence**: The parameter efficiency claim (4× improvement) is supported by ablation studies but may vary with implementation details

## Next Checks
1. Conduct user studies to empirically verify whether human preferences follow the regret-based model rather than reward-based preferences
2. Test CPL's robustness to noisy or inconsistent preference labels by varying preference data quality
3. Evaluate CPL's performance when trained on truly human-generated preferences rather than oracle-generated regret labels