---
ver: rpa2
title: Polynomial Time Cryptanalytic Extraction of Neural Network Models
arxiv_id: '2310.08708'
source_url: https://arxiv.org/abs/2310.08708
tags:
- layer
- neuron
- neurons
- network
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extracting all parameters of
  deep neural networks (DNNs) with ReLU activations from black-box access, an open
  problem studied for 30+ years. The best previous approach (Carlini et al., Crypto
  2020) required polynomial queries but exponential time due to inability to determine
  the sign of each neuron.
---

# Polynomial Time Cryptanalytic Extraction of Neural Network Models

## Quick Facts
- arXiv ID: 2310.08708
- Source URL: https://arxiv.org/abs/2310.08708
- Reference count: 21
- Primary result: Polynomial-time extraction of all parameters from ReLU DNNs with black-box access

## Executive Summary
This paper solves a 30+ year open problem in neural network cryptanalysis by developing polynomial-time algorithms to extract all parameters from deep neural networks with ReLU activations using only black-box access. The key breakthrough is developing three techniques to recover the sign of each neuron in polynomial time, overcoming the exponential barrier of previous approaches. Applied to a full-sized CIFAR10 network, these techniques successfully extract all ~1.2 million parameters in about 30 minutes on a 256-core computer.

## Method Summary
The authors develop three polynomial-time techniques for recovering neuron signs in ReLU networks: System of Equations (SOE) for the first two layers, Neuron Wiggle for middle layers, and Last Hidden Layer for the final hidden layer. These techniques replace the previous exponential-time exhaustive search (2^256 possibilities) with polynomial algorithms requiring O(d^3) time per layer. The attack works by sequentially peeling off layers, using the appropriate technique for each layer based on the network's expansion properties and available degrees of freedom.

## Key Results
- Successfully extracts all parameters from a 3072-256(8)-10 CIFAR10 network in ~30 minutes on 256 cores
- SOE handles layers 1-2 in ~16 seconds per layer using system of linear equations
- Neuron Wiggle handles layers 3-7 in ~234 seconds per neuron using statistical analysis of output variations
- Last Hidden Layer technique handles layer 8 in ~189 seconds using second-order derivatives
- Achieves polynomial time complexity (O(d^3)) versus previous exponential approaches

## Why This Works (Mechanism)

### Mechanism 1: System of Equations (SOE)
- Claim: Polynomial-time recovery of neuron signs via solving linear equations in first two layers when space of control is sufficiently large
- Mechanism: For inputs in linear neighborhood of a point, the mapping from input space to layer i values is an affine transformation. With full rank, querying multiple points allows constructing solvable systems of equations for all neuron signs simultaneously
- Core assumption: Number of degrees of freedom at layer input ≥ number of neurons in layer (d(i-1)_x ≥ di)
- Evidence anchors: SOE handles layers 1-2 in ~16 seconds per layer; technique simpler for first hidden layer when network is not expanding
- Break condition: Network is expansive or earlier layers have insufficient degrees of freedom

### Mechanism 2: Neuron Wiggle
- Claim: Statistical recovery of neuron signs by applying small input changes that maximally affect target neuron while minimizing effects on others
- Mechanism: Wiggle parallel to target neuron's weight vector maximizes its change while others experience minimal changes. Output differences at critical points in both wiggle directions reveal sign based on which direction produces larger output change
- Core assumption: Wiggle can maximize target neuron's change while others experience relatively small changes; output coefficients for target neuron are sufficiently large
- Evidence anchors: Neuron Wiggle requires 3sdi queries and O(sdid^3) time; uses critical points and wiggling in opposite directions
- Break condition: Layer too expansive relative to degrees of freedom, or target neuron has very small output coefficients

### Mechanism 3: Last Hidden Layer
- Claim: Exact recovery of neuron signs in final hidden layer by exploiting fixed linear mapping to output and using second-order derivatives
- Mechanism: Output coefficients from last hidden layer to output are constant across all inputs. Querying at multiple random inputs constructs linear equations whose unique solution yields all neuron signs
- Core assumption: Output layer is linear (no ReLUs), making output coefficients constant across all inputs
- Evidence anchors: Last Hidden Layer uses second-order derivatives to construct linear equations; allows construction of arbitrarily many equations with unique solution
- Break condition: Output layer contains non-linear activations

## Foundational Learning

- Concept: Critical points in ReLU networks
  - Why needed here: Critical points are where exactly one neuron changes sign, causing abrupt output changes that can be detected. Essential for finding neuron signatures and neuron wiggle technique
  - Quick check question: What defines a critical point for a neuron, and why are they useful for extraction attacks?

- Concept: Space of control and degrees of freedom
  - Why needed here: Space of control determines how much we can manipulate input to a hidden layer. Directly affects which sign recovery techniques are applicable and their effectiveness
  - Quick check question: How does the number of degrees of freedom at layer i relate to the rank of the collapsed matrix F(i-1)_x?

- Concept: Second-order derivatives in piecewise linear functions
  - Why needed here: Second derivatives provide output coefficients that are constant across inputs for last hidden layer, enabling construction of solvable linear systems
  - Quick check question: What property of ReLU activation function makes second derivatives useful for extracting last-layer signs?

## Architecture Onboarding

- Component map: Signatures extraction -> Sign recovery (SOE/NW/LHL) -> Layer peeling -> Next layer -> Output layer
- Critical path: Sequential processing of layers where each layer must be completely processed before moving to next. Neuron wiggle technique dominates runtime for middle layers
- Design tradeoffs: SOE fastest but only works for first two layers; Neuron Wiggle works broadly but requires statistical sampling and is slower; Last Hidden Layer specialized but very efficient for that layer
- Failure signatures: SOE fails when system of equations is underdetermined (rank too low); Neuron Wiggle fails when sign recovery has low confidence even with many samples; Last Hidden Layer fails when system doesn't have unique solution
- First 3 experiments:
  1. Implement SOE on small non-expanding network (10-20-20-1) to verify it correctly recovers signs for first two layers in polynomial time
  2. Implement Neuron Wiggle on slightly larger network (100-200-200-10) to verify sign recovery works on middle layers with statistical sampling
  3. Implement Last Hidden Layer on any network to verify it correctly recovers all signs in final hidden layer using second-order derivatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed sign recovery techniques be extended to work with other activation functions beyond ReLU, such as Leaky ReLU or Sigmoid?
- Basis in paper: The attack can be generalized to any activation function that looks like a piecewise linear function, but it cannot be applied to smooth activation functions like Sigmoid which have no critical points
- Why unresolved: Paper does not provide experimental results or detailed analysis of how techniques would perform with other activation functions
- What evidence would resolve it: Implementing and testing sign recovery techniques on networks with Leaky ReLU or Sigmoid activations to compare performance and success rates

### Open Question 2
- Question: How would the sign recovery techniques perform on neural networks with skip connections or residual connections, such as ResNet architectures?
- Basis in paper: The attack can be applied without modifications to convolutional networks, but paper does not discuss networks with skip connections or residual connections
- Why unresolved: Paper does not provide experimental results or analysis of how techniques would handle additional complexity introduced by skip connections
- What evidence would resolve it: Implementing and testing sign recovery techniques on networks with skip connections to evaluate their effectiveness and identify potential challenges

### Open Question 3
- Question: Can the sign recovery techniques be adapted to work with discrete input domains, such as text data in language models, where derivatives are not well-defined?
- Basis in paper: Dealing with discrete domains like texts in LLMs where derivatives are not well-defined is left as an open problem
- Why unresolved: Paper does not provide any insights or suggestions on how to approach this problem, as techniques heavily rely on use of derivatives
- What evidence would resolve it: Developing novel techniques that can work with discrete input domains and testing their effectiveness on language models or other text-based neural networks

## Limitations
- Architecture constraints: Techniques designed for fully-connected ReLU networks; extension to convolutional layers, batch normalization, or other activation functions requires additional work
- Numerical precision: Extraction relies on precise computation of derivatives and solving linear systems; floating-point precision and numerical stability could affect success rates
- Query efficiency: While polynomial in time complexity, techniques still require substantial queries (e.g., ~12.8M for middle layers) which could be detectable as attack vector

## Confidence

**High confidence** (evidence strongly supports claims):
- Polynomial time complexity claims for all three techniques are well-supported by algorithmic analysis
- CIFAR10 extraction results demonstrating ~30 minute runtime on 256 cores are directly stated and appear reproducible

**Medium confidence** (claims are plausible but require verification):
- Effectiveness of SOE technique on layers 1-2 (only ~16 seconds per layer) seems surprisingly fast given system solving complexity
- Statistical analysis for Neuron Wiggle successfully achieving high confidence across all neurons needs empirical validation

**Low confidence** (claims have limited supporting evidence):
- Paper's discussion of failure conditions is minimal - how often do these techniques fail in practice?
- Impact of numerical precision issues on extraction accuracy is not explored

## Next Checks

1. **Numerical precision sensitivity**: Run extraction pipeline on networks with varying numerical precision (float32 vs float64) to measure how precision affects sign recovery accuracy and runtime

2. **Scalability testing**: Test techniques on networks larger than CIFAR10 (e.g., 4096-1024(8)-1000 ImageNet-style architecture) to verify polynomial scaling holds and identify practical limits

3. **Adversarial robustness**: Evaluate whether simple defenses like input quantization or noise injection can significantly degrade extraction success rates without harming model accuracy