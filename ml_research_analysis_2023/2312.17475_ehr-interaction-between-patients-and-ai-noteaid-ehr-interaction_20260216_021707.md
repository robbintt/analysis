---
ver: rpa2
title: 'EHR Interaction Between Patients and AI: NoteAid EHR Interaction'
arxiv_id: '2312.17475'
source_url: https://arxiv.org/abs/2312.17475
tags:
- patient
- medical
- explanation
- agent
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NoteAid EHR Interaction Pipeline, a system
  using large language models to enhance patient education through interactive question-answering
  and text explanation of Electronic Health Records. The pipeline employs two LLM
  agents simulating patient and medical assistant roles to generate conversational
  interactions.
---

# EHR Interaction Between Patients and AI: NoteAid EHR Interaction

## Quick Facts
- arXiv ID: 2312.17475
- Source URL: https://arxiv.org/abs/2312.17475
- Reference count: 35
- Key outcome: GPT-4-generated dataset achieved >80% highest quality level for patient-EHR interactions

## Executive Summary
This paper introduces the NoteAid EHR Interaction Pipeline, a system that uses large language models to enhance patient education through interactive question-answering and text explanation of Electronic Health Records. The pipeline employs two LLM agents simulating patient and medical assistant roles to generate conversational interactions. Experiments using GPT-3.5 and GPT-4 on MIMIC-III and MADE datasets produced 43,504 instances of Q&A and explanation dialogues, with GPT-4-generated data achieving high quality ratings of over 80% at the highest quality level.

## Method Summary
The NoteAid EHR Interaction Pipeline processes Electronic Health Records through a dual-agent conversation system. Two LLM agents engage in three rounds of dialogue: a Mock Patient Agent generates questions or identifies confusing content, while an Assistant Agent provides answers or explanations. The system uses GPT-3.5 and GPT-4 to generate interactions from MIMIC-III discharge summaries (10,000 instances) and MADE medical notes (876 instances). Generated datasets undergo evaluation through both LLM assessment and human evaluation using a comprehensive five-dimensional rubric covering relevance, factuality, sufficiency, concision, and fluency.

## Key Results
- GPT-4-generated data achieved over 80% scoring at the highest quality level
- The system produced 43,504 instances of Q&A and explanation dialogues across MIMIC-III and MADE datasets
- LLM evaluation and human assessment validated the pipeline's effectiveness for patient education applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-agent conversation pipeline enables realistic patient-AI interactions for EHR education
- Mechanism: Two separate LLM agents simulate patient and medical assistant roles, generating three rounds of dialogue where each agent maintains consistent contextual memory through shared EHR content and accumulated conversation history
- Core assumption: LLMs can convincingly simulate patient questions and provide medically accurate explanations when given appropriate context
- Evidence anchors:
  - [abstract] "we introduced another LLM model as an agent, portraying the role of a patient, to validate the effectiveness of LLMs in enhancing patients' medical education"
  - [section 3.2] "the first round of conversation includes the EHR Note, along with the inputs and outputs of the two agents. In the second round of conversation, the historical records of each agent from the first round are differentiated, organized, and utilized as references"
- Break condition: If LLMs cannot maintain coherent context across multiple conversation turns, or if the simulated patient questions become repetitive or unrealistic

### Mechanism 2
- Claim: The pipeline generates high-quality synthetic training data for patient education systems
- Mechanism: By using GPT-4 to generate the interactions and then evaluating the outputs through both LLM assessment and human evaluation, the system produces datasets where over 80% of instances achieve the highest quality level
- Core assumption: LLM-generated data can achieve quality comparable to or exceeding human-generated data for educational purposes
- Evidence anchors:
  - [abstract] "GPT-4-generated data achieved high quality with over 80% scoring at the highest quality level"
  - [section 4.3] "the NoteAid EHR Interaction Dataset generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, consistently exhibits high quality"
- Break condition: If evaluation criteria are too lenient or if quality assessment methods are not robust to LLM biases

### Mechanism 3
- Claim: The structured evaluation criteria ensure consistent quality assessment across different evaluation methods
- Mechanism: The evaluation uses a comprehensive rubric with five dimensions (Relevance, Factuality, Sufficiency, Concision, Fluency) that can be applied consistently by both LLMs and human evaluators
- Core assumption: The five-dimensional rubric captures the essential aspects of quality for patient education interactions
- Evidence anchors:
  - [section 4.3] "The criteria Table 3 and quality level in Figure 2 for evaluation were established after discussions with medical students who have clinical experience"
  - [appendix A] Detailed breakdown of each evaluation criterion with specific scoring rules
- Break condition: If the evaluation criteria miss important aspects of patient education quality or if human evaluators apply criteria inconsistently

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities in specialized domains
  - Why needed here: The entire pipeline relies on LLMs' ability to understand medical context and generate coherent, accurate responses
  - Quick check question: What are the key differences between GPT-3.5 and GPT-4 that might explain the quality differences observed in this study?

- Concept: Electronic Health Records (EHR) structure and content
  - Why needed here: The pipeline processes discharge summaries and medication notes, requiring understanding of medical terminology and record formats
  - Quick check question: What are the main sections typically found in discharge instructions, and how might this structure affect the types of questions patients would ask?

- Concept: Reinforcement learning and multi-agent systems
  - Why needed here: The dual-agent approach where agents learn from each other's outputs through conversation history
  - Quick check question: How does maintaining conversation history across rounds improve the quality of generated interactions compared to single-turn generation?

## Architecture Onboarding

- Component map:
  - Input layer: EHR datasets (MIMIC-III discharge summaries, MADE medical notes)
  - Agent 1: Mock Patient Agent (generates questions or selects confusing content)
  - Agent 2: Assistant Agent (provides answers or explanations)
  - Context manager: Maintains and passes conversation history between rounds
  - Output layer: Structured dataset with three rounds of dialogue per instance
  - Evaluation module: LLM assessment and human evaluation

- Critical path: EHR data → Agent 1 generation → Agent 2 response → Context update → Repeat → Dataset storage → Evaluation

- Design tradeoffs:
  - Two agents vs single agent: Two agents allow more realistic simulation but increase complexity and resource usage
  - Three conversation rounds: Balances depth of interaction with computational efficiency
  - GPT-4 vs GPT-3.5: Higher quality at increased cost and potential hallucination risks

- Failure signatures:
  - Repetitive or unrealistic patient questions
  - Factual errors in medical explanations
  - Loss of context across conversation rounds
  - Evaluation inconsistencies between LLM and human assessments

- First 3 experiments:
  1. Run pipeline on a small subset of MIMIC-III (50 records) to verify conversation flow and context maintenance
  2. Compare outputs from GPT-3.5 vs GPT-4 on the same 10 records to quantify quality differences
  3. Validate evaluation rubric by having two independent human evaluators score the same 20 instances to check inter-rater reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NoteAid EHR Interaction Pipeline's performance compare to human doctors in terms of patient education effectiveness?
- Basis in paper: [inferred] The paper focuses on evaluating the pipeline's performance using LLM and human evaluations, but does not directly compare it to human doctors.
- Why unresolved: The paper only evaluates the pipeline's performance against LLM-generated data and human evaluators, without benchmarking against actual human doctors.
- What evidence would resolve it: A direct comparison study where the pipeline's outputs are evaluated alongside those of human doctors by patients or medical experts.

### Open Question 2
- Question: What are the long-term effects of using the NoteAid EHR Interaction Pipeline on patient health outcomes and adherence to treatment plans?
- Basis in paper: [inferred] The paper demonstrates the pipeline's potential to improve patient understanding of medical records but does not explore its impact on long-term health outcomes.
- Why unresolved: The study focuses on the immediate quality of interactions and data generation, not on longitudinal patient outcomes.
- What evidence would resolve it: A long-term clinical study tracking patient health outcomes and treatment adherence after using the pipeline over an extended period.

### Open Question 3
- Question: How does the NoteAid EHR Interaction Pipeline handle rare or complex medical conditions that may require specialized knowledge?
- Basis in paper: [inferred] The paper uses MIMIC-III and MADE datasets but does not specifically address the pipeline's performance on rare or complex conditions.
- Why unresolved: The datasets used are general medical records, and the paper does not discuss the pipeline's adaptability to specialized medical scenarios.
- What evidence would resolve it: Testing the pipeline on a dataset containing rare or complex medical cases and evaluating its ability to provide accurate and helpful explanations.

## Limitations

- The study relies entirely on LLM-generated data without direct validation against actual patient interactions or comprehension outcomes
- Claims about actual patient education benefits were not directly measured with real patients
- The 80% quality threshold represents self-referential evaluation where LLMs assess their own outputs

## Confidence

- **High Confidence**: The technical pipeline implementation and dataset generation methodology are well-defined and reproducible
- **Medium Confidence**: The LLM evaluation results showing 80% high-quality output, as these depend on the chosen evaluation criteria which may not fully capture real-world effectiveness
- **Low Confidence**: Claims about actual patient education benefits and improved medical record comprehension, as these were not directly measured with real patients

## Next Checks

1. Conduct a controlled user study with actual patients to assess comprehension of LLM-generated explanations versus traditional discharge instructions
2. Implement cross-validation by having independent medical experts evaluate a subset of generated interactions for factual accuracy and clinical appropriateness
3. Test the pipeline's performance on diverse EHR types beyond discharge summaries, including imaging reports and lab results, to assess generalizability