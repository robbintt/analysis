---
ver: rpa2
title: 'AI Alignment and Social Choice: Fundamental Limitations and Policy Implications'
arxiv_id: '2310.16048'
source_url: https://arxiv.org/abs/2310.16048
tags:
- rlhf
- human
- voting
- aligned
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that aligning AI agents using Reinforcement Learning
  with Human Feedback (RLHF) to democratic norms is fundamentally limited by two impossibility
  theorems from social choice theory. First, Arrow's theorem implies no unique voting
  rule can align RLHF models while treating all users equally.
---

# AI Alignment and Social Choice: Fundamental Limitations and Policy Implications

## Quick Facts
- arXiv ID: 2310.16048
- Source URL: https://arxiv.org/abs/2310.16048
- Reference count: 30
- Key outcome: This paper shows that aligning AI agents using Reinforcement Learning with Human Feedback (RLHF) to democratic norms is fundamentally limited by two impossibility theorems from social choice theory. First, Arrow's theorem implies no unique voting rule can align RLHF models while treating all users equally. Second, Sen's theorem shows that it is impossible to build an RLHF model democratically that respects the private ethical preferences of every individual user. The key policy implication is that universally aligned AI is impossible; instead, developers should focus on building narrowly aligned models for specific user groups, and mandate transparency around voting rules to enable accountability.

## Executive Summary
This paper establishes fundamental theoretical limits on democratic AI alignment through RLHF by connecting it to impossibility theorems from social choice theory. Using Arrow's theorem, the authors show that no voting protocol can simultaneously treat all users equally while producing a unique, fair alignment outcome. Sen's theorem further demonstrates that respecting all individual ethical preferences while achieving collective alignment is impossible. The paper concludes that universal AI alignment is theoretically unattainable, and recommends that developers focus on narrow alignment for specific user groups while implementing transparent voting protocols.

## Method Summary
The paper formalizes RLHF with multiple human reinforcers as a preference aggregation problem and applies Arrow's impossibility theorem to show that no voting protocol can satisfy all democratic norms simultaneously. It then extends this analysis using Sen's theorem to demonstrate that universal alignment respecting all individual preferences is impossible when users have protected domains of ethical preferences. The theoretical framework maps preference rankings over trajectory segments to social choice theory's preference aggregation problem.

## Key Results
- Arrow's theorem implies no unique voting rule can align RLHF models while treating all users equally
- Sen's theorem shows universal AI alignment respecting all individual preferences is impossible
- Different voting protocols produce different aligned AI models even with identical reinforcer preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF with multiple reinforcers inherits impossibility constraints from social choice theory
- Mechanism: When multiple human reinforcers provide preference rankings over trajectory segments, the aggregation of these rankings into a collective preference relation faces the same impossibility barriers as democratic voting systems
- Core assumption: Reinforcer preferences are complete, transitive, and independent across pairwise comparisons of trajectory segments
- Evidence anchors:
  - [abstract] "Building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align AI systems using RLHF through democratic processes."
  - [section] "Arrow's impossibility theorem states that no aggregation rule can simultaneously satisfy all four axioms listed above."
- Break condition: If reinforcer preferences are not complete/transitive, or if preferences are interdependent rather than independent across alternatives

### Mechanism 2
- Claim: Universal AI alignment respecting all individual preferences is impossible due to Sen's theorem
- Mechanism: When each user has private ethical preferences in certain domains, aggregating these preferences democratically will violate at least one user's private domain preferences, making universal alignment impossible
- Core assumption: Users have protected domains of preferences that should be respected independently of majority preferences
- Evidence anchors:
  - [abstract] "Further, we show that aligning AI agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal AI alignment using RLHF is impossible."
  - [section] "Sen's theorem implies that any choice C satisfying universal domain and Pareto can respect the individual rights of at most one individual."
- Break condition: If private preference domains don't exist, or if the protected domain constraint is relaxed

### Mechanism 3
- Claim: Different voting protocols produce different aligned AI models even with identical reinforcer preferences
- Mechanism: Since no unique voting rule satisfies democratic norms, different model developers can choose different aggregation methods, leading to inconsistent alignment outcomes across models trained on the same reinforcer data
- Core assumption: Model developers have discretion in choosing voting/aggregation protocols and may not disclose them
- Evidence anchors:
  - [abstract] "The choice of voting rule for each developer is private, i.e., there is no sharing of voting protocol between the developers."
  - [section] "Consider multiple AI model developers/companies who want to build aligned AI agents. The choice of voting rule for each developer is private."
- Break condition: If all developers standardize on a single disclosed voting protocol

## Foundational Learning

- Concept: Social choice theory and voting impossibility theorems
  - Why needed here: The paper's core arguments rely on Arrow's and Sen's theorems to establish fundamental limits on democratic AI alignment
  - Quick check question: What are the four axioms in Arrow's theorem, and why can't they all be satisfied simultaneously?

- Concept: Reinforcement Learning from Human Feedback (RLHF) mechanics
  - Why needed here: Understanding how human feedback is aggregated into reward functions is essential to grasp the alignment limitations
  - Quick check question: How does the preference aggregation function f(Ï) transform individual reinforcer rankings into collective preferences?

- Concept: Protected domains and individual rights in preference aggregation
  - Why needed here: Sen's theorem specifically addresses the conflict between individual rights and social welfare, which is central to the alignment impossibility result
  - Quick check question: What is the minimal liberalism axiom, and how does it conflict with Pareto efficiency in Sen's framework?

## Architecture Onboarding

- Component map: Trajectory segment generation -> Human preference collection -> Reward model training -> Policy fine-tuning -> Output generation
- Critical path: Human preference collection -> Reward model training -> Policy fine-tuning (alignment quality depends on how preferences are aggregated)
- Design tradeoffs: Balancing between respecting individual preferences (liberalism) and achieving consensus (Pareto efficiency) creates fundamental architectural tension
- Failure signatures: Inconsistent model outputs across similar queries, inability to satisfy diverse user groups simultaneously, unexpected model behavior when private preferences conflict
- First 3 experiments:
  1. Test different voting protocols (plurality, Borda count, random dictator) on the same preference data to observe alignment variation
  2. Simulate protected domain preferences to verify Sen's theorem holds in RLHF context
  3. Measure alignment consistency across multiple RLHF models trained on identical reinforcer data but different aggregation methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific voting rule should be universally adopted by AI developers to ensure democratic alignment through RLHF?
- Basis in paper: [explicit] The paper highlights the need for a standardized voting rule but does not specify which one should be adopted
- Why unresolved: Different voting rules can lead to different AI alignments, and there is no clear consensus on which rule would best represent democratic values
- What evidence would resolve it: Empirical studies comparing the outcomes of different voting rules in RLHF alignment scenarios, considering factors like fairness, representativeness, and alignment accuracy

### Open Question 2
- Question: How can AI models be designed to balance universal alignment with respect for individual user preferences?
- Basis in paper: [inferred] The paper suggests that universal AI alignment is impossible due to Sen's theorem, implying a need to balance collective and individual preferences
- Why unresolved: There is a tension between aligning AI to group norms and respecting individual ethical preferences, which are not fully addressed in current RLHF methodologies
- What evidence would resolve it: Development of hybrid models that incorporate both collective and individual preference mechanisms, validated through user studies and ethical assessments

### Open Question 3
- Question: What governance mechanisms can ensure transparency and accountability in RLHF processes?
- Basis in paper: [explicit] The paper discusses the need for transparent voting rules but does not specify governance mechanisms
- Why unresolved: The lack of standardized practices for reporting and auditing RLHF processes poses challenges for accountability and trust in AI systems
- What evidence would resolve it: Implementation and evaluation of governance frameworks that mandate disclosure of voting rules and RLHF methodologies, along with independent audits and public reporting

### Open Question 4
- Question: How can narrow AI alignment be effectively implemented to serve specific user groups without compromising broader ethical standards?
- Basis in paper: [inferred] The paper suggests focusing on narrowly aligned models but does not provide a roadmap for implementation
- Why unresolved: There is a risk that narrow alignment could lead to ethical blind spots or biases if not carefully managed
- What evidence would resolve it: Case studies of narrow AI alignment projects that successfully balance user-specific needs with ethical guidelines, along with metrics for evaluating ethical compliance

## Limitations

- The analysis assumes human preferences are complete, transitive, and independent, which may not reflect real-world preference elicitation
- The framework focuses on static preference profiles rather than dynamic preference evolution during training
- The policy recommendations require empirical validation beyond theoretical impossibility results

## Confidence

- High Confidence: The formal application of Arrow's and Sen's theorems to RLHF preference aggregation - these mathematical results are well-established and the mapping to RLHF is straightforward
- Medium Confidence: The claim that developers will choose different undisclosed voting protocols - while plausible, actual industry practice may converge on standard protocols
- Low Confidence: The policy implications regarding narrow alignment being superior - this requires empirical validation beyond the theoretical impossibility results

## Next Checks

1. Conduct user studies measuring actual preference completeness and transitivity in RLHF contexts to validate the social choice theory assumptions
2. Implement and compare multiple voting protocols (plurality, Borda count, random dictator) on the same RLHF preference data to measure alignment variation
3. Survey AI developers to determine actual disclosure practices around voting protocol selection and whether standard protocols are emerging in the industry