---
ver: rpa2
title: A General Framework for User-Guided Bayesian Optimization
arxiv_id: '2311.14645'
source_url: https://arxiv.org/abs/2311.14645
tags:
- prior
- optimization
- functions
- bayesian
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ColaBO, a framework for incorporating user
  beliefs beyond the typical GP kernel structure into Bayesian optimization. ColaBO
  allows users to specify priors over function properties such as the likely location
  of the optimizer or the optimal value.
---

# A General Framework for User-Guided Bayesian Optimization

## Quick Facts
- arXiv ID: 2311.14645
- Source URL: https://arxiv.org/abs/2311.14645
- Reference count: 40
- Primary result: ColaBO is a framework for incorporating user beliefs beyond the typical GP kernel structure into Bayesian optimization, allowing specification of priors over function properties such as the likely location of the optimizer or the optimal value.

## Executive Summary
This paper proposes ColaBO, a framework for incorporating user beliefs beyond the typical GP kernel structure into Bayesian optimization. ColaBO allows users to specify priors over function properties such as the likely location of the optimizer or the optimal value. The framework introduces a belief-weighted prior over functions, where the distribution over the optimum is exactly the user-specified prior. ColaBO works with Monte Carlo acquisition functions and demonstrates competitive performance for well-located priors, substantially accelerating optimization. It also shows greater robustness than prior work when applied to detrimental priors, retaining approximately baseline performance. The experiments show ColaBO's effectiveness on synthetic functions with known priors and hyperparameter tuning tasks. The main limitation is the computational expense due to multiple Monte Carlo steps.

## Method Summary
ColaBO introduces a belief-weighted prior over functions, where the distribution over the optimum is exactly the user-specified prior. The framework augments the standard GP prior with a belief-weighted prior via rejection sampling, so that the posterior over functions reflects the user's subjective belief about function properties like optimum location or optimal value. ColaBO uses Monte Carlo acquisition functions by integrating user beliefs into the pathwise sampling procedure, biasing function draws from the prior according to ρ, then updating them deterministically with observed data. This produces a posterior that Monte Carlo acquisition functions can sample from. The method is applicable to various types of user beliefs and MC acquisition functions, offering flexibility in incorporating prior knowledge.

## Key Results
- ColaBO substantially accelerates optimization when user beliefs are accurate, demonstrating significant speed-up compared to vanilla BO methods.
- ColaBO retains approximately baseline performance when user beliefs are misleading, showing greater robustness than prior work.
- The framework is applicable across different Monte Carlo acquisition functions and types of user beliefs, offering flexibility in incorporating prior knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ColaBO improves BO efficiency by introducing a user belief-weighted prior over functions, reshaping the Gaussian process to align with the practitioner's expectations about the optimizer location or optimal value.
- Mechanism: The framework augments the standard GP prior with a belief-weighted prior via rejection sampling, so that the posterior over functions reflects the user's subjective belief about function properties like optimum location or optimal value.
- Core assumption: The user belief ρ is independent of observed data and strictly positive over the input space, ensuring any function can still be optimal but with different likelihoods.
- Evidence anchors:
  - [abstract] "ColaBO allows users to specify priors over function properties such as the likely location of the optimizer or the optimal value."
  - [section] "We introduce ColaBO, a framework for incorporating user knowledge over the optimizer, optimal value and preference relations into Bayesian optimization in the form of an additional prior on the surrogate, orthogonal to the conventional prior on the kernel hyperparameters."
  - [corpus] Weak corpus match on "belief-weighted prior" but related concept in "Domain Knowledge Injection in Bayesian Search for New Materials" suggests general interest in augmenting BO with user knowledge.
- Break condition: If the user belief is highly inaccurate (e.g., strongly misaligned with the true optimum), ColaBO may initially perform worse, but experiments show it recovers baseline performance over time.

### Mechanism 2
- Claim: ColaBO is applicable to Monte Carlo acquisition functions by integrating user beliefs into the pathwise sampling procedure.
- Mechanism: ColaBO uses rejection sampling to bias function draws from the prior according to ρ, then updates them deterministically with observed data, producing a posterior that Monte Carlo acquisition functions can sample from.
- Core assumption: The user belief ρ and data D are independent, allowing clean separation of prior belief and data-driven updates.
- Evidence anchors:
  - [abstract] "The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs."
  - [section] "We demonstrate how the hierarchical prior integrates with MC acquisition functions."
  - [corpus] Weak direct match; the corpus does not mention MC integration explicitly, but "Bayesian Optimization of Function Networks with Partial Evaluations" suggests prior work exploring MC methods in BO.
- Break condition: If the MC sampling budget is insufficient, approximation error increases, potentially degrading ColaBO performance relative to vanilla MC acquisition functions.

### Mechanism 3
- Claim: ColaBO offers robustness when user beliefs are misleading, retaining approximately baseline BO performance.
- Mechanism: Even if the prior belief is inaccurate, the data-driven update step (pathwise update) corrects the posterior over time, allowing ColaBO to revert toward the vanilla GP posterior as more observations are gathered.
- Core assumption: The pathwise update is sufficiently strong to counteract poor initial beliefs given enough data.
- Evidence anchors:
  - [abstract] "We empirically demonstrate ColaBO’s ability to substantially accelerate optimization when the prior information is accurate, and to retain approximately default performance when it is misleading."
  - [section] "ColaBO shows greater robustness than prior work when applied to detrimental priors, retaining approximately baseline performance."
  - [corpus] No direct match in corpus, but the general concept of robustness under misspecified priors appears in Bayesian literature (e.g., "Hyperparameter Optimization via Interacting with Probabilistic Circuits").
- Break condition: If the search budget is too small relative to the dimensionality or the prior is extremely misleading, ColaBO may not fully recover baseline performance within the allotted iterations.

## Foundational Learning

- Concept: Gaussian Process (GP) surrogate modeling in Bayesian Optimization.
  - Why needed here: ColaBO builds on GP-based BO, so understanding how GPs model the objective function is essential to grasp how ColaBO modifies the prior.
  - Quick check question: In a GP surrogate, what determines the smoothness and output magnitude of the modeled function?
- Concept: Monte Carlo (MC) acquisition functions and the reparameterization trick.
  - Why needed here: ColaBO uses MC acquisition functions; knowing how MC approximates expectations over posteriors is key to understanding the integration of user beliefs.
  - Quick check question: How does the reparameterization trick help compute expectations in MC acquisition functions?
- Concept: Pathwise sampling and Matheron's rule in GPs.
  - Why needed here: ColaBO's sampling from the belief-weighted posterior relies on pathwise updates; understanding this method is crucial for implementing ColaBO.
  - Quick check question: In pathwise sampling, how is a sample from the posterior constructed from a prior sample and observed data?

## Architecture Onboarding

- Component map:
  - GP surrogate with kernel hyperparameters
  - User belief specification (ρ) over function properties
  - Rejection sampling module for belief-weighted prior
  - Pathwise update module for posterior sampling
  - MC acquisition function module (e.g., EI, MES)
  - Hyperparameter MAP estimation and update loop
- Critical path:
  1. Initialize GP with user belief ρ and prior over hyperparameters.
  2. For each BO iteration:
     - Sample functions from the belief-weighted prior via rejection sampling.
     - Update samples deterministically with observed data (pathwise update).
     - Estimate the posterior (ppf |D, ρq) from the updated samples.
     - Maximize the MC acquisition function over the search space.
     - Evaluate the objective at the selected point and update the dataset.
- Design tradeoffs:
  - Computational cost: Rejection sampling and multiple MC steps increase runtime, especially in high dimensions.
  - Accuracy vs. efficiency: More posterior samples and RFFs improve approximation but slow down each iteration.
  - Flexibility vs. robustness: ColaBO allows diverse user beliefs but may suffer if beliefs are highly inaccurate.
- Failure signatures:
  - Slow initial progress if user belief is misleading.
  - Numerical instability if RFF approximation is poor (e.g., with Matérn kernel).
  - Degradation in performance if MC sampling budget is too low.
- First 3 experiments:
  1. Implement ColaBO on a simple 1D synthetic function with a well-located prior; verify that optimization accelerates compared to vanilla BO.
  2. Test ColaBO with a poorly located prior on the same function; check that performance recovers to baseline over iterations.
  3. Apply ColaBO to a 2D PD1 hyperparameter tuning task with an available prior; measure speed-up in early iterations versus vanilla acquisition functions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ColaBO be made computationally efficient while maintaining accuracy, particularly for high-dimensional priors?
- Basis in paper: [explicit] The paper states that ColaBO introduces additional flexibility but suffers from computational expense due to multiple Monte Carlo steps, requiring tens of seconds per evaluation. It also mentions that obtaining draws from ρ scales exponentially in the dimensionality of the prior.
- Why unresolved: The paper acknowledges the computational burden but does not provide a concrete solution or method to mitigate this issue while preserving the accuracy of ColaBO.
- What evidence would resolve it: Empirical results demonstrating a more efficient sampling procedure or variational approach that reduces computational cost without sacrificing performance on high-dimensional priors would resolve this question.

### Open Question 2
- Question: How does ColaBO perform with priors over the optimal value compared to priors over the optimal location?
- Basis in paper: [explicit] The paper mentions that ColaBO can incorporate priors over the optimal value (πf*) and demonstrates its usage in experiments. However, it notes that the improvement is not as substantial as with priors over the optimal location.
- Why unresolved: The paper does not provide a detailed comparison of ColaBO's performance with priors over the optimal value versus priors over the optimal location, leaving the relative effectiveness unclear.
- What evidence would resolve it: A comprehensive comparison of ColaBO's performance on various tasks using both types of priors, with detailed analysis of the trade-offs and scenarios where each type of prior is more beneficial, would resolve this question.

### Open Question 3
- Question: Can ColaBO be effectively applied to multi-fidelity optimization settings?
- Basis in paper: [explicit] The paper suggests that applying ColaBO to multi-fidelity optimization is a potential future direction that could further increase its efficiency on costly deep learning pipelines.
- Why unresolved: The paper does not explore or provide results on ColaBO's application to multi-fidelity optimization, leaving its effectiveness in this context unexplored.
- What evidence would resolve it: Experimental results demonstrating ColaBO's performance on multi-fidelity optimization tasks, comparing it to existing methods and analyzing its efficiency gains, would resolve this question.

## Limitations
- Computational cost scales with the number of MC samples and RFFs; the paper does not explore how performance and runtime trade off at scale.
- The framework assumes independence between user belief ρ and observed data D, which may not hold in all real-world scenarios.
- ColaBO may not fully recover baseline performance within the allotted iterations if the search budget is too small relative to the dimensionality or the prior is extremely misleading.

## Confidence
- High confidence: ColaBO accelerates BO when user beliefs are accurate (verified by experiments on synthetic and real tasks).
- Medium confidence: ColaBO maintains approximately baseline performance with misleading priors (supported by experiments, but exact tolerance bounds not quantified).
- Medium confidence: ColaBO is applicable across different MC acquisition functions and belief types (demonstrated in experiments, but generality to all MC methods not exhaustively tested).

## Next Checks
1. **Prior Misspecification Study**: Systematically vary the degree of misalignment between the user-specified prior and the true optimum; measure how quickly ColaBO recovers baseline performance and at what point it fails to do so.
2. **Computational Scaling Analysis**: Evaluate ColaBO's runtime and optimization performance as the number of MC samples and RFFs increases; identify practical limits for high-dimensional problems.
3. **Independence Assumption Test**: Design experiments where the user belief ρ is correlated with the data D (e.g., beliefs derived from noisy observations); assess ColaBO's robustness and identify failure modes.