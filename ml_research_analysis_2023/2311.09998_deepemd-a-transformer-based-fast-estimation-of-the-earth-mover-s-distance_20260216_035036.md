---
ver: rpa2
title: 'DeepEMD: A Transformer-based Fast Estimation of the Earth Mover''s Distance'
arxiv_id: '2311.09998'
source_url: https://arxiv.org/abs/2311.09998
tags:
- point
- distance
- deepemd
- matching
- clouds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transformer-based model, DeepEMD, for approximating
  the Earth Mover's Distance (EMD) between point clouds. The EMD is a useful metric
  for point clouds but is computationally expensive to calculate, making it impractical
  as a training loss.
---

# DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance

## Quick Facts
- arXiv ID: 2311.09998
- Source URL: https://arxiv.org/abs/2311.09998
- Authors: 
- Reference count: 16
- Key outcome: DeepEMD achieves accurate EMD approximation with significant speedup, enabling its use as training loss for point cloud generative models.

## Executive Summary
This paper introduces DeepEMD, a transformer-based model that approximates the Earth Mover's Distance (EMD) between point clouds. EMD is a crucial metric for point cloud analysis but is computationally expensive (O(N^3)) when calculated using the Hungarian algorithm, making it impractical as a training loss for deep generative models. DeepEMD circumvents this limitation by predicting the optimal bipartite matching between point clouds using a transformer encoder's attention matrix, which allows for fast and accurate EMD estimation along with its gradient. The model demonstrates strong generalization across different point cloud categories and scales, and can serve as an effective surrogate reconstruction loss for training point cloud generative models.

## Method Summary
DeepEMD uses a transformer encoder to process concatenated point clouds with positional embeddings. A single-head attention layer outputs an attention matrix interpreted as the optimal matching between points. The model is trained with cross-entropy loss against ground truth matchings obtained from the Hungarian algorithm. During inference, the predicted matching is used to estimate the EMD by summing distances between matched points. The approach enables fast EMD approximation (O(N^2)) with accurate gradient estimation, making it suitable for training deep generative models. The method generalizes well to point clouds several times larger than those seen during training and maintains performance across different categories and datasets.

## Key Results
- DeepEMD achieves correlation coefficients (r, ρ, τ) above 0.99 with ground truth EMD values while being 100-1000x faster than Hungarian algorithm
- The model generalizes to point clouds up to 8192 points when trained on 1024 points, maintaining high correlation and low relative error
- When used as a reconstruction loss for point cloud VAEs, DeepEMD produces higher quality reconstructions than Chamfer distance and competitive results with exact EMD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepEMD approximates EMD by predicting the bipartite matching directly rather than computing the distance itself.
- Mechanism: The transformer-based model outputs an attention matrix that approximates the ground truth matching matrix obtained from the Hungarian algorithm. This matching is then used to estimate the EMD and its gradient.
- Core assumption: The attention matrix from the transformer can be trained to closely approximate the optimal matching matrix, and this approximation is sufficient for accurate EMD estimation.
- Evidence anchors:
  - [abstract] "To get the necessary accurate estimation of the gradients we train our model to explicitly compute the matching between point clouds instead of EMD itself."
  - [section] "Since the EMD–and consequently its gradient with respect to the point positions–is a function of the point positions and the matching array, predicting the latter leads to an accurate estimate of the former..."
  - [corpus] Weak - no direct evidence in related papers about transformer-based matching prediction for EMD.
- Break condition: If the attention matrix fails to capture the true matching, the EMD estimate and its gradient will be inaccurate, making the model unsuitable as a training loss.

### Mechanism 2
- Claim: DeepEMD generalizes well to unseen data distributions and point cloud sizes.
- Mechanism: The transformer architecture learns a robust representation of point cloud matching that transfers across different categories and scales, without needing fine-tuning.
- Core assumption: The learned matching patterns are invariant to category and scale variations in the point cloud data.
- Evidence anchors:
  - [abstract] "The model generalizes very well to point clouds during inference several times larger than during training."
  - [section] "Table 2 shows performance of the model for test point cloud sizes ranging from 256 to 8196, while training was done with only 1024 points."
  - [corpus] Weak - no direct evidence in related papers about generalization to larger point clouds.
- Break condition: If the model overfits to the training distribution or scale, it will fail to generalize to unseen categories or larger point clouds.

### Mechanism 3
- Claim: DeepEMD provides accurate gradient estimates for training deep generative models.
- Mechanism: By predicting the matching matrix, DeepEMD directly approximates the gradient of the true EMD, which is essential for effective backpropagation in training.
- Core assumption: The predicted matching matrix yields gradients that align well with the true EMD gradients.
- Evidence anchors:
  - [abstract] "We propose an attention-based model to compute an accurate approximation of the EMD that can be used as a training loss for generative models."
  - [section] "Fig. 6 shows the cdf of cosine similarity between the true and estimated gradient for all the points across all point clouds... The cdf has most mass at cosine similarity close to 1..."
  - [corpus] Weak - no direct evidence in related papers about gradient accuracy for EMD approximation.
- Break condition: If the predicted gradients deviate significantly from the true EMD gradients, the model will not converge properly when used as a training loss.

## Foundational Learning

- Concept: Earth Mover's Distance (EMD) and its computational complexity
  - Why needed here: Understanding EMD and why its O(N^3) complexity makes it impractical for training loss is crucial to appreciate the motivation behind DeepEMD.
  - Quick check question: What is the time complexity of the Hungarian algorithm for computing EMD, and why is it prohibitive for training deep generative models?

- Concept: Optimal transport and bipartite matching
  - Why needed here: DeepEMD predicts the bipartite matching between point clouds, which is directly related to the optimal transport plan in optimal transport theory.
  - Quick check question: How is the EMD between point clouds related to finding a one-to-one matching that minimizes the sum of distances between matched points?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: DeepEMD uses a transformer-based model to predict the bipartite matching, so understanding how transformers and attention work is essential.
  - Quick check question: How does a transformer layer compute attention scores between input elements, and how can this be used to predict matchings between two point clouds?

## Architecture Onboarding

- Component map:
  Input point clouds (U, V) with positional embeddings -> Transformer encoder -> Single-head attention layer -> Attention matrix (matching) -> EMD estimation

- Critical path:
  Concatenate input point clouds with positional embeddings -> Pass through transformer encoder to obtain contextualized point features -> Compute attention matrix in output layer -> Interpret attention matrix as bipartite matching -> Calculate EMD estimate based on the matching

- Design tradeoffs:
  - Accuracy vs. speed: DeepEMD trades some accuracy for significant speedup compared to exact EMD computation
  - Generalization vs. specialization: The model aims to generalize across categories and scales, which may limit its performance on specific cases
  - Complexity vs. interpretability: The transformer-based approach is more complex but can capture intricate matching patterns

- Failure signatures:
  - Poor correlation between predicted and true EMD values
  - Inaccurate matching predictions, leading to misaligned gradients
  - Lack of generalization to unseen categories or larger point clouds
  - Numerical instability in the attention matrix computation

- First 3 experiments:
  1. Train DeepEMD on a simple 2D synthetic dataset (e.g., circles and squares) and evaluate its EMD approximation accuracy and speed compared to exact methods.
  2. Test DeepEMD's generalization by training on one ShapeNet category and evaluating on another category or ModelNet40 dataset.
  3. Use DeepEMD as a reconstruction loss for a point cloud VAE and compare the generated point clouds' quality with those trained using Chamfer distance or exact EMD.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, several important questions emerge from the work:

- The scalability of DeepEMD to point clouds much larger than 8192 points remains unexplored, particularly whether the speed and accuracy advantages persist at extreme scales.
- The potential extension to handle point clouds with varying cardinalities (unbalanced matching) is not addressed, as the current formulation assumes equal-sized point clouds.
- The sensitivity of matching quality to different positional embedding strategies and attention mechanisms could benefit from systematic ablation studies beyond the brief mentions in the paper.
- The relationship between training data distribution diversity and out-of-distribution generalization performance could be more thoroughly investigated to understand optimal training strategies.

## Limitations

- Quadratic memory requirement limits practical application to very large point clouds (e.g., 16K+ points)
- Assumes balanced point clouds with equal numbers of points, limiting applicability to many real-world scenarios
- Transformer-based complexity may be unnecessary overhead compared to simpler approximation methods for some applications

## Confidence

- High Confidence: Experimental validation on standard point cloud datasets and demonstrated speed improvements over exact methods
- Medium Confidence: Generalization claims across different categories and scales, though could benefit from more extensive testing
- Low Confidence: Comparison with Sinkhorn algorithm due to unstated hyperparameters and implementation details

## Next Checks

1. Test the model on highly unbalanced point clouds (e.g., 100 vs 1000 points) to evaluate performance degradation and potential adaptation strategies.
2. Implement the Sinkhorn algorithm with carefully tuned hyperparameters to provide a fair comparison, documenting all parameter choices.
3. Evaluate the model on point clouds with non-uniform density distributions to assess robustness beyond the current synthetic and clean real-world datasets.