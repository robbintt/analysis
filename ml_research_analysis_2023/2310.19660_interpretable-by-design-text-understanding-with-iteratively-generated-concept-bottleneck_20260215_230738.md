---
ver: rpa2
title: Interpretable-by-Design Text Understanding with Iteratively Generated Concept
  Bottleneck
arxiv_id: '2310.19660'
source_url: https://arxiv.org/abs/2310.19660
tags:
- concept
- concepts
- text
- response
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Text Bottleneck Models (TBM), an interpretable-by-design
  text classification framework that discovers and measures human-interpretable concepts
  to predict labels. TBM iteratively generates a sparse set of concepts using GPT-4
  and measures their values on text examples, then uses a linear layer to predict
  labels based on concept scores.
---

# Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck

## Quick Facts
- **arXiv ID:** 2310.19660
- **Source URL:** https://arxiv.org/abs/2310.19660
- **Reference count:** 40
- **Primary result:** Text Bottleneck Models (TBM) achieve competitive performance with black-box baselines like few-shot GPT-4 and fine-tuned DeBERTa on 12 diverse datasets while providing interpretable concept-based explanations

## Executive Summary
This paper introduces Text Bottleneck Models (TBM), an interpretable-by-design text classification framework that discovers and measures human-interpretable concepts to predict labels. TBM iteratively generates a sparse set of concepts using GPT-4 and measures their values on text examples, then uses a linear layer to predict labels based on concept scores. On 12 diverse datasets, TBM achieves competitive performance with black-box baselines like few-shot GPT-4 and fine-tuned DeBERTa, though falls short of GPT-3.5 fine-tuning. Human evaluation shows generated concepts are relevant and measurable, with high agreement between LLM and human concept scores on sentiment tasks. TBM offers both global and local interpretability through concept weights and individual scores, providing a promising new approach for interpretable text understanding.

## Method Summary
Text Bottleneck Models (TBM) use a three-module architecture: concept generation, concept measurement, and prediction. The concept generation module iteratively discovers a sparse set of human-interpretable concepts using GPT-4, starting with an empty set and adding concepts that help discriminate between misclassified examples. The concept measurement module then determines the value of each concept for a given text through zero-shot prompting, returning numerical scores. Finally, the prediction layer uses a linear or logistic regression model to predict labels based on the concept scores. This approach creates an interpretable bottleneck where the model's predictions are based on interpretable concept scores rather than raw text features.

## Key Results
- TBM achieves competitive performance with established black-box baselines including few-shot GPT-4 and fine-tuned DeBERTa across 12 diverse datasets
- Generated concepts are relevant and measurable, with human evaluation showing high agreement between LLM and human concept scores on sentiment tasks
- TBM offers both global interpretability through concept weights and local interpretability through individual concept scores for each example
- Performance degrades on domain-specific tasks like news and science classification, indicating limitations in specialized domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative concept generation allows the model to progressively discover discriminative concepts by focusing on misclassified examples
- Mechanism: The system iteratively identifies training examples that are misclassified under the current concept set, then prompts an LLM to generate new concepts that can help distinguish between these examples and correctly classified ones
- Core assumption: LLM can identify discriminative concepts when given examples of misclassified data
- Evidence anchors:
  - [abstract]: "The Concept Generation module iteratively discovers a sparse set of concepts using misclassified examples"
  - [section]: "We generate concepts by prompting an LLM to iteratively discover new concepts that help discriminate between misclassified examples"
  - [corpus]: Weak - corpus contains related works but no direct evidence about iterative discovery effectiveness
- Break condition: LLM fails to generate relevant concepts from misclassified examples, or concept generation does not improve model performance

### Mechanism 2
- Claim: Concept measurement module can accurately assess concept values through zero-shot prompting
- Mechanism: Given a generated concept and text, the LLM answers a concept-specific question using context from concept description and response guide, then maps the response to a numerical score
- Core assumption: LLM has sufficient world knowledge to accurately measure concept values without training data
- Evidence anchors:
  - [abstract]: "The Concept Measurement module then determines the value of each concept... for a text as a numerical score"
  - [section]: "The Concept Measurement module determines the scores [s(t, ci)|ci ∈ C] for any given text t"
  - [corpus]: Weak - corpus mentions concept measurement but lacks evidence about zero-shot accuracy
- Break condition: LLM consistently provides incorrect concept scores, or scores show low correlation with human judgments

### Mechanism 3
- Claim: Sparse concept bottleneck preserves interpretability while maintaining competitive performance
- Mechanism: By limiting the model to only concept scores as input, the prediction layer remains interpretable while the concepts capture the essential information needed for accurate predictions
- Core assumption: A small set of well-chosen concepts can capture the information needed for accurate classification
- Evidence anchors:
  - [abstract]: "TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa"
  - [section]: "TBMs perform competitively with strong black-box baselines including few-shot GPT-4 and finetuned BERT"
  - [corpus]: Moderate - corpus contains related work on concept bottleneck models but limited evidence on sparsity-performance tradeoff
- Break condition: Performance degrades significantly compared to baselines, or concept set becomes too large to maintain interpretability

## Foundational Learning

- Concept: Concept-based explanations vs. token-based explanations
  - Why needed here: Understanding why this approach differs from existing interpretable methods is crucial for proper implementation
  - Quick check question: What is the key difference between concept-based explanations and token-based explanations like rationales?

- Concept: Concept leakage and redundancy in concept generation
  - Why needed here: These are critical failure modes that can undermine the interpretability and faithfulness of the model
  - Quick check question: How can concept leakage undermine the faithfulness of concept-based explanations?

- Concept: Zero-shot prompting for concept measurement
  - Why needed here: The concept measurement module relies entirely on zero-shot prompting without training data
  - Quick check question: What information must be provided to the LLM for accurate zero-shot concept measurement?

## Architecture Onboarding

- Component map: Concept Generation (LLM-based iterative discovery) → Concept Measurement (zero-shot scoring) → Prediction Layer (linear/logistic regression)
- Critical path: Concept Generation → Concept Measurement → Prediction Layer
- Design tradeoffs: Sparsity vs. completeness (fewer concepts improve interpretability but may reduce performance), LLM dependency vs. accuracy (LLM-based components are expensive but effective)
- Failure signatures: Poor performance despite many concepts (concept generation issues), High variance across runs (prompt instability), Concept scores uncorrelated with human judgments (measurement issues)
- First 3 experiments:
  1. Test concept generation on a simple dataset with clear concepts to verify the iterative process works
  2. Evaluate concept measurement accuracy on a small dataset with human-annotated concept scores
  3. Compare performance of TBM with different concept set sizes on a benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TBM performance change when using different LLM models (GPT-4, GPT-3.5, Claude) for concept generation and measurement?
- Basis in paper: [explicit] The paper uses GPT-4 for both concept generation and measurement, but does not explore other LLM models.
- Why unresolved: The paper only reports results using GPT-4, leaving open the question of whether other LLM models would perform similarly or better for concept generation and measurement.
- What evidence would resolve it: Conducting experiments using different LLM models (GPT-3.5, Claude, etc.) for both concept generation and measurement modules, and comparing the performance and concept quality to the GPT-4 results.

### Open Question 2
- Question: How does the number of training examples affect TBM performance and concept quality across different dataset sizes?
- Basis in paper: [inferred] The paper uses 250 examples for training on most datasets, but does not explore the impact of training set size on performance.
- Why unresolved: The paper does not investigate how varying the number of training examples affects TBM performance or concept quality, which could inform optimal dataset sizes for different tasks.
- What evidence would resolve it: Training TBMs on the same datasets with varying numbers of training examples (e.g., 50, 100, 250, 500) and measuring the impact on both end-to-end performance and concept quality metrics.

### Open Question 3
- Question: How do TBMs perform on multilingual text classification tasks compared to monolingual ones?
- Basis in paper: [explicit] The paper only evaluates TBMs on English text datasets, without testing multilingual capabilities.
- Why unresolved: The paper does not explore whether TBMs can effectively handle multilingual text classification, which would be important for real-world applications.
- What evidence would resolve it: Evaluating TBMs on multilingual datasets (e.g., XNLI, multilingual Amazon reviews) and comparing performance to monolingual English tasks to assess cross-lingual effectiveness.

## Limitations
- Heavy reliance on LLM capabilities introduces potential brittleness through prompt sensitivity and token limit constraints
- Performance degrades on domain-specific tasks like news classification and scientific text, suggesting limitations in specialized domains
- Iterative concept generation requires multiple LLM calls per dataset, creating computational overhead and potential instability

## Confidence
- **High confidence**: The competitive performance claims on general sentiment and NLI tasks are well-supported by experimental results showing TBM matching or exceeding BERT/DeBERTa baselines
- **Medium confidence**: The interpretability claims are supported by human evaluation showing high agreement between LLM and human concept scores, though this validation was limited to sentiment tasks
- **Medium confidence**: The iterative concept generation mechanism is plausible based on the described approach, but effectiveness depends on LLM's ability to identify truly discriminative concepts

## Next Checks
1. Test TBM on additional domain-specific datasets (e.g., medical, legal, technical documentation) to evaluate generalization beyond general text tasks
2. Conduct ablation studies removing the iterative component to quantify its contribution to both performance and concept quality
3. Measure concept set stability across multiple runs with different random seeds to assess reproducibility of the iterative generation process