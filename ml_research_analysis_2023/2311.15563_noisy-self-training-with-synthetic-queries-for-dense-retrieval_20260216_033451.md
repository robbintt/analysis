---
ver: rpa2
title: Noisy Self-Training with Synthetic Queries for Dense Retrieval
arxiv_id: '2311.15563'
source_url: https://arxiv.org/abs/2311.15563
tags:
- teacher
- data
- synthetic
- retrieval
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a noisy self-training framework combined with
  synthetic queries for dense retrieval, which shows that neural retrievers can be
  improved in a self-evolution manner with no reliance on any external models. Experimental
  results show that the proposed method improves consistently over existing methods
  on both general-domain and out-of-domain retrieval benchmarks.
---

# Noisy Self-Training with Synthetic Queries for Dense Retrieval

## Quick Facts
- arXiv ID: 2311.15563
- Source URL: https://arxiv.org/abs/2311.15563
- Reference count: 33
- Key outcome: Proposes a noisy self-training framework with synthetic queries that improves dense retrieval performance without external models, showing gains across general and out-of-domain benchmarks with as little as 30% labeled data.

## Executive Summary
This paper introduces a noisy self-training framework that leverages synthetic queries to improve dense retrieval models. The method uses a teacher model trained on labeled data to generate soft labels for synthetic queries, then trains a student model using noise-injected inputs. Through iterative pre-training on synthetic data and fine-tuning on labeled data, the approach achieves consistent improvements across multiple retrieval benchmarks while demonstrating strong data efficiency.

## Method Summary
The proposed method trains a teacher retriever on labeled data, then uses it to generate soft labels for synthetic queries created from passages. A student retriever is pre-trained on this synthetic data with noise injection (shuffling, deletion, and masking operations) to improve robustness, then fine-tuned on the labeled data. The process can iterate by using the student as the new teacher. The approach claims to handle the inherent noise in synthetic queries better than hard labeling while encouraging more robust representations through input perturbation.

## Key Results
- Consistent improvements over existing methods on both general-domain and out-of-domain retrieval benchmarks
- Data efficiency: outperforms competitive baselines with as little as 30% of labeled training data
- General applicability: extends to reranker training with additional gains across diverse domains
- Robustness: noise injection improves model performance on queries with perturbations

## Why This Works (Mechanism)

### Mechanism 1: Soft labeling for noisy synthetic queries
Synthetic queries often have weak relationships to their originating passages. Instead of using the originating passage as a hard positive, the teacher model generates a probability distribution over all passages, allowing the student to learn from passages that are actually more relevant to the synthetic query.

### Mechanism 2: Input perturbation for robustness
By randomly shuffling, deleting, or masking words in queries and passages during training, the student model learns to capture salient phrases and semantic meaning beyond exact token matching, making it more robust to variations in real-world queries.

### Mechanism 3: Iterative self-evolution
The framework creates a self-evolving process where each iteration produces a better teacher model that can generate more accurate pseudo labels for the next iteration, progressively improving the model's understanding of the target domain.

## Foundational Learning

- **Dense retrieval and dual-encoder architecture**: The paper builds upon the dense retrieval framework and proposes improvements to its training process. Quick check: What is the key difference between dense retrieval and traditional lexical retrieval methods like BM25?

- **Contrastive learning and negative sampling**: The training of dense retrievers relies on contrastive learning objectives to distinguish relevant from irrelevant passages. Quick check: How does the choice of negative samples impact the performance of a dense retriever trained with contrastive learning?

- **Self-training and pseudo labeling**: The core idea uses self-training with synthetic queries to improve dense retrieval models. Quick check: What is the main advantage of using soft labels generated by a teacher model compared to hard labels in self-training?

## Architecture Onboarding

- **Component map**: Query Generator -> Teacher Retriever -> Student Retriever -> Hard Negative Miner -> Noise Injector

- **Critical path**: 
  1. Train query generator on labeled data
  2. Generate synthetic queries and their originating passages
  3. Train teacher retriever on labeled data
  4. Use teacher to generate soft labels for synthetic queries
  5. Pre-train student retriever on synthetic data with noise injection
  6. Fine-tune student retriever on labeled data
  7. (Optional) Iterate by using student as new teacher

- **Design tradeoffs**: Soft vs. hard labels (robustness vs. teacher dependency), noise injection level (robustness vs. learning hindrance), number of iterations (improvement vs. error accumulation)

- **Failure signatures**: Performance degradation over iterations (error accumulation), worse performance than baseline (synthetic data issues), overfitting to synthetic data (poor generalization)

- **First 3 experiments**: 
  1. Ablation study: Remove noise injection to verify its impact
  2. Data efficiency analysis: Train models on varying amounts of labeled data
  3. Robustness test: Evaluate performance on queries with shuffled tokens

## Open Questions the Paper Calls Out

### Open Question 1
How can the proposed framework be adapted to work with unsupervised domain adaptation settings where no labeled data is available for training the query generator?

### Open Question 2
How can the quality of synthetic queries be improved to be more similar to gold standard queries, especially for tasks with different retrieval needs from general passage retrieval?

### Open Question 3
How does the proposed framework affect the robustness of the model to biased data, and how can the extent of bias propagation from language models used for generating synthetic queries be evaluated?

## Limitations

- Heavy dependency on teacher model quality, which may not be available or optimal in all domains
- Limited analysis of synthetic query quality and error rates in the generated data
- Fixed noise injection hyperparameters without sensitivity analysis
- No clear criteria for terminating the iterative self-training process

## Confidence

**High confidence**: The general framework of using synthetic queries with noisy self-training is sound and well-motivated.

**Medium confidence**: The specific mechanisms of how noise injection improves robustness and how soft labeling handles synthetic query noise are plausible but could benefit from more rigorous ablation studies.

**Low confidence**: The claim that the method works with as little as 30% of labeled training data is based on limited experiments and doesn't account for potential domain shifts.

## Next Checks

1. **Error accumulation analysis**: Track model performance across multiple self-training iterations on a held-out validation set to identify optimal iteration count and detect early performance degradation.

2. **Synthetic query quality audit**: Implement systematic evaluation of synthetic query quality through human annotation of semantic relatedness between synthetic queries and originating passages, correlating ratings with model performance.

3. **Noise injection sensitivity study**: Perform comprehensive ablation study varying noise injection probabilities (0.0, 0.05, 0.1, 0.2, 0.3) and noise types to identify optimal configurations for different domains and query characteristics.