---
ver: rpa2
title: What is the Best Automated Metric for Text to Motion Generation?
arxiv_id: '2309.10248'
source_url: https://arxiv.org/abs/2309.10248
tags:
- human
- metrics
- motion
- metric
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates automated metrics for text-to-motion generation
  by correlating them with human judgments of naturalness and faithfulness. The authors
  create a dataset of 1,400 human-rated motion-text pairs and analyze existing metrics
  (coordinate errors, FID, R-Precision, multimodal distance) as well as introduce
  MoBERT, a novel multimodal transformer-based evaluator.
---

# What is the Best Automated Metric for Text to Motion Generation?

## Quick Facts
- **arXiv ID**: 2309.10248
- **Source URL**: https://arxiv.org/abs/2309.10248
- **Reference count**: 13
- **Primary result**: MoBERT achieves sample-level correlations of 0.624 (faithfulness) and 0.528 (naturalness) with human judgments, outperforming all existing metrics

## Executive Summary
This paper evaluates automated metrics for text-to-motion generation by correlating them with human judgments of naturalness and faithfulness. The authors create a dataset of 1,400 human-rated motion-text pairs and analyze existing metrics (coordinate errors, FID, R-Precision, multimodal distance) as well as introduce MoBERT, a novel multimodal transformer-based evaluator. Their findings show that none of the existing metrics achieve strong sample-level correlation with human judgments, but coordinate errors (particularly Root AVE) and R-Precision show strong model-level correlations. MoBERT outperforms all alternatives, achieving sample-level correlations of 0.624 for faithfulness and 0.528 for naturalness, while maintaining near-perfect model-level correlation.

## Method Summary
The authors collected human judgments on 1,400 motion-text pairs using Amazon Mechanical Turk, with three ratings per sample for naturalness and faithfulness. They evaluated existing metrics including coordinate errors (AE/AVE with different joint groupings), FID, R-Precision, and multimodal distance. MoBERT was developed as a multimodal transformer that processes motion chunks and text tokens jointly in a shared encoder, producing alignment probabilities that can be calibrated to human judgment scores using regression. The model was trained on HumanML3D dataset using an alignment prediction task, and evaluated using 10-fold cross-validation with SVR/linear regression on output features.

## Key Results
- MoBERT achieves sample-level correlations of 0.624 (faithfulness) and 0.528 (naturalness) with human judgments
- Root-only coordinate errors achieve model-level correlations >0.9 Pearson, while joint-only metrics perform poorly
- Existing metrics show weak sample-level correlations (all below 0.5)
- MoBERT maintains near-perfect model-level correlation while outperforming all alternatives at sample level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoBERT achieves superior correlation by encoding motion and text jointly in a shared transformer space, allowing cross-modal attention to capture temporal relationships before collapsing to embeddings.
- Mechanism: The shared transformer encoder processes motion chunks and text tokens in a single sequence, enabling attention across modalities. This preserves temporal structure and allows nuanced relationships to be learned, unlike separate encoders that combine embeddings only after independent encoding.
- Core assumption: Temporal relationships between motion and text are better captured when processed together rather than separately.
- Evidence anchors:
  - [abstract]: "MoBERT distinguishes itself by its ability to evaluate both modalities using a shared Transformer Encoder through a multimodal sequence embedding."
  - [section]: "Compared to CLIPScore, which uses separate encoders for each modality and combines the two modalities using cosine similarity, MoBERT's single Encoder approach allows for a richer understanding of the data."

### Mechanism 2
- Claim: Sample-level correlation is improved by using a learned regression layer over MoBERT's alignment probability, fine-tuned on human judgments.
- Mechanism: The base MoBERT outputs alignment probabilities from binary classification. A lightweight regression (SVR with RBF kernel) is trained on 90% of the human judgment dataset to map these probabilities to the 0-4 Likert scale. Cross-validation ensures generalization.
- Core assumption: The alignment probability scores contain sufficient signal to predict human quality ratings when properly calibrated.
- Evidence anchors:
  - [section]: "We also test our model's performance when trained on a small set of human judgment data... We apply ten-fold cross-validation, fitting regressors on 90% of the dataset's samples to predict the remaining portion."
  - [abstract]: "MoBERT outperforms all alternatives, achieving sample-level correlations of 0.624 for faithfulness and 0.528 for naturalness."

### Mechanism 3
- Claim: Root joint errors dominate model-level evaluation because they capture global motion characteristics that differentiate model outputs more strongly than local joint variations.
- Mechanism: Coordinate error metrics (AE/AVE) computed on root joints alone show near-perfect model-level correlation (>0.9 Pearson), while joint-only metrics perform poorly. This suggests that root translations and global motion trends are the primary discriminators of model quality at the aggregate level.
- Core assumption: Model-level evaluation cares more about global motion characteristics than local pose accuracy.
- Evidence anchors:
  - [section]: "Root-only traditional AE metrics achieve nearly 0.75 Pearson's, while Root AVE metrics surpass AE with approximately 0.91 Pearson's. Interestingly, Joint versions are unreliable on their own at the model level."
  - [section]: "This supports similar claims by [Ghosh et al. 2021]."

## Foundational Learning

- **Concept**: Pearson correlation coefficient
  - Why needed here: The paper uses Pearson's r to quantify the linear relationship between automated metric scores and human judgments at both sample and model levels.
  - Quick check question: If a metric's scores increase as human ratings decrease, what sign should the Pearson correlation have?

- **Concept**: Cross-validation
  - Why needed here: To avoid overfitting when training the regression layer on the small human judgment dataset, 10-fold cross-validation is used to assess generalization.
  - Quick check question: In k-fold cross-validation, how many times is the model trained and evaluated when k=10?

- **Concept**: Embedding space alignment
  - Why needed here: Metrics like R-Precision and Multimodal Distance rely on motions and text being projected into a shared embedding space where Euclidean distance reflects semantic similarity.
  - Quick check question: If two motions have identical embeddings but different human judgments, what does this imply about the embedding space quality?

## Architecture Onboarding

- **Component map**: Motion preprocessing -> feature extraction -> chunking -> tokenization -> multimodal sequence -> shared Transformer -> CLS embedding -> alignment head -> probability -> (optional) regression -> human judgment score

- **Critical path**:
  1. Input motion → feature extraction → chunking
  2. Input text → tokenization → embedding
  3. Concatenate → positional/segment encoding
  4. Shared Transformer → CLS embedding
  5. Alignment head → probability
  6. (Optional) Regression → human judgment score

- **Design tradeoffs**:
  - Shared vs separate encoders: Shared allows richer cross-modal attention but may be harder to train
  - Chunk size and overlap: Balance temporal context vs redundancy and computational cost
  - Regression vs direct probability: Regression improves sample-level correlation but adds complexity and data requirements

- **Failure signatures**:
  - Low sample-level correlation despite high model-level correlation: Model captures global trends but not individual instance quality
  - High variance in regression predictions: Overfitting to small human judgment dataset
  - MoBERT alignment probability uncorrelated with human judgments: Training data distribution mismatch or weak alignment task design

- **First 3 experiments**:
  1. Train MoBERT with only the alignment task (no regression) and evaluate sample-level correlation on held-out human judgments.
  2. Vary chunk size (10, 14, 18 frames) and overlap (0, 2, 4 frames) to find optimal temporal context settings.
  3. Compare regression models (SVR RBF, Ridge, Linear) on the same cross-validation splits to select the best calibration method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MoBERT metric's performance degrade when evaluated on motion datasets significantly different from HumanML3D, particularly in terms of pose diversity, motion complexity, or joint structure?
- Basis in paper: [explicit] The authors note that MoBERT was trained exclusively on HumanML3D data and that while the regression versions are trained to fit human judgments using moderately out-of-distribution data, these models were trained to emulate HumanML3D data. They speculate that MoBERT may have some OOD robustness but acknowledge this remains speculative without substantially OOD datasets aligned to the 22-joint SMPL body model of HumanML3D.
- Why unresolved: The paper lacks experiments on datasets with different joint structures, motion characteristics, or pose representations that differ substantially from HumanML3D.
- What evidence would resolve it: Direct empirical testing of MoBERT on motion datasets with different joint structures (e.g., 3D skeleton models with different numbers of joints), motion characteristics (e.g., animal motions, robotic motions), or pose representations (e.g., mesh-based representations instead of skeletal) while maintaining the text-to-motion generation paradigm.

### Open Question 2
- Question: What is the optimal batch size for R-Precision calculations, and how does varying the batch size affect the correlation with human judgments?
- Basis in paper: [explicit] The authors note they hold the batch size to 32, following common practice [Guo et al. 2022a], but they do not explore how different batch sizes might affect the correlation with human judgments. They only examine different retrieval thresholds.
- Why unresolved: The paper only tests one batch size (32) for R-Precision and does not provide any analysis of how batch size affects the metric's performance or its correlation with human judgments.
- What evidence would resolve it: Systematic experiments varying batch sizes (e.g., 16, 32, 64, 128) while measuring the correlation with human judgments at both sample and model levels, along with analysis of the trade-offs between computational efficiency and evaluation accuracy.

### Open Question 3
- Question: How do different regression approaches for MoBERT (SVR vs Linear Regression vs other methods) affect its sample-level correlation with human judgments, and what is the optimal regression configuration?
- Basis in paper: [explicit] The authors compare two regression approaches (SVR with RBF kernel and Linear Regression) and find SVR performs better, but they do not exhaustively explore other regression methods or hyperparameter configurations.
- Why unresolved: The paper only tests a limited set of regression approaches and does not explore other potentially effective methods like neural networks, Gaussian processes, or ensemble methods.
- What evidence would resolve it: Comparative experiments testing multiple regression methods (including neural networks, ensemble methods, and other kernel-based approaches) with comprehensive hyperparameter tuning, along with ablation studies on the number of training samples, cross-validation strategies, and the impact of different feature representations from the MoBERT model.

## Limitations
- Conclusions based on single dataset (HumanML3D) and four baseline models, limiting generalizability
- Sample-level correlations remain modest (0.624 for faithfulness, 0.528 for naturalness), indicating metrics still struggle with human perceptual quality
- Regression layer performance depends heavily on small human judgment dataset, raising overfitting concerns

## Confidence

- **High confidence**: The finding that root-only coordinate errors achieve strong model-level correlations (>0.9 Pearson) while joint-only metrics perform poorly
- **Medium confidence**: The claim that MoBERT outperforms all alternatives, though modest absolute values suggest room for improvement
- **Low confidence**: The assertion that shared transformer encoding is the primary reason for MoBERT's success, lacking ablation studies

## Next Checks
1. Test MoBERT's generalization by evaluating it on an independent text-to-motion dataset (e.g., BABEL or HumanAct12) with human judgments to assess whether the observed correlations hold across domains
2. Conduct an ablation study comparing MoBERT with a version using separate encoders for motion and text to quantify the actual contribution of the shared transformer architecture to performance improvements
3. Evaluate whether combining MoBERT with coordinate errors and R-Precision provides additive benefits or redundancy, using a multiple regression framework to assess the unique contribution of each metric to predicting human judgments