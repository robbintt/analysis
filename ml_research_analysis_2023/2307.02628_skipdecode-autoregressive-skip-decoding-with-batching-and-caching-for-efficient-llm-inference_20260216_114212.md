---
ver: rpa2
title: 'SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient
  LLM Inference'
arxiv_id: '2307.02628'
source_url: https://arxiv.org/abs/2307.02628
tags:
- exit
- tokens
- computational
- token
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient inference for autoregressive
  large language models (LLMs) by proposing a token-level early exit method called
  SkipDecode. The core idea is to assign a unified exit point for all tokens in a
  batch at a specific sequence position, ensuring that the processing of all batched
  tokens at a certain sequence position concludes at the same time.
---

# SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference

## Quick Facts
- arXiv ID: 2307.02628
- Source URL: https://arxiv.org/abs/2307.02628
- Reference count: 29
- Achieves 2x to 5x inference speedup with negligible regression across various text generation tasks

## Executive Summary
This paper introduces SkipDecode, a novel token-level early exit method for efficient autoregressive LLM inference. Unlike prior methods that terminate computation early for individual tokens, SkipDecode bypasses lower to middle layers and performs most computation in upper layers, enabling later tokens to benefit from earlier computations. The method assigns a unified exit point for all tokens in a batch at each sequence position, ensuring compatibility with batch inference and KV caching while maintaining monotonically decreasing exit points across the sequence.

## Method Summary
SkipDecode addresses efficient LLM inference by implementing a unified exit point strategy where all tokens at the same sequence position exit at the same layer. The method bypasses lower layers entirely, performing most computation in upper layers, which allows later tokens to attend to the full computation of earlier tokens. By constructing batches column-wise and ensuring monotonically decreasing exit points, SkipDecode maintains compatibility with KV caching without requiring recomputation. The approach uses a linear decay function to determine exit points, with hyperparameters including max/min exit layers, warmup layers, and learning rate.

## Key Results
- Achieves 2x to 5x inference speedup on OPT models (1.3B and 6.7 billion parameters)
- Maintains task performance with negligible regression (Bleu, Rouge-L, Bert-F scores)
- Works effectively across E2E, Reddit-TLDR, and CNN-DM benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SkipDecode achieves significant inference speedup by allocating the computational budget to higher layers, enabling later tokens to benefit from earlier computations.
- Mechanism: Instead of terminating computation early for each token individually, SkipDecode "skips" lower layers and performs the bulk of computation in upper layers. This allows later tokens to attend to the full computation of earlier tokens, even if they exit at different layers.
- Core assumption: Tokens towards the end of a sequence are generally easier to predict due to more contextual information, allowing for reduced computation without significant performance degradation.
- Evidence anchors:
  - [abstract]: "Rather than terminating computation prematurely as in prior works, our approach bypasses lower to middle layers, devoting most of the computational resources to upper layers, allowing later tokens to benefit from the compute expenditure by earlier tokens."
  - [section]: "To overcome this limitation, we propose performing skipping instead of early termination. We ensure that the computational budget for each token is allocated to higher layers of the model."
- Break condition: If the assumption about easier prediction of later tokens is violated (e.g., in tasks where context doesn't monotonically improve predictability), the monotonic decrease in exit points may not hold, reducing effectiveness.

### Mechanism 2
- Claim: SkipDecode enables efficient batching by assigning a unified exit point for all tokens in a batch at each sequence position.
- Mechanism: Tokens are batched column-wise (i.e., tokens at the same position across all sequences in the batch). A single exit layer is determined for all tokens in a column, ensuring that processing of all tokens at a given position concludes simultaneously.
- Core assumption: A single exit point per position per batch can be determined without significantly impacting individual token performance.
- Evidence anchors:
  - [section]: "We suggest a method that designates a fixed positionwise exit point for every token in a batch at a given sequence position. This strategy ensures that the processing of all batched tokens at a certain sequence position concludes at the same time."
  - [section]: "Let B be the batch size, and N the sequence length. We construct the batches column-wise using tokens at a specific position across all the instances."
- Break condition: If tokens within a batch have highly variable computational needs at the same position, forcing a single exit point may lead to suboptimal performance for some tokens.

### Mechanism 3
- Claim: SkipDecode maintains efficient KV caching by ensuring monotonically decreasing exit points across the sequence.
- Mechanism: By guaranteeing that exit points decrease monotonically as the sequence progresses, SkipDecode ensures that no token exits at a layer higher than a previous token. This eliminates the need to recompute KV caches for preceding tokens.
- Core assumption: Tokens earlier in the sequence require more computation than later tokens, making monotonically decreasing exit points feasible.
- Evidence anchors:
  - [section]: "By ensuring that batched exit points are monotonically decreasing as the sequence progresses, we guarantee that previous tokens have performed at least as much computation as the current one, thereby trivially avoiding the need for any extra computation."
  - [section]: "The underlying rationale is that next-word prediction at the beginning of a sequence is more challenging due to limited context, and therefore earlier tokens will benefit from later exit points in the computation graph."
- Break condition: If later tokens in a sequence require more computation than earlier ones (contrary to the assumption), the monotonic decrease in exit points may lead to performance degradation.

## Foundational Learning

- Concept: Autoregressive generation in language models
  - Why needed here: SkipDecode is specifically designed for autoregressive models like GPT and OPT, which generate tokens sequentially based on previously generated tokens.
  - Quick check question: In autoregressive generation, can a token be generated before all previous tokens in the sequence are determined?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: SkipDecode modifies how attention computations are performed by skipping lower layers, requiring understanding of how attention works in transformers.
  - Quick check question: How does the attention mechanism in transformers allow a token to attend to previously generated tokens?

- Concept: Key-Value (KV) caching
  - Why needed here: SkipDecode's compatibility with KV caching is a key advantage over previous methods. Understanding how KV caching works is crucial for grasping this benefit.
  - Quick check question: What is the purpose of KV caching in transformer inference, and how does it improve efficiency?

## Architecture Onboarding

- Component map:
  Input batch of sequences -> Position-wise exit point determination -> Layer skipping mechanism -> Attention computation in upper layers -> KV caching integration -> Output tokens

- Critical path:
  1. Determine exit points for each position in the batch
  2. Process tokens in column-wise batches
  3. Skip lower layers, compute in upper layers
  4. Perform attention using KV caching
  5. Generate output tokens

- Design tradeoffs:
  - Speed vs. accuracy: More aggressive layer skipping increases speed but may reduce quality
  - Fixed vs. adaptive exit points: Fixed points simplify batching but may not be optimal for all tokens
  - Linear vs. other decay functions: Linear is simple but other functions might offer better performance

- Failure signatures:
  - Degradation in generation quality (e.g., increased perplexity, reduced ROUGE scores)
  - Incompatibility with certain batching strategies
  - Unexpected increases in computation time (e.g., if exit points don't decrease monotonically)

- First 3 experiments:
  1. Implement linear decay function for exit points and measure speedup vs. quality trade-off
  2. Compare performance with and without KV caching enabled
  3. Test different batch sizes to evaluate batching efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative decay functions, such as power-law, could yield greater acceleration compared to the linear decay used in SkipDecode?
- Basis in paper: [explicit] The paper mentions that preliminary experiments with a power law decay function did not yield improvements over the linear decay, but also states that prior research indicates a power law distribution for token exit levels.
- Why unresolved: The paper does not provide detailed results or analysis comparing different decay functions beyond the initial power law experiment. The potential of other decay functions remains unexplored.
- What evidence would resolve it: Conducting experiments with various decay functions (e.g., exponential, logarithmic) and comparing their performance in terms of speedup and task performance would provide insights into the optimal decay function for SkipDecode.

### Open Question 2
- Question: How does the decaying policy impact the prompt in SkipDecode, and can more aggressive decay functions be applied to the prompt for additional speedup gains?
- Basis in paper: [explicit] The paper suggests that the decaying policy could be extended to the prompt, as it currently uses the full network for the prompt. It mentions that more aggressive decay functions could lead to additional speedup gains.
- Why unresolved: The paper does not provide experimental results or analysis on the impact of the decaying policy on the prompt or the potential speedup gains from more aggressive decay functions.
- What evidence would resolve it: Experimenting with different decay functions for the prompt and measuring the resulting speedup and task performance would provide insights into the potential benefits and trade-offs of extending the decaying policy to the prompt.

### Open Question 3
- Question: How can SkipDecode be adapted to support the 'infinite loop' inference mode, where new samples can be included in the batch as long as their current position matches the remaining elements' positions?
- Basis in paper: [inferred] The paper mentions that SkipDecode does not naturally support the 'infinite loop' inference mode due to the decaying policy. It suggests that this is a limitation of the current approach.
- Why unresolved: The paper does not provide any discussion or proposed solutions for adapting SkipDecode to support the 'infinite loop' inference mode.
- What evidence would resolve it: Developing and implementing a mechanism to dynamically adjust the batch composition based on the decaying policy and sequence positions would demonstrate the feasibility of supporting the 'infinite loop' inference mode in SkipDecode.

## Limitations
- Limited task diversity: Results primarily on summarization and constrained generation tasks, effectiveness on open-ended generation remains uncertain
- Hyperparameter sensitivity: Performance depends on specific choices of exit layers, warmup layers, and decay functions without systematic sensitivity analysis
- Architecture constraints: Design assumes standard transformer decoder architectures, applicability to other architectures unexplored

## Confidence
- High Confidence (Mechanism 1): Core insight about allocating computational budget to higher layers is well-supported
- Medium Confidence (Mechanism 2): Batching strategy is clearly described but potential quality degradation for heterogeneous batches not fully addressed
- Medium Confidence (Mechanism 3): Monotonic decay guarantee is theoretically sound but edge cases not explored

## Next Checks
1. Conduct systematic ablation study varying max/min exit layers, warmup layers, and decay functions across a wider range to establish robustness boundaries
2. Test SkipDecode on non-OPT architectures (e.g., LLaMA, GPT-2) and encoder-decoder models to verify generalizability
3. Evaluate SkipDecode on open-ended generation tasks like story generation or code completion to assess monotonic decay assumption when context doesn't monotonically improve predictability