---
ver: rpa2
title: 'Beyond Co-occurrence: Multi-modal Session-based Recommendation'
arxiv_id: '2309.17037'
source_url: https://arxiv.org/abs/2309.17037
tags:
- information
- user
- item
- price
- mmsbr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving session-based recommendation
  by incorporating rich multi-modal information beyond item co-occurrence patterns.
  The core method, MMSBR, employs a unified framework to model both descriptive (images
  and text) and numerical (price) information.
---

# Beyond Co-occurrence: Multi-modal Session-based Recommendation

## Quick Facts
- arXiv ID: 2309.17037
- Source URL: https://arxiv.org/abs/2309.17037
- Reference count: 40
- Achieves up to 3.14% and 5.33% improvements in Prec@20 and MRR@20 respectively

## Executive Summary
This paper addresses the limitations of traditional session-based recommendation methods that rely solely on item co-occurrence patterns by incorporating rich multi-modal information. The proposed MMSBR framework models both descriptive information (images and text) and numerical information (price) through a unified approach. By employing pseudo-modality contrastive learning, hierarchical pivot transformer fusion, and Gaussian distribution embedding with Wasserstein self-attention, MMSBR significantly outperforms state-of-the-art methods while effectively addressing the cold-start problem in session-based recommendation.

## Method Summary
MMSBR employs a unified framework that encodes images using GoogLeNet, text using BERT, and price through discretization into price levels. For descriptive information, it generates pseudo-modalities using DALL-E and GoogLeNet, then applies contrastive learning to enhance representation learning by mitigating noise. A hierarchical pivot transformer fuses heterogeneous descriptive information by extracting and integrating useful information from both modalities. For numerical price information, Gaussian distribution embedding represents price preferences probabilistically, and Wasserstein self-attention calculates attention scores between price distributions. The model is trained using cross-entropy loss with contrastive learning regularization on Amazon product datasets.

## Key Results
- Outperforms state-of-the-art methods by up to 3.14% in Prec@20
- Achieves 5.33% improvement in MRR@20 over existing approaches
- Effectively alleviates the cold-start problem for new items through multi-modal modeling

## Why This Works (Mechanism)

### Mechanism 1
Pseudo-modality contrastive learning improves representation learning by mitigating noise in heterogeneous descriptive information. The model generates pseudo-modalities aligned in the same semantic space as actual modalities using data generation techniques (DALL·E for text→image, image classification for image→text). These aligned pairs are then used as positive pairs in contrastive learning to push semantically similar items close while pulling dissimilar ones apart. Core assumption: Pseudo-modalities retain core semantics while filtering out noise from actual modalities.

### Mechanism 2
Hierarchical pivot transformer effectively fuses heterogeneous descriptive information by capturing both shared and distinct information between modalities. The transformer layer includes a pivot token that hierarchically extracts and integrates information from image and text feature sequences. The pivot acts as a mixer, passing information between modalities while preserving their distinct characteristics. Core assumption: The pivot can effectively govern information flow between heterogeneous modalities without losing critical information from either source.

### Mechanism 3
Gaussian distribution embedding with Wasserstein self-attention properly models the probabilistic influence of numerical price information on user behaviors. Price is represented as Gaussian distributions (mean and covariance vectors) rather than point embeddings, and Wasserstein distance is used to calculate attention scores between price distributions, capturing the range property of price preferences. Core assumption: User price preferences exhibit range-based (probabilistic) behavior rather than point-based preferences.

## Foundational Learning

- Concept: Contrastive learning fundamentals
  - Why needed here: Understanding how positive/negative pairs work and how semantic alignment affects representation quality
  - Quick check question: What happens to representation learning if you use semantically dissimilar pairs as positives in contrastive learning?

- Concept: Transformer architecture and multi-head attention
  - Why needed here: The hierarchical pivot transformer builds on standard transformer operations, so understanding self-attention and information flow is critical
  - Quick check question: How does the pivot token in each layer affect information propagation between modalities compared to standard transformer layers?

- Concept: Gaussian distribution properties and Wasserstein distance
  - Why needed here: The probabilistic modeling of price requires understanding how Gaussian distributions represent uncertainty and how Wasserstein distance differs from other distance metrics
  - Quick check question: Why is Wasserstein distance more suitable than Euclidean distance for comparing Gaussian distribution embeddings?

## Architecture Onboarding

- Component map: Multi-modal encoding (images→GoogLeNet, text→BERT, price→discretization) → Pseudo-modality contrastive learning → Hierarchical pivot transformer → Gaussian distribution embedding + Wasserstein self-attention → Prediction layer
- Critical path: Pseudo-modality generation → Contrastive learning → Pivot fusion → Price distribution modeling → Final prediction
- Design tradeoffs: Increased model complexity and computational cost for improved representation quality and handling of cold-start scenarios
- Failure signatures: Poor performance on cold-start items suggests pseudo-modality generation is failing; degraded performance on descriptive information suggests pivot fusion issues; price prediction problems suggest distribution modeling issues
- First 3 experiments:
  1. Compare MMSBR performance with and without pseudo-modality contrastive learning on a held-out validation set
  2. Test different numbers of pivot tokens (T) to find optimal balance between information extraction and sparsity
  3. Evaluate Gaussian vs point embedding for price modeling on datasets with varying price distributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed MMSBR method perform when incorporating additional modalities beyond images, text, and price, such as user reviews or item ratings? The paper mentions future work to explore user reviews for further mining user fine-grained preferences, suggesting the potential for incorporating additional modalities.

### Open Question 2
How does the performance of MMSBR vary across different types of e-commerce domains, such as fashion, electronics, or books? The paper evaluates MMSBR on three Amazon datasets covering different domains: Cellphones, Grocery, and Sports, but does not provide a detailed analysis of performance variations across different e-commerce domains.

### Open Question 3
How does the proposed MMSBR method handle the cold-start problem for new users who have not interacted with any items yet? The paper mentions that MMSBR can alleviate the cold-start problem for new items by modeling multi-modal information, but does not explicitly discuss how the method handles the cold-start problem for new users.

## Limitations

- The effectiveness of pseudo-modality contrastive learning relies heavily on the quality of generated pseudo-modalities through DALL·E and GoogLeNet, with no ablation studies provided
- The Gaussian distribution embedding for price assumes that price preferences are inherently probabilistic, but this assumption lacks empirical validation across different product categories
- The hierarchical pivot transformer introduces additional complexity without clear evidence that the pivot mechanism provides benefits over standard multi-modal fusion approaches

## Confidence

**High Confidence**: The core experimental methodology and dataset preparation are well-defined. The reported improvements over baselines (3.14% Prec@20 and 5.33% MRR@20) are statistically significant based on the experimental setup.

**Medium Confidence**: The three proposed mechanisms are theoretically sound, but the paper lacks ablation studies to isolate their individual contributions. The claim about effectively alleviating cold-start problems is supported by experiments but not deeply analyzed.

**Low Confidence**: The assumption that pseudo-modalities generated by DALL·E and GoogLeNet effectively capture core semantics while filtering noise is not empirically validated. The choice of Gaussian distributions for price modeling over other probabilistic approaches is not justified through comparative analysis.

## Next Checks

1. **Ablation Study Priority**: Conduct controlled experiments removing each major component (pseudo-modality contrastive learning, hierarchical pivot transformer, Gaussian price modeling) to quantify their individual contributions to the reported performance gains.

2. **Pseudo-Modality Quality Analysis**: Evaluate the semantic alignment between actual and pseudo-modalities using established metrics (e.g., CLIP score, human evaluation) to validate that generated pseudo-modalities are meaningful and noise-reducing rather than noise-introducing.

3. **Cold-Start Scenario Testing**: Design experiments specifically targeting cold-start scenarios with varying degrees of item popularity to systematically measure MMSBR's effectiveness across the cold-start spectrum, rather than relying on aggregate performance metrics.