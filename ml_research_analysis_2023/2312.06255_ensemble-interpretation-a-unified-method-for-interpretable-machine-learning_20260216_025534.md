---
ver: rpa2
title: 'Ensemble Interpretation: A Unified Method for Interpretable Machine Learning'
arxiv_id: '2312.06255'
source_url: https://arxiv.org/abs/2312.06255
tags:
- interpretation
- feature
- ensemble
- learning
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ensemble interpretation, a method that combines
  multiple interpretable machine learning methods to improve stability and fidelity
  of model explanations. The authors propose a unified paradigm to represent interpretation
  methods and integrate their results into a single interpretation list.
---

# Ensemble Interpretation: A Unified Method for Interpretable Machine Learning

## Quick Facts
- arXiv ID: 2312.06255
- Source URL: https://arxiv.org/abs/2312.06255
- Reference count: 4
- One-line primary result: Ensemble interpretation improves stability and fidelity of model explanations by combining multiple interpretable methods

## Executive Summary
This paper introduces ensemble interpretation, a method that combines multiple interpretable machine learning methods to improve the stability and fidelity of model explanations. The authors propose a unified paradigm to represent interpretation methods and integrate their results into a single interpretation list. They also introduce Lscore, a supervised evaluation metric based on prior knowledge to assess interpretation quality. Experiments on wine quality and natural gas datasets demonstrate that ensemble interpretation produces more stable and human-consistent results than individual methods, with Lscore values of 1.0 and 0.9 respectively compared to 0.38-0.85 and 0.05-0.25 for individual methods.

## Method Summary
The method involves training a base model on a dataset, applying multiple interpretable methods (LIME, SHAP, PDP, ALE, PFI, GAD, GSD, FI) to generate individual interpretation lists, and then combining these lists using a scoring system based on feature ranking positions. The ensemble interpretation list is created by summing scores for each feature across all explainers and sorting by total score. The Lscore metric evaluates interpretation quality by comparing the ranked feature list against a reference interpretation label derived from human expertise.

## Key Results
- On the wine quality dataset, ensemble interpretation achieved an Lscore of 1.0 compared to 0.38-0.85 for individual methods
- On the natural gas dataset, ensemble interpretation achieved an Lscore of 0.9 compared to 0.05-0.25 for individual methods
- Using ensemble interpretation for feature selection significantly improved model generalization performance across multiple algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble interpretation improves stability by aggregating multiple explainer results through a unified mapping framework
- Mechanism: The paper constructs a composite mapping (ρ∘ξ∘τ∘σ) that transforms each interpretation model into a ranked feature list. Multiple such lists are then integrated by assigning scores based on feature ranking positions, producing a single ensemble interpretation list
- Core assumption: Different interpretable methods capture complementary aspects of feature importance, and their integration reduces variance inherent to individual methods
- Evidence anchors:
  - [abstract] "we define a unified paradigm to describe the common mechanism of different interpretation methods, and then integrate the multiple interpretation results to achieve more stable explanation"
  - [section] "The idea is to assign a score to each position in the interpretation list, then sum the scores obtained by each feature on each explainer to get the total score of each feature"
  - [corpus] Weak - no direct citation of ensemble interpretation methods from corpus, though ensemble learning is mentioned in related works
- Break condition: If the base explainers are highly correlated or capture identical perspectives, ensemble interpretation may not improve stability beyond individual methods

### Mechanism 2
- Claim: The unified paradigm (Definition 2) generalizes interpretation methods beyond binary feature attribution
- Mechanism: By representing interpretation models as g(x) = ∑φi gi(xi) + ε, the framework accommodates both linear and nonlinear feature effects, making it more flexible than binary attribution models
- Core assumption: Most interpretation methods can be expressed as a weighted sum of feature functions plus error, allowing them to be mapped to the same vector space
- Evidence anchors:
  - [section] "Compared with (2), the definition (3) can unify different interpretation methods into one framework"
  - [section] "The definition of (3) is necessary for the list (1)"
  - [corpus] Missing - no corpus evidence directly supports this generalization claim
- Break condition: If an interpretation method cannot be decomposed into a weighted sum of feature functions, it cannot be integrated into this paradigm

### Mechanism 3
- Claim: Supervised evaluation using Lscore provides a reliable metric for interpretation quality based on prior knowledge
- Mechanism: Lscore compares the ranked feature list from an interpretation method against a reference interpretation label derived from human expertise, producing a score between 0 and 1
- Core assumption: Human experts can provide reliable reference interpretation labels that represent correct feature importance orderings for specific tasks
- Evidence anchors:
  - [section] "For most machine learning tasks, human experts can get an interpretation list based on prior knowledge before explaining the model output"
  - [section] "This interpretation label is similar to the label of data in supervised learning. it is the standard answer to the Interpreting outcome that conforms to human cognition"
  - [corpus] Missing - no corpus evidence of similar supervised evaluation metrics for interpretation methods
- Break condition: If the reference interpretation label is inaccurate or biased, Lscore will provide misleading evaluations of interpretation method quality

## Foundational Learning

- **Concept**: Ensemble learning principles
  - Why needed here: Understanding how aggregating multiple models can improve stability and reduce variance is foundational to ensemble interpretation
  - Quick check question: What is the key difference between bagging and boosting in ensemble learning, and how might these principles apply to ensemble interpretation?

- **Concept**: Feature attribution methods
  - Why needed here: The unified paradigm requires understanding how different attribution methods (SHAP, LIME, etc.) quantify feature importance
  - Quick check question: How does SHAP's use of Shapley values differ from LIME's local linear approximation approach?

- **Concept**: Supervised evaluation metrics
  - Why needed here: Lscore introduces a supervised evaluation framework for interpretation methods, requiring understanding of how evaluation metrics work in machine learning
  - Quick check question: What are the key characteristics that make a good evaluation metric for interpretation methods?

## Architecture Onboarding

- **Component map**: Model → Base Explainers → Composite Mapping → Interpretation Lists → Integration → Ensemble Interpretation List → Lscore Evaluation
- **Critical path**: Model → Base Explainers → Composite Mapping → Interpretation Lists → Integration → Ensemble Interpretation List → Lscore Evaluation
- **Design tradeoffs**: The unified paradigm sacrifices some method-specific nuance for generality. Homogeneous ensemble interpretation may be simpler but less comprehensive than heterogeneous approaches. Lscore requires reliable reference labels but provides meaningful evaluation
- **Failure signatures**: Inconsistent interpretation lists across base explainers suggest instability. Poor Lscore performance indicates misalignment with human knowledge. Ensemble interpretation that doesn't improve over individual methods suggests redundant or correlated explainers
- **First 3 experiments**:
  1. Run multiple instances of LIME on a simple dataset and compare stability of individual vs ensemble interpretation lists
  2. Apply ensemble interpretation to a dataset with known feature importance and verify alignment with ground truth
  3. Compare feature selection performance using correlation analysis vs ensemble interpretation on a classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the ensemble interpretation framework be extended to handle interpretation models that cannot be mapped to interpretation lists?
- Basis in paper: [explicit] The authors note that "there may be some interpretation models that cannot be mapped to interpretation list" as a limitation of their method
- Why unresolved: The paper does not provide any proposed solutions or extensions to handle such cases, leaving this as an open challenge
- What evidence would resolve it: Proposed methods or algorithms that successfully incorporate non-mappable interpretation models into the ensemble interpretation framework

### Open Question 2
- Question: What are the most effective methods for reducing the annotation cost of obtaining interpretation labels for supervised evaluation?
- Basis in paper: [explicit] The authors state that "the acquisition of interpretation label will cause huge annotation cost" as a limitation of their Lscore evaluation metric
- Why unresolved: While the authors propose using prior knowledge to obtain interpretation labels, they do not explore methods to reduce the annotation burden or alternative evaluation metrics that do not require labeled data
- What evidence would resolve it: Studies comparing the effectiveness and efficiency of different approaches to obtaining interpretation labels, such as active learning, semi-supervised learning, or unsupervised evaluation metrics

### Open Question 3
- Question: How can the ensemble interpretation idea be extended to create more diverse implementation methods beyond the interpretation list approach?
- Basis in paper: [explicit] The authors mention that "the means to achieve ensemble interpretation can be various" and encourage future researchers to create "more implementation methods"
- Why unresolved: The paper only presents one implementation method using interpretation lists, leaving the exploration of alternative approaches open for future research
- What evidence would resolve it: Novel ensemble interpretation methods that use different paradigms or representations, such as ensemble methods based on feature importance scores, partial dependence plots, or other interpretable machine learning techniques

## Limitations
- The framework may not accommodate interpretation models that cannot be mapped to interpretation lists
- Obtaining interpretation labels for supervised evaluation (Lscore) requires significant human annotation effort
- The paper only presents one implementation method, leaving exploration of alternative approaches open

## Confidence

- **Mechanism 1 (Ensemble stability improvement)**: Medium - While the concept aligns with ensemble learning principles, the specific implementation details and mathematical proofs are not fully detailed
- **Mechanism 2 (Unified paradigm generalization)**: Low - The paper claims broad applicability but provides limited evidence that all interpretation methods can be expressed in the proposed framework
- **Mechanism 3 (Lscore reliability)**: Medium - The supervised evaluation approach is innovative but untested across multiple domains and expert panels

## Next Checks

1. **Cross-domain validation**: Test ensemble interpretation on at least 5 diverse datasets spanning different domains (image, text, tabular) to assess generalizability of both the framework and Lscore metric

2. **Expert consensus study**: Conduct a study with multiple domain experts to evaluate consistency of reference interpretation labels and their impact on Lscore reliability

3. **Comparison with ground truth**: Apply ensemble interpretation to synthetic datasets where ground truth feature importance is known, comparing results against individual methods to verify accuracy claims