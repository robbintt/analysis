---
ver: rpa2
title: Image Labels Are All You Need for Coarse Seagrass Segmentation
arxiv_id: '2303.00973'
source_url: https://arxiv.org/abs/2303.00973
tags:
- seagrass
- classi
- image
- seaclip
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a novel approach for weakly supervised multi-species
  seagrass coarse segmentation that requires only image-level labels during training.
  Our method, an ensemble of SeaFeats and SeaCLIP, outperforms prior approaches that
  require patch-level labels by 6.8% absolute F1 score on the DeepSeagrass dataset.
---

# Image Labels Are All You Need for Coarse Seagrass Segmentation

## Quick Facts
- arXiv ID: 2303.00973
- Source URL: https://arxiv.org/abs/2303.00973
- Authors: 
- Reference count: 40
- We propose a novel approach for weakly supervised multi-species seagrass coarse segmentation that requires only image-level labels during training, outperforming prior approaches by 6.8% absolute F1 score.

## Executive Summary
This paper introduces a novel weakly supervised approach for coarse multi-species seagrass segmentation using only image-level labels. The method combines two complementary techniques: SeaFeats, which uses unsupervised contrastive pretraining and feature similarity for pseudo-label generation, and SeaCLIP, which leverages the CLIP vision-language model as a supervisory signal. By requiring 25× fewer labels than traditional patch-level supervision methods, this approach achieves state-of-the-art performance on the DeepSeagrass dataset while demonstrating robust generalization to real-world deployment scenarios.

## Method Summary
The approach trains a ResNet-18 encoder using two complementary methods. SeaFeats initializes the encoder with SimCLR weights and generates pseudo-labels by comparing patch features to dynamically updated class template vectors using cosine similarity. SeaCLIP uses CLIP to generate pseudo-labels through prompt-based queries and trains a separate ResNet-18. Both models are trained with weighted cross-entropy loss and combined at inference through logit averaging. The ensemble leverages SeaFeats' generally higher confidence and SeaCLIP's conservative predictions on degraded patches to achieve robust performance.

## Key Results
- Ensemble of SeaFeats and SeaCLIP outperforms prior patch-level supervised methods by 6.8% absolute F1 score on DeepSeagrass dataset
- Method requires only image-level labels, reducing annotation burden by 25× compared to patch-level supervision
- Demonstrates robust generalization including outlier detection and successful application on imagery from autonomous surface vehicles

## Why This Works (Mechanism)

### Mechanism 1
Feature similarity to class templates allows pseudo-labeling without patch-level supervision. Patches are compared to dynamically updated class template feature vectors; if similarity to background exceeds seagrass species similarity, the patch is labeled as background, otherwise as the image-level seagrass species. Core assumption: The learned feature space from the encoder preserves semantic distinctions between seagrass species and background, so cosine similarity correlates with correct class assignment. Evidence anchors: [abstract] "SeaFeats uses unsupervised contrastive pretraining and feature similarity to separate background and seagrass patches"; [section] "After every epoch, we extract feature vectors... We perform the bundling operation... to obtain a template feature vector for each class". Break condition: If the feature space collapses or is too entangled, cosine similarity will not distinguish classes, causing incorrect pseudo-labels and training collapse.

### Mechanism 2
CLIP large vision-language model provides effective weak supervision by filtering out-of-distribution patches. CLIP is queried with natural language prompts describing seagrass; patches with highest similarity to seagrass prompts are pseudo-labeled as seagrass species, others as background. Core assumption: CLIP's general semantic understanding of "seagrass" is sufficiently robust to recognize underwater seagrass patches even without species-specific training. Evidence anchors: [abstract] "SeaCLIP leverages the CLIP vision-language model as a supervisory signal to generate pseudo-labels"; [section] "We leverage the pretrained large vision-language model, CLIP... to predict which patches in training images contain seagrass". Break condition: If CLIP misclassifies non-seagrass patches as seagrass or fails to detect blurry/distant seagrass patches, training will be corrupted with wrong pseudo-labels.

### Mechanism 3
Ensemble averaging of SeaFeats and SeaCLIP logits improves robustness and reduces overconfident misclassifications. Normalized logits from both models are averaged before softmax, allowing conservative predictions from SeaCLIP to counterbalance overconfident predictions from SeaFeats. Core assumption: The two models capture complementary strengths; SeaFeats is generally more confident but prone to errors on degraded patches, while SeaCLIP is conservative on degraded patches but less precise overall. Evidence anchors: [abstract] "Our method, an ensemble of SeaFeats and SeaCLIP... outperforms prior approaches"; [section] "We find that SeaCLIP is more likely to classify visually degraded patches as background... When combined with SeaFeats, this is a desirable property". Break condition: If both models fail in the same regions or if logit averaging does not preserve magnitude differences, ensemble benefits will vanish.

## Foundational Learning

- Concept: Contrastive self-supervised learning (SimCLR)
  - Why needed here: Initializes the feature extractor to produce discriminative embeddings without requiring labeled data, crucial for SeaFeats' template similarity
  - Quick check question: What is the purpose of using a temperature parameter τ in SimCLR's contrastive loss?

- Concept: Large vision-language models (CLIP)
  - Why needed here: Provides zero-shot semantic understanding to generate pseudo-labels, enabling weak supervision with only image-level labels
  - Quick check question: How does CLIP's zero-shot classification determine the class of an image given a set of text prompts?

- Concept: Pseudo-label generation from weak supervision
  - Why needed here: Allows training a model to predict patch-level labels when only image-level labels are available, avoiding expensive patch annotation
  - Quick check question: What is the risk of using pseudo-labels that are noisy or incorrect during training?

## Architecture Onboarding

- Component map: Encoder (ResNet-18) -> Classification head (Fully connected layers) -> Template generator (Dynamic per-class feature vector computation) -> CLIP wrapper (Prompt engineering and zero-shot inference) -> Ensemble module (Logit averaging before softmax)

- Critical path:
  1. Pretrain ResNet-18 with SimCLR
  2. For each epoch: Extract features for all patches; Compute template vectors per class; Assign pseudo-labels via cosine similarity (SeaFeats) or CLIP (SeaCLIP); Train classification heads with weighted cross-entropy
  3. At inference: run both models in parallel, average logits, softmax

- Design tradeoffs:
  - SimCLR vs ImageNet pretraining: SimCLR avoids ImageNet bias, better for domain shift
  - CLIP prompts: More prompts increase robustness but cost inference time
  - Ensemble: Improves robustness but doubles inference compute

- Failure signatures:
  - Low validation F1 with high training F1: overfitting to pseudo-labels
  - Very low SeaCLIP recall: prompts not capturing seagrass semantics
  - Template vectors collapsing: encoder not discriminative enough

- First 3 experiments:
  1. Ablation: Train SeaFeats with ImageNet vs SimCLR initialization and compare F1
  2. Ablation: Remove CLIP from ensemble, compare performance on blurry patches
  3. Ablation: Remove dynamic template updates, use static templates and measure impact

## Open Questions the Paper Calls Out

### Open Question 1
How can the SeaFeats and SeaCLIP methods be adapted for pixel-wise semantic segmentation of seagrass meadows? The authors mention that future work could involve pixel-wise semantic segmentation of multi-species seagrass images. This is unresolved because the current approach focuses on coarse segmentation at the patch level, and adapting it to pixel-level segmentation would require significant architectural changes and possibly different loss functions. Implementation and evaluation of pixel-wise semantic segmentation models using SeaFeats and SeaCLIP as a starting point, and comparison with state-of-the-art pixel-wise segmentation methods would resolve this.

### Open Question 2
Can the ensemble of SeaFeats and SeaCLIP be further improved by incorporating additional complementary methods or models? The authors discuss the complementary nature of SeaFeats and SeaCLIP, suggesting that other methods could potentially enhance the ensemble. This is unresolved because the paper only explores the combination of SeaFeats and SeaCLIP, and there may be other methods or models that could provide additional benefits when combined with the ensemble. Experiments incorporating additional methods or models into the ensemble, and evaluation of their impact on performance and robustness would resolve this.

### Open Question 3
How does the performance of SeaFeats and SeaCLIP compare to other weakly supervised segmentation methods in different domains, such as medical imaging or remote sensing? The authors demonstrate the effectiveness of their methods in the specific context of seagrass segmentation, but do not compare their performance to other weakly supervised segmentation methods in different domains. This is unresolved because the performance of SeaFeats and SeaCLIP may be specific to the seagrass segmentation task, and their effectiveness in other domains is unknown. Implementation and evaluation of SeaFeats and SeaCLIP on datasets from other domains, and comparison with state-of-the-art weakly supervised segmentation methods in those domains would resolve this.

## Limitations
- Performance depends on CLIP's semantic understanding of seagrass, which may not generalize to all underwater scenarios
- Method may struggle with highly degraded imagery where even humans have difficulty with classification
- Ensemble approach doubles inference compute requirements compared to single-model approaches

## Confidence

**High**: Image-level labels reduce annotation burden by 25×; ensemble approach improves robustness

**Medium**: CLIP provides effective weak supervision for seagrass patches; SimCLR pretraining improves feature discrimination

**Low**: Generalization to all underwater scenarios; robustness to extreme degradation; performance without any image-level labels

## Next Checks

1. Ablate CLIP from ensemble and measure performance degradation on blurry/distant patches
2. Train SeaFeats with ImageNet pretraining instead of SimCLR and compare feature space discriminability
3. Test pseudo-label quality by comparing CLIP-generated labels to human-annotated patches in validation set