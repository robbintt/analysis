---
ver: rpa2
title: Unraveling the Key Components of OOD Generalization via Diversification
arxiv_id: '2312.16313'
source_url: https://arxiv.org/abs/2312.16313
tags:
- data
- hypotheses
- diversification
- spurious
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the key components that contribute to the
  OOD generalization abilities of diversification methods, which aim to find multiple
  diverse hypotheses that rely on different features. Through theoretical and empirical
  analyses, the authors show that diversification methods are highly sensitive to
  the distribution of unlabeled data used for diversification and can underperform
  significantly when away from a method-specific sweet spot.
---

# Unraveling the Key Components of OOD Generalization via Diversification

## Quick Facts
- arXiv ID: 2312.16313
- Source URL: https://arxiv.org/abs/2312.16313
- Reference count: 40
- Primary result: Diversification methods' effectiveness depends critically on unlabeled data distribution and learning algorithm choice

## Executive Summary
This paper investigates the key components that contribute to the out-of-distribution (OOD) generalization abilities of diversification methods, which aim to find multiple diverse hypotheses that rely on different features. Through theoretical and empirical analyses, the authors show that diversification methods are highly sensitive to the distribution of unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. Additionally, they demonstrate that diversification alone is insufficient for OOD generalization and requires additional biases brought by the choice of learning algorithm, such as architecture and pretraining. The optimal choice of learning algorithm depends on the unlabeled data, and vice versa, indicating a co-dependence between them. Furthermore, the authors show that increasing the number of diverse hypotheses does not alleviate the aforementioned pitfalls. These findings provide insights into the critical design factors influencing the OOD generalization of diversification methods and can guide practitioners in using existing methods and researchers in developing new ones.

## Method Summary
The paper investigates diversification methods for OOD generalization by analyzing how they perform across different distributions of unlabeled OOD data and different learning algorithms. The core approach involves training multiple hypotheses that disagree on unlabeled data while maintaining performance on labeled training data. The authors use controlled synthetic datasets to systematically vary the spurious correlation ratio in unlabeled data and test different architectures (MLP, ResNet, ViT) with various pretraining strategies. They evaluate the performance of two diversification methods (DivDis and D-BAT) across these configurations, measuring test accuracy on OOD data to identify conditions where diversification succeeds or fails.

## Key Results
- Diversification methods show strong sensitivity to the spurious ratio of unlabeled OOD data, with optimal performance only at method-specific sweet spots
- Diversification alone cannot achieve OOD generalization without additional inductive biases from the learning algorithm
- The optimal choice of learning algorithm (architecture and pretraining) depends on the unlabeled data distribution, creating a co-dependence relationship
- Increasing the number of diverse hypotheses beyond 5 does not alleviate the sensitivity to data distribution or learning algorithm choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diversification methods' performance depends critically on the spurious ratio of unlabeled OOD data
- Mechanism: The diversification loss is designed to promote disagreement between hypotheses on unlabeled data. When the spurious ratio of unlabeled data matches the optimal point for a given diversification loss, the method can effectively distinguish between spurious and true hypotheses. At other spurious ratios, the loss either cannot differentiate hypotheses effectively or pushes toward incorrect solutions.
- Core assumption: The optimal spurious ratio for D-BAT is 0 (inversely correlated) while for DivDis-Seq it's 0.5 (balanced)
- Evidence anchors:
  - [abstract] "diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot"
  - [section 4.1] "Increasing the spurious ratio rDu from 0 to 0.5 will lead to hDB2 and hDD2 rotating counterclockwise"
  - [corpus] Weak - corpus shows related work on OOD detection but not the specific spurious ratio sensitivity
- Break condition: When unlabeled data distribution doesn't allow effective discrimination between hypotheses, or when the loss function doesn't align with the data distribution characteristics

### Mechanism 2
- Claim: Diversification alone cannot achieve OOD generalization without additional inductive biases
- Mechanism: Even with diverse hypotheses, the hypothesis space is too large to guarantee coverage of the true causal hypothesis. Without architectural or pretraining biases that favor generalizable solutions, diversification methods may find diverse but equally non-generalizable hypotheses.
- Core assumption: The hypothesis space is expressive enough to include all possible labeling functions
- Evidence anchors:
  - [abstract] "Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the model's architecture and pretraining, is crucial"
  - [section 5.1] "Proposition 2... having as many diverse hypotheses as the number of data points in Dood is still insufficient to guarantee better than a random guess accuracy"
  - [corpus] Weak - corpus shows related work on OOD detection but not the specific claim about insufficient diversity
- Break condition: When the learning algorithm's inductive biases align poorly with the true causal hypothesis, or when the hypothesis space is too constrained to include the true solution

### Mechanism 3
- Claim: The optimal choice of learning algorithm depends on the unlabeled data distribution
- Mechanism: Different architectures have different inductive biases that favor different types of solutions. When the unlabeled data distribution creates conditions where one architecture's preferred solution is the true hypothesis and another's is spurious, the optimal architecture switches based on the data.
- Core assumption: Different architectures have measurably different agreement scores with different hypotheses
- Evidence anchors:
  - [abstract] "The optimal choice of learning algorithm depends on the unlabeled data, and vice versa i.e. they are co-dependent"
  - [section 5.3] "we can change unlabeled data in a targeted way to make one architecture (e.g., MLP) generalize and the other (e.g., ResNet18) to have random guess test accuracy and vice versa"
  - [corpus] Weak - corpus shows related work on OOD detection but not the specific co-dependence claim
- Break condition: When unlabeled data distribution doesn't create sufficient contrast between architectures' preferences, or when both architectures happen to prefer the same solution

## Foundational Learning

- Concept: Spurious correlation and underspecification
  - Why needed here: The entire problem setting relies on understanding how models can learn features that work on training data but fail under distribution shift
  - Quick check question: If a feature perfectly predicts labels on training data but has no causal relationship to the label, what type of feature is this?

- Concept: Inductive bias in learning algorithms
  - Why needed here: The paper shows that diversification effectiveness depends on how the learning algorithm's biases align with the true hypothesis
  - Quick check question: Why might two different architectures trained on the same data converge to different solutions even when both achieve low training error?

- Concept: Agreement score as a measure of hypothesis alignment
  - Why needed here: The paper uses agreement score to measure how well hypotheses align with architectural inductive biases
  - Quick check question: If two models trained from different initializations on the same data make identical predictions on a test set, what does this imply about their agreement score and potential generalization?

## Architecture Onboarding

- Component map: Training data with labels -> First hypothesis via ERM -> Unlabeled OOD data for diversification -> Multiple diverse hypotheses via diversification loss -> Best hypothesis via disambiguation
- Critical path: Choose architecture → Train first hypothesis via ERM on Dt → Use Du to find diverse hypotheses via diversification loss → Select best hypothesis via disambiguation
- Design tradeoffs: The diversification loss must balance between encouraging diversity and maintaining good training performance; too much emphasis on diversity can lead to poor training accuracy, while too little fails to find the true hypothesis
- Failure signatures: When performance drops significantly with different unlabeled data distributions, when changing architectures causes large accuracy drops, or when increasing the number of hypotheses fails to improve performance
- First 3 experiments:
  1. Test diversification performance across different spurious ratios of unlabeled data (0, 0.25, 0.5) to identify the method-specific sweet spot
  2. Compare diversification performance across different architectures (MLP, ResNet, ViT) with the same unlabeled data to measure architecture sensitivity
  3. Construct targeted unlabeled data distributions that favor one architecture over another to demonstrate co-dependence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of diversification methods change with more than 5 hypotheses (K > 5)?
- Basis in paper: [inferred] The paper mentions that increasing the number of hypotheses does not bridge the performance gap between different models, but it does not test beyond K=5.
- Why unresolved: The paper only tests up to K=5 hypotheses, and the impact of larger K values on performance is unknown.
- What evidence would resolve it: Running experiments with K > 5 and measuring the test accuracy to see if there is any improvement in performance.

### Open Question 2
- Question: How does the choice of diversification loss affect the performance of diversification methods when the spurious ratio of unlabeled data is not at the optimal value?
- Basis in paper: [explicit] The paper shows that the performance of diversification methods is highly sensitive to the distribution of the unlabeled data used for diversification, and neither diversification loss is optimal in all cases.
- Why unresolved: The paper does not investigate how the choice of diversification loss affects the performance when the spurious ratio of unlabeled data is not at the optimal value.
- What evidence would resolve it: Running experiments with different diversification losses and measuring the test accuracy for various spurious ratios of unlabeled data.

### Open Question 3
- Question: How does the co-dependence between the learning algorithm and unlabeled data generalize to other architectures and pretraining methods?
- Basis in paper: [explicit] The paper demonstrates the co-dependence between the learning algorithm and unlabeled data using a specific architecture pair (MLP and ResNet18) and pretraining methods.
- Why unresolved: The paper only tests the co-dependence with one architecture pair and a limited set of pretraining methods, so it is unclear how this co-dependence generalizes to other architectures and pretraining methods.
- What evidence would resolve it: Running experiments with different architecture pairs and pretraining methods to see if the co-dependence holds.

## Limitations
- Theoretical claims about hypothesis space coverage are proven under simplified linear models but extension to deep networks remains heuristic
- Empirical evaluations focus on controlled synthetic datasets with limited validation on real-world OOD scenarios
- Co-dependence between architecture and unlabeled data is demonstrated in specific cases but lacks general theoretical framework

## Confidence

- Mechanism 1 (Spurious ratio sensitivity): High - extensively validated across multiple diversification methods
- Mechanism 2 (Need for inductive biases): Medium - theoretically sound but empirical support limited to specific architectures
- Mechanism 3 (Architecture-unlabeled data co-dependence): Medium - demonstrated through targeted experiments but requires broader validation

## Next Checks

1. Test the spurious ratio sensitivity across additional diversification methods beyond D-BAT and DivDis-Seq to establish whether the sweet spot phenomenon is method-specific or general
2. Evaluate the architecture co-dependence on larger-scale real-world datasets (ImageNet variants) to assess practical significance beyond controlled experiments
3. Measure the performance impact when combining diversification with different pretraining strategies (MoCo, SimCLR) to quantify the contribution of architectural biases