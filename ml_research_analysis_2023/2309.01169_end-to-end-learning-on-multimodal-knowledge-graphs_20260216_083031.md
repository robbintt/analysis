---
ver: rpa2
title: End-to-End Learning on Multimodal Knowledge Graphs
arxiv_id: '2309.01169'
source_url: https://arxiv.org/abs/2309.01169
tags:
- information
- structure
- which
- knowledge
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning from heterogeneous
  multimodal knowledge graphs, where raw literal values are often ignored in favor
  of purely relational information. The authors propose a multimodal message passing
  network that learns embeddings for node features belonging to five different modalities,
  including numbers, texts, dates, images, and geometries.
---

# End-to-End Learning on Multimodal Knowledge Graphs

## Quick Facts
- arXiv ID: 2309.01169
- Source URL: https://arxiv.org/abs/2309.01169
- Reference count: 40
- Primary result: Multimodal message passing networks that learn embeddings for node features across five modalities (numbers, texts, dates, images, geometries) can significantly improve performance on node classification and link prediction tasks in knowledge graphs.

## Executive Summary
This paper addresses the challenge of learning from heterogeneous multimodal knowledge graphs where raw literal values are often ignored in favor of purely relational information. The authors propose a multimodal message passing network that uses dedicated encoders for each of five modalities (numerical, textual, temporal, visual, spatial) to naturally learn embeddings, which are then projected into a joint representation space alongside relational structure via R-GCN layers. Experiments on synthetic and real-world datasets demonstrate that including multimodal information can significantly improve performance on node classification and link prediction tasks, though the impact varies depending on the dataset and task characteristics.

## Method Summary
The proposed MR-GCN model extends R-GCNs to handle multimodal node features by first inferring modalities from RDF datatype annotations, then encoding literals through modality-specific neural networks (numerical: identity, temporal: trigonometric encoding + feedforward, textual: character-level CNN, visual: MobileNet, spatial: temporal CNN). These embeddings are concatenated with entity identity vectors to form multimodal node embeddings, which are then propagated through R-GCN layers. The model is trained separately for node classification (softmax output) and link prediction (DistMult decoder), with performance evaluated using accuracy, MRR, and Hits@k metrics along with statistical significance testing.

## Key Results
- Including multimodal information significantly affects performance, but impact varies by dataset and task characteristics
- Performance gains from node features are more pronounced in synthetic datasets than real-world datasets
- When all modalities are included, overall performance approaches or equals that of the best single modality

## Why This Works (Mechanism)

### Mechanism 1
Including raw literal values in graph learning, processed according to their modalities, improves model performance over ignoring them. Dedicated encoders per modality transform literal values into meaningful embeddings, which are then projected into a joint representation space alongside relational structure via message passing. Core assumption: The raw value contains signal not already captured by graph structure, and modality-specific encoding better preserves that signal than treating all literals as generic identifiers.

### Mechanism 2
Message passing allows learned embeddings from node features to propagate through the graph, integrating multimodal and relational information. In MR-GCN, embeddings from all modalities are concatenated with entity identity vectors, then passed through R-GCN layers that propagate these enriched representations across edges. Core assumption: The graph structure meaningfully connects entities to literals, and a few message passing hops are sufficient to distribute multimodal signals.

### Mechanism 3
Using datatype annotations to infer modality enables automatic and accurate encoding of literals without manual preprocessing. The MR-GCN implementation maps XSD datatypes and related vocabularies to specific neural encoders (e.g., XSD:string â†’ CNN for text), allowing end-to-end training on raw RDF data. Core assumption: Datatype annotations in the RDF graph are reliable and consistent indicators of the literal's semantic modality.

## Foundational Learning

- Concept: Message passing neural networks (MPNNs) and graph convolutions
  - Why needed here: The MR-GCN extends R-GCNs to handle multimodal node features, so understanding how R-GCNs propagate embeddings over edges is foundational
  - Quick check question: In a 2-layer R-GCN, how are node embeddings updated in the second layer given the output of the first?

- Concept: Multimodal representation learning
  - Why needed here: Each modality requires different vectorization strategies before joint embedding; the design of modality-specific encoders is central
  - Quick check question: Why is a trigonometric encoding used for temporal cyclic values rather than raw numeric values?

- Concept: Knowledge graph data models (RDF, literals, entities)
  - Why needed here: The MR-GCN operates on RDF graphs where literals carry raw data; understanding the distinction between entities and literals, and how they are represented as nodes, is essential
  - Quick check question: What is the difference between "split literals" and "merged literals" configurations in this work?

## Architecture Onboarding

- Component map: RDF graph -> Modality encoders (Numerical, Temporal, Textual, Visual, Spatial) -> Feature matrix F -> Node embedding matrix H0 = [I | F] -> R-GCN layers -> Decoder (DistMult/softmax) -> Loss

- Critical path: 1) Parse RDF graph and infer modalities from datatype annotations 2) Vectorize literals per modality and pass through respective encoders 3) Concatenate embeddings with identity matrix to form H0 4) Run R-GCN message passing 5) Apply final layer activation and decoder 6) Compute loss and backpropagate through all encoders

- Design tradeoffs: Separate encoders per modality increase parameter count but preserve modality-specific signal; using identity for numerical values avoids extra parameters but assumes linearity is sufficient; merged vs split literals affects baseline performance and potential gains

- Failure signatures: Degraded performance on split literals vs merged: likely structure already encodes literal equivalence; drop in performance when adding features: likely negative signal or noise overpowering benefits; no gain from adding modalities: likely missing datatype annotations or irrelevant modality for task

- First 3 experiments: 1) Run MR-GCN on a small RDF graph with only numerical literals; compare against R-GCN baseline 2) Add textual literals with simple string encoder; verify embedding changes and performance 3) Introduce visual literals and MobileNet encoder; check for overfitting or underfitting patterns

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise causes of the observed performance differences between tasks (node classification vs. link prediction) when including the same modality in the SYNTH dataset? The paper notes that "the influence of certain modalities on one task does not necessarily carry over to other tasks" and mentions differences between classification and link prediction results for AIFB+ and MUTAG, but does not provide a definitive explanation.

### Open Question 2
How can the MR-GCN model be improved to better handle negative signals and noise in real-world datasets? The paper observes that the MR-GCN seems unable to overcome the negative influence of some modalities in real-world datasets, resulting in overall worse performance with node features than without, but does not provide a detailed analysis of the reasons or propose specific solutions.

### Open Question 3
What is the optimal way to combine information from different modalities to achieve the best overall performance in multimodal knowledge graphs? The paper observes that when all modalities are included, the overall performance approaches or equals that of the best performing single modality, but also notes that the message-passing model largely succeeds in learning which information to include and which to ignore, without providing a comprehensive analysis of optimal combination strategies.

## Limitations

- Performance gains are highly dataset- and task-dependent, suggesting limited generalizability
- The mechanisms rely heavily on accurate datatype annotations, which may not be consistently available in real-world knowledge graphs
- The experimental setup uses synthetic and curated datasets, raising questions about robustness to noisy, real-world data

## Confidence

- Mechanism 1 (Multimodal embeddings improve performance): Medium
- Mechanism 2 (Message passing integrates multimodal and relational info): High
- Mechanism 3 (Datatype annotations enable automatic encoding): Low

## Next Checks

1. Test MR-GCN on a real-world knowledge graph with noisy or inconsistent datatype annotations to assess robustness to annotation quality
2. Conduct ablation studies removing individual modalities from the City dataset to quantify each modality's contribution to performance gains
3. Implement and evaluate a simplified version of MR-GCN using only entity identity vectors (no modality encoders) to establish a stricter baseline for measuring multimodal benefits