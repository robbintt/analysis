---
ver: rpa2
title: 'When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making'
arxiv_id: '2308.11721'
source_url: https://arxiv.org/abs/2308.11721
tags:
- algorithm
- human
- will
- where
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a human-algorithm collaboration model where
  an algorithm presents k items from a set of n to a human, who then selects a final
  item. The goal is to determine the optimal k that maximizes the probability of selecting
  the best item.
---

# When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making

## Quick Facts
- arXiv ID: 2308.11721
- Source URL: https://arxiv.org/abs/2308.11721
- Reference count: 40
- Primary result: Complementarity (joint system outperforms both human and algorithm alone) is impossible in anchored setting but possible when k=2 in unanchored setting with equal accuracy

## Executive Summary
This paper investigates when collaboration between humans and algorithms improves decision-making performance in a setting where an algorithm presents k items from n total to a human who then selects a final item. The authors analyze this problem using Mallows and Random Utilities models of noisy permutations to determine when the joint system outperforms both individual agents. Key findings reveal that complementarity is impossible when humans are anchored to algorithm rankings, but can occur when k=2 and accuracy rates are equal in the unanchored setting. The paper also identifies asymmetric regions where a more accurate human benefits more from collaboration than a more accurate algorithm.

## Method Summary
The authors analyze a human-algorithm collaboration model using two noise distributions: Mallows model for theoretical analysis and Random Utilities Model for experimental validation. They define "good events" where the joint system outperforms individual agents and construct a bijective mapping to "bad events" to prove impossibility results. The theoretical analysis focuses on calculating probabilities of selecting the best item under different settings (anchored vs unanchored, different k values) and accuracy rates. Experimental validation uses numerical simulations with RUM to confirm theoretical predictions and explore semi-anchored cases.

## Key Results
- Complementarity is impossible in anchored setting regardless of accuracy levels
- Complementarity possible when k=2 and human/algorithm have equal accuracy in unanchored setting
- Asymmetric complementarity zones exist where more accurate humans benefit more from collaboration than more accurate algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complementarity is possible when human and algorithm have equal accuracy and k=2, due to a balance between the algorithm's ability to narrow choices and the human's final selection improving accuracy.
- Mechanism: When the algorithm presents exactly 2 items (k=2), it creates a "good event" where either the algorithm includes the best item but doesn't rank it first, and the human selects it, or the human ranks the best item first among the presented items. The human's selection improves accuracy because they can identify the best item from the narrowed set, while the algorithm's narrowing prevents the human from being overwhelmed by too many choices.
- Core assumption: The human and algorithm have equal accuracy rates (ϕh = ϕa) and are unanchored, meaning the human's ranking is independent of the algorithm's presented ordering.
- Evidence anchors:
  - [abstract]: "we show that for multiple of noise models, it is optimal to set k ∈ [2, n − 1] - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately"
  - [section 4.3]: "Theorem 2. In the unanchored setting with permutations governed by the Mallows model, the probability of picking the best arm strictly increases in the joint human-algorithm system when exactly 2 items are presented (k = 2) and ϕa = ϕh"
- Break condition: If the human and algorithm have different accuracy rates, or if the human is anchored on the algorithm's ordering, complementarity becomes impossible or much more limited.

### Mechanism 2
- Claim: Anchoring always causes worse performance because it creates a bijection between "good events" and "bad events" where the bad events are strictly more likely under Mallows model.
- Mechanism: When the human is anchored on the algorithm's ordering, the best-item mapping (which swaps the best item with the item the algorithm ranked first) creates a bijective mapping between "good events" (where the joint system picks the best item but the algorithm alone would not) and "bad events" (where the algorithm alone would pick the best item but the joint system fails). Under the Mallows model, this mapping strictly decreases the number of inversions in the algorithm's ranking, making the "bad event" more likely than the corresponding "good event."
- Core assumption: The Mallows model governs the noise distributions, where probability decreases with increasing number of inversions.
- Evidence anchors:
  - [section 4.2]: "Theorem 1. In the anchored setting with Mallows model distributions for permutations, the probability of picking the best arm strictly decreases in the joint human-algorithm system, as compared to the algorithm alone"
  - [section 4.1]: "Lemma 1 states that there exists a bijective mapping between 'good events' and 'bad events'"
- Break condition: If the noise distribution doesn't follow the Mallows model property where probability is inversely related to inversions, or if the anchoring strength is reduced (semi-anchored case).

### Mechanism 3
- Claim: Asymmetry in complementarity zones exists because the human's accuracy more directly improves joint system performance than the algorithm's accuracy.
- Mechanism: When the human is more accurate, they are more likely to select the best item from the algorithm's presented set, directly improving joint accuracy. When the algorithm is more accurate, it becomes very good at including the best item in the presented set, but the marginal benefit of further improvement diminishes. This asymmetry creates wider complementarity zones when the human is more accurate than when the algorithm is more accurate.
- Core assumption: The human and algorithm have different accuracy rates and are unanchored.
- Evidence anchors:
  - [section 5.1]: "Lemma 2 (More accurate human). Consider n = 3, k = 2 where the human and algorithm both have unanchored Mallows models with ϕa ≠ ϕh. Then, there exists a region of complementarity where a more accurate human obtains higher accuracy when collaborating with a less accurate algorithm"
  - [section 5.1]: "Lemma 4 explains these results: for this setting, the performance of the joint system is always higher when the more accurate actor is the human, rather than the algorithm"
- Break condition: If the number of items presented (k) changes significantly, or if the noise model properties change, the asymmetry may be reduced or reversed.

## Foundational Learning

- Concept: Mallows model of noisy permutations
  - Why needed here: The Mallows model provides the theoretical foundation for analyzing when complementarity occurs, as it models how noisy rankings are generated around a central ordering with an accuracy parameter ϕ.
  - Quick check question: What property of the Mallows model makes it particularly useful for proving the complementarity results in this paper?

- Concept: Kendall-Tau distance and inversions
  - Why needed here: The Kendall-Tau distance (number of inversions) is the key metric for comparing permutations in the Mallows model, and understanding how the best-item mapping affects inversion counts is crucial for proving the anchoring results.
  - Quick check question: How does the best-item mapping affect the number of inversions in the algorithm's ranking, and why does this matter for proving Theorem 1?

- Concept: Complementarity in human-algorithm collaboration
  - Why needed here: Understanding the formal definition of complementarity (joint system outperforms both human and algorithm alone) is essential for framing the research questions and interpreting the results.
  - Quick check question: According to Bansal et al. [2021b], what condition must be met for a human-algorithm system to achieve complementarity?

## Architecture Onboarding

- Component map: Mallows/RUM model -> Noise distributions (Da, Dh) -> Best-item mapping -> Probability calculations -> Complementarity conditions

- Critical path:
  1. Define good and bad events based on whether joint system outperforms individual agents
  2. Construct best-item mapping between these events
  3. Analyze how this mapping affects probability under Mallows model
  4. Derive conditions for complementarity based on accuracy rates and anchoring

- Design tradeoffs:
  - Mallows model vs. Random Utility Model: Mallows allows cleaner theoretical proofs via inversion counting, while RUM requires numerical validation
  - Anchored vs. unanchored: Anchored setting simplifies human distribution but eliminates complementarity; unanchored enables complementarity but requires more complex analysis
  - k=2 vs. other k values: k=2 provides optimal balance between narrowing choices and human selection power

- Failure signatures:
  - If accuracy rates are too different: Complementarity zones become very narrow or disappear
  - If anchoring is present: Joint system always performs worse than algorithm alone
  - If noise distributions don't follow Mallows properties: Theoretical proofs may not hold

- First 3 experiments:
  1. Verify Theorem 1 by implementing anchored Mallows model and showing joint system always performs worse than algorithm alone
  2. Test Theorem 2 by implementing unanchored Mallows model with equal accuracy and k=2, showing complementarity occurs
  3. Explore asymmetry in complementarity zones by varying accuracy rates and measuring joint system performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does complementarity exist for k > 2 when human and algorithm have equal accuracy in the unanchored setting?
- Basis in paper: [inferred] The paper proves complementarity for k=2 in the Mallows model and shows it experimentally for the RUM, but only states "complementarity occurs for all k ∈ [2, n − 1]" in the RUM case without theoretical proof.
- Why unresolved: The theoretical proof in Section 4.3 is specific to k=2, and the paper doesn't provide theoretical analysis for k > 2 in either noise model.
- What evidence would resolve it: A theoretical proof showing whether complementarity exists for k > 2 in either the Mallows model or Random Utilities model when human and algorithm have equal accuracy.

### Open Question 2
- Question: How does the size of the complementarity zone scale with n (number of items) in the unanchored setting?
- Basis in paper: [inferred] The paper provides theoretical results for n=3 in Section 5 and experimental results for n=10 in Figure 3, but doesn't analyze how the complementarity zone changes as n varies.
- Why unresolved: The paper focuses on specific small values of n but doesn't provide a general analysis of how the complementarity zone (where a more accurate agent benefits from collaborating) scales with the total number of items.
- What evidence would resolve it: Analytical results showing how the boundaries of the complementarity zone change as a function of n, or experimental results across a range of n values showing the trend.

### Open Question 3
- Question: What is the impact of partial anchoring (0 < wa < 1) on the existence and size of complementarity zones?
- Basis in paper: [explicit] The paper shows experimental results for semi-anchored cases in Figure 1 with specific values of wa (0.25, 0.5, 1), demonstrating intermediate behavior between fully anchored and unanchored cases.
- Why unresolved: The paper only explores a few discrete values of wa and doesn't provide a comprehensive analysis of how complementarity zones change as a continuous function of the anchoring parameter wa.
- What evidence would resolve it: Theoretical or experimental analysis showing how the boundaries of complementarity zones vary with different values of the anchoring parameter wa between 0 and 1.

## Limitations
- Theoretical analysis relies heavily on Mallows model properties that may not generalize to other noise distributions
- Focus on specific task of selecting best item may not generalize to other collaborative decision-making scenarios
- Semi-anchored case lacks theoretical guarantees and relies primarily on experimental validation

## Confidence

- **High Confidence**: The impossibility of complementarity in the anchored setting (Theorem 1) has a rigorous proof based on the bijective mapping argument and Mallows model properties. The optimality of k=2 for equal accuracy in the unanchored setting (Theorem 2) is also well-established through theoretical analysis.

- **Medium Confidence**: The asymmetric complementarity zones (Lemmas 2-4) are supported by theoretical proofs for small n (n=3) but require numerical validation for larger n. The experimental results using Random Utility Models align with theoretical predictions, providing additional confidence.

- **Low Confidence**: The behavior in semi-anchored settings lacks theoretical guarantees, with current understanding based primarily on experimental observations.

## Next Checks
1. **Generalization Test**: Validate the complementarity results using alternative noise models (e.g., Thurstone-Mosteller model) to assess the robustness of findings beyond Mallows and RUM frameworks.

2. **Scalability Analysis**: Extend the asymmetric complementarity zone analysis to larger n values (n > 3) to verify whether the observed patterns persist and to quantify how the complementarity regions scale with problem size.

3. **Anchoring Strength Variation**: Systematically vary the anchoring strength parameter (beyond the binary anchored/unanchored distinction) to understand how gradual changes in anchoring affect complementarity and to potentially identify transition points or thresholds.