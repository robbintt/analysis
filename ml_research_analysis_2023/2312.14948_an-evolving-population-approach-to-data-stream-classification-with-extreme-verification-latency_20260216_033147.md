---
ver: rpa2
title: An Evolving Population Approach to Data-Stream Classification with Extreme
  Verification Latency
arxiv_id: '2312.14948'
source_url: https://arxiv.org/abs/2312.14948
tags:
- stream
- each
- data
- ensemble
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using evolutionary algorithms to handle data-stream
  classification when true labels are never available after initial training. It maintains
  an ensemble of one-class classifiers per class, where each classifier is treated
  as an agent in a sub-population.
---

# An Evolving Population Approach to Data-Stream Classification with Extreme Verification Latency

## Quick Facts
- **arXiv ID**: 2312.14948
- **Source URL**: https://arxiv.org/abs/2312.14948
- **Reference count**: 26
- **Primary result**: The paper proposes using evolutionary algorithms (GA and PSO) to handle data-stream classification with extreme verification latency, achieving high F1 scores on ten non-stationary data streams while outperforming peer methods.

## Executive Summary
This paper addresses the challenge of data-stream classification when true labels are unavailable after initial training, a scenario known as extreme verification latency (EVL). The authors propose maintaining an ensemble of one-class classifiers per class, where each classifier is treated as an agent in a sub-population. Fitness is assigned based on how many incoming points each classifier recognizes, and the population evolves using either a Genetic Algorithm (GA) or Particle Swarm Optimisation (PSO) to adapt to gradual concept drift. Experiments on ten non-stationary data-streams demonstrate that the GA-based approach achieves high F1 scores, outperforming peer methods like COMPOSE and LEVELiw, while PSO is faster but less accurate.

## Method Summary
The proposed method maintains an ensemble of one-class classifiers per class, treating each classifier as an agent in a sub-population. Each classifier is represented by a center, radius, label, and fitness value. During classification, incoming points are evaluated by each classifier, and the most confident classifier determines the predicted class. Fitness is updated based on recognition success and reset to zero when conflicting predictions occur. After each tumbling window, the population evolves using either GA or PSO to adapt to gradual concept drift. The method assumes gradual change in the data distribution and does not require true labels after initial training.

## Key Results
- The GA-based approach achieves high F1 scores on ten non-stationary data-streams, outperforming peer EVL methods (COMPOSE, LEVELiw).
- PSO is faster than GA but less accurate in terms of F1 score.
- The method scales linearly in memory and time with respect to the number of agents and classes.
- Increasing the number of agents per class improves accuracy but also increases computational cost.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maintaining a one-class classifier ensemble per class enables adaptation to gradual concept drift without requiring true labels after initial training.
- Mechanism: Each classifier is treated as an agent in a sub-population. Fitness is increased when the classifier recognizes incoming points and decreased when it does not. The population evolves via selection pressure, allowing the ensemble to adapt to changes in the data distribution over time.
- Core assumption: Change in the stream is gradual, so interesting areas of the feature space in one window will overlap with interesting areas in subsequent windows.
- Evidence anchors:
  - [abstract] "Each classifier is considered as an agent in the sub-population and is subject to selection pressure to find interesting areas of the feature space."
  - [section] "Classifiers which recognise incoming points are deemed fit and classifiers which are no longer recognising points (or fewer points) are deemed unfit."
- Break condition: If concept drift is sudden or if new classes emerge that were not present in the initial training data, the method may fail to adapt.

### Mechanism 2
- Claim: The fitness-based selection and evolution process allows the ensemble to track gradual changes in the data distribution.
- Mechanism: After each window, the fitness of each classifier is calculated based on how many points it recognizes. Classifiers with higher fitness have a greater chance to reproduce and explore their locality, adapting to the gradual underlying change.
- Core assumption: The gradual-change assumption, where P(x) will drift, potentially leading to a change in P(y|x), but P(y|x) will not change without a change in P(x).
- Evidence anchors:
  - [section] "This selection pressure forces the ensemble to adapt to the underlying change in the data-stream."
  - [section] "The challenge is to propagate the original training information through several timesteps, possibly indefinitely, while adapting to underlying change in the data-stream."
- Break condition: If the change in the data distribution is not gradual or if the fitness function does not accurately capture the interesting areas of the feature space, the ensemble may not adapt effectively.

### Mechanism 3
- Claim: The use of bio-inspired algorithms (GA and PSO) for updating the ensemble allows for efficient exploration and exploitation of the feature space.
- Mechanism: The GA uses selection, crossover, and mutation to evolve the population of classifiers, while PSO uses the flocking behavior of particles to move through the feature space following the fittest members of the swarm.
- Core assumption: The fitness of a classifier can be estimated based on how many points it recognizes, even without access to true labels.
- Evidence anchors:
  - [abstract] "We experiment with two bio-inspired approaches, namely: Genetic Algorithm (GA), and Particle Swarm Optimisation (PSO)."
  - [section] "We evaluate two population-based approaches to update the ensemble: A swarm-based approach in Particle Swarm Optimsation (PSO), and an evolutionary approach with a Genetic Algorithm (GA)."
- Break condition: If the bio-inspired algorithms do not effectively explore and exploit the feature space, or if the fitness function does not accurately guide the search, the ensemble may not adapt efficiently.

## Foundational Learning

- Concept: Concept drift
  - Why needed here: The method aims to adapt to changes in the data distribution over time, which is a key aspect of concept drift.
  - Quick check question: What are the two main types of concept drift, and how do they differ?

- Concept: One-class classification
  - Why needed here: The method uses one-class classifiers as the base classifiers in the ensemble, which are specialized in recognizing a single class.
  - Quick check question: What is the main advantage of using one-class classifiers in this context?

- Concept: Evolutionary algorithms
  - Why needed here: The method uses evolutionary algorithms (GA and PSO) to evolve the population of classifiers and adapt to changes in the data distribution.
  - Quick check question: What are the key differences between GA and PSO, and how do they apply to this problem?

## Architecture Onboarding

- Component map:
  Data stream -> Tumbling window split -> Ensemble of one-class classifiers -> Fitness calculation -> Population update (GA or PSO) -> Predicted class label

- Critical path:
  1. Initialize ensemble with labeled training data
  2. For each incoming point in the current window:
     - Pass point to each classifier in the ensemble
     - Calculate fitness of each classifier based on recognition
  3. At the end of the window:
     - Update the population of classifiers using GA or PSO
     - Propagate the updated ensemble to the next window

- Design tradeoffs:
  - GA vs. PSO: GA provides better accuracy but is slower, while PSO is faster but less accurate
  - Number of agents per class: Increasing the number of agents improves accuracy but also increases computation time
  - Sensitivity of classifier radius: Smaller radius leads to more classifiers but may result in overfitting

- Failure signatures:
  - Low F1-score: Indicates that the ensemble is not adapting well to the data distribution
  - High number of unrecognized points: Suggests that the classifiers are not recognizing the incoming points effectively
  - Slow adaptation: May indicate that the population update process is not efficient

- First 3 experiments:
  1. Evaluate the static ensemble on a non-stationary data stream to establish a baseline
  2. Evaluate the ensemble with GA population update on a non-stationary data stream
  3. Evaluate the ensemble with PSO population update on a non-stationary data stream

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of agents (classifiers) per ensemble to balance accuracy and computational cost across different data-streams?
- Basis in paper: [explicit] The paper discusses the trade-off between accuracy and computational time when varying the number of agents per ensemble, showing that more agents improve accuracy but increase processing time.
- Why unresolved: The experiments show that increasing the number of agents improves accuracy up to a point, but the optimal number likely depends on the specific data-stream characteristics, such as dimensionality and class distribution.
- What evidence would resolve it: Empirical studies across a wide range of data-streams with varying characteristics, testing different numbers of agents to determine the point of diminishing returns for accuracy improvement.

### Open Question 2
- Question: How can the proposed method be extended to handle concept evolution, where new classes appear in the data-stream that were not present in the initial training data?
- Basis in paper: [explicit] The authors acknowledge that their method assumes no concept evolution, meaning new classes will not appear in the stream after the initial training phase.
- Why unresolved: The paper does not propose a mechanism for detecting and incorporating new classes that emerge in the data-stream over time.
- What evidence would resolve it: Development and testing of an extension to the method that can detect new classes and adapt the ensemble structure to include classifiers for these new classes.

### Open Question 3
- Question: How can the proposed method be adapted to handle sudden real drift, where the relationship between features and classes changes abruptly without prior virtual drift?
- Basis in paper: [explicit] The authors state that their method is designed for gradual drift and may not effectively handle sudden real drift without access to true class labels or expert input.
- Why unresolved: The paper does not provide a solution for detecting or adapting to sudden changes in the data-stream distribution that occur without prior warning.
- What evidence would resolve it: Implementation and testing of mechanisms within the method to detect sudden drift events and rapidly adjust the ensemble to maintain accuracy.

## Limitations
- The method assumes gradual concept drift and may not effectively handle sudden drift or new class emergence.
- The evaluation relies on a specific set of non-stationary data streams and uses F1-score as the primary metric, without reporting on additional performance measures.
- The paper does not discuss the computational overhead of maintaining multiple one-class classifiers per class or provide sensitivity analysis for critical hyperparameters.
- The results are compared only with peer EVL methods without a non-adaptive baseline, limiting the understanding of the relative benefit.

## Confidence

- **High confidence**: The core mechanism of using evolving one-class classifiers per class to handle gradual concept drift without labels is well-supported by the experiments and literature.
- **Medium confidence**: The comparative advantage over peer methods (COMPOSE, LEVELiw) is established but could be further validated with additional datasets and performance metrics.
- **Low confidence**: The paper's claims about scalability and efficiency are not fully substantiated due to lack of detailed analysis of computational overhead and memory usage.

## Next Checks

1. Evaluate the method's performance on data streams with sudden concept drift to assess its robustness to abrupt changes.
2. Conduct a sensitivity analysis to determine the impact of hyperparameters (numAgents, radius) on model performance and resource usage.
3. Compare the method's efficiency and effectiveness with a non-adaptive baseline to quantify the benefits of the evolving population approach.