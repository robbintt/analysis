---
ver: rpa2
title: 'Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent
  Knowledge Neurons and Degenerate Knowledge Neurons'
arxiv_id: '2308.13198'
source_url: https://arxiv.org/abs/2308.13198
tags:
- knowledge
- neurons
- degenerate
- plms
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding how factual
  knowledge is stored in multilingual pre-trained language models (PLMs). The authors
  introduce an Architecture-adapted Multilingual Integrated Gradients method to localize
  knowledge neurons more precisely across different PLM architectures and languages.
---

# Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons

## Quick Facts
- arXiv ID: 2308.13198
- Source URL: https://arxiv.org/abs/2308.13198
- Reference count: 9
- This paper introduces Architecture-adapted Multilingual Integrated Gradients method and discovers Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons, achieving up to 84.34% improvement in knowledge localization accuracy.

## Executive Summary
This paper addresses the challenge of understanding how factual knowledge is stored in multilingual pre-trained language models (PLMs). The authors propose a novel knowledge localization method called Architecture-adapted Multilingual Integrated Gradients (AMIG) that can effectively address the lack of universal methods for different PLM architectures and the lack of exploration in multiple languages. Through extensive experiments, they discover two new types of neurons: Language-Independent Knowledge Neurons that store knowledge transcending language barriers, and Degenerate Knowledge Neurons that enable robust knowledge storage through functional overlap.

## Method Summary
The authors propose Architecture-adapted Multilingual Integrated Gradients (AMIG) to localize knowledge neurons more precisely across different PLM architectures and languages. The method adapts baseline vectors to specific model architectures (auto-encoding vs. auto-regressive) and adjusts thresholds per language. Knowledge neurons are identified through gradient-based attribution scores, with language-independent neurons detected by intersecting neuron sets across languages, and degenerate neurons identified through subset relationship analysis. The method is validated on mLAMA dataset using m-BERT and m-GPT models in English and Chinese.

## Key Results
- Architecture-adapted multilingual integrated gradients achieves up to 84.34% improvement in knowledge localization accuracy
- Language-independent knowledge neurons enable effective cross-lingual knowledge editing
- Degenerate knowledge neurons help detect incorrect facts with up to 167,150% improvement in F1-score for fact-checking tasks

## Why This Works (Mechanism)

### Mechanism 1
- Language-independent knowledge neurons store factual knowledge in a form that transcends language boundaries, enabling cross-lingual knowledge editing
- These neurons are identified by intersecting knowledge neurons derived from multiple languages using the same fact
- Core assumption: Factual knowledge with identical semantics will activate overlapping neurons across languages in multilingual PLMs
- Break condition: If the multilingual training data lacks sufficient overlap or if the model architecture prevents shared representations across languages, the intersection may yield empty or misleading neuron sets

### Mechanism 2
- Degenerate knowledge neurons allow multiple distinct neuron sets to store the same fact, providing redundancy that enhances robustness
- When multiple disjoint neuron subsets can independently encode the same fact, suppressing any single subset doesn't impair performance
- Core assumption: The model can redundantly encode the same factual knowledge in different neuron sets without interference
- Break condition: If the threshold for identifying degenerate neurons is too strict, or if the model architecture doesn't support such functional overlap, the phenomenon may not be detectable

### Mechanism 3
- Architecture-adapted multilingual integrated gradients (AMIG) provides more precise knowledge neuron localization than previous methods by adapting baseline vectors to specific model architectures and languages
- By customizing baseline vectors for auto-encoding vs. auto-regressive models and adjusting thresholds per language, AMIG better isolates neurons responsible for factual knowledge
- Core assumption: Different PLM architectures and languages require different baseline configurations to accurately measure neuron attribution
- Break condition: If the baseline adaptation is incorrectly implemented or if the attribution calculation is too sensitive to noise, localization accuracy may degrade

## Foundational Learning

- Concept: Integrated Gradients
  - Why needed here: Provides a principled way to attribute a model's prediction to individual neurons by integrating gradients along a path from a baseline to the input
  - Quick check question: What is the purpose of the baseline vector in integrated gradients, and why must it contain minimal information?

- Concept: Neuron attribution and threshold filtering
  - Why needed here: Enables identification of which neurons are most responsible for specific factual knowledge by ranking neurons based on attribution scores and selecting those above a threshold
  - Quick check question: How does varying the threshold τ affect the number and quality of identified knowledge neurons?

- Concept: Neuron intersection and set operations
  - Why needed here: Critical for identifying language-independent neurons (intersection across languages) and degenerate neurons (subset relationships within neuron sets)
  - Quick check question: What mathematical operation is used to identify neurons common to multiple languages, and why is this effective?

## Architecture Onboarding

- Component map: Input preprocessing -> Architecture-adapted baseline generation -> Integrated gradients computation -> Attribution score calculation -> Threshold filtering -> Knowledge neuron localization -> Language-independent neuron detection (intersection) -> Degenerate neuron detection (subset analysis) -> Fact-checking or editing experiments
- Critical path: 1) Generate architecture-specific baseline vectors, 2) Compute integrated gradients for each neuron, 3) Normalize and threshold to identify knowledge neurons, 4) Perform intersection for language-independent neurons, 5) Analyze subset relationships for degenerate neurons
- Design tradeoffs:
  - Precision vs. recall in threshold selection: Higher thresholds yield fewer but more precise neurons; lower thresholds increase coverage but risk noise
  - Computational cost vs. thoroughness in degenerate neuron detection: Full subset enumeration is O(2^n) but can be approximated by limiting to pairs
  - Baseline complexity vs. generalization: More complex baselines may better isolate neuron contributions but risk overfitting to specific inputs
- Failure signatures:
  - Low or zero language-independent neurons despite multilingual data -> baseline adaptation may be incorrect
  - Degenerate neuron detection fails to find any pairs -> thresholds too strict or neuron attribution too noisy
  - Fact-checking recall near 1.0 with low precision -> degenerate neurons may not be encoding robustness but rather noise
- First 3 experiments:
  1. Localize knowledge neurons for a known fact in both English and Chinese using AMIG; verify overlap and attribution scores
  2. Identify language-independent neurons for the same fact; test cross-lingual editing by suppressing/enhancing these neurons and observing changes in both languages
  3. Detect degenerate neuron pairs for a fact; verify that suppressing one neuron in a pair does not degrade performance, but suppressing both does

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Language-Independent Knowledge Neurons function in cross-lingual knowledge transfer beyond factual knowledge, such as procedural or conceptual knowledge?
- Basis in paper: The paper identifies Language-Independent Knowledge Neurons that store factual knowledge across languages and demonstrates their role in cross-lingual knowledge editing
- Why unresolved: The experiments focus solely on factual knowledge, leaving open the question of whether these neurons can facilitate the transfer of other types of knowledge
- What evidence would resolve it: Conducting experiments that test the role of these neurons in tasks involving procedural or conceptual knowledge across languages

### Open Question 2
- Question: What are the specific mechanisms that allow Degenerate Knowledge Neurons to enhance the robustness of factual knowledge in PLMs?
- Basis in paper: The paper discovers Degenerate Knowledge Neurons and suggests their functional overlap enhances robustness, but does not detail the underlying mechanisms
- Why unresolved: While the paper identifies the existence and benefits of these neurons, it does not explore the detailed mechanisms behind their robustness-enhancing properties
- What evidence would resolve it: Detailed studies that map the interactions and dependencies among degenerate neurons and their impact on model performance

### Open Question 3
- Question: Can the Architecture-adapted Multilingual Integrated Gradients method be extended to other forms of knowledge beyond factual knowledge, such as emotional or cultural knowledge?
- Basis in paper: The method is designed for localizing factual knowledge neurons across different languages and architectures
- Why unresolved: The method's effectiveness and applicability to non-factual knowledge types have not been tested
- What evidence would resolve it: Experiments applying the method to tasks involving emotional or cultural knowledge, assessing its precision and adaptability

## Limitations
- The discovery of language-independent and degenerate knowledge neurons is presented as novel, but the corpus analysis reveals only 1-2 related papers per concept, suggesting limited independent validation
- The computational complexity of detecting degenerate neurons (O(2^n)) may prevent thorough exploration in large models
- The threshold-based identification relies on heuristic parameters without systematic sensitivity analysis

## Confidence

- **High confidence**: The methodological framework for knowledge localization using architecture-adapted integrated gradients is well-established and the basic experimental setup follows standard practices
- **Medium confidence**: The existence of language-independent neurons is plausible given multilingual model architecture, but the specific quantification and functional significance require further validation
- **Low confidence**: The degenerate neuron phenomenon and its claimed robustness benefits are the most speculative claims, as the detection method is computationally limited and the interpretation of functional overlap as "robustness" may conflate redundancy with noise

## Next Checks
1. **Threshold sensitivity analysis**: Systematically vary the attribution score threshold τ and degenerate neuron detection thresholds (Tlow, Thigh) to determine stability of language-independent and degenerate neuron identifications across different parameter settings
2. **Cross-architecture validation**: Apply the same AMIG method to additional PLM architectures (e.g., XLM-R, mT5) to verify whether language-independent and degenerate neurons are consistent phenomena across different model families or specific to m-BERT/m-GPT
3. **Ablation robustness testing**: Beyond the reported fact-checking improvements, conduct comprehensive ablation studies where multiple degenerate neuron pairs are suppressed simultaneously to quantify actual performance degradation patterns and distinguish true robustness from coincidental redundancy