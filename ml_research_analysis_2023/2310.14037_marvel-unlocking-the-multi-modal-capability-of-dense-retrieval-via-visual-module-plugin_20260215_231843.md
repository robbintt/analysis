---
ver: rpa2
title: 'MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual
  Module Plugin'
arxiv_id: '2310.14037'
source_url: https://arxiv.org/abs/2310.14037
tags:
- image
- retrieval
- text
- multi-modal
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multi-modAl Retrieval model via Visual modulE
  pLugin (MARVEL), which learns an embedding space for queries and multi-modal documents
  to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified
  encoder model, which helps to alleviate the modality gap between images and texts.
---

# MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin

## Quick Facts
- arXiv ID: 2310.14037
- Source URL: https://arxiv.org/abs/2310.14037
- Authors: 
- Reference count: 40
- Key outcome: MARVEL significantly outperforms state-of-the-art methods on multi-modal retrieval datasets WebQA and ClueWeb22-MM by learning a unified embedding space for queries and multi-modal documents.

## Executive Summary
This paper introduces MARVEL (Multi-modAl Retrieval model via Visual modulE pLugin), a dense retrieval approach that enables multi-modal document retrieval by incorporating visual features into a pre-trained text retrieval model. The method uses a visual module plugin that encodes image features using CLIP's visual encoder and maps them to the input space of a dense retriever (T5-ANCE), allowing the model to jointly understand text and images in a unified embedding space. The approach significantly outperforms existing methods on multi-modal retrieval tasks while maintaining the text retrieval capabilities of the original model.

## Method Summary
MARVEL extends T5-ANCE, a pre-trained dense retrieval model, by incorporating a visual module that encodes image features using CLIP's visual encoder. These encoded features are mapped to the language model's input space through an adaptation layer and concatenated with text embeddings. The unified encoder is trained using modality-balanced hard negative sampling to learn a shared embedding space for both text and image documents. By freezing the pre-trained T5-ANCE parameters, MARVEL maintains its strong text retrieval capabilities while gaining image understanding through the visual module plugin.

## Key Results
- MARVEL significantly outperforms state-of-the-art methods on WebQA and ClueWeb22-MM multi-modal retrieval datasets
- The model achieves superior performance across both text and image retrieval tasks
- Visual module plugin successfully bridges the modality gap between images and texts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The visual module plugin enables the image understanding capability of the pre-trained dense retrieval model.
- Mechanism: By incorporating the visual module's encoded image features as inputs to the dense retriever, the model can jointly model image captions and features, enabling deeper interactions between text and visual inputs.
- Core assumption: The language model's pre-trained attention heads can effectively capture and adapt the image semantics from the visual encoder.
- Evidence anchors:
  - [abstract]: "MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts."
  - [section]: "It can not only enable the visual understanding ability of large language models but also maintain the capability of large language models by freezing their parameters."
  - [corpus]: Weak evidence - the corpus does not directly support this specific mechanism but discusses related multi-modal retrieval approaches.
- Break condition: If the attention heads cannot effectively capture image semantics or the visual encoder produces features incompatible with the language model's input space.

### Mechanism 2
- Claim: The visual module plugin method bridges the modality gap between images and texts.
- Mechanism: By encoding image features and texts using the same encoder model, the model learns a unified representation space for both modalities, reducing the modality gap.
- Core assumption: The visual features encoded by the visual module can be mapped into the input embedding space of the language model.
- Evidence anchors:
  - [abstract]: "MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts."
  - [section]: "The training strategies can teach the language model to assign more appropriate attention weights to image and text features by preventing the visual module from overfitting the training signals."
  - [corpus]: Weak evidence - the corpus discusses multi-modal retrieval but does not specifically address the modality gap bridging mechanism.
- Break condition: If the mapping between visual features and language model input space is not effective, or if the visual module overfits to training signals.

### Mechanism 3
- Claim: The visual module plugin method maintains the text retrieval capabilities of the pre-trained model while adding image understanding.
- Mechanism: By freezing the parameters of the pre-trained dense retriever and only finetuning the visual module and adaptation layers, the model retains its text retrieval knowledge while learning to incorporate image features.
- Core assumption: The pre-trained dense retriever has sufficient text retrieval knowledge that can be leveraged for multi-modal retrieval.
- Evidence anchors:
  - [abstract]: "MARVEL provides an opportunity to broaden the advantages of text retrieval to the multi-modal scenario."
  - [section]: "It can not only enable the visual understanding ability of large language models but also maintain the capability of large language models by freezing their parameters."
  - [corpus]: Weak evidence - the corpus does not directly support this specific mechanism but discusses the use of pre-trained models in multi-modal retrieval.
- Break condition: If the frozen parameters of the pre-trained model interfere with the learning of multi-modal representations, or if the visual module cannot effectively incorporate image features.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To train the dense retrieval model to map queries and relevant documents close together in the embedding space while pushing apart irrelevant pairs.
  - Quick check question: How does the modality-balanced hard negative training method help alleviate the modality preference during retrieval?

- Concept: Cross-modal matching
  - Why needed here: To enable the model to effectively match queries with documents from different modalities (text and image) in the unified embedding space.
  - Quick check question: What is the role of the visual module plugin in facilitating cross-modal matching between queries and multi-modal documents?

- Concept: Modality fusion
  - Why needed here: To combine the retrieval results from different modalities (text and image) into a unified ranking that satisfies user information needs.
  - Quick check question: How does the universal multi-modal dense retrieval approach of MARVEL differ from the divide-and-conquer pipeline in terms of modality fusion?

## Architecture Onboarding

- Component map: CLIP visual encoder -> Adaptation layer -> Dense retriever (T5-ANCE) -> Unified embedding space
- Critical path:
  1. Encode image features using the visual module.
  2. Map the encoded image features to the input space of the dense retriever.
  3. Concatenate the mapped image features with text embeddings as input to the dense retriever.
  4. Train the model using modality-balanced hard negative training to learn a unified embedding space for queries and multi-modal documents.
- Design tradeoffs:
  - Freezing the pre-trained dense retriever parameters vs. finetuning them: Freezing retains text retrieval capabilities but may limit multi-modal learning.
  - Using separate encoders for text and images vs. a unified encoder: Separate encoders may lead to a modality gap, while a unified encoder requires careful design to handle both modalities effectively.
- Failure signatures:
  - Modality preference: The model consistently favors one modality (text or image) over the other in the retrieval results.
  - Modality gap: The model struggles to effectively match queries with documents from different modalities in the unified embedding space.
  - Overfitting: The visual module or adaptation layer overfits to the training data, leading to poor generalization on unseen data.
- First 3 experiments:
  1. Train MARVEL on a small subset of the multi-modal retrieval dataset and evaluate its performance on a held-out validation set.
  2. Compare the retrieval performance of MARVEL with and without the visual module plugin on a sample of multi-modal queries.
  3. Analyze the attention weight distribution of MARVEL on text and image inputs to understand how it balances the two modalities during retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of temperature parameter ùúè in the contrastive loss function impact the retrieval performance of MARVEL across different modalities?
- Basis in paper: [explicit] The paper mentions using a temperature hyperparameter ùúè = 0.01 during training but does not explore its impact on retrieval performance.
- Why unresolved: The paper does not provide an analysis of how varying ùúè affects the model's ability to distinguish between relevant and irrelevant documents across text and image modalities.
- What evidence would resolve it: Conducting experiments with different ùúè values and comparing their effects on MRR@10, NDCG@10, and Recall@100 metrics for both text and image retrieval tasks would clarify the optimal setting for multi-modal retrieval.

### Open Question 2
- Question: To what extent does the quality and diversity of the training data influence the visual understanding capability of MARVEL?
- Basis in paper: [inferred] The paper mentions building ClueWeb22-MM using anchor texts and extracting related documents, but does not discuss the quality or diversity of the training data's impact on the model's performance.
- Why unresolved: The paper does not provide insights into how variations in the quality and diversity of the training data, such as the presence of noisy or irrelevant documents, affect the model's ability to learn effective visual representations.
- What evidence would resolve it: Analyzing the performance of MARVEL on datasets with varying levels of noise and diversity in the training data, and comparing it to models trained on cleaner, more diverse datasets, would shed light on the importance of data quality.

### Open Question 3
- Question: How does the performance of MARVEL compare to other multi-modal retrieval models when applied to datasets outside of WebQA and ClueWeb22-MM?
- Basis in paper: [explicit] The paper evaluates MARVEL on WebQA and ClueWeb22-MM but does not explore its performance on other multi-modal retrieval datasets.
- Why unresolved: The paper does not provide evidence of MARVEL's generalization ability across different domains and types of multi-modal retrieval tasks.
- What evidence would resolve it: Evaluating MARVEL on additional multi-modal retrieval benchmarks, such as Flickr30k or COCO, and comparing its performance to other state-of-the-art models would demonstrate its effectiveness in diverse scenarios.

## Limitations

- The mechanism for how visual features are effectively mapped to the language model's input space remains unclear, with insufficient analysis of compatibility between visual and text embeddings.
- Evaluation is limited to WebQA and ClueWeb22-MM datasets, which may not represent the diversity and complexity of real-world multi-modal retrieval scenarios.
- The paper lacks detailed ablation studies on the visual module plugin component itself to isolate its contribution to performance gains.

## Confidence

- Unified Embedding Space Effectiveness: High
- Visual Module Plugin Mechanism: Medium
- Generalizability of Approach: Low

## Next Checks

1. **Modality Gap Analysis**: Conduct a detailed analysis of the modality gap by comparing the distribution of text-only queries, image-only queries, and multi-modal queries in the embedding space. Use t-SNE or similar visualization techniques to show how effectively the unified encoder bridges the gap between different modalities.

2. **Ablation Study on Visual Module**: Perform an ablation study by removing the visual module plugin and comparing the performance with the full MARVEL model. Additionally, test alternative visual encoders (e.g., ResNet, ViT) to understand the contribution of the visual module to the overall performance.

3. **Cross-Dataset Evaluation**: Evaluate MARVEL on additional multi-modal retrieval datasets beyond WebQA and ClueWeb22-MM, such as Fashion200k or FashionIQ, to assess the model's generalization capabilities across different domains and query types.