---
ver: rpa2
title: 'Overcoming Recency Bias of Normalization Statistics in Continual Learning:
  Balance and Adaptation'
arxiv_id: '2310.08855'
source_url: https://arxiv.org/abs/2310.08855
tags:
- learning
- statistics
- continual
- normalization
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of recency bias in normalization
  layers during continual learning, where normalization statistics (e.g., batch normalization)
  tend to overfit to recent tasks. The authors provide theoretical analysis showing
  the dilemma between balance (equal contribution from all tasks) and adaptation (fitting
  to the current task) of normalization statistics.
---

# Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation

## Quick Facts
- arXiv ID: 2310.08855
- Source URL: https://arxiv.org/abs/2310.08855
- Reference count: 40
- Key outcome: AdaB2N significantly improves continual learning performance by addressing recency bias in normalization statistics, achieving up to 7.68%, 6.86%, and 4.26% improvements on CIFAR-10, CIFAR-100, and Mini-ImageNet respectively.

## Executive Summary
This paper addresses the problem of recency bias in normalization layers during continual learning, where batch normalization statistics tend to overfit to recent tasks. The authors theoretically analyze the dilemma between balance (equal contribution from all tasks) and adaptation (fitting to current task) of normalization statistics. They propose Adaptive Balance of Batch Normalization (AdaB2N), which uses a Bayesian-based strategy to adaptively balance task-wise contributions during training and modifies the EMA momentum to balance statistics during testing. Experiments show AdaB2N significantly outperforms standard batch normalization and other normalization variants, especially in online scenarios.

## Method Summary
AdaB2N modifies batch normalization to address recency bias through two main mechanisms: (1) a Bayesian-based strategy that adaptively balances task-wise contributions using a Dirichlet prior to compute weighted normalized representations, and (2) a modified EMA momentum function that interpolates between exponential and cumulative moving averages to reconcile balance and adaptation. The method is evaluated on CIFAR-10, CIFAR-100, and Mini-ImageNet using ResNet-18 backbone with ER-ACE and DER++ continual learning frameworks, showing substantial improvements over standard BN and other normalization variants.

## Key Results
- AdaB2N achieves up to 7.68% improvement on CIFAR-10, 6.86% on CIFAR-100, and 4.26% on Mini-ImageNet compared to standard BN
- Significant performance gains observed especially in online learning scenarios
- Outperforms other normalization variants (CN, LN, IN, GN) across all tested datasets
- Effectively addresses recency bias while maintaining good adaptation to new tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch normalization statistics in continual learning suffer from recency bias because EMA weights recent tasks exponentially more than past tasks.
- Mechanism: The statistical weight of each task in BN's EMA population statistics decays exponentially, so newer tasks dominate and older task statistics become outdated.
- Core assumption: Batch statistics follow the distribution modeled by the EMA formula and task-wise contributions can be quantified.
- Evidence anchors:
  - [abstract] "Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks..."
  - [section] "Corollary 2 (Adaptation of BN statistics)...a model dealing with evolving task distributions has a strong preference for the distribution of the current task..."
- Break condition: If tasks are sampled uniformly across time or if batch size is extremely large relative to memory buffer, the exponential decay effect may be less pronounced.

### Mechanism 2
- Claim: AdaB2N uses a Bayesian-based strategy to adaptively balance task-wise contributions in normalized representations during training.
- Mechanism: It models the statistics of each batch as following a probability distribution and uses conditional expectation weighted by a Dirichlet prior to compute normalized representations.
- Core assumption: The variability of network parameters θ can be captured through a Bayesian prior over task distributions.
- Evidence anchors:
  - [abstract] "AdaB2N uses a Bayesian-based strategy to adaptively balance task-wise contributions in training..."
  - [section] "E[S|am] = ΣS Σt Pθ(S|t, am)Pθ(t|am)St" and the use of Dirichlet prior for Pϕ(τ|am).
- Break condition: If the Dirichlet prior is poorly calibrated or the concentration parameter ϕ is not properly optimized, the adaptive balance may not work.

### Mechanism 3
- Claim: AdaB2N modifies the EMA momentum function to reconcile balance and adaptation during testing.
- Mechanism: Instead of constant momentum η, it uses ηi := ηi-1 * ηi-1 + (1-η̃)κ with κ controlling the balance between EMA and CMA (cumulative moving average).
- Core assumption: The generalized momentum function can interpolate between EMA (good adaptation) and CMA (good balance).
- Evidence anchors:
  - [abstract] "...a modified momentum in exponential average to balance statistics during testing."
  - [section] "Eq. (16) degenerates to EMA when κ → 1 and to CMA when κ → 0. This allows us to find an inflection point for adaptive balance..."
- Break condition: If κ is set to extreme values (0 or 1) or if the modified momentum function doesn't converge properly.

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: EMA is the core mechanism BN uses to estimate population statistics from batch statistics during testing.
  - Quick check question: What happens to the weight of older tasks in EMA as more batches are processed?
- Concept: Catastrophic Forgetting
  - Why needed here: Continual learning aims to prevent catastrophic forgetting, and normalization statistics contribute to this problem.
  - Quick check question: How does recency bias in normalization statistics relate to catastrophic forgetting of the entire model?
- Concept: Dirichlet Distribution as Conjugate Prior
  - Why needed here: AdaB2N uses a Dirichlet distribution to model the prior over task distributions for Bayesian weighting.
  - Quick check question: Why is the Dirichlet distribution a natural choice for modeling a categorical distribution over tasks?

## Architecture Onboarding

- Component map: ResNet-18 backbone → Normalization layers (with AdaB2N modifications) → Classification head. AdaB2N modifies the normalization layers only.
- Critical path: Forward pass through network with AdaB2N normalization → Loss computation with regularization term Lada → Backward pass updating θ, ψ, and ω parameters.
- Design tradeoffs: AdaB2N adds computational overhead for Bayesian weighting and modified momentum calculation, but provides significant accuracy gains. The hyperparameters λ and κ require tuning.
- Failure signatures: Poor performance on older tasks (over-adaptation), slow convergence (over-balancing), or instability during task transitions.
- First 3 experiments:
  1. Run baseline BN on Split CIFAR-10 Task-IL to establish reference performance.
  2. Implement AdaB2N with default hyperparameters and compare to baseline.
  3. Perform ablation study: remove the Bayesian regularization term and test impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of the adaptive balance strategy in AdaB2N, and how does it perform under extreme task distribution shifts?
- Basis in paper: [inferred] The paper discusses the dilemma between balance and adaptation of BN statistics but does not provide theoretical bounds on the performance of AdaB2N under extreme conditions.
- Why unresolved: The paper lacks an in-depth theoretical analysis of the limits of AdaB2N's adaptive balance strategy, particularly under extreme task distribution shifts.
- What evidence would resolve it: Theoretical proofs or extensive empirical results demonstrating the performance limits of AdaB2N under various extreme task distribution shifts would resolve this question.

### Open Question 2
- Question: How does the choice of hyperparameters (λ and κ) in AdaB2N affect its performance across different continual learning scenarios?
- Basis in paper: [explicit] The paper mentions that hyperparameters λ and κ can influence the performance of AdaB2N, but it does not provide a comprehensive analysis of their effects across different scenarios.
- Why unresolved: The paper does not explore the sensitivity of AdaB2N to hyperparameter choices in various continual learning settings, leaving uncertainty about the robustness of the method.
- What evidence would resolve it: A detailed hyperparameter sensitivity analysis across multiple continual learning scenarios would clarify the impact of λ and κ on AdaB2N's performance.

### Open Question 3
- Question: Can the Bayesian-based strategy used in AdaB2N be extended to other normalization techniques beyond BN, such as Layer Normalization or Group Normalization?
- Basis in paper: [inferred] The paper focuses on improving BN for continual learning but does not discuss the potential application of its Bayesian-based strategy to other normalization methods.
- Why unresolved: The paper does not explore the generalizability of its Bayesian-based strategy to other normalization techniques, which could limit its broader applicability.
- What evidence would resolve it: Experimental results showing the effectiveness of the Bayesian-based strategy when applied to other normalization techniques would demonstrate its generalizability.

## Limitations

- The paper does not provide theoretical bounds on the performance limits of AdaB2N under extreme task distribution shifts.
- Implementation details for the Bayesian-based strategy and modified EMA momentum are underspecified, making faithful reproduction challenging.
- The paper lacks ablation studies isolating the contribution of each mechanism (Bayesian weighting vs. modified momentum) to overall performance.

## Confidence

- **High Confidence:** The fundamental problem of recency bias in normalization statistics during continual learning is well-established and the experimental improvements over baselines are clearly demonstrated.
- **Medium Confidence:** The theoretical analysis of the balance-adaptation dilemma is sound, though the practical effectiveness depends on hyperparameter tuning.
- **Low Confidence:** The Bayesian framework's specific implementation details and the convergence properties of the modified EMA momentum function require further validation.

## Next Checks

1. Implement and test a simplified version of AdaB2N with fixed hyperparameters to verify the core functionality before full integration.
2. Conduct ablation studies to isolate the contribution of the Bayesian weighting mechanism versus the modified EMA momentum to overall performance.
3. Test the robustness of AdaB2N across different continual learning frameworks (not just ER-ACE and DER++) and backbone architectures to assess generalizability.