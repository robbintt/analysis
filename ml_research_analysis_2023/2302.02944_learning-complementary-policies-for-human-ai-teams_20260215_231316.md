---
ver: rpa2
title: Learning Complementary Policies for Human-AI Teams
arxiv_id: '2302.02944'
source_url: https://arxiv.org/abs/2302.02944
tags:
- human
- decision
- data
- 'true'
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates and tackles the novel problem of Learning
  Complementary Policies for Human-AI Teams (LCP-HAI), where the goal is to maximize
  decision rewards by learning both a complementary algorithmic policy and a routing
  model that allocates instances to either a human or AI decision-maker. The core
  method idea involves jointly training an algorithmic policy that complements human
  strengths and a routing model that leverages this complementarity to maximize total
  reward.
---

# Learning Complementary Policies for Human-AI Teams

## Quick Facts
- arXiv ID: 2302.02944
- Source URL: https://arxiv.org/abs/2302.02944
- Authors: 
- Reference count: 26
- Key outcome: Learning Complementary Policies for Human-AI Teams (LCP-HAI) maximizes decision rewards by jointly training an algorithmic policy and routing model that allocates instances to humans or AI.

## Executive Summary
This paper introduces the novel problem of Learning Complementary Policies for Human-AI Teams (LCP-HAI), where the goal is to maximize decision rewards by learning both a complementary algorithmic policy and a routing model that allocates instances to either human or AI decision-makers. The approach involves jointly training an algorithmic policy that complements human strengths and a routing model that leverages this complementarity to maximize total reward. Empirical results using synthetic and real human responses demonstrate that the proposed method significantly outperforms independent human and algorithmic decision-making, achieving up to 25% higher reward than the best single decision-maker.

## Method Summary
The LCP-HAI framework jointly optimizes an algorithmic decision policy and a routing model to maximize expected reward. The approach uses inverse propensity weighting to train the algorithmic policy on observational data from human decisions, while simultaneously learning a routing policy that determines whether each instance should be handled by a human or the algorithm. The joint optimization accounts for the interdependence between routing decisions and policy performance, allowing both models to adapt to each other's strengths and weaknesses. The framework also incorporates personalization to leverage heterogeneous expertise across different human decision-makers.

## Key Results
- LCP-HAI achieves up to 25% higher reward than the best single decision-maker
- Joint optimization of policy and routing models outperforms sequential training
- Routing only a small fraction of instances to humans yields substantial performance improvements
- The approach is robust to misspecifications in human behavior and reward models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of policy and routing models outperforms sequential training by accounting for their interdependence.
- Mechanism: The routing model's decisions affect which instances the policy sees during training, and the policy's performance influences routing decisions. Joint optimization allows both models to adapt to each other's strengths and weaknesses.
- Core assumption: The policy and routing models are interdependent - routing decisions affect policy training data distribution, and policy performance affects routing quality.
- Evidence anchors:
  - [section]: "Both the algorithmic decision policy and router are learned and interdependent on the other policy's performance, joint learning can offer tangible benefits over those that can be produced when this interdependence is not accounted for."
  - [corpus]: Multiple related papers (A2C, Confounding-Robust Policy Improvement) suggest this joint optimization approach is a recognized pattern in human-AI collaboration research.
- Break condition: When either the policy or routing model has sufficient capacity to perfectly solve their respective sub-tasks independently, the benefits of joint optimization diminish.

### Mechanism 2
- Claim: Leveraging expert consistency in deterministic actions improves policy learning by reducing estimation bias.
- Mechanism: When humans consistently choose the same action for certain instances, we can assume these actions are optimal and impute the unobserved counterfactual rewards accordingly, reducing the bias in inverse propensity score estimation.
- Core assumption: Consistent human decisions reflect domain expertise and are likely to be optimal.
- Evidence anchors:
  - [section]: "We propose an extension of our approach that allows us to leverage this simple but useful observation of a common phenomenon in many important contexts...impute the unseen counterfactual of the deterministic actions in the observational data with the sub-optimal reward."
  - [corpus]: The Confounding-Robust Policy Improvement paper directly addresses this exact mechanism.
- Break condition: When expert consistency is driven by factors other than domain expertise (e.g., cognitive biases, rigid procedures), the assumption breaks down.

### Mechanism 3
- Claim: Personalization improves performance by routing instances to the most suitable human expert based on their demonstrated strengths.
- Mechanism: By modeling each human's decision-making policy and routing accordingly, the system can leverage heterogeneous expertise across different subsets of instances.
- Core assumption: Different human decision-makers have varying expertise across different subsets of instances.
- Evidence anchors:
  - [section]: "We propose a personalized approach that accounts for the varying expertise of different humans so as to further improve the human-AI team's performance."
  - [corpus]: Multiple related papers (Behavioral Software Engineering, Mental Models in Human-AI Collaboration) emphasize the importance of recognizing and leveraging individual differences in human-AI teams.
- Break condition: When human experts have similar capabilities across all instances, personalization provides minimal benefit.

## Foundational Learning

- Inverse Propensity Weighting
  - Why needed here: To correct for the bias introduced by the non-random assignment of actions in observational data.
  - Quick check question: If humans always choose action A for instance X, how does inverse propensity weighting help estimate the reward for action B?

- Policy Learning from Observational Data
  - Why needed here: The core task is to learn a decision policy from historical human decisions and their observed rewards.
  - Quick check question: What are the three key assumptions (ignorability, overlap, consistency) needed for policy learning from observational data?

- Out-of-Distribution Detection
  - Why needed here: To identify instances that differ significantly from the training distribution and route them to humans.
  - Quick check question: How would you modify a one-class SVM to detect OOD instances in a high-dimensional feature space?

## Architecture Onboarding

- Component map: Router (dφ) → Policy (πθ) → Human Expert(s) / Algorithm
- Critical path: New instance → Router evaluation → Human/Algorithm decision → Reward observation → Update models
- Design tradeoffs:
  - Joint vs sequential training: Joint training captures interdependence but is more complex
  - Personalization: Better utilization of expertise but requires modeling individual human policies
  - Expert consistency: Reduces bias but assumes optimal deterministic behavior
- Failure signatures:
  - Router always chooses same entity: Possible overfitting or lack of complementarity
  - Policy performance degrades with more data: Possible violation of overlap assumption
  - OOD detection fails: Distribution shift too subtle or OOD module insufficiently trained
- First 3 experiments:
  1. Train joint model vs sequential baseline on synthetic data with known complementarity
  2. Test expert consistency assumption by comparing deterministic vs non-deterministic action performance
  3. Evaluate personalization benefit by comparing heterogeneous vs homogeneous human expertise scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LCP-HAI scale with the complexity of the action space (number of possible actions) in decision-making tasks?
- Basis in paper: [inferred] The paper discusses the LCP-HAI framework for decision-making tasks involving discrete actions but does not explore how the number of actions affects performance.
- Why unresolved: The experiments focus on datasets with a limited number of actions (e.g., 2, 6, or 10), and the theoretical analysis does not address scalability to larger action spaces.
- What evidence would resolve it: Empirical results showing LCP-HAI performance on datasets with a wide range of action space sizes, and theoretical bounds on performance as a function of the number of actions.

### Open Question 2
- Question: How does the LCP-HAI framework perform when the human decision-makers have access to additional information or context not available to the algorithm?
- Basis in paper: [inferred] The paper assumes that the algorithm has access to the same features as the human decision-makers, but in practice, humans may have access to additional contextual information.
- Why unresolved: The experimental setup does not account for scenarios where humans have access to privileged information, and the theoretical analysis does not address this case.
- What evidence would resolve it: Experiments comparing LCP-HAI performance when humans have access to additional information, and theoretical analysis of how this affects the complementarity between human and AI decision-making.

### Open Question 3
- Question: How does the LCP-HAI framework handle cases where the human decision-makers' preferences or expertise change over time?
- Basis in paper: [inferred] The paper assumes that the human decision-makers' preferences and expertise remain constant, but in practice, these may evolve over time.
- Why unresolved: The experimental setup and theoretical analysis do not address the case of changing human preferences or expertise, and the proposed methods do not explicitly account for this.
- What evidence would resolve it: Experiments evaluating LCP-HAI performance when human preferences or expertise change over time, and theoretical analysis of how to adapt the framework to handle such changes.

## Limitations
- Validation on fully real-world datasets with authentic human decision patterns remains limited
- Performance gains are heavily dependent on the assumed complementarity between human and algorithmic decision-making
- Robustness claims to misspecified reward models require further empirical validation with diverse misspecification types

## Confidence

**High Confidence**: The mathematical formulation of joint optimization for policy and routing models, and the theoretical benefits of leveraging expert consistency in deterministic actions.

**Medium Confidence**: The empirical demonstration of performance improvements in semi-synthetic settings, as these rely on simulated human behavior models.

**Low Confidence**: The generalizability of results to diverse real-world scenarios with complex human-AI interaction dynamics not captured in the experimental setup.

## Next Checks
1. **Real-World Validation**: Apply the LCP-HAI framework to a domain with established human decision-making datasets (e.g., medical diagnosis or loan approval) to validate performance claims beyond synthetic environments.
2. **Robustness Testing**: Systematically vary reward model specifications and human behavior models to quantify the actual robustness of the approach under different types of misspecification.
3. **Scalability Assessment**: Evaluate the framework's performance and computational efficiency as the number of human decision-makers increases, particularly for personalized routing scenarios.