---
ver: rpa2
title: Semantic and Expressive Variation in Image Captions Across Languages
arxiv_id: '2310.14356'
source_url: https://arxiv.org/abs/2310.14356
tags:
- language
- captions
- languages
- image
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing assumption in computer vision
  that perception is objective and that images should be described similarly regardless
  of language. By comparing captions generated in 7 languages for the same images,
  it shows that multilingual descriptions have higher semantic coverage, with 29.9%
  more objects, 24.5% more relations, and 46.0% more attributes on average.
---

# Semantic and Expressive Variation in Image Captions Across Languages

## Quick Facts
- arXiv ID: 2310.14356
- Source URL: https://arxiv.org/abs/2310.14356
- Reference count: 40
- One-line primary result: Multilingual image captions capture significantly more semantic content (29.9% more objects, 24.5% more relations, 46.0% more attributes) than monolingual captions, with models inheriting this bias.

## Executive Summary
This paper challenges the prevailing assumption in computer vision that perception is objective and that images should be described similarly regardless of language. By comparing captions generated in 7 languages for the same images, the authors demonstrate that multilingual descriptions have substantially higher semantic coverage. Models like LLaVA and commercial APIs (e.g., Google Vertex) inherit this bias, describing different parts of images based on the language used. Finetuning models on multilingual data leads to consistently strong performance across languages, whereas monolingual finetuning excels only in its respective language. These findings highlight the importance of embracing linguistic and cultural diversity to improve image understanding.

## Method Summary
The study analyzes multilingual captions across three data sources: Crossmodal (3.6k images, 36 languages), Visual Genome (2k images), and human evaluation (30 VG images from US/Japan). The authors compare multilingual versus monolingual semantic coverage using scene graphs, embeddings, and linguistic metrics. They finetune GIT on translated captions from the Crossmodal dataset and evaluate performance across 64 language splits using SPICE F1. Scene graphs are parsed using FLAN-T5, and linguistic features are analyzed to quantify differences across languages.

## Key Results
- Multilingual descriptions contain 29.9% more objects, 24.5% more relations, and 46.0% more attributes on average compared to monolingual descriptions
- Models fine-tuned on multilingual data perform consistently well across all evaluation data compositions
- Commercial APIs like Google Vertex produce more comprehensive captions across languages compared to models trained on English-only data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual image descriptions capture more semantic concepts than monolingual ones
- Mechanism: Different languages lead speakers to attend to and mention different aspects of the same visual scene due to linguistic and cultural differences
- Core assumption: Cultural background and language shape what people notice and describe in images
- Evidence anchors: [abstract] "multilingual descriptions have higher semantic coverage, with 29.9% more objects, 24.5% more relations, and 46.0% more attributes on average"

### Mechanism 2
- Claim: Vision models trained on multilingual data generalize better across languages
- Mechanism: Finetuning on multilingual captions leads to more consistent performance across different language test sets compared to monolingual finetuning
- Core assumption: Models learn language-specific patterns from monolingual data, which limits their generalization to other languages
- Evidence anchors: [abstract] "finestuning models on multilingual data leads to consistently strong performance across languages, whereas monolingual finetuning excels only in its respective language"

### Mechanism 3
- Claim: Commercial APIs like Google Vertex inherit the multilingual coverage advantage
- Mechanism: APIs trained on diverse data produce more comprehensive captions across languages compared to models trained on English-only data
- Core assumption: Commercial APIs are trained on more diverse data sources, including multilingual data
- Evidence anchors: [abstract] "Models like LLaVA and commercial APIs (e.g., Google Vertex) inherit this bias, describing different parts of images based on the language used"

## Foundational Learning

- Concept: Cross-cultural psychology
  - Why needed here: The paper's core claim about linguistic and cultural differences influencing perception relies on findings from this field
  - Quick check question: Can you name one key finding from cross-cultural psychology that supports the paper's claims about perception differences across cultures?

- Concept: Linguistic relativity
  - Why needed here: The paper uses language as a proxy for culture and argues that different languages lead to different descriptions of the same image
  - Quick check question: What is the main idea of linguistic relativity, and how does it relate to the paper's claims about multilingual descriptions?

- Concept: Scene graph representation
  - Why needed here: The paper uses scene graphs to measure semantic coverage of image descriptions across languages
  - Quick check question: How does a scene graph represent the semantic content of an image, and why is it useful for comparing descriptions across languages?

## Architecture Onboarding

- Component map: Crossmodal/Visual Genome datasets -> Caption generation (human/model/API) -> Scene graph parsing (FLAN-T5) -> Semantic coverage analysis -> Model finetuning (GIT) -> Evaluation (SPICE F1)
- Critical path: Collect multilingual captions → Analyze semantic coverage → Finetune models on different caption sets → Evaluate model performance
- Design tradeoffs:
  - Using translation vs. original captions: Translation allows using the same tools but may lose nuances
  - Number of languages: 7 languages provide good coverage but limit generalizability
  - Evaluation metrics: Scene graphs are comprehensive but may miss subtleties
- Failure signatures:
  - Low coverage increase from multilingual data: Suggests cultural differences don't significantly impact perception
  - No performance difference from multilingual finetuning: Suggests models learn language-agnostic representations
  - Inconsistent API outputs: Suggests commercial APIs don't actually use diverse data
- First 3 experiments:
  1. Replicate the scene graph analysis with a different dataset to validate the coverage findings
  2. Finetune a model on synthetic multilingual captions (e.g., translations) and compare to organic multilingual data
  3. Analyze the linguistic features (e.g., concreteness, analytic thinking) of the captions to identify specific differences across languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do linguistic and cultural factors specifically influence the perception and description of objects in images?
- Basis in paper: [explicit] The paper mentions that Japanese speakers tend to mention background objects, whereas American speakers tend to mention foreground ones, based on cross-cultural psychology findings
- Why unresolved: The paper does not provide a detailed analysis of how specific linguistic and cultural factors contribute to these differences in perception
- What evidence would resolve it: Conducting a study where participants from different cultures describe images, followed by a detailed analysis of the linguistic and cultural factors influencing their descriptions

### Open Question 2
- Question: To what extent do multilingual datasets improve the performance of computer vision models compared to monolingual datasets?
- Basis in paper: [explicit] The paper shows that models finetuned on multilingual data perform consistently well across all test data compositions, whereas models finetuned on monolingual data perform best on corresponding test data from that language
- Why unresolved: The paper does not quantify the extent of performance improvement or provide a comprehensive comparison across different types of computer vision tasks
- What evidence would resolve it: Conducting experiments to compare the performance of computer vision models trained on multilingual datasets versus monolingual datasets across various tasks and metrics

### Open Question 3
- Question: How can the findings on linguistic and cultural diversity in image perception be applied to improve the inclusivity and fairness of AI models?
- Basis in paper: [explicit] The paper discusses the implications of their findings for machine vision beyond image captioning, such as open-vocabulary segmentation and image generation, and mentions the potential for improving information coverage in visual scenes
- Why unresolved: The paper does not provide specific strategies or guidelines for applying these findings to enhance the inclusivity and fairness of AI models
- What evidence would resolve it: Developing and evaluating strategies for incorporating linguistic and cultural diversity into AI model training and deployment, followed by assessing their impact on model performance and fairness metrics

## Limitations
- Dataset Scope: The study focuses on 7 languages and 3.6k images, which may not fully capture the diversity of global linguistic and cultural perspectives
- Measurement Validity: Scene graphs may miss nuanced differences in how languages describe visual scenes
- Causal Claims: The paper cannot definitively prove that cultural differences directly cause the observed semantic coverage variations

## Confidence
- High Confidence: The observation that multilingual descriptions contain more objects, relations, and attributes than monolingual ones
- Medium Confidence: The claim that commercial APIs like Google Vertex produce more comprehensive captions across languages
- Low Confidence: The assertion that cultural differences directly cause the observed semantic coverage variations

## Next Checks
1. Cross-dataset replication: Validate the multilingual coverage findings on a different image-caption dataset (e.g., Conceptual Captions or Flickr30k)
2. Controlled cultural experiment: Conduct a human study where participants from different cultural backgrounds describe the same images, controlling for caption generation method
3. Ablation on finetuning data: Perform controlled finetuning experiments varying the proportion of multilingual vs. monolingual data