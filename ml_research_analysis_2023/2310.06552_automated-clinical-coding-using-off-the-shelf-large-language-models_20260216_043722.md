---
ver: rpa2
title: Automated clinical coding using off-the-shelf large language models
arxiv_id: '2310.06552'
source_url: https://arxiv.org/abs/2310.06552
tags:
- codes
- coding
- code
- tree
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for automated ICD coding using
  off-the-shelf large language models (LLMs) without task-specific training. The method
  reframes the coding task as information retrieval, prompting the LLM to identify
  relevant mentions of ICD codes within clinical case notes.
---

# Automated clinical coding using off-the-shelf large language models

## Quick Facts
- arXiv ID: 2310.06552
- Source URL: https://arxiv.org/abs/2310.06552
- Reference count: 39
- Primary result: State-of-the-art performance on rare codes with macro-F1 of 0.225 using zero-shot learning

## Executive Summary
This paper presents a novel approach for automated ICD coding using off-the-shelf large language models without task-specific training. The method reframes ICD coding as an information retrieval task, using a tree-search algorithm to efficiently navigate the ICD ontology hierarchy. A meta-refinement stage using GPT-4 is introduced to filter false positives and improve prediction precision. The approach achieves state-of-the-art performance on rare codes, demonstrating the potential of zero-shot learning for automated ICD coding.

## Method Summary
The approach reframes ICD coding as an information retrieval task, using a tree-search algorithm to efficiently navigate the ICD ontology hierarchy. Starting at the root of the ICD tree, the LLM selects relevant branches at each decision point based on code descriptions. This process is performed recursively until candidate paths are exhausted. A meta-refinement stage using GPT-4 is then applied to filter false positives. The method is evaluated on the CodiEsp dataset using micro and macro precision, recall, and F1 scores.

## Key Results
- Achieved state-of-the-art macro-F1 of 0.225 on rare codes
- GPT-4 outperformed other LLMs (Llama-2, GPT-3.5) across all metrics
- Tree-search method efficiently navigated ICD hierarchy without iterating over all 96,000 codes
- Meta-refinement stage using GPT-4 improved prediction precision by filtering false positives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-search method improves performance on rare codes by efficiently narrowing the search space using ICD ontology hierarchy
- Mechanism: The LLM guides a depth-first search through the ICD tree, selecting relevant branches at each decision point based on code descriptions
- Core assumption: The ICD ontology hierarchy meaningfully groups related codes, and the LLM can accurately assess relevance at each branching point
- Evidence anchors:
  - [abstract] "For efficiency, rather than iterating over all codes, we leverage the hierarchical nature of the ICD ontology to sparsely search for relevant codes."
  - [section 4] "Starting at the root of the tree, the model selects relevant chapters to explore... This is performed recursively until the set of candidate paths to explore is exhausted."

### Mechanism 2
- Claim: Meta-refinement stage using GPT-4 improves prediction precision by filtering false positives
- Mechanism: GPT-4 is prompted to identify and remove "incorrect ICD-10 codes" from the set of predictions returned by the tree-search stage
- Core assumption: GPT-4 has sufficient knowledge of ICD codes and their descriptions to accurately distinguish correct from incorrect predictions
- Evidence anchors:
  - [abstract] "Additionally, a meta-refinement stage using GPT-4 is introduced to improve prediction precision by filtering out false positives."
  - [section 4] "We prompt GPT-4... to remove obvious false positives, instructing it to identify 'incorrect ICD-10 codes' which are then discarded."

### Mechanism 3
- Claim: Zero-shot learning approach works because pre-trained LLMs have sufficient medical knowledge and can be prompted effectively
- Mechanism: The LLMs are prompted to retrieve mentions of ICD codes from clinical case notes, leveraging their pre-trained knowledge of medical terminology and concepts
- Core assumption: The LLMs have been exposed to sufficient medical text during pre-training to understand ICD codes and their relationships to clinical concepts
- Evidence anchors:
  - [abstract] "Recent models have proven powerful for medical tasks such as question answering, summarisation and clinical information retrieval."
  - [section 4] "We hypothesised that LLMs might be capable ICD coders 'out-of-the-box'."

## Foundational Learning

- Concept: ICD coding and ontology
  - Why needed here: Understanding the structure and purpose of ICD codes is essential for developing and evaluating automated coding methods
  - Quick check question: What is the difference between assignable "leaf" codes and higher-level parent codes in the ICD ontology?

- Concept: Prompt engineering for LLMs
  - Why needed here: Effective prompting is crucial for eliciting the desired behavior from LLMs in zero-shot learning scenarios
  - Quick check question: How does the prompt used in this paper frame ICD coding as an information retrieval task?

- Concept: Tree search algorithms
  - Why needed here: The tree-search method relies on efficiently navigating the ICD ontology using a depth-first search guided by the LLM
  - Quick check question: How does the tree-search algorithm in this paper differ from a brute-force search over all ICD codes?

## Architecture Onboarding

- Component map:
  - ICD ontology tree -> LLM (Llama-2, GPT-3.5, or GPT-4) -> Tree-search algorithm -> Meta-refinement stage (GPT-4) -> CodiEsp dataset

- Critical path:
  1. Load ICD ontology tree
  2. For each case note, perform tree-search using LLM to identify relevant codes
  3. Apply meta-refinement stage to filter false positives
  4. Evaluate predictions against ground truth labels

- Design tradeoffs:
  - Zero-shot learning vs. supervised learning: Zero-shot learning avoids the need for labeled training data but may sacrifice some performance compared to supervised methods
  - Tree-search efficiency vs. completeness: The tree-search method is more efficient than brute-force search but may miss some relevant codes if the LLM cannot accurately assess relevance at branching points
  - Meta-refinement precision vs. recall: The meta-refinement stage improves precision by filtering false positives but may reduce recall by incorrectly removing true positives

- Failure signatures:
  - Low recall on rare codes: Indicates the tree-search method may be missing relevant codes
  - Low precision after meta-refinement: Suggests the meta-refinement stage may be incorrectly removing true positives
  - Inconsistent performance across different LLMs: May indicate sensitivity to the choice of LLM or prompt engineering

- First 3 experiments:
  1. Evaluate the tree-search method on a small subset of the CodiEsp dataset to ensure it is working as expected
  2. Compare the performance of different LLMs (Llama-2, GPT-3.5, GPT-4) on the tree-search task
  3. Assess the impact of the meta-refinement stage on precision and recall by comparing performance with and without this step

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt template for tree-search method with different LLMs?
- Basis in paper: [explicit] The paper shows different prompts for GPT and Llama-2, noting Llama-2's poor adherence to desired output format
- Why unresolved: The paper only presents two specific prompt templates without systematic comparison or ablation studies to determine the optimal format
- What evidence would resolve it: Systematic comparison of different prompt templates across multiple LLMs, measuring performance impact on coding accuracy

### Open Question 2
- Question: How does the tree-search method scale to the full ICD-10-CM ontology (96,000 codes)?
- Basis in paper: [inferred] The method is evaluated on CodiEsp dataset with 1767 codes, but ICD-10-CM contains 96,000 codes
- Why unresolved: The paper doesn't test performance on larger ontologies or analyze computational complexity at scale
- What evidence would resolve it: Testing the method on full ICD-10-CM dataset with analysis of search efficiency and accuracy degradation

### Open Question 3
- Question: What is the impact of prompt temperature on tree-search performance?
- Basis in paper: [explicit] The paper mentions using temperature 0 for GPT and 0.001 for Llama-2 due to server errors
- Why unresolved: The paper doesn't explore how different temperature settings affect coding accuracy or search behavior
- What evidence would resolve it: Systematic experiments varying temperature settings and measuring their impact on F1 scores and search path diversity

### Open Question 4
- Question: Can the meta-refinement stage be automated or improved beyond GPT-4 filtering?
- Basis in paper: [explicit] The paper introduces a meta-refinement stage using GPT-4 to filter false positives, but notes it substantially reduces macro recall
- Why unresolved: The paper doesn't explore alternative refinement strategies or analyze which false positives are most problematic
- What evidence would resolve it: Comparison of different refinement methods (rule-based, ensemble, alternative LLMs) and analysis of false positive types

## Limitations

- Performance heavily dependent on medical knowledge embedded in pre-trained LLMs
- Tree-search algorithm may miss relevant codes if LLM incorrectly assesses relevance at branching points
- Meta-refinement stage may introduce false negatives by incorrectly filtering true positive predictions

## Confidence

- High Confidence: The tree-search algorithm effectively navigates the ICD ontology hierarchy; the meta-refinement stage improves precision; zero-shot learning is viable for ICD coding tasks
- Medium Confidence: Performance on rare codes is state-of-the-art; GPT-4 provides superior performance compared to other LLMs; translation quality doesn't significantly impact results
- Low Confidence: Generalizability to other medical coding systems; scalability to larger datasets; performance consistency across different medical domains

## Next Checks

1. Conduct a detailed error analysis on false negatives to identify systematic failure patterns in the tree-search algorithm and potential improvements

2. Test the approach on a different medical coding dataset (e.g., MIMIC-III) to evaluate generalizability and identify domain-specific limitations

3. Perform an ablation study comparing precision-recall tradeoffs with and without meta-refinement across different code frequencies to better understand the impact on different code categories