---
ver: rpa2
title: 'Deep-Dispatch: A Deep Reinforcement Learning-Based Vehicle Dispatch Algorithm
  for Advanced Air Mobility'
arxiv_id: '2312.10809'
source_url: https://arxiv.org/abs/2312.10809
tags:
- evtol
- time
- dispatch
- action
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently dispatching electric
  vertical takeoff and landing (eVTOL) aircraft for passenger transportation in Advanced
  Air Mobility (AAM) operations. The unique constraints of eVTOL operations, such
  as frequent recharging needs, limited takeoff and landing pads at vertiports, and
  time-varying demand and electricity prices, make this a particularly challenging
  problem to solve.
---

# Deep-Dispatch: A Deep Reinforcement Learning-Based Vehicle Dispatch Algorithm for Advanced Air Mobility

## Quick Facts
- arXiv ID: 2312.10809
- Source URL: https://arxiv.org/abs/2312.10809
- Authors: 
- Reference count: 40
- Primary Result: Multi-agent DRL algorithm outperforms single-agent approach with optimality gaps ranging from 0.23% to 4.05% and average of 1.75% across all numerical cases.

## Executive Summary
This paper addresses the challenge of efficiently dispatching electric vertical takeoff and landing (eVTOL) aircraft for passenger transportation in Advanced Air Mobility (AAM) operations. The authors develop two deep reinforcement learning (DRL)-based eVTOL dispatch algorithms - single-agent and multi-agent deep Q-learning approaches - to maximize operating profit while handling unique constraints like frequent recharging needs, limited takeoff and landing pads, and time-varying demand. A simulation environment is built to assess performance across 36 numerical cases with varying numbers of eVTOLs, vertiports, and demand levels. Results show that the multi-agent DRL algorithm can closely approximate optimal dispatch policy with significantly less computational expense compared to the benchmark optimization model.

## Method Summary
The study develops a simulation environment using real-world data from the San Francisco Bay Area, including 5 airports as vertiports, commuter data from LODES 2019, and Archer Aviation eVTOL specifications. Two deep reinforcement learning approaches are implemented: single-agent DQL and multi-agent DQL, trained using Keras-RL and Ray libraries respectively. The algorithms are tested across 36 numerical cases with varying demand blocks (high, medium, low), number of eVTOLs (5-20), and vertiports (3-5). Performance is benchmarked against an optimization model using Gurobi solver to evaluate operating profits, optimality gaps, and training times.

## Key Results
- Multi-agent DRL algorithm outperforms single-agent counterpart with optimality gaps ranging from 0.23% to 4.05% (average 1.75%)
- Training times for multi-agent DQL algorithm were under one hour on a standard computer, while optimization model failed to converge after one day
- The multi-agent approach scales efficiently with increasing numbers of eVTOLs and vertiports due to smaller individual agent action spaces
- Reward function effectively captures trade-offs between revenue generation and operational costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-agent DRL approach scales efficiently compared to the single-agent DQL approach as the number of eVTOLs and vertiports increases.
- Mechanism: In the multi-agent formulation, each eVTOL is an independent agent with its own state space and action space, avoiding the exponential growth in action space complexity that occurs in the single-agent approach where one agent controls all eVTOLs.
- Core assumption: The state and action spaces for each individual eVTOL agent remain tractable even as the total number of eVTOLs increases.
- Evidence anchors:
  - [abstract] "The multi-agent algorithm was found to outperform the single-agent counterpart with respect to both profits generated and training time."
  - [section] "the action space of each agent in the MMDP is significantly smaller as each DRL agent controls only one eVTOL in the MMDP."
- Break condition: If the interactions between agents become too complex or if the state space for each agent becomes too large to manage effectively, the multi-agent approach may lose its scalability advantage.

### Mechanism 2
- Claim: The DRL-based algorithms can closely approximate the optimal dispatch policy with significantly less computational expense compared to the optimization model.
- Mechanism: The DRL algorithms learn optimal policies through interaction with the environment, which is more computationally efficient than solving the optimization model, especially for larger problem instances.
- Core assumption: The DRL algorithms can effectively explore the state-action space and converge to near-optimal policies within a reasonable number of training episodes.
- Evidence anchors:
  - [abstract] "the multi-agent eVTOL dispatch algorithm can closely approximate the optimal dispatch policy with significantly less computational expenses compared to the benchmark optimization model."
  - [section] "the training times of the multi-agent DQL algorithm was found to be under one hour in a standard computer with four cores. As the optimization model was not converging after one day of run time in an equivalent computer..."
- Break condition: If the DRL algorithms fail to explore the state-action space adequately or if the training process is not sufficiently long, the learned policies may not be close to optimal.

### Mechanism 3
- Claim: The reward function design effectively captures the trade-offs between revenue generation and operational costs, guiding the agents towards profitable dispatch decisions.
- Mechanism: The reward function combines the revenue from passenger transportation (proportional to the number of passengers, trip fare, and route length) and the costs (operating cost and recharging cost), providing a scalar signal that reflects the profitability of each action.
- Core assumption: The reward function accurately represents the economic objectives of the AAM air taxi operator and the operational constraints of the eVTOL fleet.
- Evidence anchors:
  - [section] "The profit obtained, and, hence, the reward received from a transport action for eVTOLð‘– in time stepð‘¡ depends on the revenue generated from passenger transportation and flight operating cost."
  - [section] "The recharging cost, and, hence, the reward associated with recharge action for a given eVTOLð‘– in a given time step is determined by the amount of electric energy the eVTOL receives from the power grid..."
- Break condition: If the reward function does not accurately capture the trade-offs or if the scaling factors are not properly tuned, the agents may learn suboptimal policies that do not maximize profit.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The eVTOL dispatch problem is formulated as an MDP, where the agents interact with the environment over time to maximize cumulative reward.
  - Quick check question: What are the key components of an MDP, and how do they relate to the eVTOL dispatch problem?

- Concept: Deep Q-Learning (DQL)
  - Why needed here: DQL combines reinforcement learning with deep neural networks to handle the large state and action spaces in the eVTOL dispatch problem.
  - Quick check question: How does DQL differ from traditional Q-learning, and why is it more suitable for the eVTOL dispatch problem?

- Concept: Multi-agent Reinforcement Learning
  - Why needed here: The multi-agent approach allows each eVTOL to be controlled by an independent agent, improving scalability compared to the single-agent approach.
  - Quick check question: What are the key differences between single-agent and multi-agent reinforcement learning, and how do they apply to the eVTOL dispatch problem?

## Architecture Onboarding

- Component map:
  - Simulation Environment -> DRL Agents -> Reward Function -> Q-Network -> Environment State Update
  - Real-world data (demand, prices) -> Environment State -> Agent Observation -> Action Selection -> State Transition -> Reward Calculation

- Critical path:
  1. Initialize the simulation environment and DRL agents
  2. For each time step in an episode:
     a. Agents observe the current state of the environment
     b. Agents select actions based on their policy (e.g., epsilon-greedy)
     c. Environment updates the state and calculates rewards based on the agents' actions
     d. Agents store the transition (state, action, reward, next state) in the replay memory
     e. Periodically, the agents sample a batch of transitions and update the Q-network weights using gradient descent
  3. Repeat until the agents converge to an optimal policy

- Design tradeoffs:
  - Single-agent vs. Multi-agent: The single-agent approach is simpler but may not scale well with the number of eVTOLs, while the multi-agent approach is more complex but more scalable
  - Exploration vs. Exploitation: The agents need to balance between exploring new actions and exploiting the current best-known actions to learn an optimal policy
  - State Space Complexity: The state space should be rich enough to capture the relevant information for decision-making but not so complex that it becomes intractable for the DRL algorithms

- Failure signatures:
  - Poor Convergence: If the agents fail to converge to an optimal policy within a reasonable number of training episodes, it may indicate issues with the exploration strategy, reward function, or neural network architecture
  - Suboptimal Performance: If the learned policy performs significantly worse than the optimization model, it may indicate that the DRL algorithms have not adequately explored the state-action space or that the reward function is not properly tuned
  - Instability: If the training process is unstable (e.g., the rewards fluctuate wildly or the Q-network weights diverge), it may indicate issues with the learning rate, replay memory size, or target network update frequency

- First 3 experiments:
  1. Run the single-agent DQL algorithm on a small problem instance (e.g., 5 eVTOLs, 3 vertiports) and compare the performance to the optimization model
  2. Run the multi-agent DQL algorithm on the same small problem instance and compare the performance to the single-agent approach and the optimization model
  3. Gradually increase the problem size (e.g., number of eVTOLs and vertiports) and observe how the performance and training times of the single-agent and multi-agent approaches scale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the deep reinforcement learning algorithms perform under dynamic trip fare conditions and stochastic demand patterns?
- Basis in paper: [explicit] The authors mention incorporating dynamic trip fare and stochastic demand as part of future work.
- Why unresolved: The current study uses fixed trip fare and deterministic demand patterns based on commuter data.
- What evidence would resolve it: Comparative analysis of DRL algorithm performance under dynamic vs. fixed fare conditions and stochastic vs. deterministic demand scenarios.

### Open Question 2
- Question: How would the performance of the deep reinforcement learning algorithms change if the eVTOL fleet is heterogeneous, with varying passenger capacities and battery capacities?
- Basis in paper: [explicit] The authors suggest extending the algorithm to consider a heterogeneous eVTOL fleet as part of future work.
- Why unresolved: The current study assumes all eVTOLs have identical specifications (passenger capacity and battery capacity).
- What evidence would resolve it: Performance comparison of DRL algorithms using homogeneous vs. heterogeneous eVTOL fleets under identical operational conditions.

### Open Question 3
- Question: How would the deep reinforcement learning algorithms perform in a competitive environment with multiple AAM air taxi operators in the same service area?
- Basis in paper: [explicit] The authors mention considering multiple operators and developing algorithms for competitive and cooperative agents as part of future work.
- Why unresolved: The current study assumes a single AAM air taxi operator servicing the entire region.
- What evidence would resolve it: Comparative analysis of DRL algorithm performance in single-operator vs. multi-operator competitive environments with similar operational constraints.

## Limitations

- The optimization model failed to converge within reasonable timeframes for larger problem instances, limiting the ability to establish true optimality gaps
- The study uses a specific eVTOL model (Archer Aviation) and geographical region, which may not generalize to other scenarios
- The demand estimation relies on 2019 commuter data, which may not reflect current or future demand patterns

## Confidence

- High Confidence: The scalability advantage of the multi-agent approach over single-agent DQL is well-supported by the experimental results, showing consistent performance improvements across all 36 numerical cases
- Medium Confidence: The claim that DRL algorithms can closely approximate optimal dispatch policies is supported, but the benchmark optimization model's convergence issues limit the ability to definitively establish optimality gaps
- Low Confidence: The generalizability of results to larger problem instances or different geographical regions is uncertain, as the study is limited to the San Francisco Bay Area with specific eVTOL specifications

## Next Checks

1. **Benchmark against exact methods**: Implement a smaller-scale version of the problem where the optimization model can converge, to establish ground truth optimality gaps for DRL algorithms
2. **Ablation study on reward function**: Systematically vary the reward function components to understand their impact on learned policies and identify potential sensitivities
3. **Cross-validation across regions**: Test the DRL algorithms on demand data from different geographical regions to assess generalizability and identify potential regional variations in performance