---
ver: rpa2
title: 'Unlocking the Potential of Similarity Matching: Scalability, Supervision and
  Pre-training'
arxiv_id: '2308.02427'
source_url: https://arxiv.org/abs/2308.02427
tags:
- learning
- neural
- similarity
- training
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores similarity matching (SM) as a biologically
  plausible alternative to backpropagation (BP) for neural network training. The authors
  address the limitations of BP in terms of biological plausibility, computational
  cost, and online learning suitability.
---

# Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training

## Quick Facts
- **arXiv ID:** 2308.02427
- **Source URL:** https://arxiv.org/abs/2308.02427
- **Reference count:** 18
- **Primary result:** PyTorch-based Convolutional Nonnegative Similarity Matching achieves comparable accuracy to conventional implementations while being significantly faster, with supervised variants improving performance over other local learning approaches on CIFAR-10.

## Executive Summary
This paper presents similarity matching (SM) as a biologically plausible alternative to backpropagation for neural network training. The authors address key limitations of backpropagation—biological plausibility, computational cost, and online learning suitability—by developing a localized learning framework based on similarity matching objectives. They demonstrate three major contributions: a scalable PyTorch implementation using tensor networks, a supervised variant enabling greedy layer stacking, and pre-training experiments showing stable feature learning. The framework achieves competitive classification accuracy while offering advantages in biological plausibility and computational efficiency.

## Method Summary
The authors develop three main technical contributions to advance similarity matching frameworks. First, they implement Convolutional Nonnegative SM using PyTorch with GPU acceleration through tensor network methods, enabling efficient scaling to large datasets. Second, they propose Supervised Similarity Matching (S2M), a localized objective similar to canonical correlation analysis that incorporates label information while maintaining local update rules for greedy layer stacking. Third, they conduct pre-training experiments using SM-based architectures like LeNet, comparing learned features against backpropagation-trained models. The approach uses local Hebbian-like update rules instead of global gradients, with closed-form solutions for parameter updates and convergence dynamics.

## Key Results
- Single-layer NSM achieves comparable accuracy to previous implementations while being significantly faster using PyTorch implementation
- S2M model demonstrates improved performance over other local learning approaches on CIFAR-10 with optimal αk hyperparameter settings
- Pre-training experiments reveal that filters learned through SM exhibit stability during fine-tuning with BP, with some filters showing minimal rotation

## Why This Works (Mechanism)

### Mechanism 1
The similarity matching framework offers a biologically plausible alternative to backpropagation by using local Hebbian-like update rules. NSM minimizes a similarity matching objective using local dynamics (Eq. 3) and closed-form updates (Eq. 4), avoiding the need for global gradient propagation. The system can converge to meaningful representations using only local computations and temporally local information. Evidence includes the paper's claim that NSM "offers online, localized, and biologically plausible algorithms" and the presentation of closed-form update rules. This mechanism would break if local dynamics fail to converge or if representational quality degrades with depth.

### Mechanism 2
PyTorch-based tensor network implementation enables efficient GPU scaling of convolutional NSM. By leveraging EinOps for Einstein notation-based tensor operations and autograd for parameter updates, the model achieves GPU acceleration while preserving mathematical structure. The tensor network reformulation is mathematically equivalent to the original NSM dynamics and preserves convergence properties. Evidence includes the paper's description of using "EinOps library... to compute the neural dynamics" and showing that "PyTorch and conventional implementations yield similar results." This mechanism would fail if GPU acceleration introduces numerical instability or if convergence speed does not improve with scale.

### Mechanism 3
Supervised similarity matching (S2M) enables local, greedy stacking of layers while incorporating label information. S2M introduces auxiliary variables Wk, Mk, Vk and runs local dynamics (Eq. 6) with closed-form updates (Eq. 8), allowing layerwise training without backpropagation. Aligning layer outputs with both previous activations and labels preserves useful representations while enabling stacking. Evidence includes the paper's proposal of "a localized supervised objective for SM... resembles canonical correlation analysis" and implementation using auto differentiation. This mechanism would break if supervised alignment degrades unsupervised feature quality or if greedy stacking leads to catastrophic forgetting.

## Foundational Learning

- **Tensor operations and Einstein notation**: Understanding EinOps and tensor reshaping is critical for implementing convolutional NSM efficiently. Quick check: Can you explain how Einstein summation eliminates explicit loops in tensor contraction?
- **Local learning rules vs global backpropagation**: The NSM framework replaces global gradient computation with local Hebbian-like updates; understanding the trade-offs is essential for debugging and optimization.
- **Similarity matching as a normative principle**: NSM is derived from a similarity matching objective rather than reconstruction error; knowing this helps debug model behavior. Quick check: How does minimizing ∥X⊤X − Z⊤Z∥²F differ conceptually from minimizing reconstruction error?

## Architecture Onboarding

- **Component map**: Input tensor (channels × width × height) → Convolutional patches → X tensor → NSM layer: Z dynamics (Eq. 3) → learned features → Auxiliary variables: W, M (unsupervised) or Wk, Mk, Vk (supervised) → Output: Z or fine-tuned features via BP
- **Critical path**: 1. Load and preprocess input into patches, 2. Run Z dynamics until convergence, 3. Update W, M (and Wk, Mk, Vk for S2M), 4. Stack layers greedily or fine-tune with BP
- **Design tradeoffs**: Local learning is faster and more biologically plausible but potentially less accurate than BP; tensor operations are more efficient on GPU but require careful axis matching; greedy stacking is simpler than end-to-end training but may accumulate errors
- **Failure signatures**: Z dynamics fail to converge (check learning rate and initialization); poor classification accuracy (verify label alignment in S2M or patch extraction in CNN-NSM); GPU memory overflow (reduce batch size or patch size)
- **First 3 experiments**: 1. Verify NSM dynamics converge on a small synthetic dataset (e.g., MNIST patches), 2. Compare classification accuracy of single-layer NSM vs K-means on CIFAR-10, 3. Test S2M with varying αk values to observe label influence on accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the stability of filters learned through unsupervised NSM compare to those learned through supervised S2M when used for pre-training LeNet architectures? The paper presents separate results for unsupervised and supervised pre-training but does not provide a direct comparison between the two approaches in terms of filter stability. This remains unresolved because the paper only shows that both approaches demonstrate filter stability without comparing them directly. Evidence would include a direct comparison experiment showing the evolution of filters during BP training for both unsupervised NSM and supervised S2M pre-trained weights, measuring metrics like cosine similarity over training epochs.

### Open Question 2
What is the relationship between the hyperparameter α1 (controlling label influence) in S2M and the final classification accuracy when using different pooling strategies on the feature vectors? The paper shows that accuracy peaks at an optimal α1 value but only uses logistic regression for comparison with other methods. This remains unresolved because the paper only evaluates one pooling strategy (simple pooling) with logistic regression. Evidence would include experiments varying both α1 and pooling strategies (e.g., max pooling, average pooling, attention-based pooling) to determine if the optimal α1 value changes with different pooling approaches.

### Open Question 3
How does the computational efficiency of the PyTorch implementation scale with increasing network depth compared to the conventional implementation? The paper shows significant speed improvements for single-layer networks using PyTorch implementation with GPU acceleration, but does not explore scaling to deeper networks. This remains unresolved because while the paper demonstrates speed improvements for single-layer networks, it does not investigate whether these improvements scale linearly or face bottlenecks as network depth increases. Evidence would include benchmarking experiments comparing computation times and memory usage of PyTorch versus conventional implementations across networks with varying depths (2-10 layers) on the same GPU hardware.

## Limitations
- Evaluation primarily focused on CIFAR-10 and MNIST datasets with limited ablation studies on hyperparameter sensitivity
- Biological plausibility claims lack direct experimental validation against neural data
- Computational efficiency comparisons limited to single baseline implementation without broader benchmarking

## Confidence
- **High**: Computational efficiency gains from PyTorch implementation; empirical classification accuracy comparisons on CIFAR-10
- **Medium**: Biological plausibility claims based on local learning rules; filter stability observations during fine-tuning
- **Low**: Generalizability to other datasets beyond CIFAR-10/MNIST; scalability to deeper networks with multiple SM layers

## Next Checks
1. Conduct ablation studies varying αk hyperparameters in S2M to quantify their impact on classification accuracy and determine optimal settings
2. Test the framework on additional datasets (e.g., ImageNet subsets) to evaluate generalizability beyond CIFAR-10 and MNIST
3. Implement deeper networks with multiple stacked SM layers to assess whether greedy stacking maintains performance as depth increases