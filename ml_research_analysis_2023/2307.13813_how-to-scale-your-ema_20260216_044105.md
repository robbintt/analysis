---
ver: rpa2
title: How to Scale Your EMA
arxiv_id: '2307.13813'
source_url: https://arxiv.org/abs/2307.13813
tags:
- scaling
- rule
- batch
- size
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper derives and validates an EMA Scaling Rule for preserving\
  \ training dynamics across batch sizes in machine learning. The key insight is that\
  \ when scaling batch size by a factor \u03BA, the EMA momentum should be scaled\
  \ by \u03BA as well (\u03C1 \u2192 \u03C1^\u03BA), along with appropriate scaling\
  \ of other optimizer hyperparameters."
---

# How to Scale Your EMA

## Quick Facts
- arXiv ID: 2307.13813
- Source URL: https://arxiv.org/abs/2307.13813
- Reference count: 40
- Key outcome: Derives and validates EMA Scaling Rule for preserving training dynamics across batch sizes in machine learning

## Executive Summary
This paper addresses the challenge of scaling Exponential Moving Average (EMA) parameters when increasing batch size during deep learning training. The authors derive a theoretically justified EMA Scaling Rule that preserves training dynamics by exponentially scaling the EMA momentum (œÅ ‚Üí œÅ^Œ∫) when batch size is scaled by factor Œ∫. This rule enables training at larger batch sizes without sacrificing model performance, achieving up to 6√ó wall-clock time reduction under idealized hardware settings.

## Method Summary
The paper presents the EMA Scaling Rule for preserving training dynamics when scaling batch size in deep learning. The rule states that when scaling batch size by factor Œ∫, the EMA momentum should be scaled exponentially as œÅ ‚Üí œÅ^Œ∫, along with appropriate scaling of other optimizer hyperparameters. The method is theoretically justified using stochastic differential equation approximations and validated across various architectures, optimizers, and data modalities including supervised, semi-supervised, and self-supervised learning. The authors demonstrate that their scaling rule enables training methods like BYOL up to batch size 24,576 without performance degradation.

## Key Results
- The EMA Scaling Rule (œÅ ‚Üí œÅ^Œ∫) preserves training dynamics when scaling batch size by factor Œ∫
- Enables training of BYOL up to batch size 24,576 with 6√ó wall-clock time reduction
- Theoretically justified using SDE approximations and empirically validated across multiple architectures and optimizers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: When scaling batch size by factor Œ∫, EMA momentum should be scaled exponentially as œÅ ‚Üí œÅ^Œ∫ to preserve training dynamics.
- **Mechanism**: The EMA parameters act as an exponential moving average of target model parameters. When scaling batch size, the discretization step changes, requiring the EMA to integrate over a different effective time horizon. Exponential scaling of momentum compensates for this change.
- **Core assumption**: The model EMA parameters don't receive gradient updates and simply track the target model, creating a functional copy with different optimization dynamics.
- **Evidence anchors**:
  - [abstract] "the EMA momentum should be scaled by Œ∫ as well (œÅ ‚Üí œÅ^Œ∫)"
  - [section 2.1] "The second row is equivalent to a single EMA update (Definition 2.3) with momentum ÀÜùúå = ùúåùúÖ"
  - [corpus] Weak - no direct corpus support for this specific exponential scaling relationship
- **Break condition**: When the model EMA contributes to the loss function itself (as in semi-supervised learning or SSL), the simple exponential scaling breaks down.

### Mechanism 2
- **Claim**: The EMA Scaling Rule preserves first-order weak approximations in the stochastic differential equation (SDE) limit.
- **Mechanism**: By deriving an SDE approximation that includes both the target model and EMA parameters, we can show that scaling both the learning rate and momentum appropriately maintains the approximation error bounds.
- **Core assumption**: The gradient noise scale scales with the square root of the learning rate, and the loss function has bounded derivatives up to order 3.
- **Evidence anchors**:
  - [abstract] "This rule is theoretically justified using stochastic differential equation approximations"
  - [section 2.2] "Corollary 2.1.1 shows that two trajectories with different batch sizes are close in the limit of small learning rate"
  - [corpus] Weak - no direct corpus support for EMA-specific SDE derivations
- **Break condition**: When the high gradient noise hypothesis fails, the SDE approximation breaks down and the scaling rule loses theoretical justification.

### Mechanism 3
- **Claim**: Progressive Scaling combined with the EMA Scaling Rule enables training at very large batch sizes by gradually transitioning during non-critical periods.
- **Mechanism**: Early in training, BYOL dynamics are particularly sensitive to batch size changes. By gradually scaling up batch size during warmup or after initial training phases, we avoid the impulse perturbations that cause training instability.
- **Core assumption**: The early training dynamics of BYOL are more sensitive to batch size changes than later stages.
- **Evidence anchors**:
  - [section 3.4] "We observe that transitioning to the higher batch size during the warmup period results in a model optimization trajectory that diverges from the baseline"
  - [section 3.4] "transitioning after warmup results in matching final trajectories of the scaled and baseline models"
  - [corpus] Weak - no direct corpus support for progressive scaling in EMA contexts
- **Break condition**: When the optimizer scaling rule itself breaks down at large batch sizes, progressive scaling cannot compensate for the fundamental mismatch in training dynamics.

## Foundational Learning

- **Stochastic Differential Equations**: Why needed here: The paper uses SDEs to derive and justify the EMA Scaling Rule theoretically. Understanding how discrete optimization maps to continuous-time approximations is essential for grasping the mathematical foundation.
  - Quick check question: If you have SGD with learning rate Œ∑ and batch size B, what is the relationship between the noise scale parameter œÉ and the learning rate in the SDE approximation?

- **Exponential Moving Average**: Why needed here: The EMA is the core component being scaled, and understanding its update rule and how it differs from standard optimization is crucial for implementing the scaling rule correctly.
  - Quick check question: If you have an EMA with momentum œÅ tracking a parameter Œ∏, what is the explicit formula for the EMA update at iteration t?

- **Scaling Rules in Deep Learning**: Why needed here: The paper builds on existing scaling rules (like linear learning rate scaling) and extends them to include EMA parameters. Understanding the existing framework is necessary to see how this work fits into the broader context.
  - Quick check question: According to the SGD Scaling Rule, if you increase batch size by a factor of 8, by what factor should you increase the learning rate?

## Architecture Onboarding

- **Component map**: Target model (optimized with SGD/Adam/LARS) -> EMA parameters (tracking target model) -> Batch Normalization statistics (optional EMA)
- **Critical path**: The EMA update rule and its momentum parameter œÅ are the critical components that must be scaled correctly when changing batch size.
- **Design tradeoffs**: The main tradeoff is between computational efficiency (larger batch sizes) and maintaining training dynamics. The EMA Scaling Rule allows you to get efficiency benefits while preserving model quality, but requires careful implementation of the exponential momentum scaling.
- **Failure signatures**: Training dynamics diverge from the baseline when EMA momentum isn't scaled correctly. This manifests as different convergence patterns, loss trajectories, or final model performance compared to the reference batch size training.
- **First 3 experiments**:
  1. Implement a simple noisy parabola optimization with SGD and EMA, then scale batch size by 8√ó with and without the EMA Scaling Rule to observe trajectory differences.
  2. Train a small ResNet on CIFAR-10 with BYOL using the EMA Scaling Rule at batch sizes 1024, 2048, and 4096 to verify performance preservation.
  3. Apply Progressive Scaling to transition from batch size 1024 to 8192 during BYOL training after warmup to test the combined approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the EMA Scaling Rule perform when applied to other self-supervised learning methods beyond BYOL and DINO, such as SimCLR or MoCo?
- Basis in paper: The paper demonstrates the EMA Scaling Rule on BYOL and DINO, but does not explore other SSL methods. It states: "we will use BYOL...for our investigation into scaling as it is well-studied" and later discusses scaling DINO.
- Why unresolved: The paper focuses on BYOL and DINO as representative SSL methods, but does not empirically test the EMA Scaling Rule on other popular SSL approaches like SimCLR or MoCo.
- What evidence would resolve it: Experiments applying the EMA Scaling Rule to train SimCLR or MoCo at large batch sizes, comparing performance with and without the scaling rule, would provide evidence for or against its effectiveness across different SSL architectures.

### Open Question 2
- Question: What is the impact of applying the EMA Scaling Rule to the Batch Normalization momentum (œÅBN) in all scenarios, not just the image classification experiments?
- Basis in paper: The paper mentions that Batch Normalization uses an EMA update and suggests applying the EMA Scaling Rule to it, but only investigates this for image classification in Appendix F.3.
- Why unresolved: The paper does not explore the effects of scaling œÅBN in other experiments like speech recognition or self-supervised learning, where Batch Normalization is also used.
- What evidence would resolve it: Systematic experiments applying the EMA Scaling Rule to œÅBN across all experimental setups (speech recognition, self-supervised learning) and comparing results with and without this scaling would clarify its importance.

### Open Question 3
- Question: How does the EMA Scaling Rule behave when the underlying optimizer's scaling rule breaks down, as mentioned in the limitations section?
- Basis in paper: The paper acknowledges that the EMA Scaling Rule depends on the validity of the underlying optimizer's scaling rule and mentions that the LARS optimizer lacks a formal scaling rule derivation.
- Why unresolved: The paper does not provide empirical evidence of how the EMA Scaling Rule performs when the underlying optimizer's scaling rule is not valid or well-defined.
- What evidence would resolve it: Experiments using optimizers without well-defined scaling rules (like LARS) at various batch sizes, with and without the EMA Scaling Rule, would demonstrate its robustness or limitations in such scenarios.

## Limitations

- Theoretical justification relies heavily on SDE approximation which assumes small learning rates and may not hold for large-scale deep learning training
- Empirical validation does not cover all possible optimizer combinations or loss functions
- Progressive Scaling approach may not generalize to all SSL methods or other training paradigms

## Confidence

- **High confidence**: The empirical observation that EMA momentum needs to be scaled when batch size changes is consistently validated across multiple experiments and architectures
- **Medium confidence**: The theoretical justification via SDE approximations is mathematically sound but may have limited practical applicability at the large learning rates typical in deep learning
- **Medium confidence**: The specific exponential scaling rule (œÅ ‚Üí œÅ^Œ∫) works well empirically, but alternative scaling functions might also be viable and weren't extensively explored

## Next Checks

1. **Robustness Testing**: Validate the EMA Scaling Rule across additional optimizer combinations (e.g., LAMB, Nesterov momentum) and loss functions to establish broader applicability beyond the tested configurations.

2. **Large Learning Rate Analysis**: Systematically investigate the breakdown of the SDE approximation at realistic deep learning learning rates (0.1-1.0) to understand the practical limits of the theoretical justification.

3. **Alternative Scaling Functions**: Experiment with alternative momentum scaling functions (e.g., polynomial scaling, piecewise scaling) to determine if the exponential relationship is optimal or if simpler functions could achieve similar results.