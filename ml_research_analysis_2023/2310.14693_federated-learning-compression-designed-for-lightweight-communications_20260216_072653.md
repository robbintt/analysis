---
ver: rpa2
title: Federated learning compression designed for lightweight communications
arxiv_id: '2310.14693'
source_url: https://arxiv.org/abs/2310.14693
tags:
- pruning
- training
- compression
- accuracy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a straightforward magnitude pruning technique
  for compressing federated learning (FL) messages. By applying unstructured pruning
  to model weights before transmission between clients and server, the method achieves
  up to 50% reduction in message size with less than 1% accuracy loss on CIFAR-10
  image classification tasks.
---

# Federated learning compression designed for lightweight communications

## Quick Facts
- arXiv ID: 2310.14693
- Source URL: https://arxiv.org/abs/2310.14693
- Reference count: 18
- One-line primary result: Magnitude pruning achieves up to 50% message compression with <1% accuracy loss on CIFAR-10

## Executive Summary
This paper proposes a straightforward magnitude pruning technique for compressing federated learning (FL) messages. By applying unstructured pruning to model weights before transmission between clients and server, the method achieves up to 50% reduction in message size with less than 1% accuracy loss on CIFAR-10 image classification tasks. The approach is compatible with existing FL frameworks and enables further integration with other compression techniques like quantization.

## Method Summary
The method applies non-structured magnitude pruning to neural network weights before transmission in federated learning. Specifically, the algorithm sorts all weights by absolute value magnitude and replaces the bottom X% with zeros, creating a sparse tensor that compresses well with standard algorithms. The pruning is applied client-side before sending model updates to the server, and also server-side before broadcasting the global model. This simple approach can be combined with quantization-aware training for additional compression benefits.

## Key Results
- Achieves up to 50% reduction in message size with less than 1% accuracy loss on CIFAR-10
- Outperforms state-of-the-art approaches in terms of accuracy preservation while providing similar or better compression ratios
- Simple implementation compatible with existing FL frameworks like Flower
- Enables further integration with other compression techniques like quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Magnitude pruning reduces message size by zeroing out the smallest absolute weight values before transmission
- Mechanism: The algorithm sorts all weights by absolute value magnitude and replaces the bottom X% with zeros, creating a sparse tensor that compresses well with standard algorithms
- Core assumption: Weight magnitude correlates with importance to model accuracy, so removing smallest weights has minimal impact
- Evidence anchors:
  - [abstract]: "a straightforward method can compresses messages up to 50% while having less than 1% of accuracy loss"
  - [section]: "Accordingly, the θ% smaller weights are substituted with zeros, thereby pruned"
  - [corpus]: Weak evidence - corpus papers focus on compression techniques but don't specifically validate magnitude pruning's effectiveness
- Break condition: When pruning rate exceeds ~90%, accuracy degradation becomes severe (>20% loss as shown in Table II)

### Mechanism 2
- Claim: Client-side pruning allows each device to adapt sparsity to its specific data distribution
- Mechanism: Each client independently applies the same pruning algorithm to its local model before sending updates, allowing customization based on local data characteristics
- Core assumption: Local data distributions vary enough that client-specific pruning can optimize the trade-off between compression and accuracy
- Evidence anchors:
  - [abstract]: "client's flexibility to perform pruning on its own better compensates for the sparsity introduced"
  - [section]: "client's flexibility to perform pruning on its own better compensates for the sparsity introduced"
  - [corpus]: No direct evidence in corpus - this appears to be a novel contribution of the paper
- Break condition: When clients prune too aggressively or inconsistently, causing aggregation to fail

### Mechanism 3
- Claim: Combining pruning with quantization-aware training provides multiplicative compression benefits
- Mechanism: After magnitude pruning creates sparsity, quantization reduces the bit-width of remaining weights, achieving both spatial and precision compression
- Core assumption: The sparse structure from pruning doesn't interfere with quantization's ability to maintain accuracy
- Evidence anchors:
  - [section]: "it is conceivable that combining quantization and pruning could further enhance message compression"
  - [section]: "During QAT, weights are still represented as floating-point numbers but are limited to power-of-two values"
  - [corpus]: Weak evidence - corpus papers discuss compression but don't validate this specific combination
- Break condition: When quantization levels become too low (e.g., 1-bit) causing accuracy collapse

## Foundational Learning

- Concept: Federated Learning basics
  - Why needed here: Understanding the client-server architecture and why communication cost is a bottleneck
  - Quick check question: In standard FL, do clients send raw data or model updates to the server?

- Concept: Neural network pruning techniques
  - Why needed here: The method relies on unstructured magnitude pruning as the core compression mechanism
  - Quick check question: What's the difference between structured and unstructured pruning?

- Concept: Non-IID data distributions
  - Why needed here: The paper tests on both IID and non-IID scenarios to validate robustness
  - Quick check question: How does the α parameter in LDA affect data heterogeneity across clients?

## Architecture Onboarding

- Component map: Client train → prune → compress → send → server aggregate → prune → send → client
- Critical path: Client train → prune → compress → send → server aggregate → prune → send → client
- Design tradeoffs:
  - Pruning rate vs accuracy: Higher pruning gives better compression but risks accuracy loss
  - Local epochs vs communication rounds: More local training per round reduces communication but may hurt convergence
  - Sparsity pattern vs compression ratio: Unstructured pruning works better with entropy coding than structured approaches
- Failure signatures:
  - Accuracy drops >1% indicate pruning rate is too high
  - Slow convergence suggests clients aren't pruning consistently
  - Communication failures may occur with extreme sparsity (>90% pruned)
- First 3 experiments:
  1. Test baseline FL without pruning on CIFAR-10 to establish accuracy reference
  2. Apply 10-90% pruning rates and measure accuracy/compression trade-off
  3. Combine 50% pruning with 4-bit quantization and evaluate total compression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of pruning rate and quantization bits for maximizing compression while minimizing accuracy loss in federated learning?
- Basis in paper: [explicit] The paper discusses both pruning and quantization techniques separately, showing their individual impacts on accuracy and compression. It suggests combining them could further enhance compression.
- Why unresolved: The paper only examines pruning and quantization independently, not their combined effects. The optimal balance between these techniques for different scenarios remains unexplored.
- What evidence would resolve it: Systematic experiments testing various combinations of pruning rates (e.g., 10%, 20%, 50%) with different quantization levels (1-bit, 4-bit, 8-bit) on multiple datasets and model architectures, measuring both compression ratios and accuracy degradation.

### Open Question 2
- Question: How does client heterogeneity in data distribution and computational capabilities affect the performance of magnitude pruning in federated learning?
- Basis in paper: [inferred] The paper mentions challenges related to client heterogeneity but only tests in relatively IID scenarios (α = 100) and doesn't explore varying computational capabilities of clients.
- Why unresolved: The experiments were conducted with controlled conditions (10 clients, same hyperparameters) without varying data distribution skewness or client computational resources, which are critical factors in real-world FL deployments.
- What evidence would resolve it: Experiments with varying degrees of non-IID data distribution (different α values) and heterogeneous client capabilities (different numbers of local epochs, varying client participation rates) to evaluate pruning robustness across diverse scenarios.

### Open Question 3
- Question: What is the relationship between the number of local epochs and the effectiveness of pruning in federated learning?
- Basis in paper: [explicit] The paper shows that increasing local epochs leads to more robust models that can tolerate higher pruning rates while maintaining accuracy, but doesn't provide a comprehensive analysis of this relationship.
- Why unresolved: While the paper demonstrates this relationship exists, it only tests 1 vs 10 local epochs without exploring intermediate values or determining the optimal number of local epochs for different pruning levels.
- What evidence would resolve it: Detailed experiments varying the number of local epochs (e.g., 1, 5, 10, 20, 50) for different pruning rates to identify the optimal trade-off between local training time and pruning effectiveness, along with convergence speed analysis.

## Limitations
- Results are limited to CIFAR-10 experiments with ResNet architectures
- Claims about generalization to other datasets and model types remain untested
- Interaction between magnitude pruning and existing FL optimization techniques is not explored

## Confidence
- **High confidence**: The basic pruning mechanism and compression ratios are well-validated within the experimental scope
- **Medium confidence**: Claims about compatibility with quantization and other compression methods, as these are suggested but not extensively tested
- **Low confidence**: Generalization claims to diverse real-world federated learning scenarios with heterogeneous devices and non-IID data distributions

## Next Checks
1. Test the pruning method on non-image datasets (text, tabular) to verify generalization beyond computer vision tasks
2. Evaluate performance with heterogeneous client capabilities, where some devices cannot handle the same pruning rates
3. Measure end-to-end training time including compression/decompression overhead to validate practical benefits