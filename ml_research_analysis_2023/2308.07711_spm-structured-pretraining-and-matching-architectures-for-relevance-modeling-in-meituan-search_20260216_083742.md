---
ver: rpa2
title: 'SPM: Structured Pretraining and Matching Architectures for Relevance Modeling
  in Meituan Search'
arxiv_id: '2308.07711'
source_url: https://arxiv.org/abs/2308.07711
tags:
- relevance
- document
- structured
- query
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel two-stage pretraining and matching
  architecture for relevance modeling in e-commerce search, especially for documents
  with rich structured contents. The authors propose an effective pretraining method
  that employs both query and multiple fields of document as inputs, including an
  effective information compression method for lengthy fields.
---

# SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search

## Quick Facts
- arXiv ID: 2308.07711
- Source URL: https://arxiv.org/abs/2308.07711
- Authors: Multiple authors from Meituan
- Reference count: 29
- Key outcome: Novel two-stage pretraining and matching architecture improves relevance modeling for e-commerce search with structured documents, achieving 1.27% AUC improvement over baseline and deployed online serving millions of users.

## Executive Summary
This paper introduces SPM (Structured Pretraining and Matching architectures) for relevance modeling in e-commerce search, specifically addressing the challenge of matching queries to documents with rich structured content. The authors propose SPBERT, a field-aware pretraining method that handles multiple document fields with different language distributions, and an Intent-Guided Extractors (IGE) layer that uses query intent signals to extract relevant document information. The system achieves significant improvements over baseline methods through offline experiments and online A/B tests, and has been deployed in production at Meituan for over a year.

## Method Summary
The SPM architecture consists of two main components: a structured pretraining stage using SPBERT and a relevance matching stage with Intent-Guided Extractors. SPBERT applies field-specific mask strategies during pretraining to learn the distributional differences among document fields like name, brand, category, and address. The matching stage uses a bi-encoder architecture where the query is processed through a BERT encoder, and documents are processed through SPBERT with IGE layers that extract information guided by query intent signals from the search engine's query understanding module. The extracted document representations are compressed and matched with query embeddings using cosine similarity for efficient online serving.

## Key Results
- SPBERT-IGE+ achieves 1.27% AUC improvement over baseline on the relevance task
- The model has been deployed online serving Meitun search traffic for over a year
- Online A/B tests show significant improvements in top-5 search result quality (Badcase@5 metric)
- The bi-encoder architecture with cached document embeddings achieves comparable performance to cross-encoder methods while being more efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured pretraining (SPBERT) outperforms unstructured pretraining when using multi-field document inputs because it learns field-specific word distributions through field-aware masking strategies.
- Mechanism: SPBERT applies different mask strategies for each document field during pretraining, allowing the model to adapt to language distribution discrepancies among fields. The paper states: "Considering the vocabulary gap between query text and document, we also add document-related query to make the pretrained model better understand query."
- Core assumption: The distributional discrepancy among document fields is significant enough to warrant field-specific pretraining strategies.
- Evidence anchors:
  - [abstract]: "there is language distribution discrepancy among different fields of structured document"
  - [section 3.1]: "we use a separate segment embedding for each of them"
  - [corpus]: Weak. No direct citations, but related work like Zhang et al. (2020) "E-BERT" also addresses field-specific modeling in e-commerce.

### Mechanism 2
- Claim: Intent-guided extractors (IGE) improve relevance matching by using query intent signals to selectively extract the most relevant document information for matching.
- Mechanism: The IGE layer uses query intent signals from the search engine's query understanding (QU) module to guide the extraction of document representations. As stated: "It utilizes the intention signal of the query to be matched in advance, and extracts information according to the guidance of this knowledge."
- Core assumption: Query intent signals are reliable and can effectively guide which document fields and tokens are most relevant for matching.
- Evidence anchors:
  - [section 3.1.2]: "we use the query understanding (QU) module in search engine to provides query intention signal"
  - [section 4.4]: "The intent signal of queries are obtained from query understanding module in search engine"
  - [corpus]: Weak. While related works like "Query-oriented Data Augmentation for Session Search" exist, there's no direct citation showing the effectiveness of intent-guided extractors in this specific architecture.

### Mechanism 3
- Claim: The bi-encoder architecture with cached document embeddings achieves comparable performance to cross-encoder methods while being more efficient for online serving.
- Mechanism: The bi-encoder architecture allows pre-computation and caching of document embeddings, reducing online computation. The paper states: "The advantage of bi-encoder architecture is that the output embeddings of the document can be calculated and cached in advance to speed up online serving."
- Core assumption: The performance gap between bi-encoder and cross-encoder can be sufficiently closed through better document representation extraction.
- Evidence anchors:
  - [abstract]: "At finetuning stage, the models used in relevance task can be categorized into two architectures: cross-encoder and bi-encoder"
  - [section 3.1.1]: "The advantage of bi-encoder architecture is that the output embeddings of the document can be calculated and cached in advance to speed up online serving"
  - [corpus]: Weak. While bi-encoder efficiency is well-established, the specific claim that IGE closes the performance gap isn't directly supported by cited works.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) pretraining
  - Why needed here: MLM is the core pretraining task used by BERT and SPBERT to learn contextual word representations from unlabeled data.
  - Quick check question: What is the difference between MLM and traditional language modeling, and why is MLM better suited for pretraining bidirectional representations?

- Concept: Attention mechanisms and transformer architecture
  - Why needed here: The attention mechanism is fundamental to both the pretraining model (SPBERT) and the matching architecture, enabling the model to focus on relevant parts of the input.
  - Quick check question: How does multi-head self-attention work, and why is it more effective than single-head attention for capturing complex relationships in text?

- Concept: Cross-encoder vs. bi-encoder architectures for text matching
  - Why needed here: Understanding the tradeoffs between these architectures is crucial for appreciating why the authors chose a bi-encoder for online serving and how they attempted to close its performance gap with cross-encoders.
  - Quick check question: What are the key differences in how cross-encoder and bi-encoder architectures process query-document pairs, and what are the implications for performance and efficiency?

## Architecture Onboarding

- Component map: Query -> BERT encoder -> IGE layer -> compression -> cosine similarity; Document fields -> SPBERT encoder -> IGE layer -> compression -> cached embeddings

- Critical path:
  1. Input: Query and structured document fields
  2. SPBERT encoding: Generate contextual representations
  3. IGE extraction: Use query intent to extract relevant document information
  4. Compression: Reduce embedding dimensionality
  5. Matching: Calculate cosine similarity between query and document embeddings

- Design tradeoffs:
  - SPBERT vs. standard BERT: SPBERT handles structured documents better but requires more complex pretraining
  - IGE vs. late interaction: IGE extracts relevant information earlier but relies on accurate query intent signals
  - Bi-encoder vs. cross-encoder: Bi-encoder is more efficient but requires sophisticated document representation extraction to match cross-encoder performance

- Failure signatures:
  - SPBERT pretraining fails: Poor performance on relevance tasks, especially when using multi-field inputs
  - IGE extraction fails: No improvement or degradation in performance compared to simpler methods, or high variance in results
  - Bi-encoder performance gap remains: Significant difference in AUC between bi-encoder and cross-encoder variants

- First 3 experiments:
  1. Compare SPBERT-CE+ (cross-encoder with structured pretraining) vs. BERT-Large-CE+ (unstructured pretraining) on a small subset of the relevance dataset to verify the benefit of structured pretraining.
  2. Implement a simplified version of the IGE layer (e.g., using only one intent type) and compare its performance to a baseline bi-encoder model to isolate the effect of the extraction method.
  3. Conduct an ablation study by removing each document field from the input and measuring the impact on relevance task performance to understand which fields are most important.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SPBERT-IGE+ change when using different numbers of intent-guided extractors corresponding to various query intent types beyond the 3 types mentioned in the paper?
- Basis in paper: [explicit] The paper states that the number of intent-guided extractors is set to 3 corresponding to the number of query intent types used, but does not explore the impact of using more or fewer intent types.
- Why unresolved: The paper does not provide experimental results for different numbers of intent-guided extractors, leaving uncertainty about the optimal number for various query intent types.
- What evidence would resolve it: Experimental results showing the performance of SPBERT-IGE+ with varying numbers of intent-guided extractors, corresponding to different query intent types, would provide insights into the optimal configuration.

### Open Question 2
- Question: What is the impact of using different compression techniques in the representation compression layer on the performance of SPBERT-IGE+?
- Basis in paper: [inferred] The paper mentions the use of a full-connected network to project the dimension of embeddings from 384 to 32 but does not explore the effects of alternative compression techniques.
- Why unresolved: The paper does not compare the performance of SPBERT-IGE+ with different compression techniques, leaving uncertainty about the most effective method for compressing document representations.
- What evidence would resolve it: Experimental results comparing the performance of SPBERT-IGE+ with different compression techniques, such as principal component analysis (PCA) or quantization, would provide insights into the most effective method for compressing document representations.

### Open Question 3
- Question: How does the performance of SPBERT-IGE+ change when using different pretraining tasks, such as token-level or sequence-level objectives, in addition to the masked language modeling (MLM) task?
- Basis in paper: [inferred] The paper focuses on the MLM task for pretraining but does not explore the effects of incorporating additional pretraining tasks.
- Why unresolved: The paper does not provide experimental results for SPBERT-IGE+ with different pretraining tasks, leaving uncertainty about the optimal pretraining strategy for structured documents.
- What evidence would resolve it: Experimental results showing the performance of SPBERT-IGE+ with different pretraining tasks, such as token-level or sequence-level objectives, would provide insights into the most effective pretraining strategy for structured documents.

## Limitations
- Limited ablation studies to isolate the independent contributions of SPBERT pretraining versus Intent-Guided Extractors
- Reliance on query intent signals from the QU module introduces a potential failure point without discussion of signal quality
- Domain-specific design may not generalize well to other e-commerce domains or different types of structured documents

## Confidence
**High Confidence**:
- The bi-encoder architecture with cached embeddings provides computational efficiency benefits for online serving
- Field-aware masking strategies can improve pretraining for structured documents
- Query intent signals can guide document representation extraction

**Medium Confidence**:
- SPBERT pretraining provides significant improvements over standard BERT for this specific task
- The Intent-Guided Extractors consistently improve relevance matching across different query types
- The performance gap between bi-encoder and cross-encoder can be sufficiently closed through better document representation extraction

**Low Confidence**:
- The specific compression method for lengthy document fields doesn't degrade relevance performance
- The system maintains its performance advantage as the document corpus grows
- The exact query intent signal types and their corresponding extractors are optimal for this task

## Next Checks
1. **Component Isolation Test**: Conduct an ablation study where SPBERT pretraining is applied without IGE extractors, and vice versa, to quantify the independent contributions of each component to the overall performance improvement.

2. **Intent Signal Quality Analysis**: Measure the correlation between QU module intent classification accuracy and relevance matching performance to establish whether intent signal quality directly impacts the system's effectiveness.

3. **Domain Transfer Experiment**: Apply the SPM architecture to a different e-commerce domain (e.g., retail product search) with different document structures and query patterns to test the generalizability of the approach.