---
ver: rpa2
title: Merging by Matching Models in Task Parameter Subspaces
arxiv_id: '2312.04339'
source_url: https://arxiv.org/abs/2312.04339
tags:
- merging
- fisher
- task
- arxiv
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method called Matching Models in their Task
  Subspace (MaTS) for merging fine-tuned models into a multitask model. The core idea
  is to match models in their respective "task subspaces" before merging them.
---

# Merging by Matching Models in Task Parameter Subspaces

## Quick Facts
- arXiv ID: 2312.04339
- Source URL: https://arxiv.org/abs/2312.04339
- Reference count: 40
- Primary result: Introduces MaTS framework achieving state-of-the-art multitask and intermediate-task model merging

## Executive Summary
The paper proposes Matching Models in their Task Subspace (MaTS) for merging fine-tuned models into a multitask model. The core innovation is matching models in their respective "task subspaces" before merging them, viewing previous merging methods as finding a single model that matches task-specific models in these subspaces. The authors introduce a new method called Block-Diagonal Fisher Merging (BFM) and demonstrate state-of-the-art performance across various datasets and tasks.

## Method Summary
MaTS merges M models by first estimating task subspaces for each model using basis vectors Q and importance scores Λ, then solving a linear system Ax = b using the conjugate gradient method. The framework flexibly supports different merging objectives by varying the choice of task subspace (Cm matrices), including simple averaging, Fisher merging, and the new block-diagonal Fisher merging. The method is evaluated on both full-model and parameter-efficient merging scenarios.

## Key Results
- MaTS achieves state-of-the-art performance in multitask and intermediate-task model merging
- Block-diagonal Fisher Merging (BFM) provides superior performance compared to diagonal approximations
- The framework successfully supports both full-model and parameter-efficient merging approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Matching models in their task subspaces prevents washing out task-relevant components during merging.
- **Mechanism:** The merging method projects each model's parameters into a subspace defined by basis vectors (Q) and importance scores (Λ), then combines them while preserving relative importance in each subspace.
- **Core assumption:** The task subspace (as defined by the chosen basis vectors and importance scores) captures the important dimensions for the task.
- **Evidence anchors:**
  - [abstract] "We view past merging methods as leveraging different notions of a 'task parameter subspace' in which models are matched before being merged."
  - [section 3] "The top column vectors in Vm and Qm can therefore viewed as forming a basis that is 'best aligned' with the data Pm. We therefore refer to Qm as the basis vectors of the 'task subspace' whose relative importance are determined by Λm."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 2
- **Claim:** Using conjugate gradient (CG) to solve the merging linear system enables more flexible and performant merging objectives.
- **Mechanism:** CG allows solving linear systems without closed-form solutions or matrix inversions, enabling use of objectives like block-diagonal Fisher merging that would otherwise be intractable.
- **Core assumption:** The linear system corresponding to the merging objective is positive semi-definite and CG converges efficiently.
- **Evidence anchors:**
  - [abstract] "we consider using the conjugate gradient method to find a solution...enables merging via linear systems that are otherwise intractable to solve"
  - [section 4] "The conjugate gradient method...can be used to solve the Fisher merging objective with a block-diagonal Fisher approximation."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.

### Mechanism 3
- **Claim:** Different merging methods correspond to different choices of task subspace, leading to different performance characteristics.
- **Mechanism:** Simple averaging uses identity subspace, Fisher merging uses Hessian-based subspace, RegMean uses input activation subspace - each prioritizes different parameter dimensions.
- **Core assumption:** The choice of task subspace meaningfully affects which parameter dimensions are preserved during merging.
- **Evidence anchors:**
  - [section 3.1] "Simple averaging sets Cm = I...Fisher Merging...Cm = Fm is the covariance of the per-example gradients...RegMean...sets Cm as a function of the scaled Gram matrix"
  - [section 3.2] "The extent to which a solution θ∗ satisfies eq. (7) depends on how close QmΛmQ⊤mθ∗ is to each QmΛmQ⊤mθm"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.

## Foundational Learning

- **Linear Algebra and Matrix Operations**
  - Why needed here: The merging framework relies heavily on matrix operations, eigen decompositions, and solving linear systems.
  - Quick check question: Can you explain how the eigen decomposition of a covariance matrix relates to finding principal components?

- **Conjugate Gradient Method**
  - Why needed here: CG is used to solve the linear systems that define the merging objectives without requiring closed-form solutions.
  - Quick check question: What are the key properties that make CG suitable for solving positive semi-definite linear systems?

- **Fisher Information Matrix and Natural Gradient**
  - Why needed here: Fisher merging uses the Fisher information matrix to estimate parameter importance and define the task subspace.
  - Quick check question: How does the Fisher information matrix relate to the Hessian and what does it capture about the loss landscape?

## Architecture Onboarding

- **Component Map:** Task Subspace Estimator -> Linear System Solver -> Initialization Handler -> Merging Objective Config

- **Critical Path:**
  1. Compute task subspaces for all models (Q, Λ matrices)
  2. Set up linear system Ax = b
  3. Initialize CG with chosen strategy
  4. Run CG iterations until convergence
  5. Return merged model parameters

- **Design Tradeoffs:**
  - CG iterations vs closed-form solutions: CG is more flexible but computationally heavier
  - Task subspace choice vs generalization: Different subspaces prioritize different parameter dimensions
  - Initialization strategy vs convergence speed: Good initialization can dramatically reduce iterations needed

- **Failure Signatures:**
  - Poor performance: Likely due to inappropriate task subspace choice or insufficient CG iterations
  - Numerical instability: May indicate ill-conditioned linear system or CG divergence
  - Slow convergence: Could be due to poor initialization or inappropriate step size

- **First 3 Experiments:**
  1. Merge two simple models using different task subspaces (averaging, Fisher, RegMean) to observe performance differences
  2. Test CG convergence with different initializations on a simple linear system
  3. Compare performance of BFM vs diagonal Fisher merging on a small-scale problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of task subspace affect the performance of MaTS in merging models with different architectures or initializations?
- Basis in paper: [explicit] The paper mentions that the task subspace is defined as the subspace implicitly used by a given merging method that corresponds to the important dimensions in parameter space for the task. However, it does not explicitly explore the impact of using different architectures or initializations.
- Why unresolved: The paper focuses on merging models that share a common architecture and initialization. It would be interesting to see how the choice of task subspace affects the performance of MaTS when merging models with different architectures or initializations.
- What evidence would resolve it: Conducting experiments that merge models with different architectures or initializations using MaTS and comparing the results with other merging methods would provide insights into the impact of the task subspace choice.

### Open Question 2
- Question: Can the conjugate gradient method be further optimized to improve the efficiency of MaTS?
- Basis in paper: [explicit] The paper mentions that the conjugate gradient method is used to solve the linear system of equations in MaTS, which enables merging via linear systems that are otherwise intractable to solve. However, it does not explore potential optimizations of the conjugate gradient method.
- Why unresolved: The paper does not discuss any potential optimizations of the conjugate gradient method that could improve the efficiency of MaTS. Exploring such optimizations could lead to faster convergence and better performance.
- What evidence would resolve it: Conducting experiments that compare the performance of MaTS with different optimizations of the conjugate gradient method would provide insights into the potential improvements in efficiency.

### Open Question 3
- Question: How does the choice of initialization affect the performance of MaTS in merging models with different training objectives?
- Basis in paper: [explicit] The paper mentions that MaTS can flexibly support different choices of an initialization for optimization. However, it does not explicitly explore the impact of using different training objectives.
- Why unresolved: The paper focuses on merging models with the same training objective. It would be interesting to see how the choice of initialization affects the performance of MaTS when merging models with different training objectives.
- What evidence would resolve it: Conducting experiments that merge models with different training objectives using MaTS and comparing the results with other merging methods would provide insights into the impact of the initialization choice.

## Limitations
- Task subspace estimation quality is assumed but not rigorously validated across different scenarios
- Conjugate gradient convergence properties are not thoroughly analyzed for different problem scales
- Limited evaluation on very large models (only tested up to 13B parameters)

## Confidence

- **High Confidence**: The linear algebra framework connecting different merging methods through task subspaces is mathematically sound and well-explained. The basic mechanism of matching models in subspaces before merging is clearly articulated.

- **Medium Confidence**: The empirical results show MaTS outperforms baselines on tested datasets, but the sample size is limited. The performance gains, while consistent, are not dramatic across all settings.

- **Low Confidence**: Claims about CG enabling new classes of merging objectives lack rigorous validation. The specific advantages of BFM over existing methods are not fully explored.

## Next Checks
1. **Ablation Study on Task Subspace Quality**: Systematically vary the quality of Q and Λ estimates (e.g., using different numbers of principal components) to quantify the impact on merging performance and identify break points.

2. **Convergence Analysis of CG**: For each merging objective, measure CG convergence rates across different initialization strategies and model scales. Identify conditions where CG fails or requires excessive iterations.

3. **Scalability Benchmark**: Test MaTS on a larger, more diverse set of tasks and model sizes (e.g., 30B+ parameters) to evaluate how performance scales and whether the claimed advantages persist at scale.