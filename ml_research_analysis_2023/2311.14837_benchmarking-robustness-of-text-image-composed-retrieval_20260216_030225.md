---
ver: rpa2
title: Benchmarking Robustness of Text-Image Composed Retrieval
arxiv_id: '2311.14837'
source_url: https://arxiv.org/abs/2311.14837
tags:
- image
- text
- robustness
- retrieval
- composed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first comprehensive robustness analysis
  for text-image composed retrieval models. The authors establish three new benchmark
  datasets to evaluate models against natural corruptions in both vision and text,
  as well as their textual understanding capabilities.
---

# Benchmarking Robustness of Text-Image Composed Retrieval

## Quick Facts
- arXiv ID: 2311.14837
- Source URL: https://arxiv.org/abs/2311.14837
- Authors: 
- Reference count: 40
- One-line primary result: This paper introduces the first comprehensive robustness analysis for text-image composed retrieval models using three new benchmark datasets.

## Executive Summary
This paper establishes a framework for evaluating the robustness of text-image composed retrieval models against natural corruptions in both vision and text. The authors introduce three new benchmark datasets - CIRR-C and FashionIQ-C with 15 visual and 7 textual corruptions, and CIRR-D with diagnostic categories for textual understanding. Experiments on six state-of-the-art models reveal that large pretrained models show better robustness, while models with aligned text-image feature spaces demonstrate superior stability. The findings suggest that enhancing pretraining data size and optimizing modality fusion are key to improving robustness in text-image composed retrieval systems.

## Method Summary
The authors establish a unified testing platform to evaluate six state-of-the-art text-image composed retrieval models against natural corruptions. They create three new benchmark datasets: CIRR-C and FashionIQ-C apply 15 visual and 7 textual corruptions to existing datasets, while CIRR-D expands the original CIRR dataset with synthetic data to probe textual understanding across five dimensions. The evaluation measures retrieval performance using Recall@K metrics and relative robustness scores γ = 1 − (Rc − Rp) /Rc, where Rc and Rp represent corrupted and original performance. Models tested include TIRG, MAAF, ARTEMIS, CIRPLANT, CLIP4CIR, and FashionViL, covering various pretraining scales and modality fusion approaches.

## Key Results
- Large pretrained models (400M, 6.5M, 1.35M image-text pairs) consistently show better robustness to natural corruptions
- Models with aligned text-image feature spaces (CLIP-based) demonstrate superior stability compared to those with independent modality encoders
- Specific text modifications that minimize feasible target candidates enhance the model's discriminative ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large pretraining dataset size improves robustness to natural corruptions in text-image composed retrieval.
- Mechanism: Pretraining on large datasets (400M, 6.5M, 1.35M image-text pairs) provides better feature representations that are more resilient to distributional shifts from natural corruptions.
- Core assumption: The pretraining data covers a diverse enough distribution to generalize to corruption scenarios.
- Evidence anchors:
  - [abstract]: "large pretrained models show better robustness"
  - [section]: "models with large pretrained datasets consistently show better robustness in both open domain and fashion domain"
  - [corpus]: Weak - no direct corpus evidence for this specific claim
- Break condition: If the pretraining data distribution is too narrow or doesn't include corruption-like variations, the benefit disappears.

### Mechanism 2
- Claim: Aligned text-image feature spaces improve robustness compared to independent modality encoders.
- Mechanism: When text and image features are aligned in a unified space during pretraining (like CLIP), the modality fusion is more stable under corruption because both modalities benefit from the same representation space.
- Core assumption: The alignment during pretraining captures semantic relationships that help both modalities under corruption.
- Evidence anchors:
  - [abstract]: "models with aligned text-image feature spaces demonstrate superior stability"
  - [section]: "Comparing Image-only (CLIP) and CLIP4CIR...we can find out CLIP4CIR consistently performs better recall performance as well as robustness"
  - [corpus]: Weak - no direct corpus evidence for this specific claim
- Break condition: If the corruption significantly degrades the alignment or if the modalities need different feature spaces for the task.

### Mechanism 3
- Claim: Modified text that minimizes feasible target candidates enhances discriminative ability.
- Mechanism: When modified text provides specific constraints that narrow down the target image set, the model can more easily distinguish the correct target from similar candidates.
- Core assumption: The model can effectively use textual constraints to filter candidates in the dense visual space.
- Evidence anchors:
  - [abstract]: "whether these models are robust across diverse textural understanding requirements"
  - [section]: "a modified text is more likely to enhance the model's discriminative ability when it minimizes the number of feasible targets"
  - [corpus]: Weak - no direct corpus evidence for this specific claim
- Break condition: If the textual constraints are too vague or contradictory, leading to multiple plausible targets.

## Foundational Learning

- Concept: Dense vs. sparse semantic representations
  - Why needed here: Text-image composed retrieval relies on the contrast between dense image representations and sparse text representations to bridge semantic gaps
  - Quick check question: Why does combining image and text queries help overcome limitations of single-modality retrieval?

- Concept: Modality fusion strategies
  - Why needed here: Different fusion approaches (early, late, cross-attention) significantly impact robustness to corruption
  - Quick check question: How does the choice of modality fusion affect the model's ability to handle corrupted inputs?

- Concept: Distribution shift and robustness
  - Why needed here: Understanding how models generalize to corrupted data requires knowledge of distribution shift concepts
  - Quick check question: What is the difference between in-distribution and out-of-distribution performance?

## Architecture Onboarding

- Component map: Image encoder (ResNet50, CLIP, etc.) -> Text encoder (LSTM, CLIP, etc.) -> Modality fusion module (residual, attention, transformer) -> Retrieval layer (cosine similarity, dot product)
- Critical path: Image feature extraction → Text feature extraction → Modality fusion → Similarity computation → Ranking
- Design tradeoffs:
  - Independent vs. aligned encoders: Independent encoders offer flexibility but may lack robustness; aligned encoders provide stability but may be less adaptable
  - Fusion timing: Early fusion captures cross-modal interactions but may lose modality-specific details; late fusion preserves details but may miss interactions
  - Pretraining scale: Larger datasets improve robustness but increase computational cost
- Failure signatures:
  - Poor performance on corrupted data but good on clean data indicates overfitting to clean distribution
  - Text-only or image-only performance exceeding composed retrieval suggests modality fusion issues
  - Inconsistent performance across corruption types indicates vulnerability to specific corruption patterns
- First 3 experiments:
  1. Compare performance of independent vs. aligned encoders on corrupted data to validate Mechanism 2
  2. Test retrieval with progressively more specific text modifications to validate Mechanism 3
  3. Evaluate models with varying pretraining dataset sizes on the same corruption benchmarks to validate Mechanism 1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size and quality of pretraining data specifically impact the robustness of text-image composed retrieval models against natural corruptions?
- Basis in paper: [explicit] The paper states that models with large pretraining datasets (FashionViL, CIRPLANT, CLIP4CIR) consistently show better robustness in both open domain and fashion domain.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between pretraining data characteristics (size, quality, domain relevance) and robustness to different types of corruptions.
- What evidence would resolve it: Experiments comparing models trained on datasets of varying sizes and qualities, analyzing their robustness to different corruption types.

### Open Question 2
- Question: How does the alignment of text and image feature spaces in the pretraining process affect the robustness of text-image composed retrieval models?
- Basis in paper: [explicit] The paper observes that CLIP4CIR, which uses aligned text and image embeddings, consistently performs better in both accuracy and robustness compared to models with independent text and image encoders.
- Why unresolved: The paper does not provide a detailed analysis of how the alignment of feature spaces impacts the model's ability to handle corruptions in either modality.
- What evidence would resolve it: Experiments comparing models with aligned and non-aligned feature spaces, analyzing their robustness to text and image corruptions separately.

### Open Question 3
- Question: How does the specificity of modified text in the query affect the model's discriminative ability in text-image composed retrieval?
- Basis in paper: [explicit] The paper observes that models achieve higher performance with attribute queries than with CIRR queries, suggesting that specific instructions can enhance discriminative ability.
- Why unresolved: The paper does not provide a detailed analysis of how different types of text modifications (e.g., numerical variations, attribute variations, object removal) impact the model's ability to retrieve the correct target image.
- What evidence would resolve it: Experiments analyzing the model's performance on queries with varying levels of specificity and different types of text modifications.

## Limitations

- The analysis focuses on specific corruption types and pretrained model families, with less coverage of adversarially crafted corruptions or novel architectural approaches
- The datasets used (CIRR-C, FashionIQ-C, CIRR-D) may not fully capture the diversity of real-world degradation scenarios
- The paper does not address potential trade-offs between robustness and retrieval accuracy on clean data

## Confidence

- High confidence: The finding that larger pretraining datasets correlate with improved robustness (Mechanism 1) is well-supported by comparative experiments across models with different training scales
- Medium confidence: The superiority of aligned feature spaces (Mechanism 2) is demonstrated through CLIP-based models outperforming independent encoders, though the analysis could benefit from more architectural variations
- Low confidence: The claim about text specificity enhancing discriminative ability (Mechanism 3) lacks direct experimental validation linking text modification strategies to retrieval performance

## Next Checks

1. Conduct ablation studies removing different corruption types from CIRR-C to identify which specific perturbations drive the observed robustness differences
2. Test whether models robust to natural corruptions maintain their advantage when evaluated on adversarially crafted corruptions
3. Analyze feature space visualizations under corruption to verify whether aligned models maintain semantic relationships better than independent encoders