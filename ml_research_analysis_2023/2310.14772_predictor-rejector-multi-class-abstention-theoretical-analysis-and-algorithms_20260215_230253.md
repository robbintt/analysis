---
ver: rpa2
title: 'Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms'
arxiv_id: '2310.14772'
source_url: https://arxiv.org/abs/2310.14772
tags:
- loss
- abstention
- surrogate
- labs
- maxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning with abstention in multi-class
  classification, where the learner can choose to abstain from making a prediction
  with a pre-defined cost. The authors introduce a predictor-rejector framework that
  explicitly models the cost of abstention and propose new surrogate losses for this
  setting.
---

# Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms

## Quick Facts
- arXiv ID: 2310.14772
- Source URL: https://arxiv.org/abs/2310.14772
- Reference count: 40
- Key outcome: Introduces predictor-rejector framework for multi-class abstention with strong consistency guarantees, resolving open questions about surrogate losses for this setting.

## Executive Summary
This paper addresses the problem of learning with abstention in multi-class classification, where a learner can choose to abstain from prediction at a predefined cost. The authors introduce a predictor-rejector framework that explicitly models the cost of abstention, proposing new surrogate losses that achieve strong theoretical guarantees. They prove non-asymptotic consistency results and analyze both single-stage (joint learning) and two-stage (sequential learning) settings. Experiments on CIFAR-10, CIFAR-100, and SVHN datasets demonstrate the effectiveness of their approach compared to score-based baselines.

## Method Summary
The paper introduces predictor-rejector surrogate losses that combine a multi-class classification loss ℓ with abstention mechanisms through functions Φ and Ψ. The single-stage loss is L(h, r, x, y) = ℓ(h, x, y)Φ(−αr(x)) + Ψ(c)Φ(βr(x)), while the two-stage loss uses a fixed predictor h learned with cross-entropy and optimizes rejector r on ℓΦ,h(r, x, y) = 1h(x)≠yΦ(−r(x)) + cΦ(r(x)). The framework allows independent optimization of predictor h and rejector r functions, with theoretical guarantees including H-consistency and realizable consistency.

## Key Results
- New predictor-rejector surrogate losses achieve strong non-asymptotic consistency guarantees, resolving open questions in the literature
- Two-stage algorithm provides practical advantages by separating predictor training from rejector optimization, crucial for applications with pre-trained predictors
- Realizable consistency guarantees ensure that when data is perfectly separable, the surrogate loss minimization leads to zero abstention loss
- Experiments on CIFAR-10, CIFAR-100, and SVHN demonstrate superior performance compared to score-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The predictor-rejector framework achieves stronger consistency guarantees than score-based approaches because it allows independent optimization of predictor and rejector functions.
- Mechanism: By learning h and r separately, the model can minimize the abstention loss more effectively than a joint score-based formulation where rejection is implicitly tied to the max score difference.
- Core assumption: The hypothesis sets H and R are symmetric and complete, allowing for optimal separation of prediction and rejection decisions.
- Evidence anchors:
  - [abstract] "We present a series of new theoretical and algorithmic results for this learning problem in the predictor-rejector framework"
  - [section] "In view of the expression of the predictor-rejector abstention loss Labs(h, r, x, y) = 1h(x)≠y1r(x)>0+ c1r(x)≤0, if ℓ is a surrogate loss for the zero-one multi-class classification loss over the set of labels Y, then, L defined as follows is a natural surrogate loss for Labs"
  - [corpus] Weak - related works focus on score-based approaches without comparing directly to predictor-rejector consistency

### Mechanism 2
- Claim: The two-stage predictor-rejector approach provides practical advantages by separating predictor training from rejector optimization.
- Mechanism: First stage learns a standard predictor h using cross-entropy, then second stage learns rejector r conditioned on h, avoiding costly retraining while maintaining strong theoretical guarantees.
- Core assumption: The predictor h learned in stage one is fixed and the rejector r can be optimized independently to minimize the abstention loss.
- Evidence anchors:
  - [abstract] "We analyze both a single-stage setting where the predictor and rejector are learned simultaneously and a two-stage setting crucial in applications, where the predictor is learned in a first stage using a standard surrogate loss such as cross-entropy"
  - [section] "The two-stage setting is crucial in many applications since the predictor h is often already learned after a costly training of several hours or days. Re-training to ensure a simultaneous learning of h and r is then inconceivable due to its prohibitive cost."
  - [corpus] Weak - most related works focus on single-stage approaches without exploring the two-stage decomposition

### Mechanism 3
- Claim: Realizable consistency ensures that when the data is perfectly separable, the surrogate loss minimization leads to zero abstention loss.
- Mechanism: By proving that the surrogate losses are realizable (H,R)-consistent, the framework guarantees that if there exists a predictor-rejector pair with zero abstention loss, the learning algorithm will find it.
- Core assumption: H and R are closed under scaling, which allows the analysis to show that the surrogate loss approaches zero when the optimal predictor-rejector pair is scaled appropriately.
- Evidence anchors:
  - [abstract] "We also prove realizable consistency guarantees for both single-stage and two-stage surrogate losses"
  - [section] "Theorem 9 Assume that H and R are closed under scaling. Let Ψ(0) = 0 and Φ satisfy Assumption 1. Then, for any ℓ that satisfies Assumption 2, the following (H, R)-consistency bound holds for any (H, R)-realizable distribution, h∈H and r ∈R: ELabs(h, r)−E∗Labs(H, R) ≤EL(h, r)−E∗L(H, R)."
  - [corpus] Weak - realizable consistency is less commonly discussed in related works focused on Bayes-consistency

## Foundational Learning

- Concept: H-consistency bounds
  - Why needed here: Provide stronger theoretical guarantees than Bayes-consistency by bounding the estimation error of the target abstention loss in terms of the surrogate loss estimation error.
  - Quick check question: Can you explain the difference between Bayes-consistency and H-consistency in the context of surrogate loss minimization?

- Concept: Minimizability gap
  - Why needed here: Quantifies the difference between the infimum of the surrogate loss over all measurable functions versus over the restricted hypothesis set H×R, affecting the tightness of consistency bounds.
  - Quick check question: How does the minimizability gap affect the practical performance of the predictor-rejector algorithms compared to their theoretical guarantees?

- Concept: Calibration gap
  - Why needed here: Measures the difference between the conditional risk of a hypothesis pair and the best-in-class conditional risk, crucial for analyzing the consistency of surrogate losses.
  - Quick check question: What role does the calibration gap play in proving the (H,R)-consistency bounds for the predictor-rejector surrogate losses?

## Architecture Onboarding

- Component map:
  - Predictor function h: Maps input x to score vector h(x, y) for each class y
  - Rejector function r: Maps input x to real value determining acceptance/rejection
  - Surrogate loss L: Combines multi-class classification loss ℓ with abstention mechanism using functions Φ and Ψ
  - Training pipeline: Either single-stage joint optimization or two-stage sequential optimization

- Critical path:
  1. Define hypothesis sets H and R (e.g., neural networks with specific architectures)
  2. Choose surrogate loss ℓ (e.g., MAE, hinge, or margin loss) and parameters α, β
  3. Implement loss function L combining ℓ with abstention mechanism
  4. Optimize L to learn (h, r) either jointly or in two stages
  5. Evaluate on abstention loss Labs and compare with baselines

- Design tradeoffs:
  - Single-stage vs. two-stage: Single-stage offers stronger theoretical guarantees but may be harder to optimize; two-stage is more practical when predictor is pre-trained
  - Choice of ℓ: MAE is theoretically sound but harder to optimize; hinge and margin losses offer better optimization properties but may have different consistency properties
  - Hypothesis set complexity: More complex H and R can improve performance but increase computational cost and risk of overfitting

- Failure signatures:
  - High abstention rate with poor accuracy on accepted samples: Indicates rejector is too conservative or predictor is poorly calibrated
  - Low abstention rate with high error: Suggests rejector is not effectively identifying uncertain predictions
  - Training instability or slow convergence: May indicate difficult optimization landscape of chosen surrogate loss

- First 3 experiments:
  1. Compare single-stage predictor-rejector with ℓ=ℓmae against score-based baseline on CIFAR-10
  2. Implement two-stage approach with pre-trained ResNet-34 predictor and optimize rejector on SVHN
  3. Test realizable consistency by training on perfectly separable synthetic data and measuring abstention loss convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design more efficient algorithms for optimizing the mean absolute error (MAE) loss in the single-stage predictor-rejector framework?
- Basis in paper: The authors acknowledge that optimizing the MAE loss is known to be challenging, as highlighted in the study by Zhang et al. (2018).
- Why unresolved: The MAE loss is non-differentiable and non-convex, making it difficult to optimize directly.
- What evidence would resolve it: Development of novel optimization techniques or approximation methods that can efficiently minimize the MAE loss in the context of multi-class abstention.

### Open Question 2
- Question: Can we extend the two-stage predictor-rejector framework to scenarios where the predictor h is not fixed but can be updated during the second stage?
- Basis in paper: The authors focus on the case where the predictor h is fixed in the second stage, as retraining a large pre-trained model is often prohibitively expensive.
- Why unresolved: The current two-stage framework assumes a fixed predictor, but there may be scenarios where updating the predictor could improve performance.
- What evidence would resolve it: Development of a hybrid approach that allows for limited updates to the predictor h in the second stage while maintaining computational efficiency.

### Open Question 3
- Question: How do the proposed predictor-rejector surrogate losses perform in real-world applications beyond image classification, such as natural language processing or speech recognition?
- Basis in paper: The authors evaluate their methods on CIFAR-10, CIFAR-100, and SVHN datasets, which are primarily used for image classification.
- Why unresolved: The effectiveness of the proposed methods in other domains is unknown.
- What evidence would resolve it: Empirical evaluation of the predictor-rejector surrogate losses on diverse datasets from various application domains, such as text classification, speech recognition, or recommendation systems.

## Limitations

- Theoretical analysis assumes complete hypothesis sets H and R, which may not hold in practice for complex deep learning architectures
- Empirical validation limited to standard image classification datasets without exploring edge cases or real-world scenarios where methods might fail
- Performance highly dependent on hyperparameter choices (α, β, Φ, Ψ) without clear guidance on optimal selection

## Confidence

- Confidence in theoretical consistency guarantees and realizable consistency proofs: High
- Confidence in practical superiority over score-based methods: Medium
- Confidence in generalizability to domains with complex class relationships: Low

## Next Checks

1. Test the predictor-rejector framework on imbalanced datasets where abstention strategies might differ significantly from balanced settings.

2. Evaluate performance when H or R is incomplete (e.g., neural networks with limited capacity) to validate theoretical assumptions in practical settings.

3. Conduct ablation studies varying the cost of abstention c to understand its impact on both prediction accuracy and rejection behavior across different domains.