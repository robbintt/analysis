---
ver: rpa2
title: Adversarial Attacks on Cooperative Multi-agent Bandits
arxiv_id: '2311.01698'
source_url: https://arxiv.org/abs/2311.01698
tags:
- agents
- attack
- agent
- target
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores adversarial attacks in cooperative multi-agent
  bandit (CMA2B) systems, where multiple agents collaborate to minimize regret. In
  homogeneous settings, it proposes strategies that target just one agent to mislead
  all agents into selecting a suboptimal target arm, incurring sublinear attack costs
  of O(K log T), independent of the number of agents M.
---

# Adversarial Attacks on Cooperative Multi-agent Bandits

## Quick Facts
- arXiv ID: 2311.01698
- Source URL: https://arxiv.org/abs/2311.01698
- Reference count: 40
- One-line primary result: Sublinear attack costs achievable in homogeneous settings, but heterogeneous settings require targeting conflict-free agent subsets

## Executive Summary
This paper investigates adversarial attacks on cooperative multi-agent bandit (CMA2B) systems, where multiple agents collaborate to minimize regret. The authors demonstrate that in homogeneous settings, an attacker can target just one agent to mislead all agents into selecting a suboptimal arm with sublinear attack costs. In heterogeneous settings, they prove that sublinear-cost attacks targeting all agents are infeasible and instead propose strategies to maximize regret by manipulating only a few agents. Theoretical analysis and experiments validate these attack strategies, revealing significant vulnerabilities in CMA2B systems.

## Method Summary
The authors propose attack strategies for both homogeneous and heterogeneous cooperative multi-agent bandit settings. In homogeneous settings, they target a single agent's rewards to influence all agents' decisions. In heterogeneous settings, they use Affected Agents Selection (AAS) and Target Agents Selection (TAS) algorithms to identify conflict-free agent subsets for optimal attack targeting. They also develop a learning-then-attack strategy for unknown environments, where the attacker first learns the arm reward rankings before executing the attack.

## Key Results
- Sublinear attack costs of O(K log T) achievable in homogeneous settings by targeting single agent
- Heterogeneous settings require targeting conflict-free agent subsets due to exclusive optimal arm constraints
- Learning-then-attack strategy effective for unknown environments under Condition 2
- Theoretical analysis validated through experiments showing significant regret increase in attacked systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeting a single agent in homogeneous settings suffices to influence all agents' arm selections with sublinear attack cost.
- Mechanism: Attacker manipulates the reward of a single agent's non-target arm pulls so that the empirical mean of all non-target arms stays below that of the target arm by a margin of `2β(ˆnt(K)) + ∆0`. This manipulated information propagates to all agents via shared observations.
- Core assumption: All agents use the same arm set and cooperate by sharing reward observations in real time.
- Break condition: Heterogeneous arm sets or non-sharing communication break the propagation of corrupted observations.

### Mechanism 2
- Claim: In heterogeneous settings, targeting all agents with sublinear cost to induce linear regret is infeasible; instead, attacker identifies a conflict-free subset of agents to maximize regret.
- Mechanism: Conflict detection via Condition 1 ensures two agents can't both be misled with sublinear cost if their optimal arms are exclusive. AAS algorithm greedily selects the largest conflict-free set; TAS selects minimal target agents within that set.
- Core assumption: Attacker knows the local optimal arms and arm sets of all agents.
- Break condition: Insufficient overlap in arm sets to form a large conflict-free group.

### Mechanism 3
- Claim: Learning-then-attack strategy enables effective attacks even without prior knowledge of arm reward rankings.
- Mechanism: Attacker forces all arms to be pulled sufficiently often by manipulating UCB values until each arm reaches a threshold L; after learning the ranking, attacker executes oracle attack strategy.
- Core assumption: Attacker can observe all agents' reward observations and manipulate a subset of them.
- Break condition: If Condition 2 (arm accessibility) fails, learning phase cannot guarantee sufficient samples for all arms.

## Foundational Learning

- Concept: Hoeffding's inequality and concentration bounds for sub-Gaussian rewards
  - Why needed here: Provides probabilistic guarantees that empirical means are close to true means with high probability, forming the basis of confidence intervals in UCB algorithms.
  - Quick check question: What is the probability that |ˆµt(k) - µ(k)| > β(ˆnt(k)) when rewards are σ-sub-Gaussian?

- Concept: Submodular function optimization
  - Why needed here: AAS algorithm's greedy selection has a (1-1/e) approximation guarantee because the conflict-free agent selection function is submodular.
  - Quick check question: Why does the greedy algorithm provide a (1-1/e)-approximation for maximizing a submodular function under a cardinality constraint?

- Concept: Upper Confidence Bound (UCB) algorithms
  - Why needed here: Both CO-UCB and UCB-TCOM are attacked; understanding their decision rule (select arm with highest UCB) is essential for designing effective attacks.
  - Quick check question: How does the UCB index for arm k at time t depend on the empirical mean and number of pulls?

## Architecture Onboarding

- Component map: Attacker -> Manipulate reward -> Agent updates UCB -> Agent selects arm -> Share observation -> All agents update UCB -> Next round
- Critical path: Attacker observes pre-attack rewards, selects target agents, manipulates rewards to influence UCB calculations, agents select arms based on corrupted UCB values, share observations, and update UCB for next round
- Design tradeoffs:
  - Attack cost vs. number of agents influenced: Targeting one agent in homogeneous settings vs. multiple in heterogeneous
  - Learning phase duration vs. attack effectiveness: Longer learning ensures accurate ranking but delays attack
  - Target agent selection: Minimal targets reduce cost but may limit attack effectiveness
- Failure signatures:
  - Attack cost grows linearly with T instead of sublinearly
  - Agents' regret does not become linear despite attacks
  - Conflict detection incorrectly groups agents that can be simultaneously misled
- First 3 experiments:
  1. Homogeneous setting: Verify single-agent attack causes all agents to pull target arm T-o(T) times with O(K log T) cost
  2. Heterogeneous setting: Test AAS algorithm finds largest conflict-free agent set and TAS selects minimal targets
  3. Learning phase: Confirm attacker can learn correct arm ranking within KL/(cM) rounds under Condition 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal attack cost for adversarial attacks on cooperative multi-agent bandit systems?
- Basis in paper: [explicit] The authors state that there is no known lower bound of the attack cost even for the single-agent bandit setting in the literature, making it unclear whether their attack strategies are order-optimal.
- Why unresolved: The lack of a lower bound prevents comparison of the proposed attack strategies' efficiency against theoretical limits.
- What evidence would resolve it: Establishing a lower bound for attack costs in cooperative multi-agent bandit systems would allow researchers to determine the order-optimality of existing attack strategies.

### Open Question 2
- Question: How can cooperative multi-agent bandit algorithms be made resilient to adversarial attacks?
- Basis in paper: [explicit] The authors conclude that their insights into the vulnerabilities of current CMA2B algorithms set the stage for crafting algorithms resistant to adversarial attacks.
- Why unresolved: While the paper identifies vulnerabilities, it does not propose concrete methods to defend against these attacks or create robust algorithms.
- What evidence would resolve it: Developing and testing algorithms that can maintain low regret in the presence of adversarial attacks would demonstrate resilience against such threats.

### Open Question 3
- Question: How do adversarial attacks perform in cooperative multi-agent bandit systems with collisions?
- Basis in paper: [inferred] The authors mention that one limitation of their work is the lack of consideration for collisions, where multiple agents selecting the same arm in the same round may lead to different reward distributions.
- Why unresolved: The current attack strategies do not account for the complexities introduced by collisions, which could affect their effectiveness.
- What evidence would resolve it: Evaluating the performance of the proposed attack strategies in environments with collisions would reveal their robustness and potential need for adaptation.

## Limitations
- No consideration of collision scenarios where multiple agents selecting the same arm may lead to different reward distributions
- Assumes attacker has full knowledge of arm sets and optimal arms in heterogeneous settings
- Theoretical analysis relies on concentration inequalities that may not hold in all practical scenarios

## Confidence
- Homogeneous settings: High
- Heterogeneous settings: Medium
- Learning phase effectiveness: Medium-Low

## Next Checks
1. Empirical validation of attack costs: Run simulations with varying T and K to verify attack costs remain sublinear (O(K log T)) in both homogeneous and heterogeneous settings.
2. Robustness to communication delays: Test attack strategies under different communication delays between agents to assess impact on attack effectiveness and information propagation.
3. Generalization to non-sub-Gaussian rewards: Extend theoretical analysis to include non-sub-Gaussian reward distributions and evaluate robustness of attack strategies under these conditions.