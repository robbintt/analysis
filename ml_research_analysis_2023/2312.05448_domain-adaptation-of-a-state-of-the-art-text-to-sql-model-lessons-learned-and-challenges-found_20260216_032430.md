---
ver: rpa2
title: 'Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons Learned
  and Challenges Found'
arxiv_id: '2312.05448'
source_url: https://arxiv.org/abs/2312.05448
tags:
- queries
- picard
- spider
- dataset
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of two leading text-to-SQL
  models, T5 and Picard, on domain adaptation for independent databases. The authors
  analyze how these models handle queries with structures not present in the Spider
  dataset, such as string functions, CTEs, and multiple conditions.
---

# Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons Learned and Challenges Found

## Quick Facts
- arXiv ID: 2312.05448
- Source URL: https://arxiv.org/abs/2312.05448
- Reference count: 8
- The paper evaluates T5 and Picard models on domain adaptation for independent databases, showing that while Picard generally performs better, it struggles with queries containing SQL structures not seen during training.

## Executive Summary
This paper investigates the performance of state-of-the-art text-to-SQL models, T5 and Picard, when adapting to independent databases not included in the Spider dataset. The authors evaluate both zero-shot performance and domain-adapted performance after fine-tuning on the new databases. They identify significant challenges when queries contain SQL structures absent from the training data, such as string functions, CTEs, and multiple conditions. The study also introduces a rule-based approach for value disambiguation that avoids online database queries during inference.

## Method Summary
The authors fine-tune a pre-trained T5-Large model on the Spider dataset, then continue training on three independent databases (HR, WH, IN) for domain adaptation. They evaluate both zero-shot and domain-adapted performance using Exact Match and Execution Accuracy metrics. Special query datasets are created by modifying existing queries to include string functions, CTEs, and multiple conditions. The Picard model, which uses incremental parsing to constrain auto-regressive decoding, is also evaluated. A rule-based value disambiguation mechanism is implemented as an alternative to online database queries.

## Key Results
- Picard outperforms T5 on simple and complex queries when SQL structures are similar to those in Spider, but both models struggle with queries containing string functions, CTEs, and multiple conditions.
- Domain adaptation with a small corpus can improve performance for new databases when using pre-trained large models, confirming few-shot learning effectiveness.
- The rule-based approach for value disambiguation reduces the need for online database queries but was only tested on 50% of questions in the IN dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain adaptation with a small corpus can improve performance for new databases when using pre-trained large models.
- Mechanism: Fine-tuning the pre-trained model on the new database's data allows it to learn the specific schema and query patterns of the new domain.
- Core assumption: The pre-trained model has learned generalizable features from the Spider dataset that can be adapted to new domains.
- Evidence anchors:
  - [abstract]: "We empirically investigated and analyzed both the T5 and Picard models by evaluating them in a zero-shot setting and by fine-tuning the pre-trained Spider models on a set of independent schemas."
  - [section 4.2.2]: "Our results confirm previous studies (Lauscher et al., 2020) that have shown Few-shot learning as an effective strategy for the Text-to-SQL task for SQL queries using similar language elements than those in the Spider dataset."
- Break condition: If the new database has significantly different SQL structures than those in the Spider dataset, the pre-trained model may not be able to adapt effectively.

### Mechanism 2
- Claim: The Picard model improves performance over the base T5 model for simple and complex queries from simple and complex databases.
- Mechanism: Picard uses incremental parsing to constrain the auto-regressive decoding of the language model, which helps it generate more valid SQL queries.
- Core assumption: The parsing algorithm in Picard can effectively handle the SQL structures present in the queries.
- Evidence anchors:
  - [abstract]: "Picard is so far one of the top models on the Spider dataset, the largest cross-domain dataset for Tex-to-SQL."
  - [section 4.2.2]: "From this experiment we see that for the HR and IN DBs the Picard model improves over the T5-LS model, even though none of the models have domain adaptation on these DBs."
- Break condition: If the queries contain SQL structures not previously considered, such as string functions, CTEs, or multiple conditions, the Picard model may not be able to parse them correctly.

### Mechanism 3
- Claim: Using a rule-based approach to extract values from the input question and map them to database columns improves performance compared to online value search.
- Mechanism: The rule-based approach uses an intermediate representation of the semantic concepts in the question to identify the schema linking between values and table columns.
- Core assumption: The intermediate representation can accurately capture the semantic concepts and their relationships in the question.
- Evidence anchors:
  - [abstract]: "To avoid accessing the DB content online during inference, we also present an alternative way to disambiguate the values in an input question using a rule-based approach that relies on an intermediate representation of the semantic concepts of an input question."
  - [section A.2]: "The advantage of the ontology-based column-value mapping mechanism is that we do not need to query the DB multiple times searching for the table-column pairs for a given value in a question."
- Break condition: If the rule-based approach fails to accurately capture the semantic concepts or their relationships, it may lead to incorrect value mappings.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: To evaluate the performance of the models on new databases that are not included in the Spider dataset.
  - Quick check question: What is the difference between zero-shot learning and domain adaptation in the context of Text-to-SQL models?

- Concept: Incremental parsing
  - Why needed here: To understand how the Picard model constrains the auto-regressive decoding of the language model.
  - Quick check question: How does incremental parsing help in generating more valid SQL queries?

- Concept: Rule-based approach for value extraction
  - Why needed here: To understand the alternative mechanism proposed in the paper for mapping values in the question to database columns.
  - Quick check question: What are the advantages of using a rule-based approach compared to online value search?

## Architecture Onboarding

- Component map: Spider dataset -> T5-Large fine-tuning -> Independent databases (HR, WH, IN) -> Zero-shot/Domain-adapted evaluation -> EM/EA metrics
- Critical path: Fine-tune T5 on Spider → Continue training on independent DBs → Generate predictions → Evaluate with EM/EA metrics
- Design tradeoffs: Zero-shot inference vs. domain adaptation cost vs. performance gain; rule-based value extraction vs. online value search
- Failure signatures: Poor performance on queries with string functions, CTEs, multiple conditions; incomplete predictions from Picard when parsing complex structures
- First 3 experiments:
  1. Evaluate the pre-trained T5 and Picard models on the new databases in a zero-shot setting.
  2. Fine-tune the pre-trained models on the new databases and evaluate their performance.
  3. Compare the performance of the models with and without the rule-based value extraction mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Text-to-SQL models be improved to handle SQL queries with complex structures not seen in the training data, such as string functions, CTEs, and multiple conditions?
- Basis in paper: explicit
- Why unresolved: The paper shows that the current models struggle with these types of queries, and more diverse SQL structures need to be incorporated into larger datasets.
- What evidence would resolve it: Experiments with models trained on datasets that include a wider variety of SQL structures, and evaluation of their performance on unseen complex queries.

### Open Question 2
- Question: What are the limitations of current evaluation metrics for Text-to-SQL models, and how can they be improved to better assess model performance?
- Basis in paper: explicit
- Why unresolved: The paper identifies issues with the current evaluation metrics, such as the exact match metric failing to correctly process SQL queries with certain structures.
- What evidence would resolve it: Development and testing of new evaluation metrics that can accurately assess model performance on a wider range of SQL query structures.

### Open Question 3
- Question: How can domain adaptation for Text-to-SQL models be improved to handle new databases with minimal training data?
- Basis in paper: explicit
- Why unresolved: The paper shows that domain adaptation can achieve good results with a small amount of training data, but there is still room for improvement, especially for complex queries.
- What evidence would resolve it: Experiments with different domain adaptation techniques, such as few-shot learning or meta-learning, and evaluation of their performance on new databases with limited training data.

## Limitations

- The evaluation relies on synthetic query modifications that may not reflect real-world query distributions, particularly for string functions, CTEs, and multiple conditions.
- The rule-based value disambiguation system has limited generalizability, having been tested on only 50% of questions in the IN dataset.
- The comparison between zero-shot and domain-adapted models is complicated by the fact that pre-trained models were already fine-tuned on Spider.

## Confidence

- **High Confidence**: The observation that both T5 and Picard models perform poorly on SQL structures not present in the Spider dataset (string functions, CTEs, multiple conditions).
- **Medium Confidence**: The claim that Picard generally outperforms T5 on simple and complex queries when SQL structures are similar to those in Spider.
- **Low Confidence**: The assertion that rule-based value disambiguation consistently outperforms online value search.

## Next Checks

1. **Validation Check 1**: Conduct a systematic ablation study comparing rule-based value disambiguation against online value search across all three independent databases (HR, WH, IN) using the same test queries to quantify the performance difference and identify failure cases.

2. **Validation Check 2**: Evaluate model performance on naturally occurring queries from the independent databases rather than synthetically modified queries to assess whether the identified failure patterns (string functions, CTEs, multiple conditions) represent common real-world scenarios.

3. **Validation Check 3**: Measure the computational resources and time required for domain adaptation on each independent database to provide practitioners with practical guidance on when domain adaptation is cost-effective versus using zero-shot inference.