---
ver: rpa2
title: 'Artificial Neuropsychology: Are Large Language Models Developing Executive
  Functions?'
arxiv_id: '2305.04134'
source_url: https://arxiv.org/abs/2305.04134
tags:
- executive
- functions
- tasks
- task
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores whether large language models like GPT are developing
  executive functions similar to those in humans. To investigate this, the researchers
  evaluated the planning and working memory of GPT models using the Towers of Hanoi
  method.
---

# Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?

## Quick Facts
- arXiv ID: 2305.04134
- Source URL: https://arxiv.org/abs/2305.04134
- Reference count: 12
- One-line primary result: GPT models demonstrate planning and working memory abilities in structured tasks but struggle with generalization to unfamiliar variants.

## Executive Summary
This study investigates whether large language models (LLMs) like GPT are developing executive functions similar to those in humans. Using the Towers of Hanoi method and a newly introduced variant called Water Filtering Platforms, researchers evaluated GPT models' planning and working memory capabilities. Results showed that GPT models generated near-optimal solutions and adhered to task constraints, suggesting potential development of executive functions. However, these abilities were limited and inferior to well-trained humans when tasks were unfamiliar. The study highlights the progress of AI in acquiring complex cognitive abilities and calls for further research on the cognitive capabilities of large language models and their implications for artificial neuropsychology.

## Method Summary
The researchers evaluated GPT models using the Towers of Hanoi (ToH) puzzle and a new variant called Water Filtering Platforms (WFP) to avoid data leakage. They used zero-shot prompting with ChatGPT to generate solutions for both tasks. Three GPT models (GPT-3.5 Legacy, GPT-3.5, and GPT-4) were tested on ToH with three disks and three pegs, and on WFP with three holes and three platforms. The evaluation measured solution optimality, planning efficiency, working memory load, rule adherence, and task-specific metrics like water collected and potability. Human programmers provided baseline comparisons for performance.

## Key Results
- GPT models generated near-optimal solutions for Towers of Hanoi tasks
- Models adhered to task constraints without placing larger disks on smaller ones
- Performance declined significantly on the Water Filtering Platforms variant when surface semantics differed from training patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate near-optimal solutions to Towers of Hanoi because they can simulate future states using self-attention to track disk configurations across multiple steps.
- Mechanism: The Transformer architecture uses multi-head self-attention to build contextual representations of the board state at each move. This allows the model to reason about valid transitions and avoid rule violations by attending to relevant disk-peg combinations.
- Core assumption: Self-attention can emulate the working memory and planning functions needed for multi-step reasoning in structured puzzles.
- Evidence anchors:
  - [abstract] "exhibits rapid planning capabilities and efficient working memory usage"
  - [section 3] "GPT models to efficiently process and generate sequences by attending to different parts of the input text"
  - [corpus] No direct evidence of attention-based reasoning in puzzles; only general architectural description.
- Break condition: If the puzzle requires more complex planning than can be captured in the fixed context window, or if attention weights fail to disambiguate between valid and invalid moves.

### Mechanism 2
- Claim: The model adheres to task constraints (no larger disk on smaller) because it has learned the relational inductive biases described in Battaglia et al. during pretraining.
- Mechanism: Through exposure to relational data in pretraining, the model internalizes rules like "larger objects cannot be placed on smaller ones" and applies them during inference without explicit programming.
- Core assumption: Pretraining on large text corpora includes sufficient examples of hierarchical or size-based constraints to induce general relational rules.
- Evidence anchors:
  - [abstract] "adheres to task constraints"
  - [section 6] "GPT consistently adhered to the ToH task constraints, with no instances of larger disks being placed on top of smaller ones"
  - [corpus] Weak: general mention of relational inductive biases but no puzzle-specific evidence.
- Break condition: If the constraint is not represented in pretraining data or if the model overfits to specific puzzle configurations.

### Mechanism 3
- Claim: Poor performance on the Water Filtering Platforms variant indicates that LLMs struggle with task generalization when surface semantics differ but underlying logic is similar.
- Mechanism: The model relies on pattern matching from pretraining; when the prompt does not match known patterns (even if the underlying logic is equivalent), it fails to transfer reasoning strategies.
- Core assumption: LLMs use surface-level similarity rather than deep structural equivalence to apply learned strategies.
- Evidence anchors:
  - [section 5] "the ToH problem is very popular" and "likely that large language models (LLMs) have encountered solutions to it in their training data"
  - [section 6] "GPT-4, upon learning of the hint, developed a solution that caused breaks in the platforms every time"
  - [corpus] Weak: no corpus studies on transfer learning in puzzle-solving.
- Break condition: If the model is explicitly told the underlying equivalence or if it receives fine-tuning on diverse variants.

## Foundational Learning

- Concept: Executive Functions (EFs) in neuropsychology
  - Why needed here: The paper frames LLM evaluation in terms of human cognitive constructs; understanding EFs is essential to interpret the results.
  - Quick check question: What are the three core components of executive functions as defined in the paper?
- Concept: Towers of Hanoi as a planning and working memory task
  - Why needed here: The experimental design uses ToH to measure EF-like behavior; knowing the task structure is key to understanding what is being tested.
  - Quick check question: In ToH, what is the constraint that must be maintained when moving disks?
- Concept: Data leakage and its impact on LLM evaluation
  - Why needed here: The paper introduces a new task variant to avoid data leakage; understanding this concept is critical for interpreting the fairness of the evaluation.
  - Quick check question: Why did the authors create the Water Filtering Platforms variant instead of using the classic ToH task?

## Architecture Onboarding

- Component map: Prompt -> Attention layers -> Next token prediction -> Output parsing -> Evaluation
- Critical path: Prompt → Attention layers → Next token prediction → Output parsing → Evaluation
- Design tradeoffs: Zero-shot evaluation avoids compositional gaps but limits control over model behavior; conversational format enables natural interaction but may introduce ambiguity.
- Failure signatures: Rule violations (larger on smaller), sub-optimal move counts, hallucinations (fabricated steps), failure to adapt to new task semantics.
- First 3 experiments:
  1. Replicate ToH task with varying disk counts to test scalability of planning.
  2. Introduce hint-free WFP variant to test transfer without semantic cues.
  3. Use step-by-step prompting to isolate working memory load from planning efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do GPT models exhibit planning and working memory abilities similar to humans when performing tasks not seen during training?
- Basis in paper: [explicit] The paper states that GPT models show limited and worse abilities than well-trained humans when tasks are unfamiliar and not part of the training data.
- Why unresolved: The study only tested GPT models on a limited set of tasks and did not explore their performance on a broader range of unfamiliar tasks.
- What evidence would resolve it: Testing GPT models on a wider variety of tasks not seen during training and comparing their performance to well-trained humans.

### Open Question 2
- Question: How do the underlying mechanisms of GPT models, such as self-attention, contribute to the development of executive functions like planning and working memory?
- Basis in paper: [inferred] The paper mentions that GPT models utilize self-attention mechanisms within a multi-layer Transformer structure, which allows them to efficiently process and generate sequences by attending to different parts of the input text.
- Why unresolved: The study does not investigate the specific role of self-attention or other mechanisms in the development of executive functions in GPT models.
- What evidence would resolve it: Analyzing the internal workings of GPT models, such as self-attention patterns and activations, during task performance to identify correlations with planning and working memory abilities.

### Open Question 3
- Question: Can the executive functions developed by GPT models be transferred to other AI domains, such as reinforcement learning and robotics?
- Basis in paper: [explicit] The paper suggests that investigating the transferability of executive functions from GPT models to other AI domains could have significant implications for the design of more adaptable and intelligent artificial systems.
- Why unresolved: The study does not explore the potential transfer of executive functions to other AI domains.
- What evidence would resolve it: Applying GPT models with developed executive functions to tasks in reinforcement learning and robotics and evaluating their performance compared to models without such functions.

## Limitations

- The study's findings are limited by the narrow task scope and lack of ablation studies to isolate specific mechanisms
- The Water Filtering Platforms variant may not fully isolate transfer learning from semantic confusion
- Evaluation relies on self-reported model outputs without independent verification of intermediate reasoning steps

## Confidence

- **High confidence**: LLMs can solve Towers of Hanoi with near-optimal solutions when task structure matches pretraining patterns
- **Medium confidence**: LLMs demonstrate planning and working memory capabilities through attention mechanisms and relational reasoning
- **Low confidence**: LLMs can generalize executive function reasoning across semantically different but structurally similar tasks

## Next Checks

1. Conduct ablation studies by systematically removing self-attention layers to quantify their contribution to planning performance
2. Implement step-by-step prompting to capture intermediate reasoning states and verify working memory usage
3. Test transfer learning with structurally equivalent but superficially different puzzles to distinguish pattern matching from genuine generalization