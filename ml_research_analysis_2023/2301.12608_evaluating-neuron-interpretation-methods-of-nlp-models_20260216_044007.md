---
ver: rpa2
title: Evaluating Neuron Interpretation Methods of NLP Models
arxiv_id: '2301.12608'
source_url: https://arxiv.org/abs/2301.12608
tags:
- methods
- neurons
- neuron
- interpretation
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of evaluating neuron interpretation
  methods in NLP models, which currently lack standardized benchmarks and metrics.
  The authors propose a voting-based compatibility framework where each interpretation
  method votes for neurons based on their relevance to a concept, and the compatibility
  score measures how well a method's rankings align with others.
---

# Evaluating Neuron Interpretation Methods of NLP Models

## Quick Facts
- arXiv ID: 2301.12608
- Source URL: https://arxiv.org/abs/2301.12608
- Reference count: 23
- Primary result: Proposes a voting-based compatibility framework for evaluating neuron interpretation methods in NLP models, showing Probeless consistently outperforms other methods across layers and concepts.

## Executive Summary
This paper addresses the lack of standardized evaluation methods for neuron interpretation techniques in NLP models. The authors propose a voting-based compatibility framework where each interpretation method ranks neurons for specific concepts, and the compatibility score measures how well these rankings align with other methods. Experiments across three pre-trained models (BERT, RoBERTa, XLM-R) and 20 linguistic concepts demonstrate that Probeless consistently achieves the highest compatibility scores, while other methods like LCA and Lasso show degradation in higher layers. The framework provides both comparative insights and a practical methodology for evaluating novel neuron interpretation approaches.

## Method Summary
The authors introduce a voting-based compatibility framework that treats each neuron interpretation method as a voter ranking neurons by their relevance to specific concepts. They propose two metrics: AvgOverlap (average neuron overlap) and NeuronVote (weighted Borda count), along with pairwise comparison analysis. Six interpretation methods are evaluated: Probeless, IoU method, Lasso, Ridge, Elastic Net (LCA), and Gaussian classifier. The framework uses a leave-one-out strategy to calculate compatibility scores, measuring how well each method's neuron rankings align with the consensus of other methods. Experiments are conducted across three pre-trained models using Penn TreeBank POS tags as concepts.

## Key Results
- Probeless consistently achieves the highest compatibility scores across all layers and models, outperforming other methods
- LCA and Lasso methods show substantial performance degradation in higher layers while maintaining good performance in lower layers
- Gaussian method exhibits the lowest compatibility scores, suggesting potential methodological concerns with the approach
- The framework successfully distinguishes between effective and less effective interpretation methods through voting-based consensus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compatibility metrics identify robust neuron interpretation methods by measuring overlap between different methods' neuron rankings.
- Mechanism: The framework treats each neuron interpretation method as a voter that ranks neurons based on their relevance to a concept. Methods that select neurons overlapping with other methods' selections are considered more reliable because they capture consistently important patterns across different analytical approaches.
- Core assumption: Neurons that are commonly discovered by different interpretation methods are more informative and represent genuine concept representations rather than methodological artifacts.
- Evidence anchors:
  - [abstract]: "We hypothesize that the more compatible a method is with the majority of the methods, the more confident one can be about its performance."
  - [section]: "We consider each neuron interpretation method as a voter that votes for every neuron with respect to a concept."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.345, average citations=0.0.
- Break condition: If different methods systematically disagree due to fundamental differences in how they capture concept representations, high compatibility might mask important methodological diversity.

### Mechanism 2
- Claim: Pairwise comparison reveals relationships between different interpretation methods beyond overall compatibility scores.
- Mechanism: By calculating the intersection between neurons selected by any two methods, researchers can identify which methods select similar neurons and which capture distinct aspects of concept representations. This reveals methodological similarities and differences that compatibility scores alone cannot show.
- Core assumption: The overlap patterns between methods reflect their underlying capture of concept representations, with high overlap indicating similar approaches to identifying relevant neurons.
- Evidence anchors:
  - [section]: "The pair-wise comparison is calculated as an intersection between the output of two methods."
  - [section]: "LCA and Lasso showed a high overlap in the discovered neurons with at most 64% overlap."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.345, average citations=0.0.
- Break condition: If methods are designed to capture fundamentally different aspects of representations, overlap might not indicate methodological similarity but rather complementary approaches.

### Mechanism 3
- Claim: Layer-wise analysis reveals how different interpretation methods perform across model architectures and representation geometries.
- Mechanism: The framework evaluates methods across different layers of pre-trained models, revealing that some methods (like Probeless) maintain consistent performance while others (like LCA and Lasso) degrade in higher layers. This indicates how representation geometry affects different analytical approaches.
- Core assumption: The geometry of representation space varies across layers, affecting how different methods can identify relevant neurons for concept understanding.
- Evidence anchors:
  - [section]: "LCA and Lasso both showed a substantial drop in their compatibility scores for the 12th layer."
  - [section]: "In contrast, Probeless did not show any substantial difference in the scores of the last layers and earlier layers."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.345, average citations=0.0.
- Break condition: If representation geometry changes are not the primary factor affecting method performance, layer-wise trends might reflect other architectural or training-related factors.

## Foundational Learning

- Concept: Voting theory and Borda count
  - Why needed here: The evaluation framework relies on voting-based metrics to aggregate neuron rankings from different methods and identify consensus patterns.
  - Quick check question: How does Borda count differ from simple plurality voting in aggregating rankings, and why is this important for neuron interpretation evaluation?

- Concept: Intersection over Union (IoU) for set comparison
  - Why needed here: The compatibility metrics use IoU to measure overlap between sets of neurons selected by different methods, providing a quantitative measure of agreement.
  - Quick check question: Why is IoU a better measure than simple intersection size for comparing neuron sets across methods with different total neuron counts?

- Concept: Regularization techniques (L1, L2, Elastic Net)
  - Why needed here: Understanding how different regularization approaches affect neuron selection is crucial for interpreting why methods like Lasso, Ridge, and LCA produce different results.
  - Quick check question: How do L1 and L2 regularization differ in their tendency to select sparse versus grouped features, and how might this affect their performance across different layers?

## Architecture Onboarding

- Component map: neuron interpretation methods -> compatibility metrics (AvgOverlap, NeuronVote) -> pairwise comparison analysis
- Critical path: To evaluate a new neuron interpretation method, run it across multiple layers of pre-trained models, calculate compatibility scores against existing methods using both AvgOverlap and NeuronVote, then perform pairwise comparisons to understand its relationship to other approaches.
- Design tradeoffs: The framework trades precision for generality by using voting-based metrics instead of ground truth annotations, which makes it applicable across different models and concepts but may miss method-specific strengths or weaknesses.
- Failure signatures: Low compatibility scores across all methods might indicate methodological issues with the new approach, while high compatibility with only specific methods might suggest methodological similarity that could bias results.
- First 3 experiments:
  1. Run the new method across all layers of BERT using 20 linguistic concepts and calculate AvgOverlap scores against existing methods.
  2. Calculate NeuronVote scores for the same runs to get a different perspective on compatibility.
  3. Perform pairwise comparisons between the new method and each existing method to identify specific relationships and potential methodological overlaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the proposed compatibility metrics (AvgOverlap and NeuronVote) to different choices of hyperparameters, such as the number of top neurons selected or the random concept set used in Probeless?
- Basis in paper: [explicit] The authors mention using top 10, 30, and 50 neurons for evaluation, but do not explore how the compatibility scores change with different numbers of neurons.
- Why unresolved: The paper does not provide a sensitivity analysis of the compatibility metrics to hyperparameter choices, leaving open the question of how stable the method rankings are across different settings.
- What evidence would resolve it: Experiments showing the compatibility scores for a range of top-k values (e.g., 5-100) and different random concept set sizes would demonstrate the robustness of the metrics.

### Open Question 2
- Question: Can the proposed voting-based framework be extended to incorporate additional information beyond neuron rankings, such as neuron activation magnitudes or pairwise neuron interactions?
- Basis in paper: [inferred] The current framework only considers the rankings of neurons, but does not utilize other potentially informative signals like activation strengths.
- Why unresolved: The paper focuses solely on ranking-based compatibility, leaving open the question of whether incorporating additional information could improve the evaluation of neuron interpretation methods.
- What evidence would resolve it: Developing and evaluating extended versions of the voting framework that incorporate activation magnitudes or pairwise interactions, and comparing their performance to the current ranking-based approach.

### Open Question 3
- Question: How do the proposed compatibility metrics compare to alternative evaluation approaches, such as those based on probing classifiers or downstream task performance?
- Basis in paper: [explicit] The authors argue that classifier-based metrics are flawed due to methodological similarity and memorization issues, but do not directly compare their proposed metrics to these alternatives.
- Why unresolved: The paper does not provide a direct comparison between the compatibility metrics and other evaluation approaches, leaving open the question of their relative effectiveness.
- What evidence would resolve it: Conducting experiments that compare the rankings of neuron interpretation methods using the proposed compatibility metrics, probing classifiers, and downstream task performance would provide insights into their relative strengths and weaknesses.

## Limitations

- Absence of ground-truth annotations for neuron-concept relationships makes the framework an indirect evaluation method
- Reliance on POS tags as concepts provides a structured but potentially limited view of what neurons represent
- Hyperparameter sensitivity of regularization-based methods is not fully explored
- Gaussian method's assumption of multivariate normality may not hold across all layers or concepts

## Confidence

- High Confidence: The comparative analysis showing Probeless achieving highest compatibility scores is well-supported by systematic experiments across multiple models and concepts.
- Medium Confidence: The layer-wise performance degradation of LCA and Lasso in higher layers is observed but requires additional validation to rule out confounding factors beyond representation geometry changes.
- Medium Confidence: The framework's utility for evaluating new methods is demonstrated through the MeanSelect case study, though the method's specific design choices may have influenced its favorable performance.

## Next Checks

1. Conduct ablation studies on regularization hyperparameters (Î» values) for Lasso, Ridge, and LCA methods to determine sensitivity and optimize performance across different layers.
2. Test the framework's robustness by evaluating it on a more diverse set of concepts beyond POS tags, including semantic and syntactic phenomena that may reveal different neuron interpretation patterns.
3. Implement a human evaluation component where linguists assess the interpretability of neurons selected by different methods for key concepts, providing ground-truth validation for the compatibility framework's rankings.