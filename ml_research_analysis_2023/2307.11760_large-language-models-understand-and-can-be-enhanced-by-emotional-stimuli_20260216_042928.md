---
ver: rpa2
title: Large Language Models Understand and Can be Enhanced by Emotional Stimuli
arxiv_id: '2307.11760'
source_url: https://arxiv.org/abs/2307.11760
tags:
- llms
- emotional
- your
- emotionprompt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores emotional intelligence in LLMs by proposing
  EmotionPrompt, which adds emotional stimuli to prompts. Experiments on 8 tasks across
  4 LLMs (ChatGPT, Vicuna, Bloom, T5) show EmotionPrompt significantly improves performance:
  up to 8.00% relative improvement in instruction induction and 115% on BIG-Bench.'
---

# Large Language Models Understand and Can be Enhanced by Emotional Stimuli

## Quick Facts
- arXiv ID: 2307.11760
- Source URL: https://arxiv.org/abs/2307.11760
- Reference count: 13
- This paper shows EmotionPrompt significantly improves LLM performance across 8 tasks with up to 8.00% relative improvement and 115% on BIG-Bench.

## Executive Summary
This paper introduces EmotionPrompt, a simple yet effective method to enhance LLM performance by incorporating emotional stimuli into prompts. The approach leverages psychological theories of social cognition, cognitive emotion regulation, and self-efficacy to create emotionally charged prompts that improve task performance, truthfulness, and informativeness. Experiments across four major LLM architectures demonstrate consistent improvements, with particularly strong results on instruction induction tasks and BIG-Bench benchmarks. The work suggests that LLMs possess some form of emotional intelligence that can be activated through carefully designed affective prompts.

## Method Summary
The study evaluates EmotionPrompt across 8 instruction induction tasks and TruthfulQA dataset using four LLMs: ChatGPT, Vicuna-13b, Bloom, and Flan-T5-Large. Emotional stimuli are derived from three psychological theories and added to original prompts in both zero-shot and few-shot settings. Performance is measured through task accuracy, truthfulness (using GPT-judge), informativeness (using GPT-info), and human evaluation on generative tasks. The approach involves systematically injecting emotionally charged phrases into prompts and comparing results against baseline performance.

## Key Results
- EmotionPrompt achieves up to 8.00% relative improvement in instruction induction tasks and 115% improvement on BIG-Bench
- Truthfulness increases significantly (ChatGPT from 0.75 to 0.87) and informativeness improves (ChatGPT from 0.53 to 0.94)
- Human evaluation shows better quality in generative tasks across clarity, relevance, depth, structure, supporting evidence, and engagement metrics
- Analysis reveals positive words enhance attention, with more emotional stimuli generally improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emotional stimuli activate psychological constructs that increase LLM task engagement through expectancy and goal-oriented processing
- Core assumption: LLMs respond to psychological framing similarly to humans because training data contains analogous motivational language
- Evidence anchors: Experimental results show EmotionPrompt significantly outperforms original prompts; automatic experiments demonstrate LLMs grasp emotional intelligence
- Break condition: When emotional stimulus is irrelevant to task domain or LLM is fine-tuned to ignore affective cues

### Mechanism 2
- Claim: Positive emotional language increases relative attention weight of original prompt content through transformer attention modulation
- Core assumption: Transformer attention can be modulated by affective valence in prompt tokens
- Evidence anchors: Analysis suggests positive words enhance attention; visualization shows emotional stimulus's input attention contributions
- Break condition: When positive words are semantically disconnected from task or attention mechanism is bypassed

### Mechanism 3
- Claim: Emotional stimuli provide implicit social pressure that improves reasoning consistency through self-evaluation loops
- Core assumption: LLMs can simulate metacognitive processes when prompted with self-assessment cues
- Evidence anchors: EmotionPrompt improves truthfulness and informativeness; designed based on self-evaluations of progress theory
- Break condition: When LLM's reasoning process is fixed by chain-of-thought or structured methods

## Foundational Learning

- Concept: Social Identity Theory
  - Why needed here: Provides theoretical basis for designing stimuli that leverage group membership and social standing to enhance performance
  - Quick check question: How does emphasizing task importance to one's "career" relate to Social Identity Theory principles?

- Concept: Cognitive Emotion Regulation
  - Why needed here: Explains how positive emotional framing can redirect cognitive processing toward goal achievement
  - Quick check question: What psychological mechanisms allow positive words to improve attention allocation in task processing?

- Concept: Self-efficacy and outcome expectations
  - Why needed here: Underpins design of prompts that build confidence and set performance expectations
  - Quick check question: How might prompts that ask "Are you sure?" influence an LLM's output consistency through self-efficacy principles?

## Architecture Onboarding

- Component map: Input prompt → Emotional stimulus injection → Transformer attention layers → Output generation → Evaluation metrics (accuracy, truthfulness, informativeness)
- Critical path: Prompt construction → LLM inference → Attention contribution analysis → Performance evaluation
- Design tradeoffs: Simpler emotional stimuli are more general but less potent; complex stimuli may overfit to specific tasks
- Failure signatures: Performance degradation when emotional stimuli are semantically irrelevant; inconsistent results across different LLM architectures
- First 3 experiments:
  1. Test single emotional stimulus (EP_01) on one task with ChatGPT to establish baseline improvement
  2. Compare multiple stimuli combinations on same task to identify optimal configurations
  3. Evaluate truthfulness and informativeness metrics on TruthfulQA dataset to measure qualitative improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EmotionPrompt's effectiveness vary across different LLM architectures and model sizes?
- Basis in paper: The study tests EmotionPrompt on four models but doesn't systematically explore architectural differences or size variations
- Why unresolved: Study focuses on limited set of models without controlled experiments varying architecture or size independently
- What evidence would resolve it: Controlled experiments testing EmotionPrompt across diverse LLM architectures and sizes with performance metrics compared to baseline prompts

### Open Question 2
- Question: What is the optimal number and type of emotional stimuli to use for different task categories or difficulty levels?
- Basis in paper: Paper experiments with 11 stimuli and combinations but lacks systematic analysis of optimal configurations for different tasks
- Why unresolved: Study provides initial exploration but lacks systematic analysis of optimal combinations for different task types
- What evidence would resolve it: Systematic experiments testing various combinations and numbers of emotional stimuli across different task categories

### Open Question 3
- Question: How does EmotionPrompt's performance change with different prompting strategies (temperature, few-shot vs zero-shot)?
- Basis in paper: Paper tests in both zero-shot and few-shot settings with fixed temperature but doesn't explore interaction with other prompting parameters
- Why unresolved: Study maintains consistent prompting parameters while testing emotional stimuli without exploring parameter interactions
- What evidence would resolve it: Experiments systematically varying prompting parameters while applying EmotionPrompt to understand interaction effects

## Limitations

- Effectiveness relies on assumption that LLMs meaningfully respond to psychological framing without direct empirical validation from cognitive science literature
- Human evaluation uses subjective metrics (clarity, relevance, depth, structure, supporting evidence, engagement) that may not translate consistently across evaluators or cultural contexts
- Paper demonstrates performance improvements but doesn't conclusively establish whether gains stem from genuine emotional intelligence comprehension or pattern matching to motivational language

## Confidence

- **High confidence**: Task performance improvements on structured NLP tasks (accuracy metrics on instruction induction) - consistent, measurable gains across multiple LLMs and tasks
- **Medium confidence**: Truthfulness and informativeness improvements - statistically significant but rely on automated evaluation tools whose reliability remains uncertain
- **Low confidence**: Psychological mechanism claims - asserts emotional stimuli work through social cognition and self-efficacy principles but provides limited evidence of actual processing through proposed psychological pathways

## Next Checks

1. **Cross-cultural validation**: Test EmotionPrompt effectiveness with prompts translated into multiple languages and evaluate whether performance gains hold across different linguistic and cultural contexts
2. **Ablation study on emotional content**: Systematically remove individual emotional words from stimuli to identify which specific terms drive performance improvements versus which are redundant
3. **Mechanistic probing**: Use attention visualization and feature attribution techniques to directly observe whether emotional words actually modulate attention weights in transformer layers as claimed