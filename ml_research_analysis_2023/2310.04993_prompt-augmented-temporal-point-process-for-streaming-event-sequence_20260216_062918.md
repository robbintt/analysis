---
ver: rpa2
title: Prompt-augmented Temporal Point Process for Streaming Event Sequence
arxiv_id: '2310.04993'
source_url: https://arxiv.org/abs/2310.04993
tags:
- event
- prompt
- time
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling streaming event
  sequences in the context of Continual Learning (CL). Traditional methods struggle
  with distribution shifts and catastrophic forgetting when new data arrives.
---

# Prompt-augmented Temporal Point Process for Streaming Event Sequence

## Quick Facts
- **arXiv ID**: 2310.04993
- **Source URL**: https://arxiv.org/abs/2310.04993
- **Reference count**: 40
- **Primary result**: PromptTPP framework achieves state-of-the-art performance on continual learning for streaming event sequences by integrating base TPP models with a continuous-time retrieval prompt pool

## Executive Summary
This paper addresses the challenge of modeling streaming event sequences in continual learning settings, where traditional methods struggle with distribution shifts and catastrophic forgetting. The authors propose PromptTPP, a novel framework that augments temporal point process models with a continuous-time retrieval prompt pool. This approach enables task-agnostic learning without requiring task-specific information during inference. Experiments on three real-world datasets demonstrate that PromptTPP consistently outperforms state-of-the-art methods, achieving significant improvements in both error rate and time RMSE metrics.

## Method Summary
PromptTPP integrates a base temporal point process (TPP) model with a continuous-time retrieval prompt pool that stores learned knowledge as learnable key-value paired prompts. During training, the model learns tasks sequentially without buffering past examples. The retrieval mechanism selects top-N most relevant prompts based on input event embeddings using maximum inner product search. These prompts are then prepended to event representations via multi-head self-attention, allowing the model to condition predictions on both current and past task knowledge. The framework also includes temporal prompts that incorporate time-varying positional encodings and an asynchronous refresh mechanism to accelerate training by reducing prompt update frequency.

## Key Results
- PromptTPP consistently outperforms state-of-the-art methods on three real-world datasets (Taobao, Amazon, StackOverflow)
- Achieves significant improvements in both error rate and time RMSE metrics
- Demonstrates effective task-agnostic learning without catastrophic forgetting
- Asynchronous refresh with C=2 provides comparable performance while accelerating training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The continuous-time retrieval prompt pool enables task-agnostic learning by storing task-specific and shared knowledge separately in key-value paired prompts, avoiding catastrophic forgetting.
- **Mechanism**: Prompts are parameterized as learnable keys and values. During inference, a retrieval mechanism selects the top-N most relevant prompts based on the input event embedding using maximum inner product search (MIPS). These prompts are then prepended to the event representations via multi-head self-attention, allowing the model to condition its predictions on both current and past task knowledge.
- **Core assumption**: Task boundaries are known at training time but task identity is unknown at test time; prompts can encode sufficient task-specific and shared knowledge without needing raw historical data.
- **Evidence anchors**:
  - [abstract]: "The prompts, small learnable parameters, are stored in a memory space and jointly optimized with the base TPP, ensuring that the model learns event streams sequentially without buffering past examples or task-specific attributes."
  - [section]: "The retrieval prompt pool shares some design principles with methods in other fields... The input sequence itself can decide which prompts to choose through query-key matching."
- **Break condition**: If task boundaries are not clear or the distribution shift is too large, the retrieval mechanism may fail to select appropriate prompts, leading to degraded performance.

### Mechanism 2
- **Claim**: Temporal prompts, which incorporate time-varying positional encodings, improve prediction accuracy by encoding the temporal dynamics of event sequences more effectively than static prompts.
- **Mechanism**: The temporal component of each prompt is computed using continuous-time positional encodings based on the estimated conditional time of the event. This allows the prompt to capture the timing information of events, which is crucial for accurate predictions in continuous-time settings.
- **Core assumption**: The timing information of events is important for accurate predictions, and the estimated conditional time can effectively represent the temporal dynamics of the event sequence.
- **Evidence anchors**:
  - [abstract]: "Specifically, we develop a module of temporal prompt that learns knowledge and further store the learned knowledge for event sequences in continuous time."
  - [section]: "Given i-th event, we estimate the arithmetic mean of inter-event times up to ti−1, denoted by Ei = E[{τj}j<i] and add this estimated inter-event time to ti−1 to get the estimated conditional time tp := bti = ti−1 + Ei."
- **Break condition**: If the estimated conditional time is not a good representation of the temporal dynamics, or if the timing information is not important for the specific task, the temporal prompts may not provide significant improvement.

### Mechanism 3
- **Claim**: Asynchronous refresh of the prompt pool accelerates training without sacrificing performance by reducing the frequency of prompt updates.
- **Mechanism**: The prompt pool is updated every C training epochs, where C is a hyperparameter. This allows the model to focus on updating the base TPP parameters more frequently, while the prompts are updated less frequently, reducing the overall computational cost.
- **Core assumption**: Updating the prompts less frequently does not significantly impact the model's ability to learn and adapt to new tasks, and the computational savings outweigh any potential loss in performance.
- **Evidence anchors**:
  - [section]: "To accelerate training, we propose to asynchronously update all embeddings in the prompt pool every C training epochs."
  - [section]: "We observe in Figure 7a that Taobao training with C = 2 has a comparable performance with C = 1 while Amazon training with C = 2 improves the convergence notably."
- **Break condition**: If the distribution shift between tasks is too large, or if the prompts are not updated frequently enough, the model may fail to adapt to new tasks effectively.

## Foundational Learning

- **Concept**: Temporal Point Processes (TPPs)
  - **Why needed here**: TPPs are the mathematical framework for modeling continuous-time event sequences, which is the core problem addressed in this paper.
  - **Quick check question**: What is the difference between a discrete-time and a continuous-time event sequence?

- **Concept**: Continual Learning (CL)
  - **Why needed here**: CL is the learning paradigm that enables the model to learn from a sequence of tasks without forgetting previous knowledge, which is essential for handling streaming event data.
  - **Quick check question**: What is the main challenge in continual learning, and how does it differ from traditional machine learning?

- **Concept**: Prompt Learning
  - **Why needed here**: Prompt learning is used to condition the model on task-specific and shared knowledge, allowing it to adapt to new tasks without catastrophic forgetting.
  - **Quick check question**: How does prompt learning differ from fine-tuning, and what are its advantages in the context of continual learning?

## Architecture Onboarding

- **Component map**: Input event sequence -> Base TPP encoder -> Prompt retrieval -> Prompt-event interaction -> Intensity prediction -> Loss computation and optimization
- **Critical path**: Input event sequence → Base TPP encoder → Prompt retrieval → Prompt-event interaction → Intensity prediction → Loss computation and optimization
- **Design tradeoffs**:
  - Prompt length (Lp) vs. model capacity and computational cost
  - Selection size (N) vs. prompt relevance and model performance
  - Prompt pool size (M) vs. model flexibility and memory usage
  - Asynchronous refresh frequency (C) vs. training speed and model performance
- **Failure signatures**:
  - Poor performance on new tasks: prompts may not be updated frequently enough or may not capture the necessary task-specific knowledge
  - Catastrophic forgetting: prompts may not effectively separate task-specific and shared knowledge, leading to interference between tasks
  - Slow convergence: prompts may be updated too frequently, causing the model to focus on prompt optimization rather than base TPP learning
- **First 3 experiments**:
  1. Train the model on a single task and evaluate its performance on the same task to verify the basic functionality of the prompt-augmented TPP.
  2. Train the model on two tasks sequentially and evaluate its performance on both tasks to test its ability to avoid catastrophic forgetting.
  3. Vary the prompt length (Lp) and evaluate its impact on model performance to understand the tradeoff between model capacity and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal prompt pool size (M) and selection size (N) for different types of event sequences and datasets?
- **Basis in paper**: [explicit] The authors state that they set M=10, N=4 for both datasets but also mention that the prompt pool size M positively contributes to performance. They note that given an optimal Lp, an overly large N makes the total prompts excessively oversized, leading to underfitting.
- **Why unresolved**: The paper only reports results for a single configuration of M and N. It does not explore how these hyperparameters should be tuned for different event sequence characteristics or datasets.
- **What evidence would resolve it**: Systematic experiments varying M and N across different datasets with varying characteristics (e.g., number of event types, sequence length distributions) would reveal optimal configurations for different scenarios.

### Open Question 2
- **Question**: How does PromptTPP's performance degrade under extreme distribution shift between tasks?
- **Basis in paper**: [inferred] The authors mention that traditional pretraining fails under distribution shift and that their method achieves state-of-the-art performance, but they do not quantify the limits of their approach under severe distribution changes.
- **Why unresolved**: The experimental setup uses consecutive time windows from real data, which likely have gradual distribution changes. The paper does not test scenarios with abrupt or extreme distribution shifts.
- **What evidence would resolve it**: Experiments where tasks are constructed to have increasingly dissimilar distributions (e.g., by mixing data from completely different sources or applying adversarial transformations) would show the breaking point of PromptTPP's continual learning capability.

### Open Question 3
- **Question**: Can the retrieval mechanism in PromptTPP be made more efficient while maintaining performance?
- **Basis in paper**: [explicit] The authors mention using maximum inner product search (MIPS) for retrieval and note that the retrieval prompt pool may be flexible to edit and can be asynchronously updated during training.
- **Why unresolved**: While the paper demonstrates the effectiveness of the retrieval mechanism, it does not explore alternatives that could reduce computational complexity, especially as the prompt pool grows.
- **What evidence would resolve it**: Comparing PromptTPP against variants using different retrieval strategies (e.g., approximate nearest neighbors, learned indexing structures, or hashing-based approaches) while measuring both accuracy and computational efficiency would identify more scalable solutions.

## Limitations
- The paper assumes task boundaries are known during training, which may not hold in truly streaming scenarios where task transitions are gradual
- The memory overhead grows linearly with the number of tasks due to the prompt pool size, potentially limiting scalability for very long streams
- The selection size N is fixed across all tasks, which may not be optimal when task complexity varies significantly

## Confidence

### Mechanism Confidence
- **Mechanism 1**: High - The prompt retrieval mechanism is well-defined with clear parameterization and matching criteria. The evidence from experiments showing improved performance on task-agnostic learning supports the claims about avoiding catastrophic forgetting.
- **Mechanism 2**: Medium - While the temporal prompt design is clearly specified, the effectiveness of estimated conditional time as a proxy for true temporal dynamics depends heavily on the quality of the base TPP model's estimates. The paper provides theoretical justification but limited empirical validation of this assumption.
- **Mechanism 3**: Medium - The asynchronous refresh mechanism shows promising results in the experiments, but the paper doesn't thoroughly explore the tradeoff space between different refresh frequencies. The claim that performance remains comparable with C=2 is based on limited experiments.

## Next Checks
1. **Ablation study on prompt pool size**: Systematically vary M across different dataset complexities to establish guidelines for optimal prompt pool sizing in various streaming scenarios.
2. **True task-agnostic evaluation**: Remove the assumption of known task boundaries during training by implementing a task detection mechanism and evaluate whether PromptTPP maintains performance without explicit task labels.
3. **Scalability analysis**: Test the framework on datasets with significantly longer event sequences (thousands of events vs. hundreds) to validate whether the O(Lp·M) memory complexity remains practical and whether retrieval efficiency degrades with larger prompt pools.