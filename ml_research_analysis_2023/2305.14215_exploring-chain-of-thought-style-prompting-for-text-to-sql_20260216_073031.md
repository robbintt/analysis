---
ver: rpa2
title: Exploring Chain-of-Thought Style Prompting for Text-to-SQL
arxiv_id: '2305.14215'
source_url: https://arxiv.org/abs/2305.14215
tags:
- prompting
- question
- name
- table
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores chain-of-thought (CoT) style prompting to enhance\
  \ reasoning ability for text-to-SQL parsing with large language models (LLMs). It\
  \ compares two existing CoT methods\u2014standard CoT and least-to-most prompting\u2014\
  with a new question-decomposition prompting (QDecomp) method."
---

# Exploring Chain-of-Thought Style Prompting for Text-to-SQL

## Quick Facts
- arXiv ID: 2305.14215
- Source URL: https://arxiv.org/abs/2305.14215
- Authors: 
- Reference count: 29
- Key outcome: QDecomp achieves 5.2 and 6.5 point absolute gains on Spider and Spider Realistic vs standard prompting

## Executive Summary
This paper investigates chain-of-thought (CoT) style prompting for improving text-to-SQL parsing with large language models (LLMs). The authors compare two existing CoT methods—standard CoT and least-to-most prompting—against a new question-decomposition prompting (QDecomp) method. Their findings reveal that iterative prompting is unnecessary for text-to-SQL parsing, and detailed reasoning steps can lead to error propagation. QDecomp achieves significant performance gains over both standard prompting and least-to-most prompting on the Spider and Spider Realistic benchmarks.

## Method Summary
The study uses in-context learning with Codex (code-davinci-002) to evaluate four prompting methods: Standard, Chain-of-Thought, Least-to-Most, and Question-Decomposition (QDecomp). Experiments vary the number of exemplars (0-8), selection method (random vs difficulty-based), and prompt format (API Docs vs Create Table + Select 3). The model generates SQL queries based on natural language questions and database schemas, with outputs evaluated using test-suite accuracy and component matching accuracy.

## Key Results
- QDecomp achieves 5.2 and 6.5 point absolute gains on Spider and Spider Realistic vs standard prompting
- QDecomp achieves 2.4 and 1.5 point gains over least-to-most prompting
- Iterative prompting (least-to-most) is unnecessary for text-to-SQL parsing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative prompting (least-to-most) is unnecessary for text-to-SQL parsing
- Mechanism: QDecomp achieves better or comparable results without iterative refinement, reducing computational cost and avoiding error propagation from intermediate steps.
- Core assumption: Generating sub-questions and the final SQL in a single pass is sufficient for complex reasoning tasks like text-to-SQL.
- Evidence anchors:
  - [abstract] "iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing"
  - [section 5.1] "QDecomp prompting with least-to-most prompting, we can see generating sub-questions and the SQL query in a single pass can even achieve better performance"
  - [corpus] Weak corpus evidence; no direct neighbor papers discuss this specific claim
- Break condition: If decomposition quality degrades significantly with more complex queries, iterative refinement might become necessary.

### Mechanism 2
- Claim: Detailed reasoning steps in chain-of-thought prompting lead to error propagation
- Mechanism: When LLMs generate detailed intermediate steps, errors in those steps propagate to the final SQL output, reducing overall accuracy.
- Core assumption: More detailed intermediate reasoning steps increase the chance of introducing and propagating errors.
- Evidence anchors:
  - [section 5.2] "CoT style prompting methods are subject to error propagation when applied to text-to-SQL parsing"
  - [table 3] Example showing how errors in reasoning steps lead to incorrect SQL
  - [corpus] Moderate corpus support; neighbor paper "Error Detection for Text-to-SQL Semantic Parsing" suggests error propagation is a known issue
- Break condition: If intermediate steps are constrained to be minimal and verifiable, error propagation might be reduced.

### Mechanism 3
- Claim: Question decomposition without detailed reasoning steps improves performance
- Mechanism: By decomposing the original question into sub-questions without generating detailed reasoning steps, the model avoids error accumulation while still benefiting from problem reduction.
- Core assumption: Problem decomposition is beneficial, but detailed intermediate steps are not necessary and may be harmful.
- Evidence anchors:
  - [section 3.3] "we follow the problem reduction stage in least-to-most prompting and instruct the LLM to decompose the original complex question as the reasoning steps"
  - [table 1] QDecomp outperforms both CoT and least-to-most prompting
  - [corpus] Weak corpus evidence; no direct neighbor papers discuss this specific mechanism
- Break condition: If decomposition becomes too coarse and loses important context, performance might degrade.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Understanding how intermediate reasoning steps are generated and their impact on final output quality
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of intermediate steps?

- Concept: Problem decomposition in reasoning tasks
  - Why needed here: QDecomp relies on breaking down complex questions into simpler sub-questions
  - Quick check question: What are the key differences between question decomposition and logical execution order in SQL?

- Concept: Error propagation in multi-step reasoning
  - Why needed here: Understanding why detailed intermediate steps can lead to worse performance
  - Quick check question: How can errors in intermediate reasoning steps affect the final output in chain-of-thought prompting?

## Architecture Onboarding

- Component map:
  Prompt template generator -> Example selector -> Schema formatter -> Model executor -> Evaluator

- Critical path:
  1. Select in-context examples based on strategy
  2. Generate prompt template with chosen reasoning method
  3. Format database schema information
  4. Execute model and collect SQL output
  5. Evaluate against test-suite

- Design tradeoffs:
  - Single-pass vs iterative prompting: computational efficiency vs potential accuracy gains
  - Detailed vs minimal intermediate steps: error propagation risk vs reasoning clarity
  - Random vs difficulty-based example selection: simplicity vs potential performance optimization

- Failure signatures:
  - Low test-suite accuracy despite high exact match on single database
  - Significant performance drop when using Create Table + Select 3 format
  - Error propagation visible in component matching accuracy breakdown

- First 3 experiments:
  1. Compare standard prompting vs QDecomp with random 8-shot examples
  2. Test QDecomp with different in-context example selection strategies (G1-G3)
  3. Evaluate impact of prompt format (API Docs vs Create Table + Select 3) on QDecomp performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed QDecomp method scale with increasing database complexity and number of tables/columns?
- Basis in paper: [inferred] The paper demonstrates effectiveness on Spider and Spider Realistic datasets but does not systematically vary database complexity
- Why unresolved: The experiments focus on existing datasets with fixed complexity rather than controlled variation of database size and complexity
- What evidence would resolve it: Experiments varying database size (number of tables/columns) and complexity across multiple orders of magnitude while measuring performance

### Open Question 2
- Question: What is the impact of error propagation in reasoning steps on long-chain reasoning tasks beyond text-to-SQL?
- Basis in paper: [explicit] The paper identifies error propagation as a key issue with detailed reasoning steps in text-to-SQL
- Why unresolved: The analysis is limited to text-to-SQL domain without exploring other reasoning-intensive tasks
- What evidence would resolve it: Comparative studies of CoT methods across multiple domains (e.g., multi-step math, logical reasoning) measuring error propagation effects

### Open Question 3
- Question: How does the trade-off between reasoning detail and error propagation change with model scale (smaller vs. larger LLMs)?
- Basis in paper: [inferred] All experiments use Codex (code-davinci-002), a large model, without exploring scale effects
- Why unresolved: The paper does not test different model sizes or the relationship between model capacity and reasoning strategy effectiveness
- What evidence would resolve it: Performance comparisons of QDecomp and standard CoT methods across multiple model scales (e.g., 7B, 13B, 175B parameters)

### Open Question 4
- Question: Can the error reduction mechanism in QDecomp be generalized to other prompting strategies beyond question decomposition?
- Basis in paper: [explicit] QDecomp reduces errors by avoiding detailed reasoning steps, but this principle isn't explored in other contexts
- Why unresolved: The paper proposes a specific implementation rather than identifying the underlying principle
- What evidence would resolve it: Development and testing of alternative prompting strategies that share the core principle of avoiding detailed intermediate reasoning steps

## Limitations
- Evaluation limited to a single LLM (Codex code-davinci-002) and two datasets
- Study focuses on in-context learning without parameter updates
- Error propagation analysis relies on qualitative examples rather than systematic attribution

## Confidence
- High: QDecomp outperforms standard CoT and least-to-most prompting on Spider and Spider Realistic datasets
- Medium: Iterative prompting is unnecessary for text-to-SQL parsing
- Medium: Detailed reasoning steps lead to error propagation in chain-of-thought prompting

## Next Checks
1. Test QDecomp prompting with different LLMs (e.g., GPT-4, LLaMA) to assess model dependence of the observed improvements
2. Conduct ablation studies varying the granularity of question decomposition to identify optimal decomposition depth
3. Implement systematic error analysis to quantify the exact contribution of error propagation to performance differences between CoT variants