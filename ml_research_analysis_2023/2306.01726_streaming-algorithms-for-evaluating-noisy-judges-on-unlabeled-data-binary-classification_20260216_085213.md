---
ver: rpa2
title: Streaming algorithms for evaluating noisy judges on unlabeled data -- binary
  classification
arxiv_id: '2306.01726'
source_url: https://arxiv.org/abs/2306.01726
tags:
- evaluation
- classifiers
- independent
- data
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes algebraic streaming algorithms for evaluating
  noisy binary classifiers on unlabeled data, treating it as a monitoring task. Two
  evaluators are constructed assuming independent classifier errors: a majority voting-based
  one and a fully inferential one.'
---

# Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification

## Quick Facts
- arXiv ID: 2306.01726
- Source URL: https://arxiv.org/abs/2306.01726
- Reference count: 18
- This paper proposes algebraic streaming algorithms for evaluating noisy binary classifiers on unlabeled data, treating it as a monitoring task.

## Executive Summary
This paper addresses the challenge of evaluating noisy binary classifiers on unlabeled data using streaming algorithms. The authors construct two evaluators assuming independent classifier errors: a majority voting-based one and a fully inferential one. They demonstrate that algebraic failure modes of the independent evaluator can identify highly correlated classifier ensembles that should be excluded from evaluation. The paper introduces a novel approach using containing varieties to refine nearly independent ensembles by constructing surfaces that must contain the true evaluation point.

## Method Summary
The method involves collecting per-item decision events from an ensemble of binary classifiers into integer counters (data sketch). Basic evaluation statistics (prevalences and label accuracies) are computed from this sketch. Two algebraic evaluators assuming independent classifier errors are applied to estimate true evaluation statistics. The independent evaluator's algebraic failures (unresolved square roots, values outside unit cube) are exploited to detect and reject correlated classifier ensembles. For nearly independent ensembles, a containing variety is constructed using polynomial generating sets for arbitrarily correlated classifiers, and independent estimates far from this surface are rejected.

## Key Results
- The independent evaluator can self-alarm when classifiers are too correlated through algebraic failure modes
- A containing variety can be constructed without knowledge of correlations, with distance indicating correlation strength
- Taylor expansion reveals inverse sensitivity near blind spots where estimates become unstable
- Independent evaluator estimates can be as accurate as 1% when classifiers are close to independent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The independent evaluator can self-alarm when classifiers are too correlated.
- Mechanism: When classifiers are independent, the evaluation variety is exactly two points. If correlations exist, the data sketch cannot be explained by the polynomial equations assuming independence, causing the evaluator to return estimates with unresolved square roots or values outside the unit cube.
- Core assumption: Classifiers' error independence is reflected in the structure of the evaluation ideal, which changes when correlations are introduced.
- Evidence anchors:
  - [abstract]: "But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates."
  - [section]: "The failures vary in their severity. The empirical hypothesis explored here is that their severity are indicative of the magnitude of the unknown decision correlations that are needed to correctly predict the observed data sketch."
  - [corpus]: Weak, as neighbors do not discuss algebraic evaluation failure modes.

### Mechanism 2
- Claim: A containing variety can be constructed without knowledge of correlations, and distance to it indicates correlation strength.
- Mechanism: The polynomial generating set for correlated classifiers includes extra variables for correlations. By finding a Gröbner basis that disentangles evaluation statistics from correlation variables, a subset of polynomials is obtained that defines a surface containing the true evaluation point. Independent evaluator estimates far from this surface suggest high correlation.
- Core assumption: The disentangled polynomials form a variety that strictly contains the true evaluation variety.
- Evidence anchors:
  - [abstract]: "At its final steps, the searches are refined by constructing a surface in evaluation space that must contain the true value point."
  - [section]: "Theorem 4 details how the basic evaluation statistics can be disentangled from the correlation ones by finding a suitable Gröbner basis for the generating set."
  - [corpus]: Weak, as neighbors do not discuss Gröbner bases or containing varieties in evaluation.

### Mechanism 3
- Claim: Taylor expansion of independent evaluator estimates on correlated classifiers reveals inverse sensitivity near blind spots.
- Mechanism: By expanding the algebraic formulas assuming independence but with small correlations, the first-order term is inversely proportional to (Pi,ℓ + Pi,β - 1). This means estimates become unstable near the line where label accuracies sum to one, creating blind spots.
- Core assumption: Small correlations can be treated as perturbations, and the Taylor expansion is valid near the true evaluation point.
- Evidence anchors:
  - [abstract]: "A Taylor expansion of the estimates produced when independence is assumed but the classifiers are, in fact, slightly correlated helps clarify how the independent evaluator has algebraic 'blind spots'."
  - [section]: "The linear term in Γi,j,ℓ has the inverse, 1/(Pk,ℓ - fk,ℓ). Consequently, independent evaluator estimates becomes worse the closer one is to the 'blindspots' in evaluation space at fi,ℓ."
  - [corpus]: Weak, as neighbors do not discuss Taylor expansions in the context of algebraic evaluation.

## Foundational Learning

- Concept: Polynomial Ideals and Varieties
  - Why needed here: The evaluation problem is formulated as finding points in evaluation space that satisfy polynomial equations derived from the data sketch. Understanding ideals and varieties is essential to grasp how the evaluator works and why it has failure modes.
  - Quick check question: What is the difference between an ideal and its variety in algebraic geometry?

- Concept: Gröbner Bases and Elimination Theory
  - Why needed here: Gröbner bases are used to simplify the polynomial system and isolate variables, which is crucial for solving the evaluation problem and constructing the containing variety. Elimination theory helps in reducing the system to a single variable polynomial.
  - Quick check question: How does a Gröbner basis help in solving a system of polynomial equations?

- Concept: Sample vs. Distributional Independence
  - Why needed here: The paper distinguishes between classifiers being independent in their errors on a finite test (sample independence) versus being independent under a distribution (distributional independence). This distinction is key to understanding the limitations of the algebraic approach.
  - Quick check question: Why might classifiers be sample independent on a finite test but not distributionally independent?

## Architecture Onboarding

- Component map:
  - Data Sketch Generator -> Basic Statistics Calculator -> Independent Evaluator -> Correlation Detector -> Containing Variety Constructor -> Distance Calculator -> Rejection Module

- Critical path:
  1. Collect per-item decision events into data sketch counters.
  2. Compute basic evaluation statistics (prevalences, label accuracies).
  3. Apply independent evaluator formulas to get estimates.
  4. Check for failure modes (unresolved square roots, out-of-bounds values).
  5. If no failures, compute distance to containing variety.
  6. If distance is low, accept the evaluation ensemble; otherwise, reject.

- Design tradeoffs:
  - Exactness vs. Practicality: The algebraic method provides exact solutions for independent classifiers but requires significant computation for correlated ones.
  - Self-alarming vs. Accuracy: The method can detect when assumptions are violated but may still return seemingly correct estimates near blind spots.
  - Universality vs. Complexity: The approach works for any number of labels and classifiers but the polynomial systems become complex quickly.

- Failure signatures:
  - Unresolved square roots in the independent evaluator's estimates.
  - Estimates outside the unit interval (prevalences or accuracies < 0 or > 1).
  - Large distance from independent estimates to the containing variety.
  - High correlation variables (Γi,j,ℓ) when estimated from the data sketch.

- First 3 experiments:
  1. Generate synthetic data with known independent classifiers and verify the evaluator returns exact estimates.
  2. Introduce small correlations and observe the appearance of failure modes (unresolved square roots).
  3. Construct a containing variety and measure the distance from independent estimates as correlations increase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the algebraic evaluator handle slightly correlated classifiers without significant accuracy loss?
- Basis in paper: [inferred] The paper discusses the challenges of handling small amounts of correlation and mentions Taylor expansion of estimates produced when independence is assumed but classifiers are slightly correlated.
- Why unresolved: The paper mentions that handling even small amounts of correlation remains a challenge and that the independent evaluator has algebraic "blind spots" that affect its estimates.
- What evidence would resolve it: Experimental results showing the performance of the algebraic evaluator on datasets with known small correlations, comparing it to other evaluation methods.

### Open Question 2
- Question: Can the containing variety approach be extended to handle more than three classifiers?
- Basis in paper: [inferred] The paper focuses on three classifiers and discusses the evaluation variety and containing variety for this case. It mentions that the methodology can be applied to any number of labels.
- Why unresolved: The paper does not explore the extension to more than three classifiers or discuss the computational challenges that may arise.
- What evidence would resolve it: Theoretical analysis and experimental results showing the performance of the containing variety approach with ensembles of more than three classifiers.

### Open Question 3
- Question: How can the algebraic evaluator be adapted to handle stream evaluation in real-time scenarios?
- Basis in paper: [explicit] The paper treats evaluation of noisy binary classifiers on unlabeled data as a streaming task and discusses the use of data sketches.
- Why unresolved: The paper focuses on batch evaluation and does not discuss the challenges of real-time stream evaluation, such as handling concept drift or updating the evaluation as new data arrives.
- What evidence would resolve it: Implementation and evaluation of the algebraic evaluator in a real-time streaming setting, demonstrating its ability to handle concept drift and provide accurate evaluations.

## Limitations
- The algebraic approach cannot guarantee detection of all correlated ensembles, particularly near algebraic blind spots.
- The computational complexity of Gröbner basis calculations grows rapidly with the number of classifiers and labels.
- Exact rejection criteria based on distance thresholds to the containing variety are not fully specified.

## Confidence
- High Confidence: The algebraic framework for independent classifiers and the mechanism of detecting correlations through failure modes are mathematically rigorous and well-supported by the literature on polynomial ideals and varieties.
- Medium Confidence: The effectiveness of the containing variety approach in practice depends on the relationship between distance metrics and actual correlation strength, which requires empirical validation across diverse scenarios.
- Low Confidence: The exact rejection criteria based on distance thresholds to the containing variety are not fully specified, making it difficult to assess the false positive and false negative rates of the correlation detection mechanism.

## Next Checks
1. **Blind Spot Analysis**: Systematically map the evaluation space to identify algebraic blind spots where the independent evaluator might return plausible but incorrect estimates despite classifier correlations. This would quantify the vulnerability of the approach.

2. **Threshold Calibration**: Empirically determine optimal distance thresholds for rejecting correlated ensembles by analyzing the trade-off between sensitivity (catching correlated classifiers) and specificity (not rejecting nearly independent ones) across multiple datasets.

3. **Scalability Assessment**: Evaluate the computational overhead of Gröbner basis calculations for larger ensembles (more than 3-4 classifiers) to establish practical limits on the approach's applicability.