---
ver: rpa2
title: 'On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes'
arxiv_id: '2306.13649'
source_url: https://arxiv.org/abs/2306.13649
tags:
- student
- teacher
- distillation
- on-policy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generalized Knowledge Distillation (GKD)
  for auto-regressive models, particularly large language models (LLMs). The main
  problem addressed is the distribution mismatch between training and inference phases
  in current distillation methods, as well as model under-specification where the
  student model cannot fully fit the teacher's distribution.
---

# On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes

## Quick Facts
- arXiv ID: 2306.13649
- Source URL: https://arxiv.org/abs/2306.13649
- Reference count: 8
- Key outcome: GKD outperforms standard supervised distillation and on-policy distillation on tasks like summarization, machine translation, and arithmetic reasoning

## Executive Summary
This paper introduces Generalized Knowledge Distillation (GKD) for auto-regressive models, particularly large language models (LLMs). The main problem addressed is the distribution mismatch between training and inference phases in current distillation methods, as well as model under-specification where the student model cannot fully fit the teacher's distribution. GKD solves this by training the student on its own generated outputs (on-policy) and optimizing alternative divergences like reverse KL that focus on generating samples likely under the teacher's distribution. The method outperforms standard supervised distillation and on-policy distillation on tasks like summarization, machine translation, and arithmetic reasoning. Notably, GKD variants with reverse KL or JSD (0.9) substantially improve performance, especially when using on-policy student-generated outputs. GKD also seamlessly integrates with RL fine-tuning, enabling direct optimization of task-specific rewards while maintaining distillation benefits.

## Method Summary
Generalized Knowledge Distillation (GKD) addresses two key limitations in LLM distillation: distribution mismatch between training and inference, and model under-specification. The method trains students on their own generated outputs (on-policy sampling) rather than fixed teacher-generated sequences, and uses alternative divergences like reverse KL or JSD that focus on generating sequences likely under the teacher's distribution. GKD generalizes both supervised and on-policy distillation through a λ parameter that controls the fraction of training data from student-generated outputs versus fixed ground-truth or teacher-generated sequences. The approach seamlessly integrates with RL fine-tuning, allowing direct optimization of task-specific rewards while maintaining distillation benefits.

## Key Results
- GKD with reverse KL or JSD (0.9) substantially improves performance over standard supervised distillation and on-policy distillation
- On-policy variants of GKD outperform those using teacher-generated outputs, particularly for summarization tasks
- GKD effectively handles model under-specification by focusing on high-probability modes rather than spreading probability mass
- The method seamlessly integrates with RL fine-tuning, enabling direct optimization of task-specific rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using student-generated outputs during training mitigates distribution mismatch between training and inference phases.
- Mechanism: Instead of training on fixed teacher-generated sequences, the student generates its own sequences and learns from teacher feedback on those sequences, aligning training distribution with deployment distribution.
- Core assumption: The student's output distribution during training will approximate its output distribution during inference.
- Evidence anchors:
  - [abstract]: "GKD mitigates distribution mismatch by sampling output sequences from the student during training"
  - [section]: "we argue for using samples from the student's distribution itself during training, akin to RL and on-policy distillation"
  - [corpus]: No direct corpus evidence found - weak support
- Break condition: If the student's training and inference behaviors diverge significantly due to optimization dynamics or hyperparameter choices.

### Mechanism 2
- Claim: Reverse KL or JSD(0.9) divergences focus the student on generating sequences likely under the teacher's distribution, addressing model under-specification.
- Mechanism: These divergences are mode-seeking rather than mean-seeking, causing the student to concentrate on modes where the teacher has high probability mass rather than spreading probability across the entire support.
- Core assumption: The student model has insufficient capacity to fully replicate the teacher's distribution, making mode-seeking objectives more appropriate.
- Evidence anchors:
  - [abstract]: "GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution"
  - [section]: "when approximating P(C) using a parameterized distribution Qθ(C), minimizing the reverse and forward KL under model under-specification results in mean and mode-seeking behavior"
  - [corpus]: No direct corpus evidence found - weak support
- Break condition: If the student model has sufficient capacity to fully represent the teacher's distribution, making forward KL more appropriate.

### Mechanism 3
- Claim: The λ parameter in GKD allows balancing between supervised and on-policy learning, providing flexibility in addressing different training scenarios.
- Mechanism: By adjusting λ, practitioners can control the fraction of training data that comes from student-generated outputs versus fixed ground-truth or teacher-generated sequences.
- Core assumption: Different tasks and dataset sizes may benefit from different balances between supervised and on-policy learning.
- Evidence anchors:
  - [abstract]: "GKD generalizes both on-policy and supervised distillation"
  - [section]: "we can optimize any divergence on token-level teacher and student distributions on a mixture of fixed dataset of output sequences, either teacher-generated or ground-truth, and on-policy student-generated sequences"
  - [corpus]: No direct corpus evidence found - weak support
- Break condition: If the optimal λ value is highly task-specific and not generalizable across different settings.

## Foundational Learning

- Concept: KL Divergence Properties
  - Why needed here: Understanding the difference between forward KL (mean-seeking) and reverse KL (mode-seeking) is crucial for selecting appropriate divergence in GKD.
  - Quick check question: What happens to the learned distribution when minimizing forward KL versus reverse KL under model under-specification?

- Concept: Exposure Bias in Auto-regressive Models
  - Why needed here: Recognizing that training on fixed datasets can create distribution mismatch during inference is key to understanding why on-policy approaches help.
  - Quick check question: Why does training on ground-truth sequences potentially create problems during auto-regressive generation?

- Concept: Reinforcement Learning Basics
  - Why needed here: GKD integrates concepts from RL like on-policy sampling and policy gradients, particularly when combined with RL fine-tuning.
  - Quick check question: How does on-policy sampling in RL relate to the approach used in GKD?

## Architecture Onboarding

- Component map:
  Teacher model (pre-trained, larger capacity) -> Student model (pre-trained, smaller capacity) -> Dataset of input contexts X -> Optional dataset of input-output pairs (X, Y) -> Divergence selection module (forward KL, reverse KL, JSD variants) -> λ scheduler for student data fraction -> Training loop with mixed sampling strategy

- Critical path:
  1. Initialize student model
  2. For each training step:
     - Sample u ~ Uniform(0,1)
     - If u ≤ λ: generate student output y ~ pθ_S(·|x)
     - Else: sample from fixed dataset (x,y)
     - Compute divergence D(pT||pθ_S)(y|x)
     - Backpropagate and update θ
  3. Evaluate on validation set

- Design tradeoffs:
  - Forward KL vs reverse KL/JSD: Forward KL encourages mode-covering but may spread probability mass inefficiently; reverse KL/JSD focus on high-probability modes but may miss coverage
  - λ value: Higher λ increases on-policy training but may slow convergence; lower λ relies more on fixed data but may not address distribution mismatch
  - Teacher temperature: Lower temperatures can improve performance contrary to conventional wisdom

- Failure signatures:
  - Student generates outputs very different from teacher despite high training loss → check divergence choice and λ value
  - Training instability or divergence → verify temperature settings and consider gradient clipping
  - Poor generalization to held-out data → examine whether λ is too high, causing overfitting to student's own distribution

- First 3 experiments:
  1. Compare forward KL vs reverse KL on a small summarization dataset with λ=0.5
  2. Sweep λ from 0 to 1 on the same task to find optimal student data fraction
  3. Test GKD with JSD(0.1) vs JSD(0.9) to understand mode-seeking behavior differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of divergence function (forward KL, reverse KL, JSD) impact the performance of GKD in tasks beyond summarization, translation, and arithmetic reasoning? Are there specific task types where certain divergences consistently outperform others?
- Basis in paper: [explicit] The paper states that "mode-seeking KL-based divergences typically outperform their mean-seeking counterparts" but notes this varies by task (e.g., reverse KL performed better than JSD for summarization but not for GSM8K).
- Why unresolved: The paper only tests these divergences on three specific tasks. The performance could vary significantly for other types of language tasks or even within sub-domains of these tasks.
- What evidence would resolve it: Comprehensive experiments testing GKD with different divergences across a wide range of language tasks (e.g., question answering, code generation, dialogue) would reveal which divergences work best for which task types.

### Open Question 2
- Question: How does the choice of teacher model size relative to the student model size affect the effectiveness of GKD? Is there an optimal ratio beyond which the teacher is too large to provide meaningful distillation signals?
- Basis in paper: [inferred] The paper uses T5-XL (∼3B params) as teacher for distilling to T5-small (77M), T5-base (250M), and T5-large (800M). The relative sizes vary significantly (38x, 12x, and 3.8x respectively), but the paper doesn't analyze how these ratios affect performance.
- Why unresolved: The paper doesn't systematically vary the teacher-student size ratio to determine if there's a point where the teacher becomes too large to be effectively distilled by the student.
- What evidence would resolve it: Experiments with multiple teacher-student size ratios (e.g., 50x, 20x, 10x, 5x, 2x) would show how performance changes with relative model sizes and identify if there's a diminishing return or optimal ratio.

### Open Question 3
- Question: How does GKD perform when the teacher model is itself trained with RL fine-tuning (e.g., using RLHF) rather than supervised fine-tuning? Does the combination of RL-trained teacher with GKD student create compounding benefits or conflicts?
- Basis in paper: [explicit] The paper mentions that "GKD facilitates the seamless integration of distillation with RL fine-tuning" and shows RL + on-policy GKD on summarization, but doesn't explore cases where the teacher itself is RL-trained.
- Why unresolved: The paper only demonstrates RL + GKD with a supervised fine-tuned teacher, leaving open the question of whether the benefits compound when both teacher and student are RL-enhanced.
- What evidence would resolve it: Experiments comparing GKD with RL-trained teachers versus supervised teachers across multiple tasks would show whether RL-enhanced teachers provide better distillation signals and whether GKD+RL performs better than either technique alone.

### Open Question 4
- Question: What is the computational overhead of GKD compared to standard supervised KD, and how does this scale with model size and dataset complexity? Is the performance gain worth the additional computational cost?
- Basis in paper: [inferred] The paper mentions that "generating sequences from the student rather than the teacher is much less expensive due to the difference in their model sizes" but doesn't provide quantitative comparisons of training time or compute requirements.
- Why unresolved: While the paper demonstrates performance improvements, it doesn't quantify the computational trade-offs involved in implementing GKD, particularly the cost of on-policy sampling during training.
- What evidence would resolve it: Detailed benchmarking comparing wall-clock training time, GPU hours, and energy consumption between GKD and standard KD methods across different model sizes and dataset scales would reveal the practical cost-benefit ratio.

## Limitations
- The superiority of reverse KL and JSD(0.9) over forward KL lacks direct empirical validation across varying model capacity gaps
- The paper doesn't systematically test how teacher-student size ratios affect GKD effectiveness
- The integration benefits of combining GKD with RL fine-tuning versus either technique alone remain unclear

## Confidence

- High confidence: The basic GKD framework (on-policy sampling + alternative divergences) works as described and improves over standard supervised distillation
- Medium confidence: The specific superiority of reverse KL and JSD(0.9) over forward KL for mode-seeking behavior under model under-specification
- Low confidence: The optimal λ values are task-independent and the RL+distillation integration provides additive benefits

## Next Checks

1. **Model Capacity Sensitivity**: Test GKD across teacher-student size ratios (e.g., 2x, 4x, 8x capacity differences) to determine when forward vs reverse KL divergence matters most for model under-specification
2. **Ablation on On-policy Fraction**: Systematically vary λ from 0 to 1 with fixed divergence to quantify the exact contribution of on-policy sampling versus divergence choice
3. **Integration Analysis**: Compare GKD + RL versus GKD alone and RL alone on the same tasks to measure the marginal benefit of combining these techniques