---
ver: rpa2
title: 'SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks'
arxiv_id: '2310.12126'
source_url: https://arxiv.org/abs/2310.12126
tags:
- sharcs
- router
- flops
- inference
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARCS introduces a router-based adaptive inference method that
  routes input samples to sub-networks of varying widths based on estimated sample
  hardness. The router is trained using a heuristic approach that leverages the model's
  prediction confidence over time.
---

# SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks

## Quick Facts
- arXiv ID: 2310.12126
- Source URL: https://arxiv.org/abs/2310.12126
- Reference count: 36
- Primary result: Achieves up to 2.75× FLOPs reduction with minimal accuracy loss on GLUE tasks

## Executive Summary
SHARCS introduces a router-based adaptive inference method that dynamically routes input samples to sub-networks of varying widths based on estimated sample hardness. The router is trained using a heuristic approach that leverages the model's prediction confidence over time, assigning hardness labels without requiring explicit ground-truth labels. Experiments demonstrate SHARCS significantly outperforms or complements existing sample-adaptive methods across multiple GLUE tasks, achieving substantial efficiency gains while maintaining accuracy.

## Method Summary
SHARCS employs a router that predicts sample hardness and dynamically adjusts network width during inference. The router is trained using a heuristic approach based on the model's prediction confidence over time, with confidence thresholds defining hardness levels. The method reduces FLOPs quadratically with width reduction factors, applying this scaling to multi-head attention, feed-forward networks, and layer normalization. SHARCS complements existing methods like token dropping by reducing width instead of dropping tokens, and can be applied to compressed models like DistilBERT and DynaBERT for further efficiency gains.

## Key Results
- Achieves up to 2.75× FLOPs reduction on GLUE tasks with less than 1% accuracy drop
- Outperforms existing sample-adaptive methods like DeeBERT and FastBERT in efficiency-accuracy trade-off
- Generalizes across different transformer architectures including compressed models
- Demonstrates over 2× latency reduction on CPU with minimal accuracy loss in real-world deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Router predicts sample hardness based on confidence thresholds from prediction history
- Mechanism: Router observes model's softmax confidence over a moving window of epochs. Samples with confidence within [T_low, T_high] intervals are labeled as belonging to that hardness level. Router trained using binary cross-entropy loss on these labels.
- Core assumption: Sample hardness can be inferred from model's confidence during training without explicit ground-truth hardness labels
- Evidence anchors:
  - [abstract] "Our approach has three main steps: (1) Obtaining labels that represent sample hardness; (2) Training a router that can predict sample hardness; (3) Adjusting the network's inference capacity according to the predicted hardness."
  - [section] "To mitigate such randomness, the ith entry is 1 only if the predicted ground truth probability is within the interval for a moving window of last W epochs."

### Mechanism 2
- Claim: Width reduction in adaptive layers reduces FLOPs quadratically
- Mechanism: Reducing input/output dimensions from d to d' reduces FLOPs by (d'/d)². Applied uniformly to MHA and FFN components in adaptive portion of transformer.
- Core assumption: Reducing dimensions uniformly across all components maintains functional capacity proportionally to width reduction
- Evidence anchors:
  - [section] "As Figure 1a illustrates, we down-project the input hidden states to the adaptive layers using a pooler module and up-project it back before feeding to the single classifier for all of the sub-networks."
  - [section] "It is worth noting that as we reduce the input and output dimensions of matrix multiplications by a factor of 1/r, the flops will be reduced by a factor of 1/r²."

### Mechanism 3
- Claim: SHARCS complements token dropping by reducing width instead of dropping tokens
- Mechanism: Token dropping reduces sequence length layer by layer while SHARCS reduces number of neurons (width) in transformer layers. These methods act orthogonally and can be combined.
- Core assumption: Width reduction and token dropping target different axes of transformer computation (feature dimension vs. sequence length)
- Evidence anchors:
  - [abstract] "Interestingly, SHARCS can further improve the efficiency of already optimized networks. For instance, inference with SHARCS on DynaBERT 0.25 width takes 10-15% less FLOPs with less than 1% drop in accuracy."
  - [section] "Interestingly token dropping and SHARCS should not interfere with each other and in fact, they can be paired together to bring more efficiency."

## Foundational Learning

- Concept: Confidence-based hardness labeling
  - Why needed here: Router requires notion of sample difficulty to decide which sub-network width to route to. Uses historical prediction confidence as proxy when explicit labels unavailable.
  - Quick check question: How does method ensure stability of hardness labels given stochastic nature of training?

- Concept: Width scaling and FLOPs reduction
  - Why needed here: Understanding relationship between width reduction factor r and FLOPs reduction (r²) critical for selecting appropriate reduction factors
  - Quick check question: If sub-network has reduction factor of 0.25, by what factor does its FLOPs reduce?

- Concept: Router placement and adaptation boundaries
  - Why needed here: Non-adaptive and adaptive parts of transformer split after K layers, with router at boundary. Knowing how to choose K and router interaction with both parts key to implementation
  - Quick check question: What is role of pooler and unpooler modules in adaptive part?

## Architecture Onboarding

- Component map: Input → Non-adaptive transformer layers (0 to K-1) → Router → Pooler → Adaptive width sub-networks → Unpooler → Classifier
- Critical path:
  1. Feed input through non-adaptive layers
  2. Router computes logits and outputs reduction factor r
  3. Pooler reduces hidden state to d_model × r
  4. Adaptive sub-network processes reduced hidden state
  5. Unpooler restores dimension to d_model
  6. Single classifier produces final prediction
- Design tradeoffs:
  - Router complexity vs. accuracy: Simpler routers reduce overhead but may underfit hardness prediction
  - Reduction factor granularity: More factors allow finer-grained routing but increase router training complexity
  - Placement of router (after which layer K): Earlier placement allows more savings but risks routing too early; later placement increases accuracy but reduces efficiency
- Failure signatures:
  - Router outputs inconsistent reduction factors for similar samples → instability in training labels
  - FLOPs reduction much lower than expected → possible bug in width scaling implementation or router always selecting r=1.0
  - Accuracy drops sharply with small reduction factors → sub-networks may be too narrow to handle complex samples
- First 3 experiments:
  1. Implement width scaling on single transformer layer and verify FLOPs reduction matches r² expectation
  2. Train router to predict hardness labels on held-out validation set and measure classification accuracy
  3. Integrate router and adaptive width sub-networks into full model; measure accuracy vs. FLOPs trade-off on small GLUE task (e.g., SST-2)

## Open Questions the Paper Calls Out
- How does SHARCS perform on larger models like GPT-3 or GPT-4?
- Can SHARCS be extended to regression tasks?
- How does performance of SHARCS vary with different numbers of reduction factors?
- How does choice of confidence thresholds affect performance of SHARCS?
- How does choice of history window size affect performance of SHARCS?

## Limitations
- Reliance on heuristic that prediction confidence correlates with sample hardness introduces potential instability
- Computational overhead of router not thoroughly quantified in terms of absolute latency impact
- Claim that SHARCS can be combined with token dropping without interference stated but not experimentally validated with comprehensive ablation studies

## Confidence
- High Confidence: Core mechanism of width scaling reducing FLOPs quadratically (r² relationship) is mathematically sound and well-established
- Medium Confidence: Router training methodology using confidence thresholds is reasonable but lacks extensive validation across different training scenarios
- Low Confidence: Claim that SHARCS can be combined with token dropping without interference needs more rigorous testing

## Next Checks
1. Conduct experiments measuring variance in router predictions across multiple training runs with different random seeds to analyze hardness label stability
2. Implement SHARCS on CPU or edge device and measure actual inference latency to quantify real-world overhead of router and pooler/unpooler modules
3. Create controlled experiments applying SHARCS and token dropping separately and in combination to measure whether claimed orthogonality holds