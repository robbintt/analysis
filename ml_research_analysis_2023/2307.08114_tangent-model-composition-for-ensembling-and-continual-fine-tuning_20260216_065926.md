---
ver: rpa2
title: Tangent Model Composition for Ensembling and Continual Fine-tuning
arxiv_id: '2307.08114'
source_url: https://arxiv.org/abs/2307.08114
tags:
- tasks
- learning
- continual
- task
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Tangent Model Composition (TMC), a method
  for composing independently fine-tuned models around a pre-trained point by treating
  them as tangent vectors. TMC enables efficient ensembling and continual fine-tuning
  by composing models at inference time via scalar combinations, reducing costs to
  that of a single model.
---

# Tangent Model Composition for Ensembling and Continual Fine-tuning

## Quick Facts
- arXiv ID: 2307.08114
- Source URL: https://arxiv.org/abs/2307.08114
- Reference count: 40
- Key outcome: TMC improves accuracy by 4.2% over non-linear fine-tuned ensembles while reducing inference cost by 2.5× to 10×

## Executive Summary
Tangent Model Composition (TMC) is a novel method for composing independently fine-tuned models around a pre-trained point by treating them as tangent vectors. The approach enables efficient ensembling and continual fine-tuning by composing models at inference time via scalar combinations, reducing costs to that of a single model. TMC demonstrates significant improvements over existing methods, achieving 4.2% higher accuracy than non-linear fine-tuned ensembles while reducing inference cost by 2.5× to 10×. The method also outperforms existing continual fine-tuning approaches on 13 experiments across 3 datasets without requiring a replay buffer.

## Method Summary
TMC works by training component models on the tangent space of a pre-trained model using the Rescaled Square Loss (RSL) with α=1, β=25 for class-incremental tasks. The component models are trained independently on disjoint task subsets and then composed via linear combination at inference time. This composition reduces inference cost from O(T) to O(1) while maintaining or improving accuracy. The method is implemented using Jacobian-vector products to linearize around the pre-trained model and can be parallelized on federated data. Each component model can be forgotten at zero cost, making TMC particularly suitable for continual learning scenarios.

## Key Results
- TMC improves accuracy by 4.2% over non-linear fine-tuned ensembles
- Reduces inference cost by 2.5× to 10× compared to traditional ensembling
- Outperforms existing continual fine-tuning methods on 13 experiments across 3 datasets without using a replay buffer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linearized models trained on the tangent space around a pre-trained model can be composed via simple scalar combinations.
- Mechanism: The tangent model hδ is defined as fw(·) + ∇wfw(·) · δ, which is linear in the parameters δ. This linearity ensures that any convex combination of tangent models remains within the same tangent space, allowing direct composition without retraining.
- Core assumption: The first-order Taylor expansion of the deep network around the pre-trained weights provides a sufficiently accurate local approximation for downstream tasks.
- Evidence anchors:
  - [abstract] "Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted..."
  - [section] "Since the tangent model hδ is linear in the parameters δ, training it with standard losses such as mean-squared error (MSE) or empirical cross-entropy yields a convex loss landscape."
- Break condition: When the pre-training and downstream tasks are highly unrelated, the first-order Taylor expansion no longer provides a good local approximation, making linear composition ineffective.

### Mechanism 2
- Claim: Composing tangent models reduces inference cost from O(T) to O(1) while maintaining or improving accuracy.
- Mechanism: Since tangent models form a vector space, their linear combination can be represented as a single model in the same space. This allows ensemble inference to be performed with a single forward pass rather than T separate passes.
- Core assumption: The tangent space H is closed under linear combination, meaning any linear combination of models in H remains in H.
- Evidence anchors:
  - [section] "In other words, since H defines a vector space with addition and scalar multiplication based on δ, any linear combination (or ensemble) of models in H is equivalent to a single model in H derived simply by the linear combination over δ."
- Break condition: When tasks are too dissimilar or antagonistic, their tangent models may live on different tangent planes, making linear combination nonsensical.

### Mechanism 3
- Claim: The Rescaled Square Loss (RSL) with appropriate β parameter improves generalization in class-incremental settings by increasing signal-to-noise ratio.
- Mechanism: RSL with larger β values scale positive output class signals while keeping negative ones close to zero, reducing interference from component models trained on other tasks when composed.
- Core assumption: In class-incremental settings, component models trained on different tasks produce non-zero noisy output signals for labels in other tasks, which can accumulate and affect final predictions.
- Evidence anchors:
  - [section] "Training using RSL with large values of β aims to mimic this effect by encouraging greater separation between the positive and negative outputs, and thus increasing the signal-to-noise ratio of each component model."
- Break condition: When tasks are highly synergistic (data-incremental), large β values can harm generalization by ignoring weak but constructive signals from different component models.

## Foundational Learning

- Concept: First-order Taylor expansion of deep networks
  - Why needed here: Provides the mathematical foundation for linearization of models around pre-trained weights
  - Quick check question: What is the form of the first-order Taylor expansion approximation for a function f around point w?

- Concept: Convex optimization in parameter space
- Concept: Vector space properties of tangent models

## Architecture Onboarding

- Component map: Pre-trained model (w) -> Tangent models hδ (perturbations around w) -> Composition mechanism (scalar combination) -> Rescaled Square Loss (training with appropriate scaling)

- Critical path:
  1. Initialize tangent model with pre-trained weights
  2. Train each tangent model on individual task using RSL
  3. Compose models via scalar combination at inference
  4. (Optional) Sequentially compose to maintain O(1) memory

- Design tradeoffs:
  - Linearity assumption vs. expressiveness
  - β parameter choice vs. task similarity
  - Sequential composition vs. parallel training capability

- Failure signatures:
  - Poor performance when pre-training and downstream tasks are unrelated
  - Degradation when tasks are highly antagonistic
  - Over-regularization with inappropriate β values

- First 3 experiments:
  1. Train tangent models on individual tasks and compose via scalar combination
  2. Compare TMC with non-linear ensembling and weight averaging baselines
  3. Vary β parameter to study its effect on class-incremental vs. data-incremental settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TMC perform when tasks are not "local" to the pre-trained embedding and what are the limits of its effectiveness?
- Basis in paper: [explicit] The paper states that TMC is designed for composing models that are local to a pre-trained embedding and that it is limited to composing models "local" to a pre-trained embedding. It also mentions that if models are trained on tasks that are far in representation space or even antagonistic, they are likely to live on different tangent planes, making linear combination nonsensical.
- Why unresolved: The paper does not provide a comprehensive analysis of TMC's performance when tasks are not local to the pre-trained embedding. It only mentions this as a limitation without exploring the boundaries of this limitation.
- What evidence would resolve it: Conducting experiments with tasks that are increasingly dissimilar to the pre-trained embedding and measuring TMC's performance. This could include using datasets from different domains or tasks that are known to be challenging for models pre-trained on a specific dataset.

### Open Question 2
- Question: What is the optimal value of the hyper-parameter β for the Rescaled Square Loss (RSL) across different datasets and task similarities?
- Basis in paper: [explicit] The paper discusses the use of β to control the interaction between component models and shows that larger values of β are better for class-incremental datasets while smaller values are better for data-incremental datasets. However, it does not provide a systematic method for choosing β.
- Why unresolved: The paper only provides a general guideline for choosing β based on task similarity but does not offer a method to determine the optimal value for a given dataset and task configuration.
- What evidence would resolve it: Performing a grid search or using a validation set to find the optimal β for different datasets and task configurations. This could involve testing a range of β values and selecting the one that yields the best performance.

### Open Question 3
- Question: How does TMC compare to other continual learning methods in terms of computational efficiency and memory usage?
- Basis in paper: [explicit] The paper claims that TMC reduces the cost of ensembling to that of a single model and does not require a replay buffer, thus reducing memory requirements. However, it does not provide a detailed comparison of computational efficiency and memory usage with other methods.
- Why unresolved: While the paper mentions the benefits of TMC in terms of computational cost and memory usage, it does not provide a comprehensive comparison with other continual learning methods.
- What evidence would resolve it: Conducting a detailed analysis of the computational efficiency and memory usage of TMC compared to other continual learning methods. This could involve measuring the time and memory required for training and inference for each method.

## Limitations

- The approach is limited to composing models "local" to a pre-trained embedding and may not work well when tasks are highly unrelated or antagonistic
- Performance depends on the choice of β parameter, which requires careful tuning based on task similarity
- The paper focuses on image classification tasks and the approach's effectiveness on other domains remains unexplored

## Confidence

- **High Confidence**: The mathematical framework for tangent model composition is rigorously defined and internally consistent. The computational efficiency gains (O(1) vs O(T) inference) are straightforward consequences of the vector space structure.
- **Medium Confidence**: The empirical results showing 4.2% accuracy improvement over non-linear ensembles are compelling but limited to three datasets. The superiority over existing continual learning methods needs broader validation.
- **Low Confidence**: The paper's claims about the generality of the approach across different task relationships (synergistic vs. antagonistic) are largely theoretical without sufficient empirical stress-testing.

## Next Checks

1. **Boundary Testing**: Systematically evaluate TMC performance across task pairs with varying degrees of similarity to pre-training data, measuring where linearization breaks down.

2. **Architectural Generalization**: Test the approach on transformer-based architectures beyond ResNet-50 to assess scalability and architecture dependence.

3. **Long-sequence Continual Learning**: Evaluate TMC in scenarios with many sequential tasks (T > 10) to verify that the O(1) memory advantage holds in practice and that forgetting at zero cost doesn't accumulate catastrophic interference.