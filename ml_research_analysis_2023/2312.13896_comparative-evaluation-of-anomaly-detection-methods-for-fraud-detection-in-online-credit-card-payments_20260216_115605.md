---
ver: rpa2
title: Comparative Evaluation of Anomaly Detection Methods for Fraud Detection in
  Online Credit Card Payments
arxiv_id: '2312.13896'
source_url: https://arxiv.org/abs/2312.13896
tags:
- detection
- methods
- anomaly
- learning
- fraud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of anomaly detection (AD)
  methods compared to supervised methods for fraud detection in online credit card
  payments. The authors test several AD methods, including deep learning and non-deep
  learning approaches, on a real-world dataset with distribution shifts and extreme
  class imbalances.
---

# Comparative Evaluation of Anomaly Detection Methods for Fraud Detection in Online Credit Card Payments

## Quick Facts
- arXiv ID: 2312.13896
- Source URL: https://arxiv.org/abs/2312.13896
- Reference count: 40
- Key outcome: LightGBM outperforms all anomaly detection methods in fraud detection, but is more susceptible to distribution shifts

## Executive Summary
This paper presents a comprehensive evaluation of anomaly detection (AD) methods compared to supervised learning approaches for fraud detection in online credit card payments. The authors test multiple AD methods including deep learning and non-deep learning approaches against LightGBM on a real-world dataset with significant distribution shifts and extreme class imbalance. Their findings reveal that while LightGBM achieves superior overall performance across all metrics, AD methods demonstrate better robustness to distribution shifts. The study also finds that combining AD methods with LightGBM provides limited additional benefit since LightGBM already captures most frauds detected by AD methods.

## Method Summary
The authors evaluate various anomaly detection and supervised learning methods on a real-world credit card payment dataset containing 192 million transactions from 2018-2021. AD methods are trained only on legitimate payments, while LightGBM is trained on all payments. The dataset is filtered to two countries with 3M and 20M payments respectively, with 2018 data split 75/25 for training/testing and 2020 used as a separate test set. Features undergo Catboost encoding for categorical variables and min-max scaling for continuous variables. Multiple AD methods (ECOD, COPOD, KNN, Isolation Forest, GOAD, NeuTraL-AD, NPT-AD, Internal Contrastive) are compared against LightGBM using F1-score, AUROC, and AUPRC metrics averaged over 10 runs.

## Key Results
- LightGBM significantly outperforms all AD methods across F1-score, AUROC, and AUPRC metrics
- LightGBM shows greater susceptibility to distribution shifts compared to AD methods
- LightGBM captures 89.02% of frauds detected by ECOD, suggesting limited benefit from AD method combinations
- Both methods show performance degradation on 2020 test data compared to 2018 test data, indicating distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LightGBM outperforms AD methods in fraud detection despite class imbalance
- Mechanism: LightGBM's gradient boosting decision trees focus on hard-to-classify examples, making it more effective at capturing minority fraud cases
- Core assumption: The fraud patterns remain relatively stable and can be learned from labeled data
- Evidence anchors:
  - [abstract] "LightGBM exhibits significantly superior performance across all evaluated metrics"
  - [section] "GBDT models such as LightGBM and XGBoost are often considered particularly suited for imbalanced and extremely imbalanced set-ups since these models focus on particularly hard-to-classify samples"
  - [corpus] Weak - corpus papers discuss similar fraud detection approaches but don't directly address this mechanism
- Break condition: If fraud patterns change rapidly or become too diverse for supervised learning to capture

### Mechanism 2
- Claim: AD methods show better robustness to distribution shifts than LightGBM
- Mechanism: AD methods characterize normal distribution using only normal samples, making them less affected by changes in fraud patterns
- Core assumption: Fraud patterns change over time while normal payment behavior remains relatively stable
- Evidence anchors:
  - [abstract] "LightGBM suffers more from distribution shifts than AD methods"
  - [section] "assuming only fraudulent behaviors change over time, AD models should be more robust to distribution shift than standard supervised approaches"
  - [corpus] Missing - corpus doesn't provide evidence on distribution shift robustness
- Break condition: If both normal and fraudulent behaviors change simultaneously, or if normal behavior becomes too diverse

### Mechanism 3
- Claim: Limited benefit from ensembling AD methods with LightGBM
- Mechanism: LightGBM already captures most frauds detected by AD methods, providing little additional value from combination
- Core assumption: The fraud cases detected by AD methods are largely a subset of those detected by LightGBM
- Evidence anchors:
  - [abstract] "LightGBM also captures the majority of frauds detected by AD methods"
  - [section] "89.02% of the fraud cases detected by ECOD were also detected by LightGBM"
  - [corpus] Weak - corpus papers discuss ensemble methods but don't provide specific evidence on this combination
- Break condition: If AD methods detect substantially different fraud patterns not captured by LightGBM

## Foundational Learning

- Concept: Imbalanced learning
  - Why needed here: Fraud detection involves extreme class imbalance (fraud cases < 1% of data)
  - Quick check question: What happens to standard classifiers when one class represents less than 1% of samples?

- Concept: Distribution shift
  - Why needed here: Fraud patterns evolve over time, creating discrepancies between training and test data distributions
  - Quick check question: How would you measure whether the payment patterns in 2020 differ significantly from those in 2018?

- Concept: Anomaly detection vs supervised learning
  - Why needed here: Understanding when to use AD methods vs supervised approaches based on data characteristics
  - Quick check question: What are the key differences in how AD methods and supervised methods use labeled data?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature engineering -> Model training -> Evaluation -> Ensemble consideration
- Critical path: Data preprocessing -> Feature engineering -> Model training -> Evaluation
- Design tradeoffs: AD methods offer robustness to distribution shifts but lower detection performance; supervised methods offer higher performance but are sensitive to distribution shifts
- Failure signatures: Performance degradation when fraud patterns change; poor generalization to new fraud types
- First 3 experiments:
  1. Compare LightGBM and ECOD on 2018 vs 2020 datasets to quantify distribution shift impact
  2. Test whether adding synthetic fraud samples to LightGBM training improves 2020 performance
  3. Evaluate ensemble performance of LightGBM + ECOD vs LightGBM alone on 2020 dataset

## Open Questions the Paper Calls Out

- How do anomaly detection methods perform on other real-world credit card payment datasets beyond the one studied in this paper?
- What techniques can improve the robustness of gradient boosted decision tree (GBDT) models against distribution shifts in fraud detection?
- Under what conditions do anomaly detection methods outperform supervised methods like LightGBM in fraud detection?

## Limitations
- Results are based on a single proprietary dataset from one payment service provider, limiting generalizability
- The study doesn't explore how different feature engineering approaches might affect the relative performance of AD vs supervised methods
- Impact of temporal ordering on model performance and potential look-ahead bias is not investigated

## Confidence
- High confidence: LightGBM outperforms AD methods on overall fraud detection metrics (F1, AUROC, AUPRC)
- Medium confidence: AD methods show better robustness to distribution shifts than LightGBM
- Medium confidence: Limited benefit from combining AD methods with LightGBM

## Next Checks
1. Test model performance across multiple time periods to quantify distribution shift impact more rigorously
2. Conduct ablation studies varying feature engineering approaches and their impact on AD vs supervised method performance
3. Evaluate ensemble methods combining LightGBM with different AD approaches to identify potential complementary benefits