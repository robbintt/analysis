---
ver: rpa2
title: Practical Contextual Bandits with Feedback Graphs
arxiv_id: '2302.08631'
source_url: https://arxiv.org/abs/2302.08631
tags:
- feedback
- graph
- regression
- problem
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a practical algorithm for contextual bandits
  with feedback graphs by reducing the problem to online regression. The approach
  involves solving a convex program that computes the optimal policy distribution
  given the current regression estimates and feedback graph.
---

# Practical Contextual Bandits with Feedback Graphs

## Quick Facts
- arXiv ID: 2302.08631
- Source URL: https://arxiv.org/abs/2302.08631
- Reference count: 18
- The paper proposes a practical algorithm for contextual bandits with feedback graphs by reducing the problem to online regression.

## Executive Summary
This paper presents a practical approach to contextual bandits with feedback graphs through reduction to online regression. The algorithm solves a convex program that computes optimal policy distributions given current regression estimates and feedback graph structure. For strongly observable graphs with self-loops, it achieves a regret bound of O(√α(G)T RegSq(T)), where α(G) is the independence number. The method is applied to a posted-price auction bidding problem, demonstrating improved statistical and computational performance compared to previous approaches.

## Method Summary
The paper proposes reducing contextual bandits with feedback graphs to online regression by solving a convex program at each round. Given a feedback graph G_t and context x_t, the algorithm uses an online regression oracle to predict losses and then solves a convex program to determine the optimal action distribution. The method achieves established minimax rates for different graph observability classes by exploiting graph structure through the decision estimation coefficient framework. For strongly observable graphs with self-loops, the algorithm achieves O(√α(G)T RegSq(T)) regret, while for weakly observable graphs it achieves O(δ(G)^(1/3)T^(2/3)RegSq(T)^(1/3)).

## Key Results
- Achieves O(√α(G)T RegSq(T)) regret bound for strongly observable graphs with self-loops
- Achieves O(δ(G)^(1/3)T^(2/3)RegSq(T)^(1/3)) regret for weakly observable graphs
- Applied to posted-price auction bidding problem with improved statistical and computational performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The reduction to online regression enables practical algorithms for contextual bandits with feedback graphs by solving a convex program whose dimensionality is one more than the number of actions.
- **Mechanism**: The algorithm solves a convex program (eq. 3) that computes the optimal policy distribution given current regression estimates and the feedback graph. This convex program upper bounds the decision estimation coefficient, which characterizes the minimax regret.
- **Core assumption**: The feedback graph is known in advance (informed setting) and the regression function class is realizable.
- **Evidence anchors**:
  - [abstract]: "We present and analyze an approach to contextual bandits with graph feedback based upon reduction to regression. The resulting algorithms are practical and achieve known minimax rates."
  - [section 3.3]: "For finite A, eq. (1) is upper bounded by the twice the value of the (convex) optimization problem"
  - [corpus]: Weak - no direct evidence about reduction mechanisms in related papers
- **Break condition**: The convex relaxation becomes too loose for strongly observable graphs without self-loops, failing to achieve optimal rates.

### Mechanism 2
- **Claim**: The algorithm achieves established minimax rates by exploiting graph structure through the decision estimation coefficient framework.
- **Mechanism**: By bounding the decision estimation coefficient dec_γ(F), the algorithm can derive regret bounds that match the minimax rates for different graph observability classes (strongly observable, weakly observable, unobservable).
- **Core assumption**: The decision estimation coefficient can be bounded constructively through the convex program solution.
- **Evidence anchors**:
  - [section 3.1]: "Because the dec is the expected difference between per-round RegCB and γ-scaled RegSq, an upper bound on dec implies an upper bound on RegCB(T) in terms of RegSq(T)"
  - [section 3.4]: "In this section we show correspondence between eq. (2), eq. (3), and known results in the graph feedback literature"
  - [corpus]: Weak - related papers focus on different aspects of graph feedback but don't directly address the decision estimation coefficient approach
- **Break condition**: When the feedback graph structure is unknown at decision time (uninformed setting), the formulation of an analogous minimax problem becomes unclear.

### Mechanism 3
- **Claim**: The approach provides computational advantages over enumeration-based methods by leveraging reduction to regression with a tractable convex program.
- **Mechanism**: Instead of enumerating over all possible policies, the algorithm uses an online regression oracle to predict losses and then solves a convex program to determine the optimal action distribution. This avoids the computational intractability of previous approaches.
- **Core assumption**: Access to an online regression oracle with bounded square-loss regret (Assumption 2).
- **Evidence anchors**:
  - [abstract]: "The resulting algorithms are practical and achieve known minimax rates, thereby reducing the statistical complexity in real-world applications."
  - [section 2]: "We assume access to an online regression oracle AlgSq for function class F"
  - [corpus]: Moderate - some related work mentions computational aspects but doesn't directly compare to enumeration-based approaches
- **Break condition**: When the regression function class is too complex or the oracle is too slow, the computational advantages may diminish.

## Foundational Learning

- **Concept: Convex optimization and duality theory**
  - Why needed here: The algorithm relies on solving convex programs and applying strong duality to derive regret bounds. Understanding how to formulate and solve these programs is essential.
  - Quick check question: Given a convex program with linear objective and linear constraints, what is the relationship between the primal and dual solutions?

- **Concept: Graph theory and feedback graphs**
  - Why needed here: The algorithm exploits structural properties of feedback graphs (independence number, domination number) to achieve optimal rates. Understanding these graph-theoretic concepts is crucial.
  - Quick check question: What is the independence number of a graph, and how does it relate to the feedback structure in bandit problems?

- **Concept: Online learning and regret analysis**
  - Why needed here: The algorithm operates in an online setting and the performance is measured by regret. Understanding regret bounds and their derivation is fundamental.
  - Quick check question: What is the difference between full information regret and bandit regret, and how does feedback graph structure affect this?

## Architecture Onboarding

- **Component map**: Regression oracle -> Convex program solver -> Policy selector
- **Critical path**: At each round, the system receives context and feedback graph, queries the regression oracle for loss predictions, solves the convex program to determine action probabilities, samples an action, observes feedback, and updates the regression oracle.
- **Design tradeoffs**: The informed setting assumption provides computational tractability but limits applicability to uninformed feedback graphs. The convex relaxation provides computational efficiency but may be too loose for some graph structures.
- **Failure signatures**: High regret despite accurate regression predictions may indicate the convex relaxation is too loose. Poor computational performance may indicate the regression oracle or convex solver is not efficient enough.
- **First 3 experiments**:
  1. Implement the convex program solver and verify it produces feasible solutions for simple feedback graphs (e.g., complete graphs, empty graphs).
  2. Test the integration with a simple regression oracle (e.g., linear regression) on synthetic data with known optimal policies.
  3. Evaluate the regret on a simple contextual bandit problem with a small feedback graph to verify the theoretical guarantees hold in practice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the algorithm perform under misspecification, i.e., when the realizability assumption (Assumption 1) does not hold?
- Basis in paper: [explicit] The paper mentions that misspecification can be incorporated while maintaining computational efficiency but does not complicate the exposition here.
- Why unresolved: The paper does not provide theoretical analysis or empirical results for the case when the realizability assumption is violated.
- What evidence would resolve it: Theoretical bounds on regret under misspecification and empirical evaluation showing performance degradation or robustness to model mismatch.

### Open Question 2
- Question: Can the algorithm be extended to handle uninformed graph feedback problems where the graph is unknown at decision time?
- Basis in paper: [inferred] The paper discusses informed graph feedback problems and mentions that practical problems are often uninformed graph feedback problems. It suggests that formulating an analogous minimax problem under uninformed conditions is unclear.
- Why unresolved: The paper does not provide a framework or algorithm for uninformed graph feedback, only suggesting potential directions.
- What evidence would resolve it: A theoretical formulation of the minimax problem for uninformed feedback and an algorithm with corresponding regret bounds.

### Open Question 3
- Question: How does the algorithm perform when graph feedback has additional structure, such as local uninformed feedback or policy constraints?
- Basis in paper: [explicit] The paper mentions that the approach is compatible with constraint enforcement via reduction to Lagrange regression and discusses local uninformed feedback in the context of posted-price selling.
- Why unresolved: The paper does not provide theoretical analysis or empirical results for these extensions.
- What evidence would resolve it: Theoretical bounds on regret with policy constraints and empirical evaluation showing performance with different feedback structures.

## Limitations

- Assumes informed setting where feedback graph is known at decision time, limiting real-world applicability
- Performance depends critically on the quality and efficiency of the online regression oracle
- Does not provide empirical comparison against simpler heuristics or quantify practical regret-performance gap

## Confidence

- **High confidence** in the theoretical framework and regret analysis
- **Medium confidence** in the practical computational advantages claimed
- **Low confidence** in the method's effectiveness for uninformed feedback graphs

## Next Checks

1. Implement the algorithm on synthetic data with varying graph structures (complete, star, cycle) to empirically verify the theoretical regret bounds.
2. Compare the computational runtime of the convex program solver against enumeration-based approaches on graphs with 10-50 actions.
3. Test the algorithm's performance when the regression oracle has bounded but non-zero square-loss regret to assess robustness to approximation errors.