---
ver: rpa2
title: 'LLMRec: Benchmarking Large Language Models on Recommendation Task'
arxiv_id: '2308.12241'
source_url: https://arxiv.org/abs/2308.12241
tags:
- recommendation
- tasks
- llms
- nail
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks large language models (LLMs) on five recommendation
  tasks: rating prediction, sequential recommendation, direct recommendation, explanation
  generation, and review summarization. The study compares off-the-shelf LLMs like
  ChatGPT and LLaMA with finetuned versions using supervised fine-tuning.'
---

# LLMRec: Benchmarking Large Language Models on Recommendation Task

## Quick Facts
- arXiv ID: 2308.12241
- Source URL: https://arxiv.org/abs/2308.12241
- Reference count: 22
- This paper benchmarks large language models on five recommendation tasks: rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization.

## Executive Summary
This paper presents a comprehensive benchmarking study of large language models (LLMs) on recommendation tasks. The study evaluates both off-the-shelf models like ChatGPT and LLaMA as well as fine-tuned versions using supervised fine-tuning. Results show that while LLMs perform moderately on accuracy-based tasks, they excel at explainability-based tasks such as explanation generation and review summarization. Fine-tuning significantly improves performance, particularly for tasks requiring formatted outputs. ChatGPT demonstrates superior qualitative performance compared to other models.

## Method Summary
The study evaluates LLMs on five recommendation tasks using the Amazon Beauty dataset. Off-the-shelf models are tested with carefully designed prompts, while fine-tuning is performed using supervised fine-tuning with P-tuning V2 and LoRA. The evaluation includes both objective metrics (RMSE, HR@k, NDCG@k) and qualitative assessments. Prompts are constructed with three modules: task description, behavior injection, and format indicator. The dataset contains 22,363 users, 12,101 items, and 198,502 reviews.

## Key Results
- LLMs achieve comparable performance to state-of-the-art methods on explainability-based tasks
- Supervised fine-tuning significantly improves LLMs' instruction compliance and performance on accuracy-based tasks
- ChatGPT outperforms other models in qualitative evaluations, generating clearer and more reasonable results
- Objective metrics like BLEU and ROUGE fail to fully capture the quality of LLM-generated explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs achieve superior explainability-based task performance due to their ability to understand and generate natural language explanations that capture user sentiment and product attributes.
- **Mechanism:** LLMs leverage their large-scale pretraining to extract and synthesize key information from user reviews, producing coherent summaries and explanations that reflect nuanced product features and user preferences.
- **Core assumption:** The pretraining corpus contains sufficient linguistic patterns and domain knowledge to generalize to recommendation-specific explainability tasks.
- **Evidence anchors:**
  - [abstract] "However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks."
  - [section] "For explainable recommendation tasks such as explanation generation and review summarization, objective metrics like BLEU and ROUGE fail to accurately measure the true capability of LLM-based recommender systems."
- **Break condition:** If the review data contains highly specialized domain jargon or rare product categories not well-represented in pretraining, LLM performance on explainability tasks will degrade significantly.

### Mechanism 2
- **Claim:** Supervised fine-tuning (SFT) improves LLM performance on accuracy-based tasks by aligning the model's output format and domain-specific understanding.
- **Mechanism:** SFT introduces task-specific prompts and recommendation data, enabling LLMs to learn the structured output formats required for rating prediction, sequential recommendation, and direct recommendation.
- **Core assumption:** The recommendation data distribution is sufficiently similar to the pretraining distribution to allow effective fine-tuning without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] "Supervised finetuning (SFT) can significantly improves LLMs' instruction compliance ability in recommendation tasks."
  - [section] "After apply Supervised Fine Tuning, these models' outputs could be restricted to the desired format, and achieve competitive performances compared to the baseline methods."
- **Break condition:** If the fine-tuning dataset is too small or unrepresentative, the model may overfit to training prompts without generalizing to new recommendation scenarios.

### Mechanism 3
- **Claim:** Chain-of-Thought (CoT) prompting enhances sequential recommendation by injecting user-item interaction history into the model's reasoning process.
- **Mechanism:** CoT prompting provides LLMs with explicit behavioral context, allowing them to capture sequential dependencies and user preferences more effectively than zero-shot prompting.
- **Core assumption:** User behavior sequences contain sufficient signal about future preferences that can be extracted through natural language reasoning.
- **Evidence anchors:**
  - [section] "The behavior injection module is designed to assess the impact of Chain-of-Thought (CoT) prompting, which incorporates user-item interaction to aid LLMs in capturing user preferences and needs more effectively."
- **Break condition:** If user behavior sequences are too short or noisy, CoT prompting may introduce confusion rather than improving recommendation accuracy.

## Foundational Learning

- **Concept:** Prompt engineering and template design
  - **Why needed here:** The effectiveness of LLMs on recommendation tasks heavily depends on how well prompts are constructed to convey task requirements and format constraints.
  - **Quick check question:** Can you explain the difference between task description, behavior injection, and format indicator modules in prompt construction?

- **Concept:** Parameter-efficient fine-tuning (PEFT) methods
  - **Why needed here:** Direct fine-tuning of large LLMs is computationally expensive, so PEFT methods like P-tuning and LoRA are used to adapt models to recommendation tasks efficiently.
  - **Quick check question:** What are the trade-offs between using P-tuning vs LoRA for fine-tuning LLMs on recommendation tasks?

- **Concept:** Evaluation metrics for recommendation systems
  - **Why needed here:** Different recommendation tasks require different evaluation approaches, from accuracy metrics (RMSE, HR@k) to explainability metrics (BLEU, ROUGE).
  - **Quick check question:** Why might BLEU and ROUGE scores not fully capture the quality of explanations generated by LLMs?

## Architecture Onboarding

- **Component map:** Input data -> Prompt construction (task description, behavior injection, format indicator) -> LLM inference (off-the-shelf or fine-tuned) -> Output validation and refinement -> Evaluation pipeline

- **Critical path:** 1. Input user/item data and task specification 2. Generate task-specific prompts using prompt construction module 3. Feed prompts to LLM inference engine 4. Validate and refine LLM outputs 5. Evaluate results using appropriate metrics

- **Design tradeoffs:**
  - Off-the-shelf vs fine-tuned LLMs: Trade-off between zero-shot generalization and task-specific performance
  - Prompt complexity vs inference cost: More detailed prompts may improve performance but increase computational overhead
  - Objective vs qualitative evaluation: Automated metrics may not capture the full quality of generated explanations

- **Failure signatures:**
  - N/A outputs indicate inability to parse or generate required formats
  - Low accuracy metrics suggest insufficient domain adaptation
  - Generic or repetitive explanations indicate lack of personalization

- **First 3 experiments:**
  1. Test off-the-shelf LLM performance on rating prediction with simple prompts to establish baseline
  2. Apply supervised fine-tuning on ChatGLM for rating prediction and compare with off-the-shelf performance
  3. Evaluate explainability tasks (explanation generation and review summarization) using both objective metrics and qualitative assessment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on recommendation tasks with different dataset sizes and sparsity levels?
- Basis in paper: [inferred] The paper mentions that the Amazon Beauty dataset used in the study has 99.93% sparsity and that the findings may not be applicable to other datasets due to different characteristics among datasets.
- Why unresolved: The study only conducted experiments on the Amazon Beauty dataset, which may not be representative of all recommendation scenarios. Different datasets may have varying levels of sparsity and size, which could affect LLM performance.
- What evidence would resolve it: Evaluating LLM performance on multiple recommendation datasets with varying sizes and sparsity levels would provide insights into how these factors impact their effectiveness.

### Open Question 2
- Question: How do LLMs perform on recommendation tasks with different types of user-item interactions and product attributes?
- Basis in paper: [inferred] The paper mentions that LLMs may underperform in accuracy-based tasks like sequential and direct recommendation due to the lack of extensive exposure to all potential candidate items without specific fine-tuning using recommendation data.
- Why unresolved: The study only evaluated LLMs on a single dataset (Amazon Beauty) with specific types of user-item interactions and product attributes. Different types of interactions and attributes may require different modeling approaches, and it is unclear how LLMs would adapt to these variations.
- What evidence would resolve it: Evaluating LLM performance on multiple recommendation datasets with diverse types of user-item interactions and product attributes would provide insights into their adaptability to different scenarios.

### Open Question 3
- Question: How do LLMs compare to task-specific models in terms of recommendation quality and explainability?
- Basis in paper: [explicit] The paper mentions that LLMs perform moderately on accuracy-based tasks but excel in explainability-based tasks like explanation generation and review summarization. However, it also notes that objective metrics like BLEU and ROUGE fail to accurately measure the true capability of LLM-based recommender systems.
- Why unresolved: While the paper provides some comparisons between LLMs and task-specific models, a more comprehensive evaluation is needed to fully understand the trade-offs between recommendation quality and explainability.
- What evidence would resolve it: Conducting a large-scale comparison between LLMs and task-specific models on multiple recommendation tasks, using both objective and subjective evaluation metrics, would provide a more complete understanding of their relative strengths and weaknesses.

## Limitations
- Limited to a single dataset (Amazon Beauty), which may not represent the full diversity of recommendation scenarios
- Does not explore cross-domain transfer capabilities or performance on sparse datasets
- Comparison with state-of-the-art baselines is limited to P5 without broader benchmarking against other specialized recommendation systems

## Confidence

- **High confidence**: The observation that LLMs perform better on explainability tasks than accuracy-based tasks is well-supported by both quantitative metrics and qualitative analysis.
- **Medium confidence**: The effectiveness of supervised fine-tuning for improving task compliance, as this is demonstrated within the study's controlled conditions but may vary with different datasets and model architectures.
- **Low confidence**: The claim that CoT prompting significantly improves sequential recommendation, as this mechanism lacks direct evidence from corpus neighbors and requires further validation.

## Next Checks

1. Test the proposed framework on multiple recommendation datasets (e.g., MovieLens, Amazon Books) to assess cross-domain generalization capabilities.
2. Conduct ablation studies on prompt components (task description, behavior injection, format indicator) to quantify their individual contributions to performance.
3. Compare LLM-based recommendation performance against a broader range of specialized recommendation systems to establish competitive positioning.