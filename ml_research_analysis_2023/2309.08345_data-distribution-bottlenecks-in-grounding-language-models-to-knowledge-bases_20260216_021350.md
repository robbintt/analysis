---
ver: rpa2
title: Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases
arxiv_id: '2309.08345'
source_url: https://arxiv.org/abs/2309.08345
tags:
- tiara
- data
- gain
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness challenges that language
  models (LMs) face when grounded to knowledge bases (KBs), focusing on inconsistent
  data distribution between training and inference. The study explores environmental
  aspects (schema-level generalization), linguistic aspects (paraphrase adaptation),
  integrated aspects (cross-dataset transfer), and learning aspects (in-context learning
  for large LMs).
---

# Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases

## Quick Facts
- **arXiv ID**: 2309.08345
- **Source URL**: https://arxiv.org/abs/2309.08345
- **Reference count**: 38
- **Primary result**: Even with data augmentation and retrieval augmentation, advanced language models exhibit poor robustness in KBQA due to data distribution inconsistencies between training and inference.

## Executive Summary
This paper investigates the robustness challenges language models face when grounded to knowledge bases, focusing on data distribution inconsistencies between training and inference. The authors propose two techniques—data augmentation (GAIN) and retrieval augmentation—to mitigate these issues. Through comprehensive experiments on multiple KBQA benchmarks, they demonstrate that current language models, even with these techniques, still struggle with schema-level generalization, paraphrase adaptation, and cross-dataset transfer, highlighting the need for improved data collection and learning paradigms.

## Method Summary
The paper proposes data augmentation via GAIN (Graph search and question generation) and retrieval augmentation to improve KBQA model robustness. GAIN generates synthetic KBQA data by sampling logical forms or triples from a knowledge base, verbalizing them into natural language questions using a pre-trained question generator, and appending these to the training corpus. For retrieval augmentation, relevant KB contexts are retrieved for k samples and combined with the input question to assist LLM grounding. The approach is evaluated across multiple benchmarks (GrailQA, GraphQuestions, WebQSP) using models like TIARA (T5-base/3B) and GPT-3.5-turbo.

## Key Results
- Fine-tuned models without WebQSP fine-tuning achieve F1 scores of only 43.0%, while state-of-the-art fine-tuned models reach 79.6%
- TIARA+GAIN achieves the highest EM scores, including on zero-shot scenes
- Despite augmentation techniques, advanced LMs still fall short of perfect performance, revealing fragility in current LM robustness
- Larger models (T5-3B vs T5-base) combined with richer GAIN data significantly enhance generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data augmentation via synthetic generation (GAIN) expands the coverage of schema items and relation types, mitigating the distributional gap between training and inference.
- **Mechanism**: GAIN samples arbitrary logical forms or triples from a KB, verbalizes them into natural language using a pre-trained question generator, and appends these synthetic pairs to the training corpus before fine-tuning the KBQA model.
- **Core assumption**: A model fine-tuned on both real and synthetic KBQA data will generalize better to unseen schema items because it has been exposed to a wider variety of domain-specific relations and classes during training.
- **Evidence anchors**: [abstract] "Our findings reveal that even when employed with such techniques (the highest EM score is achieved on the GrailQA benchmark), advanced small and large LMs still fall short..." [section] "TIARA+GAIN achieves the highest EM scores, including that on zero-shot scenes. This demonstrates promising ideas for further improving LM generalization capabilities..."
- **Break condition**: If synthetic data is too dissimilar from real data distribution, the model may overfit to synthetic patterns and perform worse on real queries.

### Mechanism 2
- **Claim**: Retrieval augmentation supplies in-context examples that bridge the gap between natural language pre-training and structured KB reasoning.
- **Mechanism**: For each query, k relevant examples are retrieved from a combined corpus of real and synthetic KBQA data; these are inserted into the prompt for an LLM, improving the quality of in-context learning.
- **Core assumption**: In-context learning benefits from contextually relevant examples that match the KB schema and linguistic style of the query.
- **Evidence anchors**: [abstract] "To assist with grounding LLM, we retrieve KB contexts with off-the-shelf retrievers for k samples and the input question." [section] "GPT-3.5 outperforms the ELF contexts but falls short compared to T5 generators."
- **Break condition**: Retrieval quality is limited by the underlying retriever; if examples are irrelevant or of low quality, in-context learning may be impaired.

### Mechanism 3
- **Claim**: Larger parametric models (e.g., T5-3B vs T5-base) combined with augmented data improve generalization across compositional and zero-shot schema levels.
- **Mechanism**: Increasing model size expands the capacity to learn complex logical form structures and generalize to unseen relations, especially when paired with synthetic data that covers a broader schema distribution.
- **Core assumption**: Model capacity is a bottleneck; with sufficient data, a larger model can capture patterns not learnable by smaller models.
- **Evidence anchors**: [abstract] "While the LM is a promising technology, the robustness of the current form in dealing with complex environments is fragile..." [section] "An increased number of model parameters, combined with richer data from GAIN, significantly enhance the generalization capabilities of T5 models."
- **Break condition**: If data augmentation is insufficient or if the task requires reasoning beyond learned patterns, simply scaling parameters may yield diminishing returns.

## Foundational Learning

- **Concept**: Semantic parsing as mapping natural language to formal logical forms.
  - Why needed here: KBQA fundamentally requires converting user queries into executable logical forms over a KB.
  - Quick check question: Given "Who designed Dark Sun: Wake of the Ravager?", what logical form should the model produce?
- **Concept**: Data distribution shift and its impact on generalization.
  - Why needed here: The paper emphasizes that LMs struggle when training and inference distributions differ, especially at schema level.
  - Quick check question: Why does a model trained on i.i.d. schema fail on zero-shot relations?
- **Concept**: Retrieval-augmented generation (RAG) paradigm.
  - Why needed here: LLM grounding relies on retrieving relevant KB contexts to reduce hallucination and improve answer fidelity.
  - Quick check question: How does retrieval quality affect in-context learning outcomes in LLM-driven KBQA?

## Architecture Onboarding

- **Component map**: Question generator (T5-base) → synthetic data creation → Schema retrievers (TIARA) → candidate entity/class/relation retrieval → Logical form generator (T5-base/3B) → final parsing → Retrieval engine (BM25 + TIARA retrievers) → context selection for LLM prompts → LLM (GPT-3.5-turbo) → in-context reasoning
- **Critical path**: Question → Entity linking → Schema retrieval → Logical form generation (or retrieval augmentation for LLM) → Answer extraction
- **Design tradeoffs**:
  - Synthetic vs real data: Synthetic expands coverage but may introduce noise.
  - Model size vs cost: Larger models improve robustness but increase inference latency.
  - Retrieval vs fine-tuning: Retrieval is cheaper but depends on quality of stored examples.
- **Failure signatures**:
  - High variance in F1 across paraphrases → poor linguistic generalization.
  - Low Hits@k on unseen schema → schema retrieval failure.
  - Substring-only LLM predictions → over-reliance on prompt rather than reasoning.
- **First 3 experiments**:
  1. Evaluate schema retrieval recall on GrailQA dev set with and without GAIN augmentation.
  2. Compare F1 and stdF1 for TIARA vs TIARA+GAIN across i.i.d., compositional, and zero-shot splits.
  3. Measure in-context LLM performance with BM25 vs TIARA-retrieved contexts on WebQSP without fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of data augmentation techniques vary across different knowledge base schemas and domains?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the proposed GAIN method improves performance across various settings, but does not provide detailed analysis of how the technique performs on different schemas and domains.
- What evidence would resolve it: Conducting experiments on a wider range of knowledge base schemas and domains, and comparing the performance of GAIN with other data augmentation techniques.

### Open Question 2
- Question: What is the impact of model architecture on the effectiveness of data augmentation and retrieval augmentation techniques?
- Basis in paper: Inferred
- Why unresolved: The paper evaluates the performance of T5 models with and without data augmentation, but does not explore the impact of different model architectures on the effectiveness of these techniques.
- What evidence would resolve it: Comparing the performance of various model architectures, such as BERT, RoBERTa, and GPT, with and without data augmentation and retrieval augmentation.

### Open Question 3
- Question: How does the quality of synthetic data generated by GAIN compare to human-annotated data in terms of improving model performance?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that synthetic data is less diverse and natural than human annotations, but does not provide a quantitative comparison of their impact on model performance.
- What evidence would resolve it: Conducting experiments to compare the performance of models trained on synthetic data generated by GAIN and human-annotated data, and analyzing the differences in their effectiveness.

## Limitations
- The study's findings are primarily based on the GrailQA benchmark, which may not fully represent the diversity of real-world KBQA scenarios.
- The effectiveness of synthetic data generation depends heavily on the quality of the question generator and the diversity of the sampling strategy.
- The retrieval augmentation approach relies on off-the-shelf retrievers and existing KBQA data, limiting performance by the quality and relevance of retrieved examples.

## Confidence
- **High confidence**: The core finding that data distribution inconsistencies between training and inference significantly impact KBQA performance is well-supported by experimental results across multiple benchmarks and model configurations.
- **Medium confidence**: The proposed GAIN augmentation technique shows promising results, but the improvements (particularly on zero-shot scenes) may be influenced by specific characteristics of the GrailQA dataset that don't generalize universally.
- **Low confidence**: The claim that simply scaling model parameters (T5-3B vs T5-base) combined with GAIN significantly improves generalization requires more extensive validation, as the paper provides limited ablation studies on model size effects.

## Next Checks
1. **Cross-domain generalization test**: Evaluate the proposed methods on KBQA datasets from different domains (e.g., biomedical, legal) to assess whether the observed robustness improvements transfer beyond the tested Freebase-based benchmarks.

2. **Synthetic data quality analysis**: Conduct a systematic study varying the diversity and quality of synthetic data generated by GAIN, measuring the trade-off between synthetic data volume and performance gains to identify optimal augmentation strategies.

3. **Long-tail schema performance**: Analyze model performance specifically on rare and unseen relations in zero-shot and compositional splits, providing per-relation metrics to understand whether improvements come from better handling of common patterns or genuine schema generalization.