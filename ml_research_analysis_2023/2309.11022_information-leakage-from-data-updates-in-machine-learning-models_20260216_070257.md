---
ver: rpa2
title: Information Leakage from Data Updates in Machine Learning Models
arxiv_id: '2309.11022'
source_url: https://arxiv.org/abs/2309.11022
tags:
- data
- updated
- records
- attribute
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates information leakage from data updates in
  machine learning models. Unlike previous work focusing on data addition or deletion,
  it examines scenarios where attribute values of existing training data points are
  changed.
---

# Information Leakage from Data Updates in Machine Learning Models

## Quick Facts
- arXiv ID: 2309.11022
- Source URL: https://arxiv.org/abs/2309.11022
- Reference count: 17
- Key outcome: Two-model attribute inference attacks outperform single-model attacks by leveraging confidence score differences, with rare values and multiple simultaneous updates increasing vulnerability.

## Executive Summary
This paper investigates information leakage from data updates in machine learning models, specifically when attribute values of existing training data points are changed rather than entire records being added or removed. The authors propose attribute inference attacks that leverage confidence score differences between pre-update and post-update models to infer the updated values. Experiments on two datasets (Census and Lending Club) demonstrate that two-model attacks significantly outperform single-model attacks, with success rates improving when multiple records are updated simultaneously and when the updated values are rare.

## Method Summary
The paper proposes two attribute inference attack methods: single-model attack using confidence scores from the updated model only, and two-model attack using confidence score differences between pre-update and post-update models. The methodology involves training original models, creating updated datasets with attribute changes, retraining updated models, and running attacks to infer updated attribute values. The evaluation uses two datasets (Census with 1.6M records and Lending Club with 1.7M records) and two model types (MLP and logistic regression), measuring attack success rates by accuracy of guessing updated attribute values.

## Key Results
- Two-model attacks outperform single-model attacks by leveraging confidence score differences between model snapshots
- Data records with rare values are significantly more vulnerable to attacks (up to 65% success rate vs 6% for common values)
- Attack success rates improve when multiple records with the same original and updated values are changed simultaneously
- Record inference attacks show limited success but demonstrate increased information leakage when comparing two models versus a single model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-model attacks outperform single-model attacks in attribute inference by leveraging differences in confidence scores between pre-update and post-update models.
- Mechanism: The attacker computes the difference in confidence vectors for each possible attribute value across both models and selects the value with the largest difference. This difference amplifies the signal of the updated attribute value, making it easier to identify compared to using only the post-update model.
- Core assumption: The confidence score differences between models are meaningful indicators of attribute value changes, and these differences are not random noise.
- Evidence anchors:
  - [abstract] "We propose attacks based on the difference in the prediction confidence of the original model and the updated model."
  - [section] "It then computes the difference between the vectors returned by the two models (updated minus original) and picks the value that corresponds to the largest difference."
  - [corpus] Weak evidence - no direct corpus paper discusses confidence score differences between model snapshots for attribute inference.
- Break condition: If the training process introduces significant randomness (different seeds) that masks the effect of individual data point updates, the confidence differences may become unreliable indicators.

### Mechanism 2
- Claim: Data records with rare values are more vulnerable to attacks because their presence in the model has a disproportionate influence on confidence scores.
- Mechanism: Rare values create distinct patterns in model behavior that are more easily detected when comparing model snapshots. When a rare value changes, it produces a larger observable effect on model confidence scores than common values.
- Core assumption: The model's internal representations and confidence scores are sensitive to rare training examples, and these sensitivities persist through model updates.
- Evidence anchors:
  - [abstract] "Moreover, we observe that data records with rare values are more vulnerable to attacks"
  - [section] "We observe that rare values suffer greater attack success rates. For example, 'CA' (California), is the most common value in the LendingClub data... When State value 'CA' is updated to another common value 'NY'... the success rates by two-model attack is 6%, but when changed to a rare value 'SD'... the attack success rates rise to 65%."
  - [corpus] Weak evidence - no direct corpus paper discusses vulnerability correlation with value rarity.
- Break condition: If the model architecture or training process explicitly downweights rare examples or uses techniques like data augmentation to reduce their influence, the attack effectiveness on rare values may diminish.

### Mechanism 3
- Claim: When multiple records with the same original and updated values are changed simultaneously, the attacker has higher success rates because the collective effect creates a stronger signal in model confidence differences.
- Mechanism: Repeated identical changes create a cumulative effect on the model's learned representations, making the confidence score differences more pronounced and easier to detect compared to single-record changes.
- Core assumption: The model's parameters change in a way that scales with the number of identical updates, and these changes are detectable through confidence score analysis.
- Evidence anchors:
  - [abstract] "When multiple records with the same original attribute value are updated to the same new value (i.e., repeated changes), the attacker is more likely to correctly guess the updated values since repeated changes leave a larger footprint on the trained model."
  - [section] "We observe that compared to single record update above, when multiple records are updated, the attacker is more likely to correctly guess the updated value given the partial target record."
  - [corpus] Weak evidence - no direct corpus paper discusses multi-record update effects on attribute inference attacks.
- Break condition: If the model uses batch normalization or other techniques that normalize across the entire dataset, the effect of multiple identical changes may be dampened and become harder to detect.

## Foundational Learning

- Concept: Black-box model access and confidence score interpretation
  - Why needed here: The attacks rely on querying models and interpreting their confidence scores without access to internal parameters or gradients.
  - Quick check question: How would you query a black-box model to obtain confidence scores for multiple candidate attribute values?

- Concept: Attribute inference attack methodology
  - Why needed here: The paper builds on existing attribute inference attack frameworks but extends them to the model update setting.
  - Quick check question: What is the key difference between single-model and two-model attribute inference attacks?

- Concept: Data rarity and its impact on machine learning models
  - Why needed here: Understanding why rare values are more vulnerable requires knowledge of how machine learning models handle imbalanced data.
  - Quick check question: Why might rare values have a disproportionate impact on model confidence scores?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (original) -> Data update -> Model training (updated) -> Attack execution -> Result analysis
- Critical path: Load dataset → Preprocess data → Train original model → Create updated dataset → Train updated model → Run attacks → Analyze results
- Design tradeoffs: Using black-box access limits attack capabilities but makes the threat model more realistic; using confidence scores is simpler than gradient-based methods but may be less powerful; focusing on tabular data limits generalizability to other domains.
- Failure signatures: Attacks fail when confidence score differences are not meaningful (e.g., due to randomization), when the update doesn't significantly change model behavior, or when the attacker's background knowledge assumptions are violated.
- First 3 experiments:
  1. Implement single-model attack on a simple synthetic dataset to verify basic functionality
  2. Implement two-model attack on the same synthetic dataset to compare performance
  3. Test both attacks on the Census dataset with a single record update to validate against real data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different random seeds in model training affect the success rates of attribute inference attacks in the update setting?
- Basis in paper: [explicit] The paper discusses that results for the Census dataset depend on random seed used for weight initialization and shuffling, and that using different seeds between original and updated models reduces attack success rates but still outperforms single-model attacks.
- Why unresolved: The paper only briefly mentions this observation and does not systematically investigate how different random seeds impact attack effectiveness or develop defense mechanisms to exploit this phenomenon.
- What evidence would resolve it: Controlled experiments varying random seeds between original and updated models, measuring attack success rates, and developing defenses that leverage randomization to mask individual data point updates.

### Open Question 2
- Question: How effective are batch updates with heterogeneous changes as a defense against attribute inference attacks in the update setting?
- Basis in paper: [explicit] The paper discusses batch updates as a potential defense, noting that heterogeneous changes in opposing directions can potentially cancel the effect of each other on the trained model, but does not empirically evaluate this defense strategy.
- Why unresolved: The paper only speculates about the potential effectiveness of batch updates with heterogeneous changes and does not conduct experiments to validate this defense approach.
- What evidence would resolve it: Experiments comparing attack success rates on models updated with homogeneous versus heterogeneous batches of data changes, measuring the impact on information leakage.

### Open Question 3
- Question: How does differential privacy at various privacy budgets affect the utility-accuracy trade-off in models updated with attribute changes?
- Basis in paper: [explicit] The paper discusses differential privacy as a potential defense, noting that it can reduce information leakage but may also reduce model utility, and that its use is only reasonable if the updated model can still maintain accuracy in utility-sensitive settings.
- Why unresolved: The paper does not conduct experiments to evaluate the trade-off between privacy protection and model utility when using differential privacy in the update setting with attribute changes.
- What evidence would resolve it: Experiments training and updating models with differential privacy at various privacy budgets, measuring both information leakage reduction and model accuracy degradation.

## Limitations
- Limited generalizability beyond tabular data to domains like images or text
- Does not address potential defenses against the proposed attacks
- Assumes attacker has access to both pre-update and post-update model snapshots

## Confidence

**High Confidence**: The observation that two-model attacks outperform single-model attacks is well-supported by experimental results across both datasets and attack types. The claim that rare values are more vulnerable to attacks is strongly supported by empirical evidence showing dramatic differences in success rates between common and rare value updates.

**Medium Confidence**: The claim that repeated changes to multiple records increase attack success rates is supported by experimental data but may depend heavily on specific dataset characteristics and update patterns. The effectiveness of the proposed attacks in real-world scenarios with different model architectures or training procedures remains uncertain.

**Low Confidence**: The generalizability of these attacks to non-tabular data (images, text, etc.) is not demonstrated and may not hold due to different model behaviors and confidence score interpretations across data modalities.

## Next Checks

1. **Randomization Robustness Test**: Repeat the experiments with different random seeds for model training to assess how training randomness affects attack success rates. This would validate whether the confidence score differences are robust enough to overcome stochastic variations in the training process.

2. **Defense Implementation Test**: Implement simple defenses such as adding noise to confidence scores or using differential privacy during model updates, then measure how these defenses impact attack effectiveness. This would provide insight into the practical feasibility of mitigating these attacks.

3. **Cross-Domain Generalization Test**: Apply the same attack methodology to image datasets (e.g., CIFAR-10 with attribute-like modifications) to assess whether the confidence score difference approach generalizes beyond tabular data. This would test the fundamental assumption about confidence score behavior across different data types.