---
ver: rpa2
title: 'Mind the spikes: Benign overfitting of kernels and neural networks in fixed
  dimension'
arxiv_id: '2305.14077'
source_url: https://arxiv.org/abs/2305.14077
tags:
- kernel
- then
- kernels
- activation
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies benign overfitting in kernel regression and
  neural networks, challenging the prevailing belief that it requires high-dimensional
  settings. The authors show that benign overfitting is possible in fixed dimension
  by designing estimators with large derivatives ("spiky-smooth" estimators).
---

# Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension

## Quick Facts
- arXiv ID: 2305.14077
- Source URL: https://arxiv.org/abs/2305.14077
- Reference count: 40
- Primary result: Benign overfitting is possible in fixed dimension through "spiky-smooth" estimators with large derivatives

## Executive Summary
This paper challenges the prevailing belief that benign overfitting requires high-dimensional settings. The authors show that benign overfitting is possible in fixed dimension by designing estimators with large derivatives ("spiky-smooth" estimators). For kernel regression, they prove that inconsistency arises when estimators overfit with moderate Sobolev norm, while consistency is achieved when estimators have large derivatives. Translating these results to neural networks via neural tangent kernels and neural network Gaussian processes, they demonstrate that ReLU networks can be modified with small high-frequency fluctuations in the activation function to enable benign overfitting. Experimental results verify that neural networks with these spiky-smooth activation functions can generalize well even when interpolating small, low-dimensional datasets.

## Method Summary
The paper analyzes consistency of kernel regression estimators with large derivatives and translates results to neural networks via NTK/NNGP. The authors design spiky-smooth activation functions by adding small high-frequency fluctuations to ReLU, then validate experimentally on synthetic low-dimensional data. They compare standard ReLU networks against networks with spiky-smooth activation, showing the latter can interpolate while maintaining low test error.

## Key Results
- Benign overfitting is possible in fixed dimension if and only if the estimator's derivatives are large enough
- Adding small high-frequency fluctuations to activation functions enables harmless interpolation with wide neural networks
- The RKHS norm of overfitting estimators must grow at a controlled rate to ensure consistency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Overfitting is benign in fixed dimension if and only if the estimator's derivatives are large enough.
- **Mechanism**: The paper shows that inconsistency arises when estimators overfit with moderate Sobolev norm (small derivatives). Conversely, consistency is achieved when estimators have large derivatives ("spiky-smooth" estimators), even in fixed dimension. This challenges the prevailing belief that benign overfitting requires high-dimensional settings.
- **Core assumption**: The smoothness of the estimator, not the dimension, is the key factor determining benign overfitting.
- **Evidence anchors**:
  - [abstract]: "benign overfitting is possible if and only if the estimator's derivatives are large enough"
  - [section 1]: "We argue that the dimension-dependent perspective does not capture the full picture of benign overfitting"
- **Break condition**: If the estimator's derivatives remain moderate despite overfitting, inconsistency will persist.

### Mechanism 2
- **Claim**: Adding small high-frequency fluctuations to activation functions enables harmless interpolation with wide neural networks.
- **Mechanism**: The paper translates spikes in kernels (which lead to benign overfitting in kernel regression) into infinitesimal fluctuations that can be added to activation functions. These small high-frequency oscillations can fit noisy observations without affecting the smooth component too much, enabling benign overfitting.
- **Core assumption**: The spikes in kernels can be translated into high-frequency fluctuations in activation functions.
- **Evidence anchors**:
  - [abstract]: "This can be fixed by adding small high-frequency fluctuations to the activation function"
  - [section 1]: "Training finite neural networks with gradient descent shows that spiky-smooth activation functions can indeed achieve good generalization even when interpolating small, low-dimensional data sets"
- **Break condition**: If the added fluctuations are too large or too small, they may either harm generalization or fail to enable interpolation.

### Mechanism 3
- **Claim**: The RKHS norm of overfitting estimators must grow at a controlled rate to ensure consistency.
- **Mechanism**: The paper shows that gradient flow and gradient descent initialized at 0 have monotonically growing H-norm, and that Assumption (N) with Cnorm = 1 holds for all estimators ft,ρ from (1). This implies that the RKHS norm of overfitting estimators must not grow faster than the minimum norm required to interpolate the training data.
- **Core assumption**: The RKHS norm of overfitting estimators is bounded by a constant factor of the minimum-norm interpolant.
- **Evidence anchors**:
  - [section 4.1]: "every differentiable function f that overfits the training data and is not much 'spikier' than the minimum RKHS-norm interpolant is inconsistent"
  - [appendix C.2]: "under gradient flow and gradient descent with sufficiently small learning rates initialized at 0, the RKHS norm grows monotonically with time t"
- **Break condition**: If the RKHS norm grows too fast, inconsistency will result.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: The paper heavily relies on RKHS theory to analyze the consistency of kernel regression and neural network estimators.
  - Quick check question: Can you explain the relationship between a kernel function and its corresponding RKHS?

- **Concept: Sobolev Spaces**
  - Why needed here: The paper characterizes the RKHS of common kernels (Laplace, Matérn, Gaussian) as Sobolev spaces and uses Sobolev norm bounds to analyze consistency.
  - Quick check question: How does the smoothness of a function relate to its Sobolev norm?

- **Concept: Neural Tangent Kernel (NTK) and Neural Network Gaussian Process (NNGP)**
  - Why needed here: The paper translates results from kernel regression to neural networks via the NTK and NNGP, showing how activation functions can be designed to enable benign overfitting.
  - Quick check question: What is the relationship between the NTK/NNGP and the function learned by a wide neural network?

## Architecture Onboarding

- **Component map**: Kernel regression (estimators, RKHS norm bounds, consistency analysis) -> Neural networks (NTK/NNGP, activation functions) -> Experimental validation (synthetic data, trained networks, performance metrics)
- **Critical path**: 1) Analyze consistency of kernel regression estimators with large derivatives, 2) Translate results to neural networks via NTK/NNGP, 3) Design spiky-smooth activation functions, 4) Validate experimentally on synthetic data
- **Design tradeoffs**: Spiky-smooth kernels vs. standard kernels (improved consistency vs. potential computational cost), large derivatives vs. smooth components (benign overfitting vs. potential overfitting of noise), high-frequency fluctuations vs. smooth activation functions (harmless interpolation vs. potential gradient issues)
- **Failure signatures**: Inconsistency of overfitting estimators with moderate derivatives, poor generalization of neural networks with standard activation functions, explosion of gradients with spiky-smooth activation functions
- **First 3 experiments**:
  1. Implement kernel regression with spiky-smooth kernels on synthetic data and evaluate consistency
  2. Train neural networks with spiky-smooth activation functions and compare performance to standard activation functions
  3. Analyze the decomposition of neural network predictions into signal and spike components

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but identifies several directions for future work including understanding when and why neural networks with spiky-smooth activation functions learn target functions well, designing architectures and learning algorithms that provably work on complex datasets, and determining statistical convergence rates.

## Limitations
- The exact boundary between benign and harmful spikes remains unclear - how sensitive is the mechanism to spike frequency and amplitude choices?
- Translation from kernel theory to practical neural networks may lose some theoretical guarantees, particularly regarding the decomposition of predictions into smooth and spike components
- The empirical validation uses synthetic data with specific noise structures, limiting generalizability to real-world datasets

## Confidence
- **High confidence**: The theoretical framework connecting RKHS norms, Sobolev smoothness, and consistency in kernel regression is well-established and rigorously proven
- **Medium confidence**: The translation of spiky kernels to spiky-smooth activation functions is conceptually sound, but practical implementation may introduce deviations from theoretical predictions
- **Medium confidence**: Experimental results show the mechanism works on synthetic data, but the small scale and simplified setup warrant cautious interpretation

## Next Checks
1. Test spiky-smooth activation functions on multiple synthetic datasets with varying noise levels and dimensionality to establish robustness boundaries
2. Implement a quantitative measure to verify the decomposition of network predictions into smooth and spike components, confirming that spikes are fitting noise while smooth components capture signal
3. Compare performance against alternative approaches to benign overfitting (such as specific regularization schemes) to establish relative effectiveness