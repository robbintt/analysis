---
ver: rpa2
title: 'CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel
  Metric'
arxiv_id: '2308.16126'
source_url: https://arxiv.org/abs/2308.16126
tags:
- image
- embeddings
- corrembed
- performance
- similar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CorrEmbed, a novel evaluation method for
  assessing pre-trained computer vision models' performance in image similarity search
  tasks. The approach computes the correlation between distances in image embeddings
  and distances in human-generated tag vectors.
---

# CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric

## Quick Facts
- arXiv ID: 2308.16126
- Source URL: https://arxiv.org/abs/2308.16126
- Reference count: 26
- Key outcome: Novel evaluation method (CorrEmbed) measures pre-trained computer vision models' performance in image similarity search by correlating embedding distances with human-generated tag distances, revealing that ImageNet1k accuracy generally predicts tag-correlation scores.

## Executive Summary
This paper introduces CorrEmbed, a novel evaluation method for assessing pre-trained computer vision models' performance in image similarity search tasks. The approach computes the correlation between distances in image embeddings and distances in human-generated tag vectors. The authors evaluate numerous pre-trained Torchvision models, revealing a strong relationship between ImageNet1k accuracy scores and tag-correlation scores. The method identifies deviations from this pattern, providing insights into how different models capture high-level image features. The top-performing model, ViT_H_14_E2E, achieves a CorrEmbed score of 0.4715, surpassing even Fashion-CLIP, a model fine-tuned to the fashion domain.

## Method Summary
CorrEmbed evaluates image similarity by calculating the Pearson correlation coefficient between pairwise cosine distances of image embeddings and corresponding tag embeddings from the FJONG dataset. The method uses entropy-based weighting to emphasize tag categories with higher information content, reflecting user purchase preferences. Rather than training models, the approach extracts embeddings from pre-trained Torchvision models and computes weighted correlations across multiple samples. The evaluation compares models across different architectures and layers, with particular attention to penultimate versus output layer embeddings.

## Key Results
- Strong linear relationship exists between ImageNet1k accuracy and CorrEmbed tag-correlation scores
- Penultimate layer embeddings generally outperform output layer embeddings for similarity tasks
- ViT_H_14_E2E achieves the highest CorrEmbed score of 0.4715, exceeding domain-specific Fashion-CLIP
- Entropy weighting improves correlation relevance by emphasizing high-information tag categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CorrEmbed evaluates image similarity by correlating distances in embedding space with distances in human-generated tag space
- Mechanism: The metric computes pairwise distances between image embeddings and pairwise distances between corresponding tag embeddings, then calculates the Pearson correlation coefficient between these two distance sets
- Core assumption: Human-generated tags capture meaningful semantic relationships between images that should be reflected in the embedding space
- Evidence anchors:
  - [abstract] "Our approach computes the correlation between distances in image embeddings and distances in human-generated tag vectors"
  - [section] "We evaluate their performance by calculating the correlation of the distances between pairs of image embeddings and the distances between pairs of tag embeddings"
- Break condition: If human tags do not accurately represent semantic similarity, or if the embedding space captures features irrelevant to the tags, the correlation would break down

### Mechanism 2
- Claim: Better ImageNet1k performance correlates with better CorrEmbed scores
- Mechanism: Models that perform well on ImageNet classification tasks tend to learn features that also capture meaningful semantic relationships useful for similarity search
- Core assumption: The features learned for classification generalize to semantic similarity tasks
- Evidence anchors:
  - [abstract] "revealing an intuitive relationship of linear scaling between ImageNet1k accuracy scores and tag-correlation scores"
  - [section] "Model performance on CorrEmbed generally increases by the model size and ImageNet performance"
- Break condition: If a model overfits to ImageNet classes without learning generalizable semantic features, or if the classification task diverges significantly from similarity search requirements

### Mechanism 3
- Claim: Using penultimate layer embeddings often outperforms final layer embeddings for similarity tasks
- Mechanism: Earlier layers capture more fine-grained, general features while final layers are specialized for classification
- Core assumption: The feature space most useful for classification is not necessarily optimal for similarity
- Evidence anchors:
  - [section] "The layers preceding the output layer tend to capture a finer representation of the embeddings"
  - [section] "Table 3 details the scores of the models based on the input embeddings to the penultimate layer rather than the output"
- Break condition: If the penultimate layer features are too general and lack discriminative power, or if the model architecture makes earlier layers less relevant for similarity

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: Core mathematical operation for measuring relationship between image distances and tag distances
  - Quick check question: What does a Pearson correlation coefficient of 0.5 indicate about the relationship between two variables?

- Concept: t-SNE for visualization
  - Why needed here: Used to visualize high-dimensional embedding relationships in 2D/3D space
  - Quick check question: What does t-SNE preserve when reducing dimensionality - absolute distances or relative relationships?

- Concept: Entropy weighting
  - Why needed here: Used to weight tag categories based on their predictive value for user preferences
  - Quick check question: How does entropy relate to information content in a probability distribution?

## Architecture Onboarding

- Component map:
  FJONG dataset -> image loading -> tag processing -> embedding generation -> pairwise distance calculation -> Pearson correlation computation -> weighted score averaging

- Critical path:
  1. Load image and corresponding tag data
  2. Extract embeddings from pre-trained model
  3. Compute pairwise distances for images and tags
  4. Calculate Pearson correlation
  5. Repeat across multiple samples and average
  6. Compare across different models

- Design tradeoffs:
  - Using ImageNet-pretrained models trades domain specificity for broader generalization
  - Computing all pairwise distances is O(n²) but ensures comprehensive evaluation
  - Entropy weighting adds complexity but improves relevance to recommendation context

- Failure signatures:
  - Correlation scores near zero indicate no relationship between image and tag spaces
  - High variance across samples suggests dataset issues or unstable embeddings
  - Poor performance on control methods indicates implementation bugs

- First 3 experiments:
  1. Implement basic CorrEmbed with random tag vectors and random embeddings to verify control methods work
  2. Test on a small subset with a single model (e.g., ResNet50) to validate pipeline
  3. Compare CorrEmbed scores between penultimate layer and final layer embeddings for the same model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CorrEmbed change when evaluated on larger datasets beyond the 18,000-image FJONG dataset used in this study?
- Basis in paper: [inferred] The authors note that the FJONG dataset is relatively small and may contain noise, suggesting potential for evaluation on larger, more diverse datasets.
- Why unresolved: The study was limited to a single dataset, and the authors explicitly mention the potential for noise and inconsistencies in the current dataset.
- What evidence would resolve it: Evaluating CorrEmbed on multiple larger datasets with varying image counts and noise levels, comparing performance metrics across datasets.

### Open Question 2
- Question: To what extent does the choice of embedding layer (e.g., penultimate vs. output layer) affect the correlation between image similarity and tag-based similarity across different model architectures?
- Basis in paper: [explicit] The authors observe that the penultimate layer generally produces better embeddings than the output layer for CorrEmbed performance, but this varies by model.
- Why unresolved: While the study shows this trend, it doesn't fully explore the underlying reasons for architectural differences or establish a comprehensive framework for layer selection.
- What evidence would resolve it: Systematic analysis of embedding quality across multiple layers for each model architecture, correlating layer choice with tag-correlation scores and model-specific characteristics.

### Open Question 3
- Question: How does CorrEmbed performance compare between models trained on ImageNet1k versus models trained on domain-specific datasets (e.g., fashion-specific datasets)?
- Basis in paper: [explicit] The authors compare ImageNet1k models to Fashion-CLIP (a fashion-domain model) and find that some ImageNet1k models outperform it, despite the domain mismatch.
- Why unresolved: The study only includes one domain-specific model (Fashion-CLIP), limiting conclusions about general trends.
- What evidence would resolve it: Benchmarking CorrEmbed scores across multiple models trained on both general and domain-specific datasets, analyzing the trade-offs between domain knowledge and generalization.

### Open Question 4
- Question: What is the impact of visual noise (e.g., background clutter, human presence) on the performance of different pre-trained models in image similarity tasks?
- Basis in paper: [inferred] The authors note that better-performing models are better at ignoring humans and background elements, but this is not systematically quantified.
- Why unresolved: The study uses a dataset with relatively uniform backgrounds and limited human presence, making it difficult to assess the impact of visual noise.
- What evidence would resolve it: Controlled experiments varying levels of visual noise in test images, measuring CorrEmbed scores and qualitative similarity judgments across models.

## Limitations
- Relies on human-generated tags which may not universally capture semantic similarity across domains
- Computationally expensive O(n²) pairwise distance calculations limit scalability
- Single dataset (FJONG) restricts generalizability to other image domains
- Entropy-based weighting assumes tag category entropy correlates with purchase preference

## Confidence

**High Confidence:** The basic CorrEmbed methodology (correlation between image and tag distances) is sound and well-established

**Medium Confidence:** The relationship between ImageNet performance and CorrEmbed scores, as this requires assumptions about feature transferability

**Low Confidence:** The specific entropy-based weighting scheme's effectiveness for different recommendation contexts

## Next Checks
1. **Cross-domain validation:** Test CorrEmbed on non-fashion datasets (e.g., medical imaging, satellite imagery) to assess generalizability beyond the fashion domain
2. **Temporal stability:** Evaluate whether CorrEmbed scores remain consistent across different time periods as fashion trends and tagging conventions evolve
3. **Baseline comparison robustness:** Compare CorrEmbed against established image similarity metrics (e.g., Recall@k, mAP) on standard benchmark datasets to validate its effectiveness