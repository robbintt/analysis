---
ver: rpa2
title: 'Decoding Concerns: Multi-label Classification of Vaccine Sentiments in Social
  Media'
arxiv_id: '2312.10626'
source_url: https://arxiv.org/abs/2312.10626
tags: []
core_contribution: The study focuses on multi-label classification of vaccine-related
  concerns expressed in tweets, identifying 12 distinct categories such as side-effects,
  misinformation, and political issues. Multiple methods were evaluated, including
  BERT-based transformer models, traditional machine learning (SVM, Naive Bayes, Random
  Forest), and a novel prompt engineering approach using GPT-3.5 (davinci-003).
---

# Decoding Concerns: Multi-label Classification of Vaccine Sentiments in Social Media

## Quick Facts
- arXiv ID: 2312.10626
- Source URL: https://arxiv.org/abs/2312.10626
- Reference count: 14
- Best model achieved macro F1 score of 0.55 and Jaccard score of 0.47

## Executive Summary
This study addresses the challenge of multi-label classification of vaccine-related concerns expressed in social media, specifically analyzing 9,921 anti-vaccine tweets to identify 12 distinct concern categories including side-effects, misinformation, and political issues. The research evaluates multiple approaches including transformer models (BERT, DistilBERT), traditional machine learning (SVM, Naive Bayes, Random Forest), and a novel prompt engineering approach using GPT-3.5. The prompt engineering approach with GPT-3.5 slightly outperformed transformer models, achieving the highest macro F1 score of 0.55 and Jaccard score of 0.47 on the test set.

## Method Summary
The study employed a comprehensive approach to multi-label classification of vaccine concerns in tweets. Data preprocessing included stop word removal, lowercasing, punctuation removal, emoji translation, contraction expansion, and URL removal, followed by tokenization and lemmatization using NLTK. The research compared traditional machine learning models (SVM, Naive Bayes, Random Forest) with TF-IDF vectorization, transformer models (BERT, DistilBERT), and a prompt engineering approach using GPT-3.5 (davinci-003). The GPT-3.5 approach utilized structured prompts combining task specification, label descriptions, keyword lists, and illustrative examples to guide the model's reasoning and label selection.

## Key Results
- GPT-3.5 with prompt engineering achieved the best performance with macro F1 score of 0.55 and Jaccard score of 0.47
- DistilBERT variants achieved slightly lower performance (0.54-0.53 F1) compared to GPT-3.5
- The model struggled with minority classes including "conspiracy," "country," and "religious" concerns
- Model hallucinations were observed in generated explanations, requiring further validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering with LLM improves multi-label classification performance by explicitly guiding the model with structured descriptions, keyword lists, and examples.
- Mechanism: The prompt template combines task specification, label descriptions, keyword lists, and illustrative examples, which helps the LLM to align its reasoning with the specific classification task and reduces hallucination.
- Core assumption: Providing detailed context and reasoning steps improves the model's ability to generalize from limited examples.
- Evidence anchors: [abstract] "Through experimental evaluation, we demonstrate the efficacy of our proposed approach - our results not only underscore the prevalence of vaccine related concerns in social media discussions but also shed light on the nuanced nature of these concerns." [section] "We noted an enhancement in model performance when prompts requiring the model to generate explanations or reasonings were incorporated."
- Break condition: If the prompt structure is not well-aligned with the model's capabilities or the task complexity, performance may degrade.

### Mechanism 2
- Claim: Multi-label classification benefits from leveraging the contextual understanding of transformer models like BERT and DistilBERT.
- Mechanism: Transformer models use attention mechanisms to capture context and relationships between words in a tweet, enabling them to identify multiple relevant labels.
- Core assumption: The contextual information encoded by transformers helps in distinguishing subtle differences between labels.
- Evidence anchors: [abstract] "We delve into the application of a diverse set of advanced natural language processing techniques and machine learning algorithms including transformer models like BERT." [section] "DistilBERT base uncased... is a lighter and faster variant of the BERT model that retains much of its capability to understand and generate human-like text."
- Break condition: If the dataset is too small or the labels are too ambiguous, transformers may not perform well.

### Mechanism 3
- Claim: The class imbalance in the dataset poses a challenge for accurate classification, especially for minority classes.
- Mechanism: Imbalanced datasets can lead to models being biased towards majority classes, resulting in poor performance on minority classes.
- Core assumption: Techniques like oversampling, undersampling, or class weighting can mitigate the effects of class imbalance.
- Evidence anchors: [abstract] "Challenges included class imbalance, difficulty with minority classes (e.g., 'conspiracy,' 'country,' 'religious')." [section] "Performing an EDA, we observe that the dataset exhibits class imbalance with certain labels having significantly higher counts compared to others."
- Break condition: If the class imbalance is extreme, even advanced techniques may not be sufficient to achieve good performance.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: The task involves assigning multiple labels to each tweet, reflecting the diverse concerns expressed about vaccines.
  - Quick check question: What is the key difference between multi-label and multi-class classification?
- Concept: Transformer models (BERT, DistilBERT)
  - Why needed here: These models provide contextual understanding of text, which is crucial for identifying subtle differences between vaccine-related concerns.
  - Quick check question: How do transformer models use attention mechanisms to capture context?
- Concept: Prompt engineering
  - Why needed here: The LLM's performance depends on the quality of the prompts, which guide the model's reasoning and label selection.
  - Quick check question: What are the key components of an effective prompt for multi-label classification?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (cleaning, tokenization, lemmatization) -> Traditional ML models (SVM, Naive Bayes, Random Forest) -> Transformer models (BERT, DistilBERT) -> LLM prompt template -> Evaluation metrics (macro F1, Jaccard score)
- Critical path: Data preprocessing → Model training and evaluation → Hyperparameter tuning → Final model selection
- Design tradeoffs:
  - LLM vs. traditional ML models: LLM provides better performance but requires more resources and careful prompt engineering.
  - DistilBERT vs. BERT: DistilBERT is faster and lighter but may sacrifice some performance.
- Failure signatures:
  - Poor performance on minority classes: Indicates class imbalance or insufficient training data.
  - Hallucinations in LLM explanations: Suggests the need for better prompt engineering or model fine-tuning.
- First 3 experiments:
  1. Compare the performance of LLM with different prompt templates.
  2. Evaluate the impact of class weighting on the performance of traditional ML models.
  3. Test the effectiveness of data augmentation techniques for improving performance on minority classes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating sentiment analysis into the multi-label classification model impact performance, especially for minority classes like "religious" and "conspiracy"?
- Basis in paper: [explicit] The paper mentions "integrating sentiment analysis" as future work and notes that the model struggles with minority classes including "religious" and "conspiracy."
- Why unresolved: The paper did not implement sentiment analysis and only experimented with basic multi-label classification approaches.
- What evidence would resolve it: A comparison of model performance (macro F1 and Jaccard scores) between the current approach and an approach incorporating sentiment analysis features or sentiment-specific prompts.

### Open Question 2
- Question: What is the optimal number of training examples needed for the LLM to achieve consistent high performance across all 12 classes, particularly for minority classes?
- Basis in paper: [explicit] The paper states they used only 58 training examples due to API limitations and observed poor performance on minority classes like "country" and "religious."
- Why unresolved: The study was constrained by API costs and only tested with a small subset of training data.
- What evidence would resolve it: A systematic study varying the number of training examples (e.g., 50, 100, 500, 1000) and measuring performance improvements across all classes.

### Open Question 3
- Question: How do different prompt engineering strategies (e.g., chain-of-thought, zero-shot, few-shot) compare in effectiveness for multi-label vaccine concern classification?
- Basis in paper: [explicit] The paper mentions experimenting with various prompting methods including CoT, zero-shot, and few-shot learning, finding that combining them was most effective.
- Why unresolved: The paper did not provide detailed comparative analysis of individual prompting strategies.
- What evidence would resolve it: A controlled experiment testing each prompting strategy separately with identical training data and measuring their respective macro F1 scores and Jaccard scores.

### Open Question 4
- Question: What causes the LLM to hallucinate explanations for label assignments, and how can this be mitigated while preserving model performance?
- Basis in paper: [explicit] The paper observes that "there are instances where the model exhibited hallucinations while generating explanations - false yet credible-sounding content."
- Why unresolved: The paper acknowledges the phenomenon but does not investigate its root causes or potential solutions.
- What evidence would resolve it: Analysis of the correlation between hallucination frequency and specific prompt structures, training data characteristics, or model configurations, along with testing mitigation strategies.

## Limitations
- Class imbalance significantly affects minority class performance (e.g., "conspiracy," "country," "religious")
- Model hallucinations in generated explanations require further validation
- Results based on specific anti-vaccine tweet dataset, limiting generalizability

## Confidence

* **High Confidence:** The methodology for prompt engineering and multi-label classification framework is sound and well-documented. The comparative evaluation between GPT-3.5 and transformer models (DistilBERT, BERT) is methodologically rigorous.
* **Medium Confidence:** The reported performance metrics are reliable for the specific dataset and task, but generalizability to other contexts remains uncertain due to dataset limitations and class imbalance issues.
* **Low Confidence:** The explanations generated by GPT-3.5 for its classifications may contain hallucinations and should be treated with caution until further validation is performed.

## Next Checks
1. Cross-dataset validation: Test the prompt engineering approach on additional vaccine-related datasets from different regions and time periods to assess generalizability.
2. Ablation study on minority classes: Systematically evaluate model performance when training with different strategies for handling minority classes (oversampling, class weighting, synthetic data generation).
3. Hallucination detection mechanism: Implement automated validation to measure and reduce hallucination rates in GPT-3.5 explanations, comparing against ground truth annotations.