---
ver: rpa2
title: 'Lost in the Middle: How Language Models Use Long Contexts'
arxiv_id: '2307.03172'
source_url: https://arxiv.org/abs/2307.03172
tags:
- context
- input
- performance
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language models struggle to effectively use information in long
  contexts, with performance significantly degrading when relevant information is
  in the middle of input contexts. Experiments on multi-document question answering
  and key-value retrieval tasks show a U-shaped performance curve - models perform
  best when relevant information is at the start or end of contexts, but suffer when
  accessing information in the middle.
---

# Lost in the Middle: How Language Models Use Long Contexts

## Quick Facts
- arXiv ID: 2307.03172
- Source URL: https://arxiv.org/abs/2307.03172
- Reference count: 13
- Key outcome: Language models struggle to effectively use information in long contexts, with performance significantly degrading when relevant information is in the middle of input contexts

## Executive Summary
This paper investigates how language models retrieve and use relevant information within long input contexts. Through controlled experiments on multi-document question answering and key-value retrieval tasks, the authors demonstrate that models exhibit a U-shaped performance curve - performing best when relevant information is at the beginning or end of contexts, but suffering when accessing information in the middle. Surprisingly, extended-context models are not necessarily better at using long contexts, and query-aware contextualization only helps in specific scenarios. These findings reveal fundamental limitations in how current transformer-based architectures process information across long sequences.

## Method Summary
The authors conduct controlled experiments varying input context length and the position of relevant information. They evaluate multiple models including GPT-3.5-Turbo, Claude variants, and Contriever on two tasks: multi-document question answering using the NaturalQuestions-Open dataset, and synthetic key-value retrieval. The experiments systematically place relevant information at different positions within contexts ranging from 1K to 128K tokens. Models are evaluated using standard accuracy metrics, with particular attention to how performance changes based on where the relevant information appears in the input sequence.

## Key Results
- Language models show U-shaped performance curves when relevant information is placed at different positions in long contexts
- Extended-context models (e.g., Claude-1.3-100k) do not show improved ability to use information in the middle of contexts
- Query-aware contextualization improves key-value retrieval but has minimal effect on multi-document QA
- Even base models without instruction fine-tuning exhibit the same positional bias patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit positional bias, performing best when relevant information is at the start or end of the input context.
- Mechanism: The model's self-attention mechanism allocates more attention resources to tokens at the beginning and end of the sequence, likely due to positional embeddings and training data distribution.
- Core assumption: Positional embeddings in transformer models create a bias that affects attention distribution across the input sequence.
- Evidence anchors:
  - [abstract]: "We observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts"
  - [section]: "Performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts"
  - [corpus]: Weak - corpus shows related work but no direct evidence of positional bias mechanism
- Break condition: If positional embeddings are modified to be uniform across the sequence, or if attention mechanisms are restructured to allocate equal resources to all positions.

### Mechanism 2
- Claim: Extended-context models are not necessarily better at using their input context.
- Mechanism: Simply increasing the maximum context length does not address the underlying attention allocation problem; the model still struggles to effectively use information in the middle of the sequence.
- Core assumption: The model's attention mechanism has inherent limitations in how it processes and weights information across different positions in the input sequence.
- Evidence anchors:
  - [abstract]: "Extended-context models are not necessarily better at using long contexts"
  - [section]: "GPT-3.5-Turbo (16K) is a version with an extended maximum context length of 16K tokens. We evaluate claude-1.3 and claude-1.3-100k with the Anthropic API; claude-1.3 has a maximum context length of 8K tokens, and claude-1.3-100k has an extended context length of 100K tokens."
  - [corpus]: Weak - corpus mentions related work but no direct evidence of extended-context limitations
- Break condition: If attention mechanisms are modified to equally weight all positions, or if new architectures are developed that can effectively process information throughout the entire sequence.

### Mechanism 3
- Claim: Query-aware contextualization improves key-value retrieval but has minimal effect on multi-document QA.
- Mechanism: Placing the query before and after the data allows decoder-only models to attend to query tokens when contextualizing documents or key-value pairs, improving retrieval performance.
- Core assumption: The position of the query in the input context affects how well the model can attend to and retrieve relevant information.
- Evidence anchors:
  - [abstract]: "Query-aware contextualization improves key-value retrieval but has minimal effect on multi-document QA"
  - [section]: "In contrast, query-aware contextualization minimally affects performance trends in the multi-document question answering task"
  - [corpus]: Weak - corpus shows related work but no direct evidence of query-aware contextualization effects
- Break condition: If the query position is not a significant factor in the model's attention mechanism, or if the model's architecture inherently allows for effective attention to all positions regardless of query placement.

## Foundational Learning

- Concept: Positional embeddings in transformer models
  - Why needed here: Understanding how positional embeddings create bias in attention distribution across the input sequence
  - Quick check question: How do positional embeddings in transformer models affect the model's attention to different positions in the input sequence?

- Concept: Attention mechanisms in language models
  - Why needed here: Understanding how attention mechanisms allocate resources across different positions in the input sequence
  - Quick check question: How do attention mechanisms in language models affect the model's ability to process information in different parts of the input sequence?

- Concept: Instruction-tuning and its effects on model behavior
  - Why needed here: Understanding how instruction-tuning might influence the model's bias towards recent tokens or the beginning of the input context
  - Quick check question: How does instruction-tuning affect the model's attention to different positions in the input sequence, particularly in relation to the position of the task specification or instruction?

## Architecture Onboarding

- Component map: Input context -> Positional embeddings -> Self-attention mechanism -> Model output
- Critical path: Input context → Positional embeddings → Self-attention mechanism → Model output
- Design tradeoffs:
  - Longer context lengths vs. computational efficiency
  - Positional bias vs. uniform attention distribution
  - Query position vs. retrieval performance

- Failure signatures:
  - U-shaped performance curve when varying the position of relevant information
  - Decreased performance on longer input contexts
  - Inability to effectively use information in the middle of the input context

- First 3 experiments:
  1. Evaluate model performance on multi-document QA with varying input context lengths and positions of relevant information
  2. Compare performance of decoder-only and encoder-decoder models on the same task
  3. Test the effects of query-aware contextualization on key-value retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes in Transformer models could improve their ability to use information in the middle of long input contexts?
- Basis in paper: Explicit
- Why unresolved: The paper identifies that language models struggle to use information in the middle of long contexts, but does not propose or test specific architectural solutions to address this issue.
- What evidence would resolve it: Experiments comparing performance of modified Transformer architectures (e.g., with different attention mechanisms or positional encodings) on the multi-document QA and key-value retrieval tasks when relevant information is placed in the middle of input contexts.

### Open Question 2
- Question: How do different decoding strategies (e.g., beam search, sampling) affect language model performance when relevant information is in the middle of long contexts?
- Basis in paper: Inferred
- Why unresolved: The paper only uses greedy decoding, leaving the potential impact of alternative decoding methods unexplored.
- What evidence would resolve it: Comparative experiments using various decoding strategies on the multi-document QA and key-value retrieval tasks with relevant information placed in different positions within long contexts.

### Open Question 3
- Question: What is the impact of instruction fine-tuning data composition on language models' ability to use long contexts?
- Basis in paper: Explicit
- Why unresolved: The paper shows that instruction tuning does not significantly affect the U-shaped performance curve, but does not explore how the composition of instruction tuning data might influence context usage.
- What evidence would resolve it: Experiments varying the proportion of long-context examples in instruction tuning data and measuring subsequent performance on multi-document QA and key-value retrieval tasks with relevant information in different positions.

## Limitations

- The evaluation focuses primarily on decoder-only models and one encoder-decoder model, which may not represent the full diversity of language model architectures.
- The synthetic nature of the key-value retrieval task may not capture the complexity of real-world retrieval scenarios.
- The study does not investigate whether alternative positional encoding schemes could mitigate the observed performance degradation.

## Confidence

**High Confidence**: The core finding that language models exhibit degraded performance when relevant information is in the middle of long contexts is well-supported by experiments across multiple tasks and models.

**Medium Confidence**: The assertion that extended-context models are not necessarily better at using long contexts is supported but limited by the small sample of extended-context models tested.

**Low Confidence**: The explanation that positional bias in attention mechanisms is the primary cause of the observed performance degradation is plausible but not directly tested.

## Next Checks

1. **Cross-Architecture Validation**: Test the U-shaped performance pattern on additional model families including different encoder-decoder architectures (T5, Pegasus) and decoder-only models (LLaMA, Falcon) to determine if this is a universal limitation or specific to the tested models.

2. **Positional Encoding Ablation**: Modify the positional embeddings in a base model to be uniform across positions or use relative positional encoding, then re-run the multi-document QA experiments to isolate whether positional embeddings are the root cause of the middle-position degradation.

3. **Retrieval-System Interaction Study**: Conduct a controlled experiment measuring the joint performance of retriever and reader models at different context lengths, specifically examining whether performance degradation occurs when retriever recall improves but reader utilization remains constant.