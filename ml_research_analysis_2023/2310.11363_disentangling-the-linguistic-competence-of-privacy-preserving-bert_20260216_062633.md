---
ver: rpa2
title: Disentangling the Linguistic Competence of Privacy-Preserving BERT
arxiv_id: '2310.11363'
source_url: https://arxiv.org/abs/2310.11363
tags:
- uni00000013
- arxiv
- uni00000014
- uni00000011
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of text-to-text privatization
  on the linguistic competence of BERT language models. Using a representational similarity
  analysis, it reveals a substantial divergence in internal representations when BERT
  is trained on differentially private text compared to non-private text.
---

# Disentangling the Linguistic Competence of Privacy-Preserving BERT

## Quick Facts
- **arXiv ID:** 2310.11363
- **Source URL:** https://arxiv.org/abs/2310.11363
- **Reference count:** 18
- **Primary result:** Text-to-text privatization substantially degrades BERT's ability to capture complex linguistic structures while preserving basic word properties.

## Executive Summary
This study investigates how differential privacy mechanisms applied during BERT pre-training affect the model's linguistic competence. Using representational similarity analysis and probing tasks, the authors demonstrate that privacy-preserving BERT shows significant divergence in internal representations compared to standard BERT. While basic word properties remain largely intact, the model struggles with contextual relationships between word spans, and attention patterns become increasingly redundant. The findings reveal a fundamental trade-off between privacy preservation and linguistic modeling capability in transformer-based language models.

## Method Summary
The researchers pre-train BERT from scratch on Wikipedia text with text-to-text privatization using differential privacy (ε = 10) and compare it against a non-private baseline (ε = ∞). They employ representational similarity analysis (RSA) to compare internal representations between models, using cosine distance and Spearman correlation on 5,000 random WikiText sentences. Linguistic competence is evaluated through surface probes (length, content, order), edge probes (syntactic and semantic tasks), and structural probes (parse depth and distance) applied to the extracted representations.

## Key Results
- Text-to-text privatization causes substantial divergence in BERT's internal representations compared to non-private training
- The model preserves localized linguistic properties (part-of-speech, word order) while significantly degrading contextual relationships between word spans
- Attention maps in privacy-preserving BERT show increased redundancy, with attention heads clustering more tightly in later layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-text privatization disproportionately affects contextual relationships between word spans compared to localized word properties
- Mechanism: The privacy mechanism perturbs individual words based on their vector representations without considering broader context, causing preserved localized features but disrupted span-level dependencies
- Core assumption: Word-level perturbations propagate differently through self-attention layers depending on whether the affected property is local or contextual
- Evidence anchors:
  - [abstract] "encoding localized properties of words while falling short at encoding the contextual relationships between spans of words"
  - [section 4.2] "basic properties of words are less disrupted than complex relations between words that require context information"
  - [corpus] Weak - no direct neighbor paper supports this specific mechanism
- Break condition: If the perturbation mechanism is modified to inject noise at the sentence or span level rather than word level

### Mechanism 2
- Claim: Privacy-preserving BERT retains hierarchical order of linguistic formalisms but shows cumulative degradation across layers
- Mechanism: Pre-training on perturbed text maintains the coarse-grained hierarchy (surface → syntactic → semantic) but introduces noise that compounds across transformer layers
- Core assumption: The hierarchical organization of linguistic properties in transformer layers is robust to small perturbations but sensitive to cumulative effects
- Evidence anchors:
  - [abstract] "text-to-text privatization affects the linguistic competence across several formalisms"
  - [section 4.1] "text-to-text privatization shows to have a cumulative impact on the linguistic competence of language models"
  - [corpus] Weak - no direct neighbor paper supports this specific mechanism
- Break condition: If layer-wise probing reveals non-cumulative, localized degradation patterns instead

### Mechanism 3
- Claim: Privacy-preserving BERT amplifies attention redundancy, reducing the diversity of attention heads
- Mechanism: Text-to-text privatization causes attention maps to cluster more tightly in later layers, indicating reduced diversity in attention head behaviors
- Core assumption: Diverse attention head behaviors are crucial for capturing complex linguistic relationships, and privatization disrupts this diversity
- Evidence anchors:
  - [abstract] "analysis of attention patterns indicates increased redundancy in privacy-preserving models"
  - [section 4.2] "text-to-text privatization amplifies the redundancy that is already present in attention heads"
  - [section 4.2] "encouraging the attention mechanism to have diverse behaviors can improve performance"
- Break condition: If attention head diversity is shown to be irrelevant for the degraded performance

## Foundational Learning

- **Concept: Differential Privacy (DP)**
  - Why needed here: Understanding how DP introduces noise to protect privacy and its impact on model training
  - Quick check question: What is the role of the privacy budget ε in DP, and how does it affect the trade-off between privacy and utility?

- **Concept: Transformer Architecture and Self-Attention**
  - Why needed here: Grasping how BERT forms contextual representations through self-attention and why perturbations affect this process
  - Quick check question: How does the self-attention mechanism in transformers integrate information from different token positions?

- **Concept: Representational Similarity Analysis (RSA)**
  - Why needed here: Learning how RSA compares internal representations across models without requiring direct correspondence
  - Quick check question: Why is RSA preferred over direct activation comparison when analyzing models trained under different conditions?

## Architecture Onboarding

- **Component map:** Tokenized text → Embedding layer → Transformer layers (with self-attention) → Output representations → Privacy module (metric DP mechanism) → Analysis tools (RSA, probing tasks, attention clustering)
- **Critical path:** 1) Apply text-to-text privatization to training corpus 2) Pre-train BERT on privatized text 3) Extract internal representations from both private and non-private BERT 4) Perform RSA to measure representation dissimilarity 5) Conduct probing tasks to assess linguistic competence 6) Analyze attention maps for redundancy patterns
- **Design tradeoffs:**
  - Privacy vs. utility: Higher ε (less privacy) yields better performance but less protection
  - Granularity of perturbation: Word-level vs. span-level vs. sentence-level noise injection
  - Probing granularity: Word-level vs. sentence-level probes for different linguistic properties
- **Failure signatures:**
  - High RSA similarity but poor probing performance: Indicates preserved representations but degraded competence
  - Low RSA similarity but good probing performance: Suggests different representation strategies achieving similar competence
  - Attention map clustering without performance drop: Redundancy may not always harm performance
- **First 3 experiments:**
  1. Compare RSA scores between BERT trained on ε=10 vs ε=∞ to quantify representation divergence
  2. Run edge probing tasks on both models to identify which linguistic properties are most affected
  3. Cluster attention maps and measure redundancy indices to correlate with probing performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the degree of linguistic property degradation scale with varying levels of differential privacy (ε)?
- Basis in paper: [inferred] The paper demonstrates linguistic degradation at ε=10 but doesn't explore a range of ε values systematically
- Why unresolved: The study focuses on a single privacy budget value, limiting understanding of the privacy-utility tradeoff curve
- What evidence would resolve it: Experiments testing multiple ε values (e.g., 1, 5, 10, 100, ∞) with corresponding linguistic probing results would establish how degradation scales

### Open Question 2
- Question: Do alternative text-to-text privatization mechanisms (e.g., syntactic guidance or custom sanitization) preserve linguistic competence better than the standard metric differential privacy approach?
- Basis in paper: [explicit] The authors note that recent developments aim to improve utility and privacy but don't evaluate these methods
- Why unresolved: The study only uses one privatization mechanism, leaving open whether architectural improvements could mitigate linguistic damage
- What evidence would resolve it: Comparative experiments using different privatization methods (syntactic guidance, custom sanitization) with identical linguistic probing would show relative preservation of competence

### Open Question 3
- Question: How do probing results translate to downstream task performance when fine-tuning privacy-preserved models?
- Basis in paper: [inferred] The study focuses on pre-training effects but doesn't examine transfer to specific tasks after fine-tuning
- Why unresolved: Probing shows linguistic degradation, but it's unclear if this manifests as performance drops on tasks like classification or question answering
- What evidence would resolve it: Fine-tuning privacy-preserved models on benchmark tasks (GLUE, SuperGLUE) and comparing to non-private baselines would demonstrate practical impact of linguistic degradation

## Limitations
- The study only examines one privacy budget value (ε=10), limiting understanding of the privacy-utility tradeoff curve
- The causal mechanisms linking privacy mechanisms to specific linguistic impairments remain largely theoretical without direct empirical validation
- Findings are limited to BERT-base architecture, with unknown generalizability to other transformer variants or larger models

## Confidence
- **High Confidence:** The empirical observations of degraded performance on probing tasks and increased attention redundancy in privacy-preserving models
- **Medium Confidence:** The interpretation that text-to-text privatization disproportionately affects contextual relationships versus localized properties
- **Low Confidence:** The proposed mechanism of cumulative degradation across layers and its relationship to the hierarchical organization of linguistic properties

## Next Checks
1. Conduct ablation studies varying the privacy budget ε to establish a dose-response relationship between privacy strength and linguistic competence degradation
2. Implement alternative perturbation mechanisms (span-level vs. word-level) to test whether the observed effects are specific to the text-to-text privatization approach
3. Perform cross-architecture validation by testing similar privacy-preserving training on RoBERTa and GPT variants to assess the generalizability of the findings