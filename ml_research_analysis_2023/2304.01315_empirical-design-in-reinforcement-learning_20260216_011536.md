---
ver: rpa2
title: Empirical Design in Reinforcement Learning
arxiv_id: '2304.01315'
source_url: https://arxiv.org/abs/2304.01315
tags:
- performance
- algorithm
- learning
- have
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This document provides a comprehensive guide to running good experiments
  in reinforcement learning, addressing challenges like algorithm sensitivity, statistical
  evidence, and fair comparisons. It covers statistical assumptions underlying performance
  measures, characterizing performance variation, hypothesis testing, handling hyperparameters,
  and selecting appropriate environments.
---

# Empirical Design in Reinforcement Learning

## Quick Facts
- arXiv ID: 2304.01315
- Source URL: https://arxiv.org/abs/2304.01315
- Authors: 
- Reference count: 40
- Primary result: Comprehensive guide to running good experiments in RL, addressing challenges like algorithm sensitivity, statistical evidence, and fair comparisons.

## Executive Summary
This document provides a comprehensive guide to running good experiments in reinforcement learning, addressing challenges like algorithm sensitivity, statistical evidence, and fair comparisons. It covers statistical assumptions underlying performance measures, characterizing performance variation, hypothesis testing, handling hyperparameters, and selecting appropriate environments. The authors emphasize the importance of understanding algorithms rather than just ranking them, and propose strategies for dealing with issues like maximization bias, multiple comparisons, and designer bias.

## Method Summary
The paper presents a systematic framework for designing RL experiments, covering statistical methods for evaluating algorithms, characterizing performance variation, hypothesis testing, handling hyperparameters, and selecting environments. The approach emphasizes understanding algorithms rather than just ranking them, and provides strategies for dealing with issues like maximization bias, multiple comparisons, and designer bias. The framework includes recommendations for using tolerance intervals to capture performance variability, reporting confidence intervals for mean estimates, and carefully selecting evaluation metrics.

## Key Results
- Statistical methods for evaluating RL algorithms must account for both variability and uncertainty, using tools like tolerance intervals and confidence intervals
- Proper experimental design requires separating sources of randomness (agent vs environment) to reduce variance through paired comparisons
- Reporting performance for best hyperparameters is statistically biased and misleading due to maximization bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Statistical methods for evaluating RL algorithms must account for both variability and uncertainty.
- **Mechanism:** The paper systematically maps statistical tools (tolerance intervals, confidence intervals, hypothesis tests) to distinct sources of variation—agent initialization, environment stochasticity, and hyperparameter settings.
- **Core assumption:** RL performance distributions can be non-Gaussian and multi-modal, so naïve summary statistics (mean, std) misrepresent true behavior.
- **Evidence anchors:**
  - [abstract] "Recent studies have highlighted how popular algorithms are sensitive to hyper-parameter settings and implementation details, and that common empirical practice leads to weak statistical evidence"
  - [section 2.4] "IfP is skewed or bimodal, then we might need more than a dozen independent runs to estimate the mean and variance"
- **Break condition:** If the performance distribution is truly Gaussian with low variance, simpler statistics suffice and complexity adds little value.

### Mechanism 2
- **Claim:** Proper experimental design requires separating sources of randomness (agent vs environment) to reduce variance.
- **Mechanism:** By using independent random seeds for agent initialization and environment dynamics, paired comparisons can cancel out common sources of variation, improving statistical power.
- **Core assumption:** Agent behavior is influenced by both its own stochasticity and the environment's stochasticity, and these are largely independent.
- **Evidence anchors:**
  - [section 4.4] "The ability to replicate and reproduce results is core to scientific investigation... A common approach is to set a global random seed for the entire simulation"
- **Break condition:** If agent and environment randomness are not independent (e.g., environment state depends on agent initialization), paired comparisons can introduce bias.

### Mechanism 3
- **Claim:** Reporting performance for best hyperparameters is statistically biased and misleading.
- **Mechanism:** Selecting the best hyperparameter configuration from a sweep and then reporting its performance overestimates true algorithm capability due to maximization bias (E[max_h G_h] > max_h E[G_h]).
- **Core assumption:** Performance estimates for each hyperparameter setting are noisy, and maximizing over noisy estimates systematically overestimates the true maximum.
- **Evidence anchors:**
  - [section 3.2] "If we run 1000 experiments of DQN on the Mountain Car domain and sweep stepsizes, target network refresh rates, and replay buffer sizes for every experiment, then approximately 96% of these experiments will over report the average performance"
- **Break condition:** If hyperparameter performance estimates are very precise (low variance) or if the goal is exploratory rather than comparative, this bias may be negligible.

## Foundational Learning

- **Concept:** Distributional assumptions in statistical inference
  - **Why needed here:** Different statistical tools (Student's t-test, bootstrap, tolerance intervals) make different assumptions about the underlying data distribution. RL performance often violates normality assumptions.
  - **Quick check question:** If your RL agent's performance histogram is clearly bimodal, which statistical method should you avoid?
    - Answer: Student's t-distribution confidence intervals (assumes normality)

- **Concept:** Paired statistical comparisons
  - **Why needed here:** RL experiments often have shared sources of variation (same environment instance, same random seed structure). Pairing controls for these, increasing power.
  - **Quick check question:** Why is a paired t-test more powerful than an unpaired test when comparing two RL algorithms?
    - Answer: It accounts for variation due to shared environment stochasticity and agent initialization

- **Concept:** Hyperparameter sensitivity analysis
  - **Why needed here:** Understanding how algorithm performance varies with hyperparameters is crucial for both algorithm understanding and fair comparison. The paper distinguishes sensitivity analysis from hyperparameter optimization.
  - **Quick check question:** What's the key difference between a sensitivity curve and a hyperparameter optimization sweep?
    - Answer: Sensitivity curves show performance variation across hyperparameter values; optimization sweeps seek the best value

## Architecture Onboarding

- **Component map:** Data collection (running RL agents) -> Statistical summarization (choosing appropriate metrics) -> Comparison methods (paired tests, confidence intervals) -> Experimental design considerations (hyperparameters, environments)
- **Critical path:** For a new RL experiment: (a) define hypothesis and evaluation metric, (b) determine required number of runs based on desired statistical power, (c) collect data with proper random seed separation, (d) apply appropriate statistical summarization, (e) use paired comparisons when possible.
- **Design tradeoffs:** More runs → tighter confidence intervals but higher computational cost. Simpler statistics → easier interpretation but potential bias. Separate random seeds → reduced variance but requires careful implementation.
- **Failure signatures:** Overlapping confidence intervals with small sample sizes may hide true differences. Reporting only means without variability measures can mislead. Using untuned baselines creates unfair comparisons.
- **First 3 experiments:**
  1. **Single algorithm evaluation:** Run SARSA on Mountain Car with 10 different seeds, plot learning curves with tolerance intervals and bootstrap confidence intervals around the mean.
  2. **Paired comparison:** Compare SARSA vs Expected SARSA on the same Mountain Car instances (same environment seeds), plot difference curves with confidence intervals.
  3. **Hyperparameter sensitivity:** Sweep the stepsize parameter for SARSA across 5 values, run 5 seeds per value, create a violin plot showing performance distribution per hyperparameter.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop a general-purpose algorithm for hyperparameter selection in reinforcement learning that works as effectively as cross-validation does in supervised learning?
- **Basis in paper:** [explicit] Section 3.3 discusses the lack of a general-purpose approach to hyperparameter selection in RL compared to supervised learning's cross-validation.
- **Why unresolved:** Current RL lacks a mechanism equivalent to cross-validation for hyperparameter selection, making it difficult to set hyperparameters appropriately across different environments and deployment scenarios.
- **What evidence would resolve it:** Development and empirical validation of a cross-validation-like procedure for RL hyperparameter selection that consistently identifies optimal hyperparameters across diverse RL tasks.

### Open Question 2
- **Question:** What are the most effective strategies for aggregating performance across environments while accounting for differences in reward scales and ensuring meaningful comparisons?
- **Basis in paper:** [explicit] Section 5.3 and Appendix E discuss challenges in creating macro-environments and normalizing performance across different environments.
- **Why unresolved:** While grouping environments into macro-environments offers benefits, there's no consensus on how to properly normalize and aggregate performance metrics across environments with different reward scales and characteristics.
- **What evidence would resolve it:** Empirical studies comparing different normalization and aggregation methods across diverse environment suites, demonstrating which approaches best capture true algorithmic differences.

### Open Question 3
- **Question:** How can we determine the optimal number of runs needed to obtain statistically significant results given the unknown distribution of performance in RL algorithms?
- **Basis in paper:** [explicit] Section 2.6 discusses the challenge of determining how many runs are needed, noting that even 30 runs can be insufficient for heavily skewed distributions.
- **Why unresolved:** The required number of runs depends on the unknown performance distribution, which can be highly skewed or multimodal, making it difficult to determine when enough data has been collected.
- **What evidence would resolve it:** Development of adaptive sampling strategies that estimate distribution characteristics during experimentation and dynamically determine when sufficient runs have been completed.

## Limitations
- Statistical methods still rely on distributional assumptions (normality) that RL performance often violates
- Optimal number of runs depends on unknown performance distribution properties, making precise recommendations difficult
- Practical implementation of tolerance intervals for non-Gaussian distributions requires further validation

## Confidence

- **High confidence:** Claims about maximization bias in hyperparameter selection and importance of paired comparisons for controlling shared sources of variation
- **Medium confidence:** Recommendations for specific statistical methods (tolerance intervals, bootstrap intervals) are conceptually sound but require more empirical validation
- **Low confidence:** Optimal number of runs needed for different statistical methods varies significantly based on unknown properties of performance distribution

## Next Checks

1. **Distribution validation:** Systematically evaluate the performance distributions of multiple RL algorithms across different domains to characterize their shape, variance, and modality patterns.
2. **Paired comparison robustness:** Test paired statistical methods across scenarios where agent and environment randomness may be correlated to assess potential bias introduction.
3. **Practical implementation study:** Implement and validate tolerance interval calculations for non-Gaussian distributions in RL, comparing coverage accuracy against theoretical guarantees.