---
ver: rpa2
title: Benchmarking quantized LLaMa-based models on the Brazilian Secondary School
  Exam
arxiv_id: '2309.12071'
source_url: https://arxiv.org/abs/2309.12071
tags:
- questions
- these
- llama
- language
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated quantized LLaMA-based models on Brazilian
  secondary school exam questions, using Alpaca, Koala, and Vicuna models (7B and
  13B parameters) quantized to Q4. A dataset of 1,006 ENEM questions from 2010-2022
  was created.
---

# Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam

## Quick Facts
- arXiv ID: 2309.12071
- Source URL: https://arxiv.org/abs/2309.12071
- Reference count: 17
- Primary result: Quantized LLaMA-based models achieved 46-49% accuracy on Brazilian secondary school exam questions

## Executive Summary
This study evaluates quantized LLaMA-based models (Alpaca, Koala, Vicuna) on Brazilian ENEM exam questions, using 7B and 13B parameter models quantized to Q4. A dataset of 1,006 ENEM questions from 2010-2022 was created and evaluated in both Portuguese and English. The best-performing models achieved approximately 46% accuracy on original Portuguese questions and 49% on English translations, running on consumer-grade AMD Ryzen 5 3600x hardware with inference times of 20-50 seconds depending on model size.

## Method Summary
The study creates a dataset of 1,006 ENEM questions from 2010-2022, filtered to exclude questions with images or tables. Eight LLaMA-based models (7B and 13B variants of Alpaca, Koala, and Vicuna) were quantized to Q4 using ggml and evaluated using llama.cpp on consumer hardware. Accuracy was measured by comparing model outputs to correct answers, with tests conducted on both Portuguese originals and English translations of the questions.

## Key Results
- LLaMA 2 13B models achieved approximately 46% accuracy on Portuguese ENEM questions
- English translations yielded slightly better performance at 49% accuracy
- 7B models averaged 20 seconds per inference, while 13B models averaged 50 seconds on AMD Ryzen 5 3600x
- Cross-lingual transfer enabled English-trained models to effectively answer Portuguese questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantized LLaMA-based models can run effectively on consumer-grade hardware.
- Mechanism: The quantization process reduces model size by representing weights with lower precision (Q4 reduces memory usage by ~75%), enabling inference on CPUs with limited memory.
- Core assumption: The accuracy loss from quantization (estimated ~2% perplexity increase at Q4) is acceptable for answering standardized test questions.
- Evidence anchors:
  - [abstract] states models ran on "AMD Ryzen 5 3600x" with "approximately 20 and 50 seconds" inference times
  - [section] explains quantization reduces memory requirements by representing weights as integer plus float parts
  - [corpus] shows related work on compressed LLM deployment on mobile platforms
- Break condition: If quantization level is increased beyond Q4, performance degradation becomes prohibitive for practical use.

### Mechanism 2
- Claim: English-trained LLaMA models can effectively answer Portuguese exam questions.
- Mechanism: Cross-lingual transfer learning enables models trained on English text to generalize to Portuguese despite not being explicitly fine-tuned on Portuguese data.
- Core assumption: The semantic similarity between English and Portuguese allows sufficient comprehension for multiple-choice question answering.
- Evidence anchors:
  - [abstract] reports "46% accuracy for original Portuguese texts" and "49% on English translations"
  - [section] discusses creating database of 1,006 ENEM questions and evaluating model performance
  - [corpus] shows related work evaluating LLMs on Brazilian university admission exams
- Break condition: If question complexity increases significantly (requiring deeper cultural or linguistic knowledge), cross-lingual transfer becomes insufficient.

### Mechanism 3
- Claim: Larger parameter models (13B) outperform smaller models (7B) on standardized test questions.
- Mechanism: Increased model capacity allows better pattern recognition and reasoning capabilities needed for complex question answering tasks.
- Core assumption: The additional parameters provide meaningful improvements in question comprehension rather than just memorization.
- Evidence anchors:
  - [abstract] shows LLaMA 2 13B models achieved "approximately 46% accuracy" versus lower performance for 7B variants
  - [section] presents accuracy data in Table III showing 13B models consistently outperform 7B models
  - [corpus] indicates related work showing performance improvements with larger model sizes
- Break condition: If computational constraints become too severe, the performance benefits of larger models may not justify the increased inference time.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: Understanding how attention mechanisms process sequential input is crucial for grasping why these models can answer exam questions
  - Quick check question: How does the self-attention mechanism in transformers differ from traditional recurrent neural networks?

- Concept: Quantization techniques
  - Why needed here: The quantization process is fundamental to understanding how these models achieve computational efficiency on consumer hardware
  - Quick check question: What is the relationship between quantization level (Q4, Q5, etc.) and memory usage/computation time?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding why English-trained models can perform on Portuguese questions is key to interpreting the results
  - Quick check question: What factors influence the effectiveness of cross-lingual transfer between languages with different linguistic structures?

## Architecture Onboarding

- Component map: LLaMA base model → Quantization (GGML) → Question processing pipeline → Answer selection → Performance evaluation
- Critical path: Data preprocessing → Model loading → Inference → Answer extraction → Accuracy calculation
- Design tradeoffs: Parameter count vs. inference speed vs. accuracy; quantization level vs. memory usage vs. performance
- Failure signatures: High perplexity values indicate poor model understanding; inconsistent answer patterns suggest data quality issues
- First 3 experiments:
  1. Test inference time on different hardware configurations (CPU vs GPU) to establish baseline performance
  2. Compare accuracy across different quantization levels (Q4 vs Q5 vs Q6) to find optimal tradeoff
  3. Evaluate model performance on English vs Portuguese versions of the same questions to measure cross-lingual capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically expand the ENEM dataset to include questions from years not covered in the current study?
- Basis in paper: [explicit] The authors note that only ENEM exams from 2010 to 2022 were considered and suggest that "the generated scripts can be generalized to other years of ENEM, further expanding this database."
- Why unresolved: The paper does not provide details on the technical challenges or methodologies for adapting the scripts to newer or older ENEM years.
- What evidence would resolve it: A detailed methodology showing successful integration of questions from additional ENEM years, including any necessary adaptations to handle changes in question formats or content over time.

### Open Question 2
- Question: What specific biases exist in the quantized models' performance on ENEM questions, and what causes these biases?
- Basis in paper: [inferred] The authors mention that they "were not able to understand the reason for the observed biases in the behavior of the models" and suggest this as a future research direction.
- Why unresolved: The paper does not investigate or identify the sources of bias in the models' responses to ENEM questions.
- What evidence would resolve it: A comprehensive analysis of model outputs identifying specific patterns of bias, coupled with an investigation into the training data or model architecture that may contribute to these biases.

### Open Question 3
- Question: How do quantized LLaMA-based models compare to other open-source models on ENEM questions when evaluated on multimodal data (text and images)?
- Basis in paper: [explicit] The authors suggest evaluating "multimodal models" in the future, noting that "GPT-4 performed impressively well on ENEM questions, in part due to its ability to process visual information in conjunction with the text."
- Why unresolved: The current study only evaluates text-based questions and does not include multimodal data.
- What evidence would resolve it: Experimental results comparing the performance of quantized LLaMA-based models and other open-source models on ENEM questions that include both text and images, demonstrating their ability to process multimodal data.

## Limitations
- Accuracy results lack baseline comparisons to human performance or commercial models on the same dataset
- Study does not address potential biases in question selection or whether questions represent balanced difficulty spectrum
- Claims about practical deployment implications exceed what the accuracy data supports

## Confidence
- **High Confidence**: The hardware specifications and quantization methodology are clearly documented and reproducible. The inference timing measurements (20s for 7B, 50s for 13B models) appear reliable given the controlled testing environment.
- **Medium Confidence**: The accuracy measurements are credible but limited by the lack of comparative analysis with alternative approaches. The cross-lingual transfer mechanism is plausible but not thoroughly validated through ablation studies.
- **Low Confidence**: Claims about practical deployment implications (e.g., "effective handling" of standardized tests) exceed what the accuracy data supports, particularly given the absence of human-level performance baselines.

## Next Checks
1. **Baseline Comparison**: Test the same ENEM question set using GPT-4 or Claude to establish human-comparable accuracy baselines and contextualize the 46-49% performance range.
2. **Cross-Lingual Ablation**: Conduct controlled experiments comparing model performance on Portuguese questions with and without prior English fine-tuning to quantify the actual cross-lingual transfer benefit.
3. **Difficulty Stratification**: Analyze accuracy by question difficulty level (categorized by ENEM scoring data) to determine whether performance limitations stem from fundamental comprehension gaps or simply question complexity.