---
ver: rpa2
title: Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals
arxiv_id: '2311.09605'
source_url: https://arxiv.org/abs/2311.09605
tags:
- attentiveness
- input
- data
- datasets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method called Counterfactual Attentiveness
  Test (CAT) to measure how much NLP models rely on specific parts of their input,
  especially when those parts may contain misleading correlations. CAT works by replacing
  one part of an input pair with a counterpart from a different example and checking
  if the model's prediction changes.
---

# Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals

## Quick Facts
- **arXiv ID**: 2311.09605
- **Source URL**: https://arxiv.org/abs/2311.09605
- **Reference count**: 40
- **Primary result**: Introduces Counterfactual Attentiveness Test (CAT) to measure and improve how well NLP models attend to all parts of their input

## Executive Summary
This paper addresses the problem of measuring how much NLP models rely on specific parts of their input, particularly when those parts contain misleading correlations. The authors introduce the Counterfactual Attentiveness Test (CAT), which measures attentiveness by replacing one part of an input pair with a counterpart from a different example and checking if the model's prediction changes. Tested across ten datasets and four tasks, CAT reveals that models' attentiveness varies widely by task and dataset. The paper also demonstrates that augmenting training data with counterfactual examples can improve models' attentiveness to both input components, though interestingly, models like GPT-3 become less attentive as the number of demonstrations increases, even while their standard accuracy improves.

## Method Summary
The method introduces Counterfactual Attentiveness Test (CAT) to measure model attentiveness to partial inputs. CAT works by replacing one component of an input pair with a counterpart from another example and observing if predictions change. The paper evaluates this on ten datasets across four tasks (natural language inference, reading comprehension, paraphrase detection, and visual reasoning) using both supervised models and in-context learning. To improve attentiveness, the authors augment training data with counterfactual examples where one input part is replaced and the label is set to a default value. The evaluation measures both standard accuracy and CAT scores to assess how well models attend to all input components versus relying on spurious correlations.

## Key Results
- Models show widely varying attentiveness levels across different tasks and datasets
- Counterfactual data augmentation effectively improves model attentiveness to both input parts
- GPT-3 and similar models become less attentive as the number of demonstrations increases, despite improving standard accuracy
- CAT scores correlate with the presence of spurious correlations in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained on partial inputs can achieve high performance even without seeing the full input.
- Mechanism: The partial input baseline trains a classifier on only one part of the input (e.g., hypothesis in NLI), showing that this part alone contains predictive information about the label.
- Core assumption: The partial input contains enough information to predict the label without the other part.
- Evidence anchors:
  - [abstract] "previous work has found that datasets with paired inputs are prone to correlations between a specific part of the input (e.g., the hypothesis in NLI) and the label; consequently, models trained only on those outperform chance."
  - [section] "Our results support and extend prior findings by showing that for a diverse class of models and a variety of tasks, models trained on partial inputs are strong."
- Break condition: If the partial input is truly independent of the label, or if the task requires both parts equally.

### Mechanism 2
- Claim: Replacing one part of the input with a counterfactual from another example reveals whether the model is attentive to that part.
- Mechanism: The Counterfactual Attentiveness Test (CAT) measures attentiveness by replacing one input component with another randomly sampled instance's corresponding component and observing if the prediction changes.
- Core assumption: The counterfactual replacement results in a "default" or unrelated label that an attentive model would change its prediction for.
- Evidence anchors:
  - [abstract] "CAT uses counterfactuals by replacing part of the input with its counterpart from a different example (subject to some restrictions), expecting an attentive model to change its prediction."
  - [section] "We verify CAT's assumptions by hand-annotating 50 counterfactual-paired instances per dataset... Overall we find the assumption to be true most of the time: 100% of instances in MNLI, WANLI, RTE, QQP, PAWS and DuoRC."
- Break condition: If the counterfactual input still maintains a relationship with the original input, making the label change unnecessary.

### Mechanism 3
- Claim: Data augmentation with counterfactuals can improve model attentiveness to both parts of the input.
- Mechanism: By training on counterfactual examples (where one part is replaced and the label is set to default), models learn to pay attention to both input components rather than relying on spurious correlations.
- Core assumption: The model generalizes from counterfactual training examples to better attend to the full input during inference.
- Evidence anchors:
  - [abstract] "Our results demonstrate that augmenting training or demonstration data with counterfactuals is effective in improving models' attentiveness."
  - [section] "We follow a similar procedure as in the evaluation to augment the training data: apart from the regular training set, for every x1, x2 pair... we sample x'1 from the training data, assigned the default label."
- Break condition: If the model overfits to the augmented data or if the counterfactuals are not representative of real-world variations.

## Foundational Learning

- Concept: Causal inference and counterfactual reasoning
  - Why needed here: The method relies on understanding how changing one part of the input (counterfactual) affects the model's prediction to measure attentiveness.
  - Quick check question: If you replace the premise in an NLI example with a completely unrelated premise, should an attentive model change its prediction? Why or why not?

- Concept: Spurious correlations in datasets
  - Why needed here: The study investigates whether models rely on correlations between parts of the input and labels that don't reflect true task understanding.
  - Quick check question: What is the difference between a genuine correlation and a spurious correlation in the context of NLP datasets?

- Concept: In-context learning and demonstration effects
  - Why needed here: The paper explores how the number of demonstrations affects model attentiveness in few-shot settings.
  - Quick check question: How might increasing the number of demonstrations in in-context learning affect a model's ability to generalize versus memorizing?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Loads datasets, creates counterfactual pairs
  - Model training: Fine-tuning or in-context learning setup
  - Evaluation: Standard accuracy + CAT metric computation
  - Analysis: Comparison of partial input baselines vs full input models

- Critical path:
  1. Load dataset and create counterfactual pairs
  2. Train or load model (fine-tuned or in-context)
  3. Evaluate standard accuracy on original data
  4. Compute CAT metric on counterfactual pairs
  5. Analyze results and compare with partial input baselines

- Design tradeoffs:
  - Automatic counterfactual generation vs manual creation: Automatic is scalable but assumes label change; manual is accurate but labor-intensive
  - Number of counterfactuals per instance (k=5): Balances statistical significance with computational cost
  - Including or excluding neutral predictions: Focuses on non-neutral cases but may miss some inattentiveness patterns

- Failure signatures:
  - CAT scores close to 100% but model fails on robust evaluation sets: Indicates reliance on lexical overlap heuristics
  - CAT scores vary widely across random seeds: Suggests instability in counterfactual generation or evaluation
  - Partial input baseline performs similarly to full input model: Indicates dataset contains strong spurious correlations

- First 3 experiments:
  1. Implement CAT on a simple NLI dataset (MNLI) with a pre-trained BERT model to verify the mechanism works as expected
  2. Compare CAT results across different model sizes (BERT-base vs BERT-large) on the same dataset to see if model capacity affects attentiveness
  3. Test CAT on a multimodal dataset (VQA) to verify the method works with image-text pairs and compare image vs text attentiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method's effectiveness vary with the diversity and size of the training dataset?
- Basis in paper: [inferred] The paper suggests that attentiveness is data-dependent and shows that models trained on MNLI (which has high partial input correlations) are less attentive than those trained on SQuAD2.0 (which has lower correlations). However, the relationship between dataset characteristics and model attentiveness is not fully explored.
- Why unresolved: The paper does not systematically investigate how factors like dataset size, diversity, or the prevalence of spurious correlations affect the method's ability to measure attentiveness. It also doesn't explore whether certain types of datasets (e.g., those with more balanced label distributions) lead to more reliable attentiveness scores.
- What evidence would resolve it: Experiments varying dataset size, diversity, and correlation levels while measuring attentiveness scores. Analysis of how these factors correlate with the method's reliability and sensitivity.

### Open Question 2
- Question: Can the method be extended to tasks with more than two input components?
- Basis in paper: [explicit] The authors mention in a footnote that "conceptually the generalization is straightforward" but they do not explore this extension in their experiments.
- Why unresolved: The paper only tests the method on paired-input tasks (NLI, PD, RC, VLR), leaving open the question of how well it generalizes to tasks with more complex input structures, such as multi-modal tasks with multiple images or documents with multiple sections.
- What evidence would resolve it: Applying the method to tasks with three or more input components and evaluating whether the counterfactual generation and attentiveness measurement still produce meaningful results.

### Open Question 3
- Question: What is the relationship between model size and attentiveness in in-context learning?
- Basis in paper: [explicit] The paper shows that GPT-3 becomes less attentive with more demonstrations, but it does not explore whether this trend holds across different model sizes or families in the in-context learning setup.
- Why unresolved: The experiments focus on a limited set of model sizes and do not compare how attentiveness scales with model capacity in in-context learning, leaving open the question of whether larger models are inherently more or less attentive in this setting.
- What evidence would resolve it: Systematic experiments varying model size and number of demonstrations in in-context learning, measuring both standard accuracy and attentiveness to identify trends.

## Limitations

- The assumption that counterfactual replacements consistently trigger label changes may not hold uniformly across all domains or languages
- Effectiveness of data augmentation with counterfactuals lacks comparison with alternative debiasing methods
- The mechanism behind decreasing attentiveness with more demonstrations in in-context learning requires further investigation

## Confidence

- **High Confidence**: The CAT metric's validity for measuring attentiveness in controlled settings, the general trend of model attentiveness varying across tasks and datasets
- **Medium Confidence**: The effectiveness of counterfactual data augmentation in improving attentiveness, the relationship between model size and attentiveness
- **Low Confidence**: The exact mechanisms behind in-context learning attentiveness patterns, generalizability of findings to non-English datasets

## Next Checks

1. **Assumption Verification**: Conduct systematic manual annotation of counterfactual pairs across all datasets to verify the label change assumption holds at >90% accuracy, particularly for languages beyond English
2. **Alternative Debiasing Comparison**: Compare CAT-based data augmentation with other debiasing approaches (e.g., invariant risk minimization) on the same model families to establish relative effectiveness
3. **Cross-Domain Generalizability**: Test CAT on medical and legal datasets where spurious correlations may have higher stakes, examining whether the metric maintains validity in high-risk domains