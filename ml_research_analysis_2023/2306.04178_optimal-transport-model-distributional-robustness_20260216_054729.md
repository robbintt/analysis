---
ver: rpa2
title: Optimal Transport Model Distributional Robustness
arxiv_id: '2306.04178'
source_url: https://arxiv.org/abs/2306.04178
tags:
- distribution
- robustness
- learning
- distributional
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an optimal transport-based distributional
  robustness framework for deep learning models in model space, addressing the problem
  of adversarial examples and data distribution shifts. The framework considers a
  model distribution within a Wasserstein ball centered on a given model distribution
  that maximizes the loss, enabling learning of the optimal robust center model distribution.
---

# Optimal Transport Model Distributional Robustness

## Quick Facts
- arXiv ID: 2306.04178
- Source URL: https://arxiv.org/abs/2306.04178
- Reference count: 40
- Key outcome: Introduces an optimal transport-based distributional robustness framework for deep learning models in model space, improving classification accuracy and uncertainty estimation on CIFAR-10 and CIFAR-100.

## Executive Summary
This work introduces an optimal transport-based distributional robustness framework for deep learning models in model space. The framework considers a model distribution within a Wasserstein ball centered on a given model distribution that maximizes the loss, enabling learning of the optimal robust center model distribution. The approach incorporates sharpness awareness into training for single models, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate remarkable improvements in classification accuracy and uncertainty estimation compared to baselines across various model settings.

## Method Summary
The OT-MDR framework optimizes a center model distribution to minimize the worst-case expected loss over a set of perturbed models sampled from a Wasserstein ball. The inner maximization problem is regularized with an entropy term, making it tractable via Gibbs sampling with SGLD. The framework is shown to be a probabilistic extension of SAM, with SAM being a specific case when using a Dirac delta distribution over a single model. Three settings are explored: single model (Dirac delta), ensemble models (uniform distribution), and Bayesian Neural Networks (BNN). The method is evaluated on CIFAR-10 and CIFAR-100 with various architectures including WideResNet28x10, Pyramid101, DenseNet121, Resnet10, Resnet18, MobileNet, and EfficientNet.

## Key Results
- OT-MDR achieves significant improvements in classification accuracy over SAM and other baselines on CIFAR-10 and CIFAR-100.
- The framework demonstrates better uncertainty estimation, as measured by Brier score, NLL, and ECE, compared to standard and ensemble methods.
- OT-MDR for BNNs provides improved NLL and ECE compared to standard SGVB training, indicating better uncertainty calibration.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OT-based distributional robustness in model space directly controls model sharpness by seeking worst-case model perturbations within a Wasserstein ball.
- Mechanism: By optimizing the center model distribution to minimize the maximum loss over a set of perturbed models sampled from a constrained distribution, the framework forces the model to be robust to parameter-space perturbations that would otherwise lead to sharp, poorly generalizing minima.
- Core assumption: The Wasserstein distance over model parameters effectively captures meaningful notions of model similarity and robustness in the loss landscape.
- Evidence anchors:
  - [abstract]: "We examine a model distribution within a Wasserstein ball centered on a given model distribution that maximizes the loss"
  - [section]: "SAM seeks a perturbed model within the vicinity of a current model that maximizes the loss over a training set"
  - [corpus]: No direct evidence found in neighbors for sharpness in model space; most focus on data-space or transport maps.
- Break condition: If the parameter space geometry is too complex or discontinuous for Wasserstein distance to meaningfully bound robustness, or if the loss landscape contains many sharp minima that are not captured by local perturbations.

### Mechanism 2
- Claim: The probabilistic extension of SAM via OT distributional robustness allows flexible incorporation of ensemble and Bayesian settings.
- Mechanism: By generalizing the Dirac delta assumption of SAM to distributions over models (uniform ensembles or Bayesian posteriors), the framework can regularize not just a single model but a whole distribution, improving generalization via model diversity.
- Core assumption: The optimal center distribution over models can be learned by minimizing the worst-case expected loss, and this expectation captures useful diversity.
- Evidence anchors:
  - [abstract]: "our framework can flexibly incorporate the concept of sharpness awareness into training... by considering specific forms of the center model distribution"
  - [section]: "we show that SAM is a specific case of our framework when using a Dirac delta distribution over a single model"
  - [corpus]: No direct neighbor evidence for ensemble/Bayesian extensions of OT-based robustness.
- Break condition: If the diversity induced by the distributional robustness is not aligned with beneficial ensemble effects, or if sampling from the posterior in BNN setting is too noisy to be useful.

### Mechanism 3
- Claim: Entropy regularization in the OT problem enables tractable sampling of worst-case models and makes the optimization problem solvable.
- Mechanism: By adding an entropy term to the inner max problem, the optimal perturbation distribution becomes a Gibbs distribution over the loss, which can be sampled efficiently via SGLD.
- Core assumption: The entropy-regularized dual form of the OT problem yields a closed-form or efficiently samplable solution for the perturbation distribution.
- Evidence anchors:
  - [section]: "we add the entropic regularization term... the inner max in the OP... has the solution which is a distribution with the density function"
  - [corpus]: No direct neighbor evidence for entropy regularization in OT for model distributional robustness.
- Break condition: If the temperature parameter λ is poorly chosen, the sampled perturbations may be too random (high entropy) or too narrow (low entropy), failing to improve generalization.

## Foundational Learning

- Concept: Wasserstein distance and optimal transport
  - Why needed here: It provides a principled way to define "closeness" between model distributions in parameter space, which is the basis for distributional robustness.
  - Quick check question: Can you explain the difference between Wasserstein distance and KL divergence in terms of how they measure distribution closeness?

- Concept: Sharpness-aware minimization (SAM)
  - Why needed here: SAM is the non-probabilistic precursor; understanding it helps see how OT-MDR generalizes SAM to distributions over models.
  - Quick check question: In SAM, what is the role of the radius ρ in the ball of perturbed models?

- Concept: Bayesian Neural Networks and variational inference
  - Why needed here: OT-MDR's BNN extension uses approximate posteriors over weights, so familiarity with reparameterization trick and SGVB is essential.
  - Quick check question: How does the reparameterization trick allow gradient-based training of a BNN?

## Architecture Onboarding

- Component map: Model distribution Qϕ -> Wasserstein ball constraint -> Loss maximization over perturbations -> Entropy regularization -> Sampling method (SGLD) -> Update rule (gradient step on ϕ)
- Critical path: Sample perturbations → compute worst-case loss → update center distribution
- Design tradeoffs:
  - Larger ρ increases robustness but may slow convergence or over-regularize.
  - Higher λ sharpens the perturbation distribution (more focused on worst cases) but may reduce diversity.
  - More particles improve robustness estimate but increase compute cost.
- Failure signatures:
  - Training loss plateaus early: too aggressive regularization or poor λ/ρ choice.
  - Validation accuracy worse than baseline: perturbations too large or diversity hurting single-model performance.
  - Unstable gradients: improper entropy/regularization balance.
- First 3 experiments:
  1. Single model, one particle, ρ=0.05, λ=1.0 on CIFAR-10 with WideResNet28x10; compare to SAM.
  2. Ensemble of 3 ResNets, K=2 particles each, same ρ/λ; compare to Deep Ensemble.
  3. BNN with diagonal Gaussian posterior, ρ=0.01, λ=0.5; compare to standard SGVB.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OT-MDR vary with different choices of the Wasserstein distance metric in model space?
- Basis in paper: [explicit] The paper mentions using a distance metric d(θ, θ̃) = ||θ - θ̃||₂ for p = ∞, but does not explore other metrics
- Why unresolved: The theoretical framework could potentially work with other metrics, but the paper only experiments with the L₂ norm
- What evidence would resolve it: Empirical comparison of OT-MDR performance using different distance metrics (e.g., L₁, L∞) on the same datasets

### Open Question 2
- Question: What is the optimal number of particle models for OT-MDR in single model settings?
- Basis in paper: [explicit] The ablation study shows results for K ∈ {1, 2, 3, 4} particles but doesn't explore beyond this range
- Why unresolved: The paper observes diminishing returns after K=2 but doesn't determine the theoretical or practical optimal value
- What evidence would resolve it: Systematic experiments with varying numbers of particles (e.g., K ∈ {1, 2, 4, 8, 16}) to identify the point of diminishing returns

### Open Question 3
- Question: How does OT-MDR perform on larger-scale datasets like ImageNet compared to SAM and other baselines?
- Basis in paper: [inferred] All experiments are conducted on CIFAR-10 and CIFAR-100, which are relatively small-scale datasets
- Why unresolved: The framework's scalability and effectiveness on larger, more complex datasets remains untested
- What evidence would resolve it: Direct comparison of OT-MDR with SAM and other baselines on ImageNet or similar large-scale datasets

### Open Question 4
- Question: What is the theoretical relationship between the entropic regularization parameter λ and the radius ρ of the Wasserstein ball?
- Basis in paper: [explicit] The paper mentions that when λ approaches +∞, the regularized problem becomes equivalent to the original problem
- Why unresolved: The paper doesn't explore how different λ and ρ values interact or what the optimal trade-off is
- What evidence would resolve it: Theoretical analysis and empirical validation of the λ-ρ relationship and its impact on model performance

### Open Question 5
- Question: How does OT-MDR perform in terms of computational efficiency compared to SAM, especially for large models?
- Basis in paper: [inferred] The paper mentions parallel computing benefits but doesn't provide detailed computational complexity analysis or runtime comparisons
- Why unresolved: While the paper claims similar computational costs to SAM, it doesn't provide empirical runtime data or scalability analysis
- What evidence would resolve it: Detailed computational complexity analysis and empirical runtime comparisons of OT-MDR vs SAM on models of increasing size

## Limitations
- The effectiveness of Wasserstein distance over model parameters for capturing meaningful robustness in high-dimensional, non-convex loss landscapes is not well-established.
- While the connection between OT-MDR and SAM is formally shown, the practical benefits of the probabilistic extension for ensemble and Bayesian settings are demonstrated empirically but lack theoretical guarantees for improved generalization.
- The computational cost of sampling K particle models per batch and the impact on training time are not discussed in detail. The scalability to larger datasets and architectures remains unclear.

## Confidence
- **High**: The framework is a valid probabilistic extension of SAM, as shown by the formal derivation and the Dirac delta special case.
- **Medium**: The improvements in classification accuracy and uncertainty estimation over baselines are significant but may depend on the specific datasets and architectures used.
- **Low**: The claim that the Wasserstein ball over model parameters effectively controls sharpness in the loss landscape is plausible but lacks rigorous theoretical support.

## Next Checks
1. **Ablation Study on Hyperparameters**: Conduct a systematic ablation study on CIFAR-10 and CIFAR-100 with different values of ρ and λ to assess the sensitivity of OT-MDR to these hyperparameters and identify optimal settings.
2. **Scaling to Larger Architectures**: Apply OT-MDR to larger architectures (e.g., EfficientNet-B0, ResNet50) and datasets (e.g., ImageNet-32) to evaluate scalability and computational efficiency.
3. **Theoretical Analysis of Sharpness Control**: Develop a theoretical analysis to quantify how the Wasserstein ball over model parameters bounds the sharpness of the loss landscape and relate this to generalization bounds.