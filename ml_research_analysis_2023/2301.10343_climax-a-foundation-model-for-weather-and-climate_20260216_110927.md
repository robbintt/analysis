---
ver: rpa2
title: 'ClimaX: A foundation model for weather and climate'
arxiv_id: '2301.10343'
source_url: https://arxiv.org/abs/2301.10343
tags:
- climax
- climate
- weather
- data
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClimaX is a foundation model for weather and climate that can be
  pre-trained on heterogeneous datasets and fine-tuned to address diverse downstream
  tasks. It extends the Transformer architecture with novel encoding and aggregation
  blocks to effectively use compute while maintaining general utility.
---

# ClimaX: A foundation model for weather and climate

## Quick Facts
- arXiv ID: 2301.10343
- Source URL: https://arxiv.org/abs/2301.10343
- Authors: Ashwin Srinivasan, Piyush Patil, Matthew Chantry, Stephan Rasp, Tom Beucler, William M. Townend, Peter Düben, Felix W. Schneider, Carlos Osés, Karsten Peters-von Gehlen, Marcel Korneli, Jonathan A. S. Chadwick, Michael J. Pritchard, Stephan Hoyer, Suman Ravuri, Remy Lam, Alex Pritzel, Daan Crommelin, Remi Lam
- Reference count: 40
- Primary result: ClimaX is a foundation model for weather and climate that can be pre-trained on heterogeneous datasets and fine-tuned to address diverse downstream tasks.

## Executive Summary
ClimaX is a foundation model for weather and climate that extends the Transformer architecture with novel encoding and aggregation blocks to effectively use compute while maintaining general utility. The model is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6 and can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. ClimaX achieves competitive performance on weather forecasting benchmarks compared to operational models like IFS, and shows significant improvements on climate projection tasks compared to existing data-driven baselines.

## Method Summary
ClimaX is built on the Vision Transformer (ViT) architecture with extensions for handling multimodal climate data. The model uses variable tokenization to process each atmospheric variable separately, followed by variable aggregation blocks that use cross-attention to reduce sequence length from V×h×w to h×w, lowering computational complexity. The model is pre-trained on heterogeneous CMIP6 datasets using a randomized forecasting objective (predicting weather conditions at random lead times from 6 to 168 hours). For downstream tasks, ClimaX is fine-tuned on specific datasets and objectives, with flexibility to handle different variables, resolutions, and lead times than seen during pretraining.

## Key Results
- Achieves competitive performance to operational IFS model on WeatherBench global weather forecasting at 5.625° resolution
- Shows 10-20% improvement in RMSE over previous methods on ClimateBench climate projection benchmarks
- Demonstrates effective transfer learning, achieving good performance on tasks with variables and lead times unseen during pretraining
- Maintains strong performance even when pre-trained at lower resolutions and compute budgets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ClimaX achieves superior performance by leveraging heterogeneous climate datasets (e.g., CMIP6) during pretraining, allowing it to learn general atmospheric dynamics that transfer to diverse downstream tasks.
- **Mechanism**: Pretraining on multiple climate models with different physical assumptions and forcings creates a rich, varied dataset that captures the full range of atmospheric variability. This exposure allows the model to learn robust, generalizable representations of weather and climate processes.
- **Core assumption**: The underlying physics governing atmospheric dynamics is consistent across different climate models, even if their specific implementations and forcings differ.
- **Evidence anchors**: [abstract]: "ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6." [section]: "We believe that CMIP6's diversity and scale presents an attractive opportunity for pretraining large-scale foundation models."
- **Break condition**: If the physical assumptions across climate models diverge significantly (e.g., different parameterizations of key processes), the learned representations may not generalize effectively.

### Mechanism 2
- **Claim**: The variable tokenization and aggregation blocks in ClimaX allow it to handle the multimodal nature of climate data efficiently, reducing computational cost while maintaining flexibility.
- **Mechanism**: By tokenizing each variable separately and then aggregating them using cross-attention, ClimaX reduces the sequence length from V×h×w to h×w, lowering the quadratic complexity of self-attention. This allows the model to handle datasets with many variables without prohibitive computational cost.
- **Core assumption**: The interactions between atmospheric variables can be effectively captured through a cross-attention aggregation step, even when variables are processed separately initially.
- **Evidence anchors**: [abstract]: "ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility." [section]: "We therefore propose variable aggregation to solve the two mentioned challenges... significantly lowering the computational cost."
- **Break condition**: If the cross-attention aggregation step fails to capture important variable interactions, performance on tasks requiring complex multivariate relationships may degrade.

### Mechanism 3
- **Claim**: The self-supervised pretraining objective (randomized forecasting) enables ClimaX to learn temporal dependencies that generalize beyond the pretraining window, allowing effective finetuning for tasks with different lead times and variables.
- **Mechanism**: By training the model to forecast arbitrary sets of variables at random lead times during pretraining, ClimaX learns a flexible representation of atmospheric dynamics that can be adapted to specific forecasting tasks during finetuning, even for lead times longer than seen during pretraining.
- **Core assumption**: The temporal dynamics of atmospheric variables are consistent enough across different lead times that a model trained on a range of horizons can generalize to unseen ones.
- **Evidence anchors**: [abstract]: "ClimaX is pre-trained with a self-supervised learning objective on climate datasets... The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks... including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining." [section]: "To obtain a pretrained model that is generally applicable to various temporal forecasting tasks, we randomize the lead time from 6 hours to 168 hours (i.e., 1 week) during pretraining."
- **Break condition**: If the atmospheric dynamics change significantly beyond the pretraining window (e.g., sub-seasonal to seasonal scales), the model may fail to extrapolate effectively.

## Foundational Learning

- **Concept: Transformer architecture and attention mechanisms**
  - Why needed here: ClimaX is built upon the Vision Transformer (ViT) architecture, and understanding how self-attention and cross-attention work is crucial for grasping how the model processes and aggregates information.
  - Quick check question: What is the computational complexity of self-attention in terms of sequence length, and how does ClimaX's variable aggregation address this?

- **Concept: Multimodal learning and variable aggregation**
  - Why needed here: Climate data is inherently multimodal, with different variables representing different physical quantities. ClimaX's variable tokenization and aggregation blocks are designed to handle this heterogeneity effectively.
  - Quick check question: How does ClimaX's variable aggregation differ from simply concatenating all variables as channels, and why is this beneficial?

- **Concept: Transfer learning and finetuning**
  - Why needed here: ClimaX is a foundation model that is pretrained on large climate datasets and then finetuned for specific downstream tasks. Understanding the principles of transfer learning is essential for effectively adapting the model to new tasks.
  - Quick check question: What are the advantages and disadvantages of finetuning all layers of ClimaX versus freezing some layers and only updating others?

## Architecture Onboarding

- **Component map**: Input representation → Variable tokenization → Variable aggregation → Transformer backbone → Prediction head → Output
- **Critical path**: 1. Input → Variable tokenization → Variable aggregation → Transformer → Prediction head → Output 2. Pretraining: Randomized forecasting objective with lead time conditioning 3. Finetuning: Task-specific adaptation with potential variable and resolution changes
- **Design tradeoffs**: Flexibility vs. efficiency: Variable tokenization allows handling datasets with different variables but increases sequence length, which is mitigated by variable aggregation; Pretraining data diversity vs. task specificity: Heterogeneous CMIP6 data provides broad generalization but may not be optimal for specific tasks; Resolution scaling: High-resolution data provides better performance but requires more compute and memory
- **Failure signatures**: Poor performance on tasks with variables unseen during pretraining: May indicate the need for additional finetuning or architectural modifications; Degraded performance when finetuning for longer lead times than seen during pretraining: May require different finetuning strategies or curriculum learning; High computational cost for datasets with many variables: May indicate the need for more aggressive variable aggregation or dimensionality reduction
- **First 3 experiments**: 1. Reproduce the global forecasting results on ERA5 at 5.625° resolution to verify basic functionality 2. Test finetuning on a regional forecasting task (e.g., North America) to validate the model's ability to handle spatially incomplete data 3. Experiment with different finetuning protocols (e.g., iterative vs. direct forecasting) to understand the trade-offs between computation and performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of ClimaX scale with the resolution of the input data, particularly when training on very high-resolution data (e.g., 0.25°)?
- **Basis in paper**: [explicit] The authors state that "scaling to higher resolutions (0.25°) is likely to lead to even better results" and that "the performance of ClimaX scales favorably with respect to data resolution."
- **Why unresolved**: The paper only reports results on data with resolutions up to 1.40625°, and the authors did not have access to high-resolution data for pretraining. This leaves the question of how well ClimaX would perform on very high-resolution data unanswered.
- **What evidence would resolve it**: Training and evaluating ClimaX on very high-resolution data (e.g., 0.25°) and comparing its performance to lower resolutions would provide the necessary evidence.

### Open Question 2
- **Question**: Can ClimaX be extended to handle even larger and more diverse datasets, such as those containing a wider range of climate variables, higher spatiotemporal resolutions, and future scenarios?
- **Basis in paper**: [inferred] The authors discuss the potential for scaling ClimaX by incorporating more diverse and heterogeneous datasets, but they only use a subset of variables from five CMIP6 datasets for pretraining.
- **Why unresolved**: The paper does not explore the limits of ClimaX's ability to handle larger and more diverse datasets, leaving the question of its scalability unanswered.
- **What evidence would resolve it**: Training ClimaX on a significantly larger and more diverse dataset, including a wider range of climate variables, higher spatiotemporal resolutions, and future scenarios, and evaluating its performance on downstream tasks would provide the necessary evidence.

### Open Question 3
- **Question**: How does the choice of finetuning protocol (e.g., separate models for each variable and lead time, lead-time-conditioned model, iterative prediction) affect the performance of ClimaX on different downstream tasks?
- **Basis in paper**: [explicit] The authors perform ablation studies comparing different finetuning protocols and discuss the trade-off between computation and performance.
- **Why unresolved**: The paper does not provide a comprehensive analysis of how the choice of finetuning protocol affects ClimaX's performance on various downstream tasks, leaving the question of the optimal protocol unanswered.
- **What evidence would resolve it**: Conducting extensive experiments comparing the performance of ClimaX using different finetuning protocols on a wide range of downstream tasks would provide the necessary evidence to determine the optimal protocol for each task.

## Limitations

- Claims comparing directly to operational models like IFS are potentially misleading due to different evaluation protocols and model objectives
- Evaluation focuses heavily on Z500 geopotential height with limited analysis of other variables like temperature and wind that showed worse performance
- Lacks extensive ablation studies to isolate contributions of different architectural components, particularly variable tokenization versus standard multi-channel approaches

## Confidence

- **High confidence**: The core architectural innovations (variable tokenization and aggregation) are well-specified and reproducible. The implementation details for the Vision Transformer backbone and the pretraining objective are clearly defined.
- **Medium confidence**: Claims about transfer learning performance across diverse tasks are supported by results but lack extensive ablation studies. The improvements over baselines are demonstrated but not thoroughly explained mechanistically.
- **Low confidence**: Claims comparing directly to operational models like IFS are overstated given the different evaluation protocols and model objectives. The paper doesn't adequately address the computational trade-offs of the variable aggregation approach versus simpler alternatives.

## Next Checks

1. **Ablation study on variable tokenization**: Implement a baseline ClimaX without variable tokenization (using standard multi-channel input) to quantify the actual contribution of this architectural choice versus the gains from pretraining on heterogeneous data.

2. **Extended evaluation on temperature and wind variables**: Re-run the weather forecasting benchmarks focusing on T850, T2m, and U10 variables that showed worse performance than Z500 to identify potential model weaknesses or data distribution issues.

3. **Pretraining data diversity analysis**: Train separate models on individual CMIP6 datasets (rather than the combined dataset) to quantify how much of the performance gain comes from data diversity versus model scale and pretraining duration.