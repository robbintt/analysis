---
ver: rpa2
title: Query-Policy Misalignment in Preference-Based Reinforcement Learning
arxiv_id: '2305.17400'
source_url: https://arxiv.org/abs/2305.17400
tags:
- query
- reward
- learning
- feedback
- on-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses query-policy misalignment in
  preference-based reinforcement learning, where queries selected to improve reward
  model quality may not align with the agent's current interests, leading to poor
  feedback efficiency. The authors propose near on-policy query selection and hybrid
  experience replay to enforce bidirectional query-policy alignment, implemented with
  minimal code changes to existing methods.
---

# Query-Policy Misalignment in Preference-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.17400
- Source URL: https://arxiv.org/abs/2305.17400
- Reference count: 40
- One-line primary result: QPA outperforms SURF and PEBBLE on DMControl and MetaWorld benchmarks while achieving superior feedback efficiency through bidirectional query-policy alignment

## Executive Summary
This paper identifies query-policy misalignment as a fundamental limitation in preference-based reinforcement learning, where queries selected to improve reward model quality may not align with the agent's current interests, leading to poor feedback efficiency. The authors propose Query-Policy Alignment (QPA), which combines near on-policy query selection and hybrid experience replay to ensure queries and value learning remain aligned with the current policy's interests. Experiments demonstrate substantial gains in both feedback and sample efficiency across DMControl and MetaWorld benchmarks, with QPA showing particular strength in early training stages where learned rewards may encode more targeted task information than ground truth rewards.

## Method Summary
QPA addresses query-policy misalignment by implementing two key mechanisms: near on-policy query selection that selects segments from recent trajectories stored in a small buffer aligned with the current policy, and hybrid experience replay that samples transitions from both the full replay buffer and the near on-policy buffer to ensure Q-function updates occur in regions where the preference predictor is accurate. The method builds on SAC-based off-policy PbRL and requires minimal code changes to existing implementations. Data augmentation through snippet pair subsampling further expands the preference buffer and improves reward learning efficiency.

## Key Results
- QPA achieves superior feedback efficiency compared to SURF and PEBBLE across all tested DMControl and MetaWorld tasks
- The method can surpass SAC with ground truth rewards during early training stages in some tasks, suggesting learned rewards may encode more targeted task information
- Near on-policy query selection and hybrid experience replay together provide substantial improvements in both sample and feedback efficiency
- QPA demonstrates robustness across diverse locomotion and manipulation tasks with varying complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-policy misalignment reduces feedback efficiency because queries are selected based on overall reward model quality rather than the current policy's interests.
- Mechanism: Existing PbRL methods optimize query selection to improve the global quality of the reward model, but this leads to selecting behaviors that are outside the current policy's visitation distribution, providing little policy learning benefit.
- Core assumption: The current policy's visitation distribution changes as training progresses, and queries should be aligned with this distribution to be effective.
- Evidence anchors:
  - [abstract] "we find that this may not necessarily lead to improved performance" and "the seemingly informative queries selected to improve the overall quality of reward model actually may not align with RL agents' interests"
  - [section 4] "We observe that the queried behaviors often fall outside the scope of the current policy's visitation distribution"
  - [corpus] "Provable Reward-Agnostic Preference-Based Reinforcement Learning" suggests theoretical frameworks for PbRL exist, supporting the need for alignment mechanisms

### Mechanism 2
- Claim: Near on-policy query selection improves performance by ensuring queries align with the current policy's interests.
- Mechanism: By selecting segments from recent trajectories stored in a near on-policy buffer, the method ensures that queries reflect behaviors the current policy is actually encountering and learning from.
- Core assumption: Recent trajectories better represent the current policy's interests than older trajectories or trajectories from completely different policies.
- Evidence anchors:
  - [section 4] "near-on-policy selection selects fresh segments that are recently visited by the current RL policy, enabling timely feedback on the current status of the policy"
  - [section 5.1] "we highlight that the segment query selection should be aligned with the on-policy distribution"
  - [corpus] "Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards" suggests exploration strategies that could complement near on-policy selection

### Mechanism 3
- Claim: Hybrid experience replay ensures value learning is aligned with the on-policy distribution where the reward model performs well.
- Mechanism: By sampling transitions from both the full replay buffer and the near on-policy buffer, the method ensures Q-function updates occur more frequently in regions where the preference predictor is accurate.
- Core assumption: The reward model learned through near on-policy queries is more accurate within the on-policy distribution, and value learning should prioritize these regions.
- Evidence anchors:
  - [section 5.2] "more attention should be paid to improving the veracity of Q-function within on-policy distribution dπ, where the preference (reward) predictor performs well"
  - [section 5.2] "we devise a hybrid experience replay mechanism" that samples from both buffers
  - [corpus] "SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning" suggests exploration techniques that could interact with hybrid replay

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper builds on standard RL formulations where the agent learns from state-action-reward transitions in an MDP framework
  - Quick check question: What are the five components of an MDP tuple (S, A, r, T, γ) and how does each relate to PbRL?

- Concept: Off-policy actor-critic algorithms
  - Why needed here: QPA is built on top of off-policy RL algorithms like SAC, requiring understanding of how policy evaluation and improvement work in this setting
  - Quick check question: How does the policy evaluation step in off-policy RL differ from on-policy RL, and why is this important for hybrid experience replay?

- Concept: Bradley-Terry model for preference prediction
  - Why needed here: The reward model is trained using preference comparisons via the Bradley-Terry model, which is fundamental to understanding how preferences translate to rewards
  - Quick check question: How does the Bradley-Terry model convert pairwise preferences into a probability distribution over segment preferences?

## Architecture Onboarding

- Component map:
  - Near on-policy buffer (Don) -> Preference buffer (Dσ) -> Reward model (brψ) -> Policy (πϕ) and Q-function (Qθ)
  - Replay buffer (D) -> Hybrid experience replay -> Q-function (Qθ) and Policy (πϕ)

- Critical path:
  1. Collect experience → Store in Don and D
  2. Near on-policy query selection from Don → Get human preferences → Store in Dσ
  3. Train reward model brψ using Dσ with data augmentation
  4. Hybrid experience replay: Sample from D and Don → Update Qθ and πϕ
  5. Repeat until convergence

- Design tradeoffs:
  - Buffer size N vs. alignment: Larger buffers provide more samples but may include outdated policies
  - Data augmentation ratio τ vs. training stability: Higher ratios expand dataset but may introduce noise
  - Query frequency vs. sample efficiency: More frequent queries provide better reward learning but slow down RL training

- Failure signatures:
  - Performance plateaus despite many queries: May indicate query-policy misalignment
  - High variance in learning curves: Could suggest insufficient data augmentation or buffer size issues
  - Poor final performance: Might indicate hybrid replay isn't properly balancing on-policy vs. off-policy updates

- First 3 experiments:
  1. Test near on-policy query selection with different buffer sizes (N=5, 10, 30) on Walker_walk task
  2. Evaluate hybrid experience replay by comparing uniform sampling vs. hybrid sampling on Cheetah_run
  3. Assess data augmentation impact by running with τ=0, 10, 20, 100 on Humanoid_stand task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the learned reward function by QPA encode more targeted task information compared to ground truth rewards, leading to faster learning in early stages?
- Basis in paper: [explicit] The paper observes that in some tasks, QPA can surpass SAC with ground truth reward during early training stages, despite experiencing stagnation of performance improvement as feedback provision is halted in the later stages.
- Why unresolved: The paper does not provide a detailed analysis of the learned reward function or compare its information content to the ground truth reward function.
- What evidence would resolve it: Analyzing the learned reward function's properties and comparing its information content to the ground truth reward function, as well as investigating the impact of learned rewards on policy learning in early stages.

### Open Question 2
- Question: How does the size of the near on-policy buffer affect the performance of QPA in different tasks?
- Basis in paper: [explicit] The paper mentions that a larger N implies that the near on-policy buffer Don contain additional historical trajectories generated by a past policy π′ that may be significantly different from the current policy π, potentially deviating from the main idea of near on-policy query selection.
- Why unresolved: The paper only provides limited ablation results on the effect of near on-policy buffer size and does not explore the optimal buffer size for different tasks.
- What evidence would resolve it: Conducting extensive experiments with different near on-policy buffer sizes in various tasks and analyzing the impact on performance.

### Open Question 3
- Question: How does the data augmentation ratio affect the performance of QPA in different tasks?
- Basis in paper: [explicit] The paper mentions that a larger data augmentation ratio does not necessarily guarantee improved performance and provides limited ablation results on the effect of data augmentation ratio.
- Why unresolved: The paper does not provide a detailed analysis of the optimal data augmentation ratio for different tasks or explore the impact of data augmentation on performance.
- What evidence would resolve it: Conducting extensive experiments with different data augmentation ratios in various tasks and analyzing the impact on performance, as well as investigating the optimal data augmentation ratio for different tasks.

## Limitations

- The theoretical analysis of query-policy misalignment remains limited, with the mechanism explanations being intuitive rather than rigorously characterized mathematically
- Scalability to more complex environments requiring longer-horizon preferences has not been demonstrated, with low confidence in the approach's effectiveness for tasks beyond the tested locomotion and manipulation domains
- The optimal buffer size and data augmentation ratio appear to be task-dependent, requiring extensive hyperparameter tuning that is not fully explored in the paper

## Confidence

- High confidence: The empirical results showing improved feedback efficiency are robust across multiple tasks and baselines, with clear quantitative improvements over state-of-the-art methods
- Medium confidence: The proposed solutions of near on-policy query selection and hybrid experience replay are likely to generalize to other PbRL tasks, though task-specific tuning may be required
- Low confidence: The scalability of these alignment techniques to more complex environments requiring longer-horizon preferences remains unproven

## Next Checks

1. Test QPA with varying buffer sizes (N=5, 10, 30) to empirically determine the optimal balance between on-policyness and sample diversity on Walker_walk task
2. Implement ablation studies comparing pure near on-policy replay vs. hybrid replay to quantify the contribution of each component on Cheetah_run
3. Evaluate QPA's performance when integrated with different preference query selection strategies (disagreement, ensemble uncertainty) to test robustness beyond uniform sampling