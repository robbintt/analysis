---
ver: rpa2
title: Guiding The Last Layer in Federated Learning with Pre-Trained Models
arxiv_id: '2306.03937'
source_url: https://arxiv.org/abs/2306.03937
tags:
- fedncm
- learning
- accuracy
- communication
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits federated learning from pre-trained models,
  showing that simple linear probing and Nearest Class Means (NCM) classifiers can
  be highly effective in the federated setting. The authors propose FedNCM, a one-round
  algorithm to compute NCM exactly and efficiently, requiring minimal communication
  and compute.
---

# Guiding The Last Layer in Federated Learning with Pre-Trained Models

## Quick Facts
- arXiv ID: 2306.03937
- Source URL: https://arxiv.org/abs/2306.03937
- Authors: [Not specified in source]
- Reference count: 17
- Key outcome: FedNCM achieves higher accuracy with faster convergence than existing methods across multiple datasets, drastically reducing communication and computation costs.

## Executive Summary
This work revisits federated learning from pre-trained models, showing that simple linear probing and Nearest Class Means (NCM) classifiers can be highly effective in the federated setting. The authors propose FedNCM, a one-round algorithm to compute NCM exactly and efficiently, requiring minimal communication and compute. They also introduce a two-phase approach (FedNCM+FT) where NCM is used to initialize a linear head, followed by fine-tuning. FedNCM+FT achieves higher accuracy with faster convergence than existing methods across multiple datasets (CIFAR-10, Flowers102, CUB-200, Cars-196, EuroSAT). It is also more robust to client heterogeneity and stable across hyperparameters. The method drastically reduces communication and computation costs compared to full fine-tuning.

## Method Summary
The paper introduces FedNCM, an efficient algorithm for computing Nearest Class Means in federated learning. Each client computes class means locally and sends them to the server, which aggregates weighted means to form a global NCM classifier. The authors also propose a two-phase approach (FedNCM+FT) where NCM is used to initialize a linear head, followed by fine-tuning. This method is evaluated across multiple datasets using pre-trained models, comparing accuracy, communication cost, and compute cost against linear probing and full fine-tuning.

## Key Results
- FedNCM achieves higher accuracy with faster convergence than existing methods across multiple datasets.
- FedNCM drastically reduces communication and computation costs compared to full fine-tuning.
- FedNCM+FT is more robust to client heterogeneity and stable across hyperparameters.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training only the last layer (linear head) is often sufficient for strong performance when starting from a pre-trained model.
- Mechanism: Pre-trained models already encode rich feature representations; only the task-specific decision boundary needs adaptation.
- Core assumption: The frozen base model provides sufficiently discriminative features for the downstream task.
- Evidence anchors:
  - [abstract]: "simply fitting a linear classification head can be efficient and effective in many cases"
  - [section]: "In many supervised learning tasks studied in the literature such as transfer learning from ImageNet, the representations are powerful and training only the linear classifier is sufficient for strong performance [Kornblith et al., 2019]"
  - [corpus]: Weak - no direct citations in neighbors about linear head sufficiency.
- Break condition: If the feature space is not aligned with the downstream task (e.g., very different domain), linear probing will underperform.

### Mechanism 2
- Claim: Nearest Class Means (NCM) can be computed exactly and efficiently in federated setting without violating privacy constraints.
- Mechanism: Each client computes class means locally and sends them to server; server aggregates weighted means to form global NCM classifier.
- Core assumption: Computing class means does not leak individual sample data beyond what's necessary for the task.
- Evidence anchors:
  - [abstract]: "We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals"
  - [section]: "Outlined in Algo. 1., FedNCM allows an efficient classifier approximation for pre-trained models that addresses many of the critical concerns in the FL setting including privacy, communication, and computation time"
  - [corpus]: Weak - neighbors focus on layer dropout and architecture heterogeneity, not NCM.
- Break condition: If class means alone do not capture task-relevant variance (e.g., multimodal classes).

### Mechanism 3
- Claim: Two-stage approach (FedNCM → fine-tuning) yields faster convergence and better accuracy than direct fine-tuning.
- Mechanism: NCM initialization provides a stable starting point for the linear head, reducing client drift and leading to more rapid and stable convergence in fine-tuning phase.
- Core assumption: Better initialization of the linear head reduces the distance to optimum in representation space.
- Evidence anchors:
  - [abstract]: "using a two-phase approach of obtaining the classifier and then fine-tuning the model can yield rapid convergence and improved generalization"
  - [section]: "We observe that FedNCM HeadTuning requires only a single forward pass through the data... can have a substantial advantage in convergence when compared to simply a fine-tuning phase"
  - [corpus]: Weak - neighbors discuss initialization but not two-stage FedNCM+FT specifically.
- Break condition: If fine-tuning phase does not sufficiently adapt the representation (e.g., too few rounds or poor learning rate).

## Foundational Learning

- Concept: Federated Learning fundamentals (FedAvg, client heterogeneity, communication rounds)
  - Why needed here: The entire paper builds on federated optimization; understanding data distribution and aggregation is critical.
  - Quick check question: What is the main challenge FedAvg faces with non-i.i.d. data, and how does it manifest in model updates?
- Concept: Transfer learning and pre-trained models
  - Why needed here: The paper assumes a pre-trained base model and leverages transfer learning literature to justify last-layer tuning.
  - Quick check question: Why is fine-tuning only the last layer often sufficient in transfer learning?
- Concept: Nearest Class Means classifier
  - Why needed here: FedNCM is the core algorithm; understanding NCM is essential to grasp how the method works.
  - Quick check question: How does NCM classify a new sample, and what data does each client need to compute locally?

## Architecture Onboarding

- Component map: Pre-trained base model (frozen) -> Local class mean computation -> Server-side weighted aggregation -> Optional fine-tuning phase
- Critical path:
  1. Server sends pre-trained weights to all clients
  2. Each client computes local class means and sends to server
  3. Server aggregates to form global NCM classifier
  4. (Optional) Initialize linear head from NCM and fine-tune
- Design tradeoffs:
  - FedNCM: minimal communication (one round), no hyperparameters, but may underfit if classes are complex
  - LP: more communication than FedNCM, needs optimization, but can adapt decision boundary more flexibly
  - FT: highest communication, most compute, but fully adapts representation
- Failure signatures:
  - FedNCM: Poor accuracy on multimodal classes or when feature space is not discriminative enough
  - LP: Slow convergence or unstable training if client data is highly heterogeneous
  - FT: Large client drift, high communication cost, sensitive to learning rates
- First 3 experiments:
  1. Run FedNCM on CIFAR-10 with α=0.1 Dirichlet; verify accuracy > random and communication cost minimal
  2. Run LP on CIFAR-10 with same setup; compare accuracy and communication cost to FedNCM
  3. Run FedNCM+FT on CIFAR-10; verify improved accuracy and faster convergence vs FT alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedNCM+FT perform with different federated optimization algorithms like SCAFFOLD or FedNova?
- Basis in paper: [explicit] The authors mention that their two-phase approach is compatible with any federated optimization algorithm, and they test it with FedAvg and FedAdam, but not with other algorithms like SCAFFOLD or FedNova.
- Why unresolved: The paper only tests FedNCM+FT with FedAvg and FedAdam, leaving open the question of its performance with other algorithms.
- What evidence would resolve it: Empirical results comparing FedNCM+FT with FedAvg, FedAdam, SCAFFOLD, and FedNova on the same datasets.

### Open Question 2
- Question: What is the impact of using different pre-trained models (e.g., VGG, MobileNet) on the performance of FedNCM and FedNCM+FT?
- Basis in paper: [inferred] The authors use SqueezeNet and ResNet18, but do not explore the impact of using different pre-trained models.
- Why unresolved: The paper does not investigate how the choice of pre-trained model affects the performance of FedNCM and FedNCM+FT.
- What evidence would resolve it: Empirical results comparing the performance of FedNCM and FedNCM+FT using different pre-trained models on the same datasets.

### Open Question 3
- Question: How does FedNCM+FT perform in the presence of more severe client heterogeneity, such as when using a Dirichlet distribution with α < 0.01?
- Basis in paper: [explicit] The authors mention that FedNCM+FT is more robust to high heterogeneity, but they only test it with α = 0.01, leaving the question of its performance with even higher heterogeneity open.
- Why unresolved: The paper does not explore the performance of FedNCM+FT in the presence of more severe client heterogeneity.
- What evidence would resolve it: Empirical results comparing the performance of FedNCM+FT with α < 0.01 and other values of α on the same datasets.

## Limitations

- The paper assumes frozen base models throughout FedNCM, which may limit adaptability in scenarios requiring representation updates.
- The analysis of privacy guarantees for class mean aggregation remains implicit, and the method's performance on highly multimodal classes or extreme non-IID distributions warrants further investigation.
- The computational savings translate to resource-constrained edge devices in practice is not validated in the paper.

## Confidence

- FedNCM efficiency and accuracy claims: High
- Two-stage FedNCM+FT convergence benefits: Medium
- Privacy preservation guarantees: Low-Medium
- Generalization to unseen domains: Medium

## Next Checks

1. Test FedNCM on a highly multimodal dataset (e.g., CIFAR-100) to evaluate class mean representation limits
2. Implement differential privacy guarantees for class mean aggregation and measure utility-privacy tradeoffs
3. Deploy FedNCM+FT on actual edge devices to validate real-world computational savings versus simulated metrics