---
ver: rpa2
title: Familiarity-Based Open-Set Recognition Under Adversarial Attacks
arxiv_id: '2311.05006'
source_url: https://arxiv.org/abs/2311.05006
tags:
- attacks
- 'false'
- adversarial
- familiarity
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of familiarity-based
  open-set recognition (OSR) methods to adversarial attacks. Specifically, it studies
  gradient-based attacks on the Maximum Logit Score (MLS) and Maximum Softmax Probability
  (MSP) familiarity scores.
---

# Familiarity-Based Open-Set Recognition Under Adversarial Attacks

## Quick Facts
- **arXiv ID:** 2311.05006
- **Source URL:** https://arxiv.org/abs/2311.05006
- **Reference count:** 26
- **One-line primary result:** False Novelty attacks are more effective in uninformed settings, while False Familiarity attacks are more effective in informed settings for manipulating familiarity-based OSR methods.

## Executive Summary
This paper investigates the vulnerability of familiarity-based open-set recognition (OSR) methods to adversarial attacks, focusing on gradient-based attacks targeting Maximum Logit Score (MLS) and Maximum Softmax Probability (MSP) familiarity scores. The authors propose and evaluate three types of attacks: False Familiarity (increasing familiar class logits), False Novelty (decreasing familiar class logits), and a combination of both. They compare the effectiveness of these attacks using Fast Gradient Sign Method (FGSM) and iterative methods (BIM, RPROP) in both informed (knowing if input is open-set or closed-set) and uninformed settings. The study reveals that iterative attacks are more effective than FGSM attacks, and that the effectiveness of False Familiarity vs. False Novelty attacks depends on the adversary's knowledge of input type.

## Method Summary
The study evaluates the vulnerability of familiarity-based OSR methods to adversarial attacks using the TinyImageNet dataset with a VGG32 CNN architecture. The authors propose three types of gradient-based attacks (False Familiarity, False Novelty, and a combination) and test them using FGSM and iterative methods (BIM, RPROP) in both informed and uninformed settings. The effectiveness of these attacks is measured by their impact on the Area Under the Receiver-Operator Curve (AUROC), which evaluates the ranking from open-set to closed-set samples. The attacks aim to lower the AUROC by manipulating the familiarity scores (MLS/MSP) through adversarial perturbations.

## Key Results
- False Novelty attacks are more effective in uninformed settings, while False Familiarity attacks are more effective in informed settings.
- Iterative attacks (BIM, RPROP) are more effective than FGSM attacks in manipulating familiarity scores.
- Informed iterative attacks can decrease the AUROC by an order of magnitude compared to informed FGSM attacks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Familiarity-based OSR is vulnerable to adversarial attacks that manipulate logit scores.
- Mechanism: Adversarial perturbations can increase or decrease logit scores of familiar classes, causing false novelty or false familiarity detections.
- Core assumption: Familiarity scores (MLS/MSP) directly reflect the presence of familiar features in the input.
- Evidence anchors:
  - [abstract] "Here, we present gradient-based adversarial attacks on familiarity scores for both types of attacks, False Familiarity and False Novelty attacks, and evaluate their effectiveness..."
  - [section] "Adversarial perturbations created with RPROP can be sparse and may therefore be less noticeable."
- Break condition: If familiarity scores become uncorrelated with actual class presence, or if adversarial perturbations are effectively bounded or detected.

### Mechanism 2
- Claim: False Novelty attacks are more effective in uninformed settings, while False Familiarity attacks are more effective in informed settings.
- Mechanism: In uninformed attacks, decreasing logits of familiar classes (FP attacks) disrupts the ranking more than increasing logits (FN attacks). In informed attacks, knowing input type allows targeted attacks that more effectively reverse the ranking.
- Core assumption: The effectiveness of adversarial attacks depends on the adversary's knowledge of input type (open-set vs. closed-set).
- Evidence anchors:
  - [abstract] "The results show that False Novelty attacks are more effective in the uninformed setting, while False Familiarity attacks are more effective in the informed setting."
  - [section] "Informed FGSM and iterative attacks are able to reverse the ranking of novel and familiar images almost perfectly..."
- Break condition: If the model's familiarity score calculation changes or if the adversarial attack space is constrained beyond effective manipulation.

### Mechanism 3
- Claim: Iterative attacks (BIM, RPROP) are more effective than FGSM attacks in manipulating familiarity scores.
- Mechanism: Iterative methods allow for more flexible optimization of the adversarial objective, leading to stronger perturbations that more effectively manipulate familiarity scores.
- Core assumption: More flexible optimization methods can find stronger adversarial perturbations than single-step methods.
- Evidence anchors:
  - [abstract] "The study also finds that iterative attacks are more effective than FGSM attacks."
  - [section] "Informed iterative attacks are able to decrease the AUROC by an order of magnitude compared to informed FGSM attacks..."
- Break condition: If the model's optimization landscape changes or if defensive mechanisms are introduced to mitigate iterative attacks.

## Foundational Learning

- Concept: Familiarity-based scoring rules (MLS, MSP)
  - Why needed here: These are the core mechanisms being attacked; understanding how they work is crucial to understanding the vulnerability.
  - Quick check question: What is the difference between MLS and MSP, and how do they rank familiarity?

- Concept: Adversarial attack methods (FGSM, BIM, RPROP)
  - Why needed here: The paper compares the effectiveness of different attack methods on familiarity-based OSR; knowing how these attacks work is essential.
  - Quick check question: How does the optimization process differ between FGSM and iterative attacks like BIM and RPROP?

- Concept: Informed vs. uninformed adversarial attacks
  - Why needed here: The paper investigates the impact of adversary knowledge on attack effectiveness; understanding this distinction is key to interpreting the results.
  - Quick check question: What is the key difference between informed and uninformed adversarial attacks, and how does this knowledge affect the attack strategy?

## Architecture Onboarding

- Component map: VGG32 CNN -> Familiarity-based OSR model -> Adversarial attack generation (FGSM, BIM, RPROP) -> Evaluation metrics (AUROC, MLS/MSP scores) -> TinyImageNet dataset

- Critical path:
  1. Train familiarity-based OSR model on closed-set data.
  2. Generate adversarial perturbations using chosen attack method.
  3. Evaluate OSR performance on adversarial examples using AUROC and familiarity scores.

- Design tradeoffs:
  - FGSM vs. iterative attacks: Speed vs. effectiveness of perturbations.
  - False Familiarity vs. False Novelty attacks: Targeted vs. untargeted manipulation of familiarity scores.
  - Informed vs. uninformed attacks: Adversary knowledge vs. general attack applicability.

- Failure signatures:
  - Low AUROC on adversarial examples, indicating successful attacks.
  - High correlation between familiarity scores and adversarial perturbations.
  - Effective reversal of ranking between familiar and novel samples.

- First 3 experiments:
  1. Apply FGSM False Familiarity and False Novelty attacks to a trained OSR model and compare AUROC scores.
  2. Repeat experiment with iterative attacks (BIM, RPROP) and compare effectiveness to FGSM.
  3. Implement informed attacks and compare effectiveness to uninformed attacks, focusing on the ability to reverse the ranking.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the findings of this study apply to other familiarity-based OSR approaches beyond the MLS and MSP scoring rules?
- Basis in paper: [explicit] The authors state that "It remains to be tested if the observed adversarial robustness holds for alternative familiarity scores, such as the MSP."
- Why unresolved: The study only investigates the vulnerability of familiarity-based OSR approaches using the MLS and MSP scoring rules. Other familiarity-based approaches may have different vulnerabilities to adversarial attacks.
- What evidence would resolve it: Testing the vulnerability of other familiarity-based OSR approaches to adversarial attacks and comparing the results with the findings of this study.

### Open Question 2
- Question: How can familiarity-based OSR approaches be made more robust to adversarial attacks?
- Basis in paper: [inferred] The study highlights the vulnerability of familiarity-based OSR approaches to adversarial attacks, particularly in the uninformed setting. The authors suggest that the findings can contribute to the design of better scoring rules in the future and to make familiarity scores robust to adversarial attacks.
- Why unresolved: The study identifies the problem but does not propose a solution. Developing robust scoring rules that can withstand adversarial attacks is an open research question.
- What evidence would resolve it: Proposing and evaluating new scoring rules or modifications to existing ones that can improve the robustness of familiarity-based OSR approaches to adversarial attacks.

### Open Question 3
- Question: How do the results of this study generalize to other datasets and model architectures?
- Basis in paper: [inferred] The study is conducted on the TinyImageNet dataset using a VGG32 architecture. The authors do not investigate the generalizability of their findings to other datasets or model architectures.
- Why unresolved: The vulnerability of familiarity-based OSR approaches to adversarial attacks may vary depending on the dataset and model architecture used. The generalizability of the findings to other settings is an open question.
- What evidence would resolve it: Replicating the study on different datasets and model architectures and comparing the results with the findings of this study.

## Limitations

- The paper lacks specific implementation details for the VGG32 architecture and hyperparameter settings for adversarial attacks, limiting reproducibility.
- The study focuses solely on gradient-based attacks and does not explore other attack vectors or defensive mechanisms that could impact real-world applicability.
- The evaluation is conducted on a single dataset (TinyImageNet), raising questions about generalizability to other domains and data distributions.

## Confidence

- **High confidence**: The mechanism that adversarial perturbations can manipulate familiarity scores (MLS/MSP) due to direct experimental evidence and established literature on adversarial attacks.
- **Medium confidence**: The comparative effectiveness of informed vs. uninformed attacks, as the results are based on a single dataset and attack scenario.
- **Low confidence**: The practical implications of the findings without further testing on diverse datasets and defensive mechanisms.

## Next Checks

1. Implement the exact VGG32 architecture and reproduce the closed-set accuracy of 84.2% on TinyImageNet to verify the foundational model performance.
2. Test the proposed adversarial attacks on additional datasets (e.g., CIFAR-10, STL-10) to assess generalizability and robustness across different data distributions.
3. Investigate the effectiveness of defensive mechanisms (e.g., adversarial training, input preprocessing) against the proposed attacks to evaluate potential countermeasures and improve OSR security.