---
ver: rpa2
title: An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19
  CT Scans and Uncurated Reports
arxiv_id: '2309.01740'
source_url: https://arxiv.org/abs/2309.01740
tags:
- data
- vision
- learning
- zero-shot
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of fine-grained multi-label classification
  in medical imaging, specifically detecting COVID-19 related lung conditions (pulmonary
  embolism, pneumonia, consolidation, infiltrates, and ground glass opacities) from
  CT scans and uncurated radiology reports. The core method involves contrastive visual
  language learning using pre-trained CLIP-based models (CheXzero, MedCLIP, BioMedCLIP)
  with class-dependent templates for zero-shot classification.
---

# An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports

## Quick Facts
- arXiv ID: 2309.01740
- Source URL: https://arxiv.org/abs/2309.01740
- Reference count: 40
- Primary result: CheXzero with class-dependent templates achieved macro average F1 of 0.71 and subset accuracy of 23% on fine-grained COVID-19 lung pathology detection from CT scans and uncurated reports

## Executive Summary
This study addresses the challenge of fine-grained multi-label classification in medical imaging, specifically detecting COVID-19 related lung conditions (pulmonary embolism, pneumonia, consolidation, infiltrates, and ground glass opacities) from CT scans and uncurated radiology reports. The core method involves contrastive visual language learning using pre-trained CLIP-based models (CheXzero, MedCLIP, BioMedCLIP) with class-dependent templates for zero-shot classification. The best model (CheXzero with class-dependent templates) achieved a macro average F1 score of 0.71 and subset accuracy of 23%, significantly outperforming frozen models and demonstrating the value of pre-training on large chest X-ray datasets for this fine-grained task. The results highlight the potential of self-supervised learning approaches for automated detection of complex lung pathologies from unstructured medical data.

## Method Summary
The study employs contrastive visual language learning using pre-trained CLIP-based models with class-dependent templates for zero-shot multi-label classification. CT scans are preprocessed into 4-slice montages, while German radiology reports are translated to English and filtered for lung parenchyma content. The method fine-tunes vision and text encoders on training data, then performs zero-shot classification by computing cosine similarity between image and text embeddings using positive-negative template pairs. Class-dependent templates are manually designed for each pathology class based on report analysis. The evaluation uses macro average F1, Hamming loss, and subset accuracy metrics on a test set of 92 patient studies.

## Key Results
- CheXzero with class-dependent templates achieved the best performance: macro F1 0.71, Hamming loss 0.35, subset accuracy 23%
- Fine-tuning pre-trained models substantially improved performance compared to frozen models (CheXzero subset accuracy increased from 0% to 23%)
- Class-dependent templates significantly outperformed class-independent templates (macro F1 0.71 vs 0.55 for CheXzero)
- RadBERT text encoder, pre-trained on large radiology report corpus, was crucial for effective cross-modal alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using class-dependent templates significantly improves zero-shot multi-label classification performance compared to class-independent templates.
- Mechanism: Class-dependent templates allow each pathology class to have its own set of positive-negative prompt pairs that better match the specific language patterns in the radiology reports for that condition. This targeted prompting increases the semantic alignment between the CT montage embeddings and the text embeddings, leading to more accurate similarity scores and better classification predictions.
- Core assumption: The language patterns and terminology used in radiology reports vary significantly between different lung pathologies, and a single generic template cannot capture these nuances effectively.
- Evidence anchors:
  - [abstract] "rather than keeping class-independent templates like standard practice [32], we design per-class templates."
  - [section] "For a given class, our prompt is a positive-negative template paired with the word CLASSNAME, which is replaced by the class...Using this and with the manual reading of the reports, we identify prompts which occur with our class names; for example, the phrase 'bilateral infiltrates' creates our prompt 'bilateral CLASSNAME' for the class infiltrates."
  - [section] "As shown in Table 3, CheXzero improves drastically in all three metrics, in particular with a subset accuracy increase of 18%, when applying class-dependent prompts."
- Break Condition: If the language patterns in the reports do not differ meaningfully between classes, or if the class-dependent templates are poorly designed and do not match actual report language.

### Mechanism 2
- Claim: Fine-tuning pre-trained CLIP models on domain-specific data substantially improves zero-shot performance compared to using frozen models.
- Mechanism: Pre-trained CLIP models have learned general visual-text alignment from large web-scale datasets. Fine-tuning these models on medical image-text pairs (even if from a different modality like X-rays) adapts their embeddings to the medical domain's semantic space, improving their ability to match CT visual features with the corresponding radiological language in reports. This adaptation is critical because the domain shift from natural images to medical images is significant.
- Core assumption: The visual and textual representations learned by pre-trained models can be effectively adapted to a new but related domain through fine-tuning, even when the fine-tuning data is from a slightly different modality.
- Evidence anchors:
  - [abstract] "Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature."
  - [section] "We see the effect of large datasets does not compensate for the shift to our domain, and the models need specific finetuning."
  - [section] "MedCLIP and BioMedCLIP respectively gain 8 and 3 percent points...CheXzero has adapted extremely well to our dataset, increasing the subset accuracy from 0% to 23%."
  - [section] "This finding suggests that previous pre-training on large chest X-ray datasets and their corresponding reports helps achieve better results in this fine-grained task."
- Break Condition: If the fine-tuning dataset is too small or unrepresentative, or if the domain shift is too large for the pre-trained representations to adapt effectively.

### Mechanism 3
- Claim: Using the RadBERT text encoder significantly improves zero-shot classification performance compared to using generic text encoders.
- Mechanism: RadBERT has been exposed to a vast amount of radiology-specific terminology and report structures during pre-training, and further fine-tuned on COVID-19 reports. This gives it a much richer and more accurate representation of medical language compared to general-purpose encoders. When combined with a vision encoder trained on medical images, this leads to better cross-modal alignment and more accurate zero-shot predictions.
- Core assumption: The semantic space of medical reports is highly specialized and domain-specific, requiring a text encoder that has been explicitly trained on large-scale, relevant medical text data.
- Evidence anchors:
  - [section] "We consider established medical-based CLIP methods whilst also experimenting with extracting their vision encoder and finetuning with the RadBERT model. The RadBERT model is pre-trained on 466 million tokens or 4.42M radiology reports...The prior representations of the text data learned in this model suit this study."
  - [section] "Interestingly, the latter is obtained with RN50, which has not been pre-trained on medical datasets." (Implies RadBERT's importance in compensating for non-medical vision encoders)
  - [section] "All text encoders use the COVID-finetuned RadBERT, which has prior knowledge of radiological terms such as 'pulmonary embolism' and 'ground glass opacities'."
- Break Condition: If the RadBERT model's pre-training or fine-tuning data is not representative of the target domain, or if the vision encoder's representations are too dissimilar from RadBERT's text space.

## Foundational Learning

- Concept: Contrastive Visual Language Learning (CLIP)
  - Why needed here: The task requires mapping between CT images and their corresponding radiology reports, which are in different modalities (visual vs. text). CLIP learns to embed images and text into a shared latent space where their similarity can be computed, enabling zero-shot classification by finding the text description most similar to a given image.
  - Quick check question: What is the primary objective function used in CLIP training to align image and text embeddings?

- Concept: Zero-Shot Learning
  - Why needed here: The study aims to classify CT scans into specific lung pathologies without having seen any labeled examples of those classes during training. Zero-shot learning allows the model to generalize to new classes based on their textual descriptions and the learned cross-modal alignment.
  - Quick check question: In the context of this paper, how does the model make a prediction for a specific class during zero-shot evaluation?

- Concept: Multi-Label Classification
  - Why needed here: A single CT scan can exhibit multiple lung pathologies simultaneously (e.g., both pneumonia and ground glass opacities). The model must be able to predict the presence or absence of each pathology independently, rather than assigning a single label.
  - Quick check question: What metric is used in the paper to evaluate the model's performance on this multi-label task, and why is it appropriate?

## Architecture Onboarding

- Component map: Data preprocessing → CT volume to 4-slice montages → German report translation → lung parenchyma extraction → vision encoder embedding → text encoder embedding → cosine similarity → softmax prediction → evaluation metrics
- Critical path: Data preprocessing → embedding generation (vision + text) → similarity calculation → softmax prediction → evaluation
- Design tradeoffs:
  - Using pre-trained models vs. training from scratch: Pre-trained models offer a strong starting point and require less data, but may have domain bias.
  - Class-dependent vs. class-independent templates: Class-dependent templates improve performance but require manual analysis of the reports.
  - Fine-tuning vs. frozen encoders: Fine-tuning adapts the model to the specific domain but risks overfitting on small datasets.
- Failure signatures:
  - Low performance across all metrics: Indicates a fundamental issue with the cross-modal alignment or the quality of the pre-trained models.
  - High Hamming loss but decent F1: Suggests the model is predicting too many or too few labels on average.
  - Good overall F1 but low subset accuracy: Indicates the model struggles with predicting the exact combination of labels for each sample.
- First 3 experiments:
  1. Baseline evaluation: Run zero-shot evaluation with frozen CheXzero (ViT-B/32 + GPT2) on the test set to establish a performance baseline.
  2. Fine-tuning impact: Fine-tune CheXzero on the training data and re-evaluate zero-shot performance to measure the improvement from domain adaptation.
  3. Template comparison: Implement both class-independent and class-dependent templates for CheXzero and compare their performance to quantify the benefit of the more targeted prompting strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between pre-training on large chest X-ray datasets versus fine-tuning on specific CT scan data for achieving maximum performance in fine-grained multi-label classification tasks?
- Basis in paper: [explicit] The paper found that CheXzero, pre-trained on large chest X-ray datasets, significantly outperformed frozen models and achieved the best results (macro F1 0.71) when fine-tuned on their CT data, but questions remain about the optimal approach.
- Why unresolved: While the paper demonstrates that pre-training helps, it doesn't explore the full spectrum of pre-training approaches or systematically compare different amounts of pre-training data versus fine-tuning strategies.
- What evidence would resolve it: A systematic study varying pre-training dataset sizes, architectures, and fine-tuning strategies on the same CT dataset would clarify the optimal balance.

### Open Question 2
- Question: How do class-dependent templates perform compared to other prompt engineering techniques (such as learned prompts or dynamic templates) in zero-shot medical image classification?
- Basis in paper: [explicit] The paper showed that class-dependent templates significantly improved performance (macro F1 0.71 vs 0.55 for CheXzero), but didn't compare to other prompt engineering approaches.
- Why unresolved: The paper only compared class-dependent versus class-independent templates, leaving open the question of whether even better results could be achieved with alternative prompt engineering methods.
- What evidence would resolve it: Direct comparison of class-dependent templates against learned prompts, dynamic templates, and other prompt engineering techniques on the same dataset would resolve this question.

### Open Question 3
- Question: What is the impact of context length and truncation strategy on performance when dealing with uncurated medical reports, and how does this vary across different medical specialties?
- Basis in paper: [explicit] The paper found that truncation from the right with shorter context length (100) performed best for their radiology reports, but acknowledged this might vary with different report structures.
- Why unresolved: The study only tested two context lengths (100 and 200) and two truncation strategies on radiology reports, without exploring the full parameter space or other medical specialties.
- What evidence would resolve it: Comprehensive ablation studies testing various context lengths and truncation strategies across multiple medical specialties would clarify the optimal approach for different report types.

## Limitations

- The study is based on a relatively small dataset (460 patient studies) from a single institution, which may limit generalizability to other populations or clinical settings.
- The zero-shot classification performance, while showing improvement over frozen models, still has significant room for improvement with a subset accuracy of only 23%.
- The manual design of class-dependent templates introduces potential human bias and may not scale well to larger label spaces.
- The translation of German reports to English could introduce semantic errors that affect the cross-modal alignment.
- The study focuses on CT scans but uses models pre-trained on chest X-rays, which may not fully capture the domain shift between these imaging modalities.

## Confidence

- High confidence: The core finding that fine-tuning pre-trained CLIP models significantly improves zero-shot performance on medical imaging tasks is well-supported by the ablation study results showing improvements from 0% to 23% subset accuracy for CheXzero.
- Medium confidence: The superiority of class-dependent templates over class-independent ones is demonstrated, but the manual nature of template design and lack of systematic evaluation of template quality introduces some uncertainty about the robustness of this approach.
- Medium confidence: The conclusion that RadBERT significantly improves text representations for medical reports is supported by the experimental setup, but the specific contribution of RadBERT versus the vision encoder's adaptation is not fully isolated.

## Next Checks

1. **Dataset generalization test**: Evaluate the best-performing model (CheXzero with class-dependent templates) on an external, multi-institutional dataset to assess real-world robustness and identify potential overfitting to the source institution's reporting style.
2. **Modality alignment validation**: Conduct a controlled experiment comparing performance when using CT-specific pre-trained models versus chest X-ray pre-trained models to quantify the impact of domain shift between imaging modalities.
3. **Template design reproducibility**: Have independent annotators design class-dependent templates for the same dataset and compare performance to the original templates to assess the reproducibility and objectivity of the manual template design process.