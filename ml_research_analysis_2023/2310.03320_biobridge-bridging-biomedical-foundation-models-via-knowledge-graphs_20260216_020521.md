---
ver: rpa2
title: 'BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs'
arxiv_id: '2310.03320'
source_url: https://arxiv.org/abs/2310.03320
tags:
- protein
- drug
- biobridge
- disease
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BioBridge bridges unimodal biomedical foundation models (FMs) by
  learning cross-modal transformations from knowledge graphs, keeping base FMs frozen
  for parameter efficiency. It outperforms knowledge graph embedding baselines by
  ~76.3% on cross-modal retrieval tasks and shows strong out-of-domain generalization.
---

# BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs

## Quick Facts
- arXiv ID: 2310.03320
- Source URL: https://arxiv.org/abs/2310.03320
- Reference count: 28
- Primary result: BioBridge outperforms KGE baselines by ~76.3% on cross-modal retrieval tasks while keeping base FMs frozen.

## Executive Summary
BioBridge bridges biomedical foundation models across different modalities by learning cross-modal transformations conditioned on knowledge graph relations, without fine-tuning the base models. The approach uses a small transformer bridge module to transform embeddings from one modality to another using InfoNCE loss on KG triples. This enables cross-modal retrieval, semantic similarity inference, and multimodal generation tasks while maintaining parameter efficiency. BioBridge demonstrates strong out-of-domain generalization capabilities, successfully bridging to unseen entities and relations.

## Method Summary
BioBridge keeps unimodal foundation models frozen and learns a bridge module to transform embeddings between modalities using knowledge graph supervision. The bridge module is a small transformer that takes projected embeddings from source modality FMs and applies relation-aware transformations to align them with target modality space. Training uses InfoNCE loss comparing transformed head embeddings to positive tail embeddings while pushing apart negative samples. The method supports multiple modalities including proteins (ESM2-3B), molecules (UniMol), and text (PubMedBERT) connected through a biomedical knowledge graph.

## Key Results
- Achieves ~76.3% relative improvement over knowledge graph embedding baselines on cross-modal retrieval tasks
- Demonstrates out-of-domain generalization by successfully bridging mouse proteins to human phenotypes
- Supports multimodal question answering and guided drug generation with retrieved evidence

## Why This Works (Mechanism)

### Mechanism 1
- BioBridge learns additive transformations conditioned on relation types to align embeddings across modalities without fine-tuning base FMs. The bridge module takes projected head embeddings and applies relation-aware transformations using a small transformer network, supervised by InfoNCE loss.

### Mechanism 2
- BioBridge generalizes to out-of-domain entities by leveraging semantic alignment between related spaces learned during training. For example, protein-to-disease transformation generalizes to protein-to-phenotype retrieval through semantic similarity between diseases and phenotypes in medical ontologies.

### Mechanism 3
- BioBridge maintains parameter efficiency by keeping all base FMs frozen and only training a small bridge module. This avoids the exponential data and parameter growth required for joint fine-tuning of multimodal encoders for every modality pair.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: To align transformed head embeddings with positive tail embeddings while pushing them apart from negative samples, creating meaningful cross-modal similarity.
  - Quick check question: How does the InfoNCE loss function ensure that the transformed embeddings are close to positive samples and far from negatives in the target modality space?

- Concept: Knowledge graph embedding and relation modeling
  - Why needed here: The KG provides structured supervision for learning transformations between modalities; relation types encode how entities in different modalities are connected.
  - Quick check question: Why is it important to include relation type embeddings as input to the bridge module, and what would happen if they were omitted?

- Concept: Inductive vs transductive learning on graphs
  - Why needed here: BioBridge must handle new nodes (proteins, drugs, diseases) not seen during training, unlike traditional KG embedding methods that assume a fixed node set.
  - Quick check question: How does encoding new nodes with pre-trained FMs instead of relying on neighbor aggregation enable inductive prediction for isolated nodes?

## Architecture Onboarding

- Component map: Modality FMs (ESM2-3B, UniMol, PubMedBERT) -> Projection heads -> Bridge module (transformer ψ) -> Transformed embeddings -> Similarity search
- Critical path: 1) Encode head entity with source FM → zi, 2) Encode relation and modalities → ci, cj, rij, 3) Bridge module transforms zi → ˆhi, 4) Compute similarity between ˆhi and candidate tail embeddings, 5) Optimize with InfoNCE loss
- Design tradeoffs: Freezing FMs vs joint fine-tuning (lower compute but limited by FM expressivity), Transformer vs simpler transformation (more expressive but higher parameters), InfoNCE vs other losses (strong signal but sensitive to negative sampling)
- Failure signatures: Poor retrieval performance (check FM quality, bridge capacity, KG coverage), Mode collapse (inspect InfoNCE loss and negative sampling), Overfitting to training KG (evaluate on out-of-domain entities/relations)
- First 3 experiments: 1) Ablation: Remove relation embeddings from bridge input and measure drop in retrieval accuracy, 2) Scalability: Vary number of modalities and relations in KG and measure training/inference time, 3) Out-of-domain: Test on mouse protein-phenotype matching to confirm generalization capability

## Open Questions the Paper Calls Out

- Question: How does BioBridge's performance scale with increasing numbers of modalities beyond the six tested?
  - Basis in paper: The paper notes that extending multimodal contrastive learning beyond two modalities is challenging due to data scarcity, but does not empirically test BioBridge's scalability.
  - Why unresolved: Current experiments only test six modalities; no theoretical or empirical analysis is provided for how the approach scales with additional modalities.
  - What evidence would resolve it: Experiments testing BioBridge with additional biomedical modalities and analysis of how performance changes with modality count.

- Question: What is the theoretical limit of BioBridge's ability to generalize to unseen relations?
  - Basis in paper: The paper states that BioBridge "generalizes to relationships that do not exist in the training KG" but does not quantify the limit of this generalization.
  - Why unresolved: The protein-protein interaction experiment only tests one out-of-domain relation type; the paper does not analyze how far BioBridge can extrapolate to completely novel relation types.
  - What evidence would resolve it: Systematic experiments testing BioBridge on progressively more distant relation types from the training data, or theoretical analysis of the transformation model's capacity for relation generalization.

- Question: How sensitive is BioBridge's performance to the choice of base foundation models?
  - Basis in paper: The paper uses specific foundation models (ESM2-3B, UniMol, PubMedBERT) but does not analyze how performance changes with different model choices or architectures.
  - Why unresolved: The paper demonstrates that BioBridge works with these specific models, but does not establish whether the approach is dependent on particular model architectures or training objectives.
  - What evidence would resolve it: Experiments comparing BioBridge performance when using different foundation models for the same modalities, or analysis of which model properties are most critical.

## Limitations
- Limited ablation studies on bridge module architecture and relation embedding impact
- Cross-species generalization assumption not rigorously validated across diverse species or ontologies
- Computational efficiency claims lack explicit comparisons with joint fine-tuning baselines

## Confidence
- High confidence: Cross-modal retrieval performance improvements over KGE baselines (~76.3% relative gain)
- Medium confidence: Out-of-domain generalization to mouse proteins and guided drug generation with retrieved evidence
- Low confidence: Computational efficiency claims without joint fine-tuning baselines for comparison

## Next Checks
1. Perform ablation study by removing relation embeddings from bridge module input and measuring impact on cross-modal retrieval performance
2. Test BioBridge on protein-phenotype matching across multiple species (rat, zebrafish) to confirm generality of semantic alignment hypothesis
3. Compare training/inference time and parameter counts of BioBridge against joint fine-tuning baseline to validate parameter efficiency claims