---
ver: rpa2
title: Removing Spurious Concepts from Neural Network Representations via Joint Subspace
  Estimation
arxiv_id: '2310.11991'
source_url: https://arxiv.org/abs/2310.11991
tags:
- spurious
- main-task
- concept
- dataset
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Joint Subspace Estimation (JSE), an algorithm
  for removing spurious concepts from neural network embeddings to improve out-of-distribution
  generalization. The key idea is to iteratively estimate two orthogonal low-dimensional
  subspaces: one for the spurious concept and one for the main task, preventing loss
  of main-task features.'
---

# Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation

## Quick Facts
- **arXiv ID**: 2310.11991
- **Source URL**: https://arxiv.org/abs/2310.11991
- **Reference count**: 40
- **Primary result**: JSE outperforms existing concept-removal methods on benchmarks for computer vision (Waterbirds, CelebA) and NLP (MultiNLI), improving OOD generalization.

## Executive Summary
This paper proposes Joint Subspace Estimation (JSE), an iterative algorithm for removing spurious concepts from neural network embeddings to improve out-of-distribution (OOD) generalization. The key innovation is jointly estimating orthogonal low-dimensional subspaces for spurious and main-task concepts, preventing loss of main-task features during spurious concept removal. JSE uses statistical tests to determine when to stop adding concept vectors, avoiding over-removal. The method is evaluated on four benchmark datasets and demonstrates superior performance compared to existing concept-removal approaches, particularly when spurious correlations are strong.

## Method Summary
JSE iteratively estimates two orthogonal low-dimensional subspaces in neural network representations - one for the spurious concept and one for the main task. The algorithm alternates between estimating basis vectors for each subspace while enforcing orthogonality constraints, using logistic regression classifiers with orthogonality penalties. Statistical tests based on binary cross-entropy differences determine when to stop adding concept vectors to each subspace. The method projects out only the spurious subspace from embeddings, preserving main-task information for OOD classification. JSE is compared against ERM, INLP, RLACE, and adversarial removal methods across multiple datasets with varying spurious correlation strengths.

## Key Results
- JSE achieves higher worst-group accuracy than existing methods on Waterbirds, CelebA, and MultiNLI datasets
- Performance advantage is particularly pronounced when spurious correlation is strong (ρ > 0.7)
- JSE maintains main-task accuracy while removing spurious correlations, unlike methods that degrade overall performance
- The method successfully handles both image and text datasets, demonstrating broad applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JSE separates spurious and main-task concepts by jointly estimating two orthogonal subspaces.
- Mechanism: It iteratively estimates basis vectors for each subspace while enforcing orthogonality, preventing the spurious subspace from absorbing main-task features.
- Core assumption: The spurious and main-task feature subspaces are linearly orthogonal in the embedding space.
- Evidence anchors:
  - [abstract] "We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation."
  - [section 2] "Orthogonality Assumption. The linear subspaces Zsp and Zmt are orthogonal, i.e. each vector vsp ∈ Zsp is perpendicular to each vector vmt ∈ Zmt."
  - [corpus] Weak. No direct corpus support found for orthogonality of spurious/main-task subspaces.
- Break condition: If the orthogonality assumption fails, JSE may still find orthogonal subspaces that best fit the data but with reduced separation quality.

### Mechanism 2
- Claim: JSE uses statistical tests to decide when to stop adding concept vectors, preventing over-removal of main-task features.
- Mechanism: Two statistical tests compare BCE differences between concept predictors and random classifiers, and between spurious and main-task predictors.
- Core assumption: Differences in BCE can reliably indicate whether a direction is spurious or main-task.
- Evidence anchors:
  - [section 3.3] "To make these criteria operational, we introduce two statistical tests in terms of differences between BCE's."
  - [section D.3] "We can use the following test statistic, where under the null hypothesis...tw = ¯dw − E[dw] / dVar(¯dw) → N(0, 1)."
  - [corpus] Weak. No direct corpus support found for using BCE differences as stopping criteria.
- Break condition: If the assumptions about BCE differences break down, the stopping criteria may misidentify concept directions.

### Mechanism 3
- Claim: JSE improves OOD generalization by removing spurious features while preserving main-task features.
- Mechanism: By projecting out only the spurious subspace, the remaining embeddings retain main-task information for accurate OOD classification.
- Core assumption: Removing spurious features while preserving main-task features improves OOD generalization.
- Evidence anchors:
  - [abstract] "show that it outperforms existing concept removal methods"
  - [section 4] "JSE outperforms the other methods, in particular when the spurious correlation is strong."
  - [corpus] Weak. No direct corpus support found for this specific claim about JSE's OOD improvement.
- Break condition: If spurious and main-task features are not well-separated, removing spurious features may still harm main-task performance.

## Foundational Learning

- Concept: Linear subspace hypothesis
  - Why needed here: JSE assumes that spurious and main-task concepts can be represented as linear subspaces in the embedding space.
  - Quick check question: What would happen to JSE if the concept subspaces were highly non-linear?

- Concept: Binary cross-entropy (BCE)
  - Why needed here: BCE is used to measure how well projected embeddings predict concept labels, forming the basis of JSE's statistical tests.
  - Quick check question: How does BCE differ from other classification loss functions like cross-entropy?

- Concept: Orthogonality in vector spaces
  - Why needed here: JSE enforces orthogonality between spurious and main-task subspaces to prevent feature mixing.
  - Quick check question: What mathematical property ensures that two vectors are orthogonal?

## Architecture Onboarding

- Component map: Logistic regression classifiers for spurious and main-task concepts -> Statistical tests for stopping criteria -> Projection operations to remove identified subspaces -> Main-task classifier training on transformed embeddings
- Critical path: Estimate concept vectors → Apply statistical tests → Project out spurious subspace → Train main-task classifier on transformed embeddings
- Design tradeoffs: JSE trades computational complexity for better separation of spurious and main-task features compared to simpler methods like INLP
- Failure signatures: If JSE fails to separate concepts, worst-group accuracy will degrade similarly to other methods; if statistical tests fail, JSE may remove too many or too few concept vectors
- First 3 experiments:
  1. Apply JSE to the Toy dataset with varying spurious correlation strengths and compare worst-group accuracy to INLP.
  2. Test JSE on Waterbirds with p(ymt=y|ysp=y) ranging from 0.5 to 0.9 and measure worst-group accuracy.
  3. Evaluate JSE on MultiNLI and compare performance degradation as spurious correlation increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does JSE perform when the orthogonality assumption between spurious and main-task subspaces does not hold in realistic settings?
- Basis in paper: [explicit] The paper mentions that the orthogonality assumption is sometimes violated and provides an analysis on the Toy dataset where the angle between spurious and main-task vectors is 75 degrees.
- Why unresolved: The paper only provides limited empirical evidence on a simplified Toy dataset. Real-world datasets may have more complex relationships between spurious and main-task concepts that could violate orthogonality more severely.
- What evidence would resolve it: Experiments applying JSE to real-world datasets where spurious and main-task concepts are known to be non-orthogonal, measuring performance degradation compared to cases where orthogonality holds.

### Open Question 2
- Question: Can JSE be extended to handle non-linear relationships between concepts in the embedding space?
- Basis in paper: [inferred] The paper mentions that future work should consider developing tests for the linearity and orthogonality assumptions, or see whether they can be relaxed. It also references related work on non-linear concept removal.
- Why unresolved: The current JSE method relies on linear subspaces and orthogonal projections, which may not capture complex non-linear relationships between concepts in neural network embeddings.
- What evidence would resolve it: Development and empirical testing of a non-linear extension of JSE, comparing its performance to the linear version on datasets with known non-linear concept relationships.

### Open Question 3
- Question: How sensitive is JSE to the choice of significance level (α) and the parameter ∆ used in statistical tests?
- Basis in paper: [explicit] The paper mentions that α = 0.05 and ∆ = 0 are used as default values, but provides a heuristic for setting ∆ in cases where one set of labels is harder to predict than the other.
- Why unresolved: The paper does not provide a systematic study of how different values of α and ∆ affect JSE performance across various datasets and spurious correlation strengths.
- What evidence would resolve it: A comprehensive sensitivity analysis varying α and ∆ values, measuring their impact on JSE performance metrics like accuracy and worst-group accuracy across multiple datasets with different characteristics.

### Open Question 4
- Question: How does JSE compare to other concept-removal methods when the spurious correlation strength varies within the same dataset?
- Basis in paper: [inferred] The paper shows that JSE outperforms other methods for strong spurious correlations, but does not explore scenarios where correlation strength varies within a single dataset.
- Why unresolved: Real-world datasets may contain regions with different levels of spurious correlation, which could affect the relative performance of JSE compared to other methods.
- What evidence would resolve it: Experiments on datasets with heterogeneous spurious correlation strengths, analyzing how JSE's performance changes across different regions and comparing it to other methods' performance in those same regions.

## Limitations
- JSE relies on the linear subspace hypothesis, which may not hold for complex real-world concept relationships
- The method's performance depends on accurate statistical tests for stopping criteria, which lack direct corpus validation
- Computational complexity is higher than simpler methods due to nested loops and iterative subspace estimation

## Confidence
- Mechanism 1: Medium - Clear description but weak corpus validation of orthogonality assumption
- Mechanism 2: Medium - Statistical test design is described but BCE difference reliability is unproven
- Mechanism 3: Medium - Experimental results support OOD improvement claims but theoretical proof is lacking

## Next Checks
1. Test JSE's sensitivity to initialization by running multiple trials with different random seeds and measuring variance in worst-group accuracy.
2. Evaluate JSE on datasets where spurious and main-task concepts are known to be non-linearly separable to test the linear subspace hypothesis.
3. Compare JSE's performance against a simplified version that uses a fixed number of concept vectors (rather than statistical stopping criteria) to isolate the impact of the statistical tests.