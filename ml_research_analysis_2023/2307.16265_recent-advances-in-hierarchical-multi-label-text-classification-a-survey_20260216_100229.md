---
ver: rpa2
title: 'Recent Advances in Hierarchical Multi-label Text Classification: A Survey'
arxiv_id: '2307.16265'
source_url: https://arxiv.org/abs/2307.16265
tags:
- classi
- cation
- label
- hierarchical
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews recent advances in hierarchical
  multi-label text classification (HMTC), covering 13 datasets, four main approaches
  (tree-based, embedding-based, graph-based, and ensemble), various learning strategies,
  evaluation metrics, and current challenges. The key methods include probabilistic
  label trees (Parabel), hyperbolic embedding (HyperIM), hierarchical transfer learning
  (HTrans), and graph neural networks (HiAGM).
---

# Recent Advances in Hierarchical Multi-label Text Classification: A Survey

## Quick Facts
- arXiv ID: 2307.16265
- Source URL: https://arxiv.org/abs/2307.16265
- Reference count: 40
- Comprehensive survey of hierarchical multi-label text classification approaches

## Executive Summary
This survey comprehensively reviews recent advances in hierarchical multi-label text classification (HMTC), covering 13 datasets, four main approaches (tree-based, embedding-based, graph-based, and ensemble), various learning strategies, evaluation metrics, and current challenges. The key methods include probabilistic label trees (Parabel), hyperbolic embedding (HyperIM), hierarchical transfer learning (HTrans), and graph neural networks (HiAGM). Major challenges identified include label sparsity and imbalance, low-resource labeled data, lower accuracy in deeper label levels, and extreme multi-label problems.

## Method Summary
The survey systematically categorizes HMTC approaches into four main categories. Tree-based methods use probabilistic label trees to partition labels hierarchically, reducing search space and enabling independent training of binary classifiers. Embedding-based approaches learn label representations in hyperbolic space or through transfer learning. Graph-based methods model documents and labels as graphs, applying GNNs for joint representation learning. Ensemble methods combine multiple models trained on different label hierarchy levels. The survey evaluates these approaches across 13 benchmark datasets including Pubmed, WOS, and Amazon 3M.

## Key Results
- Graph-based approaches are currently mainstream in HMTC research
- Ensemble methods show strong performance in practice
- Major challenges include label sparsity/imbalance and poor performance on deep label levels
- Four main approach categories: tree-based, embedding-based, graph-based, and ensemble

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical label structure enables better generalization through shared representation across label levels
- Mechanism: The tree-based approach partitions labels into a probabilistic label tree (PLT), where each internal node represents a pseudo-label (meta-label) and each leaf corresponds to an original label. This hierarchical decomposition reduces the search space and allows independent training of binary classifiers for each node, improving scalability.
- Core assumption: The hierarchical relationships between labels are meaningful and can be exploited for more efficient learning.
- Evidence anchors:
  - [section 3.1]: "This method is based on probabilistic label tree, which was originally developed for extreme multi-label classification, where a probabilistic label tree (PLT) is used to partition labels, where each leaf in PLT corresponds to an original label and each internal node corresponds to a pseudo-label (meta-label)."
  - [section 3.1]: "Parabel [17] is a traditional label tree-based method using bag-of-words (BOW) features."
  - [corpus]: Weak evidence - corpus shows similar hierarchical approaches but doesn't directly confirm PLT effectiveness
- Break condition: If the label hierarchy is artificial or doesn't reflect real semantic relationships, the tree structure becomes meaningless and the model performance degrades.

### Mechanism 2
- Claim: Graph neural networks can capture complex dependencies between labels and documents more effectively than flat classifiers
- Mechanism: The graph-based approach models the entire document and hierarchical labels as a single graph, where nodes can represent tokens, labels, sentences, topics, or metadata. A graph neural network is then applied to this graph, converting the hierarchical multi-label classification task into a node classification problem.
- Core assumption: The label hierarchy and document content can be meaningfully represented as a graph structure where relationships are captured through message passing.
- Evidence anchors:
  - [section 3.3]: "Graph based approaches regard the whole document and hierarchical labels as a single graph, where the node type may include token, label, sentence, topic or even some meta information."
  - [section 3.3]: "HiAGM [35], improved with a bidirectional computing structure encoder, enhances its ability to handle sparse data."
  - [corpus]: Moderate evidence - related papers show graph-based methods are gaining traction but don't provide direct validation
- Break condition: When the graph becomes too large or dense, the computational cost of message passing may outweigh the benefits, especially for shallow hierarchies.

### Mechanism 3
- Claim: Ensemble methods improve robustness by combining predictions from multiple models trained on different aspects of the label hierarchy
- Mechanism: Multiple models are trained for different levels of the label hierarchy, which can be conducted in parallel. During prediction, a voting mechanism is applied followed by cross-checking among predicted labels to ensure consistency with the hierarchical constraints.
- Core assumption: Different models capture complementary aspects of the hierarchical structure, and their combination provides more robust predictions than any single model.
- Evidence anchors:
  - [section 3.4]: "In terms of hierarchical multi-label classification, several models can be trained for each of the levels of the labels, which can be conducted in parallel. In the prediction time, a voting mechanism can be conducted, followed by further cross checking among the predicted labels."
  - [section 3.4]: "Another boosting ensemble method is derived from Devil's advocate [10]. This method requires at least three models to be integrated, and a new loss which uses the idea of generated confrontation to make the target model finally converge is designed."
  - [corpus]: Strong evidence - corpus shows ensemble methods are used in industrial settings and achieve strong performance
- Break condition: If the individual models are highly correlated or make similar errors, the ensemble provides little improvement and may just increase computational cost.

## Foundational Learning

- Concept: Hierarchical data structures and tree traversal algorithms
  - Why needed here: Understanding how label hierarchies are structured and traversed is fundamental to implementing tree-based approaches like Parabel and probabilistic label trees
  - Quick check question: What is the time complexity of traversing a balanced binary tree with n nodes?

- Concept: Graph neural networks and message passing
  - Why needed here: Graph-based approaches rely on GNNs to propagate information through the document-label graph structure, requiring understanding of how node features are updated through neighborhood aggregation
  - Quick check question: In a graph with N nodes, what is the computational complexity of one layer of message passing in a GNN?

- Concept: Multi-label classification metrics and evaluation
  - Why needed here: The survey discusses various evaluation metrics (precision@k, mean reciprocal rank, F1-score) that are specific to multi-label and hierarchical settings, requiring understanding of how to properly evaluate these systems
  - Quick check question: How does Hamming loss differ from subset accuracy in multi-label classification?

## Architecture Onboarding

- Component map: Input text → Encoder (BERT/Transformer) → Label structure encoder (tree/GNN) → Prediction layer → Output (hierarchical labels)
- Critical path: Text encoding → Label hierarchy processing → Joint representation learning → Prediction
- Design tradeoffs: Tree-based (fast, interpretable) vs Graph-based (flexible, captures complex dependencies) vs Ensemble (robust but expensive)
- Failure signatures: Poor performance on deep label levels, over-reliance on high-frequency labels, inability to handle label sparsity
- First 3 experiments:
  1. Implement a simple tree-based approach using Parabel on a small hierarchical dataset to validate basic functionality
  2. Compare performance of tree-based vs graph-based approaches on a dataset with complex label dependencies
  3. Implement an ensemble of two different approaches (e.g., tree-based + graph-based) to measure performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can instruction learning be effectively leveraged for hierarchical multi-label text classification, given its demonstrated strong performance in other NLP tasks?
- Basis in paper: [explicit] The authors suggest that "Designing appropriate prompt for hierarchical multi-label classifiers can be regarded as a weakly supervised learning problem."
- Why unresolved: The paper mentions instruction learning as a potential future direction but does not provide concrete methods or empirical results for applying prompt-based approaches to HMTC.
- What evidence would resolve it: Experimental results comparing prompt-based HMTC models against traditional methods, along with ablation studies on different prompt designs.

### Open Question 2
- Question: What are the most effective techniques for addressing label imbalance and sparsity in hierarchical multi-label text classification?
- Basis in paper: [explicit] The authors identify "label sparsity and imbalance" as a major challenge, noting that "The model can easily over-fit the high-frequency labels and under-fit the low frequency ones."
- Why unresolved: While the paper mentions some existing approaches like balancing loss functions and adaptive sampling, it acknowledges that these methods still struggle with bias issues and lack of overall information.
- What evidence would resolve it: Comparative studies of different imbalance handling techniques on diverse HMTC datasets, showing both quantitative improvements and qualitative analysis of learned label representations.

### Open Question 3
- Question: Can large language models like ChatGPT and GPT-4 effectively perform zero-shot hierarchical multi-label classification without any fine-tuning?
- Basis in paper: [explicit] The authors pose this as a question: "As recent big language models like ChatGPT and GPT4 are released, the ability of zero-shot learning of these language models remains as question."
- Why unresolved: The paper identifies this as an open question without providing experimental results or theoretical analysis of how these models might handle the hierarchical and multi-label aspects of the task.
- What evidence would resolve it: Systematic evaluation of zero-shot performance across multiple LLMs on various HMTC datasets, including analysis of their ability to understand hierarchical relationships and generate multiple relevant labels.

## Limitations
- Limited empirical comparison across different approach types, making it difficult to definitively determine best-performing methods
- Most studies focus on performance metrics without thoroughly examining computational efficiency or scalability to extreme label spaces
- Existing methods struggle particularly with deep label hierarchies (>3-4 levels) and rare labels

## Confidence
- High Confidence: The characterization of current challenges (label sparsity, imbalance, low-resource settings) is well-supported by the literature review
- Medium Confidence: The effectiveness of graph-based approaches as the "mainstream" method is supported by recent trends but lacks comprehensive comparative studies
- Low Confidence: Specific performance rankings between tree-based, embedding-based, graph-based, and ensemble approaches are not definitively established

## Next Checks
1. Conduct controlled experiments comparing all four approach types (tree, embedding, graph, ensemble) on identical datasets with standardized evaluation protocols to establish definitive performance rankings
2. Design benchmark datasets specifically targeting extreme multi-label scenarios (>100,000 labels) and deep hierarchies (>5 levels) to stress-test current approaches
3. Implement and evaluate the proposed future directions (instruction learning, large language models for zero-shot learning, incremental learning) to verify their practical feasibility and performance improvements over current methods