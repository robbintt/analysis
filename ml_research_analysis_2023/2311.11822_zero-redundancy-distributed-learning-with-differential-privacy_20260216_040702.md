---
ver: rpa2
title: Zero redundancy distributed learning with differential privacy
arxiv_id: '2311.11822'
source_url: https://arxiv.org/abs/2311.11822
tags:
- training
- gradient
- parameters
- zero
- dp-zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents DP-ZeRO, a novel method that combines differential
  privacy (DP) with the Zero Redundancy Optimizer (ZeRO) to enable efficient training
  of large-scale deep learning models with privacy guarantees. The key contributions
  are: DP-ZeRO enables training models with over 1 billion trainable parameters, including
  GPT-100B, for the first time with DP.'
---

# Zero redundancy distributed learning with differential privacy

## Quick Facts
- arXiv ID: 2311.11822
- Source URL: https://arxiv.org/abs/2311.11822
- Reference count: 32
- Primary result: DP-ZeRO enables efficient training of large-scale deep learning models with differential privacy by combining per-sample gradient clipping with ZeRO's state partitioning

## Executive Summary
This paper presents DP-ZeRO, a novel method that combines differential privacy (DP) with the Zero Redundancy Optimizer (ZeRO) to enable efficient training of large-scale deep learning models with privacy guarantees. The key contributions are: (1) enabling training models with over 1 billion trainable parameters for the first time with DP, (2) achieving the same computation and communication efficiency as standard ZeRO, and (3) supporting mixed-precision training by addressing loss scaling issues in DP optimization. The paper demonstrates the generality and efficiency of DP-ZeRO through extensive experiments on various model architectures across different datasets and tasks.

## Method Summary
DP-ZeRO integrates differential privacy into the ZeRO distributed training framework by introducing per-sample gradient clipping and noise addition to the back-propagation step while preserving all other distributed training operations. The method maintains the same computation and communication efficiency as standard ZeRO by isolating DP modifications to back-propagation only. DP-ZeRO supports all three stages of ZeRO (ZeRO1/2/3) and enables mixed-precision training by avoiding loss scaling, which would otherwise cause overflow issues in DP contexts. The system is designed to be compatible with popular distributed learning frameworks like DeepSpeed and FSDP.

## Key Results
- DP-ZeRO enables training models with over 1 billion trainable parameters, including GPT-100B, for the first time with DP
- DP-ZeRO achieves the same computation and communication efficiency as standard ZeRO, allowing scalable training on hundreds of GPUs
- DP-ZeRO supports mixed-precision training by addressing the loss scaling issue in DP optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DP-ZeRO achieves the same computation and communication efficiency as standard ZeRO by modifying only the back-propagation step while preserving all other distributed training operations.
- **Mechanism**: The back-propagation is the only part where DP introduces additional overhead through per-sample gradient clipping and noise addition. All other operations (forward pass, parameter updates, and communication patterns) remain identical to standard ZeRO. By isolating DP modifications to back-propagation, the system avoids introducing inefficiencies elsewhere.
- **Core assumption**: The time complexity of back-propagation with DP modifications remains comparable to standard back-propagation when using efficient per-sample gradient clipping techniques.
- **Evidence anchors**:
  - [abstract]: "DP-ZeRO achieves the same computation and communication efficiency as standard ZeRO, allowing scalable training on hundreds of GPUs."
  - [section 3.2]: "We claim that DP-ZeRO could enjoy high efficiency on-par with the standard ZeRO when (I) DP back-propagation exhibits a time efficiency comparable to the standard..."
  - [corpus]: Weak - no direct corpus evidence comparing DP-ZeRO back-propagation efficiency to standard back-propagation.

### Mechanism 2
- **Claim**: Mixed-precision training works with DP by avoiding loss scaling, which prevents overflow issues that would otherwise occur.
- **Mechanism**: Standard mixed-precision training uses loss scaling to prevent underflow, but this causes overflow in DP training because the per-sample gradient clipping already acts as a scaling mechanism. By disabling loss scaling in DP training, the system avoids both underflow and overflow issues.
- **Core assumption**: The per-sample gradient clipping factor inherently provides sufficient scaling protection for mixed-precision training.
- **Evidence anchors**:
  - [section 3.4]: "We now analyze the intricacy in mixed-precision training with DP, which is not unique to DP-ZeRO but present in the general DP optimization. We emphasize that the per-sample gradient clipping already plays the role of scaling, and hence DP mixed-precision training must not use loss scaling..."
  - [section 3.4]: "Therefore, we propose to not use loss scaling (or equivalently we set S = 1 statically for all steps) during DP mixed-precision training..."
  - [corpus]: Weak - no corpus evidence directly comparing mixed-precision training with and without loss scaling in DP contexts.

### Mechanism 3
- **Claim**: DP-ZeRO scales to models with over 1 billion parameters by partitioning model states across GPUs while maintaining DP guarantees.
- **Mechanism**: By using ZeRO's state partitioning (ZeRO1/2/3), the memory burden is distributed across multiple GPUs. Each GPU only stores a shard of the model states (parameters, gradients, optimizer states), and these are gathered/reduced just-in-time during computation. This enables training models far larger than what fits in a single GPU's memory while maintaining DP guarantees through the clipping and noise addition process.
- **Core assumption**: The communication overhead from gathering/sharding states does not negate the memory efficiency gains.
- **Evidence anchors**:
  - [abstract]: "This paper presents DP-ZeRO, a novel method that combines differential privacy (DP) with the Zero Redundancy Optimizer (ZeRO) to enable efficient training of large-scale deep learning models with privacy guarantees."
  - [section 2.2.2]: "ZeRO has three stages (ZeRO1/2/3) that partition these model states by different levels, with lower level being faster but more memory costly."
  - [corpus]: Weak - no corpus evidence showing DP-ZeRO specifically enabling training of >1B parameter models.

## Foundational Learning

- **Concept**: Differential Privacy and gradient clipping
  - Why needed here: DP-ZeRO relies on differential privacy to provide privacy guarantees during distributed training. Understanding how per-sample gradient clipping and noise addition work is essential to grasp why DP-ZeRO modifies only the back-propagation step.
  - Quick check question: What is the purpose of per-sample gradient clipping in differential privacy, and how does it differ from standard gradient clipping?

- **Concept**: Zero Redundancy Optimizer (ZeRO) and state partitioning
  - Why needed here: DP-ZeRO builds upon ZeRO's state partitioning strategy to distribute memory and communication load across GPUs. Understanding ZeRO's three stages and how they partition parameters, gradients, and optimizer states is crucial to understanding DP-ZeRO's scalability.
  - Quick check question: How does ZeRO1 differ from ZeRO2 and ZeRO3 in terms of what model states are partitioned across GPUs?

- **Concept**: Mixed-precision training and loss scaling
  - Why needed here: DP-ZeRO enables mixed-precision training by avoiding loss scaling, which would otherwise cause overflow issues in DP contexts. Understanding how mixed-precision training works and why loss scaling is typically needed is essential to grasp this design choice.
  - Quick check question: Why is loss scaling typically used in mixed-precision training, and what problem does it solve?

## Architecture Onboarding

- **Component map**: DP-ZeRO core -> Per-sample gradient clipping module -> Noise addition module -> ZeRO state partitioning -> Communication layer

- **Critical path**: Forward pass (identical to standard ZeRO) -> Output gradient computation (identical to standard ZeRO) -> Per-sample gradient clipping and noise addition (DP-specific) -> Parameter update (identical to standard ZeRO with DP gradients) -> Communication for state partitioning (ZeRO-specific)

- **Design tradeoffs**:
  - Memory vs. Communication: Higher ZeRO stages (2/3) reduce memory usage but increase communication overhead
  - Precision vs. Stability: Mixed-precision training without loss scaling reduces memory but may introduce underflow risks
  - Privacy vs. Utility: Stronger privacy (lower epsilon) requires more noise, potentially degrading model performance

- **Failure signatures**:
  - Memory overflow: Model states exceed available GPU memory despite ZeRO partitioning
  - Communication bottleneck: All-gather operations become the training bottleneck
  - Training instability: Loss becomes NaN or diverges due to numerical issues
  - Privacy degradation: Epsilon grows too large, violating privacy requirements

- **First 3 experiments**:
  1. Train a small ResNet on CIFAR-10 with DP-ZeRO1 to verify basic functionality and DP guarantees
  2. Compare training speed and memory usage of DP-ZeRO1 vs standard ZeRO on a medium-sized model
  3. Test mixed-precision training with DP-ZeRO on a small GPT model to verify no overflow issues occur

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DP-ZeRO scale when training models with sequence lengths beyond 2048 tokens, such as those used in long-context applications?
- Basis in paper: [explicit] The paper evaluates DP-ZeRO with a sequence length of 2048 in Figure 8, but does not explore longer sequences.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for sequence lengths beyond 2048 tokens, which are increasingly relevant for modern large language models.
- What evidence would resolve it: Experiments comparing DP-ZeRO's efficiency and memory usage for sequence lengths of 4096, 8192, and beyond, along with analysis of the impact on communication and computation overhead.

### Open Question 2
- Question: What is the impact of using different clipping functions (e.g., automatic clipping vs. vanilla clipping) on the final accuracy and convergence speed of DP-ZeRO across various model architectures?
- Basis in paper: [explicit] The paper mentions that different clipping functions are available but does not provide a comprehensive comparison of their effects on accuracy and convergence.
- Why unresolved: The paper focuses on the efficiency of DP-ZeRO but does not experimentally compare the impact of different clipping functions on model performance.
- What evidence would resolve it: A systematic study comparing the accuracy and convergence speed of DP-ZeRO using different clipping functions (e.g., automatic clipping, vanilla clipping, and others) across multiple model architectures and datasets.

### Open Question 3
- Question: How does the integration of DP-ZeRO with other optimization techniques, such as quantization or sparse training, affect its efficiency and scalability?
- Basis in paper: [inferred] The paper mentions that DP-ZeRO is orthogonal to techniques not tied to back-propagation, such as quantization, but does not provide experimental results or analysis of their combined impact.
- Why unresolved: The paper does not explore the potential synergies or trade-offs between DP-ZeRO and other optimization techniques that could further enhance its efficiency and scalability.
- What evidence would resolve it: Experiments evaluating the combined performance of DP-ZeRO with techniques like quantization, sparse training, or other memory-efficient methods, measuring their impact on speed, memory usage, and accuracy.

## Limitations

- The claim about training models "over 1 billion parameters" is stated but not demonstrated with concrete experimental results
- No empirical evidence comparing DP-ZeRO back-propagation efficiency to standard back-propagation is provided
- The mixed-precision training approach without loss scaling is described clearly but not extensively validated

## Confidence

- **High confidence**: The core mechanism of combining per-sample gradient clipping with ZeRO's state partitioning is well-founded and theoretically sound. The claim that DP modifications are isolated to back-propagation is supported by the mathematical formulation.
- **Medium confidence**: The efficiency claims regarding computation and communication parity with standard ZeRO are theoretically justified but lack direct empirical comparison in the paper. The mixed-precision training approach without loss scaling is described clearly but not extensively validated.
- **Low confidence**: The claim about training models "over 1 billion parameters" is stated but not demonstrated with concrete experimental results in the paper. No specific large-scale experiments are shown.

## Next Checks

1. Implement a direct timing comparison between DP-ZeRO back-propagation and standard ZeRO back-propagation on identical hardware to verify the claimed efficiency parity
2. Conduct mixed-precision training experiments with DP-ZeRO using various model sizes to empirically validate the no-loss-scaling approach and check for overflow/underflow issues
3. Scale DP-ZeRO to progressively larger models (100M → 1B → 10B parameters) to empirically verify the scalability claims and identify communication bottlenecks at each scale