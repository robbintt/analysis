---
ver: rpa2
title: 'Classification of Human- and AI-Generated Texts: Investigating Features for
  ChatGPT'
arxiv_id: '2308.05341'
source_url: https://arxiv.org/abs/2308.05341
tags:
- text
- texts
- ai-generated
- features
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new text corpus covering 10 school topics
  and a system for classifying human- and AI-generated texts, including basic and
  advanced ChatGPT-generated texts. The system uses a combination of perplexity, semantic,
  list lookup, error-based, readability, AI feedback, and text vector features.
---

# Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT

## Quick Facts
- **arXiv ID**: 2308.05341
- **Source URL**: https://arxiv.org/abs/2308.05341
- **Reference count**: 37
- **Primary result**: New feature set improves AI-generated text classification, achieving 96%+ F1-score for basic generation and outperforming GPTZero by 183.8% for rephrasing detection.

## Executive Summary
This paper presents a comprehensive system for classifying human- and AI-generated texts, specifically targeting ChatGPT outputs. The researchers developed a new corpus of approximately 500 articles across 10 school topics and implemented a classification system using eight categories of features: perplexity, semantic, list lookup, document, error-based, readability, AI feedback, and text vector features. The system achieves high accuracy in distinguishing between human-generated, basic ChatGPT-generated, advanced ChatGPT-generated, and AI-rephrased texts. The newly proposed features significantly improve classification performance, with the best system for detecting AI-rephrased text outperforming GPTZero by 183.8% in F1-score.

## Method Summary
The researchers created a new text corpus covering 10 school topics (biology, chemistry, geography, history, IT, music, politics, religion, sports, and visual arts) with approximately 500 articles. They generated basic and advanced AI texts using ChatGPT and created AI-rephrased versions of human-generated texts. The classification system extracted eight categories of features: perplexity scores (PPLmean, PPLmax), semantic features (sentiment polarity and subjectivity), list lookup features (stop words, special characters, discourse markers, title repetitions), document features (word/sentence/paragraph statistics), error-based features (grammar and spelling errors), readability scores (Flesch), AI feedback (ChatGPT confirmation), and text vector features (TF-IDF, Sentence-BERT). They evaluated three classifiers (XGBoost, Random Forest, MLP) using 5-fold cross-validation and compared results against GPTZero baseline.

## Key Results
- Best systems for classifying basic and advanced human-generated/AI-generated texts achieved F1-scores over 96%
- Best systems for classifying basic and advanced human-generated/AI-rephrased texts achieved F1-scores over 78%
- New features substantially improved classifier performance across multiple models
- The best basic text rephrasing detection system outperformed GPTZero by 183.8% relative in F1-score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity captures unpredictability differences between human and AI-generated text
- Mechanism: Human writing contains more varied, less predictable word sequences than AI-generated text, leading to higher perplexity values
- Core assumption: Language models can accurately measure perplexity for distinguishing text sources
- Evidence anchors:
  - [abstract]: "A lower perplexity indicates that the language model is better at predicting the next word in a sequence."
  - [section]: "Human-generated text tends to be more varied and unpredictable than AI-generated text."

### Mechanism 2
- Claim: Semantic subjectivity scores differ between human and AI-generated text
- Mechanism: AI-generated text tends to have higher objectivity scores due to training data and fine-tuning objectives
- Core assumption: TextBlob's subjectivity scoring accurately reflects the human vs AI distinction
- Evidence anchors:
  - [section]: "more AI-generated and AI-rephrased texts have higher sentimentsubjectivity scores than human-generated texts."
  - [section]: "ChatGPT was fine-tuned using reinforcement learning from human feedback."

### Mechanism 3
- Claim: Error-based features effectively distinguish human and AI-generated text
- Mechanism: Human writing contains more spelling and grammar errors than AI-generated text
- Core assumption: LanguageTool accurately detects spelling and grammar errors
- Evidence anchors:
  - [section]: "We observed that in AI-generated text fewer spelling and grammar errors occur than in human-generated text."
  - [section]: "LanguageTool detects more spelling and grammar errors in the human-generated than in the AI-generated and AI-rephrased texts."

## Foundational Learning

- **Concept**: Perplexity in language modeling
  - Why needed here: Core feature for distinguishing human vs AI text generation patterns
  - Quick check question: How does perplexity measure the predictability of word sequences in text?

- **Concept**: Sentiment and subjectivity analysis
  - Why needed here: Semantic features that capture differences in writing style and intent
  - Quick check question: What's the difference between sentiment polarity and subjectivity in text analysis?

- **Concept**: Feature engineering and selection
  - Why needed here: Understanding how different features contribute to classification performance
  - Quick check question: How do traditional features differ from newly proposed features in this context?

## Architecture Onboarding

- **Component map**: Data collection pipeline (Wikipedia articles + ChatGPT generation) -> Feature extraction module (8 feature categories) -> Classification models (XGBoost, Random Forest, MLP) -> Evaluation framework (5-fold cross-validation, accuracy, F1-score)

- **Critical path**: Data collection → Feature extraction → Model training → Evaluation → Comparison with GPTZero

- **Design tradeoffs**: Using multiple feature categories vs. focusing on the most effective ones; balancing model complexity with interpretability; tradeoff between detection accuracy and false positives

- **Failure signatures**: Low F1-scores across all feature categories; inconsistent performance between basic and advanced text generation detection; poor generalization to unseen text topics or styles

- **First 3 experiments**:
  1. Implement perplexity-based feature extraction and test basic classification performance
  2. Add semantic features (sentiment polarity and subjectivity) to the model and evaluate improvement
  3. Compare traditional feature categories with newly proposed features for classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the new features in detecting AI-generated text in other languages besides English?
- Basis in paper: [inferred] The paper mentions that they plan to classify text in other languages in future work
- Why unresolved: The paper only investigated the effectiveness of the new features in English text
- What evidence would resolve it: Experiments testing the new features on AI-generated text in multiple languages

### Open Question 2
- Question: What is the impact of different types of prompts on the classification performance of AI-generated text?
- Basis in paper: [explicit] The paper mentions investigating further variants of prompts in future work
- Why unresolved: The paper only evaluated two types of prompts (basic and advanced)
- What evidence would resolve it: Experiments testing classification performance using a wider variety of prompts

### Open Question 3
- Question: Can combining all features further improve the classification performance of AI-generated text?
- Basis in paper: [inferred] The paper mentions that performances of systems combining all features are very close
- Why unresolved: The paper does not provide evidence of whether combining all features would lead to better performance
- What evidence would resolve it: Experiments comparing classification performance of systems combining all features vs. subsets of features

## Limitations

- The study relies on a curated corpus of 10 school topics, which may not represent the full diversity of real-world text generation scenarios
- The classification system's effectiveness may diminish as AI language models become more sophisticated and adopt more human-like writing patterns
- The error-based mechanism assumes AI-generated text consistently contains fewer errors than human writing, which may not hold as language models improve

## Confidence

- **High Confidence**: The fundamental premise that perplexity, semantic features, and error-based features can distinguish human and AI-generated text; the empirical results showing strong F1-scores (96%+ for basic generation)
- **Medium Confidence**: The specific contribution of newly proposed features over traditional ones, and the claimed 183.8% improvement over GPTZero
- **Low Confidence**: The long-term stability of these detection mechanisms as AI language models continue to evolve

## Next Checks

1. **Cross-Domain Validation**: Test the classification system on text samples from domains outside the 10 school topics used in the original corpus to assess generalization capability

2. **Prompt Sensitivity Analysis**: Systematically vary the prompts used for advanced generation and rephrasing to determine how sensitive the detection system is to different generation instructions

3. **Temporal Robustness Testing**: Evaluate the system's performance on texts generated by newer versions of ChatGPT and other contemporary language models to assess its effectiveness against evolving AI writing patterns