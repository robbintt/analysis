---
ver: rpa2
title: Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution
  Detection
arxiv_id: '2303.05828'
source_url: https://arxiv.org/abs/2303.05828
tags:
- detection
- learning
- conference
- clip
- in-distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a comprehensive study of visual out-of-distribution
  (OOD) detection using pretrained models. The authors show that contrastive language-image
  pretrained (CLIP) models achieve state-of-the-art unsupervised OOD performance using
  nearest neighbor feature similarity as the detection score.
---

# Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2303.05828
- Source URL: https://arxiv.org/abs/2303.05828
- Authors: 
- Reference count: 40
- Primary result: CLIP models achieve state-of-the-art unsupervised OOD detection using nearest neighbor feature similarity

## Executive Summary
This work provides a comprehensive study of visual out-of-distribution (OOD) detection using pretrained models. The authors show that contrastive language-image pretrained (CLIP) models achieve state-of-the-art unsupervised OOD performance using nearest neighbor feature similarity as the detection score. They propose a new method called pseudo-label probing (PLP) that adapts vision-language models for OOD detection by training a linear layer on pseudo-labels derived from the text encoder of CLIP. PLP outperforms previous state-of-the-art on all 5 large-scale benchmarks based on ImageNet, with an average AUROC gain of 3.4% using the largest CLIP model (ViT-G). The authors also show that linear probing outperforms fine-tuning by large margins for CLIP architectures.

## Method Summary
The paper presents two main approaches for OOD detection using CLIP models. First, zero-shot OOD detection using nearest neighbor feature similarity, where cosine similarity between CLIP image features is used as the OOD score. Second, pseudo-label probing (PLP), which adapts CLIP models by training a linear layer on pseudo-labels derived from the text encoder. The PLP method computes maximum softmax probability (MSP) scores and keeps training images with at least 90% probability, then trains a linear head on the features. The authors also compare linear probing against fine-tuning, finding that linear probing consistently outperforms fine-tuning for CLIP architectures.

## Key Results
- CLIP models achieve state-of-the-art unsupervised OOD performance using nearest neighbor feature similarity
- Pseudo-label probing (PLP) outperforms previous state-of-the-art on all 5 large-scale ImageNet-based benchmarks with 3.4% average AUROC gain using ViT-G
- Linear probing outperforms fine-tuning by large margins for CLIP architectures, with CLIP ViT-H achieving 7.3% mean AUROC gain on average
- Billion-parameter CLIP models fail at detecting adversarially manipulated OOD images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models trained on large-scale image-text pairs learn representations that generalize well to unseen classes, enabling effective OOD detection without fine-tuning.
- Mechanism: The contrastive objective aligns image and text embeddings, forcing the model to capture semantic concepts rather than dataset-specific features. This semantic alignment allows nearest neighbor similarity in feature space to discriminate in-distribution from out-of-distribution samples.
- Core assumption: The semantic space learned by CLIP is sufficiently broad and consistent across diverse image-text pairs to generalize to novel classes.
- Evidence anchors:
  - [abstract]: "contrastive language-image pretrained (CLIP) models achieve state-of-the-art unsupervised OOD performance using nearest neighbor feature similarity"
  - [section 1]: "CLIP-based models were trained on different large-scale image-text datasets... LAION-2B consists of 2 billion image-text descriptions"
  - [corpus]: Weak - only 0 citations for CLIPood paper, but the abstract suggests similar generalization claims
- Break condition: If the test distribution contains concepts not well represented in the CLIP training corpus, the semantic alignment breaks down.

### Mechanism 2
- Claim: Pseudo-label probing (PLP) adapts CLIP models for OOD detection by training a linear layer on pseudo-labels derived from the text encoder.
- Mechanism: The text encoder maps class names to embeddings, and pseudo-labels are assigned based on maximum similarity. A linear layer then learns to map image features to these pseudo-labels, refining the decision boundary between in- and out-of-distribution samples.
- Core assumption: The text encoder's mapping of class names to embeddings captures the semantic relationships needed for effective OOD discrimination.
- Evidence anchors:
  - [abstract]: "We propose a new simple and scalable method called pseudo-label probing (PLP) that adapts vision-language models for OOD detection by training a linear layer on pseudo-labels derived from the text encoder of CLIP"
  - [section 3.4]: "Pseudo-labels Probing computes the MSP as in the previous setup and keeps the Dtrain in images with at least 90% probability. Then a linear head on the features of g is trained and evaluated using MSP or RMD"
  - [corpus]: Missing - no direct evidence for this specific PLP mechanism
- Break condition: If the text encoder's class name embeddings do not capture the semantic relationships relevant to the target dataset, pseudo-label probing will not improve performance.

### Mechanism 3
- Claim: Linear probing outperforms fine-tuning by large margins for CLIP architectures in OOD detection.
- Mechanism: Linear probing only updates a shallow classifier on top of frozen CLIP features, preserving the semantic structure learned during pretraining. Fine-tuning modifies these features, potentially degrading their generalization ability.
- Core assumption: The features learned by CLIP are already optimal for OOD detection, and additional adaptation is unnecessary or harmful.
- Evidence anchors:
  - [abstract]: "we show that linear probing outperforms fine-tuning by large margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of 7.3% AUROC on average on all ImageNet-based benchmarks)"
  - [section 3.4]: "Concerning linear probing, RMD consistently outperforms MSP when applied to the classifier's logits; for instance, 1.95% mean gain on CIFAR100→ CIFAR10 and CIFAR100→TinyIN"
  - [corpus]: Weak - only one related paper on CLIP fine-tuning, no direct comparison to linear probing
- Break condition: If the target distribution differs significantly from CLIP's training distribution, linear probing may be insufficient and fine-tuning may be necessary.

## Foundational Learning

- Concept: Contrastive learning and its application to vision-language models
  - Why needed here: Understanding how CLIP learns representations through contrasting image-text pairs is crucial for grasping why these models generalize well to OOD detection.
  - Quick check question: What is the primary objective function used to train CLIP models, and how does it encourage semantic alignment between images and text?

- Concept: Nearest neighbor methods for OOD detection
  - Why needed here: The paper uses 1-NN similarity as the OOD detection score, so understanding how this method works and its limitations is important.
  - Quick check question: How does 1-NN OOD detection work, and what are its key assumptions about the feature space?

- Concept: Adversarial attacks and their impact on OOD detection
  - Why needed here: The paper introduces a novel method for adversarial OOD data manipulation and shows that even large CLIP models are vulnerable, highlighting the importance of robustness in OOD detection.
  - Quick check question: What is the main difference between the proposed adversarial OOD manipulation method and traditional adversarial attacks on image classification?

## Architecture Onboarding

- Component map: CLIP model (backbone) -> Text encoder -> Linear probe -> Adversarial perturbation generator
- Critical path:
  1. Extract features from CLIP model for in-distribution samples
  2. Compute nearest neighbor similarities or train linear probe
  3. Score OOD detection performance using AUROC
  4. (Optional) Generate adversarial OOD samples and evaluate robustness
- Design tradeoffs:
  - Linear probing vs. fine-tuning: Linear probing preserves CLIP's semantic structure but may be less adaptable to specific distributions
  - Nearest neighbor vs. Mahalanobis distance: Nearest neighbor is simpler but may be less robust to feature space geometry
  - Adversarial smoothing: Improves robustness but adds computational overhead
- Failure signatures:
  - Poor OOD detection performance: Indicates that CLIP's features are not well-suited to the target distribution or that the detection method is inadequate
  - Sensitivity to adversarial attacks: Suggests that the model relies on local pixel information rather than purely semantic features
- First 3 experiments:
  1. Evaluate 1-NN OOD detection performance on CIFAR100→CIFAR10 using different CLIP model sizes
  2. Compare linear probing vs. fine-tuning on the same benchmark
  3. Generate adversarial OOD samples and measure the drop in OOD detection performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The adversarial attack evaluation uses a specific attack method that may not generalize to other attack types
- The effectiveness of pseudo-label probing depends heavily on the text encoder's ability to capture relevant semantic relationships
- The conclusion that linear probing outperforms fine-tuning is based on a limited comparison and may not hold for all distribution shifts or model architectures

## Confidence
- High confidence: CLIP's baseline OOD detection performance using nearest neighbor similarity
- Medium confidence: PLP method effectiveness and linear probing advantages
- Low confidence: Generalization of adversarial attack results to other attack methods

## Next Checks
1. Evaluate OOD detection performance using precision-recall curves and F1 scores in addition to AUROC to capture performance variations across dataset pairs
2. Test PLP effectiveness with alternative text encoders or class name representations to assess sensitivity to semantic mapping quality
3. Apply CLIP-based OOD detection to domain adaptation scenarios with gradual distribution shifts to evaluate robustness beyond discrete dataset pairs