---
ver: rpa2
title: Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive
  Learning and Self-Distillation
arxiv_id: '2308.14654'
source_url: https://arxiv.org/abs/2308.14654
tags:
- intent
- slot
- joint
- learning
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively training joint
  models for multiple intent detection and slot filling in spoken language understanding.
  The authors propose a bidirectional joint model that explicitly employs intent information
  to recognize slots and slot features to detect intents.
---

# Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation

## Quick Facts
- arXiv ID: 2308.14654
- Source URL: https://arxiv.org/abs/2308.14654
- Reference count: 34
- Key outcome: State-of-the-art performance on MixATIS and MixSNIPS datasets using a bidirectional joint model with supervised contrastive learning and self-distillation

## Executive Summary
This paper addresses the challenge of effectively training joint models for multiple intent detection and slot filling in spoken language understanding. The authors propose a bidirectional joint model that explicitly employs intent information to recognize slots and slot features to detect intents. To train the model effectively, they introduce a novel method using supervised contrastive learning and self-distillation. Experimental results on MixATIS and MixSNIPS datasets show that the proposed method outperforms state-of-the-art models, achieving 81.5% intent accuracy, 89.4% slot F1 score, and 51.5% sentence-level semantic frame accuracy on MixATIS, and 97.8% intent accuracy, 97.2% slot F1 score, and 85.4% sentence-level semantic frame accuracy on MixSNIPS.

## Method Summary
The proposed method is a bidirectional joint model for multiple intent detection and slot filling. The model first predicts intermediate intent probabilities, which are concatenated with word embeddings to form slot input representations. After slot prediction, the slot outputs are combined with the utterance representation to refine final intent predictions. To train the model effectively, the authors introduce supervised contrastive learning to improve representation quality and self-distillation to transfer knowledge from the final intent predictions back to the intermediate intent layer. The model is trained using a joint loss function combining intent loss, slot loss, contrastive losses, and self-distillation loss.

## Key Results
- Achieves 81.5% intent accuracy, 89.4% slot F1 score, and 51.5% sentence-level semantic frame accuracy on MixATIS
- Achieves 97.8% intent accuracy, 97.2% slot F1 score, and 85.4% sentence-level semantic frame accuracy on MixSNIPS
- Outperforms state-of-the-art models on both MixATIS and MixSNIPS datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bidirectional design explicitly transfers intent information to improve slot recognition and vice versa.
- Mechanism: The model first predicts intermediate intent probabilities, which are concatenated with word embeddings to form slot input representations. After slot prediction, the slot outputs are combined with the utterance representation to refine final intent predictions.
- Core assumption: Intent and slot information are mutually informative and can correct each other when explicitly shared in both directions.
- Evidence anchors:
  - [abstract] "The model should be designed so that the result of one task can be used to correct or improve the result of the other, and vice versa."
  - [section] "Given an utterance, our model employs a language model-based encoder to generate its representation and intermediate (soft) intents, which are utilized to extract slots with a biaffine classifier."
- Break condition: If the intermediate intent predictions are inaccurate, the concatenated slot representations will propagate errors, degrading slot filling performance.

### Mechanism 2
- Claim: Supervised contrastive learning improves representation quality by maximizing agreement within classes and minimizing agreement across classes.
- Mechanism: Positive samples are generated by applying dropout-based augmentations to the same utterance, and by selecting utterances with identical labels. The contrastive loss encourages embeddings of same-class utterances/spans to cluster while pushing different-class embeddings apart.
- Core assumption: Data augmentations preserve label semantics and the embedding space can be structured so that semantic similarity aligns with class labels.
- Evidence anchors:
  - [abstract] "We propose a method to generate augmentations for the intent detection and slot filling task and integrate the contrastive loss with the original classification loss."
  - [section] "Following the success of contrastive learning in computer vision, several studies have investigated contrastive learning for NLP tasks [12, 26, 32]."
- Break condition: If the augmentations introduce label changes or the model overfits to the contrastive signal, performance may degrade rather than improve.

### Mechanism 3
- Claim: Self-distillation transfers knowledge from the final intent predictions back to the intermediate intent layer, improving overall quality.
- Mechanism: The intermediate intent layer acts as a student, the final intent layer as a teacher. A KL-divergence-based loss minimizes the difference between the soft predictions of both layers, decomposed into binary classification problems.
- Core assumption: The final intent predictions are of higher quality than the intermediate ones due to additional context from slot predictions, and this knowledge can be distilled back to improve the intermediate layer.
- Evidence anchors:
  - [abstract] "We use self-distillation to transfer knowledge from the final intents to the intermediate intents, which leads to an improvement in the joint model's performance."
  - [section] "We compute the representative distance by leveraging the hidden states of both the final intent detection and intermediate intent detection layers."
- Break condition: If the final intent predictions are noisy, distilling them back may harm the intermediate layer and propagate errors.

## Foundational Learning

- Concept: Contrastive learning fundamentals (anchor, positive, negative samples; similarity maximization/minimization).
  - Why needed here: The method depends on constructing multi-viewed positive samples and selecting negatives to shape the embedding space for both intents and slots.
  - Quick check question: What defines a positive sample in the supervised contrastive loss for slots?

- Concept: Knowledge distillation (teacher-student framework, soft label transfer).
  - Why needed here: Self-distillation here uses the final intent layer as a teacher to improve the intermediate layer, requiring understanding of distillation objectives and KL divergence.
  - Quick check question: How does the binary decomposition in multi-label distillation differ from standard distillation?

- Concept: Biaffine classifiers (global span scoring for sequence tagging).
  - Why needed here: Slot filling is performed using a biaffine layer that considers all start-end span pairs globally, requiring knowledge of biaffine attention and scoring.
  - Quick check question: Why is a biaffine classifier preferred over independent CRF tagging for overlapping slots?

## Architecture Onboarding

- Component map: Encoder -> Intermediate intent detection -> Slot classifier -> Final intent detection
- Critical path: Utterance -> Encoder -> Intermediate intents -> Slot classifier -> Final intents (forward); contrastive and distillation losses backpropagate to encoder and intent layers.
- Design tradeoffs:
  - Explicit vs implicit information sharing: bidirectional design increases complexity but allows direct correction.
  - Multi-view augmentations vs single-pass: more views improve contrastive learning but increase compute.
  - Soft vs hard distillation targets: soft targets preserve gradient signal but may be noisy.
- Failure signatures:
  - Intermediate intent predictions too noisy -> slot filling degrades.
  - Contrastive loss overwhelms classification loss -> overfitting to embedding space.
  - Self-distillation KL loss too large -> unstable training or negative transfer.
- First 3 experiments:
  1. Remove the intermediate intent layer and feed encoder output directly to slot classifier; compare intent and slot performance.
  2. Disable supervised contrastive learning; compare with full model on validation set.
  3. Remove self-distillation loss; compare with full model to isolate distillation contribution.

## Open Questions the Paper Calls Out
- How does the proposed method perform when applied to more diverse and complex real-world datasets with longer utterances and more nuanced intent distinctions?
- What is the impact of varying the number of intermediate intent classes on the model's performance, and how does this affect the overall accuracy of intent detection and slot filling?
- How does the model's performance change when incorporating additional linguistic features, such as part-of-speech tags or syntactic dependencies, into the slot filling process?

## Limitations
- The effectiveness of the bidirectional information flow critically depends on the intermediate intent predictions being sufficiently accurate; no ablation or error analysis is provided to quantify how much slot performance degrades when intent predictions are incorrect.
- The supervised contrastive learning mechanism assumes that dropout-based augmentations and label-based sampling are sufficient to define positive samples for both intent and slot tasks, but the paper does not explore alternative augmentation strategies or justify why these specific views are optimal.
- The self-distillation approach assumes that the final intent predictions are consistently better than the intermediate ones; however, without comparing intermediate vs final intent accuracy on a validation set, it's unclear whether this assumption holds across different utterance types.

## Confidence
- High confidence: The paper achieves state-of-the-art performance on MixATIS and MixSNIPS datasets as reported. The results are clearly presented and directly comparable to prior work.
- Medium confidence: The bidirectional model design improves performance over unidirectional models. While the paper claims this, the ablation study is not detailed enough to quantify the exact contribution of bidirectional flow.
- Low confidence: The supervised contrastive learning and self-distillation mechanisms are necessary and optimal for this task. The paper provides limited ablation or ablation-like analysis to isolate the contribution of each component.

## Next Checks
1. **Ablation of Intermediate Intent Accuracy**: Measure the accuracy of intermediate intent predictions on a validation set and analyze how much slot F1 score degrades when the intermediate intent predictions are forced to be random or incorrect. This would quantify the risk of error propagation in the bidirectional design.

2. **Contrastive Learning Component Isolation**: Remove the supervised contrastive learning component and retrain the model. Compare intent and slot performance to the full model to determine the marginal benefit of contrastive learning. Additionally, test with alternative augmentation strategies (e.g., token masking, back-translation) to see if dropout-based views are optimal.

3. **Self-Distillation Sensitivity Analysis**: Vary the weight of the self-distillation loss (Î»4) across a range of values and plot intent and slot performance as a function of this hyperparameter. This would reveal whether the chosen value is optimal or if the method is sensitive to this hyperparameter.