---
ver: rpa2
title: Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification
arxiv_id: '2310.08123'
source_url: https://arxiv.org/abs/2310.08123
tags:
- text
- promptav
- authorship
- prompting
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the authorship verification task, aiming to
  determine if two texts are written by the same author. The authors propose PromptAV,
  a novel technique leveraging Large Language Models (LLMs) with step-by-step stylometric
  explanation prompts to analyze linguistic features indicative of authorship.
---

# Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification

## Quick Facts
- arXiv ID: 2310.08123
- Source URL: https://arxiv.org/abs/2310.08123
- Authors: Multiple
- Reference count: 13
- Primary result: PromptAV achieves 0.587 accuracy in zero-shot and 0.667 in 2-shot settings on IMDb62 dataset

## Executive Summary
This paper introduces PromptAV, a novel technique for authorship verification that leverages Large Language Models (LLMs) with step-by-step stylometric explanation prompts. The method guides LLMs to analyze linguistic features indicative of authorship, producing interpretable confidence scores rather than binary classifications. PromptAV demonstrates superior performance compared to existing prompting strategies while maintaining interpretability through detailed reasoning explanations.

## Method Summary
PromptAV employs a prompting strategy that incorporates key linguistic features (stylometric features) to guide LLMs in authorship verification tasks. The method uses step-by-step explanation prompts to direct the LLM through understanding the problem, extracting relevant stylometric features (punctuation, capitalization, writing style, etc.), and systematically evaluating each feature before producing a confidence score. The approach is evaluated in both zero-shot and few-shot settings using the gpt-3.5-turbo model with temperature 0.

## Key Results
- Achieves 0.587 accuracy in zero-shot settings on IMDb62 dataset
- Achieves 0.667 accuracy in 2-shot settings on IMDb62 dataset
- Outperforms state-of-the-art baselines including CoT and PS+ prompting strategies
- Demonstrates effectiveness with limited training data while providing interpretable explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-by-step stylometric explanation prompts enable LLMs to break down authorship verification into interpretable intermediate reasoning steps
- Mechanism: The prompt instructs the model to first understand the problem, extract relevant stylometric features (punctuation, capitalization, writing style, etc.), and then evaluate each feature systematically before producing a confidence score
- Core assumption: LLMs can reason about linguistic features if explicitly prompted to do so in a structured manner
- Evidence anchors: [abstract]: "providing step-by-step stylometric explanation prompts", [section]: "PromptAV, a prompting strategy that incorporates key linguistic features...These linguistic features serve as rich, often subtle, markers of an authorâ€™s distinct writing style"

### Mechanism 2
- Claim: Confidence scoring instead of binary classification mitigates the tendency of LLMs to default to "no" for most instances in AV tasks
- Mechanism: By instructing the model to produce a score from 0 to 1 and calibrating with a moderate strictness directive, the output distribution becomes more balanced and informative
- Core assumption: Binary prompts bias LLMs toward conservative "no" responses; a continuous scale reduces this bias
- Evidence anchors: [section]: "PromptAV instructs the LLM to generate a confidence score ranging from 0 to 1, rather than a binary response"

### Mechanism 3
- Claim: Using LLM-generated intermediate reasoning steps in few-shot settings avoids the need for manually crafted reasoning chains
- Mechanism: The prompt includes a directive like "It is given that after following the instruction, the confidence score obtained is [X]. Show the step-by-step execution..." which leverages the LLM's own reasoning capability to generate reasoning examples on the fly
- Core assumption: LLMs can generate valid reasoning steps when prompted with the desired outcome and instruction to explain how to achieve it
- Evidence anchors: [section]: "we resort to leveraging the capacity of LLMs as zero-shot reasoners to generate the required reasoning steps"

## Foundational Learning

- Concept: Stylometric features (e.g., punctuation, capitalization, acronyms, writing style, tone, sentence structure)
  - Why needed here: These features are the linguistic markers used by PromptAV to distinguish authorship. The LLM must be guided to evaluate them explicitly
  - Quick check question: What are three stylometric features that could differentiate two authors' writing styles?

- Concept: Chain-of-thought (CoT) and plan-and-solve (PS+) prompting
  - Why needed here: These prompting paradigms inspired PromptAV's step-by-step explanation format. Understanding their strengths/weaknesses contextualizes PromptAV's improvements
  - Quick check question: How does CoT prompting differ from direct prompting in terms of intermediate reasoning?

- Concept: Zero-shot vs few-shot vs supervised learning
  - Why needed here: PromptAV is evaluated in zero-shot and few-shot settings, so understanding the data requirements and performance implications of each is essential
  - Quick check question: What is the main advantage of zero-shot prompting in authorship verification?

## Architecture Onboarding

- Component map:
  - Prompt template -> LLM endpoint -> Threshold calibrator -> Evaluation harness

- Critical path:
  1. Construct prompt with stylometric feature list and step-by-step instruction
  2. Send prompt to LLM with appropriate few-shot examples (if any)
  3. Receive confidence score and optional explanation
  4. Apply threshold to convert score to binary prediction
  5. Evaluate accuracy against ground truth

- Design tradeoffs:
  - Accuracy vs interpretability: Detailed explanations improve interpretability but may slightly reduce raw accuracy
  - Prompt length vs cost: Longer prompts with more features increase API cost and latency
  - Few-shot vs zero-shot: Few-shot improves accuracy but requires careful example selection

- Failure signatures:
  - Consistently low confidence scores across all examples
  - LLM ignores stylometric features and defaults to generic reasoning
  - Explanations mention vocabulary not present in the texts (hallucinations)

- First 3 experiments:
  1. Run PromptAV on IMDb62 in zero-shot setting and compare accuracy to CoT and PS+ baselines
  2. Test PromptAV with different stylometric feature sets (8, 9, 10 features) to identify optimal feature subset
  3. Apply authorship obfuscation (Mutant-X) to IMDb62 and evaluate PromptAV's robustness against obfuscated text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features are most effective for authorship verification when using PromptAV?
- Basis in paper: [explicit] The paper discusses the use of key linguistic features for authorship verification, but does not provide a detailed analysis of which features are most effective
- Why unresolved: The paper mentions that the performance of PromptAV varies based on the chosen feature set, but does not provide a comprehensive analysis of the impact of individual features
- What evidence would resolve it: A detailed study analyzing the impact of individual linguistic features on the performance of PromptAV would provide insights into the most effective features for authorship verification

### Open Question 2
- Question: How does PromptAV perform on datasets with different characteristics, such as varying text lengths or genres?
- Basis in paper: [inferred] The paper evaluates PromptAV on the IMDb62 dataset, but does not explore its performance on datasets with different characteristics
- Why unresolved: The paper focuses on the performance of PromptAV on a specific dataset, but does not provide insights into its generalizability to other datasets with different characteristics
- What evidence would resolve it: Conducting experiments on datasets with varying text lengths, genres, and other characteristics would provide insights into the generalizability of PromptAV

### Open Question 3
- Question: How does PromptAV handle cases where the authorship obfuscation methods are more sophisticated?
- Basis in paper: [explicit] The paper tests PromptAV against the Mutant-X algorithm, but does not explore its performance against more sophisticated obfuscation methods
- Why unresolved: The paper acknowledges the need for more comprehensive testing against a broader range of obfuscation methods, but does not provide insights into PromptAV's performance against more sophisticated methods
- What evidence would resolve it: Conducting experiments using more sophisticated authorship obfuscation methods would provide insights into PromptAV's robustness against such methods

## Limitations
- The exact stylometric feature set is referenced but not explicitly enumerated in the paper
- Few-shot examples used across experiments are not specified, though same examples are indicated for all settings
- Performance evaluation limited to IMDb62 dataset without exploration of different text characteristics or genres

## Confidence

**High Confidence** (supported by extensive evidence):
- The core mechanism of using step-by-step stylometric explanation prompts with LLMs for authorship verification
- The general prompt structure including task description, feature list, and confidence scoring directive
- The evaluation methodology using IMDb62 dataset and accuracy metrics
- The finding that PromptAV outperforms CoT and PS+ baselines in both zero-shot and few-shot settings

**Medium Confidence** (mechanism described but implementation details missing):
- The exact composition of the 8 stylometric features
- The specific formatting of the prompt template
- The selection and content of few-shot examples
- The exact threshold calibration methodology

## Next Checks
1. **Feature Set Validation**: Contact authors to obtain the complete list of 8 stylometric features used, or systematically test different feature combinations (8-10 features) to identify which subset achieves optimal performance.

2. **Prompt Template Replication**: Reconstruct the prompt template based on the described components (task description, feature list, step-by-step instruction, confidence scoring directive) and test with different formatting variations to identify the most effective structure.

3. **Baseline Implementation**: Implement the CoT and PS+ baselines as described in the paper using the same IMDb62 test set (503 positive, 497 negative pairs) to verify the claimed performance improvements of PromptAV.