---
ver: rpa2
title: Bayesian Domain Invariant Learning via Posterior Generalization of Parameter
  Distributions
arxiv_id: '2310.16277'
source_url: https://arxiv.org/abs/2310.16277
tags:
- domain
- invariant
- learning
- domains
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PTG, a novel approach for domain generalization
  that learns domain invariant parameters by aggregating posteriors across training
  domains. The key idea is to infer the posterior of domain invariant parameters by
  averaging parameter distributions from multiple BNNs trained on different domains.
---

# Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions

## Quick Facts
- arXiv ID: 2310.16277
- Source URL: https://arxiv.org/abs/2310.16277
- Authors: [Redacted]
- Reference count: 25
- This paper proposes PTG, a novel approach for domain generalization that learns domain invariant parameters by aggregating posteriors across training domains, achieving SOTA performance on various benchmarks.

## Executive Summary
This paper introduces PTG, a Bayesian approach for domain generalization that learns domain invariant parameters by aggregating posterior distributions across training domains. The key innovation is inferring the domain invariant posterior implicitly through parameter distribution aggregation, rather than explicit feature alignment. PTG achieves state-of-the-art performance on multiple domain generalization benchmarks, outperforming previous methods by 2.2% on average. The paper also proposes PTG-Lite, a more practical variant that reduces memory usage by filtering out domain-specific parameters based on coefficient of variation.

## Method Summary
PTG operates by training Bayesian Neural Networks (BNNs) on each training domain to approximate posterior parameter distributions, then aggregating these distributions to infer domain invariant parameters. The method initializes featurizers with a domain generalization model, trains BNNs on each domain using variational inference, aggregates posteriors to form invariant parameter distributions, and fine-tunes the model with decayed learning rate. PTG-Lite simplifies this by dropping parameters with high coefficient of variation across domains, reducing memory usage while maintaining performance.

## Key Results
- PTG achieves state-of-the-art performance on DomainBed benchmarks (PACS, VLCS, OfficeHome, TerraIncognita)
- Outperforms previous methods by 2.2% on average across all benchmarks
- PTG-Lite provides a practical trade-off with reduced memory usage (single BNN) while maintaining competitive accuracy
- Combined with other DG methods, PTG further improves performance beyond individual approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating parameter posteriors across training domains implicitly learns the domain invariant posterior distribution.
- Mechanism: By averaging variational parameter distributions from BNNs trained on different domains, the resulting aggregated distribution captures parameters stable across domains while filtering out domain-specific variations.
- Core assumption: There exist two independent sufficient statistics: domain invariant information (Dc) and domain-specific information (Dv). Parameters conditioned on Dc are invariant.
- Evidence anchors:
  - [abstract]: "invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains"
  - [section]: "We can empirically estimate p(ω|Dc) by sampling from p(Dv). Since p(Dc) is constant, sampling from p(Dv) is the same as sampling from p(D), which is exactly {Di}N i=1"
  - [corpus]: Weak. No direct corpus papers found that explicitly validate this aggregation theorem for domain generalization.
- Break condition: If Dc and Dv are not independent or Dc is not constant across domains, the aggregation fails.

### Mechanism 2
- Claim: PTG-Lite removes domain-specific parameters by dropping those with high coefficient of variation across domains.
- Mechanism: Parameters whose values vary significantly across domain-specific BNNs (high CV) are deemed domain-specific and dropped; the rest form a domain-invariant parameter set.
- Core assumption: Domain-specific parameters show higher variability across training domains than domain-invariant ones.
- Evidence anchors:
  - [abstract]: "PTG-Lite for widespread applications" and "drop out domain specific parameters"
  - [section]: "We judge whether a parameter is domain specific by its coefficient of variation: if the coefficient of a parameter on different domains is greater than a given rate β, such as 0.1, we drop out this parameter"
  - [corpus]: Weak. No corpus papers found that empirically validate CV-based parameter filtering for domain generalization.
- Break condition: If domain-invariant parameters also have high CV (e.g., due to noise or limited domains), useful parameters may be dropped.

### Mechanism 3
- Claim: PTG preserves domain-invariant information from both invariant and useful specific features by operating on parameter distributions rather than features.
- Mechanism: Instead of forcing feature alignment (which may discard useful domain-specific cues), PTG adjusts the learned feature extractors' parameters so they capture domain-invariant structure.
- Core assumption: The same parameter can extract invariant information from both invariant and specific features.
- Evidence anchors:
  - [abstract]: "Different from other DIL methods, PTG does not need to represent domain invariant information by feature distributions"
  - [section]: "PTG can be regarded as making addition: we keep the domain invariant parameters while replace the domain specific parameters by general distributions"
  - [corpus]: Weak. No corpus papers found that directly compare parameter-based vs feature-based invariance for domain generalization.
- Break condition: If parameter-level invariance does not translate to feature-level invariance, performance may degrade.

## Foundational Learning

- Concept: Bayesian Neural Networks (BNNs)
  - Why needed here: PTG relies on variational inference to approximate posterior distributions of network parameters.
  - Quick check question: Can you explain how variational inference approximates a true posterior with a tractable distribution like a Gaussian?

- Concept: Domain Generalization (DG)
  - Why needed here: PTG is a DG method; understanding the problem setting (train on multiple source domains, test on unseen domains) is essential.
  - Quick check question: What is the difference between domain adaptation and domain generalization?

- Concept: Sufficient Statistics
  - Why needed here: PTG assumes Dc and Dv are sufficient statistics of domain data; this underpins the independence assumption.
  - Quick check question: What does it mean for Dc and Dv to be sufficient statistics of the data?

## Architecture Onboarding

- Component map:
  - BNN featurizers: One per training domain, trained with variational inference
  - Classifier: Single deterministic network shared across domains
  - Aggregation module: Computes mean and variance of parameter distributions across featurizers
  - PTG-Lite filter: Computes coefficient of variation and drops high-CV parameters

- Critical path:
  1. Initialize featurizers with a DG-trained model (ERM or other)
  2. Train each featurizer on its domain using variational inference
  3. Aggregate posteriors to form invariant parameter distribution
  4. Fine-tune aggregated featurizer + classifier with decayed learning rate

- Design tradeoffs:
  - Memory vs accuracy: PTG doubles parameter storage (BNN vs DNN) but can be reduced with PTG-Lite
  - Initialization dependence: PTG relies on a good prior model; poor initialization can hurt performance
  - Domain count: More training domains improve posterior estimation reliability

- Failure signatures:
  - Degraded accuracy when initialized with a poor DG model
  - High variance in aggregated parameters indicating insufficient domain diversity
  - Overfitting on training domains if learning rate is not decayed properly

- First 3 experiments:
  1. Run PTG with ERM initialization on PACS; verify improvement over ERM baseline
  2. Compare PTG vs PTG-Lite on memory usage and accuracy
  3. Test PTG with different α (learning rate decay) values to find optimal setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between domain invariant parameters p(ω|Dc) and their variation rate across training domains?
- Basis in paper: [explicit] The paper states that "there is a strong relationship between domain invariant parameters p(ω|Dc) and its variation rate" and discusses how parameters with larger variance across domains are more likely to extract domain-specific features.
- Why unresolved: The paper mentions this relationship exists but does not provide a detailed mathematical formulation or empirical quantification of how parameter variation correlates with domain invariance.
- What evidence would resolve it: A quantitative analysis showing how the coefficient of variation of parameters across domains correlates with their contribution to domain invariant features, along with a mathematical formulation of this relationship.

### Open Question 2
- Question: How does PTG's performance scale with the number of training domains?
- Basis in paper: [explicit] The paper states "PTG estimates the invariant posterior empirically, so the number of training domain can influence the estimation reliability" and recommends at least 3 training domains.
- Why unresolved: The paper only provides experimental results for 4 domains (PACS, VLCS, OfficeHome, TerraIncognita) and does not systematically study how performance changes with fewer or more training domains.
- What evidence would resolve it: A systematic study varying the number of training domains (e.g., 2, 3, 4, 5, 6) and measuring PTG's performance on each, along with analysis of how the reliability of the invariant posterior estimation changes.

### Open Question 3
- Question: What is the optimal strategy for parameter aggregation in PTG-Lite?
- Basis in paper: [explicit] The paper states that PTG-Lite drops out domain-specific parameters based on their coefficient of variation, but notes that "some specific parameters are reserved."
- Why unresolved: The paper uses a fixed threshold (β = 0.1) for determining which parameters to drop, but does not explore whether this is optimal or if there are better strategies for parameter aggregation.
- What evidence would resolve it: An ablation study comparing different strategies for parameter aggregation in PTG-Lite, including varying the threshold β, using weighted averaging instead of dropping, or exploring other criteria for identifying domain-specific parameters.

## Limitations

- Theoretical foundation relies on independence assumption between domain invariant and specific information that is not rigorously proven
- Memory usage doubles compared to standard DG methods due to BNN featurizers
- Performance heavily depends on initialization quality and domain diversity in training data

## Confidence

**High confidence**: 
- PTG achieves SOTA results on DomainBed benchmarks
- The variational inference implementation is technically sound
- PTG-Lite effectively reduces memory usage with minimal accuracy loss

**Medium confidence**:
- The theoretical framework for posterior aggregation is internally consistent
- Domain-specific parameter filtering via CV is a reasonable heuristic
- Learning rate decay during fine-tuning is necessary for stable convergence

**Low confidence**:
- The independence assumption between Dc and Dv holds in practice
- The aggregation theorem guarantees domain invariance without counterexamples
- PTG's advantages over simpler feature alignment methods are significant

## Next Checks

1. **Independence assumption validation**: Design experiments where domain invariant and specific information are deliberately correlated (e.g., through controlled data augmentation) to test if PTG still performs well.

2. **Ablation on initialization dependence**: Compare PTG performance when initialized with random weights versus ERM, quantifying the performance gap and testing if PTG can recover from poor initialization.

3. **Parameter vs feature invariance comparison**: Implement a feature alignment baseline using the same BNN architecture and compare both approaches on datasets with varying domain shifts to determine when parameter-level invariance is advantageous.