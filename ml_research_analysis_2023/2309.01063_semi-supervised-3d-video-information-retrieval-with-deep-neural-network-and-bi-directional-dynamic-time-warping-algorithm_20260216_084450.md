---
ver: rpa2
title: Semi-supervised 3D Video Information Retrieval with Deep Neural Network and
  Bi-directional Dynamic-time Warping Algorithm
arxiv_id: '2309.01063'
source_url: https://arxiv.org/abs/2309.01063
tags:
- video
- retrieval
- videos
- block
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel deep learning approach for retrieving
  similar 2D and 3D videos based on visual content. The method uses an autoencoder-backed
  deep neural network to convert video clips into embedding vectors, and then calculates
  a similarity measure between the sequences of embedding vectors using a bi-directional
  dynamic time-warping method.
---

# Semi-supervised 3D Video Information Retrieval with Deep Neural Network and Bi-directional Dynamic-time Warping Algorithm

## Quick Facts
- arXiv ID: 2309.01063
- Source URL: https://arxiv.org/abs/2309.01063
- Reference count: 34
- This paper proposes a novel deep learning approach for retrieving similar 2D and 3D videos based on visual content.

## Executive Summary
This paper presents a novel approach for 3D video information retrieval using a combination of deep neural networks and bi-directional dynamic time warping (Bi-DTW). The method converts video clips into embedding vectors using an autoencoder-backed deep neural network, then calculates similarity between sequences of embeddings using Bi-DTW. The algorithm is tested on multiple public datasets and shows good results compared to state-of-the-art methods, effectively solving video retrieval tasks.

## Method Summary
The method uses a four-step training scheme: pre-train an autoencoder on a large dataset, train with triplet loss on the training set, compute similarity using Bi-DTW, and fine-tune with challenging samples. The autoencoder architecture uses different block combinations (LRBP, URB, UQB, UTB) for 2D and 3D cases. Videos are processed at 30 FPS with 256x256 RGB frames, converted to 4000-dimensional embedding vectors, and compared using Bi-DTW which considers both forward and backward temporal alignments.

## Key Results
- Outperforms benchmarked state-of-the-art deep learning model on CC_WEB_VIDEO, YouTube-8m, S3DIS, and Synthia datasets
- Achieves good mAP scores for both 2D and 3D video retrieval tasks
- Bi-DTW improves matching accuracy by considering both forward and backward temporal alignments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bi-directional DTW improves matching accuracy by considering both forward and backward temporal alignments, especially for videos with reversible or symmetric content.
- Mechanism: Standard DTW only aligns sequences in one direction, potentially missing optimal matches for content that can be understood equally well in reverse (e.g., certain animations, mirrored actions). Bi-DTW computes DTW in both original and reversed clip order and takes the minimum, effectively covering both perspectives.
- Core assumption: The optimal temporal alignment for a pair of video clips can be captured by either the forward sequence or its reverse, depending on the content's temporal structure.
- Evidence anchors:
  - [abstract] "Bi-DTW was developed, with a distinguishing feature being its capacity to facilitate matching from both forward and backward directions... By integrating both forward and backward alignments, Bi-DTW improves the accuracy of video retrieval by ensuring a more robust, comprehensive temporal match."
  - [section] "The Bi-DTW algorithm computes both DTW (v1,v2) and DTW (reverse(v1),reverse(v2)) where reverse (v) is the sequence of clips in the reverse order. The output of Bi-DTW algorithm is min(DTW (v1,v2),DTW (reverse(v1),reverse(v2)))."
- Break condition: If video content has strong directional semantics (e.g., "opening a door" vs "closing a door"), Bi-DTW may incorrectly favor reversed matches and hurt accuracy.

### Mechanism 2
- Claim: The combination of ConvLSTM and residual connections in the encoder preserves both spatial and temporal features while enabling deeper architectures without overfitting.
- Mechanism: ConvLSTM maintains spatial structure within hidden states, unlike fully connected LSTMs. Residual connections allow gradients to flow through deeper networks, preventing vanishing gradients and improving generalization.
- Core assumption: Video frames have strong spatial locality and temporal dependencies that can be jointly modeled by ConvLSTM, and deeper models with residual connections generalize better than shallow ones.
- Evidence anchors:
  - [section] "ConvLSTM effectively maintains the structural representations in the output and cell state... Besides, it reduces the number of parameters within the layer and enables potential more computations for better generalization."
  - [section] "In addition to the URB block, we propose two other types of blocks to support the decoder... These blocks are used to handle the extra dimensionality in the 3D case."
- Break condition: If video content is mostly static or lacks temporal correlation, ConvLSTM's benefits diminish and simpler architectures may suffice.

### Mechanism 3
- Claim: The two-stage training (unsupervised pre-training + supervised fine-tuning with triplet loss) enables effective learning from limited labeled data while leveraging large unlabeled video datasets.
- Mechanism: Pre-training the autoencoder on a large dataset learns general video features. Fine-tuning with triplet loss on labeled data adapts these features for the specific retrieval task, optimizing relative distances between similar and dissimilar video pairs.
- Core assumption: Features learned from unsupervised reconstruction capture useful video representations that can be adapted to similarity tasks with relatively little labeled data.
- Evidence anchors:
  - [section] "We use a four-step training scheme to optimize the performance of the proposed autoencoder-based video retrieval model. In step 2, a single sample i ∈ Xc can yield multiple triplets and thus the need for ¯Xc. Step 4 intentionally increases the sample number of under-studied data to encourage the model to learn better on the difficult part of the dataset."
  - [section] "We use triplet loss to train the model and learn the above distance mapping function as a neural network... This triplet loss is also visualized in Figure 6."
- Break condition: If the unlabeled dataset is too different from the target domain, pre-training may learn irrelevant features that hinder fine-tuning.

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs) and their application to video data
  - Why needed here: The paper builds on CNN architectures for feature extraction from video frames, extending them with temporal modeling via ConvLSTM
  - Quick check question: What is the key difference between standard CNNs and ConvLSTMs when processing video sequences?

- Concept: Dynamic Time Warping (DTW) and its variants
  - Why needed here: DTW is the core similarity measure between sequences of video embeddings, with Bi-DTW being the novel improvement
  - Quick check question: How does standard DTW compute similarity between two time series, and what limitation does Bi-DTW address?

- Concept: Autoencoder architectures and their use in representation learning
  - Why needed here: The model uses a seq2seq autoencoder as the backbone for learning video representations, with different block configurations for 2D and 3D cases
  - Quick check question: What is the purpose of the reconstruction loss in an autoencoder, and how does it differ from the triplet loss used for fine-tuning?

## Architecture Onboarding

- Component map: Video clip → Encoder → Embedding vector → Sequence of embeddings → Bi-DTW → Similarity score
- Critical path: Video clip → Encoder → Embedding vector → Sequence of embeddings → Bi-DTW → Similarity score
- Design tradeoffs:
  - ConvLSTM vs standard LSTM: Preserves spatial structure but increases computational cost
  - Bi-DTW vs standard DTW: Better accuracy for reversible content but doubles computation
  - Two-stage training: Leverages unlabeled data but requires careful pre-training dataset selection
- Failure signatures:
  - High reconstruction loss during pre-training: Indicates encoder-decoder mismatch or insufficient model capacity
  - Triplet loss not decreasing: Poor negative sampling strategy or margin parameter issues
  - Bi-DTW producing identical scores for different videos: Embedding vectors may be too similar or DTW parameters need tuning
- First 3 experiments:
  1. Train the autoencoder on a small subset of YouTube-8M with only reconstruction loss, verify it can reconstruct input videos
  2. Implement and test Bi-DTW on synthetic time series with known optimal alignments (forward vs reverse)
  3. Fine-tune the pre-trained autoencoder on CC_WEB_VIDEO with triplet loss, evaluate mAP on a validation set

## Open Questions the Paper Calls Out

- Question: How does the proposed 3D models perform on videos with significant temporal variations, such as videos with large time gaps or non-linear temporal structures?
- Basis in paper: [inferred] The paper proposes Bi-DTW to handle temporal variations but does not explicitly test the model's performance on videos with significant temporal variations.
- Why unresolved: The paper does not provide evidence on how the model performs on videos with large time gaps or non-linear temporal structures.
- What evidence would resolve it: Experiments on videos with significant temporal variations, including large time gaps and non-linear temporal structures.

## Limitations

- Limited comparison with recent video retrieval methods like CoVeR or Timesformer, only benchmarking against one deep learning model
- No ablation study on the Bi-DTW component to isolate its contribution to overall performance
- Computational efficiency not discussed - Bi-DTW doubles computation time but no runtime analysis provided

## Confidence

- **High confidence**: The autoencoder architecture and training methodology are well-specified and follow established practices
- **Medium confidence**: The Bi-DTW algorithm is clearly defined, but its contribution to overall performance needs independent verification
- **Low confidence**: Claims of superiority over state-of-the-art lack comprehensive benchmarking against multiple recent methods

## Next Checks

1. Implement ablation experiments comparing standard DTW vs Bi-DTW on the same framework to isolate the contribution of bi-directional alignment
2. Benchmark against additional state-of-the-art video retrieval methods (CoVeR, Timesformer, ViViT) on the same datasets
3. Conduct runtime analysis comparing Bi-DTW to standard DTW across different video lengths and embedding dimensions to quantify the computational overhead