---
ver: rpa2
title: Reward Shaping for Happier Autonomous Cyber Security Agents
arxiv_id: '2310.13565'
source_url: https://arxiv.org/abs/2310.13565
tags:
- rewards
- agent
- reward
- baseline
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates reward shaping techniques for improving
  the training of autonomous agents in cybersecurity tasks. It examines the effects
  of altering the magnitude of penalties, introducing positive rewards alongside penalties,
  and using intrinsic curiosity to encourage exploration.
---

# Reward Shaping for Happier Autonomous Cyber Security Agents
arXiv ID: 2310.13565
Source URL: https://arxiv.org/abs/2310.13565
Reference count: 40
Key outcome: Reward shaping techniques, particularly scaling penalties and adding small positive rewards, can improve the training of autonomous agents in cybersecurity tasks, while intrinsic curiosity offers limited benefits in high-level network monitoring.

## Executive Summary
This paper investigates reward shaping techniques to improve the training of autonomous agents in cybersecurity tasks, specifically in network defense scenarios. The study examines the effects of altering penalty magnitudes, introducing positive rewards alongside penalties, and using intrinsic curiosity to encourage exploration. Experiments are conducted using the CybORG environment in the CAGE 2 Challenge scenario, where a defending agent protects a network against an attacker. Results show that scaling up penalties can improve sample efficiency, while adding small positive rewards can slightly improve performance. However, intrinsic curiosity does not provide significant benefits in this high-level network monitoring task.

## Method Summary
The study uses the CybORG environment with the CAGE 2 Challenge scenario to train and evaluate autonomous agents in network defense. The baseline agent is a PPO-based model that uses an Actor-Critic architecture. The paper explores three reward shaping techniques: scaling penalty magnitudes, adding small positive rewards, and implementing an Intrinsic Curiosity Module (ICM). Each technique is tested against two scripted red agents (Bline and Meander) over 75,000 training episodes, with performance evaluated over 1,000 episodes of varying lengths. Results are compared to the baseline agent to assess the impact of each reward shaping method on learning efficiency and final performance.

## Key Results
- Scaling up penalty magnitudes improves sample efficiency without degrading final performance.
- Adding small positive rewards can slightly improve performance but does not cause gaming behavior.
- Intrinsic curiosity does not provide significant benefits in high-level network monitoring tasks due to limited observation spaces and fixed network topologies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling up the magnitude of penalties improves sample efficiency without degrading final performance.
- Mechanism: Larger penalty magnitudes increase the magnitude of error gradients during backpropagation, providing a stronger learning signal to the agent when a compromise occurs.
- Core assumption: The baseline penalty magnitudes are too small to provide a sufficient gradient signal for efficient learning.
- Evidence anchors:
  - [abstract] "We first show that deep reinforcement learning algorithms are sensitive to the magnitude of the penalties and their relative size."
  - [section] "The main incentive for exploring adjusting the rewards in this manner are that preliminary experiments in the OpenAI Gym's Mountain Car environment demonstrated changes in the learning curve when similar experiments were trialled."
  - [corpus] Weak corpus support; neighboring papers discuss reward shaping but not specifically penalty magnitude scaling in cybersecurity.
- Break condition: If penalty magnitudes become too large, the agent may learn to avoid all actions (paralysis) or the training becomes unstable due to exploding gradients.

### Mechanism 2
- Claim: Adding small positive rewards alongside penalties can improve learning without causing the agent to game the reward system.
- Mechanism: Positive rewards provide frequent intermediate feedback for proactive defensive actions, reducing the sparsity of the reward signal and encouraging exploration of beneficial behaviors.
- Core assumption: The original environment's sparsity (mostly zero rewards except for penalties) makes learning slow, and positive rewards can fill this gap.
- Evidence anchors:
  - [abstract] "Then, we combine penalties with positive external rewards and study their effect compared to penalty-only training."
  - [section] "By adding a positive reward for states where no red adversaries are encountered, an additional positive incentive is provided to keep the network healthy (in addition to the environment's penalties)."
  - [corpus] Weak corpus support; no direct evidence in neighboring papers about positive reward addition in sparse penalty environments.
- Break condition: If positive rewards are too large relative to penalties, the agent may learn to maximize positive rewards at the expense of actually defending the network.

### Mechanism 3
- Claim: Intrinsic curiosity does not provide significant benefits in high-level network monitoring tasks due to the agent's limited observation space and the environment's characteristics.
- Mechanism: Curiosity encourages exploration of novel states, but in this environment, the agent's observation space is limited and the network topology is fixed, providing few truly novel states to explore.
- Core assumption: The observation space and environment structure do not benefit from curiosity-driven exploration.
- Evidence anchors:
  - [abstract] "Finally, we evaluate intrinsic curiosity as an internal positive reward mechanism and discuss why it might not be as advantageous for high-level network monitoring tasks."
  - [section] "The lack of positive effect from introducing internal rewards through ICM is not entirely unexpected. It is likely that the baseline agent has been able to learn all defense strategies that were within its capabilities (given its observation space and the capacity of the policy neural network) and thus ICM could not help uncover new strategies."
  - [corpus] Weak corpus support; neighboring papers do not discuss curiosity in cybersecurity environments.
- Break condition: If the observation space is expanded or the environment becomes more dynamic, curiosity might become beneficial.

## Foundational Learning

- Concept: Reinforcement Learning (RL) fundamentals, including policies, value functions, and the exploration-exploitation tradeoff.
  - Why needed here: The paper uses Proximal Policy Optimization (PPO), a policy gradient method, which requires understanding of RL basics.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms?

- Concept: Reward shaping techniques and their impact on learning efficiency and final performance.
  - Why needed here: The paper investigates various reward shaping techniques, including magnitude scaling and adding positive rewards.
  - Quick check question: How can improper reward shaping lead to suboptimal policies?

- Concept: Intrinsic motivation and curiosity-driven exploration in RL.
  - Why needed here: The paper evaluates an Intrinsic Curiosity Module (ICM) as an internal reward mechanism.
  - Quick check question: What is the primary purpose of intrinsic motivation in RL, and how does it differ from extrinsic rewards?

## Architecture Onboarding

- Component map:
  - CybORG environment -> PPO agent -> Reward shaping modules -> Evaluation framework
  - PPO agent -> Actor-Critic architecture -> Policy network and value network
  - ICM module -> Intrinsic reward based on prediction error of future states

- Critical path:
  1. Initialize CybORG environment and PPO agent with baseline parameters.
  2. Train agent for 50,000 episodes against a scripted adversary.
  3. Evaluate trained agent's performance over 1,000 episodes of 30, 50, and 100 steps.
  4. Repeat steps 2-3 with modified reward signals (scaled penalties, added positive rewards, ICM).
  5. Compare performance metrics (average scores, standard deviations) across different reward shaping techniques.

- Design tradeoffs:
  - Reward magnitude: Larger penalties provide stronger learning signals but may cause instability.
  - Positive rewards: Can reduce reward sparsity but risk encouraging gaming behavior if too large.
  - Intrinsic curiosity: Encourages exploration but may not be beneficial in environments with limited observation spaces.

- Failure signatures:
  - Agent consistently receives very low scores (e.g., -100 or worse) due to paralysis or incorrect learning.
  - Agent's performance plateaus early and does not improve with additional training.
  - Agent exhibits high variance in performance across different runs.

- First 3 experiments:
  1. Scale up the magnitude of penalties by one order of magnitude and evaluate the agent's performance against both Bline and Meander adversaries.
  2. Add small positive rewards (0.1) to the reward signal and evaluate the agent's performance against both Bline and Meander adversaries.
  3. Implement an Intrinsic Curiosity Module (ICM) and evaluate the agent's performance against both Bline and Meander adversaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different reward scaling strategies (normalization, magnitude increase, disproportionate scaling) impact the learning efficiency and final performance of autonomous agents in cybersecurity tasks?
- Basis in paper: [explicit] The paper investigates the impact of scaling rewards on the learning process and performance of agents in the CybORG environment.
- Why unresolved: The study shows that different scaling strategies have varying effects on learning efficiency and performance, but it does not definitively conclude which strategy is optimal across all scenarios.
- What evidence would resolve it: Comparative analysis of different scaling strategies across a wider range of cybersecurity scenarios and agent architectures would provide clearer insights into the optimal approach.

### Open Question 2
- Question: What is the impact of introducing positive rewards in an otherwise penalty-driven cybersecurity environment on the agent's learning and performance?
- Basis in paper: [explicit] The paper explores the effect of adding positive rewards to the environment and finds minor improvements in performance.
- Why unresolved: While the study shows some benefits, it does not fully explore the potential of positive rewards or their long-term impact on agent behavior and network security.
- What evidence would resolve it: Further experimentation with varying magnitudes and frequencies of positive rewards, along with long-term performance evaluations, would clarify their overall impact.

### Open Question 3
- Question: How does intrinsic curiosity influence the learning process and performance of autonomous agents in high-level network monitoring tasks?
- Basis in paper: [explicit] The paper evaluates the use of intrinsic curiosity and finds it does not significantly improve performance in the CAGE 2 scenario.
- Why unresolved: The study suggests that curiosity may not be as beneficial in high-level network monitoring tasks, but it does not explore alternative intrinsic reward mechanisms or their potential benefits.
- What evidence would resolve it: Investigating other intrinsic reward mechanisms and their effects on different types of cybersecurity tasks would provide a more comprehensive understanding of their utility.

## Limitations
- The ICM implementation details and hyperparameters beyond those listed in Table 8 are not fully specified, which may impact reproducibility.
- The specific neural network architectures for actor and critic networks (layer sizes, activation functions) are not provided, potentially affecting performance comparisons.

## Confidence
- High confidence: Scaling up penalty magnitudes improves sample efficiency without degrading final performance.
- Medium confidence: Adding small positive rewards can slightly improve learning without causing gaming behavior.
- Low confidence: Intrinsic curiosity does not provide significant benefits in high-level network monitoring tasks.

## Next Checks
1. Implement a sensitivity analysis for penalty scaling factors beyond the tested magnitudes to determine optimal ranges and identify potential instability thresholds.
2. Test the positive reward addition technique across multiple network topologies and adversary types to assess generalizability beyond the CAGE 2 Challenge scenario.
3. Evaluate curiosity-driven exploration in modified versions of the CybORG environment with expanded observation spaces or dynamic network configurations to determine if the lack of benefit is task-specific.