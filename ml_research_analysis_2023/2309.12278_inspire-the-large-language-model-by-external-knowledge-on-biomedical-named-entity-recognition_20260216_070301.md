---
ver: rpa2
title: Inspire the Large Language Model by External Knowledge on BioMedical Named
  Entity Recognition
arxiv_id: '2309.12278'
source_url: https://arxiv.org/abs/2309.12278
tags:
- entity
- knowledge
- category
- gene
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of biomedical named entity
  recognition (BioNER) using large language models (LLMs). The authors propose a two-step
  approach: first extracting entity spans and then determining entity categories.'
---

# Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition

## Quick Facts
- arXiv ID: 2309.12278
- Source URL: https://arxiv.org/abs/2309.12278
- Reference count: 12
- Primary result: F1 scores of 72.72 for Species, 69.02 for Gene/Protein, and 30.89 for Chemical entities on CRAFT dataset

## Executive Summary
This paper addresses biomedical named entity recognition (BioNER) using large language models by decomposing the task into span extraction and category determination steps. The authors enhance the LLM's category determination by incorporating external knowledge from UMLS, achieving significant improvements over few-shot LLM baselines. Experiments on the CRAFT dataset demonstrate the effectiveness of knowledge infusion, with the best method achieving F1 scores of 72.72 for Species, 69.02 for Gene/Protein, and 30.89 for Chemical entities. The approach shows better robustness compared to methods using random retrieval or voting.

## Method Summary
The method employs a two-step approach: first extracting entity spans using an LLM, then determining entity categories by leveraging both contextual information and external knowledge from UMLS. For category determination, the LLM is provided with the entity span and retrieved knowledge pairs containing similar entities and their categories. The external knowledge is obtained by encoding entities using SapBERT and retrieving top-k most similar entities from UMLS. This approach addresses the LLM's lack of domain-specific knowledge while maintaining flexibility in handling diverse biomedical entities.

## Key Results
- F1 score of 72.72 for Species entity recognition
- F1 score of 69.02 for Gene/Protein entity recognition  
- F1 score of 30.89 for Chemical entity recognition
- Significant improvements over few-shot LLM baselines
- Better robustness compared to methods using random retrieval or voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting NER into entity span extraction and category determination improves performance by reducing the complexity of each sub-task.
- Mechanism: The two-step approach allows the LLM to focus on one task at a time, simplifying the overall problem. Span extraction can be treated as a generative task, while category determination can leverage both contextual information and external knowledge.
- Core assumption: Breaking down complex tasks into simpler sub-tasks improves LLM performance on those tasks.
- Evidence anchors:
  - [abstract]: "we leverage the LLM to solve the Biomedical NER step-by-step: break down the NER task into entity span extraction and entity type determination"
  - [section]: "we decompose the BNER process into two distinct steps. In the first step, we need to determine the span boundaries of each entity...In the second step, we once again employ ChatGPT to predict the category of each extracted entity"

### Mechanism 2
- Claim: Incorporating external domain knowledge from UMLS improves entity category determination accuracy.
- Mechanism: By retrieving similar entities and their categories from UMLS, the LLM can make more informed decisions about the category of the entity being classified. This addresses the LLM's lack of domain-specific knowledge.
- Core assumption: External knowledge bases contain relevant information that can improve LLM predictions in specialized domains.
- Evidence anchors:
  - [abstract]: "for entity type determination, we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category"
  - [section]: "we take a step further by integrating an external knowledge repository into this model...We input both the current entity spans and pertinent external knowledge together to the LLM"

### Mechanism 3
- Claim: Using LLM for category determination with external knowledge provides better robustness than simple voting methods.
- Mechanism: The LLM can consider contextual information and reason about the entity category, rather than relying solely on similarity matching. This allows it to handle ambiguous cases and variations in knowledge base quality.
- Core assumption: LLMs can effectively integrate contextual information with external knowledge to make better predictions than simple voting.
- Evidence anchors:
  - [abstract]: "the incorporation of external knowledge significantly enhances entity category determination performance"
  - [section]: "Unlike the 'vote' method, which relies solely on entity similarity for classification...Using the GPT method in classification also allows for the simultaneous consideration of contextual information"

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: The two-step approach is inspired by the chain-of-thought method, which breaks down complex tasks into simpler sub-tasks.
  - Quick check question: How does chain-of-thought prompting improve LLM performance on complex tasks?

- Concept: Few-shot learning
  - Why needed here: The approach uses in-context learning with a small number of examples to guide the LLM's predictions.
  - Quick check question: What is the difference between few-shot learning and traditional supervised learning?

- Concept: Knowledge graphs and entity linking
  - Why needed here: The approach uses UMLS, a biomedical knowledge graph, to retrieve relevant entities and their categories.
  - Quick check question: How do knowledge graphs represent relationships between entities?

## Architecture Onboarding

- Component map:
  - Input text → Span extraction module → Entity spans
  - Entity spans + UMLS knowledge base → Category determination module → Entity categories
  - Entity spans + Entity categories → Output

- Critical path:
  - Text input → Span extraction → Category determination → Final output

- Design tradeoffs:
  - Two-step approach vs. end-to-end approach: Two-step may be more accurate but less efficient
  - External knowledge integration: Improves accuracy but adds complexity and potential for errors
  - LLM-based approach vs. traditional NER models: More flexible but potentially less reliable

- Failure signatures:
  - Poor span extraction leading to incorrect entity boundaries
  - Irrelevant or inaccurate external knowledge retrieved from UMLS
  - LLM misinterpreting the prompt or failing to integrate contextual information effectively

- First 3 experiments:
  1. Compare performance of two-step approach vs. end-to-end approach on a small dataset
  2. Evaluate the impact of different amounts of external knowledge (e.g., 50k vs. 500k entries) on category determination accuracy
  3. Test the robustness of the approach by introducing noise or errors in the external knowledge base

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed two-step BioNER approach scale with the size of the external knowledge base?
- Basis in paper: [explicit] The authors mention that the GPT-based method is less sensitive to the size of the external knowledge base compared to the direct vote method, with a 0.62 drop in overall performance even when the knowledge base quantity is reduced by a factor of 10.
- Why unresolved: While the paper provides some insights into the impact of knowledge base size on performance, it does not offer a comprehensive analysis of how performance scales with different knowledge base sizes or what the optimal size might be for different entity categories.
- What evidence would resolve it: A detailed study examining the performance of the proposed approach with various sizes of external knowledge bases, particularly focusing on different entity categories and their specific requirements.

### Open Question 2
- Question: How does the proposed BioNER approach handle ambiguous entities that could belong to multiple categories?
- Basis in paper: [inferred] The authors mention that LLM's strong generalization capabilities allow it to effectively overcome the challenge of diverse test datasets and potential misclassification due to variations in data categories, distribution, and granularity in the external knowledge base.
- Why unresolved: The paper does not provide specific examples or a detailed explanation of how the proposed approach handles ambiguous entities, such as "huntingtin," which could be both a protein and a disease name.
- What evidence would resolve it: Case studies or examples demonstrating how the proposed approach handles ambiguous entities, including a comparison with other methods that do not incorporate external knowledge.

### Open Question 3
- Question: How does the proposed BioNER approach perform on datasets with more diverse entity categories or different biomedical domains?
- Basis in paper: [explicit] The authors mention that the CRAFT dataset consists of three distinct entity categories: Species, Chemical, and Gene/Protein, and they evaluate their approach on this dataset.
- Why unresolved: The paper does not explore the performance of the proposed approach on datasets with a larger number of entity categories or different biomedical domains, which could provide insights into its generalizability and robustness.
- What evidence would resolve it: Experiments evaluating the proposed approach on datasets with more diverse entity categories or different biomedical domains, comparing its performance with other state-of-the-art methods in these settings.

## Limitations

- The paper doesn't provide error analysis showing how span extraction errors propagate to category determination
- No evaluation of external knowledge retrieval quality or relevance metrics
- Limited testing of robustness under adversarial conditions or varying knowledge base quality

## Confidence

**High Confidence**: The two-step architectural approach (span extraction followed by category determination) is technically sound and well-supported by the experimental results. The F1 scores and comparison with baseline methods are clearly reported.

**Medium Confidence**: The claim that external knowledge significantly enhances performance is supported by the results, but the mechanism by which knowledge improves predictions isn't thoroughly validated. The paper shows improved metrics but doesn't demonstrate that the LLM is actually using the external knowledge rather than relying on its own reasoning.

**Low Confidence**: The robustness claims are weakly supported. While the paper states that knowledge infusion provides better robustness, it doesn't test the approach under adversarial conditions, varying knowledge base quality, or with different types of external knowledge sources.

## Next Checks

1. **Error Propagation Analysis**: Track how span extraction errors propagate to category determination accuracy by comparing cases where span extraction succeeds vs. fails, and measuring the downstream impact on final entity recognition quality.

2. **Knowledge Relevance Verification**: Evaluate the quality of retrieved UMLS knowledge by computing precision@k for the knowledge retrieval step and measuring correlation between knowledge relevance scores and category prediction accuracy.

3. **Ablation on External Knowledge**: Systematically remove the external knowledge component and compare performance to a baseline that uses only contextual information from the LLM, controlling for all other variables including prompt structure and example selection.