---
ver: rpa2
title: 'Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets
  Via In-Context Learning'
arxiv_id: '2303.16445'
source_url: https://arxiv.org/abs/2303.16445
tags:
- gpt3
- datasets
- negation
- dataset
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of small psycholinguistic probing\
  \ datasets limiting the reliability of language model capability assessments. The\
  \ authors extend two small datasets\u2014NEG-136-SIMP (negation) and ROLE-88 (role\
  \ reversal)\u2014from 18 and 44 sentence pairs to approximately 1500 each, using\
  \ GPT3 with human-in-the-loop filtering."
---

# Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning

## Quick Facts
- **arXiv ID**: 2303.16445
- **Source URL**: https://arxiv.org/abs/2303.16445
- **Reference count**: 17
- **Primary result**: Extended psycholinguistic datasets reveal significant performance drops (20-57%) in language models compared to original small datasets, suggesting prior evaluations may have overestimated model capabilities.

## Executive Summary
This paper addresses the reliability limitations of psycholinguistic probing datasets by extending two small benchmarks—NEG-136-SIMP (negation) and ROLE-88 (role reversal)—from 18 and 44 sentence pairs to approximately 1500 each. Using GPT3 with human-in-the-loop filtering, the authors generate NEG-1500-SIMP-GEN (750 pairs), ROLE-1500 (1500 sentences), and a template-based negation set NEG-1500-SIMP-TEMP (770 pairs). Evaluating 22 language models on these extended datasets reveals substantial performance drops compared to the originals, with increased negation sensitivity in models like BERT and ALBERT. Notably, while GPT3 generated ROLE-1500, it only solved 24.6% of the examples, indicating a capability gap between data generation and task solving.

## Method Summary
The authors extended small psycholinguistic datasets using two methods: GPT3 with human-in-the-loop filtering (rejecting 72% of attempts) and template-based generation. They evaluated 22 models (BERT, RoBERTa, ALBERT, T5, GPT3) using zero-shot masked language modeling, measuring top-5 prediction accuracy and negation sensitivity (percentage of sentence pairs where top-1 prediction changes when "not" is added). The evaluation required converting masked language modeling tasks to causal language modeling for GPT models. Manual filtering was the bottleneck in the process due to high rejection rates of generated examples.

## Key Results
- Model performance decreased by 40-50% on negation and 20-30% on role reversal when evaluated on extended datasets versus original small benchmarks
- BERT and ALBERT showed 36.8% and 21-25% increased negation sensitivity on the extended NEG-1500-SIMP-TEMP dataset
- GPT3 generated ROLE-1500 but only solved 24.6% of the examples, revealing a capability gap between generation and solving
- Template-based generation produced more "straightforward" examples than original data, potentially affecting difficulty calibration

## Why This Works (Mechanism)

### Mechanism 1
Small psycholinguistic datasets can overestimate model performance due to limited statistical power. When evaluation datasets are small, random performance variations can dominate, making models appear to perform better than they actually do. Larger datasets provide more reliable estimates by averaging out these random effects. Core assumption: The performance drop observed on larger datasets reflects true model limitations rather than artifacts of the extension process.

### Mechanism 2
GPT3 can generate valid psycholinguistic examples but cannot reliably solve them. GPT3 learns the syntactic pattern of the task during generation but lacks the deeper semantic understanding needed for accurate prediction. This creates a capability gap between generation and solving. Core assumption: The generated examples are valid representations of the psycholinguistic phenomena being tested.

### Mechanism 3
Extended datasets reveal higher negation sensitivity in models compared to original datasets. Larger datasets provide more varied and challenging negation examples, exposing model weaknesses that small datasets miss. This leads to more accurate measurements of negation sensitivity. Core assumption: The increased sensitivity observed on extended datasets reflects true model behavior rather than artifacts of the extension process.

## Foundational Learning

- **Zero-shot probing methodology**: Why needed here: The evaluation method relies on models predicting masked words without fine-tuning, requiring understanding of how zero-shot evaluation differs from standard supervised learning. Quick check question: What distinguishes zero-shot probing from traditional fine-tuning approaches in terms of what the model has learned?

- **Psycholinguistic feature encoding**: Why needed here: Understanding how linguistic phenomena like negation and role reversal are represented in model embeddings is crucial for interpreting the results. Quick check question: How do negation and role reversal differ in terms of the linguistic features they test, and why might models handle them differently?

- **Dataset extension techniques**: Why needed here: The paper uses both template-based and GPT3 generation methods, requiring understanding of the trade-offs between these approaches. Quick check question: What are the key differences between template-based and GPT3 generation for dataset extension, and how might each affect the quality and diversity of the resulting data?

## Architecture Onboarding

- **Component map**: Dataset generation (template-based and GPT3) -> Manual cleaning/filtering -> Evaluation pipeline (zero-shot probing) -> Analysis of results including sensitivity metrics
- **Critical path**: Generation → Cleaning → Evaluation → Analysis, with manual cleaning being the bottleneck due to high rejection rates
- **Design tradeoffs**: Using GPT3 for generation provides more diverse examples but requires expensive manual filtering; template-based generation is cheaper but less diverse
- **Failure signatures**: High duplicate rates in GPT3 generation, semantically incorrect examples passing initial filters, performance drops that might reflect extension artifacts rather than true model limitations
- **First 3 experiments**:
  1. Generate a small subset of negation examples using both methods and compare diversity metrics before committing to full dataset creation
  2. Test the sensitivity of different models on a balanced subset of original and extended data to verify the magnitude of performance changes
  3. Create a diagnostic tool to automatically detect semantically incorrect examples to reduce manual filtering burden

## Open Questions the Paper Calls Out

### Open Question 1
How does model performance on negation tasks scale with dataset size beyond 1500 examples? Basis in paper: The authors observed a significant drop in accuracy when extending NEG-136-SIMP from 18 to 750 examples, but did not test even larger datasets. Why unresolved: The study only extended datasets to approximately 1500 examples, leaving the relationship between dataset size and model performance on negation tasks unexplored at larger scales. What evidence would resolve it: Testing models on negation datasets containing 5,000-10,000 examples and comparing performance trends to the 1500-example benchmark.

### Open Question 2
Does GPT3's ability to generate valid training examples generalize to other linguistic phenomena beyond negation and role reversal? Basis in paper: GPT3 successfully generated ROLE-1500 despite only solving 24.6% of the examples, demonstrating a gap between generation and solving capabilities. Why unresolved: The study only tested GPT3's generation capabilities on negation and role reversal tasks, not exploring whether this pattern holds for other linguistic phenomena. What evidence would resolve it: Testing GPT3's ability to generate valid examples for other psycholinguistic phenomena (e.g., anaphora resolution, semantic role labeling) and comparing generation success rates to solving performance.

### Open Question 3
How does the template-based generation method compare to GPT3-based generation for other psycholinguistic datasets in terms of quality and diversity? Basis in paper: The authors created both template-based (NEG-1500-SIMP-TEMP) and GPT3-based (NEG-1500-SIMP-GEN) negation datasets, but only evaluated one of them on the full model suite. Why unresolved: Only NEG-1500-SIMP-TEMP was evaluated on all 22 models, while NEG-1500-SIMP-GEN was only compared to the original NEG-136-SIMP dataset. What evidence would resolve it: Evaluating all 22 models on both NEG-1500-SIMP-TEMP and NEG-1500-SIMP-GEN to directly compare the quality and diversity of template-based versus GPT3-based generation methods.

## Limitations
- Extended datasets show performance drops of 20-57%, but it remains unclear whether these reflect true model limitations or artifacts of the extension process
- Manual filtering of GPT3-generated examples rejected 72% of attempts, raising questions about potential systematic biases introduced during this process
- The findings may not extend to other linguistic features or model architectures beyond the two tested phenomena

## Confidence
- **High Confidence**: The core finding that small datasets can overestimate model performance is well-supported by substantial performance drops across multiple models and datasets
- **Medium Confidence**: The claim about increased negation sensitivity in models like BERT and ALBERT is supported by the data, but the interpretation that previous findings were "skewed" requires caution
- **Low Confidence**: The assertion that GPT3's inability to solve its own generated examples represents a fundamental capability gap is based on limited evidence

## Next Checks
1. **Dataset Bias Analysis**: Conduct a systematic comparison of linguistic complexity, vocabulary diversity, and semantic patterns between original and extended datasets to quantify potential biases introduced during the extension process
2. **Cross-Validation with Independent Extensions**: Have independent researchers create alternative extensions of NEG-136-SIMP and ROLE-88 using different generation methods or prompts, then compare performance patterns across all versions
3. **Error Analysis on Generated Examples**: Perform detailed qualitative analysis of GPT3-generated examples that were rejected during filtering, as well as those that passed but showed the capability gap