---
ver: rpa2
title: 'Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations'
arxiv_id: '2308.11995'
source_url: https://arxiv.org/abs/2308.11995
tags:
- conversation
- knowledge
- conversations
- turker
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Topical-Chat is a knowledge-grounded human-human conversation dataset
  that covers 8 broad topics and includes natural, engaging conversations without
  explicitly defined roles. The dataset contains over 11K conversations with associated
  knowledge sources, including articles and facts.
---

# Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations

## Quick Facts
- arXiv ID: 2308.11995
- Source URL: https://arxiv.org/abs/2308.11995
- Reference count: 0
- Key outcome: Knowledge-grounded models outperform non-knowledge models on automated and human evaluation metrics

## Executive Summary
Topical-Chat introduces a large-scale knowledge-grounded conversation dataset with over 11K human-human dialogues spanning 8 topics. The dataset uniquely pairs conversation turns with associated knowledge sources including articles and facts, enabling training of models that can generate informative, on-topic responses. The authors train several Transformer-based models and demonstrate that incorporating knowledge significantly improves both automated metrics (perplexity, F1 score) and human evaluation scores for comprehensibility, on-topicness, and knowledge utilization.

## Method Summary
The paper trains encoder-decoder conversational models on the Topical-Chat dataset using a Transformer architecture. Models are evaluated with automated metrics (perplexity, F1 score, n-gram diversity) and human evaluation across five dimensions. Knowledge is incorporated through reading sets associated with conversation turns, with knowledge selection during training using an argmax oracle approach. The best-performing model combines pre-training on BookCorpus with knowledge integration, achieving the lowest perplexity and highest F1 scores.

## Key Results
- Models incorporating knowledge significantly outperform non-knowledge models on both automated metrics and human evaluation
- Pre-training on BookCorpus provides additional performance improvements when combined with knowledge integration
- Human evaluation shows knowledge-grounded responses are more comprehensible, on-topic, and demonstrate better knowledge utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge-grounded structure forces models to incorporate relevant facts into responses, improving informativeness
- Mechanism: Models learn to attend to and leverage knowledge when generating responses, as shown by higher F1 scores when knowledge is included
- Core assumption: Knowledge sentences are relevant and retrievable given conversation context
- Evidence anchors: Abstract states knowledge-incorporating models outperformed non-knowledge models; paper acknowledges knowledge selection as an open problem
- Break condition: Knowledge becomes irrelevant to conversation topic or too difficult to retrieve accurately

### Mechanism 2
- Claim: Asymmetric reading set configurations create natural teacher-student dynamics that improve conversation depth
- Mechanism: Information asymmetry forces one partner to explain concepts to the other, creating richer dialogue
- Core assumption: Information asymmetry leads to more engaging conversations than symmetric knowledge
- Evidence anchors: Paper leverages information asymmetry to create dual teacher-participant roles; reading sets can be symmetric or asymmetric
- Break condition: Partners become frustrated with knowledge gaps or asymmetry becomes too extreme

### Mechanism 3
- Claim: Pre-training on BookCorpus improves model performance on knowledge-grounded tasks
- Mechanism: General language understanding from pre-training transfers to better handling of knowledge integration and response generation
- Core assumption: Language modeling on general text provides useful representations for domain-specific tasks
- Evidence anchors: Table 3 shows TF (w/ p.t.) achieves lower perplexity than TF without pre-training
- Break condition: Pre-training domain becomes too different from conversation domain, causing negative transfer

## Foundational Learning

- Concept: Attention mechanisms in Transformers
  - Why needed here: Model uses attention to select relevant knowledge sentences from reading set
  - Quick check question: How does multi-head attention help the model focus on different aspects of conversation and knowledge?

- Concept: Sequence-to-sequence modeling
  - Why needed here: Task requires generating responses conditioned on conversation history and knowledge
  - Quick check question: What challenges arise when decoding sequences that need to incorporate external knowledge?

- Concept: TF-IDF for knowledge selection
  - Why needed here: Paper uses TF-IDF vectors to select most relevant knowledge sentence for response generation
  - Quick check question: Why might TF-IDF be insufficient for selecting knowledge in conversational contexts?

## Architecture Onboarding

- Component map: Conversation history → knowledge selection → knowledge encoding → response decoding
- Critical path: Conversation history → knowledge selection → knowledge encoding → response decoding
- Design tradeoffs: Knowledge selection using argmax oracle is simple but requires ground truth; using conversation context alone for selection is more realistic but harder
- Failure signatures: Low knowledge utilization scores in human evaluation, generic responses, inability to maintain topic coherence
- First 3 experiments:
  1. Compare model performance with and without knowledge to establish baseline impact
  2. Test different WH (conversation history window) sizes to optimize context utilization
  3. Evaluate knowledge selection methods (oracle vs. context-based) to understand selection impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the knowledge model handle cases where multiple knowledge sentences are equally relevant to conversation context?
- Basis in paper: Paper mentions knowledge selection is an open problem and currently uses argmax oracle approach
- Why unresolved: Current approach relies on ground-truth responses for knowledge selection, which is not available during inference
- What evidence would resolve it: Experiments comparing different knowledge selection strategies on automated metrics and human evaluation scores

### Open Question 2
- Question: What is the impact of increasing number of knowledge sources beyond current three entities and associated Wikipedia articles?
- Basis in paper: Paper uses knowledge base composed of entities, facts, and articles but does not explore effect of increasing knowledge sources
- Why unresolved: Paper does not discuss potential benefits or drawbacks of expanding knowledge base beyond current setup
- What evidence would resolve it: Experiments comparing performance of models trained on different knowledge base sizes and compositions

### Open Question 3
- Question: How does model handle cases where knowledge provided is factually incorrect or outdated?
- Basis in paper: Paper does not address issue of incorrect or outdated knowledge in provided reading sets
- Why unresolved: Paper does not discuss mechanisms for verifying accuracy of knowledge provided or handling incorrect/outdated knowledge
- What evidence would resolve it: Experiments evaluating model's performance on knowledge sources with known inaccuracies compared to control group with accurate knowledge

## Limitations
- Dataset construction relies on crowdworkers with varying knowledge expertise, potentially introducing inconsistencies in conversation quality
- Knowledge selection mechanism during training requires ground-truth responses, making it impractical for real-world deployment
- Human evaluation process may be subject to annotator bias given subjective nature of metrics like "interestingness"

## Confidence
- High Confidence: Knowledge-grounded models outperform non-knowledge models (well-supported by both automated metrics and human evaluation)
- Medium Confidence: Pre-training on BookCorpus improves performance (incremental improvement without ablation studies)
- Medium Confidence: Asymmetric reading sets create better teacher-student dynamics (supported by methodology but lacks direct empirical validation)

## Next Checks
1. Implement and evaluate knowledge selection methods that do not require ground-truth responses, then compare knowledge utilization scores against oracle-based approach
2. Create parallel conversations using symmetric knowledge configurations and conduct controlled human evaluation comparing asymmetric vs. symmetric conversations
3. Test best-performing model on different knowledge-grounded conversation dataset to assess generalization beyond Topical-Chat domain