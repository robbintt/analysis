---
ver: rpa2
title: 'Masking meets Supervision: A Strong Learning Alliance'
arxiv_id: '2306.11339'
source_url: https://arxiv.org/abs/2306.11339
tags:
- training
- augmask
- augsub
- performance
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method called AugSub to introduce additional
  regularization into supervised learning without destabilizing training. AugSub uses
  a main model trained with standard recipes and a sub-model that receives additional
  regularization, guided by a relaxed loss function similar to self-distillation.
---

# Masking meets Supervision: A Strong Learning Alliance

## Quick Facts
- arXiv ID: 2306.11339
- Source URL: https://arxiv.org/abs/2306.11339
- Reference count: 40
- Primary result: AugSub introduces additional regularization without destabilizing training, improving performance across various scenarios.

## Executive Summary
AugSub is a novel method that introduces additional regularization into supervised learning by training a main model with standard recipes alongside a sub-model that receives drop-based augmentation. The sub-model is guided by a relaxed loss function using the main model's output as a softer target, similar to self-distillation. Three variants—AugDrop, AugPath, and AugMask—are proposed, with AugMask showing the most significant gains. Experiments demonstrate consistent improvements across DeiT-III, ResNet, MAE finetuning, CLIP finetuning, and Swin Transformer architectures.

## Method Summary
AugSub uses a dual-model architecture where a main model is trained normally while a sub-model receives additional regularization via drop-based augmentation. The sub-model is trained using a relaxed loss that treats the main model's softmax output as the target, reducing the hardness of supervision compared to one-hot labels. This approach mitigates instability from aggressive augmentation while preserving the main model's convergence speed. AugMask, which applies random masking to the sub-model, demonstrates the strongest performance gains across experiments.

## Key Results
- AugMask improves ImageNet top-1 accuracy by 0.3-1.5% across various architectures including DeiT-III and ResNet.
- Transfer learning performance on CIFAR-10/100, Oxford Flowers-102, Stanford Cars, and iNaturalist shows consistent gains with AugSub.
- MAE finetuning benefits from AugMask with 50% masking, achieving better performance than standard recipes.

## Why This Works (Mechanism)

### Mechanism 1
AugSub mitigates instability from additional regularization by using the main model's relaxed output as a softer target for the sub-model. The main model trains without augmentation drops, preserving convergence speed, while the sub-model learns from this stable signal under drop-based augmentation.

### Mechanism 2
The main model preserves convergence speed by training without drop-based regularization, while the sub-model benefits from this stable signal. This separation ensures that the main model's gradients remain stable and fast-converging, while the sub-model receives additional regularization without destabilizing the overall training process.

### Mechanism 3
AugSub's prioritized loss mechanism allows adaptive training difficulty—early on, the main model leads; later, the sub-model contributes more. The sub-model's gradient magnitude grows adaptively with training progress, ensuring balanced contribution without destabilizing the main model.

## Foundational Learning

- **Self-distillation**: AugSub uses the main model's output as a soft target for the sub-model, analogous to self-distillation where a model is trained on its own output.
  - Why needed here: The relaxed target (softmax output) is smoother and less noisy than one-hot labels under drop-based augmentation.
  - Quick check question: In self-distillation, why might a softened target (softmax output) be preferable to a hard one-hot label during difficult training conditions?

- **Drop-based regularization (Dropout, DropPath, Random Masking)**: AugSub integrates three types of drop-based techniques to regularize the sub-model without harming the main model.
  - Why needed here: These techniques introduce additional regularization while the main model maintains stable training.
  - Quick check question: How does DropPath differ from Dropout in terms of what is dropped during training?

- **Cross-entropy with softmax vs. binary cross-entropy**: AugSub supports both classification settings by switching between softmax (multi-class) and sigmoid (binary) activations.
  - Why needed here: The method must handle both standard classification and multi-label scenarios.
  - Quick check question: When would you use sigmoid with binary cross-entropy instead of softmax with cross-entropy in a multi-label setting?

## Architecture Onboarding

- **Component map**: Main model (no drops) -> Standard cross-entropy loss -> Sub-model (with drops) -> Relaxed loss (main model output as target) -> Averaged loss backprop
- **Critical path**: 1) Forward pass through main model (no drops), 2) Forward pass through sub-model (with drops), 3) Compute main loss (CE with ground truth), 4) Compute sub loss (CE with main model's softmax output, stop-grad), 5) Backpropagate averaged loss
- **Design tradeoffs**: Computational cost doubles forward/backward passes; masking can reduce this via MAE-style skipping; regularization strength controlled by drop probability; main model stability preserved but sub-model must be carefully initialized
- **Failure signatures**: Main model loss diverges (sub-model gradients too large), sub-model loss plateaus (drop probability too low), overall accuracy drops (regularization too aggressive)
- **First 3 experiments**: 1) Implement AugMask with 50% masking on ViT-S for 100 epochs on ImageNet; compare training loss curves, 2) Vary masking ratio (25%, 50%, 75%) and measure impact on convergence and accuracy, 3) Swap in AugDrop (0.3 dropout) and AugPath (base + 0.2 drop-path) to compare regularization effects

## Open Questions the Paper Calls Out
- How does AugSub perform on architectures beyond hierarchical ones like ConvNeXt or EfficientNet?
- What is the optimal drop probability for AugDrop and AugPath across different model sizes and training durations?
- How does AugSub interact with other regularization techniques like MixUp, CutMix, or RandAugment?
- Can AugSub be effectively extended to tasks beyond image classification, such as object detection, instance segmentation, or video understanding?

## Limitations
- Lacks ablation studies on critical hyperparameters (masking ratio, loss weighting) across different architectures and tasks
- No analysis of computational overhead introduced by the dual-model architecture
- Claims about adaptive gradient contribution are supported only by qualitative observations without statistical tests

## Confidence
- **High**: Core claim that AugSub improves performance over baseline training is supported by experimental results across multiple architectures and tasks
- **Medium**: Mechanism that main model's relaxed output acts as stable soft target for sub-model is plausible but not rigorously proven
- **Low**: Adaptive gradient contribution claim is least substantiated, based on single figure without statistical analysis

## Next Checks
1. Run ablation experiments with AugMask using masking ratios of 25%, 50%, and 75% on DeiT-III ViT-B/16, measuring convergence speed, final accuracy, and training loss curves
2. Systematically vary weighting between main and sub-model losses (0.7:0.3, 0.5:0.5, 0.3:0.7) and evaluate effect on training stability and final performance across architectures
3. Intentionally misalign main model's output from ground truth (e.g., by adding noise) and observe how sub-model's performance degrades, quantifying risk of learning from noisy soft target