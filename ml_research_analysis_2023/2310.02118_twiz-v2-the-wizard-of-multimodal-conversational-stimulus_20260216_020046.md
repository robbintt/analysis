---
ver: rpa2
title: 'TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus'
arxiv_id: '2310.02118'
source_url: https://arxiv.org/abs/2310.02118
tags:
- user
- task
- step
- https
- twiz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the vision, challenges, and scientific contributions
  of the TWIZ team in the Alexa Prize TaskBot Challenge 2022. The team aimed to build
  TWIZ as a helpful, multimodal, knowledgeable, and engaging assistant that can guide
  users towards the successful completion of complex manual tasks.
---

# TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus

## Quick Facts
- arXiv ID: 2310.02118
- Source URL: https://arxiv.org/abs/2310.02118
- Reference count: 40
- TWIZ is an effective multimodal conversational assistant for complex manual tasks with innovative features like creative cooking and video navigation

## Executive Summary
TWIZ is a multimodal conversational assistant developed by the TWIZ team for the Alexa Prize TaskBot Challenge 2022. The system aims to guide users through complex manual tasks while providing engaging multimodal stimuli including generated images, video navigation, and contextual curiosities. The bot leverages a custom TWIZ-LLM trained on augmented conversational data and employs innovative techniques like zero-shot intent detection to handle unseen user requests without retraining.

## Method Summary
TWIZ employs a three-pronged approach: (1) Humanly-Shaped Conversations using a custom TWIZ-LLM trained on synthetic dialogues augmented from real interaction patterns, (2) Multimodal Stimulus featuring text-to-image generation, video moment retrieval via CLIP embeddings, and curiosity generation with Wikipedia grounding, and (3) Zero-shot Conversational Flows that cast dialogue state tracking as a reading comprehension problem to handle unseen intents. The system uses AWS Lambda and OpenSearch for deployment and integrates with the CoBot framework for conversation management.

## Key Results
- TWIZ successfully guides users through complex manual tasks with high effectiveness and robustness
- The multimodal features (images, videos, curiosities) provide engaging stimuli throughout task completion
- Zero-shot intent detection enables handling of unseen user requests without model retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot intent detection using LLMs handles unseen user requests without retraining
- **Mechanism:** Casts dialogue state tracking as reading comprehension by generating questions for each slot and intent
- **Core assumption:** LLM can generate meaningful questions aligned with task domain semantics
- **Evidence anchors:** [abstract] "zero-shot intent detection method", [section 5] "cast the typical zero-shot DST task as a reading comprehension one"
- **Break condition:** LLM-generated questions are ambiguous or poorly aligned with task domain

### Mechanism 2
- **Claim:** Multimodal stimulus increases user engagement and task completion rates
- **Mechanism:** Generates contextual images, retrieves relevant video moments, creates fun facts using GPT-4
- **Core assumption:** Users find generated multimodal content more engaging than text-only responses
- **Evidence anchors:** [abstract] "innovative features such as creative cooking, video navigation through voice", [section 4.2.2] "consider the captions of previously generated images"
- **Break condition:** Generated content is irrelevant or hallucinated

### Mechanism 3
- **Claim:** TWIZ-LLM provides natural and robust task execution responses
- **Mechanism:** Trains Vicuna and OPT models on synthetic dialogues generated from extracted user patterns
- **Core assumption:** Augmented data represents real user behavior patterns and is diverse enough to generalize
- **Evidence anchors:** [abstract] "robust TWIZ-LLM, a Large Language Model trained for dialoguing about complex manual tasks", [section 3.4.1] "dialogue-generation pipeline that leverages data gathered during our participation"
- **Break condition:** Synthetic data distribution differs significantly from real user interactions

## Foundational Learning

- **Concept:** Dialogue state tracking as reading comprehension
  - Why needed here: Traditional DST requires labeled data for each new intent/slot, but TWIZ needs to support new features quickly
  - Quick check question: How does converting DST to a reading comprehension task enable zero-shot learning?

- **Concept:** Multimodal retrieval using CLIP embeddings
  - Why needed here: TWIZ needs to find relevant video moments based on user voice queries in real-time
  - Quick check question: What advantage does combining text and image embeddings provide for video moment retrieval?

- **Concept:** Text-to-image generation with visual consistency
  - Why needed here: TWIZ must generate task illustrations that are both relevant to the task and consistent across steps
  - Quick check question: Why is maintaining visual consistency across recipe steps important for user guidance?

## Architecture Onboarding

- **Component map:** User interface (APL templates) → Intent detection → Dialogue manager → TWIZ-LLM → Multimodal modules (image generation, video retrieval, curiosities)
- **Critical path:** User utterance → Intent detection → Dialogue state tracking → TWIZ-LLM response generation → Multimodal stimulus selection → User interface rendering
- **Design tradeoffs:** Real-time performance vs. generation quality; Model size vs. capability; Synthetic data vs. real data
- **Failure signatures:** Intent detection failures → User stuck in fallback state; LLM generation failures → Nonsensical responses; Image generation failures → Default images shown
- **First 3 experiments:** 1) Test intent detection with unseen requests using zero-shot method; 2) Verify multimodal stimulus generation with sample tasks; 3) Validate TWIZ-LLM responses for ingredient replacement and curiosity requests

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does TWIZ-LLM handle ambiguous or context-dependent user requests not covered by training data?
- **Basis in paper:** [explicit] Paper discusses TWIZ-LLM's ability to respond to various intents but doesn't address ambiguous requests
- **Why unresolved:** Focuses on specific intents without covering complex or nuanced interactions
- **What evidence would resolve it:** Detailed analysis of TWIZ-LLM's performance on ambiguous requests with quantitative metrics and qualitative examples

### Open Question 2
- **Question:** Impact of zero-shot intent detection on user experience compared to traditional methods?
- **Basis in paper:** [explicit] Mentions zero-shot method but lacks detailed comparison with traditional approaches
- **Why unresolved:** Introduces method without evaluating effectiveness or comparing to existing approaches
- **What evidence would resolve it:** User study comparing zero-shot vs traditional methods measuring accuracy and satisfaction

### Open Question 3
- **Question:** Performance of Creative Cooking feature in terms of engagement and scaling challenges?
- **Basis in paper:** [explicit] Introduces Creative Cooking but provides no data on user engagement or scaling challenges
- **Why unresolved:** Presents feature without evaluating effectiveness or discussing scaling issues
- **What evidence would resolve it:** User study measuring engagement with Creative Cooking and analysis of scaling challenges across recipes and cuisines

## Limitations

- Zero-shot intent detection relies heavily on LLM-generated questions that may not generalize to all task domains
- Multimodal stimulus generation could produce irrelevant or hallucinated content that degrades user experience
- TWIZ-LLM training on synthetic data may not capture full complexity of real user interactions, particularly edge cases

## Confidence

- **High confidence:** Overall system architecture and core functionality as a multimodal task assistant
- **Medium confidence:** Effectiveness of zero-shot intent detection method and quality of generated multimodal stimuli
- **Low confidence:** Robustness of TWIZ-LLM in handling diverse real-world scenarios based solely on synthetic training data

## Next Checks

1. Test zero-shot intent detection with 50+ unseen user requests across different task domains to measure accuracy and identify failure patterns
2. Conduct user study with 30 participants to evaluate relevance and helpfulness of generated images, video moments, and curiosities across 10 different tasks
3. Compare TWIZ-LLM performance on synthetic vs. real user interaction data by handling 100 recorded real user sessions and measuring response quality and task completion success rates