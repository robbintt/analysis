---
ver: rpa2
title: Distributional Offline Policy Evaluation with Predictive Error Guarantees
arxiv_id: '2302.09456'
source_url: https://arxiv.org/abs/2302.09456
tags:
- learning
- have
- distribution
- which
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles distributional offline policy evaluation (OPE),
  where the goal is to estimate the distribution of a policy's return from an offline
  dataset not generated by that policy. The authors propose Fitted Likelihood Estimation
  (FLE), a simple algorithm that iteratively applies Maximum Likelihood Estimation
  (MLE) to fit a sequence of conditional distributions approximating the distributional
  Bellman operator.
---

# Distributional Offline Policy Evaluation with Predictive Error Guarantees

## Quick Facts
- arXiv ID: 2302.09456
- Source URL: https://arxiv.org/abs/2302.09456
- Reference count: 40
- Primary result: FLE learns return distributions close to ground truth under TV/Wasserstein distance, outperforming categorical and quantile regression methods

## Executive Summary
This paper addresses distributional offline policy evaluation (OPE), where the goal is to estimate the distribution of a policy's return from offline data not generated by that policy. The authors propose Fitted Likelihood Estimation (FLE), a simple algorithm that iteratively applies Maximum Likelihood Estimation (MLE) to fit a sequence of conditional distributions approximating the distributional Bellman operator. FLE is flexible, supporting both finite-horizon and infinite-horizon discounted settings, and handles multi-dimensional rewards by leveraging any probabilistic generative model trainable via MLE. Theoretically, FLE learns distributions close to the ground truth under total variation distance (finite-horizon) and p-Wasserstein distance (infinite-horizon), assuming the offline data covers the policy's distribution and MLE succeeds.

## Method Summary
FLE decomposes distributional OPE into a sequence of MLE problems, enabling the use of any probabilistic generative model trainable via MLE. Each iteration fits a conditional distribution to approximate the target distribution constructed from the previous iteration's estimate, effectively propagating distributional Bellman updates through supervised learning. The algorithm takes as input offline data, a test policy, and a function class, and outputs an estimated return distribution. For finite-horizon settings, FLE uses total variation distance, while for infinite-horizon discounted settings, it leverages Wasserstein distance contraction properties. The method supports both finite-horizon and infinite-horizon discounted settings and handles multi-dimensional rewards through the flexibility of the underlying generative models.

## Key Results
- FLE achieves theoretical guarantees learning distributions close to ground truth under TV distance (finite horizon) and p-Wasserstein distance (infinite horizon)
- Outperforms categorical algorithms and quantile regression TD on combination lock MDPs with Gaussian mixture and diffusion models
- Demonstrates effectiveness particularly for complex multi-dimensional reward distributions
- Theoretical analysis shows Bellman operator contraction properties under Wasserstein distance enable convergence in infinite-horizon settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FLE decomposes distributional OPE into a sequence of MLE problems, enabling the use of any probabilistic generative model trainable via MLE.
- Mechanism: Each iteration fits a conditional distribution to approximate the target distribution constructed from the previous iteration's estimate, effectively propagating distributional Bellman updates through supervised learning.
- Core assumption: MLE can achieve good in-distribution generalization bounds (i.e., supervised learning succeeds).
- Evidence anchors:
  - [abstract]: "FLE, which conducts a sequence of Maximum Likelihood Estimation (MLE) problems and has the flexibility of integrating any state-of-art probabilistic generative models as long as it can be trained via MLE."
  - [section]: "Thus our approach is truly a reduction to supervised learning: as long as the supervised learning procedure (in this case, MLE) learns a model with good in-distribution generalization performance, we can guarantee good prediction performance for FLE."
  - [corpus]: Weak evidence. No corpus papers directly support this mechanism; requires verification against statistical learning theory literature.

### Mechanism 2
- Claim: The distributional Bellman operator's contraction properties under Wasserstein distance enable convergence guarantees in infinite-horizon settings.
- Mechanism: For discounted MDPs, the Bellman operator is γ^(1-1/(2p))-contractive under average Wasserstein distance, allowing iterative refinement to converge to the true distribution.
- Core assumption: The offline data distribution covers the test policy's state-action distribution sufficiently.
- Evidence anchors:
  - [section]: "the distributional Bellman operator is γ1−1/(2p)-contractive under the metric (Ex,a∼dπ d2p w,p)1/(2p), i.e., for any f, f′∈X×A↦→ [0, (1− γ)−1]d, it holds that..."
  - [abstract]: "Our theoretical results show that for both finite and infinite horizon discounted settings, FLE can learn distributions that are close to the ground truth under total variation distance and Wasserstein distance, respectively."
  - [corpus]: No direct corpus support found. This appears to be a novel theoretical contribution requiring validation against existing distributional RL theory.

### Mechanism 3
- Claim: Total variation distance bounds transfer to Wasserstein distance bounds for bounded distributions, enabling unified analysis across settings.
- Mechanism: Since TV distance dominates Wasserstein distance on bounded sets (dp w,p ≤ diam(S)· dtv), error bounds in TV distance directly imply Wasserstein distance bounds.
- Core assumption: The return distributions are supported on bounded sets ([0,H]^d for finite horizon, [0,(1-γ)^(-1)]^d for infinite horizon).
- Evidence anchors:
  - [section]: "Note that from Equation (1), TD distance dominates p-Wasserstein distance which indicates that our guarantee for the finite horizon setting is stronger."
  - [abstract]: "Our theoretical results show that for both finite and infinite horizon discounted settings, FLE can learn distributions that are close to the ground truth under total variation distance and Wasserstein distance, respectively."
  - [corpus]: No corpus support found. This appears to be a technical lemma requiring verification against existing measure theory literature.

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) generalization bounds
  - Why needed here: The theoretical guarantees of FLE depend critically on MLE achieving good in-distribution generalization bounds.
  - Quick check question: What is the relationship between the bracketing number of a function class and the MLE generalization bound?

- Concept: Distributional Bellman operator properties
  - Why needed here: Understanding contraction properties under different metrics is essential for analyzing convergence in both finite and infinite horizon settings.
  - Quick check question: Why is the Bellman operator not contractive under total variation distance in the discounted setting?

- Concept: Wasserstein distance and total variation distance relationship
  - Why needed here: The theoretical analysis leverages the relationship between these distances to transfer bounds between finite and infinite horizon settings.
  - Quick check question: What is the precise relationship between total variation distance and Wasserstein distance for bounded distributions?

## Architecture Onboarding

- Component map: Data → MLE target generation → MLE training → conditional distribution update → repeat → final distribution estimation
- Critical path: Offline data → MLE target generation → MLE training → conditional distribution update → repeat → final distribution estimation
- Design tradeoffs: MLE-based approach trades off between model flexibility (can use complex generative models) and computational cost (MLE training can be expensive). The function class choice balances expressiveness against generalization.
- Failure signatures: Poor coverage of offline data by test policy distribution leads to large errors; MLE generalization bounds failing to hold due to function class complexity; Bellman completeness violations causing divergence.
- First 3 experiments:
  1. Simple tabular MDP with known reward distributions to verify basic correctness
  2. LQR system to test linear function approximation capabilities
  3. Combination lock MDP with Gaussian mixture models to validate performance on structured distributions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations of the current work.

## Limitations
- Heavy reliance on MLE generalization bounds without providing concrete analysis of when these bounds can be achieved in practice
- Assumption that any probabilistic generative model trainable via MLE can be integrated into FLE requires careful consideration of function class complexity and data coverage
- Theoretical analysis focuses on finite-horizon and discounted infinite-horizon settings, leaving open questions about continuous state-action spaces in other RL frameworks

## Confidence
- **High confidence**: The algorithmic framework (iterative MLE for distributional Bellman updates) is well-specified and implementable
- **Medium confidence**: Theoretical guarantees for finite-horizon TV distance, given the contraction arguments and bounded support assumptions
- **Low confidence**: Infinite-horizon Wasserstein guarantees and the transfer of TV bounds to Wasserstein bounds, due to lack of corpus validation and complex technical assumptions

## Next Checks
1. **Coverage sensitivity analysis**: Systematically vary the overlap between behavior and target policy distributions to quantify how quickly FLE's error grows as the coverage assumption weakens
2. **MLE generalization bounds verification**: For each generative model (GMM, diffusion), empirically verify that in-distribution generalization bounds hold on held-out validation data during training iterations
3. **Alternative metric evaluation**: Implement and compare against alternative distributional OPE algorithms (e.g., vector-valued kernel embeddings, moment-matching approaches) to establish whether FLE's advantage is specific to the MLE framework or more general