---
ver: rpa2
title: 'SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic
  Knowledge?'
arxiv_id: '2306.08374'
source_url: https://arxiv.org/abs/2306.08374
tags:
- speech
- tasks
- language
- glue
- speechglue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpeechGLUE, a speech-based adaptation of
  the GLUE benchmark, to evaluate how well self-supervised speech models capture linguistic
  knowledge. By converting GLUE text tasks into speech via TTS and freezing SSL model
  parameters, the study directly probes learned linguistic representations without
  task-specific fine-tuning.
---

# SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?

## Quick Facts
- arXiv ID: 2306.08374
- Source URL: https://arxiv.org/abs/2306.08374
- Reference count: 0
- Speech SSL models achieve close performance to text SSL models on NLU tasks

## Executive Summary
This paper introduces SpeechGLUE, a speech-based adaptation of the GLUE benchmark, to evaluate how well self-supervised speech models capture linguistic knowledge. By converting GLUE text tasks into speech via TTS and freezing SSL model parameters, the study directly probes learned linguistic representations without task-specific fine-tuning. Experiments show that speech SSL models, especially WavLM LARGE, outperform baselines and achieve close performance to text-based SSL models in sentence similarity and natural language inference tasks, though they lag in grammaticality judgments. This indicates that speech SSL models acquire substantial linguistic information, particularly in higher layers, but still trail text-based models in some linguistic aspects. The results suggest room for improvement through unified speech-text SSL approaches.

## Method Summary
The study converts GLUE text tasks to speech using a single-speaker VITS TTS model trained on LJSpeech, then passes the speech through frozen SSL models (wav2vec2.0, HuBERT, data2vec-s, WavLM) to extract features. A weighted-sum pooling across all hidden layers aggregates the representations, which are then mean-pooled and fed into a simple downstream classifier. The frozen upstream approach isolates pre-trained linguistic knowledge from task-specific adaptation. The authors compare speech SSL models against text SSL models (BERT, data2vec-t) and baselines across eight GLUE tasks, measuring performance correlations with text-based GLUE results.

## Key Results
- WavLM LARGE achieves best performance among speech SSL models on SpeechGLUE
- Speech SSL models outperform baselines and approach text SSL model performance on similarity and NLI tasks
- Speech SSL models lag significantly on grammaticality judgments (CoLA task) compared to text models

## Why This Works (Mechanism)

### Mechanism 1
Speech SSL models learn linguistic representations through masking and reconstruction, analogous to text SSL models. Masked prediction forces the model to infer missing acoustic tokens by leveraging contextual dependencies, thereby capturing linguistic structure like syntax and semantics. The core assumption is that contextual dependencies in speech carry sufficient linguistic information for downstream NLU tasks. Evidence shows speech SSL models benefit spoken language understanding tasks, implying they learn both acoustic and linguistic information. If acoustic-only variations dominate representations, linguistic cues may be masked, reducing downstream NLU performance.

### Mechanism 2
Layer-wise analysis shows that higher layers of speech SSL models specialize in linguistic abstractions. Early layers capture low-level acoustic features, while deeper layers progressively integrate longer-range linguistic dependencies, analogous to NLP Transformer hierarchies. The core assumption is that speech SSL models follow a similar hierarchical abstraction pattern as text SSL models. Evidence indicates features in latter layers are important for non-speaker-related tasks, with WavLM weights concentrated in layers 18-24, somewhat later than BERT. If fine-tuning on linguistic tasks is required to extract these abstractions, the model may not have truly learned them in a task-agnostic way.

### Mechanism 3
Freezing SSL model parameters during downstream training isolates linguistic representations learned during pre-training. By freezing upstream layers, the evaluation measures only the linguistic information captured during SSL, not task-specific fine-tuning adaptations. The core assumption is that pre-trained representations contain sufficient linguistic knowledge for diverse NLU tasks without fine-tuning. Evidence shows the objective is to evaluate speech SSL models' NLP capability by converting text sentences to speech and freezing parameters. If task-specific fine-tuning is necessary to achieve competitive NLU performance, the claim that linguistic knowledge is fully captured is weakened.

## Foundational Learning

- Concept: Self-supervised learning via masked prediction
  - Why needed here: Speech SSL models rely on masking acoustic tokens and reconstructing them, which forces the model to learn contextual dependencies that encode linguistic information.
  - Quick check question: What is the main pre-training objective used in Wav2Vec2.0 and HuBERT?

- Concept: Transformer layer hierarchy and abstraction
  - Why needed here: Understanding that deeper layers capture higher-level abstractions (e.g., linguistic features) is key to interpreting layer-wise probing results.
  - Quick check question: In NLP Transformers, which layers typically capture syntax versus semantics?

- Concept: Probing tasks and frozen upstream evaluation
  - Why needed here: Probing evaluates what information is already in representations without fine-tuning, so understanding the setup is crucial for interpreting results.
  - Quick check question: What does freezing upstream parameters during downstream training test for?

## Architecture Onboarding

- Component map:
  TTS system (VITS) → Speech SSL model (frozen) → Weighted-sum pooling → Downstream classifier/regressor

- Critical path:
  1. Convert GLUE text to speech via TTS
  2. Pass speech through frozen SSL model
  3. Apply weighted-sum over all hidden layers
  4. Mean-pool sequence representations
  5. Feed into task-specific linear layer
  6. Compute loss and update only downstream parameters

- Design tradeoffs:
  - Freezing SSL vs. fine-tuning: Freezing isolates pre-trained linguistic knowledge but may limit adaptation; fine-tuning could improve performance but obscures what was learned during SSL.
  - Single-speaker TTS: Reduces acoustic variability to focus on linguistic content but may not reflect real-world speech diversity.
  - Weighted-sum vs. single-layer selection: Weighted-sum leverages multi-layer information but adds complexity; single-layer may miss complementary features.

- Failure signatures:
  - Low performance across tasks suggests SSL model failed to capture linguistic information.
  - High variance in layer weights indicates instability in which layers contribute useful features.
  - Poor performance on CoLA (grammaticality) but good on similarity/NLI suggests SSL models capture semantics better than syntax.

- First 3 experiments:
  1. Compare frozen SSL vs. fine-tuned SSL on SpeechGLUE to quantify contribution of pre-trained linguistic knowledge.
  2. Vary TTS speaker (multi-speaker vs. single-speaker) to test robustness of linguistic representations to acoustic variability.
  3. Test layer-wise probing without weighted-sum (e.g., use only top layer) to identify which layers contribute most to linguistic tasks.

## Open Questions the Paper Calls Out

### Open Question 1
Can unified speech-text self-supervised learning models outperform speech-only models in linguistic knowledge capture?
Basis in paper: The authors note that speech SSL models lag behind text-based models and suggest room for improvement through unified speech-text SSL approaches.
Why unresolved: The paper compares speech-only SSL models with text-only models but does not evaluate unified speech-text SSL models.
What evidence would resolve it: Experimental results comparing unified speech-text SSL models against both speech-only and text-only models on SpeechGLUE tasks.

### Open Question 2
How does the performance of speech SSL models vary across different languages and language families?
Basis in paper: The authors mention that XLS-R, a multilingual model, almost matched the accuracy of WavLM BASE+ despite larger data volume, indicating potential language dependency.
Why unresolved: The paper primarily evaluates English speech SSL models and does not explore cross-linguistic performance.
What evidence would resolve it: Experimental results evaluating speech SSL models on SpeechGLUE tasks in multiple languages and language families.

### Open Question 3
What specific linguistic properties are captured by different layers of speech SSL models?
Basis in paper: The authors analyze layer-wise contributions and note that different tasks exploit information captured in multiple layers, but the tendency is not consistent across WavLM and BERT models.
Why unresolved: The paper provides layer-wise analysis but does not identify specific linguistic properties captured by each layer.
What evidence would resolve it: Detailed analysis correlating layer-wise representations with specific linguistic properties (e.g., syntax, semantics, phonetics) using linguistic probing tasks.

## Limitations

- TTS-generated speech may not fully capture the linguistic complexity of naturally spoken utterances
- GLUE tasks are inherently text-based, designed for written language phenomena
- Freezing SSL parameters prevents adaptation to speech-specific task demands

## Confidence

**High confidence**: Speech SSL models capture measurable linguistic information sufficient for NLU tasks, as evidenced by consistent performance across similarity and NLI tasks.

**Medium confidence**: Speech SSL models lag behind text SSL models on grammaticality judgments (CoLA), indicating incomplete linguistic coverage.

**Low confidence**: Speech SSL models achieve "close performance" to text SSL models on sentence similarity and NLI tasks, though they still trail by non-trivial margins.

## Next Checks

1. Repeat SpeechGLUE evaluation using multi-speaker TTS or natural speech recordings to verify that linguistic representations are stable across acoustic variations.

2. Compare frozen-parameter results with fine-tuned speech SSL models on SpeechGLUE to quantify the gap between pre-trained linguistic knowledge and task-adapted performance.

3. Evaluate each SSL layer independently (without weighted-sum) on SpeechGLUE tasks to identify which layers contribute most to linguistic performance.