---
ver: rpa2
title: Strong and Efficient Baselines for Open Domain Conversational Question Answering
arxiv_id: '2310.14708'
source_url: https://arxiv.org/abs/2310.14708
tags:
- reader
- ocqa
- topi
- or-quac
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies several limitations in the standard DPR+FiD
  pipeline for ODConvQA, including reader susceptibility to noisy input, reduced retriever
  coverage, lack of cross-encoder semantic encoding, and reader latency impacted by
  the number of input passages. To address these, the authors propose the R3FINE strategy,
  which introduces a fast reranking component (SemanticReranker) between the retriever
  and reader, and performs targeted finetuning.
---

# Strong and Efficient Baselines for Open Domain Conversational Question Answering

## Quick Facts
- arXiv ID: 2310.14708
- Source URL: https://arxiv.org/abs/2310.14708
- Reference count: 40
- Key outcome: R3FINE improves state-of-the-art ODConvQA results while reducing reader latency by 60%

## Executive Summary
This paper addresses limitations in the standard DPR+FiD pipeline for Open Domain Conversational Question Answering (ODConvQA). The authors identify three key weaknesses: reader susceptibility to noisy input, reduced retriever coverage, and reader latency impacted by the number of input passages. To address these issues, they propose the R3FINE strategy, which introduces a SemanticReranker between the retriever and reader, and performs targeted finetuning. Experiments on TopiOCQA and OR-QuAC datasets demonstrate that R3FINE significantly improves state-of-the-art results while reducing reader latency by 60%.

## Method Summary
The R3FINE strategy improves the DPR+FiD pipeline through a three-step approach: (1) Increase DPR retrieval from 50 to 1000 passages to improve coverage, (2) Add a SemanticReranker (single TransformerEncoder layer) between retriever and reader to filter irrelevant passages and reorder by relevance, and (3) Finetune the FiD reader on the top-10 passages from the SemanticReranker. The SemanticReranker uses cross-attention to capture conversation-passage relationships that DPR's dual-encoder architecture misses. The finetuned reader is trained specifically on high-quality, reranked passages to achieve better latency-performance tradeoffs.

## Key Results
- R3FINE improves state-of-the-art results on TopiOCQA and OR-QuAC datasets
- Achieves 60% reduction in reader latency through targeted finetuning on fewer passages
- Outperforms standard DPR+FiD by 1.7-2.9 F1 points through improved passage quality and filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SemanticReranker reduces reader susceptibility to noisy input by filtering out irrelevant passages while maintaining high coverage
- Mechanism: The reranker operates at the semantic level, attending to both conversation history and passage representations to reorder passages by relevance
- Core assumption: Dense semantic representations capture sufficient contextual information to distinguish relevant from irrelevant passages
- Evidence anchors: 
  - "Experiments... show that our method improves the SotA results, while reducing reader's latency by 60%"
  - "Table 1 shows that when the same reader model is provided with the relevant (i.e., gold) passage in input, the performance decreases as the number of retrieved passages increases"
- Break condition: If the reranker cannot distinguish relevant from irrelevant passages based on semantic encoding alone

### Mechanism 2
- Claim: Targeted finetuning of the reader on top-10 reranked passages creates a better latency/performance tradeoff than naive scaling
- Mechanism: By training the reader specifically on passages that have already been filtered and reordered, the model learns to extract answers from smaller, higher-quality input lists
- Core assumption: Reader performance benefits more from higher-quality input than from learning to handle larger input lists
- Evidence anchors:
  - "Experiments... show that our method improves the SotA results, while reducing reader's latency by 60%"
  - "Table 3 shows... a further finetuning step of the FiD (w/ SR + FT) outperforms the results obtained by the SemanticReranker (w/ SR) by 1.7 and 2.9 F1 points"
- Break condition: If the reader cannot learn to extract answers effectively from smaller input lists regardless of quality

### Mechanism 3
- Claim: Cross-encoder semantic encoding in the reranker captures conversation-passage relationships that dual-encoder DPR misses
- Mechanism: The SemanticReranker applies TransformerEncoder attention across conversation history and all retrieved passages simultaneously
- Core assumption: Passage relevance depends on the complete conversational context, not just the immediate question
- Evidence anchors:
  - "The DPR retriever performs independent encoding of the passages via the PassageEncoder function. This means that it is not able to exploit the semantic relationship among them"
  - "This new module is based on the TransformerEncoder and applies the following function: ˜hc, ˜hp1, ..., ˜hpk = Reranker(hc, hp1, ..., hpk) where each element of the input attends to both the conversation dense representation hc and passages dense representations hpi"
- Break condition: If conversation history cannot provide meaningful context for passage relevance beyond the immediate question

## Foundational Learning

- Concept: Dense passage retrieval (DPR)
  - Why needed here: Understanding how DPR works as the base retriever is essential to grasp what limitations the SemanticReranker addresses
  - Quick check question: How does DPR's dual-encoder architecture differ from traditional sparse retrieval methods?

- Concept: Cross-encoder architecture
  - Why needed here: The SemanticReranker uses cross-attention between conversation and passages, which is fundamentally different from DPR's independent encoding
  - Quick check question: What is the key architectural difference between dual-encoder and cross-encoder models in terms of information flow?

- Concept: Fusion-in-Decoder (FiD) reader architecture
  - Why needed here: Understanding FiD's parallel encoding and concatenation approach explains why reducing input passages improves latency
  - Quick check question: Why does FiD's parallel encoding make it more sensitive to input size compared to extractive readers?

## Architecture Onboarding

- Component map: ConversationEncoder (BERT) → DPR Retriever (dense search) → SemanticReranker (cross-attention) → FiD Reader (parallel encoding + concatenation) → Answer generation

- Critical path: Conversation history → DPR retrieval (1000 passages) → SemanticReranker (top-10 selection) → FiD reader → Answer

- Design tradeoffs:
  - DPR vs SemanticReranker: DPR provides speed through independent encoding but misses context; reranker adds context awareness but adds latency
  - FiD top-k size: Larger k improves coverage but increases latency quadratically; smaller k reduces latency but risks missing relevant passages
  - Cross-encoder vs dual-encoder: Cross-encoders capture richer relationships but are slower; dual-encoders are faster but less context-aware

- Failure signatures:
  - High latency with poor performance: Likely indicates reranker is not effectively filtering passages
  - Low latency but very poor performance: Likely indicates reranker is too aggressive in filtering
  - Consistent performance regardless of top-k: Likely indicates reranker is not reordering passages effectively

- First 3 experiments:
  1. Compare FiD performance with gold passage vs top-10 DPR passages to establish baseline noise susceptibility
  2. Measure SemanticReranker's effect on passage coverage and FiD performance with top-10 vs top-50 inputs
  3. Test different reranker configurations (layers, finetuning options) to find optimal latency/performance tradeoff

## Open Questions the Paper Calls Out

- Question: What is the optimal number of layers for the SemanticReranker in the R3FINE strategy?
- Basis in paper: The paper discusses different configurations of the SemanticReranker, including varying the number of layers from 1 to 4, but does not explicitly state the optimal number of layers
- Why unresolved: The paper provides an ablation study showing the impact of different configurations, but does not conclude on the optimal number of layers for the SemanticReranker
- What evidence would resolve it: A comprehensive study comparing the performance of the R3FINE strategy with different numbers of layers in the SemanticReranker, and identifying the configuration that yields the best results

- Question: How does the R3FINE strategy perform on non-conversational question answering datasets?
- Basis in paper: The paper mentions that further research is needed to compare the performance of the R3FINE strategy with other rerankers on non-conversational QA datasets
- Why unresolved: The paper only evaluates the R3FINE strategy on two conversational datasets (TOPI OCQA and OR-QuAC), and does not explore its performance on non-conversational datasets
- What evidence would resolve it: Experiments comparing the R3FINE strategy with other rerankers on non-conversational QA datasets, demonstrating its effectiveness in different contexts

- Question: What is the impact of the SemanticReranker on the overall latency of the ODConvQA pipeline?
- Basis in paper: The paper mentions that the SemanticReranker accounts for only 0.34% of the overall latency of the FiD reader, adding an additional 2.4ms per example on top of the 710ms taken by FiD
- Why unresolved: While the paper provides some information about the latency impact of the SemanticReranker, it does not explore the overall impact on the entire ODConvQA pipeline
- What evidence would resolve it: A comprehensive latency analysis of the entire ODConvQA pipeline with and without the SemanticReranker, quantifying its impact on the overall latency

## Limitations

- The paper does not provide detailed hyperparameter settings for the finetuning procedures, making exact reproduction challenging
- While the 60% latency reduction is reported, the absolute latency numbers are not provided, making it difficult to assess practical impact
- The SemanticReranker is described as "a single TransformerEncoder layer" without specifying the exact architecture details (number of parameters, training objectives beyond contrastive loss)

## Confidence

- High confidence in the overall approach: The three identified limitations are well-grounded and the proposed solution addresses them systematically
- Medium confidence in performance claims: Results are based on two datasets and show consistent improvements, but lack detailed ablation studies
- Medium confidence in latency improvements: The 60% reduction is stated but without absolute numbers or comparison to alternative efficiency methods

## Next Checks

1. Conduct ablation studies removing each component of R3FINE (reranking, finetuning) to quantify individual contributions
2. Test the SemanticReranker on additional ODConvQA datasets to assess generalizability beyond TopiOCQA and OR-QuAC
3. Compare R3FINE's latency-performance tradeoff against alternative efficiency methods like query routing or hybrid retrieval approaches