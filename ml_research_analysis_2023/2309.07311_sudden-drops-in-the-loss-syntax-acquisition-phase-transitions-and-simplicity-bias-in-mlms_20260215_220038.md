---
ver: rpa2
title: 'Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity
  Bias in MLMs'
arxiv_id: '2309.07311'
source_url: https://arxiv.org/abs/2309.07311
tags:
- training
- arxiv
- phase
- loss
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the emergence of Syntactic Attention Structure
  (SAS) in masked language models (MLMs) during training. The authors find that SAS
  abruptly emerges during a brief window in training, coinciding with a steep drop
  in loss and preceding the acquisition of grammatical capabilities.
---

# Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs

## Quick Facts
- arXiv ID: 2309.07311
- Source URL: https://arxiv.org/abs/2309.07311
- Reference count: 40
- Key outcome: SAS abruptly emerges during training, coinciding with steep loss drops and preceding grammatical capability acquisition; suppressing SAS early can improve model quality.

## Executive Summary
This paper investigates the emergence of Syntactic Attention Structure (SAS) in masked language models during training. The authors identify a brief window where models abruptly acquire SAS, concurrent with a steep drop in loss, which precedes the development of grammatical capabilities. Through causal interventions, they demonstrate that SAS is necessary for grammatical reasoning, but also competes with alternative strategies during training. Brief suppression of SAS can accelerate its later emergence and improve overall model quality, revealing a complex interplay between interpretability artifacts and learning dynamics.

## Method Summary
The authors pre-train BERT-base models on BookCorpus and English Wikipedia, monitoring loss curves, UAS (Unlabeled Attachment Score), and BLiMP grammatical accuracy throughout training. They manipulate SAS using a syntactic regularizer that encourages or discourages attention on syntactic neighbors. The core experimental pipeline involves pretraining with various regularization schedules, identifying phase transitions in the loss and capability curves, and evaluating final model quality on GLUE and BLiMP tasks. Key interventions include suppressing SAS at different training stages to test critical learning periods.

## Key Results
- SAS emerges abruptly during a brief window in training, coinciding with a steep drop in loss and preceding grammatical capability acquisition
- Suppressing SAS early in training allows an alternative strategy to emerge, which can outperform SAS-based learning in certain phases
- Brief suppression of SAS accelerates and augments the SAS onset, but prolonged suppression prevents it entirely

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAS emerges abruptly during training, coinciding with a steep drop in loss, and this structure onset precipitates the acquisition of grammatical capabilities.
- Mechanism: During MLM training, the model initially relies on a general, context-agnostic strategy. At a specific point in training, the model abruptly shifts to specializing attention heads on syntactic relations (SAS). This specialization compresses the representation of syntax into efficient attention patterns, reducing loss and enabling subsequent grammatical reasoning.
- Core assumption: The syntactic attention structure (SAS) is both learnable and beneficial for representing grammar in MLMs.
- Evidence anchors:
  - [abstract]: "We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss."
  - [section 4.1]: "The first stage aligns with the formation of SAS—we call this break in implicit parse UAS the structure onset."
  - [corpus]: Weak. Neighbors discuss phase transitions and abrupt learning, but none explicitly link SAS to grammar acquisition.
- Break condition: If the model never encounters enough syntactically diverse data to learn SAS, or if regularization permanently suppresses it, the structure onset never occurs and grammatical capabilities remain limited.

### Mechanism 2
- Claim: Suppressing SAS early in training allows an alternative strategy—relying on long-range semantic context—to emerge, which competes with and can outperform SAS-based learning.
- Mechanism: SAS acts as a simplicity bias, encouraging the model to focus on local syntactic neighbors. When SAS is suppressed, the model must rely on broader contextual cues to predict masked tokens. This alternative strategy can be more effective early in training because it avoids overcommitting to local structure before the model has sufficient global understanding.
- Core assumption: The alternative strategy is not just noise but a coherent, learnable strategy that can outperform SAS in certain phases.
- Evidence anchors:
  - [section 4.3]: "We find that by suppressing SAS early on, we do improve the effectiveness of training later."
  - [section I]: "The alternative strategy onset...suggests that SAS also competes with other useful traits in the network."
  - [corpus]: Weak. Neighbors mention abrupt learning and phase transitions, but do not discuss competing strategies or simplicity bias in this context.
- Break condition: If the alternative strategy is suppressed for too long, the model cannot recover SAS, and grammatical capabilities suffer. The critical period ends when the alternative strategy onset occurs.

### Mechanism 3
- Claim: Brief suppression of SAS accelerates and augments the SAS onset, but prolonged suppression prevents it entirely.
- Mechanism: Early suppression of SAS forces the model to develop alternative, more robust representations of context. When SAS is later allowed, these representations provide a stronger foundation, leading to a sharper and more effective SAS onset. However, if suppression is too long, the model loses the ability to form SAS entirely.
- Core assumption: The model can recover SAS after brief suppression, but not after prolonged suppression, indicating a critical learning period.
- Evidence anchors:
  - [section 4.3.1]: "If SAS is suppressed only briefly, it accelerates and augments the SAS onset."
  - [section J]: "As we continue to suppress SAS, we lose these benefits and further weaken the transition to SAS."
  - [corpus]: Weak. Neighbors discuss abrupt learning and phase transitions, but do not address recovery from suppression or critical learning periods.
- Break condition: If SAS is suppressed past the alternative strategy onset, recovery is impossible, and the model's grammatical capabilities are permanently impaired.

## Foundational Learning

- Concept: Phase transitions in model training
  - Why needed here: The paper's central finding is that SAS and grammatical capabilities emerge via abrupt phase transitions, not gradual improvement. Understanding phase transitions is essential to interpreting the results.
  - Quick check question: What distinguishes a phase transition from smooth scaling in model training?

- Concept: Simplicity bias in deep learning
  - Why needed here: The paper argues that SAS represents a simplicity bias—a preference for interpretable, local structure—that can compete with and sometimes hinder more complex, effective strategies.
  - Quick check question: How might a preference for simple, interpretable features both help and harm model performance?

- Concept: Causal intervention in interpretability
  - Why needed here: The paper uses causal interventions (suppressing or promoting SAS) to establish necessity, not just correlation, between SAS and grammatical capabilities.
  - Quick check question: Why is causal intervention stronger evidence than passive observation in interpretability studies?

## Architecture Onboarding

- Component map:
  - MLM with standard BERT-base architecture (12 layers, 768 dimensions)
  - Syntactic regularizer: adds a term to the loss encouraging or discouraging attention on syntactic neighbors
  - Evaluation: UAS (Unlabeled Attachment Score) for SAS, BLiMP for grammatical capabilities, GLUE for general language understanding

- Critical path:
  1. Pretrain MLM with or without syntactic regularizer
  2. Monitor loss, UAS, and BLiMP during training to identify phase transitions
  3. Apply multistage suppression to test critical learning periods
  4. Evaluate final model quality on GLUE and BLiMP

- Design tradeoffs:
  - SAS promotion: faster early convergence but worse long-term performance
  - SAS suppression: slower early convergence but better long-term performance if suppression is brief
  - Full suppression: prevents SAS entirely, harming grammatical capabilities

- Failure signatures:
  - Loss plateaus without SAS onset → model stuck in alternative strategy
  - UAS never spikes → SAS never learned, grammatical capabilities limited
  - BLiMP accuracy low despite low loss → lack of grammatical reasoning

- First 3 experiments:
  1. Train baseline BERT, monitor loss, UAS, BLiMP to confirm structure and capabilities onsets
  2. Train with SAS promoted, compare onsets and final performance
  3. Train with brief SAS suppression, release at alternative strategy onset, compare recovery and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the observed phase transitions in MLM training generalize to other architectures like LSTMs or CNNs?
- Basis in paper: [inferred] The paper focuses on Transformer-based MLMs and observes phase transitions in their training dynamics. It would be valuable to know if similar phenomena occur in other architectures.
- Why unresolved: The study is limited to Transformer-based models, and extending the analysis to other architectures would require significant computational resources and experimentation.
- What evidence would resolve it: Training and analyzing LSTMs or CNNs on similar tasks, measuring analogous metrics like syntactic attention structure and grammatical capabilities, and comparing the training dynamics to those observed in Transformers.

### Open Question 2
- Question: What is the nature of the "alternative strategy" that emerges when SAS is suppressed, and how does it contribute to model performance?
- Basis in paper: [explicit] The paper mentions an "alternative strategy" that competes with SAS and leads to improvements in MLM loss when SAS is suppressed. However, the specific nature of this strategy is not fully characterized.
- Why unresolved: While the paper provides some evidence that the alternative strategy involves using long-range semantic content rather than local syntactic structure, a more detailed understanding of its mechanisms and contributions to performance is lacking.
- What evidence would resolve it: Further analysis of attention distributions, probing techniques, and controlled experiments to isolate and characterize the alternative strategy's role in model behavior and performance.

### Open Question 3
- Question: How does the timing and duration of SAS suppression affect the long-term performance and generalization of the model?
- Basis in paper: [explicit] The paper explores the effects of suppressing SAS at different stages of training, finding that brief suppression can improve performance, but prolonged suppression damages it. However, the optimal timing and duration for SAS suppression are not determined.
- Why unresolved: The study provides initial insights into the effects of SAS suppression, but a more systematic exploration of the timing and duration parameters is needed to optimize model performance and understand the underlying mechanisms.
- What evidence would resolve it: A comprehensive set of experiments varying the timing and duration of SAS suppression, measuring performance on various tasks, and analyzing the resulting model structures and behaviors to identify the optimal conditions for SAS suppression.

## Limitations

- The study focuses exclusively on Transformer-based MLMs, limiting generalizability to other architectures
- The precise nature and mechanisms of the alternative strategy remain uncharacterized
- The optimal timing and duration for SAS suppression are not determined

## Confidence

- Phase transition identification and timing: High
- SAS as predictor of grammatical capabilities: High
- Causal role of SAS in capability development: Medium
- Generalizability to other architectures: Low

## Next Checks

1. **Architecture Generalization Test**: Replicate the core experiments with RoBERTa and GPT-style models to determine if SAS emergence patterns are architecture-specific or general phenomena.

2. **Critical Period Boundaries**: Systematically map the precise boundaries of the critical period for SAS suppression by testing suppression windows at finer temporal resolutions (e.g., 10K-step intervals instead of 50K).

3. **Alternative Strategy Characterization**: Conduct ablation studies to characterize the "alternative strategy" that emerges during SAS suppression, using techniques like probing classifiers and attention pattern analysis to understand what replaces syntactic attention.