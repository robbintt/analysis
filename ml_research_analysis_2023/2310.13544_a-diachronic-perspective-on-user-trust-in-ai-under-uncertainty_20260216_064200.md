---
ver: rpa2
title: A Diachronic Perspective on User Trust in AI under Uncertainty
arxiv_id: '2310.13544'
source_url: https://arxiv.org/abs/2310.13544
tags:
- user
- trust
- system
- intervention
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how user trust in AI systems evolves over
  time, especially in response to miscalibrated AI outputs. The authors conducted
  human-subject experiments using a betting game to study user trust under uncertainty,
  focusing on the effects of confidently incorrect and unconfidently correct AI outputs.
---

# A Diachronic Perspective on User Trust in AI under Uncertainty

## Quick Facts
- arXiv ID: 2310.13544
- Source URL: https://arxiv.org/abs/2310.13544
- Reference count: 25
- Key outcome: Confidently incorrect AI outputs cause a sharp and persistent drop in user trust, even after a small number of such instances.

## Executive Summary
This paper investigates how user trust in AI systems evolves over time, especially in response to miscalibrated AI outputs. The authors conducted human-subject experiments using a betting game to study user trust under uncertainty, focusing on the effects of confidently incorrect and unconfidently correct AI outputs. They found that even a few incorrect instances with inaccurate confidence estimates can significantly damage user trust and performance, with very slow recovery. The study revealed that confidently incorrect outputs have a more negative effect on trust than unconfidently correct ones. Additionally, the authors showed that mistrust can transfer between different types of questions, and that advanced models like recurrent networks are better at predicting user trust compared to simpler models. These findings highlight the importance of proper calibration in user-facing AI applications and provide insights into what aspects help users decide whether to trust the AI system.

## Method Summary
The study used a within-subject experimental design with human participants playing a betting game against a simulated AI system. Participants were shown trivia and math questions with AI-generated answers and confidence scores, then placed bets (0¢ to 10¢) on whether the answer was correct. The experiment had three phases: 10 calibrated stimuli, 5 miscalibrated stimuli (either confidently incorrect or unconfidently correct), and 45 calibrated stimuli. Trust evolution was measured through bet values, user accuracy, and accumulated reward. The researchers used interrupted time series analysis to quantify the causal effect of miscalibration interventions on trust, and trained predictive models (RF, logistic regression, MLP, GRU) on user behavior data to forecast trust trajectories.

## Key Results
- Confidently incorrect outputs cause a sharp and persistent drop in user trust, even after only 5 instances.
- Unconfidently correct outputs have a milder negative effect on trust compared to confidently incorrect ones.
- Mistrust generalizes across different question types; miscalibration in one domain reduces trust in another.
- Recurrent networks outperform simpler models in predicting user trust trajectories.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidently incorrect outputs cause a sharp and persistent drop in user trust, even after a small number of such instances.
- Mechanism: Users form a mental model that maps confidence to correctness. When this mapping is violated by a confidently incorrect prediction, the mental model is updated to distrust high-confidence outputs, and this distrust persists due to the slow rate of trust recovery.
- Core assumption: User trust is based on the correspondence between confidence and correctness, and a few mismatches can disproportionately affect the mental model.
- Evidence anchors:
  - [abstract] "confidently incorrect outputs have a more negative effect on trust than unconfidently correct ones."
  - [section 4.1] "We find that users are especially sensitive to confidently incorrect miscalibration" and "The difference in bet values before and after intervention... is also observable."
  - [corpus] Weak corpus support: no direct citations of this mechanism; similarity score 0.53.
- Break condition: If users are explicitly informed that the system's confidence is unreliable, the mental model update may not be as strong.

### Mechanism 2
- Claim: Unconfidently correct outputs have a milder negative effect on trust compared to confidently incorrect ones.
- Mechanism: When the system is correct but expresses low confidence, users may initially doubt the system but are more likely to adapt and continue trusting high-confidence outputs.
- Core assumption: Users are more tolerant of underconfidence than overconfidence because underconfidence does not lead to high-stakes errors.
- Evidence anchors:
  - [abstract] "unconfidently correct and confidently incorrect... have different negative effects on user trust."
  - [section 4.1] "Users are much less affected by this type of miscalibration" referring to unconfidently correct intervention.
  - [corpus] No direct corpus support for this specific mechanism; similarity score 0.48.
- Break condition: If the task has high stakes and the user cannot afford to miss correct answers, even unconfidently correct outputs may cause strong distrust.

### Mechanism 3
- Claim: Mistrust generalizes across different question types; miscalibration in one domain reduces trust in another.
- Mechanism: Users build a global mental model of the AI system rather than a domain-specific one, so a violation in one domain affects expectations in others.
- Core assumption: The AI system is perceived as a single entity, and errors in one area cast doubt on its competence in all areas.
- Evidence anchors:
  - [abstract] "mistrust can transfer between different types of questions."
  - [section 4.3] "there is a gap between trust in the unaffected and affected stimuli type" and "We find a significant decrease in bet values for both affected and unaffected questions."
  - [corpus] No direct corpus support for this mechanism; similarity score 0.62.
- Break condition: If the user is explicitly aware that the system uses different models or expertise for different domains, mistrust may not transfer.

## Foundational Learning

- Concept: Calibration of AI confidence.
  - Why needed here: The study hinges on how users interpret confidence scores and how miscalibration affects trust.
  - Quick check question: If an AI system outputs a confidence of 90% but is correct only 60% of the time, is it calibrated or miscalibrated?

- Concept: Mental models in human-AI interaction.
  - Why needed here: The paper's framework is built around how users develop and update mental models of AI reliability over time.
  - Quick check question: What happens to a user's mental model if the AI system's confidence repeatedly mismatches its correctness?

- Concept: Interrupted time series analysis.
  - Why needed here: This quasi-experimental method is used to quantify the causal effect of miscalibration interventions on user trust.
  - Quick check question: In ITS analysis, what does a significant negative coefficient after the intervention time point indicate?

## Architecture Onboarding

- Component map: User interface for betting game → Stimuli generator (QA with confidence) → User trust model (bet value, correctness) → Analysis pipeline (ITS, ML models)
- Critical path: Generate stimuli → Collect user responses (bet, decision) → Update mental model → Analyze trust trajectory
- Design tradeoffs: Simulated questions vs. real questions (avoid prior knowledge vs. realism); monetary reward vs. intrinsic motivation
- Failure signatures: Trust recovery stalls despite many calibrated examples; user performance degrades; trust transfers incorrectly between domains
- First 3 experiments:
  1. Replicate the basic intervention study with 5 confidently incorrect stimuli to observe trust drop.
  2. Vary the number of miscalibration instances to find the minimum needed to affect trust.
  3. Test mistrust transfer with more than two question types to generalize findings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rate of trust recovery differ between confidently incorrect and unconfidently correct miscalibrations over longer time periods (beyond the 60 stimuli used in the study)?
- Basis in paper: [explicit] The paper states that "user trust does not recover even after a long sequence of calibrated examples" following miscalibration, and notes that confidently incorrect miscalibrations have a more negative effect than unconfidently correct ones.
- Why unresolved: The study only tested up to 60 stimuli with a single intervention period. It's unclear if trust would eventually recover over months or years of calibrated interactions, and whether the recovery rate differs between miscalibration types.
- What evidence would resolve it: A longitudinal study tracking user trust over extended periods (e.g., 6+ months) with varying frequencies of miscalibrated examples and different types of miscalibrations.

### Open Question 2
- Question: How does the impact of miscalibration differ across different types of high-stakes AI applications (e.g., medical diagnosis vs. financial advice vs. legal advice)?
- Basis in paper: [inferred] The paper mentions that "in an AI system detecting cancer, having a doctor manually do the screening has lower cost than a misdiagnosis" but doesn't empirically test how miscalibration affects trust in different domains.
- Why unresolved: The study used a gambling/betting paradigm which may not capture the full psychological complexity of trust in different high-stakes domains where the consequences of errors vary significantly.
- What evidence would resolve it: Controlled experiments testing user trust and behavior in different domain-specific scenarios with varying risk-reward structures.

### Open Question 3
- Question: Can specific intervention strategies be designed to help users recover trust more quickly after encountering miscalibrated examples?
- Basis in paper: [explicit] The paper shows that trust recovery is very slow but doesn't explore whether specific interventions (e.g., explanations, demonstrations of calibration, or gradual exposure to calibrated examples) could accelerate the process.
- Why unresolved: The study only examined passive exposure to calibrated examples without testing active trust-building interventions.
- What evidence would resolve it: Experiments testing various intervention strategies (explanations, calibration demonstrations, trust-building exercises) and measuring their effectiveness in accelerating trust recovery compared to passive exposure alone.

## Limitations
- The study relies on simulated AI outputs rather than real AI systems, which may limit ecological validity.
- The transfer of mistrust between question types is observed but the mechanism for this generalization is not fully explored.
- The long-term persistence of trust recovery is not measured beyond the experimental timeframe, leaving open the question of whether trust can fully recover over extended periods.

## Confidence

**High Confidence**: The finding that confidently incorrect outputs have a stronger negative effect on trust than unconfidently correct ones is well-supported by both experimental data and clear statistical analysis.

**Medium Confidence**: The observation that mistrust transfers between question types is statistically significant but based on a limited set of question categories, suggesting the need for broader generalization.

**Medium Confidence**: The superiority of recurrent networks for trust prediction is demonstrated, but the sample size and diversity of prediction tasks may limit broader applicability.

## Next Checks

1. Conduct a follow-up experiment with real-world AI systems to validate whether the trust dynamics observed with simulated outputs hold in practical applications.

2. Test the generalization of mistrust transfer using a wider variety of question types and domains to assess the robustness of the findings.

3. Investigate the long-term trajectory of trust recovery by extending the experimental timeline and measuring user trust after prolonged exposure to calibrated outputs.