---
ver: rpa2
title: 'CBA: Improving Online Continual Learning via Continual Bias Adaptor'
arxiv_id: '2308.06925'
source_url: https://arxiv.org/abs/2308.06925
tags:
- learning
- training
- task
- distribution
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  online continual learning, where models struggle to retain knowledge of previous
  tasks when learning new ones due to non-stationary data streams. The authors propose
  a Continual Bias Adaptor (CBA) module that dynamically adjusts the classifier's
  output distribution during training to adapt to changing posterior probabilities
  while allowing the original classifier to learn a stable consolidation of previously
  learned knowledge.
---

# CBA: Improving Online Continual Learning via Continual Bias Adaptor

## Quick Facts
- **arXiv ID**: 2308.06925
- **Source URL**: https://arxiv.org/abs/2308.06925
- **Reference count**: 40
- **Primary result**: Proposes Continual Bias Adaptor (CBA) module that dynamically adjusts classifier outputs during training to adapt to catastrophic distribution shifts, achieving up to 9.23% improvement in average accuracy and 33.02% reduction in forgetting measure compared to rehearsal-based baselines.

## Executive Summary
This paper addresses catastrophic forgetting in online continual learning by proposing a Continual Bias Adaptor (CBA) module that dynamically adjusts classifier outputs during training. The CBA is implemented as a lightweight MLP that can be removed during inference without computational overhead. Through theoretical analysis of gradient alignment and empirical evaluation across three datasets, CBA consistently outperforms baseline rehearsal methods by adapting to changing posterior distributions while allowing the original classifier to consolidate knowledge.

## Method Summary
The Continual Bias Adaptor (CBA) is a lightweight MLP module that plugs into rehearsal-based continual learning methods during training. It works by taking classifier logits as input and outputting adjusted probabilities via softmax, allowing the model to adapt to time-varying posterior distributions P(Y|X). The method uses bi-level optimization: an inner loop updates the classifier to fit new data, while an outer loop updates CBA parameters to ensure good performance on the memory buffer. During testing, only the original classifier is used, eliminating any inference-time overhead.

## Key Results
- CBA improves average accuracy by up to 9.23% across different memory buffer sizes compared to baseline ER
- Reduces forgetting measure by up to 33.02% compared to baseline rehearsal methods
- Demonstrates effectiveness on three datasets: Split CIFAR-10, Split CIFAR-100, and Split Tiny-ImageNet
- Works with multiple rehearsal-based baselines (ER, DER++, RAR, CLSER) while maintaining zero inference overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Continual Bias Adaptor (CBA) dynamically adjusts the classifier's posterior distribution to adapt to catastrophic distribution shifts during training.
- Mechanism: CBA is implemented as a lightweight MLP that takes classifier logits as input and outputs adjusted probabilities via softmax. This allows the model to fit the time-varying posterior P(Y|X) while keeping the original classifier stable.
- Core assumption: The distribution shift in online CL can be effectively captured and compensated by a simple nonlinear transformation applied to classifier outputs.
- Evidence anchors:
  - [abstract] "propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training"
  - [section] "we propose a Continual Bias Adaptor (CBA) module that can plug in most of the rehearsal-based methods during training"
  - [corpus] Weak evidence - no direct citations in neighbor papers supporting this specific mechanism
- Break condition: If the underlying distribution shifts are too complex or non-stationary for the MLP to capture, or if the assumption that P(Y|X) changes are primarily bias-driven is incorrect.

### Mechanism 2
- Claim: The bi-level optimization framework aligns gradients between training data and memory buffer, reducing forgetting.
- Mechanism: Inner loop updates the augmented classifier Fθ,ω to fit new data; outer loop updates CBA parameters ω to ensure fθ performs well on memory buffer data. This creates gradient alignment that protects against forgetting.
- Core assumption: Gradient alignment between training and memory buffer data is sufficient to prevent catastrophic forgetting in online CL.
- Evidence anchors:
  - [section] "This theorem reveals two insights into our algorithm. On the one hand, this theorem explains why the CBA module adaptively assimilates the task-recency bias"
  - [section] "Theorem 1 also shows why our method can mitigate forgetting effectively. From Eq. (7), our method is potentially close to some previous gradient-alignment-based CL works"
  - [corpus] No direct evidence - this is a novel theoretical contribution not yet validated by neighbor papers
- Break condition: If the second-order gradient computation becomes numerically unstable, or if the assumption that memory buffer represents stable knowledge consolidation is violated.

### Mechanism 3
- Claim: The CBA module can be removed during inference without computational overhead while maintaining strong performance.
- Mechanism: During testing, only the original classifier fθ is used (ˆytst = fθ(xtst)), eliminating any inference-time computation from CBA.
- Core assumption: The original classifier fθ learns to consolidate knowledge across tasks when trained with CBA during training, making CBA unnecessary at test time.
- Evidence anchors:
  - [abstract] "In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead"
  - [section] "In the test stage, the test sample xtst is predicted by fθ, that is ˆytst = fθ(xtst). This indicates that our method does not introduce any calculation overhead in the test stage"
  - [corpus] No evidence - this inference efficiency claim is not discussed in neighbor papers
- Break condition: If the original classifier fθ fails to consolidate knowledge properly during training, or if the CBA's role in training is more fundamental than just bias correction.

## Foundational Learning

- Concept: Online continual learning and catastrophic forgetting
  - Why needed here: The paper addresses the specific problem of models forgetting previously learned knowledge when trained on non-stationary data streams
  - Quick check question: What is the key difference between online CL and offline CL that makes catastrophic forgetting more severe?

- Concept: Bi-level optimization
  - Why needed here: The method uses nested optimization loops where inner loop updates the classifier and outer loop updates the CBA parameters
  - Quick check question: In the bi-level optimization framework, which parameters are updated in the inner loop versus the outer loop?

- Concept: Gradient alignment theory
  - Why needed here: The theoretical analysis shows that the method achieves gradient alignment between training and memory buffer, which helps prevent forgetting
  - Quick check question: What does Theorem 1 claim about the relationship between gradients of the outer-loop loss and inner-loop loss?

## Architecture Onboarding

- Component map:
  Backbone (ResNet-18) -> Classifier network fθ with linear classification layer -> Continual Bias Adaptor (CBA) gω: 2-layer MLP with 256 hidden units, ReLU activation, skip connection, softmax output -> Memory buffer M for rehearsal

- Critical path:
  1. Forward pass through fθ to get logits
  2. Forward pass through CBA gω to get adjusted probabilities
  3. Compute training loss on combined batch (current task + memory buffer)
  4. Backpropagate to update θ (inner loop)
  5. Compute validation loss on memory buffer
  6. Backpropagate to update ω (outer loop)

- Design tradeoffs:
  - CBA adds training-time complexity but zero inference-time overhead
  - Second-order gradients required for ω update increase memory usage
  - Stronger CBA architectures (more layers/hidden units) may perform better but slower

- Failure signatures:
  - Poor performance on old tasks despite CBA → gradient alignment not working
  - CBA parameters diverging → learning rate too high or unstable second-order computation
  - No improvement over baseline → distribution shifts not the primary issue

- First 3 experiments:
  1. Run ER baseline on Split CIFAR-10 with M=0.2k, measure ACC and FM
  2. Add CBA to ER with default architecture, same dataset, compare improvements
  3. Vary CBA hidden units (64, 256, 1024) to find optimal architecture size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CBA scale with increasing number of tasks or classes compared to other continual learning methods?
- Basis in paper: [inferred] The paper mentions CBA improves performance across different memory buffer sizes and demonstrates effectiveness on three datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet), but does not systematically evaluate performance scaling with increasing task numbers or class counts.
- Why unresolved: The experiments focus on comparing CBA to baselines but do not investigate how performance changes as the number of tasks increases from 5 to 10 or as class counts increase from 10 to 100. This is particularly relevant since the theoretical analysis suggests CBA handles distribution shifts, which may become more challenging with more classes/tasks.
- What evidence would resolve it: Experiments showing ACC and FM metrics as a function of number of tasks/classes, comparing CBA to baselines across a spectrum from small (5 tasks) to large (50+ tasks) continual learning scenarios.

### Open Question 2
- Question: What is the impact of different replay buffer sampling strategies on CBA's performance in online continual learning?
- Basis in paper: [explicit] The paper mentions that CBA can plug into different rehearsal-based methods (ER, DER++, RAR, CLSER) but only evaluates standard reservoir sampling for buffer updates. It does not investigate how other sampling strategies like those used in GSS or MIR might affect CBA's effectiveness.
- Why unresolved: The theoretical analysis focuses on gradient alignment between training data and memory buffer, but does not consider how the composition of the memory buffer (which samples are stored) might interact with CBA's ability to model posterior distribution shifts. Different sampling strategies could lead to different buffer distributions that might be easier or harder for CBA to align with.
- What evidence would resolve it: Experiments comparing CBA's performance when combined with different memory sampling strategies (random, reservoir, GSS, MIR, MIR with CBA) across multiple datasets and buffer sizes.

### Open Question 3
- Question: How does CBA perform in non-class-incremental settings like domain-incremental learning or task-incremental learning?
- Basis in paper: [explicit] The paper focuses specifically on class-incremental learning (Class-IL) and mentions that other CL settings exist but does not evaluate CBA in these alternative scenarios. The theoretical analysis assumes a classification setting but does not restrict to Class-IL specifically.
- Why unresolved: The distribution shift problem addressed by CBA could manifest differently in other CL settings. In domain-incremental learning, the label space remains constant but the input distribution changes, while in task-incremental learning, task identity is available during training. CBA's approach of modeling posterior distribution shifts might be equally or more effective in these settings, but this remains unexplored.
- What evidence would resolve it: Experiments evaluating CBA on standard domain-incremental benchmarks (like CORe50 or Digits) and task-incremental benchmarks, comparing performance to specialized methods for those settings like MAS or EWC.

## Limitations

- The theoretical analysis relies on second-order gradient computation which may be numerically unstable in practice
- CBA's effectiveness assumes that distribution shifts can be captured by simple MLP transformations, which may not hold for complex data distributions
- The method requires careful tuning of inner and outer loop learning rates for stable bi-level optimization

## Confidence

- **Mechanism 1 (CBA adapts posterior distribution)**: Medium confidence. The theoretical framework is sound, but the assumption that simple MLP transformations can capture complex distribution shifts is not extensively validated across diverse scenarios.
- **Mechanism 2 (Gradient alignment prevents forgetting)**: High confidence. The theoretical analysis is rigorous and aligns with established gradient-based continual learning principles, though empirical validation of the gradient alignment effects would strengthen this claim.
- **Mechanism 3 (Zero inference overhead)**: Medium confidence. While the theoretical claim is valid, the paper doesn't provide ablation studies showing performance degradation when CBA is removed.

## Next Checks

1. Implement ablation studies comparing baseline ER, CBA-enhanced ER, and a variant where CBA is retained during inference to verify the claimed computational efficiency benefit.

2. Conduct experiments varying the CBA architecture complexity (number of layers and hidden units) to establish the sensitivity of performance to architectural choices.

3. Measure and report the gradient alignment between training and memory buffer losses during training to empirically validate the theoretical claims about forgetting mitigation.