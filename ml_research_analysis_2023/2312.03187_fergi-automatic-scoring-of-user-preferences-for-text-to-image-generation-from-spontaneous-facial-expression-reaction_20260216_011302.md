---
ver: rpa2
title: 'FERGI: Automatic Scoring of User Preferences for Text-to-Image Generation
  from Spontaneous Facial Expression Reaction'
arxiv_id: '2312.03187'
source_url: https://arxiv.org/abs/2312.03187
tags:
- image
- score
- uni00000013
- facial
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a system for automatically annotating user
  preferences for text-to-image generation from their spontaneous facial expression
  reactions to the generated images. The authors collect the FERGI dataset of 33 participants'
  facial expression reactions to 2827 images generated by Stable Diffusion v1.4, along
  with their manual feedback.
---

# FERGI: Automatic Scoring of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction

## Quick Facts
- **arXiv ID**: 2312.03187
- **Source URL**: https://arxiv.org/abs/2312.03187
- **Reference count**: 40
- **Key outcome**: Automatic scoring of user preferences for text-to-image generation from spontaneous facial expression reactions, showing AU4 (brow lowerer) activation is most consistently correlated with negative image evaluations.

## Executive Summary
This paper introduces FERGI, a system for automatically annotating user preferences for text-to-image generation based on spontaneous facial expression reactions. The authors collected a dataset of 33 participants' facial reactions to 2827 images generated by Stable Diffusion v1.4, along with manual feedback. By estimating facial action units (AUs) in reaction videos, they discovered that AU4 activation is most consistently correlated with negative image evaluations. The study develops an AU4 valence score that outperforms state-of-the-art scoring models at predicting user preferences between image pairs with substantial AU4 score differences. Integrating the AU4 valence score with pre-trained scoring models improves their consistency with human preferences.

## Method Summary
The FERGI system collects video recordings of facial expressions in response to generated images, along with manual feedback ratings. The videos are preprocessed (face detection, alignment, histogram equalization) and AU activation values are extracted using a pre-trained AU estimation model. The AU4 valence score is computed as a negative exponential decay function of AU4 activation value. A leave-one-participant-out (LOPO) procedure with grid search is used for parameter fitting. The AU4 valence score is then integrated with pre-trained scoring models via linear combination to improve consistency with human preferences.

## Key Results
- AU4 (brow lowerer) activation is most consistently correlated with negative image evaluations across participants
- AU4 valence score outperforms state-of-the-art scoring models at predicting user preferences between image pairs with substantial AU4 score differences
- Integrating AU4 valence score with pre-trained scoring models improves their consistency with human preferences
- The AU4 valence score captures image fidelity while pre-trained models capture image-text alignment, making them complementary signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AU4 (brow lowerer) activation reliably indicates negative evaluation of generated images.
- Mechanism: The corrugator supercilii muscle (activated in AU4) is positively associated with amygdala activity and negatively associated with ventromedial prefrontal cortex activity, which are neural correlates of negative affect. When users see low-quality or unsatisfying images, this muscle contracts more strongly.
- Core assumption: The correlation between AU4 activation and negative evaluation is consistent across different users and image types.
- Evidence anchors: [abstract] "AU4 (brow lowerer) activation is most consistently correlated with negative image evaluations"; [section] "Specifically, AU4 activation is most consistently reflective of negative evaluation of the image"

### Mechanism 2
- Claim: An exponential decay function of AU4 activation can be calibrated to linearly correlate with user evaluation scores.
- Mechanism: The raw AU4 activation values have a non-linear relationship with user ratings. By applying an exponential decay function, we can transform these values into a valence score that has a more linear and positive correlation with the associated evaluation.
- Core assumption: The relationship between AU4 activation and user evaluation can be adequately modeled with an exponential decay function.
- Evidence anchors: [abstract] "We define the AU4 valence score sAU4 as the negative of an exponential decay function of AU4 activation value"; [section] "we propose an AU4 valence score sAU4, designed to have a more linear relationship and positive correlation with the associated evaluation"

### Mechanism 3
- Claim: Integrating AU4 valence score with pre-trained scoring models improves their consistency with human preferences.
- Mechanism: Pre-trained scoring models primarily capture image-text alignment, while AU4 valence score captures image fidelity. By combining these complementary signals, we can create a more comprehensive evaluation metric that better matches human preferences.
- Core assumption: Image fidelity and image-text alignment are the two most essential aspects of text-to-image generation assessment, and they are largely independent.
- Evidence anchors: [abstract] "Integrating our FAU-Net valence score with the pre-trained scoring models improves their consistency with human preferences"; [section] "AU4 valence score is the most reflective of the evaluation of the fidelity of the generated images among all models, which is complementary to the pre-trained scoring models, which better reflect the evaluation of image-text alignment"

## Foundational Learning

- **Facial Action Coding System (FACS) and Action Units (AUs)**: The system relies on analyzing facial expressions in terms of individual muscle movements (AUs) to infer user preferences. *Quick check*: Can you explain what AU4 represents and how it's typically activated?

- **Automatic facial expression recognition**: The system needs to automatically detect and quantify AU activations from video data of users viewing generated images. *Quick check*: What are the main challenges in recognizing facial expressions from video, especially in real-world conditions?

- **Text-to-image generation evaluation metrics**: The system is designed to improve upon existing evaluation metrics by incorporating facial expression data. *Quick check*: What are the limitations of traditional evaluation metrics like CLIP score and FID when it comes to capturing human preferences?

## Architecture Onboarding

- **Component map**: Data collection pipeline -> AU estimation model -> AU4 valence score computation -> Integration module -> Evaluation framework
- **Critical path**: 
  1. Collect video data of user reactions to generated images
  2. Estimate AU activations using pre-trained AU model
  3. Compute AU4 valence scores for each image
  4. Integrate AU4 valence scores with pre-trained scoring models
  5. Evaluate the integrated model's consistency with human preferences
- **Design tradeoffs**: Using a pre-trained AU model vs. training a custom model on the specific dataset; choosing an exponential decay function for AU4 valence score vs. other transformation functions; integrating with all three pre-trained scoring models vs. selecting a subset based on performance
- **Failure signatures**: Low AU detection accuracy due to occlusion or extreme head angles; poor correlation between AU4 activation and user preferences in validation data; overfitting of the exponential decay parameters to the training data; degradation in performance when integrating with certain pre-trained scoring models
- **First 3 experiments**:
  1. Validate the correlation between AU4 activation and user ratings on a held-out test set
  2. Tune the exponential decay parameters (k and d) using a grid search on the training data
  3. Evaluate the performance of the integrated model on a variety of text-to-image generation tasks and compare against baseline models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do individual differences in facial structure, such as the presence of bangs or frequent camera angle changes, affect the reliability of AU4 estimation?
- Basis in paper: [explicit] The paper excludes 3 participants with unreliable AU4 estimation due to bangs covering the area between the eyebrows and highly unstable camera angles.
- Why unresolved: The paper only mentions excluding participants with unreliable AU4 estimation but does not provide a detailed analysis of how these individual differences impact the estimation process or the overall results.
- What evidence would resolve it: A detailed study comparing AU4 estimation accuracy across participants with varying facial structures and camera stability, along with statistical analysis of the impact on the results.

### Open Question 2
- Question: How does the AU4 valence score compare to other facial action units in predicting user preferences for text-to-image generation?
- Basis in paper: [explicit] The paper focuses on AU4 as the most consistent indicator of negative evaluations but does not extensively compare it to other AUs in terms of preference prediction.
- Why unresolved: While the paper highlights the significance of AU4, it does not provide a comprehensive comparison with other AUs that also show correlation with user evaluations.
- What evidence would resolve it: A comparative analysis of the predictive power of various AUs, including AU4, in terms of accuracy and consistency in predicting user preferences.

### Open Question 3
- Question: Can the AU4 valence score be generalized to other image generation tasks beyond text-to-image generation?
- Basis in paper: [explicit] The paper suggests that the method of automatic annotation with facial expression analysis can be potentially generalized to other generation tasks.
- Why unresolved: The paper does not provide empirical evidence or specific examples of applying the AU4 valence score to other image generation tasks.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the AU4 valence score in tasks such as image-to-image translation, image inpainting, and super-resolution.

### Open Question 4
- Question: How does the integration of the AU4 valence score with pre-trained scoring models affect the fine-tuning of text-to-image generative models?
- Basis in paper: [explicit] The paper shows that integrating the AU4 valence score with pre-trained scoring models improves their accuracy in image preference prediction.
- Why unresolved: The paper does not explore the impact of this integration on the fine-tuning process of text-to-image generative models.
- What evidence would resolve it: A study investigating the effects of using the integrated score for fine-tuning generative models, including improvements in image quality and alignment with user preferences.

### Open Question 5
- Question: What are the limitations of using facial expression analysis for automatic annotation of user preferences in text-to-image generation?
- Basis in paper: [inferred] The paper acknowledges the challenge of robustness against face occlusion and non-frontal poses for facial expression recognition models.
- Why unresolved: The paper does not provide a detailed discussion of the limitations and potential challenges of using facial expression analysis for automatic annotation.
- What evidence would resolve it: A comprehensive analysis of the limitations, including scenarios where facial expression analysis may fail or produce unreliable results, and potential solutions to address these challenges.

## Limitations

- Dataset size (33 participants, 2827 images) may limit generalizability across diverse populations and image generation scenarios
- Focus on AU4 alone may miss other relevant facial action units that could contribute to preference prediction
- Exponential decay model for transforming AU4 activation values was chosen based on empirical observation rather than theoretical grounding

## Confidence

**High Confidence**: The core finding that AU4 (brow lowerer) activation correlates with negative image evaluations is well-supported by the data analysis and multiple validation experiments.

**Medium Confidence**: The effectiveness of the AU4 valence score in improving pre-trained scoring models' consistency with human preferences is demonstrated, but the specific integration method and its generalizability could benefit from further validation.

**Low Confidence**: The assumption that image fidelity and image-text alignment are the two most essential and independent aspects of text-to-image generation assessment is not directly tested.

## Next Checks

1. **Cross-dataset validation**: Test the AU4 valence score and integrated models on a separate dataset with different participants, image generation models, and prompt distributions to assess generalizability.

2. **Ablation study on facial action units**: Systematically evaluate the contribution of other AUs beyond AU4 to preference prediction, and determine if a multi-AU model outperforms the single-AU approach.

3. **Real-world deployment study**: Conduct a longitudinal study where the system is deployed in an actual text-to-image generation application, measuring its effectiveness in improving user satisfaction over time compared to baseline scoring methods.