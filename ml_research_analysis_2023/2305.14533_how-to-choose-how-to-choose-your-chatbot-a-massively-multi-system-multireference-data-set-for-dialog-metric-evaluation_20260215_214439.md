---
ver: rpa2
title: 'How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference
  Data Set for Dialog Metric Evaluation'
arxiv_id: '2305.14533'
source_url: https://arxiv.org/abs/2305.14533
tags:
- metrics
- evaluation
- systems
- dataset
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMSMR, a Massively Multi-System MultiReference
  dataset to improve automatic dialogue evaluation. The authors create a new conversational
  dataset by scraping multi-turn dialogues from an ESL teaching website and collect
  multiple human-generated reference responses for each prompt.
---

# How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation

## Quick Facts
- arXiv ID: 2305.14533
- Source URL: https://arxiv.org/abs/2305.14533
- Reference count: 40
- One-line primary result: Introducing MMSMR, a Massively Multi-System MultiReference dataset to improve automatic dialogue evaluation metrics

## Executive Summary
This paper introduces MMSMR, a novel dataset designed to improve automatic dialogue evaluation metrics by addressing the one-to-many problem in conversational responses. The authors create a dataset with 8 human-generated reference responses per prompt by scraping dialogues from an ESL teaching website and extending existing datasets. They train 1750 diverse chatbot models and evaluate them using various metrics, finding that correlations between metric scores and human judgments are generally lower when considering only top-performing systems compared to all systems. This work provides valuable insights into the limitations of current dialogue evaluation metrics and highlights the importance of diverse reference sets.

## Method Summary
The authors create a new conversational dataset by scraping multi-turn dialogues from an ESL teaching website and collecting multiple human-generated reference responses for each prompt. They extend the NCM dataset with additional references and use this data to train 1750 chatbot models with diverse hyperparameters in FAIRSEQ. The models are evaluated using various metrics (BLEU, METEOR, ROUGE, BERTScore, BARTScore, etc.) on both the novel test set and the DailyDialog dataset. The evaluation results are then compared to human judgments to analyze the correlation between metric scores and human assessments across all systems and top-performing systems separately.

## Key Results
- MMSMR dataset contains 8 human-generated reference responses per prompt
- Training 1750 diverse chatbot models with varying architectures and hyperparameters
- Correlations between metric scores and human judgments are generally lower for top-performing systems compared to all systems
- Multiple reference responses significantly improve metric robustness by capturing the one-to-many nature of conversational responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple reference responses improve the robustness of automatic dialogue evaluation metrics by capturing the inherent one-to-many nature of conversational responses.
- Mechanism: By collecting multiple diverse human-generated responses for each prompt, the evaluation metrics can better account for the variability in appropriate responses to a given input. This reduces the bias introduced by relying on a single reference and allows for a more comprehensive assessment of the chatbot's performance.
- Core assumption: The diversity of human-generated responses is sufficient to capture the range of appropriate responses for a given prompt.
- Evidence anchors:
  - [abstract]: "We create and release an 8-reference dialog dataset by extending single-reference evaluation sets"
  - [section 3.1.1]: "Across each of our datasets the average Jaccard distance between each reference is high (at or near .9 across the board)"
  - [corpus]: The corpus provides 8 references per prompt, indicating a substantial effort to capture response diversity. However, the actual diversity and appropriateness of these references are not directly evaluated in the paper.
- Break condition: If the human-generated references are not sufficiently diverse or do not cover the range of appropriate responses, the benefit of using multiple references diminishes.

### Mechanism 2
- Claim: Training a large number of chatbot models with diverse hyperparameters enables a more robust evaluation of automatic dialogue metrics.
- Mechanism: By training 1750 models with varying architectures, hyperparameters, and decoding strategies, the evaluation dataset covers a wide spectrum of chatbot quality levels. This allows for a more comprehensive assessment of how well different metrics correlate with human judgments across various system capabilities.
- Core assumption: The trained models represent a diverse enough sample of the possible chatbot performance space.
- Evidence anchors:
  - [section 5]: "We train 1750 systems and evaluate them on our novel test set and the DailyDialog dataset"
  - [section 6]: "Figure 2 shows that top scoring systems constitute a large percentage of systems overall, which further highlights the disagreement between metrics"
  - [corpus]: The corpus mentions training 1750 models with different hyperparameters, but does not provide details on the diversity of the resulting model performances.
- Break condition: If the trained models do not sufficiently span the performance space, the evaluation of metrics may be biased towards specific model types or capabilities.

### Mechanism 3
- Claim: Evaluating metrics on both top-performing and all systems reveals insights into metric reliability and correlation with human judgments.
- Mechanism: By comparing the correlation between metric scores and human judgments across all systems and only the top-performing ones, the study can identify whether metrics are more reliable for distinguishing between high-quality systems or if they are biased towards differentiating between weak and strong systems.
- Core assumption: The correlation between metric scores and human judgments is a valid measure of metric reliability.
- Evidence anchors:
  - [section 6]: "We define top scoring as any system that is in the 99th percentile of systems on any metric. Figure 2 shows that top scoring systems constitute a large percentage of systems overall"
  - [section 7]: "Our analysis of the metrics shows that the correlations are lower when considering only the top systems than when considering all systems"
  - [corpus]: The corpus provides correlation data between metrics and human judgments for both all systems and top systems, but does not explicitly discuss the implications of this difference.
- Break condition: If the correlation between metric scores and human judgments is not a reliable indicator of metric quality, the conclusions drawn from comparing correlations may be misleading.

## Foundational Learning

- Concept: One-to-many problem in dialogue evaluation
  - Why needed here: Understanding that multiple appropriate responses can exist for a single prompt is crucial for designing robust evaluation metrics and datasets.
  - Quick check question: Why is using a single reference response potentially problematic in dialogue evaluation?

- Concept: Hyperparameter tuning and model architecture selection
  - Why needed here: Knowledge of how different hyperparameters and architectures affect chatbot performance is necessary for interpreting the results of the large-scale model training and evaluation.
  - Quick check question: How might changes in dropout rate or attention heads affect the performance of a transformer-based chatbot?

- Concept: Correlation analysis and statistical significance
  - Why needed here: Understanding how to interpret and evaluate the correlation between metric scores and human judgments is essential for assessing the reliability of different evaluation metrics.
  - Quick check question: What does a high Pearson correlation coefficient between a metric score and human judgment indicate about the metric's reliability?

## Architecture Onboarding

- Component map:
  - Data collection: Scraping conversational data, collecting multiple human references
  - Model training: Training 1750 chatbot models with diverse hyperparameters
  - Evaluation: Scoring models using various metrics and comparing to human judgments
  - Analysis: Examining metric correlations and reliability across different system qualities

- Critical path:
  1. Collect conversational data and human references
  2. Train diverse set of chatbot models
  3. Evaluate models using multiple metrics
  4. Analyze metric correlations with human judgments

- Design tradeoffs:
  - Using a large number of models (1750) provides comprehensive coverage but increases computational cost
  - Collecting multiple references per prompt improves evaluation robustness but requires more human effort
  - Focusing on top-performing systems may provide more practical insights but could miss issues with weaker systems

- Failure signatures:
  - Low correlation between metric scores and human judgments across all systems
  - Inconsistent correlations between metrics and human judgments for top-performing systems
  - Lack of diversity in human-generated references leading to biased evaluations

- First 3 experiments:
  1. Evaluate a small set of diverse chatbot models using multiple references and compare metric correlations to human judgments
  2. Analyze the impact of increasing the number of references per prompt on metric reliability
  3. Compare metric performance on top-performing systems versus all systems to identify potential biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of reference augmentation (e.g., paraphrasing, back-translation) compare to human-generated references in improving metric correlations?
- Basis in paper: [inferred] The paper mentions that prior work has explored automatically expanded reference sets (Gangal et al., 2021a) and that the authors collect human-generated references, but does not directly compare these approaches.
- Why unresolved: The paper focuses on human-generated references but does not evaluate automated reference augmentation methods against them.
- What evidence would resolve it: A direct comparison of metric correlations when using human-generated references versus automatically expanded reference sets for the same dataset and model outputs.

### Open Question 2
- Question: What is the impact of reference set size on metric robustness and correlation with human judgments?
- Basis in paper: [explicit] The paper collects 2-5 references per prompt but does not systematically analyze how varying the number of references affects metric performance.
- Why unresolved: While the paper demonstrates the value of multiple references, it does not explore the optimal number of references needed for robust evaluation.
- What evidence would resolve it: A study varying the number of references (e.g., 1, 2, 4, 8) per prompt and measuring corresponding changes in metric correlations and reliability.

### Open Question 3
- Question: How do different decoding strategies (e.g., beam search, sampling) affect the correlation between metrics and human judgments?
- Basis in paper: [explicit] The paper uses multiple decoding strategies (greedy, beam sizes 10/100, top-p sampling at 0.5/0.7/0.9, top-k at 10/100) but does not analyze their impact on metric correlations.
- Why unresolved: The paper trains models with various decoding strategies but does not investigate how these strategies influence the relationship between automatic metrics and human evaluations.
- What evidence would resolve it: An analysis comparing metric correlations across different decoding strategies for the same model and reference set.

### Open Question 4
- Question: What is the relationship between metric performance and model diversity in terms of architecture and training parameters?
- Basis in paper: [inferred] The paper trains 1750 diverse models but does not analyze how metric performance varies across different model architectures or training configurations.
- Why unresolved: While the paper demonstrates the value of diverse models, it does not explore how metric reliability changes with different model types or training approaches.
- What evidence would resolve it: A systematic analysis of metric correlations across different model architectures (e.g., Transformers, LSTMs) and training configurations within the MMSMR dataset.

## Limitations
- Limited architectural diversity: The 1750 models are primarily based on FAIRSEQ Transformer architectures, which may not fully represent the diversity of modern chatbot architectures.
- Reference quality uncertainty: While the paper reports high Jaccard distances between references, it does not directly evaluate whether these references capture the full range of appropriate responses or introduce biases.
- Temporal generalizability: The findings may not extend to more recent chatbot architectures and evaluation practices, given the rapidly evolving nature of conversational AI.

## Confidence
**High Confidence**: The fundamental finding that correlations between automatic metrics and human judgments are lower for top-performing systems compared to all systems is well-supported by the presented data and represents a meaningful contribution to the field of dialogue evaluation.

**Medium Confidence**: The claim that multiple reference responses significantly improve metric robustness is supported by the high Jaccard distances reported, but the actual impact on metric reliability is not fully quantified or isolated from other factors in the study.

**Low Confidence**: The generalizability of the 1750 trained models to represent the broader chatbot performance space is questionable given the limited architectural diversity and the rapidly evolving nature of conversational AI systems since this research was conducted.

## Next Checks
1. **Reference Diversity Validation**: Conduct a systematic evaluation of the human-generated reference responses to verify that they truly capture the range of appropriate responses for each prompt, beyond just calculating Jaccard distances. This could involve having additional annotators rate the diversity and appropriateness of the reference sets.

2. **Cross-Architecture Comparison**: Replicate the correlation analysis using a more diverse set of chatbot architectures (e.g., including GPT-style models, T5 variants, and more recent dialogue-specific architectures) to determine if the observed metric reliability patterns hold across different model families.

3. **Temporal Generalizability Test**: Apply the MMSMR evaluation methodology to a set of recently published chatbot models to assess whether the correlation patterns observed in the paper remain consistent as conversational AI technology continues to advance.