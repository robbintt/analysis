---
ver: rpa2
title: 3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and
  Point Cloud
arxiv_id: '2311.04699'
source_url: https://arxiv.org/abs/2311.04699
tags:
- keypoints
- detection
- pose
- were
- peduncle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a method for 3D pose estimation of tomato peduncle
  nodes using deep keypoint detection and point cloud data from an RGB-D camera. The
  approach detects four anatomical landmarks on the peduncle and main stem in 2D images,
  then integrates depth information to compute 3D poses.
---

# 3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud

## Quick Facts
- arXiv ID: 2311.04699
- Source URL: https://arxiv.org/abs/2311.04699
- Reference count: 12
- Primary result: Achieved 96% AP and 94% PDJ for 3D peduncle pose estimation in greenhouse conditions

## Executive Summary
This paper presents a method for estimating the 3D pose of tomato peduncle nodes using deep keypoint detection and point cloud data from an RGB-D camera. The approach detects four anatomical landmarks on the peduncle-stem junction in 2D images, then integrates depth information to compute 3D poses. Tested in a commercial greenhouse, the method achieved high accuracy in object detection (AP@0.5=0.96), keypoint localization (PDJ@0.2=94.31%), and 3D pose estimation with mean absolute errors of 11.38° and 9.93° for upper and lower angles. The method demonstrated robustness to viewpoint variations and is applicable to other greenhouse crops like pepper.

## Method Summary
The method uses a RealSense L515 RGB-D camera to capture images of tomato peduncle nodes from five different viewpoints. A Keypoints R-CNN model (Detectron2 implementation with ResNet50-FPN backbone) detects four anatomical landmarks (U, N, D, P) in 2D images. These 2D keypoints are extended with circular masks and projected into the 3D point cloud using depth data. Outlier removal filters spurious points, and the centroids of the remaining clusters represent the 3D keypoints. The relative angles between peduncle and stem are calculated from these 3D keypoints. The system was trained on 503 images and tested on 145 images collected in greenhouse conditions.

## Key Results
- Object detection achieved AP@0.5=0.96 and F1-score ranging from 0.95-1.00 across all viewpoints
- Keypoint localization showed PDJ@0.2=94.31% overall, with 'N' and 'P' performing better than 'U' and 'D'
- 3D angle estimation yielded MAE of 11.38° for upper angle and 9.93° for lower angle
- Viewpoint analysis showed canonical and higher views performed best, while rightwards view had highest errors

## Why This Works (Mechanism)

### Mechanism 1
The keypoint-detection approach achieves robustness to occlusion by localizing anatomical landmarks that are spatially distributed across the peduncle-stem junction. The four keypoints ('U', 'N', 'D', 'P') are designed to capture the full geometric configuration of the peduncle node. Even if one keypoint is occluded, the remaining three can still reconstruct the pose through vector geometry in 3D space. The core assumption is that keypoints are placed at locations that are visually distinct and recoverable under occlusion, and the relative spatial arrangement is sufficient to define the pose.

### Mechanism 2
Depth integration via point cloud projection converts sparse 2D keypoint detections into robust 3D spatial information. Each 2D keypoint mask is projected into the aligned 3D point cloud, gathering multiple depth samples around the keypoint location. The centroid of the resulting cluster represents the 3D keypoint after outlier removal. The core assumption is that the point cloud density around each keypoint is sufficient to yield a stable cluster after outlier filtering, and the depth sensor resolution is adequate for the peduncle scale.

### Mechanism 3
Viewpoint variation is handled by the combined strength of the detection network and the keypoint geometry. The deep keypoint detector is trained on multiple viewpoints, learning to recognize the peduncle node and its keypoints regardless of rotation. The geometry of the four keypoints remains consistent across viewpoints, enabling pose estimation in canonical 3D space. The core assumption is that the training dataset covers a sufficient range of viewpoints and the network can generalize to unseen viewpoints within the training distribution.

## Foundational Learning

- **2D object detection with bounding boxes and keypoints**: The initial detection of the peduncle node and its anatomical landmarks in the image plane is the foundation for all subsequent 3D pose estimation. Quick check: Can you explain how the IoU threshold affects the trade-off between false positives and false negatives in object detection?

- **Depth sensor calibration and point cloud projection**: Converting 2D pixel coordinates into 3D world coordinates requires understanding the camera's intrinsic parameters and depth alignment. Quick check: How does the alignment between RGB and depth images ensure that a pixel in the color image corresponds to the correct depth measurement?

- **Outlier removal in point clouds**: Raw point clusters from keypoint masks contain background noise; filtering based on local point density is essential to obtain stable 3D keypoints. Quick check: Why is a fixed radius (0.005 m) and minimum neighbor count (10) chosen for outlier removal, and how would these values change with sensor resolution?

## Architecture Onboarding

- **Component map**: Data acquisition -> Keypoint detection -> 2D to 3D projection -> Outlier filtering -> Pose estimation -> Evaluation

- **Critical path**: Capture RGB image + aligned depth -> Run keypoint detector -> Extend keypoints with masks -> Project to point cloud -> Filter outliers -> Compute centroids -> Calculate relative angles

- **Design tradeoffs**: Mask radius vs. background inclusion (larger masks reduce depth sparsity but increase background noise); Keypoint definition vs. learning difficulty (distance-based keypoints are harder to learn than feature-based ones); Viewpoint coverage vs. dataset size (more viewpoints improve robustness but require more annotations)

- **Failure signatures**: Low AP@0.5 indicates detection network underfitting or training data distribution mismatch; PDJ drop for 'U'/'D' suggests distance-based keypoints are ambiguous; High MAE in 3D angles indicates either noisy 2D keypoint localization or unstable 3D clusters

- **First 3 experiments**: Synthetic viewpoint augmentation to evaluate detection robustness; Fixed vs. adaptive mask radius comparison for angle estimation accuracy; Outlier removal parameter sweep to find optimal filtering for point cloud density

## Open Questions the Paper Calls Out

1. How does the accuracy of the proposed keypoint-based pose estimation method compare to template-fitting methods for 3D pose estimation of tomato peduncles? The paper mentions previous studies used template-fitting methods but does not provide direct comparison.

2. What is the impact of different lighting conditions on the performance of the keypoint detection and pose estimation algorithm? The paper collected data under ambient lighting but did not investigate varying lighting conditions.

3. How does the proposed method perform on other greenhouse crops, such as cucumber and bell pepper, and what modifications, if any, are needed for these crops? While the paper states the method is applicable to other crops, experimental validation is not provided.

## Limitations

- Limited viewpoint coverage with only 5 angles tested, which may not represent all real-world harvesting scenarios
- Performance evaluation focused on controlled greenhouse conditions without lighting variations
- Generalization to other crops (pepper) mentioned but not empirically validated

## Confidence

- **High Confidence**: Detection performance (AP@0.5=0.96) and keypoint localization (PDJ@0.2=94.31%) - these are direct measurable metrics with clear ground truth
- **Medium Confidence**: 3D angle estimation accuracy (11.38°/9.93° MAE) - depends on both 2D detection quality and depth sensor accuracy
- **Low Confidence**: Occlusion robustness claims - while the mechanism is described, actual occlusion testing is not detailed in the results

## Next Checks

1. Test viewpoint generalization beyond the 5 angles to verify robustness claims
2. Evaluate performance under varying lighting conditions and with occlusions
3. Validate cross-crop generalization by testing on pepper plants as claimed