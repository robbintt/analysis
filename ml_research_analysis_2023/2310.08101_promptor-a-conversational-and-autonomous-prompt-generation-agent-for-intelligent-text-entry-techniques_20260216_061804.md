---
ver: rpa2
title: 'Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent
  Text Entry Techniques'
arxiv_id: '2310.08101'
source_url: https://arxiv.org/abs/2310.08101
tags:
- prompt
- promptor
- text
- language
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that large language models like GPT-3.5
  can perform intelligent text entry tasks via prompting, achieving superior performance
  to fine-tuned GPT-2 models without requiring costly data collection or model fine-tuning.
  The authors introduce Promptor, an autonomous prompt generation agent that converses
  with designers to generate high-quality prompts for GPT-3.5.
---

# Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques

## Quick Facts
- arXiv ID: 2310.08101
- Source URL: https://arxiv.org/abs/2310.08101
- Reference count: 40
- Key outcome: Large language models like GPT-3.5 can perform intelligent text entry tasks via prompting, achieving superior performance to fine-tuned GPT-2 models without requiring costly data collection or model fine-tuning

## Executive Summary
This paper introduces Promptor, an autonomous prompt generation agent that leverages large language models to create high-quality prompts for intelligent text entry tasks. The authors demonstrate that GPT-3.5, when properly prompted, can outperform fine-tuned GPT-2 models without requiring costly data collection or model fine-tuning. Through a user study with 24 participants, Promptor-designed prompts achieved 35% higher similarity and 22% higher coherence scores compared to self-designed prompts, showcasing its effectiveness in assisting designers to craft prompts for intelligent text entry tasks.

## Method Summary
The paper proposes a conversational approach to prompt engineering where GPT-4 (Promptor) interacts with designers to generate task-specific prompts for GPT-3.5. The method involves three phases: 1) Promptor generates an intermediate prompt based on task requirements using few-shot examples and chain-of-thought reasoning, 2) designers test the prompt using a virtual keyboard interface, and 3) Promptor refines the prompt based on feedback. The approach leverages in-context learning rather than fine-tuning, and uses AI judgment (GPT-4) alongside human evaluation to assess prompt quality across similarity, coherence, and format correctness metrics.

## Key Results
- GPT-3.5 prompted for intelligent text entry tasks surpassed GPT-2 by an average of 3.94% in prediction scores
- Prompts generated by Promptor led to 35% increase in similarity and 22% increase in coherence over self-designed prompts
- Human and AI judgment scores showed strong correlation (Spearman's 0.88-0.96), validating AI evaluation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting GPT-3.5 can match or exceed the performance of fine-tuned GPT-2 for intelligent text entry tasks without requiring costly data collection or model fine-tuning.
- Mechanism: In-context learning allows GPT-3.5 to acquire new skills through well-designed prompts, bypassing the need for parameter updates via gradient descent.
- Core assumption: The prompt quality is sufficient to guide the model toward task-specific behavior that rivals fine-tuned models.
- Evidence anchors:
  - [abstract]: "merely prompting GPT-3.5 surpassed a GPT-2 backed system by an average of 3.94% in prediction scores"
  - [section]: "Fine-tuning GPT-3.5 showed no significant performance gain over its prompted version"
  - [corpus]: No direct evidence in corpus; assumption based on internal study results
- Break condition: If prompts fail to provide adequate context or examples, model performance degrades to baseline random outputs.

### Mechanism 2
- Claim: Promptor autonomously generates complex, task-specific prompts that outperform prompts created by designers without prompt engineering expertise.
- Mechanism: Promptor uses a conversational interface to gather task requirements, iteratively refine prompts, and validate them through a test virtual keyboard.
- Core assumption: The conversational agent can effectively extract user intent and translate it into structured prompts using few-shot examples and chain-of-thought reasoning.
- Evidence anchors:
  - [abstract]: "Promptor-designed prompts result in a 35% increase in similarity and 22% in coherence over those by designers"
  - [section]: "the prompts generated by Promptor significantly outperformed the self-designed prompts"
  - [corpus]: Weak; no direct corpus evidence supporting this specific comparison
- Break condition: If the conversational agent fails to clarify ambiguous requirements, generated prompts may not align with designer intent.

### Mechanism 3
- Claim: Human judgment and AI judgment scores are strongly correlated, validating the use of AI judges for large-scale prompt evaluation.
- Mechanism: Both human evaluators and GPT-4 (as AI judge) assess prompt-generated responses using consistent criteria, producing correlated scores across different model variants.
- Core assumption: GPT-4 can evaluate response quality objectively without introducing bias toward the model being tested.
- Evidence anchors:
  - [section]: "Spearman's correlation values range from 0.88... to 0.96... indicating a strong monotonic relationship"
  - [section]: "Cohen's Kappa score of 0.75 suggests substantial agreement"
  - [corpus]: No direct corpus evidence; based on internal study findings
- Break condition: If AI judge training data or prompt differs significantly from evaluation context, correlation with human judgment may weaken.

## Foundational Learning

- Concept: In-context learning in large language models
  - Why needed here: Understanding how GPT-3.5 can learn tasks through prompts rather than fine-tuning is fundamental to the paper's approach
  - Quick check question: What distinguishes in-context learning from traditional fine-tuning in terms of parameter updates?

- Concept: Prompt engineering techniques (few-shot prompting, chain-of-thought)
  - Why needed here: These techniques are essential for crafting effective prompts that guide model behavior for specific tasks
  - Quick check question: How does adding "Let's think step by step" to a prompt affect model reasoning performance?

- Concept: Evaluation metrics for language model outputs (BLEU, similarity, coherence)
  - Why needed here: Different metrics capture different aspects of output quality, necessary for comprehensive assessment
  - Quick check question: Why might BLEU scores not fully capture semantic similarity between generated and reference responses?

## Architecture Onboarding

- Component map:
  - Designer -> Promptor (GPT-4) -> Test virtual keyboard -> GPT-3.5 -> Evaluation (Human/AI)

- Critical path:
  1. Designer interacts with Promptor to specify task requirements
  2. Promptor generates intermediate prompt and conducts evaluation
  3. Designer tests prompt using virtual keyboard
  4. Promptor refines prompt based on feedback
  5. Final prompt deployed to GPT-3.5 for task execution

- Design tradeoffs:
  - Using GPT-4 for Promptor vs rule-based system: More flexible but potentially slower and more expensive
  - Conversational interface vs structured forms: More intuitive but may require more rounds of clarification
  - AI vs human evaluation: Scalable and consistent but may miss nuanced quality aspects

- Failure signatures:
  - Format errors in model output (non-JSON responses)
  - Low similarity/coherence scores in evaluation
  - Designer frustration with clarification rounds
  - High latency in conversational interactions

- First 3 experiments:
  1. Test basic prompt generation for a simple sentence completion task
  2. Evaluate format correctness with minimal constraints
  3. Compare Promptor-generated vs self-designed prompts for a restaurant reservation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Promptor-designed prompts compare to prompts designed by expert prompt engineers?
- Basis in paper: [explicit] The paper compares Promptor-designed prompts to those created by designers without extensive prompt engineering experience.
- Why unresolved: The paper does not compare Promptor's performance against expert prompt engineers, only against novice designers.
- What evidence would resolve it: A study comparing prompts generated by Promptor to those created by expert prompt engineers, using the same evaluation metrics (similarity, coherence, format correctness) across the same use cases.

### Open Question 2
- Question: Can Promptor be extended to handle other types of language tasks beyond intelligent text entry?
- Basis in paper: [inferred] The paper discusses Promptor's potential applicability in other fields and mentions the Prompt2Prompt methodology could provide guidelines for developers in other domains.
- Why unresolved: The paper only evaluates Promptor on intelligent text entry tasks and does not test its performance on other language tasks.
- What evidence would resolve it: Testing Promptor on a diverse set of language tasks (e.g., text summarization, question answering, translation) and comparing its performance to state-of-the-art methods.

### Open Question 3
- Question: What is the optimal number of examples to include in the "child" system prompt generated by Promptor?
- Basis in paper: [explicit] The paper states that four examples were selected to strike a balance between optimizing output performance and not exceeding the context window length.
- Why unresolved: The paper does not explore the impact of varying the number of examples on the quality of the generated prompts.
- What evidence would resolve it: Conducting experiments with different numbers of examples (e.g., 2, 4, 6, 8) and evaluating the impact on the performance of the generated prompts across various tasks.

## Limitations

- The paper's internal study results lack external validation, with confidence rated as Medium due to modest performance improvements (3.94% average prediction score increase) that may not generalize across different task domains
- The user study sample size is limited (24 participants) and does not include comparison with professional prompt engineers, potentially overstating Promptor's effectiveness
- The paper does not address potential limitations of conversational prompt engineering such as increased latency in real-time text entry applications or computational costs of multiple clarification rounds

## Confidence

- Prompting GPT-3.5 vs fine-tuning GPT-2: Medium confidence - supported by internal study but modest performance gains and limited external validation
- Promptor outperforming designer prompts: Medium confidence - significant improvement metrics but limited participant diversity and no expert comparison
- AI vs human judgment correlation: High confidence - strong statistical agreement (Spearman's 0.88-0.96, Cohen's Kappa 0.75) though potential bias concerns remain

## Next Checks

1. **Cross-domain validation**: Test Promptor's effectiveness on non-conversational tasks (e.g., code generation, creative writing) to assess generalizability beyond the studied intelligent text entry domain.

2. **Professional comparison**: Compare Promptor-generated prompts against those created by professional prompt engineers rather than novice designers to establish a more meaningful performance baseline.

3. **Real-time performance evaluation**: Measure the end-to-end latency from designer interaction through prompt refinement to final model output, comparing against traditional fine-tuned approaches for time-sensitive applications.