---
ver: rpa2
title: A Closer Look at Reward Decomposition for High-Level Robotic Explanations
arxiv_id: '2304.12958'
source_url: https://arxiv.org/abs/2304.12958
tags:
- explanations
- action
- learning
- task
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an explainable Q-Map learning framework that
  combines reward decomposition (RD) with abstracted action spaces to enable non-ambiguous,
  high-level explanations for robotic behavior. The framework addresses the challenge
  of explaining reinforcement learning agents' actions by using abstracted actions
  mapped to task-specific primitives, avoiding low-level movement explanations.
---

# A Closer Look at Reward Decomposition for High-Level Robotic Explanations

## Quick Facts
- arXiv ID: 2304.12958
- Source URL: https://arxiv.org/abs/2304.12958
- Reference count: 32
- One-line primary result: Combines reward decomposition with abstracted actions to generate high-level explanations for robotic behavior

## Executive Summary
This paper introduces an explainable Q-Map learning framework that combines reward decomposition with abstracted action spaces to provide non-ambiguous, high-level explanations for robotic behavior. The framework addresses the challenge of explaining reinforcement learning agents' actions by using abstracted actions mapped to task-specific primitives, avoiding low-level movement explanations. By decomposing rewards based on object properties like shape and color, the system generates explanations at the property level rather than the action level, making them more interpretable for both experts and non-experts.

## Method Summary
The method trains separate Q-networks (UNet-based FCNs) for each reward component using heightmap inputs from RGB-D data. Each component Q-function learns from a specific sub-reward based on object properties, and the final action is selected by taking the arg max over the summed Q-values. Explanations are generated through bar plots visualizing component contributions and templated text, with optional LLM integration for interactive querying. The approach is tested in two simulated robotic scenarios: a pick-and-place task with a robot arm and a flying agent searching for suitable landing sites.

## Key Results
- Successfully achieved 90% task success rate in the landing scenario and 80% in the grasping scenario
- Demonstrated effective generation of high-level explanations using reward decomposition artifacts
- Showed versatility of integrating explanations with LLMs for interactive querying

## Why This Works (Mechanism)

### Mechanism 1
Reward decomposition with abstracted action spaces eliminates ambiguity in one-step RL explanations. By decomposing rewards into property-specific sub-rewards and using high-level actions (motion primitives) instead of low-level movements, the framework provides explanations based on object properties rather than specific joint movements, addressing the ambiguity problem where low-level actions fail to capture future goals.

### Mechanism 2
Q-Maps with multiple Q-functions enable property-level explanations. Training separate Q-functions for each reward component creates property-specific Q-Maps that can be visualized and compared, showing which object properties contributed most to action selection through bar plots and contrastive analysis.

### Mechanism 3
LLM integration transforms RD artifacts into interactive, customizable explanations. By providing LLMs with prompts containing task summaries and Q-Map visualizations, the framework enables natural language explanations and interactive questioning, making explanations accessible to non-experts while maintaining technical accuracy.

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) fundamentals
  - Why needed here: The framework is built on MDP formalism with state space, action space, reward function, and transition probabilities
  - Quick check question: What is the difference between the state transition probability p(st+1|st,at) and the reward function R in an MDP?

- **Concept**: Deep Q-learning and function approximation
  - Why needed here: The framework uses Q-learning with neural network function approximators (UNet architecture) to learn Q-Maps from high-dimensional visual inputs
  - Quick check question: How does the TD-error δk_t update each individual Q-Map when using reward decomposition?

- **Concept**: Reward decomposition theory
  - Why needed here: The core mechanism relies on decomposing complex rewards into sub-rewards based on object properties, requiring understanding of how this affects learning and behavior
  - Quick check question: Why does training individual Q-functions on different reward components produce behavior equivalent to training on the sum of all components?

## Architecture Onboarding

- **Component map**: RGB-D input → Heightmap rendering → Q-Map generation via UNets → Action selection (arg max over summed Q-values) → Explanation generation (visual + text) → LLM interaction
- **Critical path**: Heightmap generation from RGB-D data → UNet-based FCNs for each reward component → Q-Map aggregation and action selection → Visualization module for bar plots → Template-based text generation → LLM interface for interactive explanations
- **Design tradeoffs**: Abstracted actions vs. low-level control precision; multiple Q-functions vs. single network complexity; visual vs. text explanations for different user expertise levels
- **Failure signatures**: Poor Q-Map learning (low success rates in experiments), ambiguous property distinctions, LLM misunderstanding technical concepts, visualization clutter with too many properties
- **First 3 experiments**:
  1. Verify basic Q-learning with single reward function on heightmap input
  2. Test reward decomposition with binary sub-rewards on simple pick-and-place task
  3. Evaluate property distinction capability by training with intentionally overlapping reward components

## Open Questions the Paper Calls Out

### Open Question 1
Can reward decomposition be automated to reduce manual creation of sub-rewards? The authors mention that sub-rewards are created manually based on task criteria and suggest this could be automated via extensions to approaches that extract reward channels from agent interaction data.

### Open Question 2
How does the performance of Q-Map learning compare to standard deep Q-learning with continuous action spaces? While the paper shows successful task performance, it doesn't benchmark against alternative approaches that might use continuous action spaces without reward decomposition.

### Open Question 3
How reliable are the explanations when the Q-agent has suboptimal learning performance? The authors note that the reliability of post-hoc explanations is heavily influenced by the learning performance of the Q-agent and acknowledge this as a general concern.

### Open Question 4
Can the LLM integration generalize to tasks with more than 2-3 object properties? The experiments demonstrate LLM integration with 2-3 properties but don't test scalability to more complex property spaces.

## Limitations
- Reliance on specific object properties (shape, color) for reward decomposition may not generalize to tasks where property-based explanations are insufficient
- Abstracted action space may sacrifice fine-grained control precision needed for complex manipulation tasks
- LLM integration assumes sufficient reasoning capability for technical domain interpretation without requiring extensive fine-tuning

## Confidence
- **High confidence**: Task success rates and basic explanation generation (supported by experimental results showing 90% and 80% success rates)
- **Medium confidence**: Reward decomposition equivalence and property-level explanations (theoretical foundation solid but limited empirical validation shown)
- **Low confidence**: LLM reasoning capability and interactive explanation quality (minimal evaluation of LLM effectiveness in technical contexts)

## Next Checks
1. Test framework robustness with non-visual or less-distinct object properties to assess generalization beyond shape/color-based tasks
2. Evaluate explanation quality with domain experts to verify that property-level explanations actually improve understanding compared to traditional XRL methods
3. Conduct ablation studies removing LLM components to quantify the actual value added by LLM integration versus template-based explanations alone