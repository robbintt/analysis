---
ver: rpa2
title: Understanding Self-attention Mechanism via Dynamical System Perspective
arxiv_id: '2308.09939'
source_url: https://arxiv.org/abs/2308.09939
tags:
- stiffness
- neural
- attention
- networks
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamical systems perspective to understand
  the self-attention mechanism (SAM) in neural networks, revealing its role as a stiffness-aware
  step size adaptor that enhances model representational ability. The authors show
  that stiffness phenomenon (SP) exists in high-performance neural networks and that
  SAM refines stiffness information to generate adaptive attention values, improving
  performance.
---

# Understanding Self-attention Mechanism via Dynamical System Perspective

## Quick Facts
- arXiv ID: 2308.09939
- Source URL: https://arxiv.org/abs/2308.09939
- Reference count: 40
- Primary result: StepNet achieves state-of-the-art accuracy, with up to 3.42% improvements on CIFAR100 and 2.9% mAP gains on object detection tasks

## Executive Summary
This paper introduces a dynamical systems perspective to understand the self-attention mechanism (SAM) in neural networks, revealing its role as a stiffness-aware step size adaptor that enhances model representational ability. The authors show that stiffness phenomenon (SP) exists in high-performance neural networks and that SAM refines stiffness information to generate adaptive attention values, improving performance. This insight explains the lottery ticket hypothesis in SAM and inspires StepNet, a new approach that extracts fine-grained stiffness information. Extensive experiments on CIFAR10, CIFAR100, STL10, ImageNet, and MS COCO demonstrate that StepNet achieves state-of-the-art accuracy.

## Method Summary
The paper proposes StepNet, a new approach inspired by the dynamical systems perspective of SAM. StepNet replaces the standard self-attention module with a formulation that takes both current and next feature maps as input, allowing better estimation of stiffness information. The method uses an IEBN layer (combination of batch normalization and linear transformation "IE") to refine this information and generate adaptive step sizes. The model is trained with SGD optimizer using standard data augmentation across various backbone networks (ResNet variants, ViT) on classification and object detection tasks.

## Key Results
- StepNet achieves up to 3.42% improvements on CIFAR100 classification
- StepNet improves object detection mAP by up to 2.9% on MS COCO
- StepNet outperforms various SAM variants and achieves state-of-the-art accuracy on multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-attention mechanism acts as a stiffness-aware step size adaptor that enhances the model's representational ability by refining stiffness information and generating adaptive attention values.
- Mechanism: The self-attention mechanism (SAM) functions similarly to adaptive step-size methods used in solving stiff ordinary differential equations (ODEs). By capturing coarse stiffness information from the residual network's feature maps and refining it, SAM generates adaptive attention values that serve as flexible step sizes, allowing the network to better measure and handle stiffness phenomena (SP) in the feature trajectory.
- Core assumption: The stiffness phenomenon observed in high-precision ODE solutions also exists in high-performance neural networks, and the ability to measure this SP at the feature level is necessary for achieving high performance.
- Evidence anchors:
  - [abstract]: "Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model's representational ability to measure intrinsic SP by refining the estimation of stiffness information and generating adaptive attention values."
  - [section]: "The attention value F(f(xt; θt); ϕt) in Eq.(7) can be referred to as the step size in solving ODEs... Thus the attention values in Eq.(7) are less than 1, i.e., ∆t = F(f(xt; θt); ϕt) < 1. This implies that with SAM, we can provide a smaller and more flexible step size than that from the original residual neural network."
- Break condition: If the stiffness phenomenon does not exist in neural networks or if the SAM fails to accurately estimate and adapt to the stiffness information, the mechanism would break.

### Mechanism 2
- Claim: The lottery ticket hypothesis in self-attention (LTH4SA) is an intrinsic property of the SAM, where only a small number of blocks need the self-attention module to achieve significant performance improvements.
- Mechanism: The lottery ticket hypothesis suggests that inserting the self-attention module on a small number of blocks can lead to remarkable improvements. This is consistent with the observation that only a few features in a feature trajectory cause stiffness phenomena (SP), and thus only a few blocks need the module to measure the SP of the whole trajectory.
- Core assumption: The existence of SP is an intrinsic property of ground truth (GT) trajectories, and the adaptive step size generated by SAM can improve the representational ability of the neural network.
- Evidence anchors:
  - [abstract]: "This novel perspective can also explain the lottery ticket hypothesis in SAM (LTH4SA) [29], design new quantitative metrics of representational ability, and inspire a new theoretic-inspired approach, StepNet."
  - [section]: "For each trajectory, only a few features can cause SP... So if these two properties generally hold, we argue that LTH4SA may also be an intrinsic property of the SAM."
- Break condition: If the stiffness phenomenon is not an intrinsic property of GT trajectories or if the SAM does not improve representational ability, the mechanism would break.

### Mechanism 3
- Claim: StepNet improves performance by capturing better stiffness information through a data-driven function that estimates the relation between adjacent states.
- Mechanism: StepNet replaces the standard self-attention module with a new formulation that takes both the current and next feature maps as input. This allows for a better estimation of stiffness information, which is crucial for generating adaptive step sizes and enhancing the model's ability to measure stiffness phenomena.
- Core assumption: The ability to properly estimate stiffness information is essential for the performance of the self-attention module, and using adjacent states provides better stiffness information.
- Evidence anchors:
  - [abstract]: "Based on our novel views of SAMs, we explain the lottery ticket hypothesis in SAM, design new quantitative metrics of representational ability, and propose a powerful theoretic-inspired approach, StepNet."
  - [section]: "For the the original residual neural networks in Eq.(1), we have f(xt; θt) = xt+1 − xt ∆t |∆t=1 ≡ ˇζNSI(xt), and hence the f(xt; θt) can be regarded as a kind of coarse stiffness information... Thus, the Eq.(7) can be rewritten as ˆxt+1 = xt + f(xt; θt) ⊗ F(f(xt; θt); ϕt) ← xt + f(xt; θt) ⊗ ˜F(xt + f(xt; θt), xt; ˜ϕt)."
- Break condition: If the adjacent states do not provide better stiffness information or if the data-driven function fails to accurately estimate the relation, the mechanism would break.

## Foundational Learning

- Concept: Stiffness Phenomenon (SP) in ODEs and Neural Networks
  - Why needed here: Understanding SP is crucial for grasping how the self-attention mechanism improves neural network performance by adapting to stiffness in feature trajectories.
  - Quick check question: What is the definition of stiffness in ODEs, and how is it analogous to the behavior observed in neural network feature trajectories?

- Concept: Dynamical Systems Perspective of Neural Networks
  - Why needed here: This perspective bridges the gap between numerical methods for ODEs and the behavior of residual neural networks, explaining how SAM functions as an adaptive step size.
  - Quick check question: How can residual blocks in neural networks be interpreted as steps in a numerical method for solving ODEs?

- Concept: Self-Attention Mechanism and Its Role in Neural Networks
  - Why needed here: Understanding how SAM refines stiffness information and generates adaptive attention values is key to comprehending its impact on model performance.
  - Quick check question: How does the self-attention mechanism differ from standard residual connections, and what role do attention values play in the network's forward pass?

## Architecture Onboarding

- Component map: Input -> Residual Block -> StepNet Module -> Adaptive Step Size -> Output Feature Map

- Critical path:
  1. Input feature map passes through residual block
  2. Coarse stiffness information is extracted from the feature map
  3. Self-attention module refines stiffness information
  4. Adaptive attention values (step sizes) are generated
  5. Output feature map is produced by combining the original feature with the refined information

- Design tradeoffs:
  - Using adjacent states for stiffness estimation increases computational cost but improves accuracy
  - Smaller step sizes (attention values < 1) provide finer control but may slow down training
  - The complexity of the adaptor affects both performance and efficiency

- Failure signatures:
  - Poor performance despite using SAM indicates failure to accurately estimate stiffness information
  - Overfitting or instability may result from inappropriate step sizes
  - Inability to capture SP suggests issues with the self-attention module's design or implementation

- First 3 experiments:
  1. Compare the performance of a standard residual network with and without SAM on a benchmark dataset
  2. Visualize the feature trajectories and stiffness phenomena in networks with and without SAM
  3. Implement StepNet and evaluate its performance against standard SAM on various datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal structure for the StepNet adaptor to best estimate stiffness information and generate adaptive step sizes?
- Basis in paper: [inferred] The paper mentions that while their proposed adaptor structure outperforms alternatives, "the best structure to generate the adaptive step sizes is still unknown" and suggests further improvement is possible.
- Why unresolved: The paper only tested a few alternative adaptor structures (Figure 5) and found theirs performed best, but didn't explore the full design space or provide theoretical justification for the optimal structure.
- What evidence would resolve it: Systematic ablation studies testing different adaptor architectures (varying numbers of layers, attention mechanisms, normalization strategies) on multiple datasets, combined with theoretical analysis of how different structures affect stiffness estimation accuracy.

### Open Question 2
- Question: How do the properties of stiffness phenomenon (SP) in neural networks relate to the lottery ticket hypothesis for self-attention (LTH4SA)?
- Basis in paper: [explicit] The paper identifies two properties of SP (most GT trajectories have SP, only few features cause SP) and argues these explain LTH4SA, but states "we argue that LTH4SA may also be an intrinsic property of the SAM" without definitive proof.
- Why unresolved: The paper provides theoretical reasoning connecting the properties but doesn't provide empirical validation across diverse architectures or datasets to confirm this relationship.
- What evidence would resolve it: Empirical studies showing consistent correlation between SP prevalence, LTH4SA findings (ticket sparsity patterns), and performance across multiple self-attention architectures and datasets, or formal proofs of the connection.

### Open Question 3
- Question: Why do ground truth (GT) trajectories in neural networks exhibit stiffness phenomenon, and how does this relate to feature importance?
- Basis in paper: [explicit] The paper conjectures that "a residual network can achieve high performance...probably because the network has the ability to learn such important (stiff) features" and shows correlation between TNS and accuracy, but doesn't explain the mechanism.
- Why unresolved: While the paper observes correlations between SP, TNS, and performance, it doesn't explain why SP emerges in high-performing networks or what underlying mechanisms cause certain features to become "stiff."
- What evidence would resolve it: Analysis of feature gradient norms, sensitivity analysis, or information bottleneck properties showing why specific features develop high NSI values, or experimental validation that forcing SP patterns improves performance even in non-SAM architectures.

## Limitations

- The claim that stiffness phenomena directly translate from ODEs to neural networks requires more empirical validation and may represent an oversimplification
- The specific implementation details of the IEBN layer and exact StepNet formulation are not fully specified, creating barriers to faithful reproduction
- The lottery ticket hypothesis for self-attention relies on several conditional assumptions that need more rigorous empirical validation across diverse architectures

## Confidence

- High Confidence: The core observation that self-attention mechanisms generate adaptive values functioning similarly to adaptive step sizes in numerical ODE solvers
- Medium Confidence: The claim that stiffness phenomena exist in high-performance neural networks and that capturing this information is crucial for representational ability
- Low Confidence: The lottery ticket hypothesis for self-attention (LTH4SA) as an intrinsic property of SAM, which relies on conditional assumptions about SP-network performance relationships

## Next Checks

1. Implement visualization tools to empirically measure stiffness in feature trajectories of both vanilla residual networks and SAM-enhanced networks, comparing variance and correlation structure across layers

2. Create controlled experiments isolating the impact of each component in the IEBN layer (batch normalization vs. linear transformation "IE") to determine which elements contribute most to performance gains

3. Apply StepNet to non-vision domains (e.g., NLP transformers or graph neural networks) to evaluate whether the dynamical systems perspective and stiffness-aware adaptation generalize beyond image classification and object detection tasks