---
ver: rpa2
title: Multilingual and Multi-topical Benchmark of Fine-tuned Language models and
  Large Language Models for Check-Worthy Claim Detection
arxiv_id: '2311.06121'
source_url: https://arxiv.org/abs/2311.06121
tags:
- dataset
- claim
- datasets
- detection
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares fine-tuned language models (FLMs) and extremely
  large language models (exLLMs) for multilingual and multi-topical check-worthy claim
  detection. A custom dataset was created from multiple existing datasets to ensure
  topic, language, and writing style variability.
---

# Multilingual and Multi-topical Benchmark of Fine-tuned Language models and Large Language Models for Check-Worthy Claim Detection

## Quick Facts
- arXiv ID: 2311.06121
- Source URL: https://arxiv.org/abs/2311.06121
- Authors: 
- Reference count: 40
- Primary result: Fine-tuned language models (F1-score ~90%) slightly outperformed extremely large language models (exLLMs) in multilingual check-worthy claim detection, especially in cross-domain settings

## Executive Summary
This paper presents a comprehensive comparison between fine-tuned language models (FLMs) and extremely large language models (exLLMs) for multilingual and multi-topical check-worthy claim detection. The authors created a custom dataset from multiple existing sources spanning ten languages and six topics to evaluate model performance in both in-distribution and out-of-distribution scenarios. Their results demonstrate that FLMs, including XLMRoBERTa, LESA, and mDeBERTa, achieved slightly better performance than exLLMs (ChatGPT, GPT-4, Alpaca-LoRA) used in zero-shot settings, particularly when tested on unseen domains. The study concludes that task-specific fine-tuning remains superior to zero-shot exLLM approaches for this task, despite the larger size of exLLMs.

## Method Summary
The study involved creating a unified multilingual dataset by combining CLEF-2022, CLEF-2023, MONANT, LESA, and EnvClaims datasets, which were translated into ten languages (Arabic, Bulgarian, Dutch, English, Spanish, Turkish, Slovak, Czech, Polish, Hungarian). Three FLMs (XLMRoBERTa, LESA, mDeBERTa) were fine-tuned on this dataset using batch size 32, learning rate 3e-3 (mDeBERTa/XLM-R) or 3e-5 (LESA), and 5 epochs with Adam optimizer. Three exLLMs (ChatGPT, GPT-4, Alpaca-LoRA) were evaluated using zero-shot prompting with the binary prompt "Does the input contain a check-worthy claim? Answer strictly in Yes or No." Models were tested on in-distribution data and out-of-distribution datasets (NewsClaims and ClaimBuster) to assess generalization capabilities.

## Key Results
- FLMs achieved F1-scores of approximately 90% on in-distribution data, slightly outperforming exLLMs
- XLMRoBERTa demonstrated superior cross-lingual performance compared to LESA and mDeBERTa
- exLLMs showed robustness but lacked precision, particularly in cross-domain settings
- The performance gap between models was more pronounced in out-of-distribution evaluations

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned smaller language models outperform extremely large language models (exLLMs) in multilingual and multi-topical check-worthy claim detection tasks. Fine-tuning allows models to adapt to the specific task and domain characteristics, capturing task-specific nuances and patterns in the data. The core assumption is that the task-specific fine-tuning process is more effective than zero-shot learning for this particular task. Evidence shows FLMs achieved ~90% F1-score while exLLMs performed notably worse, particularly the smaller Alpaca-LoRA model. This mechanism could break if exLLMs are specifically fine-tuned for the task or if task characteristics change significantly.

### Mechanism 2
Multilingual models perform better than monolingual models in multilingual check-worthy claim detection tasks. Multilingual models are trained on diverse language data, enabling them to capture cross-lingual patterns and transfer knowledge between languages. The core assumption is that the multilingual training data effectively represents the language distribution in the target task. Evidence shows XLMRoBERTa performed well across all languages while LESA faced challenges with non-English datasets. This mechanism could break if target languages are not well-represented in the multilingual training data.

### Mechanism 3
Out-of-distribution evaluation reveals the generalization capabilities of models in check-worthy claim detection. Testing models on unseen datasets and languages assesses their ability to handle real-world variability and adapt to new domains. The core assumption is that out-of-distribution datasets represent true task variability. Evidence shows more pronounced performance differences in out-of-distribution scenarios compared to in-distribution settings. This mechanism could break if out-of-distribution datasets are not representative of true task variability.

## Foundational Learning

- **Check-worthy claim detection**: Understanding what distinguishes check-worthy claims from non-check-worthy claims is crucial for interpreting model performance and designing effective evaluation metrics.
- **Multilingual and multi-topical data**: The diversity of languages and topics in training data directly affects model performance and generalization capabilities across different linguistic and topical domains.
- **Fine-tuning vs. zero-shot learning**: Understanding the trade-offs between task-specific adaptation through fine-tuning and the generalization potential of zero-shot approaches is essential for selecting appropriate model architectures.

## Architecture Onboarding

- **Component map**: Data preprocessing -> Dataset unification -> Fine-tuning (XLMRoBERTa, LESA, mDeBERTa) OR Zero-shot evaluation (ChatGPT, GPT-4, Alpaca-LoRA) -> In-distribution evaluation -> Out-of-distribution evaluation -> Result analysis
- **Critical path**: Out-of-distribution evaluation is the critical path as it directly tests model generalization capabilities and determines real-world applicability
- **Design tradeoffs**: The main tradeoff is between model size (exLLMs are larger but not fine-tuned) and task-specific adaptation (fine-tuned models are smaller but tailored to the task)
- **Failure signatures**: Poor performance on out-of-distribution datasets indicates lack of generalization capability or insufficient diversity in training data
- **First 3 experiments**:
  1. Evaluate fine-tuned and exLLM models on in-distribution datasets to establish baseline performance
  2. Conduct out-of-distribution evaluation using NewsClaims and ClaimBuster datasets to assess generalization
  3. Analyze cross-lingual performance differences between multilingual and monolingual models

## Open Questions the Paper Calls Out

1. **In-context learning potential**: The paper does not explore in-context learning with engineered prompts for exLLMs, which could potentially improve their performance compared to zero-shot approaches. The authors plan to investigate this in future work.

2. **Scalability with model size**: The study uses exLLMs within the 100B parameter range but does not test significantly larger models to determine if performance scales with model size beyond this threshold.

3. **Multilingual vs. monolingual training impact**: While the paper notes differences in multilingual performance between models, it does not directly compare the same model architecture trained on multilingual versus monolingual datasets.

## Limitations

- The study relies on a single binary prompt approach without exploring prompt engineering variations that might improve exLLM performance
- The 10-language dataset, while diverse, may not fully capture all linguistic patterns across languages
- Out-of-distribution evaluation is limited to two specific test sets (NewsClaims and ClaimBuster) which may not comprehensively represent real-world variability

## Confidence

- **High Confidence**: FLMs achieving ~90% F1-score on in-distribution data is well-supported by experimental results
- **Medium Confidence**: ExLLMs showing robustness but lacking precision in cross-domain settings is supported but limited by test dataset number
- **Low Confidence**: The universal superiority of fine-tuning over zero-shot exLLM approaches may not hold as exLLM capabilities continue to evolve

## Next Checks

1. **Prompt Engineering Validation**: Test multiple prompt variations (few-shot, chain-of-thought, structured output) with exLLMs to determine if performance can be improved beyond the single binary prompt approach.

2. **Language Coverage Expansion**: Evaluate model performance on additional low-resource languages not included in the current 10-language dataset to assess true multilingual generalization capabilities.

3. **Adversarial Robustness Testing**: Create and evaluate models on adversarial examples with subtle linguistic manipulations (negation, hedging, context-dependent claims) to assess real-world robustness beyond the current evaluation framework.