---
ver: rpa2
title: Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame
  Depth Estimation
arxiv_id: '2304.12685'
source_url: https://arxiv.org/abs/2304.12685
tags:
- depth
- single-frame
- estimation
- multi-frame
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel self-supervised training framework
  for monocular depth estimation that leverages the mutual influence between single-frame
  and multi-frame methods. The proposed approach introduces a pixel-wise adaptive
  depth sampling module guided by single-frame depth to train the multi-frame model,
  and uses a minimum reprojection-based distillation loss to transfer knowledge from
  the multi-frame depth network to the single-frame network.
---

# Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame Depth Estimation

## Quick Facts
- arXiv ID: 2304.12685
- Source URL: https://arxiv.org/abs/2304.12685
- Authors: 
- Reference count: 40
- This paper introduces a novel self-supervised training framework for monocular depth estimation that leverages the mutual influence between single-frame and multi-frame methods

## Executive Summary
This paper presents a self-supervised training framework that enables mutual enhancement between single-frame and multi-frame monocular depth estimation. The key innovation is a pixel-wise adaptive depth sampling module (PADS) that uses single-frame depth predictions to guide multi-frame cost volume construction, combined with a minimum reprojection-based distillation loss that transfers knowledge from multi-frame to single-frame networks. The method achieves state-of-the-art performance on KITTI and Cityscapes datasets, outperforming existing self-supervised approaches by significant margins.

## Method Summary
The method uses a two-stage training pipeline: first training a teacher model (S-DepthNet + PoseNet + M-DepthNet) with self-supervised losses, then distilling knowledge to a student model (S-DepthNet + PoseNet) using minimum reprojection-based filtering. The PADS module generates pixel-wise depth sampling ranges using a learnable uncertainty map based on single-frame depth differences, while the multi-frame network's improved predictions are used as pseudo-labels for single-frame refinement. During inference, the improved single-frame depth guides the multi-frame network's cost volume generation.

## Key Results
- Achieves state-of-the-art performance on KITTI Eigen split (AbsRel 0.102, RMSE 3.83)
- Outperforms existing methods by 1.6% to 9.4% on KITTI test set
- Shows 0.5-1.1% improvement on Cityscapes benchmark
- Demonstrates effectiveness of mutual learning between single-frame and multi-frame depth estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel-wise adaptive depth sampling improves cost volume efficiency by using both single-frame depth and depth difference uncertainty
- Mechanism: PADS uses single-frame depth as the geometric center for sampling range and updates a learnable uncertainty map δ based on the difference between single-frame and multi-frame depth
- Core assumption: The difference between single-frame and multi-frame depth contains useful information about local depth uncertainty that can guide more efficient sampling
- Evidence anchors:
  - [abstract]: "we first introduce a pixel-wise adaptive depth sampling module guided by single-frame depth to train the multi-frame model"
  - [section III-D]: "we propose the PADS module, which adopts a learnable uncertainty map δ ∈ Rh×w to indicate the pixel-wise relative width of the sampling range"
- Break condition: If the uncertainty map δ fails to converge or if the single-frame depth estimates are too inaccurate to serve as reliable priors, the sampling range could become too narrow or too wide, degrading performance

### Mechanism 2
- Claim: Minimum reprojection-based distillation loss improves single-frame depth by filtering out erroneous multi-frame depth predictions
- Mechanism: During distillation, the student single-frame network is trained using a mask that only considers multi-frame depth predictions that produce smaller photometric errors than the student's own predictions
- Core assumption: Multi-frame depth predictions are generally more accurate than single-frame predictions, but still contain errors that need filtering
- Evidence anchors:
  - [abstract]: "we leverage the minimum reprojection based distillation loss to transfer the knowledge from the multi-frame depth network to the single-frame network to improve single-frame depth"
  - [section III-E]: "we introduce the minimum reprojection error to construct distillation loss for filtering out the multi-frame depth values that generate larger errors than the student single-frame depth"
- Break condition: If the photometric error comparison fails to effectively distinguish good from bad pseudo-labels, or if the student network becomes too confident in its own predictions and ignores useful multi-frame information

### Mechanism 3
- Claim: Two-stage training creates a positive feedback loop where improved single-frame depth further enhances multi-frame depth
- Mechanism: The pipeline trains a teacher model, uses it to distill knowledge to a student single-frame network, then uses the improved student as input to the PADS module for further multi-frame training
- Core assumption: Improvements in single-frame depth provide meaningful priors that can enhance multi-frame depth estimation in subsequent iterations
- Evidence anchors:
  - [abstract]: "Finally, we regard the improved single-frame depth as a prior to further boost the performance of multi-frame depth estimation"
  - [section III-E]: "During inference, the output of the student model is used to guide the cost volume generation for M-DepthNet, which helps improve the accuracy of M-DepthNet"
- Break condition: If the improvement in single-frame depth doesn't translate to meaningful improvements in multi-frame depth, or if the two networks diverge rather than converge toward better solutions

## Foundational Learning

- Concept: Plane-sweep stereo and cost volume construction
  - Why needed here: The multi-frame depth estimation relies on building cost volumes by warping features across multiple frames and searching over depth hypotheses
  - Quick check question: How does plane-sweep stereo differ from traditional stereo matching, and why is it suitable for self-supervised learning?

- Concept: Self-supervised learning through photometric consistency
  - Why needed here: The entire framework trains without ground truth depth by minimizing photometric error between target images and synthesized views
  - Quick check question: What role does the auto-masking strategy play in preventing stationary pixels from corrupting the training loss?

- Concept: Knowledge distillation in self-supervised settings
  - Why needed here: The framework uses multi-frame depth predictions as pseudo-labels to train the single-frame network, requiring careful handling of unreliable labels
  - Quick check question: How does the minimum reprojection error mask prevent the student network from learning from erroneous teacher predictions?

## Architecture Onboarding

- Component map: S-DepthNet + PoseNet + M-DepthNet (teacher) -> distillation -> S-DepthNet + PoseNet (student) -> PADS -> M-DepthNet (improved)
- Critical path: Single-frame depth → PADS sampling range → cost volume generation → multi-frame depth → distillation → improved single-frame depth
- Design tradeoffs:
  - Using learned uncertainty vs. fixed hyperparameters for sampling range width
  - Balancing photometric loss vs. distillation loss during student training
  - Resolution differences between depth maps and cost volumes requiring downsampling
- Failure signatures:
  - Cost volumes dominated by background when sampling range too narrow
  - Student network overfitting to erroneous pseudo-labels when mask too permissive
  - PADS uncertainty map collapsing to extremes (all zeros or all ones)
- First 3 experiments:
  1. Verify PADS generates reasonable sampling ranges by visualizing δ and comparing to baseline fixed ranges
  2. Test distillation effectiveness by comparing student performance with and without minimum reprojection masking
  3. Validate two-stage training by measuring improvement from student model vs. directly training teacher with PADS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Pixel-wise Adaptive Depth Sampling (PADS) module affect performance on dynamic scenes with multiple moving objects?
- Basis in paper: [explicit] The paper mentions that the method may not perform as well on highly dynamic scenes like Cityscapes compared to KITTI, but does not provide specific quantitative results.
- Why unresolved: The paper does not include detailed analysis of performance on dynamic scenes or scenes with multiple moving objects.
- What evidence would resolve it: Quantitative results on datasets with more dynamic scenes or scenes with multiple moving objects would clarify the impact of PADS on these scenarios.

### Open Question 2
- Question: What is the impact of using different network architectures for the Single-frame Depth Network (S-DepthNet) on the overall performance?
- Basis in paper: [explicit] The paper mentions that they use HRNet and ResNet architectures for S-DepthNet, but does not provide a comprehensive comparison of different architectures.
- Why unresolved: The paper does not include a detailed ablation study on the impact of using different network architectures for S-DepthNet.
- What evidence would resolve it: A detailed ablation study comparing the performance of different network architectures for S-DepthNet would clarify the impact of the choice of architecture on the overall performance.

### Open Question 3
- Question: How does the proposed method generalize to other types of data, such as indoor scenes or aerial imagery?
- Basis in paper: [inferred] The paper evaluates the method on KITTI and Cityscapes datasets, which are outdoor driving datasets, but does not provide results on other types of data.
- Why unresolved: The paper does not include evaluation on other types of data, such as indoor scenes or aerial imagery.
- What evidence would resolve it: Quantitative results on other types of data, such as indoor scenes or aerial imagery, would clarify the generalizability of the proposed method to different types of data.

## Limitations
- Limited comparison against recent multi-frame methods (no GLPDepth baseline)
- No cross-dataset generalization tests (training on KITTI, evaluating on nuScenes)
- Two-stage training complexity may create dependencies rather than true mutual improvement
- Lack of corpus evidence for PADS and minimum reprojection distillation mechanisms

## Confidence
- Mechanism 1 (PADS sampling): Medium - theoretically sound but no ablation on uncertainty map training stability
- Mechanism 2 (Minimum reprojection distillation): Medium - intuitive filtering approach but no analysis of failure cases
- Mechanism 3 (Two-stage mutual improvement): Low-Medium - no evidence of iterative improvement beyond single two-stage cycle

## Next Checks
1. Perform ablation study removing the minimum reprojection mask to quantify its impact on student network performance and identify failure modes
2. Test PADS module stability by training with different initializations of the uncertainty map δ and measuring convergence properties
3. Evaluate cross-dataset performance by training on KITTI and testing on nuScenes to assess generalization beyond the training distribution