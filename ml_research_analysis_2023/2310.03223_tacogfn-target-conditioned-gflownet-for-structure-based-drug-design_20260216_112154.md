---
ver: rpa2
title: 'TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design'
arxiv_id: '2310.03223'
source_url: https://arxiv.org/abs/2310.03223
tags:
- docking
- molecules
- score
- pocket
- gflownet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TACOGFN, a target-conditioned GFlowNet approach
  for structure-based drug design that generates molecules with binding affinities
  proportional to their docking scores and properties. Unlike prior methods that approximate
  dataset distributions, TACOGFN learns an RL policy to explicitly reward generation
  of high-affinity molecules, enabling exploration beyond training data.
---

# TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design

## Quick Facts
- **arXiv ID:** 2310.03223
- **Source URL:** https://arxiv.org/abs/2310.03223
- **Reference count:** 17
- **Key outcome:** Achieves state-of-the-art median Vina score of -8.44 kcal/mol and success rate of 56.0% on CrossDocked2020 benchmark, with further fine-tuning reaching -10.93 kcal/mol and 88.8% success

## Executive Summary
TACOGFN introduces a target-conditioned GFlowNet approach for structure-based drug design that generates molecules with binding affinities proportional to their docking scores and properties. Unlike prior methods that approximate dataset distributions, TACOGFN learns an RL policy to explicitly reward generation of high-affinity molecules, enabling exploration beyond training data. The method uses a pre-trained pocket encoder and docking score predictor to condition generation, and incorporates active learning to improve generalization. On the CrossDocked2020 benchmark, TACOGFN achieves state-of-the-art median Vina score of -8.44 kcal/mol and success rate of 56.0%, with further fine-tuning reaching -10.93 kcal/mol and 88.8% success. Generation is also orders of magnitude faster than baselines.

## Method Summary
TACOGFN is a target-conditioned generative model that uses GFlowNets to sample molecules with probabilities proportional to their combined docking score, QED, and SA rewards. The method conditions generation on pocket embeddings from a pre-trained GVP-GNN encoder and uses a graph transformer-based GFlowNet policy to sample molecular fragments. Active learning improves the docking score predictor's generalization by iteratively querying a docking oracle on generated molecules and retraining. The approach trains offline on CrossDocked2020 data and can be fine-tuned with additional rounds of active learning, achieving superior performance compared to graph-based and sequence-based baselines.

## Key Results
- Achieves state-of-the-art median Vina score of -8.44 kcal/mol and success rate of 56.0% on CrossDocked2020 benchmark
- With fine-tuning, reaches -10.93 kcal/mol and 88.8% success rate
- Generation is orders of magnitude faster than baseline methods
- Active learning improves predictor generalization and policy performance across multiple rounds

## Why This Works (Mechanism)

### Mechanism 1
TACOGFN learns a policy that generates molecules with probabilities proportional to their combined docking score, QED, and SA rewards, enabling exploration beyond the training data distribution. The GFlowNet policy is trained to sample molecules such that the probability of generating a molecule is proportional to its reward R = w_docking * docking_score + w_QED * QED + w_SA * SA. This RL-based approach incentivizes generation of high-reward molecules rather than simply fitting the training data distribution.

### Mechanism 2
Active learning improves the generalization of the docking score predictor by iteratively querying a docking oracle on generated molecules and retraining the predictor. Generated molecules are sampled using the current GFlowNet policy, evaluated with the docking oracle, and added to the training set for the docking score predictor. This process is repeated for multiple rounds, allowing the predictor to learn from a broader distribution of protein-ligand interactions.

### Mechanism 3
Conditioning the GFlowNet on target pocket embeddings allows the model to generate molecules tailored to specific protein pockets, improving binding affinity. A pre-trained pocket encoder (GVP-GNN) generates embeddings for each protein pocket, which are used as conditioning context for the GFlowNet policy. This allows the policy to learn pocket-specific generation strategies.

## Foundational Learning

- **Generative Flow Networks (GFlowNets)**: Learn policies that generate objects with probability proportional to a reward, essential for exploring vast chemical space and generating high-affinity molecules. Quick check: How does a GFlowNet differ from a standard generative model in terms of the distribution it learns?

- **Reinforcement Learning (RL) for molecule generation**: Allows the model to be explicitly rewarded for generating molecules with desired properties (high docking score, QED, SA) rather than simply fitting the training data distribution. Quick check: What is the advantage of using RL over maximum likelihood estimation for this task?

- **Active learning for model generalization**: Improves the docking score predictor's ability to generalize to out-of-distribution molecules by iteratively querying a docking oracle and retraining on the new data. Quick check: How does active learning help address the issue of limited training data in structure-based drug design?

## Architecture Onboarding

- **Component map**: Pocket encoder (GVP-GNN) → GFlowNet policy (Graph Transformer) → Docking score predictor (Graph Transformer + MLP) → Active learning loop → Improved pocket encoder and GFlowNet policy

- **Critical path**: Pocket encoder generates embeddings → GFlowNet policy uses embeddings to generate molecules → Docking score predictor evaluates molecules → Active learning loop queries oracle and retrains predictor → Improved predictor and policy

- **Design tradeoffs**: Using pre-computed pocket embeddings speeds up generation but may limit the model's ability to adapt to pocket-specific features during generation. Active learning improves predictor generalization but requires additional docking oracle queries, which can be expensive.

- **Failure signatures**: Poor docking scores indicate GFlowNet policy may not be properly conditioned on pocket embeddings or docking score predictor may not generalize well. Low QED or SA scores suggest reward scaling or weighting may not properly incentivize generation of drug-like and synthesizable molecules.

- **First 3 experiments**:
  1. Train pocket encoder and docking score predictor on CrossDocked dataset and evaluate performance on held-out test set
  2. Train GFlowNet policy conditioned on pocket embeddings and evaluate generated molecules' docking scores, QED, and SA on unseen pockets
  3. Implement active learning loop and evaluate improvement in docking score predictor generalization and GFlowNet policy performance after a few rounds

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Docking score predictor's generalization ability outside training distribution may lead to unreliable reward signals
- Active learning requires substantial computational resources for docking oracle queries, limiting practical applicability
- Multi-objective reward weighting using Dirichlet distribution introduces stochasticity affecting reproducibility

## Confidence
- **High confidence**: Reported performance improvements over baselines on CrossDocked2020 benchmark
- **Medium confidence**: Benefits of active learning for improving docking score predictor generalization
- **Low confidence**: Scalability to entirely novel pocket structures not represented in any training data

## Next Checks
1. Evaluate docking score predictor's performance on test set from different protein families than CrossDocked2020 to assess true out-of-distribution generalization
2. Measure marginal improvement in predictor accuracy per docking oracle query across active learning rounds to determine cost-benefit tradeoff
3. Systematically vary Dirichlet α parameters and analyze how changes in reward weighting affect quality of generated molecules across multiple independent runs