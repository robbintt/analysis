---
ver: rpa2
title: Fine-Tuning Language Models Using Formal Methods Feedback
arxiv_id: '2310.18239'
source_url: https://arxiv.org/abs/2310.18239
tags:
- language
- left
- right
- turn
- light
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for fine-tuning pre-trained language
  models using automated feedback from formal verification. The approach constructs
  automaton-based controllers from the language model outputs and verifies them against
  specifications within a world model, using the compliance results as a feedback
  source for iterative fine-tuning.
---

# Fine-Tuning Language Models Using Formal Methods Feedback

## Quick Facts
- arXiv ID: 2310.18239
- Source URL: https://arxiv.org/abs/2310.18239
- Reference count: 40
- Key result: Improves specification satisfaction from 60% to 90% on autonomous driving tasks

## Executive Summary
This paper introduces a method for fine-tuning pre-trained language models using automated feedback from formal verification, specifically for autonomous driving control tasks. The approach converts language model outputs into automaton-based controllers, verifies them against temporal logic specifications, and uses the verification results to guide iterative fine-tuning via direct preference optimization. The method demonstrates significant improvement in specification satisfaction rates while reducing reliance on labor-intensive human feedback.

## Method Summary
The method constructs automaton-based controllers from language model outputs describing autonomous driving tasks, then verifies these controllers against LTL specifications using model checking. The verification results (number of satisfied specifications) provide automated ranking signals for fine-tuning the language model via Direct Preference Optimization (DPO). The approach uses low-rank adaptation (LoRA) for parameter-efficient fine-tuning and employs either abstract automaton models or high-fidelity simulators for evaluation. The entire pipeline iteratively improves the model's ability to generate task descriptions that translate into controllers satisfying safety and task completion specifications.

## Key Results
- Improves specification satisfaction rate from 60% to 90% on autonomous driving benchmarks
- Demonstrates effectiveness of formal verification feedback compared to human feedback
- Shows successful real-world applicability through vision model consistency between simulation and reality

## Why This Works (Mechanism)

### Mechanism 1
Formal verification feedback provides more precise and scalable domain-specific guidance than human feedback. The method converts language model outputs into automaton-based controllers, verifies them against specifications using model checking, and uses the number of satisfied specifications as an automated ranking signal for fine-tuning.

### Mechanism 2
The method improves language model performance on domain-specific tasks by focusing fine-tuning on specification satisfaction rather than general language quality. DPO-AF collects ranked pairs of language model outputs based on formal verification results, then uses these pairs to fine-tune the model via direct preference optimization.

### Mechanism 3
The method enables real-world applicability by grounding controller decisions in visual observations that transfer from simulation to reality. Controllers make decisions based on atomic propositions representing visual observations, and if a vision model performs consistently in both environments, the controllers should behave consistently.

## Foundational Learning

- Concept: Formal verification using temporal logic specifications
  - Why needed here: The method relies on verifying automaton-based controllers against specifications expressed in temporal logic to provide automated feedback for fine-tuning.
  - Quick check question: What is the difference between □ (always) and ♢ (eventually) operators in linear temporal logic?

- Concept: Automaton-based controller construction from natural language
  - Why needed here: The method converts language model outputs (natural language task descriptions) into automaton-based controllers that can be formally verified.
  - Quick check question: How does the GLM2FSA algorithm convert a sequence of natural language steps into a finite state automaton?

- Concept: Direct preference optimization (DPO)
  - Why needed here: DPO is used to fine-tune the language model using the ranked pairs of outputs based on formal verification results.
  - Quick check question: How does DPO differ from traditional reinforcement learning from human feedback (RLHF)?

## Architecture Onboarding

- Component map: Pre-trained language model -> Prompt engineering -> GLM2FSA controller construction -> Formal verification (NuSMV) -> DPO fine-tuning -> (Optional) High-fidelity simulator

- Critical path:
  1. Collect task prompts and specifications
  2. Query language model for task descriptions
  3. Construct automaton-based controllers from responses
  4. Verify controllers against specifications
  5. Rank responses by number of satisfied specifications
  6. Feed ranked pairs to DPO for fine-tuning
  7. Repeat until convergence

- Design tradeoffs:
  - Abstract model vs. high-fidelity simulator: Abstract models enable formal verification but may miss important dynamics; simulators capture more realism but only allow empirical evaluation.
  - Conservative vs. minimal modeling: Conservative modeling (including all possible transitions) avoids missing transitions but increases computational cost; minimal modeling is more efficient but may miss important behaviors.

- Failure signatures:
  - Controller construction fails to capture intended task steps
  - Verification produces too many counter-examples or none
  - DPO training doesn't improve specification satisfaction
  - Vision model performance degrades in real-world deployment

- First 3 experiments:
  1. Verify that controller construction from a simple task description produces the expected automaton structure
  2. Check that formal verification correctly identifies specification violations in a known counter-example
  3. Confirm that DPO fine-tuning improves specification satisfaction on a held-out validation set

## Open Questions the Paper Calls Out

- How does the proposed method handle edge cases where the model checker fails to find a counterexample, but the controller still violates specifications in real-world scenarios?
- What is the impact of the quality and completeness of the world model on the effectiveness of the fine-tuning process?
- How does the proposed method scale to more complex autonomous systems with a larger number of specifications and more intricate task descriptions?
- What is the trade-off between the specificity of the specifications and the generalization ability of the fine-tuned language model?
- How does the proposed method compare to other fine-tuning approaches, such as reinforcement learning from human feedback (RLHF) or direct preference optimization (DPO), in terms of sample efficiency and final performance?

## Limitations
- The GLM2FSA algorithm for converting natural language to FSA controllers lacks detailed specification
- The method assumes vision model consistency between simulation and reality without empirical validation
- The approach may not scale well to domains beyond autonomous driving with more complex specifications

## Confidence

**High Confidence**: The method successfully improves specification satisfaction rates from 60% to 90% on the autonomous driving benchmarks; the DPO-AF fine-tuning framework is technically sound; the automaton-based modeling approach is appropriate for autonomous driving scenarios.

**Medium Confidence**: The assumption that formal verification feedback provides more precise guidance than human feedback; the claim that improved specification satisfaction translates to better real-world performance; the effectiveness of the controller construction algorithm in capturing intended task steps.

**Low Confidence**: The vision model consistency assumption between simulation and reality; the generalizability of the approach to domains beyond autonomous driving; the scalability of the method to more complex tasks with longer sequences.

## Next Checks

1. **GLM2FSA Algorithm Validation**: Implement and test the controller construction algorithm on a simplified driving scenario to verify it produces the expected automaton structure and captures all necessary transitions.

2. **Specification Robustness Test**: Evaluate the method's performance when specifications are intentionally made more stringent or when adversarial task descriptions are provided, to test the robustness of the ranking mechanism.

3. **Vision Model Consistency Validation**: Conduct a controlled experiment comparing object detection performance in simulation versus real-world conditions for the specific traffic elements used in the autonomous driving tasks.