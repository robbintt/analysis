---
ver: rpa2
title: Action-Item-Driven Summarization of Long Meeting Transcripts
arxiv_id: '2312.17581'
source_url: https://arxiv.org/abs/2312.17581
tags:
- u1d452
- u1d45b
- summaries
- meeting
- u1d460
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for generating action-item-driven
  summaries of long meeting transcripts. The method recursively divides the transcript
  into topic-based sections, generates summaries for each section in parallel, extracts
  action items using a fine-tuned classifier and neighborhood summarization, and combines
  the sectional summaries to produce a coherent final summary.
---

# Action-Item-Driven Summarization of Long Meeting Transcripts

## Quick Facts
- arXiv ID: 2312.17581
- Source URL: https://arxiv.org/abs/2312.17581
- Authors: 
- Reference count: 33
- Primary result: 4.98% improvement in BERTScore over BART baseline on AMI corpus

## Executive Summary
This paper presents a novel approach for generating action-item-driven summaries of long meeting transcripts. The method recursively divides transcripts into topic-based sections, generates summaries in parallel, extracts action items with context, and combines sectional summaries to produce coherent final summaries. Three novel topic segmentation algorithms are introduced, with the best achieving a 1.36% improvement in BERTScore over linear segmentation. The overall pipeline achieves a BERTScore of 64.98 on the AMI corpus, a 4.98% improvement over the current state-of-the-art BART model.

## Method Summary
The approach recursively divides long meeting transcripts into topically coherent sections using three novel segmentation algorithms (chunked linear, simple cosine, complex cosine), then generates summaries for each section in parallel using a fine-tuned BART model. Action items are extracted using a fine-tuned BERT classifier combined with neighborhood summarization to provide context. The sectional summaries are then recursively combined with action items to produce the final summary. The pipeline is evaluated on the AMI corpus and shows significant improvements over baseline methods in both automated metrics and human evaluations.

## Key Results
- BERTScore of 64.98 on AMI corpus, a 4.98% improvement over BART baseline
- 1.36% improvement in BERTScore over linear segmentation using complex cosine segmentation
- 95.4% accuracy in action item classification on test dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic segmentation improves summarization performance by preserving contextual coherence and reducing memory burden.
- Mechanism: The algorithm recursively divides long meeting transcripts into topically coherent sections before summarization. This prevents the summarization model from losing context due to input token limits and ensures each section contains related content for more coherent summaries.
- Core assumption: Topic-based segmentation is more effective than linear segmentation for meeting summarization because it preserves semantic continuity within sections.
- Evidence anchors:
  - [abstract] "This paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the issue of large language models (LLMs) forgetting long-term dependencies."
  - [section] "We propose three simple but effective topic segmentation techniques that were able to generate more truthful and concise summaries when compared to linear segmentation."
- Break condition: If topic boundaries are too fine-grained, summaries may become fragmented and lose overall coherence. If similarity thresholds are too high, segmentation may fail to create meaningful topics.

### Mechanism 2
- Claim: Action-item extraction using neighborhood summarization improves the utility of generated summaries by preserving context-rich action items.
- Mechanism: The algorithm identifies sentences containing action items, then extracts context from surrounding sentences using neighborhood summarization to create context-rich action items. These are appended to sectional summaries before final recursive summarization.
- Core assumption: Context from surrounding sentences is necessary to make action items meaningful and actionable for readers.
- Evidence anchors:
  - [section] "In the next sections, we discuss existing methods to solve this problem-of-context problem, explain their limitations, and present our own technique... We define a sentence's neighborhood as the three sentences before the sentence, the sentence itself, and the two sentences after the sentence."
  - [section] "We believe the reason this technique works so well is because the reference summaries in the dialogue datasets that our BART model is fine-tuned on are naturally action-item driven, to some extent."
- Break condition: If neighborhood size is too small, context may be insufficient. If too large, summaries may drift away from the action item itself.

### Mechanism 3
- Claim: Parallel processing of sectional summaries significantly reduces computational time without sacrificing quality.
- Mechanism: The algorithm generates summaries for each topic-based chunk in parallel, then combines them recursively. This exploits the independence of sectional summaries while maintaining quality through the recursive summarization approach.
- Core assumption: Sectional summaries are largely independent and can be processed in parallel without loss of coherence in the final output.
- Evidence anchors:
  - [section] "In addition, we noticed that since each general sectional summary is independent of one another, they can be generated in parallel. To the best of our knowledge, we are the first to incorporate parallelism in the divide-and-conquer summarization algorithm as seen in Algorithm 3."
- Break condition: If topic segmentation creates overly dependent sections, parallel processing may lead to inconsistencies or missed connections in the final summary.

## Foundational Learning

- Concept: Topic segmentation using cosine similarity between sentence embeddings
  - Why needed here: To divide long transcripts into coherent topical sections that preserve context and improve summarization quality
  - Quick check question: What similarity threshold value was chosen and why? (Answer: 0, because it ensures chunks are more semantically dissimilar than similar, favoring large topics and preventing excessive fragmentation)

- Concept: Action item classification using fine-tuned BERT models
  - Why needed here: To identify which sentences in meeting transcripts contain actionable items that should be extracted and included in summaries
  - Quick check question: What was the classification accuracy achieved on the test dataset? (Answer: 95.4%)

- Concept: Recursive summarization with BART models
  - Why needed here: To handle long documents by breaking them into chunks, summarizing each, then summarizing the combined summaries until a final coherent summary is produced
  - Quick check question: What is the maximum input token limit for the BART model used? (Answer: 1024 tokens)

## Architecture Onboarding

- Component map: Input transcript → Topic segmentation → Parallel sectional summarization → Action item extraction with neighborhood summarization → Combine sectional summaries → Recursive final summarization → Output summary
- Critical path: Topic segmentation → Parallel sectional summarization → Combine and recursive summarization
- Design tradeoffs: Parallel processing vs. potential loss of cross-section coherence; complex cosine segmentation vs. simpler chunked linear segmentation; context-rich action items vs. potential summary length increase
- Failure signatures: Poor topic segmentation leads to fragmented summaries; insufficient neighborhood context produces meaningless action items; token limit violations cause model failures
- First 3 experiments:
  1. Compare BERTScore performance of simple cosine segmentation vs. linear segmentation on AMI dataset
  2. Test neighborhood size variations (2-3-2 vs 3-2-3 vs 4-2-2) for action item context extraction
  3. Evaluate parallel vs sequential processing time and quality trade-offs on longer transcripts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating additional meeting summary components like decisions made, main topics, and tension levels impact the effectiveness of the action-item-driven summarization approach?
- Basis in paper: [explicit] The paper mentions that incorporating these elements may lower automated evaluation scores but could still be more useful for human readers.
- Why unresolved: The paper does not explore or test the inclusion of these additional components in the summarization pipeline.
- What evidence would resolve it: Conducting experiments that compare summaries with and without these additional components using both automated metrics and human evaluations.

### Open Question 2
- Question: What is the optimal similarity threshold for the complex cosine segmentation algorithm that balances topic coherence and summary quality?
- Basis in paper: [inferred] The paper uses a threshold of 0 for cosine similarity but mentions that increasing it to 0.2 decreased BERTScores and ROUGE-L scores by >1%.
- Why unresolved: The paper only tests two threshold values (0 and 0.2) and does not explore the full range of possible thresholds.
- What evidence would resolve it: Systematically testing a range of threshold values and measuring their impact on summary quality using both automated metrics and human evaluations.

### Open Question 3
- Question: How does the performance of the proposed action-item extraction technique compare to other context resolution methods beyond neighborhood summarization?
- Basis in paper: [explicit] The paper introduces neighborhood summarization as an effective technique but mentions that other methods like coreference resolution were not satisfactory.
- Why unresolved: The paper does not compare neighborhood summarization to other potential context resolution techniques.
- What evidence would resolve it: Implementing and testing alternative context resolution methods, such as dependency parsing or semantic role labeling, and comparing their performance to neighborhood summarization in terms of action-item extraction accuracy and summary quality.

## Limitations

- Generalization across domains: The approach shows strong performance on AMI meeting transcripts but may not generalize to other meeting types or domains without retraining.
- Resource intensity: While parallel processing reduces computational time, the overall pipeline remains computationally intensive due to multiple fine-tuning stages and recursive summarization steps.
- Action item dependency: The pipeline's effectiveness heavily depends on accurate action item detection, which may fail on new data types.

## Confidence

- High confidence: BERTScore improvement (4.98% over BART) and effectiveness of topic segmentation in reducing context loss and improving summary quality
- Medium confidence: Action item extraction mechanism and its contribution to summary utility
- Medium confidence: Parallel processing benefits and time efficiency improvements

## Next Checks

1. **Ablation study on action item extraction**: Remove the action item detection and neighborhood summarization components and compare BERTScore performance to determine the actual contribution of action-item-driven summarization to overall quality.

2. **Cross-domain evaluation**: Test the pipeline on meeting transcripts from different domains (e.g., medical, legal, or customer service meetings) to assess generalization capabilities and identify domain-specific limitations.

3. **Computational efficiency benchmarking**: Measure wall-clock time and GPU memory usage for the complete pipeline across transcripts of varying lengths (5, 15, 30, 60+ minutes) to quantify the practical resource requirements and scalability limits.