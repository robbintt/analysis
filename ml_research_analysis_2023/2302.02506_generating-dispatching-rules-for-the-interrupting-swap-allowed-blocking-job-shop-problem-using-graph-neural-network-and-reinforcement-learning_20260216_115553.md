---
ver: rpa2
title: Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job
  Shop Problem Using Graph Neural Network and Reinforcement Learning
arxiv_id: '2302.02506'
source_url: https://arxiv.org/abs/2302.02506
tags:
- machine
- gnn-rl
- scheduling
- isbjssp
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The interrupting swap-allowed blocking job shop problem (ISBJSSP)
  is a complex scheduling problem that models manufacturing environments with limited
  storage capacity and random machine interruptions. This study proposes a method
  that combines graph neural networks (GNN) and reinforcement learning (RL) to generate
  adaptive dispatching rules for ISBJSSP.
---

# Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning

## Quick Facts
- arXiv ID: 2302.02506
- Source URL: https://arxiv.org/abs/2302.02506
- Reference count: 0
- Primary result: GNN-RL method outperforms traditional rules under low-to-moderate machine interruptions (≤10%)

## Executive Summary
This paper addresses the Interrupting Swap-Allowed Blocking Job Shop Scheduling Problem (ISBJSSP), a complex scheduling scenario where machines experience random interruptions and jobs face blocking constraints due to limited storage. The authors propose a novel approach combining Graph Neural Networks (GNN) and Reinforcement Learning (RL) to generate adaptive dispatching rules. By formulating the problem as a Markov Decision Process with a dynamic disjunctive graph representation, the method enables real-time scheduling despite machine shutdowns. The approach is trained on random ISBJSSP instances and evaluated on benchmark problems, demonstrating superior performance to traditional dispatching rules under realistic interruption probabilities.

## Method Summary
The method formulates ISBJSSP as an MDP where the state is represented by a dynamic disjunctive graph. A GNN processes this graph to generate embeddings that capture neighborhood and node features, which are then used by a PPO-based actor-critic RL agent to select dispatching actions. The system is trained on randomly generated ISBJSSP instances and evaluated on benchmark datasets. Key components include the GNN with K=3 message-passing layers, PPO with clipped surrogate objective, and a custom simulator that models machine interruptions, swapping, and blocking constraints.

## Key Results
- GNN-RL dispatchers outperform or match traditional rules under interruption probabilities up to 10%
- The method demonstrates robust generalization across job shop sizes from 10×5 to 30×10
- Performance degrades when interruption probabilities exceed 15%, with makespan approaching random dispatching levels

## Why This Works (Mechanism)

### Mechanism 1
The dynamic disjunctive graph formulation enables the RL agent to handle real-time changes in machine availability. When machines shut down, nodes and edges are temporarily removed from the graph, and when reactivated, they are added back. This preserves problem state information in a form suitable for GNN embedding.

### Mechanism 2
The GNN learns neighborhood-aware embeddings that encode both graph topology and node features. Multiple message-passing layers aggregate information from predecessors, successors, and disjunctive neighbors, enabling context-aware dispatching decisions.

### Mechanism 3
PPO's clipped surrogate objective stabilizes policy updates, allowing safe exploration even under stochastic machine interruptions. The clip operation prevents large policy updates that could destabilize learning, while the entropy bonus encourages exploration.

## Foundational Learning

- Concept: Graph Neural Networks
  - Why needed here: To embed the dynamic disjunctive graph into a fixed-size vector that captures both topology and node features for RL policy input.
  - Quick check question: How does a GNN update a node's embedding using information from its neighbors?

- Concept: Markov Decision Process (MDP)
  - Why needed here: To formalize the scheduling process as a sequential decision-making problem where the agent selects operations to load onto machines.
  - Quick check question: What are the state, action, and reward components in the ISBJSSP MDP formulation?

- Concept: Reinforcement Learning with Proximal Policy Optimization
  - Why needed here: To learn a dispatching policy that adapts to changing machine availability without requiring recomputation from scratch.
  - Quick check question: What is the purpose of the clipped surrogate function in PPO?

## Architecture Onboarding

- Component map: ISBJSSP Simulator -> GNN -> RL policy -> action -> simulator
- Critical path: 1) Simulator updates graph, 2) GNN generates embeddings, 3) RL policy selects action, 4) Simulator executes action and updates state
- Design tradeoffs:
  - Fixed GNN depth vs. full graph coverage: Shallow GNN may miss long-range dependencies
  - Reward design: Negative job count per step is simple but may not prioritize critical jobs
  - Discount factor: γ=1.0 gives full credit to later rewards; γ=0.9 may encourage short-term gains
- Failure signatures:
  - High variance in makespan across runs → possibly unstable GNN embeddings or PPO updates
  - Performance collapse when interruptions exceed ~10% → learned policy may not generalize to extreme disruption
  - Makespan comparable to or worse than random PDRs → either GNN fails to capture graph structure or RL policy fails to exploit it
- First 3 experiments:
  1. Run ISBJSSP simulator with no interruptions and verify that graph deletion/addition logic works as expected
  2. Train GNN-RL on small random instances and check that node embeddings change when graph topology changes
  3. Evaluate trained policy on SBJSSP (P=0) and compare makespan to FIFO and SPT baselines

## Open Questions the Paper Calls Out

- Question: How does the performance of GNN-RL schedulers compare to traditional optimization methods (e.g., mathematical programming) for ISBJSSP when machine interruptions are absent or minimal?
  - Basis: The authors state that under perfect job shop conditions, mathematical optimization can produce more superior schedules than GNN-RL methods.
  - Evidence needed: Experimental results comparing makespans of GNN-RL and mathematical programming methods on ISBJSSP instances without interruptions.

- Question: What is the impact of incorporating domain-specific knowledge (e.g., machine failure patterns, job priorities) into the reward function and GNN-RL training process for ISBJSSP?
  - Basis: The authors mention that domain-specific knowledge could be incorporated in real production environments to build specialized reward functions.
  - Evidence needed: Experimental results comparing GNN-RL performance with and without domain-specific knowledge incorporated.

- Question: How does the GNN-RL method scale to larger job shop instances (e.g., 50×50 or 100×100) in terms of computational time and scheduling performance?
  - Basis: The authors demonstrate the method's ability to generalize to different job shop sizes, but the largest tested is 30×10.
  - Evidence needed: Experimental results showing computational time and performance on large job shop instances.

## Limitations
- Performance degrades significantly under high interruption probabilities (>15%)
- Fixed-radius neighborhood in GNN may miss long-range dependencies in larger job shops
- Simple negative job count reward function may not optimally capture complex scheduling objectives

## Confidence
- Core claims effectiveness under moderate interruptions: High
- Generalization across job shop sizes: Medium
- Robustness under high interruption probabilities: Low

## Next Checks
1. Test the trained policy on instances with interruption probabilities exceeding 15% to evaluate performance collapse boundaries.
2. Compare makespan results when varying GNN depth (K) to determine if deeper architectures improve long-range dependency capture.
3. Implement alternative reward functions (e.g., weighted combination of makespan and tardiness) to assess impact on learned dispatching policies.