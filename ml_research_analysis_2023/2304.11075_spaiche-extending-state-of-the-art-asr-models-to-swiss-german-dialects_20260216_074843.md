---
ver: rpa2
title: 'Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects'
arxiv_id: '2304.11075'
source_url: https://arxiv.org/abs/2304.11075
tags:
- german
- whisper
- swiss
- speech
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates state-of-the-art ASR models (XLS-R and Whisper)
  on Swiss German dialects, a low-resource language, and proposes a novel semantic
  loss to improve performance. The study fine-tunes Whisper on Swiss German datasets,
  achieving a WER of 20.6 and BLEU of 66.6 on the SDS-200 dataset, outperforming previous
  results.
---

# Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects

## Quick Facts
- arXiv ID: 2304.11075
- Source URL: https://arxiv.org/abs/2304.11075
- Reference count: 24
- Key outcome: Fine-tuning Whisper on Swiss German datasets achieves WER of 20.6 and BLEU of 66.6 on SDS-200, outperforming previous results

## Executive Summary
This paper addresses the challenge of automatic speech recognition (ASR) for Swiss German dialects, a low-resource language with significant dialectal variation. The authors evaluate state-of-the-art models (XLS-R and Whisper) and propose a novel semantic loss that considers semantic distance between predicted and ground-truth transcriptions. They fine-tune Whisper on Swiss German datasets, achieving state-of-the-art results with WER of 20.6 and BLEU of 66.6 on the SDS-200 dataset. The study also identifies catastrophic forgetting as a limitation when fine-tuning on Swiss German.

## Method Summary
The authors evaluate and fine-tune state-of-the-art ASR models (XLS-R and Whisper) on Swiss German datasets, including SwissDial (3 hours/dialect across 8 dialects), SDS-200 (200 hours with 4k speakers), and SPC (293 hours of parliamentary speeches). They implement a novel semantic loss (LSemantiX) that combines cross-entropy with semantic distance computed using XLM-RoBERTa embeddings. The fine-tuning process involves first training on SwissDial and SPC, then on SDS-200 with the semantic loss. Evaluation metrics include WER, CER, BLEU, and semantic distance (SemDist).

## Key Results
- Fine-tuned Whisper achieves WER of 20.6 and BLEU of 66.6 on SDS-200 dataset
- Semantic loss (LSemantiX) improves semantic distance while maintaining competitive WER and BLEU scores
- Catastrophic forgetting observed when fine-tuning on Swiss German, degrading performance on standard German control dataset

## Why This Works (Mechanism)

### Mechanism 1
Whisper's pre-training on multilingual speech enables zero-shot performance on unseen languages like Swiss German through large-scale weak supervision on 680k hours of speech across 96 languages, creating generalizable representations that transfer to dialects without explicit training.

### Mechanism 2
The semantic loss (LSemantiX) improves alignment between predicted and ground-truth transcriptions by combining cross-entropy loss with semantic distance computed using multilingual sentence embeddings, encouraging the model to produce outputs with similar meaning even if surface forms differ.

### Mechanism 3
Fine-tuning Whisper on Swiss German datasets while preserving generalization to other German dialects through transfer learning that adapts Whisper's representations to the target domain while leveraging pre-trained multilingual knowledge.

## Foundational Learning

- **Cross-entropy loss for sequence prediction**: Used in Whisper for predicting token sequences from speech embeddings during both pre-training and fine-tuning. *Quick check: What is the mathematical form of cross-entropy loss used in ASR models for predicting token sequences?*

- **Word Error Rate (WER) and Character Error Rate (CER) metrics**: Standard evaluation metrics for ASR systems measuring edit distance between predictions and references. *Quick check: How are WER and CER computed, and what do they measure in ASR evaluation?*

- **Multilingual sentence embeddings**: Used by semantic loss to compute semantic distance between predictions and references using XLM-RoBERTa. *Quick check: How do multilingual sentence embeddings capture semantic similarity across languages?*

## Architecture Onboarding

- **Component map**: Speech input → Feature extractor → Encoder → Decoder → Token predictions → Loss computation (CE + Semantic) → Backpropagation
- **Critical path**: Audio preprocessing → Feature extraction → Encoder processing → Decoder output → Loss calculation → Parameter updates
- **Design tradeoffs**: Whisper medium (769M) balances performance and computational cost vs. tiny (39M) or large (1.5B) variants
- **Failure signatures**: Catastrophic forgetting on general German after Swiss German fine-tuning; poor semantic loss gradients if embeddings don't capture target semantics
- **First 3 experiments**:
  1. Zero-shot evaluation of Whisper on Swiss German datasets to establish baseline performance
  2. Fine-tuning Whisper with only cross-entropy loss on Swiss German data to measure adaptation
  3. Fine-tuning with LSemantiX loss to evaluate semantic loss impact on WER, BLEU, and semantic distance

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed semantic loss (LSemantiX) perform compared to traditional cross-entropy loss in terms of generalization to unseen dialects or languages? The paper introduces LSemantiX but does not extensively compare its generalization performance to cross-entropy loss.

### Open Question 2
How does the catastrophic forgetting observed in Whisper models after fine-tuning affect their performance on other languages or tasks? The paper observes catastrophic forgetting but does not investigate its extent on other languages or propose mitigation techniques.

### Open Question 3
How does the semantic distance metric (SemDist) correlate with traditional ASR metrics (WER, CER, and BLEU) in terms of capturing the quality of transcriptions? The paper introduces SemDist as providing additional insights but does not provide detailed correlation analysis with traditional metrics.

## Limitations
- Test sets overlap with training data (SwissDial test set contains 20% of SPC training data), potentially inflating performance estimates
- Catastrophic forgetting observed when fine-tuning on Swiss German, degrading performance on standard German
- Semantic loss effectiveness depends on quality of XLM-RoBERTa embeddings for Swiss German semantics, which may not be well-validated for this low-resource language

## Confidence

**High Confidence**: Whisper's zero-shot performance on Swiss German dialects is competitive with supervised models, and fine-tuning with cross-entropy loss improves WER and CER. These claims are directly supported by quantitative metrics.

**Medium Confidence**: The semantic loss (LSemantiX) provides additional semantic alignment beyond surface form accuracy. While improvements in semantic distance are reported, practical significance for end-user applications remains unclear.

**Low Confidence**: This approach represents a general solution for low-resource dialects. Results are specific to Swiss German and depend heavily on availability of transfer learning data and semantic similarity between Swiss German and languages in XLM-RoBERTa's training.

## Next Checks
1. **Cross-validation on independent dialect splits**: Re-evaluate fine-tuned models on completely independent test sets from each Swiss German dialect region to verify performance gains aren't artifacts of training-test overlap.

2. **Semantic embedding quality assessment**: Conduct human evaluation of semantic similarity judgments for Swiss German to validate whether XLM-RoBERTa embeddings capture meaningful semantic relationships in this dialect.

3. **Generalization testing to other German dialects**: Test Swiss German fine-tuned models on standard German and other German dialect datasets to quantify catastrophic forgetting and evaluate specialization-generalization tradeoff.