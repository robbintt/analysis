---
ver: rpa2
title: 'From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge
  Graph Embedding'
arxiv_id: '2303.12816'
source_url: https://arxiv.org/abs/2303.12816
tags:
- knowledge
- representations
- entity
- graph
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LiftNet, a parameter-efficient knowledge graph
  embedding method. The core idea is to replace high-dimensional entity representations
  with a narrow embedding layer and a multi-layer dimension lifting network.
---

# From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding

## Quick Facts
- arXiv ID: 2303.12816
- Source URL: https://arxiv.org/abs/2303.12816
- Reference count: 39
- Key outcome: LiftNet achieves better link prediction accuracy than strong baseline models using 4-dimensional entity representations versus 512-dimensional, saving 68.4% to 96.9% parameters.

## Executive Summary
This paper introduces LiftNet, a parameter-efficient knowledge graph embedding method that replaces high-dimensional entity representations with a narrow embedding layer and a multi-layer dimension lifting network. The core innovation uses transposed convolution layers to progressively lift low-dimensional representations (4D) to high-dimensional ones (512D) while preserving structural information. Experiments on WN18RR, FB15K237, and YAGO3-10 demonstrate that LiftNet-TransE and LiftNet-DistMult achieve superior link prediction performance with dramatically fewer parameters than traditional approaches.

## Method Summary
LiftNet implements a dimension lifting network that transforms low-dimensional entity embeddings into high-dimensional representations using multiple transposed convolution layers. The method views entity representations as a single-layer embedding network and replaces the wide embedding layer with a narrow layer plus the lifting network. The TC layers progressively reshape and upsample the input through learned filters that capture entity structure. LiftNet is integrated with TransE and DistMult scoring functions and trained end-to-end using standard KGE objectives with non-sampling loss and Adam optimizer.

## Key Results
- LiftNet-TransE achieves 0.689 MRR on WN18RR versus 0.659 MRR for standard TransE
- LiftNet-DistMult achieves 0.816 MRR on WN18RR versus 0.822 MRR for standard DistMult
- Parameter reduction of 68.4% to 96.9% compared to baseline models using 512-dimensional representations

## Why This Works (Mechanism)

### Mechanism 1
Transposed convolution layers can effectively lift low-dimensional entity embeddings to high-dimensional representations while preserving structural information. The LiftNet uses multiple TC layers to progressively reshape and upsample the low-dimensional input representation into the desired high-dimensional output through learned filters that capture entity structure. Core assumption: The compositional structure present in knowledge graphs can be captured by TC layers that progressively expand dimensions.

### Mechanism 2
Increasing depth while reducing width in the embedding network achieves parameter efficiency without sacrificing accuracy. By replacing a wide embedding layer with a narrow embedding layer plus a multi-layer dimension lifting network, the total number of parameters is reduced while maintaining representational capacity. Core assumption: Deeper networks require exponentially fewer parameters to achieve comparable expressiveness to wider networks for compositional structures.

### Mechanism 3
Using low-dimensional entity representations (4D) with LiftNet achieves better performance than high-dimensional representations (512D) without dimension lifting. The LiftNet learns to transform compact representations into expressive high-dimensional ones, effectively learning a more efficient representation than direct high-dimensional embeddings. Core assumption: The LiftNet can learn a more efficient mapping from low to high dimensions than simply using high-dimensional embeddings from the start.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: Understanding how entities and relations are represented as vectors in latent space is fundamental to grasping LiftNet's approach.
  - Quick check question: What is the primary goal of knowledge graph embedding methods?

- Concept: Transposed Convolution (TC) Layers
  - Why needed here: TC layers are the core mechanism for dimension lifting in LiftNet, replacing traditional fully-connected layers.
  - Quick check question: How do transposed convolution layers differ from regular convolution layers in terms of their effect on tensor dimensions?

- Concept: Parameter Efficiency in Neural Networks
  - Why needed here: The paper's central contribution is achieving better performance with fewer parameters, which requires understanding the relationship between model depth, width, and parameter count.
  - Quick check question: Why do deeper networks typically require fewer parameters than wider networks to achieve similar expressiveness?

## Architecture Onboarding

- Component map:
  Low-dimensional entity representations (4D) -> Multi-layer transposed convolution network -> High-dimensional entity representations (512D) -> Standard KGE scoring functions

- Critical path:
  1. Entity representations are initialized in low-dimensional space
  2. LiftNet progressively transforms these through TC layers
  3. Lifted representations are used in standard KGE scoring functions
  4. Model is trained end-to-end with standard KGE objectives

- Design tradeoffs:
  Using TC layers instead of FC layers reduces parameters but may limit expressiveness
  Lower input dimensions increase parameter efficiency but require more sophisticated lifting
  Model depth vs training complexity tradeoff

- Failure signatures:
  Poor link prediction accuracy indicates the LiftNet isn't learning effective transformations
  Overfitting on small datasets suggests too many parameters in LiftNet layers
  Training instability may indicate inappropriate TC layer configurations

- First 3 experiments:
  1. Test LiftNet-TransE with varying input dimensions (4D, 8D, 16D) on WN18RR to find optimal balance
  2. Compare LiftNet variants (TC vs FC layers) on link prediction accuracy
  3. Evaluate parameter efficiency by measuring parameter reduction vs accuracy tradeoff across different output dimensions

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
The paper relies heavily on empirical results without strong theoretical justification for why transposed convolutions would be particularly effective for dimension lifting in KGE. The comparison to traditional fully-connected layers is limited, and the paper doesn't thoroughly explore how LiftNet would perform with different input/output dimensionalities or varying numbers of TC layers.

## Confidence

**High Confidence**: The parameter efficiency claims are well-supported by experimental results showing 68.4%-96.9% parameter reduction while maintaining or improving accuracy across three datasets.

**Medium Confidence**: The mechanism claims about transposed convolutions being more effective than fully-connected layers are supported by empirical results but lack strong theoretical grounding.

**Low Confidence**: The general claim that deeper networks require exponentially fewer parameters than wider networks for compositional structures is stated but not rigorously proven within the context of KGE.

## Next Checks

1. Conduct an ablation study by removing LiftNet and using standard high-dimensional embeddings in TransE/DistMult to quantify the exact contribution of dimension lifting versus other architectural changes.

2. Systematically vary the input dimension (e.g., 2D, 4D, 8D, 16D) to determine the optimal balance between parameter efficiency and performance, and identify at what point performance degrades significantly.

3. Compare LiftNet using different numbers of TC layers (1, 2, 3 layers) and different filter configurations to understand how the lifting architecture affects performance and parameter efficiency.