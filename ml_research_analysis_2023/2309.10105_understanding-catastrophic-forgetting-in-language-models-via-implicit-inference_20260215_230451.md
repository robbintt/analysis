---
ver: rpa2
title: Understanding Catastrophic Forgetting in Language Models via Implicit Inference
arxiv_id: '2309.10105'
source_url: https://arxiv.org/abs/2309.10105
tags:
- fine-tuning
- task
- learning
- language
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can lose certain capabilities after fine-tuning,
  especially for tasks outside the fine-tuning distribution. This is particularly
  problematic for in-context learning and safety fine-tuning.
---

# Understanding Catastrophic Forgetting in Language Models via Implicit Inference

## Quick Facts
- arXiv ID: 2309.10105
- Source URL: https://arxiv.org/abs/2309.10105
- Reference count: 40
- Key outcome: Large language models can lose certain capabilities after fine-tuning, especially for tasks outside the fine-tuning distribution. Conjugate prompting via translation can recover some pretraining capabilities.

## Executive Summary
This paper investigates catastrophic forgetting in language models, proposing that fine-tuning shifts the model's implicit task inference toward the fine-tuning distribution rather than erasing capabilities entirely. The authors introduce conjugate prompting, a method that transforms prompts to appear dissimilar to fine-tuning data, enabling recovery of pretraining capabilities. Experiments demonstrate that translating prompts to other languages can recover in-context learning and safety-related capabilities lost due to fine-tuning, particularly when fine-tuning datasets are predominantly in English.

## Method Summary
The authors propose conjugate prompting as a solution to catastrophic forgetting, based on the hypothesis that fine-tuning alters task inference rather than capabilities. The method involves transforming prompts (e.g., via language translation) to reduce their likelihood under the fine-tuning distribution, then applying an inverse transformation to recover the original task answer. The approach was tested using synthetic linear regression tasks with transformer models and extended to real-world language models, demonstrating recovery of both mathematical reasoning and safety-related capabilities.

## Key Results
- Translation-based conjugate prompting consistently reduces performance degradation on out-of-distribution tasks
- Models retain pretraining capabilities but shift task inference toward fine-tuning distribution
- Safety fine-tuning can be partially reversed using conjugate prompting to recover harmful content generation
- The effectiveness correlates with the language distribution in fine-tuning datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning shifts the model's implicit task inference toward the fine-tuning distribution, degrading performance on other tasks.
- Mechanism: Language models implicitly infer the task of the prompt, and fine-tuning skews this inference toward tasks in the fine-tuning distribution.
- Core assumption: The model's capability to solve tasks is preserved, but task inference is altered.
- Evidence anchors:
  - [abstract] "We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution."
  - [section 2.6] "We hypothesize that during fine-tuning, the drop in performance on the continuous distribution is largely driven by altered task inference."
  - [corpus] Weak - no direct citations on task inference mechanism.
- Break condition: If the model's capabilities degrade alongside task inference, this mechanism fails.

### Mechanism 2
- Claim: Conjugate prompting recovers pretraining capabilities by making prompts less likely under the fine-tuning distribution.
- Mechanism: Translating prompts to other languages lowers their likelihood under the predominantly English fine-tuning distribution, causing the model to infer the task as more aligned with pretraining.
- Core assumption: The model's pretraining capabilities are preserved and can be accessed by altering task inference.
- Evidence anchors:
  - [section 4.1] "Since fine-tuning datasets are curated and primarily in English, we apply conjugate prompting with language translation to lower the likelihood of being drawn from the fine-tuning distribution while preserving the core task."
  - [table 1] "We see that translation always results in a smaller drop in ICL frequency compared to English prompts."
  - [corpus] Weak - no direct citations on language-based task inference manipulation.
- Break condition: If translation doesn't sufficiently alter task inference or if pretraining capabilities are lost.

### Mechanism 3
- Claim: The model's output can be transformed back to the original task space after conjugate prompting.
- Mechanism: Since the solution to the transformed prompt can be easily recovered from the solution to the original task, we can apply an inverse transformation to get the correct answer.
- Core assumption: The transformation preserves the core task and allows for inversion.
- Evidence anchors:
  - [section 3] "There should exist an inverse to the prompting strategy s−1 to convert the answer T (P ′) to an answer to P."
  - [section 2.8] "Therefore, to make the model perform ridge regression instead of discrete regression, we compose our insights into the following prompting strategy. Instead of directly feeding our prompt into the model, we scale the labels γ, feed the model the scaled prompt, and scale down the model output."
  - [corpus] Weak - no direct citations on inverse transformations in prompting.
- Break condition: If the inverse transformation doesn't accurately recover the original task solution.

## Foundational Learning

- Concept: Implicit task inference in language models
  - Why needed here: The paper's hypothesis relies on understanding how models infer tasks from prompts.
  - Quick check question: Can you explain why a model might interpret "What is 2+2?" differently than "Calculate 2 plus 2"?

- Concept: Catastrophic forgetting vs. capability suppression
  - Why needed here: The paper distinguishes between losing capabilities entirely versus suppressing them through task inference changes.
  - Quick check question: How would you test whether a capability is truly forgotten or just suppressed?

- Concept: Conjugate prompting strategy
  - Why needed here: The proposed solution relies on understanding how to transform prompts to access suppressed capabilities.
  - Quick check question: What properties must a conjugate prompting transformation have to work effectively?

## Architecture Onboarding

- Component map: Prompt → Transformation → Model Output → Inverse Transformation → Final Answer
- Critical path: Prompt → Transformation → Model Output → Inverse Transformation → Final Answer
- Design tradeoffs: Translation quality vs. task preservation; computational cost of transformations vs. performance gains.
- Failure signatures: Poor translation quality leading to task drift; inverse transformation errors; model's pretraining capabilities are actually lost.
- First 3 experiments:
  1. Test conjugate prompting on a simple task (e.g., math problems) with different languages.
  2. Measure performance degradation on original tasks when using different transformation intensities.
  3. Validate that inverse transformations accurately recover original task solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning truly suppress pretrained capabilities rather than forgetting them, as suggested by the authors' hypothesis?
- Basis in paper: Inferred from the authors' hypothesis and experiments on linear regression tasks.
- Why unresolved: The hypothesis is supported by experiments on synthetic linear regression tasks and real-world language models, but further investigation is needed to confirm if this mechanism applies to other tasks and model architectures.
- What evidence would resolve it: Additional experiments on diverse tasks and model architectures, along with mechanistic interpretability studies to understand how fine-tuning affects internal representations.

### Open Question 2
- Question: Can conjugate prompting be generalized beyond language translation to other transformations that lower the likelihood of prompts under the fine-tuning distribution?
- Basis in paper: Explicit discussion of conjugate prompting and its application to language translation.
- Why unresolved: The paper focuses on language translation as a conjugate prompting strategy, but it is unclear if other transformations (e.g., style transfer, task-specific manipulations) could be equally or more effective.
- What evidence would resolve it: Experiments testing conjugate prompting with various transformations on different tasks and models.

### Open Question 3
- Question: How does the effectiveness of conjugate prompting vary with the size and diversity of the fine-tuning dataset?
- Basis in paper: Inferred from the discussion of fine-tuning datasets being smaller and less diverse than pretraining data.
- Why unresolved: The paper does not explore how the size and diversity of fine-tuning datasets impact the effectiveness of conjugate prompting.
- What evidence would resolve it: Systematic experiments varying the size and diversity of fine-tuning datasets and measuring the impact on conjugate prompting effectiveness.

### Open Question 4
- Question: Can conjugate prompting be used to recover not only suppressed capabilities but also to enhance model performance on tasks outside the fine-tuning distribution?
- Basis in paper: Inferred from the discussion of recovering pretraining capabilities via conjugate prompting.
- Why unresolved: The paper focuses on recovering suppressed capabilities, but it is unclear if conjugate prompting can also be used to improve performance on tasks not seen during fine-tuning.
- What evidence would resolve it: Experiments testing conjugate prompting on tasks outside the fine-tuning distribution to measure performance improvements.

### Open Question 5
- Question: What are the limitations of conjugate prompting in terms of task complexity and domain specificity?
- Basis in paper: Inferred from the discussion of task invertibility and the challenges of preserving task-specific contextual knowledge across languages.
- Why unresolved: The paper acknowledges limitations but does not provide a comprehensive analysis of the boundaries of conjugate prompting's applicability.
- What evidence would resolve it: Experiments testing conjugate prompting on complex, domain-specific tasks to identify its limitations and potential failure modes.

## Limitations
- Conjugate prompting may not work for non-linguistic task transformations
- Translation quality significantly impacts effectiveness, especially for low-resource languages
- Requires accurate inverse transformations to recover original task solutions

## Confidence

**Major Uncertainties:**
- **Mechanism Validation**: Medium
- **Generalizability of Conjugate Prompting**: Medium
- **Capability Preservation Assumption**: Low

## Next Checks
1. **Ablation Study on Fine-tuning Data**: Test whether conjugate prompting effectiveness correlates with the proportion of English vs. non-English data in fine-tuning. This would validate the task inference mechanism.

2. **Capability Degradation Test**: Compare model weights or internal representations before and after fine-tuning on out-of-distribution tasks to determine if capabilities are truly preserved or degraded.

3. **Cross-task Generalization**: Apply conjugate prompting to non-translation transformations (e.g., prompt rephrasing, format changes) to test if the mechanism extends beyond language-based approaches.