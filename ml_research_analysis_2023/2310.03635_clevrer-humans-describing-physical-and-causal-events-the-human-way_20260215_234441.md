---
ver: rpa2
title: 'CLEVRER-Humans: Describing Physical and Causal Events the Human Way'
arxiv_id: '2310.03635'
source_url: https://arxiv.org/abs/2310.03635
tags:
- event
- causal
- events
- clevrer-humans
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLEVRER-Humans is a benchmark for physical and causal reasoning
  from videos. It extends CLEVRER by replacing synthetic events and heuristics with
  human-annotated events and causal relations.
---

# CLEVRER-Humans: Describing Physical and Causal Events the Human Way

## Quick Facts
- arXiv ID: 2310.03635
- Source URL: https://arxiv.org/abs/2310.03635
- Reference count: 40
- Primary result: CLEVRER-Humans benchmark achieves only ~50% per-option accuracy with standard models, highlighting challenges in human-like causal reasoning from videos.

## Executive Summary
CLEVRER-Humans extends the CLEVRER video dataset by replacing synthetic events and heuristic causal labels with human-annotated events and graded causal relations. The dataset is built through iterative cloze tasks and neural event generators, producing diverse event descriptions and dense causal graphs. It poses challenges in language diversity, human-like causal judgment, and data efficiency. Models trained on the original CLEVRER perform poorly on CLEVRER-Humans, with top methods achieving only ~50% per-option accuracy, indicating a significant domain gap and the need for new approaches to physical and causal reasoning.

## Method Summary
CLEVRER-Humans uses an iterative cloze task to collect diverse human-written event descriptions and build a causal event graph (CEG). A neural event description generator, trained on the small human-annotated set, augments event descriptions for all videos. Human annotators then label causal relations between events with graded scores (1-5), which are binarized using a threshold to create the final CEG. Questions and answers are generated from the CEG structure for a video question answering task.

## Key Results
- CLEVRER-Humans poses challenges in language diversity, human-like causal judgment, and data efficiency.
- Standard models trained on CLEVRER perform poorly on CLEVRER-Humans, with top methods achieving only ~50% per-option accuracy.
- The dataset reveals a significant domain gap between heuristic and human causal judgments, requiring new approaches to physical and causal reasoning from videos.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative event cloze tasks grow causal event graph (CEG) diversity by re-using annotator responses as seeds.
- **Mechanism:** Each iteration samples a node from the current CEG and collects a cause or effect event. The new event is added to the CEG, expanding coverage without requiring a full sweep.
- **Core assumption:** Human annotators can generate plausible cause/effect events given a seed, and the seed diversity propagates through iterations.
- **Evidence anchors:** [section] "In this stage, we use an iterative event cloze task to collect human-written event descriptions." [section] "The newly annotated events will be iteratively used as new seeds to progressively grow the annotated events."
- **Break condition:** If annotators converge on a narrow set of event types or the iteration becomes repetitive, diversity stalls.

### Mechanism 2
- **Claim:** Neural event description generators trained on small human-written sets can augment all videos, reducing manual annotation cost.
- **Mechanism:** Single-object and pairwise trajectory-based models generate candidate events; post-processing filters for grammar, existence, and verb diversity.
- **Core assumption:** The small human-written set captures enough event variety for the models to generalize across the full video set.
- **Evidence anchors:** [section] "we leverage the observation that neural generative models are reasonably good at generating event descriptions." [section] "In the second stage, we augment the data for all videos using neural event description generators trained on the data collected from the first stage."
- **Break condition:** If model coverage is low or generates many false positives, human filtering cost rises and savings diminish.

### Mechanism 3
- **Claim:** Human-annotated causal edges with graded scores (1-5) capture richer causal judgments than binary heuristic rules.
- **Mechanism:** Annotators rate how responsible event A is for event B; binarization uses threshold 4 to mark strong causal links.
- **Core assumption:** Graded human ratings reflect nuanced causal perception and differ meaningfully from heuristic "responsible for" definitions.
- **Evidence anchors:** [section] "CLEVRER-Humans offers 5 choices when asking MTurkers to label the causality level." [section] "The average score is 2.37. Note that this distribution is skewed towards lower scores." [corpus] "Found 25 related papers… Average neighbor FMR=0.402…" (indicates moderate relatedness to causal reasoning corpora)
- **Break condition:** If threshold choice poorly separates true causality from noise, downstream QA accuracy suffers.

## Foundational Learning

- **Concept:** Causal Event Graph (CEG) structure
  - **Why needed here:** Provides a compact representation of events and their causal relations, enabling conversion to QA pairs.
  - **Quick check question:** In a CEG, what do nodes and directed edges represent?
- **Concept:** Counterfactual intervention in causal labeling
  - **Why needed here:** Offers an alternative causal labeling method to compare with human judgment.
  - **Quick check question:** When is event A considered a cause of event B under counterfactual intervention?
- **Concept:** Neural sequence-to-sequence with attention
  - **Why needed here:** Generates natural language event descriptions from object trajectories.
  - **Quick check question:** What role does the attention mechanism play in the event description generator?

## Architecture Onboarding

- **Component map:** Input videos -> Trajectory extraction -> Single-object generator -> Pairwise generator + rule detector -> Post-processor -> Candidate CEG nodes -> Human edge labeling -> QA conversion.
- **Critical path:** Trajectory extraction -> Generation -> Human edge labeling (determines final CEG quality).
- **Design tradeoffs:**
  - Rule-based pre-filter for pairwise events reduces false positives but may miss rare events.
  - Fixed binarization threshold (4) balances human certainty vs. coverage.
  - Using pretrained language models (BERT) helps text encoding but adds complexity.
- **Failure signatures:**
  - Low node count -> Generator coverage issue.
  - Many dropped edges -> Threshold too high or human disagreement.
  - Low QA accuracy -> Domain gap between human and heuristic labels or insufficient training data.
- **First 3 experiments:**
  1. Vary the binarization threshold (3, 4, 5) and measure edge density vs. QA accuracy.
  2. Compare trajectory-based generators with pixel-based encoders to assess input modality impact.
  3. Train QA models on a subset of CEG edges with high human scores to test quality vs. quantity tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we better measure the performance of models on CLEVRER-Humans considering the dataset's limited size and the gap between heuristic and human causal judgments?
- **Basis in paper:** [explicit] The paper highlights the challenges of data-efficient learning and the discrepancy between heuristic and human causal judgments, suggesting the need for better evaluation methods.
- **Why unresolved:** The paper does not provide specific metrics or methods to address these challenges in evaluating model performance.
- **What evidence would resolve it:** Development of new evaluation metrics that account for the dataset's size and the nuances in human causal judgments, along with empirical studies demonstrating their effectiveness.

### Open Question 2
- **Question:** Can the iterative cloze task and neural event description generation techniques be effectively extended to naturalistic videos with complex events and ambiguous relations?
- **Basis in paper:** [explicit] The paper introduces these techniques for dataset collection and suggests their potential application to similar relations, but does not explore their effectiveness in more complex scenarios.
- **Why unresolved:** The paper focuses on synthetic videos and does not test the techniques on naturalistic videos with more complex events.
- **What evidence would resolve it:** Experimental results showing the application of these techniques to naturalistic videos and their impact on the quality and diversity of event descriptions and causal relations.

### Open Question 3
- **Question:** What are the specific ways in which the graded causal reasoning task can be improved to better align with human judgments in dynamic videos?
- **Basis in paper:** [explicit] The paper mentions the potential of graded causal reasoning in cognitive science studies but does not provide specific methods for improvement.
- **Why unresolved:** The paper does not propose or evaluate methods for enhancing the alignment between machine and human graded causal judgments in dynamic videos.
- **What evidence would resolve it:** Development and validation of models that can make graded causal judgments in dynamic videos, with results showing improved alignment with human judgments.

## Limitations
- The dataset size is limited, making data-efficient learning a significant challenge.
- The binarization threshold for causal edges is fixed, which may not optimally balance edge density and QA accuracy.
- The effectiveness of the iterative cloze task and neural generators in naturalistic videos with complex events is untested.

## Confidence
- **High:** Iterative cloze method for growing event diversity
- **Medium:** Neural generators can augment event descriptions
- **Medium:** Graded human causal labels capture richer judgments than heuristics

## Next Checks
1. Perform ablation study varying the binarization threshold (3, 4, 5) and analyze the tradeoff between edge density and QA accuracy.
2. Compare trajectory-based event generators with pixel-based encoders to quantify the impact of input modality on event coverage and quality.
3. Train QA models on subsets of high-scoring human causal edges versus all edges to assess the importance of label quality versus quantity.