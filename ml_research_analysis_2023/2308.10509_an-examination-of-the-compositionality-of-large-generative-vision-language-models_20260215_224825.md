---
ver: rpa2
title: An Examination of the Compositionality of Large Generative Vision-Language
  Models
arxiv_id: '2308.10509'
source_url: https://arxiv.org/abs/2308.10509
tags:
- gvlms
- arxiv
- visual
- score
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines the evaluation of compositionality in Generative
  Vision-Language Models (GVLMs), focusing on the limitations of existing benchmarks
  and metrics. The authors identify a morphological bias in current benchmarks, where
  GVLMs can exploit syntactic correctness over semantic content.
---

# An Examination of the Compositionality of Large Generative Vision-Language Models

## Quick Facts
- arXiv ID: 2308.10509
- Source URL: https://arxiv.org/abs/2308.10509
- Reference count: 10
- Primary result: The paper identifies morphological bias in current benchmarks for evaluating GVLMs and proposes a new benchmark MODE along with the MorphoBias Score to address this issue.

## Executive Summary
This paper examines the evaluation of compositionality in Generative Vision-Language Models (GVLMs), focusing on the limitations of existing benchmarks and metrics. The authors identify a morphological bias in current benchmarks, where GVLMs can exploit syntactic correctness over semantic content. To address this, they propose a MorphoBias Score to quantify the bias and a new benchmark, MODE, which calibrates the bias and introduces a challenging content-only understanding task. The study concludes that VisualGPTScore is a more suitable metric for evaluating GVLMs' multimodal compositionality compared to other metrics. The MODE benchmark provides a more robust and unbiased evaluation framework, facilitating future research in this direction.

## Method Summary
The study proposes a new benchmark called MODE to evaluate the compositionality of GVLMs. MODE addresses morphological bias in existing benchmarks by filtering datasets using LLMs to reduce bias and adding a content-only understanding challenge. The primary metric used is VisualGPTScore, which evaluates the alignment of image-text pairs. The MorphoBias Score quantifies morphological bias in benchmarks. The method involves downloading the MODE benchmark code and datasets, implementing the MorphoBias Score calculation, and evaluating GVLMs on the MODE benchmark using VisualGPTScore.

## Key Results
- The study identifies morphological bias in current benchmarks for evaluating GVLMs, where models can exploit syntactic correctness over semantic content.
- A new benchmark, MODE, is proposed to address this bias and introduce a content-only understanding challenge.
- VisualGPTScore is found to be a more suitable metric for evaluating GVLMs' multimodal compositionality compared to other metrics like BERTScore and CLIPScore.

## Why This Works (Mechanism)

### Mechanism 1
- Generative scores (VisualGPTScore, GPTScore) are more sensitive to word order than contrastive scores (BERTScore).
- Core assumption: The training objective shapes the model's sensitivity to word order and sentence structure.
- Evidence: [section 3.4] "Generative scores mitigate bags-of-words... generative metrics (VisualGPTScore & GPTScore) are more sensitive to the order and structure of reference sentences compared with contrastive metric BERTScore."

### Mechanism 2
- GVLMs sometimes prefer syntactically correct references over content-relevant ones.
- Core assumption: The linguistic capability of GVLMs, developed through training on text data, influences their preference for syntactic correctness.
- Evidence: [section 3.4] "Generative VLMs sometimes prefer syntactic correctness to contents... if the negatives are random reference sentences... the performance drops to 27.02% and 5.92%."

### Mechanism 3
- Morphological bias exists in current benchmarks for evaluating GVLMs.
- Core assumption: The construction of hard negatives in benchmarks influences the evaluation of GVLMs' compositionality.
- Evidence: [section 4.1] "we identify the existence of morphological bias in current benchmarks... benchmarks that generate hard negatives by swapping, shuffling, or replacing specific entities promote a morphological bias."

## Foundational Learning

- Concept: Vision-Language Models (VLMs)
  - Why needed here: The paper examines the compositionality of GVLMs, a specific type of VLM.
  - Quick check question: What is the main difference between contrastive VLMs and generative VLMs?

- Concept: Compositionality
  - Why needed here: The paper focuses on evaluating the compositionality of GVLMs.
  - Quick check question: What does it mean for a model to have compositional reasoning abilities?

- Concept: Morphological Bias
  - Why needed here: The paper identifies and quantifies morphological bias in current benchmarks.
  - Quick check question: How does morphological bias affect the evaluation of GVLMs?

## Architecture Onboarding

- Component map: Visual encoder -> Mapping layers -> LLM -> Generate text -> Evaluate using VisualGPTScore
- Critical path: The critical path for evaluating GVLMs' compositionality is: input image → visual encoder → mapping layers → LLM → generate text → evaluate using VisualGPTScore.
- Design tradeoffs: The choice of visual encoder (e.g., ViT, Q-Former) and LLM (e.g., LLaMA, Vicuna) impacts the model's performance and computational cost. The design of the mapping layers also affects the quality of the visual-linguistic representation.
- Failure signatures: Hallucination in generated text, preference for syntactically correct but content-irrelevant references, and sensitivity to morphological bias in benchmarks.
- First 3 experiments:
  1. Evaluate the compositionality of a GVLMs using VisualGPTScore on the Winoground benchmark.
  2. Construct hard negatives by shuffling words in reference sentences and evaluate the impact on VisualGPTScore.
  3. Create nonsensical images and evaluate the performance of GVLMs and CVLMs on existing benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hallucination problem in GVLMs be effectively mitigated to improve the reliability of generative metrics like GPTScore and BERTScore?
- Basis in paper: [explicit] The paper identifies that both zero-shot answer generation and text-generation-based metrics suffer from hallucinated predictions, with 12.13% and 4.95% of instances exhibiting hallucination on average for LLaVA and MiniGPT-4, respectively.
- Why unresolved: While the paper acknowledges the hallucination problem, it does not propose a concrete solution to address it. The issue persists and hinders the accurate evaluation of GVLMs using generative metrics.
- What evidence would resolve it: A comprehensive study that proposes and evaluates techniques to reduce hallucination in GVLMs, such as fine-tuning on curated datasets, incorporating external knowledge, or using ensemble methods to filter out nonsensical outputs.

### Open Question 2
- Question: Can the morphological bias in current multimodal compositionality benchmarks be further reduced by leveraging more advanced LLMs or alternative calibration strategies?
- Basis in paper: [explicit] The paper introduces the MorphoBias Score to quantify morphological bias and proposes a strategy to calibrate existing benchmarks using LLMs. However, it does not explore the potential of using more advanced LLMs or alternative calibration strategies.
- Why unresolved: The paper's calibration strategy relies on a single LLM (Vicuna-13B-v1.5) and does not investigate the impact of using more advanced LLMs or alternative methods for bias reduction. The effectiveness of the proposed calibration strategy is also not extensively validated.
- What evidence would resolve it: Experiments comparing the performance of different LLMs (e.g., GPT-4, LLaMA-2) in calibrating the morphological bias, as well as exploring alternative calibration strategies such as adversarial training or self-supervised learning.

### Open Question 3
- Question: How can the content-only understanding challenge in the MODE benchmark be further refined to better assess the robustness of GVLMs against their inherent inclination towards syntactically correct reference sentences?
- Basis in paper: [explicit] The paper introduces a content-only understanding challenge in the MODE benchmark to evaluate the robustness of GVLMs. However, it does not provide a detailed analysis of the challenge's effectiveness or explore ways to refine it.
- Why unresolved: The paper does not discuss the limitations of the content-only understanding challenge or propose strategies to improve its effectiveness in assessing the robustness of GVLMs. The challenge's impact on the overall evaluation of GVLMs is also not thoroughly explored.
- What evidence would resolve it: A comprehensive analysis of the content-only understanding challenge, including its strengths and weaknesses, as well as experiments comparing its performance with alternative challenge designs. Additionally, exploring the impact of the challenge on the overall evaluation of GVLMs and its correlation with human judgments would provide valuable insights.

## Limitations

- The MorphoBias Score's effectiveness depends on the judgment of the LLM used for evaluation, introducing potential subjectivity.
- The MODE benchmark may not fully capture the diverse range of compositionality challenges that GVLMs might face in real-world scenarios.
- The effectiveness of VisualGPTScore across different types of GVLMs and diverse datasets beyond those tested in this study is not fully established.

## Confidence

- **High Confidence**: The identification of morphological bias in current benchmarks and the proposal of the MorphoBias Score to quantify it are well-supported by experimental evidence.
- **Medium Confidence**: The effectiveness of the MODE benchmark in providing a more robust evaluation of GVLMs' compositionality is supported, but its generalisability to all GVLMs and diverse real-world scenarios requires further validation.
- **Medium Confidence**: The conclusion that VisualGPTScore is more suitable than other metrics for evaluating GVLMs' multimodal compositionality is supported by the experiments conducted, but may not be universally applicable to all GVLMs.

## Next Checks

1. Evaluate the MODE benchmark and VisualGPTScore across a wider range of GVLMs, including those with different architectural designs and training objectives, to assess their generalisability.
2. Test GVLMs on real-world images and texts that require complex compositional reasoning, beyond the controlled scenarios in the MODE benchmark, to validate its effectiveness in practical applications.
3. Compare VisualGPTScore with other emerging metrics designed for multimodal evaluation, such as those based on human judgment or alternative computational approaches, to further validate its superiority in assessing GVLMs' compositionality.