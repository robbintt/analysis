---
ver: rpa2
title: Prompting or Fine-tuning? A Comparative Study of Large Language Models for
  Taxonomy Construction
arxiv_id: '2309.01715'
source_url: https://arxiv.org/abs/2309.01715
tags:
- taxonomy
- taxonomies
- prompting
- construction
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparative study of large language models
  (LLMs) for taxonomy construction, a task of generating hierarchical relations between
  concepts. The authors propose a general framework for taxonomy construction that
  incorporates structural constraints and compare two approaches: fine-tuning and
  prompting.'
---

# Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction

## Quick Facts
- arXiv ID: 2309.01715
- Source URL: https://arxiv.org/abs/2309.01715
- Reference count: 33
- Key outcome: Prompting approach outperforms fine-tuning-based approaches for taxonomy construction, especially with small training datasets.

## Executive Summary
This paper presents a comparative study of large language models (LLMs) for taxonomy construction, evaluating both fine-tuning and prompting approaches. The authors propose a general framework that incorporates structural constraints and conduct experiments on two datasets: a hypernym taxonomy and a novel computer science taxonomy derived from the ACM Computing Classification System. The study finds that prompting approaches, which leverage the pre-trained capabilities of LLMs without explicit training, outperform fine-tuning methods when training data is limited. However, taxonomies generated through fine-tuning are more easily post-processed to satisfy all structural constraints.

## Method Summary
The study compares two fine-tuning approaches (layer-wise and LoRA) and a novel prompting approach for taxonomy construction. Fine-tuning adapts model parameters using relation classification tasks with pairwise sentences, while prompting uses few-shot examples with GPT-3.5-turbo and majority voting post-processing. Both approaches employ maximum spanning arborescence post-processing to enforce structural constraints. The evaluation uses precision, recall, and F1 score against ground truth, along with constraint consistency metrics.

## Key Results
- Prompting approach outperforms fine-tuning-based approaches, especially when training data is small
- Taxonomies generated by fine-tuning approach can be easily post-processed to satisfy all constraints
- Handling violations in taxonomies produced by prompting approach is more challenging
- A novel computer science taxonomy dataset derived from ACM CCS was introduced

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting outperforms fine-tuning for taxonomy construction when training data is small.
- Mechanism: Pre-trained LLMs have strong zero-shot capabilities. When given instructive prompts with a few examples, they can generalize to taxonomy tasks without additional training, while fine-tuning requires sufficient data to adapt model parameters.
- Core assumption: The pre-trained LLM has learned enough general language patterns and knowledge during training to handle taxonomy construction tasks.
- Evidence anchors:
  - [abstract] "Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches."
  - [section] "The recent advances in generative large language models (LLMs) spark growing interest in their application to diverse software engineering activities, offering the potential for tackling classic software engineering challenges with novel approaches."

### Mechanism 2
- Claim: Fine-tuned models produce more consistent taxonomies that satisfy structural constraints.
- Mechanism: Fine-tuning adapts the model parameters specifically for taxonomy construction, allowing it to learn and internalize structural constraints during training. Post-processing with maximum spanning arborescence enforces these constraints.
- Core assumption: Training data includes examples of constraint-satisfying taxonomies, and the model learns to predict relations that adhere to these constraints.
- Evidence anchors:
  - [abstract] "taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints"
  - [section] "The problem of optimizing the subgraph that adheres to all taxonomy constraints can be treated as a maximum spanning arborescence problem."

### Mechanism 3
- Claim: Majority voting post-processing improves prompting consistency and quality.
- Mechanism: Multiple generations with different few-shot examples capture diverse aspects of the taxonomy. Voting on relation occurrences reduces noise and improves reliability of the final taxonomy.
- Core assumption: Different prompt examples guide the model to generate complementary parts of the taxonomy, and voting helps identify the most reliable relations.
- Evidence anchors:
  - [section] "we propose a post-processing method that aggregates results from multiple runs through majority voting"
  - [section] "Using different few-shot examples, we obtain relation predictions for the same set of concepts N times."

## Foundational Learning

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The prompting approach relies on carefully constructed prompts with examples to guide the LLM without training. Understanding how to design effective prompts is crucial.
  - Quick check question: What are the key components of an effective few-shot prompt for taxonomy construction?

- Concept: Fine-tuning techniques (layer-wise vs LoRA)
  - Why needed here: The paper compares different fine-tuning methods. Understanding their mechanisms and trade-offs is important for implementing and improving the approach.
  - Quick check question: How do layer-wise fine-tuning and LoRA differ in terms of which parameters are updated and memory requirements?

- Concept: Constraint satisfaction and post-processing
  - Why needed here: Both approaches require post-processing to handle structural constraints. Understanding constraint satisfaction techniques is key to generating valid taxonomies.
  - Quick check question: How does the maximum spanning arborescence algorithm ensure that all taxonomy constraints are satisfied?

## Architecture Onboarding

- Component map: Relation prediction (LLM-based) -> Post-processing (constraint satisfaction) -> Evaluation (comparison with ground truth)

- Critical path: Relation prediction → Post-processing → Evaluation

- Design tradeoffs:
  - Fine-tuning: More consistent but requires training data and may not generalize well with limited data
  - Prompting: Better with limited data but may produce inconsistent taxonomies
  - Post-processing: Maximum likelihood (fast, may violate constraints) vs Maximum spanning arborescence (slower, enforces constraints)

- Failure signatures:
  - Inconsistent taxonomies (multiple roots, multiple parents)
  - Poor F1 score compared to ground truth
  - Violation of structural constraints

- First 3 experiments:
  1. Compare prompting with fine-tuning on a small dataset to verify the performance gap
  2. Test the impact of different post-processing methods on taxonomy consistency
  3. Evaluate the effect of increasing the number of prompt examples on taxonomy quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to GPT-3.5 in taxonomy construction when accounting for cost?
- Basis in paper: [explicit] The authors mention GPT-4 exists but is over 20x more expensive than GPT-3.5, making it impractical for direct comparison
- Why unresolved: The paper chose GPT-3.5 for cost reasons but acknowledges GPT-4's superior capabilities
- What evidence would resolve it: Comparative experiments using GPT-4 and GPT-3.5 on the same datasets with cost-per-performance metrics

### Open Question 2
- Question: Can the prompting approach be improved with constraint-aware post-processing similar to the fine-tuning approach?
- Basis in paper: [explicit] The authors note that prompting-generated taxonomies violate constraints and suggest developing constraint-aware post-processing for prompting
- Why unresolved: The paper only implemented majority voting post-processing for prompting, not constraint-aware methods
- What evidence would resolve it: Implementation and evaluation of constraint-aware post-processing for the prompting approach

### Open Question 3
- Question: How does the performance of prompting approaches change when concepts have ambiguous or context-dependent names?
- Basis in paper: [inferred] The authors acknowledge their setup assumes concepts have semantically meaningful names and note this may not generalize
- Why unresolved: The paper's evaluation assumes clear concept names, but real-world applications may have ambiguous naming
- What evidence would resolve it: Experiments with taxonomies containing ambiguous or context-dependent concept names compared to semantically clear ones

## Limitations
- The study focuses on limited structural constraints that may not capture all real-world taxonomy requirements
- Performance comparisons are based on specific datasets (WordNet and ACM CCS) which may limit generalizability
- Majority voting post-processing for prompting introduces computational overhead that isn't fully characterized
- The study doesn't explore intermediate approaches combining fine-tuning with prompt engineering

## Confidence

**High Confidence**: The experimental results showing prompting outperforming fine-tuning on small datasets are well-supported by the evidence. The controlled comparison using identical datasets and evaluation metrics, along with statistical significance testing, provides robust evidence for this claim.

**Medium Confidence**: The assertion that fine-tuned taxonomies are easier to post-process while prompting outputs are more challenging to constrain is plausible but relies on assumptions about how the models learn structural patterns. The mechanism described is theoretically sound, but empirical validation across diverse datasets would strengthen this claim.

**Low Confidence**: The paper's claim that prompting approaches "can outperform fine-tuning without explicit training" requires careful interpretation. While technically accurate in terms of parameter updates, this framing might overstate the practical advantages since few-shot prompting still requires careful prompt engineering and multiple generations with voting aggregation.

## Next Checks

1. **Dataset Diversity Test**: Evaluate both approaches on additional domain-specific taxonomies (e.g., biomedical, legal) to assess generalizability beyond WordNet and ACM CCS. This would validate whether the observed performance patterns hold across different knowledge domains and taxonomy structures.

2. **Hybrid Approach Exploration**: Implement a combined fine-tuning-and-prompting strategy where models are first fine-tuned on small amounts of labeled data, then enhanced with few-shot prompting. This would test whether intermediate approaches can capture benefits of both methods while mitigating their respective limitations.

3. **Constraint Robustness Analysis**: Systematically vary the number and types of structural constraints applied during post-processing to determine the breaking points for both approaches. This would quantify the true constraint-handling capabilities of each method and identify which constraints are most critical for taxonomy quality.