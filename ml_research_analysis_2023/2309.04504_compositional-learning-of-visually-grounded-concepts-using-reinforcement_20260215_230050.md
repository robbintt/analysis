---
ver: rpa2
title: Compositional Learning of Visually-Grounded Concepts Using Reinforcement
arxiv_id: '2309.04504'
source_url: https://arxiv.org/abs/2309.04504
tags:
- learning
- agent
- train
- agents
- color
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether reinforcement learning agents can
  learn to compose language-based instructions grounded to visual features. Using
  a synthetic 3D environment, we first show that RL agents trained to navigate to
  color-shape target combinations implicitly learn to decompose and recompose these
  instructions, enabling zero-shot performance on unseen combinations.
---

# Compositional Learning of Visually-Grounded Concepts Using Reinforcement

## Quick Facts
- arXiv ID: 2309.04504
- Source URL: https://arxiv.org/abs/2309.04504
- Reference count: 40
- Agents can learn to compose language-based instructions grounded to visual features, with CLIP encoders enabling efficient compositional learning and zero-shot generalization.

## Executive Summary
This study investigates whether reinforcement learning agents can learn to compose language-based instructions grounded to visual features. Using a synthetic 3D environment, the authors show that RL agents trained to navigate to color-shape target combinations implicitly learn to decompose and recompose these instructions, enabling zero-shot performance on unseen combinations. The research demonstrates that pretraining on individual concepts significantly accelerates compositional learning and enables generalization to more complex environments. Notably, only agents using CLIP-based text encoders demonstrated both efficient compositional learning and generalization to unseen colors in zero-shot experiments.

## Method Summary
The authors train RL agents in a synthetic 3D environment to navigate to objects based on language instructions specifying color-shape combinations. The agent architecture combines vision (convolutional layers) and language (text encoders) modules with an LSTM to predict navigation actions. Training uses the A2C algorithm with RMSProp optimizer. Experiments compare different text encoders (One-hot, Vanilla, BERT, CLIP), test compositional learning efficiency, and evaluate pretraining effects on concept learning versus compositional learning. The study measures episodes to performance criterion and zero-shot generalization to unseen combinations and environments.

## Key Results
- RL agents implicitly learn to decompose color-shape instructions and recombine them for zero-shot performance on unseen combinations
- Pretraining on color and shape concepts separately reduces episodes needed for compositional learning by 20×
- Agents pretrained on both concept and compositional learning can generalize to more complex, out-of-distribution environments in zero-shot fashion
- Only CLIP-based text encoders demonstrated both efficient compositional learning and generalization to unseen colors in zero-shot experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL agents can implicitly learn to decompose compositional instructions into color and shape concepts and recombine them for unseen test combinations.
- Mechanism: The agent learns a latent representation that encodes separable color and shape dimensions, allowing zero-shot generalization to novel color-shape combinations by recombining learned features.
- Core assumption: The reward signal during navigation implicitly encourages the agent to disentangle and encode color and shape as independent features.
- Evidence anchors:
  - [abstract]: "when RL agents are naively trained to navigate to target color-shape combinations, they implicitly learn to decompose the combinations, allowing them to (re-)compose these and succeed at held-out test combinations"
  - [section]: "When the agent successfully learns to ground the two distinct word concepts 'red' and 'capsule', it will be able to understand the composed instruction 'red capsule' without ever training on it, and navigate to the described object in zero-shot."
- Break condition: If the reward structure does not incentivize disentanglement, the agent may learn entangled representations and fail on unseen combinations.

### Mechanism 2
- Claim: Pretraining agents on individual color and shape concepts separately drastically reduces the number of episodes needed for compositional learning.
- Mechanism: Learning color and shape invariances independently provides a structured latent space that can be efficiently recombined for compositional tasks.
- Core assumption: Learning concepts separately creates disentangled, reusable features that accelerate higher-order compositional learning.
- Evidence anchors:
  - [abstract]: "when agents are pretrained to learn color and shape concepts separately, the number of episodes subsequently needed for compositional learning decreased by 20×"
  - [section]: "Agents pretrained on C/S for concept learning achieve performance criterion on the train and unseen test combinations 100× and 20× faster than the agents trained only on compositional learning, respectively."
- Break condition: If pretraining does not create truly disentangled features, or if the compositional task requires more than just combining two features, the speedup may not materialize.

### Mechanism 3
- Claim: Agents pretrained on both concept and compositional learning can generalize to more complex, out-of-distribution environments in zero-shot fashion.
- Mechanism: Sequential learning of individual concepts followed by composition creates a hierarchical representation that can be adapted to more complex compositional tasks.
- Core assumption: The hierarchical knowledge structure built through sequential pretraining enables zero-shot generalization to novel task structures.
- Evidence anchors:
  - [abstract]: "agents pretrained on both concept and compositional learning can generalize to more complex, out-of-distribution environments in zero-shot fashion"
  - [section]: "agents that was trained on C/S first and then C+S, achieving a rewards of 5.5 on both the familiar and unseen combinations in zero-shot"
- Break condition: If the out-of-distribution environment requires capabilities beyond simple feature composition, zero-shot generalization may fail.

## Foundational Learning

- Concept: Reinforcement Learning and Actor-Critic Methods
  - Why needed here: The paper uses A2C (Advantage Actor-Critic) algorithm to train agents to navigate to targets based on language instructions.
  - Quick check question: Can you explain the difference between value-based and policy-based RL methods, and where actor-critic methods fit in?

- Concept: Compositional Generalization
  - Why needed here: The core research question is whether agents can learn to compose learned concepts to solve unseen combinations.
  - Quick check question: What is the difference between systematic generalization and interpolation in machine learning?

- Concept: Multi-modal Learning (Vision + Language + Action)
  - Why needed here: The agents must learn to ground language instructions in visual features and map them to navigation actions.
  - Quick check question: How do vision-language models like CLIP help in grounding language to visual concepts?

## Architecture Onboarding

- Component map: Vision (RGB images → Conv layers) → Mixing (concat → 256-dim linear) → LSTM (hidden state) → Action/Value prediction (4 actions, state value)
- Critical path: Vision → Mixing → LSTM → Action/Value prediction
- Design tradeoffs:
  - Text encoder choice: CLIP encoders improve training efficiency but add dependency on external models
  - Architecture simplicity vs. expressivity: Current design is simple but may limit complex compositional reasoning
  - Training from scratch vs. transfer learning: Pretraining on concepts speeds up learning but requires additional training phases
- Failure signatures:
  - Poor performance on unseen combinations: Likely indicates lack of compositional generalization
  - Slow convergence: May indicate insufficient disentanglement of concepts or suboptimal hyperparameters
  - Collapse of learned embeddings: Could suggest training instability or poor reward shaping
- First 3 experiments:
  1. Train One-hot agent on C+S environment, measure episodes to reach performance criterion on train and test combinations
  2. Compare CLIP vs One-hot vs BERT text encoders on C+S compositional learning efficiency
  3. Pretrain on C/S, then train on C+S, measure speedup and zero-shot performance on C+S+S environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of different text encoders (BERT vs. CLIP) affect the agent's ability to generalize compositional learning to more complex environments with three or more attributes?
- Basis in paper: [explicit] The paper shows that BERT struggles to distinguish between color and shape concepts, while CLIP's prelearned word embedding is useful for increasing training efficiency.
- Why unresolved: The paper does not explore the use of BERT and CLIP in more complex environments with three or more attributes.
- What evidence would resolve it: Comparing the performance of agents using BERT and CLIP text encoders in environments with three or more attributes, such as C+S+S, to determine which encoder is more effective for generalization.

### Open Question 2
- Question: How does the complexity of the environment (e.g., presence of obstacles) affect the agent's ability to generalize compositional learning to real-world tasks?
- Basis in paper: [inferred] The paper mentions that the current environment is simple and does not include obstacles, which may limit the agent's ability to generalize to real-world tasks.
- Why unresolved: The paper does not explore the impact of environmental complexity on the agent's performance.
- What evidence would resolve it: Training agents in more complex environments with obstacles and comparing their performance to agents trained in the current simple environment to assess the impact of environmental complexity on generalization.

### Open Question 3
- Question: How does the agent's performance change when trained on more complex instructions, such as "find any object yellow"?
- Basis in paper: [inferred] The paper mentions that the current instructions are only composed of two or three words and does not explore more complex instructions.
- Why unresolved: The paper does not explore the impact of instruction complexity on the agent's performance.
- What evidence would resolve it: Training agents on more complex instructions and comparing their performance to agents trained on simpler instructions to assess the impact of instruction complexity on the agent's ability to generalize compositional learning.

## Limitations

- The study uses synthetic 3D environments that may not capture the complexity of real-world scenarios
- The research focuses on simple color-shape combinations and does not address scalability to more abstract or hierarchically structured concepts
- The reliance on CLIP text encoders raises questions about whether similar results could be obtained with more efficient or domain-specific encoders

## Confidence

- **High Confidence**: The 20× speedup from concept pretraining is well-supported by quantitative results showing clear convergence differences between training regimes.
- **Medium Confidence**: The generalization to out-of-distribution environments shows promising results but is based on limited testing with only one additional complexity level.
- **Low Confidence**: The claim that agents "implicitly" learn to decompose instructions relies on observational evidence rather than direct measurement of latent representations.

## Next Checks

1. **Ablation on Text Encoder Components**: Systematically evaluate which aspects of CLIP (visual pretraining, architecture, tokenization) are critical for compositional learning, comparing against smaller, more efficient alternatives.

2. **Scaling Complexity Experiments**: Test whether compositional learning scales to more complex combinations (e.g., adding size, texture, or spatial relations) and whether pretraining benefits persist at higher complexity levels.

3. **Representation Analysis**: Use techniques like probing classifiers or visualization to directly examine whether the agent's learned representations show the hypothesized disentanglement of color and shape concepts.