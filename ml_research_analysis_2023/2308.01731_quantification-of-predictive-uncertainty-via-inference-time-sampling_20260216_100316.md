---
ver: rpa2
title: Quantification of Predictive Uncertainty via Inference-Time Sampling
arxiv_id: '2308.01731'
source_url: https://arxiv.org/abs/2308.01731
tags:
- uncertainty
- https
- sampling
- deep
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a post-hoc sampling strategy for estimating
  predictive uncertainty in deep learning models that can generate diverse and multi-modal
  predictive distributions. The method, called Deep MH, uses Metropolis-Hastings MCMC
  sampling to evaluate the MH acceptance criterion with a new likelihood function
  that uses input backpropagation and distances in the input space.
---

# Quantification of Predictive Uncertainty via Inference-Time Sampling

## Quick Facts
- arXiv ID: 2308.01731
- Source URL: https://arxiv.org/abs/2308.01731
- Reference count: 40
- The authors propose Deep MH, a post-hoc sampling strategy using MCMC to estimate predictive uncertainty without modifying the base network

## Executive Summary
This paper introduces Deep MH, a method for quantifying predictive uncertainty in deep learning models through inference-time sampling. The approach uses Metropolis-Hastings MCMC sampling with a custom likelihood function based on input backpropagation to evaluate proposals by finding inputs close to the test sample that produce the proposed output. This makes the method architecture agnostic and applicable to any pre-trained feed-forward deterministic network without requiring changes to architecture or training procedure. The method can generate diverse and multi-modal predictive distributions, particularly effective when input-output ambiguity is high.

## Method Summary
Deep MH is a post-hoc sampling strategy that uses Metropolis-Hastings MCMC to sample from the posterior distribution p(y|x, θ, D, M) without assuming parametric forms. The key innovation is a likelihood function p(x|y′, f(·, θ)) ∝ exp(−β E(x, y′)) evaluated through input backpropagation, which finds inputs close to the test sample that produce the proposed output. This captures model sensitivity around the input, reflecting input-output ambiguity. The method is applied to pre-trained networks without architectural modifications and can generate diverse, multi-modal predictive distributions by aggregating MCMC samples.

## Key Results
- Generates diverse and multi-modal predictive distributions without assuming parametric forms
- Achieves better correlation between estimated uncertainty and prediction error compared to state-of-the-art methods
- Outperforms methods like RIO, probPCA, and MC Dropout in uncertainty quantification, especially for high input-output ambiguity cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generates diverse and multi-modal predictive distributions without modifying the base network
- Mechanism: Uses MCMC sampling with a custom likelihood function that evaluates proposals by finding inputs close to the test sample that produce the proposed output
- Core assumption: Model sensitivity around an input correlates with true aleatoric uncertainty
- Evidence anchors: [abstract] "can generate different plausible outputs for a given input"; [section] "likelihood with an energy function p(x|y′, f(·, θ)) ∝ exp(−β E(x, y′))"
- Break condition: If model sensitivity doesn't correlate with true aleatoric uncertainty, or optimization in Equation 4 fails

### Mechanism 2
- Claim: Architecture agnostic without requiring training data access
- Mechanism: Uses input backpropagation to evaluate likelihood, requiring only pre-trained network and test input
- Core assumption: Network's forward pass and gradient computation suffice to evaluate likelihood
- Evidence anchors: [abstract] "architecture agnostic and can be applied to any feed-forward deterministic network"; [section] "define a likelihood function that can be used with any pre-trained network"
- Break condition: If network gradients are unreliable (dead ReLUs, poor conditioning)

### Mechanism 3
- Claim: Captures complex, non-Gaussian posterior distributions
- Mechanism: Aggregates samples from MCMC chain to approximate arbitrary posteriors
- Core assumption: MCMC sampling can approximate arbitrary posteriors with enough samples
- Evidence anchors: [abstract] "does not assume parametric forms of predictive distributions"; [section] "crucial to note that this does not correspond to p(y|x, θ, D, M) being a Gaussian"
- Break condition: If MCMC chain doesn't converge or has high autocorrelation

## Foundational Learning

- **Markov Chain Monte Carlo (MCMC) sampling**
  - Why needed here: Used to sample from posterior distribution without assuming parametric forms
  - Quick check question: What is the role of the acceptance probability in MCMC sampling?

- **Input backpropagation**
  - Why needed here: Evaluates likelihood by finding inputs close to test sample that produce proposed output
  - Quick check question: How does input backpropagation differ from standard backpropagation?

- **Aleatoric vs. epistemic uncertainty**
  - Why needed here: Method focuses on quantifying aleatoric uncertainty (input-output ambiguity)
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty?

## Architecture Onboarding

- **Component map**: Pre-trained network (f(x|θ)) -> MCMC sampler (Metropolis-Hastings) -> Likelihood evaluator (input backpropagation) -> Proposal generator (e.g., Gaussian around current state)

- **Critical path**:
  1. Initialize MCMC chain
  2. Generate proposal y′
  3. Evaluate likelihood p(x|y′, θ, D, M) via input backpropagation
  4. Accept or reject proposal based on acceptance probability
  5. Repeat until convergence

- **Design tradeoffs**:
  - Computational cost vs. accuracy: More MCMC samples yield better posterior approximation but increase runtime
  - Proposal distribution: Wider proposals explore more but have lower acceptance rates
  - Temperature parameter β: Higher values make likelihood more peaked, affecting acceptance rates

- **Failure signatures**:
  - Low acceptance rates: Proposals too far from current state or likelihood too peaked
  - Slow convergence: Chain stuck in local modes or has high autocorrelation
  - Poor posterior approximation: Not enough samples or chain didn't converge

- **First 3 experiments**:
  1. Test on a simple 1D regression problem with known ground truth posterior
  2. Compare uncertainty estimates on held-out test set vs. validation set
  3. Vary temperature parameter β and observe effect on acceptance rates and posterior approximation

## Open Questions the Paper Calls Out

- **Open Question 1**: How can computational complexity of Deep MH be reduced while maintaining performance?
  - Basis in paper: [explicit] Main limitation is computational complexity from sampling nature and likelihood evaluation
  - Why unresolved: Authors suggest complexity stems from sampling and likelihood evaluation but don't provide specific solutions
  - What evidence would resolve it: Empirical results comparing performance with reduced computational complexity

- **Open Question 2**: How can Deep MH be extended to problems with higher dimensional targets?
  - Basis in paper: [explicit] Deployment to higher dimensional targets is open research question, though straightforward for low effective dimensions
  - Why unresolved: Paper doesn't provide strategies for extending to higher dimensional targets or scaling approach
  - What evidence would resolve it: Experimental results applying Deep MH to higher dimensional targets

- **Open Question 3**: How does choice of prior distribution p(y) impact performance?
  - Basis in paper: [explicit] Method can be applied where prior p(y) can be defined/estimated; uses probabilistic PCA prior in experiments
  - Why unresolved: Paper doesn't investigate impact of different prior choices on performance or method sensitivity
  - What evidence would resolve it: Experimental results comparing Deep MH with different prior distributions

## Limitations

- Core uncertainty in assumed correlation between model sensitivity and true aleatoric uncertainty
- Architecture-agnostic claim depends heavily on network gradient reliability
- MCMC sampling's ability to capture complex posteriors assumes proper chain convergence without sufficient diagnostic tools

## Confidence

**High Confidence** claims:
- Method can be applied post-hoc to pre-trained networks without architectural changes
- Method generates samples from distribution approximating posterior
- Approach improves uncertainty quantification compared to baselines

**Medium Confidence** claims:
- Sensitivity-based likelihood accurately captures aleatoric uncertainty
- MCMC sampling adequately represents complex posterior distributions
- Method works across diverse imaging and non-imaging tasks

**Low Confidence** claims:
- Correlation between estimated uncertainty and prediction error generalizes beyond tested datasets
- Method's computational efficiency is practical for real-world deployment
- Failure modes are well-characterized and diagnosable

## Next Checks

1. **Convergence Diagnostics**: Implement trace plots, autocorrelation analysis, and Gelman-Rubin statistics for MCMC chains across multiple runs to verify proper mixing and convergence

2. **Sensitivity Analysis**: Systematically vary temperature parameter β and proposal distribution width, then measure impact on acceptance rates, posterior approximation quality, and correlation with prediction error

3. **Gradient Reliability Test**: Apply method to networks with known gradient pathologies (dead ReLUs, poor conditioning) and compare uncertainty estimates against ground truth aleatoric uncertainty when available