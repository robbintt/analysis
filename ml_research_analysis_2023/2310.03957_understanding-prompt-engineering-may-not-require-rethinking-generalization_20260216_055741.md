---
ver: rpa2
title: Understanding prompt engineering may not require rethinking generalization
arxiv_id: '2310.03957'
source_url: https://arxiv.org/abs/2310.03957
tags:
- generalization
- bounds
- error
- prompts
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the generalization properties of prompt engineering
  in vision-language models. Despite manual tuning, prompts exhibit little overfitting,
  which this paper attributes to the discrete nature of prompts and a PAC-Bayes framework
  using a language model as prior.
---

# Understanding prompt engineering may not require rethinking generalization

## Quick Facts
- arXiv ID: 2310.03957
- Source URL: https://arxiv.org/abs/2310.03957
- Reference count: 40
- Primary result: PAC-Bayes bounds for discrete prompts are remarkably tight—within a few percentage points of true test error

## Executive Summary
This work examines the generalization properties of prompt engineering in vision-language models. Despite manual tuning, prompts exhibit little overfitting, which this paper attributes to the discrete nature of prompts and a PAC-Bayes framework using a language model as prior. Experiments on CIFAR-10, CIFAR-100, fMoW, OfficeHome, and ImageNet show that PAC-Bayes bounds are remarkably tight—often within a few percentage points of the true test error—significantly outperforming prior bounds. These bounds are useful for model selection and extend to prompts generated via greedy search. The findings suggest that prompt engineering is a principled approach with strong theoretical grounding.

## Method Summary
The paper develops PAC-Bayes generalization bounds for prompt engineering in vision-language models. It treats prompts as discrete tokens and uses a language model (LLM) as a prior over the prompt space. The method computes KL-divergence between the learned prompt and the LLM prior, yielding tight bounds. Greedy search with optional KL regularization generates prompts by incrementally building token sequences. The approach is evaluated across multiple datasets, showing that PAC-Bayes bounds significantly outperform traditional uniform convergence bounds and provide useful model selection criteria.

## Key Results
- PAC-Bayes bounds are within a few percentage points of true test error across all datasets
- Bounds are significantly tighter than uniform convergence bounds (up to 35% improvement)
- Greedy search with KL regularization improves both accuracy and bound tightness
- Prompts maintain tight bounds even when transferred across datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discrete prompts with an LLM prior yield tight PAC-Bayes generalization bounds because the KL-divergence term becomes tractable and small.
- **Mechanism:** The hypothesis space is constrained to sequences of discrete tokens. With a point-mass posterior over a specific prompt and an LLM prior, the KL-divergence reduces to the negative log-likelihood of that prompt under the LLM. If the learned prompt has high likelihood under the LLM, the KL term is small, yielding a tight bound.
- **Core assumption:** The LLM provides a meaningful prior over natural language prompts that correlates with task performance; prompts with high likelihood under the LLM are more likely to generalize.
- **Evidence anchors:** [abstract] states discrete prompts with LLM priors result in remarkably tight bounds; [section] shows KL-divergence equals negative log-likelihood for point mass posteriors; related works focus on prompt optimization but not PAC-Bayes bounds with LLM priors.

### Mechanism 2
- **Claim:** Greedy search over discrete tokens does not overfit because the prompt space is small relative to the model capacity, and the search is constrained by the LLM prior.
- **Mechanism:** The prompt space size is |V|^(L*K), but greedy search builds prompts incrementally and prunes candidates using the LLM, reducing the effective search space. The resulting prompts have low empirical risk and high prior likelihood, leading to low generalization error.
- **Core assumption:** The CLIP image encoder is fixed and was not trained on the downstream data, so the complexity of the full system is dominated by the prompt space, not the encoder.
- **Evidence anchors:** [abstract] notes manual tuning or greedy optimization performs nearly as well on test as training; [section] emphasizes discrete nature of prompts combined with PAC-Bayes prior yields tight bounds; related works study prompt optimization but not overfitting from generalization bound perspective.

### Mechanism 3
- **Claim:** Structural risk minimization using the PAC-Bayes bound as a regularizer improves generalization by balancing empirical risk and hypothesis complexity.
- **Mechanism:** During greedy search, the objective is augmented with a KL term weighted by β. This biases the search toward prompts that are not only accurate on training data but also have high prior likelihood, implicitly controlling hypothesis complexity.
- **Core assumption:** The KL term in the PAC-Bayes bound is a good proxy for hypothesis complexity in the discrete prompt space.
- **Evidence anchors:** [section] states hypothesis complexity (KL-divergence) is taken into account during token search; [section] notes this naturally leads to tighter bounds for prompts from Greedy on CIFAR-10; related works optimize prompts for accuracy but don't use PAC-Bayes bounds for regularization.

## Foundational Learning

- **Concept: PAC-Bayes bounds**
  - Why needed here: The paper uses PAC-Bayes bounds to derive generalization guarantees for prompt engineering, replacing uniform convergence bounds that are too loose.
  - Quick check question: What is the key difference between a PAC-Bayes bound and a uniform convergence bound in terms of what they measure?

- **Concept: KL-divergence**
  - Why needed here: The KL-divergence between the prior (LLM) and posterior (learned prompt) is the complexity term in the PAC-Bayes bound.
  - Quick check question: Why does the KL-divergence reduce to a negative log-likelihood when the posterior is a point mass?

- **Concept: Discrete hypothesis spaces**
  - Why needed here: Prompts are sequences of discrete tokens, making the hypothesis space finite and enabling application of PAC-Bayes bounds.
  - Quick check question: How does the size of the discrete hypothesis space scale with prompt length and vocabulary size?

## Architecture Onboarding

- **Component map:** CLIP model (image and text encoders) -> LLM (prior over prompts) -> Greedy search algorithm (prompt generation) -> PAC-Bayes bound computation (generalization guarantee)
- **Critical path:** 1. Generate class prompts via greedy search (Algorithm 1) 2. Compute PAC-Bayes bound using LLM prior 3. Evaluate bound tightness and use for model selection
- **Design tradeoffs:** Using a large vocabulary increases expressivity but may hurt generalization; pruning with LLM helps. Greedy search is efficient but may miss globally optimal prompts; beam search could improve but is more expensive. PAC-Bayes bounds are tight but rely on the LLM prior being well-aligned with the task.
- **Failure signatures:** Vacuous bounds: LLM prior is misaligned with task or image encoder is overfit. Poor performance: Greedy search gets stuck in local optima or vocabulary is too restrictive. High variance: Small training sets lead to unstable prompt learning.
- **First 3 experiments:** 1. Run greedy search with and without LLM pruning on CIFAR-10; compare test error and PAC-Bayes bounds. 2. Vary prompt length and measure impact on training/test error and bound tightness. 3. Compare PAC-Bayes bounds with and without SRM regularization (different β values) on CIFAR-100.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the tightness of PAC-Bayes bounds for prompt engineering generalize to other vision-language models beyond CLIP?
- Basis in paper: [explicit] The paper tests bounds on CLIP across multiple datasets but does not explore other vision-language models like Flamingo or BLIP.
- Why unresolved: Different architectures may have different feature spaces and tokenization schemes that could affect the hypothesis space size and language model prior effectiveness.
- What evidence would resolve it: Empirical testing of PAC-Bayes bounds using prompts on other vision-language models with similar experimental methodology.

### Open Question 2
- Question: How does the performance of prompt engineering compare to fine-tuning when both are constrained to similar hypothesis space complexities?
- Basis in paper: [inferred] The paper demonstrates strong generalization for prompts but does not directly compare to fine-tuning under equivalent complexity constraints.
- Why unresolved: Fine-tuning introduces continuous parameter optimization while prompts remain discrete, making direct comparison difficult without normalization for hypothesis space size.
- What evidence would resolve it: Experiments matching the effective hypothesis space complexity of fine-tuning to that of discrete prompts while measuring generalization performance.

### Open Question 3
- Question: Can the PAC-Bayes framework be extended to explain generalization in soft-prompt tuning methods that use continuous optimization?
- Basis in paper: [explicit] The paper explicitly states that soft prompts are "less amenable to theoretical analysis" due to their continuous nature.
- Why unresolved: Continuous optimization over embeddings creates an infinite hypothesis space that breaks the discrete combinatorial analysis used in this paper.
- What evidence would resolve it: Development of PAC-Bayes bounds that can handle the continuous nature of soft prompts, potentially through discretization or different complexity measures.

### Open Question 4
- Question: What is the relationship between prompt interpretability and generalization performance?
- Basis in paper: [inferred] The paper notes that soft prompts lack linguistic correspondence and that discrete prompts can be more interpretable, but does not systematically study this relationship.
- Why unresolved: Interpretability metrics are subjective and the paper does not correlate linguistic coherence with test performance across different prompt generation methods.
- What evidence would resolve it: Systematic experiments measuring both prompt interpretability (using human evaluation or automated metrics) and test accuracy across different prompt generation strategies.

### Open Question 5
- Question: How sensitive are the PAC-Bayes bounds to the choice of language model prior when applied to prompts from different domains or languages?
- Basis in paper: [explicit] The paper uses LLaMA-7B as a prior but notes that "LLaMs that use the same tokenizer as CLIP" might be more effective.
- Why unresolved: Different language models have different tokenization schemes and linguistic knowledge that could affect the prior probability distribution over prompts.
- What evidence would resolve it: Comparative experiments using multiple language models as priors (including multilingual models) across different domain-specific prompt engineering tasks.

## Limitations

- The paper's claims about PAC-Bayes bound tightness rely on the assumption that the language model prior is well-calibrated for the prompt space, without systematic exploration of misalignment cases.
- The effectiveness of greedy search with KL regularization depends heavily on the choice of β parameter, which appears empirically set without theoretical justification.
- The analysis does not directly compare prompt engineering to fine-tuning under equivalent complexity constraints, leaving open questions about relative performance.

## Confidence

- **High Confidence**: The empirical observation that PAC-Bayes bounds are tighter than uniform convergence bounds across multiple datasets (CIFAR-10, CIFAR-100, ImageNet, fMoW, OfficeHome). The experimental setup and results are clearly specified and reproducible.
- **Medium Confidence**: The claim that discrete prompts combined with LLM priors yield tight bounds due to tractable KL-divergence. While the mechanism is theoretically sound, the practical impact depends on the quality of the language model prior and prompt space constraints.
- **Low Confidence**: The assertion that greedy search with KL regularization consistently finds optimal prompts. The search is inherently local and may miss better solutions, though the paper does not extensively validate this limitation.

## Next Checks

1. **Prior Sensitivity Analysis**: Systematically evaluate PAC-Bayes bound tightness across different language models (varying sizes and training domains) to quantify how sensitive the bounds are to the choice of LLM prior.

2. **Cross-Domain Prompt Transfer**: Test whether prompts learned on one dataset (e.g., CIFAR-10) maintain tight PAC-Bayes bounds when applied to semantically different datasets, validating the claim about the discrete nature of prompts enabling generalization.

3. **Search Space Ablation**: Compare greedy search performance with random prompt sampling while controlling for prompt likelihood under the LLM, to isolate whether the search strategy or the prior itself drives the generalization benefits.