---
ver: rpa2
title: Using fine-tuning and min lookahead beam search to improve Whisper
arxiv_id: '2309.10299'
source_url: https://arxiv.org/abs/2309.10299
tags:
- lookahead
- beam
- fine-tuning
- whisper
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning and decoding improvements for Whisper on low-resource
  languages. LoRA fine-tuning on Vietnamese data achieves 38.49 WER reduction compared
  to zero-shot.
---

# Using fine-tuning and min lookahead beam search to improve Whisper

## Quick Facts
- arXiv ID: 2309.10299
- Source URL: https://arxiv.org/abs/2309.10299
- Reference count: 0
- Fine-tuning Whisper-Tiny with LoRA achieves 38.49 WER reduction on Vietnamese data

## Executive Summary
This paper demonstrates significant improvements in Whisper's speech recognition performance for low-resource languages through LoRA fine-tuning and enhanced decoding algorithms. The authors achieve a 38.49 WER reduction on Vietnamese FLEURS data using high-rank LoRA fine-tuning with fewer parameters than full fine-tuning. Additionally, they propose Filter-Ends and Min Lookahead decoding algorithms that reduce WER by 2.26 on average across multiple languages compared to standard beam search. The paper also provides mathematical proof that Min Lookahead decoding outperforms standard beam search.

## Method Summary
The authors fine-tune Whisper-Tiny using LoRA with rank r=192 on a 100-hour Vietnamese speech dataset, achieving substantial WER improvements over zero-shot performance. They also modify the standard beam search decoding algorithm by introducing Filter-Ends to eliminate EOT token contradictions and Min Lookahead to incorporate future token probabilities. The methods are evaluated on FLEURS and CommonVoice datasets across multiple languages, showing consistent improvements over standard Whisper performance.

## Key Results
- LoRA fine-tuning with rank r=192 achieves 38.49 WER reduction on Vietnamese FLEURS data
- Filter-Ends and Min Lookahead decoding reduce WER by 2.26 on average across multiple languages
- Min Lookahead algorithm mathematically proven to outperform standard beam search
- Results generalize across Whisper model sizes (Tiny, Base, Medium, Large-v2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning with high rank achieves comparable or better performance than full fine-tuning while using fewer trainable parameters.
- Mechanism: LoRA decomposes weight updates into low-rank matrices, allowing targeted adaptation of model parameters without modifying the original weights. By using a higher rank (r=192), more expressive adaptation is possible while keeping computational cost manageable.
- Core assumption: The rank r must be sufficiently high to capture language-specific features but not so high as to negate the parameter efficiency benefits.
- Evidence anchors:
  - [abstract]: "fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning."
  - [section]: "it is surprising that applying high-rank LoRA leads to the greatest model improvement on the FLEURS test set, with less than half the number of trainable parameters compared to full-parameter fine-tuning with decoupling embedding."
  - [corpus]: Weak evidence - corpus papers focus on fine-tuning strategies but lack direct comparison of high-rank LoRA vs full fine-tuning.
- Break condition: If the rank is too low, the adaptation becomes insufficient; if too high, computational efficiency is lost and potential overfitting occurs.

### Mechanism 2
- Claim: Filter-Ends decoding eliminates contradictions between EOT token predictions and later token predictions, improving alignment with model probabilities.
- Mechanism: By filtering out tokens less likely than an EOT token given the same base sequence, the decoding algorithm ensures that sequences ending in EOT are not unfairly penalized by length normalization, aligning beam search choices with the model's probability distribution.
- Core assumption: The model's probability distribution accurately reflects the likelihood of sequence completion, and EOT tokens are assigned appropriate probabilities.
- Evidence anchors:
  - [section]: "This normalization often creates contradictions with the probabilities that the model assigns... To correct this, we devise a modification called Filter-Ends (FE), which filters out every token less likely than an EOT token given the same base sequence."
  - [abstract]: "by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search."
  - [corpus]: Weak evidence - corpus papers discuss beam search variations but do not explicitly address EOT token filtering.
- Break condition: If the model assigns very low probabilities to EOT tokens in certain contexts, filtering may remove valid continuations and degrade performance.

### Mechanism 3
- Claim: Min Lookahead decoding mathematically outperforms standard beam search by incorporating future token probabilities weighted by their likelihood of occurrence.
- Mechanism: Instead of only considering the top token at each lookahead step, Min Lookahead incorporates the top n tokens weighted by their probability of being correct, using a minimum function to balance the influence of sequences with different probabilities.
- Core assumption: Future token probabilities are meaningful predictors of sequence quality and can be reliably estimated through greedy decoding.
- Evidence anchors:
  - [section]: "We prove that Min Lookahead is expected to outperform standard beam search... Using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search."
  - [abstract]: "Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper."
  - [corpus]: Weak evidence - corpus papers discuss lookahead variations but do not provide formal proofs or direct comparisons with Min Lookahead.
- Break condition: If lookahead steps are too far into the future, probability estimates become unreliable; if too few steps, the lookahead benefit diminishes.

## Foundational Learning

- Concept: Transformer encoder-decoder architecture
  - Why needed here: Understanding how Whisper processes audio spectrograms through the encoder and generates text through the decoder is essential for grasping how fine-tuning and decoding improvements affect performance.
  - Quick check question: What are the two main components of Whisper's architecture and their respective roles in speech recognition?

- Concept: Beam search decoding algorithm
  - Why needed here: The paper explicitly compares standard beam search with improved versions (Filter-Ends and Min Lookahead), so understanding how beam search works is critical.
  - Quick check question: How does standard beam search differ from greedy decoding in terms of token selection?

- Concept: Parameter-efficient fine-tuning methods (LoRA)
  - Why needed here: The paper uses LoRA as a primary fine-tuning strategy, so understanding how it works compared to full fine-tuning is important.
  - Quick check question: What is the key difference between LoRA fine-tuning and full-parameter fine-tuning in terms of which parameters are updated?

## Architecture Onboarding

- Component map:
  - Audio encoder: Two convolution layers → token embedding layer → Encoder Transformer layers
  - Text decoder: Trainable token embedding → Decoder Transformer layers
  - LoRA modules: Inserted into linear layers of both encoder and decoder
  - Decoding algorithms: Standard beam search, Filter-Ends, Min Lookahead

- Critical path: Audio → Mel spectrogram → Encoder → Decoder → Text output
  - Fine-tuning modifies encoder/decoder weights via LoRA or full fine-tuning
  - Decoding algorithms process decoder outputs to select final text sequence

- Design tradeoffs:
  - LoRA rank vs performance: Higher rank gives better adaptation but more parameters
  - Beam width vs runtime: Larger beam width improves accuracy but increases computation
  - Lookahead depth vs reliability: Deeper lookahead uses less reliable future predictions

- Failure signatures:
  - LoRA rank too low: Minimal performance improvement over zero-shot
  - Beam width too small: Misses better sequences
  - Lookahead too deep: Uses unreliable probability estimates

- First 3 experiments:
  1. Compare zero-shot vs full fine-tuning vs LoRA fine-tuning on Vietnamese FLEURS dataset
  2. Test Filter-Ends modification on standard beam search with Vietnamese and other languages
  3. Implement and compare Min Lookahead vs basic lookahead on Whisper-Tiny across multiple languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Min Lookahead algorithm maintain its performance advantage over standard beam search when applied to languages with extremely low resource availability (e.g., less than 10 hours of training data)?
- Basis in paper: [explicit] The paper states that "low-resource languages benefit much more from our algorithms than high-resource languages" and shows consistent improvements across multiple languages, but doesn't specifically test the algorithm on languages with very minimal training data.
- Why unresolved: The paper tests on languages with varying training data sizes but doesn't include any with extremely minimal data (less than 100 hours mentioned), leaving uncertainty about the algorithm's effectiveness in the most extreme low-resource scenarios.
- What evidence would resolve it: Testing the Min Lookahead algorithm on languages with severely limited training data (e.g., 10 hours or less) and comparing its performance against standard beam search in these conditions.

### Open Question 2
- Question: What is the optimal balance between lookahead steps and beam width for different language resource levels and model sizes?
- Basis in paper: [explicit] The paper tests various combinations of beam width (up to 20) and lookahead steps (1-6) but doesn't provide a systematic analysis of optimal combinations across different resource levels and model sizes.
- Why unresolved: While the paper shows that Min Lookahead = 3 performs well, it doesn't provide guidance on how to optimally configure these parameters based on language resource availability or model size.
- What evidence would resolve it: A comprehensive study testing different combinations of beam width and lookahead steps across various resource levels and model sizes to identify optimal configurations.

### Open Question 3
- Question: How does the performance of LoRA fine-tuning compare to full-parameter fine-tuning when scaling to extremely large Whisper models (beyond the Medium and Large-v2 sizes tested)?
- Basis in paper: [explicit] The paper notes that "the gap in performance improvement between full-parameter fine-tuning and LoRA decreases as the model size grows" but only tests up to the Large-v2 model.
- Why unresolved: The paper doesn't test the performance gap on Whisper models larger than Large-v2, leaving uncertainty about whether LoRA continues to perform comparably as models scale up further.
- What evidence would resolve it: Testing both LoRA and full-parameter fine-tuning on Whisper models larger than Large-v2 (if available) to determine if the performance gap continues to decrease or if one method becomes clearly superior.

## Limitations

- Fine-tuning experiments were conducted only on Vietnamese data, raising questions about cross-language generalization
- Computational efficiency claims lack detailed parameter counts and resource usage comparisons
- Mathematical proof of Min Lookahead's superiority assumes specific probability distributions and may not capture all practical scenarios

## Confidence

- High confidence: LoRA fine-tuning with rank r=192 significantly improves WER on Vietnamese FLEURS data (38.49 WER reduction over zero-shot)
- Medium confidence: Decoding improvements (Filter-Ends and Min Lookahead) showing 2.26 average WER reduction across multiple languages
- Low confidence: Claims about generalization to all Whisper model sizes and all low-resource languages

## Next Checks

1. Conduct LoRA fine-tuning experiments on additional low-resource languages (e.g., Amharic, Swahili) to verify if the 38.49 WER improvement generalizes beyond Vietnamese data

2. Calculate and compare exact parameter counts and GPU memory usage for LoRA vs full fine-tuning implementations across different model sizes to verify the "less than half the parameters" claim

3. Systematically test Min Lookahead performance across different lookahead depths (m = 1, 3, 5, 10) to identify optimal values and validate the robustness of the mathematical proof under varying conditions