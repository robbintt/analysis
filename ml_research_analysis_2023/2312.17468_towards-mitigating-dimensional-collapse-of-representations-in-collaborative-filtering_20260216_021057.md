---
ver: rpa2
title: Towards Mitigating Dimensional Collapse of Representations in Collaborative
  Filtering
arxiv_id: '2312.17468'
source_url: https://arxiv.org/abs/2312.17468
tags:
- learning
- collapse
- user
- embeddings
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses dimensional collapse in contrastive learning
  for collaborative filtering, where embeddings occupy only a low-dimensional subspace,
  reducing their distinguishability. The authors propose a non-contrastive learning
  objective (nCL) that explicitly mitigates this issue by optimizing two geometric
  properties on the embedding hypersphere: alignment (minimizing distances between
  positive user-item pairs) and compactness (maximizing the coding rate to utilize
  the full embedding space).'
---

# Towards Mitigating Dimensional Collapse of Representations in Collaborative Filtering

## Quick Facts
- arXiv ID: 2312.17468
- Source URL: https://arxiv.org/abs/2312.17468
- Reference count: 40
- The paper proposes nCL, a non-contrastive learning method that achieves 14.21% improvement in Recall@10 over state-of-the-art baselines on MovieLens10M

## Executive Summary
This paper addresses dimensional collapse in contrastive learning for collaborative filtering, where learned embeddings occupy only a low-dimensional subspace rather than the full embedding space. The authors propose a non-contrastive learning objective (nCL) that optimizes two geometric properties on the embedding hypersphere: alignment (minimizing distances between positive user-item pairs) and compactness (maximizing the coding rate to utilize the full embedding space). Unlike contrastive methods, nCL does not require data augmentation or negative sampling, making it scalable to large datasets. Experiments on four public datasets show that nCL significantly outperforms state-of-the-art methods.

## Method Summary
The method uses LightGCN as a backbone to generate initial user/item embeddings, then applies a non-contrastive learning objective that combines alignment loss (minimizing distances between positive pairs) with compactness loss (optimizing rate-distortion function with clustering). The compactness term prevents dimensional collapse by maximizing the volume of the embedding space, while the alignment term ensures similar representations for related users/items. The method optionally uses clustering to further improve compactness by optimizing per-cluster coding rates.

## Key Results
- nCL achieves Recall@10 of 0.2314 on MovieLens10M, a 14.21% improvement over the best baseline
- The method shows consistent improvements across four benchmark datasets (MovieLens10M, Amazon Beauty, Amazon Book, Yelp)
- nCL prevents dimensional collapse as evidenced by full-rank embedding matrices with no zero singular values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimensional collapse occurs when learned embeddings concentrate in a low-dimensional subspace, reducing their distinguishability for recommendation tasks.
- Mechanism: The nCL method optimizes both alignment and compactness on the embedding hypersphere, preventing singular values from collapsing to zero by maximizing the rate-distortion function.
- Core assumption: Embeddings on the hypersphere can be evaluated through their covariance matrix's singular values, where collapse manifests as zero singular values.
- Evidence anchors:
  - [abstract] "The key idea is to generate augmentation-invariant embeddings by maximizing the Mutual Information between different augmented views of the same instance."
  - [section] "In this work, we have empirically observed that many existing contrastive models, such as SGL [42], suffer from the issue of dimensional collapse, which results in a rank deficient embedding space."
- Break condition: If the embedding space dimensionality is too small relative to the data complexity, even optimized compactness cannot prevent collapse.

### Mechanism 2
- Claim: Non-contrastive learning can achieve geometric properties of alignment and compactness without requiring data augmentation or negative sampling.
- Mechanism: The nCL objective directly minimizes distances between positive user-item pairs (alignment) while maximizing the volume of the embedding space through rate-distortion optimization (compactness).
- Core assumption: The rate-distortion function can be used as a proxy for measuring and optimizing embedding space volume.
- Evidence anchors:
  - [abstract] "More importantly, our nCL does not require data augmentation nor negative sampling during training, making it scalable to large datasets."
  - [section] "Our nCL aims to achieve two geometric properties on the embedding hypersphere: 1) Alignment, which pushes together representations of positive-related user-item pairs; 2) Compactness, which determines the optimal coding length of user/item embeddings."
- Break condition: If the alignment term dominates too strongly, embeddings may collapse to trivial representations despite compactness optimization.

### Mechanism 3
- Claim: Clustering user/item embeddings and optimizing per-cluster compactness prevents dimensional collapse more effectively than global optimization.
- Mechanism: The method partitions embeddings into clusters and minimizes per-cluster coding rate while maximizing overall embedding volume, encouraging discriminative representations across clusters.
- Core assumption: User/item embeddings naturally form clusters that can be identified through co-occurrence graphs or deep clustering.
- Evidence anchors:
  - [section] "In practice, the vectors Eð‘¢ can be from a mixture distribution, e.g., the users can be clustered into several groups."
  - [section] "We can measure the rate distortion for each cluster... we can optimize the compactness of user embeddings Eð‘¢ by minimizing: Lcompact (Eð‘¢ ) = Rð‘ (Eð‘¢, ðœ–|ðš·) âˆ’ R (Eð‘¢, ðœ–)."
- Break condition: If clustering is poor or the number of clusters is mis-specified, the compactness optimization may not effectively prevent collapse.

## Foundational Learning

- Concept: Rate-distortion theory and its application to representation learning
  - Why needed here: The compactness term in nCL directly uses the rate-distortion function to measure and optimize embedding space volume.
  - Quick check question: What does the rate-distortion function R(E, Îµ) measure in the context of embedding representations?

- Concept: Hypersphere geometry and cosine similarity
  - Why needed here: The method normalizes embeddings to unit vectors and measures distances on the hypersphere, making geometric properties crucial.
  - Quick check question: How does normalizing embeddings to the unit hypersphere affect the relationship between Euclidean distance and cosine similarity?

- Concept: Graph neural networks and message passing
  - Why needed here: The backbone LightGCN uses graph convolutions to generate initial user/item embeddings before nCL optimization.
  - Quick check question: What is the key difference between LightGCN's message passing and traditional GCN message passing?

## Architecture Onboarding

- Component map: User-item interaction data -> LightGCN encoder -> Alignment + Compactness loss -> Optimized embeddings -> Recommendation predictions
- Critical path: User-item interaction data â†’ LightGCN embeddings â†’ Alignment + Compactness loss â†’ Optimized embeddings â†’ Recommendation predictions
- Design tradeoffs:
  - Batch size vs. computational complexity: Larger batches improve gradient estimates but increase memory usage for logdet computation
  - Number of clusters vs. representation quality: More clusters can capture finer-grained structure but may overfit
  - Alignment vs. compactness weighting: Too much emphasis on either can lead to suboptimal embeddings
- Failure signatures:
  - All singular values collapse to zero except the first few â†’ Dimensional collapse
  - Embedding distances become uniformly small â†’ Over-emphasis on alignment
  - Cluster assignments become random â†’ Poor membership matrix generation
- First 3 experiments:
  1. Train nCL on MovieLens10M with varying embedding dimensions (64, 128, 256) and measure singular value spectrum
  2. Compare nCL performance with and without the clustering component on sparse datasets
  3. Test different values of the regularization parameter Î± to find the optimal balance between alignment and compactness

## Open Questions the Paper Calls Out
The paper mentions three open questions: (1) how to accelerate the computation of compactness using variational functions, (2) extending nCL to other domains beyond collaborative filtering, and (3) understanding how the number of clusters affects performance across different datasets.

## Limitations
- The exact implementation details of the IPOT solver for cluster assignment are not specified
- The paper doesn't clearly define how the number of clusters K is determined or how sensitive the method is to this choice
- The relationship between the rate-distortion parameter Îµ and the actual embedding dimensionality is not fully explained

## Confidence
- Dimensional collapse problem identification: High - Well-documented issue in contrastive learning with clear empirical evidence
- Non-contrastive learning solution: Medium - The theoretical framework is sound, but implementation details are incomplete
- Experimental results: Medium - Significant improvements shown, but limited ablation studies and hyperparameter sensitivity analysis

## Next Checks
1. **Singular value spectrum analysis**: Train nCL with different embedding dimensions (64, 128, 256) and measure the singular value distribution to verify that the compactness term prevents dimensional collapse while maintaining discriminative power.

2. **Cluster sensitivity study**: Test nCL with varying numbers of clusters (K=2, 5, 10) and different clustering initialization methods to determine the robustness of the approach to clustering choices.

3. **Ablation on alignment-compactness balance**: Systematically vary the weighting parameter Î± between alignment and compactness terms to find the optimal balance and demonstrate the necessity of both components for preventing collapse.