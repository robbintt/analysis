---
ver: rpa2
title: 'Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech
  Recognition Models like OpenAI''s Whisper'
arxiv_id: '2309.00329'
source_url: https://arxiv.org/abs/2309.00329
tags:
- youtube
- https
- watch
- speech
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Mi-Go, a testing framework for evaluating speech
  recognition models using YouTube as a data source. Mi-Go automates the process of
  extracting, annotating, and evaluating data from YouTube, ensuring a diverse and
  representative sample for testing.
---

# Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI's Whisper

## Quick Facts
- arXiv ID: 2309.00329
- Source URL: https://arxiv.org/abs/2309.00329
- Authors: 
- Reference count: 40
- The Mi-Go framework evaluates Whisper model performance using YouTube data, achieving WER comparable to Whisper creators' benchmarks

## Executive Summary
Mi-Go is a testing framework designed to evaluate speech recognition models using YouTube videos as a data source. It automates the process of extracting, annotating, and evaluating data from YouTube, ensuring a diverse and representative sample for testing. The framework was used to evaluate OpenAI's Whisper model on 124 YouTube videos across all categories. The results showed that Mi-Go can effectively assess the performance of Whisper and identify discrepancies between model-generated transcriptions and human-made subtitles, highlighting potential subtitle misuse like SEO optimization. The median Word Error Rate (WER) for the Whisper large model tested with Mi-Go was 0.081, similar to results from tests conducted by Whisper's creators using different datasets.

## Method Summary
The Mi-Go framework automates speech recognition model evaluation using YouTube as a data source. It consists of three main components: TestPlan Generator, which creates test plans by searching YouTube for videos and retrieving their transcripts; TestRunner, which downloads audio files and runs tests using the Whisper model; and TranscriptDifference, which normalizes text and computes WER by comparing model output to human-made subtitles. The framework uses the YouTube Data API and youtube-transcript-api to access video metadata and transcripts, and stores results in both SQLite database and JSON format for analysis.

## Key Results
- Mi-Go achieved a median WER of 0.081 for Whisper large model, comparable to Whisper creators' benchmarks
- Framework successfully identified subtitle misuse cases, such as SEO optimization, through WER analysis
- Tested 124 YouTube videos across 13 categories, demonstrating framework's ability to handle diverse content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using YouTube as a test data source yields WER results comparable to Whisper model creators' benchmarks.
- Mechanism: YouTube provides diverse, multilingual, real-world audio samples with human-generated subtitles, enabling realistic performance evaluation.
- Core assumption: Human-made YouTube subtitles serve as valid ground truth for speech recognition evaluation.
- Evidence anchors:
  - [abstract]: "The median Word Error Rate (WER) for the Whisper large model tested with Mi-Go was 0.081, similar to results from tests conducted by Whisper's creators using different datasets."
  - [section]: "Traditional evaluation methods, which employ curated datasets, may not capture the broad array of real-world scenarios, hence potentially limiting a model's generalizability. Mi-Go, by leveraging YouTube's dynamic content, offers an enriched platform for testing these models."
  - [corpus]: Weak evidence. Only 5 of 25 related papers mention YouTube as a speech recognition data source; no direct comparison studies found.
- Break condition: If YouTube subtitles are systematically biased or of poor quality, WER comparisons become invalid.

### Mechanism 2
- Claim: The Mi-Go framework can automate end-to-end speech recognition testing with minimal manual intervention.
- Mechanism: TestPlan Generator searches YouTube, TestRunner downloads audio/subtitles, TranscriptDifference normalizes and computes WER, results stored automatically.
- Core assumption: YouTube Data API and youtube-transcript-api provide reliable, consistent access to required data.
- Evidence anchors:
  - [abstract]: "Mi-Go automates the process of extracting, annotating, and evaluating data from YouTube, ensuring a diverse and representative sample for testing."
  - [section]: "Mi-Go automates the process of data extraction, annotation, and evaluation from YouTube, ensuring an up-to-date and representative sample for testing purposes."
  - [corpus]: Weak evidence. No corpus papers directly test YouTube-based automated testing frameworks for speech recognition.
- Break condition: API changes or quota limits disrupt automated data collection.

### Mechanism 3
- Claim: Comparing model transcriptions with human subtitles can expose subtitle misuse like SEO optimization.
- Mechanism: WER anomalies indicate discrepancies between expected and actual subtitle content, revealing non-transcription uses.
- Core assumption: High WER values reliably indicate subtitle quality or intent issues rather than model failure.
- Evidence anchors:
  - [abstract]: "by contrasting the machine-generated transcriptions against human-made subtitles, the Mi-Go framework can help pinpoint potential misuse of YouTube subtitles, like Search Engine Optimization."
  - [section]: "We found that such discrepancies occur due to several reasons... 5. Search engine optimization (SEO)... Here is an example of such subtitles..."
  - [corpus]: No direct corpus evidence. Related papers focus on dataset curation, not misuse detection.
- Break condition: If WER anomalies stem from model limitations rather than subtitle misuse, detection fails.

## Foundational Learning

- Concept: Word Error Rate (WER) calculation and interpretation.
  - Why needed here: Mi-Go uses WER as the primary metric for comparing transcriptions.
  - Quick check question: If a transcript has 3 insertions, 2 deletions, and 1 substitution out of 100 reference words, what is the WER?

- Concept: Transformer-based sequence-to-sequence architectures for speech recognition.
  - Why needed here: Whisper uses this architecture; understanding its strengths/weaknesses informs test interpretation.
  - Quick check question: What key architectural difference distinguishes Whisper from earlier RNN-based ASR models?

- Concept: YouTube API integration and data extraction workflows.
  - Why needed here: Mi-Go depends on YouTube APIs for test data acquisition.
  - Quick check question: Which YouTube API provides video metadata, and which provides subtitle text?

## Architecture Onboarding

- Component map:
  - TestPlan Generator -> TestRunner -> TranscriptDifference -> SQLite/JSON storage

- Critical path:
  1. TestPlan Generator builds video list and metadata
  2. TestRunner downloads and preprocesses audio/subtitles
  3. TranscriptDifference runs Whisper, normalizes text, computes WER
  4. Results saved to SQLite and JSON for analysis

- Design tradeoffs:
  - Flexibility vs. complexity: Extending framework for new data sources requires modifying TestRunner
  - Automation vs. control: Full automation risks missing edge cases in subtitle quality
  - Storage choice: SQLite enables SQL queries; JSON allows easy sharing and re-running

- Failure signatures:
  - High WER across all videos: Likely API or preprocessing issue
  - Zero results in TestPlan Generator: YouTube API quota exhausted or search filters too restrictive
  - Inconsistent results across runs: Non-deterministic model behavior or API data changes

- First 3 experiments:
  1. Run TestPlan Generator with minimal filters (e.g., 5 videos, any language) to verify basic functionality
  2. Execute TestRunner on the generated plan using Whisper tiny model to confirm end-to-end pipeline
  3. Compare WER results for one video against manual calculation to validate accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of YouTube content impact the generalizability of speech recognition models compared to traditional curated datasets?
- Basis in paper: [explicit] The paper states that traditional test frameworks rely on curated datasets which may not fully represent the diversity and complexity of real-world speech scenarios, potentially limiting a model's generalizability.
- Why unresolved: While the paper introduces Mi-Go as a framework that leverages YouTube's diverse content for testing, it does not provide a direct comparison of the generalizability of models tested using YouTube versus traditional datasets.
- What evidence would resolve it: Conducting experiments that compare the performance of speech recognition models trained and tested on YouTube data versus those trained and tested on traditional curated datasets, and evaluating their performance in real-world scenarios.

### Open Question 2
- Question: Can the Mi-Go framework be extended to support other languages beyond English?
- Basis in paper: [inferred] The paper mentions that YouTube offers a rich and continuously updated collection of spoken language data, encompassing various languages, accents, dialects, speaking styles, and audio quality levels. However, the experiments conducted in the paper focus on testing English-only versions of the Whisper model.
- Why unresolved: The paper does not provide information on whether the Mi-Go framework can be extended to support other languages or if there are any limitations in doing so.
- What evidence would resolve it: Modifying the Mi-Go framework to support other languages and conducting experiments to test the performance of speech recognition models in those languages, comparing the results to models tested using traditional datasets.

### Open Question 3
- Question: How effective is the Mi-Go framework in detecting subtitle misuse on YouTube?
- Basis in paper: [explicit] The paper mentions that by contrasting machine-generated transcriptions against human-made subtitles, the Mi-Go framework can help pinpoint potential misuse of YouTube subtitles, like Search Engine Optimization.
- Why unresolved: While the paper provides examples of subtitle misuse, it does not quantify the effectiveness of the Mi-Go framework in detecting such misuse or provide a comprehensive analysis of the types of misuse that can be identified.
- What evidence would resolve it: Conducting a systematic analysis of YouTube videos with human-made subtitles, using the Mi-Go framework to identify instances of subtitle misuse, and quantifying the effectiveness of the framework in detecting different types of misuse.

## Limitations
- Framework relies on YouTube subtitles as ground truth, which may contain quality variations or optimization biases
- No systematic analysis provided of failure rates when transcripts are unavailable or API quota limitations
- Effectiveness of misuse detection mechanism not rigorously validated beyond anecdotal examples

## Confidence
- Mechanism 1: Medium - WER comparison relies on YouTube subtitle quality as ground truth
- Mechanism 2: Medium - Depends on API stability and subtitle availability
- Mechanism 3: Low - Misuse detection claims lack systematic validation

## Next Checks
1. Conduct a controlled study comparing WER distributions between known-good subtitles and known SEO-optimized subtitles to validate misuse detection claims.
2. Measure test success rates and WER variance across different YouTube API quota levels to establish reliability bounds for automated testing.
3. Run Mi-Go on a smaller dataset with manual ground truth verification to quantify the impact of subtitle quality on WER measurements.