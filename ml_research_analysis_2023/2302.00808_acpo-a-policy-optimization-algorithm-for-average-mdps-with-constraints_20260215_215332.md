---
ver: rpa2
title: 'ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints'
arxiv_id: '2302.00808'
source_url: https://arxiv.org/abs/2302.00808
tags:
- policy
- acpo
- average
- constraint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ACPO, the first policy optimization algorithm
  for constrained MDPs with the average reward criterion. The key insight is that
  the standard trust region bounds used for discounted MDPs do not generalize to the
  average case, so new sensitivity bounds are derived.
---

# ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints

## Quick Facts
- arXiv ID: 2302.00808
- Source URL: https://arxiv.org/abs/2302.00808
- Reference count: 40
- The paper introduces ACPO, the first policy optimization algorithm for constrained MDPs with the average reward criterion.

## Executive Summary
This paper introduces ACPO, the first policy optimization algorithm designed specifically for constrained Markov Decision Processes (CMDPs) with the average reward criterion. The key innovation is deriving new sensitivity bounds that connect policy divergence to stationary distribution divergence, overcoming limitations of standard trust region bounds that don't generalize to average reward settings. The algorithm uses trust region methods with policy recovery to ensure constraint satisfaction while optimizing average reward. Experimental results on MuJoCo environments demonstrate that ACPO outperforms state-of-the-art algorithms like CPO and PPO in both average reward and constraint satisfaction, particularly in complex tasks.

## Method Summary
ACPO solves constrained MDPs by optimizing average reward subject to average cost constraints. The algorithm uses trust region methods with KL divergence constraints to ensure stable policy updates. When constraint violations occur, a recovery step balances reward degradation with constraint satisfaction using a hyperparameter t. The method estimates advantage functions using Generalized Advantage Estimation and solves an approximate optimization problem using conjugate gradient and line search. The policy is parameterized as a Gaussian distribution with neural network parameters, and separate critics estimate value functions for reward and cost.

## Key Results
- ACPO outperforms CPO and PPO on average reward and constraint satisfaction in MuJoCo environments
- The algorithm is particularly effective in complex tasks like Grid and Bottleneck environments
- ACPO achieves higher average rewards with lower constraint violations compared to discounted CMDP algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACPO derives a new policy improvement bound for average-CMDPs that depends on the worst-case level of "mixture" of the irreducible Markov chain associated with a policy.
- Mechanism: The key insight is that standard trust region bounds used for discounted MDPs do not generalize to the average case. Instead, ACPO uses a bound that connects the difference in average rewards to the divergence between stationary distributions (DTV) scaled by a sensitivity factor σ⋆. This allows for tractable policy updates while maintaining theoretical guarantees.
- Core assumption: The MDP is ergodic (irreducible and aperiodic) so that stationary distributions exist and the mixing time is bounded.
- Evidence anchors:
  - [abstract]: "new sensitivity bounds are derived" and "theoretical performance guarantees"
  - [section 3.1]: Lemma 3.3 shows DTV(pdπ1 ∥dπq ≤ σ⋆E[s∼dπ][DTVp(π1∥π)[s]], connecting policy divergence to stationary distribution divergence
  - [corpus]: Missing direct empirical validation of σ⋆ bounds in experiments
- Break condition: If the MDP is not ergodic or has very slow mixing, the σ⋆ factor becomes large and the bound becomes uninformative.

### Mechanism 2
- Claim: ACPO uses a trust region approach with policy recovery to ensure feasibility while optimizing the average reward.
- Mechanism: The algorithm solves an approximate optimization problem (Eq. 13) that maximizes a surrogate reward objective subject to KL divergence and constraint satisfaction. When the solution violates constraints, a recovery step (Eq. 16) is applied that balances reward degradation with constraint satisfaction using a hyperparameter t.
- Core assumption: The KL divergence constraint can be approximated quadratically and the constraints are convex in the policy parameters.
- Evidence anchors:
  - [abstract]: "uses trust region methods with policy recovery to ensure feasibility"
  - [section 4.2]: Recovery step θk+1/2 = θk - √(2δ) [t·H⁻¹a/√(aᵀH⁻¹a) + (1-t)·H⁻¹g/√(gᵀH⁻¹g)] balances cost gradient a and reward gradient g
  - [corpus]: No direct empirical validation of recovery effectiveness across different t values
- Break condition: If the policy parameterization is highly non-linear or the constraint set is non-convex, the quadratic approximation and recovery may fail.

### Mechanism 3
- Claim: ACPO outperforms discounted CMDP algorithms (CPO, PCPO) when evaluated on average reward criteria.
- Mechanism: By directly optimizing the average reward objective rather than using large discount factors (γ→1), ACPO avoids the numerical instability and suboptimal long-term performance that affects discounted algorithms in the average setting.
- Core assumption: The average reward criterion is the appropriate performance measure for the tasks considered.
- Evidence anchors:
  - [abstract]: "Experimental results on MuJoCo environments show that ACPO outperforms state-of-the-art algorithms like CPO and PPO in terms of both average reward and constraint satisfaction"
  - [section 5.2]: Figure 2 shows ACPO achieving higher average rewards with lower constraint violations than CPO/PCPO in Grid and Bottleneck tasks
  - [corpus]: Missing comparison to other average-reward algorithms beyond CPO/PCPO
- Break condition: If the task inherently requires discounted evaluation (e.g., finite horizon), ACPO's average reward focus may be inappropriate.

## Foundational Learning

- Concept: Markov Decision Processes and ergodicity
  - Why needed here: The entire analysis relies on the MDP being ergodic to guarantee existence of stationary distributions and apply mixing time bounds
  - Quick check question: What conditions must an MDP satisfy to be ergodic, and why is this important for average-reward analysis?

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: The problem formulation includes auxiliary cost functions and constraints that must be satisfied while optimizing rewards
  - Quick check question: How does the feasible policy set ΠC = {π ∈ Π : JCi(π) ≤ li, ∀i} differ from the unconstrained policy set?

- Concept: Trust region methods and KL divergence
  - Why needed here: The policy updates are constrained by KL divergence to ensure stability and smoothness of the learning process
  - Quick check question: Why is KL divergence used as the divergence measure in trust region methods instead of total variation?

## Architecture Onboarding

- Component map:
  Policy network -> Value networks (reward and cost) -> Optimization module -> Recovery module -> Evaluation module

- Critical path:
  1. Collect trajectories using current policy πθk
  2. Estimate advantage functions using GAE framework
  3. Compute gradients g, ai and Hessian H
  4. Solve approximate optimization problem
  5. Apply recovery if needed
  6. Line search for constraint satisfaction
  7. Update policy parameters

- Design tradeoffs:
  - Quadratic approximation vs. exact trust region: Faster computation but potential approximation error
  - Separate critics vs. joint estimation: More parameters but potentially better advantage estimation
  - Recovery parameter t: Balances reward vs. constraint satisfaction but requires tuning

- Failure signatures:
  - High constraint violation despite recovery: Indicates poor Hessian approximation or non-convex constraint set
  - Slow convergence: May indicate small step size δ or poor advantage estimation
  - Numerical instability: Could be caused by ill-conditioned Hessian or exploding gradients

- First 3 experiments:
  1. Run ACPO on Point-Circle task with varying t values (0, 0.75, 1) to validate recovery effectiveness
  2. Compare ACPO with CPO using γ=0.999 on Grid task to demonstrate average reward advantage
  3. Test ACPO with different step sizes δ to find optimal trade-off between convergence speed and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the recovery parameter t in the policy recovery step, and how does it vary across different tasks and environments?
- Basis in paper: [explicit] The paper explicitly discusses the impact of the recovery parameter t in the policy recovery step and presents experimental results showing the performance of ACPO with different values of t in various environments.
- Why unresolved: The paper shows that t = 0.75 and t = 1 perform best across all tasks, but it doesn't provide a theoretical justification for why these values are optimal or how they should be selected for different tasks.
- What evidence would resolve it: Theoretical analysis of the recovery step that explains the optimal choice of t, or empirical results from a wider range of tasks showing consistent performance across different values of t.

### Open Question 2
- Question: How does ACPO's performance scale with the number of constraints in the CMDP?
- Basis in paper: [inferred] The paper focuses on the case of a single constraint in its practical implementation, but the theoretical framework could potentially be extended to multiple constraints.
- Why unresolved: The paper doesn't present experimental results or theoretical analysis for the case of multiple constraints, so it's unclear how ACPO's performance would be affected.
- What evidence would resolve it: Experimental results showing ACPO's performance with multiple constraints, or theoretical analysis of the impact of multiple constraints on the algorithm's convergence and constraint satisfaction.

### Open Question 3
- Question: What are the limitations of ACPO in high-dimensional state and action spaces, and how can they be addressed?
- Basis in paper: [inferred] The paper presents experimental results on high-dimensional environments (e.g., Grid and Bottleneck tasks), but it doesn't provide a detailed analysis of the limitations or potential solutions for scaling ACPO to even higher dimensions.
- Why unresolved: The paper doesn't discuss the specific challenges of applying ACPO to extremely high-dimensional spaces, such as the curse of dimensionality or the need for more efficient function approximation methods.
- What evidence would resolve it: Analysis of ACPO's performance on extremely high-dimensional tasks, or development of modifications to ACPO that address its limitations in high-dimensional spaces.

## Limitations
- The theoretical sensitivity bounds rely heavily on ergodicity assumptions that may not hold in practice, particularly for high-dimensional or sparse-reward environments
- Experimental validation is limited to relatively simple MuJoCo tasks, leaving scalability to more complex domains uncertain
- The policy recovery mechanism's effectiveness depends on the quadratic approximation quality, which may degrade in non-convex settings

## Confidence
- High confidence: The theoretical framework for average-reward CMDPs is sound and the connection to trust region methods is well-established
- Medium confidence: The policy recovery mechanism works as described for the tested environments, but generalization is unproven
- Low confidence: The σ⋆ sensitivity factor bounds will remain tight in high-dimensional, non-ergodic MDPs

## Next Checks
1. Test ACPO on non-ergodic MDPs with absorbing states to evaluate robustness of sensitivity bounds
2. Compare recovery effectiveness across different recovery parameter t values (0.25, 0.5, 0.75, 1.0) to identify optimal trade-offs
3. Scale experiments to more complex environments (Atari, continuous control with sparse rewards) to assess practical limitations