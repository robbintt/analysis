---
ver: rpa2
title: Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection
arxiv_id: '2311.04109'
source_url: https://arxiv.org/abs/2311.04109
tags:
- dataset
- semantics
- attention
- alignment
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pretrained language models learn
  bug semantics relevant to vulnerability detection. The authors analyze four transformer
  models using interpretability tools, attention analysis, and interaction matrix
  analysis, comparing model features to bug semantic features like buggy paths and
  Potentially Vulnerable Statements (PVS).
---

# Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection

## Quick Facts
- arXiv ID: 2311.04109
- Source URL: https://arxiv.org/abs/2311.04109
- Reference count: 40
- Key outcome: PVS annotation improves model F1 scores in 11 out of 16 cases, with up to 9.57 point improvement and 36-232% increase in alignment to bug semantic features

## Executive Summary
This paper investigates whether pretrained language models learn bug semantics relevant to vulnerability detection. The authors analyze four transformer models using interpretability tools, attention analysis, and interaction matrix analysis, comparing model features to bug semantic features like buggy paths and Potentially Vulnerable Statements (PVS). They find that better-performing models align better with PVS, but most models have less than 50% alignment, and all fail to align to buggy paths. Based on this analysis, they develop two annotation methods to highlight bug semantics in model inputs. Their best annotation method improves model F1 scores in 11 out of 16 cases, with up to 9.57 point improvement, and increases alignment to PVS by 36-232%.

## Method Summary
The authors fine-tune four transformer models (CodeBERT, UniXcoder, CodeT5, LineVul) on four vulnerability datasets (D2A, Devign, Big-Vul, ReVeal) containing C/C++ code. They implement bug semantic extraction for PVS using function call patterns and buggy paths using D2A static analyzer. Model performance is evaluated using F1 score and alignment metrics (IoU) comparing model features to bug semantics. The authors develop two annotation methods - Prepend and Mark - to highlight PVS in model inputs and measure their impact on performance and alignment.

## Key Results
- Better-performing models show stronger alignment with Potentially Vulnerable Statements (PVS)
- Models achieve less than 50% alignment with PVS on average, and fail to align substantially with buggy paths
- PVS annotation improves F1 scores in 11 out of 16 cases, with maximum 9.57 point improvement
- Annotation increases alignment to PVS by 36-232% across different models and datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Annotating potentially vulnerable statements (PVS) in model inputs improves performance by forcing the model to focus on bug-relevant features.
- **Mechanism:** The model uses the prepended or marked PVS as a "hint" that guides attention toward bug semantics during fine-tuning. This reduces reliance on spurious features and increases alignment between model attention/interpretations and PVS.
- **Core assumption:** Bug semantics (PVS) are causally relevant to vulnerability detection and their explicit presence in input will bias the model toward learning those features.
- **Evidence anchors:** "Our best annotation method improves model F1 scores in 11 out of 16 cases, with up to 9.57 point improvement" and "we developed two methods to annotate the PVS inside the model's inputs. Our best annotation method improved the model F1 score in the majority of cases"
- **Break condition:** If the context window is too small and PVS annotations push out important code tokens, performance may degrade (observed in some cases like CodeT5/Mark on Devign).

### Mechanism 2
- **Claim:** Better-performing models already align more strongly with PVS, and annotation further amplifies this alignment.
- **Mechanism:** Models that achieve higher F1 scores on vulnerability detection datasets tend to focus more on PVS during inference. By annotating PVS, we explicitly reinforce this behavior, leading to improved alignment and performance.
- **Core assumption:** Model performance correlates with alignment to true bug semantics; alignment can be increased by external guidance.
- **Evidence anchors:** "better-performing models also aligned better with PVS" and "we show that better-performing models aligned better to PVS"
- **Break condition:** If spurious features dominate or the dataset has very few PVS per program (e.g., MSR), annotation may not help and could even harm performance.

### Mechanism 3
- **Claim:** Self-attention and interaction matrix analysis reveal that models do not naturally learn to connect bug-related tokens into buggy paths, but annotation helps bridge this gap for PVS-level features.
- **Mechanism:** Attention analysis shows models attend to individual PVS but fail to form coherent buggy paths. Annotating PVS increases attention and interaction between these tokens, improving detection at the statement level even if path-level reasoning remains weak.
- **Core assumption:** Models can learn local bug features (PVS) more easily than global bug paths due to architectural limitations in connecting distant tokens.
- **Evidence anchors:** "models failed to align strongly, and failed to align substantially to buggy paths" and "all models failed to align to the buggy paths reported in D2A"
- **Break condition:** If the bug semantics require multi-step reasoning across distant code locations, simple PVS annotation will not suffice.

## Foundational Learning

- **Concept:** Transformer self-attention mechanism
  - Why needed here: The paper relies on attention analysis to measure how models focus on bug-related tokens; understanding attention is essential to interpret results.
  - Quick check question: In a transformer, what does a high attention score from token A to token B indicate about the model's encoding of A?

- **Concept:** Interpretation tools (e.g., SHAP, Saliency, DeepLift)
  - Why needed here: These tools attribute feature importance to input tokens, allowing comparison between model predictions and bug semantics.
  - Quick check question: How do gradient-based interpretation methods differ from attention-based explanations in terms of what they measure?

- **Concept:** Abstract Syntax Tree (AST) parsing for bug feature extraction
  - Why needed here: Bug features (PVS and buggy paths) are defined at the AST level, requiring static analysis to map source code to semantic bug locations.
  - Quick check question: Why is it necessary to map input tokens back to AST nodes when comparing model interpretations with bug semantics?

## Architecture Onboarding

- **Component map:** CodeBERT/UniXcoder/CodeT5/LineVul -> Fine-tuning layer (binary classification) -> Input preprocessing (BPE tokenization, PVS annotation) -> Static analysis pipeline (AST parsing, PVS/buggy path extraction) -> Evaluation (F1, alignment metrics)
- **Critical path:** Code -> AST parsing -> Bug feature extraction -> Model input (with/without annotations) -> Fine-tuning -> Prediction -> Alignment analysis
- **Design tradeoffs:** Context length limits (512 tokens) force truncation when prepending PVS; marker-based annotation increases token count and may destabilize training; path-level bug semantics are harder to represent than statement-level PVS
- **Failure signatures:** Collapse in performance when annotation dominates input (e.g., CodeT5/Mark on Devign); low alignment despite annotation (CodeBERT case); inability to detect buggy paths even with annotations
- **First 3 experiments:**
  1. Reproduce baseline models on D2A dataset and measure alignment to buggy paths using interpretability tools
  2. Implement Prepend annotation method and evaluate F1 and alignment changes on Devign dataset
  3. Compare attention head specialization before and after PVS annotation using interaction matrix analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop annotation methods that effectively guide models to align with buggy paths, not just Potentially Vulnerable Statements (PVS)?
- Basis in paper: The paper explicitly states that models failed to align to buggy paths and suggests future work should study the relationship between PVS and buggy paths, and develop approaches to make models attend to bug semantics including buggy paths.
- Why unresolved: Current annotation methods only target PVS, and simple annotation techniques for path-based bug semantics failed to improve performance in initial experiments.
- What evidence would resolve it: Developing and testing new annotation methods that successfully guide models to focus on buggy paths, showing improved alignment and performance metrics compared to current approaches.

### Open Question 2
- Question: What is the relationship between model performance on imbalanced datasets and their ability to align with bug semantic features?
- Basis in paper: The paper found that better-performing models aligned better with PVS, and that models showed lowest alignment with the imbalanced Big-Vul dataset while demonstrating higher alignment with balanced datasets.
- Why unresolved: While correlation between performance and alignment is observed, the causal mechanisms and specific features that affect alignment on imbalanced datasets remain unclear.
- What evidence would resolve it: Controlled experiments varying dataset balance while measuring both model performance and alignment to bug semantic features, identifying specific factors that influence this relationship.

### Open Question 3
- Question: Can larger context length in transformer models enable more effective annotation methods for bug semantics?
- Basis in paper: The paper mentions that current models have limited context length of 512 tokens, and that this limitation causes tradeoffs when inserting annotations, as excessive annotations may replace original code tokens.
- Why unresolved: The paper only speculated about the potential benefits of larger context length but did not test this hypothesis.
- What evidence would resolve it: Experiments with models supporting larger context lengths, testing whether more extensive or sophisticated annotation methods can be applied without sacrificing original code content, and measuring resulting improvements in alignment and performance.

## Limitations
- The study only examines four transformer architectures, limiting generalizability to other model families
- Context length constraints (512 tokens) may significantly impact annotation effectiveness, particularly when prepending PVS tokens
- The analysis focuses on PVS-level features but cannot conclusively determine whether models are learning true vulnerability semantics versus memorizing patterns
- Dataset-specific effects are observed (annotation helps on Devign but not MSR), suggesting the approach may not generalize across all vulnerability types

## Confidence
- **High confidence:** Empirical observation that PVS annotation improves model F1 scores in most cases (11/16), supported by direct quantitative evidence
- **Medium confidence:** Correlation between model performance and alignment to PVS, as the corpus evidence for this relationship is weak and based on limited prior work
- **Low confidence:** Claim that models fundamentally fail to learn buggy paths, as the analysis relies on a single static analyzer (D2A) and doesn't explore alternative path extraction methods or model architectures

## Next Checks
1. **Cross-analyzer validation:** Repeat buggy path alignment analysis using multiple static analyzers (e.g., Infer, Clang Static Analyzer, CodeQL) to determine if the observed failure to align with buggy paths is universal or analyzer-specific.

2. **Architecture ablation study:** Test the PVS annotation approach on non-transformer architectures (RNNs, CNNs, Graph Neural Networks) to determine whether the performance improvements are specific to transformer attention mechanisms or represent a more general principle.

3. **Downstream task generalization:** Evaluate whether models trained with PVS annotation on vulnerability detection show improved performance on related tasks like code summarization or bug fixing, testing whether the annotation approach teaches genuine semantic understanding or task-specific memorization.