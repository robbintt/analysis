---
ver: rpa2
title: Leveraging LLMs in Scholarly Knowledge Graph Question Answering
arxiv_id: '2311.09841'
source_url: https://arxiv.org/abs/2311.09841
tags:
- question
- sparql
- questions
- https
- scholarly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scholarly Knowledge Graph Question Answering
  (KGQA) system that uses a large language model (LLM) in a few-shot manner to answer
  bibliographic natural language questions. The model first identifies the top-n similar
  training questions using a BERT-based sentence encoder and retrieves their corresponding
  SPARQL queries.
---

# Leveraging LLMs in Scholarly Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2311.09841
- Source URL: https://arxiv.org/abs/2311.09841
- Reference count: 24
- Primary result: Achieves 99.0% F1 score on SciQA benchmark for scholarly KGQA

## Executive Summary
This paper presents a scholarly Knowledge Graph Question Answering (KGQA) system that leverages a large language model (LLM) in a few-shot manner to answer bibliographic natural language questions. The approach uses a BERT-based sentence encoder to identify similar training questions, then constructs a prompt with these examples to guide an LLM in generating SPARQL queries for the ORKG (Open Research KG) endpoint. The system achieves state-of-the-art performance on the SciQA benchmark without requiring LLM fine-tuning, demonstrating the effectiveness of few-shot prompting for scholarly KGQA tasks.

## Method Summary
The system operates by first encoding all training questions using a BERT-based sentence encoder and computing cosine similarity scores. For each test question, the top-n most similar training questions are identified, and their corresponding SPARQL queries are retrieved. A prompt is constructed by concatenating these question-SPARQL pairs with the test question, which is then passed to the Vicuna-13B LLM to generate a SPARQL query. The generated SPARQL is executed against the ORKG endpoint, and the resulting answer is returned. The approach requires no LLM fine-tuning and achieves high performance through effective prompt engineering and semantic similarity matching.

## Key Results
- Achieves 99.0% F1 score on the SciQA benchmark test set
- Demonstrates effectiveness of few-shot prompting for scholarly KGQA without LLM fine-tuning
- Shows that BERT-based sentence similarity effectively identifies relevant training examples for prompt construction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting enables LLMs to generate SPARQL queries without fine-tuning
- Mechanism: Providing top-n similar question-SPARQL pairs in the prompt gives the LLM context about the ORKG schema and query patterns, allowing it to generalize and generate correct SPARQL for new questions
- Core assumption: The LLM can effectively learn query patterns from a small number of examples
- Evidence anchors:
  - [abstract] "The model initially identifies the top-n similar training questions... Using the top-n similar question-SPARQL pairs as an example and the test question creates a prompt."
  - [section 4.2] "In the prompt, an example is created by concatenating top-n (n=1,3,5) similar questions with their respective SPARQL queries."
  - [corpus] Weak - no direct corpus evidence about few-shot effectiveness
- Break condition: If the provided examples are too dissimilar to the test question or if the LLM cannot generalize from limited examples

### Mechanism 2
- Claim: BERT-based sentence encoder effectively identifies similar questions
- Mechanism: Encodes all training questions and the test question into embeddings, then uses cosine similarity to rank and select the most similar questions
- Core assumption: Semantic similarity in embedding space correlates with similar query structures needed for SPARQL generation
- Evidence anchors:
  - [abstract] "The model initially identifies the top-n similar training questions related to a given test question via a BERT-based sentence encoder"
  - [section 4.1] "the question analyzer first generates the question embedding score of each question in the training set offline using the BERT-based sentence encoder"
  - [corpus] No corpus evidence available for BERT encoder effectiveness
- Break condition: If the sentence encoder fails to capture domain-specific terminology or if questions have different wording but similar intent

### Mechanism 3
- Claim: Vicuna LLM can generate syntactically correct SPARQL when properly prompted
- Mechanism: The LLM processes the few-shot prompt and outputs SPARQL queries that can be executed against the ORKG endpoint
- Core assumption: Vicuna has sufficient knowledge of SPARQL syntax and can adapt to the ORKG schema through examples
- Evidence anchors:
  - [abstract] "Then pass the prompt to the LLM and generate a SPARQL. Finally, runs the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint"
  - [section 4.2] "the SPARQL generator sub-component runs the prompt against our own Vicuna instance"
  - [corpus] Weak - only mentions Vicuna in corpus but no evidence of SPARQL generation capability
- Break condition: If Vicuna lacks knowledge of SPARQL syntax or cannot adapt to schema variations

## Foundational Learning

- Concept: BERT sentence embeddings
  - Why needed here: To measure semantic similarity between questions for selecting relevant training examples
  - Quick check question: What is the difference between semantic similarity and keyword matching in question retrieval?

- Concept: SPARQL query structure
  - Why needed here: To understand how natural language questions map to graph query patterns
  - Quick check question: What are the basic components of a SPARQL SELECT query?

- Concept: Few-shot learning with LLMs
  - Why needed here: To understand how to effectively prompt LLMs without fine-tuning
  - Quick check question: How does providing examples in a prompt help an LLM generate better outputs?

## Architecture Onboarding

- Component map: Question Analyzer → Query Generator → SPARQL Generator → Answer Extractor → ORKG Endpoint
- Critical path: Question → BERT embeddings → Cosine similarity → Prompt construction → Vicuna → SPARQL → ORKG endpoint → Answer
- Design tradeoffs: Few-shot prompting trades off some accuracy for zero fine-tuning vs full fine-tuning approaches
- Failure signatures: Null answers (syntax errors or incorrect query generation), low F1 scores, failure to identify similar questions
- First 3 experiments:
  1. Test BERT encoder by comparing embeddings of semantically similar vs dissimilar questions
  2. Validate Vicuna prompt format by providing examples and checking SPARQL syntax
  3. Run end-to-end with a small test set to verify answer extraction pipeline works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the model change when using different types of large language models (LLMs) instead of Vicuna-13B for SPARQL generation?
- Basis in paper: [inferred] The paper uses Vicuna-13B, a specific LLM, for generating SPARQL queries and achieves high performance. However, it does not explore the use of other LLMs.
- Why unresolved: The paper focuses on using Vicuna-13B and does not compare its performance with other LLMs.
- What evidence would resolve it: Testing the model with different LLMs like GPT-4, Llama 2, or other open-source models and comparing their performance on the SciQA benchmark.

### Open Question 2
- Question: What is the impact of using more than five similar question-SPARQL pairs in the prompt on the model's performance?
- Basis in paper: [explicit] The paper explores using one, three, and five similar question-SPARQL pairs but does not test the impact of using more than five pairs.
- Why unresolved: The paper only evaluates the performance with up to five similar pairs and does not investigate the effect of increasing this number further.
- What evidence would resolve it: Experimenting with prompts containing six, seven, or more similar pairs and analyzing how the performance changes.

### Open Question 3
- Question: How does the model's performance vary when applied to other scholarly knowledge graphs with different schemas?
- Basis in paper: [inferred] The paper demonstrates the model's effectiveness on the ORKG schema but does not test it on other scholarly knowledge graphs.
- Why unresolved: The paper focuses on the ORKG schema and does not explore the model's adaptability to other scholarly KGs with different structures.
- What evidence would resolve it: Applying the model to other scholarly KGs like DBLP or Semantic Scholar and comparing the performance across different schemas.

### Open Question 4
- Question: What is the effect of incorporating additional context or metadata about the scholarly articles in the prompt on the model's SPARQL generation accuracy?
- Basis in paper: [inferred] The paper uses only the question and similar question-SPARQL pairs in the prompt but does not explore the inclusion of additional article metadata.
- Why unresolved: The paper does not investigate whether providing more context about the articles (e.g., publication year, authors, venues) improves the model's understanding and SPARQL generation.
- What evidence would resolve it: Conducting experiments where prompts include varying levels of article metadata and measuring the impact on SPARQL accuracy.

## Limitations

- Heavy reliance on finding semantically similar training examples, with performance degradation for questions requiring novel query patterns
- Tight coupling to ORKG schema with untested generalizability to other scholarly knowledge graphs
- Assumes Vicuna can generate syntactically correct SPARQL without fine-tuning, lacking empirical validation of this assumption

## Confidence

**High Confidence Claims**:
- The end-to-end system achieves 99.0% F1 on SciQA benchmark (empirical result)
- The few-shot prompting approach can generate SPARQL queries that execute successfully on ORKG
- BERT-based sentence similarity is effective for finding relevant training examples

**Medium Confidence Claims**:
- The Vicuna LLM can generate SPARQL syntax from few-shot examples
- The prompt format with top-n similar question-SPARQL pairs is optimal
- The approach requires minimal training data compared to fine-tuning alternatives

**Low Confidence Claims**:
- The approach generalizes to scholarly KGs beyond ORKG
- The few-shot method works for questions requiring novel query patterns
- The system maintains performance with smaller training sets

## Next Checks

1. **Ablation study on training set size**: Systematically reduce the number of training examples and measure performance degradation to understand the minimum viable training set size and identify when the few-shot approach breaks down.

2. **Cross-KG evaluation**: Test the system on a different scholarly KG (such as Microsoft Academic Graph or Semantic Scholar) to validate generalizability beyond ORKG, measuring both semantic similarity matching and SPARQL generation effectiveness.

3. **Error analysis on generated SPARQL**: Conduct detailed analysis of all generated SPARQL queries to identify common syntax errors, entity matching failures, and question understanding gaps. This would reveal whether failures are systematic (suggesting prompt format issues) or random (suggesting LLM generation limitations).