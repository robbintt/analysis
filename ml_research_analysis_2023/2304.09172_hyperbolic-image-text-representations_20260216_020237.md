---
ver: rpa2
title: Hyperbolic Image-Text Representations
arxiv_id: '2304.09172'
source_url: https://arxiv.org/abs/2304.09172
tags:
- root
- meru
- clip
- photo
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERU is the first large-scale vision-language model that embeds
  images and text in a hyperbolic space, capturing the natural visual-semantic hierarchy
  more effectively than CLIP's Euclidean embeddings. It uses a modified contrastive
  loss and an entailment loss to enforce that text embeddings lie closer to the origin
  than image embeddings, reflecting that text is more generic than images.
---

# Hyperbolic Image-Text Representations

## Quick Facts
- arXiv ID: 2304.09172
- Source URL: https://arxiv.org/abs/2304.09172
- Reference count: 40
- Primary result: MERU achieves competitive zero-shot performance with more interpretable hierarchical representations in hyperbolic space

## Executive Summary
MERU is the first large-scale vision-language model that embeds images and text in hyperbolic space, capturing natural visual-semantic hierarchy more effectively than CLIP's Euclidean embeddings. It uses a modified contrastive loss and an entailment loss to enforce that text embeddings lie closer to the origin than image embeddings, reflecting that text is more generic than images. MERU matches or outperforms CLIP on zero-shot image classification and retrieval across multiple datasets while offering more interpretable hierarchical representations.

## Method Summary
MERU builds on CLIP's contrastive learning framework but adapts it to hyperbolic space using the Lorentz model. Image and text encoders (Vision Transformer and Transformer) project embeddings to hyperbolic space via exponential map, with learnable scaling parameters and curvature. The model uses Lorentzian distance in its contrastive loss and adds an entailment loss that enforces text embeddings lie closer to the origin than image embeddings through a cone-based penalty. Training uses 12M image-text pairs from RedCaps with ViT-S/B/L image encoders and a 12-layer Transformer text encoder.

## Key Results
- Matches or outperforms CLIP on zero-shot image classification across 20 datasets
- Achieves better performance with lower embedding dimensions (64-512 vs 1024 for CLIP)
- Provides more interpretable hierarchical representations verified through image traversals toward [ROOT]
- Shows competitive retrieval performance on COCO and Flickr30K benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperbolic space's exponential volume growth better captures the visual-semantic hierarchy.
- Mechanism: In hyperbolic geometry, volume increases exponentially as distance from origin grows, allowing generic concepts to have many neighbors near the origin while specific concepts can be placed farther away with fewer close neighbors.
- Core assumption: Visual and linguistic concepts naturally form a hierarchy where generic concepts (like "animal") encompass many specific concepts (like images of cats, dogs, etc.).
- Evidence anchors:
  - [abstract]: "Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text datasets."
  - [section 1]: "Such issues are handled naturally by hyperbolic spaces – the volume increases exponentially as we move away from the origin (Lee, 2019), making them a continuous relaxation of trees."
  - [corpus]: "Average neighbor FMR=0.374" (moderate relevance suggests the mechanism is plausible but not definitively proven)

### Mechanism 2
- Claim: The entailment loss enforces that text embeddings lie closer to the origin than image embeddings, reflecting the "text entails image" relationship.
- Mechanism: By using a cone-based entailment loss that penalizes image embeddings lying outside the cone projected by their paired text embeddings, MERU creates a partial order where text is more generic than images.
- Core assumption: There exists a meaningful partial order relationship between text and images where text descriptions can be more or less specific than the images they describe.
- Evidence anchors:
  - [abstract]: "uses a modified contrastive loss and an entailment loss to enforce that text embeddings lie closer to the origin than image embeddings, reflecting that text is more generic than images."
  - [section 3.2]: "We enforce that an image embedding y lies inside a cone projected by the paired text embedding x."
  - [corpus]: "HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment" (suggests this mechanism is being explored by other researchers)

### Mechanism 3
- Claim: Using Lorentzian distance instead of cosine similarity in the contrastive loss provides better numerical stability and meaningful hierarchy learning.
- Mechanism: The Lorentzian distance applies a logarithmic operator (cosh⁻¹) on the Lorentzian inner product, slowing down its growth compared to unbounded Euclidean inner products, preventing numerical overflow.
- Core assumption: The choice of similarity metric affects both the learned representations and the numerical stability of training.
- Evidence anchors:
  - [section 4.5]: "Loss diverges due to numerical overflow, as Lorentzian inner product is numerically large and unbounded in (−∞,−1/c], unlike cosine similarity∈ [−1,1]."
  - [section 3.1]: "We compute the negative Lorentzian distance as a similarity measure (Eqn. 4) for all B pairs in the batch."
  - [corpus]: "Compositional Entailment Learning for Hyperbolic Vision-Language Models" (suggests metric choice is important for hyperbolic VLMs)

## Foundational Learning

- Concept: Riemannian manifolds and hyperbolic geometry basics
  - Why needed here: Understanding how MERU lifts Euclidean embeddings onto the Lorentz hyperboloid and why hyperbolic geometry is suitable for hierarchical data
  - Quick check question: What is the key difference between Euclidean and hyperbolic spaces that makes hyperbolic spaces better for embedding tree-like hierarchies?

- Concept: Contrastive learning and metric learning
  - Why needed here: MERU builds on CLIP's contrastive learning framework but adapts it to hyperbolic space with Lorentzian distance
  - Quick check question: How does MERU's contrastive loss differ from CLIP's, and why is this difference important for hierarchical representation?

- Concept: Entailment and partial orders in representation learning
  - Why needed here: The entailment loss enforces that text embeddings lie closer to the origin than image embeddings, capturing the "text entails image" relationship
  - Quick check question: What geometric property of hyperbolic space allows MERU to naturally represent the partial order between text and images?

## Architecture Onboarding

- Component map:
  Image encoder (Vision Transformer) -> Linear projection -> Exponential map to hyperboloid
  Text encoder (Transformer) -> Linear projection -> Exponential map to hyperboloid
  Contrastive loss (Lorentzian distance) + Entailment loss (cone-based penalty)
  Learnable scalars (αimg, αtxt) for scaling before exponential map
  Learnable curvature parameter (c) for hyperboloid geometry

- Critical path:
  1. Forward pass through encoders
  2. Linear projection of embeddings
  3. Scaling with learnable scalars
  4. Exponential map to hyperboloid
  5. Compute Lorentzian distances and entailment loss
  6. Backpropagation through all components

- Design tradeoffs:
  - Using full ambient space parameterization vs. MERU's simplified space-only parameterization (MERU is simpler but may lose some flexibility)
  - Learnable curvature vs. fixed curvature (learnable is more flexible but harder to optimize)
  - Exponential map scaling with learnable scalars vs. fixed initialization (learnable helps prevent collapse but adds parameters)

- Failure signatures:
  - Training instability or divergence (may indicate issues with exponential map scaling or curvature learning)
  - No improvement over CLIP baseline (may indicate hyperbolic space isn't beneficial for this dataset or the entailment loss isn't effective)
  - Poor hierarchical structure in learned representations (may indicate the entailment loss isn't properly enforced)

- First 3 experiments:
  1. Train MERU with fixed curvature c=1 and compare to CLIP baseline to isolate the effect of hyperbolic geometry
  2. Train MERU without entailment loss (λ=0) to measure its contribution to hierarchical structure
  3. Train MERU with different embedding dimensions (64-512) to verify resource efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MERU's zero-shot performance on datasets with concepts not covered in RedCaps compare to CLIP trained on larger, more diverse datasets like LAION-5B?
- Basis in paper: [inferred] The paper notes that MERU and CLIP have near-random performance on datasets like PCAM and SST2 due to low concept coverage in RedCaps. The authors suggest that larger datasets like LAION might yield meaningful trends.
- Why unresolved: The experiments were conducted using a fixed, relatively small training dataset (RedCaps), limiting the ability to assess performance on out-of-distribution concepts.
- What evidence would resolve it: Training MERU and CLIP on LAION-5B or a similarly large, diverse dataset and evaluating zero-shot performance on the same set of 20 classification datasets, including PCAM and SST2, would provide a direct comparison.

### Open Question 2
- Question: What is the optimal curvature parameter c for MERU when scaling to larger model sizes, and how does its learned value correlate with the complexity of the visual-semantic hierarchy in the training data?
- Basis in paper: [explicit] The paper states that learning the curvature parameter c is crucial for training stability and performance when scaling to larger models (ViT-L/16), while fixed c=1 leads to poor convergence. However, the paper does not analyze the optimal or typical learned values of c.
- Why unresolved: The paper demonstrates the necessity of learning c but does not explore its optimal value or relationship to data complexity.
- What evidence would resolve it: Conducting a systematic study where MERU models of varying sizes are trained on datasets of increasing visual-semantic complexity (e.g., from simple object categories to complex scene descriptions) and analyzing the learned c values and their correlation with model performance and data complexity would provide insights.

### Open Question 3
- Question: How does MERU's hierarchical representation space impact its performance on few-shot learning and full-model fine-tuning tasks compared to CLIP?
- Basis in paper: [inferred] The paper focuses on zero-shot transfer performance and interpretability but notes that MERU's underlying Euclidean representations underperform CLIP in linear probe evaluations. The transferability to other tasks is beyond the scope of the paper.
- Why unresolved: The paper does not evaluate MERU's performance on few-shot learning or full-model fine-tuning tasks, which are important for practical deployment.
- What evidence would resolve it: Evaluating MERU and CLIP on few-shot learning benchmarks (e.g., miniImageNet, tieredImageNet) and fine-tuning MERU and CLIP on a subset of the 20 classification datasets, then measuring performance on held-out test sets, would directly compare their transferability to these tasks.

## Limitations

- Evaluation primarily benchmarks against CLIP rather than more recent VLMs like Flamingo or BLIP-2
- The claim that "text is more generic than images" relies on a specific philosophical framing of entailment that may not hold universally
- Assumes tree-like hierarchical structure when real-world visual-semantic relationships may have more complex, overlapping hierarchies
- Resource efficiency claims don't account for additional computational overhead of hyperbolic operations during training and inference

## Confidence

- **High confidence**: MERU successfully learns hierarchical representations in hyperbolic space that are interpretable and show meaningful structure through image traversals toward [ROOT]
- **Medium confidence**: MERU matches or slightly outperforms CLIP on zero-shot classification and retrieval tasks, though improvement is modest
- **Low confidence**: Hyperbolic geometry provides fundamental advantages over Euclidean space for VLMs that would generalize across all tasks and datasets

## Next Checks

1. Conduct ablation studies removing the entailment loss (λ=0) to quantify its specific contribution to hierarchical structure, and test whether hierarchical relationships hold on out-of-distribution data
2. Evaluate MERU on more recent VLM benchmarks (e.g., VQAv2, GQA, NLVR²) and compare against state-of-the-art models like BLIP-2 and Flamingo
3. Measure wall-clock training time and inference latency for MERU versus CLIP across different hardware configurations, accounting for additional hyperbolic operations, to provide complete resource efficiency analysis