---
ver: rpa2
title: Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive
  tasks
arxiv_id: '2311.01949'
source_url: https://arxiv.org/abs/2311.01949
tags:
- arxiv
- examples
- llms
- hicl
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Hint-enhanced In-Context Learning (HICL) to
  improve the performance of large language models (LLMs) on knowledge-intensive tasks.
  The core idea is to use LLMs' reasoning ability to extract query-related knowledge
  from demonstrations and concatenate it to the prompt.
---

# Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks

## Quick Facts
- **arXiv ID:** 2311.01949
- **Source URL:** https://arxiv.org/abs/2311.01949
- **Reference count:** 0
- **Key outcome:** HICL improves GPT-3.5-turbo by 2.89 EM and 2.52 F1, and LLaMA-2-Chat-7B by 7.62 EM and 7.27 F1 on open-domain QA benchmarks.

## Executive Summary
This paper introduces Hint-enhanced In-Context Learning (HICL) to improve large language models' performance on knowledge-intensive tasks. The core innovation is using LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenating this knowledge as explicit hints to the prompt. The approach also introduces a Hint-related Example Retriever (HER) that uses contrastive learning to select informative examples based on their overlap with generated hints. Experiments on three open-domain QA benchmarks show consistent performance improvements over standard in-context learning approaches.

## Method Summary
HICL enhances in-context learning by explicitly presenting query-related knowledge extracted from demonstrations. The method uses LLMs to generate hints summarizing relevant information from examples, then prepends these hints to the prompt. A Hint-related Example Retriever (HER) is trained using contrastive learning to identify examples most relevant to the query based on their overlap with generated hints. The retriever is trained on data constructed by sampling queries, retrieving similar examples, generating hints, and labeling examples as positive or negative based on F1 overlap with hints.

## Key Results
- HICL achieves average performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo
- HICL achieves average performance gains of 7.62 EM score and 7.27 F1 score on LLaMA-2-Chat-7B
- HICL demonstrates more stable performance across different example orderings compared to standard ICL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HICL improves LLM performance by explicitly presenting query-related knowledge extracted from demonstrations, preventing the model from overlooking relevant information.
- Mechanism: The model uses its reasoning ability to generate hints summarizing knowledge from examples, then prepends these hints to the prompt so the LLM processes the information more directly.
- Core assumption: LLMs can reliably extract relevant knowledge from examples when prompted with the right instruction, and this knowledge remains accurate when concatenated to the prompt.
- Evidence anchors:
  - [abstract] "HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way."
  - [section 2.1] "We prompt LLMs to extract hints via the following instruction: Please infer from the following QA-pairs step by step, and return the information related to [query]."
  - [corpus] Weak - no direct evidence of this specific mechanism from neighbors.
- Break condition: If the hint generation step produces irrelevant or incorrect knowledge, or if concatenation disrupts the model's understanding of the task format.

### Mechanism 2
- Claim: HER improves example selection by training a retriever to identify examples most relevant to the query based on their overlap with generated hints.
- Mechanism: Hints are used as supervisory labels to mark examples as positive or negative based on F1 overlap, then a contrastive learning objective trains the retriever to maximize similarity between queries and hint-related examples.
- Core assumption: Hint overlap is a reliable signal for example relevance, and the retriever can learn to generalize this signal beyond the training data.
- Evidence anchors:
  - [section 2.2] "According to the overlap with hi, we consider the example with a high F1 score as the hint-related example... We use InfoNCE loss to train the retriever."
  - [section 3.2] "According to the overlap with hi, we consider the example with a high F1 score as the hint-related example, namely, the positive one."
  - [corpus] Weak - neighbors discuss ICL but not this specific hint-based retrieval training method.
- Break condition: If hints do not reliably indicate example relevance, or if the retriever overfits to the specific hint generation style.

### Mechanism 3
- Claim: HICL provides more stable performance across different example orderings compared to standard ICL.
- Mechanism: By providing explicit hints before the examples, the model's attention is directed toward relevant knowledge regardless of example position, reducing sensitivity to ordering effects.
- Core assumption: The hint provides a stable reference point that anchors the model's reasoning, making the specific order of examples less critical.
- Evidence anchors:
  - [section 3.4] "Default indicates concatenating the most similar example closest to the query, while Reverse is the opposite. STD presents the standard deviation of each score."
  - [section 3.4] "According to the standard deviation, HICL performs more stably robustly with a lower order sensitivity."
  - [corpus] No direct evidence - neighbors don't discuss order sensitivity in ICL.
- Break condition: If the hint itself introduces ordering dependencies, or if the model still relies heavily on example position despite the hint.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: HICL builds directly on ICL by modifying how demonstrations are presented to the model.
  - Quick check question: How does ICL differ from traditional fine-tuning, and what role do demonstrations play in ICL?

- Concept: Contrastive learning
  - Why needed here: HER uses contrastive learning to train the retriever to distinguish relevant from irrelevant examples.
  - Quick check question: What is the objective of contrastive learning, and how does InfoNCE loss implement this?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The paper mentions RAG as a broader context for improving retrieved samples, suggesting HICL could integrate with such systems.
  - Quick check question: How does RAG differ from standard generation, and what role does retrieval quality play in its performance?

## Architecture Onboarding

- Component map: Query → Retriever → Example Set → Hint Generation → Hint + Query → LLM
- Critical path: Query → Retriever → Example Set → Hint Generation → Hint + Query → LLM
- Design tradeoffs:
  - Using LLM-generated hints adds computational overhead but provides more explicit knowledge presentation
  - HER requires training data construction but improves example relevance
  - More examples improve performance but increase computational cost
- Failure signatures:
  - Low hint quality → irrelevant knowledge in prompts
  - HER overfitting → poor generalization to new queries
  - Order sensitivity → performance drops when examples are reordered
- First 3 experiments:
  1. Test hint generation quality on a small sample of queries with known relevant examples
  2. Compare HER performance against base retriever on example ranking task
  3. Evaluate HICL performance gain on a single benchmark with different numbers of examples

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of HICL scale with the size of the language model beyond what was tested in the paper?
  - Basis in paper: [inferred] The paper tests HICL on gpt-3.5-turbo and LLaMA-2-Chat-7B, but does not explore performance on larger models like GPT-4 or beyond.
  - Why unresolved: The paper does not provide experimental results or theoretical analysis for models larger than those tested, leaving the scalability of HICL to larger models an open question.
  - What evidence would resolve it: Experiments comparing HICL performance across a range of model sizes, including very large models, would clarify how performance scales.

- **Open Question 2**: What is the impact of using different types of retrievers (e.g., sparse vs. dense) on the effectiveness of HICL?
  - Basis in paper: [explicit] The paper uses RoBERTa* and DPR as base semantic similarity models but does not explore other types of retrievers.
  - Why unresolved: The paper does not compare the performance of HICL with different retriever types, leaving the optimal retriever type for HICL an open question.
  - What evidence would resolve it: Comparative experiments using various retriever types (sparse, dense, hybrid) would determine the most effective retriever for HICL.

- **Open Question 3**: How does the performance of HICL vary across different domains or types of knowledge-intensive tasks?
  - Basis in paper: [explicit] The paper evaluates HICL on three open-domain QA benchmarks but does not explore other domains or task types.
  - Why unresolved: The paper's focus on open-domain QA limits the generalizability of HICL's effectiveness to other knowledge-intensive tasks or domains.
  - What evidence would resolve it: Experiments applying HICL to a diverse set of knowledge-intensive tasks across different domains would clarify its generalizability.

- **Open Question 4**: What is the computational overhead of HICL compared to standard ICL, and how does it scale with the number of examples?
  - Basis in paper: [inferred] The paper introduces HICL and HER but does not discuss computational efficiency or scalability in detail.
  - Why unresolved: The paper does not provide a detailed analysis of the computational cost of HICL, leaving its practical applicability in resource-constrained settings an open question.
  - What evidence would resolve it: Benchmarking the computational overhead of HICL under various settings would clarify its efficiency and scalability.

## Limitations
- Performance improvements are limited to three open-domain QA benchmarks, raising questions about generalizability to other knowledge-intensive tasks.
- The approach adds significant computational overhead through hint generation and HER training without addressing the trade-off between performance gains and increased computational cost.
- The mechanism relies on LLMs reliably extracting relevant knowledge through reasoning, but this capability is assumed rather than rigorously validated.

## Confidence
- **High Confidence**: The reported performance improvements (2.89-7.62 EM score gains) are well-supported by the experimental results and represent the most robust claim of the paper.
- **Medium Confidence**: The claim that HICL provides more stable performance across different example orderings is supported by standard deviation metrics but could benefit from additional analysis of why this stability occurs.
- **Low Confidence**: The proposed mechanism that LLMs reliably extract relevant knowledge through reasoning is weakly supported. The paper assumes this capability without rigorous validation of hint quality or relevance.

## Next Checks
1. Conduct an ablation study to isolate the contribution of hint quality versus example selection by comparing HICL with randomly selected examples plus hints against HER-selected examples without hints.
2. Systematically evaluate the quality of generated hints by measuring their overlap with ground truth answers and their informativeness for the QA task.
3. Test HICL on knowledge-intensive tasks outside of open-domain QA, such as multi-hop reasoning or complex instruction following, to assess generalizability of the approach.