---
ver: rpa2
title: 'TrueLearn: A Python Library for Personalised Informational Recommendations
  with (Implicit) Feedback'
arxiv_id: '2309.11527'
source_url: https://arxiv.org/abs/2309.11527
tags:
- truelearn
- learner
- library
- learning
- visualisations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TrueLearn Python library provides state-of-the-art online Bayesian
  models for personalised educational recommendation systems. Built on the "open learner"
  concept, it offers interpretable models with visualisations for user control.
---

# TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback

## Quick Facts
- arXiv ID: 2309.11527
- Source URL: https://arxiv.org/abs/2309.11527
- Reference count: 28
- Primary result: TrueLearn models achieve 78.32% accuracy and 64% F1 score on PEEK dataset

## Executive Summary
TrueLearn is a Python library implementing state-of-the-art online Bayesian models for personalised educational recommendation systems. Built on the "open learner" concept, it provides interpretable models with visualisations that put users in control of their learning experience. The library includes dataset tools, content representation utilities, learning algorithms, metrics, and visualisations designed for lifelong, privacy-preserving, and scalable learning personalisation.

## Method Summary
TrueLearn implements online Bayesian models that update learner states in real-time using implicit feedback signals like clicks and watch time. The library provides three core models (Interest, Novelty, Knowledge) that can be used individually or combined into an ensemble (INK). Models are trained using interaction logs and evaluated using hold-out validation and sequential experimental design. The system uses minimal data while respecting user privacy through exclusive reliance on individual user actions.

## Key Results
- INK ensemble model achieves 78.32% accuracy and 64% F1 score on PEEK dataset
- Individual models show strong performance: Interest (58.13% accuracy), Novelty (54.28%), Knowledge (58.13%)
- Visualisations include nine different types for representing learner states and evolution over time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TrueLearn achieves strong predictive performance by combining multiple Bayesian models into an ensemble.
- **Mechanism:** The INK ensemble integrates isolated models that capture learner interest, novelty, and knowledge simultaneously, providing complementary information.
- **Core assumption:** Individual models capture distinct aspects of learner engagement that enhance overall prediction when combined.
- **Break condition:** If individual models capture overlapping rather than complementary information, ensemble gains disappear.

### Mechanism 2
- **Claim:** TrueLearn provides interpretable models with visualisations that enable user control.
- **Mechanism:** Nine visualisation types communicate learner state representations to users in intuitive ways, facilitating metacognitive reflection.
- **Core assumption:** Users can effectively understand and interact with their models through visual representations.
- **Break condition:** If visualisations are not user-friendly or require significant learning curve, they fail to achieve user control.

### Mechanism 3
- **Claim:** TrueLearn achieves data efficiency and privacy preservation through online Bayesian models.
- **Mechanism:** Online learning algorithms update learner states using only implicit feedback signals without requiring explicit assessment data.
- **Core assumption:** Implicit feedback contains sufficient information to model learner engagement accurately.
- **Break condition:** If implicit signals are too noisy or insufficient, models fail to predict engagement accurately.

## Foundational Learning

- **Concept:** Bayesian inference and probabilistic graphical models
  - **Why needed here:** TrueLearn uses Bayesian models to estimate learner states and predict engagement probabilities
  - **Quick check question:** How does Bayesian updating work with implicit feedback signals?

- **Concept:** Item Response Theory (IRT) and Knowledge Tracing (KT)
  - **Why needed here:** TrueLearn extends these educational modeling approaches to handle implicit feedback and online learning
  - **Quick check question:** What distinguishes IRT (question difficulty) from KT (knowledge acquisition)?

- **Concept:** Online learning algorithms vs. batch learning
  - **Why needed here:** TrueLearn implements online learning for real-time updates, distinguishing it from batch approaches
  - **Quick check question:** What advantages do online learning algorithms offer for educational recommendation systems?

## Architecture Onboarding

- **Component map:** datasets → preprocessing → models → learning → metrics → visualisations
- **Critical path:**
  1. Load dataset from datasets module
  2. Extract content representations using preprocessing utilities
  3. Train learner model using learning module algorithms
  4. Evaluate performance using metrics module
  5. Visualise learner state using visualisations module

- **Design tradeoffs:**
  - Online learning vs. batch learning: Prioritizes real-time updates and privacy over model complexity
  - Minimal dependencies vs. feature richness: Reduces external dependencies for stability but may limit advanced features
  - Interpretability vs. predictive power: Human-intuitive visualisations may constrain model complexity

- **Failure signatures:**
  - Poor performance despite correct implementation: Check data quality, hyperparameter tuning, and feedback sufficiency
  - Visualisations not displaying: Verify data format compatibility and library dependencies
  - Slow performance with large datasets: Online algorithms may need optimization for scale

- **First 3 experiments:**
  1. **Basic functionality test:** Load PEEK dataset, run TrueLearn Interest model, verify accuracy exceeds baseline (58.13%)
  2. **Visualisation test:** Generate bubble plot for sample learner, verify mean and variance mapping to circle size and color
  3. **Integration test:** Implement recommendation pipeline using INK model to rank videos, verify F1 score approaches 64%

## Open Questions the Paper Calls Out

- **Open Question 1:** How effective are TrueLearn visualisations in improving learners' meta-cognitive skills and learning outcomes compared to traditional interfaces?
- **Open Question 2:** Can TrueLearn models maintain high accuracy when deployed in real-world e-learning platforms with diverse interaction data?
- **Open Question 3:** How well do TrueLearn models generalise to other informational content types beyond educational videos?

## Limitations

- Empirical evaluation relies on a single dataset (PEEK), limiting generalizability
- Performance with different implicit feedback signals beyond video engagement remains untested
- User studies validating interpretability benefits and visualisations are not yet conducted

## Confidence

- **High Confidence:** Library architecture and implementation details (Medium - based on available code and documentation)
- **Medium Confidence:** Predictive performance claims (Medium - based on single dataset evaluation)
- **Low Confidence:** Interpretability benefits and user control claims (Low - awaiting user study validation)

## Next Checks

1. Conduct user studies to evaluate whether visualisations effectively enable user understanding and control
2. Test library performance on additional educational datasets with different content types
3. Evaluate scalability of online learning algorithms with larger datasets and real-time scenarios