---
ver: rpa2
title: ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks
arxiv_id: '2303.15056'
source_url: https://arxiv.org/abs/2303.15056
tags:
- chatgpt
- content
- tasks
- moderation
- mturk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGPT outperforms crowd-workers for text-annotation tasks across
  multiple categories. For four out of five tasks, ChatGPT's zero-shot accuracy exceeded
  that of crowd-workers, with accuracy ranging from 72.8% to 78.7%.
---

# ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks

## Quick Facts
- **arXiv ID**: 2303.15056
- **Source URL**: https://arxiv.org/abs/2303.15056
- **Reference count**: 25
- **Key outcome**: ChatGPT zero-shot accuracy exceeds crowd-workers on 4 of 5 tasks, with >95% intercoder agreement at temperature 0.2 and 20x lower cost than MTurk.

## Executive Summary
This study compares ChatGPT's zero-shot text annotation performance against Amazon Mechanical Turk (MTurk) crowd-workers and trained annotators across five tasks using 2,382 tweets about content moderation. Results show ChatGPT achieves higher accuracy than MTurk on four out of five tasks (72.8%-78.7% vs 69.4%-75.9%) while maintaining significantly higher intercoder agreement (>95% vs 69.8%-79.1% for MTurk). The per-annotation cost is approximately twenty times cheaper than MTurk. ChatGPT also outperforms trained annotators on intercoder agreement while maintaining comparable accuracy for most tasks.

## Method Summary
The study uses a dataset of 2,382 tweets annotated by trained annotators for five tasks: relevance, stance, topics, and two types of frame detection. MTurk workers and ChatGPT (using GPT-3.5-turbo) independently annotated the same tweets using identical instructions. MTurk used two annotators per tweet with specific qualification filters, while ChatGPT used temperature settings of 1 and 0.2 with two responses per setting. Performance was evaluated using accuracy against the trained annotators' gold standard and intercoder agreement metrics.

## Key Results
- ChatGPT zero-shot accuracy exceeds MTurk on 4 of 5 tasks (72.8%-78.7% vs 69.4%-75.9%)
- ChatGPT intercoder agreement exceeds both MTurk (69.8%-79.1%) and trained annotators (84.2%-93.4%) for all tasks
- At temperature 0.2, ChatGPT agreement exceeds 95% across all tasks
- Per-annotation cost is <$0.003 for ChatGPT vs ~$0.06 for MTurk (20x cheaper)
- ChatGPT agreement remains above 84% even at temperature 1.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT zero-shot annotations outperform crowd-workers on accuracy and intercoder agreement.
- Mechanism: The LLM processes instructions as structured prompts, applying learned linguistic patterns without additional training, producing consistent outputs that align closely with gold-standard labels.
- Core assumption: The task instructions (codebook) map cleanly to the model's internal representation of the concepts; temperature and randomness do not erode alignment.
- Evidence anchors:
  - [abstract] "the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks"
  - [section] "ChatGPT's intercoder agreement exceeds that of both MTurk and trained annotators for all tasks"
  - [corpus] Weak: corpus neighbors mention ChatGPT as annotator but provide no direct accuracy comparisons; claim relies on paper data.
- Break condition: If task instructions are ambiguous or require nuanced domain knowledge not captured in pretraining, the model's outputs will drift from human judgments.

### Mechanism 2
- Claim: ChatGPT is ~20× cheaper per annotation than MTurk.
- Mechanism: API call cost per prompt is fixed and low (~$0.003) vs. variable crowd-worker compensation, which includes fees and higher per-unit labor costs.
- Core assumption: The cost-per-request model holds constant regardless of task complexity; batch processing does not incur hidden per-annotation overhead.
- Evidence anchors:
  - [abstract] "the per-annotation cost of ChatGPT is less than $0.003—about twenty times cheaper than MTurk"
  - [section] "$68 on ChatGPT (25,264 annotations) and $657 on MTurk (12,632 annotations)"
  - [corpus] No cost data in neighbors; paper is primary source.
- Break condition: If usage scales dramatically or if the API switches to a per-token pricing model, the cost advantage shrinks.

### Mechanism 3
- Claim: High intercoder agreement across repeated ChatGPT runs indicates deterministic behavior under low temperature.
- Mechanism: Setting temperature to 0.2 reduces stochasticity in token selection, yielding stable outputs across identical prompts.
- Core assumption: The underlying model weights and decoding remain stable across runs; no external variability (e.g., prompt history) alters outputs.
- Evidence anchors:
  - [section] "intercoder agreement consistently exceeds 84% with temp 1; over 95% with temp 0.2"
  - [abstract] "ChatGPT's intercoder agreement exceeds that of both MTurk and trained annotators for all tasks"
  - [corpus] No corpus data on temperature effects; claim from paper.
- Break condition: If the model's weights or decoding algorithm change (e.g., version update), consistency may degrade.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The model classifies without fine-tuning, relying only on prompt instructions.
  - Quick check question: What distinguishes zero-shot from few-shot prompting in this context?

- Concept: Intercoder agreement
  - Why needed here: It measures consistency among annotators (human or model) and validates reliability.
  - Quick check question: How does percent agreement differ from Cohen's kappa in this setting?

- Concept: Temperature parameter
  - Why needed here: Controls randomness in token generation, affecting output stability.
  - Quick check question: What happens to accuracy if temperature is increased from 0.2 to 1.0?

## Architecture Onboarding

- Component map: Prompt formatter -> OpenAI GPT-3.5-turbo API -> Response parser -> Agreement aggregator -> Accuracy evaluator vs. gold standard
- Critical path: For each tweet, send instruction + tweet text -> receive label -> store -> compare across annotators -> compute metrics
- Design tradeoffs: Higher temperature -> more creative outputs but lower agreement; lower temperature -> consistent but potentially less nuanced. Cost vs. accuracy trade-off if model version changes.
- Failure signatures: (1) Low agreement between ChatGPT runs -> temperature too high or prompt ambiguous. (2) Accuracy drops vs. gold standard -> instructions misaligned with concept definitions. (3) API errors -> network or quota limits.
- First 3 experiments:
  1. Vary temperature (0.0, 0.2, 0.5, 1.0) on a small sample; record agreement and accuracy.
  2. Add a "Let's think step-by-step" prompt suffix; measure effect on accuracy.
  3. Batch tweets in a single chat session vs. new session per tweet; compare consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance vary across different languages for text-annotation tasks?
- Basis in paper: [explicit] The authors suggest studying ChatGPT's performance across multiple languages as a promising next step.
- Why unresolved: The paper only evaluates ChatGPT's performance on English tweets, so there's no evidence of how it performs on other languages.
- What evidence would resolve it: Conducting the same annotation tasks on non-English text data and comparing ChatGPT's performance to human annotators or crowd workers.

### Open Question 2
- Question: How does ChatGPT's performance differ when annotating various types of text (e.g., social media, news media, legislation, speeches) compared to tweets?
- Basis in paper: [explicit] The authors propose studying ChatGPT's performance across multiple types of text as a promising next step.
- Why unresolved: The paper only evaluates ChatGPT on tweets, so there's no evidence of how it performs on other text types.
- What evidence would resolve it: Applying the same annotation tasks to different types of text data and comparing ChatGPT's performance to human annotators or crowd workers.

### Open Question 3
- Question: How does few-shot learning on ChatGPT compare to fine-tuned models like BERT and RoBERTa for text annotation tasks?
- Basis in paper: [explicit] The authors suggest implementing few-shot learning on ChatGPT and comparing it with fine-tuned models as a promising next step.
- Why unresolved: The paper only evaluates zero-shot performance of ChatGPT, so there's no evidence of how few-shot learning would affect its performance.
- What evidence would resolve it: Implementing few-shot learning on ChatGPT for the same annotation tasks and comparing its performance to fine-tuned models like BERT and RoBERTa.

### Open Question 4
- Question: How effective are chain of thought prompting and other strategies in increasing ChatGPT's zero-shot reasoning performance for text annotation tasks?
- Basis in paper: [explicit] The authors propose using chain of thought prompting and other strategies to increase zero-shot reasoning performance as a promising next step.
- Why unresolved: The paper only uses basic prompting without any advanced reasoning strategies, so there's no evidence of how these strategies would affect performance.
- What evidence would resolve it: Implementing chain of thought prompting and other reasoning strategies for the same annotation tasks and comparing the performance to the basic zero-shot approach used in the paper.

### Open Question 5
- Question: How does ChatGPT's performance on text annotation tasks compare to GPT-4?
- Basis in paper: [explicit] The authors mention implementing annotation tasks with GPT-4 as soon as availability permits as a promising next step.
- Why unresolved: The paper only evaluates ChatGPT (gpt-3.5-turbo), so there's no evidence of how GPT-4 would perform on the same tasks.
- What evidence would resolve it: Implementing the same annotation tasks using GPT-4 and comparing its performance to ChatGPT's results presented in the paper.

## Limitations

- Results based on a single dataset of 2,382 tweets, limiting generalizability to other domains
- Different annotation counts between ChatGPT (25,264) and MTurk (12,632) may introduce sampling bias
- Performance on more complex annotation tasks requiring deep domain expertise remains untested

## Confidence

- **High**: Cost advantage claim - straightforward price comparison verifiable through API documentation
- **Medium**: Accuracy comparisons - supported by experimental data but limited by single dataset scope
- **Low**: Generalizability across domains and task complexities - not tested beyond tweet annotation tasks

## Next Checks

1. **Dataset Generalization Test**: Replicate the experiment using a different corpus (e.g., medical literature or legal documents) to assess whether ChatGPT maintains its accuracy advantage across domains.

2. **Task Complexity Analysis**: Design a series of annotation tasks with increasing complexity (from binary classification to multi-label hierarchical classification) to identify the point at which ChatGPT's performance degrades relative to human annotators.

3. **Longitudinal Performance Tracking**: Conduct monthly accuracy assessments over six months to determine whether ChatGPT's performance remains stable as the model undergoes updates or fine-tuning by the provider.