---
ver: rpa2
title: Supply chain emission estimation using large language models
arxiv_id: '2308.01741'
source_url: https://arxiv.org/abs/2308.01741
tags:
- emissions
- scope
- data
- commodity
- emission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes using large language models to estimate enterprise
  Scope 3 (supply chain) emissions by classifying financial transaction descriptions
  into US EPA EEIO commodity classes. Domain-adapted foundation models (RoBERTa-base,
  BERT-base-uncased, ClimateBERT) were fine-tuned on 18,000 labeled transaction samples
  and outperformed classical approaches like TF-IDF and Word2Vec, achieving test F1
  scores above 86%.
---

# Supply chain emission estimation using large language models

## Quick Facts
- arXiv ID: 2308.01741
- Source URL: https://arxiv.org/abs/2308.01741
- Reference count: 28
- Key outcome: Large language models fine-tuned on 18,000 labeled transaction samples achieved test F1 scores above 86% for classifying financial descriptions into EEIO commodity classes, outperforming classical and zero-shot approaches.

## Executive Summary
This study proposes using large language models to estimate enterprise Scope 3 (supply chain) emissions by classifying financial transaction descriptions into US EPA EEIO commodity classes. Domain-adapted foundation models (RoBERTa-base, BERT-base-uncased, ClimateBERT) were fine-tuned on 18,000 labeled transaction samples and outperformed classical approaches like TF-IDF and Word2Vec. The approach enables scalable, data-driven Scope 3 estimation at enterprise scale, helping organizations meet climate action goals through automated emission classification.

## Method Summary
The researchers fine-tuned pre-trained transformer models (BERT, RoBERTa, ClimateBERT) on 18,000 labeled financial transaction descriptions mapped to 66 EEIO commodity classes. They experimented with different learning rates (5e-5 and 5e-6) and maximum sequence lengths (64-512 tokens) using a 70:20:10 train-validation-test split. The models were evaluated using weighted F1 score, with fine-tuned models achieving scores above 86% compared to classical approaches and zero-shot methods. The classification results were then mapped to emission factors for Scope 3 computation.

## Key Results
- Fine-tuned transformer models achieved test F1 scores above 86% for classifying transaction descriptions
- Zero-shot classification methods performed poorly with F1 scores around 27-43%
- Lower learning rates (5e-6) yielded smoother convergence and improved validation loss compared to standard rates (5e-5)
- Supervised fine-tuning significantly outperformed classical approaches like TF-IDF and Word2Vec

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning domain-adapted foundation models on labeled financial transaction descriptions enables accurate mapping to EEIO commodity classes by leveraging pre-trained linguistic representations. The models capture rich contextual embeddings that, when adapted to the specific domain, can distinguish between the 66 commodity classes. Core assumption: financial transaction descriptions share sufficient linguistic patterns with the pre-training corpus to benefit from transfer learning. Evidence: fine-tuned models achieved test F1 scores above 86%. Break condition: domain-specific jargon or insufficient labeled data would prevent effective adaptation.

### Mechanism 2
Lower learning rates during fine-tuning improve model convergence and performance by preventing catastrophic forgetting of general linguistic knowledge. A rate of 5e-6 allows gradual adaptation while preserving useful pre-trained representations. Core assumption: the general linguistic knowledge in pre-trained models is beneficial and requires careful preservation. Evidence: validation loss dropped from 0.577 to 0.557 when reducing learning rate from 5e-5 to 5e-6, improving F1 scores across all models. Break condition: if the pre-trained model is poorly matched to the domain, lower rates may cause underfitting.

### Mechanism 3
Zero-shot semantic text similarity performs poorly because financial transaction descriptions are often short, fragmented phrases rather than complete sentences. Pre-trained sentence transformers optimized for paragraph-level text cannot effectively generalize to these brief, context-limited inputs. Core assumption: the pre-training corpus contains longer, more coherent text than typical transaction descriptions. Evidence: zero-shot methods achieved F1 scores of only 27-43%, while supervised approaches performed significantly better. Break condition: if transaction descriptions become more sentence-like with richer context, zero-shot approaches might improve.

## Foundational Learning

- Concept: Transfer learning with pre-trained language models
  - Why needed here: Directly training from scratch on limited labeled data (~18,000 samples) would likely cause overfitting. Transfer learning leverages rich linguistic representations from large pre-training corpora.
  - Quick check question: What are the key differences between training from scratch versus fine-tuning pre-trained models, and when is each approach appropriate?

- Concept: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP)
  - Why needed here: Understanding these pre-training objectives helps select appropriate models and interpret their learned representations. MLM builds contextual word representations while NSP captures sentence-level relationships.
  - Quick check question: How do MLM and NSP objectives contribute to capturing linguistic patterns, and how might this impact classification task performance?

- Concept: Text classification with transformer models
  - Why needed here: The classification head converts contextualized embeddings into class probabilities. Understanding this process is crucial for interpreting outputs and debugging errors.
  - Quick check question: How does the classification head in transformer models work, and what are key considerations when designing it for specific tasks?

## Architecture Onboarding

- Component map: Data Preparation -> Domain Adaptation -> Classification -> Emission Computation
- Critical path: Generate and label training data → Fine-tune pre-trained model on labeled data → Classify new transaction descriptions → Calculate Scope 3 emissions based on classified commodity classes
- Design tradeoffs: Model selection (BERT vs RoBERTa vs ClimateBERT performance vs size), learning rate optimization (5e-6 vs 5e-5 for convergence vs speed), training data size requirements (performance vs labeling cost)
- Failure signatures: Overfitting (high training but low validation accuracy), underfitting (low accuracy across datasets), poor generalization (good training performance but poor new data results)
- First 3 experiments: Compare BERT, RoBERTa, and ClimateBERT performance on small labeled subset; experiment with learning rates (5e-5, 5e-6, 1e-5) for optimal convergence; evaluate impact of training data size (50%, 75%, 100%) on performance

## Open Questions the Paper Calls Out

### Open Question 1
How does performance scale when applied to real-world enterprise transaction datasets that are orders of magnitude larger and more diverse than the experimental dataset? The paper demonstrates strong performance on 18,000 curated samples but doesn't evaluate scalability to enterprise-scale volumes with varying noise levels and diversity. Enterprise data likely contains more diverse descriptions, class imbalances, and domain-specific jargon. Evidence needed: empirical results showing F1 scores, convergence behavior, and resource requirements on millions of records with different noise levels.

### Open Question 2
What is the impact of incorporating additional contextual information (vendor information, transaction metadata, historical spending patterns) on classification accuracy and emission estimates? The framework currently uses only transaction descriptions without leveraging valuable contextual metadata. Financial transactions contain rich metadata that could disambiguate similar descriptions or identify systematic classification errors. Evidence needed: comparative studies showing performance differences between descriptions alone versus augmented with vendor categories, purchase history, or other metadata features.

### Open Question 3
How sensitive are Scope 3 emission estimates to classification errors, and what are the confidence bounds around these estimates given classification uncertainty? The paper presents case studies showing commodity-level emission estimates but doesn't quantify uncertainty or analyze error propagation. Different commodity classes have vastly different emission factors, so misclassification could significantly impact total Scope 3 calculations. Evidence needed: sensitivity analysis quantifying emission estimate changes with varying classification accuracy, and confidence intervals derived from classification uncertainty.

## Limitations

- The 18,000 labeled samples may not fully capture the diversity of enterprise financial descriptions across different industries and accounting practices
- Performance metrics are based on a single dataset split and may not hold for different time periods or organizations with varying reporting structures
- The study focuses on classification accuracy without fully addressing downstream uncertainty propagation into emission estimates

## Confidence

- High confidence: Comparative performance advantage of supervised fine-tuning over classical and zero-shot methods is well-supported by empirical results showing consistent F1 score improvements
- Medium confidence: Specific hyperparameter recommendations (learning rate 5e-6, max_length 512) are based on single dataset experiments and may vary for different description patterns
- Medium confidence: Claim that domain-adapted models (ClimateBERT) will consistently outperform general models is supported but not definitively proven

## Next Checks

1. Cross-organizational validation: Test trained models on transaction descriptions from completely different enterprises to assess generalization across accounting practices and industry domains

2. Temporal robustness check: Evaluate model performance on historical transaction data from different fiscal years to determine if classification accuracy degrades with temporal shifts in financial description patterns

3. Error propagation analysis: Conduct sensitivity analysis to quantify how commodity classification errors propagate into emission estimate uncertainty, including confidence intervals for final Scope 3 emission calculations