---
ver: rpa2
title: 'Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic,
  Interpretable Tasks'
arxiv_id: '2311.12997'
source_url: https://arxiv.org/abs/2311.12997
tags:
- functions
- arxiv
- compositions
- data
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how well Transformers can learn to compose
  capabilities through training on synthetic data involving compositions of predefined
  functions. The key findings are: Transformers can generalize to exponentially or
  combinatorially many functions by learning compositional structures from small amounts
  of training data, especially when allowed to generate intermediate outputs.'
---

# Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks

## Quick Facts
- arXiv ID: 2311.12997
- Source URL: https://arxiv.org/abs/2311.12997
- Reference count: 40
- Key outcome: Transformers can learn to compose capabilities from small amounts of training data, generalizing to exponentially many functions through intermediate output generation and attention-based composition.

## Executive Summary
This paper investigates how well Transformers can learn to compose capabilities through training on synthetic data involving compositions of predefined functions. The authors demonstrate that Transformers can generalize to exponentially or combinatorially many functions by learning compositional structures from small amounts of training data, especially when allowed to generate intermediate outputs. The study reveals that attention layers in the latter half of the model are critical for enabling compositionality, and that biases in the order of compositions seen during training can limit a model's ability to compose certain combinations of functions. The authors use a minimal synthetic setup with task tokens specifying functions to apply and data tokens to operate on, showing that Transformers trained on this data exhibit "explosions of capabilities" - they can perform tasks involving compositions of functions never seen during training.

## Method Summary
The authors generate synthetic data by composing functions from sets of bijections (Fb) and permutations (Fp) over token sequences. Task tokens specify which functions to apply, and data tokens are the inputs. The study compares step-by-step prompting (generating intermediate outputs) versus direct prompting. Autoregressive Transformers (nanoGPT) are trained on this synthetic data using cross-entropy loss. The models are evaluated on in-order and out-of-order compositions not seen during training, measuring average accuracy. The authors also analyze attention maps and layer-wise linear probe accuracies to understand model internals.

## Key Results
- Transformers can generalize to exponentially many functions by learning compositional structures from small amounts of training data
- Attention layers in the latter half of the model are critical for enabling compositionality
- Generating intermediate outputs when composing functions is more effective for generalizing to new compositions than not generating intermediate outputs
- Biases in the order of compositions seen during training can limit a model's ability to compose certain combinations of functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can generalize to exponentially many functions by learning compositional structures from small amounts of training data.
- Mechanism: The model learns to represent individual functions and their composition patterns, allowing it to apply unseen combinations through learned intermediate representations.
- Core assumption: The training data exhibits sufficient compositional structure for the model to extract generalizable patterns.
- Evidence anchors:
  - [abstract] "Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions"
  - [section] "We find that a Transformer can capture the compositional structure in data and generalize to an exponential and combinatorial set of functions"
  - [corpus] Weak - no direct evidence from neighbors

### Mechanism 2
- Claim: Attention layers in the latter half of the model are critical for enabling compositionality.
- Mechanism: Attention layers select which capability to apply while feed-forward layers execute the selected capability, with later layers specializing in composition.
- Core assumption: The attention mechanism can effectively route information between different functional components.
- Evidence anchors:
  - [abstract] "the attention layers in the latter half of the model are critical to compositionality"
  - [section] "we show Attention layers in the latter half of the model play a crucial role in enabling compositional generalization"
  - [corpus] Weak - no direct evidence from neighbors

### Mechanism 3
- Claim: Generating intermediate outputs when composing functions is more effective for generalizing to new compositions than not generating intermediate outputs.
- Mechanism: Stepwise inference allows the model to recursively process intermediate outputs, building up compositional understanding incrementally.
- Core assumption: Recursive processing of intermediate outputs provides more learning signals than single-step composition.
- Evidence anchors:
  - [abstract] "generating intermediate outputs when composing functions is more effective for generalizing to new, unseen compositions than not generating any intermediate outputs"
  - [section] "We find that the use of intermediate outputs significantly simplifies the problem"
  - [corpus] Weak - no direct evidence from neighbors

## Foundational Learning

- Concept: Function composition and automorphisms
  - Why needed here: Understanding how functions can be composed and how they map input spaces to themselves is crucial for grasping the problem setup.
  - Quick check question: If F1 maps A→B and F2 maps B→C, what is the domain and codomain of F2∘F1?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model's ability to compose functions relies heavily on attention layers selecting and routing information between functional components.
  - Quick check question: How does multi-head attention allow a transformer to attend to different parts of the input simultaneously?

- Concept: Synthetic data generation and controlled experiments
  - Why needed here: The paper uses synthetic data to isolate and study compositional generalization, requiring understanding of how to design such experiments.
  - Quick check question: Why might synthetic data be preferred over real-world data for studying compositional generalization?

## Architecture Onboarding

- Component map:
  - Input tokens (task tokens specifying functions + data tokens) -> Embedding layer -> Transformer blocks (LayerNorm, attention, residual connection, MLP, residual connection) -> Output prediction layer
  - Key insight: Task tokens guide function selection while data tokens are transformed through composition

- Critical path:
  1. Parse task tokens to determine which functions to apply
  2. Use attention to route data tokens to appropriate functional processing
  3. Apply function compositions through MLP layers
  4. Generate intermediate or final outputs based on prompt format

- Design tradeoffs:
  - Step-by-step vs. direct prompting: Stepwise allows recursive refinement but requires more computation
  - Number of layers: More layers enable complex compositions but increase training difficulty
  - Embedding dimension: Larger dimensions provide more representational capacity but require more data

- Failure signatures:
  - Inability to generalize beyond training compositions (overfitting to specific function sequences)
  - Poor performance on out-of-order compositions (failure to learn flexible function ordering)
  - Degradation with increased model depth (optimization difficulties)

- First 3 experiments:
  1. Train on step-by-step prompts with 21 base functions and evaluate in-order generalization
  2. Train on random function compositions and test both in-order and out-of-order generalization
  3. Compare performance between 1-layer and multi-layer transformers on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the attention mechanisms in Transformers enable compositional generalization?
- Basis in paper: [explicit] The paper mentions that attention layers in the latter half of the model play a crucial role in enabling compositional generalization.
- Why unresolved: The paper does not provide a detailed mechanistic explanation of how attention enables compositionality. It only shows through linear probing and attention visualization that attention is important.
- What evidence would resolve it: Detailed analysis of attention weights and patterns during composition tasks, showing how attention selects relevant inputs and task tokens. Comparison with architectures lacking attention to highlight its specific role.

### Open Question 2
- Question: Under what conditions can Transformers generalize to out-of-order compositions?
- Basis in paper: [explicit] The paper finds that out-of-order generalization is harder than in-order generalization and requires more diverse training data.
- Why unresolved: The paper does not provide a clear threshold or precise conditions for when out-of-order generalization succeeds. It only shows that a small number of in-order or out-of-order compositions in training data can help.
- What evidence would resolve it: Systematic experiments varying the diversity and structure of training data to identify the minimum requirements for out-of-order generalization. Analysis of model performance on different types of out-of-order compositions.

### Open Question 3
- Question: How do biases in the training data affect a Transformer's ability to compose functions?
- Basis in paper: [explicit] The paper mentions that biases in the order of compositions seen during training can limit a model's ability to compose certain combinations of functions.
- Why unresolved: The paper does not provide a detailed analysis of how different types of biases in training data impact compositional generalization. It only shows that certain biases can limit compositionality.
- What evidence would resolve it: Experiments systematically varying the types and degrees of bias in training data to quantify their impact on compositional generalization. Analysis of model performance on compositions with different levels of displacement from the training data.

## Limitations
- The synthetic nature of the experimental setup may not fully capture the complexity of real-world compositional tasks
- The mechanism analysis primarily shows correlation rather than direct causation between attention layers and compositionality
- The relationship between training data order and generalization ability requires further exploration across different function sets and architectures

## Confidence
- **High Confidence**: The finding that Transformers can generalize to exponentially many functions from limited training data is well-supported by experimental results
- **Medium Confidence**: The claim about attention layers in the latter half being critical for compositionality has moderate support but lacks causal mechanism establishment
- **Low Confidence**: The assertion that biases in training data composition significantly limit compositional capabilities needs more investigation

## Next Checks
1. **Cross-Architecture Validation**: Replicate the core findings using different Transformer architectures (BERT, RoBERTa) and non-Transformer models (LSTMs, CNNs) to determine whether the observed compositional capabilities are architecture-specific or more general properties of sequence models.

2. **Noise Sensitivity Analysis**: Systematically introduce noise into the synthetic data generation process to test the robustness of compositional generalization. This would involve varying function complexity, introducing token-level noise, and testing how these perturbations affect the model's ability to learn and apply compositional structures.

3. **Semantic Composition Testing**: Extend the synthetic data generation to include semantically meaningful function compositions rather than purely syntactic operations. This could involve composing functions that represent real-world transformations (e.g., color space conversions, geometric transformations) to better understand how the model handles compositionality in more realistic scenarios.