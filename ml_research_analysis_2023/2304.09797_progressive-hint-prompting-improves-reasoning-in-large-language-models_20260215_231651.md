---
ver: rpa2
title: Progressive-Hint Prompting Improves Reasoning in Large Language Models
arxiv_id: '2304.09797'
source_url: https://arxiv.org/abs/2304.09797
tags:
- answer
- hints
- will
- question
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Progressive-Hint Prompting (PHP), a method
  that improves reasoning in large language models by leveraging previously generated
  answers as hints to iteratively refine responses. PHP operates by first obtaining
  a base answer using standard prompting methods like Chain-of-Thought (CoT) or Complex
  CoT, then generating subsequent answers through PHP prompts that incorporate prior
  answers as hints.
---

# Progressive-Hint Prompting Improves Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2304.09797
- Source URL: https://arxiv.org/abs/2304.09797
- Reference count: 40
- Primary result: PHP improves reasoning accuracy by 4.2% on GSM8K and achieves state-of-the-art results on SVAMP (91.9%), GSM8K (95.5%), and AQuA (79.9%)

## Executive Summary
This paper introduces Progressive-Hint Prompting (PHP), a method that iteratively refines LLM reasoning by using previously generated answers as hints for subsequent iterations. PHP operates by first obtaining a base answer through standard methods like Chain-of-Thought, then generating new answers that incorporate prior answers as hints until convergence. The approach significantly improves accuracy while remaining computationally efficient, achieving state-of-the-art performance on multiple benchmarks while reducing sample paths needed for self-consistency by 46.17%.

## Method Summary
PHP follows a human-like iterative refinement process where LLMs use their own previously generated answers as hints to progressively improve reasoning. The method starts with a base answer from standard prompting (CoT or Complex CoT), then iteratively generates new answers by incorporating the previous answer as a hint. The process continues until two consecutive answers are identical, indicating convergence. PHP is designed to be orthogonal to existing methods like self-consistency, allowing easy combination with state-of-the-art techniques.

## Key Results
- PHP achieves 4.2% improvement on GSM8K compared to Complex CoT with greedy decoding using text-davinci-003
- With GPT-4, PHP achieves state-of-the-art performance: SVAMP (91.9%), GSM8K (95.5%), and AQuA (79.9%)
- PHP reduces the number of sample paths needed for self-consistency by 46.17%
- Quality of base answers significantly impacts PHP performance, with Complex CoT providing better starting points than standard prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PHP improves reasoning by using previously generated answers as hints to guide subsequent responses.
- Mechanism: Creates a feedback loop where each answer becomes a hint for the next iteration, enabling the LLM to re-evaluate and potentially correct errors.
- Core assumption: LLMs can effectively use their own generated answers as hints, similar to human double-checking.
- Evidence anchors: Abstract states PHP "enables automatic multiple interactions... by using previously generated answers as hints to progressively guide toward the correct answers."

### Mechanism 2
- Claim: PHP is orthogonal to existing methods like CoT and self-consistency.
- Mechanism: Can be layered on top of CoT or self-consistency without conflict, using their base answers as starting points.
- Core assumption: Base answers from CoT/self-consistency provide solid foundations for iterative refinement.
- Evidence anchors: Abstract and section both state "PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques."

### Mechanism 3
- Claim: Quality of hints significantly affects PHP performance.
- Mechanism: Better base answers lead to better subsequent refinements; poor initial answers can mislead the model.
- Core assumption: Initial answer quality strongly predicts PHP's success in iterative refinement.
- Evidence anchors: Section shows replacing Complex CoT with Standard prompt reduces final performance, indicating hint quality matters.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: PHP builds on CoT by using its output as a base answer for iterative refinement.
  - Quick check question: How does CoT differ from standard prompting, and why is it effective for reasoning tasks?

- Concept: Self-consistency
  - Why needed here: PHP can be combined with self-consistency to further improve performance, and understanding self-consistency helps grasp PHP's iterative nature.
  - Quick check question: What is the main idea behind self-consistency, and how does it differ from PHP's approach?

- Concept: Prompt engineering
  - Why needed here: PHP is a form of prompt engineering requiring understanding of how to design effective prompts that incorporate hints.
  - Quick check question: What are key considerations when designing prompts for reasoning tasks, and how does PHP's approach differ from traditional prompt engineering?

## Architecture Onboarding

- Component map: Base Prompting -> Progressive-Hint Prompting -> LLM -> Convergence Check -> Return Answer
- Critical path:
  1. Generate base answer using CoT or Complex CoT
  2. Create PHP prompt incorporating base answer as hint
  3. Generate subsequent answer using PHP prompt
  4. Compare new answer with previous answer
  5. If different, repeat steps 2-4; if identical, return answer
- Design tradeoffs:
  - More iterations improve answers but increase computational cost
  - Stronger base prompts improve PHP effectiveness but may increase initial computation
  - Hint quality is crucial; poor hints can mislead the model
- Failure signatures:
  - Convergence on incorrect answers due to misleading hints
  - No improvement or degradation compared to base method
  - Excessive iterations without convergence
- First 3 experiments:
  1. Implement PHP with simple base prompt (CoT) on small dataset to verify basic mechanism
  2. Compare PHP performance with and without self-consistency to understand combination effectiveness
  3. Test PHP with different base prompts (Standard, CoT, Complex CoT) to determine impact of base prompt quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and proximity of hints affect PHP effectiveness?
- Basis in paper: Explicit discussion of hint quality significantly impacting performance and potential for misleading models with poor hints.
- Why unresolved: Limited testing of hint proximity effects with only a small set of examples, no systematic variation of hint distance or quality.
- What evidence would resolve it: Systematic experiments varying hint quality/proximity across multiple datasets, measuring effects of different types of incorrect hints.

### Open Question 2
- Question: Can PHP be made more efficient by using different models for base answers versus subsequent answers?
- Basis in paper: Explicit mention that "using greedy decoding... can increase costs" and suggestion to "employ a powerful model initially... then rely on cheaper models for subsequent answers."
- Why unresolved: Only mentioned as future direction, not tested experimentally.
- What evidence would resolve it: Experiments comparing cost vs accuracy when using different model combinations across various benchmarks.

### Open Question 3
- Question: What is the optimal stopping criterion for PHP beyond simple answer convergence?
- Basis in paper: Inferred from use of "two consecutive responses are identical" without exploring alternatives.
- Why unresolved: No investigation of whether this is most effective stopping condition or whether confidence metrics or other factors could improve stopping decisions.
- What evidence would resolve it: Experiments testing alternative stopping criteria (answer stability over 3+ responses, confidence thresholds, entropy measures) across multiple datasets.

## Limitations

- Reliance on base answer quality creates potential failure mode where poor initial hints can lead to convergence on incorrect answers
- Computational efficiency gains may not generalize across all LLM architectures or problem types
- Stopping criterion based on consecutive answer similarity might prematurely halt refinement before reaching optimal solutions

## Confidence

**High Confidence (8/10)**: Orthogonality to existing methods like CoT and self-consistency is well-supported by experimental design and theoretical framework, with multiple benchmark evidence strengthening this claim.

**Medium Confidence (6/10)**: Performance improvements on specific benchmarks are promising but require further validation across diverse problem domains, with claimed sample path reduction needing additional statistical significance scrutiny.

**Low Confidence (4/10)**: Mechanism explaining why PHP works remains somewhat speculative, lacking detailed ablation studies to isolate specific contributions of hint quality versus iteration count.

## Next Checks

1. **Ablation Study on Base Answer Quality**: Systematically vary initial answer quality (CoT vs Complex CoT vs random initialization) to quantify exact impact on PHP's convergence behavior and final accuracy.

2. **Cross-Architecture Generalization Test**: Apply PHP to different LLM architectures beyond GPT-3.5 and GPT-4, including open-source models, to verify if efficiency gains and performance improvements are consistent across model families.

3. **Alternative Stopping Criteria Evaluation**: Implement and compare alternative convergence metrics beyond consecutive answer similarity, such as answer stability over multiple iterations or confidence score thresholds, to determine if current stopping criteria are optimal.