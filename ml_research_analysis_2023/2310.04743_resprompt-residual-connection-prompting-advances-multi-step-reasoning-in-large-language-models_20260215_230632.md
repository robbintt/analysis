---
ver: rpa2
title: 'Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in
  Large Language Models'
arxiv_id: '2310.04743'
source_url: https://arxiv.org/abs/2310.04743
tags:
- reasoning
- beaker
- chemicals
- answer
- resprompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Residual Connection Prompting (RESPROMPT) improves multi-step reasoning
  in large language models by reconstructing complex reasoning graphs within prompts.
  Standard chain-of-thought prompting struggles with multi-step problems because later
  reasoning steps depend on results from earlier steps, not just the immediately preceding
  step.
---

# Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2310.04743
- Source URL: https://arxiv.org/abs/2310.04743
- Reference count: 40
- Authors: not provided in source
- Reference count: 40

## Executive Summary
Resprompt (Residual Connection Prompting) is a novel prompting method that improves multi-step reasoning in large language models by reconstructing reasoning graphs within prompts. Unlike standard chain-of-thought prompting which follows a linear flow, Resprompt explicitly links necessary prerequisites from earlier steps to later reasoning steps by repeating intermediate results verbatim. This approach transforms the linear reasoning flow into a graph structure that better captures the complex dependencies in multi-step problems.

The method achieves significant improvements in reasoning accuracy across six benchmarks covering math, sequential, and commonsense reasoning tasks. On average, Resprompt delivers 12.5% accuracy gains on LLaMA-65B and 6.8% on LLaMA2-70B compared to standard chain-of-thought approaches. The improvement is particularly pronounced for complex multi-step questions requiring at least five reasoning steps, where Resprompt outperforms the best CoT-based approaches by 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B.

## Method Summary
Resprompt addresses the limitation of standard chain-of-thought prompting in handling multi-step reasoning by reconstructing the reasoning graph within prompts through residual connections. The method identifies necessary connections - links present in the reasoning graph but missing in the linear CoT flow - and adds them explicitly to the prompts by repeating earlier intermediate results verbatim. This transforms the linear CoT structure into a graph representation that better captures the complex dependencies in multi-step problems. The approach uses a simple verbatim repetition strategy for building residual connections, which the authors found to be more effective than alternative methods like symbolic variables.

## Key Results
- Average reasoning accuracy improvements of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B across six benchmarks
- Particularly strong performance on complex multi-step questions (5+ steps): 21.1% improvement on LLaMA-65B and 14.3% on LLaMA2-70B over best CoT-based approaches
- Effectiveness scales with model size, demonstrating the "emergent ability" concept where larger models better comprehend residual connections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RESPROMPT improves multi-step reasoning by reconstructing the reasoning graph within prompts through residual connections.
- Mechanism: The method identifies necessary connections (links present in the reasoning graph but missing in the linear CoT flow) and adds them explicitly to the prompt by repeating earlier intermediate results verbatim. This transforms the linear CoT structure into a graph representation that better captures the complex dependencies in multi-step problems.
- Core assumption: The underlying reasoning process for multi-step problems forms a graph structure where later steps depend on results from multiple earlier steps, not just the immediately preceding step.
- Evidence anchors:
  - [abstract] "Our key idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts."
  - [section 2.2] "Our findings lead to the hypothesis that standard CoT struggles with multi-step reasoning because its nearly linear reasoning flow within prompts is not sufficient for capturing the reasoning graphs inherent in complex multi-step questions."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.401, average citations=0.0.
- Break condition: If the reasoning process in a problem is already linear or the intermediate results are not needed as prerequisites for later steps, RESPROMPT would provide no benefit and might even introduce unnecessary complexity.

### Mechanism 2
- Claim: RESPROMPT reduces reasoning difficulty by explicitly linking necessary prerequisites to their corresponding reasoning step.
- Mechanism: Instead of requiring each reasoning step to first implicitly identify necessary prerequisites and then perform reasoning on them, RESPROMPT explicitly provides these prerequisites within the step itself. This simplifies each step's task to just the core reasoning process.
- Core assumption: Large language models struggle with implicit identification and retrieval of earlier intermediate results when performing multi-step reasoning.
- Evidence anchors:
  - [section 2.2] "By explicitly linking necessary prerequisites using residual connections, RESPROMPT reduces the workload of a reasoning step to the core reasoning process itself, thus simplifying the mission of each step."
  - [section 3.4] "In later stages, LLMs may struggle to correctly utilize intermediate results from earlier steps, which highlights the significance of building residual connections for effective multi-step reasoning."
  - [corpus] Average neighbor FMR=0.401, indicating moderate relatedness to similar reasoning techniques.
- Break condition: If the model already has strong implicit retrieval capabilities for intermediate results, the explicit linking provided by RESPROMPT would be redundant.

### Mechanism 3
- Claim: RESPROMPT's effectiveness scales with model size due to the "emergent ability" of reasoning.
- Mechanism: The ability to comprehend and utilize residual connections appears to be an emergent capability that becomes effective only when the model has sufficiently large parameters, similar to how reasoning abilities generally emerge with scale.
- Core assumption: Understanding symbolic representations and dependencies is part of the emergent reasoning ability in LLMs.
- Evidence anchors:
  - [section 3.3] "Our findings confirm that our straightforward exact repeat approach is more effective in building residual connections within prompts." and "This indicates that the comprehension of residual connections might be part of the 'emergent ability'."
  - [section 3.3] "Scaling enhances reasoning: larger model sizes consistently bring stronger reasoning performance, which echoes the 'emergent ability' concept."
  - [corpus] Found 25 related papers, suggesting growing interest in reasoning enhancement techniques.
- Break condition: For smaller models (e.g., LLaMA-13B and 30B on certain tasks), RESPROMPT may not provide improvements and could even underperform compared to standard CoT.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: RESPROMPT builds upon and extends the standard CoT approach, so understanding its limitations is crucial.
  - Quick check question: Why does standard CoT prompting struggle with multi-step reasoning problems?

- Concept: Reasoning Graphs vs Linear Flows
  - Why needed here: The core innovation of RESPROMPT is transforming linear reasoning flows into graph representations that better match the underlying problem structure.
  - Quick check question: How does the structure of reasoning in multi-step problems differ from simple linear problems?

- Concept: Few-Shot Learning in LLMs
  - Why needed here: RESPROMPT is evaluated using few-shot prompting, and understanding how exemplar selection and ordering affects performance is important.
- Quick check question: How does the number and ordering of few-shot exemplars affect the performance of reasoning tasks in LLMs?

## Architecture Onboarding

- Component map:
  - Input: Question and few-shot exemplars
  - Core logic: Residual connection generation (identifying prerequisites and repeating them verbatim)
  - Output: Prompt with reconstructed reasoning graph
  - LLM: Standard inference engine (LLaMA family models)

- Critical path:
  1. Identify reasoning steps and their dependencies
  2. Generate residual connections by repeating prerequisite tokens
  3. Construct complete prompt with enhanced reasoning flow
  4. Run through LLM with greedy decoding
  5. Extract and evaluate answer

- Design tradeoffs:
  - Token efficiency vs reasoning accuracy: RESPROMPT increases prompt length by repeating intermediate results
  - Generality vs specialization: Works across math, sequential, and commonsense reasoning but may not help simple problems
  - Implementation complexity vs performance: Simple verbatim repetition outperforms symbolic variables

- Failure signatures:
  - No improvement over CoT: Indicates the problem has linear reasoning structure or the model is too small
  - Decreased performance: May occur when prompts become too long or the model struggles with repetition
  - Repetition in outputs: Suggests the model is having difficulty terminating or processing long prompts

- First 3 experiments:
  1. Compare RESPROMPT vs standard CoT on GSM8K with LLaMA-65B (8-shot) to verify baseline improvement
  2. Test different residual connection placements (first half, second half, uniform, full) on AQUA-RAT to identify optimal positioning
  3. Evaluate token efficiency by measuring prompt length vs accuracy gain across different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of RESPROMPT on larger language models beyond the LLaMA family, such as GPT-4 or PaLM?
- Basis in paper: [explicit] The paper acknowledges that the impact of RESPROMPT on closed-source larger LLMs like GPT-4 and PaLM is not clear and suggests further research in this direction.
- Why unresolved: The experiments in the paper were conducted solely on the open-sourced LLaMA family of models. Evaluating RESPROMPT on other larger LLMs would require access to these models, which are often proprietary.
- What evidence would resolve it: Conducting experiments on a diverse range of larger LLMs, including GPT-4 and PaLM, and comparing their performance with RESPROMPT to the baseline approaches used in the paper.

### Open Question 2
- Question: How does the performance of RESPROMPT vary with different combinations of few-shot exemplars?
- Basis in paper: [inferred] The paper mentions that the number of few-shot exemplars does not significantly impact reasoning accuracy, but increasing the number of exemplars can sometimes lead to a decrease in performance. This suggests that the selection and combination of exemplars may play a role.
- Why unresolved: The paper does not explore the impact of different combinations of exemplars on the performance of RESPROMPT. Further research is needed to understand how exemplar selection affects the model's reasoning ability.
- What evidence would resolve it: Conducting experiments with various combinations of few-shot exemplars and analyzing their impact on the performance of RESPROMPT on different reasoning tasks.

### Open Question 3
- Question: How does noise in prompts affect the performance of RESPROMPT?
- Basis in paper: [explicit] The paper explores the impact of noise in prompts on RESPROMPT and finds that it is robust to noise in GSM8K but shows some sensitivity in AQUA-RAT. However, the analysis is limited, and further investigation is needed.
- Why unresolved: The paper provides initial insights into the impact of noise on RESPROMPT but does not offer a comprehensive analysis. The extent of noise sensitivity and its impact on different reasoning tasks remain unclear.
- What evidence would resolve it: Conducting a thorough analysis of the impact of different types and levels of noise in prompts on the performance of RESPROMPT across various reasoning tasks and datasets.

## Limitations
- Effectiveness highly dependent on model size, with no improvement or degradation on smaller models (LLaMA-13B and 30B)
- Exact few-shot exemplars used for baseline and RESPROMPT prompting are not specified, crucial for faithful reproduction
- Does not explore alternative residual connection methods beyond verbatim repetition

## Confidence

- **High confidence**: The core observation that multi-step reasoning problems often have graph structures rather than linear flows, and that standard CoT prompting struggles with these dependencies. This is well-supported by examples and the reasoning presented.
- **Medium confidence**: The specific mechanism by which RESPROMPT improves reasoning through verbatim repetition of intermediate results. While the paper shows improved performance, the exact reasons why this approach works better than alternatives (symbolic variables, single link) are not fully explored.
- **Low confidence**: The claim that RESPROMPT's effectiveness is solely due to its reconstruction of reasoning graphs. The paper does not rule out other potential factors such as increased prompt length or the specific examples used in few-shot prompting.

## Next Checks

1. **Reproduce baseline performance**: Implement standard Long CoT prompting on GSM8K using LLaMA-65B with 8-shot exemplars and verify that the baseline reasoning accuracy matches the paper's reported values before applying RESPROMPT.

2. **Test alternative residual connection methods**: Implement and compare RESPROMPT with alternative approaches for building residual connections (such as symbolic variables or single link methods) on the same benchmarks to determine whether verbatim repetition is indeed the optimal approach.

3. **Evaluate model size scaling**: Systematically test RESPROMPT across the full range of LLaMA model sizes (7B, 13B, 30B, 65B, 70B) on a subset of benchmarks to precisely characterize where the emergent reasoning ability appears and whether the degradation on smaller models is consistent across all tasks.