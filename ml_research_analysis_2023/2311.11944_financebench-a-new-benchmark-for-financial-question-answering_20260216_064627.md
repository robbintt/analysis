---
ver: rpa2
title: 'FinanceBench: A New Benchmark for Financial Question Answering'
arxiv_id: '2311.11944'
source_url: https://arxiv.org/abs/2311.11944
tags:
- questions
- financial
- answer
- question
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FinanceBench is a new benchmark dataset of 10,231 financial question-answering
  pairs based on real SEC filings, designed to evaluate large language models on realistic
  financial analysis tasks. It includes domain-relevant, novel-generated, and metrics-generated
  questions across 40 publicly traded companies.
---

# FinanceBench: A New Benchmark for Financial Question Answering

## Quick Facts
- arXiv ID: 2311.11944
- Source URL: https://arxiv.org/abs/2311.11944
- Reference count: 38
- Primary result: FinanceBench tests 16 model configurations on 10,231 financial QA pairs, with best model achieving only 79% accuracy

## Executive Summary
FinanceBench is a new benchmark dataset of 10,231 financial question-answering pairs based on real SEC filings, designed to evaluate large language models on realistic financial analysis tasks. It includes domain-relevant, novel-generated, and metrics-generated questions across 40 publicly traded companies. Testing 16 model configurations including GPT-4-Turbo, Llama2, and Claude2 revealed significant weaknesses in current LLMs for financial analysis, with the best configuration achieving only 79% accuracy while retrieval-based approaches performed poorly.

## Method Summary
The FinanceBench benchmark evaluates 16 state-of-the-art model configurations including closed book, oracle, single vector store, shared vector store, and long context window approaches. The dataset consists of 10,231 financial QA pairs from SEC filings of 40 publicly traded companies, with questions designed to reflect real analyst workflows. Manual review of 2,400 responses across 150 evaluation cases using a three-category labeling scheme (correct, incorrect, refused) provides the primary accuracy metrics.

## Key Results
- GPT-4-Turbo with long context achieved 79% accuracy, the highest among all configurations
- Shared vector store retrieval resulted in only 19% accuracy, significantly underperforming other approaches
- Llama2 models produced more plausible but wrong answers compared to GPT-4-Turbo's refusal patterns
- All models exhibited significant weaknesses including hallucinations and incorrect reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The benchmark's design isolates specific failure modes by controlling information access.
- **Mechanism**: By testing models under closed book, oracle, single vector store, shared vector store, and long context configurations, the benchmark reveals whether performance issues stem from retrieval failures or reasoning failures.
- **Core assumption**: Models with perfect retrieval should perform well on oracle tasks; failure in oracle tasks indicates reasoning deficits.
- **Evidence anchors**:
  - [abstract] "Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions."
  - [section 4] "We test 16 state of the art model configurations... including a closed book, an oracle, two vector store implementations, and a long context window."
- **Break condition**: If models fail oracle tasks due to context length limitations rather than reasoning ability, the mechanism breaks.

### Mechanism 2
- **Claim**: Prompt ordering significantly impacts performance in long-context settings.
- **Mechanism**: Presenting relevant document context before the question (Context-First) improves model performance compared to Context-Last by reducing attention drift.
- **Core assumption**: Models maintain better attention to the question when it appears after relevant context rather than before.
- **Evidence anchors**:
  - [section 4.1] "Presenting the relevant filing first and then appending the question of interest (Context-First scheme) leads to significantly improved success rates for both GPT-4-Turbo (78% vs. 25%) and Claude2 (76% vs. 37%)"
- **Break condition**: If models process context independently of question positioning, prompt ordering becomes irrelevant.

### Mechanism 3
- **Claim**: The benchmark's ecological validity ensures results generalize to real financial analysis tasks.
- **Mechanism**: Questions are designed to reflect actual analyst workflows, using real SEC filings and realistic financial reasoning tasks.
- **Core assumption**: Performance on ecologically valid questions correlates with real-world deployment success.
- **Evidence anchors**:
  - [section 1] "Finance specialists routinely need to find information about companies and industries, summarize and analyze that information, and then reason about it."
  - [section 3] "We emphasized ecological validity at all times as we did not want to create a dataset that contains 'challenging' questions which would not be asked in a real-world setting."
- **Break condition**: If financial analysis workflows change significantly, ecological validity may no longer predict real-world performance.

## Foundational Learning

- **Concept**: Vector store retrieval systems
  - **Why needed here**: The benchmark tests different retrieval configurations (single vs. shared vector stores) to understand their impact on performance.
  - **Quick check question**: What is the key difference between single vector store and shared vector store configurations?

- **Concept**: Long context windows
  - **Why needed here**: The benchmark uses long context windows to test whether providing entire documents improves performance compared to retrieval-based approaches.
  - **Quick check question**: Why does the benchmark truncate documents to 95,000-100,000 tokens in long context experiments?

- **Concept**: Ecological validity in benchmark design
  - **Why needed here**: Understanding why the benchmark prioritizes realistic questions over artificially difficult ones is crucial for interpreting results.
  - **Quick check question**: What three criteria did annotators use to ensure questions were ecologically valid?

## Architecture Onboarding

- **Component map**: Question → Retrieval (if applicable) → Context preparation → Model inference → Response labeling
- **Critical path**: Question → Retrieval (if applicable) → Context preparation → Model inference → Response labeling
- **Design tradeoffs**: 
  - Single vector store is faster but less accurate than document-specific stores
  - Long context windows are more realistic but slower and more expensive
  - Oracle setting provides upper bounds but is unrealistic for production
- **Failure signatures**:
  - High refusal rates indicate retrieval issues
  - High incorrect answer rates indicate reasoning issues
  - Context truncation failures indicate document length limitations
- **First 3 experiments**:
  1. Compare GPT-4-Turbo closed book vs. single vector store to isolate retrieval impact
  2. Test context-first vs. context-last prompt ordering in long context setup
  3. Compare performance across the three question types (domain-relevant, novel-generated, metrics-generated)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the hallucinations in financial LLM responses vary by question type (domain-relevant, novel-generated, metrics-generated)?
- Basis in paper: Explicit
- Why unresolved: The paper identifies hallucinations as a significant weakness but doesn't analyze their distribution across different question types.
- What evidence would resolve it: A breakdown of hallucination rates by question type in the evaluation dataset.

### Open Question 2
- Question: How do retrieval-based approaches (vector stores) compare to long-context approaches for handling multi-statement financial calculations?
- Basis in paper: Inferred
- Why unresolved: The paper shows long-context performs better overall but doesn't isolate performance on questions requiring multiple financial statements.
- What evidence would resolve it: A controlled comparison of retrieval vs long-context performance on multi-statement calculation questions.

### Open Question 3
- Question: What is the minimum context length needed for LLMs to reliably answer financial questions without retrieval?
- Basis in paper: Inferred
- Why unresolved: The paper tests 128k context but doesn't explore the minimum effective context length for financial QA.
- What evidence would resolve it: A systematic study of LLM performance across varying context lengths on the FinanceBench dataset.

## Limitations
- The benchmark's ecological validity assumption may not fully capture real-world financial analysis complexity
- Manual review process introduces potential subjectivity in response labeling
- Focus on English-language SEC filings limits applicability to global financial markets

## Confidence
- High confidence: Current LLMs show significant weaknesses in financial question answering (supported by consistently low accuracy rates)
- Medium confidence: Prompt ordering effects in long-context settings (substantial performance differences observed but limited exploration of alternatives)
- Low confidence: Generalization to production environments (benchmark conditions may not represent real-world deployment constraints)

## Next Checks
1. Test the benchmark's predictive validity by deploying top-performing models in actual financial analysis workflows and measuring real-world performance
2. Evaluate model performance on multilingual financial documents to assess cross-language generalization
3. Investigate the impact of domain-specific fine-tuning on the observed failure modes to determine whether specialized training could address identified weaknesses