---
ver: rpa2
title: 'Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models:
  A Pilot Study'
arxiv_id: '2310.00108'
source_url: https://arxiv.org/abs/2310.00108
tags:
- attack
- data
- performance
- training
- membership
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates membership inference attacks (MIAs) on
  large-scale multi-modal models like CLIP. The authors propose three attack strategies:
  a baseline using cosine similarity between image and text features, an augmentation-enhanced
  attack that aggregates cosine similarity across transformations, and a weakly supervised
  attack that leverages ground-truth non-members.'
---

# Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study

## Quick Facts
- arXiv ID: 2310.00108
- Source URL: https://arxiv.org/abs/2310.00108
- Reference count: 40
- Primary result: Weakly supervised attack improves baseline accuracy by 17% and is 7X more effective at low false-positive rates

## Executive Summary
This paper investigates membership inference attacks (MIAs) against large-scale multi-modal models like CLIP, demonstrating that these models are vulnerable to membership inference even at scale. The authors propose three attack strategies: a baseline using cosine similarity between image and text features, an augmentation-enhanced attack that aggregates similarity across transformations, and a weakly supervised attack leveraging ground-truth non-members. The weakly supervised approach shows remarkable performance improvements, highlighting significant privacy risks in multi-modal model training. These findings underscore the need for privacy protection mechanisms in large-scale multi-modal learning systems.

## Method Summary
The paper proposes three membership inference attack strategies against CLIP models with black-box access to image and text features. The baseline cosine similarity attack computes similarity between image and text embeddings and uses thresholding for membership inference. The augmentation-enhanced attack applies transformations to samples and aggregates the resulting similarity drops, exploiting differential sensitivity between members and non-members. The weakly supervised attack leverages ground-truth non-members to construct pseudo-labeled data, training an attack model that significantly outperforms unsupervised approaches. The method is evaluated across multiple CLIP models trained on LAION and CC12M datasets using AUC and TPR@1%FPR metrics.

## Key Results
- Weakly supervised attack improves baseline accuracy by 17% in AUC
- Attack is approximately 7X more effective at low false-positive rates (TPR@1%FPR)
- Augmentation-enhanced attack shows modest improvements of 0.74% to 7% across settings
- Even large-scale multi-modal models are vulnerable to membership inference attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models exhibit higher cosine similarity between image and text features for training samples than non-training samples due to the contrastive learning objective.
- Mechanism: The CLIP training objective maximizes cosine similarity for correct image-text pairs from the training set. This creates a measurable gap where members have higher similarity scores than non-members, enabling membership inference via thresholding.
- Core assumption: The contrastive learning objective creates a meaningful separation in cosine similarity distributions between members and non-members.
- Evidence anchors:
  - [abstract]: "The rationale for using CS as a signal for membership inference is that CLIP is trained to maximize CS on training samples, which can result in members having higher CS than non-members."
  - [section 4.1]: "The key idea is that the target model is trained to maximize cosine similarity between image and text features on members, so the attacker will receive higher cosine similarity scores from members than from non-members."
- Break condition: If the model is trained with significant data augmentation or regularization that reduces the CS gap between members and non-members, or if the model achieves perfect generalization where training and test distributions are identical.

### Mechanism 2
- Claim: Data augmentation increases the cosine similarity gap between members and non-members, improving attack performance.
- Mechanism: When transformations are applied to samples, members experience larger drops in cosine similarity than non-members. Aggregating these drops across multiple transformations creates a more discriminative signal for membership inference.
- Core assumption: Members and non-members exhibit systematically different sensitivity to transformations in the CLIP embedding space.
- Evidence anchors:
  - [section 4.2]: "we observe that after applying transformations to each target sample, the decrease in cosine similarity is more significant for member samples than for non-member samples."
  - [section 5.2]: "AEA consistently demonstrates slightly improved performance across all settings by 0.74% to 7%."
- Break condition: If transformations affect members and non-members equally, or if the model becomes invariant to common transformations through training techniques.

### Mechanism 3
- Claim: One-sided knowledge of non-members enables construction of a weakly supervised attack model that significantly outperforms unsupervised approaches.
- Mechanism: Ground-truth non-members provide a reference distribution of cosine similarity scores. Samples with scores significantly above this distribution's mean are likely members. This pseudo-labeled data trains an attack model that can identify membership more effectively than simple thresholding.
- Core assumption: Non-members provide a reliable reference distribution, and the pseudo-member selection threshold creates high-quality training data for the attack model.
- Evidence anchors:
  - [abstract]: "our weakly supervised attack method that leverages ground-truth non-members... to further enhance the attack. This approach shows remarkable attack performance, improving the baseline accuracy by 17% and being around 7X more effective at low false-positive rates."
  - [section 4.3]: "We consider a scenario where the attacker has one-sided knowledge about non-members... We propose a weakly supervised attack that utilizes this non-member set to construct a model that predicts membership."
- Break condition: If the non-member distribution is too similar to the member distribution, or if the threshold selection creates poor-quality pseudo-member labels.

## Foundational Learning

- Concept: Contrastive learning and cosine similarity in multi-modal embeddings
  - Why needed here: The entire attack methodology relies on understanding how CLIP's contrastive learning objective creates separability in the embedding space based on membership status.
  - Quick check question: Why does contrastive learning between image and text features create a vulnerability for membership inference?

- Concept: Data augmentation and its effects on model representations
  - Why needed here: The augmentation-enhanced attack depends on understanding how different transformations affect model representations differently for members versus non-members.
  - Quick check question: How might common data augmentations like rotation, cropping, and color jitter differentially affect training versus non-training samples in CLIP's embedding space?

- Concept: Weakly supervised learning and pseudo-labeling
  - Why needed here: The weakly supervised attack constructs training data using one-sided knowledge, requiring understanding of how to effectively use noisy labels.
  - Quick check question: What are the key considerations when using one-sided non-member information to create pseudo-members for training an attack model?

## Architecture Onboarding

- Component map:
  - Black-box CLIP model interface -> Feature extraction (image and text) -> Attack strategy application (CSA/AEA/WSA) -> Membership inference output

- Critical path:
  1. Obtain black-box access to CLIP model
  2. Query model for features on target samples
  3. Apply chosen attack strategy (CSA, AEA, or WSA)
  4. Compare similarity scores against thresholds or attack model predictions
  5. Output membership inference result

- Design tradeoffs:
  - CSA: Simple and fast but limited performance at low false-positive rates
  - AEA: Slightly better performance but requires computation of multiple transformations
  - WSA: Best performance but requires access to non-member data and training an attack model

- Failure signatures:
  - Poor performance: Similar similarity score distributions for members and non-members
  - High false positives: Threshold too low or attack model overfitting to non-member distribution
  - High false negatives: Threshold too high or insufficient discriminative features in embedding space

- First 3 experiments:
  1. Implement CSA baseline on a small subset of LAION data with a pre-trained ViT-B/32 model, measuring AUC and TPR@1%FPR
  2. Add augmentation-enhanced attack with rotation and cropping transformations, comparing performance improvement
  3. Implement weakly supervised attack using LAION data for training and CC12M data for non-members, evaluating performance gains over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of membership inference attacks on large-scale multi-modal models change when using different types of non-member datasets (e.g., data from different time periods, sources, or domains)?
- Basis in paper: [explicit] The paper mentions that non-members can be obtained by scraping data posted after the model's publication date, but does not explore the impact of different types of non-member datasets.
- Why unresolved: The paper focuses on using data posted after the model's publication date as non-members, but does not investigate the effect of using non-members from different sources, time periods, or domains.
- What evidence would resolve it: Conducting experiments using non-member datasets from various sources, time periods, and domains, and comparing the attack performance across these different scenarios.

### Open Question 2
- Question: What is the impact of different data augmentation techniques on the effectiveness of membership inference attacks against large-scale multi-modal models?
- Basis in paper: [explicit] The paper explores the use of data augmentation to enhance the attack performance, but does not exhaustively investigate the impact of various augmentation techniques.
- Why unresolved: The paper evaluates a limited set of data augmentation techniques, leaving the potential benefits of other augmentation methods unexplored.
- What evidence would resolve it: Conducting experiments using a wide range of data augmentation techniques and analyzing their impact on the attack performance.

### Open Question 3
- Question: How do membership inference attacks perform against large-scale multi-modal models when the attacker has partial knowledge of the training data distribution?
- Basis in paper: [inferred] The paper considers two settings: zero-knowledge (no knowledge of training data) and one-sided-knowledge (access to non-members). However, it does not explore the scenario where the attacker has partial knowledge of the training data distribution.
- Why unresolved: The paper focuses on the two extreme cases of data knowledge, but does not investigate the intermediate scenario where the attacker has partial knowledge of the training data distribution.
- What evidence would resolve it: Designing and evaluating attack strategies that leverage partial knowledge of the training data distribution and comparing their performance with the zero-knowledge and one-sided-knowledge settings.

## Limitations
- Attack effectiveness relies heavily on training and test data distribution differences, which may not hold for well-generalized models
- Weakly supervised attack requires ground-truth non-members, which may not be readily available in real-world scenarios
- Performance gains from augmentation-enhanced attack are modest (0.74% to 7%) and may not justify additional computational overhead

## Confidence
- High Confidence: The baseline cosine similarity attack's mechanism and its measurable effectiveness (AUC improvement over random guessing) - supported by clear theoretical rationale and experimental validation across multiple models.
- Medium Confidence: The augmentation-enhanced attack's performance gains - while the paper shows modest improvements (0.74% to 7%), the practical significance of these gains in real-world scenarios remains unclear.
- Medium Confidence: The weakly supervised attack's superiority - the 17% improvement and 7X effectiveness at low false-positive rates are impressive but were demonstrated primarily on the LAION dataset; generalization to other multi-modal datasets needs verification.

## Next Checks
1. **Cross-dataset validation**: Test the weakly supervised attack on a different multi-modal dataset (e.g., CC12M instead of LAION) to verify the 17% improvement claim generalizes beyond the original dataset.

2. **Defense evaluation**: Implement and test common defense mechanisms (dropout, differential privacy, data augmentation during training) to assess whether the attack effectiveness decreases significantly, validating the proposed break conditions.

3. **Threshold sensitivity analysis**: Systematically vary the Î» threshold parameter in the weakly supervised attack to determine the optimal range and assess how sensitive the 7X effectiveness claim is to this hyperparameter choice.