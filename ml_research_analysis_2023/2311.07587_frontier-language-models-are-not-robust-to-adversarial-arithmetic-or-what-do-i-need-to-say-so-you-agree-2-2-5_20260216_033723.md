---
ver: rpa2
title: Frontier Language Models are not Robust to Adversarial Arithmetic, or "What
  do I need to say so you agree 2+2=5?
arxiv_id: '2311.07587'
source_url: https://arxiv.org/abs/2311.07587
tags:
- arithmetic
- attacks
- prompt
- warmup
- decay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Adversarial arithmetic exposes a simple yet challenging testbed\
  \ for language model alignment. Using a method called prompt inversion rejection\
  \ sampling (PIRS), attackers can generate natural-language prompts that reliably\
  \ cause models like PaLM2, GPT4, and Claude2 to make arithmetic errors\u2014even\
  \ steering them to specific incorrect answers."
---

# Frontier Language Models are not Robust to Adversarial Arithmetic, or "What do I need to say so you agree 2+2=5?

## Quick Facts
- arXiv ID: 2311.07587
- Source URL: https://arxiv.org/abs/2311.07587
- Reference count: 40
- Language models are highly vulnerable to adversarial arithmetic attacks that can be generated using their own self-knowledge

## Executive Summary
This paper introduces Prompt Inversion Rejection Sampling (PIRS), a method for generating adversarial prompts that cause language models to make arithmetic errors, including steering them to specific incorrect answers. The attacks are surprisingly effective, achieving success rates over 80% across multiple model families including PaLM2, GPT4, and Claude2. Notably, larger models do not show improved robustness to these attacks. The paper also demonstrates that models can be partially hardened against such attacks through reinforcement learning and agentic constitutional loops, though these defenses come with trade-offs including degraded auxiliary task performance and incomplete protection.

## Method Summary
The paper employs Prompt Inversion Rejection Sampling (PIRS) to generate adversarial arithmetic prompts by querying a Red (attacker) model to create prompts that will cause a Blue (defender) model to produce incorrect arithmetic results. These prompts are semantically plausible but manipulate the model's reasoning process. For hardening, the paper applies reinforcement learning fine-tuning on datasets of adversarial examples, and also tests agentic constitutional loops where an external model revises outputs based on constitutional principles. The effectiveness of both attacks and defenses is evaluated across multiple model families on arithmetic tasks while monitoring auxiliary task performance.

## Key Results
- PIRS-generated adversarial prompts achieve over 80% success rate in causing arithmetic errors across PaLM2, GPT4, and Claude2
- Larger language models do not show improved robustness to adversarial arithmetic attacks
- Adversarial training via reinforcement learning partially hardens models but degrades auxiliary task performance
- Agentic constitutional loops provide additional robustness but are sensitive to prompt variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PIRS exploits model self-awareness to generate adversarial examples that reliably cause arithmetic errors
- Mechanism: Red model uses its own knowledge of how it fails to craft prompts that induce Blue model to make specific wrong answers, even when the prompts are semantically plausible
- Core assumption: Large language models can predict their own failure modes when prompted with the right template
- Evidence anchors:
  - [abstract] "We additionally provide a simple algorithm for finding successful attacks by querying those same models, which we name 'prompt inversion rejection sampling' (PIRS)."
  - [section] "In PIRS, a Red (attacker) language model generates attacks that will steer a Blue (defender) language model to generate content that violates desired behavior."
  - [corpus] FMR score 0.665 indicates moderate relatedness to adversarial robustness literature, but corpus lacks direct evidence about self-awareness exploitation
- Break condition: If models lose introspective capability about their failure modes, or if prompt templates fail to elicit self-predictive responses

### Mechanism 2
- Claim: Adversarial training with PIRS-generated examples can partially harden models against attacks
- Mechanism: Reinforcement learning fine-tuning on dataset of adversarial examples reduces model susceptibility to similar attacks by learning to recognize and reject manipulative context
- Core assumption: Models can learn to distinguish between benign and adversarial contexts through exposure to PIRS-generated examples
- Evidence anchors:
  - [abstract] "We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops."
  - [section] "Validation performance on held-out adversarial examples did not change appreciably with increasing dataset size."
  - [corpus] No direct corpus evidence about effectiveness of RL-based adversarial training on arithmetic tasks
- Break condition: If attack distribution shifts beyond what model was trained on, or if model overfits to specific attack patterns

### Mechanism 3
- Claim: Agentic constitutional loops provide additional robustness by allowing models to revise answers based on constitutional principles
- Mechanism: External model reviews generated answers against constitutional principles and revises them if violations are detected, creating a feedback loop that catches errors missed by initial feedforward generation
- Core assumption: Constitutional principles can be encoded in prompts that external models can reliably interpret and apply to revise answers
- Evidence anchors:
  - [abstract] "We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops."
  - [section] "This constitution was provided to the revision agent, with the query and original model output, in the following prompt"
  - [corpus] Weak corpus evidence; no direct literature on constitutional loops for arithmetic robustness
- Break condition: If constitutional principles are too vague for consistent application, or if revision model cannot accurately detect violations

## Foundational Learning

- Concept: Adversarial examples in machine learning
  - Why needed here: Understanding how small, semantically meaningful changes to input can cause large changes in model output is fundamental to understanding PIRS
  - Quick check question: What makes an adversarial example effective in causing a model to fail?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: The paper uses RL-based fine-tuning to harden models against adversarial attacks, building on RLHF techniques
  - Quick check question: How does RLHF differ from supervised fine-tuning, and why might it be more effective for robustness?

- Concept: Constitutional AI
  - Why needed here: The agentic constitutional loop approach draws from Constitutional AI, using principles to guide model behavior rather than direct human feedback
  - Quick check question: What are the key differences between using a constitution versus direct reward signals in training?

## Architecture Onboarding

- Component map: Red model (attack generator) → PIRS algorithm → Attack dataset → Blue model (target) → Evaluation framework → RL fine-tuning pipeline → Hardened model
- Critical path: Red model generates attacks → Blue model evaluates attacks → Successful attacks added to training dataset → Blue model fine-tuned with RL → Evaluation of hardened model
- Design tradeoffs: Black-box attack generation (PIRS) trades computational efficiency for transferability across model families, versus white-box gradient-based methods
- Failure signatures: Complete attack success (100% error rate), complete attack failure (0% error rate), or partial success indicating model resistance
- First 3 experiments:
  1. Generate PIRS attacks with different templates on base model and measure success rate
  2. Fine-tune model with RL on PIRS-generated dataset and evaluate on held-out attacks
  3. Test hardened model with out-of-distribution attack prompts to measure robustness transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do models not become more robust against adversarial arithmetic attacks as they are made larger?
- Basis in paper: [explicit] The paper states "The overall trend is unclear, but models do not appear to become more robust against attacks as they are made larger" (Figure 5).
- Why unresolved: This contradicts common expectations that larger models should have better generalization and robustness. The authors don't provide an explanation for this surprising result.
- What evidence would resolve it: Testing a wider range of model sizes, or analyzing internal representations of attacked vs. non-attacked inputs to understand why larger models fail to resist these attacks.

### Open Question 2
- Question: What features of attack-generating prompts provide the best robustness to out-of-distribution attacks after training?
- Basis in paper: [explicit] The paper notes that "it is difficult to draw a clear conclusion" about how different prompt suffixes affect attack success after hardening (Section 4.5.2).
- Why unresolved: The authors found inconsistent results when varying attack suffixes during training and testing, suggesting the relationship between training data distribution and robustness is complex.
- What evidence would resolve it: Systematic experiments training on various attack prompt families and testing on out-of-distribution attacks with different suffixes.

### Open Question 3
- Question: Why does agentic constitutional revision reduce performance with an adversarially hardened model?
- Basis in paper: [explicit] The paper states "it should be noted that the success of this intervention depends on the ability of the revision model to accurately judge and revise generated text" (Section 5), but doesn't fully explain the performance degradation.
- Why unresolved: The authors observe that constitutional revision "reduces performance" when combined with hardening, but don't analyze why this interaction occurs.
- What evidence would resolve it: Detailed analysis of revision model behavior on hardened vs. non-hardened outputs, and testing whether this is a general issue or specific to their implementation.

## Limitations
- The study focuses exclusively on arithmetic tasks, which may not generalize to other domains
- Adversarial hardening shows clear trade-offs with auxiliary task performance
- No testing of white-box attacks or gradient-based methods for comparison
- Limited exploration of the relationship between model size and robustness

## Confidence
- **High** confidence in PIRS attack effectiveness based on consistent success rates (>80%) across multiple model families
- **Medium** confidence in adversarial hardening effectiveness due to incomplete protection and performance trade-offs
- **Low-Medium** confidence in generalizability due to arithmetic-specific focus and sensitivity to prompt variations

## Next Checks
1. Test PIRS methodology on non-arithmetic reasoning tasks to assess domain transfer
2. Evaluate hardened models against out-of-distribution attack prompts not seen during training
3. Compare PIRS attack effectiveness against gradient-based white-box attacks on the same models