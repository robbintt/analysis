---
ver: rpa2
title: LLMs cannot find reasoning errors, but can correct them given the error location
arxiv_id: '2311.08516'
source_url: https://arxiv.org/abs/2311.08516
tags:
- mistake
- traces
- trace
- step
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIG-Bench Mistake, a dataset of Chain-of-Thought
  reasoning traces with annotated logical mistakes, and shows that current LLMs struggle
  with identifying these mistakes even in simple, unambiguous cases. The authors propose
  backtracking as a method to correct reasoning errors when mistake location is provided,
  showing significant performance gains on 5 reasoning tasks.
---

# LLMs cannot find reasoning errors, but can correct them given the error location

## Quick Facts
- arXiv ID: 2311.08516
- Source URL: https://arxiv.org/abs/2311.08516
- Reference count: 11
- Key outcome: LLMs struggle to identify their own reasoning errors, but can correct them effectively when given the error location, with backtracking achieving significant performance gains when guided by a lightweight reward model at 60-70% accuracy.

## Executive Summary
This paper addresses a fundamental limitation in large language models: while they can generate Chain-of-Thought reasoning traces, they struggle to identify logical mistakes within those traces. The authors introduce BIG-Bench Mistake, a dataset of CoT reasoning traces with annotated logical errors, and demonstrate that current LLMs fail to find these mistakes even in simple, unambiguous cases. They propose backtracking as a solution, showing that when given the location of an error, LLMs can correct their reasoning effectively. The approach uses a lightweight reward model at 60-70% accuracy to guide backtracking, achieving significant performance improvements across five reasoning tasks. The work reveals a critical gap between LLMs' reasoning and correction abilities, offering a practical solution through lightweight reward modeling.

## Method Summary
The authors evaluate LLM performance on five reasoning tasks using the BIG-Bench Mistake dataset containing 2186 Chain-of-Thought traces with human-annotated first logical mistake locations. They benchmark GPT-4, GPT-4-Turbo, and GPT-3.5-Turbo with 3-shot prompting for mistake finding using three approaches: direct trace-level, direct step-level, and CoT step-level prompting. The backtracking method is implemented by regenerating only the identified erroneous step while reusing correct prior reasoning, using temperature=1 for exploration and filtering duplicate options. They also train a small reward model classifier to identify mistake locations and test its effectiveness across different generator models. The approach is validated against gold labels and simulated reward models at varying accuracy levels.

## Key Results
- Current LLMs fail to identify logical mistakes in Chain-of-Thought traces even when errors are unambiguous
- Backtracking with gold-standard mistake locations significantly improves reasoning accuracy across all tested tasks
- A lightweight reward model at 60-70% accuracy is sufficient to guide effective backtracking
- A small classifier trained for mistake finding outperforms larger models and transfers across different LLM generators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can correct reasoning errors when given explicit mistake location, but struggle to find these errors independently.
- Mechanism: Backtracking leverages mistake location to regenerate only the erroneous step while reusing correct prior reasoning, reducing error propagation.
- Core assumption: The model's correction ability is robust when error location is provided, even if it cannot identify the error itself.
- Evidence anchors:
  - [abstract] "we show that poor self-correction performance stems from LLMs' inability to find logical mistakes, rather than their ability to correct a known mistake"
  - [section 4.1.2] "The scores show that the gains from correcting incorrectans traces are larger than losses from changing originally correct answers"

### Mechanism 2
- Claim: A lightweight reward model at 60-70% accuracy can effectively guide backtracking to improve reasoning traces.
- Mechanism: The reward model identifies mistake locations with moderate accuracy, and backtracking uses these imperfect signals to selectively regenerate steps.
- Core assumption: Even partial accuracy in mistake detection provides enough signal for meaningful improvement.
- Evidence anchors:
  - [abstract] "backtracking remains effective with a reward model at 60-70% accuracy"
  - [section 4.2.1] "for most tasks, ∆accuracy✓ is already larger than ∆accuracy✗ at around 60-70% accuracyRM"

### Mechanism 3
- Claim: Separate training of a small reward model for mistake finding transfers across different LLM generators.
- Mechanism: The reward model learns to identify reasoning errors independently of the generator model, allowing it to work with any LLM as the reasoning engine.
- Core assumption: Mistake patterns are generalizable enough that a small model trained on one set of tasks can transfer to out-of-distribution tasks.
- Evidence anchors:
  - [section 4.3] "We see gains for 4 out of 5 of the tasks" when using a smaller reward model
  - [abstract] "train a small classifier that outperforms larger models in mistake finding"

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The entire approach depends on LLMs generating step-by-step reasoning traces that can be analyzed for errors
  - Quick check question: Can you explain how CoT differs from direct answer generation and why it enables error analysis?

- Concept: Reinforcement learning with a reward model
  - Why needed here: Backtracking is framed as a lightweight RL alternative where the reward model guides correction without weight updates
  - Quick check question: How does using a reward model for guidance differ from traditional RL fine-tuning of the generator model?

- Concept: Error propagation in sequential reasoning
  - Why needed here: Understanding how one error can cascade through subsequent steps is crucial for appreciating why pinpointing and correcting the first error is effective
  - Quick check question: Why might correcting only the first error be more effective than regenerating the entire trace?

## Architecture Onboarding

- Component map: Generator LLM -> Reward Model -> Backtracking Engine -> Validation Layer

- Critical path:
  1. Generate initial trace
  2. Apply reward model to identify mistake location
  3. If mistake found, regenerate that step with temperature=1
  4. Filter regenerated options to exclude previous mistake
  5. Select highest probability valid option
  6. Generate remaining steps with temperature=0
  7. Validate final answer

- Design tradeoffs:
  - Model size vs. effectiveness: Small reward models work but may need more data
  - Temperature settings: Balance between exploration (temperature=1) and determinism (temperature=0)
  - Step granularity: CoT vs. direct step-level prompting affects accuracy and computational cost

- Failure signatures:
  - No improvement despite backtracking: May indicate reward model accuracy too low or mistake location ambiguous
  - Performance degradation: Could mean reward model is overfitting or making systematic errors
  - Inconsistent corrections: May indicate temperature settings or filtering logic issues

- First 3 experiments:
  1. Implement backtracking with gold standard mistake labels to verify the core mechanism works
  2. Test backtracking with simulated reward models at different accuracy levels (50%, 60%, 70%) to find the effectiveness threshold
  3. Train and evaluate a small reward model on 4 tasks, hold out 1 task for transfer learning assessment

## Open Questions the Paper Calls Out
The paper acknowledges that its dataset features artificial and unrealistic tasks for real-world applications, suggesting that further work is needed to determine the effectiveness of backtracking in a more realistic setting. Additionally, the authors leave for future work the effect of iterative backtracking with a reward model, noting that the generator model may make another mistake after backtracking for the first time, which can then be identified and corrected again.

## Limitations
- The dataset uses artificial tasks that may not reflect real-world reasoning complexity
- The 60-70% reward model accuracy threshold may not generalize across all reasoning domains
- Computational cost of step-level prompting versus trace-level prompting tradeoffs are not fully characterized

## Confidence
**High Confidence**: LLMs cannot identify their own reasoning errors even when corrections are possible with error location.
**Medium Confidence**: Backtracking with 60-70% accurate reward models effectively improves reasoning across tested tasks.
**Medium Confidence**: Small reward models can transfer mistake-finding capability across different LLM generators.

## Next Checks
1. Test the reward model trained on BIG-Bench Mistake tasks on completely different reasoning domains (e.g., scientific reasoning, causal inference) to assess true generalization capability beyond the 1 held-out task.
2. Conduct a systematic study categorizing mistake types (logical, arithmetic, semantic) to determine if the reward model's effectiveness varies by error category, and whether backtracking performance correlates with specific mistake patterns.
3. Measure the wall-clock time and token usage for step-level versus trace-level prompting in practical applications, and quantify the tradeoff between improved accuracy and increased computational cost across different task complexities.