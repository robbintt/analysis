---
ver: rpa2
title: Solving Long-run Average Reward Robust MDPs via Stochastic Games
arxiv_id: '2312.13912'
source_url: https://arxiv.org/abs/2312.13912
tags:
- rmdps
- reward
- average
- policy
- long-run
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors establish a reduction from solving long-run average\
  \ reward polytopic RMDPs to solving long-run average reward turn-based stochastic\
  \ games, enabling a host of new results for polytopic RMDPs. This reduction shows\
  \ that solving long-run average reward polytopic RMDPs is in NP \u2229 CONP and\
  \ admits a randomized algorithm with sub-exponential expected runtime."
---

# Solving Long-run Average Reward Robust MDPs via Stochastic Games

## Quick Facts
- arXiv ID: 2312.13912
- Source URL: https://arxiv.org/abs/2312.13912
- Reference count: 31
- One-line primary result: The authors establish a reduction from solving long-run average reward polytopic RMDPs to solving long-run average reward turn-based stochastic games.

## Executive Summary
This paper addresses the problem of solving long-run average reward robust Markov decision processes (RMDPs) with polytopic uncertainty sets. The authors show that these RMDPs can be reduced to solving long-run average reward turn-based stochastic games (TBSGs), enabling the application of existing results for TBSGs to RMDPs. This reduction leads to new complexity results, showing that the threshold decision problem for polytopic RMDPs is in NP ∩ coNP and admits a randomized algorithm with sub-exponential expected runtime. Additionally, the reduction allows for the design of a policy iteration algorithm for solving polytopic RMDPs.

## Method Summary
The authors present a reduction from solving long-run average reward polytopic RMDPs to solving long-run average reward TBSGs. The reduction encodes the adversarial selection of transitions from uncertainty polytopes in the RMDP as a turn-based game where the agent (Max) chooses actions and the environment (Min) selects vertices of the polytope. By alternating turns and carefully defining rewards and transitions, the TBSG simulates the worst-case behavior of the RMDP. The reduction is shown to be polynomial-time and preserves the optimal value under long-run average reward objectives. The authors then leverage this reduction to design a policy iteration algorithm for solving polytopic RMDPs, using a policy profile evaluation (PPE) subprocedure that iteratively solves discounted TBSGs with increasing discount factors.

## Key Results
- The reduction from polytopic RMDPs to TBSGs is valid and preserves optimal values under long-run average reward objectives.
- The reduction runs in polynomial time relative to the input RMDP size.
- By reducing to TBSGs, the authors inherit complexity bounds: the decision problem is in NP ∩ coNP and admits a sub-exponential randomized algorithm.
- The proposed policy iteration algorithm (RPPI) is more efficient than existing value iteration-based methods on unichain polytopic RMDPs and is applicable to multichain polytopic RMDPs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reduction from polytopic RMDPs to TBSGs is valid and preserves the optimal value under long-run average reward objectives.
- Mechanism: The reduction encodes the adversarial selection of transitions from uncertainty polytopes in the RMDP as a turn-based game where the agent (Max) chooses actions and the environment (Min) selects vertices of the polytope. By alternating turns and carefully defining rewards and transitions, the TBSG simulates the worst-case behavior of the RMDP.
- Core assumption: Pure positional determinacy holds for TBSGs with long-run average rewards (Theorem 1 from the literature), and the constructed TBSG correctly simulates the polytopic RMDP dynamics.
- Evidence anchors:
  - [abstract] "show that it can be reduced to solving long-run average reward turn-based stochastic games with finite state and action spaces."
  - [section] "The reduction formalization turns out to be technically non-trivial and we provide a first formal proof of the correctness of the reduction."
- Break condition: If the polytopic uncertainty sets are not convex or if pure positional determinacy fails for some reason, the reduction may not preserve the optimal value.

### Mechanism 2
- Claim: The reduction runs in polynomial time relative to the input RMDP size.
- Mechanism: The TBSG constructed from an RMDP has states and actions that are polynomially bounded in the size of the RMDP: states are copies of RMDP states and state-action pairs, actions are copies of RMDP actions and vertices of polytopes. Transition and reward functions are built in polynomial time.
- Core assumption: Polytopic uncertainty sets are represented explicitly as lists of vertices, so their size is polynomial in the RMDP encoding.
- Evidence anchors:
  - [section] "The following theorem shows that the above is a polynomial-time reduction."
  - [corpus] Weak evidence; the proof of Theorem 3 in Appendix B details the polynomial bound, but the paper does not discuss alternative representations or degenerate cases.
- Break condition: If uncertainty polytopes are given implicitly (e.g., via constraints), extracting vertices may not be polynomial.

### Mechanism 3
- Claim: By reducing to TBSGs, we inherit complexity bounds: the decision problem is in NP ∩ coNP and admits a sub-exponential randomized algorithm.
- Mechanism: Since TBSGs with long-run average rewards are known to be in NP ∩ coNP and solvable by a randomized sub-exponential algorithm (via reduction to simple stochastic games), and the reduction preserves optimal values, these bounds transfer to polytopic RMDPs.
- Core assumption: The reduction preserves optimal values exactly, and the TBSG inherits the complexity class membership.
- Evidence anchors:
  - [abstract] "showing for the first time that the threshold decision problem for them is in NP ∩ coNP and that they admit a randomized algorithm with sub-exponential expected runtime."
  - [section] "The decision problems associated with solving long-run average reward TBSGs... are known to be in NP ∩ coNP."
- Break condition: If the reduction introduces non-polynomial blowup in game size, complexity bounds may not transfer.

## Foundational Learning

- Concept: Pure positional determinacy for turn-based stochastic games
  - Why needed here: The reduction relies on the fact that optimal policies in TBSGs can be chosen to be pure and positional, which ensures that the mapping between RMDP policies and TBSG policies is well-defined and that optimal values are preserved.
  - Quick check question: In a two-player turn-based stochastic game with long-run average rewards, can both players achieve their optimal values using only pure, positional strategies?

- Concept: Polytopic uncertainty sets and their representation
  - Why needed here: The reduction encodes each uncertainty polytope as a set of vertices, which become actions for the environment player in the TBSG. Understanding how polytopes are represented (explicit vertex lists) is crucial for the reduction's correctness and complexity.
  - Quick check question: Given a polytope defined by a set of vertices, how do you construct a randomized strategy that selects any point inside the polytope using only the vertices?

- Concept: Long-run average reward vs discounted reward in MDPs/TBSGs
  - Why needed here: The reduction and algorithms are specifically for long-run average reward objectives. Knowing the difference between average and discounted objectives, and how they relate (e.g., via Blackwell optimality), is essential for understanding the scope and limitations of the results.
  - Quick check question: What is the key difference between long-run average reward and discounted reward objectives, and why does this matter for policy iteration?

## Architecture Onboarding

- Component map: RMDP model (states, actions, polytopic uncertainty sets, rewards, initial state) -> TBSG construction (state/action partitions, transition function, reward function, initial state) -> Policy evaluation and iteration (PPE subprocedure, discounted TBSG policy iteration) -> Complexity analysis and experimental validation

- Critical path:
  1. Build the TBSG from the input RMDP.
  2. Iteratively solve discounted TBSGs, increasing the discount factor.
  3. Evaluate policies for long-run average optimality using PPE.
  4. Return optimal RMDP policy and value.

- Design tradeoffs:
  - Using TBSG policy iteration leverages existing efficient algorithms but requires constructing a potentially larger game.
  - The reduction assumes explicit polytope vertices; implicit representations would require additional processing.
  - Focusing on long-run average rewards allows exploiting rich TBSG theory, but excludes scenarios where discounted rewards are preferred.

- Failure signatures:
  - If the RMDP uncertainty sets are not polytopes (e.g., general convex sets), the reduction fails.
  - If the constructed TBSG is too large (e.g., polytopes with exponentially many vertices), the reduction may not be practical.
  - If the policy evaluation in PPE is numerically unstable, the algorithm may not terminate correctly.

- First 3 experiments:
  1. Run the reduction and policy iteration on a small, hand-crafted polytopic RMDP with known optimal policy to verify correctness.
  2. Measure runtime and memory usage on a range of polytopic RMDPs with increasing numbers of states, actions, and polytope vertices.
  3. Compare the performance of RPPI against value iteration baselines on both unichain and multichain polytopic RMDPs, as described in the experimental section.

## Open Questions the Paper Calls Out
- Open Question 1: Can the reduction from polytopic RMDPs to TBSGs be extended to handle non-rectangular uncertainty sets, such as those arising from probabilistic logic or data-driven approaches?
- Open Question 2: Can the complexity results for solving long-run average reward polytopic RMDPs be improved beyond the current NP ∩ coNP bound and sub-exponential randomized algorithm?
- Open Question 3: How does the performance of Robust Polytopic Policy Iteration (RPPI) compare to other policy iteration algorithms for solving long-run average reward RMDPs with different types of uncertainty sets (e.g., interval, L1, or L∞ uncertainty sets)?

## Limitations
- The reduction assumes that uncertainty polytopes are represented explicitly as lists of vertices, which may not always be the case in practice.
- The experimental results focus on policy iteration and value iteration baselines, but do not directly compare against other methods for solving RMDPs (e.g., robust dynamic programming or linear programming approaches).
- The paper does not provide an explicit implementation or empirical validation of the reduction itself.

## Confidence
- **High Confidence**: The correctness of the reduction from polytopic RMDPs to TBSGs, assuming the core assumptions about TBSGs (pure positional determinacy, polynomial-time solvability) hold.
- **Medium Confidence**: The inherited complexity bounds (NP ∩ coNP membership, sub-exponential randomized algorithm) for polytopic RMDPs, as they rely on the reduction being correct and efficient.
- **Low Confidence**: The practical efficiency and scalability of the proposed methods, as the experiments are limited in scope and do not explore the full range of RMDP instances or compare against all relevant baselines.

## Next Checks
1. Implement and validate the reduction from polytopic RMDPs to TBSGs on a set of small, hand-crafted examples with known optimal policies. Verify that the optimal values and policies are preserved under the reduction.
2. Conduct a more extensive empirical study comparing RPPI and RVI against a wider range of robust MDP solvers (e.g., robust dynamic programming, linear programming approaches) on a variety of RMDP instances, including those with non-polytopic uncertainty sets.
3. Analyze the computational complexity of the reduction and the proposed algorithms in terms of the size of the uncertainty polytopes. Investigate the impact of polytope representation (explicit vertices vs. implicit constraints) on the efficiency of the reduction and the overall solution process.