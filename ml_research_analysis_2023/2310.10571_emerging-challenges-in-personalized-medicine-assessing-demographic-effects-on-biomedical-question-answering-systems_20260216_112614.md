---
ver: rpa2
title: 'Emerging Challenges in Personalized Medicine: Assessing Demographic Effects
  on Biomedical Question Answering Systems'
arxiv_id: '2310.10571'
source_url: https://arxiv.org/abs/2310.10571
tags:
- gender
- answers
- information
- systems
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We examine the effect of irrelevant demographic information on
  biomedical QA systems, using questions whose answers do not depend on sex, ethnicity,
  or sexual orientation. We compare a KG-grounded system and a text-based system,
  finding that irrelevant demographics change up to 15% and 23% of answers respectively,
  including changes affecting accuracy.
---

# Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems

## Quick Facts
- arXiv ID: 2310.10571
- Source URL: https://arxiv.org/abs/2310.10571
- Reference count: 11
- Irrelevant demographic information changes up to 15% of KG-grounded system answers and up to 23% of text-based system answers

## Executive Summary
This paper investigates how irrelevant demographic information (sex, ethnicity, sexual orientation) affects biomedical question answering systems. Using 100 demographic-independent MedQA-USMLE vignettes expanded into 16,700 questions with controlled demographic variations, the study finds that up to 23% of answers change when demographic information is added to questions whose answers should be invariant to these attributes. The research reveals that both KG-grounded and text-based biomedical QA systems are susceptible to demographic bias, with generic language models showing even higher sensitivity. These findings raise significant fairness concerns for clinical applications of AI systems.

## Method Summary
The study compares two biomedical QA systems (QAGNN - KG-grounded, BioLinkBERT - text-based) against a generic English-text-based transformer. Researchers created demographic-enhanced datasets by adding controlled variations of ethnicity, gender, names, and sexual orientation to 100 demographic-independent MedQA-USMLE vignettes. They measured answer changes and accuracy across different demographic dimensions, focusing on questions whose answers should not depend on the added demographic information. The data will be publicly available under MIT License.

## Key Results
- Irrelevant demographic information changes up to 15% of KG-grounded system answers and up to 23% of text-based system answers
- Both systems show higher sensitivity to homosexual than bisexual or heterosexual information
- Generic language models show more demographic sensitivity than biomedical-specific models, changing up to 17% of answers for gender variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Irrelevant demographic information changes up to 15% of KG-grounded system answers and up to 23% of text-based system answers.
- Mechanism: Models encode demographic cues in their learned representations, leading to answer shifts when demographics are added to prompts, even when the correct answer should be invariant to these attributes.
- Core assumption: The models' training data contains demographic correlations that become encoded in their weights, and the input processing pipeline propagates these cues to the final prediction layer.
- Evidence anchors:
  - [abstract] "irrelevant demographic information change up to 15% of the answers of a KG-grounded system and up to 23% of the answers of a text-based system, including changes that affect accuracy."
  - [section] "We find that irrelevant demographic information change up to 15% of the answers of a KG-grounded system and up to 23% of the answers of a text-based system, including changes that affect accuracy."
  - [corpus] Weak evidence: no directly relevant corpus neighbors found for demographic bias in biomedical QA.

### Mechanism 2
- Claim: The two system types (KG-grounded vs. text-based) differ in which demographic information affects them.
- Mechanism: The KG-grounded system's grounding in a knowledge graph that excludes demographic information provides some protection, but text encoding still introduces bias. The text-based system is more susceptible because it relies entirely on learned representations from potentially biased text corpora.
- Core assumption: KG grounding can reduce but not eliminate bias because the text encoding stage still passes demographic information into the model.
- Evidence anchors:
  - [abstract] "We also observe that the two systems differ in which demographic information affects them."
  - [section] "we find that irrelevant demographic information change up to 15% of the answers of a KG-grounded system and up to 23% of the answers of a text-based system" and "both models have almost double the amount of changed answers for homosexual than bisexual or heterosexual."
  - [corpus] Weak evidence: corpus neighbors discuss demographic bias in LLMs generally but not specifically comparing KG vs text systems.

### Mechanism 3
- Claim: Generic language models show more demographic sensitivity than biomedical-specific models.
- Mechanism: Biomedical models trained on PubMed data contain less demographic noise than generic models trained on web-scale data, reducing irrelevant demographic correlations in their learned representations.
- Core assumption: Biomedical text contains fewer demographic attributes than general web text, leading to less demographic bias in the resulting models.
- Evidence anchors:
  - [section] "we also compare biomedically-trained systems to a generic one, trained on English text" and "the generic system changes even more of its answers in most cases (up to 17% for gender)."
  - [section] "the generic transformer has more than double the amount of answers change for each gender" and "it also has an equivalent amount or more for almost any ethnicity."
  - [corpus] Weak evidence: corpus neighbors discuss demographic bias in LLMs but not specifically biomedical vs generic comparisons.

## Foundational Learning

- Concept: Understanding biomedical question answering systems and their architectures
  - Why needed here: The paper compares two types of biomedical QA systems (KG-grounded vs text-based), requiring understanding of how each processes information and where demographic bias might enter.
  - Quick check question: What is the key architectural difference between QAGNN (KG-grounded) and BioLinkBERT (text-based) that could explain their different sensitivities to demographic information?

- Concept: Knowledge graph grounding and its potential for bias mitigation
  - Why needed here: The paper suggests KG grounding might reduce demographic bias, so understanding how KGs work and their limitations is crucial for interpreting the results.
  - Quick check question: How does the QAGNN system use its knowledge graph to answer questions, and at what stage could demographic information still influence the final answer?

- Concept: Demographic bias measurement and evaluation methodology
  - Why needed here: The paper creates demographic-enhanced datasets and measures answer changes, requiring understanding of experimental design for bias detection.
  - Quick check question: Why does the paper focus on questions whose answers should be independent of demographics, and how does this help isolate the effect of demographic information on model behavior?

## Architecture Onboarding

- Component map:
  - QAGNN: KG retrieval → subgraph extraction → text encoding → relevance scoring → GNN reasoning → answer selection
  - BioLinkBERT: Text encoding → document graph creation → self-supervised pretraining → QA fine-tuning
  - Generic baseline: Same as BioLinkBERT but trained on generic English text

- Critical path: Input preprocessing → demographic information injection → text encoding → knowledge retrieval/representation → answer prediction

- Design tradeoffs: KG grounding provides structured knowledge but requires entity linking; text-based approaches are more flexible but more susceptible to bias; biomedical pretraining reduces some bias but may still encode demographic correlations

- Failure signatures: Answer changes when demographics are added to questions that should be demographic-independent; different sensitivities to different demographic attributes; accuracy drops correlated with demographic information presence

- First 3 experiments:
  1. Test both systems on demographic-independent questions with no demographic information added to establish baseline performance
  2. Add single demographic attributes (gender, ethnicity, sexual orientation) to questions and measure answer changes
  3. Combine multiple demographic attributes and compare to single-attribute results to identify interaction effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of additional gender identities in the experiment affect the results compared to the binary male/female representation?
- Basis in paper: [explicit] The paper mentions using two genders but expects the results to generalize to additional genders.
- Why unresolved: The experiment was conducted with only two genders, so the impact of including a broader spectrum of gender identities is not directly tested.
- What evidence would resolve it: Conducting the same experiments with a more inclusive set of gender identities and comparing the results to the current findings would provide evidence.

### Open Question 2
- Question: How does the accuracy of the QA systems change when irrelevant demographic information is included in the training data as opposed to just the test data?
- Basis in paper: [inferred] The paper discusses the effect of irrelevant demographic information on QA systems but does not explore the impact of including such information in the training data.
- Why unresolved: The study focuses on the effect of demographic information on test data, leaving the potential impact on training data unexplored.
- What evidence would resolve it: Training the QA systems with demographic information included in the training data and evaluating their performance on both demographic-enhanced and non-enhanced test data would provide insights.

### Open Question 3
- Question: What are the specific mechanisms by which knowledge graph (KG)-grounded systems are less susceptible to demographic biases compared to text-based systems?
- Basis in paper: [explicit] The paper notes that KG-grounded systems are assumed to be less susceptible to irrelevant demographic information, but both tested systems still show susceptibility.
- Why unresolved: The study identifies susceptibility in both systems but does not delve into the underlying mechanisms that make KG-grounded systems theoretically less biased.
- What evidence would resolve it: Analyzing the internal workings and data flow of KG-grounded systems to identify how demographic information is processed and filtered could elucidate the mechanisms involved.

## Limitations
- The study focuses on questions whose answers should be independent of demographics, limiting generalizability to cases where demographic information legitimately affects medical recommendations.
- The experimental design cannot determine whether demographic answer changes represent harmful bias or potentially beneficial personalized adjustments.
- Results are based on USMLE-style vignettes that may not generalize to real-world clinical scenarios where demographic factors often legitimately affect diagnosis and treatment.

## Confidence
- High confidence: The experimental methodology for measuring demographic effects on QA systems is sound, and the observed answer changes (15-23%) are well-documented and reproducible.
- Medium confidence: The claim that generic models show more demographic sensitivity than biomedical-specific models is supported but requires further validation across different model architectures and training datasets.
- Medium confidence: The assertion that different demographic attributes affect the two systems differently is observed but the underlying reasons for these differences remain unclear.

## Next Checks
1. Conduct ablation studies removing the text encoding stage from the KG-grounded system to isolate whether demographic information enters through KG retrieval or text processing.
2. Test the same demographic enhancement methodology on real-world clinical QA datasets where some questions legitimately depend on demographics to distinguish harmful bias from appropriate personalization.
3. Implement explicit demographic debiasing techniques (such as demographic parity constraints or adversarial debiasing) and measure whether these reduce the observed answer changes without harming overall accuracy.