---
ver: rpa2
title: 'Exploring the effectiveness of ChatGPT-based feedback compared with teacher
  feedback and self-feedback: Evidence from Chinese to English translation'
arxiv_id: '2309.01645'
source_url: https://arxiv.org/abs/2309.01645
tags:
- feedback
- translation
- https
- chatgpt
- chatgpt-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the effectiveness of ChatGPT-based feedback,
  teacher feedback, and self-feedback on Chinese to English translation quality among
  advanced ESL/EFL learners. Translation texts were analyzed using BLEU scores for
  overall quality and Coh-Metrix for linguistic features across lexicon, syntax, and
  cohesion.
---

# Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation

## Quick Facts
- **arXiv ID:** 2309.01645
- **Source URL:** https://arxiv.org/abs/2309.01135
- **Reference count:** 14
- **Primary result:** ChatGPT-based feedback showed lexical advantages but was less effective than human feedback for overall translation quality and syntactic development.

## Executive Summary
This study compared three feedback methods—ChatGPT-based, teacher, and self-feedback—on Chinese-to-English translation quality among advanced ESL/EFL learners. Using BLEU scores and Coh-Metrix linguistic analysis across seven dimensions, the research found that human feedback (teacher and self) led to higher overall translation quality than ChatGPT-based feedback. While ChatGPT demonstrated superior performance in enhancing lexical capabilities and referential cohesion, it was less effective than human feedback in developing syntax-related skills, particularly passive voice usage. These findings suggest ChatGPT's potential as a supplementary tool rather than a replacement for traditional feedback methods in translation practice.

## Method Summary
The study involved 45 MTI students who translated a 424-character Chinese source text. Participants received three sequential feedback types with two-week intervals: self-feedback (SF), teacher feedback (TF), and ChatGPT-based feedback. Translation quality was assessed using BLEU scores compared against four reference translations by professional translators. Linguistic features were analyzed using seven Coh-Metrix indicators measuring lexicon, syntax, and cohesion dimensions. Confirmatory Factor Analysis (CFA) and Structural Equation Modeling (SEM) validated the measurement model and examined causal relationships between feedback types and linguistic factors.

## Key Results
- Teacher and self-feedback led to higher BLEU scores than ChatGPT-based feedback
- ChatGPT-based feedback was superior in enhancing lexical capabilities and referential cohesion
- ChatGPT-based feedback was less effective than teacher feedback in improving syntax-related skills, particularly passive voice usage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ChatGPT's large-scale training data gives it a lexical advantage over human feedback methods.
- **Mechanism:** Exposure to billions of text examples allows ChatGPT to suggest more specific and formal vocabulary choices during feedback.
- **Core assumption:** The model's vocabulary suggestions are contextually appropriate and not just statistically frequent.
- **Evidence anchors:**
  - [abstract] "ChatGPT-based feedback demonstrated superiority, particularly in enhancing lexical capability"
  - [section] "ChatGPT's extensive and diverse training data...equips the model with a vast lexical repertoire"
  - [corpus] Weak - no direct corpus evidence for vocabulary improvements
- **Break condition:** If the training data contains biased or incorrect vocabulary usage, the model's suggestions could be misleading or inappropriate for academic contexts.

### Mechanism 2
- **Claim:** Human feedback (TF and SF) provides better syntactic guidance than ChatGPT for translation tasks.
- **Mechanism:** Human instructors can analyze sentence structure contextually and understand genre-specific conventions like passive voice usage in formal news releases.
- **Core assumption:** Humans can recognize when passive voice is appropriate for the target genre, while ChatGPT cannot.
- **Evidence anchors:**
  - [abstract] "TF and SF proved more effective in developing syntax-related skills, as it addressed instances of incorrect usage of the passive voice"
  - [section] "ChatGPT lacks genre-specific feedback...fails to offer the kind of nuanced feedback that would help students understand when and why to use passive voice"
  - [corpus] Weak - no direct corpus evidence for syntactic improvements
- **Break condition:** If ChatGPT's training data included sufficient genre-specific examples with appropriate passive voice usage, it might improve its syntactic feedback capabilities.

### Mechanism 3
- **Claim:** Different feedback types impact specific linguistic dimensions differently rather than uniformly improving overall translation quality.
- **Mechanism:** Each feedback method has inherent strengths - ChatGPT excels at lexical refinement while human feedback better addresses syntactic complexity and genre conventions.
- **Core assumption:** The three feedback types (ChatGPT, TF, SF) have distinct capabilities that complement rather than compete with each other.
- **Evidence anchors:**
  - [abstract] "diverse outcomes indicate ChatGPT's potential as a supplementary resource, complementing traditional teacher-led methods"
  - [section] "ChatGPT-based feedback was not as effective as TF and SF methods in improving the overall quality...However, ChatGPT-based feedback...showed notable strengths in certain areas"
  - [corpus] Weak - no direct corpus evidence for dimension-specific improvements
- **Break condition:** If a unified feedback approach could address all linguistic dimensions equally well, the need for complementary feedback methods would diminish.

## Foundational Learning

- **Concept:** BLEU score calculation and interpretation
  - Why needed here: The study uses BLEU score as the primary metric for overall translation quality assessment
  - Quick check question: What does a 0.02 increase in BLEU score typically indicate in translation quality research?

- **Concept:** Coh-Metrix linguistic indicators
  - Why needed here: The study uses seven Coh-Metrix indicators to analyze lexical, syntactic, and cohesion dimensions
  - Quick check question: Which two Coh-Metrix indicators specifically measure lexical performance in this study?

- **Concept:** Confirmatory Factor Analysis (CFA) and Structural Equation Modeling (SEM)
  - Why needed here: These statistical methods validate the measurement model and examine causal relationships between feedback types and linguistic factors
  - Quick check question: What fit index threshold indicates an excellent model fit in CFA/SEM analysis?

## Architecture Onboarding

- **Component map:** Data collection → Feedback generation (SF, TF, ChatGPT) → BLEU scoring → Coh-Metrix analysis → CFA/SEM statistical modeling
- **Critical path:** Student submits initial translation → Student provides self-feedback and revises → Teacher provides feedback and student revises → ChatGPT generates feedback and student revises → All versions analyzed using BLEU and Coh-Metrix → Statistical analysis via CFA/SEM
- **Design tradeoffs:** Single source text limits generalizability but ensures consistency; advanced MTI students provide sophisticated baseline but may limit observable improvements; two-week intervals prevent carryover effects but extend study duration; automated Coh-Metrix analysis enables scalability but may miss nuanced linguistic features
- **Failure signatures:** Inconsistent feedback quality across student submissions; BLEU score improvements below 0.02 threshold; CFA/SEM model fit indices outside acceptable ranges; significant learning effects between feedback sessions
- **First 3 experiments:** 1) Replicate with different source text genres (technical, literary, conversational); 2) Test with intermediate-level ESL/EFL learners instead of advanced MTI students; 3) Compare ChatGPT versions (GPT-3.5 vs GPT-4) for feedback quality differences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the specific syntactic rules and patterns that ChatGPT fails to identify or provide feedback on, and how do these compare to the feedback provided by human teachers?
- **Basis in paper:** [explicit] The paper discusses that ChatGPT lacks the ability to deeply analyze or comprehend syntax rules, leading to generic and superficial feedback.
- **Why unresolved:** The paper does not provide a detailed comparison of the specific syntactic rules and patterns that ChatGPT fails to address versus those identified by human teachers.
- **What evidence would resolve it:** A detailed analysis of specific syntactic errors in student translations, comparing ChatGPT's feedback with human teacher feedback, would provide evidence of the differences in their effectiveness.

### Open Question 2
- **Question:** How does the inconsistency in ChatGPT's feedback across different student translations affect the overall learning outcomes and translation quality improvements?
- **Basis in paper:** [explicit] The paper notes that ChatGPT's feedback is inconsistent, sometimes identifying issues and sometimes not, across different student translations.
- **Why unresolved:** The paper does not explore the impact of this inconsistency on students' learning and translation quality.
- **What evidence would resolve it:** Longitudinal studies tracking student performance and translation quality over time, with and without consistent ChatGPT feedback, would provide evidence of the impact of feedback inconsistency.

### Open Question 3
- **Question:** What specific features of ChatGPT's training data contribute to its strengths in lexical capabilities and referential cohesion, and how can these be leveraged to improve its performance in other linguistic dimensions?
- **Basis in paper:** [inferred] The paper suggests that ChatGPT's extensive and diverse training data contributes to its strengths in lexical capabilities and referential cohesion.
- **Why unresolved:** The paper does not identify the specific features of ChatGPT's training data that contribute to its strengths in these areas.
- **What evidence would resolve it:** A detailed analysis of ChatGPT's training data, focusing on the types of texts and linguistic features it was trained on, would provide insights into its strengths and potential areas for improvement.

## Limitations
- Single source text and homogeneous sample of advanced MTI students limit generalizability
- Paper does not provide detailed information about the exact ChatGPT prompt used, affecting reproducibility
- BLEU score improvements observed (0.02 increase) fall within range that may not be practically significant

## Confidence

- **High Confidence:** ChatGPT's superiority in enhancing lexical capabilities and referential cohesion, as evidenced by consistent improvements across multiple Coh-Metrix indicators and supported by the model's extensive training data.
- **Medium Confidence:** Human feedback's advantage in syntactic development, particularly for passive voice usage, though this conclusion is primarily based on theoretical reasoning about genre conventions rather than direct empirical evidence.
- **Low Confidence:** The practical significance of the observed BLEU score improvements, given that a 0.02 increase is at the lower threshold of what is typically considered meaningful in translation research.

## Next Checks
1. Replicate the study with multiple source texts from different genres (technical, literary, conversational) to assess feedback effectiveness across varied translation contexts
2. Test the feedback methods with intermediate-level ESL/EFL learners to determine if the observed patterns hold across different proficiency levels
3. Conduct a controlled experiment comparing ChatGPT versions (GPT-3.5 vs GPT-4) to identify whether newer model versions provide more effective feedback for translation tasks