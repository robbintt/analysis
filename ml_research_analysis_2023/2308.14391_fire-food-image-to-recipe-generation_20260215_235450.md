---
ver: rpa2
title: 'FIRE: Food Image to REcipe generation'
arxiv_id: '2308.14391'
source_url: https://arxiv.org/abs/2308.14391
tags:
- recipe
- food
- ingredients
- cooking
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FIRE, a multimodal methodology for recipe generation
  from food images. It generates food titles, ingredients, and cooking instructions
  by leveraging BLIP for title generation, a Vision Transformer with decoder for ingredient
  extraction, and T5 for cooking instruction generation.
---

# FIRE: Food Image to REcipe generation

## Quick Facts
- arXiv ID: 2308.14391
- Source URL: https://arxiv.org/abs/2308.14391
- Reference count: 40
- Key outcome: FIRE achieves 6.02 SacreBLEU and 21.29 ROUGE-L on Recipe1M for end-to-end recipe generation

## Executive Summary
FIRE is a multimodal pipeline that generates complete recipes (title, ingredients, instructions) from food images. The system uses BLIP for title generation, a Vision Transformer with decoder for ingredient extraction, and T5 for cooking instructions. FIRE demonstrates state-of-the-art performance on Recipe1M, achieving 6.02 SacreBLEU and 21.29 ROUGE-L for end-to-end recipe generation, with significant improvements in ingredient extraction accuracy (F1-score 41.44%, IoU 32.03%).

## Method Summary
FIRE processes food images through a modular pipeline: images first pass through BLIP (fine-tuned) to generate recipe titles, then through a Vision Transformer encoder-decoder architecture to extract ingredients (with cardinality loss to constrain ingredient count), and finally through T5 (fine-tuned) to generate cooking instructions from the title and ingredients. The model was trained on a 10% subset of Recipe1M due to computational constraints, using SacreBLEU and ROUGE-L for end-to-end evaluation, and F1-score, IoU, and Jaccard similarity for ingredient extraction.

## Key Results
- End-to-end recipe generation: 6.02 SacreBLEU and 21.29 ROUGE-L on Recipe1M
- Ingredient extraction: F1-score of 41.44%, IoU of 32.03%, Jaccard similarity of 24.20%
- FIRE outperforms state-of-the-art methods like Transformer, LLaVA, and BLIP-based approaches
- Applications include recipe customization through LLM prompting and recipe-to-code generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FIRE's ingredient extraction benefits from ViT's superior feature representation compared to CNN-based extractors.
- Mechanism: ViT's self-attention mechanism captures long-range dependencies and local semantic details in food images, which are crucial for distinguishing overlapping or visually similar ingredients.
- Core assumption: The local semantic segmentation capability of ViT is more effective than traditional CNNs for ingredient-level feature extraction.
- Evidence anchors: [section] "ViT's attention mechanism enables effective handling of feature representations with stable and notably high resolution..." and "We attribute this improvement to FIRE's superior feature extraction capability that uses ViT rather than ResNet50."

### Mechanism 2
- Claim: FIRE's modular pipeline (title, ingredients, instructions) outperforms end-to-end models by decomposing the problem.
- Mechanism: Separate specialized models (BLIP for titles, ViT+decoder for ingredients, T5 for instructions) allow each component to focus on its domain, reducing error propagation and enabling targeted fine-tuning.
- Core assumption: Decomposing the recipe generation task into modular subtasks improves overall quality compared to a single monolithic model.
- Evidence anchors: [abstract] "FIRE leverages the BLIP model to generate titles, utilizes a Vision Transformer with a decoder for ingredient extraction, and employs the T5 model to generate recipes..."

### Mechanism 3
- Claim: FIRE's use of cardinality loss (losscard) improves ingredient extraction by enforcing correct set cardinality.
- Mechanism: The cardinality loss constrains the predicted ingredient count to match the ground truth, preventing over-generation or under-generation of ingredients.
- Core assumption: The number of ingredients in a recipe is a strong prior that can guide the model to generate more accurate ingredient sets.
- Evidence anchors: [section] "Furthermore, to enhance performance, we incorporate a cardinality L1 penalty (losscard), which constrains the length of the predicted ingredients to be close to the ground truth ingredients..."

## Foundational Learning

- Concept: Vision Transformers (ViT)
  - Why needed here: ViT's self-attention mechanism is crucial for capturing both local and global features in food images, which is essential for ingredient extraction where ingredients can be small and overlapping.
  - Quick check question: What is the key difference between ViT and CNN-based feature extractors in handling spatial relationships?

- Concept: Encoder-decoder architectures with attention
  - Why needed here: The ingredient decoder uses self-attention and conditional attention to sequentially predict ingredients while considering the image context and previously predicted ingredients.
  - Quick check question: How does the conditional attention mechanism in the ingredient decoder help with ingredient extraction?

- Concept: Few-shot prompting with large language models
  - Why needed here: FIRE leverages few-shot prompting to extend its capabilities to recipe customization and recipe-to-code generation, demonstrating the flexibility of combining specialized models with general-purpose LLMs.
  - Quick check question: What is the main advantage of using few-shot prompting over fine-tuning for downstream applications like recipe customization?

## Architecture Onboarding

- Component map: Image → BLIP (fine-tuned) → Recipe Title → T5 (fine-tuned) → Cooking Instructions; Image → ViT → Ingredient Decoder (with losscard) → Ingredient Set → T5 (fine-tuned) → Cooking Instructions

- Critical path: Image → ViT → Ingredient Decoder → T5 → Cooking Instructions (end-to-end recipe generation)

- Design tradeoffs:
  - Modular design vs. end-to-end: FIRE chooses modularity for specialized optimization but introduces potential error propagation
  - Fine-tuning vs. zero-shot: FIRE fine-tunes BLIP and T5 for domain alignment but relies on few-shot prompting for advanced applications to save resources
  - Feature extractor choice: ViT is chosen over CNN for better semantic feature extraction, but at higher computational cost

- Failure signatures:
  - Ingredient extraction fails: Check ViT output quality and decoder loss balance (losscard vs. binary cross-entropy)
  - Recipe generation lacks coherence: Verify title generation quality and T5 input formatting
  - Code generation hallucinations: Inspect few-shot prompt design and LLM context length limits

- First 3 experiments:
  1. Validate ViT vs. ResNet50 on ingredient extraction IoU/F1 on a small subset
  2. Test T5 with ground truth title+ingredients vs. FIRE-generated ones on SacreBLEU/ROUGE-L
  3. Run few-shot prompting with 2-3 demonstrations for recipe customization and measure human evaluation scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a reliable grounding mechanism be developed to verify the correctness of generated recipes beyond conventional text similarity metrics?
- Basis in paper: [explicit] "A major limitation of both the proposed work and existing approaches lies in the absence of a reliable grounding mechanism [16] to ascertain the correctness of generated recipes. Conventional metrics are insufficient to capture this challenge."
- Why unresolved: The paper acknowledges that existing evaluation metrics like SacreBLEU and ROUGE fail to capture the practical accuracy of recipes and detect hallucinations, which can significantly impact the final outcome of a dish.
- What evidence would resolve it: A new evaluation metric that effectively measures the coherence, plausibility, and practical feasibility of generated recipes, potentially incorporating human judgment or automated testing of recipe steps.

### Open Question 2
- Question: How can knowledge graphs be integrated into the FIRE model to handle recipe diversity based on locations, climates, and religions?
- Basis in paper: [explicit] "The diversity and availability of recipes are heavily dependent on the locations, climates, and religions [30, 44, 62], which prevent users from preparing food based on predefined recipes. One solution can be the injection of knowledge graphs [12, 43]..."
- Why unresolved: The paper identifies this as a future research direction but does not implement or evaluate any knowledge graph integration approach.
- What evidence would resolve it: Experimental results comparing FIRE's performance with and without knowledge graph integration on diverse regional recipe datasets, demonstrating improved generalization across different cultural contexts.

### Open Question 3
- Question: What methods can be implemented to reduce hallucination in recipe generation, particularly for state tracking of ingredients and cooking tools?
- Basis in paper: [explicit] "Hallucination remains a critical challenge in recipe generation by natural language and vision models. We will investigate the possibility of incorporating methods for state tracking of participants [14,24] to enhance the production of reasonable and accurate results."
- Why unresolved: While the paper acknowledges hallucination as a critical challenge and proposes investigating state tracking methods, it does not implement or evaluate such methods in the current work.
- What evidence would resolve it: Comparative results showing reduced hallucination rates in recipe generation when state tracking methods are implemented versus the current FIRE approach, measured through both automated metrics and human evaluation.

## Limitations

- The evaluation relies entirely on Recipe1M, a single dataset, raising questions about generalization to other recipe datasets or cultural cuisines
- Computational constraints limited training to 10% of Recipe1M, which may not represent full-scale performance
- The paper doesn't thoroughly analyze error propagation between modular components or provide runtime/memory comparisons for the ViT-based approach

## Confidence

- **High Confidence**: Core architectural claims (ViT for ingredient extraction, modular pipeline design, T5 for instruction generation) are well-supported by quantitative metrics on Recipe1M
- **Medium Confidence**: Claims about superiority over baselines (InverseCooking, Transformer, etc.) are supported but based on a single dataset
- **Low Confidence**: Claims about the effectiveness of cardinality loss and the generalization of few-shot prompting applications lack extensive ablation studies

## Next Checks

1. **Ablation Study on Cardinality Loss**: Remove losscard from the ingredient extraction model and measure the impact on ingredient count accuracy and overall F1/IoU scores to validate its contribution

2. **Cross-Dataset Evaluation**: Test FIRE on a different recipe dataset (e.g., Food.com or a manually curated test set) to assess generalization beyond Recipe1M

3. **Error Propagation Analysis**: Generate synthetic errors in the ingredient extraction component and measure their impact on the final cooking instruction quality to quantify modular pipeline robustness