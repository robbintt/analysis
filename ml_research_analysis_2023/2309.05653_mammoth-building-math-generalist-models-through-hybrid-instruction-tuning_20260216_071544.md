---
ver: rpa2
title: 'MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning'
arxiv_id: '2309.05653'
source_url: https://arxiv.org/abs/2309.05653
tags:
- math
- arxiv
- https
- preprint
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAmmoTH, a series of open-source large language
  models (LLMs) tailored for mathematical reasoning. MAmmoTH models are trained on
  MathInstruct, a dataset combining chain-of-thought (CoT) and program-of-thought
  (PoT) rationales across diverse math fields.
---

# MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning

## Quick Facts
- arXiv ID: 2309.05653
- Source URL: https://arxiv.org/abs/2309.05653
- Reference count: 30
- Key outcome: MAmmoTH achieves 35% accuracy on MATH dataset, surpassing best open-source 7B model by 25%, and 44% accuracy on 34B variant, exceeding GPT-4's CoT result

## Executive Summary
This paper introduces MAmmoTH, a series of open-source large language models (LLMs) tailored for mathematical reasoning. MAmmoTH models are trained on MathInstruct, a dataset combining chain-of-thought (CoT) and program-of-thought (PoT) rationales across diverse math fields. This hybrid approach allows leveraging the strengths of both reasoning styles, enabling tool use and handling a wide range of problem types. MAmmoTH models substantially outperform existing open-source models on nine mathematical reasoning datasets, with average accuracy gains between 16% and 32%. Notably, MAmmoTH-7B achieves 35% accuracy on the MATH dataset, exceeding the best open-source 7B model by 25%, and MAmmoTH-34B achieves 44% accuracy, surpassing GPT-4's CoT result. The results demonstrate the importance of diverse problem coverage and hybrid rationales in developing superior math generalist models.

## Method Summary
MAmmoTH employs instruction tuning on a hybrid dataset (MathInstruct) that combines chain-of-thought (CoT) and program-of-thought (PoT) rationales. The approach leverages Llama and Code Llama base models (7B-70B parameters) fine-tuned on 260K math instruction samples spanning diverse fields and complexity levels. Training uses DeepSpeed with ZeRO-3 for efficiency, with learning rates of 2e-5 (7B/13B) or 1e-5 (34B/70B) for three epochs. The hybrid approach allows offloading abstract reasoning to CoT and precise computation to PoT, improving generalization across mathematical domains.

## Key Results
- MAmmoTH-7B achieves 35% accuracy on MATH dataset, surpassing best open-source 7B model by 25%
- MAmmoTH-34B achieves 44% accuracy, outperforming GPT-4's CoT result
- Average accuracy gains of 16-32% across nine mathematical reasoning datasets
- Superior generalization to out-of-domain datasets compared to dataset-specific fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hybrid CoT and PoT rationales enable MAmmoTH to handle both abstract reasoning and precise computation by offloading different cognitive loads to the appropriate reasoning style.
- **Mechanism:** CoT handles abstract reasoning (e.g., formal logic, common-sense reasoning) where no built-in APIs exist, while PoT offloads computation to Python interpreter for precise mathematical operations (e.g., solving quadratic equations, matrix operations).
- **Core assumption:** Different mathematical problems require different reasoning approaches, and forcing a single reasoning style limits model performance.
- **Evidence anchors:**
  - [abstract]: "The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems."
  - [section]: "Contrary to previous work (Yuan et al., 2023; Luo et al., 2023; Lee et al., 2023; Wang et al., 2023f) that focus on CoT, our dataset strategically combines both."
- **Break Condition:** If the model cannot effectively switch between CoT and PoT reasoning styles, or if one style consistently underperforms, the hybrid approach may not yield the expected benefits.

### Mechanism 2
- **Claim:** Broad coverage of diverse math fields and complexity levels improves generalization by exposing the model to varied mathematical concepts and problem types.
- **Mechanism:** By training on datasets spanning different fields (arithmetic, algebra, probability, calculus, geometry) and complexity levels (elementary to college), the model learns to recognize and apply appropriate reasoning strategies across diverse scenarios.
- **Core assumption:** Mathematical reasoning requires domain-specific knowledge and the ability to adapt to different problem contexts.
- **Evidence anchors:**
  - [abstract]: "It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math."
  - [section]: "MathInstruct is based on seven existing math rationale datasets and six newly-curated datasets... encompassing a wide range of core mathematical fields."
- **Break Condition:** If the model overfits to specific problem types or fields, or if the diversity of training data is insufficient to cover the breadth of mathematical reasoning required in evaluation.

### Mechanism 3
- **Claim:** Dataset-specific fine-tuning improves in-domain performance but hurts out-of-domain generalization, while diverse data sources improve both.
- **Mechanism:** Training on a narrow set of datasets (e.g., GSM8K and MATH) allows the model to excel on similar problems but fails to generalize to new domains. Diverse data sources expose the model to a wider range of problem types and reasoning strategies, improving its ability to handle unfamiliar scenarios.
- **Core assumption:** Generalization in mathematical reasoning requires exposure to diverse problem types and reasoning strategies, not just mastery of specific datasets.
- **Evidence anchors:**
  - [section]: "Current efforts to bridge this gap are twofold: (1) Continued pre-training... (2) Dataset-specific fine-tuning... Although such approaches improve in-domain performance, they cannot generalize to a wider range of math reasoning tasks beyond their fine-tuning data."
  - [section]: "Compared with existing methods, our models generalize better to OOD datasets and substantially improve the performance of open-source LLMs in mathematical reasoning."
- **Break Condition:** If the diverse data sources are of low quality or if the model fails to learn the underlying reasoning patterns across different domains, the generalization benefits may not materialize.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) reasoning
  - **Why needed here:** CoT is essential for handling abstract mathematical reasoning where no built-in APIs exist, such as formal logic, common-sense reasoning, and abstract algebra.
  - **Quick check question:** Can you explain how CoT differs from direct answer generation and why it's particularly useful for complex mathematical problems?

- **Concept:** Program-of-Thought (PoT) reasoning
  - **Why needed here:** PoT leverages external tools (e.g., Python interpreter) to offload computation, improving precision in solving complex mathematical problems like solving quadratic equations or calculating matrix eigenvalues.
  - **Quick check question:** How does PoT simplify the solution process for problems requiring precise computation, and what are its limitations in handling abstract reasoning?

- **Concept:** Instruction tuning
  - **Why needed here:** Instruction tuning aligns the model with human instructions, enabling it to follow diverse problem-solving strategies and generalize to new mathematical domains.
  - **Quick check question:** What is the role of instruction tuning in activating the model's capabilities for specialized domains like mathematics, and how does it differ from traditional fine-tuning?

## Architecture Onboarding

- **Component map:**
  Base model (Llama/Code Llama) -> MathInstruct dataset (260K hybrid CoT/PoT samples) -> Instruction tuning (Huggingface, DeepSpeed) -> Evaluation on nine math datasets

- **Critical path:**
  1. Data curation: Compile and validate diverse math datasets with hybrid rationales
  2. Model fine-tuning: Train MAmmoTH models on MathInstruct using instruction tuning
  3. Evaluation: Assess model performance on in-domain and out-of-domain datasets
  4. Iteration: Refine data sources and training strategies based on evaluation results

- **Design tradeoffs:**
  - CoT vs. PoT: Balancing the use of natural language reasoning (CoT) and code-based computation (PoT) to maximize performance across diverse problem types
  - Dataset diversity vs. specificity: Prioritizing broad coverage of math fields over mastery of specific datasets to improve generalization
  - Model scale vs. efficiency: Training larger models (e.g., 70B) for better performance while managing computational costs

- **Failure signatures:**
  - Poor generalization: Model performs well on in-domain datasets but struggles with out-of-domain problems
  - Inconsistent reasoning: Model fails to switch effectively between CoT and PoT reasoning styles
  - Low precision: Model struggles with precise computation in PoT reasoning or abstract reasoning in CoT

- **First 3 experiments:**
  1. Ablation study: Compare performance of MAmmoTH trained on CoT-only, PoT-only, and hybrid datasets to assess the impact of each reasoning style
  2. Dataset contribution analysis: Gradually add major subsets (GSM8K, MATH, Camel, AQuA) to training and evaluate performance to understand their individual contributions
  3. Base model comparison: Fine-tune MAmmoTH on both Llama-2 and Code-Llama base models to assess the impact of pre-training on code-related tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MAmmoTH models scale with increasing model size beyond 70B parameters?
- Basis in paper: [inferred] The paper evaluates models up to 70B parameters and notes that MAmmoTH-Coder-34B performs comparably to MAmmoTH-70B on out-of-domain datasets. This suggests potential for further scaling.
- Why unresolved: The paper does not report results for models larger than 70B parameters, leaving the scalability of the approach an open question.
- What evidence would resolve it: Training and evaluating MAmmoTH models with parameter counts exceeding 70B, particularly focusing on out-of-domain performance to assess generalization capabilities at larger scales.

### Open Question 2
- Question: What is the impact of different fine-tuning strategies (e.g., supervised fine-tuning vs. reinforcement learning) on the mathematical reasoning capabilities of MAmmoTH models?
- Basis in paper: [explicit] The paper primarily focuses on instruction tuning with a diverse dataset. It mentions the potential for future work to explore synthetic data intervention methods and improve reasoning robustness.
- Why unresolved: The paper does not explore alternative fine-tuning strategies beyond instruction tuning, leaving their potential impact on mathematical reasoning unexplored.
- What evidence would resolve it: Conducting experiments comparing the performance of MAmmoTH models fine-tuned using different strategies (e.g., supervised fine-tuning, reinforcement learning) on various mathematical reasoning benchmarks.

### Open Question 3
- Question: How does the hybrid approach of combining CoT and PoT rationales perform on mathematical problems that require both symbolic manipulation and logical reasoning?
- Basis in paper: [explicit] The paper highlights the benefits of the hybrid approach in leveraging the strengths of both CoT and PoT for different types of mathematical problems. It also provides examples where CoT excels in formal logic questions.
- Why unresolved: While the paper demonstrates the effectiveness of the hybrid approach on various mathematical problems, it does not specifically investigate its performance on problems requiring both symbolic manipulation and logical reasoning.
- What evidence would resolve it: Designing and evaluating mathematical problems that necessitate both symbolic manipulation and logical reasoning, then comparing the performance of MAmmoTH models using the hybrid approach versus models relying solely on CoT or PoT.

## Limitations

- Limited evaluation scope: Performance is evaluated on benchmark datasets, not real-world mathematical problems or specialized domains like physics or engineering.
- Generalization uncertainty: While the model shows strong performance on in-domain and out-of-domain datasets, its ability to generalize to completely novel problem types remains untested.
- Computational resource requirements: Training larger models (34B, 70B) requires significant computational resources, with no detailed analysis of training costs or inference efficiency.

## Confidence

- High Confidence: The hybrid CoT and PoT approach and its impact on model performance are well-supported by the experimental results.
- Medium Confidence: Claims regarding improved generalization and performance on out-of-domain datasets are supported by evaluation results, but the extent of generalization to truly novel problem types remains uncertain.
- Low Confidence: Practical implications and real-world applicability of the model's improvements are not thoroughly explored, with no insights into performance in real-world scenarios or resource-constrained environments.

## Next Checks

1. **Real-world Problem Solving:** Test MAmmoTH on a diverse set of real-world mathematical problems, including those from physics, engineering, and finance, to assess its practical applicability and generalizability beyond benchmark datasets.

2. **Novel Problem Generation:** Create a set of novel mathematical problems that are not present in any existing datasets and evaluate MAmmoTH's ability to solve them. This will provide insights into the model's adaptability and generalization capabilities.

3. **Efficiency and Resource Analysis:** Conduct a comprehensive analysis of the computational resources required for training and inference, and compare MAmmoTH's efficiency with other state-of-the-art models. This will help determine the practical feasibility of deploying MAmmoTH in real-world applications.