---
ver: rpa2
title: Deep Non-Parametric Time Series Forecaster
arxiv_id: '2312.14657'
source_url: https://arxiv.org/abs/2312.14657
tags:
- time
- series
- forecasting
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a non-parametric approach to probabilistic
  time series forecasting, presenting two variants: NPTS (local) and DeepNPTS (global).
  Unlike traditional parametric models, these methods generate predictions by sampling
  from the empirical distribution of past observations, avoiding numerical instability
  issues.'
---

# Deep Non-Parametric Time Series Forecaster

## Quick Facts
- arXiv ID: 2312.14657
- Source URL: https://arxiv.org/abs/2312.14657
- Reference count: 40
- Primary result: Non-parametric time series forecasting method that samples from empirical distribution, achieving competitive performance on non-Gaussian data where parametric models struggle.

## Executive Summary
This paper introduces NPTS and DeepNPTS, non-parametric approaches to probabilistic time series forecasting that sample from the empirical distribution of past observations. Unlike traditional parametric models, these methods avoid numerical instability by constraining predictions to the observed data range. DeepNPTS extends NPTS by learning sampling probabilities via a shared neural network across multiple related time series. The methods are robust, calibrated, and particularly effective on non-Gaussian data like integer counts and rates.

## Method Summary
The paper presents two non-parametric forecasting methods: NPTS (local) and DeepNPTS (global). Both sample from the empirical distribution of past observations to generate probabilistic forecasts. NPTS uses either exponential or uniform kernels to weight past observations, while DeepNPTS learns sampling probabilities via a shared neural network that exploits information across multiple related time series. The models are trained using Ranked Probability Score (RPS), a proper scoring rule that encourages calibrated probabilistic predictions. Forecasts are generated by sampling K times from the learned distribution, providing full predictive distributions rather than point estimates.

## Key Results
- DeepNPTS achieves competitive performance across diverse datasets, particularly excelling on non-Gaussian data where parametric baselines fail
- Both NPTS and DeepNPTS produce highly calibrated forecasts with coverage close to nominal quantiles
- The methods serve as reliable fail-safe fallbacks in production settings due to their numerical stability
- DeepNPTS outperforms or matches state-of-the-art parametric models while avoiding numerical instability issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model avoids numerical instability by always sampling from observed values within the data range.
- Mechanism: By constraining predictions to reweight the empirical distribution of past observations, the model never generates values outside the support of the training data, preventing issues like overflow or invalid parameter estimates.
- Core assumption: The relevant future behavior is well-represented by the empirical distribution of the observed past values.
- Evidence anchors:
  - [abstract]: "predictions within the observed data range" and "without fail unlike classical models that suffer from numerical stability"
  - [section]: "our forecast distribution for time step T can be seen as sampling from the discrete random variable with the probability mass function given by"
- Break condition: If the underlying data-generating process changes dramatically (e.g., regime shift), the empirical distribution may no longer be representative, leading to poor generalization.

### Mechanism 2
- Claim: Global learning improves performance by sharing information across related time series to learn a better sampling strategy.
- Mechanism: A shared neural network maps time series features and recent observations to sampling probabilities, allowing patterns common across series to inform the sampling distribution.
- Core assumption: Related time series share informative patterns that can be exploited by a common model.
- Evidence anchors:
  - [abstract]: "learns the sampling strategy by exploiting the information across multiple related time series"
  - [section]: "The network outputs different sampling probabilities for each time series i, depending on its features. However, these different sampling probabilities are parametrized by a single set of common parameters, facilitating information sharing"
- Break condition: If the time series are not sufficiently related, the shared model may introduce noise and degrade performance compared to local methods.

### Mechanism 3
- Claim: The ranked probability score (RPS) provides a proper scoring rule that encourages calibrated probabilistic forecasts.
- Mechanism: RPS sums quantile losses across all distinct past observations, penalizing overconfident or miscalibrated predictions more heavily than point-wise errors.
- Core assumption: Proper scoring rules lead to better-calibrated probabilistic predictions than heuristic loss functions.
- Evidence anchors:
  - [section]: "RPS is a discrete version of the continuous ranked probability score (CRPS) [26]. Note that both RPS and CRPS are proper scoring rules for evaluating how likely the value observed is in fact generated from the given distribution"
  - [section]: "Figure 2 displays coverage for the Traffic dataset, together with the calibration error...forecasts from both the NPTS and DeepNPTS models are highly calibrated"
- Break condition: If the number of distinct past values is very small, RPS may become unstable or overly sensitive to individual observations.

## Foundational Learning

- Concept: Non-parametric vs parametric modeling
  - Why needed here: Understanding why sampling from empirical distributions avoids the numerical issues of parametric models (e.g., Gaussian assumptions) is key to grasping the method's robustness.
  - Quick check question: What could go wrong if we assumed a Gaussian distribution for integer count data?

- Concept: Ranked Probability Score (RPS) and proper scoring rules
  - Why needed here: RPS is the loss function used to train DeepNPTS; knowing how it differs from standard regression losses explains the model's calibration behavior.
  - Quick check question: How does RPS penalize overconfident predictions compared to mean squared error?

- Concept: Monte Carlo sampling for probabilistic forecasting
  - Why needed here: The model generates forecast distributions by sampling K times from the learned sampling probabilities; understanding this process is crucial for interpreting results.
  - Quick check question: Why does sampling K times give us a distribution rather than a single prediction?

## Architecture Onboarding

- Component map:
  - Input: Recent context window (observations and covariates)
  - Model: Feed-forward neural network (shared weights)
  - Output: Sampling probabilities over past indices
  - Sampling: K Monte Carlo draws from probabilities → forecast distribution

- Critical path:
  1. Prepare context window and covariates
  2. Forward pass through shared MLP
  3. Apply softmax to get probabilities
  4. Sample K times → forecast distribution
  5. Evaluate via RPS

- Design tradeoffs:
  - Local vs global: Local NPTS is robust and fast but doesn't share information; DeepNPTS can learn better sampling strategies but requires more data and training.
  - Context window size: Larger windows capture more history but increase computation and risk overfitting; smaller windows are faster but may miss long-term patterns.
  - Number of samples K: More samples give smoother distributions but increase compute; fewer samples are faster but noisier.

- Failure signatures:
  - Poor calibration: Coverage far from nominal quantiles
  - Overfitting: Validation loss increases while training loss decreases
  - Numerical issues: Sampling probabilities collapse to near-zero or one

- First 3 experiments:
  1. Train DeepNPTS with default hyperparameters on a small dataset (e.g., Exchange Rate) and check calibration plots.
  2. Compare NPTS vs DeepNPTS on a dataset with clear seasonality (e.g., Electricity) and visualize sampling probabilities.
  3. Evaluate robustness by introducing outliers into the training data and observing if forecasts remain within observed range.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NPTS and DeepNPTS be adapted to handle multivariate time series forecasting?
- Basis in paper: [inferred] The paper focuses exclusively on univariate time series forecasting, but many real-world applications require multivariate forecasting. The current approach samples from the empirical distribution of past observations, which could potentially be extended to capture cross-series dependencies.
- Why unresolved: The paper does not explore or discuss the extension of these methods to multivariate time series, leaving open questions about how to handle inter-series relationships and correlations.
- What evidence would resolve it: Experiments demonstrating the performance of adapted NPTS or DeepNPTS models on multivariate datasets, comparing them to existing multivariate forecasting methods.

### Open Question 2
- Question: What is the optimal way to learn the feature map f(t) in seasonal NPTS for capturing complex seasonality patterns?
- Basis in paper: [explicit] The paper mentions that the feature map f(t) can be learned from the data but does not provide details on how to do this or evaluate different approaches for learning it.
- Why unresolved: The paper only suggests the possibility of learning f(t) but does not explore or compare different methods for learning this feature map, leaving uncertainty about the best approach.
- What evidence would resolve it: Empirical comparisons of different feature learning approaches (e.g., learned vs. handcrafted features) on datasets with complex seasonality, showing which approach yields better forecasting performance.

### Open Question 3
- Question: How do NPTS and DeepNPTS perform in online/streaming settings where data arrives continuously and models need to adapt in real-time?
- Basis in paper: [inferred] The paper evaluates the methods in a rolling-window fashion but does not specifically address the challenges of online/streaming scenarios where models must continuously update with new data.
- Why unresolved: The paper's evaluation setup assumes fixed training and test periods, not reflecting the dynamic nature of online forecasting where data distributions may shift over time.
- What evidence would resolve it: Experiments comparing the performance of NPTS/DeepNPTS in online/streaming settings versus batch learning, measuring adaptation speed and robustness to concept drift.

## Limitations
- Core limitation is the empirical distribution assumption - may struggle with regime shifts where past values no longer represent future behavior
- Unknown neural network architecture for DeepNPTS creates uncertainty in reproducing claimed performance gains
- Focus on univariate forecasting limits applicability to multivariate real-world scenarios

## Confidence
- High confidence: The mechanism avoiding numerical instability by sampling within observed ranges is well-supported and straightforward to verify
- Medium confidence: The global learning benefits and calibration improvements are demonstrated empirically but depend heavily on the unknown neural architecture details
- Low confidence: Claims about DeepNPTS performance on non-Gaussian data are plausible but require exact replication to confirm, given the architectural unknowns

## Next Checks
1. **Architecture verification**: Implement the exact neural network architecture (layers, units, activations) used in DeepNPTS and compare calibration and RPS scores on the Electricity dataset.

2. **Robustness test**: Systematically introduce outliers and regime shifts into benchmark datasets to quantify how quickly NPTS/DeepNPTS degrade compared to parametric baselines.

3. **Parametric comparison**: Re-run experiments with Gaussian-approximating parametric models on integer-count datasets (Taxi, Wiki) to verify the claimed superiority of the non-parametric approach.