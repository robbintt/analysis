---
ver: rpa2
title: Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality
  and Missing Modality Scenarios in Semantic Segmentation
arxiv_id: '2312.02240'
source_url: https://arxiv.org/abs/2312.02240
tags:
- csk-net
- modality
- distillation
- segmentation
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of semantic segmentation in multimodal
  settings, specifically using optical and infrared images. The core method idea is
  a novel approach called CSK-Net, which uses a contrastive learning-based spectral
  knowledge distillation technique along with an automatic mixed feature exchange
  mechanism.
---

# Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality and Missing Modality Scenarios in Semantic Segmentation

## Quick Facts
- arXiv ID: 2312.02240
- Source URL: https://arxiv.org/abs/2312.02240
- Authors: 
- Reference count: 38
- Primary result: CSK-Net surpasses state-of-the-art models in multi-modal tasks and for missing modalities when exclusively utilizing IR data for inference across three public benchmarking datasets without additional computational costs.

## Executive Summary
This paper addresses semantic segmentation in multimodal settings using optical and infrared images. The proposed CSK-Net framework introduces a novel contrastive learning-based spectral knowledge distillation technique combined with an automatic mixed feature exchange mechanism. The approach extracts detailed textures from optical images and distills them into the optical branch while preserving modality-specific information through shared convolutional weights with separate batch normalization layers. CSK-Net demonstrates superior performance in both multi-modal and missing modality scenarios across three public datasets.

## Method Summary
CSK-Net employs shared convolutional weights with separate batch normalization layers for optical-infrared image pairs to capture multi-spectral information. The model uses a Gated Spectral Unit and mixed feature exchange strategy to increase correlation of modality-shared information while decreasing modality-specific information during knowledge distillation. Pixel-wise contrastive learning preserves intra-class compactness and modality-specific style information. The approach involves pre-training a DeepLabV3+ model on optical images, then training CSK-Net on co-registered EO-IR pairs while distilling knowledge from the pre-trained model.

## Key Results
- CSK-Net surpasses state-of-the-art models in multi-modal tasks
- Maintains superior performance for missing modality scenarios using exclusively IR data for inference
- Achieves performance gains without additional computational costs compared to baseline segmentation models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared convolutional weights with separate batch norm layers enable CSK-Net to retain modality-specific information while sharing modality-invariant features.
- Mechanism: Shared convolutional weights learn common features across EO and IR modalities while separate batch norm layers preserve modality-specific statistics.
- Core assumption: Modality-invariant features are primarily captured in convolutional layers, while modality-specific information is encoded in batch norm statistics.
- Evidence anchors: Abstract mentions shared convolution weights with separate BN layers; section 2.5 explains shallow layers retain style-related information.

### Mechanism 2
- Claim: Mixed feature exchange strategy preserves modality-specific information during knowledge distillation without sparsity constraints.
- Mechanism: Channel exchange swaps features based on BN scaling factors without sparsity constraints, while spatial exchange swaps features along width dimension in later encoder stages.
- Core assumption: Sparsity constraints on BN parameters are counterproductive during knowledge distillation when feature-level statistics are constantly changing.
- Evidence anchors: Section 2.5 discusses how sparsity constraints on γ parameter are counterproductive during knowledge distillation.

### Mechanism 3
- Claim: Pixel-wise contrastive learning preserves intra-class compactness and modality-specific style information during distillation.
- Mechanism: Contrastive loss brings pixel embeddings from the same category closer while pushing different categories apart.
- Core assumption: Contrastive learning can effectively maintain class boundaries and style information in the shared latent space.
- Evidence anchors: Section 2.6 describes using contrastive learning LCL for last four layers of encoders to improve intra-domain mining.

## Foundational Learning

- **Knowledge Distillation**: Transferring semantic knowledge from a pre-trained model to enhance training of another model. Needed here to transfer optical image knowledge to CSK-Net's optical branch. Quick check: What is the difference between feature distillation and output distillation in semantic segmentation?

- **Contrastive Learning**: Learning representations by comparing similar and dissimilar samples. Needed here to preserve modality-specific style information and prevent negative transfer. Quick check: How does contrastive learning prevent negative transfer when aligning features from different modalities?

- **Batch Normalization Statistics**: Batch normalization layers learn channel-wise scaling and shifting parameters during training. Needed here because separate BN layers preserve modality-specific information. Quick check: What information is encoded in batch normalization statistics that makes them modality-specific?

## Architecture Onboarding

- **Component map**: Input → Shared encoders (shared conv + separate BN) → Individual decoders (for modality-specific layers) → Gated Spectral Unit → Output predictions

- **Critical path**: EO-IR image pairs → Shared convolutional layers → Separate batch norm layers → Individual encoders → Decoders → Gated Spectral Unit → Segmentation predictions

- **Design tradeoffs**: Shared convolutions reduce parameters and encourage feature sharing, but require careful handling of modality-specific information through separate BN layers and mixed feature exchange. This balances parameter efficiency with modality preservation.

- **Failure signatures**: Performance degradation on IR-only inference indicates loss of modality-specific information; poor multi-modal performance suggests insufficient feature sharing.

- **First 3 experiments**:
  1. Train baseline DeepLabV3+ on optical images only, verify performance on optical test set
  2. Train CSK-Net with shared convolutions but shared BN layers, compare performance to separate BN variant
  3. Test CSK-Net performance on IR-only inference vs. baseline IR-only model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Gated Spectral Unit (GSU) specifically handle the balance between modality-specific and shared information during the distillation process?
- Basis in paper: The paper introduces GSU and states that it "regulates the correlation between low and high-frequency information during the knowledge distillation" and helps in "preserving the high-frequency, modality-specific information within the IR branch of the model during distillation, by preventing forced feature alignment between different modalities."
- Why unresolved: While the paper describes the purpose and structure of GSU, it does not provide detailed analysis on how GSU achieves this balance, or how it compares to other methods of handling modality-specific information.
- What evidence would resolve it: Detailed ablation studies comparing the performance of CSK-Net with and without GSU, or with variations of GSU, would provide evidence on its effectiveness in handling modality-specific information.

### Open Question 2
- Question: What is the impact of the mixed feature exchange strategy on the overall performance of the model, and how does it compare to other feature exchange methods?
- Basis in paper: The paper introduces the mixed feature exchange strategy as a "self-adaptive modality fusion method, allowing for the retention of modality-specific statistics within each branch using channel and spatial exchange."
- Why unresolved: The paper mentions the mixed feature exchange strategy but does not provide a detailed comparison with other feature exchange methods, nor does it discuss the specific impact of this strategy on the model's performance.
- What evidence would resolve it: Comparative studies with other feature exchange methods, such as channel-wise or spatial-wise exchange alone, would provide evidence on the effectiveness of the mixed feature exchange strategy.

### Open Question 3
- Question: How does the proposed CSK-Net perform in scenarios with more than two modalities, such as RGB-T (Thermal) or RGB-D (Depth) images?
- Basis in paper: The paper focuses on the fusion of optical and infrared images, but the methodology could potentially be extended to scenarios with more than two modalities.
- Why unresolved: The paper does not explore the performance of CSK-Net in scenarios with more than two modalities, which is a limitation given the increasing use of multi-modal data in various applications.
- What evidence would resolve it: Experiments on datasets with more than two modalities, such as RGB-T or RGB-D, would provide evidence on the generalizability of the CSK-Net approach.

## Limitations

- The Gated Spectral Unit and mixed feature exchange mechanisms lack detailed implementation specifics that would enable faithful reproduction
- The claim that separate batch norm layers are essential for preserving modality-specific information assumes BN statistics encode most spectral differences without empirical validation
- The assertion that sparsity constraints on BN parameters are counterproductive during knowledge distillation lacks supporting evidence and may contradict recent findings

## Confidence

- **High Confidence**: The general framework of shared convolutions with separate BN layers for multi-modal learning is well-established and the performance improvements on public benchmarks are verifiable
- **Medium Confidence**: The specific contributions around mixed feature exchange and contrastive learning for preserving style information are novel but rely on assumptions about feature statistics behavior during training that need validation
- **Low Confidence**: The claim that sparsity constraints on BN parameters are counterproductive during knowledge distillation lacks supporting evidence and contradicts some recent findings in compact model training

## Next Checks

1. **Architectural Implementation**: Implement and test the exact Gated Spectral Unit and mixed feature exchange strategy to verify they produce the claimed benefits over simpler alternatives

2. **Ablation on BN Design**: Conduct controlled experiments comparing shared-BN vs. separate-BN variants to quantify the actual contribution of separate batch norm layers to performance

3. **Contrastive Learning Analysis**: Analyze the learned embedding space to verify that contrastive loss actually maintains class boundaries and prevents negative transfer between modalities