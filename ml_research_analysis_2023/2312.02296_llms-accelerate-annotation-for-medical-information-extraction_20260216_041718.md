---
ver: rpa2
title: LLMs Accelerate Annotation for Medical Information Extraction
arxiv_id: '2312.02296'
source_url: https://arxiv.org/abs/2312.02296
tags:
- base
- annotation
- medical
- time
- annotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose using LLMs to reduce human annotation workload
  for medical information extraction from clinical notes. Their method involves generating
  initial annotations with an LLM, then having human experts refine them.
---

# LLMs Accelerate Annotation for Medical Information Extraction

## Quick Facts
- **arXiv ID**: 2312.02296
- **Source URL**: https://arxiv.org/abs/2312.02296
- **Reference count**: 39
- **Primary result**: LLM-assisted annotation reduced human annotation time by 58% while maintaining comparable quality to human-only annotation

## Executive Summary
This paper proposes using large language models (LLMs) to reduce human annotation workload for medical information extraction from clinical notes. The method involves generating initial annotations with an LLM, then having human experts refine them in a two-step approach. On a medication extraction task, the LLM-assisted method reduced total human time by 58% compared to human-only annotation, with comparable label quality to expert-only annotation. The approach leverages LLMs' ability to generate base annotations efficiently, allowing human experts to focus on refinement rather than creation.

## Method Summary
The authors developed an LLM-assisted annotation pipeline that generates base annotations using LLMs with carefully engineered prompts, then refines these annotations through human expertise. The LLM pipeline uses two prompt schemas (IOB-Token and Direct Chunk) with ensemble processing to maximize recall. Human annotators perform two phases: initial base annotation generation and subsequent refinement of LLM-generated annotations. The method was evaluated on the i2b2 2009 Workshop on NLP Challenges dataset using medication extraction as the target task, measuring both time efficiency and annotation quality through precision, recall, and F1 scores.

## Key Results
- LLM-assisted annotation reduced human annotation time by 58% compared to human-only annotation (11.3 vs 17.6 minutes per document)
- The LLM-generated base annotations achieved high recall (0.9335 phrase-level, 0.9551 token-level), enabling efficient human refinement
- Final annotation quality (F1 scores) was comparable between LLM-assisted and human-only approaches
- Human refinement over LLM base annotations was more time-efficient than refinement over human base annotations (9.1 vs 11.3 minutes per document)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated base annotations reduce the cognitive load and time required for initial labeling, enabling human experts to focus on refinement rather than creation.
- Mechanism: By providing an initial set of annotations, LLMs offload the most time-consuming part of the annotation process—generating spans and linking entities—allowing humans to focus on error correction and quality improvement.
- Core assumption: The LLM's base annotations are sufficiently accurate to make refinement more efficient than starting from scratch.
- Evidence anchors:
  - [abstract] "significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets"
  - [section] "The Base Rater has the highest time cost, with a mean of 17.6 minutes per document, compared to the Refinement over Base LLM and Refinement over Base Rater, which have means of 11.3 and 9.1 minutes per document, respectively."
- Break Condition: If LLM recall drops significantly, human time spent adding missing annotations could exceed the time saved from not starting from scratch.

### Mechanism 2
- Claim: Prioritizing recall in LLM base annotations leads to greater time efficiency than prioritizing precision.
- Mechanism: High recall ensures most relevant spans are captured, reducing the need for humans to search and add missing information. Precision errors (false positives) are easier to correct by deletion than recall errors (false negatives) are by addition.
- Core assumption: It is faster to delete incorrect annotations than to create correct ones from scratch.
- Evidence anchors:
  - [section] "We placed a higher priority on annotation recall over precision because our empirical experience suggests that deleting annotations is less time consuming than producing them from scratch."
  - [section] "the lower human time cost for deleting a label compared to adding or modifying one."
- Break Condition: If the LLM generates excessive false positives, the time spent deleting could outweigh the time saved from not having to add missing annotations.

### Mechanism 3
- Claim: The ensemble of multiple LLM prompt schemas improves annotation quality by combining strengths of different approaches.
- Mechanism: Different prompt schemas capture different aspects of the task; combining them through ensembling yields a more complete and accurate annotation set than any single schema.
- Core assumption: The errors made by different prompt schemas are complementary rather than overlapping.
- Evidence anchors:
  - [section] "We also evaluated the performance of an ensemble that combines generated annotations from the best configurations from both prompt schemas. Pseudocode for the ensemble procedure is presented in appendix A.4. The ensemble had the highest F2 score and was therefore chosen for the LLM Base Annotations on the 'test set'."
- Break Condition: If different prompt schemas make the same types of errors, ensembling provides minimal benefit and may introduce redundancy.

## Foundational Learning

- Concept: Named Entity Recognition (NER) and Relationship Extraction (RE) in medical text
  - Why needed here: The task involves identifying medication-related fields (NER) and linking them into medication entries (RE). Understanding these concepts is essential for implementing and evaluating the annotation pipeline.
  - Quick check question: What is the difference between NER and RE, and how do they apply to medication extraction from clinical notes?

- Concept: Prompt engineering and few-shot learning with LLMs
  - Why needed here: The LLM pipeline relies on carefully crafted prompts with examples to guide the model's output. Understanding how to structure these prompts and select examples is critical for achieving good performance.
  - Quick check question: How does few-shot learning differ from zero-shot learning, and why is it particularly useful for medical annotation tasks?

- Concept: Evaluation metrics for information extraction (precision, recall, F1)
  - Why needed here: The paper uses both phrase-level and token-level metrics to evaluate annotation quality. Understanding these metrics is necessary to interpret results and make informed decisions about model selection.
  - Quick check question: Why might recall be prioritized over precision in an annotation workflow, and how does this choice affect the evaluation metrics?

## Architecture Onboarding

- Component map: LLM inference -> Resolver module -> Human refinement -> Quality evaluation
- Critical path: LLM inference → Resolver module → Human refinement → Quality evaluation
- Design tradeoffs:
  - Chunk size vs. context window: Smaller chunks reduce context but increase processing time
  - Recall vs. precision: Higher recall reduces human time spent adding missing annotations but may increase time spent deleting false positives
  - Prompt schema complexity vs. interpretability: More complex schemas may capture more information but are harder to debug and refine
- Failure signatures:
  - LLM produces excessive false positives → human refinement time increases significantly
  - Resolver module fails to parse LLM outputs → annotations cannot be converted to structured format
  - Prompt examples don't cover edge cases → LLM makes systematic errors that humans must correct repeatedly
- First 3 experiments:
  1. Run LLM pipeline with IOB-Token schema on development set, evaluate recall vs. precision tradeoff
  2. Implement and test resolver module with synthetic LLM outputs to ensure correct parsing
  3. Compare human refinement time on LLM-generated vs. human-generated base annotations on a small subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-assisted annotation compare to other semi-automated approaches like active learning or weak supervision?
- Basis in paper: [inferred] The paper mentions active learning as a related approach but does not directly compare LLM-assisted annotation to these methods.
- Why unresolved: The paper focuses on demonstrating the effectiveness of LLM-assisted annotation without benchmarking against other semi-automated approaches.
- What evidence would resolve it: A head-to-head comparison study of LLM-assisted annotation versus active learning or weak supervision on the same dataset and task.

### Open Question 2
- Question: How does the quality of LLM-generated annotations vary across different types of medical documents or specialties?
- Basis in paper: [explicit] The paper mentions that their study focused on medication extraction from discharge summaries, implying potential variation across document types.
- Why unresolved: The study only evaluated one type of medical document, leaving open the question of generalizability to other document types or specialties.
- What evidence would resolve it: Conducting the same LLM-assisted annotation study on a diverse set of medical document types and specialties, then comparing the quality metrics.

### Open Question 3
- Question: What is the optimal balance between recall and precision for LLM-generated annotations in a production setting, considering the cost of human review time?
- Basis in paper: [explicit] The paper mentions that they prioritized recall over precision due to the relative ease of deleting annotations, but does not explore the optimal balance.
- Why unresolved: The study chose a fixed F2 score without exploring how different precision-recall trade-offs might affect overall efficiency in a production environment.
- What evidence would resolve it: A cost analysis comparing different precision-recall trade-offs, factoring in both the time to review and correct LLM-generated annotations and the time to add missing annotations.

## Limitations
- The study is limited to medication extraction from clinical notes, which may not generalize to other clinical domains or information extraction tasks
- The experimental design cannot definitively attribute time savings to the LLM component versus the two-phase structure itself
- The comparison to human-only annotation lacks a proper randomized controlled trial design that would isolate the LLM effect

## Confidence
- **High Confidence**: The claim that LLM-generated base annotations reduce human annotation time is well-supported by the quantitative results (58% time reduction, 11.3 vs 17.6 minutes per document)
- **Medium Confidence**: The assertion that recall prioritization leads to greater efficiency than precision prioritization is supported by empirical observation but lacks systematic investigation
- **Low Confidence**: The claim that ensembling different prompt schemas improves annotation quality is weakly supported by the paper

## Next Checks
1. **Replication on Different Clinical Domains**: Conduct the same annotation experiment using LLMs for a different medical information extraction task (e.g., disease diagnosis extraction or laboratory result extraction) to assess generalizability. Compare time savings and quality metrics across multiple clinical document types.

2. **Controlled Experiment on Prioritization Strategy**: Design a randomized controlled trial comparing annotation workflows with different recall-precision tradeoffs. Measure not just time efficiency but also the distribution of human effort between adding missing annotations and deleting incorrect ones across different LLM configurations.

3. **Inter-annotator Agreement Analysis**: Evaluate the consistency of LLM-generated base annotations by having multiple LLM models generate base annotations for the same documents, then measuring inter-annotator agreement. Additionally, measure agreement between LLM base annotations and human expert annotations to quantify the baseline quality improvement provided by the LLM.