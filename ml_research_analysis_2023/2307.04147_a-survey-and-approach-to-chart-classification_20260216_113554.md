---
ver: rpa2
title: A Survey and Approach to Chart Classification
arxiv_id: '2307.04147'
source_url: https://arxiv.org/abs/2307.04147
tags:
- chart
- classification
- images
- deep
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a survey of chart classification techniques,
  including traditional machine learning, CNN-based, and transformer-based approaches.
  The authors conduct an extensive comparative analysis of CNN-based and transformer-based
  models on the recently published CHARTINFO UB-UNITECH PMC dataset for the CHART-Infographics
  competition at ICPR 2022.
---

# A Survey and Approach to Chart Classification

## Quick Facts
- arXiv ID: 2307.04147
- Source URL: https://arxiv.org/abs/2307.04147
- Reference count: 36
- Primary result: Swin-Chart transformer achieves state-of-the-art F1-score of 0.932 on chart classification task

## Executive Summary
This paper presents a comprehensive survey of chart classification techniques, comparing traditional machine learning, CNN-based, and transformer-based approaches. The authors conduct an extensive comparative analysis on the CHARTINFO UB-UNITECH PMC dataset, which contains 15 chart categories with 22,923 training images and 13,260 test images. Their implementation of a vision-based Swin-Chart transformer model demonstrates superior performance over existing deep learning models, achieving an average F1-score of 0.932. The study identifies critical challenges in chart classification, including the lack of standard benchmark datasets, the need for robust models that handle noise, and the importance of addressing class imbalance across different chart types.

## Method Summary
The study implements multiple CNN and transformer architectures for chart classification, all initialized with pre-trained ImageNet weights and fine-tuned on the UB-PMC dataset. Models are trained using progressive resizing (224px → 384px) with batch sizes of 64 for CNNs and 16 for transformers, a learning rate of 1e-4, and 100 epochs of training. The authors compare Swin-Chart transformer against baseline models including ResNet152, DenseNet121, Xception, ConvNeXt, and DeiT, evaluating performance using precision, recall, and F1-score metrics averaged across all 15 chart categories.

## Key Results
- Swin-Chart transformer achieves state-of-the-art F1-score of 0.932 on the UB-PMC dataset
- Hierarchical attention with shifted windows enables better cross-window connectivity than standard self-attention
- Progressive resizing training (224px → 384px) contributes to improved model performance
- All models use ImageNet pre-trained weights successfully for chart classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swin-Chart transformer outperforms CNN-based models on chart classification due to its hierarchical architecture with shifted windows.
- Mechanism: The Swin-Chart model uses a hierarchical transformer structure with shifted window attention, which allows for linear computational complexity and scalability. This design enables better cross-window connectivity compared to standard self-attention, capturing long-range dependencies in chart images more effectively than CNNs.
- Core assumption: Chart images contain complex spatial relationships that benefit from hierarchical attention mechanisms rather than purely convolutional feature extraction.
- Evidence anchors:
  - [abstract] "The authors note that the hierarchical architecture provides linear computational complexity and scalability concerning image size. The limitation of self-attention calculation concerning noncoincident local windows due to the shifting windows allows for better cross-window connectivity."
  - [section] "Swin-Chart outperforms the other vision transformer and CNN-based models on the latest UB-PMC dataset."
  - [corpus] Weak evidence - corpus focuses on more recent chart understanding approaches, not specifically on Swin transformer mechanisms.
- Break condition: If chart images are simple enough that local convolutional features suffice, or if computational resources are severely limited preventing transformer deployment.

### Mechanism 2
- Claim: Transfer learning from ImageNet pre-trained models provides strong initialization for chart classification.
- Mechanism: All models (CNN and transformer) use pre-trained ImageNet weights, which provide general visual feature representations that can be fine-tuned for chart-specific classification tasks.
- Core assumption: Visual features learned from natural images (ImageNet) are transferable to chart image understanding despite domain differences.
- Evidence anchors:
  - [section] "We use the pre-trained ImageNet weights of these models and fine-tune them for our chart classification task."
  - [section] "The CNN-based models were selected based on their performance in the existing literature on the ImageNet image classification task."
  - [corpus] Weak evidence - corpus neighbors focus on newer approaches, not specifically on transfer learning effectiveness.
- Break condition: If chart images have fundamentally different visual characteristics than natural images, or if the domain gap is too large for effective transfer.

### Mechanism 3
- Claim: Progressive resizing training improves transformer performance on chart classification.
- Mechanism: Models are initially trained on a smaller input size (224) before being trained on larger sizes (384), allowing the model to learn coarse features first before refining with finer details.
- Core assumption: Chart classification benefits from multi-scale feature learning, where initial training on smaller images provides a stable foundation for later fine-tuning on larger images.
- Evidence anchors:
  - [abstract] "The authors applied a Swin Transformer Base Model with a progressive resizing technique. The models were initially trained on a scale (input size) of 224 followed by 384"
  - [section] "We use a batch size of 64 for CNN-based models and a batch size of 16 for transformer-based models. A learning rate of 10−4 is used to train each model for 100 epochs."
  - [corpus] Weak evidence - corpus neighbors do not specifically discuss progressive resizing techniques.
- Break condition: If the chart dataset is small enough that progressive resizing adds unnecessary complexity, or if computational constraints prevent multi-stage training.

## Foundational Learning

- Concept: Vision transformer architecture and attention mechanisms
  - Why needed here: Understanding how Swin-Chart's hierarchical attention with shifted windows differs from standard transformers and CNNs is crucial for implementing and improving the model.
  - Quick check question: How does the shifted window approach in Swin-Chart enable better cross-window connectivity compared to standard self-attention?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: The paper relies on ImageNet pre-trained weights for all models, requiring understanding of how to effectively fine-tune pre-trained models for domain-specific tasks.
  - Quick check question: What are the key considerations when fine-tuning pre-trained models from natural images to chart images?

- Concept: Multi-class classification evaluation metrics
  - Why needed here: The paper uses precision, recall, and F1-score averaged across all classes, requiring understanding of these metrics and their implications for model performance.
  - Quick check question: How do precision, recall, and F1-score differ in their sensitivity to class imbalance in chart classification?

## Architecture Onboarding

- Component map: Data loading → Model initialization with pre-trained weights → Progressive resizing training (224px → 384px) → Fine-tuning for 100 epochs → Evaluation on test set → Result analysis
- Critical path: Chart image loading → ImageNet weight initialization → Progressive resizing training → Fine-tuning → Test evaluation → Performance metrics calculation
- Design tradeoffs: Transformers offer superior performance but require more computational resources and memory compared to CNNs. Progressive resizing improves accuracy but extends training time. Using ImageNet pre-training provides good initialization but may introduce domain mismatch.
- Failure signatures: Poor performance on specific chart types (particularly less common ones like Venn or surface charts), overfitting on training data despite using pre-trained weights, failure to converge during training, or significant performance degradation when applied to noisy chart images.
- First 3 experiments:
  1. Baseline test: Train Swin-Chart without progressive resizing to quantify its contribution to performance.
  2. Data augmentation test: Apply various data augmentation techniques to assess their impact on model robustness to chart variations.
  3. Model ablations: Compare Swin-Chart performance against Swin-Base and Swin-Small variants to determine optimal model size for this task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can chart classification models be made robust to noisy and low-quality chart images?
- Basis in paper: [explicit] The paper mentions that "Different types of noise, such as background grids, low image quality, composite charts, and multiple components along with figures, lead to poor performance for models that perform exceptionally well on noiseless data" and identifies this as a future research direction.
- Why unresolved: Current models achieve high performance on clean datasets but performance degrades significantly when noise is introduced. The paper suggests that "if a small set of chart images could be provided that incorporates the noisy images, it would help fine-tune the models to work through the inclusion of noise and be invariant to the same."
- What evidence would resolve it: A benchmark dataset containing chart images with various types of noise (background grids, low quality, composite charts, etc.) and evaluation showing consistent model performance across clean and noisy samples.

### Open Question 2
- Question: What architectural improvements can be made to vision transformers to better handle the unique characteristics of chart images?
- Basis in paper: [explicit] The paper demonstrates that Swin-Chart achieves state-of-the-art results but suggests future work to "generalize the results of the Swin-Chart over other publicly available datasets" and improve robustness.
- Why unresolved: While Swin-Chart outperforms other models on the UB-PMC dataset, its performance across diverse chart datasets and with noisy data needs further investigation. The paper notes that "chart classification problem still needs to be solved, especially for noisy and low-quality charts."
- What evidence would resolve it: Comparative experiments across multiple chart datasets with varying noise levels, showing consistent improvement over current models, and ablation studies identifying which architectural components contribute most to performance.

### Open Question 3
- Question: How can we address the imbalance between different chart types in datasets to improve overall classification performance?
- Basis in paper: [inferred] The paper notes that "The popularity of charts such as bar, line, and scatter over others such as Venn, surface, and area adds to the problem of disparity between the number of samples in particular chart types."
- Why unresolved: Current datasets have significantly fewer samples for certain chart types (e.g., Venn, surface, area) compared to popular types like bar and line charts, which can lead to poor performance on underrepresented classes.
- What evidence would resolve it: Experiments demonstrating improved performance on underrepresented chart types through techniques like data augmentation, class-balanced sampling, or synthetic data generation, with metrics showing improved F1-scores across all chart types rather than just overall accuracy.

## Limitations

- Limited generalization: Results are demonstrated only on a single dataset (UB-PMC) without extensive cross-dataset validation
- Computational cost: Transformer models, particularly Swin-Chart, require significant computational resources not fully addressed for practical deployment
- Noise robustness: Models show poor performance on noisy and low-quality chart images despite achieving high accuracy on clean data

## Confidence

- **High confidence**: Swin-Chart transformer architecture achieving SOTA results on the UB-PMC dataset (F1-score of 0.932)
- **Medium confidence**: Transfer learning from ImageNet provides effective initialization for chart classification tasks
- **Medium confidence**: Progressive resizing training contributes meaningfully to transformer performance improvements

## Next Checks

1. **Cross-dataset validation**: Test the best-performing Swin-Chart model on other chart classification datasets (e.g., FigureQA, DVQA) to assess generalization beyond the UB-PMC dataset.

2. **Ablation study of training components**: Systematically remove progressive resizing and compare against standard single-scale training to quantify the specific contribution of this technique to performance gains.

3. **Robustness evaluation**: Evaluate model performance on chart images with controlled noise levels, compression artifacts, and visual distortions to assess real-world deployment viability.