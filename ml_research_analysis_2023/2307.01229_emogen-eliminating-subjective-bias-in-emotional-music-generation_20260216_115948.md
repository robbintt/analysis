---
ver: rpa2
title: 'EmoGen: Eliminating Subjective Bias in Emotional Music Generation'
arxiv_id: '2307.01229'
source_url: https://arxiv.org/abs/2307.01229
tags:
- music
- emotion
- emogen
- subjective
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of subjective bias in emotional
  music generation, where different annotators may label the same music with different
  emotions. To eliminate this bias, the authors propose EmoGen, a two-stage system
  that maps emotions to a set of emotion-related music attributes using supervised
  clustering, and then generates music from these attributes using self-supervised
  learning.
---

# EmoGen: Eliminating Subjective Bias in Emotional Music Generation

## Quick Facts
- arXiv ID: 2307.01229
- Source URL: https://arxiv.org/abs/2307.01229
- Reference count: 11
- Primary result: EmoGen achieves 0.650 subjective emotion control accuracy vs 0.250 for baseline CS method

## Executive Summary
EmoGen addresses subjective bias in emotional music generation by using a two-stage system that maps emotions to emotion-related music attributes using supervised clustering, then generates music from these attributes using self-supervised learning. The approach eliminates the influence of subjective emotion labels during the generation process, achieving higher emotion control accuracy and music quality compared to previous methods. Experiments demonstrate that EmoGen achieves 0.650 subjective accuracy versus 0.250 for the baseline CS method, with 0.715 objective accuracy compared to 0.439 for CS.

## Method Summary
EmoGen employs a two-stage architecture: first, it maps emotions to music attributes by clustering samples with the same emotion label and selecting attribute values closest to cluster centers; second, it trains a Transformer-based generative model using these attribute values as control signals in a self-supervised manner. The attribute selection process uses a Random Forest classifier trained on labeled data to identify emotion-related attributes from jSymbolic features, selecting the top-k most important attributes. The model is trained on unlabeled music data using extracted attribute values, allowing it to learn the relationship between attributes and music without exposure to subjective emotion labels.

## Key Results
- EmoGen achieves 0.650 subjective accuracy versus 0.250 for baseline CS method
- Objective accuracy reaches 0.715 versus 0.439 for CS
- Outperforms PUCT baseline with 0.591 subjective accuracy
- Music quality remains high while improving emotion control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised clustering reduces subjective bias in emotion labels by mapping emotions to attribute values near the cluster center.
- Mechanism: By clustering samples with the same emotion label and selecting attribute values closest to the cluster center, the method captures the "general" emotion and filters out individual annotator biases.
- Core assumption: The attribute values near the cluster center represent the general emotion rather than individual biases.
- Evidence anchors: [abstract] "the attribute values around the clustering center represent the general emotions of these samples, which help eliminate the impacts of the subjective bias of emotion labels"; [section 3.2] "we calculate the mean value for each attribute to obtain the center... the attribute values of the sample that is the closest to the center are used to represent the features of the emotion"
- Break condition: If annotator biases are systematic rather than random, clustering may not eliminate them but instead amplify them.

### Mechanism 2
- Claim: Self-supervised learning in the attribute-to-music generation stage avoids learning from biased emotion labels.
- Mechanism: By training the generative model with attribute values extracted from music as control signals, without using emotion labels, the model learns the relationship between attributes and music directly, avoiding the influence of subjective bias in emotion labels.
- Core assumption: The attribute values extracted from music accurately represent the desired musical qualities and can be used as control signals.
- Evidence anchors: [abstract] "the generation is completely disentangled from emotion labels and thus free from the subjective bias"; [section 3.3] "The values of the attributes can be directly extracted from music sequences, so this generative model can learn the relationship between control signals and music without requiring any labeled data"
- Break condition: If the attribute extraction process is noisy or inaccurate, the model may learn incorrect relationships.

### Mechanism 3
- Claim: Using emotion-related attributes as a bridge between emotions and music sequences provides more precise control than using emotion labels directly.
- Mechanism: By selecting attributes that are highly correlated with emotions (identified through a Random Forest classifier), the model can generate music with more specific and controllable qualities that correspond to the desired emotion.
- Core assumption: The selected attributes are sufficiently correlated with emotions to allow accurate emotion control.
- Evidence anchors: [abstract] "leverages a set of emotion-related music attributes as the bridge between emotion and music"; [section 3.1] "we design the attribute set by training an emotion classifier on a labeled dataset and selecting those attributes whose feature importance is high"
- Break condition: If the correlation between the selected attributes and emotions is weak, the model's emotion control accuracy will suffer.

## Foundational Learning

- Concept: Emotion representation models (e.g., Russell's 4Q model)
  - Why needed here: The paper uses Russell's 4Q model to categorize emotions for evaluation and clustering. Understanding this model is crucial for interpreting the results and the emotion control process.
  - Quick check question: How does Russell's 4Q model categorize emotions based on valence and arousal?

- Concept: Feature importance in machine learning
  - Why needed here: The paper uses a Random Forest classifier to identify emotion-related attributes by their feature importance. Understanding feature importance is essential for grasping how the attribute selection process works.
  - Quick check question: What does feature importance measure in a Random Forest classifier, and how is it used for attribute selection?

- Concept: Self-supervised learning
  - Why needed here: The paper employs self-supervised learning in the attribute-to-music generation stage to train the model without using emotion labels. Understanding self-supervised learning is important for comprehending how the model learns the relationship between attributes and music.
  - Quick check question: What is self-supervised learning, and how does it differ from supervised learning in the context of this paper?

## Architecture Onboarding

- Component map: Attribute Design -> Emotion-to-Attribute Mapping -> Attribute-to-Music Generation -> Music Generation
- Critical path:
  1. Design emotion-related attributes by training Random Forest classifier and selecting top-k attributes.
  2. Map emotions to attribute values by clustering samples by emotion label and selecting values closest to the cluster center.
  3. Train attribute-to-music generation model using attribute values extracted from music as control signals in a self-supervised manner.
  4. Generate music by feeding mapped attribute values to the trained model.

- Design tradeoffs:
  - Using a larger number of attributes may improve emotion control accuracy but could harm music quality due to increased complexity.
  - Selecting attributes based on feature importance may not capture all relevant attributes, but it helps reduce noise.
  - The choice of clustering method (closest vs. center vs. K-Means) affects the diversity and representativeness of the mapped attribute values.

- Failure signatures:
  - Poor emotion control accuracy: The selected attributes may not be sufficiently correlated with emotions, or the clustering method may not effectively capture the general emotion.
  - Low music quality: The attribute-to-music generation model may not learn the relationship between attributes and music effectively, or the mapped attribute values may not represent the desired musical qualities.
  - Overfitting: The model may overfit to the training data, especially if a large number of attributes are used or if the training data is limited.

- First 3 experiments:
  1. Evaluate the emotion control accuracy and music quality of EmoGen using different numbers of attributes (e.g., top-10, top-50, top-100, top-300, top-500) to find the optimal balance between control and quality.
  2. Compare the performance of EmoGen using different clustering methods (closest, center, K-Means) in the emotion-to-attribute mapping stage to determine the best approach for capturing general emotions.
  3. Test EmoGen's ability to generate emotional music on an arbitrary dataset with no emotion annotations to validate its generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EmoGen perform when generating music for emotions not represented in the training data?
- Basis in paper: [inferred] The paper mentions that EmoGen can be applied to arbitrary datasets with no annotations, but does not provide results for generating music for emotions not present in the training data.
- Why unresolved: The paper does not provide any experimental results or analysis on EmoGen's performance when generating music for emotions not present in the training data.
- What evidence would resolve it: Experiments showing EmoGen's performance when generating music for emotions not present in the training data, such as user studies or objective metrics.

### Open Question 2
- Question: How does the choice of emotion classification model affect EmoGen's performance?
- Basis in paper: [inferred] The paper mentions that EmoGen uses a Linear Transformer-based emotion classifier, but does not explore the impact of using different emotion classification models.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of using different emotion classification models on EmoGen's performance.
- What evidence would resolve it: Experiments comparing EmoGen's performance when using different emotion classification models, such as user studies or objective metrics.

### Open Question 3
- Question: How does EmoGen's performance compare to other state-of-the-art emotional music generation methods that do not use attribute-based control?
- Basis in paper: [inferred] The paper compares EmoGen to two representative methods (CS and PUCT) that use different emotion control manners, but does not compare to other state-of-the-art methods that do not use attribute-based control.
- Why unresolved: The paper does not provide any experimental results or analysis comparing EmoGen's performance to other state-of-the-art emotional music generation methods that do not use attribute-based control.
- What evidence would resolve it: Experiments comparing EmoGen's performance to other state-of-the-art emotional music generation methods that do not use attribute-based control, such as user studies or objective metrics.

## Limitations

- Performance improvements are substantial but limited by comparison to only one baseline method (CS), restricting generalizability.
- The assumption that clustering eliminates bias depends on biases being random rather than systematic; shared cultural or personal biases may be amplified.
- The approach's generalizability across different musical traditions and cultural contexts remains untested with Western piano music datasets.

## Confidence

- **High confidence**: The two-stage architecture (emotion-to-attribute mapping followed by attribute-to-music generation) is well-specified and the experimental methodology is clearly described. The claim that using emotion-related attributes as a bridge provides more precise control than emotion labels directly is supported by the feature importance selection process.
- **Medium confidence**: The mechanism that supervised clustering reduces subjective bias by selecting attribute values near cluster centers is reasonable but depends heavily on the assumption that biases are random. The claim that self-supervised learning avoids bias by not using emotion labels is valid in principle, but the effectiveness depends on the quality of the attribute extraction process.
- **Low confidence**: The generalizability of results across different cultural contexts and musical styles is unclear. The paper uses Western piano music datasets, and it's uncertain whether the same attributes would capture emotions equivalently in other musical traditions.

## Next Checks

1. **Bias analysis of clustering**: Analyze the distribution of annotations before and after clustering to verify that systematic biases (not just random variation) are being reduced. Test whether clustering amplifies shared cultural biases by comparing results across annotators from different cultural backgrounds.

2. **Attribute extraction quality validation**: Conduct ablation studies where attribute extraction quality is deliberately degraded (e.g., by adding noise) to quantify how much the self-supervised learning performance depends on accurate attribute extraction versus the model architecture itself.

3. **Cross-cultural generalization test**: Evaluate EmoGen on music datasets from different cultural traditions to test whether the same set of emotion-related attributes generalizes or whether attribute selection needs to be culture-specific.