---
ver: rpa2
title: 'CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based
  Tools'
arxiv_id: '2307.15770'
source_url: https://arxiv.org/abs/2307.15770
tags:
- report
- risks
- climate-related
- risk
- climate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHATREPORT is an LLM-based system for automated analysis of corporate
  sustainability reports, designed to improve accessibility and transparency. It addresses
  the challenges of LLM hallucination and lack of domain expertise by grounding outputs
  in report references and incorporating domain expert feedback into prompt development.
---

# CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools

## Quick Facts
- arXiv ID: 2307.15770
- Source URL: https://arxiv.org/abs/2307.15770
- Reference count: 40
- Primary result: 83.63% content hallucination-free rate, 75% source hallucination-free rate

## Executive Summary
CHATREPORT is an LLM-based system designed to automate analysis of corporate sustainability reports while addressing hallucination and domain expertise challenges. The system grounds LLM outputs in report references through semantic search and citation, achieving high hallucination-free rates (83.63% content, 75% source). By incorporating domain expert feedback into an automatic prompt engineering pipeline, the system improves TCFD conformity assessment and customized question answering. The authors provide an annotated dataset from human evaluation of 110 answer pairs across 10 reports, demonstrating a practical approach to making sustainability report analysis more accessible and transparent.

## Method Summary
CHATREPORT employs a four-module pipeline: Report Embedding (chunking and vector embedding), Report Summarization, TCFD Conformity Assessment, and Customized Question Answering. The system uses LangChain with OpenAI's text-embedding-ada-002 for text chunk embedding and semantic search. Expert feedback is transformed into generalizable guidelines through an automatic prompt engineering tool that prompts ChatGPT to convert specific feedback into analysis guidelines. The system evaluates 9781 sustainability reports spanning 2010-2022, with most from NASDAQ and NYSE companies.

## Key Results
- 83.63% content hallucination-free rate and 75% source hallucination-free rate achieved through citation-based grounding
- ChatGPT outperforms GPT-4 on answer honesty due to less tendency toward high-level summarization
- Question-answering format produces more granular and traceable responses than summarization format

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding LLM outputs in report references reduces hallucination impact
- Mechanism: By forcing the model to cite specific text chunks and limiting responses to extractive summaries, hallucination becomes traceable and verifiable
- Core assumption: Users can effectively verify claims by cross-referencing cited pages
- Evidence anchors:
  - [abstract] "making the answers traceable to reduce the harm of hallucination"
  - [section 3.3] "We attach source numbers to retrieved chunks and prompt the LLM to provide its attribution"
- Break condition: If reference chunks are incomplete or context is lost during retrieval, hallucinations become harder to detect

### Mechanism 2
- Claim: Expert feedback can be automatically transformed into generalizable guidelines for prompt improvement
- Mechanism: Domain experts provide feedback on specific outputs, which is then converted into general guidelines via prompt engineering that enhance future responses
- Core assumption: Expert feedback contains patterns that can be abstracted into reusable prompt instructions
- Evidence anchors:
  - [section 3.4] "we design an automatic prompt engineering tool so that the domain experts can efficiently transfer their feedback on specific outputs to general analysis guidelines"
  - [section 3.4] "we prompt ChatGPT to transform the feedback into guidelines that can be used to guide future TCFD question answering"
- Break condition: If expert feedback is too context-specific or contains contradictory elements, the abstraction process fails

### Mechanism 3
- Claim: Question-answering format produces more granular and traceable responses than summarization format
- Mechanism: By prompting the model to answer specific questions rather than summarize disclosures, responses contain more precise, actionable information
- Core assumption: Explicit questions guide the model toward extracting relevant details rather than generating generic summaries
- Evidence anchors:
  - [section 3.4] "Our prompts for both scenarios are disclosed in Appendix A. We evaluate the prompt templates with experts involved, where the expert's feedback shows that question answering outperforms disclosure summarization"
  - [section 5.3] "We surprisingly find that ChatGPT outperforms GPT-4 by a large margin in answer honesty. This is because GPT-4 tends to summarize information at a higher level and make unnecessary inferences"
- Break condition: If questions are poorly formulated or too broad, the advantage of question-answering diminishes

## Foundational Learning

- Concept: TCFD framework structure
  - Why needed here: The system evaluates reports against specific TCFD recommendations, requiring understanding of the governance-strategy-risk-metrics framework
  - Quick check question: What are the four main categories of TCFD recommendations?

- Concept: LLM hallucination and mitigation strategies
  - Why needed here: The system's core challenge is reducing false information while maintaining useful analysis
  - Quick check question: What are the two main dimensions used to evaluate hallucination in this system?

- Concept: Vector database retrieval for long documents
  - Why needed here: Reports exceed context window limits, requiring chunking and semantic search for relevant passages
  - Quick check question: What chunking strategy achieves optimal retrieval performance according to the paper?

## Architecture Onboarding

- Component map: Report → Text chunking → Vector embedding → Semantic search → LLM prompt → Answer with citations → Expert evaluation
- Critical path: The semantic search and LLM generation components are most critical for system performance
- Design tradeoffs: Higher retrieval precision vs. context window limitations; more detailed guidelines vs. prompt complexity
- Failure signatures: Low ROUGE precision indicates poor grounding; high Cohen's Kappa suggests hallucinations are hard to identify
- First 3 experiments:
  1. Compare extractive vs. abstractive summarization performance on a sample report
  2. Test different chunk sizes (300-1000 characters) to optimize retrieval accuracy
  3. Evaluate prompt effectiveness by measuring hallucination rates with and without specific guidelines

## Open Questions the Paper Calls Out

The paper identifies several limitations and future work directions:

- How can retrieval module accuracy be improved to reduce the propagation of incomplete sentence concatenations that lead to hallucinations?
- What methods can incorporate external perspectives and independent sources of information beyond the report itself?
- How can the automatic prompt engineering framework be generalized to work across different sustainability reporting frameworks beyond TCFD?

## Limitations

- The system's reliance on vector database retrieval introduces potential context loss, as text chunks may be semantically related but lack complete contextual information needed for accurate interpretation.
- The automatic prompt engineering pipeline lacks detailed specification of the transformation algorithm, making it difficult to assess whether the guidelines are truly generalizable.
- The system may struggle with complex or ambiguous questions, leading to suboptimal answers that require careful evaluation.

## Confidence

**High Confidence**: The core claim that grounding LLM outputs in report references reduces hallucination impact is well-supported by the 83.63% content and 75% source hallucination-free rates, along with the systematic evaluation methodology using human experts to identify false content and sources.

**Medium Confidence**: The claim that expert feedback can be automatically transformed into generalizable guidelines is supported by the described methodology but lacks sufficient detail on the transformation algorithm to fully assess generalizability. The surprising finding that ChatGPT outperforms GPT-4 in answer honesty needs additional validation across different report types.

**Low Confidence**: The assertion that question-answering format inherently produces more granular and traceable responses than summarization format is based on limited expert feedback without systematic comparison across multiple report types or question complexities.

## Next Checks

1. **Chunk Size Optimization**: Systematically test the impact of different text chunk sizes (300-1000 characters as suggested in the paper) on both retrieval accuracy and hallucination rates using a consistent evaluation framework across multiple reports.

2. **Generalizability Assessment**: Apply the automatic prompt engineering pipeline to expert feedback from different domains or report types to evaluate whether the generated guidelines maintain effectiveness or degrade when applied outside the original context.

3. **Algorithm Transparency Audit**: Reconstruct the automatic prompt engineering algorithm using only information provided in the paper and test whether it can successfully transform a new set of expert feedback into effective guidelines, documenting any gaps or ambiguities in the described methodology.