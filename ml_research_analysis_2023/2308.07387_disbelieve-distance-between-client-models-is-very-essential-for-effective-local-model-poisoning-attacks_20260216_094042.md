---
ver: rpa2
title: 'DISBELIEVE: Distance Between Client Models is Very Essential for Effective
  Local Model Poisoning Attacks'
arxiv_id: '2308.07387'
source_url: https://arxiv.org/abs/2308.07387
tags:
- parameters
- attack
- clients
- gradients
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DISBELIEVE, a local model poisoning attack
  designed to circumvent state-of-the-art robust aggregation methods in federated
  learning for medical image analysis. The attack leverages the observation that robust
  methods rely heavily on the distance between malicious and benign clients' parameters
  or gradients, and can be bypassed if these distances are kept low while still significantly
  degrading the global model's performance.
---

# DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks

## Quick Facts
- arXiv ID: 2308.07387
- Source URL: https://arxiv.org/abs/2308.07387
- Reference count: 27
- Primary result: DISBELIEVE attack reduces global model AUC by up to 28% on medical datasets by keeping malicious parameters close to benign ones while maximizing loss

## Executive Summary
This paper introduces DISBELIEVE, a local model poisoning attack that exploits the reliance of robust aggregation methods on Euclidean distances between client parameters or gradients. By training malicious models to maximize classification loss while keeping their parameters or gradients close to benign clients' parameters or gradients, DISBELIEVE effectively bypasses detection by methods like KRUM, Trimmed Mean, and DOS. Experiments on three medical image datasets and CIFAR-10 demonstrate significant performance degradation of global models under robust aggregation, outperforming existing attacks like LIE and Min-Max.

## Method Summary
DISBELIEVE creates malicious parameters or gradients that maximize the objective loss function while ensuring their Euclidean distance to benign parameters or gradients remains marginal. For parameter attacks, malicious models are trained to maximize loss while bounding distance from the mean of malicious clients' parameters. For gradient attacks, a binary search algorithm scales the unit gradient vector to keep its distance from the mean of gradients below a threshold while maintaining attack effectiveness. The attack is evaluated under KRUM, Trimmed Mean, and DOS robust aggregation methods across medical image datasets and CIFAR-10.

## Key Results
- DISBELIEVE reduces AUC scores by up to 28% on medical image datasets compared to no attack
- The attack outperforms state-of-the-art local model poisoning attacks like LIE and Min-Max
- Classification performance on CIFAR-10 drops significantly under DISBELIEVE across all robust aggregation methods
- Distance-based robust aggregation methods fail to detect DISBELIEVE because malicious parameters/gradients remain close to benign ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robust aggregation methods fail when malicious and benign parameters are close in Euclidean space.
- Mechanism: The attack keeps malicious parameters near the mean of benign parameters, so robust methods like KRUM and DOS cannot distinguish them as outliers.
- Core assumption: Euclidean distance is the main detection criterion for robust aggregation methods.
- Evidence anchors:
  - [abstract] "leverages the observation that robust methods rely heavily on the distance between malicious and benign clients' parameters"
  - [section] "We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients"
  - [corpus] "Weak or missing" - No direct corpus evidence supporting this specific mechanism.
- Break condition: If robust methods adopt non-distance-based outlier detection or use higher-order statistics instead of raw Euclidean distances.

### Mechanism 2
- Claim: Maximizing negative loss while bounding distance allows effective poisoning without detection.
- Mechanism: The malicious model is trained to maximize classification loss but is constrained so its parameters remain close to the benign mean, bypassing distance-based filters.
- Core assumption: Bounding parameter distance while maximizing loss creates effective yet undetectable poisoning.
- Evidence anchors:
  - [section] "creates malicious parameters or gradients such that their distance to benign clients' parameters or gradients is low respectively but at the same time their adverse effect on the global model's performance is high"
  - [section] "maximizes the objective loss function while ensuring that the Euclidean distance between the malicious parameters and benign parameters is kept marginal"
  - [corpus] "Weak or missing" - No direct corpus evidence supporting this specific mechanism.
- Break condition: If aggregation methods use loss-based detection or gradient-based methods instead of parameter distances.

### Mechanism 3
- Claim: Binary search scaling ensures malicious gradients stay within safe distance bounds.
- Mechanism: After training, gradients are scaled using binary search to keep their distance from benign gradients below the minimum inter-attacker distance.
- Core assumption: Scaling gradients while maintaining loss impact is sufficient to bypass robust aggregation.
- Evidence anchors:
  - [section] "we use a popular search algorithm known as binary search... which makes sure that after scaling the unit gradient vector, its distance to the mean of gradients... is less than Gdist"
  - [section] "we train the malicious model M with the similar loss function... however, without any thresholding. Once the model M is trained, we accumulate the malicious gradients... and scale them by a scaling factor sf"
  - [corpus] "Weak or missing" - No direct corpus evidence supporting this specific mechanism.
- Break condition: If aggregation methods detect unusual scaling patterns or use adaptive distance thresholds.

## Foundational Learning

- Concept: Federated Learning basics
  - Why needed here: Understanding how multiple clients train a shared model without sharing data is essential to grasp the attack context.
  - Quick check question: What is the main goal of federated learning and how does it differ from traditional centralized training?

- Concept: Robust aggregation methods
  - Why needed here: Knowing how KRUM, Trimmed Mean, and DOS work is critical to understanding why the attack succeeds.
  - Quick check question: How do KRUM and DOS determine which clients' updates to trust or downweight?

- Concept: Model poisoning vs data poisoning
  - Why needed here: Distinguishing between altering training data versus model parameters/gradients is key to understanding the attack type.
  - Quick check question: What is the difference between data poisoning and model poisoning in federated learning?

## Architecture Onboarding

- Component map:
  - Global server -> Initializes model, aggregates updates, applies robust aggregation
  - Benign clients -> Train locally on private data, send honest updates
  - Malicious clients -> Controlled by attacker, send poisoned updates
  - Attacker proxy model M -> Generates malicious parameters/gradients within bounded distance
  - Robust aggregation algorithms -> KRUM, Trimmed Mean, DOS (distance-based outlier detection)

- Critical path:
  1. Initialize global model and distribute to all clients
  2. Malicious clients accumulate parameters/gradients and training data
  3. Attacker trains proxy model M to maximize loss while bounding distance
  4. Malicious updates are sent to global server
  5. Robust aggregation attempts to filter outliers based on distance
  6. Global model updates and distribution cycle repeats

- Design tradeoffs:
  - Distance bound tightness vs attack effectiveness: Tighter bounds reduce detection but may weaken poisoning
  - Parameter vs gradient attack: Gradients have direction/magnitude; parameters only magnitude, requiring different strategies
  - Binary search precision vs computational cost: Finer scaling search improves stealth but increases computation

- Failure signatures:
  - Sudden drop in global model AUC without obvious outlier detection
  - Consistent parameter/gradient updates from multiple clients that are nearly identical
  - Model performance degradation despite successful robust aggregation filtering

- First 3 experiments:
  1. Implement DISBELIEVE attack on parameters using CheXpert-small dataset and measure AUC drop under KRUM aggregation
  2. Implement DISBELIEVE attack on gradients using Breakhis dataset and measure AUC drop under DOS aggregation
  3. Compare DISBELIEVE attack performance against LIE and Min-Max attacks on CIFAR-10 dataset under all three robust aggregations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can robust aggregation algorithms be designed to effectively defend against attacks like DISBELIEVE that maintain low distances between malicious and benign clients' parameters or gradients?
- Basis in paper: [explicit] The paper concludes that state-of-the-art robust aggregation methods are vulnerable to DISBELIEVE because they rely heavily on distance metrics between client parameters or gradients.
- Why unresolved: The paper identifies the vulnerability but does not propose or evaluate potential defense mechanisms that could overcome this limitation.
- What evidence would resolve it: Development and empirical validation of new robust aggregation algorithms that can detect or mitigate attacks maintaining low distance between malicious and benign parameters/gradients, demonstrated through experiments on medical and natural image datasets.

### Open Question 2
- Question: Can the DISBELIEVE attack be extended to federated learning scenarios with non-iid data distributions across clients?
- Basis in paper: [inferred] The paper evaluates DISBELIEVE on medical image datasets with some non-iid considerations but does not explicitly explore highly skewed or non-iid data distributions.
- Why unresolved: The current experiments use relatively balanced datasets, and the attack's effectiveness under extreme non-iid conditions remains unexplored.
- What evidence would resolve it: Experiments demonstrating DISBELIEVE's performance when malicious clients have significantly different data distributions compared to benign clients, potentially revealing new attack or defense strategies.

### Open Question 3
- Question: What is the impact of using differential privacy mechanisms on the effectiveness of DISBELIEVE and other local model poisoning attacks?
- Basis in paper: [inferred] The paper does not address the interaction between differential privacy and model poisoning attacks, despite differential privacy being a common privacy-preserving technique in federated learning.
- Why unresolved: The interplay between privacy-preserving techniques and poisoning attacks is not well understood, and the paper focuses solely on the attack without considering additional privacy mechanisms.
- What evidence would resolve it: Experiments comparing attack success rates with and without differential privacy applied to client updates, potentially revealing whether privacy mechanisms can also serve as defenses against poisoning attacks.

## Limitations
- The attack's effectiveness relies heavily on the assumption that robust aggregation methods primarily use Euclidean distance for outlier detection
- Binary search scaling for gradient attacks may introduce computational overhead and timing patterns that could be detectable
- The attack has not been tested against robust aggregation methods that incorporate additional detection criteria beyond distance metrics

## Confidence
- Mechanism 1 (Distance-based detection bypass): Medium - The core mechanism is plausible but relies heavily on assumptions about robust aggregation methods' detection criteria that may not fully capture real implementations
- Mechanism 2 (Loss maximization within distance bounds): High - The mathematical formulation and constraint satisfaction are clearly defined and tested
- Mechanism 3 (Binary search scaling): Medium - The algorithm is well-specified, but convergence properties and computational overhead in practice are not thoroughly explored

## Next Checks
1. Test DISBELIEVE against robust aggregation methods that incorporate loss-based detection or gradient magnitude checks in addition to Euclidean distance to determine if the attack remains effective
2. Measure the computational overhead of the binary search scaling mechanism across different network sizes and gradient dimensions to assess practical feasibility
3. Implement adaptive distance thresholds that change based on observed parameter/gradient distributions to evaluate whether DISBELIEVE can maintain effectiveness against dynamic detection mechanisms