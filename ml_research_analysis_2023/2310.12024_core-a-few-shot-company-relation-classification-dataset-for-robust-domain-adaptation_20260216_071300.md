---
ver: rpa2
title: 'CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain
  Adaptation'
arxiv_id: '2310.12024'
source_url: https://arxiv.org/abs/2310.12024
tags:
- relation
- core
- domain
- entities
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORE, a few-shot relation classification
  dataset focused on company relations and business entities. CORE includes 4,708
  instances of 12 relation types extracted from company Wikipedia pages.
---

# CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation

## Quick Facts
- arXiv ID: 2310.12024
- Source URL: https://arxiv.org/abs/2310.12024
- Reference count: 12
- Key outcome: CORE dataset shows models trained on business entities generalize better to out-of-domain datasets compared to general domain training

## Executive Summary
This paper introduces CORE, a few-shot relation classification dataset focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types extracted from company Wikipedia pages. The dataset aims to create a more challenging benchmark by leveraging the information richness of business entities, which can represent legal entities, products, people, or business divisions depending on the context. Experiments in few-shot domain adaptation settings reveal substantial performance gaps, with models trained on different domains struggling to adapt to CORE. Interestingly, models trained on CORE showcase improved out-of-domain performance, highlighting the importance of high-quality data for robust domain adaptation.

## Method Summary
CORE is a few-shot relation classification dataset with 4,708 instances of 12 relation types extracted from company Wikipedia pages, including 4,000 training instances and 708 test instances. The dataset uses BERT-Base encoder with five benchmark models (Proto-BERT, BERT-Pair, BERTEM, BERT-Prompt, HCRP) in N-way K-shot settings, specifically 5-way 5-shot configuration. Models are evaluated using micro F1 and macro F1 scores for both in-domain and out-of-domain performance comparisons across different datasets (CORE, FewRel, TACRED). The few-shot evaluation involves randomly sampling support and query sets for each episode during training.

## Key Results
- Models trained on general domain datasets (FewRel, TACRED) show substantial performance gaps when adapting to CORE, with micro/macro F1 scores significantly lower than in-domain performance
- Models trained on CORE demonstrate superior out-of-domain performance, outperforming FewRel-trained models on PubMed and SCIERC datasets
- BERT-Prompt achieves the best performance on CORE with 5-way 5-shot setting, reaching 60.8 micro F1 and 60.4 macro F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information richness in business entities reduces model reliance on superficial clues like relation-specific verbs.
- Mechanism: Models trained on CORE learn to leverage contextual nuances rather than lexical shortcuts because business entities can represent multiple roles (legal entity, product, brand, division) depending on context.
- Core assumption: The ambiguity and multi-functionality of business entities forces models to focus on deeper contextual understanding rather than entity type or lexical cues.
- Evidence anchors:
  - [abstract]: "the information richness embedded in business entities allows models to focus on contextual nuances, reducing their reliance on superficial clues such as relation-specific verbs"
  - [section 3.3]: "business entities embody a variety of information and may be used interchangeably to represent the legal entity, products or services, and brands, depending on the context"
  - [corpus]: Weak - no direct corpus evidence linking verb reliance reduction to improved performance

### Mechanism 2
- Claim: Broader entity coverage (named entities, nouns, pronouns) improves generalization across domains.
- Mechanism: Exposing models to diverse entity types during training increases linguistic variety and prevents overfitting to named entity patterns.
- Core assumption: Models trained only on named entities develop shortcuts that fail when encountering common nouns or pronouns in new domains.
- Evidence anchors:
  - [section 3.3]: "models trained on CORE display superior out-of-domain performance" and "the broader entity coverage...exposes models trained on CORE to more linguistically diverse instances"
  - [section 5.2]: "models trained on CORE outperform those trained on FewRel when evaluated on datasets with a narrow domain focus, such as PubMed and SCIERC"
  - [corpus]: Weak - no explicit corpus analysis showing entity type diversity impact

### Mechanism 3
- Claim: Few-shot domain adaptation is more challenging when entity types are easily distinguishable across domains.
- Mechanism: When entities in source and target domains are semantically distinct (e.g., people/organizations vs. companies), models can exploit entity type as a shortcut rather than learning domain-invariant features.
- Core assumption: Entity type distinguishability creates domain-specific patterns that harm generalization.
- Evidence anchors:
  - [abstract]: "most existing RC datasets comprise a mixture of domains, and the sets of entities across different domains are often easily distinguishable from one another"
  - [section 1]: "the main challenge with these general domain datasets is that few-shot models may take shortcuts during training, given the limited set of possible relation types associated with specific entities"
  - [corpus]: Weak - no corpus analysis of entity distinguishability across domains

## Foundational Learning

- Concept: Few-shot learning evaluation with N-way K-shot setting
  - Why needed here: CORE uses this paradigm to evaluate models with limited training examples per relation type
  - Quick check question: In a 5-way 5-shot setting, how many total support examples does the model see during training?

- Concept: Domain adaptation vs. in-domain performance
  - Why needed here: CORE explicitly tests whether models trained on one domain can generalize to company relations
  - Quick check question: What performance metric best captures the gap between in-domain and out-of-domain adaptation?

- Concept: NOTA (None-of-the-Above) detection in relation classification
  - Why needed here: CORE includes an "undefined" relation category corresponding to NOTA instances
  - Quick check question: How does including NOTA as a relation type change the episode sampling procedure?

## Architecture Onboarding

- Component map: Dataset collection → Annotation platform → Model training (BERT variants) → Evaluation (micro/macro F1) → Error analysis
- Critical path: Dataset preparation and annotation quality → Model training with proper few-shot configuration → Evaluation with appropriate metrics → Error analysis for insights
- Design tradeoffs: High-quality human annotation vs. scale (4,000 training instances), focused domain vs. general applicability, few-shot setting vs. full supervision
- Failure signatures: Models relying on entity type shortcuts, poor NOTA detection, domain-specific overfitting, template/vocabulary mapping suboptimal for prompt tuning
- First 3 experiments:
  1. Replicate in-domain performance results on CORE with BERT-Prompt to establish baseline
  2. Test domain adaptation from FewRel/TACRED to CORE to verify performance gaps
  3. Evaluate out-of-domain performance of CORE-trained models on PubMed/SCIERC to confirm generalization benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance change if CORE included more diverse entity types beyond business entities?
- Basis in paper: [explicit] The authors acknowledge that "company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them" and that CORE "minimizes entity type related clues." They also note a limitation that CORE is "monolingual" and suggest diversifying the dataset would introduce "linguistic variety and establish a more rigorous benchmark."
- Why unresolved: The paper only evaluates CORE on business entities and English language data. There is no empirical comparison showing how performance would change with more diverse entity types or languages.
- What evidence would resolve it: Evaluating CORE models on datasets with non-business entities (people, locations, events) and/or multilingual data to compare performance differences.

### Open Question 2
- Question: What is the optimal balance between prompt engineering and automated search for template/mapping selection in few-shot relation classification?
- Basis in paper: [explicit] The authors state "Prompt engineering, including automated search procedures for optimal templates and vocabulary mappings, remains an active research area" and acknowledge their heuristic approach is "suboptimal."
- Why unresolved: The paper uses a manually designed prompt template and relation-to-token mapping, but does not compare this to automated approaches or explore the tradeoff space.
- What evidence would resolve it: Systematic comparison of different prompt engineering methods (manual vs. automated) on the same few-shot relation classification tasks, measuring performance and efficiency tradeoffs.

### Open Question 3
- Question: How do different error patterns in out-of-domain adaptation relate to specific model architectures or training data characteristics?
- Basis in paper: [explicit] The authors conduct error analysis on BERT-Prompt models and identify patterns like reliance on relation-specific verbs and entity combinations, but note "There were no discernible patterns in the context" for correct predictions.
- Why unresolved: The error analysis is limited to one model architecture and doesn't systematically compare error patterns across different models or training datasets to identify root causes.
- What evidence would resolve it: Comprehensive error analysis comparing multiple model architectures (Proto-BERT, BERT-Pair, BERTEM, BERT-Prompt, HCRP) across different training datasets (CORE, FewRel, TACRED) to identify systematic error patterns and their sources.

## Limitations

- CORE is monolingual (English only), limiting cross-lingual domain adaptation evaluation
- The dataset focuses exclusively on business entities, which may not generalize to other relation classification domains
- Prompt engineering approach used is suboptimal, with manual template design rather than automated search procedures

## Confidence

- **High confidence**: CORE dataset construction methodology and the basic empirical finding that models trained on CORE show better out-of-domain performance than those trained on FewRel/TACRED
- **Medium confidence**: The mechanism that business entity ambiguity reduces reliance on superficial clues, and that broader entity coverage improves generalization
- **Low confidence**: The specific claim that domain adaptation is harder when entity types are easily distinguishable across domains, as this is primarily supported by reasoning rather than controlled experiments

## Next Checks

1. Conduct controlled experiments varying entity type distinguishability while holding other factors constant to test whether distinguishable entities truly harm domain adaptation
2. Perform ablation studies isolating the effects of business entity focus versus broader entity coverage versus few-shot setting on domain adaptation performance
3. Analyze model attention patterns or feature representations to empirically verify whether models trained on CORE indeed rely less on relation-specific verbs and more on contextual understanding compared to models trained on general domain datasets