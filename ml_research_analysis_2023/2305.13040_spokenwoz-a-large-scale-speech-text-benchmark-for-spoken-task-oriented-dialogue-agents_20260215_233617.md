---
ver: rpa2
title: 'SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue
  Agents'
arxiv_id: '2305.13040'
source_url: https://arxiv.org/abs/2305.13040
tags:
- slot
- space
- spoken
- dialogue
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpokenWOZ is a large-scale speech-text benchmark for spoken task-oriented
  dialogue agents. It contains 8 domains, 203k turns, 5.7k dialogues, and 249 hours
  of audio from human-to-human spoken conversations.
---

# SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents

## Quick Facts
- arXiv ID: 2305.13040
- Source URL: https://arxiv.org/abs/2305.13040
- Reference count: 31
- Key outcome: Current spoken dialogue models achieve only 25.65% joint goal accuracy on SpokenWOZ, highlighting substantial room for improvement

## Executive Summary
SpokenWOZ addresses the gap between academic TOD research and real-world spoken conversation scenarios by providing a large-scale speech-text benchmark with 8 domains, 203k turns, 5.7k dialogues, and 249 hours of human-to-human audio data. The dataset introduces two novel challenges: cross-turn slots that require multi-turn value construction, and reasoning slots that demand commonsense and mathematical inference. Experiments show that even state-of-the-art models struggle significantly on this benchmark, with the best dialogue state tracker achieving only 25.65% joint goal accuracy and the top end-to-end model correctly completing user requests in just 52.1% of dialogues.

## Method Summary
SpokenWOZ provides a speech-text benchmark for spoken task-oriented dialogue agents, containing 8 domains with 203k turns, 5.7k dialogues, and 249 hours of human-to-human spoken conversations. The dataset incorporates spoken characteristics like word-by-word processing and reasoning in spoken language, introducing cross-turn slot and reasoning slot detection as new challenges. Experiments evaluate various baselines including text-modal models (BERT+TripPy, UBAR, SPACE), dual-modal models combining SPACE with WavLM speech embeddings, and LLMs (ChatGPT, InstructGPT003) using zero-shot inference. The primary metric is Joint Goal Accuracy (JGA) for dialogue state tracking, supplemented by INFORM, SUCCESS, BLEU, and Combined Score for response generation tasks.

## Key Results
- Best dialogue state tracker achieves 25.65% joint goal accuracy
- Best end-to-end model correctly completes user requests in 52.1% of dialogues
- Dual-modal models show modest improvement over text-only approaches (1.19% JGA gain for SPACE+WavLMalign)
- LLMs perform poorly on zero-shot inference, highlighting challenges in spoken dialogue understanding

## Why This Works (Mechanism)

### Mechanism 1
Dual-modal (text + speech) encoding improves spoken dialogue state tracking over text-only baselines by concatenating text embeddings from SPACE with speech embeddings from WavLM, then fusing them with a Transformer encoder. This allows models to access acoustic cues like disfluencies and emphasis that are not recoverable from ASR text alone. The core assumption is that speech embeddings carry complementary information to text embeddings for spoken dialogue understanding. Evidence shows dual-modal methods achieve improved performance over text-modal methods. Break condition: If speech embeddings are misaligned with text tokens or ASR noise dominates the acoustic signal, the benefit disappears.

### Mechanism 2
Explicit modeling of cross-turn and reasoning slots addresses spoken dialogue characteristics absent in written TOD datasets by introducing specialized slots that capture incomplete utterances (cross-turn) and require semantic/mathematical inference (reasoning). This forces models to learn spoken language phenomena like incremental information delivery and indirect expression. The core assumption is that spoken dialogues often require multi-turn slot value construction and inference not present in written data. Evidence shows that spoken conversations frequently involve users informing slot values across multiple turns rather than in single turns. Break condition: If the model cannot align cross-turn segments or lacks reasoning capability, joint goal accuracy remains low.

### Mechanism 3
Generative DST models outperform extractive methods on spoken data due to their ability to handle noise and reconstruct missing values by autoregressively generating dialogue states. This allows them to correct ASR errors, concatenate partial slot values across turns, and infer implicit information. The core assumption is that extractive span-based methods fail when slot values are not explicitly present or are corrupted by ASR noise. Evidence shows that extractive methods cannot handle cross-turn slots and reasoning slots as their values do not directly appear in the utterance, while generative methods can learn robustness to ASR noise. Break condition: If generative models hallucinate excessively or cannot constrain outputs to valid slot values, performance degrades.

## Foundational Learning

- Concept: ASR noise modeling
  - Why needed here: SpokenWOZ introduces real ASR errors that differ from synthetic noise; models must be robust to transcription inaccuracies
  - Quick check question: Can your model handle "lovell lodge" being transcribed as "lavelle lodge" and still recover the correct entity?

- Concept: Incremental dialogue state updates
  - Why needed here: Cross-turn slots require updating a slot value over multiple turns rather than in one shot
  - Quick check question: How does your model track partial slot values across turns without overwriting earlier segments?

- Concept: Commonsense and mathematical reasoning in dialogue
  - Why needed here: Reasoning slots require inferring implicit information (e.g., total people from "me and six friends") not explicitly stated
  - Quick check question: Can your model infer that "me and six friends" means 7 people for booking?

## Architecture Onboarding

- Component map: Audio → ASR → Text embeddings (SPACE) + Speech embeddings (WavLM) → Fusion layer (Transformer) → DST decoder (TripPy or generative SPACE) → Output slot values
- Critical path: The fusion of text and speech embeddings through the Transformer encoder is critical for capturing spoken dialogue characteristics
- Design tradeoffs: Dual-modal models gain robustness but require aligned speech-text data; generative models handle noise better but risk hallucination; extractive models are efficient but brittle to spoken phenomena
- Failure signatures: Low cross-turn slot accuracy indicates poor multi-turn tracking; low reasoning slot accuracy indicates weak inference; high hallucination in LLMs suggests prompt engineering issues
- First 3 experiments:
  1. Ablate speech embeddings (text-only) vs. full dual-modal to measure acoustic contribution
  2. Test cross-turn slot accuracy with partial vs. full slot values across turns
  3. Evaluate reasoning slot performance on temporal vs. mathematical vs. semantic inference types

## Open Questions the Paper Calls Out

- How do the reasoning slots in SpokenWOZ differ in complexity and type from the reasoning required in other spoken dialogue datasets? The paper introduces reasoning slots as a new challenge but does not provide a detailed analysis of their complexity compared to other datasets.

- What are the specific challenges posed by the cross-turn slots in SpokenWOZ, and how do they impact the performance of dialogue state tracking models? While the paper introduces cross-turn slots as a new challenge, it does not provide a detailed analysis of how these slots impact performance.

- How does the inclusion of audio data in SpokenWOZ affect the performance of dual-modal models compared to text-only models? The paper introduces dual-modal models but does not provide a detailed analysis of how audio data affects their performance.

- What are the limitations of current large language models (LLMs) in handling the unique challenges of spoken dialogue, as demonstrated in SpokenWOZ? The paper evaluates LLM performance but does not provide a detailed analysis of their limitations in spoken dialogue.

- How does the diversity of speaker origins in SpokenWOZ impact the generalizability of dialogue models trained on the dataset? The paper mentions speaker diversity but does not provide a detailed analysis of its impact on model generalizability.

## Limitations

- No human upper bound established for joint goal accuracy, making it unclear what constitutes reasonable performance
- ASR system used for transcription is not specified, affecting reproducibility and understanding of noise characteristics
- Evaluation focuses primarily on DST and end-to-end response generation, with limited analysis of other spoken dialogue phenomena
- Dataset size remains relatively small compared to some written TOD datasets, potentially limiting model generalization

## Confidence

**High Confidence**: The core claim that spoken dialogue understanding remains challenging is well-supported by experimental results showing low JGA scores (25.65%) and poor end-to-end performance (52.1%). Dataset creation methodology and evaluation metrics are clearly specified.

**Medium Confidence**: The effectiveness of dual-modal models over text-only approaches is demonstrated, but improvement margin is modest (1.19% JGA gain). Ablation studies are not comprehensive enough to fully isolate modality contributions.

**Low Confidence**: Zero-shot LLM performance comparisons are difficult to interpret due to unknown prompt engineering details and potential API version variations across the 15-month evaluation period.

## Next Checks

1. **Human Upper Bound Evaluation**: Conduct human evaluation study where annotators perform dialogue state tracking on a subset of SpokenWOZ to establish realistic performance ceilings and identify which phenomena contribute most to human error.

2. **ASR System Ablation**: Re-run experiments with different ASR systems (e.g., Whisper, commercial APIs) to quantify how transcription quality affects downstream dialogue understanding performance and identify whether speech embeddings can compensate for ASR errors.

3. **Cross-Turn Slot Trajectory Analysis**: Analyze distribution of cross-turn slot value lengths and types across turns to understand whether current generative models can effectively concatenate multi-turn information, and whether specialized architectures for incremental slot construction would improve performance.