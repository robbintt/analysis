---
ver: rpa2
title: Balanced Training for Sparse GANs
arxiv_id: '2302.14670'
source_url: https://arxiv.org/abs/2302.14670
tags:
- training
- density
- discriminator
- generator
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the balance ratio (BR) metric to quantify the
  balance between the generator and discriminator in sparse GAN training. BR measures
  how much improvement the generator can achieve relative to the discriminator's distinguishing
  power.
---

# Balanced Training for Sparse GANs

## Quick Facts
- arXiv ID: 2302.14670
- Source URL: https://arxiv.org/abs/2302.14670
- Reference count: 40
- Key outcome: DDST methods achieve better FID scores with comparable computational costs on CIFAR-10 and STL-10

## Executive Summary
This paper addresses the challenge of training sparse GANs by introducing the balance ratio (BR) metric to quantify the balance between generator and discriminator during training. BR measures how much improvement the generator can achieve relative to the discriminator's distinguishing power. Based on this metric, the authors propose double dynamic sparse training (DDST) methods that automatically adjust discriminator density and connections to maintain optimal balance. Experiments demonstrate that DDST outperforms static sparse training baselines, achieving better image quality with reduced computational costs.

## Method Summary
The authors propose two DDST methods: R-DDST (relaxed) and S-DDST (strict), which dynamically adjust discriminator density during sparse GAN training. BR is computed periodically to measure training balance, comparing generator improvement against discriminator discrimination capability. When BR exceeds upper bounds, the discriminator is strengthened by increasing density; when below lower bounds, density is decreased. R-DDST allows the discriminator to become dense, while S-DDST constrains maximum density. The generator uses dynamic sparse training with SET or RigL methods for connection optimization.

## Key Results
- S-DDST achieves FID of 11.97 on CIFAR-10 with 10% generator density, outperforming static baselines
- DDST methods consistently outperform corresponding baselines across CIFAR-10 and STL-10 datasets
- R-DDST and S-DDST maintain reasonable computational costs while improving image quality
- Optimal balance achieved with discriminator density around 50% during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BR measures how much improvement the generator can achieve relative to the discriminator's distinguishing power
- Mechanism: BR = (D(Gpost(z)) - D(Gpre(z))) / (D(xr) - D(Gpre(z))) captures the relative improvement in generator's ability to fool the discriminator compared to the baseline discriminator-vs-real image gap
- Core assumption: The ratio between generator improvement and discriminator's current discrimination capability reflects training balance
- Evidence anchors:
  - [abstract] "BR measures how much improvement the generator can achieve relative to the discriminator's distinguishing power"
  - [section] "BR measures how much improvement the generator can achieve in the scale measured by the discriminator"
- Break condition: If the generator or discriminator becomes too sparse or too dense, the BR may not accurately reflect balance due to changed network capacity

### Mechanism 2
- Claim: Dynamic density adjustment maintains balance by responding to BR
- Mechanism: DA algorithm increases discriminator density when BR > B+ (discriminator too weak) and decreases when BR < B- (discriminator too strong), automatically finding optimal density ratio
- Core assumption: BR is a reliable indicator that can guide density adjustments to maintain balance throughout training
- Evidence anchors:
  - [section] "we decreasedD by ∆d when BR is smaller thanB−, and vise versa"
  - [section] "R-DDST consistently performs better than the corresponding baselines"
- Break condition: If BR is computed too infrequently or with poor averaging, density adjustments may be too slow or noisy

### Mechanism 3
- Claim: Double dynamic sparse training (DDST) outperforms single component adjustment by maintaining balance at both parameter and density levels
- Mechanism: DDST applies DST to generator while using BR-guided DA for discriminator, allowing automatic density exploration and parameter optimization
- Core assumption: Maintaining balance through both parameter-level sparsity and density-level adjustments is more effective than single-component approaches
- Evidence anchors:
  - [abstract] "DDST outperforms baselines with reasonable computational cost on several datasets"
  - [section] "S-DDST stably surpasses its corresponding baselines regardless of grow methods and initial density"
- Break condition: If the constraint dD ≤ dmax is too restrictive, it may prevent finding optimal density even with BR guidance

## Foundational Learning

- Concept: Dynamic Sparse Training (DST)
  - Why needed here: DST enables training sparse networks from scratch while maintaining performance, which is essential for efficient GAN training
  - Quick check question: What are the three main categories of pruning methods and how does DST differ from them?

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: Understanding the adversarial nature of GAN training and the balance requirement between generator and discriminator is crucial for applying sparse training techniques
  - Quick check question: Why is maintaining balance between generator and discriminator particularly challenging in sparse GAN training compared to dense training?

- Concept: Sparse training metrics and evaluation
  - Why needed here: Understanding metrics like FID, IS, and training FLOPs is essential for evaluating the effectiveness of sparse GAN training methods
  - Quick check question: How does FID differ from IS in evaluating GAN performance, and why are both important?

## Architecture Onboarding

- Component map:
  - Generator: Sparse network with dynamic connections (via DST)
  - Discriminator: Sparse network with BR-guided dynamic density adjustment
  - BR computation: Monitors training balance using pre/post generator outputs and real/fake discrimination
  - DA controller: Adjusts discriminator density based on BR bounds

- Critical path:
  1. Initialize sparse generator and discriminator
  2. Compute BR periodically during training
  3. Adjust discriminator density based on BR
  4. Apply DST to generator connections
  5. Evaluate performance and balance

- Design tradeoffs:
  - Strict vs relaxed density constraints (S-DDST vs R-DDST)
  - BR computation frequency vs training efficiency
  - Global vs layer-wise sparse updates
  - Gradient-based vs random connection growth methods

- Failure signatures:
  - Oscillating BR values (>1.0) indicate discriminator too weak
  - Consistently low BR indicates discriminator too strong
  - Poor FID despite balanced BR may indicate insufficient network capacity

- First 3 experiments:
  1. Static sparse training with varying discriminator densities to establish baseline balance requirements
  2. SDST with fixed strong discriminator to verify generator benefit from in-time over-parameterization
  3. R-DDST with BR-guided density adjustment to test automatic balance maintenance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the balance ratio (BR) metric behave when applied to larger, more complex GAN architectures like StyleGAN or BigGAN?
- Basis in paper: [explicit] The paper applies BR to SNGAN and BigGAN on CIFAR-10 and STL-10 datasets, but does not explore its behavior on more advanced GAN architectures.
- Why unresolved: The paper only tests BR on relatively simple GAN architectures, leaving its applicability to larger, more complex models unexplored.
- What evidence would resolve it: Experiments showing BR's behavior and effectiveness when applied to advanced GAN architectures like StyleGAN or BigGAN.

### Open Question 2
- Question: Can the balance ratio (BR) metric be extended to quantify balance in other types of generative models beyond GANs, such as variational autoencoders (VAEs) or normalizing flows?
- Basis in paper: [inferred] The paper introduces BR as a metric for GANs, but does not discuss its potential application to other generative models.
- Why unresolved: The paper focuses solely on GANs and does not explore whether BR can be adapted for other generative model architectures.
- What evidence would resolve it: Research demonstrating the effectiveness of BR in quantifying balance for other generative models like VAEs or normalizing flows.

### Open Question 3
- Question: What are the theoretical guarantees or limitations of the balance ratio (BR) metric in capturing the true balance between generator and discriminator in sparse GAN training?
- Basis in paper: [inferred] The paper introduces BR as a heuristic metric but does not provide theoretical analysis of its properties or limitations.
- Why unresolved: The paper does not offer a rigorous mathematical foundation for BR or discuss its potential shortcomings.
- What evidence would resolve it: A theoretical analysis of BR's properties, including its relationship to GAN training dynamics and potential failure modes.

### Open Question 4
- Question: How sensitive is the performance of double dynamic sparse training (DDST) to the choice of hyperparameters, such as the BR bounds [B-, B+] and density increment ∆d?
- Basis in paper: [explicit] The paper uses specific BR bounds and density increments for DDST, but does not explore the sensitivity of results to these choices.
- Why unresolved: The paper does not provide an ablation study or sensitivity analysis for DDST's hyperparameters.
- What evidence would resolve it: Experiments showing the impact of varying BR bounds and density increments on DDST's performance across different datasets and GAN architectures.

## Limitations
- BR metric reliability depends on accurate computation and appropriate averaging across sparse training scenarios
- Theoretical understanding of why specific density ranges ([0.45, 0.55]) work best remains limited
- Computational overhead of BR computation and density adjustment requires more detailed analysis across hardware configurations

## Confidence
- **High confidence**: The empirical results showing DDST outperforming baselines on CIFAR-10 and STL-10 datasets, as these are directly measured and reproducible
- **Medium confidence**: The BR metric's effectiveness in capturing training balance, as it relies on specific assumptions about the relationship between generator improvement and discriminator discrimination power
- **Medium confidence**: The claim that S-DDST achieves comparable computational costs to baselines, as detailed FLOPs analysis is limited

## Next Checks
1. **BR robustness test**: Evaluate BR metric stability when applied to different sparse training methods beyond DST (e.g., magnitude pruning, lottery ticket) to verify its general applicability as a balance indicator

2. **Density range sensitivity analysis**: Systematically test BR-guided density adjustment across a wider range of bounds ([0.3, 0.7], [0.2, 0.8]) to determine optimal ranges and verify the claimed effectiveness of [0.45, 0.55]

3. **Computational overhead benchmarking**: Measure the actual training time and memory overhead introduced by BR computation and density adjustment across different batch sizes and hardware configurations to validate the "reasonable computational cost" claim