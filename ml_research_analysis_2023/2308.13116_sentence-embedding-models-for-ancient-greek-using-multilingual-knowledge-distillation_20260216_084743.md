---
ver: rpa2
title: Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation
arxiv_id: '2308.13116'
source_url: https://arxiv.org/abs/2308.13116
tags:
- greek
- sentence
- ancient
- english
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating sentence embedding
  models for Ancient Greek, a historical language with limited training data compared
  to modern, high-resource languages. The authors propose using a multilingual knowledge
  distillation approach, leveraging existing sentence embedding models for English
  and training a student model to produce aligned embeddings for both Ancient Greek
  and English.
---

# Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation

## Quick Facts
- arXiv ID: 2308.13116
- Source URL: https://arxiv.org/abs/2308.13116
- Reference count: 8
- Models trained using multilingual knowledge distillation outperform contrastive learning baselines for Ancient Greek sentence embeddings

## Executive Summary
This paper addresses the challenge of creating sentence embedding models for Ancient Greek, a historical language with limited training data compared to modern, high-resource languages. The authors propose using a multilingual knowledge distillation approach, leveraging existing sentence embedding models for English and training a student model to produce aligned embeddings for both Ancient Greek and English. This approach allows the student model to inherit the vector space properties of the teacher model while requiring a relatively small amount of parallel sentence data. The authors create a parallel dataset of Ancient Greek sentences and their English translations using a two-step translation alignment process and evaluate their models on translation search, semantic similarity, and semantic retrieval tasks.

## Method Summary
The approach uses multilingual knowledge distillation where an Ancient Greek/English student model learns to mimic the embeddings of an English-only teacher model using parallel sentence data. The student model is first fine-tuned on Ancient Greek text, then trained to minimize mean squared error between its embeddings for Ancient Greek sentences and the teacher model's embeddings for corresponding English translations. Parallel data is generated through translation alignment using Hunalign and Bertalign on document-level alignments from the Perseus Digital Library and First1KGreek corpus.

## Key Results
- Models trained via knowledge distillation outperform baseline models trained using contrastive learning
- Strong performance on translation search and semantic similarity tasks
- Models exhibit some bias towards certain translation styles and struggle with semantic retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual knowledge distillation allows the student model to inherit vector space properties from a high-resource language teacher model.
- Mechanism: The student model learns to minimize mean squared error between its embeddings for Ancient Greek sentences and the teacher model's embeddings for corresponding English translations.
- Core assumption: Translated sentences preserve sufficient semantic equivalence for the teacher model's vector space to be transferable to Ancient Greek.
- Evidence anchors:
  - [abstract] "our distillation approach allows our Ancient Greek models to inherit the properties of these models while using a relatively small amount of translated sentence data"
  - [section 2.2] "we train a student model ˆM to mimic the sentence embeddings of the teacher M such that ˆM (si) ≈ M (ti) and ˆM (ti) ≈ M (si)"
- Break condition: If translations are semantically divergent from source sentences, the student model will learn incorrect vector space mappings.

### Mechanism 2
- Claim: Contrastive learning effectively trains multilingual sentence embeddings by pulling translation pairs together and pushing non-pairs apart.
- Mechanism: Using dropout as noise and treating Greek-English translation pairs as positive examples while other pairs in the batch serve as negatives.
- Core assumption: The contrastive objective will create semantically meaningful embeddings even with limited Ancient Greek data.
- Evidence anchors:
  - [section 2.3] "Contrastive learning pulls semantically-close neighbors together and pushes apart non-neighbors, and has been shown to be effective for training multilingual sentence embeddings"
  - [section 2.3] "we use each Greek sentence and its English translation as positive pairs and other pairs in the same batch as negatives"
- Break condition: If the dataset lacks sufficient semantic diversity, the contrastive model may learn to simply map all sentences to a single point.

### Mechanism 3
- Claim: Translation alignment using sentence embeddings achieves high-quality parallel sentence pairs from document-level alignments.
- Mechanism: Using an initial dictionary-based alignment to create a small parallel dataset, then training an intermediate model to improve alignment quality using Bertalign.
- Core assumption: Sentence embeddings can accurately capture semantic similarity to enable high-precision alignment.
- Evidence anchors:
  - [section 3.3] "Using this initial dataset, we trained a sentence embedding model with an aligned vector space for English and Ancient Greek using SimCSE"
  - [section 3.3] "we use this model to align all the texts again, using a better alignment method introduced by Liu and Zhu (2023), dubbed Bertalign"
- Break condition: If sentence embeddings poorly capture semantic equivalence, alignment quality will degrade and introduce noise into training data.

## Foundational Learning

- Concept: Knowledge distillation in neural networks
  - Why needed here: Understanding how the student model learns to mimic the teacher's embeddings is crucial for debugging training issues
  - Quick check question: What loss function is minimized during multilingual knowledge distillation?

- Concept: Contrastive learning objectives
  - Why needed here: SimCSE serves as a baseline and understanding its mechanics helps interpret performance differences
  - Quick check question: How does SimCSE create positive pairs when no explicit translations are used?

- Concept: Sentence embedding evaluation metrics
  - Why needed here: Different evaluation tasks (translation search, STS, semantic retrieval) require understanding appropriate metrics
  - Quick check question: What is the difference between recall@10 and mean average precision?

## Architecture Onboarding

- Component map: Teacher model (English sentence transformer) → Student model (Ancient Greek/English BERT) → Mean pooling layer → Training loop with MSE loss
- Critical path: Parallel sentence data generation → Model training → Evaluation on three tasks
- Design tradeoffs: Using translation alignment introduces potential bias but enables training with limited data; using Modern Greek pretraining helps but may introduce anachronisms
- Failure signatures: Poor translation search accuracy indicates vector space misalignment; low STS correlation suggests teacher-student gap; bad semantic retrieval shows poor semantic understanding
- First 3 experiments:
  1. Train with only human-aligned data to establish upper bound performance
  2. Train with different teacher models (mpnet vs st5) to measure impact on downstream tasks
  3. Evaluate on held-out parallel pairs to monitor overfitting during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the sentence tokenizer affect the performance of the models, and can a better tokenizer improve the results?
- Basis in paper: [inferred] The paper mentions that the mBERT and XLM-R tokenizers are not optimized for Ancient Greek morphology, which could negatively impact performance.
- Why unresolved: The paper does not explore the effect of using different tokenizers on the model performance.
- What evidence would resolve it: Comparing the performance of models using different tokenizers on the same tasks would provide insights into the impact of tokenizer quality.

### Open Question 2
- Question: What is the impact of training on Modern Greek data before Ancient Greek data on the model performance, and why does it lead to better results?
- Basis in paper: [explicit] The paper mentions that training on Modern Greek data before Ancient Greek data improves the performance of all models.
- Why unresolved: The paper does not investigate the reasons behind this improvement or explore the optimal amount of Modern Greek data to use.
- What evidence would resolve it: Conducting experiments with varying amounts of Modern Greek data and analyzing the resulting model performance would help understand the impact and potential reasons for the improvement.

### Open Question 3
- Question: How does the choice of teacher model (mpnet vs. st5) affect the performance of the student models on different tasks, and why is mpnet better for semantic retrieval?
- Basis in paper: [explicit] The paper shows that models with the mpnet teacher perform better on semantic retrieval tasks compared to models with the st5 teacher.
- Why unresolved: The paper does not investigate the reasons behind the difference in performance or explore the characteristics of the teacher models that lead to this difference.
- What evidence would resolve it: Analyzing the properties of the teacher models and conducting experiments with different teacher models would help understand the factors influencing the student model performance.

## Limitations
- Translation alignment introduces potential bias and uncertainty about semantic equivalence
- Poor semantic retrieval performance suggests limited cross-lingual semantic understanding
- Heavy dependence on quality and biases of English teacher models

## Confidence
- High confidence: Multilingual knowledge distillation effectively trains sentence embeddings for low-resource languages
- Medium confidence: Approach allows student models to inherit teacher properties, though selectively
- Low confidence: Approach enables deep semantic understanding of Ancient Greek text

## Next Checks
1. Manually evaluate a random sample of automatically aligned parallel sentences to quantify semantic divergence
2. Systematically evaluate student models with different English teacher models on all three tasks
3. Design evaluation that directly tests whether learned embeddings capture Ancient Greek semantics independent of English translations