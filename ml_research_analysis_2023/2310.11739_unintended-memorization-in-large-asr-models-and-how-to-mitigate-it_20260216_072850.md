---
ver: rpa2
title: Unintended Memorization in Large ASR Models, and How to Mitigate It
arxiv_id: '2310.11739'
source_url: https://arxiv.org/abs/2310.11739
tags:
- training
- memorization
- clipping
- examples
- per-example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new method to audit unintended memorization
  in large ASR models using sped-up utterances, which are difficult to learn from
  normal training data. This approach avoids the high computational cost of existing
  methods like hardness calibration.
---

# Unintended Memorization in Large ASR Models, and How to Mitigate It

## Quick Facts
- arXiv ID: 2310.11739
- Source URL: https://arxiv.org/abs/2310.11739
- Authors: 
- Reference count: 0
- Key outcome: Gradient clipping during training can effectively reduce unintended memorization in large ASR models while maintaining model quality.

## Executive Summary
This paper addresses unintended memorization in large non-auto-regressive ASR models by introducing a novel auditing framework using sped-up utterances and demonstrating gradient clipping as an effective mitigation strategy. The authors show that accurate transcription of 4x-sped-up training examples is clear evidence of memorization since such utterances are difficult to learn from normal training data. Experiments with per-example and per-core gradient clipping demonstrate significant reduction in memorization while maintaining model utility, with per-core clipping offering computational efficiency for large-scale distributed training.

## Method Summary
The authors fine-tune pre-trained BEST-RQ Conformer models on LibriSpeech while inserting sped-up canary utterances generated with WaveNet TTS. These canaries are sped up 4x and have transcripts from the top 10,000 words in LibriSpeech. The auditing framework measures memorization using exposure metrics (log 2 |{ri}| - log 2 rankM(c, {ri})), while gradient clipping (both per-example and per-core) is applied during training to bound the influence of individual examples. Per-core clipping is presented as an efficient approximation of per-example clipping for distributed training scenarios.

## Key Results
- Accurate transcription of 4x-sped-up training examples indicates memorization rather than generalization
- Per-example gradient clipping effectively reduces memorization for examples repeated up to 16 times
- Per-core gradient clipping maintains neutral model quality and compute cost while providing strong privacy protection
- Memorization robustness of per-core clipping decreases as per-core batch size increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sped-up utterances create a domain where normal training examples provide insufficient generalization signal, forcing memorization as the only path to accurate transcription.
- Mechanism: By altering the temporal structure of speech beyond typical human utterance patterns, the mapping from audio to text becomes uniquely difficult for standard ASR training. Since the model cannot learn this mapping from regular examples, accurate transcription of sped-up training examples directly indicates memorization.
- Core assumption: Normal ASR training data does not contain sufficient examples of sped-up speech for the model to generalize effectively.
- Evidence anchors:
  - [abstract]: "we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples"
  - [section 2.3]: "we hardly expect to encounter them in human speech and consequently ASR training data so the model can only memorize them for accurate transcription"
- Break condition: If training data includes naturally occurring or artificially generated sped-up speech, the model could generalize instead of memorizing.

### Mechanism 2
- Claim: Per-example gradient clipping bounds the influence of any individual training example, preventing extreme memorization while maintaining utility.
- Mechanism: Clipping each example's gradient to a fixed L2 norm bound ensures no single example can disproportionately affect the model parameters. This prevents the model from memorizing rare or repeated examples while allowing normal learning from the general distribution.
- Core assumption: Memorization occurs when individual examples have outsized influence on parameter updates, and bounding this influence prevents memorization.
- Evidence anchors:
  - [abstract]: "we tried gradient clipping during training to bound the influence of any individual example on the final model"
  - [section 3.1]: "with per-example clipping, how much an individual example can influence the final model is bounded, and thus the final model should not memorize too much about any training example"
- Break condition: If clipping bound is too large or too small, either memorization persists or model utility is severely degraded.

### Mechanism 3
- Claim: Per-core gradient clipping achieves similar memorization mitigation to per-example clipping without the computational overhead of materializing individual gradients.
- Mechanism: In distributed training, each compute core naturally maintains the average gradient for all examples on that core. By clipping these per-core averages instead of individual example gradients, the method approximates per-example clipping's privacy benefits while avoiding the computational cost.
- Core assumption: Per-core averaging and clipping provides sufficient influence bounding to prevent memorization while being computationally efficient.
- Evidence anchors:
  - [abstract]: "clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection"
  - [section 3.2]: "Per-core clipping is a special instantiation of micro-batch clipping under the scenario of large-scale distributed training"
- Break condition: If per-core batch size becomes too large, the averaging effect may allow memorization of examples that occur frequently within the same core.

## Foundational Learning

- Concept: Gradient clipping in optimization
  - Why needed here: Understanding how gradient clipping works is essential to grasp why per-example and per-core clipping can mitigate memorization while maintaining model utility.
  - Quick check question: What happens to the direction of the gradient when it is clipped to a fixed norm, and how does this affect the optimization trajectory?

- Concept: Differentially private machine learning
  - Why needed here: The paper positions gradient clipping as a component of differential privacy training, and understanding DP helps contextualize the privacy guarantees being sought.
  - Quick check question: How does adding noise after gradient clipping in DP training provide formal privacy guarantees that simple clipping alone does not?

- Concept: Non-auto-regressive ASR model architecture
  - Why needed here: The paper specifically addresses challenges unique to non-auto-regressive ASR models, distinguishing them from auto-regressive models where memorization is easier to audit.
  - Quick check question: What architectural differences between non-auto-regressive and auto-regressive ASR models make distinguishing memorization from generalization particularly challenging?

## Architecture Onboarding

- Component map: Pre-trained BEST-RQ encoder -> Fine-tuning pipeline with optional gradient clipping -> Memorization auditing with sped-up canaries -> Evaluation metrics
- Critical path: Training → Gradient clipping (optional) → Fine-tuning → Memorization auditing with sped-up canaries → Evaluation
- Design tradeoffs: Per-example clipping provides stronger privacy but slower training; per-core clipping offers a middle ground; no clipping maximizes speed but allows memorization.
- Failure signatures: High exposure scores with low WER improvements suggest memorization; large WER degradation with clipping suggests the bound is too restrictive; unchanged exposure with clipping suggests insufficient bound strength.
- First 3 experiments:
  1. Train with standard fine-tuning and measure exposure on sped-up canaries to establish baseline memorization.
  2. Apply per-example clipping with varying L2 bounds and measure exposure and WER to find the optimal tradeoff.
  3. Apply per-core clipping with different per-core batch sizes to evaluate the impact on memorization robustness and model utility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal per-core batch size for balancing memorization robustness and utility in large-scale distributed training?
- Basis in paper: [explicit] The paper notes that "memorization robustness of per-core clipping decreases as per-core batch size increases" but doesn't specify an optimal size
- Why unresolved: The paper only tests per-core batch size of 4 and shows it works well, but doesn't explore the trade-off curve across different batch sizes
- What evidence would resolve it: Systematic experiments varying per-core batch size from 1 to 32+ while measuring both exposure and WER would identify the optimal trade-off point

### Open Question 2
- Question: How does gradient clipping effectiveness vary across different ASR model architectures and pre-training methods?
- Basis in paper: [explicit] Experiments are limited to Conformer models with BEST-RQ pre-training, but the paper mentions "curse of dimensionality" for DP training on large ASR models
- Why unresolved: The paper only tests two Conformer model sizes with one specific pre-training method, leaving open whether results generalize to other architectures like RNN-T or hybrid models
- What evidence would resolve it: Testing gradient clipping on diverse ASR architectures (RNN-T, hybrid, attention-based) with various pre-training methods (CPC, Wav2Vec, HuBERT) would reveal architecture-specific effectiveness

### Open Question 3
- Question: What is the relationship between training duration and memorization resilience under gradient clipping?
- Basis in paper: [inferred] The paper shows clipping effectiveness at 20K fine-tuning steps but doesn't examine longer training durations or different learning rate schedules
- Why unresolved: The experiments stop at 20K steps, but longer training might reveal saturation effects or diminishing returns of clipping, or interactions with learning rate decay
- What evidence would resolve it: Extending experiments to 50K+ steps with various learning rate schedules while monitoring exposure over time would reveal temporal dynamics of clipping effectiveness

### Open Question 4
- Question: How do different types of privacy-sensitive information (speaker identity, content, metadata) respond differently to gradient clipping?
- Basis in paper: [explicit] The paper focuses on content memorization via sped-up utterances but references work showing speaker identity can leak differently
- Why unresolved: The paper only measures exposure for textual content, but different types of private information may have different gradients and respond differently to clipping bounds
- What evidence would resolve it: Designing canaries for different information types (speaker embeddings, metadata, content) and measuring their exposure under various clipping configurations would reveal differential vulnerabilities

## Limitations

- Narrow experimental scope limited to LibriSpeech/LibriLight datasets and Conformer BEST-RQ models
- Gradient clipping implementation details remain underspecified (before/after averaging, handling of zero gradients)
- Privacy analysis lacks formal differential privacy guarantees and quantitative tradeoff analysis

## Confidence

- **High** confidence in the memorization auditing mechanism - novel approach with clear empirical evidence
- **Medium** confidence in gradient clipping mitigation strategy - theoretical foundation solid but limited hyperparameter exploration
- **Medium** confidence in generalizability across architectures and domains - limited experimental scope

## Next Checks

1. **Generalization testing**: Apply the sped-up auditing framework to alternative ASR architectures (RNN-T, hybrid models) and datasets (conversational speech, multilingual corpora) to verify the memorization detection mechanism works across different model families and domains.

2. **Formal privacy analysis**: Implement the gradient clipping with formal differential privacy accounting (Rényi DP or moments accountant) to quantify the actual privacy guarantees provided by per-example and per-core clipping strategies across various L2 norm bounds.

3. **Speedup factor sensitivity**: Systematically vary the speedup factor (2x, 4x, 8x, 16x) and add different types of audio perturbations to determine how the difficulty threshold affects memorization detection and whether the 4x factor is optimal or domain-dependent.