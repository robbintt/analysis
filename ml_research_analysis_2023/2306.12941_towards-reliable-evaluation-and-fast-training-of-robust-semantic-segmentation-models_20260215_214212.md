---
ver: rpa2
title: Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation
  Models
arxiv_id: '2306.12941'
source_url: https://arxiv.org/abs/2306.12941
tags:
- robust
- segmentation
- attacks
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses adversarial robustness in semantic segmentation,
  where existing attacks and defenses have been shown to overestimate model robustness.
  It introduces novel loss functions (masked CE, balanced CE, JS divergence, masked
  spherical) and improves attack optimization via an adaptive "radius reduction" scheme.
---

# Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models

## Quick Facts
- arXiv ID: 2306.12941
- Source URL: https://arxiv.org/abs/2306.12941
- Authors: 
- Reference count: 40
- The paper addresses adversarial robustness in semantic segmentation, where existing attacks and defenses have been shown to overestimate model robustness. It introduces novel loss functions (masked CE, balanced CE, JS divergence, masked spherical) and improves attack optimization via an adaptive "radius reduction" scheme. An ensemble attack, SEA, combining these components yields stronger and more reliable evaluations than prior methods. The work also demonstrates that using pre-trained robust ImageNet backbones (ConvNeXt, ViT) enables training adversarially robust segmentation models with significantly reduced computational cost and improved performance on both PASCAL-VOC and ADE20K datasets. Robust models achieve up to 6x training efficiency gains while maintaining competitive clean accuracy.

## Executive Summary
This paper tackles the challenge of reliable adversarial robustness evaluation and efficient training in semantic segmentation. The authors identify that existing attack methods often overestimate model robustness, particularly when using fixed-radius ℓ∞ attacks on complex segmentation loss surfaces. To address this, they propose an adaptive "radius reduction" scheme and introduce novel loss functions (masked cross-entropy, balanced cross-entropy, JS divergence, and masked spherical) that improve attack optimization by focusing on misclassified pixels and avoiding gradient vanishing. Their Segmentation Ensemble Attack (SEA) combines these components to provide more comprehensive and reliable robustness evaluations. Additionally, they demonstrate that initializing segmentation models with pre-trained robust ImageNet backbones (ConvNeXt, ViT) significantly accelerates adversarial training while maintaining or improving both clean accuracy and adversarial robustness.

## Method Summary
The authors introduce SEA (Segmentation Ensemble Attack), which combines four complementary loss functions—masked cross-entropy, balanced cross-entropy, JS divergence, and masked spherical loss—with APGD optimization using an adaptive radius reduction scheme. The radius reduction starts with 2·ε, then reduces to 1.5·ε, and finally to ε over three optimization slots (3:3:4 ratio). For efficient training, they leverage robust pre-trained ImageNet backbones (ConvNeXt, ViT) to initialize segmentation models, achieving up to 6× speedup compared to training from scratch. The UPerNet architecture with these backbones is trained using AT2/AT5 schemes with PGD attacks. Evaluation uses SEA on PASCAL-VOC and ADE20K datasets, measuring average pixel accuracy and mIoU.

## Key Results
- SEA attack achieves 3.2% higher worst-case accuracy drop compared to standard PGD on robust models
- Robust ImageNet pre-training enables 6× training efficiency gains while maintaining competitive clean accuracy and adversarial robustness
- ConvNeXt-S backbone with robust initialization achieves better clean performance and marginally higher robustness than ConvNeXt-T
- Adaptive radius reduction scheme outperforms simply increasing attack iterations in finding adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
Adaptive radius reduction in APGD improves segmentation attack success more than simply increasing iterations. The algorithm starts with a larger ℓ∞ radius (2·ε) to overcome local minima from joint pixel optimization, then progressively shrinks to the target radius. This provides better gradient information for hard cases. Core assumption: Semantic segmentation loss surfaces are more complex than classification, causing optimization stagnation at fixed radius. Evidence anchors: [abstract] "adaptive 'radius reduction' scheme" and "improved attack optimization"; [section 4.4] "we split the budget of iterations into three slots (with ratio 3 : 3 : 4 ) where we run the attack with 2 · ϵ∞, 1.5 · ϵ∞ and ϵ∞ respectively."; [corpus] No direct evidence, but related work on adaptive attacks suggests this is plausible. Break condition: If the loss surface is actually smooth or if per-pixel independence is valid, fixed radius APGD with more iterations could match or exceed red-ϵ performance.

### Mechanism 2
Masked loss functions (e.g., LMask-CE, LMask-Sph.) focus attack optimization on misclassified pixels and avoid wasting effort on already-flipped ones. By masking out pixels already misclassified, the gradient only updates correctly classified pixels, preventing "overshooting" where correct predictions become incorrect again. Core assumption: Each pixel's classification is independent and can be optimized separately without cross-pixel interference. Evidence anchors: [abstract] "novel loss functions (masked CE, balanced CE, JS divergence, masked spherical)"; [section 4.1] "LMask-CE = I(arg max j=1,...,K uj = y) · LCE(u, y)" and "LMask-Sph. = −I(arg max j=1,...,K uj = y) · uy/ ∥u∥2"; [corpus] Related work on masking in classification attacks supports this idea. Break condition: If pixel interactions are significant (e.g., spatial consistency), masking could miss useful gradient signals from misclassified neighbors.

### Mechanism 3
Robust pre-trained ImageNet backbones (ConvNeXt, ViT) provide a strong initialization that accelerates adversarial training convergence for segmentation models. Transfer learning from robust classifiers injects learned robustness into the segmentation backbone, reducing the number of epochs needed to achieve comparable adversarial robustness. Core assumption: Robustness is largely a property of the feature extractor/backbone, and segmentation decoders can learn robust representations on top. Evidence anchors: [abstract] "using pre-trained robust ImageNet backbones (ConvNeXt, ViT) enables training adversarially robust segmentation models with significantly reduced computational cost"; [section 5.2] "By initializing the backbone of our segmentation model with a robust ConvNeXt, adversarially pre-trained on IMAGE NET, we achieve similar or better adversarial robustness at up to 6 times lower computational cost"; [corpus] Recent work on robust ImageNet classifiers (Singh et al., 2023) supports this assumption. Break condition: If robustness depends heavily on the decoder architecture or if adversarial training requires dataset-specific fine-tuning, pre-training transfer may fail or yield marginal gains.

## Foundational Learning

- Concept: Projected Gradient Descent (PGD) and its variants (APGD)
  - Why needed here: Core optimization method for generating adversarial examples under ℓ∞ constraint.
  - Quick check question: What is the key difference between PGD and APGD in terms of step size selection?

- Concept: Cross-entropy loss and its variants (balanced, masked, spherical)
  - Why needed here: Objective functions to guide adversarial attacks; different formulations affect which pixels are prioritized.
  - Quick check question: Why does masking misclassified pixels help avoid gradient vanishing in ℓ∞ attacks?

- Concept: Intersection over Union (IoU) and average pixel accuracy
  - Why needed here: Standard metrics for semantic segmentation performance; attacks aim to minimize these.
  - Quick check question: If all pixels are misclassified, what happens to IoU and why is it a harder objective than pixel accuracy?

## Architecture Onboarding

- Component map: APGD optimizer with selectable loss functions (LCE, LBal-CE, LJS, LMask-CE, LMask-Sph.) -> SEA orchestrator that runs multiple attacks and selects worst-case result -> UPerNet architecture with ConvNeXt backbone; adversarial training loop with PGD -> SEA attack applied to validation set; metrics: ACC, mIoU
- Critical path: Generate adversarial examples → Evaluate model robustness → (Optional) Fine-tune with robust backbone → Retest
- Design tradeoffs:
  - More attack iterations vs. radius reduction: red-ϵ often outperforms extra iterations
  - Masked vs. unmasked losses: masked losses focus effort but may ignore useful gradients
  - Clean vs. robust initialization: robust init. saves time but may limit final accuracy
- Failure signatures:
  - Attacks plateau early → consider red-ϵ or loss switching
  - Clean accuracy drops sharply → over-regularization or aggressive adversarial examples
  - Robustness gap between attacks → SEA not comprehensive enough
- First 3 experiments:
  1. Compare PGD vs. APGD on a simple loss (LCE) for a clean model; measure ACC drop at ϵ=4/255.
  2. Run SEA (all four losses) vs. single best loss on robust model; report worst-case ACC difference.
  3. Train with clean init. vs. robust init. (AT2, 50 epochs); compare ACC/mIoU at ϵ=4/255 and training time.

## Open Questions the Paper Calls Out

### Open Question 1
How would incorporating pixel-wise interaction losses or direct mIOU optimization impact the effectiveness of adversarial attacks on semantic segmentation models? Basis in paper: [explicit] The authors mention this as a potential future direction: "This may open new research directions, for example for losses which take into account the interaction of neighboring pixels or directly target mIOU to achieve stronger attacks." Why unresolved: The paper focuses on existing loss functions and optimization techniques but does not explore these specific extensions. Testing these would require developing new loss functions and evaluating their performance against current methods. What evidence would resolve it: Empirical comparison showing whether pixel-wise interaction losses or direct mIOU optimization outperform the current SEA attack in terms of reducing model accuracy and mIOU under adversarial perturbations.

### Open Question 2
What is the impact of using larger robust backbones (e.g., ConvNeXt-S vs ConvNeXt-T) on the trade-off between clean accuracy and adversarial robustness in semantic segmentation? Basis in paper: [explicit] The authors observe that ConvNeXt-S backbone leads to better clean performance and marginally higher robustness compared to ConvNeXt-T, but note that "the improvement could be much more if one increases the number of training epochs for these bigger models." Why unresolved: The paper only tests ConvNeXt-S with the same training epochs as ConvNeXt-T, leaving the question of whether longer training would yield significantly better robustness. What evidence would resolve it: Experiments training ConvNeXt-S models for extended epochs and comparing their robust accuracy and mIoU against ConvNeXt-T models trained for the same duration.

### Open Question 3
How does the stability and performance of SEA vary when evaluated against black-box attacks compared to white-box attacks? Basis in paper: [explicit] The authors acknowledge that "PGD-based attacks should be complemented by white-box attacks of different type and especially black-box methods," suggesting this is an open area for evaluation. Why unresolved: The paper primarily evaluates SEA using white-box attacks and does not test its robustness against black-box methods, which are crucial for real-world scenarios. What evidence would resolve it: Comparative analysis showing SEA's performance against both white-box and black-box attacks, demonstrating whether it maintains effectiveness under different attack paradigms.

## Limitations

- The effectiveness of SEA relies on empirical comparison rather than theoretical guarantees that the four chosen losses are exhaustive
- The 6× training speedup with robust pre-training may not generalize to different segmentation architectures or training protocols beyond ConvNeXt and AT2/AT5 schemes
- The masking mechanism assumes pixel independence, but the paper provides limited analysis of spatial correlations or when masking might actually harm attack effectiveness

## Confidence

- SEA as superior evaluation method: High confidence based on empirical comparison showing 3.2% higher worst-case accuracy drop
- Robust ImageNet pre-training accelerates training: Medium confidence; results depend heavily on specific architecture and training scheme
- Masked loss functions improve attack optimization: Medium confidence; assumes pixel independence with limited analysis of spatial correlations

## Next Checks

1. **Ablation on loss diversity**: Remove one loss function at a time from SEA and measure the impact on worst-case performance. This would test whether all four losses are truly complementary.

2. **Cross-architecture backbone transfer**: Train with robust pre-training on architectures beyond ConvNeXt (e.g., Swin, SegFormer) to verify the 6× speedup generalizes across different design paradigms.

3. **Masking sensitivity analysis**: Systematically vary the masking threshold and analyze attack success rates on spatially correlated vs. independent pixel patterns to understand when masking helps or hurts.