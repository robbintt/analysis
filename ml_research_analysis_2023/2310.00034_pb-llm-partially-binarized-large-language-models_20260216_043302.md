---
ver: rpa2
title: 'PB-LLM: Partially Binarized Large Language Models'
arxiv_id: '2310.00034'
source_url: https://arxiv.org/abs/2310.00034
tags:
- weights
- quantization
- binarization
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Partially-Binarized LLM (PB-LLM), a novel
  method for compressing large language models (LLMs) through extreme low-bit quantization.
  The approach addresses the challenge of applying traditional binarization techniques
  to LLMs, which often result in significant performance degradation.
---

# PB-LLM: Partially Binarized Large Language Models

## Quick Facts
- arXiv ID: 2310.00034
- Source URL: https://arxiv.org/abs/2310.00034
- Authors: [Not specified in input]
- Reference count: 7
- Key outcome: PB-LLM achieves competitive LLM quantization performance with 10% weights in higher precision and 90% binarized, maintaining reasoning capabilities.

## Executive Summary
This paper introduces PB-LLM, a novel method for compressing large language models through extreme low-bit quantization by selectively preserving salient weights in higher precision. The approach addresses the challenge that traditional binarization techniques fail on LLMs, causing significant performance degradation. PB-LLM explores both post-training quantization (PTQ) and quantization-aware training (QAT) frameworks, using GPTQ-inspired reconstruction guided by the Hessian matrix for PTQ and frozen salient weights with optimal scaling for QAT. Experimental results on LLaMA-7B demonstrate that PB-LLM maintains competitive performance compared to state-of-the-art quantization techniques while requiring significantly fewer training iterations.

## Method Summary
PB-LLM implements a partially-binarized approach that filters a small ratio of salient weights during binarization, allocating them to higher-bit storage. The method detects salient weights using magnitude or Hessian criteria, then applies element-wise selection rather than column-wise to avoid performance impairment. Under PTQ, PB-LLM uses GPTQ-inspired reconstruction with Hessian-guided compensation to recover reasoning capacity. For QAT, salient weights are frozen before binarization, and optimal scaling factors are derived analytically to minimize quantization error for the remaining binarized weights. The approach is evaluated on LLaMA-7B with 10% of weights preserved in 8-bit precision and 90% binarized, achieving a 2.7-bit quantization.

## Key Results
- PB-LLM maintains competitive zero-shot performance on common sense reasoning tasks (BoolQ, PIQA, HellaSwag, WinoGrante, ARC-Easy, ARC-Challenge, OBQA) with only 10% weights in higher precision
- The method achieves perplexity scores competitive with state-of-the-art quantization techniques on WikiText2 and C4 while requiring significantly fewer training iterations
- PB-LLM demonstrates potential for extreme low-bit quantization, suggesting feasibility of 1-bit quantization while preserving essential LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preserving a small fraction of salient weights in higher precision enables extreme low-bit quantization of LLMs without catastrophic performance loss.
- **Mechanism:** By identifying and preserving weights that contribute most to model capacity (via magnitude or Hessian criteria), the model retains essential linguistic reasoning capabilities even when the remaining weights are binarized.
- **Core assumption:** Salient weights are distributed uniformly and randomly across the weight matrix, requiring element-wise rather than column-wise selection.
- **Evidence anchors:**
  - [abstract] "PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization."
  - [section] "Our investigations reveal that adopting a column-wise approach for selecting salient weights has the potential to impair the performance of binarization... necessitates an element-wise approach for effective filtration in the binarization process."
  - [corpus] Weak evidence - no directly relevant corpus entries discussing this specific mechanism.
- **Break condition:** If salient weights are not uniformly distributed or if the storage overhead for preserving them exceeds acceptable limits.

### Mechanism 2
- **Claim:** GPTQ-inspired reconstruction guided by the Hessian matrix can recover the reasoning capacity of partially binarized LLMs under post-training quantization.
- **Mechanism:** Iterative quantization of un-salient weights combined with compensation factors calculated using the Hessian matrix preserves model performance by minimizing layer-wise quantization error.
- **Core assumption:** The Hessian matrix accurately captures the sensitivity of quantization error to weight perturbations, enabling effective compensation.
- **Evidence anchors:**
  - [abstract] "Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit."
  - [section] "We propose to use GPTQ to iteratively binarize the un-salient weights and quantize the salient weights to higher bit, and then apply the compensation to the remaining weights."
  - [corpus] Weak evidence - no directly relevant corpus entries discussing Hessian-guided reconstruction for binarization.
- **Break condition:** If the Hessian approximation becomes inaccurate for large-scale LLMs or if compensation factors cannot be computed efficiently.

### Mechanism 3
- **Claim:** Freezing salient weights during quantization-aware training and using optimal scaling factors for binary weights significantly improves training efficiency and performance recovery.
- **Mechanism:** Salient weights are frozen to preserve pre-trained knowledge, while optimal scaling factors derived analytically minimize quantization error for binarized weights.
- **Core assumption:** Pre-trained salient weights contain critical information that should not be disturbed during quantization-aware training.
- **Evidence anchors:**
  - [abstract] "Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights."
  - [section] "To leverage the value of pre-trained weights, we propose freezing the salient weights, determined by weight magnitude, prior to the weight binarization process."
  - [corpus] Weak evidence - no directly relevant corpus entries discussing frozen salient weights in QAT.
- **Break condition:** If frozen weights prevent the model from adapting to quantization-induced distribution shifts.

## Foundational Learning

- **Concept: Network binarization fundamentals**
  - Why needed here: Understanding the core binarization framework is essential for grasping how PB-LLM modifies it for LLMs.
  - Quick check question: What are the main challenges in applying standard binarization techniques to LLMs, and how does PB-LLM address them?

- **Concept: Hessian matrix and its role in quantization**
  - Why needed here: The Hessian-based approach is central to both salient weight detection and GPTQ-inspired reconstruction.
  - Quick check question: How does the Hessian matrix guide the selection of weights to preserve during quantization?

- **Concept: Quantization-aware training (QAT) vs. Post-training quantization (PTQ)**
  - Why needed here: PB-LLM extends to both PTQ and QAT frameworks, requiring understanding of their differences and use cases.
  - Quick check question: What are the key differences between PTQ and QAT, and why would you choose one over the other for LLM quantization?

## Architecture Onboarding

- **Component map:**
  Salient weight detector (magnitude/Hessian-based) -> Partial binarization module -> GPTQ-inspired reconstruction module (for PTQ) OR QAT training loop with frozen salient weights and optimal scaling

- **Critical path:**
  1. Identify salient weights using magnitude or Hessian criteria
  2. Partially binarize weight matrix, preserving salient weights in higher precision
  3. For PTQ: Apply GPTQ-inspired reconstruction with Hessian-guided compensation
  4. For QAT: Freeze salient weights and apply optimal scaling to binary weights during training

- **Design tradeoffs:**
  - Tradeoff between number of preserved salient weights and overall compression ratio
  - Element-wise vs. column-wise salient weight selection (uniformity vs. computational efficiency)
  - PTQ vs. QAT approaches (simplicity vs. performance recovery)

- **Failure signatures:**
  - Performance collapse if too few salient weights are preserved
  - Inefficient compression if too many weights are preserved in higher precision
  - Training instability if optimal scaling factors are not properly derived

- **First 3 experiments:**
  1. Implement partial binarization on a small LLM with varying ratios of preserved salient weights (5%, 10%, 30%) and measure performance impact.
  2. Compare element-wise vs. column-wise salient weight selection on model performance and storage efficiency.
  3. Evaluate GPTQ-inspired reconstruction under PTQ on partially binarized models with different Hessian-based compensation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of salient weights to binarized weights for achieving the best balance between compression and model performance?
- Basis in paper: [inferred] The paper discusses the importance of salient weights and their impact on model performance, suggesting that a small fraction of weights can be preserved in higher precision while binarizing the rest.
- Why unresolved: The paper mentions that retaining 10% of weights in 8 bits and binarizing the remaining 90% equates to a 2.7-bit quantization, but it does not specify the optimal ratio for different model sizes or tasks.
- What evidence would resolve it: Experiments comparing the performance of PB-LLM with different ratios of salient to binarized weights across various LLM sizes and tasks would help determine the optimal balance.

### Open Question 2
- Question: How does the choice of criteria for detecting salient weights (e.g., magnitude vs. Hessian) affect the performance of PB-LLM?
- Basis in paper: [explicit] The paper investigates two criteria for detecting salient weights: magnitude and Hessian, noting that the selection does not significantly impact the efficacy of PTQ.
- Why unresolved: While the paper suggests using magnitude as the preferred criterion for its simplicity and efficacy, it does not provide a detailed comparison of the impact of these criteria on the overall performance of PB-LLM.
- What evidence would resolve it: A comprehensive study comparing the performance of PB-LLM using different criteria for salient weight detection across various tasks and model sizes would clarify the impact of this choice.

### Open Question 3
- Question: Can PB-LLM be extended to quantize both weights and activations simultaneously, and what would be the impact on model performance and compression?
- Basis in paper: [inferred] The paper focuses on weight binarization for LLMs, mentioning that binarizing both weights and activations could theoretically provide significant compression and speed improvements.
- Why unresolved: The paper does not explore the simultaneous binarization of weights and activations, leaving the potential benefits and challenges of this approach unexplored.
- What evidence would resolve it: Implementing PB-LLM to binarize both weights and activations and evaluating its impact on model performance and compression across different tasks would provide insights into the feasibility and benefits of this approach.

## Limitations

- The claim that element-wise selection of salient weights is superior to column-wise approaches lacks comprehensive empirical validation across different model architectures and task types.
- The Hessian-based reconstruction mechanism, while theoretically sound, requires extensive computation that may not scale well to larger LLMs beyond LLaMA-7B.
- The optimal scaling factor derivation for QAT assumes frozen weights will maintain their effectiveness throughout training, which may not hold for all quantization scenarios.

## Confidence

- **High Confidence:** The core concept of partially binarizing LLMs by preserving salient weights is technically sound and aligns with established principles in model compression. The experimental results showing performance retention with 10% salient weights are promising and reproducible.
- **Medium Confidence:** The GPTQ-inspired reconstruction approach and the QAT with frozen salient weights are reasonable extensions of existing techniques, but their effectiveness may be model and task-dependent. The specific implementation details and hyperparameter choices could significantly impact results.
- **Low Confidence:** The claim about pushing binarization to 1-bit while maintaining reasoning capabilities is largely speculative and not adequately validated in the paper. The efficiency gains from column-wise vs. element-wise selection are asserted but not rigorously proven.

## Next Checks

1. **Scalability Test:** Apply PB-LLM to larger LLM architectures (LLaMA-13B, LLaMA-33B) and evaluate whether the 10% salient weight threshold remains optimal or requires adjustment for different model scales.

2. **Storage Overhead Analysis:** Conduct a detailed measurement of actual storage requirements including the bitmap indices and sparse matrix representations, comparing against theoretical predictions to verify the claimed compression benefits.

3. **Task Generalization Evaluation:** Test PB-LLM on specialized domains beyond general reasoning tasks (e.g., code generation, mathematical reasoning, domain-specific fine-tuning) to assess whether the method generalizes across diverse LLM applications or is optimized for specific task types.