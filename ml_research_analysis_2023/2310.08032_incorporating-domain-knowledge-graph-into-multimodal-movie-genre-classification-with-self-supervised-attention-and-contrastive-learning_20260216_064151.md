---
ver: rpa2
title: Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification
  with Self-Supervised Attention and Contrastive Learning
arxiv_id: '2310.08032'
source_url: https://arxiv.org/abs/2310.08032
tags:
- graph
- knowledge
- attention
- module
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a novel multimodal movie genre classification
  framework (IDKG) that incorporates domain knowledge graph embeddings to address
  three key issues: 1) unutilized group relations in metadata, 2) unreliable attention
  allocation, and 3) indiscriminative fused features. The approach constructs a domain
  knowledge graph from metadata, retrieves relevant embeddings using group relations,
  and integrates them with other modalities.'
---

# Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning

## Quick Facts
- arXiv ID: 2310.08032
- Source URL: https://arxiv.org/abs/2310.08032
- Reference count: 40
- Key outcome: Proposed IDKG framework achieves 84.9% micro-F1 on MM-IMDb dataset

## Executive Summary
This paper addresses three key challenges in multimodal movie genre classification: unutilized group relations in metadata, unreliable attention allocation, and indiscriminative fused features. The proposed IDKG framework incorporates domain knowledge graph embeddings, introduces a self-supervised Attention Teacher module, and implements Genre-Centroid Anchored Contrastive Learning. The approach demonstrates state-of-the-art performance on MM-IMDb and MM-IMDb 2.0 datasets, achieving 84.9% micro-F1 and outperforming existing methods through effective integration of metadata relationships.

## Method Summary
The IDKG framework constructs a domain knowledge graph from movie metadata (directors, casts, titles, genres) and retrieves relevant embeddings using group relations. CLIP extracts visual and text features from posters and plot summaries, while a translation model (RotateE) generates knowledge graph embeddings. The Attention Teacher module learns distribution features from the knowledge graph to produce reliable attention weights through self-supervised learning. Genre-Centroid Anchored Contrastive Learning strengthens discriminative ability of fused features by using genre centroids as positive anchors. The method is evaluated on two datasets (MM-IMDb and MM-IMDb 2.0) and demonstrates superior performance compared to state-of-the-art approaches.

## Key Results
- Achieves 84.9% micro-F1, 83.2% macro-F1, 84.8% weighted-F1, and 83.9% samples-F1 on MM-IMDb dataset
- Outperforms state-of-the-art methods on both MM-IMDb and MM-IMDb 2.0 datasets
- Ablation studies confirm effectiveness of each component (knowledge graph, attention teacher, contrastive learning)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating domain knowledge graph embeddings addresses unutilized group relations in metadata by capturing co-appearance patterns between directors, casts, and genres.
- Mechanism: Knowledge graph construction uses metadata to create entities and relations (e.g., director-title, cast-title, director-genre), then applies TransE-style embedding models to learn relation patterns. Group relations are retrieved as additional modality features.
- Core assumption: Group relations in metadata contain predictive information about movie genres that is not captured by traditional multimodal approaches.
- Evidence anchors: [abstract] "we present a novel framework that exploits the knowledge graph from various perspectives to address the above problems... construct a domain knowledge graph using metadata"; [section] "The group relations in metadata can aid the prediction of movie genres... we traverse all the director and cast entities in metadata... the embedding of entities from ùëÄùëéùë° ùëí"
- Break condition: If group relations in metadata are not predictive of genres or if the knowledge graph embedding fails to capture meaningful relations between entities.

### Mechanism 2
- Claim: Self-supervised attention teacher module addresses unreliable attention allocation by generating pseudo labels from knowledge graph distribution features.
- Mechanism: The Attention Teacher module computes pseudo labels based on the number and degree of entities in knowledge graph features, then trains attention module using a logarithmic regression loss to produce more reliable attention weights.
- Core assumption: The distribution of entities in knowledge graph features correlates with the importance of the knowledge graph modality for genre prediction.
- Evidence anchors: [abstract] "we introduce an Attention Teacher module for reliable attention allocation based on self-supervised learning. It learns the distribution of the knowledge graph and produces rational attention weights"; [section] "To ensure the reliable attention allocation of the attention module, we introduce the Attention Teacher (AT) module... we mine the distribution feature from the knowledge graph"
- Break condition: If pseudo labels do not correlate with actual importance or if the self-supervised training fails to improve attention reliability.

### Mechanism 3
- Claim: Genre-Centroid Ancholed Contrastive Learning addresses indiscriminative fused features by creating positive anchors from genre centroids in embedding space.
- Mechanism: For multi-genre samples, computes centroid of genre embeddings as positive anchor, defines negative samples as complement set of genre embeddings, and applies contrastive loss to push fused features toward their genre centroids.
- Core assumption: A single centroid representation can effectively capture the semantics of multiple genres better than treating each genre as a separate positive anchor.
- Evidence anchors: [abstract] "we propose a Genre-Centroid Ancholed Contrastive Learning module to strengthen the discriminative ability of fused features"; [section] "To overcome this limitation, we attempt to represent the semantics of multiple genres in single anchor... we enlarge the genre embedding space by computing the centroid ùê∂ùëñ of ùê∫ùëñ"
- Break condition: If centroid representation fails to capture multi-genre semantics or if negative sampling strategy is ineffective.

## Foundational Learning

- Concept: Knowledge Graph Embedding (TransE/TransR/TransH)
  - Why needed here: To capture relation patterns between movie metadata entities (directors, casts, genres) that inform genre classification
  - Quick check question: What is the difference between TransE and TransR in handling relation types?

- Concept: Self-supervised Learning for Attention
  - Why needed here: To generate supervision signals for attention module without explicit labels, using knowledge graph distribution features
  - Quick check question: How does the logarithmic loss function differ from standard L1/L2 in this context?

- Concept: Contrastive Learning with Multi-label Classification
  - Why needed here: To enhance discriminative ability of fused features when samples have multiple genres
  - Quick check question: Why is defining positive pairs challenging in multi-label classification compared to single-label?

## Architecture Onboarding

- Component map: Input (text, image, metadata) ‚Üí Clip feature extraction ‚Üí Knowledge Graph construction ‚Üí TransE embedding ‚Üí Attention Teacher ‚Üí Attention module ‚Üí Genre-Centroid Contrastive Learning ‚Üí Classifier
- Critical path: Metadata ‚Üí Knowledge Graph ‚Üí Entity Embedding ‚Üí Knowledge Graph Feature ‚Üí Attention scores ‚Üí Fused Feature ‚Üí Classification
- Design tradeoffs: Knowledge graph adds complexity but captures metadata relations; self-supervised attention avoids label dependence but requires careful pseudo-label design; centroid-based contrastive learning handles multi-label but needs genre embedding space
- Failure signatures: Poor genre prediction despite good individual modality performance suggests attention or contrastive learning issues; knowledge graph features not contributing suggests embedding or retrieval problems
- First 3 experiments:
  1. Verify knowledge graph construction and entity embedding quality using link prediction metrics
  2. Test attention module with and without AT supervision to measure attention reliability improvement
  3. Evaluate contrastive learning module by comparing with random initialization vs. knowledge graph initialized anchors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IDKG change when using different knowledge graph embedding techniques beyond the six compared in the ablation study?
- Basis in paper: [explicit] The paper compares six different translate models (TransH, TransR, TransD, ComplEx, ConvE, RotateE) and finds RotateE performs best, but only reports results for RotateE in the main experiments.
- Why unresolved: The ablation study only provides performance metrics for six specific embedding techniques, leaving uncertainty about whether other knowledge graph embedding methods might yield better results.
- What evidence would resolve it: Testing IDKG with additional knowledge graph embedding techniques (e.g., RotatE variants, other embedding methods) and comparing their performance metrics.

### Open Question 2
- Question: What is the impact of varying the knowledge graph construction strategy on the model's performance?
- Basis in paper: [inferred] The paper constructs the knowledge graph using metadata from directors, casts, titles, and genres, but doesn't explore alternative construction strategies or the effect of including/excluding certain entity types.
- Why unresolved: The paper uses a specific knowledge graph construction approach without exploring how changes to this strategy might affect performance, such as including additional metadata fields or using different relation types.
- What evidence would resolve it: Experiments varying the knowledge graph construction strategy (e.g., different entity types, relation definitions) and measuring the impact on IDKG's performance.

### Open Question 3
- Question: How does the Attention Teacher module perform when applied to other multimodal classification tasks beyond movie genre classification?
- Basis in paper: [explicit] The AT module is specifically designed for and tested on the movie genre classification task, with no mention of its applicability to other domains.
- Why unresolved: While the AT module shows effectiveness in this specific task, its generalizability to other multimodal classification problems remains unexplored.
- What evidence would resolve it: Applying the AT module to different multimodal classification tasks (e.g., image-text classification in other domains) and evaluating its performance compared to task-specific attention mechanisms.

## Limitations
- Effectiveness heavily depends on metadata quality and completeness, which may vary across datasets or domains
- Self-supervised attention relies on pseudo labels that may not always correlate with true modality importance
- Centroid-based contrastive learning assumes single centroid can adequately represent multi-genre semantics

## Confidence
- High confidence in general framework design and three core components
- Medium confidence in specific implementation details of Attention Teacher module
- Low confidence in generalizability to domains with sparse metadata

## Next Checks
1. Conduct ablation studies on knowledge graph embedding quality by comparing TransE vs. TransR vs. TransH models and measuring their impact on genre classification performance
2. Validate the pseudo-label generation mechanism by comparing attention weights trained with AT supervision against ground-truth human attention annotations (if available) or alternative supervision sources
3. Test the centroid-based contrastive learning approach on datasets with varying numbers of genres per sample to assess its effectiveness across different multi-label distributions