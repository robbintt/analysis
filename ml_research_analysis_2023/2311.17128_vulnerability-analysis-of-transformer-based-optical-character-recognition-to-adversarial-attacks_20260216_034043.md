---
ver: rpa2
title: Vulnerability Analysis of Transformer-based Optical Character Recognition to
  Adversarial Attacks
arxiv_id: '2311.17128'
source_url: https://arxiv.org/abs/2311.17128
tags:
- attacks
- trocr
- adversarial
- perturbation
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of Transformer-based
  Optical Character Recognition (TrOCR) models to adversarial attacks. The authors
  develop novel attack algorithms for both targeted and untargeted attacks, evaluating
  their effectiveness on the IAM handwriting dataset.
---

# Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks

## Quick Facts
- arXiv ID: 2311.17128
- Source URL: https://arxiv.org/abs/2311.17128
- Reference count: 40
- Key outcome: TrOCR is highly vulnerable to untargeted attacks (CER > 1) and moderately vulnerable to targeted attacks (25% success rate)

## Executive Summary
This paper investigates the vulnerability of Transformer-based Optical Character Recognition (TrOCR) models to adversarial attacks. The authors develop and evaluate novel attack algorithms for both targeted and untargeted attacks using the IAM handwriting dataset. They demonstrate that TrOCR is highly susceptible to untargeted attacks, where small, imperceptible perturbations can cause the model to output completely nonsensical text. For targeted attacks, they achieve approximately 25% success rates when attempting to make the model output specific tokens. The study highlights significant security concerns for OCR systems, particularly for high-risk applications and compliance with emerging AI regulations.

## Method Summary
The study evaluates multiple attack algorithms (FGSM, DeepFool, C&W, Backward Error) against TrOCR models using the IAM handwriting dataset. For untargeted attacks, the researchers measure Character Error Rate (CER) as the primary metric, while targeted attacks are evaluated using success ratios. The experiments involve applying various perturbation sizes to input images and measuring their impact on OCR accuracy. The authors compare the effectiveness of different attack algorithms and analyze how perturbation size affects attack success rates.

## Key Results
- Untargeted attacks can cause CER > 1 with perturbations imperceptible to human observers
- Targeted attacks achieve approximately 25% success rates when targeting the tenth most likely token
- TrOCR inherits vulnerabilities from both its vision and language transformer components
- C&W attack outperforms other algorithms for targeted attacks despite requiring larger perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TrOCR is highly vulnerable to untargeted attacks because small perturbations can cause misclassifications across multiple tokens, leading to high CER.
- Mechanism: Adversarial perturbations exploit the transformer architecture's sensitivity to input changes, particularly in the token generation process. By increasing the loss function, the attack causes the model to output incorrect tokens, resulting in high Character Error Rate (CER).
- Core assumption: The transformer model's reliance on token-level classification makes it susceptible to perturbations that alter token predictions, even if the perturbations are imperceptible to humans.
- Evidence anchors:
  - [abstract] "On a benchmark handwriting data set, untargeted attacks can cause a CER of more than 1 without being noticeable to the eye."
  - [section 4.3] "From these tests, it can be concluded that TrOCR is highly vulnerable to untargeted attacks. The output can be completely changed to nonsensical text with a perturbation that is not visible to the human eye."
  - [corpus] "Found 25 related papers (using 8)." (Weak evidence; limited direct support for this mechanism)
- Break condition: If the perturbation size exceeds a threshold where human perception detects changes, the attack may become less effective as it no longer remains imperceptible.

### Mechanism 2
- Claim: TrOCR is somewhat less vulnerable to targeted attacks because achieving specific token outputs requires larger perturbations.
- Mechanism: Targeted attacks must manipulate the model to produce a specific output token, which is more challenging than causing any misclassification. This requires larger perturbations to overcome the model's inherent robustness to targeted changes.
- Core assumption: The transformer model's design includes some resistance to targeted changes due to its training on diverse datasets and regularization techniques.
- Evidence anchors:
  - [abstract] "With a similar perturbation size, targeted attacks can lead to success rates of around 25%—here we attacked single tokens, requiring TrOCR to output the tenth most likely token from a large vocabulary."
  - [section 4.3] "C&W, although requiring larger perturbation sizes, achieved a success rate of approximately 25%, outperforming the other algorithms."
  - [corpus] "Found 25 related papers (using 8)." (Weak evidence; limited direct support for this mechanism)
- Break condition: If the vocabulary size is reduced or if the model is fine-tuned on a more specific dataset, the success rate of targeted attacks may increase.

### Mechanism 3
- Claim: The combination of vision and language transformer components in TrOCR creates unique vulnerabilities not present in standalone vision or language models.
- Mechanism: TrOCR's hybrid architecture, combining vision transformers for image processing and language transformers for text generation, introduces new attack vectors that exploit the interaction between these components.
- Core assumption: The integration of vision and language transformers creates a complex system where vulnerabilities in one component can affect the other, leading to compounded attack success.
- Evidence anchors:
  - [section 2.3] "Since TrOCR has image inputs, like the ViTs, but has generative text output, like language transformer models, it is substantially different from both cases."
  - [section 4.3] "TrOCR uniquely combines the advantages of CV and NLP models to create a powerful OCR process. However, our study demonstrates that TrOCR also inherits the vulnerabilities of the components that make up the overall computational pipeline."
  - [corpus] "Found 25 related papers (using 8)." (Weak evidence; limited direct support for this mechanism)
- Break condition: If either the vision or language component is made more robust independently, the overall vulnerability of TrOCR may decrease.

## Foundational Learning

- Concept: Adversarial attacks and their types (targeted vs. untargeted)
  - Why needed here: Understanding the nature of adversarial attacks is crucial to comprehend how they can be applied to TrOCR and why certain attacks are more effective than others.
  - Quick check question: What is the main difference between targeted and untargeted adversarial attacks?

- Concept: Transformer architecture and its application in OCR
  - Why needed here: Knowledge of how transformers work in both vision and language tasks is essential to understand the unique vulnerabilities of TrOCR.
  - Quick check question: How does the transformer architecture differ when applied to vision tasks compared to language tasks?

- Concept: Character Error Rate (CER) as a metric
  - Why needed here: CER is the primary metric used to evaluate the effectiveness of adversarial attacks on TrOCR, so understanding how it is calculated and what it represents is important.
  - Quick check question: What does a high CER indicate about the performance of an OCR model?

## Architecture Onboarding

- Component map:
  - Image preprocessor (equivalent to TrOCR's built-in preprocessor)
  - Vision transformer encoder (for image patch encoding)
  - Language transformer decoder (for token generation)
  - Loss function (cross-entropy for training, custom for attacks)

- Critical path:
  1. Input image is preprocessed and converted to patches
  2. Patches are encoded by the vision transformer
  3. Encoded patches are decoded by the language transformer to generate tokens
  4. Tokens are evaluated against the target or correct output

- Design tradeoffs:
  - Tradeoff between model size and vulnerability (smaller models like TrOCRSMALL are more vulnerable)
  - Tradeoff between perturbation size and attack success rate
  - Tradeoff between targeted and untargeted attack effectiveness

- Failure signatures:
  - High CER with small perturbations indicates vulnerability to untargeted attacks
  - Low success rate for targeted attacks suggests some resistance to specific manipulations
  - Inability to maintain attack effectiveness as perturbation size increases may indicate robustness

- First 3 experiments:
  1. Apply FGSM attack to a sample TrOCR model and measure CER increase
  2. Implement C&W attack to achieve a specific token output and measure success rate
  3. Compare vulnerability of TrOCRSMALL vs. TrOCRLARGE to adversarial attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would defense strategies, such as adversarial training, be in mitigating the vulnerabilities of TrOCR to both targeted and untargeted attacks?
- Basis in paper: [explicit] The authors suggest that since their attack strategies build on established methodologies, existing defense strategies, including adversarial training, could potentially be adapted to defend TrOCR.
- Why unresolved: The paper focuses on developing and assessing attack algorithms against TrOCR but does not explore or implement any defense mechanisms. The effectiveness of these defenses in the context of TrOCR remains untested.
- What evidence would resolve it: Implementing and evaluating various defense strategies, particularly adversarial training, on TrOCR models and measuring their performance against the developed attack algorithms in both targeted and untargeted settings.

### Open Question 2
- Question: Would using different perturbation norms (e.g., ℓ0, ℓ∞) in the attack algorithms lead to different levels of vulnerability in TrOCR compared to the ℓ2 norm used in this study?
- Basis in paper: [explicit] The authors mention that many attacks use ℓ0, ℓ2, or ℓ∞ norms to measure perturbations and that these norms can be interpreted as proxies for the visibility of the attack. They chose to use the ℓ2 norm for their study.
- Why unresolved: The paper only investigates the use of the ℓ2 norm for measuring perturbations. The impact of using other norms on the effectiveness of attacks and the vulnerability of TrOCR is not explored.
- What evidence would resolve it: Conducting experiments using the same attack algorithms but with different perturbation norms (ℓ0, ℓ∞, etc.) and comparing the results to those obtained with the ℓ2 norm in terms of attack success rates and perturbation visibility.

### Open Question 3
- Question: How does the vulnerability of TrOCR to adversarial attacks compare when using different token targets in targeted attacks, rather than just the tenth most likely token as used in this study?
- Basis in paper: [explicit] The authors note that in the targeted case, the size of the vocabulary complicates the choice of objective and chose to compute results for the tenth most likely class as a representative example, but suggest that it would be of interest to consider other options.
- Why unresolved: The study only investigates targeted attacks using the tenth most likely token as the target. The vulnerability of TrOCR to targeted attacks using other token targets (e.g., most likely, least likely, or randomly selected tokens) is not explored.
- What evidence would resolve it: Conducting targeted attack experiments using different token targets (e.g., most likely, least likely, randomly selected tokens) and comparing the success rates and required perturbation sizes to those obtained using the tenth most likely token.

## Limitations

- Narrow focus on single TrOCR model architecture without comparison to other OCR approaches
- Evaluation limited to IAM handwriting dataset, which may not generalize to other OCR use cases
- Perturbation size measurements presented qualitatively rather than with precise quantitative thresholds
- Targeted attack success rate may be artificially lowered by the specific choice of tenth most likely token as target

## Confidence

**High confidence**: The finding that TrOCR is highly vulnerable to untargeted adversarial attacks causing CER > 1 with imperceptible perturbations.

**Medium confidence**: The claim that targeted attacks achieve approximately 25% success rates with similar perturbation sizes.

**Low confidence**: The assertion that TrOCR's hybrid vision-language architecture creates unique vulnerabilities not present in standalone models.

## Next Checks

1. **Quantitative perturbation threshold analysis**: Measure the exact L2 perturbation norms required to achieve CER > 1 across different attack algorithms, establishing precise vulnerability thresholds rather than qualitative descriptions.

2. **Targeted attack methodology validation**: Repeat targeted attack experiments using multiple target selection strategies (e.g., random tokens, specific semantic targets) to determine whether the 25% success rate is consistent across different attack objectives.

3. **Architecture comparison study**: Implement identical attack scenarios against pure vision transformer OCR models and pure language transformer text classifiers to empirically validate whether TrOCR's hybrid architecture creates additional vulnerabilities beyond those inherited from its component parts.