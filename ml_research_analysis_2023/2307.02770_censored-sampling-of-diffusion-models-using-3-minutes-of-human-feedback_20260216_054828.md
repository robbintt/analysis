---
ver: rpa2
title: Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback
arxiv_id: '2307.02770'
source_url: https://arxiv.org/abs/2307.02770
tags:
- images
- malign
- reward
- human
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for censoring diffusion model outputs
  using minimal human feedback. The approach trains a lightweight reward model on
  small amounts of human-provided labels to guide a pre-trained diffusion model away
  from undesirable outputs.
---

# Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback

## Quick Facts
- arXiv ID: 2307.02770
- Source URL: https://arxiv.org/abs/2307.02770
- Reference count: 40
- Key outcome: Method censors diffusion model outputs using 10-100 human labels (3 minutes), reducing malign images from 10-70% to under 1% while maintaining diversity.

## Executive Summary
This paper introduces a method for censoring unwanted outputs from pre-trained diffusion models using minimal human feedback. The approach trains a lightweight reward model on small amounts of human-provided labels to guide the diffusion model away from undesirable outputs during generation. By avoiding costly fine-tuning and instead modifying the generation process via reward-based guidance, the method achieves significant improvements in precision (reducing malign outputs) with as few as 10-100 human labels. The authors demonstrate this across multiple tasks including removing crossed 7s from MNIST, watermarks from LSUN church images, human faces from ImageNet tench images, and "broken" images from LSUN bedroom generation.

## Method Summary
The method trains a reward model to approximate the probability that an image is benign, then uses this model to guide diffusion sampling away from malign outputs. For benign-dominant setups, ensemble training combines multiple reward models trained on bootstrap-sampled data. For malign-dominant setups, imitation learning iteratively refines the reward model using feedback from increasingly censored generations. The approach employs backward guidance (gradient ascent on the reward) and recurrence (iterative refinement) to further improve performance. The key insight is that pre-trained diffusion models already contain the capability to generate both benign and malign images, so the task is one of selection rather than capability generation.

## Key Results
- Reduces proportion of malign images from ~10-70% down to under 1% across multiple tasks
- Achieves these results using only 10-100 human feedback labels (3-20 minutes of annotation)
- Maintains high sample diversity while significantly improving precision
- Ensemble training and imitation learning improve sample efficiency in benign- and malign-dominant setups respectively
- Backward guidance and recurrence further strengthen the effect of reward-based guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward models trained on minimal human feedback can effectively guide diffusion models away from malign outputs.
- Mechanism: A lightweight reward model is trained to approximate the probability that an image is benign. This model is then used in the diffusion sampling process to steer generation toward benign outputs via reward-based guidance.
- Core assumption: The pre-trained diffusion model already has the capability to generate both benign and malign images, so the task is one of selection, not capability generation.
- Evidence anchors:
  - [abstract] "The approach trains a lightweight reward model on small amounts of human-provided labels to guide a pre-trained diffusion model away from undesirable outputs."
  - [section] "We mathematically formalize our goal as: Sample from the censored distribution pcensor(x) ∝ pdata(x)r(x)."
- Break condition: If the diffusion model cannot generate benign images at all, or if the reward model fails to learn the benign/malign distinction, this mechanism breaks.

### Mechanism 2
- Claim: Ensemble training and imitation learning improve sample efficiency and performance in benign- and malign-dominant setups, respectively.
- Mechanism: In benign-dominant setups, multiple reward models are trained on bootstrap-sampled benign data plus shared malign data and combined by product. In malign-dominant setups, imitation learning is used to iteratively refine the reward model using feedback from increasingly censored generations.
- Core assumption: The relative frequency of benign vs. malign images in uncensored generation affects which training method is most effective.
- Evidence anchors:
  - [abstract] "The method combines ensemble training, imitation learning, and backward guidance with recurrence for optimal performance."
  - [section] "To efficiently utilize the imbalanced data in a sample-efficient way, we propose an ensemble method... In some setups, malign images constitute the majority of uncensored generation. Section 5.3 considers such a malign-dominant setup..."
- Break condition: If the human feedback is inconsistent or the imbalance is too extreme, ensemble or imitation learning may not help.

### Mechanism 3
- Claim: Backward guidance and recurrence further strengthen the effect of reward-based guidance in diffusion sampling.
- Mechanism: After initial reward-guided sampling, the method performs gradient ascent on the reward function and uses the resulting image as a starting point for further sampling steps, repeating this process over several steps (recurrence).
- Core assumption: Iterative refinement using the reward model can improve the quality of censored outputs beyond single-step reward guidance.
- Evidence anchors:
  - [abstract] "The approach... modifies the generation process via reward-based guidance... Combining ensemble training, imitation learning, and backward guidance with recurrence for optimal performance."
  - [section] "We describe backward guidance and recurrence, techniques inspired by the universal guidance of [2]."
- Break condition: If the reward model is too noisy or the backward guidance steps destabilize sampling, performance may degrade.

## Foundational Learning

- Concept: Diffusion probabilistic models and score matching
  - Why needed here: The paper builds on denoising diffusion probabilistic models as the base generative model. Understanding how the reverse-time SDE is approximated by a score network is key to grasping how reward-based guidance modifies sampling.
  - Quick check question: What is the role of the error network εθ in the reverse-time SDE approximation?

- Concept: Reinforcement learning with human feedback (RLHF) and reward modeling
  - Why needed here: The methodology uses a reward model trained on human feedback to guide generation, analogous to how RLHF is used to align language models. Knowing how binary classifiers or reward models are trained and used for guidance is essential.
  - Quick check question: How does the weighted binary cross entropy loss prioritize accurate classification of malign images?

- Concept: Ensemble methods and imitation learning in RL
  - Why needed here: The paper adapts ensemble training (for benign-dominant cases) and imitation learning (for malign-dominant cases) from RL to improve sample efficiency in reward model training.
  - Quick check question: Why might ensemble methods be more effective than single models when benign images are abundant?

## Architecture Onboarding

- Component map:
  Pre-trained diffusion model (score network) → generates images from pdata
  Reward model rψ (trained on human feedback) → approximates P(Y=1|X) or P(Y=1|Xt,t)
  Sampling procedure → uses reward-based guidance (time-dependent or universal) with backward guidance and recurrence
  Human feedback loop → provides binary labels for training reward model

- Critical path:
  1. Collect minimal human feedback labels (10-100) on images from baseline diffusion model
  2. Train reward model (with ensemble or imitation learning as needed)
  3. Use reward model to guide diffusion sampling via time-dependent or universal guidance
  4. Optionally apply backward guidance and recurrence for further refinement
  5. Evaluate precision (proportion of benign images) and recall (diversity)

- Design tradeoffs:
  - Reward model complexity vs. sample efficiency: Using pre-trained backbones (e.g., ResNet18) and transfer learning improves efficiency but adds dependency.
  - Time-dependent vs. time-independent reward models: Time-dependent models require training in the latent space or adapting the diffusion model, while time-independent models use universal guidance but may be less precise.
  - Ensemble vs. imitation learning: Ensemble is better for benign-dominant cases, imitation learning for malign-dominant cases; choosing the wrong method can waste human feedback.

- Failure signatures:
  - High proportion of malign images after censoring → reward model not learning the distinction, or guidance weight too low
  - Very low recall (diversity) after censoring → guidance too strong (ω too high) or backward guidance destabilizing sampling
  - Reward model training diverges or accuracy low → inconsistent human feedback, insufficient data, or poor reward model architecture

- First 3 experiments:
  1. Train reward model on 10 malign and 10 benign images, use for censoring, measure precision
  2. Repeat with ensemble of 5 reward models, compare precision
  3. Apply backward guidance and recurrence to best ensemble model, measure impact on precision and recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the censored sampling approach scale to more complex censoring tasks requiring nuanced human judgment, such as removing subtle bias or offensive content?
- Basis in paper: [explicit] The paper demonstrates success on relatively clear-cut censoring tasks (e.g., removing watermarks, specific digits, human faces) but does not explore more nuanced or subjective criteria.
- Why unresolved: The paper focuses on tasks with well-defined, objective criteria for malign images, and it is unclear how the method would perform when the distinction between benign and malign is more subjective or context-dependent.
- What evidence would resolve it: Testing the approach on tasks with more subjective censoring criteria (e.g., removing subtle stereotypes or offensive content) and evaluating the precision and recall of the censored generation would clarify its scalability to complex tasks.

### Open Question 2
- Question: Can the human feedback efficiency be further improved by incorporating active learning or semi-supervised learning techniques?
- Basis in paper: [inferred] The paper achieves impressive results with minimal human feedback (as few as 10-100 labels) but does not explore techniques to reduce the feedback requirement further.
- Why unresolved: The paper does not investigate whether methods like active learning (selecting the most informative samples for labeling) or semi-supervised learning (leveraging unlabeled data) could reduce the amount of human feedback needed.
- What evidence would resolve it: Experiments comparing the current approach with versions that incorporate active learning or semi-supervised learning, measuring the reduction in human feedback required for similar censoring performance, would resolve this question.

### Open Question 3
- Question: How does the performance of censored sampling compare to fine-tuning the diffusion model when the number of human feedback labels is increased?
- Basis in paper: [explicit] The paper emphasizes the advantage of avoiding costly fine-tuning by using minimal human feedback, but it does not compare the approach to fine-tuning when more labels are available.
- Why unresolved: The paper does not explore whether the censored sampling approach remains competitive with fine-tuning when the human feedback budget is increased, potentially making fine-tuning more feasible.
- What evidence would resolve it: Experiments comparing the censored sampling approach to fine-tuning the diffusion model with varying amounts of human feedback labels, evaluating precision, recall, and computational cost, would resolve this question.

## Limitations
- The method assumes the diffusion model can generate benign images - if the model lacks this capability, reward guidance cannot recover it
- The approach may not generalize well to more nuanced or subjective censoring tasks requiring finer-grained human judgment
- Backward guidance and recurrence steps show promise but their sensitivity to hyperparameters is not thoroughly explored

## Confidence

**High Confidence**: The core mechanism of using reward models to guide diffusion sampling is well-established and the precision improvements on benchmark tasks are clearly demonstrated. The sample efficiency claims are supported by empirical results across multiple domains.

**Medium Confidence**: The effectiveness of ensemble vs. imitation learning for different benign/malign ratios is supported by results but the theoretical justification for why these methods specifically improve sample efficiency in this context could be stronger.

**Low Confidence**: The backward guidance and recurrence steps show promise but the ablation studies are limited. The claim that these steps "further strengthen" performance lacks comprehensive analysis of when they help vs. hurt, and the sensitivity to hyperparameters is unclear.

## Next Checks

1. **Generalization test**: Apply the method to censoring more subtle or complex concepts (e.g., removing bias in generated faces beyond simple presence/absence) to test limits of sample efficiency.

2. **Hyperparameter sensitivity**: Systematically vary the guidance weight ω, backward guidance learning rate, and recurrence steps to identify robust ranges and failure conditions.

3. **Reward model robustness**: Test reward model performance on out-of-distribution benign/malign examples to assess generalization beyond the small training set.