---
ver: rpa2
title: Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks
arxiv_id: '2310.01820'
source_url: https://arxiv.org/abs/2310.01820
tags:
- graph
- explanation
- fidelity
- have
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of evaluating explainability\
  \ in Graph Neural Networks (GNNs) by identifying the limitations of existing fidelity\
  \ metrics. The authors introduce a formal, information-theoretic framework and demonstrate\
  \ that popular metrics like Fid+, Fid-, and Fid\u0394 fail to align with this framework\
  \ due to distribution shift issues."
---

# Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.01820
- Source URL: https://arxiv.org/abs/2310.01820
- Reference count: 40
- Primary result: Proposed fidelity measures (Fidα1,+ and Fidα2,−) reduce distribution shift issues and better align with gold standard metrics compared to existing methods.

## Executive Summary
This paper addresses the critical challenge of evaluating explainability in Graph Neural Networks (GNNs) by identifying fundamental limitations in existing fidelity metrics. The authors demonstrate that popular metrics like Fid+, Fid−, and Fid∆ suffer from distribution shift problems when subgraphs are removed for evaluation. To overcome this, they introduce a generalized class of robust fidelity measures that are resilient to distribution shifts through edge sampling strategies. The proposed metrics are theoretically justified and empirically validated on both synthetic and real datasets, showing superior alignment with gold standard evaluation metrics compared to existing approaches.

## Method Summary
The proposed method introduces generalized fidelity measures (Fidα1,+, Fidα2,−, and Fidα1,α2,∆) that mitigate distribution shift by sampling edges from explanation and non-explanation subgraphs with probabilities α1 and α2 respectively, rather than removing or keeping entire subgraphs. The approach involves training GNN models (GCN, GIN) on benchmark datasets, generating explanation subgraphs through random edge sampling from ground-truth motifs, and computing fidelity scores using the proposed metrics. The measures are evaluated against gold standard edit distance and AUC metrics, with extensive empirical analysis conducted on Tree-Circles, Tree-Grid, BA-2motifs, and MUTAG datasets.

## Key Results
- Proposed fidelity measures show higher Spearman correlation coefficients with gold standard edit distance metrics compared to existing methods.
- The measures are robust to distribution shift issues that plague Fid+, Fid−, and Fid∆ metrics.
- Empirical validation demonstrates superior alignment with gold standard metrics on both synthetic and real datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed fidelity measures (Fidα1,+ and Fidα2,−) reduce distribution shift by sampling edges from the explanation subgraph and non-explanation subgraph respectively.
- Mechanism: Instead of removing or keeping entire subgraphs, edges are sampled with probability α1 or α2. This ensures that the modified graphs remain "typical" with respect to the original graph distribution, mitigating the out-of-distribution (OOD) issue that plagues existing fidelity metrics.
- Core assumption: The original graph distribution is sufficiently smooth such that sampling a subset of edges preserves the statistical properties relevant for classification.
- Evidence anchors:
  - [abstract] states that "the reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures" and that the proposed measures are "resilient to distribution shift issues".
  - [section 3.2] provides the formal definition of the sampling function Eα and argues that if PY|G(y|g) is "smooth" as a function of g, then OOD does not manifest.
  - [corpus] does not directly support this mechanism, as the related papers focus on explainability evaluation but not on distribution shift mitigation.
- Break condition: If the graph distribution is not smooth, or if the informative subgraph is very small compared to the whole graph, sampling may not preserve the necessary information, leading to inaccurate fidelity scores.

### Mechanism 2
- Claim: The generalized fidelity measures are well-behaved for a wide range of scenarios, not just deterministic tasks and completely explainable classifiers.
- Mechanism: By introducing the parameters α1 and α2, the fidelity measures can be tuned to avoid OOD issues in various statistical scenarios. The paper provides a theoretical analysis showing that for a class of tasks with certain properties (existence of motifs, smoothness of PY|G), the proposed measures are monotonically increasing in the quality of the explanation.
- Core assumption: There exists a set of motifs gy, y∈Y, such that P(Y=y|G=g) is 1 if gy⊆g and ∀y′≠y: gy′⊈g, 0 if ∃!y′≠y: gy⊈G, gy′⊆G, and 1/|Y| otherwise.
- Evidence anchors:
  - [section 3.2] introduces the generalized class of surrogate fidelity measures and argues that they are robust to OOD issues in a wide range of scenarios.
  - [section 3.2] provides Proposition 3, which shows that for the described class of tasks, the proposed fidelity measures are well-behaved.
  - [corpus] does not directly support this mechanism, as the related papers focus on explainability evaluation but not on the theoretical properties of fidelity measures.
- Break condition: If the task does not have the described properties (e.g., no clear motifs, or PY|G is not smooth), the proposed fidelity measures may not be well-behaved.

### Mechanism 3
- Claim: The proposed fidelity measures are empirically validated to be more coherent with gold standard metrics compared to existing methods.
- Mechanism: The paper conducts extensive empirical analysis on both synthetic and real datasets, comparing the proposed fidelity measures to the gold standard edit distance and AUC scores. The results show that the proposed measures have higher Spearman correlation coefficients with the gold standard metrics.
- Core assumption: The gold standard metrics (edit distance and AUC) accurately capture the quality of the explanation.
- Evidence anchors:
  - [section 5.2] reports the Spearman correlation coefficients between the proposed fidelity measures and the gold standard edit distance, showing consistently high negative correlations for Fidα1,+ and high positive correlations for Fidα2,−.
  - [section 5.2] compares the proposed measures to their counterparts (Fid+, Fid−, Fid∆) and shows that the proposed measures are more aligned with the gold standard metrics.
  - [corpus] does not directly support this mechanism, as the related papers focus on explainability evaluation but not on the empirical validation of fidelity measures.
- Break condition: If the gold standard metrics do not accurately capture the quality of the explanation, or if the empirical results are not reproducible, the proposed fidelity measures may not be more coherent with the gold standard metrics.

## Foundational Learning

- Concept: Information theory and mutual information
  - Why needed here: The paper uses information-theoretic concepts to define explainability and fidelity measures. Understanding mutual information and its properties is crucial for grasping the theoretical foundations of the proposed approach.
  - Quick check question: What is the relationship between mutual information and the explainability of a classifier? (Hint: Look at Definitions 2 and 3 in the paper)

- Concept: Distribution shift and out-of-distribution (OOD) samples
  - Why needed here: The paper argues that existing fidelity metrics fail due to distribution shifts when subgraphs are removed. Understanding the concept of distribution shift and its implications for model evaluation is key to appreciating the need for the proposed measures.
  - Quick check question: Why is it problematic if the distribution of the modified graph (e.g., after removing the explanation subgraph) is different from the original graph distribution? (Hint: Think about the assumptions underlying most machine learning evaluation methods)

- Concept: Graph neural networks (GNNs) and their explainability
  - Why needed here: The paper focuses on evaluating the explainability of GNNs. Understanding the basic principles of GNNs and the challenges in explaining their decisions is necessary for contextualizing the proposed approach.
  - Quick check question: What are the main challenges in evaluating the explainability of GNNs compared to other machine learning models? (Hint: Consider the structured nature of graph data and the complexity of GNN architectures)

## Architecture Onboarding

- Component map:
  Input -> Pre-trained GNN model, graph data, explanation function
  Core -> Fidelity measure computation (Fidα1,+, Fidα2,−, or Fidα1,α2,∆)
  Output -> Fidelity score
  Supporting -> Edge sampling function, graph modification function

- Critical path:
  1. Load pre-trained GNN model and graph data
  2. Generate explanation subgraph using explanation function
  3. Sample edges from explanation subgraph and non-explanation subgraph based on α1 and α2
  4. Compute fidelity score using the proposed measure
  5. Return fidelity score

- Design tradeoffs:
  - Choice of α1 and α2: Higher values may lead to more severe distribution shifts, while lower values may not provide enough information for accurate fidelity evaluation.
  - Number of samples (M): Higher values lead to more stable estimates but increase computational cost.
  - Choice of fidelity measure (Fidα1,+, Fidα2,−, or Fidα1,α2,∆): Each measure has different properties and may be more suitable for certain scenarios.

- Failure signatures:
  - Low fidelity scores for high-quality explanations: May indicate that the chosen α1 and α2 values are not appropriate for the given graph distribution.
  - High variance in fidelity scores across different runs: May indicate that the number of samples (M) is too low.
  - Unexpected behavior of fidelity scores with respect to explanation quality: May indicate that the task does not satisfy the assumptions required for the proposed measures to be well-behaved.

- First 3 experiments:
  1. Reproduce the empirical results on a small synthetic dataset (e.g., Tree-Cycles) to verify the proposed approach.
  2. Investigate the effect of different α1 and α2 values on the fidelity scores and their alignment with gold standard metrics.
  3. Apply the proposed fidelity measures to a real-world graph classification task (e.g., MUTAG) and compare the results to existing methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed fidelity measures perform on datasets with more complex graph structures or larger graph sizes?
- Basis in paper: [inferred] The paper mentions using four benchmark datasets with ground truth explanations, including Tree-Cycles, Tree-Grid, BA-2motifs, and MUTAG. However, the performance of the proposed fidelity measures on datasets with more complex graph structures or larger graph sizes is not explored.
- Why unresolved: The paper does not provide any information on the performance of the proposed fidelity measures on datasets with more complex graph structures or larger graph sizes.
- What evidence would resolve it: Empirical results comparing the performance of the proposed fidelity measures on datasets with more complex graph structures or larger graph sizes to existing fidelity measures would resolve this question.

### Open Question 2
- Question: How does the choice of hyperparameters α1 and α2 affect the performance of the proposed fidelity measures?
- Basis in paper: [explicit] The paper mentions that α1 and α2 are hyperparameters in the proposed fidelity measures, but does not provide a detailed analysis of their impact on performance.
- Why unresolved: The paper only provides a brief discussion on the effects of α1 and α2, but does not conduct an in-depth analysis of their impact on performance.
- What evidence would resolve it: A comprehensive analysis of the impact of different values of α1 and α2 on the performance of the proposed fidelity measures would resolve this question.

### Open Question 3
- Question: How do the proposed fidelity measures compare to existing fidelity measures in terms of computational efficiency?
- Basis in paper: [inferred] The paper does not provide any information on the computational efficiency of the proposed fidelity measures compared to existing fidelity measures.
- Why unresolved: The paper focuses on the theoretical and empirical performance of the proposed fidelity measures, but does not discuss their computational efficiency.
- What evidence would resolve it: A comparison of the computational efficiency of the proposed fidelity measures to existing fidelity measures would resolve this question.

## Limitations

- Theoretical claims about distribution shift mitigation rely heavily on the assumption of "smooth" graph distributions, which may not hold for complex structural dependencies.
- Empirical validation is limited to specific synthetic datasets and a single real-world dataset (MUTAG), limiting generalizability to other graph types and sizes.
- Computational complexity implications of the sampling-based approach compared to existing metrics are not addressed.

## Confidence

- **High confidence**: The core theoretical contribution regarding distribution shift issues in existing fidelity metrics is well-supported by formal analysis and aligns with established information-theoretic principles.
- **Medium confidence**: The proposed sampling-based fidelity measures demonstrate empirical improvement over existing metrics on tested datasets, but generalizability across diverse graph domains requires further validation.
- **Medium confidence**: The claim that the measures are "well-behaved" for a wide range of scenarios is theoretically justified but empirically demonstrated only on limited task classes.

## Next Checks

1. **Cross-architecture validation**: Apply the proposed fidelity measures to evaluate explanations from diverse GNN architectures (e.g., GAT, GraphSAGE) on additional real-world datasets like IMDB-BINARY and PROTEINS to assess generalizability beyond GCN/GIN models.

2. **Distribution smoothness analysis**: Conduct controlled experiments varying graph structural complexity (e.g., adding community structure, varying clustering coefficients) to empirically test the "smoothness" assumption and identify conditions where edge sampling may fail to preserve distribution properties.

3. **Computational efficiency benchmarking**: Compare the runtime complexity and memory requirements of the sampling-based fidelity measures against existing metrics across graphs of varying sizes to evaluate practical deployment considerations for large-scale applications.