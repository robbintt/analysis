---
ver: rpa2
title: Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew
arxiv_id: '2309.14568'
source_url: https://arxiv.org/abs/2309.14568
tags:
- hebrew
- language
- corpus
- data
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce DictaLM, a 7B parameter large language model designed
  specifically for Modern Hebrew. The model is trained on a Hebrew-centric dataset
  totaling 7.5 billion tokens, with 80% from a cleaned version of the HeDC4 corpus
  and 20% from other sources including news, blogs, subtitles, and novels.
---

# Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew

## Quick Facts
- arXiv ID: 2309.14568
- Source URL: https://arxiv.org/abs/2309.14568
- Reference count: 8
- Primary result: 7B parameter Hebrew-specific transformer trained on 7.5B tokens with specialized tokenizer and released under Creative Commons license

## Executive Summary
DictaLM is a 7B parameter large language model specifically designed for Modern Hebrew, trained on a Hebrew-centric dataset totaling 7.5 billion tokens. The model uses a transformer architecture with 32 layers and incorporates specialized components including a custom Hebrew tokenizer with 56,000 vocabulary size and rotary positional embeddings. Two additional models were released: DictaLM-Rab for Rabbinic/Historical Hebrew and an instruction-tuned version for modern Hebrew. The project addresses the scarcity of high-quality Hebrew language models by providing open-source resources for the Hebrew NLP community.

## Method Summary
The model was trained using the NeMo framework on 8 H100 GPUs with a transformer architecture consisting of 32 layers, 4096 hidden size, and 32 attention heads. Training employed rotary positional embeddings, GeLU activation, and FusedAdam optimizer with cosine-annealing schedule. The dataset comprised 7.5 billion tokens, with 80% from a cleaned version of the HeDC4 corpus and 20% from other sources including news, blogs, subtitles, and novels. A specialized Hebrew BPE tokenizer with 56,000 vocabulary was developed to address tokenization challenges specific to Hebrew's morphological complexity.

## Key Results
- Trained 7B parameter transformer achieving stable performance across Hebrew-specific tasks
- Developed specialized Hebrew tokenizer achieving approximately 1.3 tokens per word
- Released three models: DictaLM for modern Hebrew, DictaLM-Rab for Rabbinic Hebrew, and instruction-tuned variant
- Made all models available under Creative Commons license for Hebrew NLP research

## Why This Works (Mechanism)

### Mechanism 1
The specialized Hebrew tokenizer with 56,000 vocabulary size improves token efficiency for Modern Hebrew text by addressing Hebrew's high morphological complexity through custom BPE training, achieving approximately 1.3 tokens per word compared to 1.1 tokens per character in generic tokenizers.

### Mechanism 2
Training on predominantly Hebrew-centric data (80% from cleaned HeDC4 corpus) creates strong language modeling capability by allowing the model to learn Hebrew linguistic patterns, syntax, and semantics effectively through large-scale pretraining on 7.5 billion tokens.

### Mechanism 3
The transformer architecture with rotary positional embeddings and specific optimizations enables stable training through 32-layer design with 4096 hidden size, rotary embeddings, and optimized normalization/activation functions that create a capable architecture for Hebrew language modeling.

## Foundational Learning

- Concept: Byte-Pair Encoding (BPE) tokenization
  - Why needed here: Hebrew's morphological richness requires subword tokenization to handle word variations efficiently
  - Quick check question: Why might a 56,000 vocabulary size be chosen for Hebrew versus a smaller size?

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding attention mechanisms and layer stacking is crucial for modifying or extending the model
  - Quick check question: How does rotary positional embedding differ from absolute positional embeddings in transformers?

- Concept: Hebrew language characteristics
  - Why needed here: Morphological richness, right-to-left script, and limited available text resources affect model design
  - Quick check question: What specific challenges does Hebrew's morphology present for language modeling compared to English?

## Architecture Onboarding

- Component map: 32 transformer layers → LayerNorm normalization → Rotary positional embeddings → GeLU activation → Separate embedding/output weights
- Critical path: Tokenizer → Pretraining corpus → Model architecture → Fine-tuning datasets
- Design tradeoffs: Larger vocabulary improves Hebrew tokenization but increases memory requirements; rotary embeddings extend sequence length but add computational overhead
- Failure signatures: Poor tokenization shows as high perplexity on Hebrew text; training instability may indicate batch size or learning rate issues
- First 3 experiments:
  1. Test tokenization efficiency on Hebrew text samples
  2. Evaluate model perplexity on held-out Hebrew corpus
  3. Assess generation quality with simple prompt completions

## Open Questions the Paper Calls Out

### Open Question 1
How does DictaLM's performance on Hebrew-specific tasks compare to existing multilingual models like GPT-3 or LLaMA when fine-tuned on the same Hebrew datasets? The paper lacks comparative benchmarks against other models.

### Open Question 2
What is the long-term effectiveness of DictaLM-Rab for Rabbinic Hebrew tasks compared to using a fine-tuned DictaLM model on Rabbinic texts? The paper mentions "encouraging results" but lacks detailed performance comparisons.

### Open Question 3
How does the specialized Hebrew tokenizer impact tokenization efficiency and downstream task performance compared to standard multilingual tokenizers? While the tokenizer's design is described, empirical validation is lacking.

### Open Question 4
What are the specific limitations of DictaLM in handling offensive or biased material, and how can these be addressed in future iterations? The paper acknowledges potential for generating offensive content but doesn't detail the nature or extent of such outputs.

### Open Question 5
How does the use of rotary positional embeddings in DictaLM's architecture affect its ability to handle longer sequence lengths compared to standard positional embeddings? The paper mentions RoPE effectiveness but lacks empirical evidence.

## Limitations
- Performance validation lacks comparative benchmarks against existing Hebrew language models
- Data quality concerns due to unspecified composition of 20% non-HeDC4 corpus sources
- Generalization across Hebrew variants remains unclear with minimal description of Rabbinic variant training

## Confidence

**High Confidence**: Technical specifications of model architecture (32 layers, 4096 hidden size, rotary positional embeddings) are clearly stated and follow established transformer practices.

**Medium Confidence**: Methodology for developing specialized Hebrew tokenizer and overall training pipeline are well-described, though empirical validation is limited.

**Low Confidence**: Claims about model performance and superiority over existing Hebrew language models lack supporting benchmark data.

## Next Checks

1. Conduct controlled experiments comparing token-per-word ratios and perplexity scores between DictaLM's specialized tokenizer and generic multilingual tokenizers on identical Modern Hebrew text samples.

2. Evaluate DictaLM on established Hebrew NLP benchmarks or create standardized evaluation datasets for Modern Hebrew language understanding and generation tasks, comparing performance against DictaBERT and other Hebrew language models.

3. Test the model's performance across diverse Hebrew domains (news, social media, literature, Rabbinic texts) to evaluate capability for handling different Hebrew variants and measure performance degradation when shifting between Modern and Historical Hebrew contexts.