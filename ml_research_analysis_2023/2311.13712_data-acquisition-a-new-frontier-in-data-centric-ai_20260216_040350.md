---
ver: rpa2
title: 'Data Acquisition: A New Frontier in Data-centric AI'
arxiv_id: '2311.13712'
source_url: https://arxiv.org/abs/2311.13712
tags:
- data
- acquisition
- providers
- datasets
- provider
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAM, a benchmark for modeling the interaction
  between data providers and acquirers in a data marketplace. It addresses the challenge
  of data acquisition for machine learning, where ad-hoc processes and lack of consistent
  methodologies lead to inefficiency.
---

# Data Acquisition: A New Frontier in Data-centric AI

## Quick Facts
- arXiv ID: 2311.13712
- Source URL: https://arxiv.org/abs/2311.13712
- Reference count: 40
- Primary result: No single data acquisition strategy outperforms others universally across all data market instances, highlighting the need for customized approaches.

## Executive Summary
This paper introduces DAM, a benchmark for modeling the interaction between data providers and acquirers in a data marketplace. It addresses the challenge of data acquisition for machine learning, where ad-hoc processes and lack of consistent methodologies lead to inefficiency. The DAM challenge is designed to offer budget-awareness, information and price transparency, and multiple data sources. The benchmark was released as part of DataPerf, and the evaluation of submitted strategies underlines the need for effective data acquisition strategies in ML. The primary result is that no single strategy outperforms others universally across all data market instances, highlighting the importance of customizing data acquisition strategies for different marketplaces.

## Method Summary
The DAM benchmark simulates a data marketplace where K=20 data providers sell sentiment analysis datasets, each with feature vectors and labels. Providers share summary statistics (100-quantiles of features/labels, correlations) and 5 sample data points. The acquirer has a small evaluation dataset and budget of $150. Five acquisition strategies are evaluated: purchasing from single providers, all providers equally, or selecting providers based on feature correlation similarity to acquirer's data. Logistic regression models are trained on purchased data and evaluated on the evaluation dataset using a weighted accuracy-cost metric across five market instances with different data distributions.

## Key Results
- No single acquisition strategy outperforms others universally across all five data market instances
- Strategy-CoFR approach gives the best performance on second, fourth and fifth data marketplaces
- Strategy-RFE performs better for the third market, demonstrating strategy customization is essential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data acquirers cannot evaluate dataset quality without pre-purchase insights, leading to inefficient acquisitions.
- Mechanism: The DAM benchmark provides summary statistics (100-quantiles of features and label correlations) and a few samples to allow acquirers to estimate dataset utility before purchase.
- Core assumption: Summary statistics are sufficient for acquirers to approximate the performance of models trained on purchased data.
- Evidence anchors:
  - [abstract] "lack of platforms offering detailed information about datasets, transparent pricing, standardized data formats"
  - [section 2.4] "Data acquirers need information about the dataset properties to validate whether the dataset is useful for their applications"
  - [corpus] Weak: no direct neighbor mentions of summary statistics, but adjacent works discuss benchmarking and data valuation.
- Break condition: If the correlation structure between features and labels differs substantially across datasets, summary statistics may fail to predict performance.

### Mechanism 2
- Claim: No single acquisition strategy dominates across all data market instances, so customization is required.
- Mechanism: The benchmark evaluates multiple strategies (Strategy-All, Strategy-Single, Strategy-CoFR, etc.) and shows that different strategies excel in different market instances.
- Core assumption: Market instances vary in the distribution and quality of data providers, making universal strategies suboptimal.
- Evidence anchors:
  - [abstract] "no single strategy outperforms others universally across all data market instances"
  - [section 4] "There is no universally 'best' strategy... Strategy-CoFR approach gives the best performance on the second, fourth and fifth data marketplace. However, Strategy-RFE is better for the third market"
  - [corpus] Weak: neighbors discuss data-centric AI evolution but not multi-strategy evaluation.
- Break condition: If all market instances had similar data provider distributions, a single dominant strategy might emerge.

### Mechanism 3
- Claim: Budget awareness enables acquirers to optimize spending and avoid over-purchasing.
- Mechanism: DAM sets a fixed budget for acquirers and requires strategies to stay within it while maximizing model accuracy.
- Core assumption: Acquirers have a known, fixed budget and must optimize within it.
- Evidence anchors:
  - [section 3.2] "The acquirer's budget is $150"
  - [section 3.1] "The pricing function for Di by pi : N → R+. If q ∈ N samples from Di is purchased, then one needs to pay pi(q)"
  - [corpus] Weak: neighbors discuss data acquisition but not budget constraints explicitly.
- Break condition: If data pricing is too variable or unpredictable, budget optimization may become infeasible.

## Foundational Learning

- Concept: Correlation coefficient and feature relevance
  - Why needed here: Acquirers use feature-label correlations to estimate dataset utility before purchase.
  - Quick check question: What does a high positive correlation between a feature and the label indicate about that feature's usefulness for prediction?

- Concept: Euclidean distance in high-dimensional spaces
  - Why needed here: Strategies like Strategy-p% use Euclidean distance to measure similarity between acquirer and provider data distributions.
  - Quick check question: If two vectors of correlation coefficients have a large Euclidean distance, what does that imply about the similarity of their datasets?

- Concept: Recursive feature elimination (RFE)
  - Why needed here: Strategy-RFE reduces dimensionality to identify the most important features for model training.
  - Quick check question: How does RFE determine which feature to remove at each iteration?

## Architecture Onboarding

- Component map: Data provider → summary statistics + pricing → broker → acquirer strategy → evaluation dataset → model training
- Critical path: Acquirer observes summaries → selects strategy → purchases data within budget → trains model → evaluates on Db
- Design tradeoffs: Summary statistics vs. full data access; budget constraints vs. model performance; strategy complexity vs. computational cost
- Failure signatures: High variance in strategy performance; strategies failing to stay within budget; models performing poorly on evaluation set
- First 3 experiments:
  1. Run all strategies on a single market instance and compare accuracy and budget adherence.
  2. Vary the budget and observe how strategy performance changes.
  3. Remove summary statistics and measure the impact on strategy performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective data acquisition strategies for machine learning in different data marketplaces?
- Basis in paper: [explicit] The paper states that no single strategy outperforms others universally across all data market instances, highlighting the importance of customizing data acquisition strategies for different marketplaces.
- Why unresolved: The paper presents various strategies and their performance across different market instances, but it does not provide a clear guideline on how to choose the most effective strategy for a specific marketplace.
- What evidence would resolve it: A comprehensive study comparing the performance of different strategies across various types of marketplaces, considering factors such as domain, data format, and pricing models.

### Open Question 2
- Question: How can we design a data marketplace for machine learning that offers budget-awareness, information and price transparency, and multiple data sources?
- Basis in paper: [explicit] The paper introduces the DAM challenge, a benchmark for a data marketplace that offers budget-awareness, information and price transparency, and multiple data sources.
- Why unresolved: While the DAM challenge provides a framework for such a marketplace, it is unclear how to implement these features in a real-world setting, considering the complexities of data acquisition and the reluctance of data providers to share detailed information.
- What evidence would resolve it: A successful implementation of a data marketplace that incorporates the desired features and demonstrates improved efficiency and transparency in data acquisition.

### Open Question 3
- Question: How can we effectively evaluate the value of data with limited information in a data marketplace?
- Basis in paper: [explicit] The paper highlights the challenge of evaluating the value of data with limited information, as most data providers are reluctant to offer the full details of their datasets to data acquirers.
- Why unresolved: The paper presents some strategies for data acquisition based on limited information, but it does not provide a comprehensive solution for evaluating data value in the context of a data marketplace.
- What evidence would resolve it: A robust method for evaluating data value based on limited information, validated through experiments in real-world data marketplaces.

## Limitations
- The DAM benchmark's effectiveness depends heavily on the quality of summary statistics as proxies for dataset utility, which may not capture complex feature interactions or non-linear relationships.
- The evaluation uses only logistic regression as the base model, limiting generalizability to other model architectures.
- The five market instances, while designed to be distinct, may not fully represent the diversity of real-world data marketplaces where pricing dynamics and data quality distributions can be more complex.

## Confidence

- High confidence: The finding that no single strategy universally dominates across all market instances is supported by empirical results across five distinct evaluations.
- Medium confidence: The claim that summary statistics enable effective pre-purchase dataset evaluation is theoretically sound but depends on data characteristics not fully explored in the paper.
- Medium confidence: The budget constraint mechanism works as specified, though real-world data marketplaces may have more complex pricing structures.

## Next Checks

1. Test the framework with more complex models (e.g., transformers, ensemble methods) to verify strategy effectiveness generalizes beyond logistic regression.
2. Evaluate the impact of removing summary statistics entirely to quantify how much they contribute to strategy performance.
3. Create additional market instances with varying degrees of data provider similarity to test strategy robustness across a broader distribution of marketplace conditions.