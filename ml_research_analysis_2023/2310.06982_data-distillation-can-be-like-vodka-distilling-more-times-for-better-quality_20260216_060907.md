---
ver: rpa2
title: 'Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality'
arxiv_id: '2310.06982'
source_url: https://arxiv.org/abs/2310.06982
tags:
- training
- synthetic
- distillation
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Progressive Dataset Distillation (PDD), a
  novel framework that generates multiple synthetic datasets sequentially, with each
  dataset conditioned on the previous ones, to better capture the evolving training
  dynamics of deep networks. The key insight is that single-stage dataset distillation
  methods fall short because they only match early training dynamics, while later
  stages require different types of examples.
---

# Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality

## Quick Facts
- **arXiv ID:** 2310.06982
- **Source URL:** https://arxiv.org/abs/2310.06982
- **Reference count:** 10
- **Primary result:** Progressive Dataset Distillation (PDD) improves state-of-the-art dataset distillation methods by 0.4-4.3% accuracy and enables 90% of full data accuracy with only 5-8% of original data size

## Executive Summary
This paper introduces Progressive Dataset Distillation (PDD), a novel framework that generates multiple synthetic datasets sequentially, with each dataset conditioned on the previous ones, to better capture the evolving training dynamics of deep networks. The key insight is that single-stage dataset distillation methods fall short because they only match early training dynamics, while later stages require different types of examples. PDD addresses this by progressively distilling new synthetic datasets that capture increasingly complex training dynamics. Experiments on CIFAR-10/100 and Tiny-ImageNet show that PDD consistently improves upon state-of-the-art methods like IDC and MTT by 0.4-4.3% in accuracy. Notably, PDD enables generating larger synthetic datasets, achieving 90% of full data accuracy with only 5-8% of the original data size. The method also demonstrates strong cross-architecture generalization and shows promise for continual learning applications.

## Method Summary
PDD is a progressive framework that generates multiple synthetic datasets sequentially, with each dataset conditioned on previous ones to capture different training dynamics. Starting with an empty set and random parameters, it iteratively generates synthetic subsets Si using a base dataset distillation algorithm (e.g., IDC, MTT), where each subset is conditioned on all previous subsets. The model is trained progressively on cumulative unions of these subsets to prevent forgetting. At later stages, easy-to-learn examples with low forgetting scores are discarded to improve efficiency. The framework can be applied to any dataset distillation method, requiring only that the method supports conditioning on previous synthetic data.

## Key Results
- PDD consistently improves baseline methods (IDC, MTT, DC, DSA) by 0.4-4.3% accuracy on CIFAR-10/100 and Tiny-ImageNet
- Achieves 90% of full data accuracy with only 5-8% of original data size
- Strong cross-architecture generalization (evaluated on ResNet-10/18 using ConvNet-distilled datasets)
- Progressive training outperforms both union and sequential training approaches
- Later-stage synthetic datasets contain more abstract features compared to earlier, more colorful examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive Dataset Distillation (PDD) improves performance by generating multiple synthetic subsets conditioned on previous ones to capture training dynamics at different stages.
- Mechanism: Each subset Si is generated such that S1 ∪ S2 ∪ ... ∪ Si captures the training dynamics at stage i, with each stage focusing on progressively more complex training phases.
- Core assumption: Training dynamics of deep networks change significantly across training stages, requiring different types of examples at each stage.
- Evidence anchors:
  - [abstract]: "current dataset distillation techniques fall short, showing a notable performance gap when compared to training on the original data" and "multiple synthetic subsets are required to capture the training dynamics at different phases of training."
  - [section]: "synthetic examples generated based on early training dynamics can only train low-complexity neural networks that perform well on easy examples" and "at later stages, we can safely discard the examples that are learned early in training with lower-complexity functions."
  - [corpus]: Weak evidence - related papers focus on single-stage distillation methods, supporting the novelty of PDD's multi-stage approach.
- Break condition: If training dynamics do not change significantly across stages, or if conditioning on previous subsets does not provide meaningful information.

### Mechanism 2
- Claim: Progressive training on cumulative unions of synthetic subsets prevents forgetting and enables smooth transitions between stages.
- Mechanism: At test time, the model is trained on S1, then S1 ∪ S2, and so on, progressively building up knowledge while preventing catastrophic forgetting.
- Core assumption: Training on individual subsets sequentially would cause the model to forget previously learned information.
- Evidence anchors:
  - [abstract]: "trains the model on the cumulative union of these subsets without requiring additional training time."
  - [section]: "to prevent forgetting the information learned from the previous subsets, we first train the model on S1, then S1 ∪ S2, and keep training on the union of the previous subsets in addition to the new one."
  - [section]: Table 3 shows progressive training outperforms union and sequential training methods.
- Break condition: If the model can learn from individual subsets without forgetting, or if progressive training significantly increases computational cost.

### Mechanism 3
- Claim: Discarding easier-to-learn examples at later stages improves efficiency without harming performance.
- Mechanism: At each distillation stage, examples with low forgetting scores (easy examples) are dropped, focusing the distillation on increasingly difficult examples.
- Core assumption: Early training stages learn easy examples that can be classified by low-complexity functions, while later stages require more complex examples.
- Evidence anchors:
  - [abstract]: "at later stages, we can safely discard the examples that are learned early in training with lower-complexity functions from the distillation pipeline."
  - [section]: "Examples with higher forgetting scores are more difficult to learn for the network and are learned later during the training" and Table 6 shows comparable performance when discarding easy examples.
  - [corpus]: Weak evidence - related papers do not discuss this specific mechanism of discarding easy examples.
- Break condition: If easy examples provide necessary regularization or if discarding them significantly harms performance.

## Foundational Learning

- Concept: Training dynamics of deep networks
  - Why needed here: PDD is based on the observation that training dynamics change significantly across training stages, requiring different synthetic examples at each stage.
  - Quick check question: What evidence supports the claim that neural networks learn simpler functions early in training and more complex functions later?

- Concept: Catastrophic forgetting in sequential learning
  - Why needed here: PDD's progressive training approach is designed to prevent catastrophic forgetting when training on multiple synthetic subsets sequentially.
  - Quick check question: How does PDD's progressive training approach differ from simply training on individual synthetic subsets sequentially?

- Concept: Forgetting score as a measure of example difficulty
  - Why needed here: PDD uses forgetting scores to identify and discard easy examples at later distillation stages, improving efficiency.
  - Quick check question: How is the forgetting score calculated, and what does it tell us about the difficulty of training examples?

## Architecture Onboarding

- Component map:
  - Input: Full dataset T and dataset distillation algorithm A (e.g., IDC, MTT)
  - Progressive distillation loop: Generates P synthetic subsets Si, each conditioned on previous subsets
  - Transition mechanism: Updates model parameters θθθi based on training on cumulative unions
  - Output: Series of synthetic datasets S1, S2, ..., SP for training and evaluation

- Critical path:
  1. Initialize empty set S0 and random parameters θθθ0
  2. For each stage i from 1 to P:
     - Generate Si using algorithm A, conditioned on previous subsets
     - Update θθθi by training on cumulative union ∪i-1j=1Sj
  3. Evaluate by training on cumulative unions at each stage

- Design tradeoffs:
  - Number of stages P vs. per-stage IPC: More stages capture longer dynamics but may reduce per-stage quality
  - Conditioning vs. independent generation: Conditioning prevents redundancy but increases complexity
  - Progressive training vs. union/sequential: Progressive prevents forgetting but increases training time

- Failure signatures:
  - Performance plateaus or degrades with increasing stages
  - Significant gap between PDD and full data training accuracy
  - Progressive training fails to outperform union or sequential methods
  - Conditioning on previous subsets does not improve performance

- First 3 experiments:
  1. Apply PDD to IDC/MTT on CIFAR-10 with 5 stages and IPC=10/50, compare to single-stage baselines
  2. Vary number of stages while keeping total IPC constant, measure impact on performance
  3. Test cross-architecture generalization by evaluating on ResNet-10/18 using ConvNet-distilled datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of PDD vary with the choice of dataset distillation algorithm used as the base method?
- Basis in paper: [explicit] The paper applies PDD to multiple baseline methods including IDC, MTT, DC, and DSA, showing consistent improvements across all methods.
- Why unresolved: The paper does not provide a systematic comparison of how much improvement PDD provides for different base methods, nor does it explore whether some base methods are more compatible with PDD than others.
- What evidence would resolve it: A comprehensive ablation study comparing PDD applied to a wider range of dataset distillation algorithms, quantifying the relative improvement each time, and analyzing which characteristics of base methods make them more amenable to PDD.

### Open Question 2
- Question: What is the theoretical limit of dataset distillation, and how close does PDD get to this limit?
- Basis in paper: [inferred] The paper shows that PDD achieves 90% of full data accuracy with only 5-8% of the data size, but does not discuss the theoretical bounds of dataset distillation or the gap to the optimal achievable performance.
- Why unresolved: The paper focuses on empirical results and improvements over baselines, but does not address the fundamental question of how good dataset distillation can theoretically get, or what the maximum achievable accuracy is with a given number of synthetic examples.
- What evidence would resolve it: Theoretical analysis establishing the information-theoretic limits of dataset distillation, combined with experiments showing the performance gap between PDD and these theoretical limits across various datasets and model architectures.

### Open Question 3
- Question: How does PDD perform on real-world, large-scale datasets beyond the benchmark datasets tested?
- Basis in paper: [inferred] The paper demonstrates PDD on CIFAR-10/100 and Tiny-ImageNet, which are standard benchmark datasets, but does not explore its performance on larger, more complex real-world datasets.
- Why unresolved: The experiments are limited to relatively small-scale image classification tasks. It remains unclear how PDD would scale to more challenging, high-resolution, or domain-specific datasets.
- What evidence would resolve it: Experiments applying PDD to large-scale datasets such as ImageNet, video datasets, or domain-specific datasets like medical imaging or satellite imagery, measuring performance and computational requirements compared to training on the full data.

## Limitations

- The framework assumes training dynamics evolve predictably across stages, which may not hold for all datasets or model architectures
- Limited testing on significantly different architectures beyond ResNet variants
- Qualitative visualization of abstract features emerging in later stages lacks rigorous quantitative analysis

## Confidence

- Progressive conditioning mechanism: Medium confidence - supported by empirical results but lacking rigorous theoretical justification
- Discarding easy examples: Low confidence - works in practice but doesn't fully explain why these examples can be safely removed
- Cross-architecture generalization: Medium confidence - promising but limited to ResNet variants

## Next Checks

1. Ablation study isolating the impact of progressive conditioning versus increased total IPC across stages
2. Testing on significantly different architectures (e.g., Vision Transformers) to verify cross-architecture claims
3. Theoretical analysis of why easy examples can be discarded without harming generalization