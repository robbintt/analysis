---
ver: rpa2
title: Large Language Models Help Humans Verify Truthfulness -- Except When They Are
  Convincingly Wrong
arxiv_id: '2310.12558'
source_url: https://arxiv.org/abs/2310.12558
tags:
- explanation
- retrieval
- explanations
- accuracy
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares large language models (LLMs) with traditional
  search engines for helping humans verify truthfulness of claims. The authors conducted
  experiments with 80 crowdworkers to evaluate whether ChatGPT-generated explanations
  or retrieved Wikipedia passages are more effective for fact-checking.
---

# Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong

## Quick Facts
- arXiv ID: 2310.12558
- Source URL: https://arxiv.org/abs/2310.12558
- Reference count: 40
- LLMs provide faster but less reliable fact-checking assistance compared to traditional search engines

## Executive Summary
This study compares large language models (LLMs) with traditional search engines for helping humans verify truthfulness of claims. The authors conducted experiments with 80 crowdworkers to evaluate whether ChatGPT-generated explanations or retrieved Wikipedia passages are more effective for fact-checking. While both approaches achieve similar accuracy (74% vs 73%), LLM explanations are significantly faster (1.01 min vs 2.53 min per claim). However, users over-rely on LLM explanations when they are wrong, leading to below-random accuracy. The study explores two mitigation strategies: contrastive explanations (showing both supporting and refuting arguments) and combining explanations with retrieval, but neither significantly outperforms retrieval alone. The results suggest that while LLM explanations offer efficiency benefits, they may not be reliable replacements for reading retrieved passages, especially in high-stakes scenarios where incorrect AI explanations could have serious consequences.

## Method Summary
The study used a between-subjects design with 80 crowdworkers across five conditions: Baseline (no evidence), Retrieval (top-10 Wikipedia passages), Explanation (ChatGPT-generated explanations), Contrastive Explanation (both supporting and refuting arguments), and Retrieval+Explanation (combined). The FoolMeTwice dataset provided 200 claims, with 20 claims per participant and 16 per condition. ChatGPT generated explanations by concatenating retrieved passages with claims, while the GTR-XXL retriever obtained Wikipedia passages. Human accuracy, time per claim, and confidence were measured across all conditions.

## Key Results
- LLM explanations achieved 74% accuracy versus 73% for retrieval, with no significant difference in overall accuracy
- LLM explanations were significantly faster (1.01 min vs 2.53 min per claim)
- Users achieved below-random accuracy (0.35 Â± 0.22) when LLM explanations were wrong, demonstrating over-reliance
- Neither contrastive explanations nor combined retrieval+explanation approaches significantly improved accuracy over retrieval alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language explanations from LLMs provide efficiency benefits over retrieved passages for human fact-checking.
- Mechanism: Users can process concise, synthesized explanations faster than reading multiple retrieved paragraphs, reducing verification time from ~2.53 minutes to ~1.01 minutes per claim.
- Core assumption: The explanation accurately captures the key evidence and reasoning from retrieved passages.
- Evidence anchors:
  - [abstract] "LLM explanations are significantly faster (1.01 min vs 2.53 min per claim)"
  - [section] "reading natural language explanations is significantly faster than reading retrieved passages(ð‘§ = âˆ’5.09, ð‘ = 9.1ð‘’ âˆ’ 6)"
- Break condition: The explanation is inaccurate or incomplete, causing users to waste time verifying wrong conclusions or missing key evidence.

### Mechanism 2
- Claim: Contrastive explanations (showing both supporting and refuting arguments) reduce over-reliance on LLM explanations.
- Mechanism: By presenting both sides of an argument, users must actively evaluate and choose between competing explanations rather than passively accepting a single answer.
- Core assumption: Users can distinguish between stronger and weaker arguments when presented with both sides.
- Evidence anchors:
  - [abstract] "we ask LLMs to provide contrastive informationâ€”explain both why the claim is true and false"
  - [section] "Contrastive explanation achieves higher human accuracy than non-contrastive explanation when the non-contrastive explanation is wrong"
- Break condition: Both contrastive explanations are similarly convincing or contain similar errors, making it difficult for users to determine the correct answer.

### Mechanism 3
- Claim: Over-reliance occurs when users trust LLM explanations without sufficient verification, especially when explanations are wrong.
- Mechanism: Users perceive LLM explanations as authoritative and don't verify them against their own knowledge or retrieved evidence, leading to below-random accuracy on incorrect explanations.
- Core assumption: Users lack domain knowledge or confidence to critically evaluate LLM explanations.
- Evidence anchors:
  - [abstract] "users over-rely on LLM explanations when they are wrong, leading to below-random accuracy"
  - [section] "When the explanation is wrong, users tend to over-trust the explanations and only achieve an accuracy of (0.35 Â± 0.22)"
- Break condition: Users have sufficient domain knowledge or are explicitly instructed to verify explanations against other sources.

## Foundational Learning

- Concept: Fact-checking task formulation
  - Why needed here: Understanding how fact-checking is structured (claim + evidence â†’ true/false label) is essential for designing effective AI assistance
  - Quick check question: What are the three possible outputs in a standard fact-checking task?

- Concept: Over-reliance in human-AI collaboration
  - Why needed here: The study reveals a critical failure mode where users trust AI predictions even when wrong, which is central to the paper's findings
  - Quick check question: What is the accuracy difference between correct and incorrect LLM explanations in this study?

- Concept: Retrieval vs generation tradeoffs
  - Why needed here: The paper directly compares these two approaches for fact-checking assistance, highlighting efficiency vs reliability tradeoffs
  - Quick check question: What is the time difference per claim between using explanations vs retrieved passages?

## Architecture Onboarding

- Component map: User interface -> Evidence presentation (claims, explanations, retrieved passages) -> User decision (true/false + confidence) -> Data collection (accuracy, time, rationales)
- Critical path: Generate/retrieve evidence -> Present to user -> User makes decision -> Record outcome
- Design tradeoffs: Efficiency (explanations faster) vs reliability (retrieved passages more accurate when explanations are wrong)
- Failure signatures: User accuracy drops below baseline when explanations are wrong; contrastive explanations reduce over-reliance but don't improve overall accuracy
- First 3 experiments:
  1. Test baseline accuracy without any evidence to establish difficulty level
  2. Compare explanation vs retrieval conditions to measure efficiency vs accuracy tradeoffs
  3. Test contrastive explanation to measure over-reliance reduction effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does contrastive explanation significantly outperform retrieval alone in terms of accuracy?
- Basis in paper: [explicit] The authors note that contrastive explanation significantly reduces over-reliance but does not significantly outperform search engines.
- Why unresolved: The study does not provide a definitive answer on whether contrastive explanation leads to higher accuracy than retrieval alone.
- What evidence would resolve it: Additional experiments comparing the accuracy of contrastive explanation to retrieval alone across different datasets and tasks.

### Open Question 2
- Question: Can combining retrieval and explanation provide complementary benefits in accuracy and efficiency?
- Basis in paper: [explicit] The authors state that combining retrieval and explanation does not achieve significant complementary improvement in accuracy.
- Why unresolved: The study does not explore whether there are specific conditions or user types for which combining retrieval and explanation might be beneficial.
- What evidence would resolve it: Further experiments testing the effectiveness of combining retrieval and explanation under different conditions and with diverse user groups.

### Open Question 3
- Question: How can over-reliance on LLM explanations be mitigated in high-stakes settings?
- Basis in paper: [inferred] The authors suggest that over-reliance on LLM explanations is a significant issue, especially in high-stakes settings.
- Why unresolved: The study does not provide a comprehensive solution to mitigate over-reliance on LLM explanations.
- What evidence would resolve it: Research into developing and testing new strategies to reduce over-reliance on LLM explanations, particularly in critical applications.

## Limitations
- The study uses crowdworkers rather than domain experts, limiting applicability to high-stakes scenarios
- The experimental design may not capture all types of misinformation challenges beyond Wikipedia-based claims
- ChatGPT model version and exact prompting strategies are not fully specified, affecting reproducibility

## Confidence
- High confidence: The efficiency advantage of explanations over retrieval (1.01 min vs 2.53 min, p < 0.001) is robustly demonstrated with large effect sizes
- Medium confidence: The over-reliance phenomenon is well-documented, but the exact mechanisms of why users trust wrong explanations remain unclear
- Medium confidence: Neither contrastive explanations nor combined retrieval+explanation approaches significantly improve accuracy, though sample sizes may limit detection of smaller effects

## Next Checks
1. Replicate the over-reliance finding with domain experts and high-stakes misinformation examples to assess real-world applicability
2. Test alternative explanation formats (e.g., step-by-step reasoning, evidence highlighting) to identify whether presentation style affects over-reliance
3. Conduct longitudinal studies to determine if users learn to critically evaluate LLM explanations over time or become increasingly reliant