---
ver: rpa2
title: 'BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write
  Better Stories?'
arxiv_id: '2310.16681'
source_url: https://arxiv.org/abs/2310.16681
tags:
- language
- performance
- reward
- story
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores the impact of reinforcement learning from human
  feedback (RLHF) on language models pretrained with a limited training corpus, as
  part of the BabyLM shared task. Specifically, the study compares two GPT-2 variants,
  GPT-2 Base and GPT-2 Large, in their storytelling capabilities after RLHF fine-tuning.
---

# BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?

## Quick Facts
- **arXiv ID**: 2310.16681
- **Source URL**: https://arxiv.org/abs/2310.16681
- **Reference count**: 14
- **Primary result**: RLHF fine-tuning improves storytelling for larger GPT-2 models but has minimal or negative effects on smaller models in limited data settings.

## Executive Summary
This study investigates how reinforcement learning from human feedback (RLHF) affects language models trained on limited data, specifically examining GPT-2 Base and GPT-2 Large models for storytelling tasks. The research finds that larger models significantly benefit from RLHF fine-tuning, showing improved narrative coherence and instruction adherence, while smaller models show little to no improvement or even degradation. These results suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity, particularly in scenarios with constrained training data.

## Method Summary
The researchers pretrained GPT-2 Base (125M parameters) and GPT-2 Large (774M parameters) from scratch on the BabyLM STRICT dataset (100M words) using a custom tokenizer with 32K vocabulary. They then trained a reward model using human preferences collected through Best-Worst Scaling annotations on 500 pairwise story continuations. The models were fine-tuned using Proximal Policy Optimization (PPO) with the trained reward model, and performance was evaluated through automated metrics (BLiMP, SuperGLUE, MSGS) and human evaluation of grammar, creativity, consistency, and plot coherence.

## Key Results
- GPT-2 Large shows significant improvement in storytelling coherence and instruction adherence after RLHF fine-tuning, while GPT-2 Base shows little or negative effects
- PPO training generally improves GPT-2 Large's performance but has minimal or negative impact on GPT-2 Base
- Larger models demonstrate better ability to maintain narrative focus and coherence when fine-tuned with RLHF in limited data settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger models (GPT-2 Large) benefit more from RLHF fine-tuning than smaller models (GPT-2 Base) in limited data settings.
- **Mechanism**: Larger models have higher learning and adaptation capacity, allowing them to better integrate human feedback signals from the reward model during PPO training.
- **Core assumption**: The learning capacity of a model is directly related to its parameter count and ability to capture nuanced linguistic patterns.
- **Evidence anchors**:
  - [abstract]: "the larger model performs better in storytelling tasks after RLHF fine-tuning. These findings suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity"
  - [section]: "we observe that RLHF has a little or negative effect on the smaller model. However, a substantial increase in model parameters noticeably enhances the larger model's performance in storytelling tasks"
  - [corpus]: Weak - The corpus neighbors don't directly address model size effects on RLHF, though related work on RLHF is present
- **Break condition**: If the reward model is too simplistic or the human feedback is too noisy, even larger models may not benefit significantly from RLHF.

### Mechanism 2
- **Claim**: PPO training improves model consistency and plot coherence in storytelling tasks, particularly for larger models.
- **Mechanism**: The KL divergence penalty in PPO helps maintain coherence by discouraging the policy from generating outputs that differ significantly from those seen by the reward model during training.
- **Core assumption**: Human evaluators can reliably identify narrative inconsistencies, and the reward model can effectively learn to reward coherent story continuations.
- **Evidence anchors**:
  - [abstract]: "enhancing their ability to maintain narrative focus and coherence while adhering better to initial instructions in storytelling tasks"
  - [section]: "the GPT2-Large-PPO model significantly improves consistency and plot coherence scores compared to the standard GPT2-Large model"
  - [corpus]: Weak - No direct corpus evidence about coherence improvements from RLHF
- **Break condition**: If the reward model overfits to superficial patterns rather than genuine coherence, PPO training could reinforce incoherent but reward-maximizing behaviors.

### Mechanism 3
- **Claim**: RLHF has minimal or negative effect on smaller models' storytelling performance due to their limited capacity to capture linguistic features.
- **Mechanism**: Small models may not have sufficient capacity to effectively learn from the human feedback signals, potentially leading to overfitting or failure to generalize the feedback.
- **Core assumption**: Small models are inherently limited in their ability to capture the full range of linguistic features needed for coherent storytelling.
- **Evidence anchors**:
  - [abstract]: "PPO training has little or negative effect on the smaller GPT-2-Base model"
  - [section]: "We observe that RLHF has a little or negative effect on the smaller model"
  - [corpus]: Weak - No direct corpus evidence about why small models fail with RLHF
- **Break condition**: If the reward model is specifically designed for small models or the feedback focuses on simpler linguistic features, small models might benefit from RLHF.

## Foundational Learning

- **Concept: Reward Modeling and Preference Learning**
  - Why needed here: Understanding how reward models are trained to predict human preferences is crucial for grasping why RLHF works better for larger models
  - Quick check question: How does the reward model learn to distinguish between preferred and non-preferred story continuations?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the reinforcement learning algorithm used for fine-tuning, and its components (like KL divergence penalty) are key to understanding the results
  - Quick check question: What role does the KL divergence term play in preventing the policy from deviating too far from the initial model?

- **Concept: Best-Worst Scaling for Preference Annotation**
  - Why needed here: This is the annotation method used to create the human feedback dataset, which is fundamental to how the reward model is trained
  - Quick check question: Why might Best-Worst Scaling be more effective than direct rating scales for collecting human preferences on story quality?

## Architecture Onboarding

- **Component map**: Data preparation -> Model pretraining -> Reward model training -> PPO fine-tuning -> Evaluation
- **Critical path**: Data preparation → Model pretraining → Reward model training → PPO fine-tuning → Evaluation
- **Design tradeoffs**: Smaller tokenizer vocabulary (32K vs 50K) for efficiency vs. potential loss of rare word handling; limited reward model dataset size vs. comprehensive human evaluation
- **Failure signatures**: 
  - Small model performance degradation with RLHF suggests capacity limitations
  - Large model improvements in consistency but not all metrics suggest reward model effectiveness varies by task type
  - PPO training time and resource constraints indicate scalability challenges
- **First 3 experiments**:
  1. Ablation study: Compare GPT-2 Base and Large without RLHF to establish baseline performance differences
  2. Reward model analysis: Examine reward model predictions on held-out data to assess if it captures coherence vs. superficial patterns
  3. Hyperparameter sweep: Test different KL divergence weights in PPO to find optimal balance between reward maximization and policy stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of RLHF compare across different language model architectures beyond GPT-2?
- Basis in paper: [explicit] The paper acknowledges a limitation in not comparing RLHF effectiveness across different architectures.
- Why unresolved: The study focused on GPT-2 variants due to resource constraints, limiting insights into RLHF's applicability to other architectures.
- What evidence would resolve it: Conducting experiments with a variety of language model architectures, such as BERT, RoBERTa, and others, to compare RLHF effectiveness across different models.

### Open Question 2
- Question: What is the impact of reward model dataset size and variety on RLHF performance in storytelling tasks?
- Basis in paper: [explicit] The paper mentions the limited size of the reward model dataset as a constraint and suggests exploring its impact.
- Why unresolved: The study used a relatively small and homogeneous dataset for training the reward model, which may not capture the full range of human preferences in storytelling.
- What evidence would resolve it: Experiments with reward models trained on larger and more diverse datasets to assess the impact on RLHF performance in storytelling tasks.

### Open Question 3
- Question: How do different hyperparameter settings for the reward model and loss function affect RLHF performance?
- Basis in paper: [explicit] The paper notes the lack of exploration into different hyperparameter settings and alternative methods for reward training.
- Why unresolved: The study used default settings for the reward model and loss function, potentially missing optimizations that could enhance RLHF performance.
- What evidence would resolve it: Systematic experiments varying hyperparameters and exploring alternative reward training methods to identify optimal configurations for RLHF in storytelling tasks.

## Limitations
- Limited dataset size (100M words) may artificially constrain smaller models more than larger ones
- Human evaluation relies on only two annotators per prompt, potentially missing variability in preferences
- Reward model trained on relatively small dataset (500 pairwise comparisons) may not capture nuanced preferences

## Confidence
- **High Confidence**: GPT-2 Large benefits more from RLHF fine-tuning than GPT-2 Base in storytelling tasks
- **Medium Confidence**: RLHF techniques are generally more advantageous for larger models due to their higher learning capacity
- **Low Confidence**: Small models have "little or negative effect" from RLHF due to capacity limitations

## Next Checks
1. **Model Size Scaling Analysis**: Conduct experiments with intermediate model sizes (e.g., GPT-2 Medium at 355M parameters) to determine if the benefit from RLHF scales smoothly with model size or exhibits threshold effects.

2. **Reward Model Robustness Testing**: Evaluate the reward model on held-out test sets with varying levels of narrative coherence to assess whether it genuinely captures coherence versus superficial features.

3. **Extended Human Evaluation**: Increase the number of human annotators per prompt to at least 5-7 and include diverse reader demographics to assess whether the observed improvements in consistency and plot coherence generalize across different reader preferences.