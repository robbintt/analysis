---
ver: rpa2
title: The Past, Present, and Future of Typological Databases in NLP
arxiv_id: '2310.13440'
source_url: https://arxiv.org/abs/2310.13440
tags:
- language
- typological
- linguistics
- languages
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically explores inconsistencies across major
  typological databases such as WALS and Grambank, and highlights their limited applicability
  in NLP, especially for low-resource languages. By comparing feature encodings across
  languages and macroareas, the authors quantify mismatches due to factual errors,
  linguistic variation, and discretization of data.
---

# The Past, Present, and Future of Typological Databases in NLP

## Quick Facts
- arXiv ID: 2310.13440
- Source URL: https://arxiv.org/abs/2310.13440
- Reference count: 10
- Key outcome: This paper systematically explores inconsistencies across major typological databases such as WALS and Grambank, and highlights their limited applicability in NLP, especially for low-resource languages.

## Executive Summary
This paper investigates the inconsistencies and limitations of major typological databases like WALS and Grambank, and their impact on Natural Language Processing (NLP) applications. By comparing feature encodings across languages and macroareas, the authors identify mismatches stemming from factual errors, linguistic variation, and discretization of data. Experiments with large language models reveal poor performance in predicting typological features from Wikipedia snippets, underscoring the challenges posed by these inconsistencies. The authors advocate for a continuous view of typological features to better support language modeling and downstream NLP tasks in diverse linguistic contexts.

## Method Summary
The authors compare feature encodings from WALS and Grambank across languages and macroareas to quantify mismatches due to factual errors, linguistic variation, and discretization. They analyze linguistic variation using Universal Dependencies treebanks for Dutch, English, French, and Japanese. Large language models (BERT, RoBERTa) are fine-tuned to predict typological features from Wikipedia snippets, and their performance is evaluated against a baseline model. The study also examines the representation of typological features in these databases and their applicability in NLP, particularly for low-resource languages.

## Key Results
- Significant inconsistencies exist between WALS and Grambank databases, with exact matches ranging from 0.07 to 0.44 and soft matches from 0.13 to 0.60.
- Factual errors in typological databases arise from both source grammar errors and encoding errors during database creation.
- Linguistic variation across time, domain, and speaker groups creates inconsistencies between typological databases and linguistic sources.
- Large language models show poor performance in predicting typological features from Wikipedia snippets, highlighting the impact of database inconsistencies on NLP applications.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discretization of linguistic features into discrete categories in typological databases leads to inconsistencies with linguistic reality.
- Mechanism: When linguistic features that are inherently continuous (like word order) are forced into discrete categories, nuanced variation is lost, leading to mismatches between databases and linguistic sources.
- Core assumption: Linguistic features exist on a continuum rather than in discrete categories.
- Evidence anchors:
  - [section] "Language does not fit into discrete categories, and forcing it to do so in these databases removes crucial information (Levshina et al., 2023)."
  - [section] "For example, W ALS feature 81A has a label 'No dominant order', which allows for the capture of languages that do not have one main way of ordering subjects, objects, and verbs. This puts a language that alternates between SVO, SOV, and VSO only in the same category as a language that has completely free word order, erasing any information on the fundamental ways that these languages differ."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.436, average citations=0.0." (Weak corpus evidence for this specific mechanism)
- Break condition: If linguistic features can be accurately represented in discrete categories without loss of meaningful information, this mechanism breaks down.

### Mechanism 2
- Claim: Factual errors in typological databases stem from both source grammar errors and encoding errors during database creation.
- Mechanism: Errors propagate from source materials (grammars) to databases, and additional errors are introduced during the encoding process, leading to inconsistencies across databases.
- Core assumption: Source grammars and encoding processes are error-prone.
- Evidence anchors:
  - [section] "Factual errors can be either due to an error in the source grammar, or an error made when encoding the data from the source into the database."
  - [section] "Plank (2009) carry out an in-depth investigation of the features described for German, noting that 'a non-negligible proportion of the values assigned is found to be problematic, in the sense of being arbitrary or uncertain in view of analytic alternatives, unappreciative of dialectal variation, unclear as to what has been coded, or factually erroneous'"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.436, average citations=0.0." (Weak corpus evidence for this specific mechanism)
- Break condition: If source grammars and encoding processes are error-free or errors can be perfectly corrected, this mechanism breaks down.

### Mechanism 3
- Claim: Linguistic variation across time, domain, and speaker groups creates inconsistencies between typological databases and linguistic sources.
- Mechanism: When databases under-specify the context (time, domain, speaker group) of the language they describe, they fail to capture variation, leading to apparent inconsistencies.
- Core assumption: Language varies across contexts and this variation is not captured in typological databases.
- Evidence anchors:
  - [section] "Inconsistencies can occur when one source describes language from one time, domain, and speaker group, while another describes language from a different time, domain, or speaker group."
  - [section] "Table 3 shows the preference of Noun-Adjective ordering, based on a count of dependency links. Interestingly, French has a substantial difference across linguistic variations, with spoken language having a slight preference for A-N ordering, as compared to the medical domain containing mostly N-A ordering."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.436, average citations=0.0." (Weak corpus evidence for this specific mechanism)
- Break condition: If linguistic variation across contexts is negligible or can be perfectly captured in typological databases, this mechanism breaks down.

## Foundational Learning

- Concept: Linguistic typology
  - Why needed here: Understanding the field of linguistic typology is essential to grasp why typological databases are created and how they are used in NLP.
  - Quick check question: What is the primary goal of linguistic typology?
- Concept: Discrete vs. continuous data representation
  - Why needed here: The paper argues that forcing continuous linguistic features into discrete categories leads to inconsistencies and loss of information.
  - Quick check question: What is the main difference between discrete and continuous data representation, and why is this distinction important in the context of typological features?
- Concept: Cross-linguistic transfer in NLP
  - Why needed here: The paper discusses how typological information can potentially improve NLP models, particularly for low-resource languages, through cross-linguistic transfer.
  - Quick check question: How can typological information potentially improve cross-linguistic transfer in NLP models?

## Architecture Onboarding

- Component map: Typological databases (WALS, Grambank) -> Linguistic sources (grammars, Wikipedia) -> NLP models (large language models) -> Evaluation metrics (accuracy, agreement scores)
- Critical path: 1. Extract typological features from databases 2. Compare features across databases and sources 3. Identify inconsistencies and errors 4. Investigate causes of inconsistencies (factual errors, linguistic variation, discretization) 5. Experiment with NLP models using typological features 6. Evaluate model performance and draw conclusions
- Design tradeoffs: Categorical vs. continuous representation of typological features, Granularity of linguistic features vs. database usability, Domain-specific vs. general linguistic descriptions
- Failure signatures: Low agreement scores between databases, Poor performance of NLP models using typological features, Discrepancies between database entries and linguistic sources
- First 3 experiments: 1. Replicate the comparison of WALS and Grambank features for a different set of languages or features to validate the consistency findings. 2. Conduct a more detailed analysis of linguistic variation by examining additional domains, time periods, or speaker groups in the UD treebanks. 3. Implement a continuous representation of typological features and evaluate its impact on NLP model performance compared to categorical representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would continuous typological features be in improving NLP model performance for low-resource languages compared to current categorical approaches?
- Basis in paper: [explicit] The authors argue that a continuous view of typological features is beneficial and could improve language modeling in low-resource scenarios.
- Why unresolved: The paper discusses the theoretical advantages but does not empirically test continuous typological features in NLP models.
- What evidence would resolve it: Experiments comparing NLP model performance using continuous vs. categorical typological features on low-resource language tasks.

### Open Question 2
- Question: What is the extent of variation in typological features across different domains and speaker groups within the same language?
- Basis in paper: [explicit] The authors demonstrate significant variation in noun-adjective ordering across different domains and treebanks within the same languages.
- Why unresolved: While the paper shows examples of variation, it does not systematically quantify or map the full extent of such variations across typological features.
- What evidence would resolve it: Comprehensive analysis of typological feature variation across multiple domains, speaker groups, and linguistic contexts.

### Open Question 3
- Question: How can we systematically identify and correct factual errors in existing typological databases?
- Basis in paper: [explicit] The authors identify factual errors as one category of inconsistencies in typological databases but note that identifying and fixing these errors is challenging.
- Why unresolved: The paper acknowledges the existence of errors but does not provide a concrete methodology for systematic error detection and correction.
- What evidence would resolve it: Development and validation of a systematic approach for identifying and correcting factual errors in typological databases, potentially using multiple independent sources.

## Limitations
- The analysis is limited to a small set of languages and features, which may not be representative of the full diversity of typological databases.
- The paper focuses on categorical features, potentially underrepresenting the full spectrum of linguistic variation.
- The evaluation of large language models using Wikipedia snippets may not fully capture the broader applicability of typological features in NLP.

## Confidence
- Claims about discretization errors and factual errors: High
- Claims about linguistic variation: Medium

## Next Checks
1. Replicate the WALS-Grambank comparison across a broader set of 20+ languages spanning different macroareas to verify consistency findings hold beyond the initial sample
2. Conduct a systematic error analysis of source grammars to quantify the proportion of factual errors vs. encoding errors in typological databases
3. Implement and evaluate a continuous representation of typological features using regression models rather than classification to assess whether this better captures linguistic reality