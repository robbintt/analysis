---
ver: rpa2
title: 'Learn to Follow: Decentralized Lifelong Multi-agent Pathfinding via Planning
  and Learning'
arxiv_id: '2310.01207'
source_url: https://arxiv.org/abs/2310.01207
tags:
- agents
- agent
- maps
- each
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FOLLOWER, a method for decentralized lifelong
  multi-agent pathfinding (LMAPF) that combines heuristic search with reinforcement
  learning. The key idea is to use A search to construct individual paths while penalizing
  transitions to frequently used cells, then employ a learnable policy to follow these
  paths and resolve conflicts.
---

# Learn to Follow: Decentralized Lifelong Multi-agent Pathfinding via Planning and Learning

## Quick Facts
- arXiv ID: 2310.01207
- Source URL: https://arxiv.org/abs/2310.01207
- Reference count: 36
- Key outcome: Decentralized lifelong MAPF method combining A* planning with reinforcement learning outperforms state-of-the-art learnable solvers in throughput and scales better than centralized approaches

## Executive Summary
This paper proposes FOLLOWER, a decentralized approach for lifelong multi-agent pathfinding that combines heuristic search planning with reinforcement learning. The method uses A* search with dynamic cost penalization to construct individual paths while a learnable policy trained via PPO handles local conflict resolution. The approach aims to balance computational efficiency with solution quality, avoiding the high computational cost of centralized solvers while outperforming pure rule-based methods.

## Method Summary
FOLLOWER employs a hybrid architecture where agents use A* search with dynamic cost penalization based on observed agent positions to construct initial paths. A neural network policy trained via Proximal Policy Optimization (PPO) learns to follow these paths and resolve conflicts using only sparse waypoint-reaching rewards. The policy operates with local observations of nearby agents and outputs actions to navigate between waypoints. This design enables decentralized execution without communication while maintaining computational efficiency through the combination of planning and learning.

## Key Results
- FOLLOWER achieves higher throughput than state-of-the-art learnable solvers (PRIMAL2, SCRIMP, PICO) on unseen maps
- Outperforms the rule-based solver PIBT in throughput while being an order of magnitude faster than the centralized search-based solver RHCR
- Demonstrates effective generalization to unseen maps without reward shaping or demonstrations
- Scales better to large numbers of agents compared to centralized approaches in terms of computation time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic cost penalization based on observed agent positions prevents congestion by encouraging path dispersion.
- Mechanism: The agent accumulates dynamic costs for cells frequently visited by other agents and incorporates these into the A* transition costs. This discourages agents from following paths that have been heavily used recently, spreading them across the map.
- Core assumption: Agents have consistent and accurate observations of nearby agents, and the dynamic cost accumulation effectively reflects potential congestion hotspots.
- Evidence anchors:
  - [abstract] "We enhance our planning algorithm with a dedicated technique tailored to avoid congestion and increase the throughput of the system."
  - [section] "The dynamic cost, costdyn(c, t), is based on the personal experience of an agent and changes during the episode. It is computed as follows. costdyn(c, t) = Σ[t′∈[0,t]] AgentAtCell(c, t′)"
  - [corpus] Weak evidence; no related work directly tests this specific cost penalization mechanism.
- Break condition: If observations are noisy or delayed, the dynamic costs may misrepresent actual congestion, leading to suboptimal path dispersion.

### Mechanism 2
- Claim: Reinforcement learning without reward shaping enables effective short-term conflict resolution while following heuristic paths.
- Mechanism: A neural network policy is trained via PPO to navigate from one waypoint to the next on the heuristic path, using only a small positive reward for reaching waypoints. The policy learns to make necessary detours to avoid collisions without explicit shaping.
- Core assumption: The waypoint-following task is learnable with sparse rewards and the learned policy generalizes to unseen maps.
- Evidence anchors:
  - [abstract] "We employ reinforcement learning to discover the collision avoidance policies that effectively guide the agents along the paths. The policy is implemented as a neural network and is effectively trained without any reward-shaping or external guidance."
  - [section] "During the decentralized inference, each agent uses a copy of the trained weights... the reward function used is simple and does not require involved manual shaping."
  - [corpus] Weak evidence; no related work demonstrates effective training without reward shaping for this problem.
- Break condition: If the environment is too stochastic or the observation range is insufficient, the policy may fail to learn effective conflict resolution.

### Mechanism 3
- Claim: Combining planning with learning balances computational efficiency and solution quality for large agent counts.
- Mechanism: Heuristic A* search constructs initial paths quickly, while the learned policy handles local conflicts without full replanning. This avoids the computational burden of centralized solvers while outperforming pure rule-based methods.
- Core assumption: The heuristic paths provide reasonable guidance that the learned policy can refine efficiently, and the computational overhead of learning is offset by faster runtime than search-based solvers.
- Evidence anchors:
  - [abstract] "Empirically, we compare our method... to show that it... scales much better to the large numbers of agents in terms of computation time compared to the state-of-the-art search-based centralized solver."
  - [section] "FOLLOWER needs much less time to choose an action and, consequently, scales better to the increasing number of agents compared to RHCR."
  - [corpus] Weak evidence; no related work explicitly compares this hybrid approach's scalability to centralized solvers.
- Break condition: If the heuristic paths are too suboptimal or the learned policy is too slow, the hybrid approach may not provide the claimed efficiency benefits.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The agents operate with only local observations of other agents, not full state information, which fits the POMDP framework.
  - Quick check question: In this problem, what information does each agent lack that would be present in a fully observable MDP?

- Concept: Reinforcement Learning (RL) policy optimization (PPO)
  - Why needed here: The learnable follower policy is trained using Proximal Policy Optimization to maximize cumulative reward without demonstrations or reward shaping.
  - Quick check question: What is the main advantage of using PPO with clipped loss for this multi-agent setting?

- Concept: Heuristic search (A* algorithm)
  - Why needed here: A* is used to construct individual paths to goals, which are then followed and refined by the learned policy to handle conflicts.
  - Quick check question: How does the dynamic cost penalization modify the standard A* algorithm in this implementation?

## Architecture Onboarding

- Component map: Heuristic Path Decider (A* with dynamic/static costs) → Learnable Follower (PPO-trained policy) → Local observation encoder → Action decoder → Execution
- Critical path: Agent observes → encodes observation → passes to policy → policy outputs action → action executed → repeat until waypoint reached → new waypoint selected → repeat
- Design tradeoffs:
  - Using local observations vs. full state information for scalability
  - Sparse rewards vs. reward shaping for training stability
  - Dynamic cost penalization vs. static costs for congestion avoidance
- Failure signatures:
  - High congestion in narrow passages despite cost penalization
  - Policy fails to avoid collisions even with local observations
  - Runtime increases dramatically with agent count despite hybrid design
- First 3 experiments:
  1. Run FOLLOWER on a small grid with 4 agents and visualize the heatmap of visited cells to verify dynamic cost penalization effect
  2. Disable the learnable policy component and compare throughput to verify its contribution
  3. Test generalization by running FOLLOWER trained on mazes on random maps and measure throughput degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FOLLOWER's performance compare to centralized solvers when communication is unreliable or intermittent?
- Basis in paper: [explicit] The paper focuses on decentralized MAPF where agents cannot reliably communicate, but doesn't compare against centralized solvers under communication constraints.
- Why unresolved: The paper only compares against other decentralized approaches, leaving the question of how well decentralized methods handle partial connectivity compared to centralized ones.
- What evidence would resolve it: Experiments comparing FOLLOWER against centralized solvers where communication is artificially limited or disrupted, measuring throughput and solution quality.

### Open Question 2
- Question: What is the theoretical bound on completeness and optimality guarantees for FOLLOWER?
- Basis in paper: [inferred] The paper acknowledges that like other learnable MAPF solvers, FOLLOWER cannot provide theoretical guarantees of completeness/optimality.
- Why unresolved: The paper focuses on empirical evaluation but doesn't attempt to characterize the theoretical properties of the hybrid planning/learning approach.
- What evidence would resolve it: Formal analysis of the conditions under which FOLLOWER guarantees finding a solution (completeness) and bounds on solution quality (optimality), potentially through analysis of the A* component combined with the learned policy.

### Open Question 3
- Question: How does FOLLOWER scale with different types of graph structures beyond 4-connected grids?
- Basis in paper: [explicit] The experiments are conducted on 4-connected grids, but the method is presented as generally applicable to graphs.
- Why unresolved: The paper doesn't evaluate performance on graphs with different connectivity patterns, such as 8-connected grids, triangular meshes, or real-world road networks.
- What evidence would resolve it: Systematic evaluation of FOLLOWER on various graph topologies, measuring throughput and runtime as graph connectivity and structure varies.

## Limitations
- Limited empirical evidence on the effectiveness of dynamic cost penalization mechanism for congestion avoidance
- Claims of successful reinforcement learning without reward shaping lack rigorous ablation studies
- Computational efficiency comparisons rely heavily on a single centralized solver baseline

## Confidence
- **High Confidence**: The hybrid architecture combining heuristic planning with learned local control is technically sound and aligns with established multi-agent pathfinding principles.
- **Medium Confidence**: The scalability improvements over centralized solvers are supported by runtime comparisons, though the absolute performance gains need verification.
- **Low Confidence**: The effectiveness of training without reward shaping and the specific contribution of dynamic cost penalization to throughput improvements require more rigorous ablation studies.

## Next Checks
1. **Ablation Study**: Run FOLLOWER with and without the learnable policy component on identical maps to quantify the policy's contribution to throughput improvements.
2. **Dynamic Cost Analysis**: Visualize the evolution of dynamic costs during execution and measure their correlation with actual congestion patterns in narrow passages.
3. **Training Verification**: Test the trained policy on maps from outside the training distribution (different obstacle densities, map sizes) to verify generalization claims and measure throughput degradation.