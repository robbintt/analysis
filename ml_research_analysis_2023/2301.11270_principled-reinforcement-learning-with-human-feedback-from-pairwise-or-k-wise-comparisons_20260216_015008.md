---
ver: rpa2
title: Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise
  Comparisons
arxiv_id: '2301.11270'
source_url: https://arxiv.org/abs/2301.11270
tags:
- learning
- bound
- policy
- have
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework for Reinforcement Learning
  with Human Feedback (RLHF). The authors analyze the sample complexity of learning
  a reward model from pairwise or K-wise comparisons under the Bradley-Terry-Luce
  (BTL) and Plackett-Luce (PL) models.
---

# Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons

## Quick Facts
- **arXiv ID**: 2301.11270
- **Source URL**: https://arxiv.org/abs/2301.11270
- **Authors**: 
- **Reference count**: 40
- **Key outcome**: Theoretical framework for RLHF with sample complexity bounds for MLE, pessimistic MLE, and unification with max-entropy IRL

## Executive Summary
This paper provides the first theoretical analysis of Reinforcement Learning from Human Feedback (RLHF) with pairwise and K-wise comparisons under the Bradley-Terry-Luce and Plackett-Luce models. The authors establish convergence guarantees for maximum likelihood estimation (MLE) of reward parameters and show that while standard MLE converges in parameter space, it fails to induce good policies. Instead, a pessimistic MLE approach provides improved policy performance under certain coverage assumptions. The work also unifies RLHF with max-entropy inverse reinforcement learning (IRL) and provides sample complexity bounds for both problems.

## Method Summary
The paper analyzes RLHF through the lens of statistical learning theory, modeling human comparisons using Plackett-Luce distributions. For pairwise comparisons, it establishes convergence of MLE to the true parameter with error bounds scaling as $O(\sqrt{d/n})$. For K-wise comparisons, it compares two approaches: true MLEK (modeling full K-way rankings) and MLE2 (splitting into pairwise comparisons). The pessimistic MLE approach introduces conservatism by optimizing for worst-case parameters within a confidence set, addressing the coverage problem where standard MLE overfits to observed data. The framework unifies RLHF and max-entropy IRL by showing both can be viewed as special cases of reward learning from preference data under the Plackett-Luce model.

## Key Results
- MLE converges in parameter space with error bounds $O(\sqrt{d/n})$ but fails to induce good policies
- Pessimistic MLE provides improved policy performance under coverage assumptions
- MLEK is asymptotically more efficient than MLE2 due to preserving full ranking information
- The framework unifies RLHF and max-entropy IRL with shared theoretical foundations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The pessimistic MLE estimator converges to a near-optimal policy under coverage assumptions, while standard MLE fails to provide good policies despite converging to the true parameter under semi-norm.
- **Mechanism**: Pessimistic MLE introduces conservatism by constructing a confidence set around the estimated parameter and optimizing for the worst-case parameter within this set. This penalizes actions that are underrepresented in the observed data, addressing the coverage problem where MLE can overfit to observed actions.
- **Core assumption**: The feature space coverage is sufficient to bound the single concentratability coefficient, ensuring the pessimistic policy remains close to optimal.
- **Evidence anchors**:
  - [abstract] "we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions"
  - [section 3.1] "The idea of pessimism is to assign larger reward for the responses that lie on the manifold, and penalize the rarely seen responses that do not lie on manifold"
- **Break condition**: If the coverage assumption fails (concentratability coefficient unbounded), pessimism provides no benefit and may degrade performance.

### Mechanism 2
- **Claim**: MLEK (true MLE for K-wise comparisons) is asymptotically more efficient than MLE2 (pairwise-split estimator) due to the statistical efficiency of maximum likelihood estimation.
- **Mechanism**: MLEK directly models the full K-wise Plackett-Luce distribution, preserving all ranking information, while MLE2 reduces K-wise data to pairwise comparisons, losing information through marginalization. The asymptotic variance of MLEK is lower than that of MLE2.
- **Core assumption**: The Plackett-Luce model correctly specifies the human comparison behavior, and the comparison set size K is fixed and small.
- **Evidence anchors**:
  - [section 4.1] "MLEK is the true MLE and MLE2 belongs to the family of M-estimators, asymptotically MLEK shall be more efficient than MLE2"
  - [theorem 4.3] Provides asymptotic variance calculations showing MLEK has lower variance
- **Break condition**: If the Plackett-Luce model is misspecified or if K is large (making human comparisons unreliable), the efficiency gain may disappear.

### Mechanism 3
- **Claim**: The framework unifies RLHF with max-entropy IRL by showing both can be viewed as special cases of reward learning from preference data under the Plackett-Luce model.
- **Mechanism**: Both RLHF and max-entropy IRL assume human behavior follows a Plackett-Luce model, but differ in data collection - RLHF observes comparisons while IRL observes single choices. The same MLE framework applies to both, with different data structures leading to different convergence guarantees.
- **Core assumption**: Human choices can be modeled by the Plackett-Luce distribution, and the true reward is linear in known features.
- **Evidence anchors**:
  - [abstract] "our results unify the problem of RLHF and Max Entropy Inverse Reinforcement Learning, and provide the first sample complexity analysis for both problems"
  - [section 6.1] "In max-entropy IRL, it is also assumed that the human selection of trajectory follows a PL model"
- **Break condition**: If human behavior deviates significantly from the Plackett-Luce model, the unified framework breaks down and separate models may be needed.

## Foundational Learning

- **Concept**: Bradley-Terry-Luce (BTL) model for pairwise comparisons
  - Why needed here: Forms the theoretical foundation for understanding pairwise comparison data in RLHF, providing the likelihood function for MLE
  - Quick check question: What is the probability that action a beats action b under the BTL model when rewards are r_a and r_b?

- **Concept**: Plackett-Luce (PL) model for K-wise comparisons
  - Why needed here: Generalizes BTL to K-way rankings, essential for analyzing the more complex comparison scenarios used in practice
  - Quick check question: How does the PL model decompose the probability of a full ranking into sequential choices?

- **Concept**: Strong convexity and its role in estimation error bounds
  - Why needed here: Provides the mathematical foundation for proving convergence rates of MLE and establishing confidence intervals
  - Quick check question: Why does strong convexity of the negative log-likelihood imply concentration of the MLE estimate?

## Architecture Onboarding

- **Component map**: Data Collection -> MLE Estimation -> Pessimistic Policy Construction -> Performance Evaluation
- **Critical path**: Data collection → MLE estimation → Pessimistic policy construction → Performance evaluation
- **Design tradeoffs**: 
  - MLEK vs MLE2: Statistical efficiency vs computational simplicity
  - Pessimism strength: Coverage guarantee vs potential over-conservatism
  - Feature representation: Expressiveness vs sample complexity
- **Failure signatures**:
  - MLE converging to wrong policy: Indicates poor coverage of feature space
  - Pessimistic MLE being too conservative: Concentratability coefficient too small
  - Slow convergence: Insufficient comparison data or poorly designed feature space
- **First 3 experiments**:
  1. Replicate Figure 1: Compare MLE vs pessimistic MLE on the simple linear bandit from Appendix B.3
  2. Test MLEK vs MLE2: Generate synthetic K-wise comparison data and compare estimation errors
  3. Coverage analysis: Measure the single concentratability coefficient on real RLHF datasets to validate assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of the reference vector v in the pessimistic MLE algorithm affect the performance guarantee, particularly in cases where the feature space has complex geometry?
- **Basis in paper**: Explicit in Remark 3.5, where the authors discuss the importance of choosing an appropriate v to improve the concentratability coefficient.
- **Why unresolved**: The paper provides a heuristic for choosing v (the most common feature vector in the data) but does not rigorously analyze the impact of different choices of v on the performance guarantee.
- **What evidence would resolve it**: Theoretical analysis or empirical experiments comparing the performance of pessimistic MLE with different choices of v under various feature space geometries.

### Open Question 2
- **Question**: Can the theoretical framework for RLHF be extended to handle cases where the reward function is nonlinear and non-convex, and if so, what are the implications for the sample complexity bounds?
- **Basis in paper**: Explicit in Appendix A, where the authors discuss the analysis for nonlinear rewards and show that the true parameter θ⋆ is a global minimum of the population negative log likelihood for MLE2.
- **Why unresolved**: The paper provides a guarantee for the case when rθ is nonlinear and non-convex, but the bound does not vanish as n→∞ when the Hessian is bounded, leaving open the question of whether vanishing rates are achievable.
- **What evidence would resolve it**: Theoretical results showing vanishing rates for nonlinear reward functions under certain conditions, or counterexamples demonstrating the impossibility of such rates.

### Open Question 3
- **Question**: How does the performance of the pessimistic MLE algorithm compare to other pessimism-based algorithms for offline RL, such as Conservative Q-Learning or Implicit Q-Learning, in the context of RLHF?
- **Basis in paper**: Explicit in Remark 3.6, where the authors mention that heuristic approximations of the pessimistic MLE, such as Conservative Q-Learning and Implicit Q-Learning, may be more practical in the case of neural network reward functions.
- **Why unresolved**: The paper focuses on the theoretical analysis of pessimistic MLE and does not provide empirical comparisons with other pessimism-based algorithms.
- **What evidence would resolve it**: Empirical experiments comparing the performance of pessimistic MLE with other pessimism-based algorithms on benchmark RLHF tasks.

### Open Question 4
- **Question**: Can the theoretical framework for RLHF be extended to handle cases where the policy is not greedy with respect to the learned reward, but instead is fine-tuned using policy gradient methods or PPO?
- **Basis in paper**: Explicit in the conclusion section, where the authors mention that the current analysis assumes a greedy policy and that extending the framework to handle policy fine-tuning is an interesting open problem.
- **Why unresolved**: The paper's analysis is limited to the case of a greedy policy, and the interaction between reward learning and policy fine-tuning is not addressed.
- **What evidence would resolve it**: Theoretical results extending the sample complexity bounds to the case of policy fine-tuning, or empirical experiments demonstrating the effectiveness of the proposed framework in practice.

## Limitations
- Coverage assumptions required for pessimistic MLE may be difficult to verify in practice
- Linear reward function assumption may not hold in complex real-world domains
- Computational feasibility of MLEK for large K in high-dimensional spaces remains unclear

## Confidence
- **Convergence of MLE**: Medium confidence - well-supported by theory but requires empirical validation
- **Pessimistic MLE policy improvement**: Medium confidence - theoretical guarantees under coverage assumptions
- **MLEK vs MLE2 efficiency**: Medium confidence - asymptotic theory is sound but practical gains need testing
- **RLHF-IRL unification**: Medium confidence - conceptually elegant but novel theoretical contribution

## Next Checks
1. Empirical comparison of MLEK vs MLE2 on real human preference datasets to verify asymptotic efficiency gains
2. Coverage analysis on existing RLHF datasets to measure the single concentratability coefficient and validate pessimistic MLE assumptions
3. Robustness testing of the unified RLHF-IRL framework under varying degrees of model misspecification and non-linear reward structures