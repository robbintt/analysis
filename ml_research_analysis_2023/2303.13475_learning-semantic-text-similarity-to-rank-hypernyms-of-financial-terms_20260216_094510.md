---
ver: rpa2
title: Learning Semantic Text Similarity to rank Hypernyms of Financial Terms
arxiv_id: '2303.13475'
source_url: https://arxiv.org/abs/2303.13475
tags:
- terms
- https
- hypernyms
- financial
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to rank hypernyms of financial terms
  using fine-tuned sentence embeddings. The approach leverages FinBERT and FinISH
  embeddings, fine-tuned using a novel negative sampling strategy based on hierarchical
  relationships in the Financial Industry Business Ontology (FIBO).
---

# Learning Semantic Text Similarity to rank Hypernyms of Financial Terms

## Quick Facts
- arXiv ID: 2303.13475
- Source URL: https://arxiv.org/abs/2303.13475
- Reference count: 40
- Key outcome: Proposed method achieves Mean Rank 1.053 and Accuracy 0.967 on FinSim-3 validation set, outperforming previous state-of-the-art solutions

## Executive Summary
This paper presents a novel approach for ranking hypernyms of financial terms using fine-tuned sentence embeddings. The method leverages domain-specific embeddings (FinBERT and FinISH) enhanced with a negative sampling strategy based on hierarchical relationships in the Financial Industry Business Ontology (FIBO). By augmenting the training data with definitions from multiple sources and using ensemble techniques, the model achieves superior performance on the FinSim-3 dataset, demonstrating both accuracy and scalability for handling new financial terms and hypernyms.

## Method Summary
The approach fine-tunes FinBERT and FinISH embeddings using sentence transformers with a novel negative sampling strategy that leverages FIBO's hierarchical structure. Financial terms are augmented with definitions from DBpedia, Investopedia, and FIBO, and acronym expansion is performed using spaCy. The model is trained using multiple negative ranking loss with carefully sampled negative examples based on their hierarchical distance from correct hypernyms. Predictions are made by computing cosine similarity between term embeddings and hypernym embeddings, with results ensembled across both models.

## Key Results
- Achieves Mean Rank of 1.053 and Accuracy of 0.967 on FinSim-3 validation set
- Outperforms previous state-of-the-art solutions on the same dataset
- Demonstrates scalability for handling new hypernyms without requiring model retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning domain-specific embeddings using negative sampling improves hypernym ranking accuracy
- Mechanism: Hierarchical relationships from FIBO create informative negative samples that help the model distinguish between closely related categories
- Core assumption: FIBO hierarchy accurately reflects semantic relationships between financial terms
- Evidence anchors: Abstract mentions leveraging FIBO hierarchy; section describes using hierarchy for negative sampling
- Break condition: If FIBO hierarchy doesn't accurately represent semantic relationships

### Mechanism 2
- Claim: Data augmentation with definitions from external sources improves model performance
- Mechanism: External definitions provide richer semantic context than terms alone
- Core assumption: External definitions accurately capture financial term meanings
- Evidence anchors: Abstract mentions corpora from DBpedia, Investopedia, FIBO; section describes acronym expansion
- Break condition: If external definitions contain inconsistencies or errors

### Mechanism 3
- Claim: Ensemble of FinBERT and FinISH embeddings performs better than either model alone
- Mechanism: Different embeddings capture complementary aspects of financial semantics
- Core assumption: FinBERT and FinISH capture different complementary semantic aspects
- Evidence anchors: Abstract states best model is ensemble of two models; section describes mean similarity calculation
- Break condition: If models capture similar patterns rather than complementary ones

## Foundational Learning

- Concept: Sentence transformer architecture for fine-tuning embeddings
  - Why needed here: Enables semantic similarity comparison between financial terms and hypernyms through cosine similarity
  - Quick check question: How does sentence transformer architecture differ from standard BERT fine-tuning for classification?

- Concept: Hierarchical relationships in ontologies
  - Why needed here: Provides structure for generating informative negative samples during training
  - Quick check question: How would you traverse a tree ontology to find all ancestors of a given node?

- Concept: Cosine similarity for ranking
  - Why needed here: Measures semantic closeness between terms and hypernyms in embedding space
  - Quick check question: What range of values can cosine similarity take, and what does a value of 0.8 indicate?

## Architecture Onboarding

- Component map: Data preprocessing -> Embedding fine-tuning -> Similarity calculation -> Ensemble -> Ranking
- Critical path: Data augmentation → Negative sample generation → Embedding fine-tuning → Similarity calculation → Ensemble → Ranking
- Design tradeoffs: Using two embeddings increases accuracy but computational cost; negative sampling is more informative but complex; data augmentation improves performance but may introduce noise
- Failure signatures: Similar similarity scores across all hypernyms (embedding issues); random-looking rankings (sampling problems); extremely slow training (batch size/sequence length issues)
- First 3 experiments: 1) Baseline test using only FinBERT without negative sampling or augmentation; 2) Ablation test each data augmentation source separately; 3) Hyperparameter sweep varying batch size and epochs

## Open Questions the Paper Calls Out
- How does the proposed model's performance compare to other state-of-the-art models on a larger, more diverse financial dataset?
- What is the impact of using different pre-trained language models (e.g., RoBERTa, FinBERT) on the model's performance?
- How does the proposed model handle out-of-vocabulary words and unseen hypernyms?

## Limitations
- Limited evaluation on a single dataset (FinSim-3) without test set availability
- Missing detailed implementation specifications for critical components like FinISH embeddings
- Heavy reliance on FIBO hierarchy accuracy, which could introduce harmful noise if incorrect

## Confidence

**High**: Methodology assumes cosine similarity correlates with semantic hypernym relationships, which may not hold for complex financial concepts.

**Medium**: Strong validation performance but limited ablation studies and no statistical significance testing to verify individual component contributions.

**Low**: Critical components like FinISH embeddings are referenced but not directly accessible; negative sampling implementation details are incomplete; single dataset evaluation prevents generalization assessment.

## Next Checks

1. Implement ablation experiments testing the system without negative sampling, without data augmentation, and without ensemble to isolate component contributions.

2. Verify FinISH accessibility through Yseop Labs or alternative sources, or test using only FinBERT to determine incremental benefit.

3. Apply the trained model to a different financial dataset (e.g., SEC filings, financial news) to assess generalization beyond FinSim-3 challenge data.