---
ver: rpa2
title: Online Recommendations for Agents with Discounted Adaptive Preferences
arxiv_id: '2302.06014'
source_url: https://arxiv.org/abs/2302.06014
tags:
- which
- item
- menu
- each
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online recommendation problems where an agent's
  preferences over items evolve as a function of past selections. The agent's preferences
  are modeled using a preference model that maps a memory vector encoding past selections
  to a probability distribution over items.
---

# Online Recommendations for Agents with Discounted Adaptive Preferences

## Quick Facts
- arXiv ID: 2302.06014
- Source URL: https://arxiv.org/abs/2302.06014
- Reference count: 40
- Key outcome: This paper studies online recommendation problems with discounted adaptive preferences, providing algorithms and barriers for different memory regimes and preference model classes.

## Executive Summary
This paper addresses online recommendation problems where an agent's preferences evolve based on past selections, using a discounted memory model where recent selections are weighted more heavily than older ones. The authors analyze different regimes of discount factors (γ) and preference model classes to determine when efficient sublinear regret is achievable. They establish both algorithmic results for specific preference structures and information-theoretic barriers for general cases, showing that the discount factor fundamentally changes the feasibility of competing against various regret benchmarks.

## Method Summary
The method extends prior work by introducing discounted memory updates where past selections are multiplied by γ^t at time t, creating an effective memory horizon of O(1/(1-γ)). For long-term memory regimes where the effective horizon scales sublinearly with T, the authors show sublinear regret is achievable against the EIRD set for smooth preference models. For short-term memory with constant horizons, they develop algorithms like EXP-φ that work for pseudo-increasing preferences against the smoothed simplex. The approach combines bandit linear optimization techniques with preference model estimation and careful menu design strategies.

## Key Results
- For long-term memory regimes, efficient sublinear regret is obtainable with respect to the EIRD set for any smooth preference model
- Scale-bounded (pseudo-increasing) preferences enable efficient sublinear regret against nearly the entire item simplex, even without smoothness under restricted adversarial rewards
- NP-hardness result shows fundamental barriers for competing against targets beyond EIRD in general preference models
- Information-theoretic barriers exist for competing against EIRD under arbitrary smooth preference models in short-term memory regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discount factors enable efficient exploration of memory vectors that are infeasible under uniform memory.
- Mechanism: When γ < 1, the effective memory horizon is bounded, allowing the agent's history to be "washed out" after O(1/(1-γ)) rounds. This enables exploration of different memory states that would otherwise be inaccessible.
- Core assumption: The preference model allows item selection probabilities to change significantly when memory changes.
- Evidence anchors:
  - [abstract]: "we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round"
  - [section 3.3]: "Considering discount factors of γ < 1 introduces the possibility that we can efficiently explore the space of feasible vectors"
  - [corpus]: Weak - related papers focus on nonlinear control and multi-agent settings, not discounted memory exploration
- Break condition: If the preference model is insensitive to memory changes, exploration gains disappear.

### Mechanism 2
- Claim: Pseudo-increasing preference models enable competition against nearly the entire simplex by leveraging local feasibility properties.
- Mechanism: When scoring functions satisfy fi(v) ≥ (1-λ)vi + λ, the required "menu time" for any point in the smoothed simplex neighborhood is bounded, enabling realization of those distributions.
- Core assumption: Scoring functions are bounded below by linear functions of memory weights.
- Evidence anchors:
  - [section 3.3]: "We identify a structural property which enables this, namely that scoring functions are pseudo-increasing"
  - [section 3.4]: "This presents a stark contrast with the EIRD benchmark, as the pseudo-increasing property now suggests that it may be possible to persuade the agent to pick the best item in nearly every round"
  - [corpus]: Weak - no direct evidence about pseudo-increasing properties in related work
- Break condition: If scoring functions can be arbitrarily non-monotonic with respect to memory weights.

### Mechanism 3
- Claim: Short-term memory regimes with constant horizons enable sublinear regret through state-space discretization.
- Mechanism: When γ ∈ [0,1) is constant, memory vectors become essentially discrete, creating a finite state space where each state corresponds to a truncated history. This allows treating the problem as multi-armed bandit over states.
- Core assumption: The effective memory horizon is O(1) with respect to T, creating a discrete state space.
- Evidence anchors:
  - [section 4]: "This includes cases where the set of feasible memory vectors is essentially discrete, as there may be a finite grid of well-separated points"
  - [section 4.1]: "The idea behind EXP-φ is to view each vertex of the smoothed simplex as an action for a multi-armed bandit problem"
  - [corpus]: Weak - related work focuses on different bandit formulations without state discretization
- Break condition: If the discount factor approaches 1, the state space becomes continuous again.

## Foundational Learning

- Concept: Bandit linear optimization with adversarially perturbed actions
  - Why needed here: The algorithm needs to handle both the stochastic nature of agent choices and adversarial rewards, while dealing with the uncertainty in preference models
  - Quick check question: How does the algorithm maintain sublinear regret when both the preference model and rewards are uncertain?

- Concept: Discounted regret and effective memory horizons
  - Why needed here: Understanding how discount factors affect the effective memory horizon is crucial for determining which benchmarks are feasible
  - Quick check question: What happens to the effective memory horizon as γ approaches 1 versus when γ is constant?

- Concept: Local learnability and model estimation
  - Why needed here: The algorithm must maintain accurate estimates of the preference model in the neighborhood of the current memory vector to make good decisions
  - Quick check question: How does the algorithm ensure that its model estimates remain accurate as the memory vector changes?

## Architecture Onboarding

- Component map: Memory update module -> Preference model estimator -> Menu distribution optimizer -> Regret benchmark comparator
- Critical path: Memory update → Preference model estimation → Menu optimization → Reward observation → Regret calculation
- Design tradeoffs:
  - Exploration vs exploitation: Balancing between learning the preference model and optimizing menu choices
  - Model accuracy vs computational efficiency: More frequent queries improve accuracy but increase computational cost
  - Discount factor choice: Larger γ preserves more memory information but makes exploration harder
- Failure signatures:
  - Linear regret growth indicates the algorithm cannot effectively compete with the chosen benchmark
  - Model estimation errors accumulating over time suggest insufficient query frequency
  - Memory vector drift exceeding estimation radius indicates poor burn-in or query scheduling
- First 3 experiments:
  1. Test the algorithm with known preference models and fixed γ to verify regret bounds match theory
  2. Vary γ across the spectrum (0, 0.5, 0.9, 0.99) to observe how regret changes with memory horizon
  3. Compare performance against different benchmarks (EIRD vs smoothed simplex) with pseudo-increasing vs general models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we achieve sublinear regret for general smooth preference models in the short-term memory regime without requiring the pseudo-increasing property?
- Basis in paper: [inferred] The paper shows an information-theoretic barrier for competing against the EIRD set under arbitrary smooth preference models even when losses are constant in the short-term memory regime.
- Why unresolved: The paper does not provide an algorithm for this case, and instead shows a lower bound suggesting it is difficult to achieve sublinear regret without additional structural assumptions.
- What evidence would resolve it: A proof of either an upper bound showing sublinear regret is achievable for general smooth models, or a lower bound showing that sublinear regret is impossible without additional assumptions.

### Open Question 2
- Question: What is the optimal regret rate for agents with long-term memory and smooth preference models when competing against the EIRD set?
- Basis in paper: [explicit] The paper gives an algorithm that obtains regret O(T^(1-c/4) + T^c) for this case, but does not claim this is optimal.
- Why unresolved: The paper does not provide a matching lower bound for this specific regret rate.
- What evidence would resolve it: A proof of either a matching lower bound for the regret rate achieved by the algorithm, or an algorithm with a better regret rate.

### Open Question 3
- Question: Can we efficiently compute the EIRD set for a given preference model?
- Basis in paper: [inferred] The paper shows NP-hardness results for expanding to targets beyond EIRD, suggesting that computing EIRD may be difficult.
- Why unresolved: The paper does not provide an algorithm for computing EIRD, and instead focuses on competing against it.
- What evidence would resolve it: An algorithm that can efficiently compute the EIRD set for a given preference model, or a proof that computing EIRD is NP-hard.

## Limitations

- The theoretical guarantees depend heavily on specific structural properties of preference models (pseudo-increasing, smoothness) that may not hold in practice
- NP-hardness results suggest fundamental computational barriers for general preference models, limiting practical applicability
- The transition between short-term and long-term memory regimes is not precisely characterized, making it difficult to determine which algorithms apply in intermediate cases

## Confidence

- **High confidence**: The framework for modeling discounted memory and the general approach to regret minimization against the EIRD set. The mathematical formulation is rigorous and the connection to existing bandit optimization literature is well-established.
- **Medium confidence**: The pseudo-increasing preference model results and the EXP-φ algorithm for short-term memory regimes. While the theoretical analysis appears sound, the practical applicability depends heavily on the assumption about scoring function structure.
- **Low confidence**: The NP-hardness result for general preference models and the precise boundaries between different memory regimes. These claims rely on theoretical constructions that may not reflect practical scenarios.

## Next Checks

1. Implement the EXP-φ algorithm and test it across different discount factors (γ = 0.5, 0.9, 0.99) to empirically verify the transition between short-term and long-term memory regimes and measure regret scaling.

2. Construct concrete preference model examples that satisfy the pseudo-increasing property and test whether the theoretical regret bounds hold in practice, particularly focusing on how the λ parameter affects performance.

3. Design a set of experiments comparing the EIRD benchmark against the smoothed simplex benchmark for both smooth and non-smooth preference models to validate the NP-hardness claim and identify conditions where efficient algorithms exist.