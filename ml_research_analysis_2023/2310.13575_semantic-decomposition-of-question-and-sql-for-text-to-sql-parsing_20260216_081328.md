---
ver: rpa2
title: Semantic Decomposition of Question and SQL for Text-to-SQL Parsing
arxiv_id: '2310.13575'
source_url: https://arxiv.org/abs/2310.13575
tags:
- question
- language
- queries
- table
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Query Plan Language (QPL), a modular intermediate
  representation for complex SQL queries that decomposes them into simple, regular
  sub-queries. The authors developed an automatic translator from SQL to QPL using
  SQL Server query optimization plans, and enriched the Spider dataset with QPL programs.
---

# Semantic Decomposition of Question and SQL for Text-to-SQL Parsing

## Quick Facts
- arXiv ID: 2310.13575
- Source URL: https://arxiv.org/abs/2310.13575
- Authors: [Multiple authors listed]
- Reference count: 31
- Primary result: QPL approach achieves 83.8% execution accuracy on Spider development set vs 74.2% for GPT-3.5-turbo

## Executive Summary
This paper introduces Query Plan Language (QPL), a modular intermediate representation for complex SQL queries that decomposes them into simple, regular sub-queries. The authors developed an automatic translator from SQL to QPL using SQL Server query optimization plans, and enriched the Spider dataset with QPL programs. They demonstrate that training text-to-QPL parsers is more effective than text-to-SQL parsing for semantically equivalent queries, achieving 83.8% execution accuracy on the Spider development set. The QPL approach enables schema-sensitive question decomposition, producing readable natural language explanations of complex queries, with user studies showing non-experts can better detect incorrect complex queries using QPL (67% accuracy) compared to SQL (34% accuracy).

## Method Summary
The authors introduce Query Plan Language (QPL) as a modular intermediate representation for SQL queries, derived from SQL Server execution plans. They automatically translate SQL queries to QPL using SQL Server's query optimizer, creating a corpus of 7,694 QPL programs from the Spider dataset. They then fine-tune Flan-T5-XL models for text-to-QPL and text-to-question-decomposition (QD) tasks using schema encoding methods. QDs are generated from QPL using GPT-3.5-turbo, creating aligned training data. The models are evaluated on execution accuracy against the Spider development set, and user studies compare QPL and SQL interpretability for non-expert users.

## Key Results
- QPL text-to-SQL parsing achieves 83.8% execution accuracy on Spider development set vs 74.2% for GPT-3.5-turbo
- Non-expert users detect incorrect complex queries with 67% accuracy using QPL vs 34% using SQL
- QPL models outperform SQL models on semantically equivalent queries with up to 10% accuracy difference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QPL's modular decomposition improves neural model learning efficiency
- Mechanism: QPL breaks complex SQL queries into simple, regular sub-queries that align better with neural architectures' learning capabilities
- Core assumption: Neural models struggle with SQL's syntactic complexity but can learn simpler, more regular query structures
- Evidence anchors:
  - [abstract]: "training text-to-QPL parsers is more effective than text-to-SQL parsing for semantically equivalent queries"
  - [section 2.3]: "SQL is a challenging target language for neural models" with up to 10% accuracy difference
  - [corpus]: Weak evidence - only 8 related papers found with average FMR 0.342

### Mechanism 2
- Claim: Schema-sensitive question decomposition improves accuracy
- Mechanism: QPL's structure enables creating schema-aware question decompositions that provide better training signals
- Core assumption: Decompositions that reference specific schema elements provide more effective supervision than generic decompositions
- Evidence anchors:
  - [abstract]: "Question Decomposer for data retrieval that is sensitive to database schemas"
  - [section 4.2]: QPL → QD generator creates schema-grounded decompositions
  - [corpus]: No direct evidence found in related papers

### Mechanism 3
- Claim: QPL's interpretability enables better error detection by non-experts
- Mechanism: QPL's step-by-step structure is more accessible than SQL for non-experts to validate query correctness
- Core assumption: Non-experts can better understand modular query steps than complex SQL syntax
- Evidence anchors:
  - [abstract]: "non-experts can better detect incorrect complex queries using QPL (67% accuracy) compared to SQL (34% accuracy)"
  - [section 5.2]: User experiment shows 67% vs 34% accuracy for QPL vs SQL
  - [corpus]: No direct evidence found in related papers

## Foundational Learning

- Concept: SQL execution plans and query optimization
  - Why needed here: QPL is derived from SQL Server query optimization plans, so understanding execution plans is essential
  - Quick check question: Can you explain the difference between a logical query plan and a physical query plan?

- Concept: Common Table Expressions (CTEs) and their execution
  - Why needed here: QPL plans are executed by translating to CTE SQL statements
  - Quick check question: How does SQL Server execute a query with multiple CTEs defined?

- Concept: Schema encoding and database structure
  - Why needed here: Both the QPL generation and question decomposition depend on schema information
  - Quick check question: What's the difference between primary keys and foreign keys, and why are they important for joins?

## Architecture Onboarding

- Component map: SQL Parser → QPL Translator → QPL Executor → Question → Question Decomposer → QPL Generator → Schema Encoder → Text-to-QPL Model → QPL Validator

- Critical path:
  1. SQL query received
  2. Query optimizer generates execution plan
  3. QPL translator converts plan to QPL
  4. QPL validated and executed via CTE conversion
  5. Results returned to user

- Design tradeoffs:
  - QPL verbosity vs. SQL conciseness: QPL is more interpretable but longer
  - Schema encoding richness vs. model complexity: Richer schemas improve accuracy but increase model size
  - Decomposition granularity vs. performance: More decomposition steps improve interpretability but may slow processing

- Failure signatures:
  - Invalid QPL syntax indicates translation errors
  - Mismatched result sets between SQL and QPL execution
  - Question decomposer alignment scores below 0.7 suggest poor decompositions
  - Execution accuracy drops significantly on complex queries

- First 3 experiments:
  1. Test QPL translator on simple SELECT queries to verify basic functionality
  2. Compare execution results of SQL vs. QPL for queries with joins
  3. Measure accuracy of question decomposer on queries with different complexity levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QPL performance scale with increasingly complex queries beyond those in the Spider development set?
- Basis in paper: [inferred] The paper notes that models were tested on Spider dev set with QPLs up to 13 lines and acknowledges this limitation, suggesting the method hasn't been tested on longer QPLs.
- Why unresolved: The evaluation was constrained to relatively simple queries in the Spider dataset, and the authors explicitly acknowledge not testing on longer QPLs.
- What evidence would resolve it: Systematic testing of QPL models on datasets with progressively longer and more complex queries, including benchmarks specifically designed to test scalability of intermediate representation approaches.

### Open Question 2
- Question: What is the impact of incorporating schema information (primary-foreign key relationships, column types, database content) on QPL model accuracy?
- Basis in paper: [explicit] The authors acknowledge that during training they did not use schema information such as primary-foreign key relationships and column types, nor database content, and note this may lead to incorrect join predicates and scan predicates.
- Why unresolved: The current models achieve good performance without this information, but the authors suggest this could be a source of errors and potential improvement.
- What evidence would resolve it: Controlled experiments comparing QPL model performance with and without rich schema information, and measuring specific error type reductions when incorporating this information.

### Open Question 3
- Question: Can user feedback on predicted question decompositions (QDs) be effectively incorporated to improve QPL prediction accuracy?
- Basis in paper: [explicit] The authors mention they are exploring whether users can provide interactive feedback on predicted QDs as a way to guide QPL prediction.
- Why unresolved: This represents a proposed direction for future work rather than an evaluated approach, and the effectiveness of such interactive refinement is unknown.
- What evidence would resolve it: User studies measuring the impact of interactive QD refinement on final QPL accuracy, including metrics on how much human input is needed and how it affects model performance across different query complexities.

## Limitations

- SQL Server dependency: The QPL translation relies on SQL Server's query optimizer, creating potential vendor lock-in and limiting portability to other database systems.
- Limited user study scope: The interpretability evaluation involved only 5 participants and 30 simple queries, raising questions about statistical significance and generalizability.
- Unproven scalability: The method's effectiveness on highly complex queries beyond the Spider dataset remains untested, with explicit acknowledgment of this limitation.

## Confidence

**High Confidence**: The core technical contribution of defining QPL and demonstrating its automatic generation from SQL Server execution plans is well-supported. The execution accuracy improvements on the Spider dataset are clearly demonstrated through controlled experiments.

**Medium Confidence**: The interpretability claims for QPL versus SQL are supported by user studies, but the small sample size and limited query diversity warrant caution in generalizing these results. The mechanism by which QPL improves neural learning efficiency is plausible but not definitively proven through ablation studies.

**Low Confidence**: Claims about QPL's generalizability across database systems and its performance on highly complex queries (beyond those in Spider) are not empirically validated. The long-term maintenance implications of the SQL Server dependency are also uncertain.

## Next Checks

1. **Cross-DBMS Validation**: Test the SQL-to-QPL translation pipeline with PostgreSQL and MySQL to assess portability and identify system-specific assumptions in the current implementation.

2. **Ablation Study on Model Architecture**: Train equivalent models (same architecture, same dataset splits) on SQL, QPL, and a control intermediate representation to isolate the contribution of QPL's specific design choices to performance improvements.

3. **Extended User Study**: Conduct a larger-scale user study (n≥30 participants) with diverse database expertise levels, testing both simple and complex queries across multiple database schemas to validate the generalizability of interpretability claims.