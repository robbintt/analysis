---
ver: rpa2
title: 'KoMultiText: Large-Scale Korean Text Dataset for Classifying Biased Speech
  in Real-World Online Services'
arxiv_id: '2310.04313'
source_url: https://arxiv.org/abs/2310.04313
tags:
- dataset
- bias
- classification
- performance
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'KoMultiText is a large-scale Korean text dataset designed for
  multi-task classification of biased speech, including user preferences, profanities,
  and nine types of bias. It contains 150,000 comments annotated for three tasks:
  multi-class preference (hate to love), binary profanity detection, and multi-label
  bias classification.'
---

# KoMultiText: Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services

## Quick Facts
- arXiv ID: 2310.04313
- Source URL: https://arxiv.org/abs/2310.04313
- Authors: 
- Reference count: 40
- Key outcome: Multi-task BERT models achieve human-level accuracy on Korean biased speech classification with 33% fewer parameters than single-task approaches

## Executive Summary
KoMultiText is a comprehensive dataset of 150,000 Korean online comments annotated for multi-task classification of user preferences, profanities, and nine types of bias. The dataset addresses the challenge of Korean hate speech detection by providing extensive annotations across three classification tasks: multi-class preference (hate to love), binary profanity detection, and multi-label bias classification. Using KR-BERT, KoBigBird, KoELECTRA, and RoBERTa (Korean) models in a multi-task learning framework, the research achieves strong performance across all tasks while reducing parameter count by approximately 67% compared to single-task models.

## Method Summary
The dataset contains 150,000 comments from the DC Inside Real-time Best Gallery, with 38,000 manually labeled samples and 110,000 unlabeled samples. Three classification tasks are defined: Preference (multi-class with 7 levels from hate to love), Profanity (binary detection), and Bias (multi-label with 9 categories including disability, gender, age, and appearance). Four Korean pre-trained BERT-based models are fine-tuned using multi-task learning with shared encoders and task-specific heads. Training uses 3×10^-6 initial learning rate with linear scheduler and 10% warmup, AdamW optimizer with weight decay, and class weights to handle imbalanced data.

## Key Results
- Preference task: 72.28% accuracy and 70.88 F1-score across 7 classes
- Profanity task: 98.43 AUROC and 95.84 F1-score for binary detection
- Bias task: 94.34 average AUROC across all 9 bias categories
- Multi-task models use ~33% fewer parameters than single-task models while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning enables simultaneous classification of user preferences, profanities, and bias, reducing total parameter count by ~67% compared to single-task models.
- Mechanism: By sharing the base BERT encoder across all three tasks, the model learns task-agnostic representations while task-specific heads handle classification.
- Core assumption: Shared representations capture common linguistic patterns across preference, profanity, and bias detection.
- Evidence anchors:
  - [section] "Our proposed architecture effectively solves multiple tasks simultaneously, reporting improved classification performance in each classification task."
  - [section] "multi-task models use ~33% fewer parameters than single-task models while maintaining competitive performance."
  - [corpus] Weak - no direct citation evidence found for this parameter reduction claim in corpus.
- Break condition: If tasks have conflicting optimization objectives or require very different feature representations, performance may degrade.

### Mechanism 2
- Claim: Multi-label bias classification with 9 distinct bias categories improves granularity compared to binary hate speech detection.
- Mechanism: Annotating each comment with multiple binary bias labels allows the model to learn fine-grained discrimination between different types of biased language.
- Core assumption: Comments often contain multiple overlapping biases that cannot be captured by single-label or binary classification.
- Evidence anchors:
  - [abstract] "multi-label classification scheme" and "Nine distinct types of Biases per text"
  - [section] "Our dataset includes extensive annotations for User Preferences, Profanities, and Nine distinct types of Biases per text"
  - [corpus] Weak - corpus shows related work but no direct validation of multi-label approach effectiveness.
- Break condition: If data distribution is too sparse for minority bias categories, model may fail to learn meaningful representations.

### Mechanism 3
- Claim: KR-BERT and other Korean-specific BERT variants achieve superior performance on Korean text classification tasks compared to general-purpose models.
- Mechanism: Korean-specific pretraining on large Korean corpora allows these models to capture unique linguistic features like agglutinative morphology and honorifics.
- Core assumption: Language-specific pretraining provides better initial representations than multilingual or general-purpose models for Korean text.
- Evidence anchors:
  - [section] "We utilize four different Korean pre-trained BERT-based models for our experiments: KR-BERT, KoBigBird, KoELECTRA, and RoBERTa (Korean ver)."
  - [section] "Leveraging state-of-the-art BERT-based language models, our approach surpasses human-level accuracy across diverse classification tasks"
  - [corpus] Weak - corpus shows related Korean BERT work but no direct comparison with non-Korean models.
- Break condition: If Korean-specific pretraining corpus is too small or domain-mismatched, benefits may be limited.

## Foundational Learning

- Concept: Multi-task learning optimization
  - Why needed here: Understanding how to balance multiple loss functions and share parameters across tasks
  - Quick check question: How does gradient flow differ between single-task and multi-task training?

- Concept: Multi-label classification metrics
  - Why needed here: Proper evaluation requires understanding AUROC, PRROC, and F1-score for multi-label settings
  - Quick check question: Why is PRROC particularly important when dealing with imbalanced multi-label datasets?

- Concept: Korean language characteristics
  - Why needed here: Understanding agglutinative morphology and honorifics helps interpret model behavior on Korean text
  - Quick check question: How might Korean morphological structure affect tokenization and subword representation?

## Architecture Onboarding

- Component map: Input Korean text → KR-BERT encoder → Task-specific heads (Preference, Profanity, Bias) → Output predictions
- Critical path: Data preprocessing → Tokenization with Korean tokenizer → BERT encoding → Multi-head classification → Loss computation and backprop
- Design tradeoffs: Multi-task sharing vs. single-task specialization; model complexity vs. computational efficiency; Korean-specific vs. general-purpose models
- Failure signatures: Performance degradation on minority bias classes; overfitting to majority classes; poor generalization to new slang or emerging biases
- First 3 experiments:
  1. Single-task vs. multi-task comparison on Preference task to measure parameter efficiency
  2. Class weight adjustment experiment to address Preference class imbalance
  3. Cross-lingual transfer experiment using non-Korean BERT variants for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features of Korean text make hate speech detection particularly challenging compared to other languages?
- Basis in paper: [explicit] The paper states "classifying Korean hate speech is challenging because the Korean text contains unique linguistic features" and references specialized datasets for Korean hate speech detection.
- Why unresolved: The paper mentions that Korean has unique linguistic features but doesn't specify what these features are or how they specifically impact hate speech detection.
- What evidence would resolve it: A detailed linguistic analysis comparing Korean hate speech patterns to other languages, identifying specific Korean language characteristics that complicate detection.

### Open Question 2
- Question: How would the model performance change if additional platforms like YouTube, online news, and other SNS platforms were included in the dataset?
- Basis in paper: [explicit] The authors state "Future studies will aim to include comments from additional platforms such as YouTube, online news, and other SNS platforms" and note their dataset is limited to one Korean online community.
- Why unresolved: The current dataset is limited to one specific Korean forum, so the model's generalizability to other platforms and contexts is unknown.
- What evidence would resolve it: Experimental results comparing model performance when trained on multi-platform data versus single-platform data, showing changes in accuracy and bias detection capabilities.

### Open Question 3
- Question: What is the optimal balance between computational efficiency and classification performance in multi-task versus single-task models for this application?
- Basis in paper: [explicit] The paper shows multi-task models use ~33% fewer parameters than single-task models while maintaining competitive performance, but single-task models generally outperform multi-task on classification metrics.
- Why unresolved: The paper presents trade-offs between efficiency and performance but doesn't identify an optimal balance point or provide guidelines for choosing between architectures.
- What evidence would resolve it: Systematic experiments varying task complexity, dataset size, and computational constraints to determine when the efficiency gains of multi-task learning outweigh the performance benefits of single-task approaches.

## Limitations

- Dataset composition uncertainty regarding selection methodology for manually labeled samples
- Severe class imbalance in Preference task (Hate: 0.5%, Love: 1.2%) may bias model performance
- Substantial performance variability across bias categories, with some achieving near-perfect AUROC while others perform poorly

## Confidence

**High Confidence**: The multi-task learning framework and overall experimental methodology are well-established approaches. The reported performance metrics for individual tasks (particularly Profanity at 98.43 AUROC) are consistent with expectations for binary classification on well-defined tasks.

**Medium Confidence**: The absolute performance numbers for the Preference task (72.28% accuracy, 70.88 F1) are reasonable but should be interpreted cautiously given the severe class imbalance. The human-level accuracy claim needs context about how human performance was measured.

**Low Confidence**: The cross-task generalization benefits of the multi-task approach are asserted but not rigorously validated. The paper claims improved classification performance through multi-task learning but does not provide direct comparisons showing that the shared representations actually improve individual task performance.

## Next Checks

1. **Class Weight Sensitivity Analysis**: Re-run the Preference task experiments with varying class weight schemes (e.g., inverse frequency weighting, focal loss) to determine if the reported 72.28% accuracy is robust to different imbalance handling approaches.

2. **Category-Specific Model Architecture**: For the poorly performing bias categories (particularly Appearance at 44.65 AUROC), experiment with category-specific model heads or hierarchical classification approaches to determine if the shared multi-task architecture is suboptimal for certain bias types.

3. **Cross-Lingual Transfer Validation**: Compare the performance of Korean-specific BERT variants against non-Korean multilingual models (e.g., mBERT, XLM-R) fine-tuned on the same KoMultiText dataset to empirically validate the claimed benefits of Korean-specific pretraining.