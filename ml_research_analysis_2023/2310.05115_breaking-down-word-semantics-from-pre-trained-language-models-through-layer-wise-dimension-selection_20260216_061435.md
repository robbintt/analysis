---
ver: rpa2
title: Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise
  Dimension Selection
arxiv_id: '2310.05115'
source_url: https://arxiv.org/abs/2310.05115
tags:
- word
- sense
- semantic
- hidden
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to disentangle semantic sense from
  BERT embeddings by applying binary masks to middle outputs across layers, without
  updating pre-trained parameters. The approach leverages layer-wise information and
  trains masks to select relevant dimensions for semantic representation.
---

# Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection

## Quick Facts
- arXiv ID: 2310.05115
- Source URL: https://arxiv.org/abs/2310.05115
- Reference count: 20
- This paper presents a method to disentangle semantic sense from BERT embeddings by applying binary masks to middle outputs across layers, without updating pre-trained parameters.

## Executive Summary
This paper introduces a method to extract disentangled semantic representations from BERT embeddings without updating the pre-trained parameters. The approach uses binary masks to select relevant dimensions across layers, trained via triplet loss to maximize semantic similarity for same-sense pairs while minimizing it for different-sense pairs. The method achieves 70.5% accuracy on the WiC testset, comparable to models explicitly fine-tuned for sense disambiguation, and shows 1-2% improvements over baseline methods on multiple semantic tasks.

## Method Summary
The method applies binary masks to BERT's layer-wise outputs (attention or hidden states) to select k dimensions that encode semantic information. These masks are trained using triplet loss contrastive learning, where anchor-positive pairs (same sense) should have high similarity and anchor-negative pairs (different sense) should have low similarity. Layer-wise cosine similarity is computed between masked representations, and a binary classifier makes final predictions. The approach preserves head-wise information by using attention outputs and captures temporal patterns by computing similarities at each layer rather than collapsing them.

## Key Results
- Achieves 70.5% accuracy on WiC testset, comparable to models explicitly fine-tuned for sense disambiguation
- Incorporating layer-wise information improves performance by 1-2% over baseline methods
- Disentangling semantic sense provides an additional 2% improvement on binary classification tasks

## Why This Works (Mechanism)

### Mechanism 1
Disentangled semantic sense improves classification accuracy over baseline by reducing noise from non-semantic dimensions. Binary masks trained via triplet loss select k dimensions per layer that maximize semantic similarity for same-sense pairs while minimizing it for different-sense pairs, effectively filtering out syntax and other aspects. This works under the assumption that semantic sense is encoded in separable dimensions across BERT layers that can be isolated through contrastive learning.

### Mechanism 2
Layer-wise similarity calculation captures temporal patterns in semantic encoding across BERT layers. Computing cosine similarity between target words at each layer (rather than collapsing layers) preserves information about how semantic representation develops through the network. This mechanism assumes that semantic information emerges progressively through BERT layers and is not uniformly distributed.

### Mechanism 3
Attention outputs preserve head-wise interpretability better than hidden states for semantic representation. Using attention outputs before WAO transformation retains information about which attention heads contribute to semantic encoding, unlike hidden states that lose this information through subsequent transformations. This assumes that attention heads encode interpretable aspects of language information that can be selectively preserved.

## Foundational Learning

- Concept: Triplet loss contrastive learning
  - Why needed here: Enables learning binary masks that separate semantic dimensions from non-semantic ones without labeled semantic sense data
  - Quick check question: What is the mathematical form of the triplet loss used to train the semantic mask, and how does it ensure separation between semantic and non-semantic dimensions?

- Concept: Layer-wise feature extraction
  - Why needed here: Captures how semantic information develops through BERT's architecture rather than losing temporal patterns by collapsing layers
  - Quick check question: How does computing cosine similarity layer-by-layer differ from using a single similarity score, and what advantage does this provide for semantic discrimination?

- Concept: Binary masking for feature selection
  - Why needed here: Reduces dimensionality while preserving most informative dimensions for semantic sense, avoiding noise from irrelevant dimensions
  - Quick check question: What is the size relationship between original BERT dimensions (768) and selected dimensions (k=128), and how does this reduction affect performance?

## Architecture Onboarding

- Component map: Input sentences → BERT tokenization → Layer-wise embeddings (attention/hidden states) → Binary masks (semantic/PoS/SVO) → Masked outputs → Layer-wise cosine similarity → Binary classifier → Prediction
- Critical path: BERT → Mask training (triplet loss) → Mask application → Layer-wise similarity → Binary classification
- Design tradeoffs: Attention outputs preserve interpretability but may be noisier; hidden states are more stable but lose head-wise information. Masking reduces dimensionality but risks losing semantic information if masks are too restrictive.
- Failure signatures: Performance doesn't improve over baseline, masks select random dimensions, layer-wise information doesn't correlate with semantic similarity, attention vs hidden states performance is similar
- First 3 experiments:
  1. Run baseline (sum last 4 layers) on WiC dataset to establish performance floor
  2. Implement layer-wise calculator on attention outputs to test if preserving head information helps
  3. Train semantic mask with n_dim=128 on hidden states and evaluate on CoarseWSD-20 to test masking effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How do disentangled representations perform on Cased vs. Uncased BERT models across different NLP tasks? The paper found Cased BERTbase performs better on their tasks, speculating that cased tokens may contain unique information like proper nouns. This needs systematic evaluation across multiple NLP tasks and model architectures.

### Open Question 2
What is the relationship between tokenizer vocabulary composition and the effectiveness of disentangled representations? The paper hypothesizes that Cased PLM vocabularies are more "fragmented" than Uncased, potentially affecting embedding quality and disentanglement potential. This relationship needs empirical testing with controlled tokenization experiments.

### Open Question 3
Can the masking approach be extended to disentangle aspects beyond semantic sense, such as syntactic structure or pragmatic information? The paper experimented with disentangling PoS and SVO but found no additional improvement. The potential for disentangling other linguistic aspects remains unexplored.

## Limitations

- The assumption that semantic senses can be cleanly separated into k=128 dimensions may not preserve all semantic information comprehensively
- The claim that attention outputs are inherently superior to hidden states lacks strong theoretical grounding and comparative evidence
- The 1-2% performance improvements may not justify the added complexity in practical applications

## Confidence

- **High Confidence**: The experimental methodology is sound with proper train/dev/test splits and clear evaluation metrics
- **Medium Confidence**: The architectural choices are reasonable but lack strong comparative evidence; performance improvements are modest
- **Low Confidence**: The fundamental claim that semantic senses are encoded in separable, filterable dimensions across BERT layers is the most speculative aspect

## Next Checks

1. Run experiments with different values of k (e.g., 64, 256, 512) to determine if the 128-dimension choice is optimal and assess semantic information preservation

2. Implement a direct comparison where the same mask training procedure is applied to both attention outputs and hidden states, controlling for all other variables

3. Conduct an ablation study removing individual layers from the layer-wise similarity calculation to determine which layers contribute most to semantic discrimination