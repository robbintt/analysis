---
ver: rpa2
title: Calibration-compatible Listwise Distillation of Privileged Features for CTR
  Prediction
arxiv_id: '2312.08727'
source_url: https://arxiv.org/abs/2312.08727
tags:
- loss
- distillation
- features
- ranking
- listwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of utilizing privileged features
  (features available during offline training but inaccessible for online serving)
  for click-through rate (CTR) prediction in recommendation systems. The authors identify
  that traditional pointwise knowledge distillation methods fail to capture the non-i.i.d.
---

# Calibration-compatible Listwise Distillation of Privileged Features for CTR Prediction

## Quick Facts
- arXiv ID: 2312.08727
- Source URL: https://arxiv.org/abs/2312.08727
- Authors: 
- Reference count: 40
- Key outcome: Proposed Calibration-compatible Listwise Distillation (CLID) significantly outperforms state-of-the-art baselines in CTR prediction ranking while maintaining or improving calibration.

## Executive Summary
This paper addresses the challenge of utilizing privileged features (features available during offline training but inaccessible for online serving) for click-through rate (CTR) prediction in recommendation systems. Traditional pointwise knowledge distillation methods fail to capture the non-i.i.d. nature of data distribution and relative item order, leading to suboptimal ranking performance. The authors propose Calibration-compatible Listwise Distillation (CLID), which extends privileged features distillation with a carefully designed listwise distillation loss that improves ranking ability while preserving calibration. The method is theoretically proven to be calibration-compatible and validated through extensive experiments on public and production datasets.

## Method Summary
The proposed method consists of a teacher model that uses both non-privileged and privileged features, and a student model that uses only non-privileged features for serving. The teacher and student share the same base architecture but differ in their input feature sets. Knowledge is distilled from teacher to student using a combination of pointwise cross-entropy loss and a calibration-compatible listwise distillation loss based on cross-entropy between normalized predicted click-through rate (pCTR) distributions. The overall loss is a weighted sum of these two components, with the weight ratio tuned to balance ranking performance and calibration.

## Key Results
- CLID significantly outperforms baselines (Base, Base+Pointwise, Base+ListMLE, Base+ListNet) on public datasets in terms of NDCG@10 and on production data in terms of GAUC
- CLID maintains or improves calibration metrics (LogLoss, ECE) compared to baseline methods
- The method demonstrates effectiveness across both public LTR datasets (Web30K, Istella-S) and production advertising data from Alibaba
- Self-distillation variant (CLID-SD) still achieves significant improvements, validating the effectiveness of listwise distillation alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Listwise distillation loss captures inter-item dependencies better than pointwise loss, improving ranking ability.
- **Mechanism:** The listwise loss treats each list of items as a single optimization instance, naturally accounting for the non-i.i.d. nature of data distribution where the click probability of an item is affected by other items on the same page. This enables the student model to learn the relative item order from the teacher model's predictions, which is essential for ranking.
- **Core assumption:** The click probability of an item is influenced by other items displayed on the same page.
- **Evidence anchors:**
  - [abstract] "First, it does not consider the non-i.i.d. characteristic of the data distribution, i.e., other items on the same page significantly impact the click probability of the candidate item."
  - [section] "The listwise loss treats each list as an optimization instance and naturally accounts for the non-i.i.d. nature of items' pCTR."
  - [corpus] Weak evidence; no directly comparable listwise distillation work found.
- **Break condition:** If the non-i.i.d. assumption does not hold (e.g., items are truly independent), the listwise loss provides no benefit over pointwise loss.

### Mechanism 2
- **Claim:** CLID's calibration-compatible listwise loss preserves the probabilistic meaning of predictions while improving ranking.
- **Mechanism:** CLID uses cross-entropy to measure the discrepancy between normalized pCTR distributions from student and teacher models. This design ensures that when both PointCE losses achieve global minima, the listwise distillation loss also achieves global minima, preserving the model's calibration ability.
- **Core assumption:** The PointCE losses of both student and teacher models can achieve global minima simultaneously.
- **Evidence anchors:**
  - [abstract] "We then define the calibration-compatible property of distillation loss and show that commonly used listwise losses do not satisfy this property when employed as distillation loss, thus compromising the model's calibration ability."
  - [section] "We theoretically prove it is calibration-compatible" referring to the listwise loss in Eq. (15).
  - [corpus] No direct evidence; this appears to be a novel theoretical contribution.
- **Break condition:** If the PointCE losses cannot achieve global minima (e.g., due to model capacity limitations), the calibration-compatible property may not hold.

### Mechanism 3
- **Claim:** Privileged features distillation (PFD) framework enables utilization of contextual features without training-serving inconsistency.
- **Mechanism:** The teacher model uses both privileged and non-privileged features for training, while the student model uses only non-privileged features and acts as the final model for serving. Knowledge is distilled from the teacher to the student, allowing the student to benefit from privileged features without the covariate shift issue.
- **Core assumption:** The teacher model's performance with privileged features can be effectively transferred to the student model.
- **Evidence anchors:**
  - [abstract] "A typical practice is privileged features distillation (PFD): train a teacher model using all features (including privileged ones) and then distill the knowledge from the teacher model using a student model (excluding the privileged features), which is then employed for online serving."
  - [section] "The teacher model utilizes the vector obtained from both non-privileged and privileged features as input... The student model only employs the vector obtained from non-privileged features as input and acts as the final model for serving."
  - [corpus] Moderate evidence; several PFD papers cited ([30, 49, 51]) but none with calibration-compatible listwise loss.
- **Break condition:** If the teacher model overfits to privileged features that are not representative of the serving environment, the distilled knowledge may be less effective.

## Foundational Learning

- **Concept:** Knowledge Distillation
  - **Why needed here:** To transfer the ranking ability learned by the teacher model (which has access to privileged features) to the student model (which only has access to non-privileged features).
  - **Quick check question:** What is the difference between knowledge distillation and model ensemble?

- **Concept:** Listwise vs. Pointwise Learning-to-Rank
  - **Why needed here:** Understanding why listwise losses are more suitable for CTR prediction tasks where items are displayed in lists and their click probabilities are interdependent.
  - **Quick check question:** How does ListNet's loss function differ from pointwise cross-entropy in terms of what it optimizes?

- **Concept:** Calibration in CTR Prediction
  - **Why needed here:** Calibration is crucial for business applications like online advertising where predicted click probabilities are used for ranking and charging.
  - **Quick check question:** What is the difference between a well-calibrated model and a model with high accuracy?

## Architecture Onboarding

- **Component map:** Input features → Base Module → Teacher Model (train) → Distillation Loss → Student Model (train) → Student Model (serve)
- **Critical path:** Input features → Base Module → Teacher Model (train) → Distillation Loss → Student Model (train) → Student Model (serve)
- **Design tradeoffs:**
  - Listwise vs. Pointwise distillation: Listwise improves ranking but risks calibration; CLID solves this with calibration-compatible design.
  - Model capacity: Teacher and student models share the same architecture, limiting the potential performance gap but ensuring fair comparison.
  - Hyperparameter sensitivity: The weight ratio (1-α)/α balances pointwise and listwise losses, requiring tuning for optimal performance.
- **Failure signatures:**
  - Calibration degradation: If listwise loss dominates (high weight ratio), the model may lose probabilistic meaning.
  - No ranking improvement: If teacher and student models are too similar, distillation provides little benefit.
  - Training instability: If the distillation loss is not properly scaled, it may cause optimization difficulties.
- **First 3 experiments:**
  1. **Baseline comparison:** Run Base, Base+Pointwise, Base+ListMLE, Base+ListNet, and CLID on a small public dataset (e.g., Web30K) to verify ranking and calibration improvements.
  2. **Weight ratio sensitivity:** Test different (1-α)/α values (e.g., 0.01, 1, 100) on validation data to find the optimal balance between ranking and calibration.
  3. **Ablation study:** Implement SD (CLID) (self-distillation without privileged features) to isolate the effect of privileged features distillation from listwise distillation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed calibration-compatible listwise distillation loss compare to other listwise distillation losses in terms of preserving the model's calibration ability?
- Basis in paper: [explicit] The paper mentions that the commonly used listwise losses can compromise the model's calibration ability when employed as distillation loss.
- Why unresolved: The paper only compares the proposed CLID method to other baselines in terms of ranking and calibration performance, but does not provide a detailed analysis of the calibration compatibility of different listwise distillation losses.
- What evidence would resolve it: Conducting experiments to compare the calibration compatibility of different listwise distillation losses, including the proposed CLID method, and analyzing the results to determine which loss is most effective in preserving the model's calibration ability.

### Open Question 2
- Question: How does the proposed CLID method perform in real-world scenarios with large-scale datasets?
- Basis in paper: [inferred] The paper mentions that the proposed CLID method is validated on both public and production datasets, but does not provide specific details on the performance in real-world scenarios.
- Why unresolved: The paper does not provide a detailed analysis of the performance of the CLID method in real-world scenarios with large-scale datasets.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the CLID method in real-world scenarios with large-scale datasets, and comparing the results to other state-of-the-art methods.

### Open Question 3
- Question: How does the proposed CLID method handle the issue of feature unavailability in online serving?
- Basis in paper: [explicit] The paper mentions that privileged features are not readily available for online serving, and proposes the CLID method to address this issue.
- Why unresolved: The paper does not provide a detailed explanation of how the CLID method handles the issue of feature unavailability in online serving.
- What evidence would resolve it: Providing a detailed explanation of how the CLID method handles the issue of feature unavailability in online serving, including the specific steps and techniques used to ensure the model's performance in the absence of privileged features.

## Limitations

- The calibration-compatible property relies on both teacher and student models achieving global minima for their respective PointCE losses, which may not hold in practice due to model capacity limitations or optimization challenges.
- The effectiveness of listwise distillation depends on the assumption that items' click probabilities are non-i.i.d., which may not hold for all recommendation scenarios.
- The teacher and student models share identical architectures, which may limit the potential performance gap and distillation effectiveness compared to using a more powerful teacher model.

## Confidence

- **High Confidence:** The fundamental approach of using listwise distillation to improve ranking while maintaining calibration through cross-entropy between normalized distributions is well-supported by the theoretical framework and experimental results.
- **Medium Confidence:** The assumption that non-privileged features can effectively capture inter-item dependencies through listwise distillation is reasonable but requires empirical validation across diverse datasets and recommendation scenarios.
- **Medium Confidence:** The calibration-compatible property of the proposed loss function is theoretically proven, but practical implementation may face challenges in achieving the assumed global minima for both teacher and student models.

## Next Validation Checks

1. **Robustness Testing:** Evaluate CLID's performance across diverse recommendation scenarios (e.g., news feeds, e-commerce, music streaming) to verify the non-i.i.d. assumption holds across different item types and user behaviors.

2. **Calibration Analysis:** Conduct detailed analysis of calibration curves before and after distillation to quantify how well the model maintains probabilistic meaning, particularly for extreme probability values (near 0 and 1).

3. **Teacher-Student Gap Analysis:** Systematically vary the capacity gap between teacher and student models to determine the optimal relationship for effective knowledge transfer, as the current paper uses identical architectures which may limit distillation effectiveness.