---
ver: rpa2
title: Towards frugal unsupervised detection of subtle abnormalities in medical imaging
arxiv_id: '2309.02458'
source_url: https://arxiv.org/abs/2309.02458
tags:
- online
- mixtures
- detection
- mixture
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an online Expectation-Maximization algorithm
  for frugal unsupervised anomaly detection in medical imaging. The key idea is to
  use mixtures of multivariate distributions (Gaussian and Student-t) as reference
  models for normal data, and to update the model parameters incrementally as data
  arrives, avoiding the high memory costs of batch processing.
---

# Towards frugal unsupervised detection of subtle abnormalities in medical imaging

## Quick Facts
- arXiv ID: 2309.02458
- Source URL: https://arxiv.org/abs/2309.02458
- Reference count: 40
- Primary result: Online EM for mixture models achieves comparable anomaly detection to auto-encoders while reducing energy consumption from kJ to 10s of kJ and memory from GB to MB

## Executive Summary
This paper proposes an online Expectation-Maximization algorithm for frugal unsupervised anomaly detection in medical imaging. The key idea is to use mixtures of multivariate distributions (Gaussian and Student-t) as reference models for normal data, and to update the model parameters incrementally as data arrives, avoiding the high memory costs of batch processing. The method is demonstrated on detecting subtle structural abnormalities in MR brain scans of de novo Parkinson's patients, where anomalies consistent with disease progression are identified. The approach achieves comparable detection performance to auto-encoder-based methods while significantly reducing energy consumption and memory usage.

## Method Summary
The paper presents an online EM algorithm for fitting mixture models to normal medical imaging data, enabling incremental updates without storing the full dataset. The method uses either Gaussian or multiple-scale t-distributions as component models, with per-dimension weighting via inverse Mahalanobis distances for anomaly scoring. Key innovations include using Sherman-Morrison updates to avoid matrix inversions and maintaining only sufficient statistics for parameter updates. The approach is validated on detecting subtle structural abnormalities in MR brain scans of de novo Parkinson's patients using T1-weighted, FA, and MD volumes.

## Key Results
- Achieved median g-mean scores of 0.56-0.70 for classifying Parkinson's patients by disease stage
- Reduced energy consumption from kJ to 10s of kJ compared to batch processing
- Reduced memory usage from GB to MB through incremental updates

## Why This Works (Mechanism)

### Mechanism 1
Incremental updates using stochastic approximation avoid full-data recomputation. Online EM updates sufficient statistics s(i) using a weighted combination of current observation and prior estimate: s(i) = γi·s̄(yi; θ(i-1)) + (1 - γi)·s(i-1). This makes each step O(M) rather than O(N·M). Core assumption: The data-generating process admits a data augmentation scheme yielding an exponential family complete likelihood.

### Mechanism 2
Using inverse Mahalanobis weights per dimension gives better anomaly scores than global distance. Proximity r(yv; ΘH) = max_m w̄yv_m where w̄yv_m = E[Wm|y; ΘH]. This focuses on dimensions with poor model fit. Core assumption: The MST distribution's scale variables Wm capture local feature relevance.

### Mechanism 3
Sherman-Morrison updates avoid matrix inversions in precision matrix computation. Update S2^{-1} and Σ^{-1} recursively using the Sherman-Morrison formula, so each step avoids O(M^3) inversion. Core assumption: The covariance update can be expressed as a rank-one modification of the previous estimate.

## Foundational Learning

- **Expectation-Maximization (EM) algorithm**
  - Why needed here: Core estimation routine for mixture models; must be adapted to online setting.
  - Quick check question: In EM, what are the two steps computed at each iteration?

- **Data augmentation and exponential family form**
  - Why needed here: Enables online EM updates by providing complete-data sufficient statistics.
  - Quick check question: For a Gaussian component, what is the form of the complete-data sufficient statistics vector s(x)?

- **Sherman-Morrison formula**
  - Why needed here: Allows rank-one updates to matrix inverses/determinants without full recomputation.
  - Quick check question: Given A^{-1} and vectors u, v, what is (A + uv^T)^{-1} in terms of A^{-1}, u, v?

## Architecture Onboarding

- **Component map**: Data pipeline -> Model layer -> Scoring layer -> Decision layer -> Evaluation layer
- **Critical path**: 1. Preprocess control data 2. Fit reference mixture model on control voxels using online EM 3. For each test subject, compute proximity scores voxelwise 4. Apply threshold τα and compute global anomaly percentage 5. Use global percentage as feature for subject-level classifier
- **Design tradeoffs**: Gaussian vs MST (Gaussian cheaper, MST better for elongated clusters); mini-batch vs single-sample (mini-batch more stable but higher memory); precision vs covariance storage (precision saves inverses but needs Sherman-Morrison)
- **Failure signatures**: EM not converging (learning rates too high or initialization poor); proximity scores flat (mixture model underfits or dimensionality too high); threshold selection poor (too many false positives/negatives; re-estimate empirical quantiles)
- **First 3 experiments**: 1. Run online EM on synthetic Gaussian mixture; verify convergence matches batch EM 2. Compare proximity scores for Gaussian vs MST on a small 2D dataset with known outliers 3. Benchmark memory usage of online EM vs batch EM on increasing voxel counts

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, potential open questions include: How would the proposed online EM algorithm perform on larger-scale datasets with more complex brain abnormalities beyond early-stage Parkinson's Disease? What is the optimal trade-off between computational frugality and detection accuracy when using different mixture model configurations? How does the proposed method compare to emerging foundation model approaches for medical image anomaly detection in terms of both performance and resource efficiency?

## Limitations
- The online EM algorithm's convergence and stability depend critically on the learning rate schedule, which is not fully specified in the paper
- The MST mixture model's performance relies on the accurate estimation of the orthogonal matrix D through STIEF optimization
- The anomaly detection results are based on a specific preprocessing pipeline and atlas registration

## Confidence
- **High Confidence**: The core claim that online EM reduces memory usage from GB to MB is well-supported by the algorithmic description and the use of incremental updates
- **Medium Confidence**: The claim of comparable performance to auto-encoder-based methods is supported by the reported g-mean scores, but the direct comparison lacks detailed analysis of false positive/negative rates
- **Low Confidence**: The energy consumption reduction claim (kJ to 10s of kJ) is based on a single comparison and lacks broader benchmarking across different hardware and implementations

## Next Checks
1. **Convergence Analysis**: Run the online EM algorithm on synthetic data with known ground truth and compare convergence behavior against batch EM under different learning rate schedules
2. **Robustness to Preprocessing**: Evaluate the anomaly detection performance on the same dataset but with varying levels of preprocessing quality (e.g., different denoising or registration methods)
3. **Energy Benchmarking**: Measure the actual energy consumption of the online EM implementation on different hardware platforms and compare it against multiple baseline methods (auto-encoders, traditional UAD) under identical conditions