---
ver: rpa2
title: Recursive Algorithmic Reasoning
arxiv_id: '2307.00337'
source_url: https://arxiv.org/abs/2307.00337
tags:
- stack
- hints
- recursive
- node
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes augmenting graph neural networks (GNNs) with
  a stack to enable reasoning over recursive algorithms. The authors argue that existing
  GNN-based algorithmic reasoning approaches cannot properly execute recursive algorithms
  because they lack the memory to store and recall intermediate states like a call
  stack does.
---

# Recursive Algorithmic Reasoning

## Quick Facts
- arXiv ID: 2307.00337
- Source URL: https://arxiv.org/abs/2307.00337
- Reference count: 16
- Key outcome: Stack-augmented GNNs achieve 73% accuracy on DFS for larger graphs compared to 54% for standard GNNs

## Executive Summary
This paper proposes augmenting graph neural networks with a stack to enable reasoning over recursive algorithms. The authors argue that existing GNN-based algorithmic reasoning approaches cannot properly execute recursive algorithms because they lack the memory to store and recall intermediate states like a call stack does. They introduce a stack-augmented GNN where the stack stores and recalls portions of the network state at different timesteps. Empirically, their stack-augmented GNNs significantly outperform standard GNNs on out-of-distribution generalization for DFS, achieving 73% accuracy on larger graphs compared to 54% for the baseline.

## Method Summary
The paper proposes augmenting graph neural networks with a stack to enable recursive algorithmic reasoning, specifically targeting depth-first search (DFS). The approach involves modifying the CLRS-30 benchmark by replacing per-node hints with graph hints that force the network to use the stack for state recall, and collecting outputs sequentially during execution rather than predicting all at once. The stack-augmented GNN learns to store and recall network states analogous to how a call stack operates in recursive algorithms, with stack operations (push, pop, noop) being supervised during training.

## Key Results
- Stack-augmented GNNs achieve 73% accuracy on DFS for larger graphs (out-of-distribution)
- Standard GNNs without stack augmentation achieve only 54% accuracy on the same task
- The proposed approach significantly outperforms baselines on recursive algorithm generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A stack-augmented GNN can learn to store and recall states like a call stack, enabling it to execute recursive algorithms.
- Mechanism: The network learns to push processed node features onto the stack when entering a recursive call and pop them when returning, allowing it to maintain and restore state across recursive steps.
- Core assumption: The network can learn to correctly predict stack operations (push, pop, noop) through supervision, and the stack can store sufficient information to reconstruct prior states.
- Evidence anchors:
  - [abstract]: "The stack allows the network to learn to store and recall a portion of the state of the network at a particular time, analogous to the action of a call stack in a recursive algorithm."
  - [section 3]: "Stack operations are supervised such that the network learns to push and pop at precisely the same times as the recursive algorithm."
  - [corpus]: Weak evidence for recursive algorithm learning; corpus neighbors focus on other NAR approaches without stack-based memory.
- Break condition: If the stack operations are not correctly predicted or the stored state is insufficient, the network will fail to generalize recursive reasoning.

### Mechanism 2
- Claim: Replacing per-node hints with graph hints forces the network to use the stack for state recall rather than relying on implicit global information.
- Mechanism: By providing only hints relative to the current node (e.g., predecessor of the current node, discovery time), the network cannot deduce the next step without storing and recalling state, mimicking recursive algorithm behavior.
- Core assumption: The network cannot implicitly learn per-node information in the recurrent state when only graph hints are provided.
- Evidence anchors:
  - [section 4.1]: "The use of per-node hints in the CLRS-30 implementation is problematic as it implies that the network will not be closely aligned with a recursive algorithm."
  - [section 4.1]: "These new hints are relative only to the current node being explored... With this change, the network will not have enough information to execute DFS without storing and restoring state."
  - [corpus]: No direct corpus evidence; this is a novel design choice in the paper.
- Break condition: If the network finds a way to implicitly encode per-node information despite the hint change, the stack will not be necessary.

### Mechanism 3
- Claim: Collecting outputs sequentially during execution, rather than predicting all at once, aligns the network's behavior with recursive algorithms and improves generalization.
- Mechanism: The network predicts outputs for individual nodes as they are explored, and these predictions are accumulated into a final result, mirroring how recursive algorithms generate results step-by-step.
- Core assumption: Sequential output collection requires the network to maintain intermediate results, encouraging the use of the stack for state management.
- Evidence anchors:
  - [section 4.3]: "Since we use graph hints instead of per-node hints and replace the node-wise hidden state by a graph-level stack, we need to memorize the previous results as part of the graph-level stack element."
  - [section 5]: "Collecting the outputs is a crucial component of aligning with the DFS algorithm as it has a significant impact on accuracy."
  - [corpus]: No direct corpus evidence; this is a specific modification proposed in the paper.
- Break condition: If the network can predict all outputs at once without sequential accumulation, the need for state recall via the stack diminishes.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to process graph-structured data and learn algorithmic reasoning by passing messages between nodes.
  - Quick check question: How does a GNN aggregate information from neighboring nodes during message passing?

- Concept: Recurrent Neural Networks (RNNs)
  - Why needed here: RNNs are used in the CLRS-30 benchmark to apply the GNN across multiple steps, maintaining a hidden state between steps.
  - Quick check question: What is the role of the hidden state in an RNN, and how does it relate to maintaining information over time?

- Concept: Call Stack in Recursive Algorithms
  - Why needed here: Understanding how a call stack stores and recalls state in recursive algorithms is essential to designing the stack-augmented GNN.
  - Quick check question: What happens to the state of a recursive function when a recursive call is made, and how is it restored upon return?

## Architecture Onboarding

- Component map:
  - GNN Processor -> Stack -> Hint Decoder -> Output Decoder
  - Linear layers -> GNN processor -> Stack operations -> Hint and output prediction

- Critical path:
  1. Encode input features using linear layers
  2. Pass features through the GNN processor
  3. Predict stack operation and update the stack
  4. Decode hints and outputs
  5. Accumulate outputs sequentially if using the modified approach

- Design tradeoffs:
  - Node-wise vs. graph-level stack: Node-wise stacks can store per-node information but may be more complex; graph-level stacks are simpler but require more sophisticated pooling mechanisms
  - Teacher forcing: Improves learning stability but may lead to overfitting if used excessively

- Failure signatures:
  - Poor generalization on larger graphs: Indicates the network is not effectively using the stack to recall state
  - High in-distribution accuracy but low out-of-distribution accuracy: Suggests the network is overfitting to the training distribution without learning the underlying recursive reasoning

- First 3 experiments:
  1. Implement the stack-augmented GNN with a graph-level stack and modified hints; evaluate on DFS with sequential output collection
  2. Replace the graph-level stack with a node-wise stack; compare performance on larger graphs
  3. Remove the stack entirely and use only modified hints and sequential output collection; assess the impact on generalization

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but raises implicit questions about the generality of the approach for other recursive algorithms beyond DFS and the extent to which the network truly learns recursive reasoning versus memorizing stack operations.

## Limitations
- The extent to which the network genuinely learns recursive reasoning versus memorizing supervised stack operations remains unclear
- The improvements are only validated on DFS, with no exploration of more complex recursive algorithms
- The impact of teacher forcing on the network's ability to generalize without supervision is not fully characterized

## Confidence

**Confidence Labels:**
- **High confidence**: The empirical results showing improved DFS generalization with stack-augmented GNNs are robust and well-supported by the experimental data
- **Medium confidence**: The architectural design choices (stack augmentation, modified hints, sequential output collection) are logically sound, though their individual contributions to success are difficult to disentangle
- **Low confidence**: The claim that the network learns to "execute recursive algorithms" rather than just memorize stack operations is plausible but not definitively proven

**Major Uncertainties:**
- The extent to which the network genuinely learns recursive reasoning versus memorizing stack operations
- Whether the improvements would transfer to more complex recursive algorithms beyond DFS
- The impact of teacher forcing on the network's ability to generalize without supervision

## Next Checks

1. **Ablation study on stack supervision**: Train the network with reduced supervision on stack operations (e.g., sparse rewards rather than full supervision) to test whether the network can learn stack operations autonomously while maintaining performance.

2. **Transfer to other recursive algorithms**: Evaluate the stack-augmented GNN on recursive algorithms beyond DFS, such as merge sort or quicksort, to assess the generality of the approach for recursive reasoning.

3. **Dynamic stack size analysis**: Systematically vary the maximum stack size during training and testing to determine the minimum stack capacity required for successful recursive reasoning, providing insights into whether the network truly learns state recall or simply uses the stack as additional memory.