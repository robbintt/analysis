---
ver: rpa2
title: 'Explanation Shift: How Did the Distribution Shift Impact the Model?'
arxiv_id: '2303.08081'
source_url: https://arxiv.org/abs/2303.08081
tags:
- shift
- data
- explanation
- distribution
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to monitor machine learning
  models by detecting shifts in their explanations rather than relying solely on shifts
  in input or prediction distributions. The authors introduce the concept of "explanation
  shift" and define the "Explanation Shift Detector," which uses Shapley values to
  attribute feature contributions and detects out-of-distribution model behavior.
---

# Explanation Shift: How Did the Distribution Shift Impact the Model?

## Quick Facts
- arXiv ID: 2303.08081
- Source URL: https://arxiv.org/abs/2303.08081
- Reference count: 40
- This paper proposes detecting out-of-distribution behavior by monitoring shifts in model explanations (Shapley values) rather than input or prediction distributions.

## Executive Summary
This paper introduces the concept of "explanation shift" as a more sensitive indicator of model behavior changes than traditional input or prediction distribution shifts. The Explanation Shift Detector trains a discriminator on the explanation space (Shapley values) to detect when new data interacts differently with the model than training data. Experiments show this approach achieves higher AUC scores for out-of-distribution detection compared to methods based on input or prediction spaces, particularly for detecting multivariate distribution shifts that univariate tests miss.

## Method Summary
The method trains a model fθ on in-distribution data, computes Shapley value explanations for both in-distribution (ID) and out-of-distribution (OOD) data, then trains a linear classifier gψ on the explanation space to distinguish ID vs OOD behavior. The approach uses TreeSHAP for tree-based models and creates labeled datasets of explanations with ID/OOD labels to train the discriminator. The key insight is that explanation shifts capture changes in how features contribute to model predictions, which can occur even when input or prediction distributions remain stable.

## Key Results
- Explanation shift detection achieves higher AUC scores for OOD detection compared to input or prediction space methods
- The approach is more sensitive to multivariate distribution shifts where feature correlations change but marginal distributions remain stable
- Changes in uninformative features don't trigger false positives due to the Shapley value "Dummy" property
- Real-world experiments on ACS datasets show PR18 achieves highest OOD AUC with citizenship as key driver

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation shift detection is more sensitive than input or prediction distribution shifts for detecting model behavior changes.
- Mechanism: The Explanation Shift Detector trains a discriminator on the explanation space (Shapley values) to detect whether new data interacts with the model differently than training data. This space captures the model's internal attribution of feature contributions, which can change even when input or prediction distributions remain stable.
- Core assumption: The explanation space (Shapley values) provides a higher-dimensional representation of model behavior than input or output spaces alone.
- Evidence anchors:
  - [abstract]: "We find that the modeling of explanation shifts can be a better indicator for detecting out-of-distribution model behaviour than state-of-the-art techniques."
  - [section 4.1]: "Proposition 1. Given a model fθ : X→ Y . If fθ(x′)̸= fθ(x), then S(fθ, x′)̸=S(fθ, x)."
  - [corpus]: Weak evidence - no directly comparable methods found in neighbors.

### Mechanism 2
- Claim: Explanation shift can detect multivariate distribution shifts that univariate statistical tests miss.
- Mechanism: When features maintain their marginal distributions but their correlations change, univariate tests on input data fail to detect the shift. However, Shapley values incorporate feature interactions, making explanation distributions sensitive to these changes.
- Core assumption: Shapley values properly account for feature dependencies and interactions in the model.
- Evidence anchors:
  - [section 4.2.1]: "One type of distribution shift that is challenging to detect comprises cases where the univariate distributions for each feature j are equal between the source Dtr and the unseen dataset Dnew, but where interdependencies among different features change."
  - [section 5.1.1]: Empirical demonstration showing SHAP values correctly indicate interaction changes while individual distribution comparisons cannot detect them.
  - [corpus]: Weak evidence - no directly comparable methods found in neighbors.

### Mechanism 3
- Claim: Explanation shift detection avoids false positives from changes in uninformative features.
- Mechanism: Features that don't affect model predictions have Shapley values of zero (the "Dummy" property). Changes in the distribution of such features won't affect explanation distributions, preventing false positive detections.
- Core assumption: The Shapley value satisfies the "Dummy" property - features that don't change predicted values have zero attribution.
- Evidence anchors:
  - [section 4.2.2]: "Another typical problem is false positives when a statistical test recognizes differences between a source distribution and a new distribution, though the differences do not affect the model behavior."
  - [section 5.1.2]: Synthetic example showing that shifting an unused feature changes input distribution but not explanations or predictions.
  - [corpus]: Weak evidence - no directly comparable methods found in neighbors.

## Foundational Learning

- Concept: Shapley values and their properties (efficiency, dummy, symmetry)
  - Why needed here: The method relies on Shapley values as the explanation function S, and their properties determine what explanation shifts can and cannot detect
  - Quick check question: Why do changes in uninformative features not affect explanation distributions? (Answer: Because of the dummy property - features that don't affect predictions have zero Shapley values)

- Concept: Distribution shift types (covariate shift, concept shift, label shift)
  - Why needed here: Understanding what types of shifts the method can and cannot detect requires knowing these fundamental categories
  - Quick check question: Can this method detect concept shift? (Answer: No - it requires labeled data to understand changes in the relationship between predictors and target)

- Concept: Out-of-distribution detection and its challenges
  - Why needed here: The method is positioned as an alternative to traditional OOD detection approaches
  - Quick check question: What is the key limitation of methods that detect OOD based only on input or prediction distributions? (Answer: They don't relate changes to how they interact with trained models)

## Architecture Onboarding

- Component map:
  Model fθ -> Explanation function S -> Explanation Shift Detector gψ

- Critical path:
  1. Train model fθ on training data
  2. Compute explanations S(fθ, Xval) for validation data
  3. Compute explanations S(fθ, Xnew) for new data
  4. Create labeled dataset E with explanations and ID/OOD labels
  5. Train discriminator gψ on E
  6. Evaluate gψ's AUC to determine if explanations differ

- Design tradeoffs:
  - Computational cost: Computing Shapley values is expensive, especially for large datasets
  - Model dependency: The method is specific to the trained model fθ
  - Explanation method choice: Uses TreeSHAP for tree-based models, correlation-dependent for linear models
  - Linear gψ assumption: Simplifies interpretation but may limit detection capability

- Failure signatures:
  - Low AUC (~0.5) on ID data: Indicates method is not sensitive to this dataset
  - High AUC on ID data: Suggests method is too sensitive (false positives)
  - Feature attributions of gψ don't align with known shifts: Indicates model isn't capturing the right aspects
  - Performance degradation without explanation shift detection: Suggests concept shift or other limitations

- First 3 experiments:
  1. Reproduce synthetic multivariate shift example (ρ = 0.2) and verify explanation shift detection while input shift tests fail
  2. Test with an unused feature shift and confirm no explanation change while input distribution changes
  3. Run on real ACS dataset (e.g., ACS TravelTime) and validate that PR18 shows highest OOD AUC with citizenship as key driver

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the Explanation Shift Detector generalize to computer vision and natural language processing tasks?
- Basis in paper: [inferred] The authors note that "The potential utility of explanation shifts as distribution shift indicators that affect the model in computer vision or natural language processing tasks remains an open question."
- Why unresolved: The proposed method relies on Shapley values to derive explanations, which may not be directly applicable to non-tabular data types commonly used in computer vision and NLP tasks.
- What evidence would resolve it: Experiments applying the Explanation Shift Detector to computer vision and NLP tasks, along with comparisons to existing methods for those domains, would demonstrate its effectiveness or limitations.

### Open Question 2
- Question: Can the Explanation Shift Detector detect concept shifts in addition to distribution shifts?
- Basis in paper: [explicit] The authors state "Our approach is not able to detect concept shifts, as concept shift requires understanding the interaction between prediction and response variables."
- Why unresolved: The method relies on comparing explanation spaces between training and new data, but concept shifts involve changes in the relationship between inputs and outputs that cannot be detected without labeled data.
- What evidence would resolve it: Experiments where concept shifts are introduced into the data, along with labeled data for the new concepts, would determine if the Explanation Shift Detector can identify these shifts when the true labels are available.

### Open Question 3
- Question: Are there other AI explanation techniques that could be used in place of Shapley values for the Explanation Shift Detector?
- Basis in paper: [explicit] The authors mention "We have used Shapley values to derive indications of explanation shifts, but we believe that other AI explanation techniques may be applicable and come with their own advantages."
- Why unresolved: While the paper uses Shapley values, it is unclear how other explanation methods like LIME, integrated gradients, or counterfactual explanations would perform in detecting explanation shifts.
- What evidence would resolve it: Implementing the Explanation Shift Detector using different explanation techniques and comparing their performance on various datasets and shift types would reveal which methods are most effective for this task.

## Limitations
- Cannot detect concept shift (changes in the relationship between predictors and target)
- Computational cost of computing Shapley values for large datasets may be prohibitive
- Method is model-specific and requires access to the original model's explanations

## Confidence
- High confidence: The mathematical properties of Shapley values (dummy property, efficiency) and their role in the method are well-established
- Medium confidence: The empirical validation on synthetic datasets is strong, but the real-world ACS dataset experiments lack detail about implementation choices and data preprocessing
- Medium confidence: The claim about superior sensitivity to multivariate shifts is supported by synthetic examples but would benefit from more diverse real-world scenarios

## Next Checks
1. **Concept Shift Detection Gap**: Design an experiment with a dataset where input and label distributions remain stable but the relationship between them changes (concept shift), and verify that explanation shift detection fails while other methods also fail
2. **Computational Scalability**: Measure the runtime and memory requirements for computing Shapley values on progressively larger datasets (10K, 100K, 1M samples) to establish practical limits
3. **Cross-Model Generalization**: Apply the same trained explanation discriminator to explanations from a different but related model (e.g., random forest vs. gradient boosting) trained on the same data to test sensitivity to model architecture