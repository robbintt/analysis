---
ver: rpa2
title: 'Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback'
arxiv_id: '2311.07215'
source_url: https://arxiv.org/abs/2311.07215
tags: []
core_contribution: This paper introduces Coffee, a dataset and framework for code
  editing with feedback. The core idea is to use open-source code LLMs to generate
  corrective feedback for erroneous code by leveraging edit histories from competitive
  programming platforms.
---

# Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback

## Quick Facts
- arXiv ID: 2311.07215
- Source URL: https://arxiv.org/abs/2311.07215
- Reference count: 5
- Primary result: State-of-the-art performance on HumanEvalFix benchmark using feedback generation and preference tuning

## Executive Summary
This paper introduces Coffee, a dataset and framework for code editing with feedback. The core idea is to use open-source code LLMs to generate corrective feedback for erroneous code by leveraging edit histories from competitive programming platforms. CoffeePots, the proposed framework, employs preference-optimized tuning and selection to align feedback generation with the model preference of the code editor. Experiments on the HumanEvalFix benchmark show that CoffeePots achieves state-of-the-art performance, significantly outperforming open-source baselines and even closed-source LLMs like GPT-4.

## Method Summary
CoffeePots is a framework for code editing with feedback that consists of three main components: a critic model that generates feedback for incorrect code, an editor model that edits code based on feedback, and a feedback selector that chooses the best feedback from multiple candidates. The framework uses supervised fine-tuning to train the critic and editor models on the Coffee dataset, which contains incorrect and correct code pairs with feedback annotations. Preference tuning is then applied to align the critic's feedback generation with the editor's model preference, using a preference set constructed from pass ratios on hidden test cases. Finally, the feedback selector uses a binary classifier to choose the most helpful feedback from the N candidates generated by the critic.

## Key Results
- CoffeePots achieves state-of-the-art performance on the HumanEvalFix benchmark, outperforming open-source baselines and even closed-source LLMs like GPT-4.
- Preference tuning significantly improves the quality of feedback generated by the critic model, leading to higher pass@1 rates on the HumanEvalFix benchmark.
- Sampling multiple feedback candidates and using a feedback selector to choose the best one reduces the risk of superficial feedback and improves overall performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference tuning aligns feedback generation with the editor's model preference, improving feedback helpfulness.
- Mechanism: The critic model is trained using preference pairs where feedback candidates are scored based on the editor's pass ratio on hidden test cases. This trains the critic to generate feedback that leads to higher pass ratios.
- Core assumption: The pass ratio of edited code on hidden test cases is a reliable proxy for feedback helpfulness.
- Evidence anchors:
  - [abstract]: "we further align it with the model preference of the editor via preference tuning on the critic"
  - [section 4.3]: "we leverage the preference set to train our feedback selector ϕ to classify the chosen (c+) and rejected (c−) feedback"
  - [corpus]: Weak evidence - corpus contains papers on feedback evaluation but no direct evidence for pass ratio as preference proxy.
- Break condition: If the pass ratio is not a reliable proxy for feedback quality, or if the preference pairs are not representative of true helpfulness.

### Mechanism 2
- Claim: Sampling multiple feedback candidates and selecting the best one reduces the risk of superficial feedback.
- Mechanism: The framework samples N feedback candidates from the critic, then uses a feedback selector to choose the candidate most likely to be helpful based on the editor's preference.
- Core assumption: Among multiple plausible feedback candidates, at least one will be helpful and the selector can identify it.
- Evidence anchors:
  - [abstract]: "we sample multiple plausible feedback candidates from the solution space and select the most helpful feedback from the candidate set"
  - [section 4.4]: "our selector ψ selects the feedback c∗ with the highest probability among the N feedback candidates"
  - [corpus]: Weak evidence - corpus mentions "Coffee-Gym" which evaluates feedback quality but doesn't specifically address sampling strategies.
- Break condition: If the feedback selector is not accurate enough, or if none of the sampled candidates are actually helpful.

### Mechanism 3
- Claim: Using edit histories from competitive programming platforms provides realistic training data for code editing.
- Mechanism: The dataset is constructed from edit histories where users corrected their own incorrect submissions, capturing the natural pattern of error correction.
- Core assumption: Edit histories from competitive programming reflect general patterns of code error correction.
- Evidence anchors:
  - [section 3.1]: "we leverage edit histories from online competitive programming platforms"
  - [section 3.3]: "we collect 742 programming problems from competitive programming platforms"
  - [corpus]: Weak evidence - corpus contains "Coffee-Gym" which also uses competitive programming data but doesn't validate the assumption about generalizability.
- Break condition: If competitive programming error patterns don't generalize to other coding contexts, or if the collected edit histories are biased in some way.

## Foundational Learning

- Concept: Supervised fine-tuning on code editing tasks
  - Why needed here: The critic and editor models need to learn the basic task of generating feedback and editing code respectively before preference tuning can be applied
  - Quick check question: Can the critic generate coherent feedback and can the editor make basic edits without preference tuning?

- Concept: Preference optimization for aligning model outputs with human preferences
  - Why needed here: To ensure the feedback generated is not just plausible but actually helpful for the specific editor model being used
  - Quick check question: Does the preference-tuned critic generate feedback that leads to higher pass rates than the SFT-only critic?

- Concept: Binary classification for feedback selection
  - Why needed here: To efficiently select the most helpful feedback from multiple candidates without requiring expensive evaluation on test cases
  - Quick check question: Does the feedback selector achieve higher accuracy than random selection on held-out preference pairs?

## Architecture Onboarding

- Component map:
  - Critic -> Feedback Selector -> Editor

- Critical path: Critic → Feedback Selector → Editor
  - The critic generates candidates, selector chooses one, editor applies it

- Design tradeoffs:
  - Sampling N candidates increases chances of finding good feedback but increases computation
  - Using pass ratio as preference signal is efficient but may not capture all aspects of feedback quality
  - Binary selector is simpler than ranking but may lose information

- Failure signatures:
  - Low pass@1 indicates either poor feedback or poor editing
  - If selector confidence is uniformly low, the critic may not be generating useful candidates
  - If editor performance drops with feedback, the feedback may be misleading

- First 3 experiments:
  1. Compare pass@1 of SFT-only critic + editor vs baseline editor without feedback
  2. Evaluate preference-tuned critic's feedback quality using GPT-4 scoring
  3. Test feedback selector accuracy on held-out preference pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the feedback selector's confidence measure relate to the actual helpfulness of the feedback?
- Basis in paper: [explicit] "We assume that such confidence serves as a good approximation to the helpfulness of feedback."
- Why unresolved: The paper assumes that the feedback selector's confidence is a good approximation for feedback helpfulness, but this assumption is not empirically validated.
- What evidence would resolve it: An experiment comparing the feedback selector's confidence with human evaluations of feedback helpfulness would provide evidence to support or refute this assumption.

### Open Question 2
- Question: What is the impact of the number of feedback candidates (K) on the performance of the preference tuning process?
- Basis in paper: [inferred] The paper mentions sampling K feedback candidates for preference tuning, but does not explore the effect of varying K.
- Why unresolved: The optimal number of feedback candidates for preference tuning is not explored, which could affect the quality of the preference-tuned critic model.
- What evidence would resolve it: An ablation study varying the number of feedback candidates (K) and measuring the impact on the performance of the preference-tuned critic model would provide insights into the optimal value of K.

### Open Question 3
- Question: How does the performance of COFFEE POTS compare to other state-of-the-art code editing models on benchmarks other than HumanEvalFix?
- Basis in paper: [explicit] The paper only evaluates COFFEE POTS on the HumanEvalFix benchmark.
- Why unresolved: The paper does not provide a comparison of COFFEE POTS with other code editing models on different benchmarks, limiting the generalizability of the results.
- What evidence would resolve it: Evaluating COFFEE POTS on additional code editing benchmarks and comparing its performance with other state-of-the-art models would provide a more comprehensive understanding of its capabilities.

## Limitations
- The pass ratio on hidden test cases is assumed to be a reliable proxy for feedback helpfulness, but this assumption is not directly validated.
- The specific closed-source LLM used for feedback annotation in the Coffee dataset is not disclosed, limiting reproducibility and raising potential bias concerns.
- The generalization of competitive programming error patterns to broader coding contexts is not empirically validated.

## Confidence
- **High confidence** in the technical implementation of the CoffeePots framework, including the supervised fine-tuning, preference tuning, and feedback selection components.
- **Medium confidence** in the effectiveness of the preference tuning approach for aligning feedback generation with editor preferences.
- **Low confidence** in the universal applicability of competitive programming edit histories as training data for general code editing tasks.

## Next Checks
1. Validate the pass ratio preference signal by conducting a human evaluation study comparing developer feedback ratings with pass ratio-based preferences.
2. Test generalization across coding domains by evaluating CoffeePots performance on code editing tasks from diverse sources (e.g., GitHub repositories, Stack Overflow snippets).
3. Perform an ablation study varying the number of sampled feedback candidates (N) and different selection strategies to quantify the impact of these design choices on overall performance.