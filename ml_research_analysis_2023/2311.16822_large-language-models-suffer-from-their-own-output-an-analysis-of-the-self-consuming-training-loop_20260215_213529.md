---
ver: rpa2
title: 'Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming
  Training Loop'
arxiv_id: '2311.16822'
source_url: https://arxiv.org/abs/2311.16822
tags:
- data
- training
- cycle
- self-consuming
- loop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of using AI-generated content in
  training subsequent generations of large language models (LLMs). The authors create
  a novel dataset of logical expressions to accurately measure quality and diversity
  of generated outputs.
---

# Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop

## Quick Facts
- **arXiv ID**: 2311.16822
- **Source URL**: https://arxiv.org/abs/2311.16822
- **Reference count**: 9
- **Key outcome**: Self-consuming training loops with AI-generated data initially improve correctness but eventually cause diversity collapse in LLM outputs.

## Executive Summary
This study examines the impact of using AI-generated content in training subsequent generations of large language models (LLMs). The authors create a novel dataset of logical expressions to accurately measure quality and diversity of generated outputs. They find that iterative retraining on synthetic data initially improves correctness but eventually leads to a collapse in diversity, with models converging to a single output. The rate of diversity loss depends on the proportion of real vs. generated data and the data cycle used. These findings suggest that self-consuming training loops can harm model performance over time, highlighting the need for careful data curation and further research into maintaining diversity in LLM outputs.

## Method Summary
The authors generate a dataset of 10,000 unique True Boolean expressions using a recursive tree construction algorithm with depths between 1 and 5. They train a GPT-style model (6 layers, 6 heads, 384 dim, ~10.6M parameters) on this dataset, sample new expressions, and construct new datasets using four data cycles (full synthetic, balanced, incremental, expanding). The model is retrained from scratch for 50 generations, measuring correctness (syntactic and semantic) and diversity (Levenshtein distance) at each step.

## Key Results
- Iterative retraining on synthetic data initially improves correctness of generated outputs.
- Diversity collapses over generations, with models converging to a single output pattern.
- The rate of diversity loss depends on the proportion of synthetic data and the data cycle used.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-consuming training loops initially improve correctness of generated outputs.
- Mechanism: Each new generation is trained on data that has been increasingly curated toward semantically correct expressions, so the model learns to output more True Boolean expressions.
- Core assumption: The training data distribution becomes progressively biased toward semantically correct expressions across generations.
- Evidence anchors:
  - [abstract] "We find that this self-consuming training loop initially improves both quality and diversity."
  - [section] "Interestingly, the number of semantically correct expressions increases afterwards over the course of the self-consuming training loop."
- Break condition: Once diversity begins to degenerate, correctness gains plateau and diversity collapses.

### Mechanism 2
- Claim: Diversity loss is accelerated by higher proportions of synthetic data in each generation.
- Mechanism: As more synthetic data is used in training, the model's output distribution becomes narrower, leading to convergence toward a single output pattern.
- Core assumption: Synthetic data lacks the variability of real data, so repeated use leads to mode collapse.
- Evidence anchors:
  - [section] "We observe, that the loss in diversity is more profound the more generated data is added in each generation."
  - [section] "Even with as little as 位 = 0.25 we reach the complete loss of diversity within 50 generations."
- Break condition: When synthetic data proportion reaches a threshold, diversity collapse becomes irreversible within a few generations.

### Mechanism 3
- Claim: Different data cycles affect the rate and severity of diversity degeneration.
- Mechanism: Full synthetic cycles cause fastest collapse, balanced cycles slow it, and expanding cycles delay but do not prevent it.
- Core assumption: The way new data is mixed with old data determines how quickly the model's learned distribution becomes degenerate.
- Evidence anchors:
  - [section] "We can see the fastest increase for the full synthetic data cycle... The incremental data cycle takes consistently longer but also nearly reaches this point by generation t = 36."
  - [section] "Only the expanding data cycle sees no decrease yet and is still increasing in diversity at generation t = 50."
- Break condition: Regardless of cycle, given enough generations, all cycles will eventually lead to zero diversity.

## Foundational Learning

- Concept: Self-consuming training loops
  - Why needed here: Understanding how models can be retrained on their own outputs is critical to analyzing the observed phenomena.
  - Quick check question: What is the difference between a full synthetic cycle and a balanced cycle in terms of data composition?

- Concept: Diversity metrics (Levenshtein diversity)
  - Why needed here: Measuring diversity loss quantitatively is essential to track model degeneration.
  - Quick check question: How does normalized Levenshtein distance capture diversity in logical expressions?

- Concept: Data cycles and their impact on model training
  - Why needed here: Different ways of combining old and new data affect how quickly models degenerate.
  - Quick check question: In an incremental data cycle, what happens if 位 = 1 versus 位 = 0.1?

## Architecture Onboarding

- Component map: Dataset generation -> Model training -> Sampling -> Evaluation -> Loop control
- Critical path:
  1. Generate initial dataset D0
  2. Train model Mt on Dt
  3. Sample St from Mt
  4. Construct Dt+1 from D0 and St according to data cycle
  5. Repeat until generation T
- Design tradeoffs:
  - Smaller model size vs. computational feasibility
  - Logical expressions vs. natural language for verifiable correctness
  - Fixed dataset size vs. expanding data cycles
- Failure signatures:
  - Sudden drop in diversity metrics
  - Convergence to a single output pattern
  - Increase in semantically incorrect expressions
- First 3 experiments:
  1. Run full synthetic data cycle for 10 generations and track correctness/diversity
  2. Compare balanced vs. incremental data cycles for diversity retention
  3. Vary 位 in incremental cycle to find threshold for diversity collapse

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the language model affect the rate and severity of diversity loss in self-consuming training loops?
- Basis in paper: [explicit] The authors acknowledge that their experiments were conducted on a smaller GPT-style model due to computational limitations and note that larger models with billions of parameters may exhibit different behavior.
- Why unresolved: The study was limited to a smaller model (10.6 million parameters) due to computational constraints, leaving uncertainty about how larger models would perform in self-consuming training loops.
- What evidence would resolve it: Conducting experiments on larger models with billions of parameters to compare the rate and severity of diversity loss in self-consuming training loops.

### Open Question 2
- Question: Can methods like quality diversity techniques effectively maintain diversity in language model outputs during self-consuming training loops?
- Basis in paper: [explicit] The authors suggest that the machine learning community needs to find ways to maintain diversity of generative model outputs and mention quality diversity methods as a potential avenue for future research.
- Why unresolved: While the authors propose quality diversity methods as a potential solution, they have not been tested or validated in the context of self-consuming training loops for language models.
- What evidence would resolve it: Implementing and evaluating quality diversity techniques in self-consuming training loops to determine their effectiveness in maintaining diversity.

### Open Question 3
- Question: How does the proportion of fresh real data in each generation impact the diversity of language model outputs in self-consuming training loops?
- Basis in paper: [explicit] The authors mention that previous work in the image generation domain suggests that fresh and real data can lead to stability within self-consuming training loops and plan to study this in future work.
- Why unresolved: The study did not explore the impact of varying proportions of fresh real data in each generation on the diversity of language model outputs.
- What evidence would resolve it: Conducting experiments with different proportions of fresh real data in each generation to measure the impact on diversity in self-consuming training loops.

## Limitations

- The study uses synthetic logical expressions rather than natural language, which may not fully capture the complexity of real-world LLM degeneration.
- The controlled environment with a single model architecture and fixed dataset size may not generalize to larger models or more diverse training data scenarios.
- The study only examines one type of logical expression language, potentially limiting the generalizability of diversity collapse patterns.

## Confidence

- **High Confidence**: The observed diversity collapse phenomenon and its dependence on synthetic data proportion (Mechanism 2) are well-supported by the empirical evidence across multiple generations and data cycles.
- **Medium Confidence**: The initial correctness improvements in self-consuming loops (Mechanism 1) are observed, but the study doesn't fully explain why this occurs or how long it persists before plateauing.
- **Low Confidence**: Claims about the universality of these findings across different model architectures and natural language tasks require further validation beyond the controlled synthetic environment.

## Next Checks

1. Replicate the study using natural language generation tasks (e.g., story completion or code generation) to verify if diversity collapse patterns hold for more complex output spaces.
2. Test the self-consuming loop phenomenon with larger model architectures (100M+ parameters) to determine if scale affects the rate or severity of diversity loss.
3. Implement periodic injection of fresh real data at different intervals to measure its effectiveness in preventing or slowing diversity collapse compared to continuous synthetic retraining.