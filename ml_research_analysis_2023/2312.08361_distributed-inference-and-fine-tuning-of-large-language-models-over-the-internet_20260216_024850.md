---
ver: rpa2
title: Distributed Inference and Fine-tuning of Large Language Models Over The Internet
arxiv_id: '2312.08361'
source_url: https://arxiv.org/abs/2312.08361
tags:
- inference
- servers
- server
- each
- throughput
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of cost-efficient inference and
  fine-tuning of large language models (50B+ parameters) using distributed systems
  over the Internet. It proposes a novel fault-tolerant distributed inference algorithm
  that can handle unreliable devices and high-latency networks.
---

# Distributed Inference and Fine-tuning of Large Language Models Over The Internet

## Quick Facts
- arXiv ID: 2312.08361
- Source URL: https://arxiv.org/abs/2312.08361
- Reference count: 40
- One-line primary result: Petals achieves 2.29 steps/s for Llama 2 (70B) inference with 3 T4 GPUs and 1.71 steps/s for BLOOM (176B) inference with 3 A100 GPUs under 1 Gbit/s bandwidth and < 5 ms latency conditions.

## Executive Summary
This paper addresses the challenge of cost-efficient inference and fine-tuning of large language models (50B+ parameters) using distributed systems over the Internet. The authors propose Petals, a fault-tolerant distributed inference system that leverages dual attention caches and decentralized load balancing to achieve high throughput even with unreliable devices and high-latency networks. The system demonstrates that for very large models, distributed inference over the Internet can be faster than local offloading due to the communication overhead being less than parameter loading time during sequential generation.

## Method Summary
The method proposes a fault-tolerant distributed inference algorithm using dual attention caches to quickly recover from server failures and a decentralized load-balancing protocol to maximize system throughput. The approach partitions transformer blocks across multiple devices, with clients holding embedding layers and trainable parameters while servers process transformer blocks. The system uses a distributed hash table for servers to announce capabilities and find optimal block assignments without central coordination. Quantization (4-bit for Llama 2, 8-bit for BLOOM) reduces memory and bandwidth requirements while maintaining model quality.

## Key Results
- Petals achieves 2.29 steps/s for Llama 2 (70B) inference with 3 T4 GPUs
- Petals achieves 1.71 steps/s for BLOOM (176B) inference with 3 A100 GPUs
- System runs up to 10× faster than local offloading for interactive generation of 50B+ parameter models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributed inference of LLMs over the internet can be faster than local offloading when models exceed 50B parameters.
- Mechanism: For very large models, communication overhead of transferring activations over a slow network is less than the time required to load/unload model parameters from local RAM or SSD during sequential autoregressive generation.
- Core assumption: Network latency dominates parameter loading time for 50B+ parameter models during token-by-token inference.
- Evidence anchors:
  - [abstract]: "we observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network"
  - [section 3.1]: "communicating activations over a slow network can be faster than swapping layers from local RAM or SSD"
  - [corpus]: Weak - no direct comparison of communication vs loading times for 50B+ models found in neighbors
- Break condition: If network latency drops significantly below parameter loading latency, or if model size decreases below the threshold where communication overhead becomes prohibitive.

### Mechanism 2
- Claim: Dual attention caches enable fault-tolerant inference by allowing quick recovery from server failures without restarting generation.
- Mechanism: Maintain server-side cache for attention keys/values and client-side cache for intermediate activations. When a server fails, client retrieves next best server and uses client-side cache to restore server state.
- Core assumption: Client can reconstruct server state from cached activations without recomputing entire prefix.
- Evidence anchors:
  - [section 3.2]: "we develop special fault-tolerant inference algorithms and load-balancing protocols" and description of dual cache approach
  - [algorithm 1]: Detailed implementation showing cache restoration on server failure
  - [corpus]: Weak - neighbors discuss distributed inference but not specific dual cache fault tolerance mechanism
- Break condition: If client-side cache becomes too large to store or network latency for cache transfer exceeds recomputation time.

### Mechanism 3
- Claim: Decentralized load balancing maximizes system throughput by optimally assigning transformer blocks to servers based on their capabilities and network conditions.
- Mechanism: Servers periodically announce throughput and block assignments to distributed hash table, new servers join by finding contiguous block interval that increases total throughput the most.
- Core assumption: Throughput can be estimated accurately enough to make optimal block assignment decisions in a decentralized manner.
- Evidence anchors:
  - [section 3.3]: "we develop a decentralzied load-balancing algorithm that assigns transformer blocks to every server to maximize the total system throughput"
  - [appendix D]: Detailed description of load balancing algorithms and throughput estimation
  - [corpus]: Weak - neighbors discuss distributed systems but not specific load balancing for LLM pipeline parallelism
- Break condition: If throughput estimation becomes inaccurate due to changing network conditions or if communication overhead for load balancing updates exceeds benefits.

## Foundational Learning

- Concept: Pipeline parallelism for large language models
  - Why needed here: The system distributes LLM inference across multiple devices by partitioning transformer blocks, requiring understanding of how pipeline parallelism works and its tradeoffs
  - Quick check question: What is the main performance overhead of pipeline parallelism compared to tensor parallelism?

- Concept: Attention mechanism and cache management in transformers
  - Why needed here: The fault-tolerant algorithm relies on maintaining and restoring attention caches across server failures, requiring understanding of how attention works and why caching is necessary
  - Quick check question: Why does autoregressive generation require storing past attention keys and values, and what happens if they're lost?

- Concept: Distributed hash tables and decentralized coordination
- Concept: Why needed here: The load balancing algorithm uses DHT for servers to announce capabilities and find optimal block assignments without central coordination
  - Quick check question: How does a distributed hash table enable decentralized load balancing in this system?

## Architecture Onboarding

- Component map: Clients -> Servers -> Network layer -> Load balancer -> Validator nodes
- Critical path:
  1. Client initiates inference with token prefix
  2. Client finds optimal server chain via shortest path routing
  3. Data flows through pipeline: client → server1 → server2 → ... → serverN → client
  4. Client generates next token and repeats
  5. On failure: client finds replacement server and restores state from dual cache

- Design tradeoffs:
  - Memory vs communication: Dual caches increase client memory usage but reduce recovery time
  - Centralization vs decentralization: Distributed load balancing adds complexity but improves scalability and fault tolerance
  - Precision vs efficiency: Quantization reduces memory/bandwidth but may impact model quality
  - Client complexity vs server simplicity: More logic on clients simplifies server implementation but increases client resource requirements

- Failure signatures:
  - Server failure: RPC timeout, missing cache entries, need for replacement server
  - Network partition: Inability to reach certain servers, degraded throughput
  - Load imbalance: Some servers underutilized while others overloaded
  - Cache corruption: Inconsistent attention states leading to generation errors

- First 3 experiments:
  1. Single-client inference with 2-3 servers: Verify basic pipeline operation and attention cache flow
  2. Induced server failure during inference: Test fault tolerance and cache restoration mechanism
  3. Multi-client load balancing: Verify that servers automatically redistribute blocks for optimal throughput

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The fault tolerance mechanism relies on dual attention caches that could become memory-prohibitive for very long sequences.
- The load balancing algorithm assumes predictable throughput patterns that may not hold in dynamic real-world deployments.
- Security implications of distributed inference over the Internet are not addressed, potentially leaving the system vulnerable to adversarial attacks.

## Confidence
- **High confidence**: The fundamental insight that distributed inference can outperform local offloading for very large models (50B+) is well-supported by the experimental results and aligns with established principles of pipeline parallelism.
- **Medium confidence**: The specific performance claims are likely accurate under the reported experimental conditions, but may not generalize across different network topologies or device configurations.
- **Low confidence**: The security aspects of distributed inference over the Internet are not explored, and the system may be vulnerable to various attacks.

## Next Checks
1. **Network failure simulation**: Implement a comprehensive failure injection framework to test the fault tolerance mechanism under various failure patterns (random, correlated, cascading) and measure recovery time and generation quality degradation.
2. **Memory overhead analysis**: Quantify the client-side memory requirements for dual attention caches across different sequence lengths and model sizes to determine the practical limits of the fault tolerance mechanism.
3. **Security vulnerability assessment**: Conduct penetration testing of the distributed inference system to identify potential attack vectors, including cache poisoning, model extraction, and denial-of-service attacks through strategic server failures.