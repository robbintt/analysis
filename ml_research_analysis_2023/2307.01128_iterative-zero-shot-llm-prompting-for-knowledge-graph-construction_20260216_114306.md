---
ver: rpa2
title: Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction
arxiv_id: '2307.01128'
source_url: https://arxiv.org/abs/2307.01128
tags:
- entities
- entity
- knowledge
- text
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel iterative zero-shot LLM prompting approach
  for knowledge graph construction. The method leverages GPT-3.5 to extract entities,
  relations, and their attributes from unstructured text through a series of well-designed
  prompts, without requiring external resources or human effort.
---

# Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2307.01128
- Source URL: https://arxiv.org/abs/2307.01128
- Reference count: 32
- Key outcome: Novel iterative zero-shot LLM prompting approach for KG construction achieving 98.82% precision and 93.18% recall in entity extraction

## Executive Summary
This paper introduces a novel iterative zero-shot LLM prompting approach for constructing knowledge graphs directly from unstructured text without requiring external resources or human examples. The method leverages GPT-3.5 through a carefully designed sequence of prompts to extract entities, relations, and attributes while maintaining entity consistency across iterations. A semantic aggregation and resolution strategy handles entity and predicate disambiguation through similarity-based clustering and LLM-driven refinement. Experiments on a tourist website dataset demonstrate high precision in entity extraction and successful KG generation with 761 entities and 616 triplets.

## Method Summary
The approach implements a pipeline of iterative zero-shot prompts using GPT-3.5 to extract entities, relations, and attributes from unstructured text. The process begins with text preprocessing and splitting to manage token limits, followed by entity extraction with detailed descriptions and types. Iterative triplet extraction maintains entity consistency while extracting relations, then semantic aggregation groups similar entities and predicates. Cluster disambiguation identifies semantically identical concepts, and schema inference generates hierarchical relationships through iterative hypernym generation. The entire pipeline operates without external knowledge bases or training examples.

## Key Results
- Achieved 98.82% precision and 93.18% recall in entity extraction from tourist website dataset
- Generated KG with 761 entities and 616 triplets covering tourist attractions and services
- Demonstrated 75.31% precision in relation extraction with effective handling of entity disambiguation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative zero-shot prompting with GPT-3.5 can extract entities, relations, and attributes without external resources or human examples.
- Mechanism: The approach decomposes KG construction into specialized prompts, each targeting specific tasks while maintaining consistent context through iterative refinement.
- Core assumption: GPT-3.5's language understanding capabilities are sufficient to identify relevant entities and relationships when guided by well-formed prompts.
- Evidence anchors:
  - [abstract]: "proposes a novel iterative zero-shot LLM prompting approach for knowledge graph construction... without requiring external resources or human effort"
  - [section 4.3.1]: "The devised triplet extraction approach begins with a pre-processing stage... followed by entity extraction and iterative triplet extraction"

### Mechanism 2
- Claim: Semantic aggregation and resolution strategy can disambiguate entities and predicates without relying on external knowledge bases.
- Mechanism: Entities and relations are clustered based on similarity scores from labels, types, and descriptions, then resolved through LLM prompting to identify semantically identical concepts.
- Core assumption: Semantic similarity between entities/relations can be effectively captured through weighted combination of label, type, and description similarity metrics.
- Evidence anchors:
  - [section 4.3.2]: "The first step is to aggregate all semantically similar entities... The final similarity scores are, in both cases, a weighted combination of the previous contributions"
  - [section 4.3.2]: "each group of equal entities or relations returned by the Cluster Disambiguation module is used to compose a further prompt aimed at asking for a unique label"

### Mechanism 3
- Claim: LLM-based schema inference can generate meaningful KG schema without human expertise.
- Mechanism: The approach iteratively generates hypernyms for entity types, creates hierarchical relationships, and refines taxonomy through multiple rounds of aggregation and hypernym generation.
- Core assumption: GPT-3.5 can identify meaningful hypernyms and taxonomic relationships from sets of entity types without explicit domain knowledge.
- Evidence anchors:
  - [section 4.3.3]: "For each cluster, after removing the possible duplicates... finding a set of appropriate hypernyms, each one being related to a distinct cluster subset"
  - [section 4.3.3]: "subsequently, all generated hypernyms and relations are merged across all clusters to remove redundancies"

## Foundational Learning

- Concept: Large Language Model prompting techniques
  - Why needed here: The entire KG construction pipeline relies on carefully designed prompts to extract entities, relations, and schemas from text
  - Quick check question: What are the key differences between zero-shot and few-shot prompting approaches?

- Concept: Semantic similarity metrics and clustering algorithms
  - Why needed here: The entity/predicate resolution stage uses similarity scores to group semantically related concepts before final disambiguation
  - Quick check question: How does the weighted combination of label, type, and description similarity affect clustering quality?

- Concept: Knowledge Graph schema and taxonomy design
  - Why needed here: The schema inference module must understand how to represent hierarchical relationships between entity types
  - Quick check question: What distinguishes a taxonomy from an ontology in the context of KG schemas?

## Architecture Onboarding

- Component map:
  Text Preprocessing -> Entity Extraction -> Iterative Triplet Extraction -> Entity/Predicate Resolution -> Schema Inference -> Final KG Output

- Critical path:
  1. Text split and preprocessing to handle token limits
  2. Entity extraction with detailed descriptions and types
  3. Iterative triplet extraction maintaining entity consistency
  4. Semantic aggregation for efficient resolution
  5. Cluster disambiguation to identify identical concepts
  6. Schema inference through iterative hypernym generation

- Design tradeoffs:
  - Zero-shot approach vs. fine-tuning on domain-specific data
  - Computational cost of multiple LLM calls vs. quality of extraction
  - Granularity of entity types vs. schema complexity
  - Similarity threshold tuning for resolution accuracy

- Failure signatures:
  - High false positive rate in entity extraction (prompts too permissive)
  - Entity resolution failures (similarity thresholds misconfigured)
  - Schema inference producing incoherent taxonomies (hypernym generation flawed)
  - Token limit exceeded during text processing (chunking strategy insufficient)

- First 3 experiments:
  1. Test entity extraction precision on a small corpus with known entities
  2. Evaluate triplet extraction accuracy with varying context window sizes
  3. Measure resolution quality by clustering synthetic entity pairs with known relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed iterative zero-shot LLM prompting approach compare to state-of-the-art supervised methods for knowledge graph construction in terms of accuracy and scalability?
- Basis in paper: [inferred] The paper discusses the challenges of supervised methods and claims that the proposed approach addresses these issues, but does not provide a direct comparison with state-of-the-art methods.
- Why unresolved: The paper lacks a direct comparison with state-of-the-art supervised methods, making it difficult to assess the relative performance of the proposed approach.
- What evidence would resolve it: A comprehensive experimental evaluation comparing the proposed approach with state-of-the-art supervised methods on the same datasets and metrics.

### Open Question 2
- Question: How does the proposed approach handle the generation of high-quality entity descriptions and relation predicates without relying on external knowledge bases or examples?
- Basis in paper: [explicit] The paper claims that the proposed approach generates detailed entity descriptions and relation predicates without relying on external knowledge bases or examples.
- Why unresolved: The paper does not provide a detailed explanation of how the approach generates high-quality entity descriptions and relation predicates without external resources.
- What evidence would resolve it: A detailed analysis of the generated entity descriptions and relation predicates, demonstrating their quality and relevance to the input text.

### Open Question 3
- Question: How does the proposed semantic aggregation and resolution strategy perform in handling entity and predicate disambiguation compared to existing methods?
- Basis in paper: [explicit] The paper proposes a novel semantic aggregation and resolution strategy for handling entity and predicate disambiguation, but does not provide a direct comparison with existing methods.
- Why unresolved: The paper lacks a direct comparison with existing methods for entity and predicate disambiguation, making it difficult to assess the relative performance of the proposed strategy.
- What evidence would resolve it: A comprehensive experimental evaluation comparing the proposed semantic aggregation and resolution strategy with existing methods on the same datasets and metrics.

## Limitations

- Quality depends heavily on prompt design, which is not fully specified in the paper
- Zero-shot nature may struggle with domain-specific terminology requiring expert knowledge
- Similarity thresholds for entity resolution appear arbitrary and may not generalize across domains

## Confidence

- Entity and relation extraction performance claims: **Medium** - High precision/recall reported but based on single tourist website dataset
- Semantic resolution effectiveness: **Low-Medium** - Similarity approach described but lacks detailed validation of threshold choices
- Schema inference quality: **Low** - Iterative process outlined but not rigorously evaluated against ground truth schemas

## Next Checks

1. **Cross-domain evaluation**: Apply the approach to at least two additional domains (e.g., biomedical and legal documents) to assess generalizability of entity extraction and relation identification beyond the tourist website domain.

2. **Ablation study on similarity thresholds**: Systematically vary the α, β, γ similarity coefficients and threshold values to quantify their impact on entity resolution quality and determine optimal configurations for different text characteristics.

3. **Schema quality assessment**: Compare the automatically inferred schemas against manually constructed ground truth schemas using ontology evaluation metrics (coverage, precision, recall) and conduct expert review to assess semantic coherence.