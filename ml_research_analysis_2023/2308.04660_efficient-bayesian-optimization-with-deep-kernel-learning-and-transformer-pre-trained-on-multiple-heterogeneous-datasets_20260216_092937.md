---
ver: rpa2
title: Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained
  on Multiple Heterogeneous Datasets
arxiv_id: '2308.04660'
source_url: https://arxiv.org/abs/2308.04660
tags:
- optimization
- search
- learning
- space
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to Bayesian optimization (BO)
  by introducing a pre-trained surrogate model based on a Gaussian process with a
  deep kernel defined on features learned from a Transformer-based encoder. The model
  is pre-trained on multiple heterogeneous datasets with potentially different input
  spaces, enabling transfer learning to new tasks.
---

# Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets

## Quick Facts
- arXiv ID: 2308.04660
- Source URL: https://arxiv.org/abs/2308.04660
- Reference count: 40
- Key outcome: Achieves state-of-the-art results on HPO-B public datasets, outperforming other transfer BO methods

## Executive Summary
This paper introduces a novel approach to Bayesian optimization that leverages transfer learning through a pre-trained surrogate model based on a Gaussian process with a deep kernel. The model uses a Transformer-based encoder (FT-Transformer) to learn transferable features from multiple heterogeneous datasets with potentially different input spaces. A mix-up initialization strategy is proposed for handling unseen input variables in target tasks, enabling rapid adaptation and improved optimization efficiency.

## Method Summary
The method involves pre-training an FT-Transformer with a deep kernel Gaussian process on multiple heterogeneous source datasets, where the Feature-Tokenizer layer handles variable-length inputs through union embedding tables. For target tasks, the pre-trained model is transferred with mix-up initialization for unseen input variables, followed by fine-tuning using an ELBO loss. The resulting surrogate model is used to construct acquisition functions for efficient Bayesian optimization.

## Key Results
- Demonstrates significant improvements in optimization efficiency compared to existing methods
- Achieves state-of-the-art results on HPO-B public datasets
- Shows effective transfer learning across heterogeneous datasets with different input spaces

## Why This Works (Mechanism)

### Mechanism 1
The FT-Transformer's Feature-Tokenizer layer can handle variable-length input by initializing embedding tables with the union of all source features. During forward-pass, each feature is transformed using its corresponding embedding vector, allowing datasets with different parameter sets to be jointly trained.

### Mechanism 2
For input variables not seen in source tasks, the model initializes their embedding vectors as convex combinations of two randomly selected source embedding vectors, weighted by a random number α ~ U(0,1). This mix-up initialization strategy provides reasonable initial representations that accelerate fine-tuning.

### Mechanism 3
The deep kernel GP uses the learned FT-Transformer features as input to a Gaussian process, combining the representational power of deep learning with the uncertainty quantification of GPs. This provides better uncertainty estimates than standard neural network surrogates while maintaining strong predictive performance.

## Foundational Learning

- **Concept: Gaussian Processes and their role in Bayesian Optimization**
  - Why needed here: The paper uses GP with deep kernel as the surrogate model, so understanding GP fundamentals is essential.
  - Quick check question: What is the key advantage of using GPs over neural networks for surrogate modeling in BO?

- **Concept: Transformer architecture and attention mechanisms**
  - Why needed here: The FT-Transformer is the backbone feature extractor, so understanding its components is crucial.
  - Quick check question: How does the Feature-Tokenizer layer in FT-Transformer handle tabular data differently from standard transformers?

- **Concept: Transfer learning and multi-task learning concepts**
  - Why needed here: The paper's main contribution is enabling transfer learning across heterogeneous tasks.
  - Quick check question: What are the key challenges in transferring knowledge from source tasks with different input spaces to a target task?

## Architecture Onboarding

- **Component map**: Data normalization → Multi-source FT-Transformer pre-training (MSE loss) → GP layer integration (ELBO loss) → Target task transfer (mix-up initialization + fine-tuning)
- **Critical path**: Data normalization → Multi-source FT-Transformer pre-training (MSE loss) → GP layer integration (ELBO loss) → Target task transfer (mix-up initialization + fine-tuning)
- **Design tradeoffs**: Larger FT-Transformer models may provide better zero-shot performance but require more GPU memory and training time; quantile transformation for normalization improves prediction accuracy but may cause issues with GP uncertainty estimation
- **Failure signatures**: Poor pre-training convergence (check if union of source features is too large); slow target task convergence (verify mix-up initialization quality); overfitting during fine-tuning (reduce model capacity or add regularization)
- **First 3 experiments**:
  1. Implement and test the multi-source FT-Transformer pre-training on synthetic heterogeneous datasets
  2. Evaluate the mix-up initialization strategy by comparing it against random initialization on a transfer task
  3. Compare the deep kernel GP performance against standard GP and neural network surrogates on a benchmark HPO problem

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed mix-up initialization strategy compare to other initialization methods for unseen input variables in terms of optimization efficiency and final performance? The paper only mentions the mix-up initialization strategy without comparing it to other potential initialization methods, leaving the effectiveness of this specific strategy unclear.

### Open Question 2
How does the size of the Transformer model impact the sample efficiency and optimization performance of the pre-trained FT-DKL model? The paper only briefly mentions the impact of model size without providing a detailed analysis of how different model sizes affect optimization performance across various tasks.

### Open Question 3
How does the proposed method handle optimization tasks with both numerical and categorical input variables? The paper focuses on numerical features and does not provide empirical results or detailed discussion on how the method handles optimization tasks with mixed input spaces.

## Limitations

- Scalability concerns when the union of source features becomes too large for available GPU memory
- Limited empirical validation of mix-up initialization strategy across diverse types of unseen variables
- Lack of thorough analysis of how source dataset quality and diversity impact transfer learning performance

## Confidence

- **High Confidence**: The mechanism of using FT-Transformer to handle heterogeneous input spaces through union embedding tables is well-supported by the described architecture and experimental results
- **Medium Confidence**: The mix-up initialization strategy's effectiveness across diverse scenarios, while promising, needs more extensive validation across different types of input variable distributions
- **Medium Confidence**: The claim of achieving state-of-the-art results on HPO-B datasets is supported by experimental evidence, though direct comparisons with all relevant methods would strengthen this claim

## Next Checks

1. **Memory Scalability Test**: Systematically evaluate the model's performance and memory usage as the union of source features grows, identifying the practical limits of the approach

2. **Cross-Domain Transfer Validation**: Test the mix-up initialization strategy on target tasks with input variables that are structurally very different from source variables to assess robustness

3. **Ablation Study on Pre-training Duration**: Determine the minimum effective pre-training duration needed to achieve good transfer performance, helping establish practical guidelines for practitioners