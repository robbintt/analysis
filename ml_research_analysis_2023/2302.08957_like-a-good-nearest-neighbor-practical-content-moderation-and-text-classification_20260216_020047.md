---
ver: rpa2
title: 'Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification'
arxiv_id: '2302.08957'
source_url: https://arxiv.org/abs/2302.08957
tags:
- lagonn
- setfit
- robertaf
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a modification to SetFit, a practical few-shot
  text classification method, by incorporating nearest neighbor information from the
  training data into the input text. This modification, called Like a Good Nearest
  Neighbor (LaGoNN), enhances the performance of SetFit without introducing additional
  learnable parameters.
---

# Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification

## Quick Facts
- arXiv ID: 2302.08957
- Source URL: https://arxiv.org/abs/2302.08957
- Reference count: 12
- Primary result: LAGONN improves SetFit performance on content moderation tasks without additional parameters, especially under label imbalance

## Executive Summary
This paper proposes Like a Good Nearest Neighbor (LAGONN), a method that enhances few-shot text classification by incorporating nearest neighbor information from training data into test instances. The approach modifies input text by retrieving the label, distance, and text of the nearest neighbor during optimization, effectively decorating test instances to appear more similar to training examples. Through extensive experiments across five content moderation datasets under various label distributions, LAGONN demonstrates performance matching or exceeding more expensive methods like RoBERTa, particularly in imbalanced scenarios. The method introduces no additional learnable parameters while providing significant practical benefits for real-world moderation systems.

## Method Summary
LAGONN modifies input text by retrieving information about the nearest neighbor (NN) seen during optimization - specifically the label, distance, and text from the training data. This augmented input is then processed by a Sentence Transformer (ST) which is fine-tuned using contrastive learning objectives. The approach can be applied with or without fine-tuning the ST encoder: LAGONN freezes the encoder after initial training while LAGONN exp continues fine-tuning at each step. The method works by forcing the embedding model to map test examples closer to relevant training points it has already optimized for. Experiments were conducted on five content moderation datasets (LIAR, Quora Insincere Questions, Hate Speech Offensive, Amazon Counterfactual, Toxic Conversations) under four label distributions (extreme, imbalanced, moderate, balanced).

## Key Results
- LAGONN outperforms or matches RoBERTa across all label distributions while being significantly faster
- LAGONN excels particularly on imbalanced datasets where traditional methods struggle
- LAGONN cheap (freezing encoder) outperforms all other methods on the Insincere Questions dataset across all balance regimes
- The method shows consistent improvements in average precision and macro-F1 scores compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
Nearest neighbor information decorates test instances to make them appear more similar to training examples, boosting classifier confidence. The model retrieves the nearest neighbor's label, distance, and text from the training set, appends it to the test instance, and encodes this augmented input. This forces the embedding model to map the test example closer to a training point it already optimized for. Core assumption: The embedding model's internal representation is sensitive to appended textual context, and that the nearest neighbor in feature space is semantically relevant. Break condition: If the nearest neighbor is semantically unrelated, the appended context may mislead the model and hurt performance.

### Mechanism 2
Fine-tuning the Sentence Transformer on a small subset and freezing it for later steps preserves learned semantic structure while reducing overfitting on large balanced datasets. After initial SetFit training on a small sample, the embedding model is frozen and the classifier head retrained on all remaining data. This avoids the degradation seen in full fine-tuning on large, balanced sets. Core assumption: The embedding space learned on few examples generalizes to the full dataset; subsequent classifier training is sufficient for final decision boundaries. Break condition: If the initial subset is not representative, the frozen embeddings may bias predictions toward that subset's distribution.

### Mechanism 3
Prepending nearest neighbor information exploits the ST's pretrained or fine-tuned knowledge without introducing additional learnable parameters. By retrieving the NN's label and text from the training set and concatenating it to the test input, the model leverages existing embeddings to improve classification accuracy. Core assumption: The Sentence Transformer's knowledge is transferable to new, unseen examples when provided with context from similar training examples. Break condition: If the nearest neighbor is not semantically similar, the appended information may introduce noise rather than useful context.

## Foundational Learning

- **Contrastive learning in Sentence Transformers**: SetFit fine-tunes Sentence Transformers under a contrastive paradigm to separate examples belonging to different labels in feature space. Quick check: How does cosine similarity loss encourage separation of different labels in the embedding space?

- **Nearest neighbor search in high-dimensional space**: LAGONN relies on retrieving the nearest neighbor from the training set to augment test instances. Quick check: What distance metric is used to find the nearest neighbor in LAGONN, and why?

- **Imbalanced vs balanced label distributions**: The paper studies LAGONN's performance under different label distributions to simulate realistic moderation scenarios. Quick check: How does label imbalance affect the difficulty of learning in text classification tasks?

## Architecture Onboarding

- **Component map**: Sentence Transformer embedding model (e.g., MPNET) -> Nearest neighbor lookup (scikit-learn) -> Text augmentation (appending label, distance, and neighbor text) -> Classifier (logistic regression or kNN)

- **Critical path**: 1. Embed training data with ST. 2. Perform nearest neighbor lookup on embeddings. 3. Augment training and test instances with NN information. 4. Fine-tune ST (optional) and train classifier. 5. For inference, retrieve NN for test instance, augment, embed, classify.

- **Design tradeoffs**: Using nearest neighbor vs. kNN classifier: LAGONN uses NN lookup for augmentation, but a separate classifier is trained on the augmented embeddings. Full fine-tuning vs. freezing ST: Full fine-tuning improves performance on imbalanced data but can degrade on balanced data; freezing preserves learned structure.

- **Failure signatures**: Poor nearest neighbor retrieval (semantic mismatch) → degraded performance. Overly long text augmentation → increased computational cost and potential loss of focus. Inappropriate label distribution → model may overfit or underfit.

- **First 3 experiments**: 1. Compare LAGONN LABEL, TEXT, and BOTH configurations on a small binary dataset to determine the most effective augmentation method. 2. Test LAGONN with and without fine-tuning the ST to assess the impact of embedding adaptation. 3. Evaluate LAGONN's performance under different label distributions (extreme, imbalanced, moderate, balanced) to identify optimal use cases.

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal k-nearest neighbor values for different label distributions and dataset sizes? The paper used k=2 for training and k=1 for inference but did not systematically explore how k values affect performance across different conditions. An ablation study varying k from 1-10 across different label distributions and dataset sizes would resolve this.

### Open Question 2
How does the choice of separator token affect LAGONN performance? The paper used MPNET's separator token "</s>" but did not compare performance using different separator tokens. Comparative experiments using different separator tokens ([SEP], </s>, [MASK], custom tokens) across multiple datasets would provide evidence.

### Open Question 3
What is the impact of LAGONN on multilingual text classification beyond English? The paper only tested on English data and explicitly identifies this as a limitation. Systematic testing of LAGONN on multilingual datasets with diverse language families and scripts would resolve this.

### Open Question 4
How does LAGONN perform in few-shot learning scenarios with fewer than 100 examples? The paper states "We did not study our method when there are fewer than 100 examples" and identifies this as a fascinating topic for future study. Testing LAGONN with 1-99 examples per class across multiple datasets would provide evidence.

### Open Question 5
What is the relationship between the distance metric used for nearest neighbor retrieval and classification performance? The paper uses Euclidean distance but does not explore alternative distance metrics. Comparative experiments using different distance metrics (cosine similarity, Manhattan distance, etc.) for nearest neighbor retrieval while keeping all other aspects constant would provide evidence.

## Limitations
- The computational overhead of nearest neighbor searches during both training and inference is non-trivial and not thoroughly characterized
- The effectiveness of appending nearest neighbor information depends critically on the semantic similarity of retrieved neighbors, yet limited analysis of neighbor quality across different datasets and distributions is provided
- Results may not generalize to domains beyond content moderation and scalability to larger datasets remains unclear

## Confidence

- **High Confidence**: The empirical observation that LAGONN improves SetFit performance across multiple datasets and label distributions, particularly in imbalanced scenarios
- **Medium Confidence**: The mechanism by which nearest neighbor information improves performance (semantic alignment assumption)
- **Low Confidence**: The generalizability of results to domains beyond content moderation and the scalability of the approach to larger datasets

## Next Checks

1. **Neighbor Quality Analysis**: Conduct a systematic analysis of nearest neighbor semantic similarity across datasets and label distributions to quantify how often retrieved neighbors are truly semantically relevant.

2. **Ablation on Text Augmentation**: Systematically test which components of the appended neighbor information (label, distance, text) contribute most to performance gains, and determine optimal formatting strategies.

3. **Computational Overhead Characterization**: Measure and compare the computational costs of LAGONN versus baselines, including both training time and inference latency, across different dataset sizes and dimensions.