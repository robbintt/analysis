---
ver: rpa2
title: Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings
arxiv_id: '2308.02575'
source_url: https://arxiv.org/abs/2308.02575
tags:
- style
- ratings
- content
- feedback
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines GPT-4\u2019s reliability in rating student\
  \ responses in macroeconomics. Eleven iterations of 14 responses were rated across\
  \ content and style dimensions."
---

# Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings

## Quick Facts
- arXiv ID: 2308.02575
- Source URL: https://arxiv.org/abs/2308.02575
- Reference count: 4
- High ICC values (0.94-0.99) demonstrate GPT-4's consistent performance in rating student responses

## Executive Summary
This study examines GPT-4's reliability in rating student responses to macroeconomics questions across 11 iterations over 14 weeks. The model demonstrated high interrater reliability with ICC values ranging from 0.94-0.99, indicating consistent application of evaluation criteria. GPT-4 maintained content scores even when style was altered, showing effective separation of content and style criteria. The study also found high correlation (r=0.87) between content and style ratings, though style rephrasing did not significantly affect content scores. A control test weeks later showed lower ICC values, suggesting potential model drift. The research provides a prompt framework and sample responses for replication.

## Method Summary
The study used 14 student responses to 7 macroeconomics questions, rated across 11 iterations over 14 weeks. GPT-4 was prompted with a structured framework including role specification, stepwise task description, and explicit formatting instructions. The model was set to temperature=0.0 to ensure deterministic behavior. Ratings were collected via API calls and analyzed using ICC calculations, correlation analysis, and paired t-tests. Style rephrasing was tested by altering answers while preserving content. Statistical analysis focused on interrater reliability, correlation between content and style ratings, and mean rating differences before and after style changes.

## Key Results
- ICC values of 0.94-0.99 indicate extremely high interrater reliability
- Content and style ratings are highly correlated (r=0.87)
- GPT-4 maintains content scores even when style is altered
- ICC values decreased in control test conducted weeks later, suggesting model drift

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High ICC values indicate GPT-4's consistent application of evaluation criteria across multiple iterations.
- Mechanism: The Intraclass Correlation Coefficient (ICC) quantifies agreement among raters. High ICC values (0.94-0.99) demonstrate that GPT-4's ratings remain stable and reliable when applied repeatedly to the same set of responses under controlled conditions.
- Core assumption: The test responses are sufficiently varied and representative to test the model's consistency across different types of content and style.
- Evidence anchors:
  - [abstract] "High interrater reliability was found with ICC values of 0.94-0.99, indicating consistent performance."
  - [section] "The ICC values for both absolute agreement and consistency for content and style are extremely high (0.999), suggesting almost perfect agreement and consistency among raters."
  - [corpus] Weak evidence - the corpus neighbors focus on binary classification and latent content analysis, not ICC-based consistency in educational rating contexts.
- Break condition: If the test set lacks sufficient variation or the prompt framework is ambiguous, the high ICC could reflect overfitting to a narrow task rather than true consistency.

### Mechanism 2
- Claim: GPT-4 maintains content ratings even when style is altered, showing effective separation of content and style criteria.
- Mechanism: The model is prompted to evaluate content and style separately with clear instructions and examples. This allows it to isolate semantic correctness from stylistic appropriateness, ensuring that content scores are not unduly influenced by stylistic changes.
- Core assumption: The prompt structure successfully constrains the model to evaluate content and style as distinct dimensions.
- Evidence anchors:
  - [abstract] "GPT-4 maintained content scores even when style was altered, showing effective separation of these criteria."
  - [section] "Rephrasing the answers stylistically did not significantly affect the content ratings, implying that GPT-4 was able to separate content from style in its evaluations."
  - [corpus] No direct evidence in corpus; related studies focus on style transfer and style vectors but not on maintaining content ratings across style variations.
- Break condition: If the prompt is ambiguous or the model's understanding of content vs. style boundaries is fuzzy, altering style could inadvertently affect content ratings.

### Mechanism 3
- Claim: The prompt framework enables adaptation to new questions while maintaining consistency in output format and evaluation quality.
- Mechanism: A tightly structured prompt with role prompting, stepwise task description, and explicit formatting instructions constrains the model's behavior. This "scaffold" reduces variability and guides the model to produce consistent, well-formatted feedback across different input pairs.
- Core assumption: The prompt's rigid structure and deterministic settings (e.g., temperature=0) effectively control the model's output variability.
- Evidence anchors:
  - [abstract] "The prompt framework and sample responses are provided for replication."
  - [section] "By forcing the model into a deterministic behavior, it becomes more consistent in its outputs, while the chances to produce very good or very bad generations decrease."
  - [corpus] Weak evidence - corpus studies discuss prompt engineering and consistency in classification but not specifically in the context of educational feedback with sample solutions.
- Break condition: If the prompt framework is too rigid, it may fail to adapt to questions that require different evaluation criteria or if the model's understanding of the task shifts over time.

## Foundational Learning

- Concept: Intraclass Correlation Coefficient (ICC)
  - Why needed here: ICC is the primary metric used to quantify GPT-4's consistency across multiple rating iterations. Understanding ICC helps interpret the high agreement scores reported in the study.
  - Quick check question: If ICC = 0.95, what does this say about the consistency of ratings among raters?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The study relies on a carefully crafted prompt framework to guide GPT-4's evaluations. Understanding how prompt structure and sample solutions influence model behavior is key to replicating or improving the approach.
  - Quick check question: How does providing a sample solution in the prompt help control the quality and consistency of GPT-4's feedback?

- Concept: Style transfer and content preservation
  - Why needed here: The study tests whether GPT-4 can maintain content ratings when the style of answers is altered. Understanding style transfer helps interpret the results of this manipulation.
  - Quick check question: If an answer is rephrased in a very informal style, what would you expect to happen to its content vs. style ratings, and why?

## Architecture Onboarding

- Component map:
  - Prompt framework (role, task, stepwise instructions, format) -> Test response set (14 answers per question, varying in quality and style) -> GPT-4 API interface (classEx system, temperature=0) -> Statistical analysis pipeline (ICC calculation, correlation analysis, skewness measures) -> Replication materials (prompt, sample solutions, test responses)

- Critical path:
  1. Prepare prompt framework with question and sample solution
  2. Input test responses to GPT-4 via API
  3. Collect content and style ratings
  4. Calculate ICC to measure consistency
  5. Analyze correlation between content and style ratings
  6. Test robustness by altering style and recalculating ratings

- Design tradeoffs:
  - High consistency (ICC) vs. adaptability to new question types
  - Rigid prompt structure vs. flexibility in evaluation criteria
  - Deterministic settings (temperature=0) vs. model creativity

- Failure signatures:
  - ICC drops significantly in later iterations (model drift)
  - Content ratings change when style is altered (poor separation of criteria)
  - High variance in ratings for similar responses (inconsistent application)

- First 3 experiments:
  1. Replicate the ICC calculation using a different set of test responses to check if the high consistency holds for new material.
  2. Alter the prompt's stepwise instructions to be less rigid and measure the impact on ICC and correlation.
  3. Introduce a new style manipulation (e.g., academic vs. casual) and test whether content ratings remain stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance and reliability of GPT-4 compare to GPT-3.5 when used for automated essay grading in higher education settings?
- Basis in paper: [explicit] The paper mentions that GPT-4's capabilities are compared to GPT-3.5 as an emergent capability, suggesting a need for direct comparison to substantiate claims about GPT-4's superiority.
- Why unresolved: The study primarily focuses on GPT-4's performance and does not provide a comparative analysis with GPT-3.5. The emergent nature of GPT-4's capabilities in providing feedback is highlighted, but a direct comparison is not conducted.
- What evidence would resolve it: A study that directly compares the performance, consistency, and reliability of GPT-4 and GPT-3.5 in automated essay grading tasks, using the same set of criteria and evaluation methods.

### Open Question 2
- Question: What are the long-term implications of using AI models like GPT-4 for educational assessments, particularly in terms of model drift and consistency over time?
- Basis in paper: [explicit] The paper notes that ICC values were lower in a control test conducted weeks later, suggesting potential model drift and the need for ongoing evaluation for long-term use.
- Why unresolved: The study is preliminary and conducted over a 14-week period, which is insufficient to fully understand the long-term implications of using AI models for educational assessments. The potential for model drift and changes in consistency over extended periods is acknowledged but not fully explored.
- What evidence would resolve it: Longitudinal studies that track the performance and consistency of AI models like GPT-4 over several years, examining how model drift and other factors affect their reliability and effectiveness in educational settings.

### Open Question 3
- Question: How can the transparency and interpretability of AI models like GPT-4 be improved to address the "black box" problem in educational settings?
- Basis in paper: [inferred] The paper discusses the challenges of AI models' lack of transparency and interpretability, which can hinder their effective use in educational settings. It suggests that further research is needed to enhance the transparency and interpretability of AI models.
- Why unresolved: While the paper acknowledges the "black box" problem, it does not provide specific solutions or methodologies to improve the transparency and interpretability of AI models. This remains an open challenge in the field.
- What evidence would resolve it: Research that develops and tests methods to make AI models like GPT-4 more transparent and interpretable, such as explainable AI techniques, model visualization tools, or user-friendly interfaces that allow educators to understand and trust the model's decisions.

## Limitations
- The study relies on a single macroeconomic test set and a fixed prompt framework, raising questions about generalizability to other domains or evaluation tasks
- Potential model drift over time is observed but not fully characterized; the exact API version and seed settings are not documented
- No comparison with human rater consistency or other automated rating systems is provided for benchmarking

## Confidence
- High confidence: GPT-4 demonstrates high ICC values (0.94-0.99) and consistent rating behavior under controlled conditions with a fixed prompt and temperature=0
- Medium confidence: GPT-4 effectively separates content and style criteria, as evidenced by stable content scores across style variations
- Low confidence: The prompt framework's generalizability to new questions and long-term reliability without ongoing evaluation

## Next Checks
1. Replicate the ICC calculation using a different subject domain (e.g., literature or history) with new test responses to verify consistency across domains
2. Compare GPT-4's interrater reliability against human raters on the same test set to establish relative performance
3. Document and test across multiple GPT-4 API versions and seeds to characterize the extent of model drift and its impact on reliability