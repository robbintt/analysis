---
ver: rpa2
title: Replication of Multi-agent Reinforcement Learning for the "Hide and Seek" Problem
arxiv_id: '2310.05430'
source_url: https://arxiv.org/abs/2310.05430
tags:
- learning
- arxiv
- agents
- agent
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper replicates and extends the OpenAI hide-and-seek multi-agent
  RL experiment by adding 3D flying agents and curriculum learning. The authors trained
  two agent types: hiders (using PPO) that hide from seekers (using MA-POCA) by locking
  props in doorways, and seekers that navigate and tag hiders.'
---

# Replication of Multi-agent Reinforcement Learning for the "Hide and Seek" Problem

## Quick Facts
- arXiv ID: 2310.05430
- Source URL: https://arxiv.org/abs/2310.05430
- Reference count: 40
- Primary result: Replicated and extended OpenAI hide-and-seek with 3D flying agents, curriculum learning, and reduced training resources

## Executive Summary
This paper replicates the OpenAI hide-and-seek multi-agent RL experiment by introducing 3D flying agents, enhanced observations, and curriculum learning. The authors trained hiders using PPO and seekers using MA-POCA to develop emergent strategies like fort-building and chasing. Results demonstrated that curriculum learning accelerated strategy emergence while reducing batch size (3072 vs 64000) and training time compared to the original work.

## Method Summary
The study uses Unity ML-Agents to simulate a hide-and-seek environment with two agent types: hiders (PPO) and seekers (MA-POCA). Hiders hide from seekers by locking props in doorways, while seekers navigate and tag hiders. The system incorporates 3D flying mechanics, spatial and frontal sensors, ray-casts, and a progressive curriculum with four difficulty levels. Training proceeds through parallel environment instances, with cumulative rewards triggering level progression. Hiders receive positive rewards for remaining hidden, while seekers earn rewards for visual contact and successful tagging.

## Key Results
- Hiders developed chasing and fort-building strategies earlier than the original work
- Training achieved similar emergent behaviors with 95% smaller batch size (3072 vs 64000)
- Seekers learned to maintain visual contact and navigate around obstacles using collective rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum learning accelerates policy convergence by progressively increasing environmental complexity.
- Mechanism: Agents start with simple tasks and gradually face more difficult challenges as their cumulative reward exceeds a threshold, allowing them to build foundational skills before tackling complex scenarios.
- Core assumption: Agents can transfer knowledge learned in simpler environments to more complex ones, and that gradual difficulty scaling prevents overwhelming the learning process.
- Evidence anchors:
  - [abstract] "Results showed hiders developed chasing and fort-building strategies earlier and more efficiently than the original work, with a smaller batch size... and significantly less training time."
  - [section] "Curriculum learning is applied to educate the agents on how to play the game of hide and seek more effectively... By gradually escalating the level of difficulty, curriculum learning is integrated into the game."
  - [corpus] Weak - no direct corpus evidence found.
- Break condition: If the reward threshold is set too high, agents may never progress to harder levels; if set too low, they may skip important learning stages.

### Mechanism 2
- Claim: Reducing negative rewards improves navigational learning by preventing the agent from being penalized for correct directional movement.
- Mechanism: By minimizing constant negative rewards for not having the target in view, agents can explore and learn optimal paths without being discouraged by temporary setbacks during navigation.
- Core assumption: The absence of negative rewards does not lead to stagnation or random behavior, and positive rewards are sufficient to guide learning.
- Evidence anchors:
  - [abstract] "Seekers are given a +0.001 reward for every frame they see hiders in their field of vision... and no reward when they do not have them in sight."
  - [section] "This is done because if we give a negative reward for such a constant frame rate just because the seeker’s field of vision does not contain hiders, we are damaging the path navigation behavior."
  - [corpus] Weak - no direct corpus evidence found.
- Break condition: If positive rewards are too sparse, agents may not learn to seek the target effectively.

### Mechanism 3
- Claim: Multi-agent competition with shared rewards encourages cooperative behavior among seekers.
- Mechanism: When multiple seekers receive cumulative rewards for successful actions, they learn to coordinate and share information to maximize collective success rather than competing against each other.
- Core assumption: Shared reward structures align individual agent goals with team objectives, and that agents can effectively communicate or infer the best cooperative strategies.
- Evidence anchors:
  - [abstract] "Seekers learned to maintain visual contact and navigate around obstacles... Seekers are trained in a group of 2-4 multi-agents and assigned cumulative rewards."
  - [section] "Seeker’s behavior shows they tend to, with time, explore random paths less and use the target’s location observation to calculate the shortest path... This behavior helps the seekers to put hiders easily in their FOV."
  - [corpus] Weak - no direct corpus evidence found.
- Break condition: If the reward sharing is not balanced, some agents may become freeloaders, relying on others to do the work.

## Foundational Learning

- Concept: Reinforcement Learning Fundamentals
  - Why needed here: The entire study relies on agents learning optimal policies through trial and error using reward signals.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms, and which is used by PPO and MA-POCA?

- Concept: Multi-Agent Systems and Competition
  - Why needed here: The study involves multiple agents interacting in a competitive environment, requiring understanding of how agents can learn from each other.
  - Quick check question: How does the reward structure in multi-agent RL differ from single-agent RL, and what challenges arise from shared environments?

- Concept: Curriculum Learning
  - Why needed here: The study uses curriculum learning to progressively increase task difficulty, which is central to the accelerated learning claims.
  - Quick check question: What are the key components of a curriculum learning framework, and how do you determine the difficulty progression?

## Architecture Onboarding

- Component map:
  Unity ML-Agents Environment -> Hider Agents (PPO) -> Spatial/Frontal Sensors -> Ray-casts -> Curriculum Manager -> Reward Signal System
  Unity ML-Agents Environment -> Seeker Agents (MA-POCA) -> Spatial/Frontal Sensors -> Ray-casts -> Curriculum Manager -> Reward Signal System

- Critical path:
  1. Initialize environment with basic level (Level 1).
  2. Agents collect observations and take actions.
  3. Rewards are calculated and sent back to agents.
  4. Neural networks update policies based on experiences.
  5. Cumulative rewards are checked against thresholds.
  6. If threshold met, advance to next level; else, continue training.
  7. Repeat until desired behavior emerges or maximum steps reached.

- Design tradeoffs:
  - Sensor resolution vs. computational cost: Higher resolution sensors provide more detailed information but increase processing time.
  - Batch size vs. learning stability: Smaller batch sizes reduce memory usage but may lead to noisier gradient estimates.
  - Reward shaping vs. sparse rewards: Dense rewards can guide learning but may lead to exploitation of the reward function.

- Failure signatures:
  - Agents get stuck in local optima: They learn suboptimal strategies that don't generalize.
  - Training plateaus: Cumulative rewards stop improving despite continued training.
  - Unexpected behaviors: Agents exploit glitches or unintended mechanics in the environment.
  - Resource exhaustion: Memory or processing power limits prevent scaling up experiments.

- First 3 experiments:
  1. Train hider agents with curriculum learning disabled to establish baseline performance.
  2. Train seeker agents with shared rewards to observe cooperative behavior emergence.
  3. Run both agent types together in the simplest environment to test interaction dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does curriculum learning benefit hider agents for using tools?
- Basis in paper: [explicit] The paper states: "We also used curriculum learning and self-play" and "Curriculum learning is applied to educate the agents on how to play the game of hide and seek more effectively."
- Why unresolved: While the paper shows that curriculum learning was used, it does not explicitly state whether it benefited hider agents for using tools.
- What evidence would resolve it: A comparison of the performance of hider agents using tools with and without curriculum learning would resolve this question.

### Open Question 2
- Question: Does designing reward function specific to the goal accelerate the emergent strategy?
- Basis in paper: [explicit] The paper states: "We also discuss the importance of reward function design and deployment in a curriculum-based environment to encourage agents to learn basic skills."
- Why unresolved: While the paper discusses the importance of reward function design, it does not explicitly state whether designing a reward function specific to the goal accelerates the emergent strategy.
- What evidence would resolve it: A comparison of the performance of agents with and without a reward function specific to the goal would resolve this question.

### Open Question 3
- Question: Does constant negative reward damage Agent's navigational phase?
- Basis in paper: [explicit] The paper states: "We did two experiments by training the hider agents until 3.3M steps in both traditional and curriculum learning. In comparison to traditional non-curriculum learning experiments, there was no increase in reward cumulative rewards as shown in the graph Figure 1."
- Why unresolved: While the paper shows that constant negative reward was used, it does not explicitly state whether it damages Agent's navigational phase.
- What evidence would resolve it: A comparison of the performance of agents with and without constant negative reward would resolve this question.

## Limitations
- The study lacks corpus evidence for key theoretical mechanisms like curriculum learning effectiveness and reward shaping
- Evaluation focuses on strategy emergence timing without comprehensive analysis of strategy robustness or generalization
- Claims about accelerated learning rely on comparison to a single baseline without broader benchmarking

## Confidence
- **High confidence**: The replication of basic hide-and-seek mechanics and demonstration of emergent behaviors are well-supported
- **Medium confidence**: Claims about curriculum learning acceleration and reduced resource requirements have comparative timing data but lack statistical validation
- **Low confidence**: Theoretical mechanisms underlying observed behaviors are asserted without empirical validation or citation support

## Next Checks
1. Conduct ablation studies removing curriculum learning to quantify its specific contribution to learning acceleration and strategy emergence timing.
2. Implement statistical significance testing comparing convergence rates and resource usage across multiple training runs with different random seeds.
3. Test strategy robustness by evaluating agent performance across varied environment configurations and against alternative opponent strategies.