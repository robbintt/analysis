---
ver: rpa2
title: Weakly-supervised Automated Audio Captioning via text only training
arxiv_id: '2309.12242'
source_url: https://arxiv.org/abs/2309.12242
tags:
- audio
- text
- embeddings
- training
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a weakly-supervised approach for Automated
  Audio Captioning (AAC) that requires no audio data during training, addressing the
  challenge of limited paired audio-caption datasets. The method leverages the aligned
  multi-modal latent space of a pre-trained Contrastive Language-Audio Pretraining
  (CLAP) model.
---

# Weakly-supervised Automated Audio Captioning via text only training

## Quick Facts
- arXiv ID: 2309.12242
- Source URL: https://arxiv.org/abs/2309.12242
- Reference count: 0
- Primary result: Achieves up to 83% relative performance compared to fully supervised methods on Clotho and AudioCaps datasets

## Executive Summary
This paper introduces a weakly-supervised approach for Automated Audio Captioning (AAC) that eliminates the need for paired audio-caption datasets during training. The method leverages a pre-trained Contrastive Language-Audio Pretraining (CLAP) model to align audio and text embeddings in a shared latent space. By training a lightweight decoder on text embeddings from the target domain, the approach can generate captions directly from audio embeddings during inference. The key innovation addresses the modality gap between audio and text embeddings through noise injection and embedding shift strategies during training, as well as nearest-neighbor and projection-based decoding during inference.

## Method Summary
The approach uses a frozen pre-trained CLAP model with text and audio encoders to extract embeddings. A lightweight 4-layer Transformer decoder is trained on text embeddings from target domain captions using prefix language modeling with cross-entropy loss. To bridge the modality gap between audio and text embeddings, the method employs noise injection (adding Gaussian noise with variance σ² = 0.013 to text embeddings) and embedding shift (moving text embeddings toward audio embedding centers) during training. During inference, audio embeddings are decoded using either nearest-neighbor selection from memory or projection-based weighted combination of text embeddings. The entire system requires only text data from target domains and the pre-trained CLAP model, avoiding the need for audio data during training.

## Key Results
- Achieves up to 83% relative performance compared to fully supervised methods on Clotho and AudioCaps datasets
- Demonstrates effectiveness of weakly-supervised training for AAC using only text data
- Shows that inference strategies (nearest-neighbor and projection-based decoding) can effectively bridge modality gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training a decoder to reconstruct text from CLAP text embeddings allows the same decoder to generate captions from audio embeddings due to the aligned multi-modal embedding space.
- Mechanism: CLAP is trained to map semantically similar audio-text pairs to similar embeddings in a shared space. By training a decoder on text embeddings, the decoder learns to invert the text encoder's mapping, and because audio embeddings occupy the same space, the decoder can directly process audio embeddings during inference.
- Core assumption: The CLAP text and audio embeddings are sufficiently aligned such that a decoder trained on text embeddings can generalize to audio embeddings without additional adaptation.
- Evidence anchors:
  - [abstract] "During training, a lightweight decoder is learned to reconstruct text from CLAP text embeddings, and during inference, the decoder generates captions using audio embeddings."
  - [section 2] "Since the CLAP text embedding is optimized to be similar to the CLAP audio embedding, we can directly infer the text decoder using the audio embeddings za without any pairwise training on the target dataset."
  - [corpus] Weak corpus evidence; no direct support from cited papers, but related papers suggest alignment is a common approach.

### Mechanism 2
- Claim: Noise injection during training creates a region in the embedding space that maps to the same caption, increasing the likelihood that audio embeddings fall within this region.
- Mechanism: Gaussian noise is added to text embeddings during training, forcing the decoder to learn to reconstruct captions from slightly perturbed embeddings. This creates a "region" in embedding space associated with each caption, making it more likely that audio embeddings (which may differ slightly) still fall within this region.
- Core assumption: The noise variance is appropriately set to match the typical difference between audio and text embeddings in the CLAP space.
- Evidence anchors:
  - [section 3.1.1] "Following [10], we add zero-mean Gaussian noise of standard deviation σ to the text embedding before feeding it to the decoder."
  - [section 3.1.1] "Since we assume no access to target audio data we estimate σ using 50 audio-caption pairs from the WavCaps dataset."
  - [corpus] No direct support from corpus; this is an adaptation of a technique from image captioning to audio.

### Mechanism 3
- Claim: Shifting text embeddings toward the center of audio embeddings during training reduces the modality gap, making audio embeddings more compatible with the decoder.
- Mechanism: The modality gap is defined as the difference between the mean audio embedding and the mean text embedding. By shifting text embeddings toward the audio embedding center during training, the decoder learns to handle embeddings that are more representative of the audio modality.
- Core assumption: The shift is sufficient to align the text and audio embeddings without distorting the semantic content of the captions.
- Evidence anchors:
  - [section 3.1.2] "we shift every text embedding toward closing the modality gap, and thus the prefix in Eq. 1 becomes p = f (zt + ∆gap)."
  - [section 3.1.2] "Building upon the findings of [13], who investigated the impact of shifting embeddings in various multi-modal contrastive learning models on downstream tasks"
  - [corpus] No direct support from corpus; this is an extension of CLIP-based techniques to CLAP.

## Foundational Learning

- Concept: Contrastive Language-Audio Pretraining (CLAP)
  - Why needed here: CLAP provides the aligned embedding space that enables the weakly-supervised approach. Understanding how CLAP works is crucial for grasping why the decoder can be trained on text embeddings and then applied to audio embeddings.
  - Quick check question: What is the primary goal of training CLAP, and how does it achieve this goal?

- Concept: Modality Gap
  - Why needed here: The modality gap is the core challenge that the proposed strategies aim to mitigate. Understanding its nature and impact is essential for evaluating the effectiveness of the proposed solutions.
  - Quick check question: How is the modality gap defined in this paper, and what are its implications for the weakly-supervised approach?

- Concept: Prefix Language Modeling
  - Why needed here: The decoder is trained using the prefix language modeling paradigm, which is crucial for understanding how the model generates captions from embeddings.
  - Quick check question: How does prefix language modeling differ from standard language modeling, and why is it used in this context?

## Architecture Onboarding

- Component map:
  CLAP model (frozen) -> Mapping network (f) -> Auto-regressive language model (4-layer Transformer) -> Decoder output
  Noise injection module -> Text embeddings during training
  Embedding shift module -> Text embeddings during training
  Nearest-neighbor decoder -> Inference using text embeddings memory
  Projection-based decoder -> Inference using text embeddings memory

- Critical path:
  1. Extract text embeddings from CLAP text encoder
  2. Apply noise injection or embedding shift (training only)
  3. Transform embeddings using mapping network
  4. Generate prefixes for decoder
  5. Train decoder to reconstruct captions
  6. At inference, extract audio embeddings from CLAP audio encoder
  7. Apply nearest-neighbor or projection-based decoding
  8. Generate final captions

- Design tradeoffs:
  - Training strategies (noise injection, embedding shift) vs. inference strategies (nearest-neighbor, projection-based): Training strategies modify the training process to reduce the modality gap, while inference strategies use in-domain text embeddings to bridge the gap during inference
  - Complexity vs. performance: Training strategies are simpler to implement but may be less effective than inference strategies, which require maintaining a memory of in-domain text embeddings

- Failure signatures:
  - High modality gap: Decoder struggles to generalize from text to audio embeddings, resulting in poor caption quality
  - Inappropriate noise variance: Decoder may become too robust or fail to generalize
  - Aggressive embedding shift: Semantic structure of text embeddings may be distorted, leading to poor caption generation
  - Insufficient memory size: Nearest-neighbor or projection-based decoding may fail to find sufficiently similar text embeddings

- First 3 experiments:
  1. Baseline experiment: Train decoder on text embeddings without any modality gap mitigation strategies and evaluate on audio embeddings
  2. Noise injection experiment: Train decoder with noise injection and evaluate on audio embeddings to assess the impact of this strategy
  3. Embedding shift experiment: Train decoder with embedding shift and evaluate on audio embeddings to assess the impact of this strategy

## Open Questions the Paper Calls Out

- Question: How does the performance of the weakly-supervised approach compare to fully supervised methods when trained on larger text-only datasets?
  - Basis in paper: [inferred] The paper demonstrates comparable performance to fully supervised methods using limited text data, but does not explore the impact of increasing the size of the text dataset
  - Why unresolved: The paper only evaluates the method using the standard Clotho and AudioCaps text datasets, without exploring the potential benefits of larger text-only datasets
  - What evidence would resolve it: Experiments training the model on progressively larger text-only datasets (e.g., COCO captions or other large-scale text corpora) and comparing performance to fully supervised methods trained on equivalent audio-caption pairs

- Question: Can the modality gap be effectively addressed by learning a mapping function between audio and text embeddings during training, rather than using inference-time strategies?
  - Basis in paper: [explicit] The paper mentions this as a potential direction for future work: "We further aim to train a mapping network to learn the gap between the two modalities in a supervised manner."
  - Why unresolved: The paper only explores training-free inference strategies and noise injection/embedding shift during training, without investigating learned mapping functions
  - What evidence would resolve it: Developing and evaluating a model that learns a mapping function between audio and text embeddings during training, and comparing its performance to the inference-time strategies presented in the paper

- Question: How does the proposed weakly-supervised approach generalize to other audio-language tasks, such as music captioning and audio question answering?
  - Basis in paper: [explicit] The paper states: "For future work, we plan to study the effectiveness of our proposed approach on other tasks, such as Music Captioning and Audio Question Answering."
  - Why unresolved: The paper only evaluates the approach on automated audio captioning, without exploring its applicability to other audio-language tasks
  - What evidence would resolve it: Adapting the weakly-supervised approach to music captioning and audio question answering tasks, and evaluating its performance on relevant datasets for these tasks

## Limitations
- Performance degradation: Achieves only 83% relative performance compared to fully supervised methods, suggesting substantial quality loss
- Reliance on CLAP quality: Performance depends heavily on the quality of CLAP's cross-modal alignment
- Limited noise estimation: Noise variance estimation uses only 50 audio-caption pairs, which may not generalize well

## Confidence

High confidence: The fundamental mechanism of using pre-trained CLAP embeddings for weakly-supervised training is sound, as it leverages established contrastive learning principles. The architectural design choices (4-layer Transformer decoder, prefix language modeling) are well-justified and follow standard practices in the field.

Medium confidence: The effectiveness of the proposed modality gap mitigation strategies (noise injection and embedding shift) is supported by ablation studies but lacks direct comparison with alternative approaches. The choice of hyperparameters (noise variance σ² = 0.013, embedding shift magnitude) appears arbitrary without sensitivity analysis.

Low confidence: The claim that the approach achieves "competitive" performance (83% relative to fully supervised methods) is difficult to verify without access to the full supervised baselines and understanding of the evaluation protocol's sensitivity to different decoding strategies.

## Next Checks
1. **Modality Gap Visualization**: Generate t-SNE plots of CLAP text and audio embeddings from the Clotho and AudioCaps datasets to quantify the actual separation between modalities. This would validate whether the reported modality gap justifies the proposed mitigation strategies and whether the embedding shift approach meaningfully reduces this gap.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the noise variance σ² and embedding shift magnitude across a wider range (e.g., σ² ∈ [0.001, 0.05], shift percentages from 10% to 90%) to identify optimal values and determine whether the chosen parameters are near-optimal or could be significantly improved.

3. **Cross-Dataset Generalization Test**: Evaluate the trained models on a held-out test set from a different domain (e.g., AudioSet or FSD50K) to assess whether the text-only training approach generalizes beyond the target datasets, which would provide stronger evidence for the method's practical utility.