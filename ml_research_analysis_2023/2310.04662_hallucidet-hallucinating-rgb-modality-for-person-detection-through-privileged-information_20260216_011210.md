---
ver: rpa2
title: 'HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged
  Information'
arxiv_id: '2310.04662'
source_url: https://arxiv.org/abs/2310.04662
tags:
- hallucidet
- images
- detection
- detector
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HalluciDet, a novel approach for IR to RGB
  image translation that leverages privileged information from pre-trained RGB detectors
  to improve object detection on IR images. The core idea is to guide the hallucination
  network using a detection loss from a frozen RGB detector, rather than relying on
  pixel reconstruction losses.
---

# HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information

## Quick Facts
- **arXiv ID**: 2310.04662
- **Source URL**: https://arxiv.org/abs/2310.04662
- **Reference count**: 40
- **Key outcome**: HalluciDet achieves 88.34% AP@50 on LLVIP dataset with Faster R-CNN, significantly outperforming fine-tuning (71.83%) and CycleGAN (38.92%).

## Executive Summary
HalluciDet introduces a novel approach for improving person detection in infrared (IR) images by hallucinating an RGB-like representation that leverages privileged information from pre-trained RGB detectors. Instead of traditional image-to-image translation methods that focus on pixel-level reconstruction, HalluciDet uses a detection loss from a frozen RGB detector to guide the hallucination network. This approach generates an intermediate representation that enhances objects of interest while reducing background clutter, leading to significant improvements in detection performance. The method maintains RGB detection performance while improving IR detection, requires fewer training samples than fine-tuning, and shows consistent gains across multiple detector architectures and datasets.

## Method Summary
HalluciDet trains a U-Net-based hallucination network to translate IR images into a representation optimized for a pre-trained RGB detector. The key innovation is using the detection loss (classification + regression terms) from the frozen RGB detector as the hallucination loss, rather than traditional reconstruction losses. The hallucination network learns to generate representations that maximize the RGB detector's response to objects of interest. During inference, IR images are passed through the hallucination network and then directly through the pre-trained RGB detector without any modification to its parameters. The approach is evaluated on LLVIP and FLIR ADAS datasets using AP@50 as the primary metric, with experiments showing improvements across different detector architectures including Faster R-CNN, RetinaNet, and FCOS.

## Key Results
- Achieves 88.34% AP@50 on LLVIP dataset with Faster R-CNN, compared to 71.83% for fine-tuning and 38.92% for CycleGAN
- Requires only 10% of training data to achieve >85% AP@50, outperforming fine-tuning approaches
- Maintains RGB detection performance (99.3% to 99.0% AP) while significantly improving IR detection
- Shows consistent improvements across multiple detector architectures (Faster R-CNN, RetinaNet, FCOS)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HalluciDet improves IR detection by hallucinating an intermediate RGB-like representation that enhances objects of interest and reduces background clutter.
- Mechanism: The hallucination network uses a detection loss (Lhall) from a frozen RGB detector to guide the translation of IR images into a representation space where the RGB detector can perform well, rather than reconstructing pixel-level details.
- Core assumption: The RGB detector has learned features that are useful for the detection task, and translating IR images into a representation that activates these features will improve IR detection performance.
- Evidence anchors:
  - [abstract] "This model produces a new image representation that enhances objects of interest in the scene and greatly improves detection performance."
  - [section 3] "The representation space, R ⊂ RW ×H×3, is conditioned to the subset of plausible RGB images that are sufficient to obtain a proper response from the RGB detector fθ."
- Break condition: If the RGB detector's learned features are not transferable to the IR domain or if the hallucination network fails to generate a representation that activates these features effectively.

### Mechanism 2
- Claim: HalluciDet maintains RGB detection performance while improving IR detection by not updating the RGB detector's parameters during adaptation.
- Mechanism: The hallucination network learns to translate IR images into a representation that is compatible with the pre-trained RGB detector, allowing the detector to be used directly on the hallucinated images without any modification to its parameters.
- Core assumption: The RGB detector's parameters are well-suited for the detection task, and freezing them during adaptation will prevent catastrophic forgetting of learned features.
- Evidence anchors:
  - [section 3] "As a side advantage, our model allows evaluating both modalities by providing the appropriate modality identifier during the forward pass, i.e., RGB or IR."
  - [section 4] "This improvement aligns with the quality of the representation observed in Figure 3, where confusing factors, such as car heat, have been removed from the image."
- Break condition: If the hallucination network introduces significant artifacts or distortions that negatively impact the RGB detector's performance on RGB images.

### Mechanism 3
- Claim: HalluciDet requires fewer training samples than fine-tuning the RGB detector on IR data, reducing the cost of annotation.
- Mechanism: By leveraging the pre-trained RGB detector's knowledge through the hallucination network, HalluciDet can learn an effective IR-to-RGB translation with a smaller amount of annotated IR data compared to fine-tuning the entire detector.
- Core assumption: The hallucination network can effectively learn the mapping from IR to RGB using the privileged information from the pre-trained RGB detector, reducing the need for extensive IR-specific annotations.
- Evidence anchors:
  - [section 4] "Notably, while a mere 1% of the training images didn't yield satisfactory results, an increase to 10% already led to the performance of over 85 mAP@50 which is an improvement over fine-tuning for the Faster R-CNN detector."
  - [section 4] "The proposed framework offers the additional advantage of maintaining performance in the RGB task, which is beneficial for applications requiring accurate responses in both modalities."
- Break condition: If the hallucination network fails to learn an effective translation with limited training samples, resulting in poor IR detection performance.

## Foundational Learning

- Concept: Learning Using Privileged Information (LUPI)
  - Why needed here: HalluciDet leverages the concept of LUPI by using the pre-trained RGB detector's knowledge as privileged information to guide the hallucination network's learning process.
  - Quick check question: What is the key difference between LUPI and standard supervised learning, and how does HalluciDet apply this concept?

- Concept: Image-to-Image Translation
  - Why needed here: HalluciDet performs image-to-image translation from the IR modality to an RGB-like representation that is compatible with the pre-trained RGB detector.
  - Quick check question: How does HalluciDet's approach to image-to-image translation differ from traditional methods like CycleGAN or FastCUT?

- Concept: Object Detection Loss Functions
  - Why needed here: HalluciDet uses a detection loss (Lhall) composed of classification and regression terms to guide the hallucination network's learning process, similar to how object detectors are trained.
  - Quick check question: What are the main components of a typical object detection loss function, and how does HalluciDet adapt this for its hallucination network?

## Architecture Onboarding

- Component map:
  Hallucination Network (U-Net) -> Pre-trained RGB Detector (Faster R-CNN/RetinaNet/FCOS) -> Detection Output

- Critical path:
  1. Pre-train an RGB detector on an RGB dataset
  2. Freeze the RGB detector's parameters
  3. Initialize the hallucination network
  4. Train the hallucination network using the detection loss (Lhall) and IR images with annotations
  5. During inference, pass IR images through the hallucination network and then the pre-trained RGB detector for detection

- Design tradeoffs:
  - Tradeoff between hallucination network capacity and computational efficiency: Larger networks may provide better translation but increase inference time
  - Tradeoff between detection loss components: Balancing the classification and regression terms in Lhall can affect the hallucination network's performance
  - Tradeoff between hallucination network and detector architecture: The hallucination network should be compatible with the pre-trained detector's input requirements

- Failure signatures:
  - Poor IR detection performance: Indicates issues with the hallucination network's translation or the compatibility between the hallucinated representation and the RGB detector
  - Significant degradation in RGB detection performance: Suggests that the hallucination network introduces artifacts or distortions that negatively impact the RGB detector's performance on RGB images
  - Overfitting to the training data: Manifests as poor generalization to unseen IR images or datasets

- First 3 experiments:
  1. Train the hallucination network with different values of the hyperparameter λ in the detection loss (Lhall) to find the optimal balance between classification and regression terms
  2. Compare the performance of HalluciDet with different pre-trained RGB detectors (e.g., Faster R-CNN, FCOS, RetinaNet) to identify the best combination for the target dataset
  3. Evaluate the impact of different hallucination network architectures (e.g., varying the number of layers or attention mechanisms) on the IR detection performance

## Open Questions the Paper Calls Out
- [explicit] The paper does not explicitly call out open questions, focusing instead on presenting results and methodology.

## Limitations
- Limited dataset generalization: Evaluated only on two automotive-focused datasets (LLVIP and FLIR ADAS), leaving effectiveness for other IR detection scenarios unproven
- Modality fidelity trade-off: Prioritizes detection performance over visual fidelity of hallucinated images, potentially limiting utility for other computer vision tasks
- Modest confidence in RGB performance claims: Single evaluation shows minimal degradation (99.3% to 99.0% AP) without rigorous testing across different RGB datasets

## Confidence
- **High Confidence**: The core claim that HalluciDet improves IR detection performance using privileged RGB information is well-supported by experimental results showing consistent AP@50 improvements across both datasets and multiple detector architectures.
- **Medium Confidence**: The claim about requiring fewer training samples is supported but could benefit from more systematic analysis; the paper shows improvement with 10% of training data but doesn't provide a complete learning curve.
- **Low Confidence**: The assertion that HalluciDet maintains RGB detection performance is demonstrated but relies on a single evaluation showing minimal degradation, requiring more rigorous testing across different RGB datasets.

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate HalluciDet on non-automotive IR datasets (medical, surveillance, or industrial inspection) to assess whether the approach generalizes beyond automotive scenarios and validates whether the privileged information transfer mechanism is domain-agnostic.

2. **Few-Shot Scaling Analysis**: Conduct a more granular analysis of training sample requirements by testing with 1%, 5%, 10%, 25%, 50%, and 75% of the training data, including statistical significance testing to determine if improvements are consistent across different random splits.

3. **Visual Quality Assessment**: Implement a quantitative evaluation of hallucinated image quality using established metrics (FID, LPIPS) and conduct a human perception study to assess whether hallucinated images are visually plausible, helping understand the trade-off between detection performance and visual fidelity.