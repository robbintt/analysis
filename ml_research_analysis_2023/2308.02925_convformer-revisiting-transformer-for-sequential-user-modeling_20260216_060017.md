---
ver: rpa2
title: 'ConvFormer: Revisiting Transformer for Sequential User Modeling'
arxiv_id: '2308.02925'
source_url: https://arxiv.org/abs/2308.02925
tags:
- user
- sequential
- convformer
- performance
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits the Transformer architecture for sequential
  user modeling in recommendation systems. The authors identify three key criteria
  for effective token-mixers: order sensitivity, large receptive field, and lightweight
  architecture.'
---

# ConvFormer: Revisiting Transformer for Sequential User Modeling

## Quick Facts
- **arXiv ID**: 2308.02925
- **Source URL**: https://arxiv.org/abs/2308.02925
- **Reference count**: 40
- **Primary result**: ConvFormer achieves state-of-the-art performance on four public datasets and one large industrial dataset by replacing self-attention with depth-wise convolution.

## Executive Summary
This paper identifies three key criteria for effective token-mixers in sequential user modeling: order sensitivity, large receptive field, and lightweight architecture. The authors propose ConvFormer, which replaces self-attention with depth-wise convolution to satisfy all three criteria simultaneously. Through extensive experiments on public and industrial datasets, ConvFormer demonstrates state-of-the-art performance in sequential recommendation tasks. The paper also introduces an accelerated variant, ConvFormer-F, which leverages Fourier transform acceleration to handle long sequences efficiently.

## Method Summary
ConvFormer modifies the Transformer architecture by replacing self-attention with depth-wise convolution layers, creating a lightweight token-mixer that maintains order sensitivity and large receptive fields. The model consists of an embedding layer with position encoding, LighTCN layers (depth-wise convolution followed by channel-wise convolution), and a dot-product scorer. ConvFormer-F is an accelerated variant that uses Fourier transform to compute convolutions efficiently for long sequences. The model is trained using pairwise ranking loss and evaluated on top-k hit ratio, normalized discounted cumulative gain, and mean reciprocal rank metrics.

## Key Results
- ConvFormer achieves state-of-the-art performance on four public datasets (Beauty, Sports, Toys, Yelp) and one large industrial dataset
- The depth-wise convolution approach outperforms standard self-attention while maintaining lightweight architecture
- ConvFormer-F provides efficient computation for long sequences through Fourier transform acceleration
- Ablation studies validate the importance of each criterion: order sensitivity, large receptive field, and lightweight design

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Depth-wise convolution with large receptive field outperforms self-attention for sequential user modeling.
- **Mechanism**: Replacing self-attention's item-to-item paradigm with depth-wise convolution maintains order sensitivity while capturing long-range dependencies through a large receptive field. The DWC layer operates independently on each channel, avoiding overparameterization risk.
- **Core assumption**: User behavior sequences exhibit high-order Markovian properties with minimal coupling between channels in the latent space.
- **Evidence anchors**: [abstract] Identifies three criteria satisfied by ConvFormer's DWC approach; [section 4.2] Shows DWC with large receptive field while maintaining lightweight architecture.
- **Break condition**: If user behavior sequences show strong cross-channel dependencies or low-order Markovian properties, depth-wise approach would underperform.

### Mechanism 2
- **Claim**: Order-sensitive token-mixers are essential for sequential recommendation performance.
- **Mechanism**: Simple order-sensitive alternatives to self-attention achieve competitive performance by preserving item ordering information while avoiding computational complexity of full self-attention.
- **Core assumption**: Item order carries critical information for predicting next-item preferences, and models must be explicitly sensitive to this ordering.
- **Evidence anchors**: [section 3.1] Demonstrates SAR-O, SAR-P, and SAR-R (all order-sensitive) perform competitively with standard SAR; [section 3.3] Shows SAR-R's competitive performance despite lacking item-to-item correlations.
- **Break condition**: If item order becomes irrelevant to prediction task or if position encoding alone suffices.

### Mechanism 3
- **Claim**: Lightweight architecture prevents overfitting when using large receptive fields.
- **Mechanism**: Depth-wise convolution keeps parameter count proportional to embedding dimension rather than sequence length, allowing large receptive fields without overfitting risk.
- **Core assumption**: Sequential recommendation models face a trade-off where large receptive fields improve performance but increase overfitting risk without architectural constraints.
- **Evidence anchors**: [section 3.3] Shows SAR-N and SAR-N+ variants suffer performance drops compared to lightweight SAR; [section 5.3.2] Demonstrates ConvFormer's DWC outperforms vanilla convolution variants.
- **Break condition**: If regularization techniques or smaller datasets eliminate overfitting concerns, or if large receptive fields become unnecessary.

## Foundational Learning

- **Concept**: Depth-wise convolution operation
  - Why needed here: Forms the core token-mixing mechanism in ConvFormer, replacing self-attention
  - Quick check question: How does depth-wise convolution differ from standard convolution in terms of parameter count and receptive field?

- **Concept**: Fourier Transform and FFT acceleration
  - Why needed here: Enables efficient computation of large receptive field convolutions through the convolution theorem
  - Quick check question: What is the computational complexity of DFT vs FFT, and how does this affect ConvFormer's scalability?

- **Concept**: Meta-former architecture
  - Why needed here: Provides the overall framework (residual connections, layer normalization) that makes ConvFormer effective
  - Quick check question: How does the meta-former architecture differ from standard Transformer architecture in terms of layer organization?

## Architecture Onboarding

- **Component map**: Input sequence → Embedding layer → Depth-wise convolution (DWC) → Channel-wise convolution (FFN) → Residual connection + LayerNorm → Output representation → Dot product scoring
- **Critical path**: The depth-wise convolution layer is the performance-critical component that replaces self-attention and determines the model's ability to capture long-range dependencies while maintaining order sensitivity.
- **Design tradeoffs**: Large receptive field vs computational cost (solved by FFT acceleration), order sensitivity vs parameter efficiency (solved by depth-wise approach), model capacity vs overfitting (solved by lightweight architecture)
- **Failure signatures**: Poor performance on datasets where item order is irrelevant, slow inference on very long sequences without FFT acceleration, degraded performance when kernel size is too small to capture dependencies
- **First 3 experiments**:
  1. Compare ConvFormer with different kernel sizes (K=10, 30, 50) to find optimal receptive field
  2. Test different padding modes (circular, reflect, zero) to handle sequence boundaries
  3. Benchmark FFT-accelerated ConvFormer-F against standard ConvFormer on long sequences (L=500, 1000)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ConvFormer perform on sequential user modeling tasks beyond the two-tower architecture, such as single-tower ranking or masked language modeling?
- Basis in paper: [inferred] The paper focuses on the two-tower retrieval paradigm and auto-regressive setting, acknowledging limitations in not thoroughly studying single-tower structures or masked language models.
- Why unresolved: The authors explicitly state that single-tower structures and masked language models are outside the scope of their current study.
- What evidence would resolve it: Experiments comparing ConvFormer's performance on single-tower ranking tasks and masked language modeling tasks against state-of-the-art baselines.

### Open Question 2
- Question: What is the impact of different padding modes (circular, reflect, zero) on ConvFormer's performance in various sequential user modeling scenarios with varying levels of user behavior periodicity?
- Basis in paper: [explicit] The paper mentions investigating the role of padding in the convolution operator and finding that circular padding performs better in scenarios with strong behavior periodicity.
- Why unresolved: While the paper provides some insights into the impact of padding modes, a comprehensive analysis across different datasets and scenarios is needed.
- What evidence would resolve it: A detailed study comparing ConvFormer's performance using different padding modes across multiple datasets with varying levels of user behavior periodicity.

### Open Question 3
- Question: How does ConvFormer's performance scale with increasing sequence lengths, and what are the practical limitations in terms of computational efficiency and memory usage?
- Basis in paper: [inferred] The paper mentions the potential inefficiency of ConvFormer with extremely long sequences due to the computational complexity of the large receptive field.
- Why unresolved: While the paper presents ConvFormer-F as a solution for handling long sequences, a thorough analysis of ConvFormer's performance and limitations with increasing sequence lengths is needed.
- What evidence would resolve it: Experiments evaluating ConvFormer's performance and computational efficiency/memory usage on datasets with varying sequence lengths, along with a comparison to ConvFormer-F.

## Limitations

- Limited empirical validation of the theoretical claims about why depth-wise convolution outperforms self-attention for sequential user modeling
- The depth-wise convolution assumption relies heavily on the claim that user behavior sequences exhibit high-order Markovian properties with minimal cross-channel coupling
- The accelerated ConvFormer-F variant's efficiency gains depend on the assumption that Fourier transform acceleration will scale well to industrial-sized datasets

## Confidence

- **High confidence**: ConvFormer achieves state-of-the-art performance on the tested datasets, as evidenced by the reported metrics across multiple public benchmarks and one industrial dataset
- **Medium confidence**: The three criteria (order sensitivity, large receptive field, lightweight architecture) are necessary for effective sequential user modeling, based on ablation studies showing performance drops when criteria are violated
- **Low confidence**: The specific claim that depth-wise convolution is superior to self-attention due to reduced cross-channel coupling, as this mechanism is theoretically proposed but not empirically validated

## Next Checks

1. **Cross-domain validation**: Test ConvFormer on datasets from different recommendation domains (e.g., music streaming, news recommendation) to verify whether the depth-wise convolution advantage holds when user behavior sequences exhibit different correlation structures.

2. **Mechanism isolation**: Conduct controlled experiments comparing depth-wise convolution with standard convolution and separable convolution while holding receptive field and parameter count constant, to isolate whether the performance gains come from the depth-wise approach specifically or other factors.

3. **Long sequence stress test**: Evaluate ConvFormer-F's performance and efficiency on sequences longer than 200 items to verify the FFT acceleration's scalability claims for industrial applications with very long user histories.