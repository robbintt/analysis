---
ver: rpa2
title: Having Beer after Prayer? Measuring Cultural Bias in Large Language Models
arxiv_id: '2305.14456'
source_url: https://arxiv.org/abs/2305.14456
tags:
- language
- cultural
- bias
- arabic
- western
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates cultural bias in large language models,
  finding that they exhibit a significant preference towards Western culture when
  generating Arabic text. A novel metric, the Cultural Bias Score, is introduced to
  quantify this bias using naturally occurring Arabic prompts collected from Twitter.
---

# Having Beer after Prayer? Measuring Cultural Bias in Large Language Models

## Quick Facts
- **arXiv ID**: 2305.14456
- **Source URL**: https://arxiv.org/abs/2305.14456
- **Reference count**: 26
- **Key outcome**: Large language models exhibit significant Western cultural bias when generating Arabic text, with even Arabic monolingual models showing bias though less than multilingual models.

## Executive Summary
This paper introduces a novel metric, the Cultural Bias Score (CBS), to quantify cultural bias in large language models when generating Arabic text. The study analyzes 16 different models across eight cultural aspects using naturally occurring Arabic prompts collected from Twitter. Results show that models consistently prefer Western cultural entities over Arab ones, with the best-performing zero-shot transfer model exhibiting the most bias. The research also demonstrates that providing culture-indicating tokens or culturally-relevant demonstrations can help mitigate this bias, particularly for multilingual models.

## Method Summary
The authors collected 628 naturally occurring Arabic prompts from Twitter spanning eight cultural aspects (person names, food, clothing, location, literature, beverage, religion, sports) with 20,368 entities. They created equal-sized target lists for Arab and Western cultures for each aspect and computed CBS by masking contexts and measuring language model probabilities of filling masks with Arab versus Western cultural targets. The methodology involved running experiments on 16 different language models (both monolingual and multilingual) and calculating average CBS across all cultural aspects. Human evaluation was also conducted on BLOOM and ChatGPT to validate the CBS metric.

## Key Results
- Large language models exhibit significant Western cultural bias when generating Arabic text, with average CBS indicating preference for Western targets
- Arabic monolingual models show less bias than multilingual models, though both exhibit significant Western preference
- Models exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English grammatical structures
- Providing culture-indicating tokens or culturally-relevant demonstrations can significantly reduce bias, especially for multilingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training data distribution directly shapes cultural bias in LMs.
- Mechanism: Models learn associations between cultural entities and linguistic patterns from their training corpus. When pre-training data is dominated by Western cultural content, the model assigns higher likelihood to Western entities even when generating text in non-Western languages.
- Core assumption: The pre-training corpus contains disproportionate Western cultural content compared to target culture content.
- Evidence anchors:
  - [abstract] "Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. This could be due to pre-training data being predominantly focused on Western content, even if written in Arabic"
  - [section 4.2] "Interestingly, the best-performing model in zero-shot transfer from English to Arabic exhibits the most bias"
  - [corpus] Weak - corpus analysis shows Wikipedia articles in Arabic are fewer than English articles with Arabic versions, suggesting Western content dominance
- Break condition: If pre-training corpus is balanced or augmented with culture-specific data, bias would decrease.

### Mechanism 2
- Claim: Grammatical structure alignment with English increases cultural bias.
- Mechanism: When Arabic prompts follow English-like grammatical patterns (explicit subject pronouns), models are more likely to generate Western cultural content due to stronger learned associations between such structures and Western contexts in pre-training data.
- Core assumption: Pre-training data contains translated or English-influenced Arabic text with explicit pronouns.
- Evidence anchors:
  - [section 4.3] "Arabic prompts which are more grammatically aligned with English sentence structure have more preference towards Western targets"
  - [abstract] "Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English"
  - [corpus] Weak - no direct corpus evidence, but inference from observation that pronoun-dropping reduces bias
- Break condition: If models are trained on Arabic text with native grammatical structures, this bias mechanism would be weaker.

### Mechanism 3
- Claim: Culture-indicating tokens and demonstrations can debias LMs.
- Mechanism: Adding explicit cultural signals (tokens or demonstrations) provides additional context that overrides default Western associations, guiding the model to generate culturally-relevant content.
- Core assumption: Models can utilize additional context tokens to adjust their generation distribution.
- Evidence anchors:
  - [section 4.3] "Demonstrations helped reduce bias significantly for the majority of models, helping adjust their distribution to favor culturally-relevant targets"
  - [abstract] "Our analyses show that providing culture-indicating tokens or culturally-relevant demonstrations to the model can help in debiasing"
  - [corpus] Weak - no corpus evidence, but experimental results show effectiveness
- Break condition: If models are too large or the cultural signal is not sufficiently clear, demonstrations may not help.

## Foundational Learning

- Concept: Cultural bias measurement in LMs
  - Why needed here: To quantify and compare bias across models and cultural aspects
  - Quick check question: How does the Cultural Bias Score (CBS) work, and what does a high score indicate?

- Concept: Pre-training data influence on model behavior
  - Why needed here: Understanding how training data composition affects cultural relevance
  - Quick check question: Why might multilingual models show more bias than monolingual models trained on the same language?

- Concept: Prompt engineering for cultural context
  - Why needed here: To effectively evaluate and mitigate cultural bias
  - Quick check question: How do culture-indicating tokens and demonstrations help reduce bias in LMs?

## Architecture Onboarding

- Component map: Data collection → Prompt design → Model evaluation (intrinsic and extrinsic) → Human evaluation → Analysis
- Critical path: Collect natural Arabic prompts → Create Arab and Western target lists → Compute CBS for each model → Analyze results and identify bias patterns
- Design tradeoffs: Using naturally occurring prompts vs. synthetic prompts (authenticity vs. control), monolingual vs. multilingual models (cultural relevance vs. performance)
- Failure signatures: High CBS across most models indicates systemic bias; models performing well on standard tasks but poorly on cultural relevance
- First 3 experiments:
  1. Replicate CBS calculation for a new monolingual Arabic model to verify methodology
  2. Test effect of adding culture-indicating tokens to prompts for different model sizes
  3. Compare CBS results using prompts with explicit pronouns vs. pronoun-dropping to verify grammatical structure effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Cultural Bias Score (CBS) correlate with downstream task performance in non-Western languages?
- Basis in paper: [inferred] The paper introduces CBS to measure cultural bias but does not explore its relationship with task performance.
- Why unresolved: The study focuses on measuring bias, not its impact on model effectiveness.
- What evidence would resolve it: Experiments comparing CBS values with performance metrics on culturally relevant tasks across different languages.

### Open Question 2
- Question: Would using dialects instead of Modern Standard Arabic in pre-training significantly reduce cultural bias in Arabic language models?
- Basis in paper: [explicit] The paper mentions MARBERT trained on Arabic tweets and AraBERT-Twi trained on dialectal data, noting lower bias, but doesn't isolate the dialect effect.
- Why unresolved: The study uses various models but doesn't specifically test the impact of dialect vs. MSA in pre-training.
- What evidence would resolve it: Controlled experiments comparing models pre-trained on MSA vs. dialectal data with identical architectures and training procedures.

### Open Question 3
- Question: How do different cultural aspects (e.g., religion, food) contribute to overall cultural bias, and are some aspects more resistant to debiasing than others?
- Basis in paper: [explicit] The paper measures bias across eight cultural aspects but doesn't analyze the variance or difficulty of debiasing each aspect.
- Why unresolved: While individual CBS scores are reported, there's no analysis of inter-aspect differences in bias magnitude or debiasing effectiveness.
- What evidence would resolve it: Detailed analysis of CBS variance across aspects and experiments measuring debiasing success rates for each cultural aspect.

### Open Question 4
- Question: Would increasing the diversity of pre-training data sources beyond Wikipedia and books significantly reduce cultural bias in multilingual models?
- Basis in paper: [explicit] The paper suggests pre-training data is predominantly Western-focused and hints at social media data being more culturally relevant.
- Why unresolved: The study identifies data source issues but doesn't experimentally test the impact of diverse data sources.
- What evidence would resolve it: Comparative experiments with models pre-trained on diverse cultural datasets vs. traditional sources, measuring CBS and downstream performance.

### Open Question 5
- Question: How does the size of language models interact with cultural bias, and is there an optimal scale for balancing performance and cultural relevance?
- Basis in paper: [explicit] The paper observes that larger monolingual models are less biased, while the opposite is true for multilingual models.
- Why unresolved: The study notes scale effects but doesn't explore the underlying mechanisms or identify an optimal model size.
- What evidence would resolve it: Systematic scaling experiments with detailed analysis of bias-performance trade-offs across different model sizes and architectures.

## Limitations
- Data provenance and timing: Unclear whether pre-training data for the models was collected before or after the Twitter prompts (March 2023 cutoff), potentially affecting CBS scores.
- Corpus analysis granularity: Lacks detailed breakdown of Western versus Arab cultural content proportion in pre-training corpus, making it difficult to quantify data imbalance impact.
- Generalizability across cultures: Study focuses on Arabic and Western cultural bias, with unclear applicability to other language pairs or low-resource languages.

## Confidence
- High Confidence: Models exhibit significant Western cultural bias when generating Arabic text (supported by CBS scores and human evaluation); Arabic monolingual models show less bias than multilingual models (consistent across experiments); Grammatical alignment with English increases bias (observed across multiple prompts and models)
- Medium Confidence: Pre-training data distribution directly causes cultural bias (inferred from corpus size differences and performance correlations, but not directly measured); Culture-indicating tokens and demonstrations effectively reduce bias (supported by experimental results but not tested for side effects or long-term robustness)
- Low Confidence: The exact mechanism by which grammatical structure influences bias (lack of corpus evidence and potential confounding factors); Generalizability of findings to other languages and cultural pairs (limited to Arabic and Western contexts); CBS as a comprehensive proxy for human cultural relevance judgments (only validated for two models)

## Next Checks
1. **Corpus content analysis**: Perform a detailed breakdown of the pre-training data to quantify the proportion of Western versus Arab cultural content. This will clarify the extent to which data imbalance drives the observed bias.

2. **Temporal validation**: Verify that all pre-training data for the models used was collected before the Twitter prompts were posted (before March 2023). This ensures the CBS scores reflect genuine bias rather than memorization.

3. **Cross-cultural generalization**: Replicate the CBS methodology for another language pair (e.g., Hindi and Western, or Mandarin and Western) to test whether the observed mechanisms and mitigation strategies are broadly applicable.