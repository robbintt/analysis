---
ver: rpa2
title: 'DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic Topic
  Management for Flexible Task-Oriented Dialogue'
arxiv_id: '2308.08043'
source_url: https://arxiv.org/abs/2308.08043
tags:
- topic
- dialogue
- your
- user
- current
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiagGPT is a multi-agent LLM system for task-oriented dialogue
  that uses a Topic Manager to automatically track and control dialogue flow. The
  system employs separate modules (Chat Agent, Topic Enricher, Context Manager) with
  carefully engineered prompts to enable proactive questioning, topic switching, and
  goal-oriented guidance.
---

# DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic Topic Management for Flexible Task-Oriented Dialogue

## Quick Facts
- **arXiv ID**: 2308.08043
- **Source URL**: https://arxiv.org/abs/2308.08043
- **Reference count**: 5
- **Key outcome**: Multi-agent LLM system for task-oriented dialogue with automatic topic management and flexible medical consultation capabilities

## Executive Summary
DiagGPT is a multi-agent LLM system designed for task-oriented dialogue that uses a Topic Manager to automatically track and control dialogue flow. The system employs separate modules (Chat Agent, Topic Enricher, Context Manager) with carefully engineered prompts to enable proactive questioning, topic switching, and goal-oriented guidance. Through experiments, DiagGPT demonstrates the ability to conduct flexible medical consultation dialogues by automatically managing topic stacks and adapting to user needs, successfully simulating realistic doctor-patient interactions.

## Method Summary
DiagGPT implements a multi-agent LLM architecture where separate modules coordinate to manage task-oriented dialogue. The system uses a Topic Manager as a meta-controller that analyzes user queries and decides topic transitions from a predefined action list. Topics are tracked using a stack data structure following FIFO order, while a Topic Enricher transforms abstract topic labels into detailed prompts for the Chat Agent. The Context Manager maintains chat history and user information. This architecture enables the system to handle complex, multi-turn dialogues through internal interactions among AI agents.

## Key Results
- DiagGPT successfully simulates realistic doctor-patient interactions through flexible topic management
- The system automatically manages topic stacks and adapts to user needs without manual intervention
- DiagGPT demonstrates superior performance in handling complex, multi-turn task-oriented scenarios compared to basic Q&A chatbots

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic Manager acts as a meta-controller that uses LLM reasoning to decide topic transitions
- Mechanism: The Topic Manager module ingests user query, action list, topic stack state, and chat history, then selects one action from a predefined list based on prompts that explicitly describe tool descriptions and when to use them
- Core assumption: LLM can perform one-step reasoning to analyze context and choose the correct topic transition action
- Evidence anchors: [abstract] "The system employs separate modules...with carefully engineered prompts", [section] "Topic Manager...is responsible for determining the topic development"
- Break condition: LLM fails to correctly infer user intent, leading to wrong topic transitions and dialogue confusion

### Mechanism 2
- Claim: Stack-based topic management enables FIFO topic tracking and natural dialogue flow
- Mechanism: Topics are stored in a stack data structure; new topics are pushed when created, finished topics are popped, and the system always operates on the top topic
- Core assumption: Stack operations (push/pop) correctly model the state transitions needed for task-oriented dialogue
- Evidence anchors: [abstract] "automatically managing topic stacks and adapting to user needs", [section] "The topic stack is a data structure...follows a first-in, first-out (FIFO) order"
- Break condition: Stack becomes corrupted or user actions cannot be reconciled with stack state

### Mechanism 3
- Claim: Topic Enricher transforms abstract topic labels into detailed prompts for the Chat Agent
- Mechanism: Topic Enricher takes the raw topic from the stack and chat history, then generates a detailed prompt that specifies whether the agent should ask or answer
- Core assumption: LLM can generate detailed, contextually appropriate prompts from minimal topic labels and chat history
- Evidence anchors: [abstract] "employs separate modules...with carefully engineered prompts to enable proactive questioning", [section] "The Topic Enricher is designed to bridge this gap"
- Break condition: Enriched prompt becomes too verbose or loses critical information, causing Chat Agent to generate irrelevant responses

## Foundational Learning

- Concept: Multi-agent LLM orchestration
  - Why needed here: DiagGPT uses separate LLM agents that must coordinate through structured inputs/outputs
  - Quick check question: What are the four main LLM modules in DiagGPT and what is each responsible for?

- Concept: Prompt engineering for tool use
  - Why needed here: Each agent uses carefully engineered prompts that include tool descriptions, action lists, and context to guide LLM behavior
  - Quick check question: How does the Topic Manager decide which action to take from its action list?

- Concept: Stack-based state management
  - Why needed here: The topic stack tracks dialogue state and enables topic switching while maintaining conversation context
  - Quick check question: What data structure does DiagGPT use to track dialogue topics and why?

## Architecture Onboarding

- Component map:
  - Topic Manager: Meta-controller that decides topic transitions using action list
  - Context Manager: Maintains chat history and user information
  - Topic Enricher: Converts raw topics into detailed prompts for Chat Agent
  - Chat Agent: Generates user-facing responses based on enriched topics
  - Action list: Predefined operations (create, finish, jump topics, load task)

- Critical path: User query → Topic Manager → Action execution → Topic stack update → Topic Enricher → Chat Agent → Response
- Design tradeoffs: Multiple LLM calls increase cost and latency vs. single LLM approach; stack-based state management vs. flat state tracking
- Failure signatures: Topic Manager chooses wrong action → conversation derails; Topic Enricher generates poor prompt → Chat Agent responds incorrectly; Stack operations mismatch → topic tracking fails
- First 3 experiments:
  1. Test Topic Manager with simple dialogues to verify correct action selection
  2. Test Topic Enricher with various topic labels to ensure proper prompt generation
  3. Test full stack with a predefined checklist to verify topic progression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiagGPT compare to other state-of-the-art task-oriented dialogue systems that use fine-tuning or specialized architectures?
- Basis in paper: [inferred] The paper mentions that previous fine-tuning models have underperformed in TOD and current LLMs do not inherently possess the capability for task-oriented dialogue, but does not provide a direct comparison with other systems
- Why unresolved: The paper focuses on demonstrating the capabilities of DiagGPT rather than comparing it to other systems
- What evidence would resolve it: Conducting experiments where DiagGPT is compared to other state-of-the-art TOD systems on the same tasks and using the same evaluation metrics

### Open Question 2
- Question: How does the cost and efficiency of DiagGPT scale with the complexity of the task and the length of the dialogue?
- Basis in paper: [explicit] The paper mentions that DiagGPT involves multiple LLMs and requires internal interactions among AI agents, which increases the cost and time to provide user feedback
- Why unresolved: The paper does not provide quantitative data on how the cost and efficiency scale with task complexity and dialogue length
- What evidence would resolve it: Conducting experiments to measure the cost and efficiency of DiagGPT for tasks of varying complexity and dialogues of different lengths

### Open Question 3
- Question: How stable is DiagGPT across different domains and how much manual adjustment of prompts is required for each new domain?
- Basis in paper: [explicit] The paper mentions that the performance of DiagGPT is not as stable as some rule-based or fine-tuned dialogue models and that meticulous and detailed prompt adjustments are needed for every different applied scenario
- Why unresolved: The paper does not provide quantitative data on the stability of DiagGPT across different domains or the amount of manual adjustment required
- What evidence would resolve it: Conducting experiments to measure the stability of DiagGPT across different domains and the amount of manual adjustment required for each new domain

## Limitations
- Lack of quantitative metrics and comparative results to support performance claims
- Missing specific implementation details including exact prompt templates and variable substitutions
- No data on cost, efficiency scaling, or stability across different domains

## Confidence

**Confidence Labels:**
- **High Confidence**: The multi-agent LLM architecture with separate modules is clearly specified and the general workflow is well-defined
- **Medium Confidence**: The stack-based topic management mechanism and its FIFO operations are theoretically sound, though practical effectiveness depends on prompt quality
- **Low Confidence**: Claims about system performance and superiority over basic chatbots lack quantitative evidence or comparative results

## Next Checks

1. **Topic Manager Action Selection Validation**: Create a test suite with diverse user queries and verify that the Topic Manager consistently selects the correct action from its predefined list based on context analysis

2. **Prompt Enrichment Quality Assessment**: Evaluate the Topic Enricher by measuring the relevance and completeness of generated prompts across different topic types and contexts, using both automated metrics and human evaluation

3. **Stack State Consistency Testing**: Implement stress tests where users rapidly switch between topics, skip topics, or revisit previous topics, then verify that the topic stack maintains accurate state throughout complex dialogue patterns