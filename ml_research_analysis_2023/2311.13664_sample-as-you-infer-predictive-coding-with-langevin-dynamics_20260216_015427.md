---
ver: rpa2
title: 'Sample as You Infer: Predictive Coding With Langevin Dynamics'
arxiv_id: '2311.13664'
source_url: https://arxiv.org/abs/2311.13664
tags:
- uni00000013
- langevin
- uni00000048
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Langevin Predictive Coding (LPC), a novel
  algorithm for training deep generative models that combines predictive coding with
  Langevin dynamics. The key idea is to interpret the standard predictive coding inference
  procedure as an overdamped Langevin sampling by injecting appropriately scaled Gaussian
  noise.
---

# Sample as You Infer: Predictive Coding With Langevin Dynamics

## Quick Facts
- arXiv ID: 2311.13664
- Source URL: https://arxiv.org/abs/2311.13664
- Authors: 
- Reference count: 40
- Key outcome: The paper introduces Langevin Predictive Coding (LPC), a novel algorithm for training deep generative models that combines predictive coding with Langevin dynamics.

## Executive Summary
This paper presents Langevin Predictive Coding (LPC), a novel approach to training deep generative models that bridges predictive coding inference with Langevin dynamics sampling. By reinterpreting standard predictive coding as overdamped Langevin sampling through appropriate Gaussian noise injection, the authors enable optimization with respect to a tight evidence lower bound (ELBO) using Langevin samples. The method incorporates an encoder network for amortized warm-starts and evaluates three objectives for its optimization, while also introducing a lightweight preconditioning technique inspired by Riemannian Manifold Langevin dynamics. Experiments on CIFAR-10, SVHN, and CelebA demonstrate that LPC outperforms or matches standard VAE training in terms of sample quality and diversity while converging faster.

## Method Summary
The LPC algorithm reinterprets predictive coding inference as overdamped Langevin sampling by injecting appropriately scaled Gaussian noise into the standard predictive coding gradient ascent update. This allows the use of Langevin samples to compute gradients with respect to a tight ELBO for model parameter optimization. The method improves upon encoder-free training by incorporating an encoder network for amortized warm-starts, evaluating three different objectives (forward KL, reverse KL, and Jeffrey's divergence) for training the inference network. Additionally, the authors propose a lightweight form of preconditioning based on Adam-style second-moment estimates to increase robustness to sampling step size and reduce sensitivity to curvature.

## Key Results
- LPC outperforms or matches standard VAE training in sample quality and diversity while converging faster
- Preconditioned LPC models achieve better FDD and density metrics compared to VAEs trained for 3x as many iterations
- The lightweight preconditioning significantly improves robustness to Langevin step size and reduces curvature sensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard predictive coding inference can be reinterpreted as overdamped Langevin sampling by injecting appropriately scaled Gaussian noise.
- Mechanism: The predictive coding gradient ascent update $z(t) = z(t-1) + \gamma\nabla_z \log p(x(i), z(t-1)|\theta\theta\theta)$ becomes equivalent to an Euler-Maruyama discretization of overdamped Langevin dynamics $z(t) = z(t-1) + \gamma\nabla_z \log p(x(i), z(t-1)|\theta\theta\theta) + \sqrt{2\gamma}\eta$ when Gaussian noise $\eta \sim N(0, I)$ is added.
- Core assumption: The added noise must be scaled appropriately ($\sqrt{2\gamma}$) to match the theoretical Langevin diffusion formulation.
- Evidence anchors:
  - [section]: "By injecting appropriately scaled Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling"
  - [abstract]: "By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling"
  - [corpus]: Weak - no direct corpus evidence provided for this specific noise scaling relationship
- Break condition: If noise is incorrectly scaled, the sampling will not target the true posterior distribution, breaking the theoretical connection to Langevin dynamics.

### Mechanism 2
- Claim: Langevin samples can be used to compute gradients with respect to a tight evidence lower bound (ELBO) for model parameter optimization.
- Mechanism: By treating Langevin samples as approximate posterior samples, we can compute $\nabla_{\theta\theta\theta} E_{\tilde{p}(z|x)}[\log p(x, z|\theta\theta\theta)]$ as a Monte Carlo estimate, ignoring the intractable entropy term since we only need gradients w.r.t. parameters.
- Core assumption: The Langevin samples are sufficiently close to true posterior samples to provide useful gradient estimates.
- Evidence anchors:
  - [section]: "Utilizing these Langevin samples, we compute gradients with respect to a tight evidence lower bound (ELBO), which we then optimize our model parameters against"
  - [abstract]: "This allows optimization with respect to a tight evidence lower bound (ELBO) using Langevin samples"
  - [corpus]: Weak - no direct corpus evidence provided for using Langevin samples in ELBO optimization
- Break condition: If Langevin chain hasn't mixed well, the samples will be biased, leading to poor gradient estimates and suboptimal model training.

### Mechanism 3
- Claim: Adaptive preconditioning using Adam-style second-moment estimates increases robustness to Langevin step size and reduces sensitivity to curvature.
- Mechanism: The preconditioning matrix $\hat{m}(t) = \sqrt{m(t)/(1-\beta^t)}$ is computed from accumulated squared gradients of $\log p(x, z|\theta\theta\theta)$, scaling the Langevin drift to account for local curvature.
- Core assumption: The accumulated second moments provide a reasonable approximation of local geometry that improves sampling efficiency.
- Evidence anchors:
  - [section]: "we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers"
  - [abstract]: "Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning"
  - [corpus]: Weak - no direct corpus evidence provided for this specific preconditioning approach
- Break condition: If the preconditioning is too aggressive or the decay rate $\beta$ is poorly chosen, it may destabilize the sampling dynamics.

## Foundational Learning

- Concept: Langevin dynamics as a sampling algorithm
  - Why needed here: The entire method relies on interpreting predictive coding as Langevin sampling, so understanding the mathematical foundations is crucial
  - Quick check question: What is the stationary distribution of overdamped Langevin dynamics with potential $U(z)$?

- Concept: Evidence lower bound (ELBO) and variational inference
  - Why needed here: The method optimizes model parameters using gradients of an ELBO computed from Langevin samples
  - Quick check question: Why can we ignore the entropy term of the approximate posterior when computing gradients w.r.t. model parameters?

- Concept: Kullback-Leibler divergence and its forward/reverse forms
  - Why needed here: Three different objectives (forward KL, reverse KL, Jeffrey's divergence) are evaluated for training the approximate inference network
  - Quick check question: What is the key difference in behavior between forward and reverse KL divergence in terms of mode vs. moment matching?

## Architecture Onboarding

- Component map:
  Generative model $p(x, z|\theta\theta\theta)$ -> Langevin sampler -> ELBO estimator -> Parameter optimizer
  Warm-start network $q(z|x, \phi\phi\phi)$ -> Langevin sampler -> ELBO estimator -> Parameter optimizer

- Critical path:
  1. Sample warm-start $z(0) \sim q(z|x, \phi\phi\phi)$
  2. Run Langevin dynamics for T steps to obtain samples
  3. Compute ELBO estimate using these samples
  4. Backpropagate gradients to update $\theta\theta\theta$ and $\phi\phi\phi$
  5. Repeat for each training batch

- Design tradeoffs:
  - Chain length T vs. computational cost: Longer chains give better samples but increase per-iteration cost
  - Preconditioning strength (step size and decay rate) vs. stability: Larger steps are faster but less stable
  - Warm-start quality vs. inference network capacity: Better warm-starts reduce mixing time but require more complex networks

- Failure signatures:
  - Poor sample quality (high FID/FDD): Likely issues with chain mixing or step size
  - Exploding gradients: Possible with forward KL objective or aggressive preconditioning
  - Slow convergence: Could indicate need for better warm-starts or preconditioning

- First 3 experiments:
  1. Train a simple VAE on MNIST using standard reparameterization trick to establish baseline
  2. Implement basic Langevin PC (without preconditioning or warm-starts) and compare convergence
  3. Add warm-start network and compare forward vs. reverse KL objectives for training it

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal choice of Langevin step size and preconditioning decay rate for different datasets and model architectures?
- Basis in paper: [inferred] The paper evaluates the impact of step size and preconditioning on sample quality, but does not determine optimal hyperparameters.
- Why unresolved: The authors only test a limited range of step sizes and decay rates, and the optimal values likely depend on the specific dataset and model.
- What evidence would resolve it: Systematic experiments varying step size and decay rate across multiple datasets and architectures, identifying the combinations that maximize sample quality and training speed.

### Open Question 2
- Question: How does Langevin Predictive Coding compare to other inference methods for training deep generative models, such as Hamiltonian Monte Carlo or normalizing flows?
- Basis in paper: [explicit] The authors state that Langevin Predictive Coding is a novel algorithm that builds upon predictive coding, but do not compare it to other inference methods.
- Why unresolved: The paper focuses on comparing Langevin Predictive Coding to standard VAEs, without exploring other inference techniques.
- What evidence would resolve it: Experiments training the same generative models using different inference methods, and comparing sample quality, diversity, and training speed.

### Open Question 3
- Question: Can Langevin Predictive Coding be extended to models with hierarchical latent variables, such as those found in state-of-the-art VAEs?
- Basis in paper: [explicit] The authors mention that hierarchical models may require a corresponding top-down hierarchical warm-start model, but do not explore this extension.
- Why unresolved: The paper only considers models with a single layer of latent variables, and the authors suggest that hierarchical models may require different warm-start objectives.
- What evidence would resolve it: Experiments training hierarchical generative models using Langevin Predictive Coding, and comparing the performance to standard VAEs and other inference methods.

## Limitations
- No ablation studies isolating the contribution of each component (Langevin sampling, warm-starts, preconditioning)
- Limited analysis of chain mixing and convergence diagnostics
- No comparison to more sophisticated sampling methods like Hamiltonian Monte Carlo

## Confidence
- Theoretical foundations of interpreting predictive coding as Langevin dynamics: Medium
- Claim that Langevin samples can effectively optimize ELBO gradients: Medium
- Preconditioning approach inspired by Riemannian Manifold Langevin dynamics: Low

## Next Checks
1. Implement trace plots and autocorrelation analysis for Langevin chains to verify mixing properties across different step sizes
2. Compare FID convergence rates with varying numbers of Langevin steps to determine optimal computational trade-offs
3. Perform an ablation study testing LPC performance with and without warm-starts and preconditioning to quantify individual component contributions