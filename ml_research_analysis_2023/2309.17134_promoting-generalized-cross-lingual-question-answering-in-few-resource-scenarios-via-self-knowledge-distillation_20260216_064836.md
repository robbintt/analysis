---
ver: rpa2
title: Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios
  via Self-knowledge Distillation
arxiv_id: '2309.17134'
source_url: https://arxiv.org/abs/2309.17134
tags:
- cross-lingual
- languages
- language
- transfer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how to improve cross-lingual question answering
  (QA) performance in low-resource languages. The authors propose using self-knowledge
  distillation and cross-lingual sampling to transfer QA knowledge from a high-resource
  language (English) to multiple low-resource target languages.
---

# Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation

## Quick Facts
- arXiv ID: 2309.17134
- Source URL: https://arxiv.org/abs/2309.17134
- Reference count: 40
- Primary result: Self-knowledge distillation with cross-lingual sampling significantly improves cross-lingual QA performance in low-resource languages

## Executive Summary
This paper addresses the challenge of cross-lingual question answering (QA) in low-resource languages by proposing a self-knowledge distillation approach with cross-lingual sampling. The method leverages a pre-trained multilingual BERT model fine-tuned on English SQuAD data, then further refines it using a small set of aligned QA examples across multiple languages. The key innovation is the use of mAP@k loss coefficients that dynamically weight the teacher's cross-lingual knowledge based on prediction quality, addressing the issue of incorrect teacher predictions. Experiments on MLQA, XQuAD, and TyDiQA-goldp datasets demonstrate significant performance improvements over standard fine-tuning methods, even achieving competitive results compared to strong baselines using machine-translated data.

## Method Summary
The approach involves fine-tuning a pre-trained mBERT model on English SQuAD data, then performing cross-lingual sampling to generate QA pairs with mixed question and context languages across target languages. Self-knowledge distillation is applied using a combination of cross-entropy and KL divergence losses, weighted by mAP@k coefficients that quantify the quality of teacher predictions. The model is trained for 3 epochs with a learning rate of 3e-5 and batch size of 24, evaluating on MLQA-dev set and selecting the best model based on G-XLT F1 score.

## Key Results
- Self-knowledge distillation with mAP@k coefficients significantly outperforms standard cross-entropy fine-tuning
- The approach achieves competitive results compared to strong baselines using machine-translated data
- G-XLT performance improves with increasing number of target languages, up to a point
- Ablation study shows the cross-entropy term is essential for good performance

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual sampling creates more diverse QA training pairs by mixing question and context languages. For each seed QA example in a source language, the method samples a subset of target languages and generates all possible question-context language combinations. This creates Ncross-lingual = Nseed · (1 + ntl)² examples where the teacher and student models learn to generalize across language boundaries. The approach assumes multilingual models can map semantically equivalent QA pairs in different languages to similar representations.

### Mechanism 2
Self-knowledge distillation with mAP@k coefficients dynamically weights teacher predictions based on their quality. The mAP@k coefficient is computed per example using the proportion of correct predictions within the top-k ranked answers. High coefficients indicate reliable teacher predictions, increasing their influence on the student model. This prevents propagation of incorrect cross-lingual knowledge by assuming the quality of teacher predictions varies across language pairs and ranks, and can be quantified using IR metrics.

### Mechanism 3
Removing the cross-entropy term from the distillation loss degrades performance. Unsupervised self-distillation relies solely on KL divergence between teacher and student distributions. The ablation study shows significant performance drops, indicating the cross-entropy term is essential for guiding student learning. This assumes the cross-entropy term provides necessary grounding to the ground-truth labels that KL divergence alone cannot supply.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper builds upon multilingual representations learned by mBERT and seeks to transfer QA knowledge across languages.
  - Quick check question: What is the difference between zero-shot and few-shot cross-lingual transfer?

- Concept: Knowledge distillation
  - Why needed here: The approach uses self-knowledge distillation where the student model learns from the teacher's soft predictions.
  - Quick check question: How does self-knowledge distillation differ from traditional knowledge distillation?

- Concept: Information retrieval metrics (mAP@k)
  - Why needed here: The mAP@k metric is used to dynamically weight the distillation loss based on prediction quality.
  - Quick check question: What does mAP@k measure in the context of QA predictions?

## Architecture Onboarding

- Component map: Input data -> Cross-lingual sampling -> Teacher model (mBERT-qa-en) -> mAP@k coefficient computation -> Self-distillation loss -> Student model -> Output model
- Critical path: 1) Fine-tune mBERT on SQuAD to create teacher model 2) Sample cross-lingual QA pairs from parallel dataset 3) Compute mAP@k coefficients for each example 4) Apply self-distillation loss with dynamic weighting 5) Update student model parameters 6) Synchronize teacher with student at epoch boundaries
- Design tradeoffs: Cross-lingual sampling increases training data but introduces redundancy; higher temperature in distillation softens predictions but may reduce precision; more target languages improve G-XLT but increase computational cost and overfitting risk
- Failure signatures: Poor performance on distant language pairs despite good results on related languages; high variance in mAP@k coefficients indicating inconsistent teacher quality; degraded performance when cross-entropy term is removed
- First 3 experiments: 1) Verify cross-lingual sampling generates correct number of examples using small parallel dataset 2) Test mAP@k coefficient computation on sample predictions with known correct answers 3) Run ablation study comparing models with and without cross-entropy term on development set

## Open Questions the Paper Calls Out

### Open Question 1
How does the mAP@k coefficient approach compare to other dynamic weighting techniques for knowledge distillation in cross-lingual QA? The paper only compares their mAP@k approach to standard static weighting and does not explore other dynamic weighting techniques. Experiments comparing the mAP@k approach to other dynamic weighting methods (e.g., based on prediction entropy or uncertainty) on cross-lingual QA tasks would resolve this.

### Open Question 2
Can the self-knowledge distillation and cross-lingual sampling approach be extended to other NLP tasks beyond extractive QA? The paper only evaluates the approach on extractive QA tasks and does not explore its applicability to other NLP tasks. Experiments applying the approach to other NLP tasks (e.g., text classification, named entity recognition) and comparing its performance to standard fine-tuning would resolve this.

### Open Question 3
How does the quality and quantity of parallel QA data affect the performance of the cross-lingual sampling approach? The paper acknowledges that the availability of high-quality parallel QA datasets relies on human annotations, which can be resource-intensive. Experiments varying the quality (e.g., using machine-translated vs. human-translated data) and quantity of parallel QA data and measuring its impact on the performance of the cross-lingual sampling approach would resolve this.

## Limitations
- Limited ablation studies don't systematically isolate individual components beyond the cross-entropy term
- Evaluation focuses primarily on Indo-European languages, with insufficient analysis of truly low-resource or distant language pairs
- Implementation complexity and reproducibility concerns due to lack of specificity in key components like mAP@k coefficient computation

## Confidence

**High confidence**: Core experimental results showing performance improvements over baseline fine-tuning methods on MLQA and XQuAD datasets.

**Medium confidence**: G-XLT metric and its interpretation as a summary of cross-lingual transfer performance.

**Low confidence**: Scalability claims to truly low-resource scenarios, as evaluation uses datasets with reasonable amounts of parallel data rather than extreme low-resource regimes.

## Next Checks

1. **Ablation of cross-lingual sampling strategy**: Conduct a controlled experiment isolating the effect of the quadratic sampling approach by comparing against simpler cross-lingual fine-tuning methods while keeping all other components constant.

2. **Stress test on distant language pairs**: Evaluate the approach on language pairs from different families with varying degrees of typological similarity (e.g., English-Japanese, English-Swahili) to assess the limits of cross-lingual transfer.

3. **Resource efficiency analysis**: Measure the actual computational overhead of the cross-lingual sampling approach and quantify the trade-off between performance gains and training time/memory requirements across different values of ntl (number of target languages).