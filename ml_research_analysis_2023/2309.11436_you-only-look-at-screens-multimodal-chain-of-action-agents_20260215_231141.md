---
ver: rpa2
title: 'You Only Look at Screens: Multimodal Chain-of-Action Agents'
arxiv_id: '2309.11436'
source_url: https://arxiv.org/abs/2309.11436
tags:
- action
- text
- class
- language
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Auto-GUI, a multimodal chain-of-action agent
  for autonomous GUI control. Auto-GUI directly interacts with the interface, bypassing
  the need for environment parsing or application-specific APIs.
---

# You Only Look at Screens: Multimodal Chain-of-Action Agents

## Quick Facts
- arXiv ID: 2309.11436
- Source URL: https://arxiv.org/abs/2309.11436
- Reference count: 40
- Primary result: 90% action type prediction accuracy, 74% overall action success rate

## Executive Summary
Auto-GUI introduces a multimodal chain-of-action agent for autonomous GUI control that directly processes screen images and action histories without environment parsing or application-specific APIs. The model uses a vision-language architecture with chain-of-action technique to predict actions by considering both previous histories and future plans. Evaluated on the new AITW benchmark with 30K unique instructions, Auto-GUI achieves state-of-the-art performance with 90% action type prediction accuracy and 74% overall action success rate across multi-step tasks including application operation, web searching, and shopping.

## Method Summary
Auto-GUI is a multimodal approach that directly interacts with GUI interfaces by processing raw screen images and action histories through a vision encoder (BLIP-2) and language encoder, then fusing features via self-attention before generating future action plans and current action predictions. The chain-of-action technique leverages previous action histories and future action plans to improve decision-making, while coordinate normalization standardizes touch and lift points. The model is trained on the AITW benchmark using up to 10 epochs with learning rate 1e-4 and batch size 4, employing FLAN-Alpaca initialization across parameter sizes from 60M to 700M.

## Key Results
- 90% action type prediction accuracy on AITW benchmark
- 74% overall action success rate across 715K episodes
- Outperforms baseline methods by significant margins in both click region accuracy and scroll direction prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-action leverages previous action histories and future action plans to improve action prediction accuracy.
- Mechanism: The model uses a multimodal encoder to process both the current screen and a chain of previous actions, then generates a chain of future action plans before predicting the current action. This creates a bidirectional temporal context that guides decision-making.
- Core assumption: The sequence of actions leading to a goal contains sufficient information to predict the next action, even without environment parsing.
- Evidence anchors:
  - [abstract]: "we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute"
  - [section]: "a chain of action consists of two parts in the procedure above: a chain of previous action histories on the input side and a chain of future action plans on the output side"
  - [corpus]: Weak - corpus lacks direct evidence about this specific mechanism, though related papers discuss chain-of-action techniques
- Break condition: If the action history becomes too long or the future plans are too uncertain, the model may lose focus and make incorrect predictions.

### Mechanism 2
- Claim: Direct screen interaction without environment parsing reduces inference latency and error propagation.
- Mechanism: Instead of converting UI elements to HTML/text, the model directly processes raw screen images with a vision encoder and generates executable actions, bypassing the need for OCR, icon detection, or API calls.
- Core assumption: Vision features contain sufficient information for action prediction without needing intermediate textual representations.
- Evidence anchors:
  - [abstract]: "directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs"
  - [section]: "Auto-UI is a multimodal approach that directly interacts with the interface"
  - [corpus]: Weak - corpus mentions similar approaches but lacks direct evidence for this specific claim
- Break condition: If the screen contains complex layouts or text-heavy interfaces where visual features alone are insufficient, performance may degrade.

### Mechanism 3
- Claim: Coordinate normalization improves convergence and reduces ambiguity in action predictions.
- Mechanism: The model uses normalized [0,1] coordinates for touch and lift points instead of raw pixel values, with specific formatting for clicks (same coordinates) and scrolls (directional templates).
- Core assumption: High-precision coordinates are unnecessary for representing user actions, and normalization helps the model generalize better.
- Evidence anchors:
  - [section]: "We apply normalized values of the coordinates, which helps accelerate convergence and mitigate the ambiguity of coordinates"
  - [section]: "For scroll actions, we first determine the scroll direction with the touch point and lift point. Then, we transform the touch and lift points into fixed directional coordinates"
  - [corpus]: Weak - corpus lacks direct evidence about coordinate normalization techniques
- Break condition: If the normalization scheme doesn't capture important spatial relationships or if the screen resolution varies significantly, action accuracy may suffer.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: The model must integrate visual screen features with textual instruction and action history
  - Quick check question: What are the two main input modalities that need to be fused in this architecture?

- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: The chain-of-action technique is inspired by chain-of-thought prompting, requiring the model to plan future actions
  - Quick check question: How does chain-of-action differ from standard chain-of-thought prompting?

- Concept: Coordinate normalization and gesture representation
  - Why needed here: Actions are represented as normalized coordinates rather than raw pixels, requiring understanding of spatial relationships
  - Quick check question: Why might normalized coordinates be more effective than raw pixel values for this task?

## Architecture Onboarding

- Component map: Screen → Vision encoder (BLIP-2) → Self-attention → Gated fusion → Decoder → Action prediction
- Critical path: Screen → Vision encoder → Self-attention → Gated fusion → Decoder → Action prediction
- Design tradeoffs:
  - Using raw screens vs. parsed HTML: Faster inference but potentially less precise spatial understanding
  - Chain-of-action vs. reactive prediction: Better accuracy but increased computational complexity
  - Coordinate normalization vs. raw coordinates: Improved generalization but potential loss of fine-grained spatial information
- Failure signatures:
  - High action type accuracy but low click region accuracy: Vision encoder may not capture sufficient spatial detail
  - Good performance on simple tasks but failure on complex multi-app workflows: Chain-of-action context may be insufficient
  - Fast initial learning but plateauing performance: Model may need larger context window or better vision features
- First 3 experiments:
  1. Test with and without chain-of-action history to measure its impact on accuracy
  2. Compare different vision encoders (CLIP vs. BLIP-2) to evaluate feature quality
  3. Vary the length of action history and future plans to find optimal context window size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Auto-UI vary with different vision feature encoders beyond CLIP and BLIP-2?
- Basis in paper: [explicit] The paper compares CLIP and BLIP-2 vision features and finds BLIP-2 achieves better performance.
- Why unresolved: The paper only compares two vision feature encoders. There may be other vision feature encoders that could perform better.
- What evidence would resolve it: Conduct experiments with a wider range of vision feature encoders and compare their performance with Auto-UI.

### Open Question 2
- Question: How does the model scale affect the performance of Auto-UI?
- Basis in paper: [explicit] The paper mentions that scaling parameter size provides relatively marginal performance gains and focuses on base and large models.
- Why unresolved: The paper does not explore the effect of using smaller or larger models on the performance of Auto-UI.
- What evidence would resolve it: Conduct experiments with models of different scales and compare their performance with Auto-UI.

### Open Question 3
- Question: Can Auto-UI be further improved by incorporating screen descriptions containing icon and text information?
- Basis in paper: [explicit] The paper mentions that Auto-UI can perform better when annotated screen descriptions are available.
- Why unresolved: The paper does not explore the effect of incorporating screen descriptions on the performance of Auto-UI in detail.
- What evidence would resolve it: Conduct experiments with and without screen descriptions and compare their performance with Auto-UI.

## Limitations
- Evaluation relies on a single benchmark (AITW) with specific device and OS configuration, limiting generalizability
- 74% overall success rate indicates significant failure rates in real-world deployment scenarios
- Performance may degrade with dynamic interfaces or ambiguous visual contexts

## Confidence
- High confidence: The core chain-of-action mechanism and multimodal architecture design are well-supported by experimental results
- Medium confidence: Reported performance metrics are internally consistent but lack external validation
- Medium confidence: Coordinate normalization approach appears effective but needs testing across different screen resolutions

## Next Checks
1. Cross-platform generalization test: Evaluate Auto-GUI on at least two additional GUI control benchmarks with different operating systems, device types, and application ecosystems to assess true domain generalization beyond the AITW dataset.

2. Failure mode analysis: Conduct systematic ablation studies where chain-of-action histories are progressively shortened or corrupted to identify the minimum viable context length and characterize performance degradation patterns.

3. Real-world deployment trial: Implement Auto-GUI in a controlled user study with diverse application workflows to measure success rates on tasks not seen during training, particularly focusing on multi-app interactions and error recovery scenarios.