---
ver: rpa2
title: Efficient Post-training Quantization with FP8 Formats
arxiv_id: '2309.14592'
source_url: https://arxiv.org/abs/2309.14592
tags:
- quantization
- accuracy
- arxiv
- e4m3
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FP8 quantization as a more efficient alternative
  to INT8 for deep neural network inference, addressing the need for improved quantization
  methods to meet the computational demands of modern architectures like LLMs and
  Diffusion models. The authors study three different FP8 representations (E5M2, E4M3,
  and E3M4) to analyze the trade-off between dynamic range and precision on model
  accuracy.
---

# Efficient Post-training Quantization with FP8 Formats

## Quick Facts
- **arXiv ID**: 2309.14592
- **Source URL**: https://arxiv.org/abs/2309.14592
- **Reference count**: 25
- **Primary result**: FP8 quantization achieves 92.64% workload coverage vs 65.87% for INT8, with better accuracy preservation across diverse model architectures.

## Executive Summary
This paper proposes FP8 quantization as a more efficient alternative to INT8 for deep neural network inference, addressing the need for improved quantization methods to meet the computational demands of modern architectures like LLMs and Diffusion models. The authors study three different FP8 representations (E5M2, E4M3, and E3M4) to analyze the trade-off between dynamic range and precision on model accuracy. They develop a generalized quantization workflow that can be applied across different network architectures and conduct extensive experiments on 75 unique network architectures covering a wide range of tasks. The results show that FP8 formats outperform INT8 in terms of workload coverage, model accuracy, and suitability for a broader range of operations.

## Method Summary
The method involves post-training quantization using three FP8 formats (E5M2, E4M3, E3M4) applied to 75 network architectures across various domains. The workflow includes calibration data preparation, range calibration using max scaling, application of standard quantization schemes (per-channel weights, per-tensor activations), and extended quantization with mixed formats and expanded operator coverage. Models are evaluated on workload pass rate (<1% relative accuracy loss vs FP32), accuracy metrics, and FID scores for image generation tasks, with comparisons against INT8 baseline.

## Key Results
- FP8 formats achieve 92.64% workload coverage versus 65.87% for INT8
- E4M3 is better suited for NLP models (96.32% coverage) while E3M4 performs marginally better on computer vision tasks (78.95% vs 73.68%)
- Mixed FP8 formats show higher accuracy than single format approaches on NLP workloads
- Expanded operator coverage enables quantization of operations like LayerNorm and BatchNorm that are difficult with INT8

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FP8 formats outperform INT8 by providing a wider dynamic range with sufficient precision, reducing quantization error for distributions with outliers.
- Mechanism: The floating-point representation in FP8 uses a mantissa and exponent, allowing non-uniform spacing of values. This denser representation near zero improves accuracy for small values, while the exponent provides a larger range to represent outliers without excessive quantization error.
- Core assumption: The data distributions in deep learning models exhibit long-tailed normal distributions with outliers that benefit from the non-uniform grid spacing of floating-point formats.
- Evidence anchors:
  - [abstract]: "FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of operations."
  - [section]: "The data also suggests that E4M3 is better suited for a broad range of NLP models with a coverage of 96.32% compared to E3M4 (92.11%), while E3M4 performs slightly better on computer vision models with 78.95% coverage compared to E4M3 (73.68%)."
  - [corpus]: Weak evidence; neighboring papers discuss FP8 quantization but do not directly confirm the specific claim about dynamic range vs precision trade-offs.

### Mechanism 2
- Claim: Mixed FP8 formats (using different formats for weights and activations) optimize the trade-off between dynamic range and precision for different tensor types.
- Mechanism: Weights in NLP and CV models follow normal distributions requiring more mantissa bits for precision, while activations in NLP models contain outliers requiring more exponent bits for range. Assigning E4M3 (more exponent) to activations and E3M4 (more mantissa) to weights balances these needs.
- Core assumption: Different tensor types in neural networks have distinct statistical properties that can be better represented by tailored floating-point formats.
- Evidence anchors:
  - [section]: "Figure 3 shows typical distributions of weight and activation tensors in NLP and computer vision workloads... We balance this trade-off by assigning E5M2 or E4M3 format for range-bound tensors and E3M4 for precision-bound tensors."
  - [section]: "Table 5: Model Accuracy of FP8 Format (Single vs. Mixed). Mixed FP8 formats (in bold) show higher accuracy than all the other single FP8 formats on the below NLP workloads."
  - [corpus]: No direct evidence; neighboring papers mention FP8 quantization but do not discuss mixed format strategies.

### Mechanism 3
- Claim: Expanded operator coverage with FP8 enables quantization of operations like LayerNorm and BatchNorm that are difficult with INT8, preserving accuracy.
- Mechanism: FP8's floating-point representation can more accurately model the scaling and normalization operations without the approximation errors that occur when using integer formats, which require piecewise linear approximations.
- Core assumption: Operations such as LayerNorm and BatchNorm are sensitive to quantization errors and benefit from the continuous value representation of floating-point formats.
- Evidence anchors:
  - [abstract]: "Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of operations."
  - [section]: "Our experiments show that FP8 formats are capable of handling these operators without sacrificing model accuracy."
  - [corpus]: No direct evidence; neighboring papers discuss quantization but do not specifically address operator coverage with FP8.

## Foundational Learning

- Concept: Floating-point representation (sign, exponent, mantissa)
  - Why needed here: Understanding how FP8 encodes numbers explains why it can represent a wider range with better precision for certain distributions.
  - Quick check question: How does the exponent in a floating-point number affect its dynamic range and precision?

- Concept: Quantization error and its impact on model accuracy
  - Why needed here: Quantization reduces precision; understanding error propagation helps in choosing the right format and quantization strategy.
  - Quick check question: What is the difference between uniform quantization (like INT8) and non-uniform quantization (like FP8) in terms of error distribution?

- Concept: Tensor distribution characteristics (normal vs. outlier-heavy)
  - Why needed here: Different layers produce tensors with different statistical properties; choosing quantization parameters depends on these characteristics.
  - Quick check question: How do the distributions of weights and activations typically differ in NLP models compared to CV models?

## Architecture Onboarding

- Component map:
  - Data type emulation (FP8 emulation toolkit running on FP32 hardware)
  - Model quantization (Neural Compressor with standard and extended schemes)
  - Calibration (max scaling, BatchNorm calibration with data augmentation)
  - Operator coverage (Conv2d, Linear, Embedding, BatchNorm, LayerNorm, Add, Mul)
  - Format selection (E5M2, E4M3, E3M4 with mixed formats option)

- Critical path:
  1. Calibration data preparation with appropriate augmentations
  2. Range calibration using max scaling for E4M3 and E3M4
  3. Application of standard quantization scheme to common operators
  4. Application of extended quantization scheme (mixed formats, expanded operator coverage)
  5. BatchNorm calibration for CV models
  6. Accuracy evaluation and tuning if necessary

- Design tradeoffs:
  - Dynamic range vs. precision in choosing between E4M3 and E3M4
  - Static vs. dynamic quantization (static is default for efficiency, dynamic may improve accuracy)
  - Mixed formats add complexity but can improve accuracy
  - Expanded operator coverage increases implementation complexity but broadens applicability

- Failure signatures:
  - Significant accuracy loss (>1%) indicating poor format choice or calibration
  - Low workload coverage suggesting certain operators or model types are not handled well
  - High variability in accuracy loss across models indicating sensitivity to specific patterns

- First 3 experiments:
  1. Run standard quantization on a simple CNN (e.g., ResNet-50) with E4M3 and E3M4 to compare accuracy and coverage.
  2. Apply mixed FP8 formats to an NLP model (e.g., BERT) to verify improved accuracy over single format.
  3. Enable expanded operator coverage (LayerNorm, BatchNorm) on a transformer model to test if accuracy is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mantissa bit allocation for FP8 formats across different model architectures and tasks?
- Basis in paper: [explicit] The paper shows that E4M3 is better suited for NLP models while E3M4 performs marginally better on computer vision tasks, but doesn't definitively determine the optimal bit allocation.
- Why unresolved: The study shows general trends but doesn't explore the full design space of possible FP8 formats or their impact on different model types.
- What evidence would resolve it: A comprehensive study comparing various FP8 format combinations across all model architectures and tasks to identify the optimal mantissa bit allocation for each scenario.

### Open Question 2
- Question: How does the choice of quantization scheme (static vs dynamic) impact model accuracy across different FP8 formats?
- Basis in paper: [explicit] The paper mentions that dynamic quantization can improve accuracy for E4M3 and E3M4 formats on selected models, but doesn't provide a comprehensive comparison.
- Why unresolved: The study only briefly touches on the impact of dynamic quantization and doesn't provide a detailed analysis of its effects on different FP8 formats.
- What evidence would resolve it: A thorough comparison of static and dynamic quantization across all FP8 formats and model architectures to determine the optimal quantization scheme for each scenario.

### Open Question 3
- Question: What is the impact of outlier handling techniques on model accuracy when using FP8 formats?
- Basis in paper: [explicit] The paper mentions that simple max scaling is sufficient for handling outliers in FP8 formats, but doesn't explore other outlier handling techniques.
- Why unresolved: The study only briefly mentions outlier handling and doesn't provide a detailed analysis of its impact on model accuracy.
- What evidence would resolve it: A comprehensive study comparing different outlier handling techniques (e.g., percentile, KL divergence) across all FP8 formats and model architectures to determine the optimal approach for each scenario.

## Limitations

- The study relies on standardized benchmark models rather than custom architectures that might exhibit different quantization characteristics.
- FP8 quantization benefits are contingent on specific hardware support (Xeon 4th/5th generation CPUs with Intel Advanced Matrix Extensions).
- The mixed format approach adds significant complexity to deployment without clear guidelines for automated format selection.

## Confidence

**High Confidence Claims** (supported by extensive empirical evidence):
- FP8 formats achieve higher workload coverage (92.64%) compared to INT8 (65.87%)
- E4M3 format provides better coverage for NLP models (96.32%) than E3M4
- FP8 quantization preserves accuracy for models that INT8 quantizes successfully
- Expanded operator coverage enables quantization of previously challenging operations

**Medium Confidence Claims** (supported by evidence but with limitations):
- E3M4 performs better for computer vision models than E4M3
- Mixed FP8 formats improve accuracy compared to single format approaches
- The trade-off between dynamic range and precision justifies format selection

**Low Confidence Claims** (weakly supported or require further validation):
- FP8 will consistently outperform INT8 across all future model architectures
- The calibration procedures described are optimal for all model types
- Hardware-specific optimizations will translate directly to other platforms

## Next Checks

**Validation Check 1: Cross-Platform Performance Evaluation**
Implement the FP8 quantization workflow on multiple hardware platforms (including non-Intel architectures) and measure performance overhead differences, accuracy consistency across implementations, and impact of hardware-specific optimizations.

**Validation Check 2: Format Selection Automation Study**
Develop and evaluate an automated format selection mechanism that analyzes tensor statistics to predict optimal format choice, measures accuracy loss when using automated vs manual selection, and tests across broader set of custom production models.

**Validation Check 3: Calibration Sensitivity Analysis**
Conduct comprehensive study of calibration sensitivity by varying calibration dataset sizes and sampling strategies, comparing static vs dynamic quantization effectiveness across model types, and measuring impact of different calibration augmentation techniques on final accuracy.