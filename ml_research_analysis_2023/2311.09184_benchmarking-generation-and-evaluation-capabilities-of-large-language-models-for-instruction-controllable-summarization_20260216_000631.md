---
ver: rpa2
title: Benchmarking Generation and Evaluation Capabilities of Large Language Models
  for Instruction Controllable Summarization
arxiv_id: '2311.09184'
source_url: https://arxiv.org/abs/2311.09184
tags:
- evaluation
- summary
- llms
- arxiv
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks large language models (LLMs) on instruction
  controllable text summarization, where both an article and a natural language requirement
  are given. The authors curate a human-annotated evaluation dataset and conduct human
  evaluation of five LLM-based summarization systems.
---

# Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization

## Quick Facts
- **arXiv ID**: 2311.09184
- **Source URL**: https://arxiv.org/abs/2311.09184
- **Authors**: 
- **Reference count**: 16
- **Key outcome**: Benchmarks LLMs on instruction controllable summarization, finding significant challenges in both generation and evaluation capabilities with large performance gaps between models.

## Executive Summary
This paper presents a comprehensive benchmark of large language models (LLMs) on instruction controllable text summarization, where both a source article and natural language requirements are provided. The authors create a human-annotated evaluation dataset and assess five LLM-based summarization systems through human evaluation across four quality dimensions. They also benchmark 40 LLM-based automatic evaluation methods using 11 different LLMs and four evaluation protocols. The study reveals that instruction controllable summarization remains challenging for current LLMs, with significant performance variations between models and weak alignment between LLM-based evaluators and human judgment.

## Method Summary
The authors curate an evaluation-only dataset of 100 article-requirement pairs from the XL-Sum dataset, then generate summaries using five representative LLMs (text-davinci-002, text-davinci-003, gpt-3.5-turbo-0301, gpt-4-0314, and hybrid LLM-human summaries). Human annotations are collected through Amazon Mechanical Turk across four quality dimensions (overall quality, missing information, irrelevant information, factual consistency), with annotators passing qualification tests. The study benchmarks 40 LLM-based automatic evaluation methods using different evaluation protocols (LLMScore, LLMEval, LLMCompare, LLMRank) and analyzes generator-evaluator consistency across LLMs.

## Key Results
- All evaluated LLMs make factual and other errors in instruction controllable summarization, with large performance gaps between models
- GPT-4 significantly outperforms GPT-3.5 models, while supervisedly fine-tuned text-davinci-002 performs worse than RLHF fine-tuned models
- LLM-based evaluation methods show weak alignment with human annotators and exhibit bias toward longer summaries
- Generation performance does not always align with evaluation performance across different LLMs
- Missing information is identified as a more challenging quality aspect than irrelevant information for LLM evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models (LLMs) struggle with instruction controllable summarization due to difficulty handling multiple dimensions of control simultaneously.
- Mechanism: The task requires LLMs to integrate source article content, summary requirement, and desired output characteristics, creating cognitive load that exceeds current LLM capacity for complex reasoning across multiple constraints.
- Core assumption: Instruction controllable summarization is inherently more complex than generic summarization due to the need for simultaneous constraint satisfaction.
- Evidence anchors:
  - [abstract] "Our study reveals that instruction controllable text summarization remains a challenging task for LLMs"
  - [section 2.2] "we found that as the complexity of the summarization task rises, evaluating the summaries becomes increasingly difficult"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.348, average citations=0.0. Top related titles: Benchmarking Large Language Models on Controllable Generation under Diversified Instructions, Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards
- Break condition: If evaluation methods can achieve strong alignment with human annotators, suggesting the difficulty is more about evaluation than generation capability.

### Mechanism 2
- Claim: LLM-based evaluation methods fail to align with human judgment due to bias toward summary length and format rather than content quality.
- Mechanism: LLMs as evaluators tend to favor longer summaries that appear more comprehensive, conflating length with quality rather than evaluating the actual adherence to summary requirements.
- Core assumption: LLMs have inherent biases in evaluation that mirror human biases toward verbosity and completeness over precision and relevance.
- Evidence anchors:
  - [section 4.2] "since different generations of LLMs on both ins-controllable summary generation and evaluation, suggesting its higher complexity compared with the generic summarization task"
  - [section 3.3] "The performance of the LLM-based evaluation methods differs on different evaluation quality aspects. In particular, Missing Information is a more challenging aspect than Irrelevant Information"
  - [corpus] Average neighbor FMR=0.348 suggests moderate relatedness between this work and corpus papers
- Break condition: If evaluation methods can consistently rank shorter, more focused summaries higher when they better meet requirements.

### Mechanism 3
- Claim: Different LLMs show large performance gaps in both generation and evaluation due to architectural differences in training approaches.
- Mechanism: Supervised fine-tuning produces different capabilities than reinforcement learning from human feedback, affecting both the ability to follow instructions and evaluate others' outputs.
- Core assumption: Training methodology creates fundamental differences in how models understand and execute complex tasks requiring multiple constraint satisfaction.
- Evidence anchors:
  - [section 2.3] "There is a large performance gap among the different LLMs. Specifically, GPT-4 is significantly better than the GPT-3.5 models, and the supervisedly fine-tuned text-davinci-002 archives worse performance than the LLMs fine-tuned with reinforcement learning from human feedback"
  - [section 3.4] "The generation performance does not always align with the evaluation performance"
  - [corpus] Found papers on benchmarking LLMs for controllable generation and evaluation
- Break condition: If similar performance is achieved across different training methodologies on the same task.

## Foundational Learning

- Concept: Instruction following in language models
  - Why needed here: The entire task depends on models' ability to understand and execute natural language instructions specifying summary requirements
  - Quick check question: Can the model distinguish between "summarize the timeline" and "summarize the causes and effects" when given the same article?

- Concept: Multi-dimensional quality assessment
  - Why needed here: The evaluation framework uses four distinct quality dimensions (overall quality, missing information, irrelevant information, factual consistency) that must be understood separately
  - Quick check question: Can you explain why a summary might score high on overall quality but low on factual consistency?

- Concept: Evaluation protocol design
  - Why needed here: Different evaluation protocols (LLMScore, LLMEval, LLMCompare, LLMRank) produce varying results, requiring understanding of their strengths and weaknesses
  - Quick check question: Which evaluation protocol would you use if you wanted to compare two summaries directly rather than score them independently?

## Architecture Onboarding

- Component map: Article selection → Requirement writing → Summary generation → Human evaluation → LLM-based evaluation → Analysis and benchmarking
- Critical path: The system flows from curated article-requirement pairs through multiple LLM generations, human annotations, and automatic evaluation methods to produce benchmarking insights
- Design tradeoffs: Manual dataset creation ensures quality but limits scale; using multiple evaluation protocols provides comprehensive assessment but increases complexity; comparing LLMs as both generators and evaluators reveals consistency issues but requires careful experimental design
- Failure signatures: Low inter-annotator agreement suggests task difficulty; LLM evaluators showing length bias indicates need for normalization; generation-evaluation gaps reveal model limitations
- First 3 experiments:
  1. Compare summary quality when using different requirement types (informational vs formatting vs meta) on the same articles
  2. Test whether adding chain-of-thought prompting improves LLM evaluation performance on identifying missing information
  3. Evaluate whether controlling summary length differences eliminates evaluator bias in automatic benchmarking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the length bias in LLM-based evaluation be mitigated through prompt engineering or alternative evaluation protocols?
- Basis in paper: [explicit] The paper demonstrates that GPT-4 tends to favor longer summaries in its evaluation, leading to biased results when comparing different LLMs.
- Why unresolved: While the authors suggest the need for more robust evaluation methods, they do not explore potential solutions or techniques to address this bias.
- What evidence would resolve it: Experiments comparing the performance of different evaluation protocols and prompt engineering techniques in reducing length bias would help determine if this issue can be effectively mitigated.

### Open Question 2
- Question: How does the performance of LLMs on instruction controllable summarization compare to other complex summarization tasks, such as multi-document or aspect-based summarization?
- Basis in paper: [inferred] The paper focuses on instruction controllable summarization, but does not provide a direct comparison with other challenging summarization settings.
- Why unresolved: While the authors suggest that instruction controllable summarization is more complex than generic summarization, a comprehensive comparison with other tasks is needed to validate this claim.
- What evidence would resolve it: Benchmarking LLMs on multiple complex summarization tasks and comparing their performance across these tasks would provide insights into the relative difficulty and model capabilities.

### Open Question 3
- Question: Can the quality of instruction controllable summaries be improved by incorporating feedback mechanisms or interactive dialogue between the user and the summarization system?
- Basis in paper: [explicit] The paper highlights the challenges in evaluating and improving instruction controllable summaries, suggesting the need for more advanced techniques.
- Why unresolved: The current approach relies on static instructions and does not explore the potential benefits of dynamic feedback or interactive dialogue in refining the summaries.
- What evidence would resolve it: Developing and evaluating interactive summarization systems that incorporate user feedback and dialogue would demonstrate the effectiveness of this approach in improving summary quality.

## Limitations

- Human-annotated evaluation dataset consists of only 100 examples, potentially limiting statistical power and generalizability
- Inter-annotator agreement scores were relatively low, particularly for factual consistency (Krippendorff's α=0.244), complicating validation of evaluation methods
- Evaluation covers only 5 LLM-based summarization systems and 11 LLMs, potentially missing performance variations from other models in the rapidly evolving LLM landscape

## Confidence

**High Confidence:** The claim that instruction controllable text summarization remains challenging for LLMs is strongly supported by consistent findings across multiple evaluation metrics and human annotations. The observation of large performance gaps between different LLMs is also well-established through direct comparative analysis.

**Medium Confidence:** The claim about LLM-based evaluation methods failing to strongly align with human annotators has moderate confidence due to the low inter-annotator agreement in the human annotations themselves, which complicates validation of automatic evaluation methods. The specific finding that LLM evaluators show bias toward longer summaries is supported but requires further validation with controlled experiments.

**Low Confidence:** The mechanism explaining why different training methodologies (supervised fine-tuning vs reinforcement learning from human feedback) create fundamental differences in task understanding is speculative and not directly tested in this study. The assertion that evaluation difficulty scales with summarization complexity is inferred but not empirically validated through systematic complexity manipulation.

## Next Checks

1. **Replication with larger dataset:** Create and evaluate a 500-example version of INSTRU SUM to determine if the observed performance patterns and inter-annotator agreement scores remain consistent at larger scale.

2. **Controlled bias experiments:** Systematically vary summary length while keeping content quality constant to definitively establish whether LLM evaluators are biased toward verbosity or whether observed patterns reflect genuine quality differences.

3. **Cross-task generalization:** Test whether the identified LLM performance gaps in instruction controllable summarization extend to other complex instruction-following tasks (e.g., multi-hop question answering, code generation with constraints) to validate whether the findings reflect general LLM limitations or task-specific challenges.