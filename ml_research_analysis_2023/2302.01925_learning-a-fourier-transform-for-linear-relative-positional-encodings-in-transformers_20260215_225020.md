---
ver: rpa2
title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
arxiv_id: '2302.01925'
source_url: https://arxiv.org/abs/2302.01925
tags:
- attention
- learning
- linear
- https
- performer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FourierLearner-Transformers (FLTs), a new
  class of linear Transformers that efficiently incorporate relative positional encoding
  (RPE) mechanisms. The key idea is to learn the optimal RPE mechanism implicitly
  by learning its spectral (Fourier) representation, enabling linear attention complexity
  without requiring additional assumptions about the RPE-mask structure.
---

# Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers

## Quick Facts
- arXiv ID: 2302.01925
- Source URL: https://arxiv.org/abs/2302.01925
- Authors: 
- Reference count: 40
- Key outcome: FourierLearner-Transformers (FLTs) achieve 1.0 point lower perplexity than regular Performer on language modeling, 2.3% accuracy gain on ImageNet classification, and 0.04eV lower MAE on molecular property prediction tasks.

## Executive Summary
This paper introduces FourierLearner-Transformers (FLTs), a new class of linear Transformers that efficiently incorporate relative positional encoding (RPE) mechanisms. The key innovation is learning the optimal RPE mechanism implicitly by learning its spectral (Fourier) representation, enabling linear attention complexity without requiring additional assumptions about the RPE-mask structure. FLTs achieve significant performance improvements across multiple tasks while maintaining practical computational efficiency and supporting a wide range of RPE mechanisms including local RPEs that inject locality bias.

## Method Summary
FLTs learn the Fourier Transform of the RPE function f, allowing random feature maps to approximate f(x-y) as an inner product E[ϕ(x)⊤ψ(y)]. This enables low-rank decomposition of the RPE-mask without explicit matrix construction, supporting arbitrary shift-invariant kernels and local RPE mechanisms via smooth Fourier approximations of indicator functions. The method uses learned spectral parameters to compute random features for RPE encoding, combining them with standard attention computation for linear complexity.

## Key Results
- 1.0 point lower perplexity than regular Performer on WikiText-103 language modeling
- 2.3% accuracy gain on ImageNet classification tasks
- 0.04eV lower MAE on molecular property prediction tasks
- First Transformers providing RPE-enhanced linear attention for 3D molecular data

## Why This Works (Mechanism)

### Mechanism 1
Learning the Fourier Transform of RPE functions enables efficient linear-complexity attention with general RPE support. By learning the spectral representation g of RPE function f, random feature maps approximate f(x-y) as inner products, enabling low-rank decomposition without explicit matrix construction. The approach assumes RPE functions are bandlimited or have smooth spectral representations. Evidence shows FLTs construct optimal RPE mechanisms implicitly via spectral learning, though the method may struggle with sharp discontinuities requiring many Fourier terms.

### Mechanism 2
Local RPE mechanisms are modeled via Fourier Transforms that factor into products of sinc functions. Local RPEs depending on nearby tokens (fv,C(∆r) = C·1[|∆r|≤v]) have Fourier Transforms of form sin(2πvξ)/(πξ), allowing smooth approximations of indicator functions. The approach assumes local RPEs can be well-approximated by continuous functions with closed-form Fourier expressions. Evidence shows RPEs can be generalized to higher-dimensional local RPEs with factorized Fourier transforms, though complex non-separable locality patterns may exceed sinc approximation capabilities.

### Mechanism 3
Learning spectral representations allows FLT to support arbitrary shift-invariant kernels for RPE-masks without structural constraints. By Bochner's theorem, shift-invariant kernels K(x-y) have Fourier representations, and FLT learns the spectral density directly. The approach assumes RPE-masks correspond to valid positive definite kernels with well-defined Fourier representations. Evidence shows the method can apply to any shift-invariant kernel matrix, though performance may degrade if RPE-masks aren't derived from such kernels or have negative eigenvalues.

## Foundational Learning

- Concept: Fourier Transform and its properties (linearity, convolution theorem, Parseval's theorem)
  - Why needed here: Core innovation relies on representing RPE functions in spectral domain using random features from Fourier transforms
  - Quick check question: What is the Fourier Transform of f(x) = exp(-x²/2σ²)? (Answer: g(ξ) = √(2π)σ exp(-2π²σ²ξ²))

- Concept: Random feature maps for kernel approximation (Rahimi & Recht, 2007)
  - Why needed here: FLT uses random feature maps ϕ and ψ to approximate RPE function f(x-y) as inner product, enabling linear attention
  - Quick check question: How many random features needed to approximate Gaussian kernel within ε accuracy with probability 1-δ? (Answer: O(log(1/δ)/ε²))

- Concept: Low-rank matrix approximation and associativity of matrix multiplication
  - Why needed here: FLT decomposes RPE-mask into N1N2⊤ using spectral representation, then applies associativity for efficient computation
  - Quick check question: If A ≈ UV⊤ is rank-r approximation, what is computational complexity of computing Ax for vector x? (Answer: O(rd) instead of O(d²))

## Architecture Onboarding

- Component map: Input tokens -> Query/Key/Value projections -> Attention computation module -> Output
- Critical path: 1) Token embeddings projected to Q, K, V 2) RPE random features computed using learned spectral parameters 3) Low-rank RPE-mask approximation via ϕ(x)ψ(y)⊤ 4) Combined attention matrix computation (RPE + content-based) 5) Output values weighted by attention
- Design tradeoffs: Number of random features r vs. approximation accuracy; number of Fourier modes T vs. expressiveness; fixed vs. learnable spectral distribution p(ξ)
- Failure signatures: Training instability when T is too large; poor performance on tasks requiring sharp RPE boundaries; memory issues if r is set too high
- First 3 experiments: 1) Implement FLT with single Gaussian RPE mode (T=1) and compare perplexity on WikiText-103 against regular Performer 2) Test local RPE variant with learnable radius on synthetic task requiring locality bias 3) Apply shift-invariant kernel RPE to molecular property prediction and compare against standard Performer

## Open Questions the Paper Calls Out

### Open Question 1
How does the learned Fourier Transform in FLTs compare to hand-crafted RPE functions in terms of expressivity and efficiency? The paper suggests the learned approach is more general and flexible than hand-crafted RPE functions, but doesn't provide direct comparison. Evidence showing FLTs outperform other methods doesn't isolate the impact of learned Fourier Transform. A controlled experiment comparing FLTs with hand-crafted RPE functions of similar complexity on various tasks, measuring both performance and computational efficiency, would resolve this question.

### Open Question 2
Can the FourierLearner-Transformer architecture be extended to handle non-Euclidean data structures, such as graphs or manifolds? The paper focuses on RPE mechanisms for Euclidean spaces (sequences, point clouds, 3D molecular data) without discussing non-Euclidean structures. The applicability to non-Euclidean data structures remains unexplored. Successful application of FLTs to graph-based tasks or tasks involving data on manifolds would demonstrate whether the Fourier-based RPE mechanism can be adapted to non-Euclidean geometries.

### Open Question 3
What is the impact of the choice of probability distribution P on the performance of FLTs? The paper mentions P needs to be chosen for efficient sampling and density computation, but doesn't explore how different choices affect performance. The impact of different P choices on FLT performance remains unstudied. A systematic study comparing FLT performance with different P choices (e.g., Gaussian, truncated Gaussian, uniform) on various tasks would identify optimal P selection for different data modalities.

## Limitations
- Limited ablation studies showing which components drive performance improvements
- Performance gains vary significantly between tasks without clear explanation of factors
- Claims about supporting arbitrary shift-invariant kernels and advantages for 3D molecular data require more extensive validation

## Confidence

**High Confidence**: The core mathematical framework connecting Fourier transforms to RPE learning is sound. Random feature maps for kernel approximation follow established theory, and decomposition enabling linear attention complexity is mathematically rigorous.

**Medium Confidence**: Empirical results demonstrate consistent improvements across multiple tasks, suggesting broad effectiveness. However, magnitude of improvements varies significantly between tasks, and factors influencing this variation are not fully explained.

**Low Confidence**: Claims about supporting arbitrary shift-invariant kernels and specific advantages for 3D molecular data require more extensive validation. Practical limitations of spectral approximation, particularly for high-frequency RPE functions, are not thoroughly characterized.

## Next Checks

1. **Ablation Study on Fourier Mode Complexity**: Systematically vary number of Fourier modes (T) and random features (r) across different tasks to determine minimum effective configuration and identify point of diminishing returns. This will clarify practical computational trade-offs and establish guidelines for hyperparameter selection.

2. **RPE Function Approximation Analysis**: Evaluate quality of Fourier approximations for different RPE functions (local, global, sharp boundaries) on synthetic datasets where ground truth RPE functions are known. This will identify which RPE patterns are well-suited to spectral learning approach and which may require alternative representations.

3. **Memory and Runtime Benchmarking**: Conduct comprehensive benchmarking comparing FLT memory usage and runtime against explicit RPE-mask computation across varying sequence lengths, dimensionalities, and hardware configurations. This will validate claimed memory efficiency advantages and identify practical scaling limits.