---
ver: rpa2
title: 'Shedding the Bits: Pushing the Boundaries of Quantization with Minifloats
  on FPGAs'
arxiv_id: '2311.12359'
source_url: https://arxiv.org/abs/2311.12359
tags:
- quantization
- integer
- minifloat
- bits
- bit-width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of low-precision floating-point representations
  (minifloats) for post-training quantization of neural networks on FPGAs. The authors
  present a novel PTQ framework for minifloats and comprehensively compare integer
  and minifloat quantization schemes across a range of precisions (3-8 bits) for weights
  and activations.
---

# Shedding the Bits: Pushing the Boundaries of Quantization with Minifloats on FPGAs

## Quick Facts
- arXiv ID: 2311.12359
- Source URL: https://arxiv.org/abs/2311.12359
- Reference count: 30
- Primary result: Minifloats outperform integers at 4 bits or higher for vision model accuracy, but integers remain more FPGA-efficient due to simpler hardware.

## Executive Summary
This paper investigates the use of low-precision floating-point (minifloat) representations for post-training quantization of neural networks on FPGAs. The authors propose a PTQ framework for minifloats and compare it against integer quantization across various precisions (3-8 bits) for weights and activations. Their experiments on ResNet-18, MobileNetV2, and ViT-B-32 show that minifloats achieve up to 6% better accuracy than integers at 4 bits or higher, particularly for vision transformers. However, when considering FPGA hardware costs, integer quantization remains more efficient due to its simpler implementation. The study also evaluates various PTQ techniques, finding that gradient-based learned rounding and GPTQ work well for both integers and minifloats.

## Method Summary
The paper presents a post-training quantization framework for minifloats on FPGAs, evaluating both integer and minifloat quantization schemes across 3-8 bit precisions for weights and activations. The method uses Brevitas for quantization-aware training with per-channel scaling, weight equalization, bias correction, SmoothQuant (Î±=0.5), and gradient-based learned rounding or GPTQ for weight quantization. Experiments are conducted on ResNet-18, MobileNetV2, and ViT-B-32 using ImageNet-1K, with calibration on 1000 images. The study measures top-1 accuracy and FPGA LUT utilization for parameterized integer and minifloat MAC units, comparing Pareto frontiers of accuracy vs. hardware cost.

## Key Results
- Minifloats outperform integers at 4 bits or higher, achieving up to 6% better accuracy for vision transformers.
- Gradient-based learned rounding and GPTQ are effective PTQ techniques for both integer and minifloat quantization.
- Despite accuracy advantages, integer quantization remains more FPGA-efficient due to simpler hardware implementation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minifloat quantization outperforms integer quantization for bit-widths of 4 bits or higher because it offers superior dynamic range and precision control.
- Mechanism: By adjusting the number of exponent and mantissa bits, minifloats can maintain a wider range of representable values and better precision near zero, reducing quantization error for activations with outliers.
- Core assumption: The model's activations and weights benefit from the floating-point's ability to represent both very small and very large values without saturation.
- Evidence anchors:
  - [abstract]: "minifloats offer a promising alternative for emerging workloads such as vision transformers."
  - [section 3.2]: "Minifloats with reduced bit-widths can exhibit better model performance when compared to their integer counterparts, primarily due to their superior dynamic range."
  - [corpus]: Weak evidence - related works focus on LLM quantization but do not directly compare dynamic range benefits.
- Break condition: If the model's activation distribution lacks significant outliers, the floating-point advantage diminishes, and integer quantization becomes equally effective.

### Mechanism 2
- Claim: Gradient-based learned rounding improves quantization accuracy for both integer and minifloat representations by optimizing the rounding decision per element.
- Mechanism: Learned rounding introduces a learnable parameter that adjusts the rounding behavior to minimize the squared error between quantized and original weights, rather than using a fixed round-to-nearest rule.
- Core assumption: The optimal rounding strategy varies per weight element and can be learned from a small calibration dataset without full retraining.
- Evidence anchors:
  - [section 4.5]: "Nagel et al. 2020 analytically compare the performance of the vanilla round-to-nearest operator with 100 different stochastic rounding operators... They notice up to a 10% improvement in the accuracy of the quantized model."
  - [section 6.3]: "learned rounding outperforms GPTQ clearly when subjected to the same hardware constraints across all three models."
  - [corpus]: Weak evidence - corpus papers focus on LLM quantization but do not specifically validate learned rounding in this context.
- Break condition: If the calibration dataset is too small or unrepresentative, the learned rounding may overfit and fail to generalize.

### Mechanism 3
- Claim: FPGA hardware cost for minifloat MACs increases exponentially with exponent width, making narrow-exponent formats more resource-efficient.
- Mechanism: The accumulator width for floating-point MACs grows as 2^ea + ma + 2^eb + mb + log2(n), so increasing exponent bits drastically increases LUT usage.
- Core assumption: The FPGA fabric's LUTs and routing resources are the limiting factor, and wider accumulators require more logic and routing complexity.
- Evidence anchors:
  - [section 5]: "The accumulator width for the floating-point MAC grows exponentially with the exponent widths of the operand formats..."
  - [section 6.2]: "minifloat MAC units require more intricate operations, such as shift operations and comparators."
  - [corpus]: No direct evidence in corpus about FPGA MAC resource scaling with exponent width.
- Break condition: If the FPGA fabric supports hard floating-point DSP blocks, the LUT-based MAC resource scaling may not apply.

## Foundational Learning

- Concept: IEEE 754 floating-point representation and subnormal numbers
  - Why needed here: Understanding the minifloat format's structure (sign, exponent, mantissa) is essential to grasp how it balances range and precision.
  - Quick check question: What is the role of the exponent bias in determining the representable value range?

- Concept: Post-training quantization techniques (cross-layer equalization, bias correction, SmoothQuant)
  - Why needed here: These methods mitigate quantization error without retraining, which is crucial for deploying models on resource-constrained devices.
  - Quick check question: How does SmoothQuant transfer quantization difficulty from activations to weights?

- Concept: FPGA MAC design and resource utilization metrics (LUTs, accumulator width)
  - Why needed here: Hardware cost modeling determines whether the accuracy gains of minifloats justify their higher resource usage.
  - Quick check question: Why does the floating-point MAC accumulator width grow exponentially with exponent bits?

## Architecture Onboarding

- Component map: Calibration module -> Quantization engine -> FPGA MAC library -> Pareto analysis tool

- Critical path:
  1. Load model and calibration data.
  2. Apply cross-layer equalization if enabled.
  3. Perform activation calibration to determine scaling factors.
  4. Quantize weights using learned rounding or GPTQ.
  5. Apply bias correction.
  6. Evaluate accuracy on validation set.
  7. Synthesize MACs and compute LUT usage.

- Design tradeoffs:
  - Per-tensor vs. per-channel scaling: Per-channel gives better accuracy but increases memory for scaling factors.
  - Exponent vs. mantissa bits: More exponent bits increase range but also LUT cost; more mantissa bits improve precision but reduce range.
  - Learned rounding vs. GPTQ: Learned rounding is more accurate but requires gradient computation; GPTQ is one-shot but may be less precise.

- Failure signatures:
  - Accuracy collapse: Likely due to poor scaling factor choice or inadequate calibration data.
  - LUT explosion: Exponent bits too wide for target FPGA fabric.
  - Calibration failure: Learned rounding overfits to calibration set.

- First 3 experiments:
  1. Run ResNet-18 with 4-bit weights/activations using integer quantization only; record accuracy and LUT usage.
  2. Repeat with minifloat quantization using (2,2) exponent/mantissa bits; compare accuracy and LUT usage.
  3. Enable learned rounding and compare accuracy gains for both integer and minifloat configurations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the results change if the study included models from other modalities (e.g., natural language processing, speech recognition) or topologies (e.g., transformers with different architectures)?
- Basis in paper: [explicit] The paper states that the experiments primarily focus on a small set of vision models, which possibly limits how well it generalizes to other modalities and topologies, such as LLMs.
- Why unresolved: The study's findings are limited to vision models, and the authors acknowledge that the results may not generalize to other model types or architectures.
- What evidence would resolve it: Conducting experiments on a diverse set of models from different modalities and architectures would provide insights into the generalizability of the findings.

### Open Question 2
- Question: How would the results be affected if the study allowed for a more diverse design space of configurations, rather than assuming a uniform choice of data formats for all weights and activations in a given model?
- Basis in paper: [explicit] The paper mentions that the experiments assume a uniform choice of data formats for all weights and activations in a given model, which limits the design space of configurations that can be explored.
- Why unresolved: The study's findings are based on a limited set of configurations, and exploring a more diverse design space could potentially yield different results.
- What evidence would resolve it: Experimenting with various combinations of data formats for weights and activations across different layers would provide insights into the impact of configuration diversity on the results.

### Open Question 3
- Question: How would the hardware resource utilization Pareto frontier change if the study accounted for memory savings obtained when higher accuracy is achieved at a given bit-width?
- Basis in paper: [explicit] The paper states that the hardware resource utilization Pareto frontier focuses on compute costs without accounting for memory savings obtained when higher accuracy is achieved at a given bit-width.
- Why unresolved: The study's analysis is based solely on compute costs, and considering memory savings could potentially alter the conclusions regarding the trade-offs between accuracy and hardware resource utilization.
- What evidence would resolve it: Incorporating memory savings into the analysis and evaluating the impact on the Pareto frontier would provide a more comprehensive understanding of the trade-offs involved.

## Limitations
- The study's findings are limited to vision models and may not generalize to other model types or architectures.
- The hardware cost model assumes a specific MAC implementation that may not reflect actual resource usage on different FPGA architectures.
- The learned rounding mechanism lacks comprehensive validation across different model families and calibration dataset sizes.

## Confidence

- **High Confidence**: The fundamental claim that minifloats can achieve higher accuracy than integers at the same bit-width due to better dynamic range and precision control. This is well-supported by the mathematical structure of floating-point representation and is consistent with known quantization theory.

- **Medium Confidence**: The assertion that learned rounding outperforms GPTQ and improves quantization accuracy for both integer and minifloat representations. While the paper provides experimental evidence, the mechanism's generalizability across different model architectures and calibration dataset sizes requires further validation.

- **Low Confidence**: The hardware cost model that claims integer quantization is more efficient on FPGAs despite lower accuracy. The LUT utilization estimates are based on a specific MAC implementation and may not reflect the actual resource usage on different FPGA architectures or with alternative implementation strategies.

## Next Checks

1. **Calibration Dataset Sensitivity**: Validate learned rounding performance across different calibration dataset sizes (e.g., 100, 500, 1000 images) to assess overfitting risk and generalization capability.

2. **MAC Implementation Variability**: Compare LUT utilization estimates against actual synthesis results on multiple FPGA architectures (e.g., Xilinx, Intel) to verify the hardware cost model's accuracy.

3. **Architecture-Agnostic Generalization**: Test the quantization framework on additional model architectures (e.g., ConvNeXt, Swin Transformer) to confirm whether the accuracy gains and hardware tradeoffs hold beyond the initial vision model set.