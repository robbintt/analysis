---
ver: rpa2
title: Fast Training of Diffusion Models with Masked Transformers
arxiv_id: '2306.09305'
source_url: https://arxiv.org/abs/2306.09305
tags:
- training
- diffusion
- masked
- maskdit
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to train large diffusion models
  more efficiently using masked transformers. The core idea is to randomly mask out
  50% of image patches during training, significantly reducing computational cost
  per iteration.
---

# Fast Training of Diffusion Models with Masked Transformers

## Quick Facts
- arXiv ID: 2306.09305
- Source URL: https://arxiv.org/abs/2306.09305
- Reference count: 40
- Key outcome: Achieves competitive FID scores with DiT models while using ~30% of training time

## Executive Summary
This paper introduces MaskDiT, a method to efficiently train large diffusion transformers by randomly masking 50% of image patches during training. The approach uses an asymmetric encoder-decoder architecture where the encoder processes only unmasked patches while a lightweight decoder handles the full set including learnable mask tokens. By combining denoising score matching on unmasked patches with an auxiliary MAE reconstruction loss, the method achieves state-of-the-art generative performance while significantly reducing computational cost.

## Method Summary
MaskDiT employs an asymmetric encoder-decoder architecture where the encoder processes only unmasked patches and a lightweight decoder handles the full image including learnable mask tokens. During training, 50% of patches are randomly masked and the model learns through a combined objective: denoising score matching on unmasked patches plus MAE reconstruction loss on masked patches. After training, an unmasking tuning phase with zero-ratio or cosine-ratio schedules improves generation quality. The approach is validated on ImageNet-256×256 and ImageNet-512×512, achieving competitive performance with baseline DiT models while using only ~30% of the original training time.

## Key Results
- Achieves competitive FID scores with DiT-XL/2 models on ImageNet-256×256 and 512×512
- Reduces training time by approximately 70% compared to baseline DiT models
- Demonstrates that 50% masking ratio provides optimal balance between efficiency and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking 50% of image patches during training significantly reduces computational cost without harming generative quality.
- Mechanism: By removing half the patches, the encoder only processes unmasked tokens, cutting FLOPs per iteration roughly in half. The decoder then processes full tokens (including learnable mask tokens), but its lightweight design keeps total cost low.
- Core assumption: Images contain sufficient redundancy in pixel space, so a diffusion model can learn accurate score functions from partial observations.
- Evidence anchors:
  - [abstract] "Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training."
  - [section 2.2] "Driven by this hypothesis, we propose an efficient approach to train large transformer-based diffusion models, termed Masked Diffusion Transformer (MaskDiT)."
  - [corpus] Weak support: neighbor papers focus on masked generative transformers but do not directly validate the redundancy assumption for diffusion models.

### Mechanism 2
- Claim: The asymmetric encoder-decoder architecture enables masked training by allowing the encoder to operate only on unmasked patches while the decoder reconstructs the full image.
- Mechanism: Encoder processes only visible patches; learnable mask tokens are added before feeding to the decoder, which performs both score estimation on unmasked tokens and MAE reconstruction on masked tokens.
- Core assumption: A lightweight decoder can effectively integrate masked and unmasked information to maintain generation fidelity.
- Evidence anchors:
  - [section 2.2] "we apply an asymmetric encoder-decoder architecture: 1) the encoder has the same architecture as the original DiT except without the final linear projection layer, and it only operates on the unmasked patches; 2) the decoder is another DiT architecture adapted from the lightweight MAE decoder..."
  - [abstract] "For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches."

### Mechanism 3
- Claim: Adding an MAE reconstruction loss on masked patches to the denoising score matching loss improves generation quality by enforcing global image understanding.
- Mechanism: The MAE loss forces the model to predict pixel values for masked patches, encouraging it to reason about relationships across the entire image rather than overfitting to local unmasked regions.
- Core assumption: Global coherence from MAE reconstruction complements local score estimation, preventing overfitting to visible patches.
- Evidence anchors:
  - [section 2.2] "To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches."
  - [abstract] "To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches."

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: The paper trains a diffusion model to estimate gradients of log-density (scores) via denoising score matching (DSM).
  - Quick check question: In the EDM formulation, what is the relationship between the denoising function D_θ(xxx, t) and the estimated score?

- Concept: Vision transformers and patch-based processing
  - Why needed here: The backbone is a transformer operating on image patches, enabling straightforward masking of patches.
  - Quick check question: How does patchifying an image of size H×W with patch size p×p determine the number of patches N?

- Concept: Asymmetric encoder-decoder architectures
  - Why needed here: The encoder processes only unmasked patches, while the decoder reconstructs the full image, reducing computation.
  - Quick check question: What is the role of learnable mask tokens in the decoder input?

## Architecture Onboarding

- Component map: Input → Masking → Encoder (unmasked) → Decoder (full) → Loss computation → Gradient update
- Critical path: Input → Masking → Encoder (unmasked) → Decoder (full) → Loss computation → Gradient update
- Design tradeoffs:
  - Masking ratio vs. performance: Higher masking reduces cost but may hurt quality (optimal ~50%)
  - Decoder size vs. reconstruction quality: Larger decoder improves masked patch reconstruction but increases cost
  - λ (MAE coefficient) vs. training stability: Too large shifts focus away from score estimation
- Failure signatures:
  - FID degradation with high masking ratio (>75%)
  - Training instability when λ is too large
  - Poor classifier-free guidance performance without unmasking tuning
- First 3 experiments:
  1. Train with 50% masking ratio, no MAE loss (λ=0), compare FID vs baseline DiT
  2. Train with 50% masking ratio, MAE loss (λ=0.1), compare FID vs baseline DiT
  3. Train with 75% masking ratio, MAE loss (λ=0.1), compare FID and compute cost vs 50% ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal masking ratio for different model sizes and datasets?
- Basis in paper: [explicit] The paper mentions that a masking ratio of 50% was used, but notes that "there may exist an optimal spot where we achieve both good performance and high training efficiency" and that "the failure of masking 75% suggests that we cannot arbitrarily increase the masking ratio for higher efficiency."
- Why unresolved: The paper only experiments with masking ratios of 0%, 50%, and 75% on one model architecture (DiT-XL/2) and one dataset (ImageNet-256x256). The optimal ratio may vary depending on the specific model architecture, dataset, and task.
- What evidence would resolve it: A systematic study testing different masking ratios (e.g., 25%, 50%, 75%, 90%) across various model sizes, architectures, and datasets would help identify the optimal masking ratio for each scenario.

### Open Question 2
- Question: How does the asymmetric encoder-decoder architecture compare to other architectural modifications for improving training efficiency?
- Basis in paper: [explicit] The paper introduces an asymmetric encoder-decoder architecture as a key design choice, but notes that "it is directly tied to particular applications" and "our algorithmic innovation is orthogonal to the improved infrastructure and implementation."
- Why unresolved: The paper only compares the asymmetric encoder-decoder architecture to the original DiT architecture, and does not explore other potential architectural modifications (e.g., different attention mechanisms, normalization techniques, or network depth/width).
- What evidence would resolve it: A comprehensive study comparing the asymmetric encoder-decoder architecture to other architectural modifications (e.g., different attention mechanisms, normalization techniques, or network depth/width) would help identify the most effective approach for improving training efficiency.

### Open Question 3
- Question: How does the unmasked tuning strategy affect the final performance of the model?
- Basis in paper: [explicit] The paper mentions that "we still need a few steps of unmasking tuning to match the state-of-the-art FID with guidance" and explores two unmasking tuning strategies (zero-ratio and cosine-ratio schedules).
- Why unresolved: The paper only experiments with two unmasking tuning strategies and does not explore the impact of other factors (e.g., the number of tuning steps, the learning rate schedule, or the batch size) on the final performance of the model.
- What evidence would resolve it: A systematic study exploring different unmasking tuning strategies (e.g., varying the number of tuning steps, the learning rate schedule, or the batch size) would help identify the most effective approach for improving the final performance of the model.

## Limitations

- Experimental coverage limited to ImageNet-256×256 and ImageNet-512×512 datasets
- No ablation studies isolating contributions of individual architectural components
- Limited exploration of masking ratios beyond 50% and 75%

## Confidence

- High Confidence: The core mechanism of masking patches during training to reduce computational cost is well-supported by experimental results
- Medium Confidence: The effectiveness of the asymmetric encoder-decoder architecture and combined loss function is plausible but not fully validated through detailed ablations
- Low Confidence: The claim of being first to explore masked training for diffusion transformers is questionable given prior MAE work; generalizability to other datasets is uncertain

## Next Checks

1. Conduct detailed ablation study to isolate contributions of asymmetric encoder-decoder architecture and combined loss function by comparing with symmetric architectures and without MAE loss

2. Validate approach on more challenging datasets (COCO, LSUN) and in conditional generation settings (text-to-image) to assess generalizability

3. Provide detailed FLOPs and wall-clock time breakdown for each configuration to independently verify claimed efficiency gains, including comparisons with other efficient diffusion training methods