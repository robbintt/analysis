---
ver: rpa2
title: 'Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample
  selection'
arxiv_id: '2311.04588'
source_url: https://arxiv.org/abs/2311.04588
tags:
- learning
- ensemble
- thief
- samples
- victim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Army of Thieves (AOT), an ensemble-based
  approach to black-box model extraction attacks. The method leverages multiple diverse
  thief models trained on consensus entropy or label disagreement to select samples
  for querying a victim model.
---

# Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection

## Quick Facts
- arXiv ID: 2311.04588
- Source URL: https://arxiv.org/abs/2311.04588
- Reference count: 29
- Primary result: Army of Thieves achieves 3-21% higher accuracy and adversarial transferability compared to state-of-the-art model extraction techniques

## Executive Summary
Army of Thieves (AOT) introduces an ensemble-based approach to black-box model extraction attacks that leverages multiple diverse thief models to improve query efficiency and accuracy. The method uses consensus entropy or label disagreement to select informative samples for querying the victim model, followed by semi-supervised learning with pseudo-labels after exhausting the query budget. This approach outperforms existing techniques on multiple datasets including CIFAR-10, CIFAR-100, Caltech-256, and CUBS-200, achieving significantly higher accuracy and better adversarial transferability.

## Method Summary
Army of Thieves employs an ensemble of five diverse thief models (AlexNet, ResNet-34, DenseNet121, MobileNetV3_Large, EfficientNet_B2) trained on the ILSVRC-2012 challenge dataset. The attack proceeds through active learning cycles where the ensemble selects samples based on consensus entropy or label disagreement metrics, querying the black-box victim model for labels. After exhausting the query budget, semi-supervised learning is applied using pseudo-labels from the ensemble. The method specifically targets improving query efficiency while maintaining high extraction quality, with experiments conducted on CIFAR-10, CIFAR-100, Caltech-256, and CUBS-200 datasets.

## Key Results
- AOT achieves 3-21% higher accuracy compared to state-of-the-art model extraction techniques
- Consensus entropy sample selection outperforms random sampling across all tested datasets
- Semi-supervised learning with ensemble pseudo-labels improves thief model performance after query budget exhaustion
- AOT demonstrates superior adversarial sample transferability compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using an ensemble of diverse thief models improves query efficiency and accuracy in black-box model extraction by leveraging collective intelligence to select more informative samples.
- Mechanism: The ensemble approach reduces noise and uncertainty in sample selection by aggregating predictions from multiple models with varying architectures and capacities. This collective decision-making process helps identify samples that are genuinely challenging for the victim model rather than being misleading due to individual model biases.
- Core assumption: Diversity in model architectures leads to complementary strengths in identifying informative samples, and the ensemble's consensus provides a more reliable uncertainty metric than any single model.
- Evidence anchors:
  - [abstract]: "By combining the knowledge and predictions of multiple thief models, we can alleviate individual model limitations, reduce noise and uncertainty, and ultimately increase the extraction success rate."
  - [section]: "Ensemble techniques leverage the collective intelligence of multiple models to improve overall performance. By combining the knowledge and predictions of multiple thief models, we can alleviate individual model limitations, reduce noise and uncertainty, and ultimately increase the extraction success rate."
- Break condition: If ensemble members are too similar in architecture or training data, the diversity benefit disappears and the ensemble performs no better than the best individual model.

### Mechanism 2
- Claim: The consensus entropy and label disagreement metrics used for sample selection effectively identify samples that are both difficult for the victim model and informative for training thief models.
- Mechanism: Consensus entropy measures the uncertainty across the ensemble's probability distributions, while label disagreement counts how many models disagree on the predicted label. High values in either metric indicate samples that are likely near decision boundaries or otherwise challenging, making them valuable for training.
- Core assumption: Samples that cause disagreement or high entropy among diverse models are more likely to be informative for improving model performance than samples where all models agree.
- Evidence anchors:
  - [abstract]: "Based on the ensemble's collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data."
  - [section]: "Sample selection is based on the disagreement observed among ensemble members, as it serves as a metric for quantifying sample complexity. Specifically, samples with high disagreement are considered more valuable for training since they pose greater classification challenges."
- Break condition: If the victim model has very smooth decision boundaries or the attacker has access to high-quality labeled data, the benefit of selecting challenging samples diminishes.

### Mechanism 3
- Claim: Semi-supervised learning with pseudo-labels from the ensemble after query budget exhaustion improves generalization without additional queries to the victim model.
- Mechanism: The ensemble's predictions on unlabeled data are more reliable than any single model's predictions, allowing for effective use of consistency regularization and confidence-based filtering to improve thief model performance.
- Core assumption: The ensemble's collective predictions have lower noise and higher confidence than individual models, making them suitable for generating pseudo-labels that won't degrade training quality.
- Evidence anchors:
  - [abstract]: "The higher confidence leads to better semi-supervised learning once the query budget is depleted."
  - [section]: "Moreover, the output labels of an ensemble are less noisy and have higher confidence than a single model. The higher confidence leads to better semi-supervised learning once the query budget is depleted."
- Break condition: If the ensemble's predictions are systematically biased or if the unlabeled data distribution differs significantly from the labeled data, pseudo-label quality degrades and semi-supervised learning becomes counterproductive.

## Foundational Learning

- Concept: Active Learning principles
  - Why needed here: The paper relies on active learning strategies (consensus entropy and label disagreement) to select the most informative samples for querying the victim model, which is central to query efficiency.
  - Quick check question: How does the consensus entropy method differ from traditional uncertainty sampling in active learning, and why is it more suitable for ensemble-based model extraction?

- Concept: Ensemble methods and diversity
  - Why needed here: The entire approach is built on using diverse ensemble members to improve sample selection and model performance, requiring understanding of how ensemble diversity contributes to better predictions.
  - Quick check question: What are the three main sources of diversity in ensemble methods, and which ones are leveraged in this Army of Thieves approach?

- Concept: Semi-supervised learning with consistency regularization
  - Why needed here: After exhausting the query budget, the paper applies semi-supervised learning using pseudo-labels from the ensemble, requiring understanding of how consistency regularization works in SSL.
  - Quick check question: How does the weak augmentation + strong augmentation paradigm in the semi-supervised learning stage relate to the FixMatch algorithm, and what's the purpose of the confidence threshold?

## Architecture Onboarding

- Component map: Ensemble of five pretrained thief models (AlexNet, ResNet-34, DenseNet121, MobileNetV3_Large, EfficientNet_B2) -> Subset selector module (consensus entropy or label disagreement) -> Semi-supervised learning pipeline with augmentation and pseudo-label filtering -> Interfaces to query black-box victim model
- Critical path: 1) Initialize with random query set Q0, 2) Query victim model, 3) Train ensemble on labeled data, 4) Use subset selector to choose next query set based on consensus entropy or label disagreement, 5) Repeat until query budget exhausted, 6) Apply semi-supervised learning on remaining unlabeled data
- Design tradeoffs: Larger ensembles could improve performance but increase computational cost and complexity; simpler sample selection methods (like random sampling) are cheaper but less effective; using more diverse architectures improves sample selection but may complicate model management
- Failure signatures: 1) AlexNet shows sudden spikes in accuracy for CIFAR-10 but is stable for other datasets (indicating architecture-specific learning patterns), 2) Accuracy for some models drops as more samples are selected (contradicting typical active learning expectations), 3) Semi-supervised learning helps CIFAR-10 but fails for Caltech-256 due to class imbalance issues
- First 3 experiments:
  1. Compare consensus entropy vs label disagreement selection methods on CIFAR-10 with the full ensemble to identify which works better for different datasets
  2. Test the semi-supervised learning stage with and without the confidence threshold filtering to measure its impact on model generalization
  3. Evaluate adversarial sample transferability for each individual ensemble member to understand which architectures capture the victim model's decision boundaries best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of ensemble members affect model extraction performance when attacking different victim architectures?
- Basis in paper: [explicit] The authors note they chose models based on size, complexity, and learning capacity, but were limited by computational resources and couldn't test larger models like ViT or InceptionNet.
- Why unresolved: The paper only tests five specific architectures and acknowledges the potential impact of model selection remains unexplored.
- What evidence would resolve it: Systematic experiments testing various combinations of ensemble members (including larger architectures) across different victim model types would reveal optimal ensemble compositions for different extraction scenarios.

### Open Question 2
- Question: What is the optimal balance between active learning cycles and semi-supervised learning for different dataset characteristics?
- Basis in paper: [explicit] The authors note that semi-supervised learning helped CIFAR-10 but failed for Caltech-256 due to class imbalance issues, suggesting the approach may need adaptation based on dataset properties.
- Why unresolved: The paper uses fixed parameters (10 cycles, 30K query budget) without exploring how these should vary based on dataset characteristics like number of classes or class distribution.
- What evidence would resolve it: Experiments varying the number of active learning cycles, query budget, and semi-supervised learning parameters across datasets with different characteristics would reveal optimal parameter settings for each scenario.

### Open Question 3
- Question: How does the ensemble approach perform in non-image domains like text or speech?
- Basis in paper: [inferred] The authors state their ensemble approach is "generic enough to be applied to any domain like text, speech etc." but only test on image classification tasks.
- Why unresolved: The paper focuses exclusively on image classification, leaving open questions about whether the consensus entropy and label disagreement selection strategies transfer effectively to other domains.
- What evidence would resolve it: Applying the AOT method to text classification (using models like BERT variants) or speech recognition tasks with appropriate active learning and semi-supervised techniques would demonstrate domain generalizability.

## Limitations
- Computational overhead of training and maintaining five diverse models simultaneously
- Variable effectiveness of semi-supervised learning across datasets with different characteristics
- Limited exploration of optimal ensemble composition for different victim model architectures

## Confidence
- Ensemble diversity benefits: Medium
- Consensus entropy effectiveness: Medium
- Semi-supervised learning generalization: Low (dataset-dependent)

## Next Checks
1. Quantify the contribution of each architecture diversity dimension (depth, width, initialization) to ensemble performance
2. Test the method's robustness when victim model decision boundaries are smooth vs. complex
3. Evaluate whether simpler ensemble configurations (3 models instead of 5) maintain performance while reducing computational cost