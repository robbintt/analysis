---
ver: rpa2
title: ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots
  at scientific writing?
arxiv_id: '2309.08636'
source_url: https://arxiv.org/abs/2309.08636
tags:
- scientific
- chatbots
- content
- chatgpt
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested six AI chatbots for scientific writing in humanities
  and archaeology, focusing on factual correctness and original scientific contribution.
  ChatGPT-4 showed highest quantitative accuracy (11% original contribution), followed
  by ChatGPT-3.5 and Bing.
---

# ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing?

## Quick Facts
- arXiv ID: 2309.08636
- Source URL: https://arxiv.org/abs/2309.08636
- Reference count: 0
- This study found ChatGPT-4 achieved highest quantitative accuracy (11% original contribution) among six AI chatbots tested for scientific writing in humanities and archaeology.

## Executive Summary
This study evaluated six AI chatbots (ChatGPT-4, ChatGPT-3.5, Bing Chatbot, Bard, Aria, Claude 2) for scientific writing in humanities and archaeology, focusing on factual correctness and original scientific contribution. ChatGPT-4 demonstrated the highest quantitative accuracy at 64%, followed by ChatGPT-3.5 and Bing Chatbot, while Claude 2 and Aria scored notably lower. All AI chatbots excelled at recombining existing knowledge but failed to generate original scientific content, highlighting the complex nature of human research that involves data acquisition, analysis, and synthesis beyond text recombination.

## Method Summary
The study used two one-shot prompts about South Slavic migration and Alpine Slavic settlement, comparing AI-generated responses against human expert-generated content. Six AI chatbots were tested using identical prompts, with their outputs tagged for quantitative accuracy (correct, inadequate, unverifiable, with errors, incorrect) and qualitative precision (original scientific contribution, derivative scientific contribution, generic content, incorrect). Accuracy scores were calculated using the formula: Correct% - (2 × Incorrect%).

## Key Results
- ChatGPT-4 achieved the highest quantitative accuracy (64%) among tested AI chatbots
- All chatbots failed to generate original scientific contributions, only recombining existing knowledge
- Systematic biases were observed in reference generation, with 92% of references in English despite more relevant content in Balkan languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI chatbots show linear improvement in quantitative accuracy with model size, but qualitative precision plateaus due to lack of original scientific contribution.
- Mechanism: Larger models with more parameters have better contextual knowledge and pattern recognition, leading to higher correct content percentages. However, qualitative precision depends on synthesizing new knowledge, which LLMs cannot do since they only recombine existing data.
- Core assumption: Model size directly correlates with performance in factual recall tasks.
- Evidence anchors:
  - [abstract]: "ChatGPT-4 showed highest quantitative accuracy (11% original contribution), followed by ChatGPT-3.5 and Bing."
  - [section]: "The up to 10% improvement in generated content of ChatGPT-4 compared to ChatGPT-3.5 is not negligible, but for many use cases it might not be noticeable. Given the tenfold increase in the number of parameters..."
- Break condition: If a smaller model demonstrates higher qualitative precision, or if AGI-capable systems emerge that can generate original contributions.

### Mechanism 2
- Claim: AI chatbots exhibit systematic biases (language, neo-colonial, citation) that reflect training data rather than domain-specific knowledge.
- Mechanism: LLMs trained on web data inherit biases from sources like Google Scholar and Wikipedia, which favor English publications, Western authors, and highly-cited older papers. These biases manifest in reference generation and content framing.
- Core assumption: Training data distribution determines output bias.
- Evidence anchors:
  - [section]: "Language bias. Although there is far more relevant scholarly content written in Balkan languages than in English, 92% of the references generated by the AI chatbots in our test referred to English..."
  - [section]: "Neo-colonial bias. 88% of the references are by authors from the global West... This reflects a scholarly hierarchy created by colonialism..."
- Break condition: If training data is balanced across languages and regions, or if models are explicitly trained to avoid these biases.

### Mechanism 3
- Claim: AI chatbots cannot perform critical comparison of sources or generate original scientific contributions because they lack ability to analyze data and create knowledge.
- Mechanism: Human researchers obtain data, analyze information, and synthesize knowledge through recursive loops. LLMs skip data acquisition and analysis steps, only recombining existing knowledge, which prevents original contribution generation.
- Core assumption: Original scientific contribution requires data analysis and synthesis beyond text recombination.
- Evidence anchors:
  - [abstract]: "While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content."
  - [section]: "Human researchers approached Q2 by first obtaining and refining the data, which was followed by analysing the information using appropriate software... LLMs, on the other hand, are pre-trained on existing knowledge (texts, books) and only able to recombine it in new permutations."
- Break condition: If models are integrated with data analysis tools and can perform iterative knowledge synthesis.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding how AI chatbots generate content and their limitations in scientific writing.
  - Quick check question: What is the primary mechanism by which LLMs generate text responses?

- Concept: Bias in AI Systems
  - Why needed here: Recognizing how training data distribution affects reference generation and content framing in scientific writing.
  - Quick check question: How do language and neo-colonial biases manifest in AI-generated scientific references?

- Concept: Data-Information-Knowledge-Wisdom (DIKW) Hierarchy
  - Why needed here: Understanding why AI chatbots fail to generate original scientific contributions despite high quantitative accuracy.
  - Quick check question: At which level of the DIKW hierarchy do AI chatbots operate, and why does this limit their ability to generate original knowledge?

## Architecture Onboarding

- Component map: User prompt → Prompt processing → LLM generation → Reference generation → Output formatting → User response
- Critical path: User prompt → Prompt processing → LLM generation → Reference generation → Output formatting → User response
- Design tradeoffs: Larger models provide better accuracy but increase computational costs and latency. Plugin systems offer specialized capabilities but introduce complexity and potential integration issues.
- Failure signatures: Incorrect or fabricated references, inability to generate original content, systematic biases in language and source selection, reasoning errors in causal relationships, and hallucinations.
- First 3 experiments:
  1. Test baseline accuracy: Compare quantitative accuracy scores of different AI chatbots on a standardized humanities question set.
  2. Analyze bias patterns: Examine reference language, author geography, and publication age distributions across multiple AI chatbot responses.
  3. Evaluate qualitative precision: Assess the ability of AI chatbots to generate original scientific contributions versus derivative content on complex interdisciplinary topics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How will the plateauing of LLM parameter growth impact the quality and capabilities of future AI chatbots for scientific writing?
- Basis in paper: [explicit] The authors note that "the growth of LLMs seems to have plateaued" and suggest that "the returns on scaling up the model size will diminish."
- Why unresolved: The paper observes this trend but does not explore its implications for future AI capabilities in scientific writing.
- What evidence would resolve it: Comparative studies of AI chatbot performance over time as LLM parameter growth slows, tracking changes in scientific writing quality.

### Open Question 2
- Question: Can purpose-built LLM-powered software overcome the limitations of current AI chatbots in generating original scientific contributions?
- Basis in paper: [explicit] The authors propose that "appropriate LLM-powered software might be" able to generate original scientific content by combining LLMs with data analysis tools and other resources.
- Why unresolved: This is a forward-looking suggestion that requires empirical testing of new software architectures.
- What evidence would resolve it: Development and testing of LLM-powered software designed for scientific research, measuring its ability to generate original contributions compared to human researchers.

### Open Question 3
- Question: How can academic peer review processes be adapted to effectively evaluate AI-generated scientific content?
- Basis in paper: [inferred] The authors discuss the challenges of AI-generated content in scientific writing and note that "peer review processes need to be rapidly adapted to compensate for this."
- Why unresolved: The paper identifies the need for adaptation but does not propose specific solutions or evaluate potential approaches.
- What evidence would resolve it: Case studies of modified peer review processes applied to AI-generated content, assessing their effectiveness in maintaining scientific rigor.

## Limitations
- Limited generalizability from only two prompts in humanities/archaeology domain
- Subjective human expert judgments for qualitative precision assessment may introduce bias
- Does not account for potential improvements from prompt engineering or iterative refinement

## Confidence
**High Confidence**: Quantitative accuracy comparisons showing ChatGPT-4 > ChatGPT-3.5 > Bing Chatbot > other models, based on clear numerical scoring and consistent ranking across multiple metrics.

**Medium Confidence**: Systematic bias patterns in language, colonial perspectives, and citation practices, supported by specific statistical distributions but limited to the humanities/archaeology domain tested.

**Low Confidence**: The assertion that AI chatbots fundamentally cannot generate original scientific contributions due to their text recombination nature, as this depends on evolving definitions of originality and potential future architectural improvements.

## Next Checks
1. **Replication with expanded prompt set**: Test the same six chatbots on 20-30 diverse prompts across multiple humanities disciplines to validate the observed accuracy rankings and bias patterns hold beyond the initial two prompts.

2. **Temporal stability analysis**: Re-run the same prompts monthly over a 6-month period to assess whether model updates, training data changes, or prompt engineering improvements affect the quantitative accuracy and qualitative precision scores.

3. **Hybrid system evaluation**: Test AI chatbots integrated with domain-specific databases and reasoning tools to determine whether the inability to generate original contributions is an inherent LLM limitation or can be overcome through system augmentation.