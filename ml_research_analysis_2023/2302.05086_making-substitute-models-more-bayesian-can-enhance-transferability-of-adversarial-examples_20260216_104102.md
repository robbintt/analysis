---
ver: rpa2
title: Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial
  Examples
arxiv_id: '2302.05086'
source_url: https://arxiv.org/abs/2302.05086
tags:
- adversarial
- bayesian
- transferability
- substitute
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to improve the transferability of adversarial
  examples by using Bayesian DNNs as substitute models. Instead of relying on input
  diversity, the authors introduce diversity in the model space by treating DNN weights
  as random variables with Gaussian priors.
---

# Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples

## Quick Facts
- arXiv ID: 2302.05086
- Source URL: https://arxiv.org/abs/2302.05086
- Reference count: 17
- Key outcome: Bayesian DNNs as substitute models improve adversarial example transferability by up to 19% on ImageNet

## Executive Summary
This paper proposes improving adversarial example transferability by treating DNN weights as random variables with Gaussian priors, creating diversity in the model space rather than relying on input diversity. The authors derive a principled finetuning strategy to optimize the mean and covariance of the posterior distribution, which can be combined with existing Gaussian posterior approximations like SWAG. Extensive experiments on CIFAR-10 and ImageNet show significant improvements over state-of-the-art transferability attacks.

## Method Summary
The method introduces Bayesian formulation to substitute models by treating weights as random variables with Gaussian priors. During adversarial example generation, the attack maximizes average prediction loss over multiple samples from the posterior distribution. The authors propose a principled finetuning strategy that updates the model using gradients from worst-case parameters within the posterior confidence region. They also show compatibility with SWAG for more flexible posterior approximation, capturing training dynamics through diagonal and low-rank covariance components.

## Key Results
- Achieves up to 19% improvement in average attack success rate on ImageNet compared to state-of-the-art methods
- Method outperforms TIM, SIM, Admix, LinBP, ILA++, TAIG, and LGV transferability attacks
- Compatible with existing techniques, showing further performance boosts when combined
- Demonstrates effectiveness across multiple architectures including ResNet, VGG, DenseNet, and vision transformers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Introducing diversity in substitute model parameters through Bayesian posterior sampling improves adversarial example transferability.
- **Mechanism:** By treating DNN weights as random variables with Gaussian priors and optimizing their mean and covariance, the method creates an ensemble of infinitely many models from a single training run. Adversarial examples are crafted by maximizing average prediction loss over this distribution, effectively attacking multiple model variants simultaneously.
- **Core assumption:** The distribution of victim model parameters overlaps sufficiently with the posterior distribution of the substitute model parameters.
- **Evidence anchors:**
  - [abstract] "by introducing diversity in the model space by treating DNN weights as random variables with Gaussian priors"
  - [section 3.1] "By introducing probability measures to weights and biases of the substitute model, all these parameters are represented under assumptions of some distributions to be learned"
  - [corpus] Weak - corpus papers focus on input diversity or specific attack strategies, not Bayesian model space diversity
- **Break condition:** If the victim model's parameter distribution is highly dissimilar from the substitute model's posterior, transferability gains will diminish or disappear.

### Mechanism 2
- **Claim:** Finetuning the Bayesian substitute model with principled gradient updates based on worst-case parameters from the posterior improves transferability.
- **Mechanism:** The optimization problem reformulates to find worst-case parameters within the posterior confidence region, then updates the model using gradients from these worst-case samples. This improves the quality of substitute models sampled from the posterior.
- **Core assumption:** Worst-case parameters within the posterior confidence region provide useful gradients for improving the substitute model's ability to generate transferable attacks.
- **Evidence anchors:**
  - [section 3.2] "we focus on the worst-case parameters from the posterior, whose loss in fact bounds the objective from below"
  - [section 3.2] "the outer gradient for solving Eq. (7) is∇ ˆwL(x,y, ˆw) + H∆w∗"
  - [corpus] Missing - corpus does not contain similar finetuning strategies using worst-case Bayesian parameters
- **Break condition:** If the worst-case parameter assumption leads to overfitting to the substitute model distribution rather than improving generalization across victim models.

### Mechanism 3
- **Claim:** Combining Bayesian posterior approximation with SWAG (Stochastic Weight Averaging Gaussian) provides a more flexible and data-driven posterior estimate than simple isotropic Gaussian assumptions.
- **Mechanism:** SWAG approximates the posterior using a Gaussian with SWA solution as mean and a covariance matrix decomposed into diagonal and low-rank components. This captures training dynamics and provides a richer posterior distribution.
- **Core assumption:** The SWAG approximation accurately captures the true posterior distribution of model parameters, which aligns with victim model distributions.
- **Evidence anchors:**
  - [section 3.3] "Taking SW AG (Maddox et al., 2019) as an example, which is a simple and scalable one"
  - [section 3.3] "It introduces an improved approximation to the posterior over parameters"
  - [corpus] Weak - corpus contains Bayesian attack papers but doesn't mention SWAG specifically
- **Break condition:** If SWAG's assumptions about posterior structure (Gaussian, specific covariance decomposition) don't match the true posterior or victim model distributions.

## Foundational Learning

- **Concept: Bayesian deep learning and posterior inference**
  - Why needed here: The entire method relies on Bayesian formulation of DNNs where weights are treated as random variables with distributions rather than point estimates
  - Quick check question: What is the difference between maximum likelihood estimation and Bayesian inference in the context of neural network training?

- **Concept: Monte Carlo sampling for approximate integration**
  - Why needed here: Exact Bayesian inference is intractable for DNNs, so Monte Carlo sampling approximates the integral over model parameters when crafting adversarial examples
  - Quick check question: How does Monte Carlo sampling approximate the integral in Bayesian model averaging?

- **Concept: Adversarial attack transferability principles**
  - Why needed here: Understanding why adversarial examples transfer between models is crucial for appreciating why model diversity through Bayesian methods improves transferability
  - Quick check question: What factors typically influence the transferability of adversarial examples between different neural network models?

## Architecture Onboarding

- **Component map:**
  - Bayesian substitute model training (with/without SWAG) -> Finetuning module with principled gradient updates -> Adversarial example generation using I-FGSM or similar -> Posterior sampling during attack iterations -> Ensemble attack aggregation

- **Critical path:**
  1. Train/finetune substitute model with Bayesian formulation
  2. Approximate posterior (isotropic Gaussian or SWAG)
  3. During each attack iteration, sample multiple models from posterior
  4. Craft adversarial examples by maximizing average loss across samples
  5. Evaluate transferability to victim models

- **Design tradeoffs:**
  - Isotropic vs. learned covariance: Simplicity vs. flexibility
  - Number of posterior samples per iteration: Attack strength vs. computational cost
  - Finetuning vs. no finetuning: Performance vs. setup complexity
  - SWAG vs. simpler approximations: Accuracy vs. implementation complexity

- **Failure signatures:**
  - Low transferability despite Bayesian formulation (posterior mismatch)
  - Computational overhead becomes prohibitive (too many samples or complex posterior)
  - Finetuning destabilizes model training (learning rate too high)
  - Attack performance plateaus (insufficient posterior diversity)

- **First 3 experiments:**
  1. Implement basic Bayesian formulation with isotropic Gaussian posterior on CIFAR-10, compare to deterministic baseline
  2. Add SWAG posterior approximation and measure improvement in transferability
  3. Test different numbers of posterior samples per attack iteration to find sweet spot for performance vs. computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Bayesian approach to transferability scale with increasingly complex and diverse model architectures beyond the ones tested?
- Basis in paper: [explicit] The paper suggests that their method is agnostic to the choice of architecture and can be combined with existing techniques, but does not explore a wide range of model types.
- Why unresolved: The experiments are limited to specific architectures like ResNet, VGG, and DenseNet, and the authors acknowledge the need for further exploration.
- What evidence would resolve it: Testing the method on a broader range of architectures, including more recent and complex models, to see if the Bayesian approach consistently enhances transferability.

### Open Question 2
- Question: What are the limitations of the Gaussian posterior approximation in the Bayesian approach, and how might alternative approximations impact transferability?
- Basis in paper: [explicit] The authors discuss using a Gaussian posterior with SWAG and isotropic Gaussian assumptions but suggest that more general Gaussian distributions might align with victim parameters.
- Why unresolved: The paper focuses on Gaussian approximations and does not explore other potential posterior distributions that could be more effective.
- What evidence would resolve it: Comparing the performance of the method using different posterior approximations, such as Laplace approximation or variational inference, to assess their impact on transferability.

### Open Question 3
- Question: How does the Bayesian approach perform against adaptive defenses specifically designed to counter Bayesian attacks?
- Basis in paper: [explicit] The paper tests against robust models and vision transformers but does not explore defenses tailored to Bayesian methods.
- Why unresolved: The focus is on transferability rather than the robustness of the method against specialized defenses.
- What evidence would resolve it: Evaluating the method against defenses that specifically target Bayesian techniques, such as those exploiting the probabilistic nature of Bayesian models, to determine their effectiveness.

## Limitations

- The method requires training Bayesian substitute models, which adds computational overhead compared to standard transferability attacks
- Performance depends on the quality of posterior approximation, with Gaussian assumptions potentially limiting effectiveness for highly non-Gaussian victim models
- The 19% improvement on ImageNet needs independent validation, as it's a single experimental result

## Confidence

- **High confidence** in the core mechanism of using Bayesian model space diversity to improve transferability
- **Medium confidence** in the specific implementation of SWAG-based posterior approximation and its claimed improvements
- **Medium confidence** in the 19% improvement on ImageNet, as this is a single experimental result that needs independent validation

## Next Checks

1. Implement basic Bayesian formulation with isotropic Gaussian posterior on CIFAR-10 and verify it improves transferability over deterministic baseline (expected: 2-5% improvement)

2. Reproduce the SWAG-based posterior approximation with proper low-rank matrix computation and validate against the isotropic case on CIFAR-10 (expected: additional 2-3% improvement)

3. Run ablation study on posterior sample count during attack iterations (test 1, 5, 10, 20 samples) to identify optimal tradeoff between performance and computational cost on CIFAR-10 (expected: diminishing returns beyond 5-10 samples)