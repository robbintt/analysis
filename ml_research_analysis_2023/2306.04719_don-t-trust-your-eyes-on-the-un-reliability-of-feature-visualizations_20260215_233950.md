---
ver: rpa2
title: 'Don''t trust your eyes: on the (un)reliability of feature visualizations'
arxiv_id: '2306.04719'
source_url: https://arxiv.org/abs/2306.04719
tags:
- relu
- conv
- feature
- visualizations
- bottleneck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the reliability of feature visualizations\u2014\
  a common interpretability method for neural networks. The authors develop fooling\
  \ circuits that trick feature visualizations into showing arbitrary patterns while\
  \ preserving original network accuracy, demonstrating that visualizations can be\
  \ manipulated if the model is not trained by the user."
---

# Don't trust your eyes: on the (un)reliability of feature visualizations

## Quick Facts
- arXiv ID: 2306.04719
- Source URL: https://arxiv.org/abs/2306.04719
- Reference count: 40
- This paper shows feature visualizations can be manipulated and don't reliably explain neural network behavior.

## Executive Summary
This paper demonstrates that feature visualizations—a popular interpretability technique—are fundamentally unreliable for understanding neural network behavior. The authors show that both through malicious manipulation (fooling circuits) and naturally occurring phenomena, feature visualizations can be disconnected from actual network processing. Their findings reveal that visualizations can be made to show arbitrary patterns while preserving model accuracy, and even in unmodified networks, visualizations follow different computational paths than natural images. The theoretical analysis proves that without strong assumptions, feature visualization through activation maximization cannot be used to understand black-box functions.

## Method Summary
The authors employ three complementary approaches: (1) developing fooling circuits that trick feature visualizations into showing arbitrary patterns while maintaining network accuracy, (2) empirically analyzing path similarity between natural images and feature visualizations using Spearman correlation across network layers, and (3) providing theoretical impossibility results showing that feature visualization cannot reliably predict function behavior without strong assumptions. They also investigate the relationship between path linearity and interpretability through correlation analysis.

## Key Results
- Fooling circuits can make feature visualizations display arbitrary patterns disconnected from actual network behavior while preserving accuracy
- Feature visualizations are processed along different neural paths than natural images, even in unmodified networks
- Theoretical proofs show that without strong assumptions, feature visualization cannot reliably predict network behavior
- Preliminary evidence suggests more linear units may be easier to interpret

## Why This Works (Mechanism)

### Mechanism 1
Feature visualizations can be arbitrarily manipulated if an adversary embeds a fooling circuit in the network. The fooling circuit uses a classifier to distinguish between natural images and optimization inputs, routing information through different computational paths depending on the input type. This works because natural images and feature visualization optimization inputs have disjoint distributions. The classifier routes natural images through normal processing while optimizing inputs take a different path that produces arbitrary visualizations.

### Mechanism 2
Even unmodified networks process feature visualizations along different neural paths than natural images, casting doubt on visualizations' explanatory power. The mechanism is that feature visualizations activate different units and follow different computational paths compared to natural images, as measured by Spearman rank-order correlation of activations across layers. If visualizations were processed along the same path as natural images, their activations would be highly correlated with natural image activations throughout the network.

### Mechanism 3
Without strong assumptions about the function being visualized, feature visualization cannot reliably predict network behavior. The mechanism is that the arg max of a function does not constrain the function sufficiently to enable reliable prediction of function values at other inputs. Feature visualization corresponds to finding the arg max of the function being visualized, but knowing the arg max provides minimal information about function values at other points.

## Foundational Learning

- Concept: Adversarial examples and model manipulation
  - Why needed here: Understanding the fooling circuit mechanism requires knowledge of how adversarial examples work and how models can be manipulated to behave differently on different inputs.
  - Quick check question: What is the key insight behind the fooling circuit in this paper, and how does it exploit the difference between natural and optimization inputs?

- Concept: Neural network architecture and path analysis
  - Why needed here: Understanding the different processing paths for natural images and feature visualizations requires knowledge of neural network architecture and how activations flow through the network.
  - Quick check question: How do the authors measure the similarity between the processing paths of natural images and feature visualizations, and what does a low similarity indicate?

- Concept: Function approximation and optimization
  - Why needed here: Understanding the theoretical results about the limitations of feature visualization requires knowledge of function approximation, optimization, and the relationship between arg max and function values.
  - Quick check question: Why does the arg max of a function not constrain the function sufficiently to enable reliable prediction of function values at other inputs?

## Architecture Onboarding

- Component map: Fooling circuit (classifier + gate units + deceptive unit) -> Feature visualization optimization -> Path similarity analysis (Spearman correlation) -> Theoretical analysis (impossibility results) -> Correlation analysis between path linearity and human interpretability

- Critical path: 1. Embed fooling circuit or manipulate silent units, 2. Run feature visualization optimization, 3. Compare visualizations to original network behavior, 4. Analyze processing paths using activation similarity, 5. Apply theoretical results to understand limitations

- Design tradeoffs: Fooling circuit vs. silent unit manipulation (fooling circuit is more flexible but requires explicit classifier; silent unit manipulation is simpler but less controllable), Absolute vs. normalized similarity plots (absolute plots show raw values but may be harder to interpret; normalized plots facilitate comparison but may obscure absolute differences)

- Failure signatures: If fooling circuit does not work (classifier cannot distinguish between natural and optimization inputs, or gate units do not properly route information), If path similarity analysis is inconclusive (correlation values are high for both same-class and different-class comparisons, making it difficult to assess processing differences)

- First 3 experiments: 1. Implement fooling circuit and verify that it produces arbitrary visualizations while maintaining original network accuracy, 2. Analyze path similarity between natural images and feature visualizations using Spearman correlation, focusing on later layers where correlation increases, 3. Apply theoretical results to a specific function class (e.g., piecewise affine) and verify that feature visualization cannot reliably predict function values without additional assumptions

## Open Questions the Paper Calls Out

- What specific architectural modifications would make feature visualizations more reliable for understanding neural network behavior? The authors suggest that "a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations."

- Can we develop practical methods to distinguish between benign and deceptive feature visualizations in real-world applications? The authors note that "once it is known that a certain fooling method might be used, it is easy to develop a detection mechanism" but acknowledge this leads to an arms race between attacks and defenses.

- How does the reliability of feature visualizations vary across different network architectures and training procedures? The authors note that "the investigated networks, Inception-V1 and ResNet-50, are of course not exhaustive" and suggest this is an interesting direction for future work.

- What is the relationship between path linearity and human interpretability of feature visualizations, and how can this be leveraged to improve interpretability? The authors find "preliminary evidence in favor of the hypothesis that more linear units, at least at the beginning of the optimization trajectory, might be easier to interpret."

## Limitations

- The theoretical results assume feature visualization corresponds to arg max optimization, which may not capture all visualization methods
- The empirical analysis relies on specific architectures (Inception-V1, ResNet-50) and datasets (ImageNet), potentially limiting generalizability
- The correlation analysis methodology, while standard, may not fully capture the complexity of neural processing paths

## Confidence

- High confidence in the theoretical impossibility results showing that arg max optimization cannot constrain function values
- Medium confidence in the fooling circuit attack demonstrating vulnerability to manipulation
- Medium confidence in the empirical evidence that natural images and feature visualizations follow different processing paths
- Low confidence in the preliminary evidence linking linearity to interpretability, as this requires further validation

## Next Checks

1. Test the fooling circuit attack across multiple architectures and datasets to assess generalizability beyond Inception-V1 and ImageNet
2. Conduct ablation studies on the correlation analysis methodology to determine how sensitive the path similarity results are to different distance metrics and comparison methods
3. Validate the linearity-interpretability hypothesis by systematically varying model linearity and measuring both feature visualization quality and human interpretability across multiple tasks