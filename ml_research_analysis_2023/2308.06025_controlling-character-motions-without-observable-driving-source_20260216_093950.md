---
ver: rpa2
title: Controlling Character Motions without Observable Driving Source
arxiv_id: '2308.06025'
source_url: https://arxiv.org/abs/2308.06025
tags:
- policy
- sequence
- token
- driving
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating natural, diverse,
  and unlimited-length character motion sequences without any external driving source.
  The authors propose a novel framework that combines a VQ-VAE for discrete token
  space discretization with a reinforcement learning-based low-level policy for decoding
  tokens into continuous motion.
---

# Controlling Character Motions without Observable Driving Source

## Quick Facts
- arXiv ID: 2308.06025
- Source URL: https://arxiv.org/abs/2308.06025
- Authors: 
- Reference count: 40
- Key outcome: This paper addresses the challenge of generating natural, diverse, and unlimited-length character motion sequences without any external driving source. The authors propose a novel framework that combines a VQ-VAE for discrete token space discretization with a reinforcement learning-based low-level policy for decoding tokens into continuous motion. A high-level policy generates token sequences to avoid periodic patterns. The method is evaluated on two public body skeleton datasets and a self-collected VTuber face dataset, showing significantly better performance than previous approaches across multiple metrics including Frechet distance in PCA space.

## Executive Summary
This paper tackles the problem of generating diverse and natural character motion sequences without relying on external driving sources. The authors propose a novel framework that combines vector quantization (VQ-VAE) for discrete token space discretization with reinforcement learning for decoding tokens into continuous motion. By using a random high-level policy to generate token sequences and an RL-based low-level policy to produce the actual motion, the method effectively avoids the periodic patterns and out-of-distribution issues that plague autoregressive approaches. The approach is evaluated on both body skeleton datasets and a VTuber face dataset, demonstrating superior performance across multiple metrics.

## Method Summary
The proposed method employs a VQ-VAE to discretize continuous motion features into a finite set of tokens, creating a bounded output space that prevents out-of-distribution issues in long sequence generation. A reinforcement learning framework is then used to train a low-level policy that generates natural and diverse continuous motion from these tokens. The policy network takes tokens, past frames, and random noise as input, guided by three reward functions: realistic (natural-looking sequences), correspondence (adherence to input tokens), and diversity (encouraging varied outputs). To avoid periodic patterns common in autoregressive models, a random high-level policy is used to generate token sequences independently for each clip rather than conditioning on previous tokens.

## Key Results
- The proposed method significantly outperforms previous approaches on Frechet distance metrics in both raw and PCA spaces
- The random token generation approach (Ours-R) produces better results than autoregressive methods in most cases
- The framework successfully generates unlimited-length character motion sequences without observable driving sources
- The method demonstrates effectiveness across both body skeleton datasets and VTuber face data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The discrete token space derived from VQ-VAE prevents out-of-distribution (OOD) issues in long sequence generation.
- Mechanism: By quantizing continuous features into a finite set of discrete tokens, the high-level policy can only generate tokens that exist in the learned codebook, constraining outputs to the in-distribution space.
- Core assumption: The VQ-VAE's codebook adequately covers the distribution of natural motion sequences.
- Evidence anchors:
  - [abstract] "To tackle the out-of-distribution issue, we discretize the continuous feature space using quantization algorithms like vector-quantization variational autoencoder (VQ-VAE)..."
  - [section 4.1] "The token space serves as both the output space of the high-level policy and the input space of the low-level policy. Once the token space is determined, the high-level policy and the low-level policy can be disentangled and separately developed."
- Break condition: If the VQ-VAE fails to learn a codebook that adequately represents the natural motion distribution, the token space will not cover important regions, limiting the diversity of generated sequences.

### Mechanism 2
- Claim: The reinforcement learning-based low-level policy generates diverse and natural continuous motion from discrete tokens.
- Mechanism: The low-level policy network takes tokens, past frames, and random noise as input to produce the next frame. Reward functions encourage natural-looking sequences (realistic reward), adherence to the token (correspondence reward), and diversity (diversity reward).
- Core assumption: The reward functions are properly balanced and sufficient to guide the policy toward producing diverse yet natural sequences.
- Evidence anchors:
  - [abstract] "We employ a reinforcement learning (RL) framework to replace the VQ-VAE decoder... The policy network... is responsible for continuously generating the next frame based on the current token and the past frames."
  - [section 4.2] "Three types of rewards are designed for different purposes: 1) the realistic reward forces the generated sequence to look natural; 2) the diversity reward is proposed to encourage the policy to produce diverse outputs and 3) the correspondence reward requires the produced result to hit the input token."
- Break condition: If the reward functions are poorly designed or imbalanced, the policy may converge to degenerate solutions that ignore one of the key objectives (naturalness, diversity, or token correspondence).

### Mechanism 3
- Claim: The random high-level policy avoids periodic patterns in generated sequences.
- Mechanism: Instead of autoregressively generating token sequences based on previous tokens, the random policy selects tokens independently for each 20-frame clip. This breaks the deterministic loop that causes periodic repetition in autoregressive models.
- Core assumption: The low-level policy can smoothly transition between different tokens, allowing for coherent sequences even with randomly selected tokens.
- Evidence anchors:
  - [abstract] "to avoid the periodic pattern issue caused by the autoregressive generation procedure, we instead design a random generation scheme, which we call a high-level policy, to produce the token sequence."
  - [section 4.3] "We also consider using a random prior. Specifically, for each 20-frame clip, we randomly choose a token from the codebook. With this setting, even the past feature x<t is ignored by the high-level policy."
- Break condition: If the low-level policy cannot smoothly transition between different tokens, the random high-level policy will produce sequences with abrupt, unnatural transitions.

## Foundational Learning

- Concept: Vector Quantization Variational Autoencoder (VQ-VAE)
  - Why needed here: VQ-VAE provides a way to discretize continuous motion features into a finite set of tokens, which helps prevent OOD issues in long sequence generation.
  - Quick check question: How does the VQ-VAE codebook learning process ensure that the tokens represent the natural motion distribution?

- Concept: Reinforcement Learning with Reward Shaping
  - Why needed here: The RL framework allows the low-level policy to learn how to generate natural and diverse motion sequences from tokens, guided by carefully designed reward functions.
  - Quick check question: How do the three reward functions (realistic, correspondence, diversity) work together to guide the policy, and how might their relative weights affect the learned behavior?

- Concept: Markov Decision Process (MDP) Formulation
  - Why needed here: The motion generation problem can be framed as an MDP where the state includes tokens, past frames, and noise, and the action is the next frame. This formulation enables the use of RL algorithms.
  - Quick check question: What are the state, action, and reward components in this MDP formulation, and how do they map to the motion generation task?

## Architecture Onboarding

- Component map:
  VQ-VAE -> High-level policy -> Low-level policy -> Discriminator and Noise Encoder

- Critical path:
  VQ-VAE codebook training → High-level policy token generation → Low-level policy motion generation (using RL rewards)

- Design tradeoffs:
  - Using a random vs. autoregressive high-level policy: Random policy avoids periodic patterns but may produce less coherent long-term structure.
  - Reward function design: Balancing realistic, correspondence, and diversity rewards is crucial for good performance.
  - Token space granularity: The codebook size and VQ-VAE architecture affect the expressiveness and diversity of generated motions.

- Failure signatures:
  - OOD issues: Generated sequences contain unrealistic or implausible motions.
  - Lack of diversity: Generated sequences are repetitive or fail to capture the full range of natural motion.
  - Periodic patterns: Generated sequences exhibit repeating patterns or get stuck in fixed points.
  - Poor transitions: Generated sequences have abrupt, unnatural transitions between different motion states.

- First 3 experiments:
  1. Train VQ-VAE on a subset of the motion data and visualize the learned codebook tokens to ensure they represent natural motion states.
  2. Train the low-level policy with only the realistic reward and generate sequences to verify that it can produce natural-looking motion from tokens.
  3. Compare the performance of the random and autoregressive high-level policies on a validation set to confirm that the random policy avoids periodic patterns.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation:

- How would the proposed framework perform if the high-level policy were trained using reinforcement learning instead of the random token generation approach?
- Can the token space discretization approach effectively handle more complex, higher-dimensional motion data such as full-body human motion with multiple interacting characters?
- How does the proposed framework scale to longer sequences (e.g., hours of continuous motion) compared to autoregressive approaches?

## Limitations

- The effectiveness of the VQ-VAE-based discretization relies heavily on the codebook adequately representing the natural motion distribution, which is not extensively analyzed
- The random high-level policy may sacrifice long-term coherence in generated sequences compared to autoregressive approaches
- Evaluation primarily focuses on Frechet Distance metrics, lacking user studies or qualitative assessments that would provide complementary evidence

## Confidence

**High Confidence:** The technical approach of using VQ-VAE for discretization to prevent out-of-distribution issues in long sequence generation is well-founded and supported by clear reasoning. The combination of realistic, correspondence, and diversity rewards in the low-level policy represents a reasonable and principled approach to balancing competing objectives.

**Medium Confidence:** The claim that the random high-level policy effectively avoids periodic patterns while maintaining sequence quality. While the method is logically sound, the evaluation could benefit from more direct comparisons between random and autoregressive approaches in terms of both diversity and coherence.

**Low Confidence:** The assertion that the method generalizes well across different motion types (body skeleton vs. face motion). The paper provides limited discussion of how the approach might need to be adapted for different motion domains or what limitations might arise when scaling to more complex motion patterns.

## Next Checks

1. **Codebook Coverage Analysis:** Visualize and analyze the learned VQ-VAE codebook tokens to verify they represent a comprehensive and balanced sample of natural motion states. Check for clusters of similar tokens or regions of the motion space that may be underrepresented.

2. **Random vs. Autoregressive Comparison:** Conduct a systematic comparison between the random high-level policy and autoregressive alternatives, measuring not only diversity metrics but also long-term coherence and perceptual quality through user studies or additional objective metrics.

3. **Cross-Domain Generalization Test:** Apply the trained model to a held-out test set from a different motion domain (e.g., using a model trained on body motion to generate facial expressions) to assess the method's robustness and identify potential domain-specific limitations.