---
ver: rpa2
title: 'Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning
  in Math Word Problems'
arxiv_id: '2310.01991'
source_url: https://arxiv.org/abs/2310.01991
tags:
- answer
- cards
- reasoning
- question
- blank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the challenge of backward reasoning in math
  word problems (MWPs), where the goal is to find a missing numerical value given
  the question, answer, and context. The authors modify three benchmark datasets (GSM8k,
  SVAMP, and MultiArith) to create backward reasoning problems and demonstrate that
  large language models (LLMs) struggle with this task compared to forward reasoning.
---

# Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems

## Quick Facts
- arXiv ID: 2310.01991
- Source URL: https://arxiv.org/abs/2310.01991
- Reference count: 40
- Key outcome: LLMs struggle with backward reasoning in math word problems, but Bayesian ensembling of three base methods (Rephrase, PAL-Tools, Check your Work) improves accuracy by up to 45%

## Executive Summary
This paper investigates backward reasoning in math word problems (MWPs), where the goal is to find a missing numerical value given the question, answer, and context. The authors find that large language models (LLMs) perform significantly worse on backward reasoning tasks compared to forward reasoning, with accuracy drops up to 40%. To address this challenge, they propose three base methods - Rephrase (reformulating problems as forward reasoning), PAL-Tools (combining LLMs with symbolic solvers), and Check your Work (interleaving solving and verification). They also introduce a novel Bayesian ensembling method that combines these base methods with a verifier, achieving up to 45% accuracy gains on modified GSM8k, SVAMP, and MultiArith datasets.

## Method Summary
The authors modify three benchmark datasets to create backward reasoning problems by blanking out one numerical quantity in each question. They then propose three base methods: Rephrase (reformulating problems into forward reasoning tasks), PAL-Tools (combining program-aided language models with external solvers like SymPy), and Check your Work (interleaving solving and verification steps). To improve performance further, they introduce a Bayesian ensembling method that combines these base methods with a verifier to compute posterior probabilities and select the most likely correct answer. The minimum viable reproduction plan involves modifying the datasets, implementing the three base methods with specific prompts, and creating the Bayesian ensemble with frequency voting and verifier integration.

## Key Results
- LLMs show up to 40% accuracy drop on backward reasoning compared to forward reasoning across multiple models
- Individual base methods improve backward reasoning performance over baseline LLM approaches
- Bayesian ensemble method significantly outperforms both individual base methods and forward reasoning accuracy
- The ensemble achieves up to 45% accuracy gains in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backward reasoning is inherently harder for LLMs because it requires inverting the forward reasoning process, which most LLMs are not optimized for.
- Mechanism: The difficulty arises from the non-sequential nature of backward reasoning, where LLMs must rearrange equations and isolate unknown variables instead of simply applying a known formula.
- Core assumption: LLMs have stronger forward reasoning capabilities due to their training data distribution and prompting strategies.
- Evidence anchors:
  - [abstract] "we find a significant drop in the accuracy of models on this task compared to forward reasoning across SOTA LLMs"
  - [section] "We observe a significant drop in the performance of all the LLMs compared to forward reasoning, the drop being as high as 40% in most cases."
  - [corpus] Weak evidence - no direct comparison of forward vs backward reasoning in corpus
- Break condition: If LLMs are trained specifically on backward reasoning tasks or if inversion operations become easier through architectural changes.

### Mechanism 2
- Claim: Rephrasing backward problems into forward problems significantly improves LLM performance.
- Mechanism: By replacing the blank with an unknown variable x and asking the model to solve for x given the answer, the problem becomes a standard forward reasoning task that LLMs handle better.
- Core assumption: LLMs can effectively solve forward reasoning problems when given clear algebraic formulations.
- Evidence anchors:
  - [abstract] "Rephrase reformulates the given problem into a forward reasoning problem"
  - [section] "We ask the language model to produce a rephrased question R, which incorporates the forward answer Af into the question Q and changes the objective"
  - [section] Table 2 shows rephrasing improves accuracy across multiple techniques
- Break condition: If the rephrasing process introduces ambiguity or if the algebraic formulation becomes too complex for the LLM to handle.

### Mechanism 3
- Claim: Ensembling multiple base methods with Bayesian verification provides significant accuracy gains.
- Mechanism: The ensemble combines priors from multiple methods with verifier confidence to compute posterior probabilities, allowing the system to leverage the strengths of different approaches.
- Core assumption: Different base methods solve different subsets of problems correctly, and a high-accuracy verifier can distinguish correct from incorrect answers.
- Evidence anchors:
  - [abstract] "we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier"
  - [section] "The probability that Al is the correct answer for a question Q, given the prior probability and the verifier output Zl, by application of Bayes Rule"
  - [section] Table 3 shows ensemble accuracy surpasses forward reasoning accuracy
- Break condition: If the verifier itself becomes unreliable or if the base methods are highly correlated in their errors.

## Foundational Learning

- Concept: Bayes' Theorem and probabilistic reasoning
  - Why needed here: The ensemble method relies on computing posterior probabilities using Bayes' rule to combine multiple model predictions with verifier confidence
  - Quick check question: How do you update prior beliefs about an answer when you receive new evidence from a verifier?

- Concept: Program-aided language models (PAL) and symbolic solvers
  - Why needed here: PAL-Tools combines LLM capabilities with external symbolic solvers like SymPy to handle the mathematical structure of backward reasoning problems
  - Quick check question: What advantages does combining LLMs with symbolic solvers provide over pure LLM approaches?

- Concept: Chain-of-thought reasoning and prompt engineering
  - Why needed here: The paper relies heavily on prompt engineering techniques like chain-of-thought to guide LLMs through complex reasoning tasks
  - Quick check question: How does providing step-by-step reasoning in prompts affect LLM performance on mathematical word problems?

## Architecture Onboarding

- Component map: Data preprocessing -> Base methods (Rephrase, PAL-Tools, Check your Work) -> Ensemble system -> Verifier
- Critical path: 1. Load dataset and convert to backward format 2. Apply base methods to generate candidate answers 3. Compute prior probabilities from base method outputs 4. Use verifier to update priors to posteriors 5. Select answer with highest posterior probability
- Design tradeoffs:
  - Rephrasing vs direct solving: Rephrasing adds complexity but leverages LLM strengths in forward reasoning
  - Single method vs ensemble: Ensembling increases accuracy but adds computational overhead and complexity
  - Verifier accuracy vs cost: More accurate verifiers improve ensemble performance but may be expensive to compute
- Failure signatures:
  - Low diversity in base method outputs (indicates methods are solving similar problem subsets)
  - Verifier confidence not correlated with actual correctness
  - Base methods produce many invalid answers (mathematical impossibilities)
- First 3 experiments:
  1. Compare baseline LLM performance on backward vs forward reasoning tasks to establish difficulty gap
  2. Test each base method individually on a validation set to identify which subsets of problems each method solves well
  3. Implement and evaluate the Bayesian ensemble on a small holdout set before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical computational complexity difference between forward and backward reasoning in math word problems for LLMs?
- Basis in paper: [explicit] The paper establishes that backward reasoning is significantly harder for LLMs compared to forward reasoning, with accuracy drops up to 40% across various models.
- Why unresolved: While the paper demonstrates the empirical difficulty difference, it doesn't provide a formal theoretical analysis of why this gap exists or whether it reflects an inherent complexity difference similar to what has been observed in human reasoning studies.
- What evidence would resolve it: A formal computational complexity analysis comparing the two reasoning directions, or empirical studies measuring the exact relationship between problem structure and reasoning difficulty for LLMs.

### Open Question 2
- Question: How do the proposed methods generalize to more complex mathematical domains beyond elementary arithmetic, such as algebra, calculus, or multi-step reasoning problems?
- Basis in paper: [inferred] The paper focuses on grade-school level arithmetic problems, but the methods proposed (Rephrase, PAL-Tools, Check your Work) could theoretically be applied to more complex domains.
- Why unresolved: The paper's experiments are limited to simple arithmetic problems, leaving uncertainty about how well these techniques would scale to more advanced mathematical reasoning tasks.
- What evidence would resolve it: Extensive experiments applying these methods to higher-level mathematics problems, including algebra word problems, calculus applications, and problems requiring multiple reasoning steps.

### Open Question 3
- Question: What is the optimal ensemble composition and weighting strategy for combining multiple backward reasoning methods?
- Basis in paper: [explicit] The paper uses a Bayesian ensembling approach with equal weighting of methods, but acknowledges that each method solves a different subset of problems.
- Why unresolved: While the paper demonstrates that ensembling improves performance, it doesn't explore optimal combinations, weighting schemes, or whether certain methods should be prioritized for specific problem types.
- What evidence would resolve it: Systematic experiments varying ensemble compositions, comparing different weighting strategies (learned weights vs. equal weights), and analyzing which method combinations work best for different problem categories.

## Limitations
- The paper doesn't provide exact prompts and few-shot examples for the three base methods, making exact reproduction challenging
- Evaluation is limited to three specific arithmetic datasets, leaving uncertainty about generalization to other mathematical domains
- The method's success depends heavily on verifier reliability, which isn't thoroughly analyzed for failure modes

## Confidence
- Backward reasoning difficulty claim: High
- Bayesian ensemble effectiveness: Medium
- Generalization to complex domains: Low

## Next Checks
1. **Verifier reliability analysis**: Systematically test the verifier's accuracy across different problem types and difficulty levels to understand its failure modes and establish confidence bounds for the ensemble method.
2. **Cross-dataset generalization**: Apply the backward reasoning techniques to additional MWP datasets (e.g., ASDiv, MathQA) to assess whether the observed performance improvements generalize beyond the three benchmark datasets used.
3. **Ablation study on prompt engineering**: Conduct controlled experiments varying the prompts and few-shot examples for each base method to determine the sensitivity of the approach to prompt design choices and establish more robust implementation guidelines.