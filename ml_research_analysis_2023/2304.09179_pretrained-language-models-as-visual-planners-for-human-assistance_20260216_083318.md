---
ver: rpa2
title: Pretrained Language Models as Visual Planners for Human Assistance
arxiv_id: '2304.09179'
source_url: https://arxiv.org/abs/2304.09179
tags:
- action
- actions
- vlamp
- history
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Visual Planning for Assistance (VPA), a new
  task where an AI assistant must generate a plan of actions to help a user achieve
  a goal based on their progress so far. The authors decompose VPA into video action
  segmentation and forecasting, leveraging pre-trained language models for the latter.
---

# Pretrained Language Models as Visual Planners for Human Assistance

## Quick Facts
- arXiv ID: 2304.09179
- Source URL: https://arxiv.org/abs/2304.09179
- Authors: 
- Reference count: 40
- Primary result: VLaMP outperforms baselines on plan generation metrics for Visual Planning for Assistance task

## Executive Summary
This paper introduces Visual Planning for Assistance (VPA), a task where an AI assistant must generate action plans to help users achieve goals based on their progress so far. The authors propose decomposing VPA into video action segmentation and forecasting, leveraging pretrained language models for efficient sequence modeling. Their approach, VLaMP, demonstrates superior performance across multiple metrics that evaluate plan quality. The method effectively combines visual observations with language understanding to generate contextually relevant plans.

## Method Summary
The approach decomposes VPA into two stages: video action segmentation and plan forecasting. The segmentation module converts untrimmed video histories into action-aligned segments, which are then processed by a forecasting module using a pretrained transformer-based language model (GPT-2). Visual observations are encoded using S3D features and mapped to the action embedding space via a trainable transformer mapper. The model generates plans autoregressively using beam search, conditioning on both the segmented action history and natural language goal description.

## Key Results
- VLaMP outperforms baselines across multiple metrics including Success Rate, Mean Accuracy, Mean Intersection over Union, and Next Accuracy
- Ablation studies demonstrate the importance of language pretraining, visual observations, and goal information for performance
- The approach effectively handles the high-dimensional visual input while leveraging strong priors from pretrained language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposition into segmentation and forecasting modules enables efficient sequence modeling with pretrained language models
- Mechanism: Segmentation extracts action-aligned segments from video history, allowing pretrained language models to leverage action ordering priors while a mapper handles visual input
- Core assumption: Segmentation module accurately extracts action-aligned segments and pretrained language model contains relevant action planning priors
- Evidence anchors: [abstract], [section 4.1]
- Break condition: Inaccurate segmentation or lack of relevant language model priors would degrade performance

### Mechanism 2
- Claim: Goal-conditioning focuses the model on relevant actions and their ordering
- Mechanism: Natural language goal prompts filter irrelevant actions and provide context for correct action ordering
- Core assumption: Goal prompts provide sufficient context for identifying relevant actions and their ordering
- Evidence anchors: [abstract], [section 3.1]
- Break condition: Ambiguous or complex goals would hinder action identification and ordering

### Mechanism 3
- Claim: Multimodal sequence modeling effectively combines visual and textual information
- Mechanism: Separate encoders for actions and observations with a mapper network aligning visual and action embeddings
- Core assumption: Visual observations provide useful planning information and mapper effectively aligns embeddings
- Evidence anchors: [section 4.3]
- Break condition: Uninformative visual observations or ineffective embedding alignment would reduce performance

## Foundational Learning

- Concept: Video action segmentation
  - Why needed here: Extracts action-aligned segments from untrimmed video history for forecasting input
  - Quick check question: How does the segmentation module handle irrelevant background frames in untrimmed videos?

- Concept: Transformer-based language models
  - Why needed here: Provide strong sequence modeling capabilities with action ordering priors from pretraining
  - Quick check question: What advantages do transformer-based models offer over other sequence modeling approaches?

- Concept: Multimodal sequence modeling
  - Why needed here: Combines visual observations and action history for effective planning
  - Quick check question: How does the mapper network align visual and action embeddings?

## Architecture Onboarding

- Component map: Video History -> Segmentation Module -> Action Encoder + Observation Encoder -> Sequence Model -> Beam Search Inference

- Critical path:
  1. Segment untrimmed video history into action-aligned segments
  2. Encode action history and visual observations
  3. Autoregressively predict next action and observation tokens
  4. Generate plan using beam search

- Design tradeoffs:
  - Pretrained vs. randomly initialized sequence model: Pretrained offers priors but requires fine-tuning
  - Including visual observations vs. action history only: Visual context vs. model complexity
  - Beam search vs. greedy decoding: Diverse plans vs. computational cost

- Failure signatures:
  - High segmentation error leading to incorrect action history
  - Low planning accuracy despite accurate segmentation
  - Slow inference making system impractical for real-time use

- First 3 experiments:
  1. Ablation study on visual observations importance
  2. Ablation study on language pretraining importance
  3. Error analysis on segmentation errors' impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would joint optimization of segmentation and forecasting modules affect performance?
- Basis in paper: [explicit] The paper states joint training is intractable and inefficient, but doesn't explore potential performance gains
- Why unresolved: Paper explicitly mentions intractability but doesn't explore performance comparison
- What evidence would resolve it: Empirical results comparing joint vs. separate training on same datasets

### Open Question 2
- Question: How would advanced segmentation models or additional modalities affect performance?
- Basis in paper: [inferred] Current segmentation accuracy is 80.2% (CrossTask) and 68.7% (COIN), but other models/modalities not explored
- Why unresolved: Paper doesn't experiment with different segmentation models or modalities
- What evidence would resolve it: Empirical results comparing different segmentation models and modalities

### Open Question 3
- Question: How would different pretrained language models affect performance?
- Basis in paper: [inferred] Uses GPT-2 but doesn't explore benefits of other pretrained models like GPT-3 or BERT
- Why unresolved: Paper doesn't experiment with different pretrained language models
- What evidence would resolve it: Empirical results comparing different pretrained language models

## Limitations

- The approach is currently limited to procedural activities and may not generalize to more complex, non-sequential tasks
- Performance depends heavily on segmentation accuracy, which is not perfect (80.2% and 68.7% on test datasets)
- The method requires paired video-action segmentation data, limiting its applicability to domains without such annotations

## Confidence

- High confidence: Effectiveness of decomposing VPA into segmentation and forecasting modules
- Medium confidence: Generalizability to diverse tasks and scalability to complex planning scenarios
- Low confidence: Impact of segmentation errors and robustness to out-of-distribution examples

## Next Checks

1. Conduct ablation study on segmentation errors' impact by introducing varying noise levels into segmentation output
2. Evaluate model performance on diverse tasks with longer action sequences and non-procedural activities
3. Test model's ability to handle out-of-distribution examples with significantly different visual observations or goal prompts