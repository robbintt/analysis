---
ver: rpa2
title: 'ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking
  Real Prenatal Development'
arxiv_id: '2311.17232'
source_url: https://arxiv.org/abs/2311.17232
tags:
- retinal
- training
- waves
- wave
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReWaRD, a novel approach to pre-training
  artificial neural networks (ANNs) that mimics the prenatal development of the primate
  visual system through retinal waves. The authors generate retinal wave datasets
  and use them to pre-train ANNs, then fine-tune these networks on standard image
  classification tasks.
---

# ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development

## Quick Facts
- arXiv ID: 2311.17232
- Source URL: https://arxiv.org/abs/2311.17232
- Reference count: 25
- Key outcome: Retinal wave pre-training produces V1-like features and competitive performance on image classification tasks

## Executive Summary
This paper introduces ReWaRD, a novel approach to pre-training artificial neural networks (ANNs) that mimics the prenatal development of the primate visual system through retinal waves. The authors generate retinal wave datasets and use them to pre-train ANNs, then fine-tune these networks on standard image classification tasks. The key finding is that this biologically plausible pre-training leads to features in early layers that closely match V1 features in the primate visual system. When fine-tuned, these networks achieve performance gains similar to state-of-the-art pre-training methods, with the added benefit of reduced bias compared to datasets like ImageNet. The authors release their code, datasets, and pre-trained networks to facilitate further research on visual development and the innate vs. learned properties of the primate visual system.

## Method Summary
The authors generate simulated retinal wave images using parameters derived from animal experiments, creating two datasets with 1024 and 4096 classes respectively. They pre-train ResNet50 and other CNN architectures on these datasets in a supervised fashion, using the wave parameters as labels. After pre-training, the networks are fine-tuned on standard image classification datasets like CIFAR100 and ImageNet1k. The pre-training uses the FractalDB codebase with 90 epochs and a learning rate of 0.01. The method aims to mimic prenatal visual system development by exposing networks to structured spatial patterns similar to those experienced by biological visual systems before eye opening.

## Key Results
- Retinal wave pre-training produces early-layer features that closely resemble V1 Gabor filters in the primate visual system
- Networks pre-trained on retinal waves achieve classification accuracy comparable to state-of-the-art methods like FractalDB when fine-tuned on CIFAR100 and ImageNet1k
- Brain-score metrics show improved representation in early layers compared to training from scratch, suggesting better alignment with biological neural activity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on retinal wave images initializes convolutional filters in early layers to resemble V1 Gabor-like features found in biological visual systems.
- Mechanism: Retinal waves provide structured, wave-like spatial patterns that activate early visual areas during prenatal development. When ANNs are pre-trained on simulated retinal wave images, the gradient updates shape early convolutional filters toward orientations and spatial frequencies similar to biological V1 neurons.
- Core assumption: The simulated retinal waves capture enough of the statistical structure of prenatal retinal activity to induce biologically relevant feature learning.
- Evidence anchors:
  - [abstract] "The resulting features of this biologically plausible pre-training closely match the V1 features of the primate visual system."
  - [section] "Filters: The learned filters of early layers of pre-trained ResNet50 ANNs closely match gabor-filters, making them similar to the real V1 filters in the human visual system."
  - [corpus] Weak signal; no direct corpus matches to retinal wave pre-training mechanisms found.
- Break condition: If the simulated retinal waves lack key spatiotemporal statistics present in real prenatal retinal activity, the learned filters may not match V1 properties and performance gains would disappear.

### Mechanism 2
- Claim: Pre-training on retinal waves improves generalization and reduces the number of labeled examples needed for downstream tasks compared to training from scratch.
- Mechanism: Early layers learn generalizable low-level features (edges, orientations) from retinal waves before fine-tuning, so later layers can focus on task-specific patterns. This mirrors how prenatal retinal waves prime the visual system for real-world visual input after eye opening.
- Core assumption: Early feature learning from retinal waves transfers effectively to natural image statistics, providing a useful inductive bias.
- Evidence anchors:
  - [section] "In comparison to training from scratch, we observe faster generalization when pre-training on retinal waves and fine-tuning on CIFAR100."
  - [section] "We observe accuracies similar to pre-training with FractalDB."
  - [corpus] No direct corpus evidence for this specific transfer claim; based on experimental results.
- Break condition: If downstream tasks have very different low-level statistics than those captured by retinal waves, the pre-training benefit may vanish.

### Mechanism 3
- Claim: Retinal wave pre-training induces activation patterns in artificial networks that more closely match primate brain activity than standard ImageNet pre-training.
- Mechanism: Because retinal waves mimic early prenatal visual experience, networks pre-trained on them develop internal representations aligned with the initial wiring and tuning of primate visual cortex, as measured by brain-score metrics.
- Core assumption: Brain-score is a valid proxy for measuring similarity between artificial and biological visual representations.
- Evidence anchors:
  - [section] "Models pre-trained on retinal waves and fine-tuned on CIFAR100 show increased scores in all regions compared to training from scratch."
  - [section] "Especially the improved representation in early layers makes sense in our eyes, as retinal waves might enable to learn those features in a less noisy and cluttered way."
  - [corpus] No corpus evidence directly addressing brain-score comparisons for retinal wave pre-training.
- Break condition: If brain-score fails to capture the relevant aspects of neural similarity, or if retinal wave statistics diverge significantly from real prenatal activity, the claimed match may not hold.

## Foundational Learning

- Concept: Convolutional Neural Networks and their role in feature extraction
  - Why needed here: The paper uses CNNs (ResNet50, etc.) as the model architecture; understanding how early convolutional layers learn spatial features is key to interpreting why retinal wave pre-training affects them.
  - Quick check question: What is the difference between the first convolutional layer and later layers in a CNN?

- Concept: Transfer Learning and Pre-training
  - Why needed here: The method relies on pre-training on retinal waves, then fine-tuning on standard datasets. Knowing how feature reuse and weight initialization affect downstream performance is critical.
  - Quick check question: Why does pre-training on a task with similar low-level statistics help when fine-tuning on a different high-level task?

- Concept: Biological Visual System Development
  - Why needed here: The motivation is to mimic prenatal visual development; understanding retinal waves and their role in V1 formation contextualizes the computational approach.
  - Quick check question: What is the functional role of retinal waves in early mammalian visual system development?

## Architecture Onboarding

- Component map:
  Retinal Wave Generator → Pre-training Pipeline → Fine-tuning Pipeline → Evaluation Suite

- Critical path:
  1. Generate retinal wave dataset (parameter selection, image rendering, labeling)
  2. Pre-train CNN on retinal wave dataset (supervised classification of wave parameters)
  3. Fine-tune pre-trained CNN on downstream task (e.g., CIFAR100 or ImageNet1k)
  4. Evaluate with accuracy, brain-score, and filter visualizations

- Design tradeoffs:
  - Supervised vs. unsupervised pre-training: Supervised allows direct gradient flow but assumes labels exist for prenatal activity; unsupervised might be more biologically plausible but harder to train.
  - Image resolution and retinal wave complexity: Higher resolution yields more detailed features but increases compute cost.
  - Choice of CNN architecture: ResNet50 used here; others may yield different feature alignments.

- Failure signatures:
  - No performance gain over training from scratch → pre-training not transferring useful features
  - Brain-score similar to baseline → representations not matching biological patterns
  - Filters not Gabor-like → pre-training not shaping early layers as intended

- First 3 experiments:
  1. Generate a small retinal wave dataset (e.g., 1024 classes, 100 images/class) and train a shallow CNN to verify supervised pre-training works.
  2. Pre-train ResNet50 on the small retinal wave dataset, then fine-tune on CIFAR10; compare to training from scratch.
  3. Visualize first-layer filters of the pre-trained network; check for Gabor-like patterns compared to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the results change if retinal waves were simulated using human-specific parameters rather than animal-derived parameters?
- Basis in paper: [explicit] The authors note that "The parameters for the retinal wave simulator used for our experiment are based on animal experiments and primate or human retinal waves might behave slightly different."
- Why unresolved: The paper acknowledges this limitation but does not provide any data or analysis using human-specific parameters, which could potentially yield different feature representations in the pre-trained networks.
- What evidence would resolve it: Conducting experiments using retinal wave parameters derived from human data and comparing the resulting network features, Brain-Score results, and classification performance to those obtained using animal-derived parameters.

### Open Question 2
- Question: Would unsupervised pre-training on retinal wave data lead to features that better match biological visual systems compared to supervised pre-training?
- Basis in paper: [explicit] The authors mention that "We only investigated pre-training in a fully supervised fashion. One could argue, that unsupervised learning would be more biologically plausible."
- Why unresolved: The paper focuses exclusively on supervised learning, despite acknowledging that unsupervised learning might be more biologically plausible, leaving open the question of whether this would yield better results.
- What evidence would resolve it: Comparing features learned through unsupervised pre-training on retinal wave data with those from supervised pre-training, using metrics like similarity to V1 features, Brain-Score, and classification performance after fine-tuning.

### Open Question 3
- Question: How would incorporating neural architecture search during pre-training with retinal waves affect the emergence of biologically plausible features?
- Basis in paper: [inferred] The authors note that "As the visual cortex has not yet fully developed when retinal waves do occur, a fixed ANN architecture is a limiting factor."
- Why unresolved: The paper uses fixed CNN architectures (ResNet50, AlexNet, etc.) for pre-training, but does not explore how dynamically adapting the architecture might better mimic biological development.
- What evidence would resolve it: Implementing neural architecture search during pre-training with retinal waves and comparing the resulting features, Brain-Score results, and classification performance to those obtained with fixed architectures.

## Limitations

- The exact retinal wave simulator parameters and implementation details are not fully specified, making faithful reproduction challenging
- Brain-score comparisons rely on proxy metrics that may not fully capture neural similarity
- Direct comparisons to more recent unsupervised/self-supervised pre-training methods are limited

## Confidence

- Medium confidence that retinal wave pre-training shapes early CNN filters toward Gabor-like patterns, based on visualization evidence but lacking direct biological recordings for validation
- Medium confidence that performance gains are competitive with FractalDB, though comparisons to more recent pre-training approaches would strengthen this claim
- Low confidence that brain-score improvements reflect genuine neural similarity, given limited ground truth for prenatal visual system activity

## Next Checks

1. Replicate filter visualizations using the provided retinal wave datasets to verify Gabor-like patterns emerge in early layers
2. Test retinal wave pre-training on additional downstream tasks (e.g., object detection, segmentation) to assess transfer generality
3. Compare brain-score metrics to baseline models using alternative biological plausibility measures to triangulate neural similarity claims