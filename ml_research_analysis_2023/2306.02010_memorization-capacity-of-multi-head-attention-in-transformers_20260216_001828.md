---
ver: rpa2
title: Memorization Capacity of Multi-Head Attention in Transformers
arxiv_id: '2306.02010'
source_url: https://arxiv.org/abs/2306.02010
tags:
- rank
- attention
- assumption
- layer
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves a new lower bound on the memorization capacity
  of multi-head attention layers under mild linear independence assumptions. The key
  idea is to establish a tight bound on the rank of intermediate representations,
  showing that an attention layer with $O(d^2 H)$ parameters can memorize $O(nH)$
  examples, where $n$ is the context size and $H$ is the number of heads.
---

# Memorization Capacity of Multi-Head Attention in Transformers

## Quick Facts
- **arXiv ID**: 2306.02010
- **Source URL**: https://arxiv.org/abs/2306.02010
- **Reference count**: 40
- **Primary result**: Proves a new lower bound showing that multi-head attention can memorize O(Hn) examples under mild linear independence assumptions

## Executive Summary
This paper establishes a theoretical lower bound on the memorization capacity of multi-head attention layers in Transformers. The key insight is that under mild linear independence assumptions, an attention layer with O(d²H) parameters can memorize O(Hn) examples, where n is the context size and H is the number of heads. The proof leverages an inductive technique to construct attention weights that yield high-rank intermediate representations, followed by solving a linear system to achieve memorization. Experiments on vision transformers and synthetic data confirm that the assumptions often hold in practice and that memorization scales linearly with the number of heads as predicted by theory.

## Method Summary
The paper investigates memorization capabilities through theoretical analysis and experimental validation. The method constructs attention weights that exploit softmax saturation to isolate specific context tokens while maintaining linear independence. The approach relies on establishing that intermediate representation matrices have sufficient rank under mild assumptions about query and context vectors. Experiments use synthetic data with controlled properties and vision transformers on image classification tasks, employing Adam optimizer with linear warmup and cosine decay scheduling.

## Key Results
- Proves a lower bound of O(Hn) memorization capacity under linear independence assumptions
- Shows that context matrices have full rank with sinusoidal positional encoding in practice
- Experimental validation confirms linear scaling of memorization with number of heads
- Demonstrates that theoretical assumptions hold reasonably well on real data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An attention layer with O(d²H) parameters can memorize O(Hn) examples under mild linear independence assumptions.
- Mechanism: The proof constructs attention weights that yield a high-rank intermediate representation matrix Z, then solves a linear system to achieve memorization.
- Core assumption: The union of query vectors has Kruskal Rank at least n, and each context matrix E(t) has rank n.
- Evidence anchors:
  - [abstract] "under mild linear independence assumptions... can memorize O(Hn) examples"
  - [section] "We introduce a set of assumptions and under those assumptions, we prove that a single-layer MHA layer... can memorize O(Hn) examples"
  - [corpus] Weak evidence - related works focus on capacity bounds but don't directly verify this specific mechanism
- Break condition: If the Kruskal Rank of query vectors is less than n, the memorization bound weakens to H(Q-1)+1 where Q is the actual rank.

### Mechanism 2
- Claim: The softmax operator's saturation property allows tuning attention weights for new data points while keeping previous representations fixed.
- Mechanism: When attention weights are scaled by a large constant, softmax saturates and produces one-hot attention patterns that isolate specific context tokens.
- Core assumption: The attention logits can be made sufficiently large to saturate softmax.
- Evidence anchors:
  - [section] "We exploit the saturation property of Softmax to keep Z''_a fixed while tuning Z''_b and increasing the rank of Z"
  - [section] "Since cm+1, cm+2, ..., cd are all standard gaussian, then W e(t) ~ N(0, ...)" (shows how large weights create saturation)
  - [corpus] No direct evidence found - this appears to be a novel proof technique
- Break condition: If the context vectors are too similar, saturation may not produce sufficiently distinct one-hot patterns.

### Mechanism 3
- Claim: Positional encoding ensures context matrices have full rank in practice.
- Mechanism: Sinusoidal positional encodings create linearly independent vectors across positions, making context matrices full-rank.
- Core assumption: The positional encoding scheme used produces full-rank matrices.
- Evidence anchors:
  - [section] "In most tasks, a positional encoding is added to each token representation before feeding the data into the Transformer architecture... makes context matrix E(t) full-rank as well"
  - [section] "One of the widely used positional encoding schemes is sinusoidal positional encodings [27], where the positional encoding matrix is full rank"
  - [corpus] No direct evidence found - this is stated as a common practice
- Break condition: If a different positional encoding scheme is used that doesn't produce full-rank matrices, Assumption 2 may be violated.

## Foundational Learning

- Concept: Kruskal Rank and linear independence
  - Why needed here: The proof relies on the Kruskal Rank of query vectors being at least n to ensure the intermediate representation matrix has sufficient rank
  - Quick check question: If we have 5 query vectors in R⁴, what is the maximum possible Kruskal Rank?

- Concept: Matrix rank and linear systems
  - Why needed here: The memorization proof involves constructing a matrix Z with high rank, then solving Zw = y to find weights that achieve perfect memorization
  - Quick check question: If Z is a 10 × 8 matrix with rank 8, can we always find a solution to Zw = y for any y ∈ R¹⁰?

- Concept: Softmax saturation and attention mechanisms
  - Why needed here: The proof exploits softmax saturation to isolate specific context tokens when tuning attention weights
  - Quick check question: What happens to softmax output when the input logits are scaled by a very large constant?

## Architecture Onboarding

- Component map: Input → Attention heads → Intermediate representation Z → Linear system solution → Memorized output
- Critical path: Input → Attention heads → Intermediate representation Z → Linear system solution → Memorized output
- Design tradeoffs:
  - More heads (H) → Higher memorization capacity but more parameters
  - Larger context (n) → More examples can be memorized but requires higher rank in Z
  - Dimension d → Affects both parameter count and rank constraints
- Failure signatures:
  - Low rank in intermediate matrix Z → Cannot memorize enough examples
  - Violation of linear independence assumptions → Proof breaks down
  - Insufficient parameter count → Cannot solve the required linear system
- First 3 experiments:
  1. Verify that context matrices have full rank with sinusoidal positional encoding on real data
  2. Test memorization capacity scaling with number of heads on synthetic data with shared context
  3. Check Kruskal Rank of query vectors after one attention layer with skip connection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact upper bound on memorization capacity of multi-head attention under more realistic assumptions beyond the current lower bound of O(Hn)?
- Basis in paper: [explicit] The paper proves a lower bound of O(Hn) but acknowledges that even with strong assumptions like General Position, the rank is limited to H(d-1)+1, suggesting a potential gap between upper and lower bounds.
- Why unresolved: The paper provides a lower bound but explicitly states that establishing tighter upper bounds remains an important open question. The current proof techniques establish lower bounds but don't address what the true maximum capacity might be.
- What evidence would resolve it: Experiments showing memorization scaling with H(d-1)+1 rather than Hn for various realistic data distributions, or theoretical proofs establishing tighter upper bounds on rank(Z) under different assumptions.

### Open Question 2
- Question: How does the memorization capacity of multi-head attention scale when the context vectors are not shared across examples but are instead unique to each example?
- Basis in paper: [inferred] The current proof technique assumes shared context in Proposition 1 to establish tightness of the lower bound, but experiments in Appendix E.1 suggest different behavior when contexts are non-shared.
- Why unresolved: The paper focuses on shared context settings for theoretical analysis, but real-world applications often involve unique contexts per example. The relationship between head count, context size, and memorization in non-shared settings remains unexplored.
- What evidence would resolve it: Experiments systematically varying the degree of context sharing across examples while measuring memorization capacity, or theoretical analysis of rank(Z) when contexts are non-shared.

### Open Question 3
- Question: Can the memorization capacity bounds be extended to deeper architectures with multiple layers of attention, and how do inter-layer dependencies affect the overall capacity?
- Basis in paper: [explicit] The paper concludes by noting that extending theoretical results beyond one layer of attention networks is a promising avenue for future research.
- Why unresolved: The current analysis focuses on a single attention layer, but transformers typically have multiple layers. The interaction between layers, including how earlier layers' representations affect later layers' memorization capacity, is not characterized.
- What evidence would resolve it: Theoretical bounds on memorization capacity for deep transformer architectures, or empirical studies showing how memorization scales with depth in addition to width (heads) and context size.

## Limitations

- The theoretical analysis relies on stringent linear independence assumptions that may not hold in practical settings
- The proof technique for constructing attention weights using softmax saturation lacks empirical demonstration of practical feasibility
- The theory focuses on single-layer attention, leaving unclear how bounds extend to deep multi-layer transformers

## Confidence

**High confidence**: The linear relationship between memorization capacity and number of heads (O(Hn)) is mathematically proven under the stated assumptions. The connection between positional encodings and full-rank context matrices is well-established in prior literature.

**Medium confidence**: The assumption that sinusoidal positional encodings produce full-rank context matrices in practice, and that this translates to the theoretical capacity bounds. The experimental validation on Vision Transformers supports this but with limited scope.

**Low confidence**: The practical applicability of the softmax saturation technique for constructing attention weights in real training scenarios. The paper provides theoretical construction but no empirical demonstration that this approach can be effectively implemented with gradient-based optimization.

## Next Checks

1. **Kruskal Rank Validation**: Systematically test the Kruskal Rank of query vectors across multiple layers in pre-trained transformers to quantify how often the critical assumption holds in practice. Measure the empirical distribution of ranks and correlate with memorization performance.

2. **Multi-Layer Extension**: Extend the theoretical analysis to deep transformers by tracking how the rank of intermediate representations evolves across layers. Conduct experiments to verify whether the single-layer bounds provide reasonable approximations for deeper architectures.

3. **Saturation Technique Implementation**: Design and implement a concrete algorithm that explicitly constructs attention weights using the softmax saturation technique. Compare its performance against standard gradient-based training on memorization tasks to validate the practical feasibility of the proof construction.