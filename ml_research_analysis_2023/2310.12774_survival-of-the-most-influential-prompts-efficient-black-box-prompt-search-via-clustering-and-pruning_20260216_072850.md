---
ver: rpa2
title: 'Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search
  via Clustering and Pruning'
arxiv_id: '2310.12774'
source_url: https://arxiv.org/abs/2310.12774
tags:
- search
- claps
- prompt
- space
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that only a small fraction of tokens in the
  vocabulary significantly influence large language model predictions when used as
  discrete prompts. This motivates a novel method, Clustering and Pruning for Efficient
  Black-box Prompt Search (CLAPS), which first clusters and prunes the search space
  to focus on these influential tokens, then performs black-box prompt search.
---

# Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning

## Quick Facts
- arXiv ID: 2310.12774
- Source URL: https://arxiv.org/abs/2310.12774
- Reference count: 22
- Primary result: CLAPS achieves state-of-the-art performance across multiple tasks and LLMs using simple search methods after pruning to focus on influential tokens

## Executive Summary
This paper addresses the challenge of efficient prompt search for large language models in a black-box setting. The key insight is that only a small fraction of tokens in the vocabulary significantly influence model predictions, enabling dramatic search space reduction. The proposed CLAPS method combines clustering and pruning to identify and focus on influential tokens, then performs black-box prompt search in the reduced space. Experiments show CLAPS outperforms state-of-the-art methods like RLPrompt while requiring significantly less computational resources.

## Method Summary
CLAPS is a black-box prompt search method that first clusters and prunes the search space to focus on influential prompt tokens. The method computes incremental influence scores (∆R(v)) for tokens using 16-shot validation, then prunes the vocabulary to retain only the top-α% most influential tokens. A black-box search algorithm (evolutionary, greedy, or PSO) is then applied to find optimal K-token prompts. The approach achieves state-of-the-art performance across GLUE benchmark tasks and SNLI using even simple search strategies.

## Key Results
- CLAPS achieves state-of-the-art performance across various tasks and LLMs
- Simple search methods (evolutionary, greedy) outperform complex baselines in the pruned space
- Computational cost reduced by orders of magnitude compared to RLPrompt
- Top 1% of tokens exert disproportionate influence on predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Only a small fraction of tokens in the vocabulary have disproportionate influence on LLM predictions when used as discrete prompts.
- Mechanism: The influence of tokens follows a highly non-uniform distribution where top tokens contribute significantly more to task performance than the majority of tokens.
- Core assumption: The sensitivity analysis measuring incremental reward (∆R(v)) accurately captures each token's contribution to model predictions.
- Evidence anchors:
  - [abstract]: "we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions"
  - [section]: "we find the distribution of influence over the vocabulary of tokens is, in fact, heavily non-uniform, with a small fraction (roughly 1%, marked in green) of all tokens exerting a disproportionate amount of influence"
  - [corpus]: Weak - corpus shows related work on prompt optimization but doesn't directly confirm token influence distribution
- Break condition: If sensitivity analysis methodology fails to capture true token influence, or if token distributions vary significantly across different LLMs or tasks.

### Mechanism 2
- Claim: Pruning the search space to focus only on influential tokens dramatically improves search efficiency without sacrificing performance.
- Mechanism: By removing the vast majority of tokens that either harm or have negligible influence on predictions, the search space becomes exponentially smaller while retaining the most valuable tokens.
- Core assumption: The tokens identified as influential through sensitivity analysis remain influential when combined with other tokens in multi-token prompts.
- Evidence anchors:
  - [abstract]: "we propose the Clustering and Pruning for Efficient Black-box Prompt Search (CLAPS), a simple black-box search method that first clusters and prunes the search space to focus exclusively on influential prompt tokens"
  - [section]: "with a representative K = 5 and if we retain the top-1% tokens in terms of ∆R(v) given by Eq. 3, there is a O(1010) reduction in |P|"
  - [corpus]: Weak - related work mentions prompt pruning but doesn't specifically validate this exponential reduction claim
- Break condition: If influential tokens are task-specific and don't generalize, or if the pruning process removes tokens that are only influential in combination with others.

### Mechanism 3
- Claim: CLAPS achieves state-of-the-art performance using simple search strategies because the pruned search space is already highly optimized.
- Mechanism: The search space reduction from O(10^4) to O(10^2) tokens means that even basic search algorithms can find optimal prompts without getting lost in irrelevant options.
- Core assumption: The quality of the search space matters more than the sophistication of the search algorithm for this problem.
- Evidence anchors:
  - [abstract]: "By employing even simple search methods within the pruned search space, CLAPS achieves state-of-the-art performance across various tasks and LLMs"
  - [section]: "we find that after pruning, even the simplest search strategy (e.g., random or evolutionary search) can outperform state-of-the-art methods with much more complicated search strategies"
  - [corpus]: Weak - related work focuses on complex search strategies but doesn't validate the hypothesis that simpler strategies work better in pruned spaces
- Break condition: If complex search strategies could discover better prompts in the full space, or if the pruned space becomes too restrictive for certain tasks.

## Foundational Learning

- Concept: Combinatorial optimization and discrete search spaces
  - Why needed here: Understanding how the exponential growth of search space (|P| = |V|^K) makes brute-force search infeasible and why pruning is essential
  - Quick check question: If you have a vocabulary of 50,000 tokens and want 5-token prompts, how many possible prompts exist in the full space?

- Concept: Sensitivity analysis and incremental reward calculation
  - Why needed here: The core technique for identifying influential tokens by measuring how much each token changes model performance when added as a prompt
  - Quick check question: How would you modify the incremental reward calculation if you wanted to measure the influence of token pairs instead of individual tokens?

- Concept: Clustering algorithms and vector space representation
  - Why needed here: Used to reduce the vocabulary to a representative subset before pruning, making the influence calculation more efficient
  - Quick check question: What clustering algorithm is used in CLAPS and why is it appropriate for this application?

## Architecture Onboarding

- Component map: Sensitivity analysis module → Clustering module (optional) → Pruning module → Search module → Evaluation module
- Critical path: Sensitivity analysis → Clustering (optional) → Pruning → Search → Evaluation
- Design tradeoffs:
  - Clustering size vs. computational cost: Larger clusters retain more tokens but increase runtime
  - Pruning fraction vs. performance: More aggressive pruning saves time but may lose useful tokens
  - Search algorithm sophistication vs. simplicity: More complex algorithms may help but are less necessary
- Failure signatures:
  - Poor performance on tasks where no single token has high influence
  - Search getting stuck in local optima if pruning is too aggressive
  - Computational cost not reducing if clustering/enumeration is inefficient
- First 3 experiments:
  1. Run sensitivity analysis on a simple task (SST-2) to verify the non-uniform token influence distribution
  2. Test pruning effectiveness by comparing random sampling from full vs. pruned space on RTE
  3. Validate that simple evolutionary search on pruned space matches RLPrompt performance on SNLI

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLAPS compare to other methods when using different search algorithms like particle swarm optimization?
- Basis in paper: [explicit] The paper mentions that CLAPS can be used with various search algorithms and compares the performance of CLAPS with genetics and greedy search algorithms in the main text, while the results with particle swarm optimization are shown in Appendix B.
- Why unresolved: The paper does not provide a direct comparison of CLAPS with particle swarm optimization against other methods like RLPrompt and BDPL in the main text.
- What evidence would resolve it: A direct comparison of CLAPS with particle swarm optimization against other methods like RLPrompt and BDPL in the main text would provide a clearer picture of its performance.

### Open Question 2
- Question: How does the number of clusters in the clustering phase affect the performance of CLAPS?
- Basis in paper: [explicit] The paper mentions that CLAPS uses an optional clustering step to reduce the computational cost and provides an ablation study that conducts the CLAPS pipeline with different numbers of clusters in Appendix B.
- Why unresolved: The paper does not provide a detailed analysis of how the number of clusters affects the performance of CLAPS in the main text.
- What evidence would resolve it: A detailed analysis of the effect of the number of clusters on the performance of CLAPS in the main text would provide a clearer understanding of its impact.

### Open Question 3
- Question: How does the token length in the black-box prompt search stage affect the performance of CLAPS?
- Basis in paper: [explicit] The paper mentions that CLAPS performs black-box prompt search over the reduced search space and provides an ablation study with various token lengths in Appendix B.
- Why unresolved: The paper does not provide a detailed analysis of how the token length affects the performance of CLAPS in the main text.
- What evidence would resolve it: A detailed analysis of the effect of token length on the performance of CLAPS in the main text would provide a clearer understanding of its impact.

## Limitations
- The token influence distribution findings may not generalize across all LLMs and task types
- The method may miss synergistic token combinations where individually low-influence tokens create high-influence combinations
- The claim that simple search strategies are sufficient lacks direct experimental validation

## Confidence
- **High Confidence**: The core experimental results showing CLAPS outperforming baselines on tested tasks and models
- **Medium Confidence**: The generalizability of the token influence distribution findings across different LLMs and task types
- **Low Confidence**: The claim that simple search strategies are sufficient due to the quality of the pruned space

## Next Checks
1. **Cross-Model Generalization Test**: Apply CLAPS to different LLM families (BERT, RoBERTa, LLaMA) on same GLUE tasks to verify token influence distribution consistency
2. **Synergy Detection Experiment**: Design controlled experiment to test whether CLAPS pruning removes tokens only influential in combination with others
3. **Search Strategy Comparison**: Run CLAPS with both simple and complex search strategies on pruned space across multiple tasks to test claimed simplicity advantage