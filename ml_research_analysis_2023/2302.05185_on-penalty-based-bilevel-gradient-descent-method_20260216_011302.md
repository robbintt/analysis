---
ver: rpa2
title: On Penalty-based Bilevel Gradient Descent Method
arxiv_id: '2302.05185'
source_url: https://arxiv.org/abs/2302.05185
tags:
- bilevel
- holds
- problem
- theorem
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles bilevel optimization problems with non-convex
  lower-level objectives by using penalty methods. The authors reformulate the bilevel
  problem into a single-level problem by penalizing the distance between the lower-level
  solution set and the current iterate.
---

# On Penalty-based Bilevel Gradient Descent Method

## Quick Facts
- **arXiv ID**: 2302.05185
- **Source URL**: https://arxiv.org/abs/2302.05185
- **Reference count**: 40
- **Primary result**: Finite-time convergence of penalty-based bilevel gradient descent for non-convex lower-level problems without requiring strong convexity

## Executive Summary
This paper addresses bilevel optimization problems with non-convex lower-level objectives by reformulating them as single-level problems using penalty methods. The authors propose a penalty-based bilevel gradient descent (PBGD) algorithm that achieves finite-time convergence to stationary points for both unconstrained and constrained lower-level problems. The method works without requiring the strong convexity typically assumed in bilevel optimization literature, instead relying on Polyak-Łojasiewicz (PL) inequality or quadratic growth conditions. Experiments demonstrate the efficiency of PBGD compared to several baselines in terms of speed, memory usage, and solution quality.

## Method Summary
The method reformulates bilevel optimization by adding a penalty term that enforces the lower-level optimality constraint. The penalty function measures the distance between the current iterate and the lower-level solution set, with the squared distance serving as the penalty term when the lower-level function satisfies the PL inequality or quadratic growth condition. The PBGD algorithm iteratively solves the lower-level problem approximately and performs projected gradient descent on the penalized objective. The approach establishes finite-time convergence guarantees with complexity dependent on the penalty constant, making it applicable to constrained lower-level problems without requiring strong convexity.

## Key Results
- Proposes PBGD algorithm with finite-time convergence for bilevel problems with non-convex lower-level objectives
- Achieves convergence without requiring strong convexity, instead using PL inequality or quadratic growth conditions
- Demonstrates improved performance compared to baselines in data hyper-cleaning tasks on MNIST dataset
- Establishes conditions under which penalized solutions recover (local) solutions of the original bilevel problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating bilevel optimization via penalty methods recovers (local) solutions of the original problem under generic conditions
- Mechanism: The penalty term enforces lower-level optimality by penalizing squared distance between current iterate and lower-level solution set. As penalty constant increases, penalized objective forces iterates toward solution set
- Core assumption: Lower-level function satisfies PL inequality or has quadratic growth
- Evidence anchors:
  - [abstract]: "we show that under certain conditions, the penalty reformulation recovers the (local) solutions of the original bilevel problem"
  - [section 3.1]: "Lemma 1 shows that the above penalty functions are squared-distance bound functions" with proof using PL inequality
  - [corpus]: Weak evidence - no direct citations to similar penalty reformulation mechanisms
- Break condition: When lower-level objective is neither PL nor has quadratic growth, penalty term may not effectively enforce lower-level optimality

### Mechanism 2
- Claim: Function value gap penalty term g(x,y) - v(x) is Lipschitz-smooth under verifiable conditions
- Mechanism: When ∇ₓg(x,y) is constant over all y in solution set S(x), the function v(x) = min_{y∈U} g(x,y) becomes Lipschitz-smooth, enabling efficient gradient evaluation
- Core assumption: ∇ₓg(x,y) is constant for all y in lower-level solution set S(x)
- Evidence anchors:
  - [section 4.2]: "Lemma 6: Suppose conditions in Lemma 5 hold. Given any x∈C, if ∇ₓg(x,y₁) = ∇ₓg(x,y₂) for any y₁,y₂∈S(x), then ∇v(x) = ∇ₓg(x,y*) for any y*∈S(x)"
  - [Example 2]: Provides concrete case where this condition holds without requiring S(x) to be singleton
  - [corpus]: No direct evidence found in neighbor papers for this specific smoothness condition
- Break condition: When ∇ₓg(x,y) varies across different y in solution set S(x), v(x) becomes non-smooth and penalty-based method loses efficiency

### Mechanism 3
- Claim: PBGD algorithm achieves finite-time convergence to stationary points with complexity dependent on penalty constant
- Mechanism: By iteratively solving lower-level problem approximately and using function value gap as penalty, algorithm converges to stationary points of penalized problem, which under appropriate conditions are also stationary points of original bilevel problem
- Core assumption: Lower-level function is Lipschitz-smooth and satisfies either PL inequality or quadratic growth
- Evidence anchors:
  - [abstract]: "establish its finite-time convergence for the constrained bilevel problem with lower-level constraints yet without lower-level strong convexity"
  - [section 3.4]: Theorem 3 establishes convergence with complexity O(γϵ⁻¹) to find ϵ-stationary point
  - [corpus]: No neighbor papers directly address finite-time convergence for penalty-based bilevel methods without strong convexity
- Break condition: When lower-level function violates smoothness or growth conditions, convergence guarantees no longer hold

## Foundational Learning

- Concept: Polyak-Łojasiewicz (PL) inequality
  - Why needed here: PL inequality is crucial for establishing penalty terms as valid squared-distance bounds and proving convergence rates without requiring strong convexity
  - Quick check question: If g(x,·) satisfies PL inequality with modulus μ, what lower bound can we derive on ∥∇yg(x,y)∥² in terms of g(x,y) - v(x)?

- Concept: Quadratic growth condition
  - Why needed here: Quadratic growth provides alternative to PL inequality for establishing error bounds and convergence when PL doesn't hold, particularly useful for constrained lower-level problems
  - Quick check question: How does the 1/μ-quadratic-growth condition relate to squared-distance bound property of penalty terms?

- Concept: Directional derivatives and Lipschitz-smoothness
  - Why needed here: Understanding when v(x) = min_{y∈U} g(x,y) is Lipschitz-smooth requires analyzing directional derivatives and conditions on ∇ₓg(x,y) across solution set
  - Quick check question: Under what condition on ∇ₓg(x,y) across S(x) does v(x) become Lipschitz-smooth?

## Architecture Onboarding

- Component map: Lower-level problem solver -> Penalty term evaluation -> Projected gradient update on penalized objective
- Critical path:
  1. Verify lower-level function satisfies PL or quadratic growth
  2. Select appropriate penalty term based on verification
  3. Implement inner loop to compute approximate lower-level solution
  4. Implement outer loop with projected gradient updates
  5. Monitor convergence via gradient norm of penalized objective
- Design tradeoffs:
  - PL-based penalty: Requires PL condition but gives tighter bounds and simpler analysis
  - Quadratic growth penalty: More general but requires proximal error bounds
  - Inner loop accuracy: Higher accuracy improves outer loop convergence but increases computation
  - Penalty constant γ: Larger γ improves lower-level accuracy but may require smaller step sizes
- Failure signatures:
  - Divergence: Penalty constant too large relative to problem constants
  - Slow convergence: Insufficient inner loop accuracy or inappropriate penalty term selection
  - Oscillations: Step size too large relative to smoothness constants
  - Stuck at suboptimal points: Lower-level solution set has varying ∇ₓg(x,y) violating smoothness condition
- First 3 experiments:
  1. Test on simple bilevel problem where S(x) is singleton to verify basic functionality
  2. Vary penalty constant γ to observe tradeoff between lower-level accuracy and convergence speed
  3. Compare PL-based vs quadratic growth penalties on problems where both conditions hold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the tight bounds on penalty constant γ needed to achieve ϵ-approximate solutions for non-convex bilevel problems?
- Basis in paper: [explicit] Paper discusses relationship between penalty constants and solution quality in Theorems 1 and 2, and Corollary 1 provides lower bound of γ = Ω(δ^(-0.5)) for achieving ϵ_γ = O(δ)
- Why unresolved: While paper establishes necessary conditions for penalty constant, it does not provide matching upper bounds or explore full range of γ values that guarantee solution quality
- What evidence would resolve it: Comprehensive analysis providing both upper and lower bounds on γ, along with empirical validation across various problem instances

### Open Question 2
- Question: How does choice of penalty function impact convergence and solution quality of bilevel optimization problems?
- Basis in paper: [explicit] Paper explores different penalty functions in Sections 3.1 and 4.1, such as function value gap and squared distance bound, and their respective impacts on convergence
- Why unresolved: Paper provides theoretical analysis for specific penalty functions but does not compare their practical performance or explore other potential penalty functions
- What evidence would resolve it: Comparative studies of various penalty functions in terms of convergence speed, solution quality, and computational efficiency across diverse bilevel problem instances

### Open Question 3
- Question: What are the implications of using stochastic gradients in proposed penalty-based bilevel gradient descent methods?
- Basis in paper: [explicit] Section B.4 extends deterministic V-PBGD algorithm to stochastic case, discussing convergence of stochastic gradients
- Why unresolved: Paper provides convergence analysis for stochastic gradients but does not explore impact of noise levels, batch sizes, or choice of gradient estimators on solution quality and convergence
- What evidence would resolve it: Empirical studies varying noise levels, batch sizes, and gradient estimators to assess their impact on convergence and solution quality in stochastic bilevel optimization

### Open Question 4
- Question: How do proposed methods scale with problem size and complexity in real-world applications?
- Basis in paper: [inferred] Paper discusses computational efficiency and memory usage in Section 5.2, comparing proposed methods with baselines in data hyper-cleaning tasks
- Why unresolved: While paper provides some insights into scalability through experiments, it does not systematically explore scaling behavior with increasing problem size or complexity
- What evidence would resolve it: Systematic scaling studies on synthetic and real-world problems of varying sizes and complexities, measuring runtime, memory usage, and solution quality

## Limitations
- Theoretical guarantees rely heavily on specific assumptions about lower-level problem structure (PL inequality or quadratic growth conditions)
- Limited empirical evidence for constrained lower-level problems, focusing primarily on unconstrained case in experiments
- Performance claims compared to baselines lack comprehensive validation across diverse problem instances

## Confidence
- **High Confidence**: Convergence theory for unconstrained problems when PL inequality holds, as this follows directly from established results in literature
- **Medium Confidence**: Convergence results for constrained problems using quadratic growth conditions, as these extend known techniques but require additional technical assumptions
- **Low Confidence**: Practical performance claims compared to baselines, given limited experimental validation and lack of comparison with state-of-the-art bilevel optimization methods

## Next Checks
1. **Assumption Verification**: For each experimental problem, verify whether lower-level objective actually satisfies PL inequality or quadratic growth conditions required by theory
2. **Robustness Testing**: Evaluate algorithm's performance across wider range of problem instances, including cases where theoretical assumptions are violated to understand practical limitations
3. **Baseline Comparison**: Implement and compare against more recent bilevel optimization methods (e.g., TTSA, PALM) to establish relative performance of penalty-based approach in current context