---
ver: rpa2
title: A Neural Network Based Choice Model for Assortment Optimization
arxiv_id: '2308.05617'
source_url: https://arxiv.org/abs/2308.05617
tags:
- choice
- assortment
- neural
- product
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes neural network architectures for choice modeling
  that capture assortment effects without requiring hand-crafted features or complex
  estimation procedures. The authors develop two neural choice models - Gated-Assort-Net
  and Res-Assort-Net - that map assortment inputs to choice probabilities, along with
  an integer programming formulation for assortment optimization.
---

# A Neural Network Based Choice Model for Assortment Optimization

## Quick Facts
- arXiv ID: 2308.05617
- Source URL: https://arxiv.org/abs/2308.05617
- Reference count: 40
- Key outcome: Neural choice models achieve 92-98% approximation ratios in assortment optimization and outperform traditional models on real datasets while being more flexible to train.

## Executive Summary
This paper introduces neural network architectures for choice modeling that can capture assortment effects without requiring hand-crafted features or complex estimation procedures. The authors develop two models - Gated-Assort-Net and Res-Assort-Net - that map binary assortment vectors directly to choice probabilities. They also provide an integer programming formulation for assortment optimization over these neural models. Extensive experiments on synthetic and real-world data show the neural models perform comparably to or better than traditional choice models like MNL and MCCM, while being more flexible and easier to train.

## Method Summary
The authors propose two neural choice model architectures: Gated-Assort-Net uses a gated softmax operator at the output layer to allow utilities to depend on the assortment, while Res-Assort-Net incorporates residual connections between layers. Both models take binary assortment vectors as input and output choice probabilities. The models are trained using cross-entropy loss on synthetic data generated from various choice models (MNL, MCCM, NP, MMNL) and real-world datasets. For assortment optimization, they formulate the forward pass of the neural network as linear constraints in an integer program, enabling standard MIP solvers to find revenue-maximizing assortments.

## Key Results
- Neural models achieve 92-98% approximation ratios in assortment optimization across various generative models
- Out-of-sample cross-entropy loss on real datasets is lower than traditional benchmarks (MNL, MCCM)
- Neural models better capture "irrational" choice behavior observed in behavioral experiments
- Performance remains stable with deeper networks, but MIP solving time increases significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural choice model can capture assortment effects without hand-crafted features or complex estimation procedures.
- Mechanism: By using the assortment vector as direct input to a neural network and applying a gated softmax operator at the output layer, the model allows the utilities to depend on which products are offered together.
- Core assumption: The assortment vector adequately encodes the relevant context for customer choice behavior.
- Evidence anchors:
  - [abstract] "develop two neural choice models - Gated-Assort-Net and Res-Assort-Net - that map assortment inputs to choice probabilities"
  - [section] "The zL is a function of z0 = S, i.e., the assortment effect is encoded into each zL,i and subsequently in Yi through the neural network layers"
- Break condition: If assortment effects are too complex or high-order interactions are needed beyond what a shallow network can capture.

### Mechanism 2
- Claim: The neural network can approximate complex choice models like NP and MMNL when sufficient training data is available.
- Mechanism: The universal approximation property of neural networks allows them to learn the mapping from assortment to choice probabilities, capturing patterns that parametric models miss.
- Core assumption: The training data is representative of the true choice behavior and sufficiently large.
- Evidence anchors:
  - [abstract] "neural models perform comparably to or better than traditional choice models like MNL and MCCM, while being more flexible and easier to train"
  - [section] "the neural choice models perform consistently well in recovering these true models with a large amount of data, m = 100000"
- Break condition: If training data is too limited or the choice behavior involves unobserved factors that cannot be inferred from assortment alone.

### Mechanism 3
- Claim: The MIP formulation enables assortment optimization over the trained neural network.
- Mechanism: By representing the neural network forward pass as linear constraints with binary decision variables for assortment selection, standard MIP solvers can find revenue-maximizing assortments.
- Core assumption: The neural network is sufficiently small (shallow, narrow) that the resulting MIP is tractable.
- Evidence anchors:
  - [abstract] "develop an integer programming formulation for assortment optimization"
  - [section] "we formulate the assortment optimization problem under the GAsN as follows, max ... s.t. zl − z̃l = Wlzl−1 + bl, for l = 1, ..., L"
- Break condition: If the network becomes too deep or wide, making the MIP intractable even with big-M constraints.

## Foundational Learning

- Concept: Random Utility Models (RUM) and their limitations
  - Why needed here: Understanding RUM provides the theoretical foundation for why assortment effects matter and why neural networks can generalize beyond RUM assumptions.
  - Quick check question: What is the regularity property of RUM models, and how does it differ from observed "irrational" choice behavior?

- Concept: Maximum Likelihood Estimation (MLE) for discrete choice models
  - Why needed here: The paper uses MLE as a benchmark and the neural network training implicitly optimizes a similar likelihood function.
  - Quick check question: How does the cross-entropy loss used in neural network training relate to the negative log-likelihood in MLE?

- Concept: Integer Programming formulations for assortment optimization
  - Why needed here: The MIP formulation is a key contribution that enables optimization over the neural choice model.
  - Quick check question: What is the big-M method and how is it used to linearize the ReLU activation in the MIP?

## Architecture Onboarding

- Component map: Input layer (binary assortment vector) → Hidden layers (fully connected with ReLU or residual connections) → Output layer (gated softmax) → MIP formulation for optimization
- Critical path: Training data → Neural network estimation → MIP formulation → Assortment optimization
- Design tradeoffs: Depth vs. width of network (affects model capacity and MIP tractability), feature inclusion vs. feature-free approach (affects data requirements and model generality)
- Failure signatures: Poor out-of-sample performance suggests overfitting or insufficient model capacity; slow MIP solving suggests network is too large
- First 3 experiments:
  1. Train a 1-layer Gated-Assort-Net on synthetic MNL data and compare CE loss to MNL-MLE
  2. Generate synthetic MCCM data and compare NN(1) vs MCCM-EM estimation quality
  3. Solve the MIP for a small problem instance (n=10) and verify it finds the optimal assortment under known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neural choice models scale when the number of products increases beyond 60, particularly for the MIP-based assortment optimization formulation?
- Basis in paper: [explicit] The paper notes that MIP formulation cannot scale to more than around a hundred products and mentions this as a limitation requiring further research.
- Why unresolved: The paper only tests up to 60 products in experiments, and the computational complexity of MIP formulation is known to grow exponentially with problem size.
- What evidence would resolve it: Experimental results showing performance and computational time for assortment optimization with 100-500 products, or a proposed scalable algorithm for larger product sets.

### Open Question 2
- Question: What is the optimal depth and width configuration for neural choice models that balances model capacity and generalization across different types of choice data?
- Basis in paper: [explicit] The paper discusses that deeper networks lead to overfitting and mentions the need to balance depth and width, but provides limited systematic analysis of optimal configurations.
- Why unresolved: The paper only tests 1-2 layer networks and mentions that performance remains stable with more layers, but doesn't provide guidance on optimal architecture selection for different scenarios.
- What evidence would resolve it: Systematic ablation studies varying network depth (1-10 layers) and width (varying neurons per layer) across multiple synthetic and real datasets to identify optimal configurations.

### Open Question 3
- Question: How do neural choice models perform when incorporating pricing optimization alongside assortment optimization, and what would be an appropriate neural architecture for this extension?
- Basis in paper: [explicit] The paper mentions pricing as an interesting future direction and notes current models assume fixed prices, but doesn't explore this extension.
- Why unresolved: The paper treats price as a fixed parameter and doesn't develop or test models that can optimize both assortment and pricing simultaneously.
- What evidence would resolve it: Experimental results comparing neural choice models with and without price optimization capabilities on datasets with varying price points, showing revenue impact and model performance.

## Limitations
- Model expressiveness vs. optimization tractability trade-off limits network depth and width
- Large training data requirements (100,000+ samples) may not be feasible in many real-world applications
- Feature-free approach cannot capture customer heterogeneity through demographics or contextual features

## Confidence

**High confidence**: The neural architectures (Gated-Assort-Net, Res-Assort-Net) are correctly specified and can be implemented as described

**Medium confidence**: The approximation ratios (92-98%) on synthetic data will generalize to other generative models

**Medium confidence**: The cross-entropy performance improvements on real data will persist with different datasets

## Next Checks

1. **Ablation study on network depth**: Systematically test how approximation ratios degrade as network depth increases beyond 2-3 layers, and measure MIP solving times.

2. **Data efficiency test**: Re-run synthetic experiments with progressively smaller training sets (10k, 1k, 100 samples) to identify the minimum data requirements for maintaining performance.

3. **Feature inclusion experiment**: Modify the neural architecture to incorporate customer demographic features and compare performance against the feature-free baseline on the SwissMetro and Expedia datasets.