---
ver: rpa2
title: Private GANs, Revisited
arxiv_id: '2302.02936'
source_url: https://arxiv.org/abs/2302.02936
tags:
- discriminator
- training
- accuracy
- privacy
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines the canonical approach to training differentially
  private GANs using differentially private stochastic gradient descent (DPSGD) on
  the discriminator. The authors find that this approach suffers from an imbalance
  between the generator and discriminator caused by noise being added only to discriminator
  updates.
---

# Private GANs, Revisited

## Quick Facts
- arXiv ID: 2302.02936
- Source URL: https://arxiv.org/abs/2302.02936
- Reference count: 13
- This paper shows that differentially private GAN training can be improved by taking more discriminator steps between generator steps, achieving state-of-the-art FID scores on MNIST and FashionMNIST.

## Executive Summary
This paper revisits the canonical approach to training differentially private GANs using DPSGD on the discriminator. The authors identify that adding noise only to discriminator updates disrupts the generator-discriminator balance necessary for successful GAN training. By taking more discriminator steps between generator steps, using larger batch sizes, and employing adaptive discriminator step frequency, they significantly improve generation quality on MNIST and FashionMNIST. Their approach achieves state-of-the-art FID scores compared to previous DP GAN approaches, demonstrating that properly tuned DPSGD can outperform custom DP GAN schemes.

## Method Summary
The authors implement a DCGAN architecture and train it using DPSGD with the Opacus library. They modify the standard training recipe by increasing the number of discriminator steps (nD) between generator steps, using larger batch sizes (128), and implementing an adaptive discriminator step frequency that ramps up when discriminator accuracy drops. The privacy budget is tracked using Rényi differential privacy accounting. Experiments are conducted on MNIST and FashionMNIST datasets, comparing FID scores and downstream classification accuracy against previous DP GAN methods.

## Key Results
- Increased discriminator steps (nD) significantly improves generation quality on MNIST and FashionMNIST
- Larger batch sizes (128) further improve results compared to smaller batches
- Adaptive discriminator step frequency provides additional gains over fixed nD schedules
- The proposed method achieves state-of-the-art FID scores compared to previous DP GAN approaches
- Proper tuning of DPSGD hyperparameters outperforms custom DP GAN schemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding noise only to discriminator updates disrupts the balance between generator and discriminator, weakening the discriminator relative to the generator.
- Mechanism: In GAN training, the generator and discriminator must be balanced for successful training. DP noise added only to discriminator gradients makes the discriminator's task of distinguishing real from fake data more difficult, leading to poor discriminator performance and limited generator improvement.
- Core assumption: The generator-discriminator balance is crucial for GAN training success, and asymmetric noise addition disrupts this balance.
- Evidence anchors:
  - [abstract] "Existing instantiations of this approach neglect to consider how adding noise only to discriminator updates disrupts the careful balance between the generator and discriminator necessary for successful GAN training."
  - [section] "The asymmetric noise addition introduced by DP to the discriminator makes such a task difficult, resulting in limited generator improvement."
  - [corpus] Weak - no direct evidence in corpus, but related work on DP-SGD and GANs supports this mechanism.
- Break condition: If the generator can improve without a strong discriminator, or if the noise level is too high to recover discriminator accuracy.

### Mechanism 2
- Claim: Taking more discriminator steps between generator steps restores parity and improves results by allowing the discriminator to train longer on a fixed generator.
- Mechanism: Increasing the frequency of discriminator updates (nD) allows the discriminator to improve its accuracy on the fixed generator before the generator is updated. This recovery of discriminator accuracy enables the generator to receive better gradients and improve.
- Core assumption: More discriminator training on a fixed generator improves discriminator accuracy, which in turn enables better generator training.
- Evidence anchors:
  - [abstract] "We show that a simple fix – taking more discriminator steps between generator steps – restores parity and improves results."
  - [section] "Allowing the discriminator to train longer on a fixed generator improves its accuracy, recovering the non-private case where the generator and discriminator are balanced."
  - [corpus] Weak - no direct evidence in corpus, but related work on GAN training supports this mechanism.
- Break condition: If the optimal nD is too high, preventing the generator from taking enough steps before the privacy budget is exhausted.

### Mechanism 3
- Claim: Larger batch sizes and adaptive discriminator step frequency further improve DPGAN training by effectively using the privacy budget to maximize generator steps when the discriminator has high accuracy.
- Mechanism: Larger batch sizes in DP settings can lead to higher accuracy. Adaptive discriminator step frequency starts with low nD and ramps up when discriminator accuracy drops, ensuring the discriminator is strong when the generator is updated.
- Core assumption: Larger batch sizes and adaptive step frequency can improve discriminator accuracy and generator training in DP settings.
- Evidence anchors:
  - [abstract] "We employ our explanation as a principle for designing better private GAN training recipes, and indeed are able to improve over the aforementioned results."
  - [section] "We experiment with modifications to the private GAN training recipe towards these ends, which translate to improved generation."
  - [corpus] Weak - no direct evidence in corpus, but related work on DP-SGD and adaptive learning rates supports this mechanism.
- Break condition: If the privacy budget is exhausted before the generator can converge, or if the adaptive step frequency is not tuned properly.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: The paper is about training differentially private GANs, so understanding DP is fundamental to understanding the problem and the proposed solutions.
  - Quick check question: What is the definition of (ε, δ)-differential privacy, and how does it relate to the privacy budget?

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The paper is about training GANs, so understanding the GAN framework, including the generator, discriminator, and their training dynamics, is essential.
  - Quick check question: What is the minimax objective in GAN training, and how do the generator and discriminator interact during training?

- Concept: Differential Privacy Stochastic Gradient Descent (DPSGD)
  - Why needed here: The paper uses DPSGD to privatize the discriminator updates, so understanding how DPSGD works and how it affects training is crucial.
  - Quick check question: How does DPSGD privatize gradient updates, and what are the key hyperparameters (clipping norm, noise scale, batch size) that affect its performance?

## Architecture Onboarding

- Component map: Generator (G) -> Discriminator (D) -> DPSGD -> G update
- Critical path: D update (with DPSGD) → G update → D update (with DPSGD) → ...
- Design tradeoffs:
  - nD: Higher nD improves D accuracy but reduces G update frequency
  - Batch size: Larger batch sizes can improve accuracy but require more compute
  - Noise scale: Higher noise improves privacy but reduces accuracy
- Failure signatures:
  - Mode collapse: Generator produces limited variety of outputs
  - Discriminator overfitting: Discriminator achieves near-perfect accuracy on real data
  - Slow convergence: Training takes many iterations to improve generation quality
- First 3 experiments:
  1. Vary nD: Test different frequencies of D updates (e.g., nD = 1, 10, 50) to find the optimal setting for a given privacy budget.
  2. Vary batch size: Test different batch sizes (e.g., B = 128, 512, 2048) to see how it affects training in the DP setting.
  3. Vary noise scale: Test different noise scales (e.g., σ = 0.6, 1.0, 1.4) to find the best tradeoff between privacy and accuracy for a given batch size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal discriminator step frequency (nD) for different privacy budgets (ε) and datasets?
- Basis in paper: [explicit] The paper shows that nD significantly impacts generation quality and varies between datasets (e.g., nD=50 for MNIST vs nD=100-200 for FashionMNIST), but doesn't provide a general formula or comprehensive analysis across different privacy budgets.
- Why unresolved: The paper only explores a limited range of nD values (1-200) and focuses on two specific privacy levels (ε=1 and ε=10). The relationship between nD, ε, dataset complexity, and other hyperparameters remains unclear.
- What evidence would resolve it: Systematic experiments varying ε, dataset size/complexity, batch size, and other hyperparameters while measuring optimal nD would establish clearer guidelines for selecting this parameter.

### Open Question 2
- Question: Does the proposed adaptive discriminator step frequency method converge to the optimal nD faster than grid search?
- Basis in paper: [explicit] The paper introduces an adaptive step frequency method but only compares it to results from extensive hyperparameter search, not to its own convergence speed.
- Why unresolved: The paper doesn't analyze how quickly the adaptive method reaches good performance compared to fixed nD settings or whether it avoids suboptimal local minima.
- What evidence would resolve it: Experiments tracking FID/accuracy over training steps for both adaptive and fixed nD methods would show convergence speed differences and stability properties.

### Open Question 3
- Question: How does the clipping norm C affect the optimal noise scale σ and discriminator step frequency nD?
- Basis in paper: [explicit] The paper fixes C=1 throughout experiments but notes that clipping affects gradient sensitivity and noise requirements.
- Why unresolved: The paper doesn't explore how varying C would change the relationship between σ and nD, or whether larger/smaller clipping norms would require different hyperparameter tuning strategies.
- What evidence would resolve it: Experiments systematically varying C while measuring optimal σ and nD combinations would reveal how clipping interacts with other privacy parameters.

## Limitations
- The analysis of why DPGAN training breaks rests on theoretical mechanisms that lack direct empirical validation
- The proposed adaptive discriminator step frequency appears somewhat heuristic without clear theoretical justification
- The study is limited to MNIST and FashionMNIST datasets, and results may not generalize to more complex data

## Confidence
- **High confidence**: The experimental results showing improved FID scores with increased discriminator steps and larger batch sizes
- **Medium confidence**: The mechanism explaining that noise asymmetry disrupts generator-discriminator balance
- **Low confidence**: The claim that this is the "canonical" failure mode of DPGANs and that the proposed modifications constitute a fundamental solution

## Next Checks
1. **Ablation study on noise asymmetry**: Train a DPGAN where noise is added to both generator and discriminator gradients (using privacy budget for both) to test if this eliminates the need for increased discriminator steps.

2. **Privacy accounting verification**: Implement and compare both Rényi DP (Mironov et al. 2019) and zero-Concentrated DP (Gopi et al. 2021) accounting methods to verify the privacy guarantees and ensure fair comparison with prior work.

3. **Cross-dataset generalization**: Test the proposed modifications (nD, batch size, adaptive steps) on more complex datasets like CIFAR-10 or LSUN to evaluate whether the mechanisms scale beyond simple MNIST/FashionMNIST settings.