---
ver: rpa2
title: Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series
arxiv_id: '2307.10923'
source_url: https://arxiv.org/abs/2307.10923
tags:
- loss
- data
- dataset
- learning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Sequential Multi-Dimensional Self-Supervised
  Learning (SMD SSL) for multimodal clinical time series data. The method addresses
  the challenge of learning representations from sequences containing both structured
  features and high-dimensional physiological signals.
---

# Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series

## Quick Facts
- arXiv ID: 2307.10923
- Source URL: https://arxiv.org/abs/2307.10923
- Reference count: 39
- Key outcome: SMD SSL improves performance over baselines in both unimodal and multimodal settings, with consistent improvements across different self-supervised loss functions like SimCLR and VICReg.

## Executive Summary
This paper proposes Sequential Multi-Dimensional Self-Supervised Learning (SMD SSL) for multimodal clinical time series data. The method addresses the challenge of learning representations from sequences containing both structured features and high-dimensional physiological signals. SMD SSL uses a two-level loss function: a global loss applied at the trajectory level and a component loss applied at the individual signal level. This approach allows the model to capture information at both scales. The method is evaluated on two real-world clinical datasets using two downstream tasks: detecting elevated pulmonary pressures and predicting 24-hour mortality. Results show that SMD SSL improves performance over baselines in both unimodal and multimodal settings, with consistent improvements across different self-supervised loss functions like SimCLR and VICReg.

## Method Summary
SMD SSL is a self-supervised learning framework for multimodal clinical time series that uses a two-level loss function. The method applies a global loss at the trajectory level and a component loss at the individual signal level, allowing it to capture information at both scales. The approach is agnostic to the specific form of loss function used at each level, supporting both contrastive losses (SimCLR) and non-contrastive losses (VICReg). Data augmentation involves independently transforming each modality within a trajectory. The method is evaluated on clinical time series datasets containing structured features, vitals signs, lab values, and high-dimensional physiological signals like ECGs.

## Key Results
- SMD SSL consistently improves performance over baselines in both unimodal and multimodal settings
- Improvements are observed across different self-supervised loss functions (SimCLR and VICReg)
- The method shows particular benefit when learning from high-dimensional physiological signals
- Optimal performance is achieved by balancing the component and global loss terms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-level loss function enables the model to capture information at both the individual signal and sequence levels, improving representation learning for multimodal clinical time series.
- Mechanism: By applying separate self-supervised losses at the component (signal) level and global (trajectory) level, the model learns richer representations that encode both fine-grained signal features and higher-level temporal patterns. The component loss prevents the global loss from being dominated by structured data features alone.
- Core assumption: High-dimensional signals and structured features provide complementary information that requires separate learning objectives to capture effectively.
- Evidence anchors:
  - [abstract]: "where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales."
  - [section]: "Our approach builds on multi-view SSL... since prior work has successfully used these strategies on clinical data... SMD SSL uses a loss function with two terms – a global loss, computed at the trajectory level, and a component loss, computed at the individual signal level."
  - [corpus]: Weak - no direct citations about multi-level SSL for clinical time series found in related papers.
- Break condition: If the structured data dominates the learning process, the component loss becomes redundant and offers no benefit.

### Mechanism 2
- Claim: The two-level loss formulation allows SMD SSL to be agnostic to the specific form of loss function used at each level, enabling flexibility across different applications.
- Mechanism: By decoupling the global and component losses, SMD SSL can be instantiated with either contrastive losses (SimCLR) or non-contrastive losses (VICReg), allowing adaptation to different data characteristics and downstream tasks.
- Core assumption: Different loss functions may be more effective for different types of clinical time series data or tasks.
- Evidence anchors:
  - [abstract]: "Our strategy is agnostic to the specific form of loss function used at each level – it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg."
  - [section]: "Flexibility in the form of the loss function is beneficial since different applications might benefit from different losses."
  - [corpus]: Weak - related papers focus on single loss function approaches rather than multi-level agnostic frameworks.
- Break condition: If one specific loss function consistently outperforms all others across all tasks, the flexibility advantage diminishes.

### Mechanism 3
- Claim: The augmentation strategy for trajectories, which independently transforms each data modality, provides effective views for contrastive learning while maintaining computational efficiency.
- Mechanism: By applying modality-specific augmentations (signal splitting/masking for ECGs, noise/cutout for structured data), the model learns robust representations that are invariant to common variations in clinical data while preserving the complementary information across modalities.
- Core assumption: Independent augmentation of each modality preserves the multimodal structure while creating effective positive pairs for contrastive learning.
- Evidence anchors:
  - [section]: "We form an augmented trajectory by separately augmenting each of the data modalities within the trajectory, using the following approach for each data type."
  - [section]: "The intuition is that two segments of a signal that are close in time should encode similar physiology, and can therefore be considered paired views."
  - [corpus]: Weak - no direct citations about augmentation strategies for multimodal clinical time series found in related papers.
- Break condition: If cross-modal augmentations (e.g., correlating ECG segments with structured data variations) prove more effective than independent modality augmentation.

## Foundational Learning

- Concept: Self-supervised learning for time series
  - Why needed here: Clinical time series data is abundant but labeled data is scarce, making self-supervised pre-training essential for effective representation learning.
  - Quick check question: What is the key difference between supervised and self-supervised learning in the context of clinical time series?

- Concept: Multimodal representation learning
  - Why needed here: Clinical time series often contain both high-dimensional signals (like ECGs) and structured features (like vitals), requiring models that can effectively integrate information across modalities.
  - Quick check question: Why is it challenging to extend unimodal SSL methods to multimodal clinical time series?

- Concept: Contrastive vs non-contrastive self-supervised learning
  - Why needed here: SMD SSL can be instantiated with either contrastive losses (SimCLR) or non-contrastive losses (VICReg), requiring understanding of when each approach is more appropriate.
  - Quick check question: What is the fundamental difference between contrastive and non-contrastive self-supervised learning approaches?

## Architecture Onboarding

- Component map:
  Static features encoder (2-layer MLP) → 128-dim embedding
  Signals encoder (ResNet-18 CNN) → 128-dim embedding per timestep
  Sequence model (4-layer GRU) → trajectory-level embedding
  Projection heads (2-layer MLP with batch norm) → contrastive space
  Component loss (signal-level) and Global loss (trajectory-level)

- Critical path:
  1. Input trajectory → static features + sequence of (structured data, signal)
  2. Per-timestep: structured data → MLP, signal → CNN → concatenate
  3. Sequence → GRU → trajectory embedding
  4. Compute component loss (signal projections) and global loss (trajectory projections)
  5. Combine losses with weights α and β

- Design tradeoffs:
  - Using separate encoders for static features and signals allows modality-specific feature extraction but increases model complexity
  - The two-level loss approach captures both signal and sequence information but requires careful hyperparameter tuning of loss weights
  - Modality-independent augmentation is simple but may not be optimal compared to coordinated cross-modal augmentations

- Failure signatures:
  - If component loss weight is too low: model focuses only on structured data, ignoring signal information
  - If global loss weight is too low: model fails to capture temporal patterns across the sequence
  - If augmentation is too aggressive: model cannot learn meaningful representations due to information loss

- First 3 experiments:
  1. Compare unimodal performance (signals only) with RandInit baseline to establish pre-training benefit
  2. Test different loss function combinations (component-only, global-only, SMD SSL) to find optimal configuration
  3. Evaluate multimodal performance by comparing structured data only, signals only, and full multimodal inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would SMD SSL perform with different data augmentation strategies or multiview generation approaches?
- Basis in paper: [explicit] The paper mentions that their approach of forming augmented trajectories by independently transforming each individual data type is straightforward but not necessarily optimal, and exploring other strategies is an important direction of future work.
- Why unresolved: The paper uses existing effective data augmentations for clinical data but does not investigate novel augmentation strategies or different multiview generation approaches.
- What evidence would resolve it: Empirical comparisons of SMD SSL performance using various augmentation strategies and multiview generation methods on the same datasets and tasks.

### Open Question 2
- Question: Would a more thorough hyperparameter search, perhaps using efficient gradient-based methods, improve performance of SMD SSL with VICReg?
- Basis in paper: [explicit] The paper notes that when using VICReg, improvements are less consistent and component-only VICReg often performs best, possibly because VICReg has many loss weighting terms that were not jointly tuned with the component loss weight in SMD SSL.
- Why unresolved: The paper conducted a reduced hyperparameter search due to computational expense and did not jointly tune VICReg loss weights with the component loss weight.
- What evidence would resolve it: A comprehensive hyperparameter search for SMD SSL with VICReg, including joint tuning of all loss weights, and comparison of results to current findings.

### Open Question 3
- Question: How would SMD SSL perform when extended to include additional data modalities such as medical imaging?
- Basis in paper: [explicit] The paper discusses that SMD SSL could be readily extended to scenarios with additional modalities like medical imaging, and exploring this could be a valuable direction.
- Why unresolved: The experiments focused on clinical time series consisting of structured data and high-dimensional physiological signals, not including predictive information from other modalities.
- What evidence would resolve it: Empirical evaluation of SMD SSL performance when applied to multimodal data including medical images, compared to current results with only structured data and physiological signals.

## Limitations
- The augmentation strategy uses independent transformations rather than coordinated cross-modal augmentations, which may not be optimal for capturing multimodal relationships
- The paper doesn't extensively compare against other state-of-the-art self-supervised learning methods for time series data
- Ablation studies focus primarily on loss function components rather than exploring the full design space of hyperparameters, architectures, or augmentation strategies

## Confidence

- High confidence: The core mechanism of using two-level loss functions (component and global) for multimodal clinical time series is well-supported by the empirical results showing consistent improvements across different tasks and loss functions.
- Medium confidence: The flexibility claim regarding agnosticism to specific loss function forms is supported but could benefit from more extensive comparison across diverse loss functions and clinical tasks.
- Low confidence: The optimality of the specific augmentation strategy (independent modality transformations) is not well-established, as the paper doesn't compare against alternative augmentation approaches or cross-modal augmentation strategies.

## Next Checks

1. Conduct ablation studies varying the component loss weight (α) across a wider range to identify the optimal balance between component and global losses for different clinical tasks and data modalities.

2. Compare the independent modality augmentation strategy against cross-modal augmentation approaches to determine if coordinated transformations improve representation learning for multimodal clinical time series.

3. Test the SMD SSL framework with additional loss functions beyond SimCLR and VICReg (such as Barlow Twins or DINO) to evaluate the claimed flexibility and identify which loss functions work best for different types of clinical time series data.