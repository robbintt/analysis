---
ver: rpa2
title: 'SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models'
arxiv_id: '2306.14066'
source_url: https://arxiv.org/abs/2306.14066
tags:
- ensemble
- seeds
- lead
- gefs-2
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEEDS (Scalable Ensemble Envelope Diffusion Sampler) addresses
  the computational cost of generating large weather forecast ensembles for uncertainty
  quantification. The method uses diffusion models to emulate physics-based ensemble
  forecasts by conditioning on a small number of seed forecasts (as few as 2 members).
---

# SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models

## Quick Facts
- arXiv ID: 2306.14066
- Source URL: https://arxiv.org/abs/2306.14066
- Reference count: 40
- Key outcome: Diffusion models generate realistic weather ensembles conditioned on few physics-based seeds, achieving 10× computational speedup while matching or exceeding operational forecast skill

## Executive Summary
SEEDS (Scalable Ensemble Envelope Diffusion Sampler) addresses the computational cost of generating large weather forecast ensembles for uncertainty quantification. The method uses diffusion models to emulate physics-based ensemble forecasts by conditioning on a small number of seed forecasts (as few as 2 members). Once trained, the model can generate hundreds of realistic weather forecasts at less than 1/10th the computational cost of operational systems. Generated ensembles match or exceed the skill of full physics-based ensembles in predictive skill metrics (RMSE, ACC, CRPS) and show improved reliability and accuracy in forecasting extreme events. This hybrid AI-physics framework offers a scalable alternative to current ensemble forecasting while maintaining forecast quality.

## Method Summary
SEEDS trains a diffusion model on GEFS reforecast data to learn the distribution of atmospheric states. The model conditions generation on K seed forecasts (typically K=2) from an operational ensemble system, then generates N additional ensemble members (N=128-512) that approximate the full ensemble distribution. A generative post-processing variant blends the physics-based forecasts with reanalysis data to correct systematic biases. The model uses a Vision Transformer with axial attention for spatial, field, and temporal dimensions, operating on cubed sphere grid data. Training uses score matching on perturbed samples with diffusion time conditioning, and inference generates samples in under 3 minutes on TPUv3 hardware.

## Key Results
- Generated ensembles achieve higher correlation with full GEFS-31 than GEFS-2 seed ensemble in medium-range forecasts (4-10 days)
- Computational speedup of 10× achieved by generating 128 ensemble members in <3 minutes versus hours for physics-based forecasts
- Improved extreme event forecasting with lower Brier scores and log-loss for ±2σ events compared to operational ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can efficiently generate realistic weather ensemble members by learning the score function of the data distribution conditioned on seed forecasts
- Mechanism: The diffusion process iteratively denoises random samples using a learned score network that captures the statistical structure of atmospheric states. By conditioning on seed forecasts, the model generates additional ensemble members that maintain realistic spatial and temporal correlations
- Core assumption: The atmospheric state distribution is learnable by a diffusion model and can be conditioned on a small number of seed forecasts
- Evidence anchors:
  - [abstract] "Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system."
  - [section 2.3] "The use of probabilistic diffusion models to parameterize the target distributions, conditioned on a few 'seeds', is at the core of our statistical modeling algorithm"
- Break condition: If the atmospheric state distribution has features that cannot be captured by the diffusion model architecture or training procedure

### Mechanism 2
- Claim: Generative post-processing corrects systematic biases in physics-based ensemble forecasts by blending with reanalysis data
- Mechanism: The diffusion model is trained to approximate a mixture distribution combining the physics-based forecast distribution with reanalysis data. This allows the model to generate ensemble members that correct known biases while maintaining forecast uncertainty
- Core assumption: The reanalysis data represents the true atmospheric state distribution and can be used to correct biases in physics-based forecasts
- Evidence anchors:
  - [abstract] "When designed to correct biases present in the operational forecasting system, the generated ensembles show improved probabilistic forecast metrics. They are more reliable and forecast probabilities of extreme weather events more accurately."
  - [section 2.1] "In generative post-processing, the sampler generates N > K samples such that they approximate a mixture distribution where p(v) is just one of the components."
- Break condition: If the bias structure is too complex or the reanalysis data itself contains systematic errors

### Mechanism 3
- Claim: Diffusion models can capture dynamical features beyond model climatology by leveraging conditioning on seed forecasts
- Mechanism: The generative model learns to represent forecast uncertainty that goes beyond simple climatology by conditioning on specific seed forecasts. This allows generation of ensemble members that capture short and medium-range dynamical features that vary from forecast to forecast
- Core assumption: The conditioning information in seed forecasts contains enough information to guide the generation of dynamical features beyond climatology
- Evidence anchors:
  - [section 3.4] "However, in the medium range (more than 4 days but less than 10 days ahead), the generative ensembles display a higher correlation with the GEFS-FULL than both the model climatology and GEFS-2."
  - [section 3.4] "This suggests that the generative models are indeed able to generate information beyond the two conditioning forecasts."
- Break condition: If the conditioning information is insufficient to distinguish between different forecast scenarios

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: Understanding how diffusion models learn to denoise and generate samples from complex distributions is fundamental to grasping the methodology
  - Quick check question: How does the score function relate to the data distribution in diffusion models?

- Concept: Ensemble forecasting and uncertainty quantification
  - Why needed here: The motivation for using diffusion models stems from limitations in traditional ensemble forecasting approaches
  - Quick check question: Why are traditional ensemble forecasts computationally expensive and what limitations do they face?

- Concept: Atmospheric state representation and preprocessing
  - Why needed here: The model operates on standardized climatological anomalies of atmospheric variables, requiring understanding of meteorological data preprocessing
  - Quick check question: Why are climatological anomalies used instead of raw atmospheric values in this approach?

## Architecture Onboarding

- Component map:
  Data pipeline: GEFS reforecasts → standardization → cubed sphere grid → model input
  Model architecture: Axial Vision Transformer with spatial, field, and sequence attention
  Training loop: Score matching on perturbed samples with diffusion time conditioning
  Generation pipeline: Seed forecasts → diffusion sampling → ensemble output

- Critical path:
  1. Load and preprocess GEFS reforecast data
  2. Train diffusion model with score matching objective
  3. Generate ensemble members conditioned on seed forecasts
  4. Post-process and evaluate generated ensembles

- Design tradeoffs:
  - Patch size vs. spatial resolution: Larger patches reduce computational cost but may miss fine-scale features
  - Number of conditioning seeds: More seeds improve generation quality but reduce computational savings
  - Ensemble size: Larger ensembles improve reliability but increase computational cost

- Failure signatures:
  - Mode collapse: Generated ensembles lack diversity or fail to capture extreme events
  - Bias amplification: Post-processing introduces new biases instead of correcting existing ones
  - Physical inconsistency: Generated samples violate known physical constraints or correlations

- First 3 experiments:
  1. Train a simple diffusion model on a single atmospheric variable to verify basic functionality
  2. Generate ensembles conditioned on K=2 seeds and compare with GEFS-2 ensemble skill
  3. Implement and test the post-processing variant with ERA5 blending to verify bias correction capability

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise mechanism by which the diffusion model captures dynamical information beyond the K seed ensemble members, particularly in the medium-range forecasts (4-10 days)?
  - Basis in paper: [inferred] The paper states "the diffusion models are learning to emulate dynamically-relevant features beyond model biases" and shows that generative ensembles achieve higher correlation with GEFS-FULL than GEFS-2 or climatology in the medium range, but the specific mechanisms are not explained.
  - Why unresolved: The paper identifies this as an important finding but doesn't provide detailed analysis of what dynamical features are being captured or how the model architecture enables this.
  - What evidence would resolve it: Detailed analysis of what specific atmospheric patterns or dynamical features the diffusion model captures beyond the seed members, and comparison of attention patterns in the transformer to known dynamical modes.

- **Open Question 2**: How does the performance of SEEDS scale with increasing ensemble size beyond N=128 members, particularly for extreme event prediction?
  - Basis in paper: [explicit] The paper explores N up to 512 members in Appendix C.3.1 but notes that "increasing N offers diminishing marginal gains beyond N=128" for most metrics, though extreme event metrics like LOGLOSS at -2σ still improve.
  - Why unresolved: The paper provides limited results beyond N=128 and doesn't explore very large ensemble sizes that might be needed for rare event prediction (e.g., 10,000 members for 1% events).
  - What evidence would resolve it: Comprehensive testing of SEEDS performance with ensemble sizes ranging from 128 to 10,000 members, particularly focusing on extreme event skill metrics and computational efficiency scaling.

- **Open Question 3**: What are the limitations of SEEDS when applied to different weather regimes or geographical regions, and how does performance vary across different atmospheric variables?
  - Basis in paper: [inferred] The paper provides results averaged globally and for a subset of variables, but doesn't examine regional performance variations or regime-dependent behavior.
  - Why unresolved: The paper focuses on global average metrics and doesn't explore spatial or regime-dependent variations in model performance, which could be important for operational applications.
  - What evidence would resolve it: Detailed regional analysis of SEEDS performance across different climate zones, examination of performance during specific weather regimes (e.g., blocking patterns, tropical cyclones), and comprehensive evaluation across all atmospheric variables in the dataset.

## Limitations

- Computational domain and resolution constraints limit testing to 6×48×48 cubed sphere grid, with scalability claims to higher resolutions untested
- Performance generalization across weather regimes and geographical regions is not established beyond North American winter patterns
- Dependence on seed forecast quality creates fundamental limitations, though the method shows benefits even with as few as 2 seeds

## Confidence

**High confidence**: Claims about computational efficiency gains (10× speedup), basic ensemble generation capability, and improved reliability metrics (rank histogram δ) have direct experimental support from the paper's ablation studies and baseline comparisons.

**Medium confidence**: Claims about extreme event forecasting improvements and skill superiority over GEFS-FULL are supported by specific metric comparisons but are limited to particular forecast horizons and weather patterns. The generative post-processing bias correction mechanism shows promise but has limited ablation study validation.

**Low confidence**: Scalability to operational global forecasting systems at high resolution, generalization to all weather regimes and geographical regions, and long-term stability of generated ensembles beyond the 2001-2018 training period are not demonstrated.

## Next Checks

1. **Resolution scalability test**: Implement SEEDS at operational resolution (0.25° or higher) and measure actual computational savings versus claimed 1/10th cost reduction. Compare forecast skill degradation at higher resolutions.

2. **Weather regime generalization**: Train and evaluate SEEDS on tropical and monsoon regions, and test performance on rare extreme events (e.g., hurricanes, atmospheric rivers) outside the 2001-2018 North American winter training distribution.

3. **Seed dependency analysis**: Systematically vary K from 1 to 10 seeds and measure the relationship between seed quality/quantity and generated ensemble skill across all forecast lead times. Identify the minimum viable seed count for operational use.