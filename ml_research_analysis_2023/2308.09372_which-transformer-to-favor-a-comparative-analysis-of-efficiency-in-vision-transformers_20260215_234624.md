---
ver: rpa2
title: 'Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision
  Transformers'
arxiv_id: '2308.09372'
source_url: https://arxiv.org/abs/2308.09372
tags:
- attention
- vision
- efficiency
- efficient
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a large-scale benchmark of more than 45 Vision
  Transformer models for image classification, evaluating efficiency across multiple
  dimensions: accuracy, speed, memory usage, and number of parameters. The study reveals
  that despite claims of superior efficiency, a well-trained ViT remains Pareto optimal
  across multiple metrics.'
---

# Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers

## Quick Facts
- arXiv ID: 2308.09372
- Source URL: https://arxiv.org/abs/2308.09372
- Reference count: 11
- Well-trained ViT remains Pareto optimal across multiple efficiency metrics despite newer architectures

## Executive Summary
This paper presents a comprehensive benchmark of over 45 Vision Transformer models for image classification, evaluating efficiency across accuracy, speed, memory usage, and parameter count. The study systematically compares various transformer architectures including standard ViTs, hybrid attention-CNN models, and specialized efficient variants. Through standardized training and evaluation procedures, the research identifies key efficiency patterns and provides practical guidance for model selection based on specific deployment requirements.

## Method Summary
The study benchmarks Vision Transformer models using the ImageNet-1k dataset with a standardized training pipeline consisting of 90 epochs pretraining on ImageNet-21k followed by 50 epochs fine-tuning on ImageNet-1k. Models are evaluated across multiple dimensions including accuracy, throughput (images/second), VRAM usage during training and inference, and parameter count. The benchmark includes more than 45 different transformer architectures, ranging from standard ViTs to hybrid attention-CNN models and specialized efficient variants, all trained under consistent hyperparameters with adjustments made only when necessary to address training instability.

## Key Results
- A well-trained ViT remains Pareto optimal across multiple efficiency metrics despite claims of superior efficiency from newer architectures
- Hybrid attention-CNN models demonstrate exceptional performance in terms of inference memory and parameter efficiency
- Scaling model size is generally more efficient than using higher resolution images due to better hardware parallelization
- Strong positive correlation (0.85) observed between FLOPS and training memory, enabling VRAM estimation from theoretical measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard ViT remains Pareto optimal across multiple efficiency metrics despite newer architectures.
- Mechanism: A well-trained ViT on the updated DeiT training pipeline achieves high accuracy while maintaining competitive throughput and memory usage. Its full self-attention mechanism, though theoretically expensive, is effectively optimized on modern hardware and balanced by model scaling benefits.
- Core assumption: Training pipeline updates and hyperparameter tuning can close the efficiency gap between ViT and newer methods.
- Evidence anchors:
  - [abstract] "we discover that ViT is still Pareto optimal across multiple efficiency metrics"
  - [section] "a well-trained ViT remains Pareto optimal, showcasing remarkable efficiency in terms of throughput while preserving its high accuracy"
- Break condition: If the training pipeline is not optimized for ViT's full self-attention, newer architectures with reduced attention complexity may outperform ViT in practical throughput and memory usage.

### Mechanism 2
- Claim: Hybrid attention-CNN models excel in low inference memory and parameter efficiency.
- Mechanism: Combining convolutional sub-sampling with attention reduces the token sequence length and computational cost, enabling efficient information extraction without large model sizes. Convolutional layers focus on local patterns, while attention captures global dependencies.
- Core assumption: Hybrid architectures can effectively leverage the strengths of both CNNs and transformers without introducing significant overhead.
- Evidence anchors:
  - [abstract] "hybrid attention-CNN models exhibit remarkable inference memory- and parameter-efficiency"
  - [section] "EfficientFormerV2-S0 and CoaT-Ti... exhibit the highest accuracy per parameter, significantly outperforming other attention-based models"
- Break condition: If the hybrid model introduces excessive computational overhead in the attention mechanism or the CNN layers are not well-integrated, the efficiency gains may be negated.

### Mechanism 3
- Claim: Scaling model size is more efficient than using higher resolution images.
- Mechanism: Larger models can parallelize operations more effectively, resulting in higher throughput despite increased FLOPs. Higher resolution images require more tokens, increasing computational cost quadratically without proportional accuracy gains.
- Core assumption: Hardware acceleration and parallelization benefits of larger models outweigh the additional computational cost.
- Evidence anchors:
  - [abstract] "our benchmark shows that using a larger model in general is more efficient than using higher resolution images"
  - [section] "opting for the next larger model turns out to be more efficient. Although a larger model may involve more floating-point operations, these are parallelized more effectively"
- Break condition: If hardware limitations prevent effective parallelization, or if the dataset does not benefit from increased model capacity, higher resolution images may become more efficient.

## Foundational Learning

- Concept: Computational complexity of self-attention
  - Why needed here: Understanding the O(NÂ²) complexity explains why many efficient ViT variants focus on reducing token sequence length or approximating attention.
  - Quick check question: What is the computational complexity of the self-attention mechanism in terms of the number of tokens N?

- Concept: Pareto optimality
  - Why needed here: Identifying Pareto optimal models requires understanding the trade-offs between accuracy, speed, and memory usage.
  - Quick check question: In the context of model efficiency, what does it mean for a model to be Pareto optimal?

- Concept: Hybrid attention-CNN architectures
  - Why needed here: Hybrid models are shown to be efficient in memory and parameter usage, so understanding their design is crucial for model selection.
  - Quick check question: How do hybrid attention-CNN models reduce computational complexity compared to standard ViTs?

## Architecture Onboarding

- Component map:
  Input -> Token embedding and positional encoding -> Token mixing (attention or alternative) -> Feed-forward MLP processing -> Classification head

- Critical path:
  1. Token embedding and positional encoding
  2. Token mixing (attention or alternative)
  3. Feed-forward MLP processing
  4. Classification head

- Design tradeoffs:
  - Attention vs. CNN: Attention captures global dependencies but is computationally expensive; CNNs are efficient but may miss long-range interactions.
  - Token reduction vs. accuracy: Reducing tokens improves efficiency but may lose important information.
  - Model size vs. resolution: Larger models may be more efficient than higher resolution inputs due to better parallelization.

- Failure signatures:
  - Low accuracy despite high parameter count: Possible overfitting or ineffective attention mechanism.
  - High memory usage with low throughput: Inefficient token mixing or excessive token sequence length.
  - Training instability: Hyperparameter issues or architectural incompatibilities.

- First 3 experiments:
  1. Compare throughput and accuracy of ViT vs. hybrid models on same hardware.
  2. Measure memory usage during inference for models with different token reduction strategies.
  3. Evaluate scaling efficiency by training models of different sizes at standard vs. high resolution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the efficiency gains of ViTs compare to hybrid attention-CNN models in real-world deployment scenarios with limited computational resources?
- Basis in paper: [explicit] The paper states that hybrid attention-CNN models exhibit remarkable inference memory- and parameter-efficiency, suggesting they may be more suitable for resource-constrained environments.
- Why unresolved: While the paper provides theoretical comparisons, it does not explicitly test these models in real-world deployment scenarios to validate their efficiency gains.
- What evidence would resolve it: Empirical studies comparing the performance of ViTs and hybrid attention-CNN models in real-world deployment scenarios with varying levels of computational resources.

### Open Question 2
- Question: What are the underlying factors contributing to the strong correlation between FLOPS and training memory, and how can this relationship be leveraged to optimize model training?
- Basis in paper: [explicit] The paper identifies a strong positive correlation (0.85) between FLOPS and training memory, suggesting that VRAM requirements can be estimated from theoretical measurements alone.
- Why unresolved: The paper does not delve into the specific factors driving this correlation or explore how this relationship can be utilized to optimize model training processes.
- What evidence would resolve it: Detailed analysis of the factors contributing to the FLOPS-training memory correlation and studies on how this relationship can be leveraged to optimize model training strategies.

### Open Question 3
- Question: How do the efficiency gains of efficient ViTs translate to performance on diverse vision tasks beyond image classification, such as object detection and semantic segmentation?
- Basis in paper: [inferred] The paper focuses on image classification tasks, but efficient ViTs have potential applications in other vision tasks. The efficiency gains observed in image classification may not necessarily translate to other tasks.
- Why unresolved: The paper does not evaluate the performance of efficient ViTs on diverse vision tasks, leaving the question of their effectiveness in other domains unanswered.
- What evidence would resolve it: Empirical studies comparing the performance of efficient ViTs on various vision tasks, including object detection and semantic segmentation, to assess the generalizability of their efficiency gains.

## Limitations

- Findings are derived from a single dataset (ImageNet-1k) and classification task, potentially limiting applicability to other vision domains
- Computational efficiency measurements depend heavily on specific hardware configuration (NVIDIA A100 GPUs) and may vary significantly on other hardware
- Training pipeline involved hyperparameter adjustments for certain models experiencing instability, introducing potential variability in baseline comparisons

## Confidence

- High Confidence: ViT's Pareto optimality claim - supported by comprehensive benchmarking across multiple models and consistent with established training pipelines
- Medium Confidence: Hybrid models' efficiency advantages - while measurements are robust, the generalization to other tasks and datasets remains uncertain
- Medium Confidence: Model scaling efficiency - the conclusion depends on specific hardware parallelization capabilities that may not transfer to all deployment scenarios

## Next Checks

1. **Cross-Dataset Validation**: Replicate key efficiency comparisons on domain-specific datasets (medical imaging, satellite imagery) to assess generalizability beyond ImageNet classification.

2. **Hardware Portability Analysis**: Benchmark the top-performing models on multiple hardware configurations (including consumer GPUs and specialized AI accelerators) to quantify performance portability across deployment scenarios.

3. **Dynamic Workload Testing**: Evaluate model performance under varying batch sizes and inference patterns to identify efficiency breakpoints that static benchmarks might miss, particularly for real-time applications.