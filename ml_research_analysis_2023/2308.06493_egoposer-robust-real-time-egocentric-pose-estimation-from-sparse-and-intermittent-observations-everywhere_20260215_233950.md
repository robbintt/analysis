---
ver: rpa2
title: 'EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent
  Observations Everywhere'
arxiv_id: '2308.06493'
source_url: https://arxiv.org/abs/2308.06493
tags:
- pose
- body
- estimation
- input
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EgoPoser introduces a robust approach for full-body egocentric
  pose estimation from sparse and intermittent hand and head tracking data in MR headsets.
  It addresses key limitations of existing methods by introducing a Global-in-Local
  motion decomposition that enables position-invariant predictions, realistic field-of-view
  modeling to handle intermittent hand visibility, and size-aware pose estimation
  to adapt to different body shapes.
---

# EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere

## Quick Facts
- arXiv ID: 2308.06493
- Source URL: https://arxiv.org/abs/2308.06493
- Reference count: 40
- Key outcome: Achieves state-of-the-art accuracy with MPJPE of 4.79cm and inference speed over 600fps

## Executive Summary
EgoPoser introduces a robust approach for full-body egocentric pose estimation from sparse and intermittent hand and head tracking data in MR headsets. It addresses key limitations of existing methods by introducing a Global-in-Local motion decomposition that enables position-invariant predictions, realistic field-of-view modeling to handle intermittent hand visibility, and size-aware pose estimation to adapt to different body shapes. The method achieves state-of-the-art accuracy with an inference speed over 600fps, demonstrating superior performance in both numerical metrics and visual quality while maintaining real-time applicability.

## Method Summary
EgoPoser processes head and hand poses over time windows using a Transformer-based architecture with Global-in-Local motion decomposition, SlowFast feature fusion, and field-of-view modeling. The method predicts global orientation, local joint rotations, and shape parameters, which are combined with the SMPL body model to generate full-body poses. Key innovations include position-invariant pose estimation through spatial and temporal normalization, efficient temporal context capture through SlowFast fusion, and realistic handling of intermittent hand visibility using geometric field-of-view constraints.

## Key Results
- MPJPE reduction from 6.36cm to 4.79cm compared to state-of-the-art methods
- Inference speed over 600fps, enabling real-time applications
- Robust performance across different body shapes and large scene positions

## Why This Works (Mechanism)

### Mechanism 1
Global-in-Local (GiL) motion decomposition makes pose estimation robust to different user positions in large scenes. GiL replaces direct global position input with a hybrid representation: spatial normalization keeps vertical position global while making horizontal position relative to head, and temporal normalization tracks relative trajectory over time. This preserves position-invariance while retaining global motion priors. Core assumption: Vertical position and relative horizontal trajectory contain sufficient global motion information for pose estimation, while eliminating absolute global coordinates prevents overfitting to specific recording locations. Break condition: If vertical position alone doesn't contain enough global motion information, or if the relative trajectory loses critical absolute positioning cues needed for accurate pose estimation.

### Mechanism 2
Realistic Field-of-View (FoV) modeling handles intermittent hand visibility more accurately than random masking. Instead of randomly dropping hand inputs, FoV modeling uses actual geometric visibility constraints based on headset position, orientation, and hand relative position to determine when hands are visible. This captures both spatial and temporal continuity of visibility. Core assumption: Hand visibility follows predictable geometric patterns based on FoV constraints, and modeling these patterns captures the true distribution of visibility better than random sampling. Break condition: If the geometric FoV model doesn't accurately reflect actual headset camera capabilities, or if temporal continuity assumptions break down in real-world usage.

### Mechanism 3
SlowFast feature fusion captures longer motion context without quadratic computational cost. Instead of using all frames at high temporal resolution, SlowFast fuses dense recent frames (FAST) with sparse but temporally distributed frames (SLOW) sampled with stride 2. This maintains temporal context while halving sequence length. Core assumption: Sparse sampling over longer windows captures sufficient temporal information for motion understanding, and dense recent frames provide necessary fine-grained motion details. Break condition: If the temporal patterns in human motion require higher sampling rates throughout the window, or if the stride-2 sampling misses critical motion transitions.

## Foundational Learning

- Concept: Transformer-based motion modeling
  - Why needed here: The paper uses a Transformer Encoder to process the temporal sequences of head and hand poses, requiring understanding of self-attention mechanisms and sequence modeling
  - Quick check question: How does the quadratic complexity of self-attention affect the design choice to use SlowFast fusion?

- Concept: 3D human body representation (SMPL)
  - Why needed here: The method outputs pose parameters that feed into the SMPL body model, requiring understanding of skeleton topology, joint rotations, and shape parameters
  - Quick check question: What are the 22 joints used in the output representation, and how do they relate to the SMPL kinematic tree?

- Concept: Motion capture data normalization and augmentation
  - Why needed here: The method relies on AMASS dataset and uses various normalization strategies, requiring understanding of common preprocessing techniques and their effects
  - Quick check question: How does spatial normalization differ from mean normalization, and what information does each preserve or lose?

## Architecture Onboarding

- Component map: Input (head and hand poses) → GiL (spatial and temporal normalization) → SlowFast (feature fusion) → Transformer Encoder → Multi-Head Motion Decoder → SMPL body model → Output (22 joints)
- Critical path: Input → GiL → SlowFast → Transformer → Decoder → SMPL → Output
- Design tradeoffs:
  - Position-invariance vs information loss: GiL removes absolute position but preserves vertical and relative motion
  - Computational efficiency vs temporal context: SlowFast reduces complexity while maintaining context
  - Real-time performance vs prediction quality: Single-frame output vs future frame usage
- Failure signatures:
  - Large MPJPE with position offsets: GiL decomposition not working
  - High MPJVE with FoV changes: FoV modeling not handling visibility transitions
  - Poor performance on different body sizes: Size-aware components not working
- First 3 experiments:
  1. Test position-invariance: Train with GiL vs mean normalization, evaluate at different offsets
  2. Test FoV modeling: Compare random masking vs geometric FoV on hand visibility scenarios
  3. Test size-awareness: Evaluate with true shape parameters vs mean shape on body size variation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the evaluation scope and methodology.

## Limitations
- Performance in multi-user environments or with occlusions from other people is not evaluated
- Method has not been tested on seated or non-upright postures
- No specific analysis of performance on fast or sudden movements

## Confidence
- **High Confidence**: Inference speed over 600fps, MPJPE of 4.79cm, SlowFast fusion effectiveness
- **Medium Confidence**: Position-invariant predictions, geometric FoV modeling benefits, body shape generalization
- **Low Confidence**: Cross-hardware generalization, extreme motion scenario performance, unseen motion pattern handling

## Next Checks
1. **Position-Invariance Validation**: Train EgoPoser with and without Global-in-Local decomposition, then evaluate performance across different position offsets to verify that GiL truly enables position-invariant predictions.

2. **FoV Modeling Comparison**: Implement a baseline with random masking of hand observations and compare its performance against the geometric FoV modeling approach on the same test scenarios.

3. **Cross-Hardware Generalization**: Evaluate EgoPoser performance when trained on data from one MR headset type (e.g., Quest 2) but tested on another (e.g., Hololens 2) to assess hardware generalization.