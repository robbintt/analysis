---
ver: rpa2
title: Generator-Retriever-Generator Approach for Open-Domain Question Answering
arxiv_id: '2307.11278'
source_url: https://arxiv.org/abs/2307.11278
tags:
- documents
- question
- retrieval
- arxiv
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Generator-Retriever-Generator (GRG) approach
  for open-domain question answering (QA), which integrates document generation and
  retrieval processes to enhance answer generation. GRG uses a large language model
  (InstructGPT) to generate contextual documents based on a given question while employing
  a dense passage retrieval system to retrieve relevant documents from an external
  corpus.
---

# Generator-Retriever-Generator Approach for Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2307.11278
- Source URL: https://arxiv.org/abs/2307.11278
- Reference count: 37
- Key outcome: GRG outperforms existing SOTA models on TriviaQA, NQ, and WebQ datasets with +5.2, +4.2, and +1.6 EM score improvements respectively

## Executive Summary
This paper introduces the Generator-Retriever-Generator (GRG) approach for open-domain question answering, which integrates document generation and retrieval processes to enhance answer generation. GRG uses InstructGPT to generate contextual documents based on a given question while employing a dense passage retrieval system to retrieve relevant documents from an external corpus. The generated and retrieved documents are then processed by LLaMA to produce the final answer. Experimental results show that GRG outperforms existing state-of-the-art models, achieving improvements of at least +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively.

## Method Summary
GRG implements a parallel pipeline where a question is processed simultaneously by InstructGPT for document generation and by a dense passage retrieval system for document retrieval. The generated and retrieved documents are then encoded using vector representations and passed to a fine-tuned LLaMA model (with optional LoRA adapters) to synthesize the final answer. The approach leverages the complementary strengths of generated contextual documents and retrieved factual documents, processing them through a powerful LLM to produce accurate open-domain QA responses.

## Key Results
- GRG achieves at least +5.2, +4.2, and +1.6 EM score improvements on TriviaQA, NQ, and WebQ datasets respectively compared to state-of-the-art baselines
- Using 5 generated documents consistently outperforms using 2 documents across all datasets
- GRG with LoRA fine-tuning outperforms both GRG without LoRA and the strong GENREAD baseline on all three datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel processing of generated and retrieved documents improves answer accuracy by combining complementary strengths.
- Mechanism: The GRG approach generates contextual documents using InstructGPT while simultaneously retrieving relevant documents via a dense passage retrieval system. This dual-stream input allows the final LLM (LLaMA) to synthesize both generated and retrieved information, capturing both explicit knowledge and contextually relevant details.
- Core assumption: Generated documents can provide contextual richness while retrieved documents offer factual grounding, and both are complementary rather than redundant.
- Evidence anchors:
  - [abstract] "By combining document retrieval and LLM generation, our approach addresses the challenges of open-domain QA, such as generating informative and contextually relevant answers."
  - [section] "Our proposed GRG then uses InstructGPT to generate context by providing an input prompt... Utilizing InstructGPT, we generate informative and contextually rich documents that provide relevant information for answering a given question."
- Break condition: If generated documents are consistently irrelevant or retrieved documents are too noisy, the parallel processing advantage diminishes.

### Mechanism 2
- Claim: Vector-based retrieval with high-dimensional embeddings improves document relevance compared to traditional sparse retrieval methods.
- Mechanism: The Vector Index Retriever encodes documents into 768/384-dimensional vectors using GTR-T5-large/MiniLM-L6, then retrieves top-k documents based on cosine similarity. This dense representation captures semantic relationships better than TF-IDF or BM25.
- Core assumption: Dense vector representations can capture contextual similarity more effectively than sparse representations for open-domain QA.
- Evidence anchors:
  - [section] "We propose a vector-based retrieval (Liu, 2022) method to increase relevance of knowledge in generated documents using the Vector Index Retriever... This approach leverages vector representations and the Vector Store Index to efficiently retrieve documents based on their similarity to the input question."
- Break condition: If the embedding model fails to capture domain-specific terminology or if the vector space becomes too sparse at scale.

### Mechanism 3
- Claim: Fine-tuning LoRA adapters on the LLaMA model provides efficient parameter adaptation for domain-specific QA tasks.
- Mechanism: The GRG approach uses LoRA (Low-Rank Adaptation) to efficiently fine-tune the large LLaMA model with domain-specific data, reducing computational overhead compared to full fine-tuning while maintaining performance.
- Core assumption: Low-rank decomposition can capture the essential parameter changes needed for QA adaptation without full model retraining.
- Evidence anchors:
  - [section] "Additionally, we report the performance of GRG without LoRA, utilizing the same number of generated documents."
  - [section] "For this experiment, we followed a slightly different approach. Instead of recreating the entire model from scratch, we generated a fine-tuning file that would be applied to the base Llama model."
- Break condition: If the low-rank approximation cannot capture complex domain-specific patterns or if the task requires significant architectural changes.

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR) dual-encoder architecture
  - Why needed here: Understanding how DPR works is crucial for implementing the retriever component that encodes questions and documents separately before computing similarity scores
  - Quick check question: How does the dual-encoder architecture in DPR differ from a cross-encoder approach in terms of computational efficiency and scalability?

- Concept: Vector embeddings and cosine similarity for document retrieval
  - Why needed here: The Vector Index Retriever relies on encoding documents into high-dimensional vectors and using cosine similarity for retrieval, which requires understanding vector space models and similarity metrics
  - Quick check question: What is the relationship between vector dimensionality, retrieval accuracy, and computational cost in dense retrieval systems?

- Concept: LoRA (Low-Rank Adaptation) for efficient fine-tuning
  - Why needed here: LoRA is used to efficiently adapt the large LLaMA model for the specific QA task, requiring understanding of parameter-efficient fine-tuning techniques
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning, and what are the trade-offs in terms of model capacity?

## Architecture Onboarding

- Component map:
  - Question → [Parallel] Document Generator (InstructGPT) + Document Retriever (DPR) → Vector Index Retriever → LLaMA (with LoRA) → Answer

- Critical path:
  - Question processing → Dual document streams (generation + retrieval) → Vector-based filtering → LLaMA answer generation
  - Bottleneck: LLaMA inference time, especially with larger document sets

- Design tradeoffs:
  - Document quantity vs. computational cost (2 vs 5 documents)
  - Embedding model choice (GTR-T5-large vs MiniLM-L6) for retrieval accuracy vs speed
  - LoRA vs full fine-tuning for parameter efficiency vs model capacity
  - Parallel processing complexity vs. potential accuracy gains

- Failure signatures:
  - Low recall in document retrieval → Check embedding model quality and similarity thresholds
  - Inconsistent answers → Verify document quality and generation prompts
  - Slow inference → Profile LLaMA processing time and consider document truncation
  - Memory issues → Monitor GPU memory during document encoding and LLaMA inference

- First 3 experiments:
  1. Compare performance with 2 vs 5 documents using same embedding model
  2. Test different embedding models (GTR-T5-large vs MiniLM-L6) for retrieval quality
  3. Evaluate LoRA vs full fine-tuning on a small validation set for parameter efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of GRG's generated documents compare to the accuracy of documents retrieved from Wikipedia?
- Basis in paper: [inferred] The paper mentions that GRG uses both generated and retrieved documents, and that GRG outperforms baselines that use only retrieved documents.
- Why unresolved: The paper does not directly compare the accuracy of GRG's generated documents to the accuracy of Wikipedia documents.
- What evidence would resolve it: A comparison of the accuracy of GRG's generated documents to the accuracy of Wikipedia documents on a set of test questions.

### Open Question 2
- Question: How does the performance of GRG vary with the number of generated documents?
- Basis in paper: [explicit] The paper states that GRG with 5 generated documents consistently outperforms GRG with 2 generated documents.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of GRG varies with the number of generated documents.
- What evidence would resolve it: A plot of the performance of GRG as a function of the number of generated documents.

### Open Question 3
- Question: How does the performance of GRG compare to other state-of-the-art models on a wider range of datasets?
- Basis in paper: [explicit] The paper reports that GRG outperforms existing state-of-the-art models on TriviaQA, NQ, and WebQ datasets.
- Why unresolved: The paper does not provide a comprehensive comparison of GRG to other state-of-the-art models on a wider range of datasets.
- What evidence would resolve it: A comparison of the performance of GRG to other state-of-the-art models on a wider range of datasets.

## Limitations
- Key implementation details such as exact prompt templates for InstructGPT and specific hyperparameters for the Vector Index Retriever are not provided, limiting reproducibility
- The paper lacks analysis of failure cases and performance degradation under adverse conditions such as low retrieval recall or high document noise
- No comprehensive comparison of GRG's performance against other state-of-the-art models across a wider range of datasets beyond the three evaluated

## Confidence

**High confidence** in the core mechanism: The parallel processing of generated and retrieved documents is conceptually sound and supported by the experimental results showing consistent improvements across all three datasets.

**Medium confidence** in the retrieval approach: While dense vector retrieval is well-established, the specific implementation details and performance characteristics of the Vector Index Retriever are not fully specified.

**Medium confidence** in the LoRA fine-tuning: The parameter-efficient approach is theoretically sound and supported by the ablation showing improved performance, but the specific training dynamics and hyperparameter choices are not detailed.

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary the InstructGPT prompts and document generation parameters to quantify their impact on final answer quality, as prompt engineering is likely a critical factor in GRG's success.

2. **Retrieval quality assessment**: Conduct detailed analysis of the Vector Index Retriever's recall@K performance across different embedding models and similarity thresholds to identify optimal configurations and potential failure modes.

3. **Ablation under stress conditions**: Test GRG's performance when: (a) retrieval recall drops below 50%, (b) generated documents contain increasing amounts of hallucination, and (c) document sets are reduced to minimum viable sizes to understand the approach's robustness boundaries.