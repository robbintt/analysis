---
ver: rpa2
title: Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning
arxiv_id: '2310.10735'
source_url: https://arxiv.org/abs/2310.10735
tags:
- persona
- human
- dialogue
- training
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an offline reinforcement learning framework
  to improve the persona consistency of dialogue agents. The key idea is to train
  a dialogue model using existing dialogue datasets with rewards for persona consistency,
  avoiding the expense of online RL.
---

# Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.10735
- Source URL: https://arxiv.org/abs/2310.10735
- Reference count: 14
- Key outcome: Offline RL framework improves persona consistency of dialogue agents using VaRMI importance sampling method, outperforming supervised learning and online RL baselines on PersonaChat dataset.

## Executive Summary
This paper addresses the challenge of building persona consistent dialogue agents. While current state-of-the-art systems use supervised learning or online reinforcement learning (RL), they often lack consistency as they are never punished for uttering contradictions. The authors propose an offline RL framework to improve persona consistency by explicitly punishing contradictory utterances during training. They introduce a novel importance sampling method called VaRMI that reduces variance in offline RL training by setting the importance weights of persona entailing utterances to one and setting the weight of contradictory utterances to their likelihood under the policy. Experiments on the PersonaChat dataset show that the proposed framework improves both persona consistency and dialogue quality compared to baselines, according to automatic and human evaluations. The VaRMI method in particular achieves the best overall results, balancing consistency and quality.

## Method Summary
The authors propose an offline RL framework to improve persona consistency of dialogue agents. The key idea is to train a dialogue model using existing dialogue datasets with rewards for persona consistency, avoiding the expense of online RL. A novel importance sampling method called VaRMI is introduced to reduce variance in offline RL training. The framework uses a pre-trained BlenderBot3 model and a persona consistency critic constructed from human-annotated rewards in the DNLI dataset. The model is trained for 4 epochs using policy gradient methods with VaRMI importance sampling, and evaluated on both automatic metrics and human evaluation.

## Key Results
- The proposed offline RL framework improves persona consistency compared to supervised learning and online RL baselines on the PersonaChat dataset.
- The VaRMI importance sampling method achieves the best overall results, balancing persona consistency and dialogue quality.
- Automatic evaluation shows improvements in persona consistency metrics (Hits@1, Entail@1, Rand@1, Contradict@1) compared to baselines.
- Human evaluation rates the VaRMI method as having the best combination of persona consistency and dialogue quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline RL with importance sampling improves persona consistency by explicitly punishing contradictory utterances during training.
- Mechanism: The offline RL framework trains the dialogue model using existing dialogue datasets with rewards for persona consistency, where contradictory utterances receive a reward of -1 and entailing utterances receive a reward of 1. This is different from supervised learning which only encourages entailing examples without properly punishing contradictions.
- Core assumption: The training samples include both fluent responses and responses that contradict the persona, and the model can learn from both types of examples.
- Evidence anchors:
  - [abstract] "Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions."
  - [section 3.1] "Unlike supervised learning, offline RL explicitly punishes contradictory utterances during training. This further improves persona consistency by making the bot more sensitive to contradictions."
- Break condition: If the training dataset does not contain enough contradictory examples, the model may not learn to avoid contradictions effectively.

### Mechanism 2
- Claim: The VaRMI importance sampling method reduces variance in offline RL training by setting the importance weights of persona entailing utterances to one and setting the weight of contradictory utterances to their likelihood under the policy.
- Mechanism: VaRMI takes advantage of the fact that the policy is initialized to the MLE solution before offline RL training. This means the policy has already learned the behavioral policy that generates "good" examples to an arbitrary degree. Therefore, VaRMI sets the importance weights of entailing utterances to one and the weights of contradictory utterances to their likelihood under the policy, effectively eliminating a large portion of the importance weights to reduce variance.
- Core assumption: The policy has been initialized to the MLE solution for the task, and there is a notion of absolute positive and negative rewards for the task.
- Evidence anchors:
  - [section 3.2] "Our use of VaRMI is limited to persona consistency, but can be applied to other tasks as long as the following conditions hold... The acting policy has been initialized to the MLE solution for the task."
- Break condition: If the policy is not initialized to the MLE solution or if the task does not have absolute positive and negative rewards, VaRMI may not be effective.

### Mechanism 3
- Claim: The persona consistency critic uses human annotated rewards instead of noisy, classifier-based rewards, which reduces the chance of training failures due to policy divergence.
- Mechanism: The critic is constructed by performing a mapping between the dialogue natural language inference (DNLI) dataset and the PersonaChat dataset. The DNLI dataset contains human annotated triples for each sentence, indicating entailment, neutrality, or contradiction with respect to the persona. This allows the use of human annotated rewards for persona consistency instead of classifier-based rewards.
- Core assumption: The DNLI dataset contains accurate human annotated rewards for persona consistency, and the mapping between DNLI and PersonaChat is accurate.
- Evidence anchors:
  - [section 3.3] "Our critic is constructed by performing a mapping between the dialogue natural language inference (DNLI) (Welleck et al., 2019b) and PersonaChat (Zhang et al., 2018) datasets... Since the sentences in DNLI come directly from PersonaChat, we can easily perform a mapping between the two datasets to obtain a set of dialogue samples and corresponding entailment labels."
- Break condition: If the DNLI dataset does not contain accurate human annotated rewards or if the mapping between DNLI and PersonaChat is inaccurate, the critic may not provide reliable rewards.

## Foundational Learning

- Concept: Importance sampling in offline RL
  - Why needed here: Offline RL requires importance sampling to correct for the distributional shift between the policy being optimized and the behavioral policy that generated the training samples.
  - Quick check question: How does importance sampling help correct for the distributional shift in offline RL?

- Concept: Policy gradient methods
  - Why needed here: The offline RL framework uses a policy-gradient method to optimize the RL objective, which requires computing the gradient of the objective with respect to the policy.
  - Quick check question: What is the main advantage of using policy gradient methods in offline RL compared to Q-learning methods?

- Concept: Variance reduction techniques
  - Why needed here: Offline RL training can suffer from high variance due to the need for importance sampling. The VaRMI method is introduced to reduce the variance of importance weights in offline RL training.
  - Quick check question: How does the VaRMI method reduce the variance of importance weights in offline RL training?

## Architecture Onboarding

- Component map: BlenderBot3 model -> Offline RL framework with VaRMI importance sampling -> Persona consistency critic (DNLI dataset)
- Critical path: (1) Initialize BlenderBot3 to MLE solution on PersonaChat, (2) Construct persona consistency critic using DNLI dataset, (3) Perform offline RL training with VaRMI importance sampling, (4) Evaluate on DNLI and human evaluation
- Design tradeoffs: Offline RL framework is more complex and resource-intensive than supervised learning but can improve persona consistency more effectively. Compared to online RL, it's less resource-intensive during training but requires a larger initial dataset.
- Failure signatures: If the trained model does not improve persona consistency, it could be due to: (1) Training dataset lacking contradictory examples, (2) VaRMI not effectively reducing variance, (3) Persona consistency critic providing inaccurate rewards.
- First 3 experiments:
  1. Train dialogue model using supervised learning on PersonaChat and evaluate persona consistency on DNLI.
  2. Train using offline RL with GOLD importance sampling and evaluate on DNLI.
  3. Train using offline RL with VaRMI importance sampling and evaluate on DNLI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VaRMI compare to other importance sampling methods in terms of variance reduction and training stability across different RL tasks beyond persona consistency?
- Basis in paper: [explicit] The paper introduces VaRMI as a novel importance sampling method and shows it reduces variance compared to GOLD on the persona consistency task. However, the authors acknowledge that more work is needed to test VaRMI on tasks with more complex rewards, longer time steps, and other tasks unrelated to persona consistency.
- Why unresolved: The paper only tests VaRMI on a single task (persona consistency in dialogue). Its performance on other RL tasks is unknown.
- What evidence would resolve it: Applying VaRMI to a diverse set of RL benchmark tasks and comparing its variance reduction and training stability against other importance sampling methods.

### Open Question 2
- Question: What is the optimal level of persona consistency for a dialogue agent, and how does it vary based on conversation length and context?
- Basis in paper: [inferred] The authors note that users complained the GOLD method was overly fixated on persona, making conversations feel scripted. They suggest the optimal consistency level may depend on the conversation context. However, they don't empirically study this.
- Why unresolved: The paper doesn't experimentally determine the optimal persona consistency level or how it should vary based on conversation properties.
- What evidence would resolve it: Human evaluations of dialogue agents with varying levels of persona consistency across conversations of different lengths and contexts to identify the optimal consistency level in each case.

### Open Question 3
- Question: How does offline RL compare to online RL in terms of sample efficiency and final performance for improving persona consistency?
- Basis in paper: [explicit] The authors propose offline RL as an alternative to online RL to avoid the expense of online training. However, they only compare their offline method to an online RL baseline on persona consistency, not sample efficiency or final performance.
- Why unresolved: The paper doesn't measure the sample efficiency (data required) or final persona consistency achieved by online and offline RL methods.
- What evidence would resolve it: Measuring the data required and final persona consistency achieved by online and offline RL methods across multiple runs.

## Limitations
- The VaRMI method's effectiveness on tasks beyond persona consistency is unknown.
- The paper does not analyze potential biases in the DNLI dataset that could affect persona consistency rewards.
- The optimal level of persona consistency and how it varies based on conversation context is not empirically studied.

## Confidence
- Overall effectiveness of offline RL in improving persona consistency: **High**
- Mechanism explanations for importance sampling and VaRMI: **Medium**
- Claim that VaRMI reduces variance: **Medium**

## Next Checks
1. **Ablation Study**: Remove the VaRMI importance sampling method and compare training stability and final performance to validate the claimed variance reduction benefits.

2. **Generalization Test**: Apply the offline RL framework with VaRMI to a different dialogue task (e.g., task-oriented dialogue) to test the generalizability of the approach.

3. **Bias Analysis**: Analyze the DNLI dataset for potential biases in the human-annotated rewards and assess their impact on the persona consistency of the trained models.