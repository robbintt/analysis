---
ver: rpa2
title: 'Preferences Evolve And So Should Your Bandits: Bandits with Evolving States
  for Online Platforms'
arxiv_id: '2307.11655'
source_url: https://arxiv.org/abs/2307.11655
tags:
- algorithm
- state
- reward
- regret
- arms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies a bandit learning problem with evolving, deterministic\
  \ states that captures the effect of user preference changes in recommendation systems\
  \ and ad platforms. The state evolves based on the chosen arm and a known evolution\
  \ rate \u03BB."
---

# Preferences Evolve And So Should Your Bandits: Bandits with Evolving States for Online Platforms

## Quick Facts
- arXiv ID: 2307.11655
- Source URL: https://arxiv.org/abs/2307.11655
- Reference count: 40
- Primary result: Proposes algorithms for bandit learning with evolving deterministic states, achieving sublinear regret for all evolution rates λ with different scaling in T and K depending on λ

## Executive Summary
This paper addresses bandit learning problems where user preferences evolve deterministically based on chosen actions, capturing long-term effects in recommendation systems and ad platforms. The state evolution is governed by a known rate λ, and the goal is to minimize regret against the best sequence of actions. The authors propose different algorithms tailored to different ranges of λ: dynamic programming for moderate λ, EXP3.P for small λ, and a batched approach for "sticky arms" (λ near 1). The key insight is that the optimal strategy depends critically on how quickly preferences evolve, requiring fundamentally different approaches for each regime.

## Method Summary
The paper proposes three distinct algorithms depending on the evolution rate λ. For moderate λ, it uses dynamic programming with estimators for arm rewards and end states, leveraging a "replenishing arm" technique to disentangle learning about immediate rewards versus long-term effects. For small λ (λ ∈ [0, 1/T)), it applies EXP3.P treating states as exogenous inputs. For λ near 1 ("sticky arms"), where the optimal policy alternates between two arms in a cycle, it uses a batched algorithm on meta-arms (pairs of arms). The algorithms are designed to handle the fundamental challenge of learning with long-term effects while achieving sublinear regret across all λ ranges.

## Key Results
- Sublinear regret achieved for all λ values, with different scaling: O(√(KT log K)) for small λ, O(T^(2/3) K^(1/3)) for moderate λ, and O(T^(3/4) K^(1/2)) for sticky arms
- Proves that for λ=1, the optimal policy alternates between two arms in a cycle, using the rearrangement inequality
- Introduces a novel "replenishing arm" technique to disentangle learning of immediate rewards versus long-term state effects
- Provides the first algorithm for bandits with deterministic state evolution that handles all regimes of λ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm disentangles learning IV rewards (ri) and end states (bi) despite only observing state-augmented rewards.
- Mechanism: By exploiting the deterministic state evolution, the algorithm uses a "replenishing arm" to reset the state close to 1, allowing unbiased estimation of ri. Then it uses ri to infer bi by observing rewards when the state is near bi.
- Core assumption: There exists an arm with bi close to 1 that can reset the state, or equivalently, the bi's can be scaled to make such an arm exist.
- Evidence anchors:
  - [abstract]: "The state evolves based on the chosen arm and a known evolution rate λ"
  - [section]: "Lemma 3.5. Fix an arm i ∈ [K] and a scalar ε > 0. Assume that at some round s... playing repeatedly arm i for N(λ) ≤ c(λ) · log(1/(λε)) rounds... makes the state become qN(λ), such that: |qN(λ) − bi| ≤ ε"
  - [corpus]: Weak evidence - no direct mention of state resetting mechanisms in related papers.
- Break condition: If no arm has bi close enough to 1 (even after scaling), the state cannot be reset, preventing disentanglement of ri and bi.

### Mechanism 2
- Claim: For λ near 1 ("sticky arms"), the optimal policy alternates between two arms in a cycle.
- Mechanism: When λ=1, the state becomes exactly the previous arm's end state. Using the rearrangement inequality, it's proven that cycling between two arms maximizes reward, as it allows optimal pairing of ri and bj.
- Core assumption: The reward structure allows for a two-arm cycle to be optimal, which is proven using mathematical inequalities.
- Evidence anchors:
  - [abstract]: "For λ near 1 ('sticky arms'), the optimal policy alternates between two arms"
  - [section]: "Lemma 5.2. For λ = 1, the optimal sequence of actions is a cycle of size 2."
  - [corpus]: Weak evidence - no direct mention of cycle-based optimal policies in related papers.
- Break condition: If the reward structure changes such that a longer cycle or single arm becomes optimal, the two-arm cycle mechanism fails.

### Mechanism 3
- Claim: For small λ (λ ∈ [0, 1/T)), treating states as exogenous and using EXP3.P yields sublinear regret.
- Mechanism: When λ is small, state changes are slow, so treating qt as exogenous input to EXP3.P is approximately correct. The regret analysis accounts for the approximation error.
- Core assumption: The state evolution is slow enough that treating it as exogenous input to a bandit algorithm is a reasonable approximation.
- Evidence anchors:
  - [abstract]: "For small λ, EXP3.P is applied, treating states as exogenous"
  - [section]: "Lemma 4.2. For λ ∈ (0, 1/T ], EXP3.P (Algorithm 6) incurs regret RDES(T) = O(√KT log K) + (1 − (1 − λ)T ) · OPT"
  - [corpus]: Weak evidence - no direct mention of treating states as exogenous in related papers.
- Break condition: If λ becomes too large (λ ≥ 1/T), the state changes too quickly for the exogenous treatment to be valid.

## Foundational Learning

- Concept: Dynamic Programming (DP) with approximate inputs
  - Why needed here: The algorithm needs to find the optimal sequence of arms given estimates of (ri, bi), which is a knapsack-style problem.
  - Quick check question: Can you explain how the DP algorithm uses the closed-form state equation to construct the optimal sequence?

- Concept: Hoeffding's inequality and concentration bounds
  - Why needed here: The algorithm needs to build estimators for ri and bi with bounded error, requiring concentration inequalities to guarantee accuracy.
  - Quick check question: Can you derive the concentration bound for the estimator bri using Hoeffding's inequality?

- Concept: Rearrangement inequality
  - Why needed here: For the "sticky arms" case, the proof that a two-arm cycle is optimal relies on the rearrangement inequality.
  - Quick check question: Can you state and explain the rearrangement inequality, and how it's used to prove the two-arm cycle optimality?

## Architecture Onboarding

- Component map:
  State evolution module -> Estimator module -> DP solver -> Execution module
  State evolution module -> EXP3.P module -> Execution module
  State evolution module -> Meta-arm explorer -> Execution module

- Critical path:
  1. Estimate ri using replenishing arm to reset state
  2. Estimate bi using estimated ri and state observations
  3. Feed estimates to DP solver to get optimal sequence
  4. Execute optimal sequence while continuing to refine estimates

- Design tradeoffs:
  - Accuracy vs. exploration: More exploration rounds improve estimator accuracy but reduce time for exploitation
  - λ-dependent algorithms: Different algorithms for different λ ranges, requiring λ to be known or estimated
  - Replenishing arm assumption: Requires an arm with bi close to 1, or ability to scale bi's to make this true

- Failure signatures:
  - Linear regret: Indicates failure to estimate ri and bi accurately, or wrong algorithm for λ range
  - High variance in estimators: Suggests insufficient exploration or poor concentration bounds
  - Suboptimal sequences: May indicate errors in DP solver or incorrect state evolution modeling

- First 3 experiments:
  1. Test estimator accuracy: Run algorithm with known (ri, bi) and measure error in bri and bbi
  2. Validate DP solver: Compare DP solution reward to brute-force optimal sequence for small K
  3. Verify λ-dependent behavior: Run algorithm with different λ values and confirm it switches between EXP3.P, general λ algorithm, and sticky arms algorithm

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal regret bound for the case where λ = Θ(1/T) in Bandits with Deterministically Evolving States (B-DES)?
- Basis in paper: [explicit] The paper explicitly states that the current approach only gives (1 - Θ(1/T))-approximate regret guarantees for λ = Θ(1/T), and that novel algorithms are needed to address this case further.
- Why unresolved: The paper suggests that the current algorithms fundamentally do not work for this specific case of λ, indicating a gap in the theoretical understanding and algorithmic design for this regime.
- What evidence would resolve it: A new algorithm that achieves sublinear regret for λ = Θ(1/T) or a proof that such an algorithm cannot exist would resolve this question.

### Open Question 2
- Question: Are the regret bounds presented in the paper optimal for all ranges of λ in the B-DES model?
- Basis in paper: [explicit] The paper conjectures that the regret bounds are optimal, especially noting that in the worst-case (general λ) the size of the optimal cycle cannot be upper bounded, and in the "sticky" arms case, the linear dependence on the number of meta-arms is reminiscent of tight linear dependence in classical MAB.
- Why unresolved: The paper does not provide a formal proof of optimality for the regret bounds, leaving room for potential improvements or matching lower bounds.
- What evidence would resolve it: Either a matching lower bound proof for each range of λ or an algorithm that improves upon the stated regret bounds would provide a definitive answer.

### Open Question 3
- Question: How does the interplay between contexts and states change the regret rates in a contextual version of bandit learning with long-term effects?
- Basis in paper: [explicit] The paper mentions that studying a "contextual" version of bandit learning with long-term effects is an intriguing question, but it is currently unclear how the interplay between contexts and states would affect the obtainable regret rates.
- Why unresolved: The paper does not explore contextual bandits with long-term effects, leaving the theoretical and algorithmic implications of adding context to this setting unexplored.
- What evidence would resolve it: A formal model of contextual B-DES, along with algorithms and regret analysis, would provide insights into how contexts influence the regret rates in this setting.

## Limitations

- The algorithm assumes the existence of a "replenishing arm" with bi close to 1, or the ability to scale bi's to make this true - this assumption is critical but not extensively validated
- The analysis separates ri and bi estimation, but the error propagation from these estimates to the final regret bound is not fully quantified
- Limited empirical validation of the theoretical bounds, with no experiments shown in the paper

## Confidence

- High confidence: The existence of optimal cycles for λ=1 (proven via rearrangement inequality)
- Medium confidence: The regret bounds for general λ and small λ cases (proofs are provided but rely on several technical assumptions)
- Low confidence: The practical effectiveness of the algorithms without extensive empirical validation

## Next Checks

1. **Estimator Accuracy Validation**: Implement the state evolution simulator with known (ri, bi) values and measure the empirical error in the bri and bbi estimators across multiple runs. Compare this to the theoretical concentration bounds.

2. **DP Solver Correctness**: For small K (e.g., K≤5), implement a brute-force optimal sequence finder and compare its reward against the DP solution for various (ri, bi) configurations to verify correctness.

3. **λ Transition Analysis**: Implement all three algorithm variants and run experiments across a spectrum of λ values (e.g., λ ∈ {0.01, 0.1, 0.5, 0.9, 0.99}). Measure when the algorithm should theoretically switch between regimes and validate this empirically.