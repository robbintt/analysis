---
ver: rpa2
title: Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence
  End-to-End Spoken Language Understanding
arxiv_id: '2310.06103'
source_url: https://arxiv.org/abs/2310.06103
tags:
- speech
- text
- adaptor
- pretraining
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of data sparsity in End-to-End
  Spoken Language Understanding (E2E-SLU) by proposing a unified approach that leverages
  multilingual self-supervised pretrained speech and text models to perform E2E-SLU
  tasks on six datasets across four languages. The core method integrates a multilingual
  SSL pretrained speech encoder with a fine-tuned text-to-text model, using a novel
  Modality Correlation (MC) objective to align hidden representations of speech and
  text.
---

# Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding

## Quick Facts
- arXiv ID: 2310.06103
- Source URL: https://arxiv.org/abs/2310.06103
- Reference count: 0
- Key outcome: Achieves Concept/Value Error Rate of 23.65% on PortMEDIA-Language dataset, nearly halving the best previous result

## Executive Summary
This paper addresses the data sparsity challenge in End-to-End Spoken Language Understanding (E2E-SLU) by proposing a unified approach that leverages multilingual self-supervised pretrained speech and text models. The method integrates a multilingual SSL pretrained speech encoder with a fine-tuned text-to-text model, using a novel Modality Correlation (MC) objective to align hidden representations of speech and text. The approach is evaluated on six datasets across four languages, demonstrating competitive performance and achieving state-of-the-art results on two SLU datasets. Pretraining on 7000 hours of multilingual data significantly improves performance, particularly for low-resource languages.

## Method Summary
The method combines a multilingual SSL pretrained speech encoder (XLS-R) with a fine-tuned text-to-text encoder-decoder model (mBART50). An Adaptor module maps speech features to text-like embeddings, while a novel Modality Correlation (MC) objective aligns hidden representations across modalities. The approach involves three main stages: (1) fine-tuning mBART50 on ground truth transcriptions of each SLU dataset, (2) pretraining the Adaptor module using CTC, AED, and MC losses on 7000 hours of multilingual ASR data, and (3) fine-tuning the complete SLU model on each dataset. The architecture freezes text encoder and decoder parameters during fine-tuning to preserve general linguistic knowledge.

## Key Results
- Achieves Concept/Value Error Rate of 23.65% on PortMEDIA-Language dataset, nearly halving the best previous result
- Outperforms state-of-the-art on two SLU datasets and partly on two more
- Demonstrates competitive performance across diverse SLU benchmarks, particularly excelling in tasks with large prediction spaces

## Why This Works (Mechanism)

### Mechanism 1
The Modality Correlation (MC) objective aligns hidden representations of speech and text to improve transferability from text-only NLU to speech SLU. MC loss minimizes the mean square error between the cross-modal correlation matrix of speech and text representations and the text-text self-correlation matrix, encouraging similar internal representations across modalities. Core assumption: Speech and text representations can be aligned in a shared semantic space such that their internal correlations match. Evidence: The abstract states the method uses MC to align hidden representations. Break condition: If speech and text modalities produce fundamentally incompatible internal correlations, the MC loss may fail to converge or degrade performance.

### Mechanism 2
Pretraining the Adaptor on large multilingual ASR data (7000 hours) significantly improves SLU performance, especially on low-resource languages. The Adaptor module learns to map speech encoder outputs to text-like embeddings by optimizing on ASR transcriptions, reducing the domain gap between speech and text representations before fine-tuning on SLU tasks. Core assumption: ASR data shares sufficient linguistic and acoustic patterns with SLU tasks to transfer effectively through the Adaptor. Evidence: The abstract mentions pretraining on 7000 hours allows outperforming state-of-the-art on two SLU datasets. Break condition: If ASR data distribution is too different from SLU data, pretraining may introduce noise rather than useful alignment.

### Mechanism 3
Freezing text encoder and decoder parameters during NLU fine-tuning preserves their general linguistic knowledge, enabling better transfer to speech modality. By freezing the text encoder and decoder, the fine-tuned text-to-text model retains general language understanding capabilities, which the speech encoder and Adaptor can leverage without corrupting learned linguistic patterns. Core assumption: General linguistic knowledge encoded in pretrained text models is largely task-agnostic and can be effectively reused for SLU. Evidence: The paper states parameters of the model's encoder and token embeddings are frozen during fine-tuning. Break condition: If the SLU task requires significant adaptation of linguistic representations beyond what freezing allows, performance may plateau or degrade.

## Foundational Learning

- **Self-Supervised Learning (SSL) for speech and text**: SSL pretrained models provide rich, general representations without requiring labeled data, addressing the data sparsity problem in SLU. Quick check: What is the key difference between supervised and self-supervised pretraining in the context of speech/text models?

- **Sequence-to-sequence modeling for generative SLU**: Treating SLU as a generation problem allows prediction of lexical fillers and variable-length outputs, which classification-based approaches cannot handle. Quick check: How does sequence-to-sequence modeling differ from classification for slot filling tasks?

- **Modality alignment via cross-modal contrastive learning**: Aligning speech and text representations in a shared space enables effective transfer from text NLU to speech SLU. Quick check: What is the purpose of minimizing the distance between speech-text correlation matrices in the MC objective?

## Architecture Onboarding

- **Component map**: Raw speech → Speech encoder (XLS-R) → Adaptor → Text encoder (mBART50) → Text decoder (fine-tuned mBART50) → Output tokens

- **Critical path**: Raw speech → Speech encoder → Adaptor → Text encoder → Text decoder → Output tokens

- **Design tradeoffs**:
  - Freezing text encoder/decoder vs. full fine-tuning: Preserves general knowledge but may limit task-specific adaptation
  - MC loss vs. CTC/AED losses: MC is more flexible but may be less direct for sequence alignment
  - Multilingual pretraining vs. single language: Better generalization but potentially slower convergence

- **Failure signatures**:
  - Poor SLU-F1/CVER scores indicate mismatch between speech and text representations
  - High CER/CVER on MEDIA dataset suggests decoder incompatibility after NLU fine-tuning
  - Low transfer performance on low-resource languages indicates insufficient multilingual pretraining

- **First 3 experiments**:
  1. Baseline SLU without Adaptor pretraining to establish performance gap
  2. Adaptor pretraining with MC loss on PreEnc level to test alignment effectiveness
  3. Adaptor pretraining with PostDec AED loss to evaluate decoder-level transfer

## Open Questions the Paper Calls Out

- **How does the MC loss compare to other methods of aligning speech and text representations in terms of computational efficiency and effectiveness?**
  - Basis: The paper discusses MC loss effectiveness but lacks detailed computational efficiency comparison with CTC and AED methods.
  - Resolution: A detailed comparison of training time, memory usage, and performance across the three alignment methods.

- **What is the impact of pretraining data size on the performance of the proposed SLU model, and is there a point of diminishing returns?**
  - Basis: The paper mentions scaling up pretraining data improves results but lacks detailed analysis of size impact.
  - Resolution: A systematic analysis showing performance vs. pretraining data size with identification of diminishing returns.

- **How does the proposed model handle languages with very different phonetic and grammatical structures, such as tonal languages or languages with complex morphology?**
  - Basis: The paper evaluates on multiple languages but lacks detailed analysis of performance on structurally diverse languages.
  - Resolution: Detailed performance analysis on languages with diverse phonetic and grammatical structures, including tonal languages and those with complex morphology.

## Limitations

- Evaluation on only six datasets across four languages may not represent real-world SLU diversity
- Inconsistent performance across datasets suggests approach may be dataset-specific
- Lack of detailed implementation specifications for Modality Correlation objective and Adaptor module

## Confidence

**High Confidence (90-100%):**
- Unified architecture combining multilingual speech encoder with text-to-text models is technically sound
- Pretraining on multilingual ASR data improves performance compared to training from scratch
- Freezing text encoder/decoder parameters during NLU fine-tuning preserves general linguistic knowledge

**Medium Confidence (60-80%):**
- MC objective meaningfully improves cross-modal alignment compared to standard CTC or AED losses alone
- Adaptor module successfully bridges the gap between speech and text representations for all tested languages
- Reported performance improvements on PortMEDIA-Language represent genuine state-of-the-art advancement

**Low Confidence (30-50%)**
- Approach will generalize to languages and domains not represented in the pretraining data
- Method's performance advantage is consistent across all SLU task types (IC, SF, NER)
- Proposed architecture will outperform specialized models designed for specific SLU subtasks

## Next Checks

1. **Cross-dataset consistency validation**: Evaluate the model on additional SLU datasets beyond the six tested, particularly focusing on languages and domains not included in the 7000-hour pretraining corpus.

2. **Ablation study of MC objective**: Conduct controlled experiments comparing the full model with versions using only CTC loss, only AED loss, and the MC objective to quantify the actual contribution of the MC alignment mechanism.

3. **Parameter sensitivity analysis**: Systematically vary key hyperparameters including learning rates, batch sizes, and pretraining durations to establish the robustness of the approach and identify critical dependencies.