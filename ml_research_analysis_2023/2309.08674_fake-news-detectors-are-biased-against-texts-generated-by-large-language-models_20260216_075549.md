---
ver: rpa2
title: Fake News Detectors are Biased against Texts Generated by Large Language Models
arxiv_id: '2309.08674'
source_url: https://arxiv.org/abs/2309.08674
tags:
- news
- fake
- mfmf
- llm-generated
- detectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the bias of fake news detectors towards
  large language model (LLM)-generated content. Despite concerns about LLM-generated
  fake news, detectors were found to be more accurate at identifying LLM-generated
  fake news compared to human-written fake news.
---

# Fake News Detectors are Biased against Texts Generated by Large Language Models

## Quick Facts
- arXiv ID: 2309.08674
- Source URL: https://arxiv.org/abs/2309.08674
- Reference count: 11
- Fake news detectors show bias towards LLM-generated content

## Executive Summary
This study investigates a critical bias in fake news detection systems: they perform significantly better at identifying fake news generated by large language models (LLMs) compared to human-written fake news. The research reveals that detectors exploit distinct linguistic patterns inherent to LLM outputs, leading to a bias that causes them to classify LLM-generated texts as fake news regardless of their truthfulness. To address this issue, the authors propose an adversarial training approach using LLM-paraphrased real news, which effectively mitigates the bias and improves performance on both human-written and LLM-generated content.

## Method Summary
The study introduces two new datasets, GossipCop++ and PolitiFact++, to facilitate research in this domain. The authors train fake news detectors (BERT, RoBERTa, ELECTRA, ALBERT, DeBERTa) on combined datasets of human-written real news, human-written fake news, and LLM-generated fake news using Structured Mimicry Prompting (SMP). They analyze NELA features to identify linguistic differences between human-written and LLM-generated content. To mitigate the identified bias, the authors implement adversarial training with LLM-paraphrased real news, challenging the model during its training phase to improve generalization across diverse news content.

## Key Results
- Fake news detectors show higher accuracy (95.7%) on LLM-generated fake news compared to human-written fake news (68.2%)
- NELA feature analysis reveals significant linguistic differences between human-written and LLM-generated fake news
- Adversarial training with LLM-paraphrased real news effectively mitigates the bias, improving performance on both human-written and LLM-generated content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fake news detectors are more accurate at identifying LLM-generated fake news compared to human-written fake news due to distinct linguistic patterns inherent to LLM outputs.
- Mechanism: Detectors exploit unique linguistic features in LLM-generated texts, leading to a bias in classification.
- Core assumption: LLM-generated texts have identifiable linguistic patterns that differ from human-written texts.
- Evidence anchors:
  - [abstract]: "This unexpected bias appears to arise from distinct linguistic patterns inherent to LLM outputs."
  - [section]: "We find that fake news detectors perform much worse on LLM-paraphrased real news than human-written ones. Based on these observations, we conclude that fake news detectors are biased towards LLM-generate texts and tend to classify them as fake news regardless of their truthfulness."
- Break condition: If linguistic patterns in LLM-generated texts become indistinguishable from human-written texts, the bias may diminish.

### Mechanism 2
- Claim: The bias in fake news detectors can be mitigated by training on selective features with two regression detectors.
- Mechanism: By focusing on features that do not significantly differ between human-written and LLM-generated fake news, the detectors can be debiased.
- Core assumption: Certain NELA features do not significantly differ between human-written and LLM-generated fake news, and focusing on these features can reduce bias.
- Evidence anchors:
  - [section]: "We propose that these detectors may learn ‘shortcuts’, identifying fake news based on unique linguistic features in LLM-generated texts."
  - [section]: "For GossipCop++, we retained NELA features that exhibited no significant disparity between human-written and LLM-generated fake news."
- Break condition: If the selected features are not truly unbiased or if new features emerge that are not accounted for, the debiasing may not be effective.

### Mechanism 3
- Claim: Adversarial training with LLM-paraphrased real news can effectively mitigate the bias in fake news detectors.
- Mechanism: By augmenting the training set with high-quality LLM-generated real news, the detectors learn to generalize across diverse news content rather than being narrowly focused on a specific subset.
- Core assumption: LLM-generated real news can be of high quality and indistinguishable from human-written real news, allowing detectors to learn without bias.
- Evidence anchors:
  - [section]: "To mitigate such biases, we first study whether fake news detectors may take ‘shortcuts’ to learn the LLM-generated fake news."
  - [section]: "We show that our approach can effectively mitigate the biases and narrow the performance gap between LLM-generated and human-written texts."
- Break condition: If the LLM-generated real news is not of sufficient quality or if the adversarial training is not properly implemented, the bias may persist.

## Foundational Learning

- Concept: NELA features
  - Why needed here: NELA features are used to analyze the linguistic patterns in news articles, which is crucial for understanding the bias in fake news detectors.
  - Quick check question: What are the six dimensions of news content that NELA features encapsulate?

- Concept: Adversarial training
  - Why needed here: Adversarial training is used to mitigate the bias in fake news detectors by challenging the model during its training phase.
  - Quick check question: How does adversarial training help in reducing bias in fake news detectors?

- Concept: LLM-generated text detection
  - Why needed here: Understanding how to detect LLM-generated text is essential for evaluating the performance of fake news detectors in the LLM era.
  - Quick check question: What are the challenges in detecting LLM-generated fake news compared to human-written fake news?

## Architecture Onboarding

- Component map: Fake news detectors (RoBERTa, BERT, ELECTRA) -> NELA feature extraction -> Adversarial training module -> Datasets (GossipCop++, PolitiFact++)
- Critical path: Data preprocessing → Feature extraction (NELA) → Model training (with adversarial training) → Evaluation on test sets
- Design tradeoffs: Balancing between detecting LLM-generated and human-written fake news, ensuring high-quality LLM-generated real news for adversarial training, and managing computational resources for training complex models
- Failure signatures: High accuracy on LLM-generated fake news but low accuracy on human-written fake news, failure to generalize across diverse news content, and overfitting to specific linguistic patterns
- First 3 experiments:
  1. Evaluate the performance of fake news detectors on GossipCop++ and PolitiFact++ datasets
  2. Analyze the NELA features to identify potential biases in the detectors
  3. Implement and test the adversarial training approach to mitigate the identified biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of adversarial training with LLM-paraphrased real news vary across different LLMs?
- Basis in paper: [explicit] The paper introduces a debiasing technique using adversarial training with LLM-paraphrased real news and tests it with ChatGPT.
- Why unresolved: The paper only uses ChatGPT for adversarial training. It does not explore how the effectiveness of this approach might differ when using other LLMs.
- What evidence would resolve it: Empirical results comparing the effectiveness of adversarial training with paraphrased real news generated by different LLMs (e.g., GPT-3, LLaMA) on fake news detection performance.

### Open Question 2
- Question: What are the long-term implications of the identified bias towards LLM-generated content for the evolution of fake news detection models?
- Basis in paper: [inferred] The paper discusses a bias in fake news detectors towards LLM-generated content, but does not explore the long-term implications of this bias.
- Why unresolved: The paper does not provide a comprehensive analysis of how this bias might influence the development and effectiveness of future fake news detection models.
- What evidence would resolve it: Longitudinal studies tracking the performance of fake news detection models over time, with a focus on how their ability to detect human-written versus LLM-generated fake news evolves.

### Open Question 3
- Question: How do different NELA features contribute to the bias in fake news detectors towards LLM-generated content?
- Basis in paper: [explicit] The paper analyzes NELA features to understand the bias in fake news detectors, finding significant differences between human-written and LLM-generated fake news.
- Why unresolved: While the paper identifies significant differences in NELA features, it does not delve into how each feature individually contributes to the bias.
- What evidence would resolve it: Detailed analysis of the impact of each NELA feature on the bias in fake news detectors, potentially through feature importance ranking or ablation studies.

## Limitations

- The specific prompts used in Structured Mimicry Prompting (SMP) for generating LLM fake news are not detailed, limiting reproducibility
- NELA feature selection for debiasing is described qualitatively without specifying statistical thresholds or selection criteria
- The linguistic patterns identified as distinctive may change rapidly as LLMs evolve, potentially invalidating current findings

## Confidence

- **High Confidence**: The core finding that fake news detectors show bias toward LLM-generated content (detection rates 95.7% for LLM fake news vs 68.2% for human fake news). This is well-supported by experimental results across multiple models and datasets.
- **Medium Confidence**: The proposed adversarial training mitigation strategy is effective, though the magnitude of improvement could vary with different implementation details not specified in the paper.
- **Medium Confidence**: The linguistic feature analysis identifying why detectors are biased, as the qualitative descriptions are compelling but would benefit from more granular statistical validation.

## Next Checks

1. Test the same experimental setup with multiple different LLM prompting strategies to verify that the observed bias is consistent across prompting approaches and not specific to the undisclosed SMP method.

2. Apply formal statistical feature selection methods (e.g., ANOVA, mutual information) to identify truly unbiased features, rather than relying on qualitative assessment of "no significant disparity."

3. Replicate the experiments after a 3-6 month interval using updated LLM models to assess whether the identified linguistic patterns and detector biases persist or have shifted with model evolution.