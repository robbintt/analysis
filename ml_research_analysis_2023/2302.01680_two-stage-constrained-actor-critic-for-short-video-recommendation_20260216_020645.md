---
ver: rpa2
title: Two-Stage Constrained Actor-Critic for Short Video Recommendation
arxiv_id: '2302.01680'
source_url: https://arxiv.org/abs/2302.01680
tags:
- policy
- video
- learning
- user
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimizing main objectives
  (e.g., watch time) while balancing auxiliary objectives (e.g., user interactions)
  in short video recommendation systems using constrained reinforcement learning.
  The authors propose a two-stage constrained actor-critic method (TSCAC) to tackle
  this challenge.
---

# Two-Stage Constrained Actor-Critic for Short Video Recommendation

## Quick Facts
- arXiv ID: 2302.01680
- Source URL: https://arxiv.org/abs/2302.01680
- Reference count: 40
- This paper proposes a two-stage constrained actor-critic method (TSCAC) that improves both main and auxiliary objectives in short video recommendation systems.

## Executive Summary
This paper addresses the challenge of optimizing main objectives (e.g., watch time) while balancing auxiliary objectives (e.g., user interactions) in short video recommendation systems. The authors propose a two-stage constrained actor-critic method (TSCAC) that first learns individual policies for each auxiliary response, then optimizes the main response while being softly regularized to stay close to these auxiliary policies. The method demonstrates significant improvements in both offline and live experiments, with TSCAC outperforming baseline methods in watch time and interactions by 2.23% and 18.80% respectively in offline experiments, and 0.379% and 3.376% respectively in live experiments.

## Method Summary
The paper proposes a two-stage constrained actor-critic algorithm for short video recommendation. In stage one, individual policies are learned to optimize each auxiliary response separately. In stage two, a policy is learned to optimize the main response while being softly regularized to stay close to the policies learned in the first stage. The method uses separate value models for each reward type (WatchTime and interactions) to prevent interference between dense and sparse reward signals. The algorithm is evaluated through extensive offline and live experiments, demonstrating its effectiveness in improving both the main and auxiliary objectives.

## Key Results
- TSCAC improves watch time by 2.23% and interactions by 18.80% over baseline methods in offline experiments
- Live experiments show 0.379% improvement in watch time and 3.376% improvement in interactions
- The method has been successfully deployed in a production short video recommendation system
- Multi-critic policy estimation improves correlation with WatchTime and interactions by 0.19% and 0.14% respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-critic policy estimation improves accuracy by isolating dense and sparse reward signals.
- Mechanism: Separate value models are learned for each reward type, allowing response-specific discount factors and preventing interference between dense (WatchTime) and sparse (Like/Follow/Share) signals during evaluation.
- Core assumption: The temporal dynamics and observation frequencies of different rewards are sufficiently distinct that joint estimation would lead to biased or noisy value estimates.
- Evidence anchors:
  - [abstract] "we separately evaluate each response via its own value model, which allows for response-specific discount factors and mitigates the interference on evaluation from one response on another"
  - [section] "we learn two value models V_w and V_i with reward as WatchTime and interactions respectively... V_separate is more correlated with WatchTime and interactions by 0.19% and 0.14% respectively"

### Mechanism 2
- Claim: Two-stage learning ensures auxiliary objectives are met while optimizing the main objective.
- Mechanism: Stage one learns individual policies for each auxiliary response; stage two optimizes the main response while softly constraining the policy to stay close to the auxiliary policies.
- Core assumption: Auxiliary objectives can be optimized independently first, and their policies can then be used as soft constraints for the main policy without conflict.
- Evidence anchors:
  - [abstract] "At stage one, we learn individual policies to optimize each auxiliary signal. At stage two, we learn a policy to (i) optimize the main signal and (ii) stay close to policies learned at the first stage"

### Mechanism 3
- Claim: Closed-form solution for soft constraints provides an effective way to handle multiple constraints.
- Mechanism: The Lagrangian of the constrained optimization has a closed-form solution that combines auxiliary policies with the main policy's advantage, weighted by Lagrangian multipliers.
- Core assumption: The optimal policy under soft constraints can be expressed as a product of auxiliary policies and an exponential term of the main advantage.
- Evidence anchors:
  - [abstract] "we learn a policy to (i) optimize the main signal and (ii) stay close to policies learned in the first stage"
  - [section] "The Lagrangian of Eq. (5) has the closed form solution π*(a|s) ∝ Πᵢ₌₂ π_θᵢ(a|s)^λᵢ Σⱼ₌₂ λⱼ exp(A^(k)₁ / Σⱼ₌₂ λⱼ)"

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDP)
  - Why needed here: The problem involves optimizing a main objective (WatchTime) while satisfying constraints on auxiliary objectives (interactions), which is naturally formulated as a CMDP.
  - Quick check question: What is the difference between a standard MDP and a CMDP in terms of objective formulation?

- Concept: Reinforcement Learning with Multiple Critics
  - Why needed here: Different reward signals (WatchTime and interactions) have different characteristics (dense vs sparse), requiring separate value models to accurately estimate their values.
  - Quick check question: How does using multiple critics for different rewards help in handling the bias introduced by combining dense and sparse signals?

- Concept: Actor-Critic Methods
  - Why needed here: The algorithm uses actor-critic architecture to learn policies (actors) and value functions (critics) for both the main and auxiliary objectives.
  - Quick check question: What is the role of the critic in an actor-critic method, and how does it help in policy improvement?

## Architecture Onboarding

- Component map:
  - State representation: User features, user history, and candidate video features
  - Action space: Video ID or user preference vector
  - Reward structure: Multi-dimensional vector (WatchTime, interactions)
  - Critic models: Separate value models for each reward type
  - Actor models: Policies for main and auxiliary objectives
  - Training loop: Two-stage learning with soft constraints

- Critical path:
  1. Pre-training auxiliary policies (Stage One)
  2. Learning main policy with soft constraints (Stage Two)
  3. Policy evaluation using multiple critics
  4. Deployment in production system

- Design tradeoffs:
  - Separate critics vs joint critics: Better accuracy vs increased complexity
  - Two-stage learning vs joint optimization: Clearer separation of concerns vs potential suboptimality
  - Soft constraints vs hard constraints: Flexibility vs potential constraint violation

- Failure signatures:
  - Poor performance on main objective: Too strong soft constraints
  - Poor performance on auxiliary objectives: Too weak soft constraints
  - High variance in training: Importance sampling bias correction issues

- First 3 experiments:
  1. Validate multi-critic estimation on a small dataset with known reward characteristics
  2. Test two-stage learning with synthetic constraints to verify soft constraint effectiveness
  3. Evaluate closed-form solution for soft constraints against numerical optimization methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of user interactions (like, share, comment) contribute differently to the overall user satisfaction in short video recommendation systems?
- Basis in paper: [explicit] The paper discusses various types of user interactions such as Like, Follow, Share, Comment, Collect, and how they reflect user satisfaction levels.
- Why unresolved: The paper does not provide a detailed analysis of how each type of interaction contributes differently to user satisfaction or how they should be weighted against each other in the optimization process.
- What evidence would resolve it: A detailed study analyzing user behavior data to quantify the impact of each interaction type on user satisfaction and engagement metrics would help resolve this question.

### Open Question 2
- Question: How does the two-stage constrained actor-critic method (TSCAC) perform in scenarios with a larger number of auxiliary objectives?
- Basis in paper: [inferred] The paper discusses the challenges of optimizing the main goal while balancing multiple auxiliary objectives in short video recommendation systems.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the scalability of TSCAC when dealing with a larger number of auxiliary objectives.
- What evidence would resolve it: Experimental results comparing TSCAC's performance with an increasing number of auxiliary objectives, along with an analysis of computational complexity and convergence properties, would help resolve this question.

### Open Question 3
- Question: How does the performance of TSCAC compare to other constrained optimization methods in terms of computational efficiency and convergence speed?
- Basis in paper: [explicit] The paper mentions that traditional constrained reinforcement learning algorithms fail to work well in the setting of short video recommendation due to the difficulty in optimizing with multiple constraints.
- Why unresolved: The paper does not provide a comprehensive comparison of TSCAC's computational efficiency and convergence speed with other constrained optimization methods.
- What evidence would resolve it: A detailed analysis comparing the computational complexity, training time, and convergence speed of TSCAC with other constrained optimization methods on the same problem domain would help resolve this question.

## Limitations

- The paper does not specify the exact neural network architectures and hyperparameters used in experiments, creating uncertainty in reproducing the reported performance
- Soft constraint implementation details and Lagrangian multiplier tuning are not specified, which may affect the effectiveness of the constraint balancing mechanism
- Evaluation methodology lacks detail on behavior policy construction and statistical significance testing for live experiments

## Confidence

**High Confidence Claims**:
- The two-stage learning framework structure and its general approach to handling main and auxiliary objectives
- The theoretical formulation of constrained optimization with soft regularization
- The general benefit of using separate value models for different reward types

**Medium Confidence Claims**:
- The specific performance improvements reported in both offline and live experiments
- The effectiveness of the closed-form solution for soft constraints
- The practical utility of the method in a production environment

**Low Confidence Claims**:
- The exact contribution of each algorithmic component to the final performance
- The generalizability of the approach to other recommendation domains
- The sensitivity of results to hyperparameter choices and implementation details

## Next Checks

1. **Component Ablation Study**: Conduct controlled experiments to isolate the contribution of each key component (multi-critic estimation, two-stage learning, soft constraints) to overall performance.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary critical hyperparameters (learning rates, Lagrangian multipliers, network architectures) across a reasonable range to understand their impact on performance.

3. **Cross-Domain Validation**: Apply the TSCAC method to a different recommendation domain (e.g., news recommendation or e-commerce) with different reward structures to test generalizability.