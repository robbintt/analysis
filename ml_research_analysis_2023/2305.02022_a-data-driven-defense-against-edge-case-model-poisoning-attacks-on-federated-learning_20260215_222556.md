---
ver: rpa2
title: A Data-Driven Defense against Edge-case Model Poisoning Attacks on Federated
  Learning
arxiv_id: '2305.02022'
source_url: https://arxiv.org/abs/2305.02022
tags:
- dataset
- defense
- learning
- poisoned
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of defending against targeted
  model poisoning attacks in federated learning, specifically edge-case attacks that
  target a small fraction of the input space. The proposed method, LearnDefend, uses
  a defense dataset containing a mix of poisoned and clean examples to learn a poisoned
  data detector (PDD) model and a client importance model.
---

# A Data-Driven Defense against Edge-case Model Poisoning Attacks on Federated Learning

## Quick Facts
- arXiv ID: 2305.02022
- Source URL: https://arxiv.org/abs/2305.02022
- Authors: 
- Reference count: 40
- Key outcome: LearnDefend significantly reduces attack success rate by at least 40% on standard attack setups and by more than 80% on some setups, requiring as few as five defense examples to achieve near-optimal reduction in attack success rate.

## Executive Summary
This paper addresses the challenge of defending against targeted model poisoning attacks in federated learning, specifically edge-case attacks that target a small fraction of the input space. The proposed method, LearnDefend, uses a defense dataset containing a mix of poisoned and clean examples to learn a poisoned data detector (PDD) model and a client importance model. By identifying and down-weighting malicious client updates, LearnDefend significantly reduces the attack success rate while maintaining high model accuracy.

## Method Summary
LearnDefend is a data-driven defense framework that learns a poisoned data detector (PDD) model and a client importance model using a defense dataset containing a mix of poisoned and clean examples. The PDD model marks each example in the defense dataset as poisoned or clean, while the client importance model estimates the probability of a client update being malicious. The global model is then updated as a weighted average of the client models' updates, with weights determined by the client importance model. The PDD and client importance model parameters are updated using an alternating minimization strategy over the Federated Learning rounds.

## Key Results
- LearnDefend significantly reduces attack success rate by at least 40% on standard attack setups.
- The defense requires as few as five defense examples to achieve near-optimal reduction in attack success rate.
- LearnDefend maintains high model accuracy while defending against poisoning attacks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The LearnDefend framework reduces attack success rate by learning a poisoned data detector (PDD) and client importance model that jointly identify and down-weight malicious client updates.
- **Mechanism**: The PDD model assigns scores to examples in the defense dataset indicating their likelihood of being poisoned, while the client importance model uses these scores to weight client updates in the global model aggregation. This joint optimization ensures that poisoned data and malicious clients are identified and mitigated over time.
- **Core assumption**: The defense dataset contains a mix of poisoned and clean examples, and the PDD can learn to distinguish them based on their impact on the global model's loss.
- **Evidence anchors**:
  - [abstract]: "The defense dataset contains a mix of poisoned and clean examples, with only a few known to be clean. The proposed method, DataDefense, uses this dataset to learn a poisoned data detector model which marks each example in the defense dataset as poisoned or clean."
  - [section]: "The defense dataset, Dd, consists of both clean and poisoned datapoints. However, the datapoints in Dd need not be marked as poisoned or clean, and hence can be collected from unreliable sources, e.g. user feedback, or inputs from some clients."
  - [corpus]: Weak - no direct evidence in corpus about PDD learning from defense dataset.
- **Break condition**: If the defense dataset does not contain enough poisoned examples, or if the PDD cannot distinguish poisoned from clean examples effectively, the mechanism fails.

### Mechanism 2
- **Claim**: The client importance model learns to assign lower weights to clients whose updates are more likely to be malicious, based on their performance on the defense dataset.
- **Mechanism**: The client importance model uses three features - average cross-entropy loss on clean defense data, average cross-entropy loss on poisoned defense data, and L2-distance from the global model - to estimate the likelihood of a client being malicious. Clients with higher loss on poisoned data and larger distance from the global model are given lower weights.
- **Core assumption**: Malicious clients will have higher loss on poisoned defense data and larger deviation from the global model compared to clean clients.
- **Evidence anchors**:
  - [abstract]: "The proposed method, DataDefense, uses this dataset to learn a poisoned data detector model which marks each example in the defense dataset as poisoned or clean. It also learns a client importance model that estimates the probability of a client update being malicious."
  - [section]: "The first two features describe the performance of the client model w.r.t. the current defense dataset, while the third feature provides a metric for deviation of client model from the consensus model for performing robust updates."
  - [corpus]: Weak - no direct evidence in corpus about feature-based client importance estimation.
- **Break condition**: If malicious clients can mimic clean client behavior on the defense dataset, or if the features are not discriminative enough, the mechanism fails.

### Mechanism 3
- **Claim**: The alternating minimization strategy for learning the PDD and client importance model parameters ensures that both models improve over time, leading to better defense against poisoning attacks.
- **Mechanism**: The PDD parameters are updated to minimize a cost function that encourages correct ranking of poisoned vs clean examples, while the client importance parameters are updated to minimize the loss on the defense dataset. This alternating optimization allows both models to adapt to the evolving threat landscape.
- **Core assumption**: The alternating minimization can find a good solution where both the PDD and client importance model are effective.
- **Evidence anchors**:
  - [abstract]: "The poisoned data detector and the client importance model parameters are updated using an alternating minimization strategy over the Federated Learning rounds."
  - [section]: "This is a coupled optimization problem since Lθ depends on ψ through Ddc, Ddp, and Lψ depends on θ through ¯φ. Algorithm 1 describes our for estimating θ∗ and ψ∗ on the master node. We use alternating minimization updates for both θ and ψ, along with the federated learning rounds for learning ¯φ, to achieve the above objective."
  - [corpus]: Weak - no direct evidence in corpus about alternating minimization effectiveness.
- **Break condition**: If the alternating minimization gets stuck in a poor local minimum, or if the updates are not sufficiently coordinated, the mechanism fails.

## Foundational Learning

- **Concept**: Federated Learning
  - **Why needed here**: The paper proposes a defense mechanism specifically for federated learning systems, which are vulnerable to model poisoning attacks.
  - **Quick check question**: What is the main difference between federated learning and traditional centralized learning?

- **Concept**: Model Poisoning Attacks
  - **Why needed here**: The paper focuses on defending against model poisoning attacks in federated learning, which aim to corrupt the global model by injecting malicious updates from compromised clients.
  - **Quick check question**: What is the difference between Byzantine attacks and targeted backdoor attacks in the context of federated learning?

- **Concept**: Defense Dataset
  - **Why needed here**: The paper introduces the concept of a defense dataset, which is used to learn the poisoned data detector and client importance models. Understanding how to construct and use such a dataset is crucial for implementing the proposed defense mechanism.
  - **Quick check question**: What are the key properties that a defense dataset should have to be effective in learning a poisoned data detector?

## Architecture Onboarding

- **Component map**: Master node -> Client nodes -> Defense dataset -> PDD model -> Client importance model
- **Critical path**: 1) Collect defense dataset, 2) Initialize PDD and client importance models, 3) For each FL round: a) Update PDD using defense dataset and current global model, b) Identify poisoned examples in defense dataset, c) Update client importance model using defense dataset partitions, d) Update global model using weighted average of client models.
- **Design tradeoffs**:
  - Defense dataset size vs. effectiveness of PDD and client importance models.
  - Complexity of PDD and client importance models vs. computational overhead.
  - Frequency of PDD and client importance model updates vs. adaptation speed.
- **Failure signatures**:
  - High attack success rate despite defense mechanism.
  - Slow convergence of PDD and client importance models.
  - Inability to distinguish poisoned from clean examples in defense dataset.
- **First 3 experiments**:
  1. Implement LearnDefend on a simple federated learning setup with synthetic data and a known poisoning attack. Measure attack success rate and compare with baseline defenses.
  2. Vary the size and composition of the defense dataset to understand its impact on the effectiveness of the PDD and client importance models.
  3. Introduce different types of poisoning attacks (e.g., label flipping, trigger-based) and evaluate the robustness of LearnDefend against each attack type.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does LearnDefend perform against more sophisticated poisoning attacks beyond PGD, such as adaptive attacks that specifically target the defense mechanism?
  - **Basis in paper**: [inferred] The paper evaluates LearnDefend against PGD attacks but does not explore other attack types or adaptive attacks designed to circumvent the defense.
  - **Why unresolved**: Evaluating against a broader range of attack types would require significant additional experimentation and may reveal weaknesses in the defense mechanism.
  - **What evidence would resolve it**: Conducting experiments with various attack types, including adaptive attacks, and comparing the performance of LearnDefend against these attacks would provide insights into its robustness.

- **Open Question 2**: How does the performance of LearnDefend scale with larger federated learning systems involving a higher number of clients and more diverse data distributions?
  - **Basis in paper**: [inferred] The experiments use a relatively small number of clients and assume a certain level of data heterogeneity, but do not explore the scalability of LearnDefend to larger systems.
  - **Why unresolved**: Evaluating the scalability of LearnDefend to larger federated learning systems would require significant computational resources and may reveal limitations in its performance.
  - **What evidence would resolve it**: Conducting experiments with a larger number of clients and varying levels of data heterogeneity would provide insights into the scalability and robustness of LearnDefend.

- **Open Question 3**: How does LearnDefend handle the case where the defense dataset contains a mix of clean and poisoned examples, but the ratio of poisoned to clean examples is unknown or changes over time?
  - **Basis in paper**: [explicit] The paper mentions that the defense dataset contains a mix of clean and poisoned examples, but assumes a known ratio of poisoned to clean examples (β) for the PDD.
  - **Why unresolved**: In practice, the ratio of poisoned to clean examples in the defense dataset may be unknown or change over time, which could impact the performance of LearnDefend.
  - **What evidence would resolve it**: Conducting experiments with varying ratios of poisoned to clean examples in the defense dataset and evaluating the performance of LearnDefend would provide insights into its robustness to changes in the dataset composition.

## Limitations
- The paper's effectiveness relies heavily on the assumption that the defense dataset contains representative poisoned examples, but the paper does not thoroughly explore scenarios where poisoned data distribution differs significantly from the defense dataset.
- The alternating minimization strategy's convergence properties are not rigorously proven, and the method's performance in highly dynamic attack scenarios remains uncertain.
- The computational overhead of maintaining and updating the PDD and client importance models is not fully characterized.

## Confidence
- **High confidence**: The general framework of using a defense dataset to learn poisoned data detection and client importance is sound and well-motivated.
- **Medium confidence**: The specific feature choices for the client importance model (cross-entropy loss and L2-distance) are reasonable but may not be optimal for all attack types.
- **Low confidence**: The alternating minimization strategy's ability to consistently find good solutions in complex, high-dimensional parameter spaces is not well-established.

## Next Checks
1. Test LearnDefend's performance when the defense dataset contains a significantly different proportion of poisoned examples compared to the actual attack scenario.
2. Evaluate the method's robustness against adaptive attackers who can modify their poisoning strategy based on the defense mechanism's behavior.
3. Analyze the computational overhead of LearnDefend in large-scale federated learning settings with many clients and high-dimensional models.