---
ver: rpa2
title: Towards Verifiable Text Generation with Symbolic References
arxiv_id: '2311.09188'
source_url: https://arxiv.org/abs/2311.09188
tags:
- data
- score
- json
- name
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes symbolically grounded generation (SymGen) to
  enable easier verification of LLM-generated text by interleaving output with explicit
  symbolic references to fields in conditioning data (e.g., JSON tables). The LLM
  is prompted to generate text using Jinja-style placeholders referencing data fields,
  which are then parsed and rendered with actual values.
---

# Towards Verifiable Text Generation with Symbolic References

## Quick Facts
- arXiv ID: 2311.09188
- Source URL: https://arxiv.org/abs/2311.09188
- Reference count: 24
- Key outcome: LLMs can generate fluent text with symbolic references while maintaining accuracy, enabling easier verification of generated content

## Executive Summary
This paper introduces SymGen, a framework for verifiable text generation that interleaves symbolic references to structured data fields within LLM-generated text. By prompting LLMs to use Jinja-style placeholders referencing JSON fields, SymGen enables users to easily verify the provenance of generated content. Experiments across data-to-text, counterfactual generation, financial QA, and arithmetic reasoning tasks demonstrate that LLMs can produce fluent text with symbolic references while maintaining accuracy. Human evaluation confirms that these annotations streamline the verification process, offering a simple approach to improve verifiability without significantly degrading output quality.

## Method Summary
SymGen works by prompting LLMs to generate text that includes Jinja-style expressions referencing fields in structured input data (typically JSON). The LLM outputs text with these symbolic placeholders, which are then parsed and rendered with actual values from the source data. The approach supports both zero-shot and few-shot settings and can be implemented through direct generation or indirect rewriting strategies. A parser resolves the symbolic references to their corresponding values, and the final output can be augmented with visual cues showing the provenance of different text spans. This enables users to easily trace generated content back to its source data fields.

## Key Results
- LLMs successfully generate fluent text with symbolic references across multiple domains while maintaining accuracy
- SymGen achieves comparable or slightly improved performance to baseline approaches in data-to-text and counterfactual generation tasks
- Symbolic references enable users to readily verify the provenance of generated text spans, streamlining manual verification
- For complex arithmetic reasoning tasks, SymGen outperforms Chain-of-Thought approaches when using larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be prompted to generate fluent text that interleaves symbolic references to fields in conditioning data.
- Mechanism: The LLM is prompted with structured input (e.g., JSON) and asked to generate text using Jinja-style expressions that reference specific fields in the data structure. A parser then replaces these symbolic references with actual values to produce the final rendered text.
- Core assumption: LLMs have been sufficiently exposed to templated text during pretraining that they can perform zero- and few-shot text generation with symbolic references across multiple domains.
- Evidence anchors:
  - [abstract] "Across a range of data-to-text and question-answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy."
  - [section 2] "We exploit the fact that LLMs have likely been sufficiently exposed to such kind of templated text during pretraining that they are able perform zero- and few-shot text generation with symbolic references across multiple domains."
- Break condition: The LLM fails to generate text with symbolic references, produces incorrect references, or the rendered text is not fluent or accurate.

### Mechanism 2
- Claim: Symbolic references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification.
- Mechanism: The rendered text can be augmented with visual cues that show the correspondence between the rendered references and the underlying source data. This allows users to readily verify the provenance of a particular span of text.
- Core assumption: Displaying the provenance of text spans through symbolic references makes verification easier for users.
- Evidence anchors:
  - [abstract] "The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification."
  - [section 2] "One could thus see SymGen as similar in spirit to citation-enabled LLMs... except that our approach focuses on providing very precise and easy to validate citations with respect to some data."
- Break condition: Users do not find the provenance information helpful for verification, or the visual cues are confusing or misleading.

### Mechanism 3
- Claim: SymGen can maintain the same quality as the baseline when using symbolic references, improving verifiability without significantly degrading quality.
- Mechanism: Experiments on various tasks (data-to-text, counterfactual obituary generation, question-answering, arithmetic word problems) compare the performance of SymGen with and without symbolic references to a baseline approach. Results show comparable or slightly improved performance in some cases.
- Core assumption: The performance penalty incurred by symbolic generation approaches is acceptable compared to the improvement in verifiability.
- Evidence anchors:
  - [abstract] "Across a range of data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy."
  - [section 3.1] "For SynthBio, in the few-shot case, we find that both symbolic generation strategies we considered yield comparable or superior performance to the baseline."
- Break condition: The performance degradation is too significant, making the approach impractical despite the improvement in verifiability.

## Foundational Learning

- Concept: Templated text generation
  - Why needed here: SymGen is based on the idea of templated text, where regular prose is interleaved with symbolic references that can be grounded to some context at a later time. Understanding this concept is crucial for grasping how SymGen works.
  - Quick check question: What is the main difference between SymGen and classic templated approaches for text generation?

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: SymGen can be used for reasoning tasks, such as solving arithmetic word problems. CoT reasoning is a related concept where the model generates intermediate reasoning steps before providing the final answer. Understanding CoT is important for comparing SymGen's performance in reasoning tasks.
  - Quick check question: How does SymGen's approach to reasoning differ from Chain-of-Thought (CoT) reasoning?

- Concept: Question-answering (QA) over structured data
  - Why needed here: SymGen can be used to enhance verifiability in QA over structured data, such as company information. Understanding the challenges and approaches in this domain is important for evaluating SymGen's effectiveness.
  - Quick check question: What is the main advantage of using SymGen for QA over structured data compared to traditional approaches?

## Architecture Onboarding

- Component map: LLM -> Prompt template -> Jinja-style expressions -> Parser -> Rendering engine
- Critical path:
  1. Provide structured input (e.g., JSON) to the LLM
  2. Prompt the LLM to generate text with symbolic references
  3. Parse the symbolic expressions and replace them with actual values
  4. Render the final text with visual cues for provenance

- Design tradeoffs:
  - Direct vs. indirect symbolic generation strategies (direct is more efficient but may have lower quality; indirect maintains baseline quality but is more costly)
  - Choice of templating language (Jinja-style expressions are used in this work, but other options may be explored)
  - Granularity of symbolic references (fine-grained references provide more provenance information but may be harder to generate)

- Failure signatures:
  - LLM fails to generate text with symbolic references
  - Incorrect or unresolvable symbolic references (e.g., referencing non-existent fields)
  - Rendering errors (e.g., Jinja parser fails to run)
  - Grammatical or typographical errors in the rendered text (due to noisy or suboptimal input data)

- First 3 experiments:
  1. Evaluate SymGen on a simple data-to-text task (e.g., SynthBio) with both zero- and few-shot settings to compare performance with and without symbolic references.
  2. Test SymGen's ability to generate counterfactual text (e.g., counterfactual obituaries) and evaluate the factuality of the generated text using a question-answering approach.
  3. Assess SymGen's performance on a reasoning task (e.g., GSM8K arithmetic word problems) and compare it with Chain-of-Thought (CoT) and Program-aided Language Models (PAL) approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SymGen performance scale with increasing complexity and size of structured data inputs?
- Basis in paper: [inferred] The paper evaluates on datasets with varying field counts (18.96 to 644.16 fields) but doesn't systematically study performance degradation with scale
- Why unresolved: The evaluation focuses on comparing different generation strategies and models, but doesn't explore the relationship between data complexity and symbolic generation quality
- What evidence would resolve it: Systematic experiments varying data size and complexity while measuring accuracy, error rates, and generation quality across SymGen and baseline approaches

### Open Question 2
- Question: What is the impact of SymGen on user trust and verification behavior in real-world applications?
- Basis in paper: [explicit] The paper mentions human evaluation found annotations streamline verification but doesn't provide detailed methodology or results
- Why unresolved: The human study is described as preliminary and doesn't explore how SymGen affects trust, verification time, or decision-making in practical scenarios
- What evidence would resolve it: Controlled user studies measuring verification time, accuracy, and perceived trust across SymGen vs baseline outputs in various domains

### Open Question 3
- Question: Can SymGen be extended to automatically generate structured data from unstructured text to enable verification?
- Basis in paper: [inferred] The paper notes GSM experiments explore cases where structured data isn't available but doesn't propose solutions
- Why unresolved: The paper focuses on settings with readily available structured data but acknowledges this limitation without exploring workarounds
- What evidence would resolve it: Development and evaluation of methods to automatically extract structured data from text and use it for SymGen-style verification

## Limitations

- Performance varies across model sizes (GPT-3.5 vs GPT-4), suggesting architectural dependencies not fully explored
- Limited scope of tasks evaluated (4 specific domains) with unknown generalizability to other domains
- Indirect symbolic generation strategy introduces computational overhead not fully quantified

## Confidence

- **High confidence**: LLMs can generate text with symbolic references and maintain fluency (validated across multiple experiments)
- **Medium confidence**: SymGen improves verifiability through provenance display (based on human evaluation but limited sample size)
- **Low confidence**: Performance parity with baseline approaches (results show mixed outcomes across tasks and models)

## Next Checks

1. Test SymGen on a new domain (e.g., medical records summarization) to evaluate generalizability beyond the 4 studied tasks
2. Conduct a controlled experiment measuring time-to-verification for SymGen vs baseline outputs with larger user samples
3. Analyze the computational overhead of indirect generation by measuring latency and cost differences compared to baseline approaches