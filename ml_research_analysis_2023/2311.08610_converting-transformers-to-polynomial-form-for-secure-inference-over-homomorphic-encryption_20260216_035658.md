---
ver: rpa2
title: Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic
  Encryption
arxiv_id: '2311.08610'
source_url: https://arxiv.org/abs/2311.08610
tags:
- polynomial
- attention
- training
- transformer
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the first demonstration of secure inference
  over homomorphic encryption with transformer models by converting them to polynomial
  form. The core method involves developing HE-friendly alternatives to Softmax attention
  and LayerNorm, along with a specialized training procedure to obtain HE-friendly
  weights.
---

# Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption

## Quick Facts
- arXiv ID: 2311.08610
- Source URL: https://arxiv.org/abs/2311.08610
- Reference count: 14
- Primary result: First demonstration of secure transformer inference over homomorphic encryption using polynomial conversion

## Executive Summary
This paper presents the first demonstration of secure inference over homomorphic encryption (HE) with transformer models by converting them to polynomial form. The authors develop HE-friendly alternatives to Softmax attention and LayerNorm, along with a specialized training procedure to obtain HE-friendly weights. The method achieves competitive perplexity scores on Wikitext-103 with polynomial BERT-like transformers and attains accuracy within 4% of original vision transformers on CIFAR-100 and Tiny-ImageNet benchmarks after polynomial conversion.

## Method Summary
The paper develops a method to convert transformer models into polynomial form suitable for secure inference over homomorphic encryption. This involves replacing Softmax attention with pointwise activation-based attention (Attentionσ), substituting LayerNorm with BatchNorm or polynomial approximations through range minimization training, and applying specialized training to obtain HE-friendly weights. The approach enables privacy-preserving transformer inference without additional communication overhead or client involvement.

## Key Results
- Achieves competitive perplexity scores on Wikitext-103 with polynomial BERT-like transformers
- Attains accuracy within 4% of original vision transformers on CIFAR-100 and Tiny-ImageNet benchmarks
- Demonstrates first secure inference capability for transformer models over homomorphic encryption

## Why This Works (Mechanism)

### Mechanism 1
Softmax attention can be replaced by pointwise activation-based attention (Attentionσ) while maintaining competitive performance. By replacing the non-polynomial softmax function with an element-wise activation function σ, the attention computation becomes a sequence of polynomial operations that can be approximated with polynomials.

### Mechanism 2
Layer normalization can be replaced by BatchNorm or approximated by polynomials through range minimization training. BatchNorm can be implemented as a constant affine transformation in HE, while range minimization training constrains input values to make polynomial approximation feasible.

### Mechanism 3
HE-friendly weights can be obtained through specialized training that minimizes input ranges to non-polynomial layers. Additional training with range minimization loss encourages the model to produce weights that result in input values within ranges that can be accurately approximated by polynomials.

## Foundational Learning

- **Concept: Homomorphic Encryption (HE) and Fully Homomorphic Encryption (FHE)**
  - Why needed here: Understanding the constraints of HE is crucial since the paper's goal is to enable secure inference with transformers under HE. HE only supports polynomial operations, which drives the need for polynomial approximations.
  - Quick check question: What is the key limitation of HE that necessitates converting transformers to polynomial form?

- **Concept: Polynomial approximation techniques**
  - Why needed here: The paper relies on approximating non-polynomial functions (Softmax, LayerNorm, activations) with polynomials. Understanding approximation methods like the Remez algorithm and Newton-Raphson method is essential.
  - Quick check question: Why is directly approximating Softmax and LayerNorm by polynomials within each transformer block challenging?

- **Concept: Transformer architecture components**
  - Why needed here: The paper modifies core transformer components (attention, normalization, activations). Understanding these components is necessary to grasp the modifications and their implications.
  - Quick check question: What are the three main non-polynomial components in standard transformer models that need to be addressed?

## Architecture Onboarding

- **Component map**: Input → LayerNorm → Self-Attention (σ-attention) → Add & Norm → MLP → Output
- **Critical path**: The critical path for secure inference is the sequence of polynomial operations that must be evaluated under HE. This includes the σ-attention mechanism, any BatchNorm layers, and the MLP with polynomial activations.
- **Design tradeoffs**: Accuracy vs. security (more accurate polynomial approximations may require higher-degree polynomials, increasing computational cost under HE); Model complexity vs. HE compatibility (simpler models may be easier to convert but sacrifice performance); Training time vs. inference efficiency (additional training for HE-friendly weights may improve inference efficiency under HE)
- **Failure signatures**: Training instability (exploding or vanishing gradients during training, especially with σ-attention without proper scaling); Poor approximation (large approximation errors leading to significant performance degradation after polynomial conversion); HE incompatibility (inability to evaluate the polynomial model efficiently under HE due to excessive multiplication depth)
- **First 3 experiments**:
  1. Implement and train a BERT-like transformer with σ-attention (without Softmax) on Wikitext-103 to verify that pointwise attention can match softmax attention performance.
  2. Train a vision transformer with BatchNorm instead of LayerNorm on CIFAR-100 to test if BatchNorm provides sufficient stability for vision tasks.
  3. Apply range minimization training to a small transformer and verify that it successfully constrains variance values to a manageable range for polynomial approximation.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of polynomial transformers scale with model size (depth and width) compared to traditional transformers? The paper only tests polynomial transformers up to 12 layers. There's no systematic scaling study to determine how polynomial approximation quality degrades with model size.

### Open Question 2
What is the theoretical relationship between the variance range of LayerNorm inputs and the achievable approximation accuracy for the inverse square root function? While the paper shows empirically that reducing the variance range helps, there's no theoretical analysis of the tradeoff between range minimization (which may hurt model capacity) and approximation accuracy.

### Open Question 3
How do polynomial transformers compare to traditional transformers in terms of robustness to adversarial attacks and out-of-distribution samples? The paper focuses on standard benchmark performance but doesn't test robustness properties that are crucial for real-world deployment.

## Limitations

- The quality of polynomial approximations for complex transformer architectures remains uncertain, particularly for higher-degree polynomials needed for more accurate approximations
- Scalability to larger models (e.g., GPT-3 sized models) is uncertain due to potential computational overhead of polynomial evaluation under HE
- Specialized training procedures introduce additional complexity that may lead to training instability and hyperparameter sensitivity

## Confidence

**High Confidence Claims**:
- The fundamental approach of converting transformers to polynomial form for HE inference is sound and technically feasible
- σ-attention can effectively replace Softmax attention in transformer models while maintaining competitive performance
- BatchNorm provides a viable HE-friendly alternative to LayerNorm for vision transformers

**Medium Confidence Claims**:
- The range minimization training technique successfully constrains variance values for polynomial approximation in NLP transformers
- Polynomial approximations of non-linear activations (GELU, ReLU) maintain sufficient accuracy for transformer performance
- The overall methodology achieves privacy-preserving inference without additional communication overhead

**Low Confidence Claims**:
- The method scales efficiently to state-of-the-art large language models beyond BERT-like architectures
- The computational overhead of polynomial evaluation under HE is acceptable for real-time applications
- The approach generalizes seamlessly across diverse transformer variants and tasks without significant tuning

## Next Checks

1. **Approximation Error Analysis**: Conduct a systematic evaluation of polynomial approximation errors for Softmax, LayerNorm, and activation functions across different input ranges. Measure how approximation errors propagate through transformer layers and affect final task performance.

2. **Scalability Testing**: Implement and test the polynomial transformer conversion on progressively larger models (from BERT-base to BERT-large, then to GPT-2 variants) to identify the scalability limits and computational bottlenecks under HE.

3. **Robustness to Hyperparameters**: Perform an ablation study on the range minimization training and σ-attention scaling parameters to determine their sensitivity and impact on model convergence and final performance. Identify the most critical hyperparameters for successful training.