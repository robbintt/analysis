---
ver: rpa2
title: Identification of the Relevance of Comments in Codes Using Bag of Words and
  Transformer Based Models
arxiv_id: '2308.06144'
source_url: https://arxiv.org/abs/2308.06144
tags:
- comments
- corpus
- code
- training
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of identifying relevant comments
  in source code, a critical task for efficient software development and maintenance.
  The authors explore both classical bag-of-words (BOW) models and transformer-based
  architectures to classify comments as useful or not useful.
---

# Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models

## Quick Facts
- arXiv ID: 2308.06144
- Source URL: https://arxiv.org/abs/2308.06144
- Reference count: 20
- One-line primary result: Bag-of-words models outperform transformers on code comment classification task

## Executive Summary
This study tackles the challenge of identifying relevant comments in source code, a crucial task for efficient software development and maintenance. The authors explore both classical bag-of-words (BOW) models and transformer-based architectures to classify comments as useful or not useful. They experiment with classifiers like logistic regression, random forest, and SVM using TF-IDF and entropy-based term weighting, alongside fine-tuned BERT, RoBERTa, and ALBERT models. Results show that BOW-based models outperform transformers on the training corpus, with logistic regression and SVM achieving F1 scores of 0.72 and 0.71, respectively. However, all models perform poorly on the test set, with the best F1 score of 0.53 from an entropy-based SVM. The study highlights the limitations of current approaches due to small training data and suggests the need for domain-specific embeddings to improve performance.

## Method Summary
The authors approach the problem of classifying source code comments as useful or not useful using both classical machine learning and deep learning methods. They employ bag-of-words models with TF-IDF and entropy-based term weighting, using logistic regression, random forest, and SVM classifiers. Additionally, they fine-tune pre-trained transformer models including BERT, RoBERTa, and ALBERT. The study uses the FIRE 2022 IRSE shared task dataset, which consists of training and test corpora with comments, corresponding C code snippets, and binary labels. The evaluation focuses on F1 score, precision, recall, and accuracy on both training and test sets.

## Key Results
- BOW models with TF-IDF weighting outperform transformer models on the training corpus (F1 scores of 0.72 and 0.71 for logistic regression and SVM, respectively)
- Entropy-based SVM classifier achieves the best performance on the test set with an F1 score of 0.53
- All models, including transformers, perform poorly on the test set, indicating potential overfitting or domain mismatch
- Small training corpus size is identified as a major limitation for transformer model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bag-of-words models with TF-IDF weighting outperform transformer-based models on small, domain-specific datasets.
- Mechanism: TF-IDF emphasizes rare but informative terms within code comments, while transformer models require large datasets to learn semantic context.
- Core assumption: Code comments have domain-specific vocabulary that can be effectively weighted without semantic context.
- Evidence anchors:
  - [abstract] "The empirical results show that the bag of words model outperforms the transformer based models"
  - [section 4] "The logistic regression (LR) and SVM classifiers using the classical TF-IDF based term weighting scheme of the bag of words model respectively achieves the best and the second best F1 scores"
  - [corpus] Weak evidence - no explicit comparison of vocabulary overlap between training and test sets
- Break condition: If the test corpus contains substantially different code comment styles or new domain terms not present in training, TF-IDF weighting may fail to capture relevance patterns.

### Mechanism 2
- Claim: Entropy-based term weighting can better capture comment relevance in sparse datasets.
- Mechanism: Entropy weighting accounts for term distribution across documents, giving more weight to terms that distinguish relevant comments from irrelevant ones.
- Core assumption: The entropy of term frequencies reflects the discriminative power of comments in source code.
- Evidence anchors:
  - [abstract] "The term frequency (TF) and inverse document frequency (IDF) i.e., TF-IDF and Entropy based method were used as the term weighting schemes"
  - [section 4] "The entropy based SVM classifier outperforms the other models on test corpus"
  - [corpus] Weak evidence - no explanation of why entropy weighting specifically helps in this domain
- Break condition: If term distributions in comments are uniform or highly skewed, entropy weighting may not provide additional discriminative power.

### Mechanism 3
- Claim: Transformer models underperform on small training corpora due to insufficient semantic context capture.
- Mechanism: Pre-trained transformers like BERT require fine-tuning on large datasets to adapt to domain-specific language patterns.
- Core assumption: The training corpus is too small to allow transformer models to learn the semantic nuances of code comments.
- Evidence anchors:
  - [abstract] "all models perform poorly on the test set, with the best F1 score of 0.53 from an entropy-based SVM"
  - [section 4] "The major reason behind this may be the semantics that were necessary for the comments of the test corpus were not captured during training stage by the transformer based models as the size of the training corpus was insufficient"
  - [corpus] Moderate evidence - corpus size limitations are mentioned but not quantified
- Break condition: If a larger, more diverse training corpus becomes available, transformer models may outperform bag-of-words approaches.

## Foundational Learning

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF)
  - Why needed here: TF-IDF helps distinguish important terms in code comments by downweighting common terms and emphasizing rare, informative ones.
  - Quick check question: How does TF-IDF handle terms that appear in most documents versus terms that appear in only a few?

- Concept: Entropy-based term weighting
  - Why needed here: Entropy weighting captures the distribution of terms across documents, which may be more informative for classifying code comment relevance.
  - Quick check question: What does high entropy indicate about a term's distribution across documents?

- Concept: Chi-squared statistic for feature selection
  - Why needed here: Chi-squared tests identify the most relevant features by measuring the independence between term occurrence and class labels.
  - Quick check question: How does the chi-squared statistic determine which terms are most informative for classification?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature engineering -> Classifier training -> Evaluation -> Model selection -> Test set prediction
- Critical path: Data preprocessing → Feature engineering → Classifier training → Evaluation → Model selection → Test set prediction
- Design tradeoffs:
  - Bag-of-words vs. transformer models: Simpler models perform better on small datasets but may miss semantic context
  - Feature selection methods: Chi-squared vs. mutual information - different statistical approaches to identify relevant terms
  - Hyperparameter tuning: Cross-validation for traditional models vs. epoch tuning for transformers
- Failure signatures:
  - Large performance gap between training and test sets indicates overfitting or domain mismatch
  - Transformer models underperforming suggests insufficient training data or poor semantic capture
  - Entropy-based models outperforming TF-IDF may indicate specific term distribution patterns in code comments
- First 3 experiments:
  1. Compare TF-IDF vs. entropy weighting performance on the training set using logistic regression
  2. Test different numbers of features selected by chi-squared statistic (e.g., 1000, 3000, 5000) and measure impact on SVM performance
  3. Fine-tune BERT with different learning rates and batch sizes to find optimal configuration for this domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do domain-specific embeddings impact the performance of transformer-based models for code comment classification?
- Basis in paper: [inferred] The authors note that pre-trained models like BERT, RoBERTa, and ALBERT were developed using the Books corpus and Wikipedia, which may not capture the semantics of code from comments. They suggest that domain-specific embeddings could improve performance.
- Why unresolved: The study did not explore the use of domain-specific embeddings, and the impact of such embeddings on model performance remains untested.
- What evidence would resolve it: Experiments comparing transformer models using domain-specific embeddings (e.g., trained on code-related data) versus general-purpose embeddings would provide evidence of their impact on classification performance.

### Open Question 2
- Question: What are the specific reasons for the poor performance of transformer-based models on the test corpus compared to their performance on the training corpus?
- Basis in paper: [explicit] The authors state that they could not find the reasons behind the poor performance of many models on the test corpus due to time constraints.
- Why unresolved: The study did not investigate the specific factors contributing to the performance gap between training and test sets.
- What evidence would resolve it: A detailed analysis of model predictions on training and test data, including error analysis and comparison of feature distributions, would help identify the reasons for the performance discrepancy.

### Open Question 3
- Question: How does the size of the training corpus affect the performance of transformer-based models in code comment classification?
- Basis in paper: [inferred] The authors suggest that the small size of the training corpus may be a major reason for the poor performance of transformer-based models, as they could not capture the necessary semantics for the test corpus.
- Why unresolved: The study did not experiment with varying sizes of training data to assess the impact on model performance.
- What evidence would resolve it: Experiments training transformer models on different-sized subsets of the training data and evaluating their performance on the test set would provide insights into the relationship between training data size and model effectiveness.

## Limitations
- Small training corpus size limits the effectiveness of transformer-based models
- FIRE 2022 IRSE dataset is not publicly available, restricting reproducibility
- No clear explanation for why entropy-based weighting outperforms TF-IDF in this domain
- Lack of analysis on potential domain shift between training and test sets

## Confidence
- High: Bag-of-words models outperform transformers on the training corpus
- Medium: Entropy-based weighting improves classification performance
- Low: Claims about transformer model limitations due to small corpus

## Next Checks
1. **Dataset Availability and Characteristics**: Obtain the FIRE 2022 IRSE dataset to verify the training and test split characteristics. Analyze vocabulary overlap and domain-specific terms to understand why TF-IDF weighting performs well.

2. **Cross-Dataset Validation**: Test the trained models on a different code comment dataset to assess generalization. This will help determine if the observed performance gap between training and test sets is due to overfitting or domain-specific patterns.

3. **Transformer Model Scaling**: Experiment with larger pre-trained models (e.g., GPT-2, XLNet) and domain-specific embeddings to see if transformer performance improves with more sophisticated architectures or domain adaptation.