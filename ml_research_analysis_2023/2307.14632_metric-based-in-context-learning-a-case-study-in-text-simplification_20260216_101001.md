---
ver: rpa2
title: 'Metric-Based In-context Learning: A Case Study in Text Simplification'
arxiv_id: '2307.14632'
source_url: https://arxiv.org/abs/2307.14632
tags:
- sentence
- examples
- sari
- asset
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Metric-Based in-context Learning (MBL), a
  novel approach for selecting examples in text simplification tasks. The method leverages
  standard evaluation metrics like SARI, compression ratio, and BERT-Precision to
  choose the most relevant examples for in-context learning with large language models.
---

# Metric-Based In-context Learning: A Case Study in Text Simplification

## Quick Facts
- arXiv ID: 2307.14632
- Source URL: https://arxiv.org/abs/2307.14632
- Reference count: 29
- Primary result: Metric-based selection achieves SARI scores up to 47.94 on ASSET dataset

## Executive Summary
This paper introduces Metric-Based in-context Learning (MBL), a novel approach for selecting examples in text simplification tasks. The method leverages standard evaluation metrics like SARI, compression ratio, and BERT-Precision to choose the most relevant examples for in-context learning with large language models. Through extensive experiments on GPT models of various sizes (175B, 13B, 6.7B) using TurkCorpus and ASSET datasets, MBL demonstrates state-of-the-art results, achieving SARI scores up to 47.94 on ASSET. The approach shows robustness to example ordering and out-of-domain test sets, outperforming strong baselines and fine-tuned models. Notably, MBL allows implicit control over GPT-175B's behavior through the chosen metric, paving the way for more accurate and efficient natural language generation systems.

## Method Summary
MBL selects high-quality examples for in-context learning by calculating metric scores between complex sentences and their reference simplifications. For each development set pair, the approach measures distance according to a chosen metric (SARI, compression ratio, or BERTScore Precision), then selects top-k pairs to form the prompt. The selected examples are formatted into the model input template and used to generate simplifications with GPT models. Performance is evaluated using SARI and BLEU scores on test outputs, demonstrating that metric-based selection consistently outperforms random baselines across model sizes and datasets.

## Key Results
- MBL achieves SARI scores up to 47.94 on ASSET dataset, surpassing state-of-the-art results
- The approach demonstrates robustness to example ordering (high-to-low, low-to-high, random) with minimal performance variance
- Out-of-domain performance shows competitive results, though with expected degradation (4-7 SARI point drop when applying ASSET-selected examples to TurkCorpus)
- Implicit control mechanism works as SARI-based selection optimizes SARI scores while BERTScore Precision-based selection optimizes BLEU scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metric-based selection leverages task-specific evaluation metrics to find high-quality examples for in-context learning
- Mechanism: The approach calculates metric scores between complex sentences and their reference simplifications, then selects top-scoring pairs to form the prompt. Higher SARI scores indicate better simplification quality, while compression ratio signals significant content reduction.
- Core assumption: Standard evaluation metrics can effectively measure simplification quality and guide example selection
- Evidence anchors:
  - [abstract] "MBL demonstrates state-of-the-art results, achieving SARI scores up to 47.94 on ASSET"
  - [section] "we go through each l in the development set and measure the distance between each c and ri according to a metric, m"
  - [corpus] Weak - no direct citations of this specific mechanism
- Break condition: If evaluation metrics don't correlate with actual simplification quality or if metric optimization diverges from task objectives

### Mechanism 2
- Claim: Large language models can implicitly learn simplification patterns from metric-selected examples
- Mechanism: When prompted with high-quality simplification examples, GPT-175B learns to mimic the transformation patterns, with SARI-based selection optimizing for SARI score and BERTScore precision optimizing for BLEU score
- Core assumption: LLMs can extract and generalize simplification patterns from limited in-context examples
- Evidence anchors:
  - [abstract] "GPT-175B can be implicitly controlled via optimal metric-based learning"
  - [section] "BERTScore Precision-based learning optimizes BLEU, while SARI-based selection optimizes SARI scores"
  - [corpus] Weak - no direct citations of this implicit control mechanism
- Break condition: If model size is insufficient to capture patterns from few examples or if selected examples lack diversity

### Mechanism 3
- Claim: Metric-based selection is robust to example ordering and performs well in out-of-domain settings
- Mechanism: The approach maintains performance across different orderings (high-to-low, low-to-high, random) and transfers learned patterns to new datasets, with some degradation expected for significantly different domains
- Core assumption: Simplification patterns are transferable across datasets and not overly sensitive to prompt ordering
- Evidence anchors:
  - [abstract] "demonstrate that MBL is generally robust to example orderings and out-of-domain test sets"
  - [section] "For each metric we perform three different order arrangements, namely as highest → lowest, lowest → highest, and random ordering"
  - [corpus] Weak - no direct citations of this robustness claim
- Break condition: If domain shift is too large or if specific orderings significantly impact learning

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The entire approach relies on LLMs learning from examples provided in the prompt without parameter updates
  - Quick check question: What's the key difference between in-context learning and fine-tuning?

- Concept: Text simplification evaluation metrics
  - Why needed here: MBL uses SARI, compression ratio, and BERTScore to select examples, requiring understanding of what each metric measures
  - Quick check question: How does SARI differ from BLEU in evaluating text simplification?

- Concept: Few-shot learning performance variation
  - Why needed here: The paper addresses variability in ICL performance based on example quality, quantity, and ordering
  - Quick check question: Why might the order of examples in a prompt affect ICL performance?

## Architecture Onboarding

- Component map:
  Metric calculation module -> Example selection module -> Prompt construction module -> LLM inference module -> Evaluation module

- Critical path:
  1. Calculate metric scores for all development set pairs
  2. Select top-k pairs based on chosen metric
  3. Construct prompt with selected examples
  4. Generate simplifications using LLM
  5. Evaluate outputs with SARI/BLEU

- Design tradeoffs:
  - Metric choice: SARI optimizes SARI scores but compression ratio may work better for smaller models
  - k value: More examples may improve performance but increase computational cost
  - Model size: Larger models generally perform better but at higher cost

- Failure signatures:
  - Random baseline performance varies widely - indicates sensitivity to example selection
  - Zero-shot performance is consistently weak - confirms need for in-context examples
  - Mismatch between selection metric and evaluation metric - suggests implicit control mechanism

- First 3 experiments:
  1. Compare random selection vs SARI-based selection on GPT-175B with k=5
  2. Test robustness by reordering SARI-selected examples (high→low, low→high, random)
  3. Evaluate out-of-domain performance using ASSET examples on TurkCorpus test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of metric-based selection vary across different types of text simplification tasks beyond the ones tested (e.g., scientific text simplification, medical text simplification)?
- Basis in paper: [inferred] The paper mentions that the robustness of the approach to out-of-domain test sets was tested, but only on a cognitive simplification dataset. It also suggests that the approach's scalability to other downstream TS tasks outside of cognitive simplification is yet to be tested, especially in different domains.
- Why unresolved: The paper does not provide empirical evidence or results for other types of text simplification tasks. The authors only tested the approach on TurkCorpus, ASSET, and FestAbility datasets, which may not cover all types of text simplification tasks.
- What evidence would resolve it: Conducting experiments on a wider variety of text simplification tasks and datasets, including scientific, medical, and legal text simplification, would provide evidence on the generalizability of the metric-based selection approach.

### Open Question 2
- Question: How does the choice of metric for example selection affect the interpretability and quality of the simplified text from a human perspective?
- Basis in paper: [explicit] The paper mentions that the behaviour of large GPT models can be implicitly controlled via optimal metric-based learning, with SARI-based selection optimizing SARI scores and BERTScore Precision-based selection optimizing BLEU. However, it does not provide a detailed analysis of how these choices affect the interpretability and quality of the simplified text from a human perspective.
- Why unresolved: While the paper provides quantitative results on the performance of different metrics, it does not include a qualitative analysis of the human interpretability and quality of the simplified text. This is an important aspect of text simplification that goes beyond automated evaluation metrics.
- What evidence would resolve it: Conducting human evaluations to assess the readability, simplicity, and interpretability of the simplified text generated using different metric-based selection approaches would provide evidence on the impact of metric choice on human-perceived quality.

### Open Question 3
- Question: How does the performance of metric-based selection compare to other example selection methods, such as those based on semantic similarity or perplexity, in terms of both effectiveness and efficiency?
- Basis in paper: [explicit] The paper compares the proposed metric-based selection approach to random selection and KATE-GPT, which uses cosine similarity for example selection. However, it does not compare to other example selection methods, such as those based on semantic similarity or perplexity, which have been proposed in other studies.
- Why unresolved: The paper provides a comparison with only a few baseline methods, leaving open the question of how metric-based selection performs relative to other example selection approaches in terms of both effectiveness and efficiency.
- What evidence would resolve it: Conducting experiments comparing metric-based selection to other example selection methods, such as those based on semantic similarity, perplexity, or mutual information, would provide evidence on the relative effectiveness and efficiency of different approaches.

## Limitations
- Reliance on standard evaluation metrics assumes they accurately capture simplification quality without human validation
- Performance degradation when applying selected examples across different datasets (4-7 SARI point drop)
- Computational cost of calculating metrics for all development set pairs may be prohibitive for larger datasets
- Dependence on having multiple reference simplifications in development set limits applicability to datasets without reference outputs

## Confidence

**High Confidence Claims:**
- Metric-based selection outperforms random selection across all model sizes (supported by extensive experiments)
- MBL is robust to example ordering (validated through systematic testing of three different orderings)
- SARI-based selection consistently performs well on ASSET dataset (achieving 47.94 SARI score)

**Medium Confidence Claims:**
- Implicit control through metric selection works as described (observational evidence but limited theoretical explanation)
- Out-of-domain robustness is acceptable (some degradation observed but performance remains competitive)

**Low Confidence Claims:**
- Compression ratio being optimal for smaller models (only briefly mentioned without extensive validation)
- Specific threshold values for k and temperature settings (not thoroughly explored across different settings)

## Next Checks

1. **Human Evaluation Validation**: Conduct human judgments to verify whether SARI/BERTScore-optimized outputs actually represent better simplifications than random selection, addressing the gap between automatic metrics and human quality assessment.

2. **Cross-Dataset Transfer Analysis**: Systematically test MBL on datasets with different simplification styles (e.g., Newsela, Wikipedia) to quantify the limits of cross-domain generalization and identify patterns in performance degradation.

3. **Metric Correlation Study**: Analyze the correlation between development set metric scores and test set performance across different datasets to determine whether metric-based selection reliably predicts generalization ability, or if it overfits to specific dataset characteristics.