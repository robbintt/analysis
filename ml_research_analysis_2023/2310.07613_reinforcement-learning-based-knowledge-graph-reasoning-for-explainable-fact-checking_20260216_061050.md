---
ver: rpa2
title: Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking
arxiv_id: '2310.07613'
source_url: https://arxiv.org/abs/2310.07613
tags:
- path
- entity
- agent
- reasoning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the problem of misinformation detection by
  proposing a reinforcement learning (RL)-based knowledge graph (KG) reasoning approach
  for explainable fact-checking. The core method idea involves using an RL agent to
  traverse a KG and extract evidential paths that connect a fact claim's subject and
  object entities.
---

# Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking

## Quick Facts
- arXiv ID: 2310.07613
- Source URL: https://arxiv.org/abs/2310.07613
- Reference count: 40
- One-line primary result: RL-based KG reasoning approach achieves high accuracy in finding evidential paths for explainable fact-checking

## Executive Summary
This paper proposes a reinforcement learning-based approach for explainable fact-checking using knowledge graph reasoning. The method employs an RL agent to traverse a knowledge graph and extract evidential paths connecting subject and object entities in fact claims. These paths serve as human-readable explanations for the fact-checking process. A voting mechanism then classifies claims as true or false based on the extracted paths. The approach is evaluated on FB15K-237 and NELL-995 datasets, demonstrating high accuracy in finding correct paths and providing explanations for classifications.

## Method Summary
The approach uses a policy-based RL agent to traverse a knowledge graph and extract evidential paths for fact-checking. The agent receives the claim and current path as state, chooses relations to extend the path, and is rewarded when reaching the claimed target entity. Beam search with a KG embedding-based heuristic improves path quality and diversity. Multiple paths are extracted and a voting mechanism classifies the claim based on the most common tail entity among the paths. The policy network is trained using REINFORCE algorithm, while KG embeddings are learned using ComplEx model.

## Key Results
- The method achieves high accuracy in finding paths to the true target entity, especially with higher beam numbers
- Voting mechanism provides human-readable explanations for classifications, with a slight reduction in accuracy
- The approach outperforms traditional methods in terms of explainability while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL agent can find evidential paths that prove or disprove factual claims by traversing a knowledge graph.
- Mechanism: The agent receives the entire claim and current path as state, chooses relations to extend the path, and is rewarded when the final entity matches the claimed target. Beam search with a heuristic improves path diversity and quality.
- Core assumption: The KG embeddings capture sufficient semantic similarity so that a path found by the agent meaningfully connects the head entity to the target entity.
- Evidence anchors:
  - [abstract] "The RL reasoning agent computes a path that either proves or disproves a factual claim"
  - [section III-A] "The path describes the relation between those two entities and this path is used to classify a statement as true or false"
  - [corpus] Weak evidence; no direct supporting paper found in the immediate neighbor set.
- Break condition: If KG embeddings do not encode semantic similarity accurately, the agent will learn to follow irrelevant paths and fail to find correct evidential paths.

### Mechanism 2
- Claim: Voting on the most common tail entity among multiple extracted paths yields a correct veracity verdict.
- Mechanism: Multiple paths are extracted using beam search; the tail entity appearing most frequently is chosen as the predicted target. A weighted vote based on the beam search heuristic resolves ties and adds confidence weighting.
- Core assumption: When the claim is true, multiple evidential paths will converge on the same true tail entity; when false, they will not.
- Evidence anchors:
  - [section III-B] "The veracity verdict is arrived upon by grouping the extracted paths and counting all unique path tail entities"
  - [section IV-C] "the correct path and target entity may be correctly identified but still be undermined by a higher majority of an entity in the vicinity of the source entity"
  - [corpus] Weak evidence; no direct supporting paper found in the immediate neighbor set.
- Break condition: If multiple true paths lead to different tail entities (e.g., ambiguous relations like "origin"), the voting mechanism may select an incorrect entity.

### Mechanism 3
- Claim: The beam search heuristic that scores entities using the KG triple scoring function improves path quality.
- Mechanism: At each step, the probability from the policy network is augmented by a score from the KG embedding model, which evaluates how well an entity fits as the tail of the current partial path.
- Core assumption: The KG triple scoring function reliably estimates the likelihood that a candidate tail entity is correct for the current relation and head entity.
- Evidence anchors:
  - [section III-A] "This function is the base of an exponential function, where the step number is the exponent... added to the probability calculated for a given action"
  - [section IV-C] "The results show that the method is able to achieve high accuracy in finding a path to the true target entity with a high enough beam number"
  - [corpus] Weak evidence; no direct supporting paper found in the immediate neighbor set.
- Break condition: If the scoring function is poorly calibrated, the heuristic may mislead the beam search and select low-quality paths.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation for sequential decision making.
  - Why needed here: The path extraction process is modeled as an MDP where the agent takes actions (relations) in states (claim + current path) to maximize expected reward.
  - Quick check question: What are the four components of an MDP and how do they map to the path extraction process?

- Concept: Knowledge Graph Embeddings (e.g., ComplEx) and triple scoring.
  - Why needed here: KG embeddings represent entities and relations in continuous space, enabling the scoring function used in beam search and the semantic similarity check for rewards.
  - Quick check question: How does the ComplEx scoring function compute the plausibility of a triple?

- Concept: Beam search and heuristic augmentation in search algorithms.
  - Why needed here: Beam search increases the number of paths considered, and the KG-based heuristic prioritizes entities more likely to be correct, improving both recall and precision.
  - Quick check question: What is the difference between greedy search and beam search, and why does beam search help here?

## Architecture Onboarding

- Component map: KG Embedding Model -> Policy Network -> Beam Search Module -> Voting Mechanism -> MDP Environment
- Critical path: Claim -> Policy Network -> Beam Search -> Path Extraction -> Voting -> Veracity Verdict
- Design tradeoffs:
  - More beam paths -> higher recall but increased computation
  - Longer fixed path length -> more opportunity to find target but risk of irrelevant detours
  - Embedding dimensionality -> richer semantics but slower training/inference
- Failure signatures:
  - Agent consistently returns inconclusive paths (stays at source entity)
  - Voting accuracy much lower than hits@k, indicating correct paths but wrong majority
  - Beam search never finds target entity even with high beam numbers, suggesting KG sparsity or poor embeddings
- First 3 experiments:
  1. Run policy network on a small KG with known true/false claims; verify that top actions are semantically plausible relations.
  2. Perform beam search with beam=1 and beam=3 on the same data; compare tail entity distributions and hits@k.
  3. Apply the full voting mechanism; compare voting accuracy vs. hits@k to quantify the accuracy drop.

## Open Questions the Paper Calls Out

- Question: How does the performance of the RL-based approach compare to traditional multi-hop reasoning methods that rely on mined patterns or pre-extracted subgraphs?
- Basis in paper: [inferred] The paper discusses previous approaches that use mined patterns or pre-extracted subgraphs for fact-checking, but does not directly compare the performance of the RL-based approach to these methods.
- Why unresolved: The paper does not provide a direct comparison between the RL-based approach and traditional multi-hop reasoning methods in terms of accuracy, efficiency, or explainability.
- What evidence would resolve it: A direct comparison of the RL-based approach to traditional multi-hop reasoning methods on the same benchmark datasets, using the same evaluation metrics, would provide evidence to resolve this question.

## Limitations
- The voting mechanism may not handle ambiguous relations effectively, potentially leading to incorrect classifications
- Performance depends heavily on the quality and completeness of the knowledge graph embeddings
- The approach may struggle with claims requiring multi-hop reasoning in sparse parts of the KG

## Confidence
- RL agent's ability to find evidential paths: Medium
- Voting mechanism's effectiveness: Medium
- Beam search heuristic's impact on path quality: Low

## Next Checks
1. Evaluate the model's performance on claims with ambiguous relations (e.g., "origin") to assess the voting mechanism's robustness.
2. Analyze the distribution of path lengths and entity coverage to identify potential sparsity issues in the KG.
3. Conduct ablation studies on the beam search heuristic to quantify its contribution to path quality and overall accuracy.