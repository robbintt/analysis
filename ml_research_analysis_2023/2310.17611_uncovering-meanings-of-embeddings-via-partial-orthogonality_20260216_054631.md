---
ver: rpa2
title: Uncovering Meanings of Embeddings via Partial Orthogonality
arxiv_id: '2310.17611'
source_url: https://arxiv.org/abs/2310.17611
tags:
- embeddings
- markov
- independence
- boundary
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how semantic structure is encoded in the algebraic
  structure of text embeddings using partial orthogonality. The authors formalize
  semantic independence using independence models and propose partial orthogonality
  as a natural encoding of this structure in embeddings.
---

# Uncovering Meanings of Embeddings via Partial Orthogonality

## Quick Facts
- arXiv ID: 2310.17611
- Source URL: https://arxiv.org/abs/2310.17611
- Reference count: 30
- Primary result: This paper studies how semantic structure is encoded in the algebraic structure of text embeddings using partial orthogonality

## Executive Summary
This paper explores how semantic relationships between words are encoded in the algebraic structure of text embeddings. The authors formalize semantic independence using independence models and propose partial orthogonality as a natural way to represent this structure in embedding spaces. They introduce generalized Markov boundaries to capture semantic independence and prove their existence and approximation properties. Experiments on CLIP text embeddings demonstrate that partial orthogonality and generalized Markov boundaries effectively encode semantic structure.

## Method Summary
The paper proposes using partial orthogonality to define conditional independence in embedding spaces, where target embeddings are considered independent given conditioning embeddings if their residuals are orthogonal after projection. The authors introduce generalized Markov boundaries as a relaxation of classical Markov boundaries to handle noisy embeddings and non-unique solutions. They develop an algorithm to discover these boundaries by finding embeddings whose linear span minimizes average cosine similarity between target residuals and test residuals across random subspaces. The method is validated through experiments on CLIP text embeddings using cosine similarity measurements and Wu-Palmer similarity scores.

## Key Results
- Partial orthogonality effectively captures semantic independence in text embeddings
- Generalized Markov boundaries can be discovered through an approximation algorithm
- The method successfully encodes semantic structure in CLIP text embeddings
- Random projections reveal semantically meaningful embeddings

## Why This Works (Mechanism)

### Mechanism 1
Partial orthogonality captures semantic independence by ensuring residuals are orthogonal after projection onto conditioning embeddings. The method projects target and test embeddings onto the span of conditioning embeddings, then measures cosine similarity between residuals. Low similarity indicates semantic independence. Core assumption: Semantic independence between words like "eggplant" and "tomato" given "vegetable" can be represented algebraically through orthogonality of projection residuals.

### Mechanism 2
Generalized Markov boundaries capture minimal sets of embeddings that preserve semantic information about a target word. The algorithm finds embeddings whose linear span minimizes average cosine similarity between target residual and test residuals across random subspaces. Core assumption: The span of a generalized Markov boundary contains all relevant semantic information about the target word, even with noisy embeddings.

### Mechanism 3
Independence preserving embeddings can maintain conditional independence structures from random variables in vector space. The construction uses eigenvectors of perturbed adjacency matrices from minimal I-maps of distributions. Core assumption: The partial orthogonality structure in the embedding space can mirror the conditional independence structure of the underlying distribution.

## Foundational Learning

- Concept: Independence models and graphoid axioms
  - Why needed here: The paper builds on abstract independence models to formalize semantic independence, requiring understanding of symmetry, decomposition, weak union, contraction, intersection, and composition axioms
  - Quick check question: What is the key difference between a semi-graphoid and a graphoid?

- Concept: Partial orthogonality in vector spaces
  - Why needed here: This is the core algebraic structure used to represent semantic independence in embeddings, requiring understanding of projections and residual orthogonality
  - Quick check question: How does partial orthogonality differ from regular orthogonality in capturing conditional independence?

- Concept: Markov boundaries and their relaxation
  - Why needed here: The paper extends classical Markov boundary concepts to embeddings and introduces generalized Markov boundaries to handle noise and non-unique solutions
  - Quick check question: Why might the intersection axiom fail for practical embeddings, leading to non-unique Markov boundaries?

## Architecture Onboarding

- Component map: Embedding space with semantic structure -> Partial orthogonality measurement functions -> Generalized Markov boundary discovery algorithm -> Validation framework for semantic coherence
- Critical path: For a target word, the system first projects embeddings onto conditioning subspaces, measures residual similarities, discovers generalized Markov boundaries through random sampling and optimization, then validates semantic coherence
- Design tradeoffs: The method trades exact mathematical properties (like unique Markov boundaries) for practical applicability in noisy embedding spaces
- Failure signatures: High average cosine similarity values in boundary discovery indicate poor semantic separation; failure to find coherent semantic patterns in validation suggests the method isn't capturing intended structure
- First 3 experiments:
  1. Validate partial orthogonality captures semantic independence by measuring cosine similarity reduction when conditioning on category words
  2. Test generalized Markov boundary discovery by comparing semantic coherence of discovered boundaries versus random subspaces
  3. Verify random projection reveals semantically meaningful embeddings by comparing Wu-Palmer similarities before and after projection

## Open Questions the Paper Calls Out

### Open Question 1
How do independence preserving embeddings (IPE) perform in preserving semantic structures for large-scale language models with millions of tokens?
- Basis in paper: The paper discusses IPE maps and their potential for preserving independence structures in random variables, but does not explore their application to large-scale language models
- Why unresolved: The paper focuses on theoretical foundations and small-scale experiments, leaving the scalability and effectiveness of IPE in large models unexplored
- What evidence would resolve it: Empirical studies comparing semantic structures preserved by IPE in large-scale models versus traditional embeddings

### Open Question 2
Can the generalized Markov boundary approach be extended to handle multi-modal embeddings (e.g., text and image embeddings)?
- Basis in paper: The paper focuses on text embeddings and their semantic structures, but does not address multi-modal scenarios
- Why unresolved: The unique characteristics of multi-modal embeddings, such as their alignment and interaction, are not covered in the current framework
- What evidence would resolve it: Experiments demonstrating the effectiveness of generalized Markov boundaries in capturing semantic relationships in multi-modal embeddings

### Open Question 3
What are the computational trade-offs of using random projections for dimension reduction in IPE maps?
- Basis in paper: The paper mentions random projections as a method for dimension reduction in IPE maps but does not discuss computational costs or trade-offs
- Why unresolved: The theoretical guarantees for dimension reduction are provided, but practical computational implications are not explored
- What evidence would resolve it: Performance benchmarks comparing the computational efficiency of IPE maps with and without random projections

## Limitations
- The core claims about partial orthogonality as a natural encoding of semantic independence lack strong validation from external corpus evidence
- The practical applicability of generalized Markov boundaries is demonstrated but their robustness to noise and non-linear semantic structures remains unclear
- The construction of Independence Preserving Embeddings (IPE) is theoretically sound but lacks comprehensive empirical validation

## Confidence
- High: The theoretical framework of partial orthogonality and generalized Markov boundaries is well-defined and internally consistent
- Medium: The empirical evidence supporting the semantic structure preservation in CLIP text embeddings is promising but could benefit from additional validation
- Low: The construction and validation of IPE maps are theoretically proposed but lack comprehensive empirical testing

## Next Checks
1. Conduct a controlled experiment to measure the impact of noise on partial orthogonality by introducing varying levels of noise to the embeddings and observing the degradation in semantic independence detection
2. Test the robustness of generalized Markov boundaries by applying the method to embeddings from different models (e.g., BERT, GloVe) and comparing the semantic coherence of discovered boundaries
3. Validate the IPE construction by applying it to synthetic distributions with known conditional independence structures and measuring the preservation of these structures in the resulting embeddings