---
ver: rpa2
title: 'FonMTL: Towards Multitask Learning for the Fon Language'
arxiv_id: '2308.14280'
source_url: https://arxiv.org/abs/2308.14280
tags:
- language
- learning
- tasks
- task
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present the first multitask learning (MTL) approach
  for the Fon language, focusing on the tasks of Named Entity Recognition (NER) and
  Part of Speech Tagging (POS). They propose a hard parameter sharing method that
  uses two language model heads (AfroLM-Large and XLMR-Large) as encoders to build
  shared representations for the inputs, and linear layers for classification for
  each task.
---

# FonMTL: Towards Multitask Learning for the Fon Language

## Quick Facts
- arXiv ID: 2308.14280
- Source URL: https://arxiv.org/abs/2308.14280
- Authors: 
- Reference count: 10
- The first multitask learning approach for Fon language, using hard parameter sharing for NER and POS tasks

## Executive Summary
This paper presents the first multitask learning (MTL) approach for the Fon language, focusing on Named Entity Recognition (NER) and Part of Speech Tagging (POS). The authors propose a hard parameter sharing method using AfroLM-Large and XLMR-Large as dual encoders with equal-weighted loss combination. Their results show that the MTL models are competitive with and sometimes outperform single-task baselines, indicating that NER and POS tasks benefit from shared knowledge. The equal-weighted sum of both losses worked better than the unweighted sum, and training with multilingual data improved performance compared to training solely on Fon data.

## Method Summary
The paper uses a hard parameter sharing approach for multitask learning on Fon NER and POS tasks. The model employs two language model encoders (AfroLM-Large and XLMR-Large) to build shared representations through multiplicative combination, with separate linear classification layers for each task. The authors use equal weighting (α=β=0.5) for the task-specific losses and train with AdamW optimizer (learning rate 3e-5) for 50 epochs. The Fon datasets used are MaskhaNER 2.0 (4343/621/1240 train/dev/test tokens) for NER and MasakhaPOS (798/159/637 train/dev/test tokens) for POS.

## Key Results
- MTL models are competitive with and sometimes outperform single-task baselines
- Equal-weighted sum of losses worked better than unweighted sum
- Training with multilingual data improved performance compared to Fon-only training
- The approach shows promise for low-resource African languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard parameter sharing across NER and POS tasks enables cross-task knowledge transfer that improves generalization in low-resource settings.
- Mechanism: The model shares all layers except the final classification heads, forcing both tasks to learn common linguistic features from the same representations.
- Core assumption: NER and POS tasks share sufficient linguistic structure that common representations will be useful for both.
- Evidence anchors:
  - [abstract] "Multitask learning is a learning paradigm that aims to improve the generalization capacity of a model by sharing knowledge across different but related tasks"
  - [section] "both tasks data and representations are informative and useful for the downstream performances"
- Break condition: If NER and POS share little common structure, the shared layers may learn conflicting representations.

### Mechanism 2
- Claim: Using two different language model encoders captures complementary linguistic information.
- Mechanism: Each encoder captures different aspects of the language (AfroLM pretrained on African languages, XLM-R on 100+ languages), and multiplicative combination amplifies the shared signal.
- Core assumption: Different pretrained models capture complementary linguistic features that are both useful for Fon NER and POS.
- Evidence anchors:
  - [section] "We use two language model (LM) heads: AfroLM-Large and XLMR-Large... Each LM head is used to build representations of the inputs from each task, and both representations are then combined (in a multiplicative way)"
- Break condition: If the two encoders produce redundant or conflicting representations, the multiplicative combination may not improve performance.

### Mechanism 3
- Claim: Equal weighting of task-specific losses (α = β = 0.5) balances the learning signals from NER and POS tasks.
- Mechanism: During backpropagation, both tasks contribute equally to updating shared parameters, preventing one task from dominating the shared representation.
- Core assumption: NER and POS tasks are equally important and benefit equally from the shared representation.
- Evidence anchors:
  - [section] "we perform a few ablation studies to leverage the efficiency of two different loss combination strategies and find out that the equal loss weighting approach works best in our case"
  - [section] "we can see that the equal-weighted sum of both losses worked better than the unweighted sum"
- Break condition: If one task is significantly harder or more important than the other, equal weighting may slow convergence or lead to suboptimal performance.

## Foundational Learning

- Concept: Hard parameter sharing in multitask learning
  - Why needed here: The paper uses hard parameter sharing as the primary multitask learning approach, sharing all layers except task-specific heads
  - Quick check question: What is the key difference between hard parameter sharing and soft parameter sharing in multitask learning?

- Concept: Loss combination strategies in multitask learning
  - Why needed here: The paper compares unweighted sum vs equal weighting of task losses, finding equal weighting superior
  - Quick check question: How does equal weighting of task losses differ from unweighted sum in terms of backpropagation?

- Concept: Transfer learning with multilingual pretrained models
  - Why needed here: The model uses AfroLM (pretrained on 23 African languages) and XLM-R (pretrained on 100+ languages) as feature extractors
  - Quick check question: What is the key advantage of using multilingual pretrained models for low-resource languages like Fon?

## Architecture Onboarding

- Component map: Input → AfroLM-Large → XLM-R-Large → Multiplicative combination → Shared representation → NER linear layer → NER loss + POS linear layer → POS loss → Backpropagation

- Critical path: Input → Dual encoders → Feature mixer → Shared representation → Task heads → Losses → Backpropagation

- Design tradeoffs:
  - Using two encoders vs one: Captures more diverse linguistic features but increases computational cost
  - Equal weighting vs dynamic weighting: Simpler to implement but may not adapt to task difficulty during training
  - Multiplicative vs additive combination: May capture interactions between features but could amplify noise

- Failure signatures:
  - Both tasks perform worse than single-task baselines: Shared representation may be conflicting
  - One task performs well, other performs poorly: Loss weighting may be unbalanced
  - Training diverges: Learning rate may be too high or task losses may have different scales

- First 3 experiments:
  1. Train with AfroLM only as encoder, compare to baseline
  2. Train with XLM-R only as encoder, compare to baseline
  3. Train with both encoders but with unweighted sum of losses, compare to equal weighting approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific reasons why training on multilingual data improves performance for low-resource languages like Fon in multitask learning settings?
- Basis in paper: [explicit] The authors state that their results show training with data from all languages helped downstream performance, but they plan to investigate why this is the case in future work.
- Why unresolved: While the authors observe this phenomenon, they do not provide an explanation for the underlying mechanisms that cause multilingual training to be beneficial for low-resource languages in multitask learning.
- What evidence would resolve it: Controlled experiments comparing multilingual vs monolingual training on related and unrelated languages, analysis of feature overlap and transfer between languages, and investigation of linguistic distances and typological similarities.

### Open Question 2
- Question: How does the effectiveness of multitask learning for Fon vary across different NLP tasks and language families?
- Basis in paper: [inferred] The authors only explore multitask learning for NER and POS tasks on Fon, but do not investigate other NLP tasks or compare to languages from different families.
- Why unresolved: The paper does not provide evidence on how multitask learning would perform on other NLP tasks (e.g., sentiment analysis, machine translation) or for languages from different families with varying degrees of similarity to Fon.
- What evidence would resolve it: Systematic experiments on a diverse set of NLP tasks and languages from different families, with analysis of task and language relatedness effects on multitask learning performance.

### Open Question 3
- Question: What are the optimal strategies for combining and weighting losses in multitask learning for low-resource languages?
- Basis in paper: [explicit] The authors experiment with unweighted sum and equal weighting of losses, finding the latter to work better, but acknowledge the need to explore dynamic weighted average loss strategies.
- Why unresolved: The paper does not investigate the full range of loss combination and weighting strategies, nor does it provide insights into how the optimal strategy may depend on task characteristics, data size, or language relatedness.
- What evidence would resolve it: Extensive experiments on different loss combination and weighting strategies, including dynamic approaches, with analysis of their performance across various tasks, data sizes, and language scenarios.

## Limitations

- Extremely small dataset sizes (4,343 and 798 training tokens) raise questions about generalizability
- Limited ablation studies on encoder contributions and loss weighting strategies
- No theoretical justification for why NER and POS tasks should benefit from shared representations
- No linguistic analysis of Fon to support the MTL approach

## Confidence

- High confidence: The technical implementation of hard parameter sharing is correctly described and follows standard MTL methodology
- Medium confidence: The empirical results showing competitive performance with baselines are reproducible, but the interpretation of why MTL helps is speculative
- Low confidence: Claims about the specific mechanisms (dual encoders, equal weighting) being optimal for Fon MTL are not well-supported by ablation studies

## Next Checks

1. **Ablation study on encoder contribution**: Train and evaluate models using only AfroLM, only XLM-R, and both encoders to quantify the individual and combined contributions to performance.

2. **Loss weighting hyperparameter search**: Systematically explore different values of α and β (e.g., 0.2/0.8, 0.8/0.2, 0.3/0.7) to determine if the equal weighting is truly optimal or if task-specific weighting provides better results for Fon.

3. **Linguistic analysis of task complementarity**: Conduct a detailed linguistic analysis of Fon to identify specific features where NER and POS tasks overlap (e.g., morphological markers, syntactic patterns) and where they might conflict, providing theoretical justification for the MTL approach beyond empirical results.