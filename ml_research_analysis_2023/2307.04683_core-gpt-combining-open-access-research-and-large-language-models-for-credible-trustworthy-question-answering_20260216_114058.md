---
ver: rpa2
title: 'CORE-GPT: Combining Open Access research and large language models for credible,
  trustworthy question answering'
arxiv_id: '2307.04683'
source_url: https://arxiv.org/abs/2307.04683
tags:
- core-gpt
- answers
- domains
- core
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that GPT-3.5 and GPT-4 cannot be relied
  upon to provide accurate citations, with 72.5% and 71.2% of citations being fictional,
  respectively. To address this, CORE-GPT was developed, combining GPT models with
  over 32 million open access articles from CORE.
---

# CORE-GPT: Combining Open Access research and large language models for credible, trustworthy question answering

## Quick Facts
- arXiv ID: 2307.04683
- Source URL: https://arxiv.org/abs/2307.04683
- Reference count: 19
- Primary result: CORE-GPT generates evidence-based answers with genuine citations, reducing LLM hallucinations from 71-72% to <25% across 100 questions in 20 domains

## Executive Summary
CORE-GPT addresses the critical problem of hallucination in large language models by grounding answers in real scientific literature. The system combines GPT models with over 32 million open access articles from CORE to produce evidence-based responses with genuine citations. Evaluation across 100 questions in 20 scientific domains shows that CORE-GPT significantly outperforms standard GPT models in citation accuracy while maintaining high answer quality, with strong correlations between citation relevance and answer comprehensiveness, trust, and utility scores.

## Method Summary
CORE-GPT employs a three-stage workflow: first, GPT-4 generates an enriched search query from the user's question; second, the CORE API retrieves the five most relevant papers based on this query; third, GPT-4 generates a comprehensive answer based solely on the titles and abstracts of the retrieved papers, with explicit instructions to cite sources accurately. The system is evaluated using 100 questions across 20 scientific domains, with answers scored on comprehensiveness, trustworthiness, utility, and citation relevance by multiple annotators.

## Key Results
- Standard GPT-3.5 and GPT-4 models generate 72.5% and 71.2% fictional citations respectively
- CORE-GPT achieves high answer quality (comprehensiveness, trust, utility scores of 8+) in 75% of scientific domains
- Strong correlation (r = 0.77-0.83) between citation relevance and answer quality metrics
- The system successfully references recently published research, overcoming the knowledge cutoff issue of standard LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models generate hallucinated citations when asked to provide references.
- Mechanism: LLMs are autoregressive predictive models trained on statistical patterns rather than curated knowledge. When prompted for citations, they fabricate plausible-looking references by matching structural patterns from training data rather than verifying existence.
- Core assumption: GPT models lack internal mechanisms for validating reference existence.
- Evidence anchors:
  - "GPT3.5 and GPT4 cannot be relied upon to provide references or citations for generated text" and "72.5% and 71.2% of citations being fictional"
  - "LLMs have demonstrated remarkable capabilities... However, these are predictive models and cannot be relied upon to provided reliable sources or citations for any generated text"

### Mechanism 2
- Claim: Constraining GPT outputs to content from retrieved abstracts reduces hallucinations.
- Mechanism: By limiting the model's input to specific abstracts and explicitly instructing it to base answers only on that content, the model's generation is bounded within verified source material, preventing fabrication.
- Core assumption: GPT-4 can accurately process and synthesize information from short input passages when explicitly constrained.
- Evidence anchors:
  - "This critical third stage is largely effective at constraining the model to base its reply only on the supplied input"
  - "Generate a comprehensive answer... solely based on the content provided"

### Mechanism 3
- Claim: Correlation between citation relevance and answer quality indicates literature grounding drives answer credibility.
- Mechanism: When retrieved references are highly relevant to the question, the model can produce more accurate, comprehensive, and trustworthy answers because the source material directly addresses the query.
- Core assumption: Relevance of source material directly influences the quality of synthesized answers.
- Evidence anchors:
  - "we found that there is a very strong correlation between the relevance of the retrieved references and comprehensiveness, trust and utility across domains"
  - "The results reported in Table 4 indeed confirms this trend"

## Foundational Learning

- Concept: Information Retrieval and Semantic Search
  - Why needed here: CORE-GPT relies on retrieving the most relevant papers from a large corpus using search queries derived from user questions.
  - Quick check question: How would you design a search query to retrieve papers about "machine learning applications in healthcare"?

- Concept: Large Language Model Prompt Engineering
  - Why needed here: The system's effectiveness depends on carefully crafted prompts that instruct the model to base answers on provided content and format citations correctly.
  - Quick check question: What prompt instructions would you give GPT-4 to ensure it only answers based on supplied abstracts?

- Concept: Inter-Annotator Agreement and Evaluation Metrics
  - Why needed here: The evaluation uses Cohen's Kappa to measure agreement between annotators scoring answer quality and citation relevance.
  - Quick check question: Why is Cohen's Kappa with quadratic weights more appropriate than simple agreement percentage for this evaluation?

## Architecture Onboarding

- Component map:
  User Interface -> Query Processor -> CORE API -> Answer Generator -> Results Display

- Critical path:
  1. User submits question
  2. Query processor generates enriched search query
  3. CORE API retrieves relevant papers
  4. Answer generator produces response based on abstracts
  5. Results displayed to user with citations

- Design tradeoffs:
  - Token limits vs. answer quality: Using only abstracts limits context but enables processing within GPT-4's token constraints
  - Retrieval precision vs. coverage: Retrieving only 5 papers balances relevance with computational efficiency
  - Open Access limitation vs. comprehensiveness: Excluding paywalled content reduces available literature but maintains accessibility

- Failure signatures:
  - Low comprehensiveness scores across domains suggest retrieval system is not finding relevant papers
  - Low trust scores indicate model may be ignoring constraints and generating unsupported content
  - High variance in citation relevance scores suggests inconsistent retrieval quality across domains

- First 3 experiments:
  1. Test whether increasing the number of retrieved papers (from 5 to 10) improves answer quality and comprehensiveness
  2. Evaluate the impact of using full-text content instead of abstracts on answer quality (when token limits permit)
  3. Measure how answer quality changes when using different prompt formulations for constraining the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of CORE-GPT's answers vary across different scientific domains?
- Basis in paper: The paper states that CORE-GPT performs exceptionally well across most domains, but is less successful in a few areas, with lower scores in domains like Geology, History, and Art.
- Why unresolved: The paper does not provide a detailed analysis of why certain domains have lower performance.
- What evidence would resolve it: A detailed analysis of the specific factors contributing to the variation in answer quality across different domains.

### Open Question 2
- Question: Does the length of the abstracts used by CORE-GPT influence the quality of the generated answers?
- Basis in paper: The paper mentions that there is a wide variance in mean abstract length across the domains, but observes no correlation between these scores and the mean abstract lengths.
- Why unresolved: The paper does not provide a conclusive answer on whether abstract length affects answer quality.
- What evidence would resolve it: A study that manipulates abstract length and measures the resulting answer quality to determine if there is a causal relationship.

### Open Question 3
- Question: How does the relevance of retrieved references impact the quality of CORE-GPT's answers?
- Basis in paper: The paper finds a very strong correlation between the relevance of retrieved references and the comprehensiveness, trust, and utility of the answers.
- Why unresolved: The paper does not explore the specific mechanisms by which reference relevance influences answer quality.
- What evidence would resolve it: An investigation into the specific ways in which the relevance of references affects the content and quality of the generated answers.

## Limitations

- CORE's coverage is limited to open access literature, excluding potentially relevant paywalled research
- The evaluation relied on 100 questions across 20 domains, which may not capture all edge cases
- Answer quality is heavily dependent on the retrieval system's ability to find relevant papers

## Confidence

- High Confidence: Claims about GPT-3.5 and GPT-4 hallucination rates (72.5% and 71.2% fictional citations) are well-supported by the cited evaluation results
- Medium Confidence: Claims about CORE-GPT's comprehensive, trustworthy, and useful answers (75% of domains achieving high scores) are supported by evaluation but limited by the relatively small question set
- Medium Confidence: The strong correlation (r = 0.77-0.83) between citation relevance and answer quality is demonstrated but may vary with different retrieval algorithms or question types

## Next Checks

1. Test CORE-GPT's performance on questions requiring integration of information from multiple papers to verify its ability to synthesize complex, multi-source answers
2. Evaluate the system's performance on questions from domains with sparse open access literature to assess coverage limitations
3. Compare CORE-GPT's citation accuracy and answer quality against other LLM-based scientific QA systems using both open and closed access literature