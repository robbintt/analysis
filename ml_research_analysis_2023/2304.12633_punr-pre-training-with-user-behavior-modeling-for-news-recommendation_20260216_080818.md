---
ver: rpa2
title: 'PUNR: Pre-training with User Behavior Modeling for News Recommendation'
arxiv_id: '2304.12633'
source_url: https://arxiv.org/abs/2304.12633
tags:
- user
- news
- behavior
- pre-training
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes PUNR, a pre-training method for user behavior
  modeling in news recommendation. PUNR introduces two pre-training tasks: user behavior
  masking and user behavior generation.'
---

# PUNR: Pre-training with User Behavior Modeling for News Recommendation

## Quick Facts
- arXiv ID: 2304.12633
- Source URL: https://arxiv.org/abs/2304.12633
- Reference count: 11
- Primary result: PUNR achieves AUC scores of 68.89 on MIND-Small and 71.03 on MIND-Large datasets

## Executive Summary
This paper proposes PUNR, a pre-training method specifically designed for user behavior modeling in news recommendation. PUNR introduces two novel pre-training tasks: user behavior masking, which recovers masked user behaviors using contextual information, and user behavior generation, which enhances user representation vectors through auto-regressive generation. The method employs a Siamese encoder architecture that shares parameters between news and user encoders, enabling better alignment between news and user representations. Experiments on MIND datasets demonstrate significant performance improvements over existing baselines across multiple evaluation metrics.

## Method Summary
PUNR pre-trains a Siamese encoder architecture using two tasks: user behavior masking and user behavior generation. During pre-training, the model learns to recover masked user behavior spans using contextual information and generates entire user behavior texts based on user representations. The same pre-trained encoder is then shared between news and user encoders during fine-tuning for the recommendation task. The model uses dot product similarity with negative sampling to rank candidate news items for users.

## Key Results
- Achieves AUC scores of 68.89 on MIND-Small dataset
- Achieves AUC scores of 71.03 on MIND-Large dataset
- Demonstrates significant performance improvements over existing baselines across multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
User behavior masking improves model's ability to recover masked behavior spans using contextual information. Random masking of entire news segments in user history forces the model to predict masked content based on remaining unmasked contextual behaviors during pre-training. Core assumption: Contextual user behaviors contain sufficient information to recover masked behaviors, enabling stronger pattern learning.

### Mechanism 2
User behavior generation enhances user representation vectors through auto-regressive generation of entire user history. A single-layer auto-regression decoder generates entire user behavior texts based on user vector, compressing token-level information into the representation. Core assumption: The decoder needs to rely on user vector information to complete generation, thus enriching user representation during backpropagation.

### Mechanism 3
Siamese architecture sharing parameters between news and user encoders improves alignment and performance. Using the same pre-trained encoder for both news and user representations creates better vector alignment through shared parameter space. Core assumption: Shared parameters between news and user encoders create more meaningful similarity comparisons through better alignment.

## Foundational Learning

- **Pre-trained Language Models (PLMs) like BERT**
  - Why needed here: PLMs provide strong text representation capabilities that can be fine-tuned for specific tasks like news recommendation
  - Quick check question: What is the main advantage of using PLMs over training from scratch for NLP tasks?

- **Masked Language Modeling (MLM) pre-training**
  - Why needed here: MLM task helps the model learn contextual representations by predicting masked tokens based on surrounding context
  - Quick check question: How does the masking ratio affect the difficulty of the MLM task?

- **Siamese Networks**
  - Why needed here: Siamese architecture allows sharing of learned representations between different input types (news and user behaviors)
  - Quick check question: What is the main benefit of using a Siamese network compared to separate encoders?

## Architecture Onboarding

- **Component map:**
  User Encoder (Siamese) -> Processes concatenated user clicked news
  News Encoder (Siamese) -> Processes candidate news
  User Behavior Masking Task -> MLM on masked user behaviors
  User Behavior Generation Task -> Auto-regressive generation of user history
  Fine-tuning Layer -> Dot product similarity with negative sampling

- **Critical path:**
  1. Pre-training with user behavior masking and generation tasks
  2. Sharing pre-trained encoder for both news and user representations
  3. Fine-tuning on recommendation task with dot product similarity

- **Design tradeoffs:**
  - Siamese vs separate encoders: Siamese provides better alignment but may limit specialization
  - Masking ratio: Higher ratios increase difficulty but may hurt recovery ability
  - Decoder initialization: Pre-training decoder on general corpus improves generation quality

- **Failure signatures:**
  - Poor pre-training convergence: May indicate inappropriate masking ratios or generation task difficulty
  - Fine-tuning performance worse than baseline: Could indicate over-regularization from pre-training
  - Mode collapse in generation: May suggest decoder relying too heavily on patterns rather than user vector

- **First 3 experiments:**
  1. Test different total masking ratios (15%, 30%, 45%) to find optimal balance
  2. Compare Siamese vs separate encoders in fine-tuning stage
  3. Test different pooling methods (CLS, Average, Attention) for user vector extraction

## Open Questions the Paper Calls Out

### Open Question 1
How does the user behavior masking pre-training task contribute to the effectiveness of user modeling in news recommendation? The paper introduces the user behavior masking task to recover masked user behaviors based on contextual behaviors, aiming to capture a stronger and more comprehensive user news reading pattern. The paper does not provide a detailed analysis of the specific contributions of the user behavior masking task to the effectiveness of user modeling. Comparative experiments evaluating the performance of PUNR with and without the user behavior masking task would provide insights into its contribution.

### Open Question 2
How does the user behavior generation pre-training task enhance the user representation vector in news recommendation? The paper incorporates the user behavior generation task to enhance the user representation vector derived from the user encoder. The paper does not provide a detailed analysis of the specific mechanisms by which the user behavior generation task enhances the user representation vector. Comparative experiments evaluating the performance of PUNR with and without the user behavior generation task would provide insights into its contribution.

### Open Question 3
How do the total mask ratio and user behavior masking ratio influence the performance of PUNR? The paper mentions that the total mask ratio and user behavior masking ratio are hyperparameters that can be adjusted, but it does not provide a detailed analysis of their influence on the performance of PUNR. The paper does not provide a systematic study on the impact of different values of the total mask ratio and user behavior masking ratio on the performance of PUNR. Experiments evaluating the performance of PUNR with different values of the total mask ratio and user behavior masking ratio would provide insights into their influence.

## Limitations

- Experimental validation limited to MIND datasets, which may not generalize to other news recommendation scenarios
- Lack of ablation studies to quantify relative importance of each pre-training task
- Theoretical analysis of shared parameters benefits is primarily empirical rather than providing rigorous mathematical justification

## Confidence

- User Behavior Masking Effectiveness: Medium
- User Behavior Generation Impact: Low-Medium
- Siamese Architecture Benefits: Medium

## Next Checks

1. **Ablation Study on Pre-training Tasks**: Remove the user behavior generation task and measure the performance drop to quantify its actual contribution beyond the masking task alone.

2. **Masking Strategy Comparison**: Implement and compare PUNR with token-level masking (BERT-style) versus the proposed segment-level masking to empirically validate the claimed superiority of the segment approach.

3. **Encoder Architecture Comparison**: Train PUNR with separate news and user encoders (no parameter sharing) to quantify the actual performance gain from the Siamese architecture and test whether the claimed alignment benefit holds across different datasets.