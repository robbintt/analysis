---
ver: rpa2
title: 'JCoLA: Japanese Corpus of Linguistic Acceptability'
arxiv_id: '2309.12676'
source_url: https://arxiv.org/abs/2309.12676
tags:
- language
- japanese
- data
- sentences
- acceptability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JCoLA, a Japanese dataset for syntactic evaluation
  of language models. JCoLA consists of 10,020 sentences extracted from textbooks,
  handbooks, and journal articles, and split into in-domain (86%) and out-of-domain
  (14%) data.
---

# JCoLA: Japanese Corpus of Linguistic Acceptability

## Quick Facts
- arXiv ID: 2309.12676
- Source URL: https://arxiv.org/abs/2309.12676
- Reference count: 27
- Key outcome: Several Japanese language models surpass human performance on in-domain syntactic acceptability data but struggle with complex out-of-domain linguistic phenomena

## Executive Summary
This paper introduces JCoLA, a Japanese dataset for syntactic evaluation of language models, consisting of 10,020 sentences extracted from textbooks, handbooks, and journal articles. The dataset is split into in-domain (86%) and out-of-domain (14%) data, with the latter categorized by 12 linguistic phenomena. The authors evaluate 9 different Japanese language models on JCoLA, finding that while several models surpass human performance on in-domain data, none exceed human performance on out-of-domain data. Error analyses reveal that models perform well on local syntactic dependencies like argument structure but struggle with long-distance dependencies such as verbal agreement and NPI licensing.

## Method Summary
The authors created JCoLA by extracting 10,020 Japanese sentences from textbooks, handbooks, and journal articles, then categorizing them into in-domain and out-of-domain splits. They fine-tuned 9 different Japanese language models (BERT, RoBERTa, XLM-RoBERTa variants) for 5 epochs using AdamW optimizer with linear warmup. Models were evaluated using Matthews Correlation Coefficient (MCC) and accuracy for binary classification of sentence acceptability. The out-of-domain data was categorized into 12 linguistic phenomena to analyze model performance across different syntactic structures.

## Key Results
- Several models surpassed human performance on in-domain data (86% of corpus)
- No models exceeded human performance on out-of-domain data (14% of corpus)
- Models perform well on local dependencies (argument structure, filler-gap) but struggle with long-distance dependencies (verbal agreement, NPI/NCI licensing)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-domain data performance surpasses human-level accuracy because models learn simpler grammatical rules (Class II judgments) reliably.
- Mechanism: Language models are trained on abundant in-domain examples from textbooks and handbooks, capturing frequent, straightforward syntactic patterns.
- Core assumption: The in-domain data contains fewer complex phenomena and is easier to model due to simpler, more regular grammatical structures.
- Evidence anchors:
  - [abstract]: "The results demonstrated that several models could surpass human performance for the in-domain data"
  - [section 5.1]: "In the in-domain data, several models demonstrate performance surpassing that of human individuals"
- Break condition: If the in-domain data includes subtle or infrequent syntactic phenomena, models may fail to generalize even within this set.

### Mechanism 2
- Claim: Out-of-domain performance lags because models struggle with complex, theoretically significant phenomena (Class III judgments) not well represented in training data.
- Mechanism: Out-of-domain examples from journal articles introduce complex linguistic dependencies like long-distance agreement and NPI licensing, which require deeper syntactic understanding.
- Core assumption: The complexity and rarity of these phenomena make them harder to learn from limited examples.
- Evidence anchors:
  - [abstract]: "no models were able to exceed human performance for the out-of-domain data"
  - [section 5.2]: "language models show lower accuracy on linguistic phenomena such as NPI/NCI and verbal agreement"
- Break condition: If the training corpus is expanded to include more complex examples, models might better handle out-of-domain data.

### Mechanism 3
- Claim: Models perform well on local dependencies (e.g., argument structure) but poorly on long-distance dependencies (e.g., verbal agreement, NPI licensing).
- Mechanism: Local dependencies involve nearby words and simpler patterns, which are easier for models to capture, while long-distance dependencies require maintaining context over longer spans.
- Core assumption: The model's architecture and training data emphasize local patterns, making long-distance dependencies more challenging.
- Evidence anchors:
  - [section 5.2]: "language models perform relatively well on certain linguistic phenomena, such as binding, argument structure, and filler-gap, but struggle with others"
  - [section 5.2]: "language models show lower accuracy on linguistic phenomena such as NPI/NCI and verbal agreement"
- Break condition: If the model architecture is modified to better handle long-range dependencies, performance on these phenomena may improve.

## Foundational Learning

- Concept: Acceptability judgment tasks
  - Why needed here: Understanding how humans judge sentence acceptability is key to interpreting model performance on JCoLA.
  - Quick check question: What distinguishes a grammatical from an ungrammatical sentence in acceptability judgments?

- Concept: Local vs. long-distance syntactic dependencies
  - Why needed here: Models handle these dependencies differently, affecting performance on various linguistic phenomena.
  - Quick check question: Can you give an example of a local dependency and a long-distance dependency?

- Concept: In-domain vs. out-of-domain data
  - Why needed here: Differentiating these data types explains why models perform differently on them.
  - Quick check question: Why might a model perform better on in-domain data than out-of-domain data?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing -> Model training -> Evaluation -> Error analysis
- Critical path: 1. Data collection and preprocessing 2. Model training and evaluation 3. Error analysis by linguistic phenomena
- Design tradeoffs: Using in-domain data for training ensures model familiarity but may limit generalization to complex phenomena. Including out-of-domain data for evaluation tests generalization but may highlight model weaknesses.
- Failure signatures: Poor performance on out-of-domain data indicates difficulty with complex linguistic phenomena. High variance in model performance across linguistic phenomena suggests uneven learning.
- First 3 experiments:
  1. Evaluate model performance on a balanced subset of linguistic phenomena to identify strengths and weaknesses.
  2. Test model generalization by training on a mix of in-domain and out-of-domain data.
  3. Analyze the impact of tokenization methods on model performance, particularly for long-distance dependencies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do some language models perform better on out-of-domain data despite its increased complexity?
- Basis in paper: [explicit] The authors note that models using SentencePiece with a unigram language model for tokenization exhibited higher performance on out-of-domain data, but the reason is unclear.
- Why unresolved: The paper observes this pattern but does not provide a definitive explanation for why this specific tokenization method leads to better generalization.
- What evidence would resolve it: Systematic experiments comparing different tokenization methods on both in-domain and out-of-domain data, along with ablation studies to isolate the effect of tokenization on model performance.

### Open Question 2
- Question: How do the linguistic phenomena categories in JCoLA relate to each other, and can this relationship be leveraged to improve model performance?
- Basis in paper: [inferred] The authors mention that examples in JCoLA can belong to multiple linguistic phenomena categories, but they do not explore how these categories interact or influence each other.
- Why unresolved: The paper does not investigate the relationships between different linguistic phenomena or how they might affect model learning and generalization.
- What evidence would resolve it: Analysis of correlations between different linguistic phenomena categories and their impact on model performance, potentially using techniques like hierarchical classification or multi-task learning.

### Open Question 3
- Question: What specific aspects of long-distance dependencies make them particularly challenging for language models?
- Basis in paper: [explicit] The authors observe that models struggle with long-distance dependencies like verbal agreement and NPI licensing, but do not explore the underlying reasons.
- Why unresolved: The paper identifies the difficulty but does not delve into the specific linguistic or computational factors that contribute to this challenge.
- What evidence would resolve it: Detailed error analysis of model predictions on long-distance dependencies, comparison with human processing of these structures, and computational experiments to isolate the factors contributing to model difficulty.

## Limitations

- The out-of-domain data comprises only 14% of the total corpus, potentially limiting the representation of complex syntactic phenomena
- The methodology for handling context-dependent unacceptability is not fully specified
- Human performance baseline may not capture the full range of individual variation in acceptability judgments

## Confidence

- **High Confidence**: The observation that multiple models surpass human performance on in-domain data is well-supported by the experimental results and aligns with known patterns in NLP where models can excel on frequently encountered patterns.
- **Medium Confidence**: The claim about models struggling with long-distance dependencies is reasonably supported but could benefit from more detailed analysis of specific failure cases and their relationship to model architecture choices.
- **Medium Confidence**: The categorization of linguistic phenomena and their impact on model performance is methodologically sound but may not capture all relevant factors affecting model behavior.

## Next Checks

1. Verify the distribution of linguistic phenomena in the out-of-domain data to ensure it adequately represents the full range of Japanese syntactic complexity.
2. Evaluate whether models trained on English CoLA data show similar patterns of performance when applied to Japanese JCoLA, controlling for language-specific factors.
3. Systematically compare different model architectures (RNN, Transformer, etc.) and their ability to handle long-distance dependencies in Japanese, controlling for training data and parameter count.