---
ver: rpa2
title: 'Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models'
arxiv_id: '2310.01290'
source_url: https://arxiv.org/abs/2310.01290
tags:
- blank
- acted
- constraints
- knowledge
- film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge Crosswords introduces a geometric knowledge reasoning
  benchmark where LLMs must infer missing entities in incomplete knowledge networks
  under structured factual constraints. The dataset includes 2,101 problems with three
  difficulty levels, requiring abilities such as backtracking, verification, and handling
  uncertainty.
---

# Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2310.01290
- Source URL: https://arxiv.org/abs/2310.01290
- Reference count: 38
- Primary result: LLMs struggle with geometric knowledge reasoning when distractors satisfy partial constraints; Verify-All approach improves performance by 16.3% PC and 15.5% FC on hard problems

## Executive Summary
Knowledge Crosswords introduces a geometric knowledge reasoning benchmark where LLMs must infer missing entities in incomplete knowledge networks under structured factual constraints. The dataset includes 2,101 problems with three difficulty levels, requiring abilities such as backtracking, verification, and handling uncertainty. Experiments show baseline LLMs struggle with harder problems and distractors that satisfy partial constraints. Two new methods, Staged Prompting and Verify-All, improve performance by enabling backtracking and constraint verification, with Verify-All achieving the best results and robustness on harder problems.

## Method Summary
The paper constructs a geometric knowledge reasoning benchmark by filtering YAGO knowledge graphs, sampling subgraphs, masking entities to create blanks, and generating distractors at different difficulty levels. The benchmark evaluates LLM performance on inferring missing entities while satisfying all given constraints. Two proposed approaches—Staged Prompting and Verify-All—promote backtracking and verification abilities through structured prompting strategies. The methods are evaluated using Partial-Credit (PC) and Full-Credit (FC) metrics across three difficulty levels and two knowledge settings (open-book vs closed-book).

## Key Results
- Baseline LLMs show significant performance drops on harder problems and when no relevant knowledge is provided
- Verify-All approach achieves 16.3% PC and 15.5% FC improvements on hard problems compared to best baseline
- LLMs remain sensitive to option order, structural patterns, and absence of "none of the above" instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geometric reasoning in LLMs fails when distractors satisfy partial constraints because models lack robust constraint verification and backtracking
- Mechanism: LLMs rely on left-to-right reasoning patterns, so when an incorrect answer satisfies some constraints, the model continues without verifying the full constraint set, leading to accumulation of errors
- Core assumption: Left-to-right reasoning is the dominant pattern in LLM inference, and models do not naturally backtrack or verify intermediate states
- Evidence anchors: [abstract] "baseline approaches struggle with larger knowledge networks and semantically-equivalent entity distractors"; [section] "all baseline approaches struggle with harder problems and have significant performance drops when no relevant knowledge is provided"

### Mechanism 2
- Claim: Staged Prompting improves performance by forcing sequential constraint satisfaction with verification at each step
- Mechanism: By breaking the problem into stages where each blank is solved based on remaining constraints, the model can verify partial solutions before proceeding, preventing error propagation
- Core assumption: LLMs can follow stage-by-stage instructions and correctly update the constraint set after each blank is filled
- Evidence anchors: [abstract] "we introduce two instruction-based methods that promote these abilities"; [section] "At the beginning of each stage, LLMs maintain a current status of solved blanks and unresolved constraints"

### Mechanism 3
- Claim: Verify-All improves performance by considering all candidate combinations simultaneously and verifying them against all constraints before committing to an answer
- Mechanism: By proposing all candidates for each blank upfront and then systematically verifying the complete constraint set, the model can detect conflicts that would be missed in sequential reasoning
- Core assumption: LLMs can generate multiple candidate combinations and perform exhaustive verification of all constraints
- Evidence anchors: [abstract] "we additionally propose the V ERIFY -ALL approach: candidates for each blank are simultaneously proposed"; [section] "A verification step is then employed to assess the validity of all filled constraints"

## Foundational Learning

- Concept: Constraint satisfaction problems
  - Why needed here: The benchmark requires finding solutions that satisfy all given constraints simultaneously, which is fundamentally a constraint satisfaction problem
  - Quick check question: If you have constraints (A, married to, B), (A, has gender, male), and options A: female/male, B: Joan/Suzanne, which combination satisfies all constraints?

- Concept: Graph traversal and subgraph sampling
  - Why needed here: The benchmark is built from subgraphs of knowledge graphs, requiring understanding of how to sample and manipulate graph structures
  - Quick check question: If you start with a 5-hop neighborhood of a node and need a subgraph of size 8, what strategies can you use to select which nodes to keep?

- Concept: Negative sampling for multiple-choice questions
  - Why needed here: The benchmark uses distractors at different difficulty levels, requiring systematic approaches to generate semantically plausible wrong answers
  - Quick check question: Given a correct answer and relation, what criteria would you use to generate distractors that satisfy 0, 1, or all but one of the constraints?

## Architecture Onboarding

- Component map: Knowledge graph → Question generation → Prompt construction → Model inference → Answer verification → Performance evaluation
- Critical path: Knowledge graph → Question generation → Prompt construction → Model inference → Answer verification → Performance evaluation
- Design tradeoffs: Context window vs. completeness (longer prompts can include more constraints but may exceed model limits); Distractor difficulty vs. solvability (harder distractors make problems more challenging but may become unsolvable); External knowledge vs. internal knowledge (providing relevant knowledge helps but may introduce confounding factors)
- Failure signatures: Low FC but high PC (model finds some blanks correctly but fails to satisfy all constraints together); Performance drops with option order changes (model is sensitive to presentation order rather than content); No improvement with more exemplars (model cannot learn the reasoning pattern from examples)
- First 3 experiments: 1) Run baseline ZERO-SHOT on easy subset to establish performance floor; 2) Test VERIFY-ALL on hard subset with and without relevant knowledge to measure knowledge utilization; 3) Evaluate STAGED PROMPTING with GPT-4 vs GPT-3.5 to compare model capability differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on geometric knowledge reasoning when the knowledge graph contains more complex structural patterns like cycles or longer chains (A-B-C-D)?
- Basis in paper: [inferred] The paper mentions that certain structural patterns like A-B (two blanks connected by an edge) and A-B-C (three blanks linked together) affect performance, with cycles showing performance gains despite being limited in the dataset
- Why unresolved: The current dataset only includes relatively simple structural patterns
- What evidence would resolve it: A systematic study testing LLM performance on knowledge crosswords with varying structural complexities, particularly longer chains and more complex cycles

### Open Question 2
- Question: Can instruction tuning with knowledge crosswords improve LLM performance more effectively than in-context learning, and what is the optimal training strategy?
- Basis in paper: [explicit] The paper shows that instruction-tuning Llama 2-7B with 1,471 knowledge crosswords improves performance by 17.7% (PC) and 12.0% (FC) compared to zero-shot prompting
- Why unresolved: The paper only tests instruction tuning on a relatively small open-source model (Llama 2-7B)
- What evidence would resolve it: Experiments testing instruction tuning across different model sizes, with various training set sizes, and exploring different training strategies

### Open Question 3
- Question: How do LLMs handle geometric knowledge reasoning when distractors satisfy multiple but not all constraints, and what reasoning mechanisms are involved?
- Basis in paper: [explicit] The paper states that "LLMs face difficulties in excluding options that meet a subset of all the constraints presented"
- Why unresolved: While the paper identifies that partial constraint satisfaction by distractors is problematic, it doesn't investigate the specific reasoning mechanisms LLMs use when faced with such distractors
- What evidence would resolve it: Detailed analysis of LLM reasoning traces when selecting between distractors that satisfy different numbers of constraints

## Limitations
- Performance improvements may be partially attributable to prompt engineering rather than fundamental reasoning improvements
- Evaluation relies on a single knowledge graph (YAGO), limiting generalizability to other domains
- Reproducibility challenges due to incomplete specification of question generation process

## Confidence
- High confidence: The benchmark design and metrics (PC/FC) are clearly specified and the overall performance trends across difficulty levels are reproducible
- Medium confidence: The mechanism explanations for why baseline models fail on harder problems, as the specific reasoning patterns are inferred rather than directly measured
- Medium confidence: The comparative effectiveness of Staged Prompting vs Verify-All, as the differences are significant but the underlying reasons for relative performance are not fully explored

## Next Checks
1. Cross-domain validation: Replicate the benchmark using a different knowledge graph (e.g., Wikidata or ConceptNet) to test whether the performance patterns hold across knowledge sources
2. Ablation studies on prompt components: Systematically remove or modify components of the Staged Prompting and Verify-All prompts to isolate which elements drive performance improvements
3. Human expert comparison: Have human experts solve a subset of the knowledge crosswords to establish human performance baselines and determine whether the difficulty levels appropriately reflect human reasoning complexity