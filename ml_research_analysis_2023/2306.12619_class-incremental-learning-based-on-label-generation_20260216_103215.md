---
ver: rpa2
title: Class-Incremental Learning based on Label Generation
arxiv_id: '2306.12619'
source_url: https://arxiv.org/abs/2306.12619
tags:
- learning
- generation
- label
- replay
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to class-incremental learning
  (CIL) by formulating the problem as continual label generation. The proposed Vocabulary-Aware
  Label Generation (VAG) system leverages the generation framework of pre-trained
  encoder-decoder models to mitigate catastrophic forgetting and retain generalizable
  representations.
---

# Class-Incremental Learning based on Label Generation

## Quick Facts
- arXiv ID: 2306.12619
- Source URL: https://arxiv.org/abs/2306.12619
- Authors: Multiple authors from University of Science and Technology of China
- Reference count: 36
- Key outcome: Vocabulary-Aware Label Generation (VAG) system significantly outperforms traditional classifier-based methods and other generation-based baselines in class-incremental learning by leveraging label generation framework with vocabulary-aware loss and pseudo replay.

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning (CIL) by reformulating the problem as continual label generation rather than classification. The proposed Vocabulary-Aware Label Generation (VAG) system leverages pre-trained encoder-decoder models to generate label sequences directly, preserving generalizable representations while mitigating forgetting. VAG employs vocabulary-aware generation loss to focus updates on task-specific vocabulary subsets and uses label-based pseudo replay to create diverse training samples. Experimental results on five datasets demonstrate significant improvements over traditional classifier-based methods and other generation-based baselines in both non-exemplar and exemplar-based CIL settings.

## Method Summary
The method fine-tunes pre-trained encoder-decoder models (e.g., BART-base) to generate label sequences directly rather than mapping features to class probabilities. For each incremental task, the model is trained using a vocabulary-aware generation loss that masks out tokens not used in the current task's labels, creating sparse updates that reduce interference with previously learned knowledge. Pseudo replay data is generated by augmenting label sequences with related tokens based on contextual embeddings, and these samples are combined with real data for training. At inference, the model generates label sequences and retrieves the most relevant label from a stored label pool using similarity search.

## Key Results
- VAG significantly outperforms traditional classifier-based methods and other generation-based baselines across five datasets in both non-exemplar and exemplar-based CIL settings
- The generation framework maintains more stable representation ability during incremental learning, as evidenced by lower neural collapse metrics
- Vocabulary-aware generation loss effectively reduces catastrophic forgetting by focusing updates on task-specific vocabulary subsets
- Label-based pseudo replay with a small ratio (λ) already yields substantial improvements in accuracy and forgetting mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formulation of CIL as continual label generation drastically reduces catastrophic forgetting and preserves generalizable representations of pre-trained models.
- Mechanism: Instead of using a classification head to map features to class probabilities, the model is fine-tuned to directly generate label sequences. This approach leverages the natural sparsity of vocabulary and focuses model updates on task-specific subsets, reducing interference between tasks.
- Core assumption: The generation framework inherently prevents representation collapse better than the classifier framework during fine-tuning.
- Evidence anchors:
  - [abstract]: "This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained."
  - [section]: "Figure 2 (b) compares the change of the PLM’s representation ability in the two frameworks by using the neural collapse metric (N C) proposed in Zhu et al. (2021c)... When learning more and more tasks, both frameworks witness a drop of the PLM’s representation ability. However, the PLM in the generation framework keeps a relatively steady representation ability in the CIL process."
  - [corpus]: Weak or missing. No direct evidence found in related papers.

### Mechanism 2
- Claim: The vocabulary-aware generation loss (VAG loss) eases catastrophic forgetting by focusing updates on task-specific vocabulary subsets.
- Mechanism: By masking the probability of tokens not used in the current task before calculating the generation loss, the model update becomes sparse and reduces interference with previously learned knowledge.
- Core assumption: The vocabulary used for label generation in each task is a small subset of the overall vocabulary.
- Evidence anchors:
  - [abstract]: "V AG modifies the generation loss by focusing on different vocabulary subsets when learning different tasks."
  - [section]: "Since |Vt| ≪ |V| , maximizing the modified probability leads to a sparse update of E and effectively eases the forgetting of previous classes."
  - [corpus]: Weak or missing. No direct evidence found in related papers.

### Mechanism 3
- Claim: Label-based pseudo replay creates diverse training samples that help construct decision boundaries across tasks and mitigate forgetting.
- Mechanism: Pseudo replay data is created by augmenting label sequences with related tokens based on contextual word embeddings, preserving label semantics while increasing sample diversity.
- Core assumption: Augmenting label sequences with related tokens maintains the semantic integrity of the labels.
- Evidence anchors:
  - [abstract]: "V AG... creates pseudo-replay samples by using label semantics."
  - [section]: "To preserve the label semantics as well as to create diverse samples, we implement aug(·) by randomly adding related tokens to the original label sequence based on contextual word embeddings."
  - [corpus]: Weak or missing. No direct evidence found in related papers.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why neural networks forget previously learned knowledge is crucial for addressing the core challenge in CIL.
  - Quick check question: What is catastrophic forgetting, and why does it occur during sequential task learning?

- Concept: Neural collapse
  - Why needed here: Neural collapse is a metric used to assess the representation ability of the model during CIL, indicating how well the model maintains its learning capacity.
  - Quick check question: How does neural collapse relate to the model's ability to learn new tasks without forgetting old ones?

- Concept: Sparse model updates
  - Why needed here: Sparse updates are leveraged in VAG to minimize interference between tasks by focusing on task-specific vocabulary subsets.
  - Quick check question: Why are sparse model updates beneficial in the context of CIL?

## Architecture Onboarding

- Component map:
  Pre-trained encoder-decoder model (e.g., BART) -> Vocabulary-aware generation loss module -> Label-based pseudo replay data generator -> Label pool for storing label sequences -> Sentence-BERT model for label retrieval

- Critical path:
  1. Initialize the pre-trained model and label pool.
  2. For each task, fine-tune the model using the vocabulary-aware generation loss on the task's data.
  3. Generate pseudo replay data by augmenting label sequences.
  4. Combine real and pseudo replay data with the current task data for training.
  5. At inference, generate label sequences and retrieve the most relevant label from the label pool.

- Design tradeoffs:
  - Using generation instead of classification increases model complexity but improves CIL performance.
  - Label-based pseudo replay adds diversity but may introduce noise if not implemented carefully.
  - Vocabulary-aware loss reduces forgetting but relies on the assumption of sparse vocabulary usage per task.

- Failure signatures:
  - Increased neural collapse metric indicates representation degradation.
  - Model shows strong bias towards predicting the latest learned classes.
  - Vocabulary subsets are not sufficiently small, reducing the effectiveness of sparse updates.

- First 3 experiments:
  1. Implement the generation framework and compare performance with the classifier framework on a small dataset.
  2. Add the vocabulary-aware generation loss and measure its impact on catastrophic forgetting.
  3. Introduce label-based pseudo replay and evaluate its effect on model accuracy and forgetting.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but potential areas for future work include exploring automatically generated label sequences, investigating the impact of varying label-based replay ratios beyond the tested range, and comparing performance with larger buffer sizes against state-of-the-art methods.

## Limitations
- The vocabulary-aware loss mechanism's effectiveness may diminish when tasks have overlapping vocabularies, as the sparse update assumption becomes less valid
- The label augmentation approach for pseudo replay lacks empirical validation regarding semantic preservation, with no direct evidence supporting this technique
- The neural collapse metric comparison, while showing promising results, needs further investigation regarding sensitivity to task ordering and dataset characteristics

## Confidence
- High confidence in the generation framework's superior performance over classification-based CIL methods
- Medium confidence in the vocabulary-aware loss mechanism's effectiveness, pending validation on vocabulary-overlap scenarios
- Medium confidence in label-based pseudo replay benefits, requiring empirical verification of semantic preservation

## Next Checks
1. Test VAG on datasets with high vocabulary overlap between tasks to assess robustness of the sparse update mechanism
2. Conduct ablation studies isolating the effects of label augmentation from other components to verify semantic preservation
3. Evaluate the approach with different pre-trained model architectures (beyond BART) to test generalizability of the generation framework