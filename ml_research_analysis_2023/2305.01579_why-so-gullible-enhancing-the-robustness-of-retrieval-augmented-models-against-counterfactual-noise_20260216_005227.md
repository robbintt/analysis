---
ver: rpa2
title: Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against
  Counterfactual Noise
arxiv_id: '2305.01579'
source_url: https://arxiv.org/abs/2305.01579
tags:
- documents
- gpt-3
- context
- retrieved
- perturbed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines the brittleness of retrieval-augmented QA models
  to misinformation in retrieved documents. It introduces a discriminator-based approach
  to identify and ignore counterfactual documents during answer generation.
---

# Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise

## Quick Facts
- **arXiv ID**: 2305.01579
- **Source URL**: https://arxiv.org/abs/2305.01579
- **Reference count**: 13
- **Primary result**: Discriminator-based approaches significantly improve retrieval-augmented QA robustness against misinformation, with DiscFiD achieving up to 42.84 EM score at 50% perturbation versus 34.26 without discriminator

## Executive Summary
This paper addresses the vulnerability of retrieval-augmented QA models to misinformation in retrieved documents. The authors propose discriminator-based approaches that identify and ignore counterfactual documents during answer generation. Experiments show that models like FiD and GPT-3 suffer significant performance drops when retrieved documents contain misinformation, but incorporating a fine-tuned discriminator improves robustness substantially. The work demonstrates that lightweight discriminators can effectively filter misinformation and that their output can stabilize GPT-3's in-context learning process.

## Method Summary
The paper introduces discriminator-based approaches to handle knowledge conflicts among retrieved documents. For FiD, a discriminator is jointly trained with the QA task using a combined loss of answer generation and perturbation classification. For GPT-3, the fine-tuned discriminator's output is incorporated into the prompt to improve stability across different in-context samples. The approach uses entity replacement to create counterfactual documents for training and evaluation, with perturbation rates of 0%, 25%, 50%, and 75%.

## Key Results
- FiD with DiscFiD achieves 42.84 EM score at 50% perturbation versus 34.26 without discriminator
- GPT-3 with DiscFiD shows better stability across different in-context samples compared to baseline
- The fine-tuned LM demonstrates high precision in discerning authentic from counterfactual documents
- GPT-3's in-context learning instability is reduced when discriminator output is injected into the prompt

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discriminator fine-tuning injects perturbation-awareness into encoder embeddings
- Mechanism: The discriminator is jointly trained with the QA task, optimizing both the decoder answer generation loss and a binary cross-entropy loss on perturbation labels. This causes the encoder to produce embeddings that encode whether each document is authentic or counterfactual, allowing the decoder to focus on trustworthy information.
- Core assumption: Perturbation labels are reliable and the discriminator can learn to distinguish authentic from counterfactual documents during joint training
- Evidence anchors:
  - [abstract] "We propose approaches for handling knowledge conflicts among retrieved documents by explicitly fine-tuning a discriminator"
  - [section 2] "We add a discriminator on top of the encoder to be jointly learned with the decoder's answer generation by optimizing the following objective"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: GPT-3's in-context learning can be stabilized by providing discriminator output as part of the prompt
- Mechanism: The fine-tuned discriminator's output (which documents are perturbed) is injected into the GPT-3 prompt, reducing the variance in GPT-3's in-context sample sensitivity and improving stability across different sampled contexts
- Core assumption: GPT-3 can effectively use the discriminator output to guide its answer generation when provided in the prompt
- Evidence anchors:
  - [abstract] "We also provide our findings on incorporating the fine-tuned discriminator's decision into the in-context learning process"
  - [section 4.4] "injecting the fine-tuned discriminator into the in-context learning process (GPT-3 (Semi-parametric w/ DiscFiD)) greatly improves the stability"
  - [corpus] Weak - limited corpus evidence for this specific stabilization mechanism

### Mechanism 3
- Claim: Fine-tuning on perturbation classification provides high-precision discrimination capability
- Mechanism: By training a discriminator specifically to identify perturbed documents, the model learns high-precision features that can distinguish authentic from counterfactual information, even though recall may be lower
- Core assumption: The perturbation classification task is learnable and provides useful features for the downstream QA task
- Evidence anchors:
  - [abstract] "Our empirical results on open-domain QA show that these approaches significantly improve LMs' robustness to knowledge conflicts"
  - [section 4.3] "the fine-tuned LM demonstrates high precision in discerning authentic from counterfactual documents"
  - [corpus] Weak - no direct corpus evidence for precision claims

## Foundational Learning

- Concept: Binary cross-entropy loss for binary classification
  - Why needed here: The discriminator needs to classify documents as either perturbed (1) or authentic (0), requiring a binary classification objective
  - Quick check question: What loss function would you use to train a model that outputs a probability of a document being perturbed?

- Concept: In-context learning and few-shot prompting
  - Why needed here: GPT-3 relies on few-shot examples in the prompt to perform tasks, and the paper explores how to improve this process by adding discriminator output to the prompt
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- Concept: Entity replacement for data augmentation
  - Why needed here: The paper uses entity replacement (substituting gold answers with same-type named entities) to create counterfactual documents for training and evaluation
  - Quick check question: What are the advantages and potential pitfalls of using entity replacement for creating counterfactual training data?

## Architecture Onboarding

- Component map: Retriever (DPR) → Encoder (BERT/T5) → Discriminator (binary classifier) → Decoder (T5) for FiD; Retriever → Prompt engineering + GPT-3 for LLM approach
- Critical path: Document retrieval → Discriminator classification → Answer generation (using only authentic documents)
- Design tradeoffs: Joint training of discriminator with QA task vs. separate training; using fine-tuned discriminator output vs. relying solely on GPT-3's own capabilities
- Failure signatures: Significant performance drop when perturbation percentage increases; GPT-3 performance worse than parametric-only baseline in high-perturbation settings; low discriminator precision/recall
- First 3 experiments:
  1. Evaluate FiD performance on NQ-Open with 0%, 25%, 50%, 75% perturbation rates
  2. Train FiD with joint discriminator and evaluate robustness improvements
  3. Test GPT-3 with and without discriminator output in the prompt across different perturbation rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed discriminator-based approach perform on Web-scale document collections with real-world misinformation?
- Basis in paper: [inferred] The paper acknowledges using Wikipedia documents (NQ-Open) instead of real-world Web-scale documents and states that "the problems that occur due to misinformation in such a realistic setting require further investigation."
- Why unresolved: The current evaluation is limited to Wikipedia data, which is generally more reliable than the broader Web. Real-world Web-scale data contains more diverse and potentially more sophisticated misinformation.
- What evidence would resolve it: Testing the discriminator approach on a Web-scale document collection with ground-truth misinformation labels would provide evidence of its effectiveness in realistic settings.

### Open Question 2
- Question: What is the optimal balance between fine-tuned discriminator precision and GPT-3's few-shot learning capability in different resource settings?
- Basis in paper: [explicit] The paper discusses combining fine-tuned discriminator output with GPT-3's in-context learning, noting that "lightweight fine-tuned LMs trained on an easily accessible subtask can, therefore, maximize GPT-3's capability" and observing different performance patterns across perturbation levels.
- Why unresolved: The paper provides initial evidence but doesn't systematically explore the tradeoff space or identify optimal configurations for different scenarios (high vs low resource settings, different perturbation levels).
- What evidence would resolve it: A systematic study varying discriminator model size, training data amount, and integration methods with GPT-3 across multiple resource scenarios would clarify the optimal balance.

### Open Question 3
- Question: How does the entity-perturbation framework compare to other misinformation generation methods in terms of realism and model robustness evaluation?
- Basis in paper: [explicit] The paper acknowledges that "the adoption of the entity-perturbation framework in this paper may appear artificial" and suggests that "it would be beneficial for future works to explore the application of our approach on a larger scale, Web environment."
- Why unresolved: The entity-perturbation framework used is a simplification that may not capture the complexity and subtlety of real-world misinformation, limiting the generalizability of the robustness findings.
- What evidence would resolve it: Comparing model robustness across different misinformation generation methods (e.g., sentence-level perturbations, contextual manipulations, adversarial examples) would reveal whether current findings generalize to more realistic misinformation.

## Limitations
- GPT-3 approach shows instability across different in-context samples, with parametric-only baseline sometimes outperforming semi-parametric approach at high perturbation rates
- Discriminator precision is high but recall is notably lower, meaning some counterfactual documents may still be used in answer generation
- Perturbation method (entity replacement) may not fully capture the complexity of real-world misinformation

## Confidence
**High Confidence**: The core finding that retrieval-augmented models are vulnerable to retrieved misinformation is well-established through systematic evaluation across perturbation levels. The effectiveness of discriminator-based approaches in improving FiD robustness is supported by clear quantitative improvements.

**Medium Confidence**: The mechanism by which joint discriminator training improves encoder embeddings is theoretically sound but lacks direct empirical validation. The claim about GPT-3 stability improvements through discriminator injection is supported but could benefit from more extensive sampling.

**Low Confidence**: The generalizability of these approaches to real-world misinformation scenarios remains uncertain given the synthetic nature of the perturbations. The optimal perturbation rate for training discriminators is empirically determined but not theoretically justified.

## Next Checks
1. **Statistical significance testing**: Perform t-tests or bootstrap confidence intervals on GPT-3 in-context learning results across the 5 samples to determine if stability improvements are statistically significant rather than anecdotal.

2. **Real misinformation evaluation**: Test the trained discriminators on a small set of manually curated misinformation examples from real-world sources to assess whether the synthetic perturbation training transfers to authentic misinformation patterns.

3. **Recall-precision tradeoff analysis**: Systematically vary the discriminator threshold to plot precision-recall curves, identifying the optimal operating point that balances robustness with the risk of discarding authentic information.