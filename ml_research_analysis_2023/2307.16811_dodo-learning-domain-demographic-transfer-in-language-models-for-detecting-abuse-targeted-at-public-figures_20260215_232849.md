---
ver: rpa2
title: 'DoDo Learning: DOmain-DemOgraphic Transfer in Language Models for Detecting
  Abuse Targeted at Public Figures'
arxiv_id: '2307.16811'
source_url: https://arxiv.org/abs/2307.16811
tags:
- abuse
- data
- fb-m
- fb-w
- mp-m
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies domain-demographic transfer in abuse classification,
  fine-tuning language models on a novel dataset of 28K tweets targeting public figures
  across domains (sport, politics) and demographics (women, men). Experiments show
  that small amounts of diverse data are more effective than larger domain-specific
  datasets for generalisable performance.
---

# DoDo Learning: DOmain-DemOgraphic Transfer in Language Models for Detecting Abuse Targeted at Public Figures

## Quick Facts
- **arXiv ID**: 2307.16811
- **Source URL**: https://arxiv.org/abs/2307.16811
- **Reference count**: 30
- **Key outcome**: Cross-demographic transfer is more effective than cross-domain transfer; diverse training data improves generalisation more than larger domain-specific datasets.

## Executive Summary
This paper studies domain-demographic transfer in abuse classification by fine-tuning language models on a novel dataset of 28K tweets targeting public figures across domains (sport, politics) and demographics (women, men). Experiments demonstrate that small amounts of diverse data are more effective than larger domain-specific datasets for generalisable performance. The study reveals that cross-demographic transfer (e.g., from women to men) is more effective than cross-domain transfer (e.g., from football to politics), and models trained on cross-domain data are more generalisable than those trained within a single domain. Dataset similarity correlates with transferability, and models can be efficiently adapted to new domains/demographics with only small amounts of additional data.

## Method Summary
The study fine-tunes deBERTa-v3 on combinations of four domain-demographic pairs (footballers-men, footballers-women, MPs-men, MPs-women) with two budget scenarios: full budget (3,000×4=12,000 samples) and fixed budget (3,000 total samples). Data collection used Twitter's streaming API with boosted sampling to ensure balanced training sets of 3K entries per pair. Annotations employed a 4-class schema (Positive, Neutral, Critical, Abusive) using crowdworkers and expert annotators with majority agreement requirements. Model training ran for 5 epochs with early stopping, and evaluation used Macro-F1 scores averaged across 3 random seeds, tested on both seen and unseen domain-demographic pairs. Transferability was analyzed using dataset similarity metrics including Jaccard similarity, Sørensen-Dice, and Kullback-Leibler divergence.

## Key Results
- Small amounts of diverse training data are more effective than larger domain-specific datasets for generalisable performance
- Cross-demographic transfer (e.g., women to men) is more effective than cross-domain transfer (e.g., football to politics)
- Dataset similarity correlates with transferability (correlation coefficient 0.7)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse training data improves generalisation more than larger domain-specific datasets
- Mechanism: Models trained on mixed domain-demographic pairs learn shared linguistic patterns and contextual cues, reducing overfitting to a single domain's style
- Core assumption: Domain-specific features are not universally transferable, but shared abusive language patterns exist across domains
- Evidence anchors:
  - [abstract] "small amounts of diverse data are hugely beneficial to generalisation and model adaptation"
  - [section] "the increase in performance from adding data from new domains or demographics is not linear: the full budget dodo2 models only attain a one percentage point (pp) average increase in Macro-F1 Score for an additional 3,000 training entries"

### Mechanism 2
- Claim: Cross-demographic transfer (e.g., women to men) is more effective than cross-domain transfer (e.g., football to politics)
- Mechanism: Demographic groups share more similar abusive language patterns than different domains, making it easier for models to transfer learned abusive language detection between genders
- Core assumption: The abusive language patterns are more similar within a demographic group across domains than within a domain across demographic groups
- Evidence anchors:
  - [abstract] "Cross-demographic transfer (e.g., from women to men) is more effective than cross-domain transfer (e.g., from football to politics)"
  - [section] "For demographic transfer, training on the male pairs and testing on female pairs faces no drop in performance. In contrast, training on women and testing on men leads to a small reduction in performance on the male data"

### Mechanism 3
- Claim: Dataset similarity is a signal of transferability
- Mechanism: Models perform better on unseen data when the training data has similar linguistic and distributional characteristics
- Core assumption: The more similar two datasets are in terms of language, topics, and context, the more effectively a model trained on one will transfer to the other
- Evidence anchors:
  - [abstract] "Dataset similarity correlates with transferability"
  - [section] "We plot Macro-F1 scores (of unseen single dodos) against Jaccard similarity for each pair of dodos. The correlation coefficient is 0.7, demonstrating a positive relationship between dataset similarity and unseen dodo performance"

## Foundational Learning

- Concept: Domain adaptation in machine learning
  - Why needed here: Understanding how models can be adapted to new domains with limited data is crucial for building generalisable abuse classifiers
  - Quick check question: What is the difference between domain adaptation and transfer learning in the context of abuse detection?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: When adapting models to new domains or demographics, it's important to ensure that performance on previously learned tasks doesn't degrade significantly
  - Quick check question: How can we measure and mitigate catastrophic forgetting when adapting abuse detection models to new domains?

- Concept: Dataset similarity metrics (e.g., Jaccard similarity, Kullback-Leibler divergence)
  - Why needed here: These metrics are used to quantify the similarity between datasets, which is shown to correlate with transferability
  - Quick check question: What are the advantages and disadvantages of using Jaccard similarity vs. Kullback-Leibler divergence for measuring dataset similarity in abuse detection?

## Architecture Onboarding

- Component map: Data collection pipeline -> Annotation pipeline -> Model training pipeline -> Evaluation pipeline -> Analysis pipeline
- Critical path: Data collection → Annotation → Model training → Evaluation → Analysis
- Design tradeoffs: More diverse training data vs. larger domain-specific datasets; Cross-demographic transfer vs. cross-domain transfer; Fixed budget vs. full budget for training data
- Failure signatures: Poor performance on unseen domains/demographics; Significant catastrophic forgetting when adapting models; Low correlation between dataset similarity and transferability
- First 3 experiments:
  1. Fine-tune deBERTa-v3 on a single domain-demographic pair (dodo1) and evaluate on seen and unseen dodos
  2. Fine-tune deBERTa-v3 on two domain-demographic pairs (dodo2) with fixed and full budget and compare performance
  3. Measure dataset similarity (Jaccard, Dice, KL divergence) between all pairs of dodos and correlate with transferability scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does cross-domain transfer from footballers to MPs show better generalisability than the reverse (MPs to footballers), and why?
- Basis in paper: [explicit] The paper notes that models trained on footballers perform worse on MPs than vice versa, suggesting an asymmetry in cross-domain transfer.
- Why unresolved: The paper only presents empirical results without a detailed analysis of the underlying reasons for this asymmetry.
- What evidence would resolve it: A comparative analysis of linguistic features, tweet length, and sentiment distributions between the two domains could clarify why one direction of transfer is more effective.

### Open Question 2
- Question: How do specific subgroups within the dataset contribute differently to model generalisability, and what factors determine their importance?
- Basis in paper: [explicit] The paper finds that some domain-demographic pairs (e.g., footballers-men) contribute more to generalisable performance than others.
- Why unresolved: The study does not explore the characteristics of these subgroups that make them more impactful, such as data quality, class balance, or linguistic diversity.
- What evidence would resolve it: An in-depth analysis of the linguistic and statistical properties of each subgroup could reveal why certain pairs are more beneficial for generalisability.

### Open Question 3
- Question: Can active learning techniques improve the efficiency of cross-domain and cross-demographic transfer in abuse detection models?
- Basis in paper: [inferred] The paper mentions that active learning is a promising avenue for future work but does not explore it empirically.
- Why unresolved: The study focuses on traditional fine-tuning methods and does not test the potential of active learning to optimise data selection for transfer learning.
- What evidence would resolve it: Experiments comparing traditional fine-tuning with active learning approaches in cross-domain and cross-demographic settings could demonstrate the effectiveness of active learning for this task.

## Limitations
- Reliance on English-language tweets targeting UK-based public figures limits generalizability to other languages and cultural contexts
- Fixed keyword-based data collection methodology may have introduced selection bias
- Study focuses exclusively on fine-tuning pre-trained language models without exploring alternative approaches like prompt engineering

## Confidence

**High Confidence** (mechanisms well-supported by empirical results):
- Diverse training data improves generalisation more than larger domain-specific datasets
- Dataset similarity correlates with transferability
- Cross-domain training produces more generalisable models than single-domain training

**Medium Confidence** (supported by results but with some caveats):
- Cross-demographic transfer is more effective than cross-domain transfer
- The fixed budget approach (3,000 total samples) yields comparable performance to full budget approaches

**Low Confidence** (limited empirical support or complex mechanisms):
- The specific mechanisms by which demographic similarities facilitate transfer
- The stability of findings across different language models and abuse detection tasks

## Next Checks

1. **Domain-Generalization Stress Test**: Evaluate model performance when transferred to completely unseen domains (e.g., entertainment celebrities, business leaders) and demographics (non-binary, transgender individuals) to test the robustness of observed transferability patterns beyond the four domain-demographic pairs studied.

2. **Dataset Similarity Correlation Robustness**: Replicate the dataset similarity analysis using multiple similarity metrics (beyond Jaccard and KL divergence) and statistical approaches (e.g., partial correlation controlling for dataset size) to confirm whether the observed 0.7 correlation coefficient is stable across different measurement approaches.

3. **Temporal and Linguistic Generalization**: Test whether models trained on historical data maintain performance on temporally shifted data (e.g., post-2020 tweets with different linguistic patterns) and across different linguistic registers (formal vs. informal, dialectal variations) to assess real-world deployment readiness.