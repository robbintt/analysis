---
ver: rpa2
title: 'Neural Conversation Models and How to Rein Them in: A Survey of Failures and
  Fixes'
arxiv_id: '2308.06095'
source_url: https://arxiv.org/abs/2308.06095
tags:
- dialogue
- they
- language
- conversation
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic survey of neural conversation
  models and their shortcomings. The authors frame the problem in terms of Grice's
  maxims of cooperative conversation, highlighting the need for models to be fluent,
  informative, consistent, coherent, and follow social norms.
---

# Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes

## Quick Facts
- arXiv ID: 2308.06095
- Source URL: https://arxiv.org/abs/2308.06095
- Reference count: 20
- Key outcome: Systematic survey framing neural conversation model failures through Grice's maxims, identifying intervention points and evaluation challenges

## Executive Summary
This paper provides a systematic survey of neural conversation models and their shortcomings, framing the problem through Grice's maxims of cooperative conversation. The authors identify five key intervention points where improvements can be made: data, training regime, model architecture, decoding strategies, and post-filtering. The survey highlights the lack of standardized evaluation metrics as a major impediment to progress, while suggesting that targeted interventions at specific points can address particular conversational failures like repetition, inconsistency, and lack of informativity.

## Method Summary
The paper conducts a systematic literature review of neural conversation models, organizing approaches according to Gricean principles of cooperative conversation. It categorizes conversational qualities (fluency, informativity, consistency, coherence, social norm adherence) and maps them to intervention points in the neural conversation model pipeline. The survey analyzes various approaches including unlikelihood training, knowledge injection, weighted decoding, and post-filtering methods. The methodology emphasizes the need for better evaluation metrics that correlate with human judgments of conversational quality.

## Key Results
- Grice's maxims provide a systematic framework for categorizing neural conversation model failures
- Five key intervention points identified: data, training regime, model architecture, decoding, and post-filtering
- Current evaluation methods rely heavily on human judgment and lack standardized automated metrics
- Pre-training emerges as crucial for model performance across multiple conversational qualities
- Targeted interventions can address specific failures but often involve tradeoffs between different qualities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper's Gricean framing provides a systematic lens for categorizing conversational failures.
- Mechanism: By mapping Grice's maxims to measurable conversational qualities, the authors create a taxonomy linking linguistic theory to technical intervention points.
- Core assumption: Grice's maxims are sufficiently comprehensive to cover essential qualities of cooperative conversational contributions in neural models.
- Evidence anchors: The operationalization of each Gricean category into measurable features; corpus search limitations suggest novelty in this framing.

### Mechanism 2
- Claim: Identifying specific intervention points allows for targeted improvements in conversational quality.
- Mechanism: Categorizing research efforts into five intervention points enables structured analysis of how different approaches address specific conversational failures.
- Core assumption: The identified intervention points are the most impactful areas for improving neural conversation models.
- Evidence anchors: Schematic organization of neural conversation models; examples of approaches targeting specific failures at intervention points.

### Mechanism 3
- Claim: Lack of standardized evaluation metrics hinders progress in improving neural conversation models.
- Mechanism: Highlighting reliance on human judgment and self-defined metrics makes it difficult to compare approaches and quantify intervention effectiveness.
- Core assumption: Standardized, automated metrics correlating with human judgment are necessary for efficient field progress.
- Evidence anchors: Discussion of metric limitations (BLEU, perplexity); corpus search suggests this is an under-addressed issue.

## Foundational Learning

- Concept: Grice's Cooperative Principle and maxims
  - Why needed here: Understanding theoretical foundation of cooperative conversation is crucial for interpreting the paper's framework and categorization of failures and interventions.
  - Quick check question: Can you explain how the maxim of Quantity (be as informative as required) relates to "informativity" in neural conversation models?

- Concept: Neural conversation model architecture (encoder-decoder, transformers, pre-training)
  - Why needed here: Familiarity with technical aspects is necessary to understand intervention points and specific approaches discussed.
  - Quick check question: How does pre-training on general text data differ from fine-tuning on dialogue-specific data in terms of model capabilities and limitations?

- Concept: Evaluation metrics for natural language generation
  - Why needed here: Understanding strengths and limitations of different metrics is crucial for interpreting results and claims.
  - Quick check question: Why might reference-based metrics like BLEU be insufficient for evaluating dialogue response coherence?

## Architecture Onboarding

- Component map: Pre-training data -> Domain data -> Training regime -> Decoding -> Post-filtering -> Core model (Transformer-based encoder-decoder)
- Critical path: Pre-training → Domain data fine-tuning → Decoding with repetition control → Post-filtering (if needed)
- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models perform better but are more expensive to train and deploy
  - Pre-training data diversity vs. domain relevance: More diverse data improves generalization, but domain-specific data may be necessary for certain tasks
  - Decoding randomness vs. attribute control: More randomness can improve informativity but may reduce coherence or control
- Failure signatures:
  - Repetitive or generic responses: Likely due to language modeling loss and decoding strategy
  - Inconsistent personality or facts: Likely due to lack of explicit knowledge injection or consistency training
  - Off-topic or incoherent responses: Likely due to insufficient context encoding or coherence modeling
- First 3 experiments:
  1. Evaluate base model's performance on held-out test set using perplexity and human evaluation of five conversational qualities
  2. Implement and evaluate decoding strategy with repetition control (e.g., n-gram blocking) to improve informativity
  3. Implement and evaluate knowledge injection (e.g., concatenation of relevant facts to input) to improve consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop automated metrics that accurately capture dialogue coherence, beyond current word-overlap based approaches like BLEU?
- Basis in paper: The paper states that reference-based word overlap metrics such as BLEU do not correlate well with coherence and that no automated metrics have yet been proposed that cover these complex notions of dialogue coherence.
- Why unresolved: Existing metrics focus on lexical similarity rather than pragmatic coherence, and the paper notes the difficulty in capturing complex notions of dialogue coherence with automated metrics.
- What evidence would resolve it: Development and validation of new automated metrics that better capture dialogue coherence, with strong correlation to human judgments across diverse conversational contexts.

### Open Question 2
- Question: How can we improve the consistency of neural conversation models with respect to factual knowledge and personality traits without requiring large amounts of labeled training data?
- Basis in paper: The paper discusses the challenge of ensuring factual and personality consistency, noting the lack of precise metrics and the resource-intensive nature of collecting labeled data.
- Why unresolved: Various approaches have been proposed, but they often require additional labeled data or complex model architectures, and there's a need for more efficient methods without extensive data collection.
- What evidence would resolve it: Development of novel techniques that improve consistency using limited labeled data, or methods that effectively leverage unsupervised or semi-supervised approaches for consistency training.

### Open Question 3
- Question: How can we effectively balance informativeness and coherence in neural conversation models, as increasing one often comes at the expense of the other?
- Basis in paper: The paper notes that increasing informativeness can sacrifice dialogue coherence, and that current approaches often face a trade-off between these two aspects of conversation quality.
- Why unresolved: Current methods for improving informativeness often lead to less coherent responses, while methods for improving coherence may limit diversity and informativeness.
- What evidence would resolve it: Development of novel approaches or training strategies that can jointly optimize for both informativeness and coherence, potentially through multi-task learning or more sophisticated decoding algorithms.

## Limitations

- The Gricean theoretical foundation, while compelling, relies on assumptions about universality of cooperative principles that may not fully capture all aspects of conversational appropriateness in neural systems
- Operationalization of conversational qualities into measurable features shows promise but lacks extensive empirical validation across diverse model architectures and datasets
- Framework completeness is uncertain as the relative impact of each intervention point on different conversational failures is not rigorously quantified

## Confidence

- High: The identification of key conversational failures (repetition, generic responses, inconsistency) is well-supported by literature
- Medium: The Gricean framework's applicability to neural conversation models requires more empirical validation
- Medium: The intervention point categorization is logically structured but lacks quantitative comparison of relative effectiveness

## Next Checks

1. Conduct systematic evaluation of proposed metrics (perplexity, token distributions, rare word counts) across multiple model architectures to validate correlation with human judgments of conversational quality
2. Implement controlled experiments varying one intervention point at a time to quantify relative impact on each conversational quality
3. Test framework's completeness by identifying and analyzing conversation model failures that cannot be explained by Gricean maxims to identify potential gaps in theoretical foundation