---
ver: rpa2
title: Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation
arxiv_id: '2309.07998'
source_url: https://arxiv.org/abs/2309.07998
tags:
- dialogue
- groups
- evaluator
- evaluation
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how different human evaluator groups affect
  chat-oriented dialogue system evaluation. It compares 4 state-of-the-art dialogue
  systems using 4 distinct evaluator groups: chatbot developers, professional annotators,
  interactive university students, and external university students.'
---

# Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation

## Quick Facts
- **arXiv ID**: 2309.07998
- **Source URL**: https://arxiv.org/abs/2309.07998
- **Reference count**: 11
- **Primary result**: Chatbot developers produce dissimilar evaluations due to more extreme ratings; Likert ratings are more robust across evaluator groups than pairwise selections

## Executive Summary
This paper investigates how different human evaluator groups affect the evaluation of chat-oriented dialogue systems. The study compares 4 state-of-the-art dialogue systems using 4 distinct evaluator groups: chatbot developers, professional annotators, interactive university students, and external university students. Evaluations were conducted using both Likert ratings and pairwise selections across 8 dialogue metrics. Results show that Likert evaluations demonstrate greater robustness across evaluator groups compared to pairwise selections. A key finding is that chatbot developers produce evaluations that differ significantly from less-experienced groups, likely due to their tendency to give more extreme ratings.

## Method Summary
The study collected human evaluations from 400 dialogues generated by 4 chatbots (Blender2, Emora, Blender-Decode, BART-FiD-RAG) using 4 evaluator groups. Each group evaluated dialogues using both Likert ratings and pairwise selections across 8 dialogue metrics. The analysis compared evaluator groups using dialogue-level and bot-level measures, including Krippendorff's alpha for interannotator agreement and Cohen's d/h for effect size differences. The evaluation interface and instructions were identical across all groups to ensure consistency in the evaluation process.

## Key Results
- Likert evaluations demonstrate greater robustness across evaluator groups than pairwise selections
- Chatbot developers produce significantly different evaluations compared to less-experienced groups due to more extreme ratings
- External evaluators show better consistency for objective dialogue metrics, suggesting their suitability for evaluating factual aspects of dialogue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Likert evaluations are more stable than Pairwise selections across evaluator groups.
- Mechanism: Likert ratings provide a fixed numerical scale, reducing variability in judgment criteria, whereas Pairwise selections depend on comparative context and relative differences that can shift across groups.
- Core assumption: Numerical scales constrain evaluator responses more than relative comparisons.
- Evidence anchors:
  - [abstract] "Results show that Likert evaluations are more robust across evaluator groups than pairwise selections, with only minor differences observed."
  - [section] "Overall, there is a lower difference in effect sizes between groups for the Likert evaluations compared to Pairwise evaluations."
  - [corpus] "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation" discusses multi-dimensional evaluation alignment, supporting the notion that structured scales aid consistency.
- Break condition: If evaluator groups interpret numerical scales differently or anchor points shift, stability could degrade.

### Mechanism 2
- Claim: Chatbot developers produce dissimilar evaluations to less-experienced groups due to more extreme ratings.
- Mechanism: Developers have deeper technical knowledge, leading them to rate responses more stringently and notice subtle flaws that others overlook, causing divergence in evaluation patterns.
- Core assumption: Expertise changes sensitivity to model flaws.
- Evidence anchors:
  - [abstract] "A key finding is that chatbot developers produce dissimilar evaluations compared to less-experienced groups, likely due to more extreme ratings."
  - [section] "Devx consistently identifies higher bot-pair effect sizes than any other group," indicating more extreme evaluations.
  - [corpus] "An Empirical Analysis of Uncertainty in Large Language Model Evaluations" examines evaluator bias, supporting the role of expertise in evaluation consistency.
- Break condition: If developers adjust their evaluation standards or focus on different criteria, dissimilarity may reduce.

### Mechanism 3
- Claim: Objective dialogue metrics achieve better consistency among external evaluators.
- Mechanism: External evaluators, not emotionally invested in conversations, focus more on factual correctness, leading to higher agreement on objective metrics like coherence and relevance.
- Core assumption: Emotional detachment increases objectivity in evaluation.
- Evidence anchors:
  - [abstract] "external evaluators show better consistency for objective dialogue metrics, suggesting their suitability for evaluating factual aspects of dialogue."
  - [section] "Objectivity Favors External Point-of-View... Con is seen between Stux/Surx... for both Likert and Pairwise evaluations."
  - [corpus] "Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems" suggests perspective affects evaluation, supporting the role of external viewpoints.
- Break condition: If external evaluators misinterpret objective criteria or context is lost, consistency may drop.

## Foundational Learning

- Concept: Krippendorff's alpha for interannotator agreement.
  - Why needed here: To measure consistency of evaluations across different evaluator groups, essential for understanding robustness.
  - Quick check question: What does an alpha value of 0.2 indicate about agreement between evaluator groups?

- Concept: Effect size (Cohen's d, Cohen's h) for comparing bot performance.
  - Why needed here: To quantify differences in bot rankings across evaluator groups beyond simple score differences.
  - Quick check question: Why is effect size preferred over raw score differences when comparing bot performances across groups?

- Concept: Monte Carlo case resampling for confidence intervals.
  - Why needed here: To estimate uncertainty in effect size differences, ensuring statistical robustness of conclusions.
  - Quick check question: How does resampling improve the reliability of confidence intervals compared to parametric methods?

## Architecture Onboarding

- Component map: Evaluator group → Dialogue dataset → Evaluation method (Likert/Pairwise) → Metrics (8 types) → Agreement analysis → Bot performance analysis
- Critical path: Data collection → Interannotator agreement calculation → Effect size comparison → Interpretation of robustness
- Design tradeoffs: Likert ratings offer stability but may lack nuance; Pairwise selections capture relative performance but are more sensitive to evaluator bias
- Failure signatures: Low interannotator agreement across all groups suggests issues with metric definitions or instructions; high developer dissimilarity indicates need for calibration
- First 3 experiments:
  1. Replicate interannotator agreement analysis with a subset of dialogues to validate alpha calculations
  2. Compare effect sizes between groups using simulated data to confirm methodology
  3. Conduct a small pilot with mixed evaluator types to test interaction effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do evaluation results vary when using additional evaluator groups beyond the four tested (e.g., domain experts, cross-cultural evaluators)?
- Basis in paper: [explicit] The paper states "This study was limited to 4 chatbots and 4 evaluator groups" and calls for "further work on additional group characteristics and dialogue models"
- Why unresolved: The study only examined four specific evaluator groups and did not test other potentially relevant groups like domain experts or cross-cultural evaluators
- What evidence would resolve it: Comparative evaluation results using additional evaluator groups on the same dialogue datasets, measuring agreement levels and bot performance rankings

### Open Question 2
- Question: What is the optimal balance between evaluator expertise and objectivity for different dialogue evaluation metrics?
- Basis in paper: [explicit] The paper found that "Objective dialogue metrics achieve better consistency among external evaluators" and noted "discrepancies between evaluators with different levels of chatbot expertise"
- Why unresolved: The study identified correlations between expertise levels and evaluation consistency but did not determine optimal expertise-objective balances for specific metrics
- What evidence would resolve it: Controlled experiments varying evaluator expertise levels while measuring consistency across different metric types, identifying optimal expertise-objective combinations

### Open Question 3
- Question: How do extreme rating tendencies of chatbot developers impact the reliability of effect size comparisons between bots?
- Basis in paper: [explicit] The paper observed that "Devx consistently identifies higher bot-pair effect sizes than any other group" and suggested developers "produced more extreme ratings on the disagreed upon instances"
- Why unresolved: While the paper noted the pattern of extreme ratings from developers, it did not quantify how these extremes specifically affect effect size reliability or determine if normalization could help
- What evidence would resolve it: Analysis of rating distributions and effect size calculations with and without extreme rating normalization, comparing reliability metrics across groups

### Open Question 4
- Question: How does evaluator group consistency change when evaluating more complex dialogue scenarios (e.g., multi-domain conversations, task-oriented dialogues)?
- Basis in paper: [inferred] The paper only tested single-domain chit-chat dialogues and noted the need for "additional dialogue models" in future work
- Why unresolved: The study's findings are limited to relatively simple chit-chat scenarios and may not generalize to more complex dialogue types
- What evidence would resolve it: Comparative evaluation studies using the same evaluator groups but with increasingly complex dialogue scenarios, measuring consistency changes across scenario types

## Limitations
- Findings are limited to 400 dialogues and four chatbot systems, restricting generalizability to other dialogue domains
- Analysis relies on self-reported evaluator expertise and group assignments, potentially introducing selection bias
- Study focuses on English-language dialogues and university-affiliated participants, limiting external validity

## Confidence

**High**: Likert evaluations are more robust across evaluator groups than pairwise selections.
**Medium**: Chatbot developers produce systematically more extreme ratings compared to less-experienced groups.
**Medium**: External evaluators show better consistency for objective dialogue metrics.

## Next Checks

1. **Cross-Domain Validation**: Replicate the evaluation framework with dialogues from a different domain (e.g., task-oriented or multilingual) to test robustness of group effects.
2. **Expert Calibration Study**: Conduct a controlled study where chatbot developers and annotators evaluate the same dialogues after standardized training to isolate expertise effects from rating style.
3. **Automated Consistency Benchmark**: Compare human inter-annotator agreement with automated metrics (e.g., LLM-based evaluators) to assess whether observed group differences reflect true evaluation variance or methodological artifacts.