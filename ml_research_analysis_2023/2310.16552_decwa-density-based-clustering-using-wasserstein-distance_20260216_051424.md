---
ver: rpa2
title: 'DECWA : Density-Based Clustering using Wasserstein Distance'
arxiv_id: '2310.16552'
source_url: https://arxiv.org/abs/2310.16552
tags:
- clusters
- clustering
- density
- distance
- decw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DECWA is a density-based clustering method that overcomes limitations
  of existing approaches in detecting low-density clusters, near clusters with similar
  densities, and high-dimensional data. It uses a probabilistic approach based on
  pairwise distance distributions and the Wasserstein metric to agglomerate sub-clusters.
---

# DECWA : Density-Based Clustering using Wasserstein Distance

## Quick Facts
- arXiv ID: 2310.16552
- Source URL: https://arxiv.org/abs/2310.16552
- Reference count: 25
- Key outcome: DECWA outperforms state-of-the-art density-based clustering methods by an average of 20% in clustering quality, particularly excelling with low-density clusters and high-dimensional data.

## Executive Summary
DECWA is a novel density-based clustering algorithm that addresses key limitations of existing approaches in detecting low-density clusters, near clusters with similar densities, and high-dimensional data. It uses a probabilistic approach based on pairwise distance distributions and the Wasserstein metric to agglomerate sub-clusters. The method builds sub-clusters from spatial density represented as probability density functions of pairwise distances, then merges them based on both density similarity and spatial proximity. Experimental results demonstrate DECWA's superior performance over methods like DBCLASD, DENCLUE, and HDBSCAN.

## Method Summary
DECWA constructs a k-nearest neighbor graph, reduces it to a minimum spanning tree to preserve significant distances, then uses kernel density estimation to model the probability density function of pairwise distances within sub-clusters. These sub-clusters are merged using the Wasserstein distance metric, which measures similarity between probability distributions. This approach allows DECWA to detect low-density clusters and handle high-dimensional data more effectively than traditional density-based clustering methods. The algorithm iteratively merges sub-clusters based on both density similarity (using Wasserstein distance) and spatial proximity until a stopping criterion is met.

## Key Results
- Outperforms state-of-the-art density-based clustering methods by an average of 20% in Adjusted Rand Index
- Excels at detecting low-density clusters that other methods miss
- Shows superior performance on high-dimensional datasets (up to 26,833 dimensions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DECW A improves low-density cluster detection by modeling each cluster as a region with its own probability law.
- Mechanism: The method represents clusters as sets of pairwise distances that follow a specific probability distribution. By using kernel density estimation (KDE) on these distances, it creates a probability density function (p.d.f.) for each cluster, allowing detection of low-density regions that traditional methods miss.
- Core assumption: Clusters can be characterized by their pairwise distance distributions following unique probability laws, rather than requiring uniform density assumptions.
- Evidence anchors:
  - [abstract]: "Our proposals are a new characterization of clusters and a new clustering algorithm based on spatial density and probabilistic approach."
  - [section]: "To address the limits described earlier, we propose to consider a cluster as a contiguous region of points where density follows its own law of probability."
  - [corpus]: Weak - no direct evidence in corpus papers about this specific probabilistic cluster characterization.
- Break condition: The approach fails when clusters have non-stationary density patterns that cannot be captured by pairwise distance distributions, or when the dataset is too sparse for reliable KDE estimation.

### Mechanism 2
- Claim: Wasserstein distance enables more sensitive merging of sub-clusters with similar but non-identical densities.
- Mechanism: DECW A uses Wasserstein metric to measure similarity between probability distributions of sub-clusters, allowing it to detect and merge clusters with similar but not identical density patterns that traditional distance metrics would miss.
- Core assumption: Wasserstein distance captures meaningful similarity between density distributions of neighboring sub-clusters, even when densities vary slightly.
- Evidence anchors:
  - [abstract]: "The key idea we propose is to use the Wasserstein metric, a powerful tool to measure the distance between p.d.f of sub-clusters."
  - [section]: "We have opted for the Wasserstein [12] distance as a measure of similarity between probability distributions because it can capture small differences on similar density clusters."
  - [corpus]: Weak - no direct evidence in corpus papers about Wasserstein distance usage in clustering.
- Break condition: When sub-clusters have significantly different shapes or when Wasserstein distance becomes computationally prohibitive for very large datasets.

### Mechanism 3
- Claim: MST-based graph reduction preserves significant distances while improving computational efficiency.
- Mechanism: By constructing a k-nearest neighbor graph and then reducing it to a Minimum Spanning Tree (MST), DECW A keeps only the most significant distances for clustering while reducing computational complexity from O(n²) to O(n log n).
- Core assumption: The MST preserves the essential distance relationships needed for clustering while removing redundant edges.
- Evidence anchors:
  - [section]: "To gain efficiency and accuracy, it is possible to get rid of many unnecessary distances. We favor short distances, avoiding keeping too large distances in the k-nearest neighbors."
  - [section]: "To gain efficiency and accuracy, it is possible to get rid of many unnecessary distances. We favor short distances, avoiding keeping too large distances in the k-nearest neighbors."
  - [corpus]: Weak - no direct evidence in corpus papers about MST-based graph reduction for clustering.
- Break condition: When the dataset contains noise points that create spurious connections in the MST, or when the optimal k value is difficult to determine for the specific dataset.

## Foundational Learning

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: KDE is used to estimate the probability density function of pairwise distances without assuming a specific distribution form, which is crucial for the probabilistic clustering approach.
  - Quick check question: What is the main advantage of using KDE over assuming a specific distribution (like Gaussian) for density estimation?

- Concept: Wasserstein Distance
  - Why needed here: This metric is used to measure similarity between probability distributions of sub-clusters, enabling sensitive detection of clusters with similar but non-identical densities.
  - Quick check question: How does Wasserstein distance differ from traditional distance metrics like Euclidean distance when comparing probability distributions?

- Concept: Minimum Spanning Tree (MST) construction
  - Why needed here: MST is used to reduce the k-nearest neighbor graph to its most essential edges, preserving significant distance relationships while improving computational efficiency.
  - Quick check question: What property of the MST ensures that it preserves the most important distance relationships for clustering purposes?

## Architecture Onboarding

- Component map:
  Input -> KNN Graph Builder -> MST Reducer -> KDE Estimator -> Sub-cluster Extractor -> Wasserstein Merger -> Final Clusters

- Critical path:
  KNN Graph → MST → KDE → Sub-cluster Extraction → Wasserstein Merging → Final Clusters

- Design tradeoffs:
  - k value in KNN graph: Larger k provides more robust distance estimates but increases computation
  - Bandwidth h in KDE: Smaller h captures more detail but may overfit noise
  - Wasserstein threshold α: Lower α requires more similar densities for merging, potentially splitting natural clusters

- Failure signatures:
  - Poor clustering quality with very high-dimensional data (curse of dimensionality affects distance distributions)
  - Failure to detect clusters when density variations are too extreme
  - Sensitivity to k parameter choice in KNN graph construction

- First 3 experiments:
  1. Test on synthetic 2D datasets with varying density patterns (e.g., jain, compound datasets)
  2. Test on high-dimensional real datasets (e.g., GCM, plant datasets) to verify high-dimensional performance claims
  3. Test with different k values in KNN graph construction to find optimal parameter range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DECW A's performance scale with increasing dataset sizes beyond those tested in the paper?
- Basis in paper: [inferred] The paper tested datasets of various sizes but did not explore scalability limits or performance on extremely large datasets.
- Why unresolved: The paper does not provide information on how the algorithm's performance (accuracy, runtime, memory usage) changes as dataset sizes increase significantly beyond the tested examples.
- What evidence would resolve it: Experimental results showing DECW A's performance metrics (ARI, runtime, memory usage) on datasets ranging from small to extremely large sizes, demonstrating scalability characteristics.

### Open Question 2
- Question: What is the optimal value for the bandwidth parameter h in the KDE estimation for different types of datasets?
- Basis in paper: [explicit] The paper mentions that the bandwidth h is an important hyperparameter but does not provide a systematic method for determining optimal values across different dataset types.
- Why unresolved: The paper uses random search to find good parameter values but does not analyze how the optimal bandwidth varies with dataset characteristics or provide guidance for selecting it in practice.
- What evidence would resolve it: A comprehensive study analyzing the relationship between dataset characteristics (dimensionality, density distribution, noise levels) and optimal bandwidth values, along with practical recommendations for parameter selection.

### Open Question 3
- Question: How does DECW A handle datasets with varying noise levels and what is its robustness to outliers?
- Basis in paper: [inferred] While the paper reports outlier ratios for tested datasets, it does not systematically evaluate DECW A's performance across different noise levels or analyze its robustness to varying outlier proportions.
- Why unresolved: The paper does not provide information on how DECW A's clustering quality degrades as noise levels increase or how it compares to other methods in noisy conditions.
- What evidence would resolve it: Experimental results showing DECW A's performance on datasets with systematically varied noise levels, comparison with other methods under different noise conditions, and analysis of its outlier detection capabilities.

## Limitations

- Missing detailed hyperparameter specifications (k value for k-NN, bandwidth h for KDE, λ and α for agglomeration) makes exact reproduction challenging
- Incomplete description of the iterative agglomeration process with unclear stopping criteria
- Limited discussion of computational complexity analysis, particularly for high-dimensional datasets

## Confidence

- **High confidence** in the core conceptual framework (using probability distributions of pairwise distances and Wasserstein metric for clustering)
- **Medium confidence** in experimental results and performance claims, as they show statistically significant improvements but lack detailed methodology
- **Low confidence** in reproducibility due to missing implementation details and hyperparameter values

## Next Checks

1. Implement and test the algorithm with multiple k values (e.g., 5, 10, 15) on synthetic datasets to determine optimal parameter ranges
2. Compare DECWA's performance against HDBSCAN and DBSCAN on high-dimensional real datasets (GCM, plant datasets) to verify the claimed superiority
3. Conduct sensitivity analysis on the Wasserstein distance threshold parameter to understand its impact on clustering quality and stability