---
ver: rpa2
title: Information based explanation methods for deep learning agents -- with applications
  on large open-source chess models
arxiv_id: '2309.09702'
source_url: https://arxiv.org/abs/2309.09702
tags:
- chess
- training
- information
- given
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work replicates concept detection results from McGrath et
  al. (2022) using open-source chess models from the Leela Chess Zero project, demonstrating
  comparable detection of concepts like "in check," "material advantage," and "queen
  threat" across multiple model iterations.
---

# Information based explanation methods for deep learning agents -- with applications on large open-source chess models

## Quick Facts
- **arXiv ID**: 2309.09702
- **Source URL**: https://arxiv.org/abs/2309.09702
- **Reference count**: 17
- **Key outcome**: This work replicates concept detection results from McGrath et al. (2022) using open-source chess models from the Leela Chess Zero project, demonstrating comparable detection of concepts like "in check," "material advantage," and "queen threat" across multiple model iterations. Additionally, it introduces a novel explainable AI method—the Information Importance Map (II-map)—which guarantees exhaustive and exclusive identification of the information used by a chess-playing model by training a masker module to control input visibility during inference. The II-map is trained alongside a custom chess model and generates saliency maps that highlight relevant pieces and positions with high fidelity, as validated on chess puzzles and famous games. While the method ensures strong interpretability guarantees, training stability remains a challenge. The masker model and interactive tool are made publicly available.

## Executive Summary
This paper presents two main contributions to explainable AI for deep learning chess agents. First, it successfully replicates the concept detection methodology from McGrath et al. (2022) using open-source Leela Chess Zero models, demonstrating that these models learn similar intermediate representations for chess concepts like "in check," "material advantage," and "queen threat." Second, the paper introduces the Information Importance Map (II-map), a novel method that guarantees exhaustive and exclusive identification of information used by a chess-playing model by controlling information flow through a trainable masker module. The II-map provides strict interpretability guarantees by directly controlling what parts of the input are available to the model during training and inference.

## Method Summary
The study employs two primary methodologies: concept detection and the Information Importance Map. For concept detection, logistic probes are trained on intermediate activations of Leela Chess Zero models to detect nine chess concepts, with binary accuracy corrected for random guessing. The II-map method involves training a masker module alongside the main chess model, where the masker learns to predict a binary mask that stochastically removes irrelevant pieces from the input. This is achieved through an L1 penalty to minimize information usage while maintaining prediction accuracy, with the straight-through estimator enabling gradient-based training of the binarized mask. The method is validated on chess puzzles and famous games, producing saliency maps that highlight relevant pieces and positions.

## Key Results
- Concept detection successfully replicated on Leela Chess Zero models, showing similar patterns to AlphaZero for concepts like "in check," "material advantage," and "queen threat"
- II-map generates high-fidelity saliency maps that highlight relevant pieces and positions in chess puzzles and famous games
- The masker module effectively learns to identify and remove irrelevant information while maintaining model performance
- Training stability remains a challenge, particularly when applying the method to larger models or different domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Information Importance Map (II-map) guarantees exhaustive and exclusive identification of information used by a chess-playing model by controlling information flow through a trainable masker.
- **Mechanism**: The masker module learns to predict a binary mask that stochastically removes irrelevant pieces from the input, ensuring the main model never sees information deemed unimportant. This is achieved by training the masker alongside the main model with an L1 penalty to minimize information usage while maintaining prediction accuracy.
- **Core assumption**: The masker can learn to identify truly irrelevant information without harming model performance, and the straight-through estimator provides adequate gradient signals for training the binarized mask.
- **Evidence anchors**:
  - [abstract]: "Our presented method has the desirable property of controlling the information flow between any input vector and the given model, which in turn provides strict guarantees regarding what information is used by the trained model during inference."
  - [section]: "We achieve this by appending a structure providing direct control over what parts of a given input sample is made available to the model during training."
- **Break condition**: If the masker cannot distinguish between relevant and irrelevant information (e.g., due to adversarial representations or insufficient training data), the guarantees break down.

### Mechanism 2
- **Claim**: The II-map method provides more reliable saliency explanations for chess than gradient-based methods like GradCAM.
- **Mechanism**: Unlike perturbation-based or gradient-based methods that can produce misleading saliency maps in discrete domains like chess, the II-map directly controls what information reaches the model, making the explanations inherently correct by construction.
- **Core assumption**: In chess, small perturbations drastically change the nature of positions, making perturbation-based methods unreliable, while gradient-based methods may highlight pieces that the model doesn't actually depend on.
- **Evidence anchors**:
  - [section]: "We found that the usefulness of existing gradient based methods, such as the widely used GradCAM... is limited for our case of chess... the generated saliency maps do not necessarily apply to the entire position."
  - [section]: "Perturbation based methods... are not viable since it is not obvious how to perturb a given state while assuring that the perturbation remains 'close' in the model's input space."
- **Break condition**: If the masker training becomes unstable or the L1 regularization is improperly tuned, the explanations may become unreliable or overly aggressive in removing information.

### Mechanism 3
- **Claim**: The concept detection results from AlphaZero can be replicated using open-source Leela Chess Zero models.
- **Mechanism**: By applying the same concept detection methodology (logistic probes on intermediate activations) to Leela Chess Zero models, the study demonstrates that similar patterns of concept learning emerge, validating that open-source models internalize comparable chess knowledge.
- **Core assumption**: The Leela Chess Zero models, despite being trained differently and without access to AlphaZero's specific architecture, learn similar intermediate representations for chess concepts.
- **Evidence anchors**:
  - [abstract]: "We obtain results similar to those achieved on AlphaZero, while relying solely on open-source resources."
  - [section]: "For all the presented concepts, we see a striking similarity to the corresponding concept-results as presented in McGrath et al. (2022)."
- **Break condition**: If the open-source models have fundamentally different architectures or training procedures that lead to different internal representations, the concept detection results may diverge significantly.

## Foundational Learning

- **Concept: Binary concept detection with logistic probes**
  - Why needed here: To verify that the chess models learn interpretable intermediate representations for chess concepts like "in check" or "material advantage"
  - Quick check question: How does the binary accuracy correction for random guessing work in concept detection?

- **Concept: Monte Carlo Tree Search (MCTS) in reinforcement learning**
  - Why needed here: Understanding how Leela Chess Zero models are trained through self-play with MCTS is crucial for interpreting their learned representations
  - Quick check question: Why does combining MCTS with neural network evaluation improve the quality of self-play games compared to using the network alone?

- **Concept: Straight-through estimator for binarized operations**
  - Why needed here: Essential for training the II-map's stochastic binary masker module using backpropagation
  - Quick check question: How does the straight-through estimator approximate gradients through the Heaviside function?

## Architecture Onboarding

- **Component map**: Input (8, 8, 21) tensor → Masker (II-map) → Masked input → Residual network (20 blocks) → Policy/Value outputs

- **Critical path**: Input → Masker (II-map) → Masked input → Residual network → Policy/Value outputs

- **Design tradeoffs**:
  - II-map adds training complexity and instability vs. providing strong interpretability guarantees
  - L1 regularization on mask encourages sparsity but may reduce model accuracy
  - Training masker alongside main model is computationally expensive for large models

- **Failure signatures**:
  - Masker produces all-ones masks (no information reduction)
  - Masker produces all-zeros masks (excessive information removal)
  - Training instability with exploding/vanishing gradients
  - Main model performance degradation when II-map is active

- **First 3 experiments**:
  1. Train a small chess model with II-map on a simplified chess variant (e.g., 6x6 board) to verify basic functionality
  2. Apply concept detection to a trained Leela Chess Zero model checkpoint to validate replication of results
  3. Test II-map on a set of chess puzzles to evaluate explanation quality and identify failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the maximum acceptable L1-penalty weight for the masker across different input spaces?
- **Basis in paper**: [inferred] from "it remains to be seen how large the range of acceptable values for the weight of the L1-penalty used for the masker for all such input spaces"
- **Why unresolved**: The paper notes this is unknown and requires further investigation
- **What evidence would resolve it**: Empirical testing across various input spaces (images, chess, text, etc.) to determine stable ranges of L1-penalty values that maintain training stability while achieving good masker performance

### Open Question 2
- **Question**: How does masker model architecture affect training stability and performance?
- **Basis in paper**: [inferred] from "how the architecture chosen for the masker model affects the rest of the training procedure"
- **Why unresolved**: The paper mentions this affects training stability but doesn't provide systematic analysis
- **What evidence would resolve it**: Controlled experiments varying masker architectures while keeping other parameters constant, measuring training stability metrics and final model performance

### Open Question 3
- **Question**: Can the II-map method be made more stable for image-based models?
- **Basis in paper**: [explicit] from "our preliminary investigations show that it is possible to train image-based models using a II-map module. However, this training procedure was significantly more unstable than with chess as the input space"
- **Why unresolved**: The paper identifies this as a challenge but doesn't provide solutions
- **What evidence would resolve it**: Successful training of image-based models using II-map with stable convergence, ideally through architectural modifications or training procedure improvements

### Open Question 4
- **Question**: Does the II-map method produce truly adversarial representations or just unintuitive ones?
- **Basis in paper**: [explicit] from "its potential as a pure explanatory method can be said to be limited by the fact that it seems to cause the model to learn somewhat adversarial representations"
- **Why unresolved**: The paper acknowledges this limitation but doesn't provide clear criteria for distinguishing between adversarial and unintuitive representations
- **What evidence would resolve it**: Systematic analysis comparing representations learned with and without II-map, examining whether removing masker-learned pieces actually harms performance on standard evaluation tasks

## Limitations

- The II-map method introduces significant training complexity and instability that was only partially addressed through hyperparameter tuning
- The method's effectiveness on large-scale chess models remains untested, as the authors trained it on a custom model rather than applying it to existing LCZero checkpoints
- The concept detection replication relies on a limited set of nine chess concepts, and results may not generalize to other concepts or domains beyond chess

## Confidence

- **High confidence**: The claim that II-map provides strict guarantees about information usage through direct control of information flow
- **Medium confidence**: The replication of AlphaZero concept detection results using LCZero models, as the methodology is well-established but the specific model versions used are not fully specified
- **Medium confidence**: The visual quality of II-map saliency explanations, as they appear intuitive but lack quantitative validation metrics

## Next Checks

1. Apply the II-map method to a large, pre-trained LCZero model checkpoint to verify scalability and effectiveness on production-grade models
2. Conduct ablation studies varying the L1 regularization strength to quantify the tradeoff between information removal and model performance
3. Test the concept detection methodology on a broader set of chess concepts and potentially apply it to other board games to assess generalizability