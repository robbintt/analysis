---
ver: rpa2
title: 'Generalizing to Unseen Elements: A Survey on Knowledge Extrapolation for Knowledge
  Graphs'
arxiv_id: '2302.01859'
source_url: https://arxiv.org/abs/2302.01859
tags:
- unseen
- entities
- knowledge
- entity
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey paper provides a comprehensive overview of the field
  of knowledge extrapolation for knowledge graphs, where the goal is to generalize
  to unseen entities or relations during model testing. The authors propose a taxonomy
  to categorize existing methods based on their model design, including entity encoding-based,
  subgraph predicting-based, and rule learning-based approaches for entity extrapolation,
  as well as relation encoding-based and entity pair matching-based approaches for
  relation extrapolation.
---

# Generalizing to Unseen Elements: A Survey on Knowledge Extrapoles for Knowledge Graphs

## Quick Facts
- arXiv ID: 2302.01859
- Source URL: https://arxiv.org/abs/2302.01859
- Reference count: 2
- Primary result: Comprehensive survey of knowledge extrapolation methods for knowledge graphs, proposing a taxonomy based on model design and discussing future research directions

## Executive Summary
This survey paper comprehensively reviews the field of knowledge extrapolation for knowledge graphs, addressing the challenge of generalizing to unseen entities or relations during model testing. The authors propose a taxonomy to categorize existing methods based on their model design, including entity encoding-based, subgraph predicting-based, and rule learning-based approaches for entity extrapolation, as well as relation encoding-based and entity pair matching-based approaches for relation extrapolation. The paper introduces commonly used benchmarks and compares methods based on their assumptions, the information used in the support set, and their ability to handle semi- and fully-entity extrapolation. The authors also discuss future research directions, such as exploring diverse applications, multi-modal support information, and temporal and lifelong settings.

## Method Summary
The paper surveys existing knowledge extrapolation methods for knowledge graphs, classifying them into distinct categories based on their approach to handling unseen entities and relations. For entity extrapolation, the taxonomy includes entity encoding-based methods that learn to encode unseen entities from support information, subgraph predicting-based methods that learn from enclosing subgraphs between entities, and rule learning-based methods that induce logical rules from support triples. For relation extrapolation, the paper discusses relation encoding-based methods that encode unseen relations from support information and entity pair matching-based methods that learn matching patterns for relation prediction. The survey analyzes these methods across various dimensions including their assumptions, support information requirements, and capabilities for handling different extrapolation scenarios.

## Key Results
- Proposed a comprehensive taxonomy categorizing knowledge extrapolation methods into entity encoding-based, subgraph predicting-based, and rule learning-based approaches for entity extrapolation, and relation encoding-based and entity pair matching-based approaches for relation extrapolation
- Identified key challenges in knowledge extrapolation including handling fully-entity extrapolation, incorporating diverse support information types, and addressing temporal and lifelong learning settings
- Highlighted the need for more diverse applications beyond link prediction and the potential of incorporating visual information into knowledge extrapolation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity encoding-based methods work by learning to encode unseen entities from support information rather than relying on fixed embedding tables.
- Mechanism: Instead of looking up pre-learned embeddings, these methods use learnable encoders (e.g., GNNs, Transformers) that transform support information (triples, text, etc.) into embeddings for unseen entities during inference.
- Core assumption: The support set contains sufficient information about unseen entities to generate meaningful embeddings.
- Evidence anchors:
  - [abstract]: "we use a set of general terminologies to unify these methods and refer to them as Knowledge Extrapolation"
  - [section]: "Conventional knowledge graph embedding methods learn an embedding lookup table for entities, and such a paradigm hinders models from extrapolating to unseen entities"
  - [corpus]: No direct corpus evidence found; this is a theoretical inference from the survey's description.
- Break condition: When support information is sparse, noisy, or entirely absent for unseen entities, the encoder cannot produce meaningful embeddings.

### Mechanism 2
- Claim: Subgraph predicting-based methods work by encoding entity-independent relational semantics from subgraphs.
- Mechanism: These methods extract enclosing subgraphs between entity pairs and use GNNs or other encoders to learn patterns that are independent of specific entities, allowing generalization to unseen entities.
- Core assumption: The relational semantics captured in subgraphs are sufficient to predict missing relations, regardless of which specific entities are involved.
- Evidence anchors:
  - [abstract]: "we comprehensively summarize these methods, classified by our proposed taxonomy"
  - [section]: "This perspective assumes that the semantics underlying the subgraph between two entities can be used to predict their relation"
  - [corpus]: No direct corpus evidence found; this is derived from the survey's taxonomy description.
- Break condition: When subgraphs become too sparse or lack sufficient relational patterns, the model cannot generalize effectively to unseen entities.

### Mechanism 3
- Claim: Relation encoding-based methods work by learning to encode unseen relations from support information rather than relying on pre-trained relation embeddings.
- Mechanism: These methods use learnable encoders that transform support information (triples, text, ontology) into embeddings for unseen relations during inference, enabling zero-shot or few-shot relation prediction.
- Core assumption: The support set contains sufficient information about unseen relations to generate meaningful embeddings.
- Evidence anchors:
  - [abstract]: "we introduce commonly used benchmarks and compare methods based on their assumptions"
  - [section]: "the shortcoming of conventional knowledge graph embedding methods on relation extrapolation is that they cannot give unseen relations reasonable embeddings"
  - [corpus]: No direct corpus evidence found; this is a theoretical inference from the survey's framework.
- Break condition: When support information for unseen relations is insufficient or incompatible with the encoding mechanism, the model fails to generate proper relation embeddings.

## Foundational Learning

- Concept: Knowledge Graph Embedding (KGE)
  - Why needed here: KGE is the foundation that knowledge extrapolation methods build upon and extend. Understanding conventional KGE helps explain why and how extrapolation methods are necessary.
  - Quick check question: What is the main limitation of conventional KGE methods when dealing with unseen entities or relations?

- Concept: Meta-learning
  - Why needed here: Many knowledge extrapolation methods use meta-learning to learn how to generalize to unseen elements from limited support information.
  - Quick check question: How does optimization-based meta-learning help relation encoding-based methods generate embeddings for unseen relations?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are a key component in many knowledge extrapolation methods for encoding structural information from KGs.
  - Quick check question: Why are GNNs particularly suitable for subgraph predicting-based entity extrapolation?

## Architecture Onboarding

- Component map:
  - Support Set Processor -> Encoder -> Score Function -> Training Module

- Critical path:
  1. Receive support information for unseen elements
  2. Process support information through encoder
  3. Generate embeddings for unseen elements
  4. Use embeddings in score function to predict links
  5. Update encoder parameters based on prediction performance

- Design tradeoffs:
  - Fixed vs. learnable embeddings: Fixed tables are faster but cannot handle unseen elements; learnable encoders are more flexible but computationally expensive
  - Structural vs. non-structural support: Structural information is more domain-specific but may be sparser; non-structural (text) is more abundant but may be noisier
  - Entity-pair vs. individual encoding: Entity-pair encoding captures relational context but may miss individual entity characteristics

- Failure signatures:
  - Poor performance on unseen entities/relations despite good performance on seen elements
  - Overfitting to support information, leading to poor generalization
  - Inability to handle completely new KG structures (fully-entity extrapolation)

- First 3 experiments:
  1. Implement a simple entity encoding-based method on the WN11 dataset and compare with conventional KGE baselines
  2. Test a subgraph predicting-based method on the WN18RR dataset to evaluate fully-entity extrapolation capability
  3. Implement a relation encoding-based method with textual support information on the NELL-ZS dataset to test zero-shot learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge extrapolation methods be effectively evaluated on diverse applications beyond simple link prediction?
- Basis in paper: [explicit] The paper mentions that most existing methods are evaluated by link prediction, but suggests exploring diverse applications like KG enhanced question answering and other NLP tasks.
- Why unresolved: Existing benchmarks and evaluation protocols primarily focus on link prediction, limiting the understanding of method performance on other tasks.
- What evidence would resolve it: Development of new benchmarks and evaluation metrics for knowledge extrapolation methods on various applications, along with empirical studies comparing method performance across these tasks.

### Open Question 2
- Question: How can visual information be effectively incorporated into knowledge extrapolation methods?
- Basis in paper: [explicit] The paper suggests that images can help generalize KGE to unseen elements, as they can be universally understood by pre-trained encoders, but notes that few works address this capability.
- Why unresolved: Current knowledge extrapolation methods primarily focus on textual and structural information, with limited exploration of visual modalities.
- What evidence would resolve it: Development of novel knowledge extrapolation methods that effectively integrate visual information, along with empirical studies demonstrating their effectiveness compared to methods using only textual and structural information.

### Open Question 3
- Question: How can knowledge extrapolation methods handle the simultaneous emergence of unseen entities and relations?
- Basis in paper: [explicit] The paper mentions that existing research mainly focuses on entity or relation extrapolation separately, while in real-world applications, unseen entities and relations may emerge simultaneously.
- Why unresolved: Current methods are designed to handle either entity or relation extrapolation, and there is a lack of effective integration of these approaches to address the simultaneous emergence of both.
- What evidence would resolve it: Development of new knowledge extrapolation methods that can effectively handle the simultaneous emergence of unseen entities and relations, along with empirical studies demonstrating their performance compared to methods addressing each scenario separately.

## Limitations
- Limited empirical validation: The paper is a survey that synthesizes existing work but does not provide new experimental results to validate the proposed taxonomy or mechanisms.
- Incomplete implementation details: Many methods mentioned lack specific architectural details, hyperparameter settings, or training procedures that would be necessary for faithful reproduction.
- Limited scope of evaluation: The survey focuses primarily on link prediction tasks and does not extensively discuss other potential applications or downstream tasks where knowledge extrapolation might be valuable.

## Confidence
- Mechanism 1 (Entity encoding-based): Medium confidence - The mechanism is logically sound based on the survey description, but lacks empirical validation and specific implementation details.
- Mechanism 2 (Subgraph predicting-based): Medium confidence - The theoretical framework is clear, but the effectiveness depends heavily on the quality and density of subgraphs, which is not extensively evaluated.
- Mechanism 3 (Relation encoding-based): Medium confidence - The approach is theoretically justified, but practical challenges in encoding diverse relation types are not fully explored.

## Next Checks
1. **Empirical validation of taxonomy**: Implement and compare at least two methods from each of the three entity extrapolation categories (entity encoding-based, subgraph predicting-based, rule learning-based) on the same benchmark to validate that the taxonomy meaningfully captures distinct approaches with different strengths and weaknesses.

2. **Support information quality analysis**: Systematically vary the quality, quantity, and type of support information (triples, text, ontology) across different methods to determine which types are most effective for different extrapolation scenarios and identify failure conditions.

3. **Cross-task generalization study**: Test knowledge extrapolation methods originally designed for link prediction on other KG tasks such as entity classification, relation extraction, or question answering to evaluate their broader applicability beyond the surveyed benchmarks.