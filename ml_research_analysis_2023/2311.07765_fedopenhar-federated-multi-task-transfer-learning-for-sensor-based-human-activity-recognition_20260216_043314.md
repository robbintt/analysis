---
ver: rpa2
title: 'FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human
  Activity Recognition'
arxiv_id: '2311.07765'
source_url: https://arxiv.org/abs/2311.07765
tags:
- learning
- federated
- data
- activity
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy-preserving multi-task learning for
  sensor-based human activity recognition and device position identification. It proposes
  a federated multi-task transfer learning framework using DeepConvLSTM in the Flower
  environment, trained on the OpenHAR dataset containing ten smaller datasets from
  211 participants.
---

# FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition

## Quick Facts
- arXiv ID: 2311.07765
- Source URL: https://arxiv.org/abs/2311.07765
- Reference count: 3
- Key outcome: Federated multi-task transfer learning achieves 72.4% accuracy, comparable to individual client training (72.6%) and significantly outperforming simple federated training (42.7%)

## Executive Summary
This paper presents a federated multi-task transfer learning framework for sensor-based human activity recognition and device position identification. The method uses a layered hierarchy of pre-trained, common, task-specific, and personalized layers within a DeepConvLSTM architecture implemented in the Flower federated learning environment. The framework is trained on the OpenHAR dataset containing ten smaller datasets from 211 participants, demonstrating that federated multi-task transfer learning can achieve high accuracy while preserving privacy with limited labeled data.

## Method Summary
The method employs a layered hierarchy of pre-trained, common, task-specific, and personalized layers to enable effective parameter sharing across heterogeneous client datasets with varying label types. Training involves pre-training on one dataset, common layer training across all clients, task-specific layer training for activities and positions, and personalization using each client's own data. The DeepConvLSTM architecture is implemented in the Flower federated learning environment using FedAvg aggregation.

## Key Results
- Federated multi-task transfer learning achieves 72.4% accuracy
- Individual client training baseline achieves 72.6% accuracy
- Simple federated multi-task training achieves only 42.7% accuracy
- Centralized training achieves 69.8% for activities and 59.2% for positions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layered hierarchy enables effective parameter sharing across heterogeneous client datasets with varying label types.
- Mechanism: The federated multi-task transfer learning framework uses a layered hierarchy consisting of pre-trained, common, task-specific, and personalized layers. The pre-trained layer provides initial weights from one client, the common layer aggregates parameters from all clients without task restriction, and task-specific layers are trained separately for each task. Finally, personalized layers are fine-tuned on each client's own data.
- Core assumption: Different clients have heterogeneous datasets with varying label types, but some underlying patterns can be shared across tasks and clients.
- Evidence anchors:
  - [abstract]: "The method employs a layered hierarchy of pre-trained, common, task-specific, and personalized layers to enable effective parameter sharing across heterogeneous client datasets with varying label types."
  - [section]: "The main layered hierarchy can be seen in Figure 2. However, in this study, instead of using separate servers for each task, the same server trains each layer of the hierarchy, and every time a layer's training is finished, its parameters are frozen, and the training is applied to the subsequent layers."
  - [corpus]: The corpus neighbors include papers on transfer learning and invariant feature learning for HAR, suggesting this mechanism is relevant to the field.
- Break condition: If the heterogeneity between label types is too extreme, parameter sharing may not be effective, leading to poor performance.

### Mechanism 2
- Claim: Transfer learning enables learning new labels and tasks with limited labeled data.
- Mechanism: By using a transfer learning approach, the clients can get similar results with a limited amount of labeled data, which is used in the personalization step. The pre-trained and common layers provide a good starting point for learning new tasks, and the task-specific layers allow for fine-tuning on the specific tasks.
- Core assumption: There is some similarity between the tasks (activity recognition and device position identification) that can be leveraged for transfer learning.
- Evidence anchors:
  - [abstract]: "This demonstrates that the proposed method enables effective privacy-preserving learning with limited labeled data while maintaining high performance."
  - [section]: "Due to space limitations, we cannot present the accuracy results for each client. However, we observe that at every layer, the accuracy increases and gets closer to the individual training baseline."
  - [corpus]: The corpus neighbors include papers on transfer learning in HAR, supporting the relevance of this mechanism.
- Break condition: If the tasks are too dissimilar, transfer learning may not be effective, and training from scratch may be necessary.

### Mechanism 3
- Claim: Federated learning preserves privacy while enabling collaborative learning.
- Mechanism: Instead of sharing data among learning participants (clients), model parameters are shared, and the learning is realized in a distributed manner. This way, data privacy is assured, and the wireless communication costs decrease because only model parameters are shared between participants instead of big chunks of data.
- Core assumption: Data privacy is a concern, and centralized data collection may violate user privacy.
- Evidence anchors:
  - [abstract]: "This paper explores Federated Transfer Learning in a Multi-Task manner for both sensor-based human activity recognition and device position identification tasks."
  - [section]: "Federated learning is a novel technique in machine learning first announced by Google. The main goal of federated learning is to treat data in a privacy-preserving manner."
  - [corpus]: The corpus neighbors include papers on federated learning for HAR, indicating the relevance of this mechanism.
- Break condition: If the privacy concerns are not significant, centralized learning may be preferred for its simplicity and potential for higher accuracy.

## Foundational Learning

- Concept: Deep learning for sensor-based human activity recognition
  - Why needed here: The paper uses DeepConvLSTM, a deep learning architecture that combines convolutional neural networks with LSTM networks, for sensor-based human activity recognition.
  - Quick check question: What are the key components of the DeepConvLSTM architecture, and how do they contribute to its effectiveness for HAR?

- Concept: Federated learning
  - Why needed here: The paper uses federated learning to enable collaborative learning across multiple clients while preserving data privacy.
  - Quick check question: How does federated learning differ from centralized learning, and what are the key benefits and challenges of using federated learning for HAR?

- Concept: Transfer learning
  - Why needed here: The paper uses transfer learning to enable learning new labels and tasks with limited labeled data.
  - Quick check question: What are the key principles of transfer learning, and how can they be applied to sensor-based human activity recognition?

## Architecture Onboarding

- Component map: Pre-trained layer -> Common layer -> Task-specific layers (activities and positions) -> Personalized layers
- Critical path:
  1. Pre-train initial weights on one client's dataset
  2. Train common layers on all clients' datasets without task restriction
  3. Train task-specific layers separately for each task (activity recognition and device position identification)
  4. Fine-tune personalized layers on each client's own data
- Design tradeoffs:
  - Privacy vs. accuracy: Federated learning preserves privacy but may result in lower accuracy compared to centralized learning
  - Complexity vs. performance: The layered hierarchy adds complexity but enables effective parameter sharing and transfer learning
- Failure signatures:
  - Low accuracy: Indicates issues with the model architecture, training process, or data quality
  - Communication failures: May occur due to network issues or client unavailability in federated learning
  - Privacy breaches: May occur if data is leaked during the training process
- First 3 experiments:
  1. Train the DeepConvLSTM model on a single client's dataset using centralized learning
  2. Train the DeepConvLSTM model using federated learning with all clients, but without the layered hierarchy
  3. Train the DeepConvLSTM model using federated multi-task transfer learning with the layered hierarchy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the federated multi-task transfer learning framework perform when the number of available labeled data per client is significantly reduced?
- Basis in paper: [explicit] The paper states "by using a transfer learning approach, the clients can get similar results with a limited amount of labelled data, which is used in the personalization step."
- Why unresolved: The paper does not provide experiments with varying amounts of labeled data per client, only mentioning the potential for limited labeled data without testing it.
- What evidence would resolve it: Experiments showing performance metrics (accuracy, F1-score) across different fractions of labeled data (e.g., 10%, 25%, 50%, 75%, 100%) for each client would demonstrate the framework's robustness to label scarcity.

### Open Question 2
- Question: How would different federated learning aggregation algorithms (beyond FedAvg) affect the performance of the multi-task transfer learning approach?
- Basis in paper: [explicit] The paper mentions "different averaging algorithms than FedAvg" as a future study direction and notes that "FedProx, and Federated Matched Averaging are some of the most commonly used averaging algorithms in Federated Learning."
- Why unresolved: The paper only uses FedAvg for aggregation, so there is no comparison with alternative aggregation methods.
- What evidence would resolve it: Running the same experiments with FedProx, FedNova, and SCAFFOLD aggregation algorithms and comparing their performance metrics against FedAvg would show which aggregation method is most effective for this specific problem.

### Open Question 3
- Question: How does the performance of the federated multi-task transfer learning framework scale when new activity labels or device positions are introduced after initial training?
- Basis in paper: [explicit] The paper states the model "opens the path to learning new labels and tasks with its layered hierarchy" but does not test this capability.
- Why unresolved: The experiments only use the fixed set of 14 different labels from the OpenHAR dataset, without testing the framework's ability to adapt to new label types.
- What evidence would resolve it: Retraining the framework with additional label types (either from the same dataset or new datasets) and measuring the transfer learning effectiveness and accuracy on both old and new labels would demonstrate the framework's scalability.

## Limitations

- The experiments are conducted on a single dataset (OpenHAR) with specific preprocessing choices, limiting generalizability
- The paper does not address potential performance degradation when scaling to larger numbers of clients or more diverse sensor types
- Hyperparameter choices are not fully specified, creating uncertainty about the sensitivity of the layered hierarchy approach

## Confidence

- High confidence: The core finding that the proposed method achieves comparable accuracy (72.4%) to individual client training (72.6%) while significantly outperforming simpler federated approaches (42.7%)
- Medium confidence: The generalizability of these results to other datasets and sensor configurations
- Major uncertainties include sensitivity to hyperparameter choices and potential impact of label distribution heterogeneity across clients

## Next Checks

1. **Ablation study**: Test each layer of the hierarchy independently to quantify the contribution of pre-trained, common, task-specific, and personalized layers to overall performance.
2. **Cross-dataset evaluation**: Validate the model on additional HAR datasets with different sensor configurations to assess generalizability beyond the OpenHAR dataset.
3. **Communication efficiency analysis**: Measure the actual communication overhead and bandwidth requirements compared to baseline federated learning approaches to quantify privacy-accuracy-communication tradeoffs.