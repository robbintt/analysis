---
ver: rpa2
title: Learning to Intervene on Concept Bottlenecks
arxiv_id: '2308.13453'
source_url: https://arxiv.org/abs/2308.13453
tags:
- interventions
- cb2m
- concept
- mistakes
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Concept Bottleneck Memory Models (CB2M) to
  improve upon concept bottleneck models (CBMs) by leveraging a two-fold memory of
  previous interventions and mistakes. CB2M can detect potential model mistakes and
  generalize interventions to similar unseen data points.
---

# Learning to Intervene on Concept Bottlenecks

## Quick Facts
- arXiv ID: 2308.13453
- Source URL: https://arxiv.org/abs/2308.13453
- Reference count: 13
- This work introduces Concept Bottleneck Memory Models (CB2M) to improve upon concept bottleneck models (CBMs) by leveraging a two-fold memory of previous interventions and mistakes.

## Executive Summary
This paper addresses a critical limitation of Concept Bottleneck Models (CBMs): the inefficiency of requiring repeated human interventions for similar mistakes. CB2M introduces a two-fold memory system that stores both past model mistakes and their corresponding human interventions. This enables the model to detect potential errors on new inputs by comparing their encodings to stored mistakes, and automatically apply previously learned interventions when similar errors are detected. The approach significantly reduces the need for repeated human corrections while improving both concept and class accuracy.

## Method Summary
CB2M extends standard CBMs by adding two memory components: a mistake memory that stores encodings of inputs where the model made errors, and an intervention memory that stores the corresponding human-provided concept corrections. During inference, new inputs are compared to stored mistakes using Euclidean distance in the bottleneck encoding space. If a new input is sufficiently similar to a stored mistake, the associated intervention is automatically applied without requiring new human input. This generalization mechanism allows CB2M to reuse interventions across similar unseen examples, substantially reducing the intervention burden while improving model accuracy.

## Key Results
- On CUB dataset: Concept accuracy reaches 99.0% and class accuracy 88.7% on identified data points, compared to 86.4% and 5.0% for standard CBM
- On Parity MNIST: Concept accuracy reaches 98.7%, demonstrating effectiveness on unbalanced and confounded datasets
- On Parity C-MNIST: Concept accuracy reaches 95.5%, showing robustness across different dataset types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-fold memory structure enables CB2M to generalize interventions to similar unseen data points.
- Mechanism: The mistake memory stores encodings of past model errors, while the intervention memory stores corresponding corrections. When a new input has an encoding similar to a stored mistake (based on Euclidean distance), the associated intervention is automatically reapplied.
- Core assumption: Similar input encodings correspond to similar model errors and require similar corrections.
- Evidence anchors:
  - [abstract]: "CB2M can detect potential model mistakes and generalize interventions to similar unseen data points."
  - [section]: "The intervention memory directly keeps track of known interventions and associates them to elements of the mistake memory, meaning that the memorized intervention i can be used to correct the memorized mistake of xe."
  - [corpus]: Weak evidence. The related papers discuss memory reasoning and post-hoc CBMs but do not specifically address intervention generalization via two-fold memory.
- Break condition: If the model's decision boundary changes significantly between training and deployment, or if concept relationships shift, similar encodings may no longer require the same interventions.

### Mechanism 2
- Claim: CB2M can detect potential model mistakes before human intervention by comparing new inputs to stored mistakes.
- Mechanism: The mistake memory contains encodings of inputs where the model made errors. For a new input, if its encoding is within a threshold distance of k stored mistakes, it is flagged as a potential error requiring intervention.
- Core assumption: If a new input is similar to previous mistakes, it is likely to be misclassified by the model.
- Evidence anchors:
  - [abstract]: "CB2M can detect potential model mistakes and generalize interventions to similar unseen data points."
  - [section]: "If we find k mistakes with a distance to ˆxe smaller than td, we consider a model to be making a known mistake."
  - [corpus]: Weak evidence. Related work on error detection (e.g., TrustScore) uses softmax probabilities, while CB2M specifically targets bottleneck network errors via similarity.
- Break condition: If the model improves significantly after training, the mistake memory becomes outdated and false positives increase. Alternatively, if concept representations change drastically, similarity in encoding space may not indicate similar errors.

### Mechanism 3
- Claim: CB2M reduces the need for repeated human interventions by reusing previously provided corrections.
- Mechanism: When a human intervenes on a CBM's concept predictions, CB2M stores both the corrected concepts and the input encoding. Future inputs with similar encodings automatically receive the stored correction without requiring new human input.
- Core assumption: Human interventions are correct and applicable to similar inputs.
- Evidence anchors:
  - [abstract]: "CB2M learns to automatically improve model performance from a few initially obtained interventions."
  - [section]: "The intervention memory directly keeps track of known interventions and associates them to elements of the mistake memory, meaning that the memorized intervention i can be used to correct the memorized mistake of xe."
  - [corpus]: Weak evidence. While related papers discuss interventions in CBMs, they focus on ordering concepts for intervention rather than reusing interventions via memory.
- Break condition: If interventions are applied to inputs that are only superficially similar but contextually different, or if human interventions contain errors, reusing them may propagate mistakes.

## Foundational Learning

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: CB2M is an extension of CBMs, so understanding their structure and intervention mechanism is fundamental.
  - Quick check question: In a CBM, what are the two main components that transform input to output, and what role do concepts play?

- Concept: Distance metrics in embedding space
  - Why needed here: CB2M relies on Euclidean distance between input encodings to determine similarity and decide when to reuse interventions.
  - Quick check question: Why might Euclidean distance be a reasonable choice for comparing input encodings in CB2M, and what are its limitations?

- Concept: Model mistake detection
  - Why needed here: CB2M's ability to detect potential errors before human intervention is a key feature that distinguishes it from standard CBMs.
  - Quick check question: How does CB2M's approach to mistake detection differ from using softmax probabilities, and what advantage does it offer for interventions?

## Architecture Onboarding

- Component map:
  CBM backbone (bottleneck + predictor networks) -> Mistake memory (stores encodings of misclassified inputs) -> Intervention memory (stores corresponding concept corrections) -> Similarity comparison module (computes distances between new inputs and stored mistakes) -> Intervention application module (overwrites concept predictions with stored corrections)

- Critical path:
  1. Input → CBM → concept predictions
  2. Compare input encoding to mistake memory
  3. If similar mistake found → retrieve intervention
  4. Apply intervention → recompute output
  5. Return final prediction

- Design tradeoffs:
  - Memory size vs. generalization: Larger memories may improve coverage but risk storing irrelevant interventions.
  - Threshold selection: Higher thresholds increase generalization but may apply incorrect interventions.
  - Similarity metric: Euclidean distance is simple but may not capture semantic similarity in all cases.

- Failure signatures:
  - High false positive rate in mistake detection: Indicates threshold is too low or memory contains too many false mistakes.
  - Low intervention success rate: Suggests similarity metric doesn't capture relevant features or interventions are being generalized inappropriately.
  - Memory bloat without performance improvement: Memory is growing but not providing useful generalizations.

- First 3 experiments:
  1. Test mistake detection on a validation set where ground truth errors are known, measuring AUROC and AUPR.
  2. Test intervention generalization by simulating human corrections on validation data and measuring performance on augmented similar test data.
  3. Evaluate the effect of different distance thresholds on both detection accuracy and intervention success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CB2M's performance change when combined with different CBM architectures (e.g., CEM, post-hoc CBMs, tabular CBMs)?
- Basis in paper: [explicit] The paper mentions that CB2M can be combined with various CBM architectures, but does not provide experimental results comparing performance across different CBM types.
- Why unresolved: The paper only demonstrates CB2M's effectiveness with a specific CBM architecture on limited datasets. The flexibility claim is theoretical without empirical validation across CBM variants.
- What evidence would resolve it: Experimental results comparing CB2M's performance when combined with multiple CBM architectures (CEM, post-hoc CBMs, tabular CBMs) on diverse datasets, showing whether performance gains are consistent or architecture-dependent.

### Open Question 2
- Question: What is the optimal strategy for selecting the distance threshold td when the model performance is already very high with few outliers?
- Basis in paper: [explicit] The paper discusses the importance of selecting td for intervention generalization and mentions that CB2M performs poorly when model mistakes are few individual outliers, but does not provide a solution for this scenario.
- Why unresolved: The paper acknowledges the limitation but only suggests potential future directions like including correctly classified instances in memory or using mistake density, without testing these approaches.
- What evidence would resolve it: Experimental results comparing different threshold selection strategies (density-based, incorporating correctly classified instances, dynamic thresholding) on datasets with varying model performance levels, demonstrating which approach yields best results when mistakes are sparse outliers.

### Open Question 3
- Question: How does CB2M handle malicious or incorrect human interventions, and what are the consequences for the memory module?
- Basis in paper: [explicit] The paper briefly mentions that humans might provide incorrect feedback with malicious intentions and that this "has to be considered," but does not explore how CB2M would handle such scenarios.
- Why unresolved: The paper acknowledges the potential danger of incorrect human feedback but treats it as a future consideration rather than investigating mechanisms to detect or mitigate malicious interventions.
- What evidence would resolve it: Experiments testing CB2M's behavior when exposed to varying proportions of incorrect interventions, including analysis of how these affect memory quality and whether the system can identify and filter out malicious feedback patterns.

### Open Question 4
- Question: What is the computational overhead of maintaining and querying the two-fold memory structure during inference, especially as the memory grows over time?
- Basis in paper: [explicit] The paper describes the two-fold memory structure (mistake memory and intervention memory) but does not provide any analysis of computational costs or scalability concerns.
- Why unresolved: While the paper presents the memory architecture conceptually, it does not address practical implementation concerns regarding memory size, query efficiency, or computational overhead during deployment.
- What evidence would resolve it: Empirical measurements of inference time and memory usage as a function of memory size, comparisons with baseline CBM inference times, and analysis of how memory efficiency scales with the number of stored interventions across different datasets.

## Limitations
- The effectiveness depends on the assumption that similar encodings indicate similar errors, which may not hold under significant concept drift or distribution shifts
- CB2M requires initial human interventions to build the memory, creating a cold-start problem for completely new concepts
- The method's performance on more complex backbone architectures beyond simple MLPs remains untested

## Confidence

- **High confidence**: The two-fold memory architecture is clearly specified and implementable. The experimental results showing substantial accuracy improvements on CUB (concept accuracy 99.0% vs 86.4%, class accuracy 88.7% vs 5.0%) are well-documented and reproducible.
- **Medium confidence**: The generalization mechanism relies on Euclidean distance similarity, which may not capture semantic similarity in all cases. The choice of threshold td and the number of neighbors k are critical hyperparameters without clear optimal values.
- **Low confidence**: The claim that CB2M is "model-agnostic" is not fully validated - the experiments primarily use MLP-based CBMs. The method's performance on more complex backbone architectures remains untested.

## Next Checks

1. **Generalization robustness test**: Evaluate CB2M on a benchmark with known concept drift (e.g., CIFAR-10-C) to assess whether stored interventions remain valid under distribution shifts and corruptions.

2. **Memory efficiency analysis**: Implement memory pruning strategies (prototype selection, temporal decay) and measure the trade-off between memory size and intervention success rate on long-running deployments.

3. **Semantic similarity validation**: Replace Euclidean distance with a learned metric (e.g., contrastive learning embeddings) and compare intervention success rates to isolate the impact of the similarity measure.