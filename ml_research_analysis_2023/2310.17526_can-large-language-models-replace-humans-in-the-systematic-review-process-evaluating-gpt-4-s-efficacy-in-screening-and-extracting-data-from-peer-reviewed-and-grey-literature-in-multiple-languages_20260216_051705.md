---
ver: rpa2
title: Can large language models replace humans in the systematic review process?
  Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed
  and grey literature in multiple languages
arxiv_id: '2310.17526'
source_url: https://arxiv.org/abs/2310.17526
tags:
- gpt-4
- systematic
- studies
- literature
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This pre-registered study evaluated GPT-4's autonomous performance
  in screening and extracting data from peer-reviewed and grey literature across multiple
  languages. The findings suggest that, under specific conditions, GPT-4's performance
  rivals human reviewers, particularly when given highly reliable prompts during full-text
  screening.
---

# Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages

## Quick Facts
- arXiv ID: 2310.17526
- Source URL: https://arxiv.org/abs/2310.17526
- Reference count: 0
- This pre-registered study evaluated GPT-4's autonomous performance in screening and extracting data from peer-reviewed and grey literature across multiple languages, finding that under specific conditions, GPT-4's performance rivals human reviewers.

## Executive Summary
This pre-registered study evaluated GPT-4's autonomous performance in screening and extracting data from peer-reviewed and grey literature across multiple languages. The findings suggest that, under specific conditions, GPT-4's performance rivals human reviewers, particularly when given highly reliable prompts during full-text screening. However, its performance is heavily influenced by chance agreement and dataset imbalance, often leading to underperformance when these factors are considered. The study underscores the importance of prompt engineering and highlights the need for caution when using LLMs for systematic reviews.

## Method Summary
The study used a pre-registered 'human-out-of-the-loop' approach with ChatGPT interface and GPT-4 to evaluate autonomous performance in systematic review tasks. The evaluation included 300 titles/abstracts and 150 full-texts across English peer-reviewed, English grey literature, and non-English documents, with 30 documents for data extraction. Prompt engineering was conducted with separate testing of inclusion/exclusion criteria, and test-retest reliability assessment used 10 studies per criterion. Performance was measured using sensitivity, specificity, accuracy, Cohen's Kappa, PABAK, and weighted Kappa metrics.

## Key Results
- GPT-4's performance in systematic review tasks is highly dependent on prompt reliability and dataset balance
- Under specific conditions with highly reliable prompts, GPT-4's performance rivals human reviewers during full-text screening
- Performance varies significantly across different literature types and languages, with non-English studies showing particularly skewed results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's performance in systematic review tasks is highly dependent on prompt reliability.
- Mechanism: Reliable prompts provide clear, structured guidance that reduces ambiguity and improves consistency in GPT-4's outputs.
- Core assumption: GPT-4 can accurately interpret and execute tasks when given precise and reliable prompts.
- Evidence anchors:
  - [abstract] "The study underscores the importance of prompt engineering and highlights the need for caution when using LLMs for systematic reviews."
  - [section] "GPT-4 performed best when assessing empirical data and refugees (100% reliability) and struggled with the concepts of parenting behaviour (50% reliability) and protracted refugee situations (70% reliability)."
  - [corpus] Weak evidence; related studies focus on LLM performance but not specifically on prompt reliability impact.
- Break condition: If prompts are ambiguous or poorly structured, GPT-4's performance may significantly decline.

### Mechanism 2
- Claim: Dataset imbalance affects the accuracy of GPT-4's performance evaluation.
- Mechanism: Imbalanced datasets can lead to misleading accuracy metrics, as the model may appear to perform well by chance.
- Core assumption: Adjusting for dataset imbalance provides a more accurate assessment of GPT-4's true performance.
- Evidence anchors:
  - [abstract] "The findings suggest that, under specific conditions, GPT-4's performance rivals human reviewers, particularly when given highly reliable prompts during full-text screening. However, its performance is heavily influenced by chance agreement and dataset imbalance..."
  - [section] "We applied four metrics: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN)... Imbalance = TP + FP / TN + FN."
  - [corpus] Weak evidence; no direct mention of dataset imbalance impact in related papers.
- Break condition: If the dataset is balanced, the impact of imbalance on performance evaluation is minimized.

### Mechanism 3
- Claim: GPT-4's ability to process and extract data is influenced by the type of literature and language.
- Mechanism: Training data distribution affects GPT-4's proficiency in handling different types of literature and languages.
- Core assumption: GPT-4's training data includes diverse sources, affecting its performance across various literature types and languages.
- Evidence anchors:
  - [abstract] "This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages..."
  - [section] "Unlike non-English studies (.05), the grey literature was fully balanced at the title/abstract stage, but then both grey literature and non-English studies were skewed towards irrelevance in the full-text screening..."
  - [corpus] Weak evidence; related studies do not specifically address GPT-4's performance across different literature types and languages.
- Break condition: If GPT-4 is trained on more diverse and balanced data, its performance across different literature types and languages may improve.

## Foundational Learning

- Concept: Systematic Review Process
  - Why needed here: Understanding the systematic review process is crucial for evaluating GPT-4's performance in automating such tasks.
  - Quick check question: What are the key stages in a systematic review process, and why is each stage important?

- Concept: Prompt Engineering
  - Why needed here: Effective prompt engineering is essential for optimizing GPT-4's performance in systematic review tasks.
  - Quick check question: How can prompt structure and clarity influence the accuracy of an LLM's outputs?

- Concept: Dataset Imbalance
  - Why needed here: Recognizing dataset imbalance is important for accurately interpreting GPT-4's performance metrics.
  - Quick check question: How does dataset imbalance affect the reliability of performance metrics in machine learning models?

## Architecture Onboarding

- Component map:
  - GPT-4 Interface -> Prompt Engineering Module -> Screening Module -> Data Extraction Module -> Evaluation Metrics

- Critical path:
  1. Develop and test prompts
  2. Conduct screening and data extraction
  3. Evaluate performance using defined metrics
  4. Adjust prompts and repeat as necessary

- Design tradeoffs:
  - Balancing between prompt complexity and GPT-4's ability to process information
  - Ensuring dataset balance to avoid skewed performance metrics
  - Managing computational resources for large-scale text processing

- Failure signatures:
  - Low kappa scores indicating poor agreement between GPT-4 and human reviewers
  - High false rejection rates in weighted kappa scores
  - Inconsistent outputs across multiple prompt tests

- First 3 experiments:
  1. Test GPT-4 with varying prompt complexities to determine optimal structure
  2. Evaluate GPT-4's performance on balanced vs. imbalanced datasets
  3. Compare GPT-4's accuracy across different literature types and languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 vary when applied to systematic reviews in different research domains (e.g., medical, social sciences, engineering)?
- Basis in paper: [inferred] The paper tested GPT-4 on a parenting in protracted refugee situations review, but did not compare performance across different domains.
- Why unresolved: The study focused on a single systematic review topic, limiting generalizability to other domains with different vocabulary, document structures, and inclusion criteria.
- What evidence would resolve it: Conducting similar evaluations of GPT-4 across systematic reviews from diverse research domains would reveal domain-specific strengths and weaknesses.

### Open Question 2
- Question: What specific prompt engineering techniques maximize GPT-4's performance in systematic review tasks?
- Basis in paper: [explicit] The study found prompt reliability strongly influenced GPT-4 performance, but did not systematically test different prompt engineering approaches.
- Why unresolved: The paper used a trial-and-error approach to prompt engineering without testing specific techniques like few-shot learning, chain-of-thought prompting, or prompt templates.
- What evidence would resolve it: Comparative studies testing GPT-4 with systematically varied prompt engineering techniques would identify optimal approaches.

### Open Question 3
- Question: How does GPT-4's performance in systematic reviews compare to other large language models like Claude or PaLM?
- Basis in paper: [explicit] The paper focused exclusively on GPT-4 and did not compare it to other LLMs.
- Why unresolved: The study did not include head-to-head comparisons with other state-of-the-art LLMs that may have different architectures or training data.
- What evidence would resolve it: Direct comparative evaluations of multiple LLMs on the same systematic review tasks would reveal relative strengths and weaknesses.

## Limitations

- GPT-4's performance is heavily influenced by chance agreement and dataset imbalance, which can artificially inflate accuracy metrics
- The study does not provide the specific prompts used, making exact replication difficult
- The evaluation focuses on a limited set of literature types and languages, which may not generalize to all systematic review contexts

## Confidence

- **Medium Confidence:** GPT-4 can perform systematic review tasks with accuracy comparable to human reviewers under specific conditions (e.g., highly reliable prompts and balanced datasets)
- **Low Confidence:** The generalizability of GPT-4's performance to all systematic review contexts, especially for non-English literature and grey literature, is uncertain
- **Medium Confidence:** Prompt engineering is crucial for optimizing GPT-4's performance, but the study does not provide detailed guidance on creating effective prompts

## Next Checks

1. **Prompt Replication Study:** Conduct a replication study using the same dataset and evaluation metrics but with different prompts to assess the impact of prompt engineering on GPT-4's performance

2. **Dataset Balance Analysis:** Perform a sensitivity analysis by creating balanced and imbalanced datasets to evaluate how dataset composition affects GPT-4's performance metrics

3. **Cross-Language and Literature Type Evaluation:** Extend the evaluation to include a wider range of languages and literature types to assess the generalizability of GPT-4's performance across diverse systematic review contexts