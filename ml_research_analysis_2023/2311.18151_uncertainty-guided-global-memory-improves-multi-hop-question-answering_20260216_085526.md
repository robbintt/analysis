---
ver: rpa2
title: Uncertainty Guided Global Memory Improves Multi-Hop Question Answering
arxiv_id: '2311.18151'
source_url: https://arxiv.org/abs/2311.18151
tags:
- memory
- tokens
- entropy
- answer
- supporting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GEMFormer, a method for improving multi-hop
  question answering (MHQA) by using uncertainty-guided global memory. The approach
  collects relevant information from a document into memory and then combines it with
  local context to solve the task.
---

# Uncertainty Guided Global Memory Improves Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2311.18151
- Source URL: https://arxiv.org/abs/2311.18151
- Reference count: 23
- Multi-hop QA with uncertainty-guided memory improves over baselines

## Executive Summary
This paper introduces GEMFormer, a method for improving multi-hop question answering (MHQA) by using uncertainty-guided global memory. The approach collects relevant information from a document into memory based on language model uncertainty (entropy), then combines it with local context to solve the task. The key innovation is using low-entropy tokens - those the model is most certain about - as memory, under the hypothesis that these tokens are more likely to be answer-relevant. The method was evaluated on three MHQA datasets (HotpotQA, 2WikiMultiHopQA, MuSiQue-Ans) and showed consistent improvements over baselines, particularly when using low-entropy tokens for memory after fine-tuning.

## Method Summary
GEMFormer uses a two-stage approach for multi-hop QA on long documents. First, it processes document segments concatenated with the question using a RoBERTa model with a language model head to estimate token-wise entropy. Tokens with low entropy (high certainty) are selected for global memory storage. In the second stage, the model is fine-tuned on the MHQA task using memory-augmented inputs where each segment is processed along with the question and stored memory tokens. The model is trained with a multi-task objective including losses for question type classification, answer span prediction, supporting paragraph identification, and evidence sentence extraction. The method aims to improve performance by providing the model with explicit access to information from supporting facts identified through uncertainty estimation.

## Key Results
- GEMFormer with low-entropy memory consistently outperforms RoBERTa baseline and YAKE! keyword memory across all three MHQA datasets
- Performance improvements are most pronounced on MuSiQue-Ans dataset (7.1% joint F1 improvement)
- Larger memory sizes correlate with better performance when they contain more supporting facts, but too much memory can introduce noise
- The two-stage training approach (frozen LM head during fine-tuning) is critical for the method to work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low entropy tokens from the document are more likely to be relevant to the answer.
- Mechanism: The model uses language model uncertainty (entropy) to select which tokens to store in global memory. Tokens with low entropy are considered more predictable given the question and context, and thus more likely to be answer-relevant.
- Core assumption: The entropy of a token is lower when it is relevant to answering the question than when it is a distractor.
- Evidence anchors:
  - [abstract]: "We also found that the global explicit memory contains information from supporting facts required for the correct answer."
  - [section]: "This observation points to the hypothesis that low entropy tokens might be helpful for generation because they overlap with the context with information relevant to the answer."
  - [corpus]: Weak - the corpus shows related papers but no direct evidence for this specific entropy-based mechanism.
- Break condition: If the question context is too broad or ambiguous, entropy may not effectively distinguish relevant from irrelevant tokens.

### Mechanism 2
- Claim: Fine-tuning the model for the target task reduces uncertainty on answer-relevant tokens.
- Mechanism: During the second stage of training, the model is fine-tuned on the MHQA task using memory-augmented input. This process reduces the uncertainty of tokens that are important for answering the question.
- Core assumption: The fine-tuning process will specifically reduce uncertainty on answer-relevant tokens while increasing uncertainty on distractors.
- Evidence anchors:
  - [abstract]: "Our experimental results show that fine-tuning a pre-trained model with memory-augmented input, including the most certain global elements, improves the model's performance on three MHQA datasets compared to the baseline."
  - [section]: "We hypothesize that a model combining a tuned RoBERTa encoder with a frozen LM head for uncertainty estimation tends to detect parts of a text that are essential for the answer by reducing their uncertainty."
  - [corpus]: Weak - related papers focus on different mechanisms like retrieval or reasoning strategies, not uncertainty reduction through fine-tuning.
- Break condition: If the fine-tuning data is too small or unrepresentative, the model may not effectively learn which tokens are important.

### Mechanism 3
- Claim: Larger memory size with better coverage of supporting facts improves model performance.
- Mechanism: The global memory stores tokens from the document, and models with larger memory sizes that better cover supporting facts show improved performance.
- Core assumption: Having more answer-relevant tokens in memory provides more useful context for the model to reason with.
- Evidence anchors:
  - [abstract]: "Our experimental results show that fine-tuning a pre-trained model with memory-augmented input...improves the model's performance on three MHQA datasets compared to the baseline."
  - [section]: "We analyzed the dev set predictions of GEMFormer Low (H < 0.3) trained on HP with three different random seeds...and found that instances, where the model utilized larger average memory sizes, corresponded to higher joint F1 scores."
  - [corpus]: Weak - related papers don't specifically discuss memory size or coverage as a mechanism for improvement.
- Break condition: If the memory contains too many irrelevant tokens, it may distract the model and reduce performance.

## Foundational Learning

- Concept: Language model uncertainty (entropy)
  - Why needed here: Entropy is used to measure how predictable a token is given the context, which determines whether it should be stored in memory.
  - Quick check question: If a token has high entropy, is it more or less likely to be stored in memory according to GEMFormer's approach?

- Concept: Multi-hop reasoning
  - Why needed here: MHQA requires combining information from multiple parts of a document to answer a question, which is the core problem GEMFormer addresses.
  - Quick check question: In a multi-hop question, how many distinct pieces of information from the document are typically needed to answer correctly?

- Concept: Fine-tuning vs pre-training
  - Why needed here: GEMFormer uses a two-stage approach where the model is first pre-trained and then fine-tuned for the specific MHQA task with memory augmentation.
  - Quick check question: What is the key difference between pre-training and fine-tuning in the context of language models?

## Architecture Onboarding

- Component map:
  RoBERTa base model -> Language model head -> Entropy computation -> Memory selection -> Memory-augmented input processor -> MHQA task heads

- Critical path:
  1. Split document into segments
  2. Process segments with RoBERTa + LM head to get entropy values
  3. Select low entropy tokens for memory based on threshold
  4. Concatenate question + memory + context for each segment
  5. Fine-tune on MHQA task with multi-task objective

- Design tradeoffs:
  - Memory size vs. computational cost: Larger memory provides more context but increases computation and may introduce noise
  - Entropy threshold selection: Too high includes irrelevant tokens, too low may miss important information
  - Fixed vs. dynamic threshold: Fixed is simpler but may not adapt well to different document types

- Failure signatures:
  - Performance degrades when memory is filled with random tokens (as shown in ablation study)
  - Model fails to improve if LM head is fine-tuned jointly with encoder
  - Large memory sizes sometimes correlate with lower answer F1 due to noise

- First 3 experiments:
  1. Test GEMFormer with highest entropy tokens (should show no improvement over baseline)
  2. Test with various fixed entropy thresholds to find optimal ˆθ value
  3. Compare memory-free baseline vs. GEMFormer with optimal memory configuration on dev set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of entropy threshold affect the trade-off between memory size and model performance?
- Basis in paper: [explicit] The paper discusses using a fixed entropy threshold (Low H in Equation 1) and a dynamic threshold (fifth percentile) to select tokens for memory, showing different results for HotpotQA, 2WikiMHQA, and MuSiQue datasets.
- Why unresolved: The paper shows that different entropy thresholds lead to varying levels of performance across datasets, but does not explore the full range of possible thresholds or their impact on memory size and model efficiency.
- What evidence would resolve it: Conducting a comprehensive study varying the entropy threshold across a wide range and analyzing the resulting memory sizes and model performance metrics for each dataset.

### Open Question 2
- Question: Can the memory selection process be improved to reduce the inclusion of irrelevant tokens?
- Basis in paper: [inferred] The paper mentions that the current memory selection process results in storing a significant fraction of irrelevant tokens, which interferes with the calculation of correct predictions.
- Why unresolved: The paper acknowledges the issue of irrelevant tokens in memory but does not propose or test specific methods to improve the relevance of information stored in memory.
- What evidence would resolve it: Developing and testing new methods for memory token selection that prioritize relevance, such as using attention mechanisms or incorporating additional context from the question, and measuring the impact on model performance.

### Open Question 3
- Question: How does the performance of GEMFormer compare to other memory-augmented transformer models on long-document tasks?
- Basis in paper: [explicit] The paper compares GEMFormer to a RoBERTa fine-tuned on the task without memory and a RoBERTa fine-tuned with memory bank of YAKE! keyword tokens, showing improvements in performance.
- Why unresolved: The comparison is limited to specific baselines, and the paper does not explore how GEMFormer stacks up against other memory-augmented transformer models designed for long-document tasks.
- What evidence would resolve it: Conducting experiments comparing GEMFormer to other memory-augmented transformer models, such as those using different memory architectures or retrieval mechanisms, on a variety of long-document tasks.

## Limitations

- The method relies on specific entropy thresholds that may not generalize well across different document types or question styles
- The two-stage training approach adds complexity and computational cost compared to end-to-end fine-tuning methods
- The memory selection process still includes a significant fraction of irrelevant tokens, which can interfere with performance

## Confidence

- **High Confidence**: GEMFormer improves MHQA performance over baselines on all three datasets
- **Medium Confidence**: Low-entropy tokens are more likely to be answer-relevant
- **Low Confidence**: The two-stage training approach is essential for the method's success

## Next Checks

**Validation Check 1**: Perform systematic sensitivity analysis on entropy thresholds across all three datasets. For each dataset, test a range of threshold values (e.g., 0.1 to 0.6 in 0.05 increments) and report performance curves showing how answer F1, evidence F1, and joint F1 vary with threshold. This would quantify the robustness of the method to threshold selection and identify optimal ranges rather than single values.

**Validation Check 2**: Analyze the quality vs. quantity trade-off in memory by measuring the ratio of relevant to irrelevant tokens in memory at different sizes. For each dataset, track how this ratio changes as memory size increases and correlate it with performance metrics. This would help determine whether the performance gains come from better coverage of relevant information or simply from having more context.

**Validation Check 3**: Conduct ablation studies comparing different training strategies: (1) fully joint fine-tuning of both encoder and LM head, (2) progressive unfreezing of LM head during fine-tuning, and (3) the proposed two-stage approach. This would validate whether the two-stage approach is truly essential or if alternative training strategies could achieve similar results.