---
ver: rpa2
title: Amplifying Limitations, Harms and Risks of Large Language Models
arxiv_id: '2307.04821'
source_url: https://arxiv.org/abs/2307.04821
tags:
- technology
- data
- language
- have
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights key limitations of large language models
  (LLMs) like ChatGPT. The authors argue that current LLMs lack true understanding
  of language and meaning, can generate biased or harmful content, and have security/privacy
  risks.
---

# Amplifying Limitations, Harms and Risks of Large Language Models

## Quick Facts
- **arXiv ID**: 2307.04821
- **Source URL**: https://arxiv.org/abs/2307.04821
- **Reference count**: 40
- **Primary result**: Highlights key limitations of large language models (LLMs) like ChatGPT, including lack of true understanding, generation of biased/harmful content, and security/privacy risks

## Executive Summary
This paper critically examines the limitations, harms, and risks associated with large language models (LLMs) such as ChatGPT. The authors argue that current LLMs lack true understanding of language and meaning due to the absence of intent in their training data, can generate biased or harmful content through their generative stochasticity, and pose security and privacy risks. The paper cautions against anthropomorphizing LLMs or overestimating their capabilities, advocating for greater awareness of their limitations. While acknowledging the potential utility of LLMs as productivity tools when used with human oversight, the authors call for more diverse, interdisciplinary teams in AI development, greater transparency, and regulation to address issues like rights violations and environmental impact.

## Method Summary
The paper presents a literature review and analysis of LLM technology and its associated risks. It examines theoretical arguments about LLM limitations, particularly focusing on meaning generation, stochasticity, anthropomorphism, biases, security/privacy issues, and regulatory concerns. The analysis relies on cited academic sources rather than original empirical research or systematic testing of specific LLM implementations.

## Key Results
- LLMs lack true understanding of language and meaning due to training on expression-only data without the joint attention and embodied context present in human communication
- Generative stochasticity causes inconsistent outputs and potential nonsense generation through probabilistic word selection
- Anthropomorphization of LLMs leads to overestimation of their capabilities and potential misuse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs lack true understanding of language due to absence of intent in training data
- Mechanism: LLMs are trained on expression-only data without the joint attention and embodied context that humans use to create meaning
- Core assumption: Human meaning arises from shared intent and embodied experience, not just structural language patterns
- Evidence anchors:
  - [abstract] "current LLMs lack true understanding of language and meaning"
  - [section] "Bender & Koller [12], highlight that the data upon which LLMs are trained does not contain intent that is present in human communication"
  - [corpus] No direct corpus evidence for this specific mechanism; evidence is primarily from cited papers rather than neighbors
- Break condition: If LLMs develop multimodal training with embodied experience or if intent can be captured through alternative means

### Mechanism 2
- Claim: Generative stochasticity causes inconsistent outputs and potential nonsense generation
- Mechanism: LLMs use probabilistic selection among likely next words, leading to different responses for identical prompts and potential generation of nonsensical content
- Core assumption: The stochastic nature of word selection is inherent to current LLM architecture
- Evidence anchors:
  - [abstract] "can generate biased or harmful content"
  - [section] "One of these words is chosen at random. If you have ever used ChatGPT, this explains why if you give the same prompt over and over again, each time the response will be different"
  - [corpus] No direct corpus evidence for this specific mechanism; evidence is primarily from cited papers rather than neighbors
- Break condition: If deterministic generation methods are developed or if stochasticity is removed from the architecture

### Mechanism 3
- Claim: Anthropomorphization of LLMs leads to overestimation of their capabilities and potential misuse
- Mechanism: Human cognitive biases cause people to attribute human-like understanding and intent to LLM outputs, especially when presented through conversational interfaces
- Core assumption: Humans naturally anthropomorphize systems that present human-like interfaces
- Evidence anchors:
  - [abstract] "caution against anthropomorphizing LLMs or overestimating their capabilities"
  - [section] "we anthropomorphise the bot. We may attribute personality traits to it. We may attribute an intended meaning to the generated text"
  - [corpus] No direct corpus evidence for this specific mechanism; evidence is primarily from cited papers rather than neighbors
- Break condition: If interfaces are redesigned to clearly distinguish AI from human interaction or if human cognitive biases are effectively mitigated

## Foundational Learning

- Concept: Difference between correlation and causation in statistical models
  - Why needed here: LLMs are fundamentally statistical models that capture correlations in training data but cannot reason about causation
  - Quick check question: Why can't LLMs distinguish between correlation and causation, and what are the implications for their outputs?

- Concept: Limitations of deep learning as black-box models
  - Why needed here: Understanding why LLMs lack transparency and explainability is crucial for evaluating their risks and limitations
  - Quick check question: What makes deep learning models "black boxes" and why is this problematic for accountability?

- Concept: Training data biases and their propagation through models
  - Why needed here: The quality and characteristics of training data directly impact the behavior and outputs of LLMs
  - Quick check question: How do biases in training data manifest in LLM outputs, and what are the potential consequences?

## Architecture Onboarding

- Component map: Transformer-based neural networks with billions of parameters → Massive text corpora training → Optional fine-tuning layers (RLHF) → Deployment interfaces → User interaction → Feedback incorporation
- Critical path: Data collection → Model training → Fine-tuning → Guardrail implementation → Deployment → User interaction → Feedback incorporation
- Design tradeoffs: Scale vs. interpretability, performance vs. safety, open vs. proprietary development, cost vs. accessibility
- Failure signatures: Hallucinations (factual errors), bias propagation, security/privacy leaks, inconsistent responses, harmful content generation
- First 3 experiments:
  1. Test same prompt multiple times to observe stochastic variation in outputs
  2. Feed known factual information and test whether model can reproduce it accurately
  3. Test prompts designed to trigger biased or harmful responses to understand guardrail effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models be designed to capture intent in human communication?
- Basis in paper: [explicit] The paper states that LLMs lack true understanding of language and meaning because they are trained on data that does not contain intent, which is present in human communication.
- Why unresolved: Capturing intent in language is a complex problem that involves understanding joint attention, embodiment, world models, and multimodal inputs, which current LLMs do not possess.
- What evidence would resolve it: Research demonstrating successful integration of intent-capturing mechanisms in LLMs, such as incorporating multimodal inputs or modeling joint attention and embodiment.

### Open Question 2
- Question: What are the potential unintended consequences of using large language models as part of larger technology solutions or systems?
- Basis in paper: [explicit] The paper mentions that emergent abilities of LLMs can lead to unintended consequences when integrated into larger systems.
- Why unresolved: The limitations and biases of LLMs can propagate and amplify through larger systems, leading to unforeseen negative impacts on individuals and society.
- What evidence would resolve it: Case studies and empirical research on the real-world deployment of LLMs in various systems, documenting both positive and negative outcomes and identifying best practices for mitigating risks.

### Open Question 3
- Question: How can the environmental impact of training and deploying large language models be reduced?
- Basis in paper: [explicit] The paper highlights the unsustainable nature of current LLM technology due to the increasing energy and resource requirements.
- Why unresolved: Developing more energy-efficient LLM architectures and training methods, as well as using renewable energy sources, is an ongoing challenge that requires further research and innovation.
- What evidence would resolve it: Comparative studies on the energy consumption and carbon footprint of different LLM architectures and training methods, as well as successful implementation of sustainable practices in real-world deployments.

## Limitations

- Training data transparency is limited as the specific datasets used for commercial LLMs like ChatGPT are not publicly disclosed
- Analysis generalizes from observations of ChatGPT specifically to all LLMs without clearly distinguishing between different architectures
- Lacks systematic empirical validation of claims, relying instead on theoretical arguments and selective examples

## Confidence

**High Confidence**: Claims about basic LLM architecture and the stochastic nature of generation. These are well-established technical facts about transformer models.

**Medium Confidence**: Claims about anthropomorphization risks and the need for human oversight. While supported by cognitive science research, implications vary by context.

**Low Confidence**: Claims about specific security and privacy vulnerabilities. Many lack concrete evidence or documented cases of exploitation.

## Next Checks

1. **Stochastic Output Consistency Test**: Systematically test whether identical prompts produce meaningfully different outputs across multiple LLM iterations, measuring variance in responses for controlled inputs.

2. **Bias Propagation Analysis**: Design and execute a structured prompt suite that systematically tests for documented biases (gender, racial, cultural) across multiple LLM models, comparing outputs to baseline expectations.

3. **Guardrail Effectiveness Assessment**: Create a comprehensive test suite of potentially harmful prompts covering various categories (hate speech, misinformation, self-harm, etc.) to empirically measure the effectiveness and consistency of current safety guardrails.