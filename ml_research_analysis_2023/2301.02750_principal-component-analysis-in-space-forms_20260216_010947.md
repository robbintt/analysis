---
ver: rpa2
title: Principal Component Analysis in Space Forms
arxiv_id: '2301.02750'
source_url: https://arxiv.org/abs/2301.02750
tags:
- subspace
- space
- hyperbolic
- spherical
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Principal Component Analysis (PCA) in space
  forms, which are complete, simply connected Riemannian manifolds with constant curvature,
  including spherical, Euclidean, and hyperbolic spaces. The main challenge addressed
  is generalizing PCA to non-Euclidean data structures, which is common in applications
  like hierarchical data, microbiome studies, and graph embeddings.
---

# Principal Component Analysis in Space Forms

## Quick Facts
- arXiv ID: 2301.02750
- Source URL: https://arxiv.org/abs/2301.02750
- Reference count: 40
- The paper proposes Principal Component Analysis (PCA) in space forms, complete, simply connected Riemannian manifolds with constant curvature.

## Executive Summary
This paper addresses the challenge of generalizing Principal Component Analysis to non-Euclidean data structures by introducing PCA in space forms - Riemannian manifolds with constant curvature (spherical, Euclidean, and hyperbolic). The authors propose specific cost functions that lead to closed-form solutions where optimal subspaces are determined by eigenvectors of the covariance matrix. The resulting algorithms for spherical and hyperbolic PCA demonstrate superior convergence speed and accuracy compared to existing methods.

## Method Summary
The paper introduces Riemannian affine subspaces defined by a point and tangent vectors on manifolds, with cost functions measuring projection quality. For spherical PCA, optimal subspaces are found via eigenvectors of the covariance matrix, while hyperbolic PCA reduces to solving a JD-eigenequation in Lorentzian space. The algorithms use exponential/logarithmic maps for projection and dimensionality reduction, with isometry Q mapping points to lower-dimensional space forms. Performance is evaluated using normalized output error comparing distances between measurements and true subspace versus denoised data points and true affine subspace.

## Key Results
- Optimal subspaces in spherical and hyperbolic PCA are solutions to eigenequations and form nested sets
- The proposed SFPCA algorithms outperform PGA, SPCA, RFPCA, HoroPCA, and BSA in convergence speed and accuracy
- Isometric mapping to lower-dimensional space forms enables efficient dimensionality reduction while preserving geometry

## Why This Works (Mechanism)

### Mechanism 1
The nested optimality of principal subspaces follows from the specific choice of distortion function sin²(d) in spherical PCA and sinh²(d) in hyperbolic PCA. These distortion functions lead to closed-form solutions where optimal subspaces are determined by eigenvectors of the covariance matrix, and the nesting property holds because the eigenvector decomposition is monotonic in eigenvalue ordering.

### Mechanism 2
The hyperbolic PCA problem reduces to solving a JD-eigenequation in Lorentzian space, which can be efficiently computed using power methods. By reformulating the hyperbolic PCA cost function, the optimal base point and tangent vectors correspond to specific JD-eigenvectors (negative for base point, positive for tangent vectors) of the covariance matrix.

### Mechanism 3
The spherical and hyperbolic affine subspaces are isometric to lower-dimensional space forms of the same curvature, enabling dimensionality reduction while preserving geometry. The isometry Q maps points from the higher-dimensional space form to a lower-dimensional one, preserving distances and allowing for efficient computation of principal components.

## Foundational Learning

- Concept: Riemannian manifolds and space forms
  - Why needed here: The paper extends PCA to data lying on Riemannian manifolds with constant curvature (spherical, Euclidean, hyperbolic)
  - Quick check question: What is the difference between a Riemannian manifold and a Euclidean space?

- Concept: Exponential and logarithmic maps
  - Why needed here: These maps are used to project points onto and from tangent spaces, which is essential for defining affine subspaces and computing distances on manifolds
  - Quick check question: How do the exponential and logarithmic maps relate to geodesics on a manifold?

- Concept: Eigenequations in Lorentzian spaces
  - Why needed here: The hyperbolic PCA problem involves solving a JD-eigenequation, which is a generalization of the standard eigenequation to spaces with indefinite inner products
  - Quick check question: What is the difference between a standard eigenequation and a JD-eigenequation?

## Architecture Onboarding

- Component map: Data → Cost Function → Eigenequation → Dimensionality Reduction
- Critical path: Data → Cost Function → Eigenequation → Dimensionality Reduction
- Design tradeoffs: The choice of distortion function affects both the accuracy of the PCA and the computational complexity of solving the eigenequation
- Failure signatures: If the covariance matrix is not JD-diagonalizable, the hyperbolic PCA may fail to converge. If the distortion function is not proper, the nesting property may not hold.
- First 3 experiments:
  1. Generate synthetic data on a known spherical affine subspace and test the accuracy of the spherical PCA algorithm
  2. Generate synthetic data on a known hyperbolic affine subspace and test the accuracy of the hyperbolic PCA algorithm
  3. Compare the running time and accuracy of the proposed algorithms to existing methods (PGA, SPCA, RFPCA, HoroPCA, BSA) on real-world datasets

## Open Questions the Paper Calls Out

### Open Question 1
Can the choice of distortion function f in Riemannian PCA be extended to other non-constant curvature manifolds beyond spherical and hyperbolic spaces?
- Basis in paper: The paper discusses proper cost functions for spherical and hyperbolic PCA, suggesting the methodology might be applicable to other geodesically complete manifolds
- Why unresolved: The paper focuses specifically on spherical and hyperbolic spaces and does not explore other Riemannian manifolds
- What evidence would resolve it: Testing the proposed methodology on other Riemannian manifolds (e.g., positive definite matrices, Lie groups) and demonstrating its effectiveness would provide evidence for broader applicability

### Open Question 2
How does the computational complexity of SFPCA scale with the dimensionality of the data and the ambient space?
- Basis in paper: The paper compares the running time of SFPCA to other methods, but does not provide a detailed analysis of its computational complexity
- Why unresolved: The paper presents experimental results on running time but does not provide a theoretical analysis of the computational complexity
- What evidence would resolve it: Deriving a theoretical bound on the computational complexity of SFPCA as a function of data dimensionality and ambient space dimension would provide a quantitative understanding of its scalability

### Open Question 3
Can the power method for computing JD-eigenpairs be improved for cases with repeated JD-eigenvalues?
- Basis in paper: The paper mentions that having unique eigenvalues is necessary to use power methods to compute JD-eigenvectors, and conjectures that symmetry might be sufficient even with repeated eigenvalues
- Why unresolved: The paper does not provide a solution for computing JD-eigenpairs when the covariance matrix has repeated eigenvalues
- What evidence would resolve it: Developing an algorithm to compute JD-eigenpairs for matrices with repeated eigenvalues and demonstrating its effectiveness would address this open question

## Limitations

- The nesting property critically depends on the specific choice of distortion functions (sin²(d) for spherical, sinh²(d) for hyperbolic), with limited exploration of alternatives
- The algorithms assume JD-diagonalizability of covariance matrices and unique eigenvalue decomposition, which may not hold for real-world data
- Computational complexity analysis is absent, and the iterative methods for base point estimation may become prohibitive for high-dimensional data

## Confidence

- High Confidence: The theoretical framework connecting Riemannian affine subspaces to eigenequations in space forms is well-established
- Medium Confidence: The proposed algorithms (SFPCA) are theoretically sound, but empirical validation is limited to synthetic data
- Low Confidence: Claims about computational efficiency compared to existing methods lack rigorous benchmarking

## Next Checks

1. Apply the proposed algorithms to benchmark datasets from hierarchical clustering, microbiome studies, and graph embedding tasks, comparing performance metrics against existing methods

2. Test algorithm performance under various covariance matrix conditions (non-JD-diagonalizable, repeated eigenvalues, near-singular matrices) and quantify sensitivity to initialization and parameter choices

3. Implement and benchmark all algorithms (SFPCA, PGA, SPCA, RFPCA, HoroPCA, BSA) on datasets with varying dimensions (N, K, D), measuring running time, memory usage, and convergence behavior to establish computational complexity empirically