---
ver: rpa2
title: Recognition of Mental Adjectives in An Efficient and Automatic Style
arxiv_id: '2307.11767'
source_url: https://arxiv.org/abs/2307.11767
tags:
- mental
- physical
- words
- word
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new lexical inference task called Mental
  and Physical Classification (MPC) to handle commonsense reasoning. The task aims
  to classify adjectives as either Mental (relating to mental activities like emotion,
  need, reasoning, etc.) or Physical (describing physical attributes of objects).
---

# Recognition of Mental Adjectives in An Efficient and Automatic Style

## Quick Facts
- arXiv ID: 2307.11767
- Source URL: https://arxiv.org/abs/2307.11767
- Reference count: 40
- Primary result: BERT model with entropy-based active learning achieves 0.72 Mental F1 and 0.87 Physical F1 on mental/physical adjective classification using only ~300 labeled words

## Executive Summary
This paper introduces Mental and Physical Classification (MPC), a novel lexical inference task for classifying adjectives as either Mental (relating to mental activities like emotion, need, reasoning) or Physical (describing physical attributes of objects). The authors propose using BERT fine-tuning with active learning algorithms to reduce annotation costs. Their approach achieves strong performance with minimal labeled data, demonstrating that active learning can efficiently select informative examples for fine-tuning. The work also reveals that many adjectives in existing semantic resources like SentiWordNet have mental functionalities under the MPC definition, highlighting the task's relevance for commonsense reasoning.

## Method Summary
The method involves extracting adjectives from Amazon Fine Food Reviews, validating them against WordNet, and then using active learning strategies (ENTROPY, CORESET, CAL, Random) to iteratively select examples for human annotation. BERT-base-uncased is fine-tuned on the labeled data using standard hyperparameters (learning rate 2e-5, batch size 32, dropout 0.3). The active learning framework aims to maximize classification performance while minimizing the number of required annotations by selecting high-entropy examples that provide the most information gain for the model.

## Key Results
- BERT with ENTROPY strategy achieves 0.72 Mental F1 and 0.87 Physical F1 on test set
- Only ~300 labeled words required for satisfactory accuracy
- Entropy-based active learning outperforms CORESET, CAL, and Random strategies
- Many SentiWordNet adjectives show mental functionalities under MPC definition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning reduces annotation cost by selecting high-entropy examples
- Mechanism: ENTROPY strategy selects words whose predicted probability is closest to 0.5, maximizing uncertainty and information gain
- Core assumption: High-entropy examples provide more useful signal for fine-tuning than random selection
- Evidence anchors:
  - [abstract] "The model using ENTROPY strategy achieves satisfactory accuracy and requires only about 300 labeled words"
  - [section 4] "ENTROPY. Samples with the highest predicted entropy are selected [13]. For binary classification, the closer the prediction probability of a sample is to 0.5, the higher its entropy is."
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: If the model predictions are highly confident on all remaining samples, entropy selection provides no benefit

### Mechanism 2
- Claim: BERT's pre-training provides semantic representations that transfer well to mental/physical classification
- Mechanism: BERT's contextualized embeddings capture the abstract vs concrete distinction between mental and physical adjectives
- Core assumption: The distributional hypothesis holds - words used in similar contexts have similar meanings
- Evidence anchors:
  - [section 1] "BERT is pre-trained on the BooksCorpus (800M words) and English Wikipedia (2,500M words). By pre-training on such large text data, BERT grasps rich semantic information."
  - [section 4] "BERT fine-tuning and inference procedure is shown in Figure 2"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: If mental and physical adjectives appear in similar contexts, BERT cannot distinguish them

### Mechanism 3
- Claim: WordNet definitions provide sufficient signal for mental/physical classification
- Mechanism: Aggregated WordNet glosses serve as input features that capture the abstract/concrete distinction
- Core assumption: WordNet definitions accurately represent the mental or physical nature of adjectives
- Evidence anchors:
  - [section 3] "WordNet maps words into sets of cognitive synonyms, each expressing a distinct concept, therefore more than one piece of definition text are provided by WordNet for a given word."
  - [section 3] "For example, 'shining' belongs to three clusters as an adjective with three different definitions"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: If WordNet definitions are ambiguous or don't capture the mental/physical distinction, classification fails

## Foundational Learning

- Concept: Active learning and uncertainty sampling
  - Why needed here: The task requires efficient use of limited annotation resources
  - Quick check question: What is the difference between entropy-based and uncertainty-based active learning?

- Concept: BERT fine-tuning and transfer learning
  - Why needed here: The model needs to adapt pre-trained language representations to the specific mental/physical classification task
  - Quick check question: How does BERT's masked language modeling objective relate to its ability to capture semantic relationships?

- Concept: Semantic textual similarity and word embeddings
  - Why needed here: CORESET strategy uses FastText embeddings to measure semantic distance between words
  - Quick check question: What is the relationship between cosine similarity and semantic similarity in word embedding space?

## Architecture Onboarding

- Component map: Unlabeled word pool → Active learning strategy → Human annotation → Labeled pool → BERT fine-tuning → Test evaluation → Pipeline deployment
- Critical path: Active learning selection → Annotation → Fine-tuning → Evaluation
- Design tradeoffs: Active learning vs random sampling (efficiency vs simplicity), BERT vs smaller models (accuracy vs speed), gloss-level vs sense-level classification (simplicity vs precision)
- Failure signatures: High disagreement rate among annotators, low entropy scores across all remaining samples, BERT predictions plateauing
- First 3 experiments:
  1. Run ENTROPY strategy with different batch sizes to find optimal annotation efficiency
  2. Compare BERT-base vs BERT-large to assess if larger model improves classification
  3. Test different threshold values for binary classification (currently 0.5) to optimize F1 scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using gloss-level classification (rather than lexical-level) on the performance of the Mental and Physical Classification (MPC) task?
- Basis in paper: [inferred] The paper mentions that the best granularity for MPC is gloss level rather than lexical level, but lexical level is used for development simplicity.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of using gloss-level classification instead of lexical-level classification.
- What evidence would resolve it: Experimental results comparing the performance of the MPC task using gloss-level and lexical-level classification.

### Open Question 2
- Question: How does the performance of the BERT model for MPC task change with the size of the training dataset?
- Basis in paper: [inferred] The paper mentions that the model using the ENTROPY strategy requires only around 300 labeled words to achieve satisfactory accuracy, but it does not explore the impact of varying the size of the training dataset.
- Why unresolved: The paper does not provide any analysis or experimental results on how the performance of the BERT model changes with different sizes of the training dataset.
- What evidence would resolve it: Experimental results showing the performance of the BERT model for MPC task with different sizes of the training dataset.

### Open Question 3
- Question: How does the active learning strategy affect the performance of the BERT model for MPC task?
- Basis in paper: [explicit] The paper compares four active learning strategies (ENTROPY, CORESET, CAL, and Random) and finds that the ENTROPY strategy outperforms the others.
- Why unresolved: The paper does not provide a detailed analysis of how each active learning strategy affects the performance of the BERT model, such as the impact on precision, recall, or F1 scores.
- What evidence would resolve it: A detailed analysis of the impact of each active learning strategy on the performance of the BERT model, including precision, recall, and F1 scores.

## Limitations
- Domain specificity: Evaluation conducted only on Amazon Fine Food Reviews dataset, limiting generalizability to other domains
- Annotation quality concerns: Inter-annotator agreement of only 77.8% suggests substantial ambiguity in task definition
- Active learning efficiency: No baseline comparison to quantify actual efficiency gains versus random sampling

## Confidence
- **High confidence**: BERT fine-tuning methodology and active learning framework implementation are technically sound
- **Medium confidence**: Reported F1 scores are likely accurate for the specific dataset but may not generalize well
- **Low confidence**: Claims about task difficulty and novelty compared to existing semantic resources are not well-supported by empirical evidence

## Next Checks
1. Cross-domain evaluation: Test the trained model on different text domains (news, academic writing, social media) to assess generalizability
2. Baseline comparison: Implement a random sampling baseline for active learning to quantify actual efficiency gains
3. Annotation refinement study: Conduct follow-up annotation study focusing on most ambiguous cases to refine task definition and improve inter-annotator reliability