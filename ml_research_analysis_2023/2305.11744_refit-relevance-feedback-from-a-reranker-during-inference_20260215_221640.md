---
ver: rpa2
title: 'ReFIT: Relevance Feedback from a Reranker during Inference'
arxiv_id: '2305.11744'
source_url: https://arxiv.org/abs/2305.11744
tags:
- retrieval
- query
- re-ranker
- recall
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel method to leverage a cross-encoder re-ranker
  to improve the recall of a dual-encoder retriever at inference time. Given a query,
  the re-ranker's scores over retrieved candidates are used to update the retriever's
  query representation via a lightweight distillation step.
---

# ReFIT: Relevance Feedback from a Reranker during Inference

## Quick Facts
- arXiv ID: 2305.11744
- Source URL: https://arxiv.org/abs/2305.11744
- Reference count: 18
- Key outcome: This work proposes a novel method to leverage a cross-encoder re-ranker to improve the recall of a dual-encoder retriever at inference time. Given a query, the re-ranker's scores over retrieved candidates are used to update the retriever's query representation via a lightweight distillation step. This updated query is then used for a second retrieval, yielding better recall. Experiments show significant gains in recall across multiple datasets, languages, and modalities, with minimal additional latency compared to re-ranking a larger set of candidates.

## Executive Summary
This paper introduces ReFIT, a novel method that leverages cross-encoder re-rankers to improve the recall of dual-encoder retrievers at inference time. The key insight is that while cross-encoders generally provide better ranking quality through full query-passage interaction, they are computationally expensive and limited to seeing only top-k candidates. ReFIT addresses this by using the re-ranker's scores to update the retriever's query representation via KL-divergence minimization, enabling a second retrieval that achieves better recall with minimal additional latency.

The approach works by performing an initial retrieval, re-ranking the candidates with a cross-encoder, then updating the query vector through gradient descent on a distillation loss that aligns the retriever's scoring distribution with the re-ranker's. This updated query is used for a second retrieval, yielding significant recall improvements across multiple datasets, languages, and even cross-modal scenarios, all while maintaining or improving ranking performance.

## Method Summary
The method takes a dual-encoder retriever and cross-encoder re-ranker as inputs, both pre-trained and frozen. For each query, it performs an initial retrieval to get top-k candidates, then re-ranks these with the cross-encoder. The query vector is updated through n iterations of gradient descent on a KL-divergence loss that aligns the retriever's scoring distribution with the re-ranker's. This updated query is then used for a second retrieval. The approach is evaluated on recall@K metrics across multiple datasets, showing significant improvements while maintaining ranking quality and adding minimal latency compared to re-ranking more candidates.

## Key Results
- Significant recall@100 improvements across multiple datasets (BEIR, NQ, TriviaQA, FiQA, CQADupStack) with gains of 3-9% absolute
- Cross-lingual effectiveness demonstrated with 3-4% absolute recall improvements on non-English BEIR languages
- Cross-modal effectiveness shown with 1-3% absolute recall improvements on multi-modal BEIR datasets
- Minimal additional latency (1-2x initial retrieval time) compared to re-ranking larger candidate sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The re-ranker's scoring distribution captures richer relevance information than the retriever's dot-product similarity
- Mechanism: The cross-encoder re-ranker uses full query-passage interaction via cross-attention, allowing it to capture nuanced semantic relationships that the dual-encoder's independent encoding misses
- Core assumption: The re-ranker consistently outperforms the retriever in ranking quality across the candidate set
- Evidence anchors:
  - [abstract] "The re-ranker generally produces better candidate scores than the retriever, but is limited to seeing only the top K retrieved candidates"
  - [section 2.1] "The cross-encoder typically provides better ranking than the dual-encoder—thanks to its more elaborate computation of query-passage similarity informed by cross-attention"
  - [corpus] Found related work on re-ranking improvements, suggesting this is an active research area
- Break condition: If the re-ranker performs worse than or equal to the retriever on a given instance, the distillation would degrade rather than improve the query representation

### Mechanism 2
- Claim: KL-divergence minimization aligns the retriever's scoring distribution with the re-ranker's distribution for the specific test instance
- Mechanism: By minimizing the KL-divergence between DCE(q,P) and DQq(ˆP), the query vector is updated to produce scores that match the re-ranker's ranking for that specific query
- Core assumption: The re-ranker's score distribution provides a reliable signal for which passages should be ranked higher
- Evidence anchors:
  - [section 2.2] "The distillation loss is designed to bring the retriever's candidate scores closer to those of the re-ranker"
  - [section 3.3.1] "the new query vector is also closer to the initially retrieved positives by 5-16%"
  - [corpus] Related work on knowledge distillation suggests this approach can transfer model knowledge effectively
- Break condition: If the re-ranker's distribution is too peaky or unreliable (e.g., many ties or random scores), the distillation may produce unstable query updates

### Mechanism 3
- Claim: Updating only the query vector (not model parameters) enables fast, instance-specific adaptation without retraining
- Mechanism: Gradient descent updates on the query representation Qq allow the retriever to "learn" the re-ranker's preferences for the current instance, while keeping the retriever model frozen
- Core assumption: The query representation space is sufficiently rich to capture the re-ranker's preferences through linear adjustment
- Evidence anchors:
  - [section 2.2] "we update the retriever's corresponding query vector by minimizing a distillation loss"
  - [section 3.1] "The proposed distillation step is fast, considerably increasing recall without any loss in ranking performance"
  - [corpus] Limited evidence in corpus, but this approach aligns with inference-time adaptation literature
- Break condition: If the query representation space is too constrained or the adjustment needed is non-linear, simple gradient updates may be insufficient

## Foundational Learning

- Concept: KL-divergence as a measure of distribution alignment
  - Why needed here: The method uses KL-divergence to measure how well the retriever's scoring distribution matches the re-ranker's
  - Quick check question: What does it mean if KL(DCE||DQq) is high vs low for a given query?

- Concept: Dual-encoder vs cross-encoder architectures
  - Why needed here: Understanding the architectural difference is crucial to grasping why the re-ranker can provide better feedback
  - Quick check question: How does the computational complexity of cross-encoder re-ranking scale with the number of candidates?

- Concept: Temperature scaling in softmax
  - Why needed here: The method applies temperature scaling to the re-ranker's distribution to handle peaky scores
  - Quick check question: What effect does increasing temperature have on the softmax output distribution?

## Architecture Onboarding

- Component map:
  - Retriever (Contriever dual-encoder) -> Re-ranker (cross-encoder) -> Distillation module -> Index (pre-computed passage representations)

- Critical path:
  1. Initial retrieval with frozen retriever
  2. Re-ranking with frozen re-ranker
  3. Distillation to update query vector
  4. Second retrieval with updated query

- Design tradeoffs:
  - Query update vs model fine-tuning: Updating query vectors is faster but may be less powerful than fine-tuning retriever parameters
  - Number of distillation updates (n): More updates improve quality but increase latency
  - Temperature parameter: Affects how peaky the re-ranker distribution is during distillation

- Failure signatures:
  - If distillation increases latency too much: Check if n can be reduced without significant performance loss
  - If recall doesn't improve: Verify re-ranker is actually better than retriever on your data
  - If training instability: Check if temperature scaling is appropriate for your re-ranker's score distribution

- First 3 experiments:
  1. Compare recall@100 with and without distillation on a small validation set to verify the basic mechanism works
  2. Vary the number of distillation updates (n) to find the latency-performance tradeoff point
  3. Test with different re-ranker models to verify the approach is architecture-agnostic as claimed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of re-ranker model architecture affect the quality and efficiency of the re-ranker relevance feedback distillation process?
- Basis in paper: [explicit] The paper discusses that their approach is independent of the choice of architecture for the bi-encoder and cross-encoder, and can be used for neural retrieval in any domain, language or modality. It also mentions that using a more powerful re-ranker could potentially improve results, but this was not directly tested.
- Why unresolved: The paper does not empirically compare the effects of using different re-ranker architectures (e.g., smaller vs larger models, different pre-training objectives) on the quality of the distilled query vectors and the overall retrieval performance. It only speculates that a stronger re-ranker would likely improve results.
- What evidence would resolve it: Experiments comparing the performance of the re-ranker feedback approach using different re-ranker architectures (e.g., different sizes, pre-training objectives) on the same retrieval tasks, measuring both recall and ranking performance as well as computational efficiency.

### Open Question 2
- Question: Can the re-ranker relevance feedback distillation process be extended to update token-level representations within the query vector, rather than just the overall query representation?
- Basis in paper: [inferred] The paper mentions that future work will explore relevance feedback for token-level query representations, indicating that this is a potential direction but has not been investigated yet.
- Why unresolved: The current approach updates the entire query vector as a single representation, without considering the importance or relevance of individual tokens within the query. This could potentially limit the effectiveness of the feedback process, especially for longer or more complex queries.
- What evidence would resolve it: Experiments comparing the performance of the current approach (updating the overall query vector) with an approach that updates token-level representations within the query, on the same retrieval tasks. The results would show whether token-level updates lead to better recall and ranking performance.

### Open Question 3
- Question: How does the number of distillation update iterations (n) affect the trade-off between computational efficiency and retrieval performance in the re-ranker relevance feedback process?
- Basis in paper: [explicit] The paper discusses the effect of varying the number of distillation updates (n) on both the latency and performance of the approach, showing that increasing n leads to better performance but also higher latency.
- Why unresolved: While the paper provides some insights into how n affects performance and latency, it does not provide a comprehensive analysis of the trade-off between these two factors. It also does not explore whether there are optimal values of n for different retrieval tasks or datasets.
- What evidence would resolve it: A detailed analysis of the relationship between n, performance, and latency across different retrieval tasks and datasets, identifying optimal values of n for each scenario. This could involve experiments varying n and measuring both performance and latency, as well as investigating the reasons behind the observed trade-offs.

## Limitations

- The method's effectiveness critically depends on the re-ranker providing better relevance signals than the retriever, which may not hold for all query types or domains
- Computational overhead scales linearly with the number of distillation updates, and the latency-recall tradeoff has not been fully characterized across different hardware configurations
- The approach assumes that updating only the query vector is sufficient to capture the re-ranker's preferences, which may not hold for complex query-passage relationships requiring non-linear adjustments

## Confidence

- **High Confidence**: The claim that cross-encoders generally outperform dual-encoders in ranking quality is well-established in the literature and supported by the paper's experimental results across multiple datasets and languages
- **Medium Confidence**: The claim that KL-divergence minimization effectively transfers the re-ranker's knowledge to the retriever is supported by empirical results but could benefit from additional theoretical analysis of why this particular alignment mechanism works well
- **Medium Confidence**: The claim about minimal latency overhead is supported by runtime measurements but lacks comparison with alternative recall improvement strategies and does not explore the full latency-recall tradeoff space

## Next Checks

1. **Re-ranker Quality Validation**: Systematically evaluate the re-ranker's performance across different query types and candidate sets to verify that it consistently provides better ranking signals than the retriever. This would validate the core assumption that the re-ranker's distribution is a reliable source for distillation.

2. **Cross-Architecture Generalization**: Test the ReFIT method with re-rankers that have different architectures or training objectives (e.g., cross-encoders trained with different loss functions) to verify that the approach is truly architecture-agnostic as claimed, rather than being specific to certain re-ranker designs.

3. **Latency-Recall Tradeoff Analysis**: Conduct a comprehensive study varying the number of distillation updates (n) and measuring both recall improvements and latency costs across different hardware configurations. This would provide clearer guidance on the practical deployment of the method and identify optimal parameter settings for different use cases.