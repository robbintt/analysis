---
ver: rpa2
title: 'ARN: Analogical Reasoning on Narratives'
arxiv_id: '2310.00996'
source_url: https://arxiv.org/abs/2310.00996
tags:
- mappings
- narratives
- analogies
- narrative
- analogical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Analogical reasoning is a key cognitive skill that enables humans
  to transfer information across domains, but there is a gap between how it is studied
  in cognitive psychology (using narratives and system mappings) and in NLP (focusing
  on word-level analogies). To bridge this, the authors propose ARN, a computational
  framework for matching narratives based on system mappings, operationalizing cognitive
  psychology theories.
---

# ARN: Analogical Reasoning on Narratives

## Quick Facts
- arXiv ID: 2310.00996
- Source URL: https://arxiv.org/abs/2310.00996
- Reference count: 16
- Key outcome: LLMs perform well on near analogies but poorly on far analogies in zero-shot settings, highlighting challenges in recognizing system mappings.

## Executive Summary
Analogical reasoning enables humans to transfer information across domains, but current NLP benchmarks focus on word-level analogies rather than narrative-level reasoning. This paper introduces ARN, a computational framework for matching narratives based on system mappings, operationalizing cognitive psychology theories. The framework extracts narrative components and uses them to form attribute, relational, and system mappings. Evaluating LLMs on ARN shows that while models can recognize near analogies well, they struggle with far analogies, with GPT-4.0 performing below random. Providing solved examples or chain-of-thought reasoning improves performance, but even then, the best model performs only halfway between random and human baselines.

## Method Summary
The ARN framework extracts narrative components (agents, locations, actions, goals, high-level messages) using LLMs and uses them to form attribute, relational, and system mappings. System mappings are based on high-level messages (proverbs) and serve as higher-order constraints. The framework constructs a dataset of 1096 narrative triples (query, analogy, distractor) covering near/far analogies and distractors. LLMs are evaluated in zero-shot and few-shot settings to identify system mappings between narratives, with performance compared to human baselines and random performance.

## Key Results
- LLMs perform well on near analogies but poorly on far analogies in zero-shot settings.
- GPT-4.0 performs below random on far analogies in zero-shot settings.
- Providing solved examples or chain-of-thought reasoning improves model performance, but the best model still performs only halfway between random and human baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: System mappings serve as higher-order constraints that override lower-order attribute/relational similarities during analogy detection.
- Mechanism: By forcing models to map narratives based on high-level messages (proverbs), the framework creates a system-level abstraction that should ideally dominate literal and relational similarities.
- Core assumption: The high-level message abstraction is both meaningful and computationally accessible to models when explicitly prompted.
- Evidence anchors: [abstract] "by introducing the task of matching narratives according to their high-level messages in a binary classification setting"
- Break condition: If lower-order mappings are too salient or the system mapping abstraction is too vague, models will default to literal similarities.

### Mechanism 2
- Claim: Narrative component extraction via LLM provides sufficient semantic granularity to construct diverse, challenging mappings.
- Mechanism: Using an LLM to extract agents, locations, relations, actions, and goals from narratives allows for systematic construction of attribute, relational, and system mappings at multiple abstraction levels.
- Core assumption: GPT-3.5 can extract these components with enough fidelity that they form meaningful bases for mapping generation.
- Evidence anchors: [section 3.1] "we opt to utilize an LLM for this task to save both time and money and based on the performance of GPT3.5 that has been deemed promising and comparable to crowd-workers"
- Break condition: If extraction errors are systematic or semantic ambiguities are not resolved, constructed mappings will be misleading or too noisy.

### Mechanism 3
- Claim: Near vs. far analogy categorization effectively creates controlled difficulty levels for model evaluation.
- Mechanism: By varying the semantic distance between query and analogy narratives, the framework creates two distinct settings: near (with supporting lower-order mappings) and far (without them), isolating the effect of system mapping complexity.
- Core assumption: Semantic distance correlates meaningfully with cognitive load and mapping difficulty.
- Evidence anchors: [section 3.5] "far analogies are analogies that include system mappings without lower-order mappings and are in different semantic domains, while near analogies are analogies that include system mappings alongside lower-order mappings and are in the same semantic domains"
- Break condition: If models can bypass semantic distance via other heuristics (e.g., word overlap), the difficulty gradient may not be meaningful.

## Foundational Learning

- Concept: System mapping vs. surface similarity distinction
  - Why needed here: Models must differentiate between literal/shared surface features and deeper relational structures; this is core to analogical reasoning.
  - Quick check question: Given two narratives about training, one for a marathon and one for a guitar competition, can you articulate why they are analogous without relying on shared words like "training"?

- Concept: Narrative component semantics (agents, locations, actions, goals, relations)
  - Why needed here: These components are the building blocks for creating controlled mappings; understanding their abstraction levels is essential for framework comprehension.
  - Quick check question: Which component is most abstract: "Sarah" (agent), "kitchen" (location), or "winning the competition" (goal)?

- Concept: Chain-of-thought prompting and in-context learning
  - Why needed here: Few-shot experiments use these methods to improve model reasoning; knowing their effect is key for interpreting results.
  - Quick check question: How does adding intermediate reasoning steps in the prompt potentially change the model's output?

## Architecture Onboarding

- Component map: Narrative -> Component Extraction -> Similarity Computation -> Mapping Classification -> Triple Construction -> LLM Evaluation
- Critical path: Narrative → Component Extraction → Similarity Computation → Mapping Classification → Triple Construction → LLM Evaluation. Any failure in extraction or similarity scoring corrupts the entire pipeline.
- Design tradeoffs: Using LLM for extraction trades accuracy for scalability; choosing semantic similarity thresholds balances precision vs. coverage; using proverbs as system mapping proxies trades depth for explicitness.
- Failure signatures: High error in similarity scoring will produce spurious mappings; weak high-level message alignment will break system mapping validity; corpus imbalance in far/near categories will bias results.
- First 3 experiments:
  1. Validate extraction accuracy by sampling 20 narratives and comparing LLM output to manual annotation.
  2. Run a similarity threshold sweep to find optimal cutoffs for meaningful attribute/relational mappings.
  3. Generate a small balanced test set (10 triples) and run zero-shot GPT-3.5 evaluation to verify task solvability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the narrative components (agents, locations, actions, goals) differentially impact the effectiveness of system mappings versus lower-order mappings in analogical reasoning?
- Basis in paper: [explicit] The paper discusses extracting these components and their role in forming attribute, relational, and system mappings.
- Why unresolved: While the paper mentions that locations and actions are particularly distracting for models, it doesn't provide a detailed analysis of how each component type affects the balance between system and lower-order mappings.
- What evidence would resolve it: Controlled experiments isolating the impact of each narrative component on model performance for both system and lower-order mappings would provide clarity.

### Open Question 2
- Question: Can neuro-symbolic approaches or other machine learning methods outperform large language models on the Analogical Reasoning on Narratives (ARN) dataset?
- Basis in paper: [inferred] The paper mentions evaluating only large language models and suggests that other models could be assessed in the future.
- Why unresolved: The paper focuses on LLMs and doesn't explore alternative computational approaches.
- What evidence would resolve it: Benchmarking various machine learning models, including neuro-symbolic approaches, on the ARN dataset and comparing their performance to LLMs.

### Open Question 3
- Question: How does the flexibility of implicit versus explicit knowledge representation in large language models affect their ability to transfer analogical reasoning skills across different domains?
- Basis in paper: [inferred] The paper discusses the effect of solved examples as hints and observes that implicit knowledge of far analogies helps in finding near analogies but not in far analogies.
- Why unresolved: The paper hints at this distinction but doesn't provide a detailed analysis of the mechanisms behind it.
- What evidence would resolve it: Experiments comparing the performance of models trained with different levels of explicit versus implicit knowledge representation on analogical reasoning tasks across various domains.

## Limitations

- The framework's reliance on LLM-extracted narrative components introduces potential noise, but the paper does not provide quantitative error analysis of this extraction step.
- The use of proverbs as proxies for high-level messages in system mappings is a pragmatic simplification, but proverbs may not always capture the full conceptual abstraction needed for robust system mapping.
- The corpus construction method is transparent in process but not in validation; the paper does not report inter-annotator agreement or systematic evaluation of whether generated analogies and distractors are correctly classified by semantic distance.

## Confidence

- High confidence: LLMs struggle with far analogies in zero-shot settings. The performance gap between near and far analogies is substantial and consistent across models.
- Medium confidence: System mappings can override lower-order mappings. While the framework is designed to test this, the paper does not directly analyze model attention or decision-making to confirm that models are actually using system mappings as intended.
- Low confidence: Completeness of the narrative component extraction process. The paper asserts comparability to crowd-workers but provides no direct validation or error analysis.

## Next Checks

1. Conduct a blind human evaluation of a stratified sample of ARN triples to verify that generated analogies and distractors are correctly classified by semantic distance and that system mappings are genuinely higher-order.
2. Implement a diagnostic evaluation where models are tested on modified distractors with specific components (e.g., agents, locations) removed, to quantify the distracting effect of each component type and validate the framework's claims about mapping interference.
3. Perform an ablation study on the narrative component extraction step: manually annotate a subset of narratives and compare extracted components to ground truth, measuring the impact of extraction errors on downstream mapping quality and model performance.