---
ver: rpa2
title: Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques
arxiv_id: '2308.03171'
source_url: https://arxiv.org/abs/2308.03171
tags:
- anomaly
- feature
- detection
- bagging
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles anomaly detection in multivariate time series
  by introducing ensemble techniques to boost the performance of existing models.
  The core approach combines Feature Bagging, which uses subsets of features, with
  a transformation based on Nested Rotations derived from PCA to increase diversity
  and model effectiveness.
---

# Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques

## Quick Facts
- arXiv ID: 2308.03171
- Source URL: https://arxiv.org/abs/2308.03171
- Reference count: 25
- Primary result: Ensemble techniques combining Feature Bagging and Nested Rotations improve anomaly detection accuracy by at least 10% in semi-supervised setup compared to individual models

## Executive Summary
This study introduces ensemble techniques to improve anomaly detection in multivariate time series by combining Feature Bagging with PCA-based Nested Rotations. The approach is applied to five deep learning architectures in both unsupervised and semi-supervised configurations. Experimental results on the SKAB dataset demonstrate significant performance improvements, with the semi-supervised ensemble achieving at least 10% higher accuracy than baseline methods.

## Method Summary
The method combines Feature Bagging (randomly selecting feature subsets) with Nested Rotations (PCA-based transformations of partitioned feature subsets) to create diverse base models. These transformations are applied to five deep learning architectures (Autoencoder, LSTM, LSTM Autoencoder, LSTM Variational Autoencoder, and Convolutional Autoencoder). Predictions from individual models are aggregated using either majority voting (unsupervised) or logistic regression (semi-supervised) to produce final anomaly scores.

## Key Results
- Semi-supervised ensemble with logistic regression outperforms baseline methods by at least 10% in accuracy
- Unsupervised ensemble improves performance by approximately 2% compared to individual models
- The combination of Feature Bagging and Nested Rotations provides consistent improvements across all tested architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Bagging improves anomaly detection by focusing on subsets of features where anomalies are more likely to be visible, reducing the curse of dimensionality.
- Mechanism: Randomly selects subsets of features for training individual models; anomalies that are masked in high-dimensional space become detectable in lower-dimensional subspaces.
- Core assumption: Anomalies in multivariate time series are not uniformly distributed across all features but are more prominent in specific feature subsets.
- Evidence anchors: [abstract] "the anomaly can arise from a small subset of the feature set"; [section] "it is pointless to detect point anomalies based on the similarity (or distance) in high dimensions since the respective metrics lose their meaning when the number of dimensions increases"
- Break condition: If anomalies are uniformly distributed across all features, Feature Bagging would provide no advantage and might even hurt performance by discarding informative features.

### Mechanism 2
- Claim: Nested Rotations (PCA-based transformations) increase model diversity and improve anomaly detection by creating orthogonal feature representations.
- Mechanism: Applies PCA to partitions of feature subsets, rotates the data into principal component space, and trains models on these transformed representations to capture variance and inject diversity.
- Core assumption: PCA can reveal hidden patterns in data that are not apparent in the original feature space, and rotating different partitions independently increases ensemble diversity.
- Evidence anchors: [section] "a transformation based on nested rotation computed from Principal Component Analysis (PCA) to improve the effectiveness and generalization"; [section] "apply a PCA on the randomly selected subsets of features to get a 'rotation matrix' (the principal components)"
- Break condition: If the principal components do not capture meaningful variance or if the data is already well-structured in the original space, the rotation may add unnecessary complexity without benefit.

### Mechanism 3
- Claim: Combining multiple diverse base models through ensemble techniques (majority voting or logistic regression) leads to more robust and accurate anomaly detection than any single model.
- Mechanism: Trains multiple models with different architectures and feature subsets, then aggregates their predictions to leverage complementary strengths and reduce individual model weaknesses.
- Core assumption: Different deep learning architectures capture different aspects of temporal and spatial patterns in multivariate time series, and their combination is more effective than any single approach.
- Evidence anchors: [abstract] "we propose an ensemble technique that combines multiple base models toward the final decision"; [section] "the semi-supervised approach using a Logistic Regressor to combine the base models' outputs is proposed"; [section] "the proposed ensemble technique outperforms the basic algorithms"
- Break condition: If the base models are too similar or if their errors are highly correlated, ensemble methods may not provide significant improvement and could even amplify systematic biases.

## Foundational Learning

- Concept: Curse of Dimensionality
  - Why needed here: Multivariate time series often have many features, making distance-based anomaly detection ineffective as dimensions increase.
  - Quick check question: Why does anomaly detection become harder as the number of features increases in multivariate time series?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to transform feature subsets into orthogonal representations that capture maximum variance, improving model diversity and anomaly detection.
  - Quick check question: How does PCA help in revealing hidden patterns in multivariate time series data?

- Concept: Ensemble Learning
  - Why needed here: Combining multiple diverse models reduces individual model weaknesses and provides more robust predictions than any single model.
  - Quick check question: What is the main advantage of using ensemble techniques for anomaly detection in multivariate time series?

## Architecture Onboarding

- Component map: Multivariate time series data -> Feature Bagging module -> Nested Rotations module -> Base model training (5 architectures) -> Ensemble aggregation -> Performance evaluation

- Critical path:
  1. Preprocess multivariate time series data
  2. Apply Feature Bagging to create feature subsets
  3. Apply Nested Rotations (PCA) to each subset
  4. Train individual base models on transformed subsets
  5. Aggregate predictions using ensemble technique
  6. Evaluate performance using F1-score and AUC

- Design tradeoffs:
  - More base models and partitions increase diversity but also computational cost
  - Higher PCA dimensions capture more variance but may introduce noise
  - Semi-supervised approach needs labeled data for logistic regression training
  - Unsupervised approach is more flexible but may have lower performance

- Failure signatures:
  - Performance plateaus or degrades with more models/rotations
  - Training time becomes prohibitively long
  - Model predictions become highly correlated (low diversity)
  - PCA components do not capture meaningful variance

- First 3 experiments:
  1. Test Feature Bagging alone on a single base model architecture with varying numbers of feature subsets
  2. Add Nested Rotations to Feature Bagging and compare performance against Feature Bagging alone
  3. Combine all base model architectures with Feature Bagging + Nested Rotations and test both unsupervised (majority voting) and semi-supervised (logistic regression) ensembles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative ensemble aggregation functions (beyond majority voting and logistic regression) impact anomaly detection performance in multivariate time series?
- Basis in paper: [explicit] The paper mentions that ensemble models heavily depend on the aggregation function applied to the multiple scores, and suggests investigating whether another function or even a non-linear model can boost performance further.
- Why unresolved: The paper only uses majority voting and logistic regression as aggregation methods, without exploring other potential functions or non-linear models.
- What evidence would resolve it: Empirical comparisons of anomaly detection performance using various aggregation functions (e.g., weighted voting, ensemble pruning, neural network-based aggregators) on benchmark datasets.

### Open Question 2
- Question: How does the proposed ensemble method scale to multivariate time series with significantly higher dimensionality than the SKAB dataset?
- Basis in paper: [explicit] The authors note that their methods (Feature Bagging and Nested Rotations) are robust to high dimensionality, but explicitly state that more tests with time series in higher dimensions are needed to confirm this.
- Why unresolved: The experiments were conducted on the SKAB dataset with 8 features. There is no validation on datasets with significantly more features.
- What evidence would resolve it: Testing the ensemble method on multivariate time series datasets with 50+ features and comparing performance to baseline methods.

### Open Question 3
- Question: What is the precise effect of applying Principal Component Analysis (PCA) on the performance and diversity of the ensemble models?
- Basis in paper: [explicit] The authors state that "Nested Rotations inject diversity and boost the performance of the ensemble" and that "Integrating multiple PCAs with feature bagging leads to increased effectiveness in most cases," but acknowledge that "a deeper investigation of the effect of PCA on the model ensemble is left as future work."
- Why unresolved: The paper provides qualitative observations about PCA's benefits but does not quantify its specific impact on model diversity or performance through ablation studies.
- What evidence would resolve it: Systematic ablation studies comparing ensemble performance with and without PCA at different stages, measuring diversity metrics and performance on benchmark datasets.

## Limitations

- The approach was validated only on the SKAB dataset with water pump sensor data, limiting external validity
- Computational complexity of training 17 models per architecture may not be practical for real-time applications
- No sensitivity analysis for key hyperparameters like number of feature subsets or PCA dimensions

## Confidence

- **High confidence**: The ensemble methodology combining Feature Bagging and Nested Rotations is clearly described and the empirical improvements over baseline models are well-documented.
- **Medium confidence**: The theoretical justification for why Feature Bagging and PCA-based transformations improve anomaly detection is reasonable but lacks strong empirical validation across diverse datasets.
- **Low confidence**: The claim that this approach generalizes well to other multivariate time series domains beyond industrial sensor data is not supported by evidence.

## Next Checks

1. Test the ensemble approach on at least two additional multivariate time series datasets from different domains (e.g., healthcare monitoring, financial transactions) to assess generalizability.
2. Conduct ablation studies to quantify the individual contribution of Feature Bagging versus Nested Rotations to overall performance improvements.
3. Measure and report the computational overhead of the ensemble approach compared to single-model baselines, including training and inference times.