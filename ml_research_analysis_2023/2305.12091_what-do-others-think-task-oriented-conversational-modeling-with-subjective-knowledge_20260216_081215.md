---
ver: rpa2
title: '"What do others think?": Task-Oriented Conversational Modeling with Subjective
  Knowledge'
arxiv_id: '2305.12091'
source_url: https://arxiv.org/abs/2305.12091
tags:
- knowledge
- dialogue
- pages
- subjective
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SK-TOD, a novel task that incorporates subjective
  knowledge, such as user reviews, into task-oriented dialogue systems to address
  subjective user requests. A large-scale dataset is created and released, containing
  19,696 subjective knowledge-seeking dialogue contexts and manually annotated responses
  grounded in 143 entities and 1,430 reviews.
---

# "What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge

## Quick Facts
- arXiv ID: 2305.12091
- Source URL: https://arxiv.org/abs/2305.12091
- Authors: 
- Reference count: 38
- Key outcome: Large performance gap exists between human and machine-generated responses, particularly in capturing opinion diversity and proportion (BLEU score 10.27 vs human reference 10.80)

## Executive Summary
This paper introduces SK-TOD, a novel task that incorporates subjective knowledge (e.g., user reviews) into task-oriented dialogue systems to address subjective user requests. The authors create a large-scale dataset containing 19,696 subjective knowledge-seeking dialogue contexts with manually annotated responses grounded in 143 entities and 1,430 reviews. The proposed pipeline approach, consisting of Knowledge-Seeking Turn Detection, Entity Tracking, Knowledge Selection, and Response Generation components, achieves state-of-the-art performance on the SK-TOD task. Experiments reveal that while the system can effectively identify subjective knowledge-seeking requests and select relevant knowledge snippets, there remains a significant gap in generating responses that faithfully capture the diversity and proportion of opinions compared to human-generated responses.

## Method Summary
The SK-TOD pipeline employs a four-component sequential approach to generate responses grounded in subjective knowledge. Knowledge-Seeking Turn Detection uses fine-tuned pre-trained language models (BERT, RoBERTa, ALBERT, DeBERTa) to classify whether user requests require subjective knowledge. Entity Tracking identifies relevant entities mentioned in the dialogue context. Knowledge Selection employs pairwise text scoring using either bi-encoder or cross-encoder architectures to retrieve relevant knowledge snippets, with cross-encoders showing superior performance. Response Generation uses sequence-to-sequence models (BART, T5) fine-tuned to generate responses conditioned on dialogue context and selected knowledge snippets, optionally enhanced with aspect-based sentiment analysis (ABSA) to improve sentiment representation.

## Key Results
- The system achieves 99.67% accuracy in identifying subjective knowledge-seeking user requests
- Cross-encoder knowledge selection models reach up to 86.07% instance-level F1 score
- Response generation achieves BERTScore of 41.12 with T5, significantly lower than human references
- Large performance gap exists in sentiment accuracy (3.98 vs 4.50) between best model-generated responses and human references

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can identify subjective knowledge-seeking user requests with high accuracy (99.67% accuracy in KTD task).
- Mechanism: A pre-trained language model (e.g., BERT, RoBERTa, ALBERT, DeBERTa) is fine-tuned to classify whether a user request requires subjective knowledge by encoding the dialogue context and applying a binary classifier on the hidden state of the first token.
- Core assumption: The semantic patterns distinguishing subjective knowledge-seeking requests from factual ones are learnable from the training data.
- Evidence anchors:
  - [abstract]: "When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets."
  - [section]: "All models achieve similar and near-perfect performance, which is in line with the findings of Kim et al. (2020)."
  - [corpus]: Weak evidence - the corpus shows related work on task-oriented dialogue but lacks specific evidence about subjective knowledge detection performance.
- Break condition: The classifier may not generalize well to unseen domains or knowledge types beyond the training data distribution.

### Mechanism 2
- Claim: The model can select relevant knowledge snippets with high precision and recall (up to 86.07% instance-level F1 with DeBERTa cross-encoder).
- Mechanism: A pairwise text scoring approach is used where the relevance between dialogue context and knowledge snippets is calculated using either bi-encoder or cross-encoder architectures, with cross-encoder showing superior performance.
- Core assumption: The semantic relevance between dialogue context and knowledge snippets can be effectively captured by pre-trained language models.
- Evidence anchors:
  - [abstract]: "When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets."
  - [section]: "Cross-encoder models outperform bi-encoder models by a large margin."
  - [corpus]: Weak evidence - the corpus contains related dialogue modeling work but lacks specific evidence about knowledge snippet selection performance.
- Break condition: The model may struggle with unseen aspects not present in the training data, as evidenced by the performance gap between seen (88.80% macro-F) and unseen (82.68% macro-F) subsets.

### Mechanism 3
- Claim: The model can generate responses that capture sentiment diversity and proportion (BERTScore of 41.12 with T5).
- Mechanism: A sequence-to-sequence model (BART or T5) is fine-tuned to generate responses conditioned on dialogue context and relevant knowledge snippets, with optional ABSA enhancement to incorporate sentiment information.
- Core assumption: The model can learn to faithfully aggregate and represent diverse opinions from multiple knowledge snippets in the generated response.
- Evidence anchors:
  - [abstract]: "We find that there is a large performance gap between human-generated and machine-generated responses, particularly in faithfully capturing the diversity and proportion of opinions."
  - [section]: "With the help of the ABSA model, both BART and T5 correctly generate the responses with all negative opinions."
  - [corpus]: Weak evidence - the corpus contains related work on knowledge-grounded dialogue but lacks specific evidence about sentiment aggregation performance.
- Break condition: The model may struggle to balance positive and negative opinions accurately, as evidenced by the large gap (3.98 vs 4.50) in sentiment accuracy between the best model-generated responses and human references.

## Foundational Learning

- Concept: Subjective knowledge vs. factual knowledge
  - Why needed here: Understanding the difference between subjective (opinions, experiences) and factual (objective information) knowledge is crucial for this task, as it requires different handling approaches.
  - Quick check question: What are the key differences between subjective and factual knowledge in the context of task-oriented dialogue?

- Concept: Aspect-based sentiment analysis
  - Why needed here: The model needs to understand the sentiment polarity of each knowledge snippet with respect to specific aspects to generate balanced responses.
  - Quick check question: How does aspect-based sentiment analysis differ from general sentiment analysis?

- Concept: Knowledge grounding in dialogue systems
  - Why needed here: Understanding how external knowledge is incorporated into dialogue systems is essential for grasping the SK-TOD task and its unique challenges.
  - Quick check question: What are the key challenges in knowledge grounding for task-oriented dialogue systems?

## Architecture Onboarding

- Component map: Knowledge-Seeking Turn Detection (KTD) -> Entity Tracking (ET) -> Knowledge Selection (KS) -> Response Generation (RG)
- Critical path: The critical path for generating a subjective-knowledge-grounded response is: KTD → ET → KS → RG, where errors in earlier components propagate to later ones.
- Design tradeoffs: The main design tradeoff is between precision and recall in knowledge selection - selecting fewer snippets may miss important opinions, while selecting more may introduce noise. The cross-encoder approach offers better accuracy but at higher computational cost compared to bi-encoder.
- Failure signatures: Common failure modes include: missing relevant entities in ET, failing to capture diverse opinions in KS, and generating unbalanced or incoherent responses in RG.
- First 3 experiments:
  1. Evaluate KTD performance on a held-out test set to verify it can accurately identify subjective knowledge-seeking requests.
  2. Compare bi-encoder and cross-encoder KS models on a validation set to determine the best architecture for the task.
  3. Fine-tune the RG model with and without ABSA enhancement to measure the impact on sentiment accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the generalizability of subjective knowledge-seeking turn detection models to unseen domains or knowledge types?
- Basis in paper: [explicit] The paper states that the KTD classifier may work well when restricted only to the dataset or similar, and its generalizability to unseen domains or knowledge types needs to be further explored in future works.
- Why unresolved: The current KTD models achieve near-perfect performance on the SK-TOD dataset, but their performance on unseen domains or knowledge types is unknown.
- What evidence would resolve it: Experiments evaluating the performance of KTD models on datasets from different domains or knowledge types, or studies investigating techniques to improve the generalizability of these models.

### Open Question 2
- Question: What are the most effective methods for tracking aspects being questioned over multiple turns in entity tracking?
- Basis in paper: [inferred] The current entity tracking method uses word-matching-based approaches, which may not capture the full context of the dialogue or track aspects being questioned over multiple turns.
- Why unresolved: The current entity tracking method is limited in its ability to track aspects being questioned over multiple turns, which may impact the performance of downstream tasks like knowledge selection and response generation.
- What evidence would resolve it: Comparative studies of different entity tracking methods, particularly those that can track aspects being questioned over multiple turns, and their impact on downstream task performance.

### Open Question 3
- Question: How can we improve the faithfulness of sentiment proportion aggregation in response generation?
- Basis in paper: [explicit] The paper states that there is still a large gap in sentiment accuracy between the best model-generated responses and the references, indicating that it is challenging for current models to faithfully aggregate sentiment information from multiple knowledge snippets.
- Why unresolved: Current models struggle to accurately capture the diversity and proportion of opinions expressed in multiple knowledge snippets, leading to less faithful responses.
- What evidence would resolve it: Studies investigating techniques to improve the aggregation of sentiment information from multiple knowledge snippets, such as incorporating more sophisticated sentiment analysis models or developing novel architectures for response generation.

## Limitations

- The approach heavily relies on customer reviews as the source of subjective knowledge, which may limit its applicability to domains where structured review data is unavailable or sparse.
- The cross-encoder architecture used for knowledge selection shows superior performance but at significantly higher computational cost compared to bi-encoder approaches, raising concerns about real-world deployment efficiency.
- The system follows a strict pipeline architecture where errors in early stages propagate to later stages, and the paper does not provide detailed error analysis showing how these cascading failures impact overall system performance.

## Confidence

**High Confidence Claims** (Supported by strong empirical evidence):
- The SK-TOD task is feasible and presents unique challenges in capturing diverse opinions
- Cross-encoder architectures outperform bi-encoder approaches for knowledge selection
- There exists a significant performance gap between human and machine-generated responses, particularly in sentiment accuracy

**Medium Confidence Claims** (Supported by evidence but with notable limitations):
- The proposed pipeline approach achieves state-of-the-art performance on the SK-TOD task
- ABSA enhancement improves response generation quality by incorporating sentiment information
- The dataset is sufficiently large and diverse to support robust model training

**Low Confidence Claims** (Limited or indirect evidence):
- The system can generalize to unseen aspects and entities beyond the training distribution
- The approach would maintain performance when scaled to multi-domain applications
- The computational efficiency is acceptable for real-world deployment

## Next Checks

1. **Cross-domain validation**: Test the SK-TOD pipeline on at least two additional domains (e.g., hotels, products) using the same model architecture to evaluate generalization capability. Compare performance metrics against the restaurant domain baseline.

2. **Ablation study on pipeline components**: Systematically remove or replace each component (KTD, ET, KS, RG) with simpler alternatives to quantify the contribution of each stage to overall performance and identify potential bottlenecks.

3. **Human evaluation reliability assessment**: Conduct a reliability analysis of the human evaluation process by calculating inter-annotator agreement (e.g., Cohen's kappa) and testing whether the observed performance gap between human and machine responses is statistically significant.