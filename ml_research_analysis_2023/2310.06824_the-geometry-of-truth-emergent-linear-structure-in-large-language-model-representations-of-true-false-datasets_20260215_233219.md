---
ver: rpa2
title: 'The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations
  of True/False Datasets'
arxiv_id: '2310.06824'
source_url: https://arxiv.org/abs/2310.06824
tags:
- 'true'
- 'false'
- statements
- cities
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether language models linearly represent
  the truth or falsehood of factual statements. The authors curate high-quality datasets
  of simple true/false statements and study the structure of LLM representations using
  three lines of evidence: 1) visualizations revealing clear linear structure in the
  top principal components, 2) transfer experiments showing that probes trained on
  one dataset generalize to others, and 3) causal interventions demonstrating that
  truth directions are causally implicated in model outputs.'
---

# The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets

## Quick Facts
- arXiv ID: 2310.06824
- Source URL: https://arxiv.org/abs/2310.06824
- Reference count: 21
- Key outcome: LLMs linearly represent truth as a direction in latent space, with true/false statements separating in top principal components and mass-mean probing outperforming logistic regression for identifying truth directions

## Executive Summary
This paper investigates whether large language models linearly represent the truth or falsehood of factual statements. The authors curate high-quality datasets of simple true/false statements and study LLM representations using three lines of evidence: visualizations revealing clear linear structure in top principal components, transfer experiments showing probe generalization across topically different datasets, and causal interventions demonstrating that truth directions are causally implicated in model outputs. The key findings are that truth and falsehood separate linearly in the top principal components of LLM representations, this structure emerges rapidly in early-middle layers, and mass-mean probing generalizes better than logistic regression. Finally, the authors show that surgically intervening in an LLM's forward pass by adding truth vectors can cause it to treat false statements as true and vice versa.

## Method Summary
The authors curate datasets of true/false statements and extract residual stream activations from LLM layers. They apply PCA visualizations to identify linear structure, train linear probes (logistic regression and mass-mean probing) on one dataset, test probe generalization to other datasets, and perform causal interventions by adding truth vectors to the residual stream. The evaluation framework measures probe accuracy and intervention effectiveness across multiple datasets including cities, numerical comparisons, and logical conjunctions/disjunctions.

## Key Results
- True and false statements separate linearly in the top few principal components of LLM representations
- Mass-mean probing outperforms logistic regression in both generalization and causal effectiveness
- Causal interventions along identified truth directions can flip the model's classification of statements from true to false and vice versa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs linearly represent truth as a direction in their latent space
- Mechanism: The model's internal representations separate true and false statements along principal components, creating a linear decision boundary that can be identified through probing techniques
- Core assumption: Truth is encoded as a single linear feature rather than a complex nonlinear pattern
- Evidence anchors: [abstract] "Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements"; [section 3.1] "True and false statements separate in the top few PCs"; [corpus] "Probing the Geometry of Truth"

### Mechanism 2
- Claim: Mass-mean probing generalizes better than logistic regression for identifying truth directions
- Mechanism: Mass-mean probing identifies the direction from the mean of false statements to the mean of true statements, which better captures the underlying truth feature even when other correlated features exist
- Core assumption: The mean vector between true and false statement representations captures the true underlying feature direction
- Evidence anchors: [section 4.1] "mass-mean probing, a simple, optimization-free probing technique"; [section 4.3] "Mass-mean probing outperforms logistic regression"; [corpus] "From Directions to Cones"

### Mechanism 3
- Claim: Causal interventions along identified truth directions can flip the model's classification of statements
- Mechanism: Adding scaled truth vectors to specific token positions in the residual stream causes the model to treat false statements as true and vice versa
- Core assumption: The identified truth direction is causally implicated in the model's output generation
- Evidence anchors: [abstract] "surgically intervening in a LLM's forward pass by adding truth vectors"; [section 5.2] "Mass-mean probe directions are highly causal"; [corpus] "Linear Spatial World Models Emerge in Large Language Models"

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to visualize and identify the linear structure in LLM representations that separates true from false statements
  - Quick check question: How does PCA help identify whether two classes of data are linearly separable?

- Concept: Linear probing techniques
  - Why needed here: Different probing methods (logistic regression, mass-mean probing, CCS) are compared for their ability to identify truth directions
  - Quick check question: What is the key difference between logistic regression and mass-mean probing in how they identify feature directions?

- Concept: Causal mediation analysis
  - Why needed here: Causal interventions are used to test whether identified truth directions are actually used by the model for truth classification
  - Quick check question: How can we determine if a feature direction identified by probing is causally implicated in model outputs?

## Architecture Onboarding

- Component map: Dataset curation pipeline -> LLM inference system -> PCA visualization module -> Probing implementation -> Causal intervention module -> Evaluation framework

- Critical path: 1. Curate high-quality true/false datasets 2. Extract residual stream activations from target LLM layers 3. Apply PCA to visualize linear structure 4. Train probes on one dataset 5. Test probe generalization to other datasets 6. Perform causal interventions to validate directions 7. Analyze results and refine techniques

- Design tradeoffs:
  - Dataset complexity vs. generalization: Simpler statements may generalize better but capture less nuanced truth representation
  - Layer selection: Earlier layers may show cleaner structure but later layers may be more causally relevant
  - Intervention strength: Must balance between effective manipulation and avoiding model breakdown

- Failure signatures:
  - Poor generalization across datasets suggests probing technique or linear separability assumption is flawed
  - Ineffective causal interventions suggest identified directions are not causally relevant
  - Inconsistent visualization patterns suggest dataset quality or extraction method issues

- First 3 experiments:
  1. Run PCA visualization on layer 12 representations of cities dataset to confirm linear separation
  2. Train mass-mean probe on cities dataset and test generalization to sp_en_trans dataset
  3. Perform causal intervention using mass-mean direction on sp_en_trans statements and measure classification flip rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs represent the truth of statements containing negations compared to their positive counterparts?
- Basis in paper: [inferred] The paper discusses misalignment of naive truth directions (NTDs) for cities and neg cities datasets, where NTDs are approximately orthogonal. It also mentions that probes trained on cities and neg cities generalize better than those trained only on cities, providing evidence for the "misalignment from correlational inconsistency" hypothesis.
- Why unresolved: The paper does not provide a detailed analysis of how LLMs represent the truth of negated statements internally. It only observes that NTDs for cities and neg cities are misaligned and that training on both datasets improves generalization.
- What evidence would resolve it: Detailed analysis of LLM representations of true/false statements with and without negations, including visualizations, probing experiments, and causal interventions.

### Open Question 2
- Question: Do LLMs linearly represent the truth of statements containing conjunctions and disjunctions?
- Basis in paper: [explicit] The paper introduces cities cities conj and cities cities disj datasets consisting of conjunctions and disjunctions of statements from the cities dataset. It also mentions that linear structure emerges later for statements with more complicated logical structure, like conjunctive statements.
- Why unresolved: The paper does not provide a detailed analysis of how LLMs represent the truth of statements containing conjunctions and disjunctions. It only mentions that linear structure emerges later for these types of statements and does not investigate the internal representations or perform probing experiments on these datasets.
- What evidence would resolve it: Detailed analysis of LLM representations of true/false statements containing conjunctions and disjunctions, including visualizations, probing experiments, and causal interventions.

### Open Question 3
- Question: How do LLMs represent the truth of statements containing numerical comparisons compared to statements about factual associations?
- Basis in paper: [explicit] The paper introduces larger than and smaller than datasets consisting of statements about numerical comparisons. It also mentions that probes trained on these datasets generalize well to other datasets, including the Spanish-English translation dataset. However, it does not provide a detailed comparison of how LLMs represent the truth of numerical comparisons versus factual associations.
- Why unresolved: The paper does not provide a detailed analysis of how LLMs represent the truth of numerical comparisons compared to factual associations. It only mentions that probes trained on numerical comparison datasets generalize well and does not investigate the internal representations or perform visualizations of these datasets.
- What evidence would resolve it: Detailed analysis of LLM representations of true/false statements containing numerical comparisons and factual associations, including visualizations, probing experiments, and causal interventions.

## Limitations
- Findings based on simple factual statements may not generalize to more complex reasoning or nuanced truth representations
- Study focuses on narrow class of propositions (simple facts about cities, companies) rather than mathematical statements, moral claims, or contextual truths
- Causal intervention experiments performed on limited scale and may not fully capture model's behavior across all contexts

## Confidence

**High Confidence**: Core finding that true and false statements separate linearly in LLM representations (PCA visualizations showing clear structure in top components). This is supported by multiple datasets and visualizations across different models.

**Medium Confidence**: Generalization of probes across different datasets and superiority of mass-mean probing over logistic regression. While results show strong generalization, differences between probing techniques could be influenced by dataset-specific factors.

**Low Confidence**: Causal intervention results, particularly interpretation that adding truth vectors causes model to "believe" false statements are true. Interventions are performed on limited set of positions and may not fully capture model's complex reasoning processes.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate probe generalization on more diverse truth domains including mathematical statements, temporal facts, and probabilistic claims to test whether linear truth representation is universal or specific to simple factual statements.

2. **Multi-Directional Truth Structure Analysis**: Investigate whether truth is better represented by a single linear direction or multiple dimensions by analyzing the full eigenvalue spectrum of true/false statement representations and testing interventions along multiple directions simultaneously.

3. **Temporal and Context Stability Test**: Track how truth directions change across different layers and time steps within the model's forward pass, and test whether identified truth directions remain stable under different prompt contexts and temperature settings.