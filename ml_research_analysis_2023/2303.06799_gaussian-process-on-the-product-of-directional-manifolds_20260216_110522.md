---
ver: rpa2
title: Gaussian Process on the Product of Directional Manifolds
arxiv_id: '2303.06799'
source_url: https://arxiv.org/abs/2303.06799
tags:
- kernel
- gaussian
- proposed
- kernels
- circular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a principled study on defining Gaussian processes
  (GPs) with inputs on the product of directional manifolds, specifically on hypertori.
  The proposed method introduces a novel hypertoroidal von Mises (HvM) kernel for
  establishing topology-aware GPs on hypertori with consideration of correlational
  circular components.
---

# Gaussian Process on the Product of Directional Manifolds

## Quick Facts
- arXiv ID: 2303.06799
- Source URL: https://arxiv.org/abs/2303.06799
- Authors: 
- Reference count: 26
- Primary result: Novel hypertoroidal von Mises (HvM) kernel enables topology-aware Gaussian processes on product manifolds, achieving superior tracking accuracy in sensor networks compared to parametric models and conventional kernels.

## Executive Summary
This paper introduces a principled approach for Gaussian process (GP) modeling on the product of directional manifolds, specifically hypertori, through the development of a hypertoroidal von Mises (HvM) kernel. The proposed method addresses the challenge of handling periodic correlation structures in circular components by combining component-wise von Mises terms with cross-term correlation modeling. The approach is demonstrated through multi-output GP regression for learning vector-valued functions on hypertori using the intrinsic coregionalization model, with applications in recursive localization for sensor networks.

## Method Summary
The method introduces the HvM kernel for establishing topology-aware GPs on hypertori by defining a distance metric component-wise across circular dimensions and adding a quadratic correlation term with a nonnegative, symmetric weighting matrix Λ. The kernel is integrated within an intrinsic coregionalization model framework to handle multi-output GP regression. Hyperparameter optimization is performed via maximum likelihood estimation using closed-form derivatives, implemented through gradient-based optimization with trust-regions methods. The approach is validated on a synthetic ranging-based sensor network with angle-of-arrival inputs, employing particle filtering with GP-based likelihood reweighting for recursive localization.

## Key Results
- The HvM-based GP achieves superior tracking accuracy compared to parametric models and GPs with conventional kernel designs (PSE, PvM) in synthetic ranging-based sensor networks.
- The method demonstrates improved Average Position Error (APE) performance across three different trajectories (Lemniscate of Bernoulli, Lissajous, Limacon) under varying noise levels.
- The topology-aware HvM kernel preserves periodicity in the covariance function, avoiding discontinuities at periodic boundaries that affect standard Euclidean kernels.

## Why This Works (Mechanism)

### Mechanism 1
The HvM kernel handles periodic correlation structure on hypertori by combining component-wise von Mises terms with cross-term correlation modeling. It extends the von Mises kernel to product manifolds by defining distance metric `d(u,v)` component-wise and adding a quadratic correlation term `d(u,v)⊤Λd(u,v)` where Λ is a symmetric nonnegative matrix encoding correlations between circular components. Core assumption: The underlying process on the hypertorus exhibits both periodic behavior on each circle and non-trivial correlations between components that can be captured by a quadratic form in the distance metric.

### Mechanism 2
The HvM kernel enables topology-aware GP regression by preserving periodicity in the covariance function, unlike standard Euclidean kernels. By using the inner product `(ui)⊤vj` as the distance metric for each circular component and exponentiating, the kernel automatically wraps around at 2π, producing identical posterior values at periodic boundaries. Core assumption: The periodic nature of circular data means that distances should be measured on the unit circle, not in Euclidean space.

### Mechanism 3
The HvM kernel within the intrinsic coregionalization model enables multi-output GP regression on hypertori with analytically tractable hyperparameter optimization. By structuring the covariance as `kHvM(u,v)B` where B is a positive semidefinite coregionalization matrix, the model captures both the toroidal geometry and correlations between outputs, with closed-form derivatives for efficient gradient-based optimization. Core assumption: The output correlations can be modeled independently of the input geometry, allowing separation into a kernel term and a coregionalization matrix.

## Foundational Learning

- Concept: von Mises distribution and its properties
  - Why needed here: The HvM kernel is fundamentally built upon the von Mises distribution's properties on the unit circle, using its exponential form with inner product similarity measure.
  - Quick check question: What property of the von Mises distribution makes it suitable for modeling circular data compared to wrapped normal distributions?

- Concept: Positive definite kernels and Bochner's theorem
  - Why needed here: Understanding why the HvM kernel is positive definite requires knowledge of kernel theory, particularly how the quadratic form in the exponent preserves positive definiteness.
  - Quick check question: How does adding a quadratic term `d(u,v)⊤Λd(u,v)` to the exponent of a kernel affect its positive definiteness when Λ is nonnegative and symmetric?

- Concept: Intrinsic coregionalization model for multi-output GPs
  - Why needed here: The paper uses this model to handle vector-valued functions on hypertori, requiring understanding of how to structure matrix-valued covariance functions.
  - Quick check question: What is the form of the covariance function in the intrinsic coregionalization model and what property must the coregionalization matrix B satisfy?

## Architecture Onboarding

- Component map: Input preprocessing -> Kernel computation -> Covariance matrix assembly -> Hyperparameter optimization -> GP inference -> Particle filtering with GP-based reweighting -> Application output
- Critical path: Kernel computation → Covariance matrix assembly → Hyperparameter optimization → GP inference → Application output
- Design tradeoffs:
  - HvM kernel vs. product kernels: HvM captures correlations between components but has more hyperparameters (Λ matrix) increasing optimization complexity
  - Analytic derivatives vs. numerical: Analytic derivatives enable efficient optimization but require careful implementation of all partial derivatives
  - Multi-output vs. independent GPs: Coregionalization captures output correlations but increases computational cost and requires positive definite B
- Failure signatures:
  - Optimization divergence: May indicate poor initialization or overly complex correlation structure in Λ
  - Periodic discontinuities in predictions: Suggests kernel implementation error or inappropriate periodic boundary assumptions
  - Particle filter degeneracy: Could indicate poor likelihood modeling or insufficient particle diversity
- First 3 experiments:
  1. Implement HvM kernel on a single hypertorus (T²) with synthetic data showing periodic behavior, compare predictions with SE and product vM kernels
  2. Add the coregionalization matrix B and test multi-output regression on synthetic correlated outputs with toroidal inputs
  3. Implement the full particle filtering application with synthetic range measurements and AoA inputs, comparing HvM-GP with parametric and product kernel baselines on tracking accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the computational complexity of the proposed HvM kernel and its derivatives compared to conventional kernel designs?
- Basis in paper: The paper mentions that the HvM kernel and its derivatives are derived in closed form, but does not provide explicit computational complexity analysis or runtime benchmarks compared to other kernel designs.
- Why unresolved: The paper focuses on demonstrating the tracking accuracy of the proposed HvM-based GP, but does not provide a detailed analysis of its computational efficiency.
- What evidence would resolve it: A detailed computational complexity analysis comparing the HvM kernel and its derivatives to other kernel designs, along with runtime benchmarks on representative datasets.

### Open Question 2
- Question: How does the performance of the HvM kernel scale with the dimensionality of the input space?
- Basis in paper: The paper demonstrates the proposed HvM kernel on a 3-dimensional hypertorus (T³), but does not investigate its performance on higher-dimensional hypertori or other product manifolds with more than three circular components.
- Why unresolved: The scalability of the HvM kernel to higher-dimensional input spaces is an important aspect that would determine its applicability to more complex real-world problems.
- What evidence would resolve it: Experiments evaluating the performance of the HvM kernel on hypertori with more than three circular components, as well as on other product manifolds with varying dimensionality.

### Open Question 3
- Question: How robust is the proposed HvM kernel to noise and outliers in the input data?
- Basis in paper: The paper demonstrates the proposed HvM kernel on a synthetic ranging-based sensor network with varying levels of measurement noise, but does not investigate its performance in the presence of outliers or corrupted input data.
- Why unresolved: Real-world data often contains outliers and noise that can affect the performance of machine learning models.
- What evidence would resolve it: Experiments evaluating the performance of the HvM kernel on datasets with varying levels of outliers and noise, as well as techniques for robustifying the kernel against such perturbations.

## Limitations
- The method assumes that correlations between circular components can be captured by a quadratic form in the distance metric, which may not hold for all applications.
- The approach's effectiveness depends on the appropriateness of periodic boundary conditions at 2π for the specific problem domain.
- The coregionalization assumption between outputs may fail when output correlations depend strongly on input values.

## Confidence
- High confidence in the mathematical formulation of the HvM kernel and its positive definiteness properties
- Medium confidence in the optimization procedure and hyperparameter tuning methodology
- Low confidence in the real-world applicability without further validation on non-synthetic data

## Next Checks
1. Test the HvM kernel on real sensor network data with known periodic correlations to validate performance beyond synthetic scenarios.
2. Evaluate sensitivity to hyperparameter initialization and optimization convergence across different random seeds.
3. Compare against alternative periodic kernel formulations (e.g., wrapped kernels, periodic kernel with automatic relevance determination) to quantify the benefit of the HvM approach.