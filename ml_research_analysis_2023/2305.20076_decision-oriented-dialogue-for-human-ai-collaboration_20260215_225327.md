---
ver: rpa2
title: Decision-Oriented Dialogue for Human-AI Collaboration
arxiv_id: '2305.20076'
source_url: https://arxiv.org/abs/2305.20076
tags:
- user
- message
- 'true'
- good
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces decision-oriented dialogues, a new class
  of collaborative tasks where AI assistants must help humans make complex decisions
  through natural language. The authors formalize three everyday domains: reviewer
  assignment, itinerary planning, and group scheduling, and develop environments where
  agents are rewarded based on decision quality.'
---

# Decision-Oriented Dialogue for Human-AI Collaboration

## Quick Facts
- arXiv ID: 2305.20076
- Source URL: https://arxiv.org/abs/2305.20076
- Reference count: 40
- Key outcome: Large language models significantly underperform humans in decision-oriented dialogues, achieving lower rewards despite longer conversations

## Executive Summary
This paper introduces decision-oriented dialogues as a new class of collaborative tasks where AI assistants must help humans make complex decisions through natural language. The authors formalize three everyday domains (reviewer assignment, itinerary planning, group scheduling) and develop environments where agents are rewarded based on decision quality. Human-human dialogues serve as baselines, achieving around 90% of optimal scores. When evaluated in self-play, large language models (GPT-3) perform significantly worse than humans, with longer dialogues and lower rewards, indicating challenges in efficient communication and optimization.

## Method Summary
The authors create three procedural environments where agents collaborate to make decisions in reviewer assignment, itinerary planning, and group scheduling tasks. They collect human-human dialogue datasets (5253 messages, 58K words) to establish baselines, then evaluate GPT-3 models in self-play and prompted self-play settings. The evaluation uses a reward function based on decision quality normalized by optimal scores, plus a communication cost function. Models are prompted with different strategies including partial human dialogues and explicit decision objectives.

## Key Results
- Human-human dialogues achieve approximately 90% of optimal scores in decision-oriented tasks
- GPT-3 self-play achieves significantly lower rewards with longer dialogues compared to human baselines
- Models struggle with goal-directed questioning, reasoning, and integrating dialogue history for optimal proposals
- Hallucination emerges as a problem when models lack complete information about decision domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models fail in decision-oriented dialogues because they lack goal-directed information exchange.
- Mechanism: In decision-oriented dialogues, agents must exchange information strategically to maximize decision quality while minimizing communication cost. Models fail because they cannot infer which information is critical for the final decision.
- Core assumption: The underlying optimization problem requires specific pieces of information that are not directly observable to all agents.
- Evidence anchors:
  - [abstract] "Models struggle with goal-directed questioning, reasoning, and integrating information from dialogue history to make optimal proposals."
  - [section] "agents must determine what their partners already know and what information is likely to be decision-relevant, asking clarification questions and making inferences as needed."
  - [corpus] Weak evidence; no direct citations to prior work on goal-directed dialogue.
- Break condition: If the optimization problem becomes trivial or all information becomes observable to all agents.

### Mechanism 2
- Claim: Language models fail because they cannot perform the final optimization step in structured decision problems.
- Mechanism: While models can communicate naturally, they cannot translate dialogue history into an optimal decision in domains requiring structured reasoning (e.g., bipartite matching, knapsack-like constraints).
- Core assumption: The decision space has combinatorial structure that requires explicit computation rather than pattern matching.
- Evidence anchors:
  - [abstract] "models fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues."
  - [section] "models fail to do the optimization step of the proposal: proposals are often only slightly better than random."
  - [corpus] Weak evidence; no direct citations to prior work on structured reasoning in dialogue.
- Break condition: If the decision problem becomes fully observable or can be solved by simple heuristics.

### Mechanism 3
- Claim: Language models fail due to hallucination when they lack complete information.
- Mechanism: When agents lack complete information about the decision space, models generate plausible-sounding but incorrect responses rather than acknowledging uncertainty.
- Core assumption: The model's pretraining data does not contain sufficient examples of the specific decision domains.
- Evidence anchors:
  - [section] "hallucinations are a problem, as with other tasks involving language models."
  - [section] "models ask general questions such as 'Do you have any other preferences?' and sometimes slightly more specific ones such as 'Do you have a price point?', but the questions are not goal-directed in eliciting decision-critical information."
  - [corpus] Weak evidence; no direct citations to prior work on hallucination in decision-making.
- Break condition: If the model has access to complete information or can verify responses against ground truth.

## Foundational Learning

- Concept: Optimization under uncertainty
  - Why needed here: Agents must make decisions when they have incomplete information about the world state
  - Quick check question: How would you formalize the trade-off between communication cost and decision quality in a simple two-agent scenario?

- Concept: Structured reasoning in combinatorial spaces
  - Why needed here: The final decision requires solving an optimization problem with constraints
  - Quick check question: What algorithms could you use to solve the reviewer matching problem if all information were available?

- Concept: Goal-directed information seeking
  - Why needed here: Agents must ask questions that reduce uncertainty about critical decision factors
  - Quick check question: How would you determine which questions to ask first in a multi-step decision problem?

## Architecture Onboarding

- Component map: Environment simulator -> Dialogue manager -> Information access layer -> Optimization engine -> Evaluation metrics

- Critical path: Agent generates message based on current knowledge and dialogue history → Message is parsed and forwarded to other agents → If proposal is made, environment computes reward and determines if dialogue ends → Agent updates knowledge based on new information → Repeat until acceptance or timeout

- Design tradeoffs:
  - Complete information vs. partial observability: Affects communication complexity
  - Natural language vs. formal messages: Affects parsing complexity and flexibility
  - Self-play vs. human-in-the-loop: Affects evaluation reliability and scalability

- Failure signatures:
  - Long dialogues with low reward: Indicates inefficient information exchange
  - Short dialogues with low reward: Indicates failure to gather critical information
  - High variance in performance: Indicates sensitivity to initialization or prompting

- First 3 experiments:
  1. Compare model performance with and without tool use for information access
  2. Evaluate different prompting strategies for goal-directed questioning
  3. Test the impact of providing models with partial optimization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be enhanced to better reason about and optimize complex decision problems in dialogue?
- Basis in paper: [explicit] The paper discusses that current LLMs struggle with the optimization step of choosing the best solution given the entire dialogue history, as seen in the self-play and prompted self-play results.
- Why unresolved: While the paper identifies this as a challenge, it does not propose specific solutions for improving the reasoning and optimization capabilities of LLMs in dialogue.
- What evidence would resolve it: Future work could develop and evaluate techniques such as integrating external tools for optimization, improving chain-of-thought reasoning, or training models specifically on decision-oriented dialogue tasks to enhance their performance.

### Open Question 2
- Question: What strategies can be employed to improve the efficiency of communication between AI assistants and humans in decision-oriented dialogues?
- Basis in paper: [explicit] The paper highlights that models engage in longer dialogues than humans while achieving lower rewards, indicating challenges in efficient communication.
- Why unresolved: The paper does not explore specific methods for improving the efficiency of communication, such as developing models that can better prioritize and ask goal-directed questions.
- What evidence would resolve it: Future research could investigate techniques like active learning, reinforcement learning, or improved natural language understanding to develop models that can communicate more efficiently with humans.

### Open Question 3
- Question: How can decision-oriented dialogue systems be designed to handle more complex and realistic scenarios, such as those involving privacy concerns or multi-party negotiations?
- Basis in paper: [inferred] The paper focuses on three specific domains (reviewer matching, itinerary planning, and group scheduling) and mentions that real-world scenarios may involve additional complexities like privacy issues.
- Why unresolved: The paper does not explore how the proposed tasks and evaluation methods can be extended to handle more complex scenarios or address practical concerns like privacy.
- What evidence would resolve it: Future work could develop and evaluate decision-oriented dialogue systems in more realistic and complex scenarios, incorporating privacy-preserving techniques and handling multi-party negotiations with diverse objectives.

## Limitations

- The study focuses on three specific decision domains, making it unclear whether observed LLM failures generalize to other types of decision-making tasks
- Self-play evaluation may not fully capture the complexity of human-AI collaboration where humans have different communication patterns and goals than AI agents
- The paper uses relatively simple prompting strategies, and observed failures might be partially attributable to suboptimal prompting rather than fundamental LLM limitations

## Confidence

- High confidence: Human-human dialogues achieve high rewards (90% of optimal) and LLM self-play achieves significantly lower rewards
- Medium confidence: Characterization of specific failure modes (inefficient questioning, failure to optimize, hallucination) is supported by qualitative analysis
- Low confidence: Claims about fundamental limitations of LLMs versus implementation-specific issues (like prompting or training data coverage)

## Next Checks

1. Cross-domain generalization test: Evaluate the same LLM models on a fourth, structurally different decision domain (e.g., medical diagnosis assistance or legal document review) to test whether the observed failure patterns are domain-specific or represent broader limitations.

2. Hybrid evaluation protocol: Conduct human-AI interaction experiments where humans interact with LLM agents (not just LLM self-play) to verify whether the performance gap persists in realistic usage scenarios and to identify any adaptation patterns humans develop when collaborating with imperfect AI assistants.

3. Enhanced prompting ablation: Systematically test different prompting strategies including chain-of-thought prompting, tool-integrated prompting for information lookup, and decision-focused prompting to establish whether the observed failures are due to fundamental LLM limitations or could be substantially mitigated through better prompting.