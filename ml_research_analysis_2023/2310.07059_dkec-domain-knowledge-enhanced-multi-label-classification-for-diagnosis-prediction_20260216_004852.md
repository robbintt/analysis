---
ver: rpa2
title: 'DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction'
arxiv_id: '2310.07059'
source_url: https://arxiv.org/abs/2310.07059
tags:
- medical
- labels
- text
- knowledge
- dkec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DKEC is a domain knowledge enhanced multi-label classification
  method for diagnosis prediction in electronic health records. It addresses the long-tail
  label distribution problem by constructing a heterogeneous graph from medical guidelines
  to capture semantic relations among diverse medical entities, and incorporating
  this knowledge into text features using a label-wise attention mechanism.
---

# DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction

## Quick Facts
- arXiv ID: 2310.07059
- Source URL: https://arxiv.org/abs/2310.07059
- Authors: 
- Reference count: 11
- Key outcome: DKEC outperforms state-of-the-art methods, particularly for few-shot classes, achieving up to 32.8% increase in macro F1 score on the RAA dataset and up to 21.3% improvement in top-5 recall for tail labels on the MIMIC III dataset.

## Executive Summary
This paper proposes DKEC, a domain knowledge enhanced multi-label classification method for diagnosis prediction in electronic health records. The approach addresses the long-tail label distribution problem by constructing a heterogeneous graph from medical guidelines to capture semantic relations among diverse medical entities, and incorporating this knowledge into text features using a label-wise attention mechanism. DKEC also proposes a group-wise training method to cluster similar labels and alleviate data imbalance. Evaluated on two real-world medical datasets, DKEC outperforms state-of-the-art methods, particularly for few-shot classes, and enables smaller language models to achieve comparable performance to large language models.

## Method Summary
DKEC is a domain knowledge enhanced multi-label classification method that constructs a heterogeneous graph from medical guidelines to capture semantic relations among diverse medical entities (diagnoses, signs/symptoms, medications, procedures). This graph is processed by a heterogeneous graph transformer to update diagnosis embeddings, which are then combined with text embeddings using a label-wise attention mechanism. The method also employs group-wise training that clusters semantically similar labels to share training data and alleviate class imbalance. The system is evaluated on real-world medical datasets (RAA and MIMIC III) and compared against baseline models.

## Key Results
- DKEC achieves up to 32.8% increase in macro F1 score on the RAA dataset compared to state-of-the-art methods
- DKEC shows up to 21.3% improvement in top-5 recall for tail labels on the MIMIC III dataset
- Smaller language models with DKEC achieve comparable performance to large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous graph captures semantic relations among diverse medical entities to provide contextual knowledge for few-shot classes.
- Mechanism: A heterogeneous graph is constructed from medical guidelines where nodes represent different entity types (diagnosis codes, signs/symptoms, medications, procedures) and edges encode relations (has/indicates, suggests/administers, takes/performs). A heterogeneous graph transformer (HGT) aggregates information from neighboring entities to update diagnosis embeddings, which are then used in a label-wise attention mechanism to weight text tokens relevant to each diagnosis.
- Core assumption: Semantic relationships between diverse medical entities in guidelines are predictive of diagnosis codes and can be effectively encoded in a heterogeneous graph structure.
- Evidence anchors:
  - [abstract] "automated construction of heterogeneous knowledge graphs from external sources to capture semantic relations among diverse medical entities"
  - [section] "We utilize external medical domain knowledge... to construct a heterogeneous graph of medical entities... There are four different types of nodes... and three types of bidirectional edges..."
  - [corpus] No direct evidence found in neighbors about heterogeneous graph construction for medical diagnosis prediction.

### Mechanism 2
- Claim: Group-wise training clusters semantically similar labels to alleviate data imbalance by combining samples from related diagnoses.
- Mechanism: Labels are grouped based on semantic similarity (cosine similarity of embeddings > threshold α) and combined training samples (total > threshold β). During training, group-wise binary cross-entropy is computed, and logic rules (e.g., age-based) distinguish labels within groups during inference.
- Core assumption: Labels representing similar medical conditions (e.g., adult vs. pediatric versions) have similar features and can be grouped to share training data, with post-processing rules to distinguish them.
- Evidence anchors:
  - [abstract] "incorporating the heterogeneous knowledge graphs... using a label-wise attention mechanism" and "group-wise training method based on similarity of labels to alleviate data imbalance"
  - [section] "We propose a simple yet effective group-wise training method that clusters classes with similar medical semantics into disjoint groups during training... The first criterion is that the labels within each group should have semantic similarity or share similar features."
  - [corpus] No direct evidence found in neighbors about group-wise training for label imbalance in medical diagnosis prediction.

### Mechanism 3
- Claim: Label-wise attention mechanism incorporates domain knowledge into text features by assigning diagnosis-specific weights to text tokens.
- Mechanism: Updated diagnosis embeddings (from HGT) are combined with text embeddings using a label-wise attention mechanism. For each label, attention weights are computed for each text token based on the label's knowledge embedding, creating label-specific text representations that emphasize relevant information.
- Core assumption: Different diagnoses require focusing on different aspects of the medical text, and domain knowledge can guide which text features are most relevant for each diagnosis.
- Evidence anchors:
  - [abstract] "incorporating the heterogeneous knowledge graphs... using a label-wise attention mechanism"
  - [section] "We then designed an HLA to combine knowledge from each label representation D⋆k ∈ D⋆ with text representation Et, by having the labels assign different weights for each token in the text representation."
  - [corpus] No direct evidence found in neighbors about label-wise attention for medical diagnosis prediction.

## Foundational Learning

- Concept: Multi-label text classification (MLTC)
  - Why needed here: The task involves assigning multiple diagnosis codes to a single patient record based on free-form medical text, which is inherently a multi-label problem.
  - Quick check question: Can a single patient record have more than one diagnosis code assigned to it in this system?

- Concept: Long-tail label distribution
  - Why needed here: Medical diagnosis codes follow a power-law distribution where frequent conditions (e.g., chest pain) have many training samples while rare conditions (e.g., overdose) have few, creating a class imbalance problem.
  - Quick check question: What is the ratio of training samples between the most frequent and least frequent diagnosis codes in the dataset?

- Concept: Heterogeneous graph neural networks
  - Why needed here: The system constructs a heterogeneous graph with different node types (diagnoses, signs/symptoms, medications, procedures) and relations to capture complex medical knowledge from guidelines.
  - Quick check question: How many different types of nodes and edges are used in the heterogeneous graph construction?

## Architecture Onboarding

- Component map: Text Encoder → Heterogeneous Graph Construction → Heterogeneous Label-wise Attention → Group-wise Training → Classification Layer
- Critical path: Input text → Text Encoder → HLA mechanism → Classification → Output diagnosis probabilities
- Design tradeoffs: Larger language models (LLMs) have better few-shot abilities but are resource-intensive; smaller models with DKEC can achieve comparable performance while being more deployable
- Failure signatures: Poor performance on tail labels indicates group-wise training issues; failure to incorporate domain knowledge suggests HLA mechanism problems; overall poor performance could indicate text encoder issues
- First 3 experiments:
  1. Evaluate baseline model (e.g., TinyClinicalBERT) on tail labels to establish performance without DKEC
  2. Apply HLA mechanism alone to assess improvement from domain knowledge incorporation
  3. Apply group-wise training alone to assess improvement from data imbalance mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DKEC scale when applied to even larger medical knowledge graphs with more diverse entity types and relations?
- Basis in paper: [explicit] The paper mentions constructing a heterogeneous graph from medical guidelines, but does not explore scaling to larger or more diverse knowledge graphs.
- Why unresolved: The evaluation focuses on a specific heterogeneous graph construction method and does not test the limits of graph size or diversity.
- What evidence would resolve it: Experiments comparing DKEC performance on increasingly larger and more diverse medical knowledge graphs, measuring impact on tail label performance.

### Open Question 2
- Question: Can the group-wise training method be extended to handle more complex grouping criteria beyond semantic similarity and pre-defined logic rules?
- Basis in paper: [explicit] The paper proposes three criteria for group-wise training but does not explore more sophisticated grouping strategies.
- Why unresolved: The current grouping method relies on simple similarity thresholds and rule-based post-processing, which may not capture all relevant label relationships.
- What evidence would resolve it: Comparison of DKEC performance using alternative grouping methods (e.g., learned embeddings, clustering algorithms) on the same datasets.

### Open Question 3
- Question: How does DKEC perform on medical datasets with different label distribution characteristics, such as those with fewer head labels or more balanced distributions?
- Basis in paper: [inferred] The evaluation focuses on datasets with long-tail label distributions, but does not test DKEC on datasets with different characteristics.
- Why unresolved: The paper does not explore how DKEC's performance generalizes to datasets with varying label distribution properties.
- What evidence would resolve it: Experiments applying DKEC to medical datasets with different label distribution characteristics and comparing performance across these datasets.

## Limitations
- Limited external validation of the proposed heterogeneous graph and label-wise attention mechanisms in medical diagnosis prediction contexts
- Lack of detailed ablation studies to isolate the contribution of each component
- Novel combination of techniques in this specific domain with minimal direct supporting evidence in the corpus

## Confidence
- Medium: The empirical results demonstrate significant improvements over baseline methods, particularly for few-shot classes, but the novel combination of techniques and limited external validation prevent High confidence.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of heterogeneous graph knowledge incorporation versus group-wise training on overall performance.

2. Validate the approach on additional medical datasets from different clinical domains to assess generalizability beyond emergency care reports and MIMIC III.

3. Perform error analysis on misclassified samples to identify failure modes and determine whether they stem from knowledge graph limitations, attention mechanism issues, or data imbalance challenges.