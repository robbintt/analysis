---
ver: rpa2
title: Lexically-Accelerated Dense Retrieval
arxiv_id: '2307.16779'
source_url: https://arxiv.org/abs/2307.16779
tags:
- retrieval
- ladr
- dense
- documents
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LADR (Lexically-Accelerated Dense Retrieval),
  a simple-yet-effective technique that improves the efficiency of existing dense
  retrieval models without compromising on retrieval effectiveness. LADR uses lexical
  retrieval techniques to seed a dense retrieval exploration that uses a document
  proximity graph.
---

# Lexically-Accelerated Dense Retrieval

## Quick Facts
- arXiv ID: 2307.16779
- Source URL: https://arxiv.org/abs/2307.16779
- Authors: 
- Reference count: 40
- Key outcome: Establishes new dense retrieval effectiveness-efficiency Pareto frontier using lexical seeding of dense exploration on document proximity graphs

## Executive Summary
This paper introduces LADR (Lexically-Accelerated Dense Retrieval), a technique that improves the efficiency of dense retrieval models by using BM25 to retrieve seed documents which are then expanded through exploration of their document proximity graph neighbors. LADR achieves performance comparable to exhaustive search while requiring only single-CPU deployment, eliminating the need for expensive hardware accelerators. The method explores two variants: a proactive approach that expands all seed document neighbors, and an adaptive approach that iteratively expands only the most promising neighborhoods until convergence.

## Method Summary
LADR uses BM25 with BlockMax-WAND optimization to efficiently retrieve a small set of seed documents that contain query terms. These seeds are then expanded by exploring their k nearest neighbors in a pre-computed document proximity graph based on dense vector similarity. Two exploration strategies are proposed: Proactive LADR expands all seed neighbors in a single pass, while Adaptive LADR iteratively scores and expands only the top-c documents until convergence or budget exhaustion. The method is evaluated across multiple dense retrieval models (TAS-B, TCT-ColBERT-HNP) on MS MARCO passage corpus and TREC DL 2019/2020 datasets.

## Key Results
- Achieves precision and recall on par with exhaustive search while maintaining ~8ms query latency on single CPU
- Establishes new effectiveness-efficiency Pareto frontier among approximate k nearest neighbor techniques
- Reduces deployment costs by eliminating need for GPU acceleration while maintaining state-of-the-art retrieval performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical seed documents provide a strong initial recall baseline that complements dense retrieval.
- Mechanism: BM25 efficiently retrieves documents containing query terms, leveraging decades of optimization and the clustering hypothesis.
- Core assumption: Documents matching query terms lexically are likely relevant, and their dense vector neighbors are also likely relevant.
- Evidence anchors: [abstract]: "LADR uses lexical retrieval techniques to seed a dense retrieval exploration"; [section]: "We posit that this is due to two reasons. First, lexical signals are inherently valuable in many textual search tasks"
- Break condition: If BM25 recall is very low for a query, the seed set will be poor and dense exploration will not recover effectively.

### Mechanism 2
- Claim: Exploring document proximity graph neighbors efficiently expands recall beyond lexical matches.
- Mechanism: After obtaining seed documents via BM25, LADR explores their k nearest neighbors in dense vector space, retrieving documents without lexical overlap but semantic similarity.
- Core assumption: Dense vector similarity captures semantic relevance beyond lexical overlap, and the document proximity graph accurately reflects these relationships.
- Evidence anchors: [abstract]: "LADR uses lexical retrieval techniques to seed a dense retrieval exploration that uses a document proximity graph"; [section]: "LADR then explores the neighboring documents in the dense retrieval model's semantic space, allowing for documents to be retrieved that do not contain lexical matches with the query"
- Break condition: If the document proximity graph is poorly constructed or dense vectors don't capture true semantic relationships, neighbor exploration will not improve recall.

### Mechanism 3
- Claim: Adaptive exploration selectively allocates computational budget to the most promising document neighborhoods.
- Mechanism: Instead of exploring all seed document neighbors, Adaptive LADR iteratively scores top-c results and explores only their neighbors, repeating until convergence or budget exhaustion.
- Core assumption: Highest-scoring documents from current iteration are more likely to lead to relevant neighbors than lower-scoring documents.
- Evidence anchors: [abstract]: "an adaptive approach that selectively searches the documents with the highest estimated relevance in an iterative fashion"; [section]: "Adaptive LADR... iteratively scores neighbors of documents in the top ð‘ documents until they converge"
- Break condition: If top-c documents are not actually most promising, adaptive strategy will waste budget on unproductive neighborhoods.

## Foundational Learning

- Concept: Document proximity graphs and nearest neighbor search
  - Why needed here: LADR relies on pre-computed document proximity graphs to efficiently explore semantically similar documents without exhaustive search
  - Quick check question: How does LADR compute the document proximity graph, and what similarity measure does it use?

- Concept: Approximate k-nearest neighbor (ANN) search methods
  - Why needed here: LADR is compared against and competes with ANN methods like HNSW, IVF, and ScaNN in terms of effectiveness-efficiency trade-offs
  - Quick check question: What are the key differences between HNSW, IVF, and ScaNN in terms of their search strategies and trade-offs?

- Concept: BM25 and inverted index optimization
  - Why needed here: LADR uses BM25 over a BlockMax-WAND index to efficiently retrieve seed documents, leveraging decades of optimization
  - Quick check question: How does BlockMax-WAND improve upon standard BM25 query processing in terms of efficiency?

## Architecture Onboarding

- Component map: BM25 index with BlockMax-WAND -> Pre-computed document proximity graph -> Dense retrieval model -> LADR exploration algorithm -> Scoring function

- Critical path:
  1. BM25 query execution to retrieve seed documents
  2. Neighbor retrieval from document proximity graph
  3. Dense vector scoring of seed + neighbor documents
  4. Ranking and output of final results

- Design tradeoffs:
  - Storage vs. latency: Larger k in proximity graph improves recall but increases storage requirements
  - Exploration strategy: Proactive explores all seed neighbors (bounded cost), adaptive explores selectively (potentially unbounded but more efficient)
  - Seed set size n: Larger n improves recall but increases initial BM25 cost

- Failure signatures:
  - Low recall despite high n and k: Proximity graph may be poorly constructed or dense vectors may not capture semantic relationships
  - High latency despite low n and k: BM25 index may be inefficient or proximity graph may have high overlap between neighbors
  - Poor nDCG: Seed documents may not be relevant or dense scoring may not align with true relevance

- First 3 experiments:
  1. Verify BM25 seed retrieval effectiveness on a small dataset with known relevance judgments
  2. Test proximity graph construction and neighbor retrieval for a sample of documents
  3. Compare proactive vs. adaptive LADR on a small query set to understand trade-offs in effectiveness and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the number of neighbors (k), seed set size (n), and exploration depth (c) for different latency requirements?
- Basis in paper: [explicit] Section 5.3 discusses the effects of varying these parameters on effectiveness and efficiency.
- Why unresolved: The paper shows that these parameters are complementary up to a point, but does not provide a definitive formula or strategy for determining the optimal balance for specific latency requirements.
- What evidence would resolve it: Empirical studies across a wider range of datasets and latency requirements, potentially with machine learning models to predict optimal parameter settings.

### Open Question 2
- Question: How would training dense retrieval models with an intra-document similarity objective impact the effectiveness of LADR?
- Basis in paper: [inferred] Section 6 suggests this as a future direction, implying that current models are trained solely on query-document similarity.
- Why unresolved: This approach has not been tested, and its potential benefits and challenges are unknown.
- What evidence would resolve it: Comparative studies of LADR performance with models trained with and without an intra-document similarity objective.

### Open Question 3
- Question: Are there smarter exploration strategies than the proactive and adaptive variants of LADR?
- Basis in paper: [explicit] Section 6 mentions that while proactive and adaptive LADR are simple and effective, there likely exist smarter exploration strategies.
- Why unresolved: The paper does not explore or propose specific alternative strategies.
- What evidence would resolve it: Development and testing of new LADR variants with improved exploration strategies, comparing their performance to the current variants.

## Limitations

- Hardware Dependency: The reported 8ms latency target is achieved on specific hardware configurations that are not fully specified, creating uncertainty about reproducibility across different setups.
- Seed Set Quality Assumption: Effectiveness heavily depends on BM25's ability to retrieve relevant seed documents, which may not hold for all query types or domains where lexical matches don't correlate with semantic relevance.
- Parameter Sensitivity: While demonstrated across multiple models, the sensitivity of key parameters (n, k, c) to different datasets and domains is not extensively explored, raising questions about robustness.

## Confidence

- High Confidence: The core mechanism of using lexical seeds to bootstrap dense exploration is well-supported by empirical results and aligns with established IR principles like the clustering hypothesis.
- Medium Confidence: The adaptive variant's selective exploration strategy shows promise but relies on assumptions about correlation between current top scores and future relevance discovery.
- Low Confidence: Claims about LADR's superiority in single-CPU deployment scenarios lack comprehensive comparison with other single-CPU ANN methods.

## Next Checks

1. **Cross-Hardware Validation**: Reproduce LADR's performance on different hardware configurations (varying CPU specifications, GPU acceleration) to verify the claimed 8ms latency target and understand hardware dependencies in the effectiveness-efficiency trade-off.

2. **Domain Transferability Test**: Apply LADR to datasets from different domains (e.g., medical literature, legal documents, social media) where BM25 performance may vary significantly from web search, testing the robustness of the lexical-seed assumption across diverse retrieval scenarios.

3. **Parameter Sensitivity Analysis**: Conduct a systematic ablation study varying n, k, and c parameters across different query types and dataset characteristics to understand LADR's sensitivity to parameter choices and identify optimal configuration strategies for different retrieval scenarios.