---
ver: rpa2
title: 'XAI meets Biology: A Comprehensive Review of Explainable AI in Bioinformatics
  Applications'
arxiv_id: '2312.06082'
source_url: https://arxiv.org/abs/2312.06082
tags:
- data
- learning
- bioinformatics
- deep
- explainable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reviews Explainable AI (XAI) methods applied to bioinformatics,
  addressing the interpretability challenges of deep learning models in analyzing
  complex biological data. It introduces both model-agnostic (e.g., SHAP, LIME, LRP)
  and model-specific (e.g., Grad-CAM, attention scores, self-explainable networks)
  techniques across four bioinformatics domains: sequence analysis, structural analysis,
  gene expression/genomics, and bioimaging.'
---

# XAI meets Biology: A Comprehensive Review of Explainable AI in Bioinformatics Applications

## Quick Facts
- arXiv ID: 2312.06082
- Source URL: https://arxiv.org/abs/2312.06082
- Reference count: 20
- This paper reviews Explainable AI (XAI) methods applied to bioinformatics, addressing the interpretability challenges of deep learning models in analyzing complex biological data.

## Executive Summary
This comprehensive review examines how Explainable AI methods can enhance interpretability in bioinformatics applications. The paper systematically analyzes both model-agnostic (SHAP, LIME, LRP) and model-specific (Grad-CAM, attention scores, self-explainable networks) XAI techniques across four key bioinformatics domains: sequence analysis, structural analysis, gene expression/genomics, and bioimaging. Through case studies ranging from kinase-specific phosphorylation prediction to cancer classification and medical image diagnosis, the review identifies critical limitations of current approaches and calls for developing domain-specific XAI tools that integrate biological knowledge while ensuring clinical applicability.

## Method Summary
The review employs a systematic literature analysis approach, categorizing XAI methods based on their compatibility with different deep learning architectures and their effectiveness across various bioinformatics data types. The methodology involves examining case studies of XAI applications in sequence analysis (DNA, RNA, protein sequences), structural analysis (protein 3D structures), gene expression/genomics, and bioimaging. The review evaluates both the technical implementation of XAI methods and their biological interpretability, identifying patterns of successful applications and documenting limitations in current approaches.

## Key Results
- XAI methods show promise in identifying biologically meaningful patterns in kinase-specific phosphorylation prediction and protein binding site identification
- Current XAI approaches face limitations when applied to biological data, including lack of positional importance consideration and difficulty handling discrete sequence data
- There is a critical need for developing domain-specific XAI tools that integrate biological knowledge and establish standardized validation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP and Grad-CAM provide complementary insights into model decisions by capturing global feature importance and local decision boundaries respectively.
- Mechanism: SHAP uses cooperative game theory to quantify each feature's marginal contribution to model output across all feature combinations, while Grad-CAM uses gradient signals to highlight spatial regions that most influence CNN predictions. This dual approach validates findings across both feature-level and spatial-level explanations.
- Core assumption: Biological data contains both global patterns (gene pathways, sequence motifs) and local patterns (specific binding sites, mutation hotspots) that require different explanation granularities.
- Evidence anchors:
  - [abstract] "We introduce both model-agnostic (e.g., SHAP, LIME, LRP) and model-specific (e.g., Grad-CAM, attention scores, self-explainable networks) techniques"
  - [section] "SHAP values were assigned to histone-associated training data sets that match with a previously known binding preference of the histone subunits"
  - [corpus] Weak - corpus contains papers on XAI but none specifically validating SHAP-Grad-CAM complementarity in bioinformatics
- Break Condition: The assumption breaks if biological features are inherently inseparable into global vs local components, or if gradient-based methods fail due to vanishing gradients in deep networks.

### Mechanism 2
- Claim: Attention-based XAI methods learn biologically meaningful patterns without explicit supervision by capturing sequence-context dependencies.
- Mechanism: Attention mechanisms compute soft alignments between query and key vectors, allowing models to discover long-range dependencies and positional relationships in biological sequences. The attention weights directly indicate which sequence positions contribute most to predictions.
- Core assumption: Biological systems exhibit hierarchical structure where local sequence motifs interact with distant regulatory elements in predictable ways that attention mechanisms can capture.
- Evidence anchors:
  - [section] "In the Phosformer model Zhou et al. [2023] for kinase-specific phosphorylation prediction, the attention values are utilized to validate the model's ability to recognize protein substrate specificity motifs"
  - [section] "AttentionSplice Y AN et al. [2022] predicts splice acceptor and donor sites from protein-coding gene DNA sequences... The attention layer to assign importance to each nucleotide in the input sequence"
  - [corpus] Weak - corpus contains general XAI papers but none specifically validating attention mechanisms in biological sequence contexts
- Break Condition: This mechanism fails when biological interactions are too complex for linear attention mechanisms, or when attention weights are dominated by noise rather than true biological signals.

### Mechanism 3
- Claim: Knowledge-primed neural networks (KPNNs) provide inherently interpretable models by constraining architecture to match known biological pathways.
- Mechanism: KPNNs assign biological meaning to each neuron (e.g., specific genes, pathways, or molecular functions), making model decisions directly interpretable through the relationships between activated neurons. This eliminates the need for post-hoc explanation methods.
- Core assumption: Biological knowledge can be accurately encoded into neural network architecture such that each neuron's activation corresponds to a specific biological entity or process.
- Evidence anchors:
  - [section] "DeepGONet, is a self-explainable deep fully-connected neural network, that is constrained by prior biological knowledge from GO"
  - [section] "Nodes in the neural network correspond to genes and pathways, highlighting the nonlinear effects of these factors on cancer patient survival outcomes"
  - [corpus] Weak - corpus contains papers on explainable AI but none specifically discussing knowledge-primed architectures in biological contexts
- Break Condition: The mechanism breaks when biological knowledge is incomplete or incorrect, leading to misleading interpretations, or when the complexity of biological systems exceeds the representational capacity of the constrained architecture.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values
  - Why needed here: Understanding how SHAP calculates feature contributions is essential for interpreting XAI results in bioinformatics applications
  - Quick check question: What does a positive SHAP value indicate about a feature's contribution to a model's prediction?

- Concept: Gradient-based saliency mapping
  - Why needed here: Grad-CAM and related methods rely on backpropagation of gradients to identify important image regions
  - Quick check question: How does Grad-CAM differ from simple gradient magnitude visualization in terms of localization quality?

- Concept: Attention mechanism mathematics
  - Why needed here: Attention-based XAI methods depend on understanding how query-key-value transformations produce interpretable weights
  - Quick check question: What role does the scaling factor (1/âˆšdk) play in attention score computation?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (biological sequences, structures, images) -> Deep learning model architecture (CNNs, RNNs, Transformers) -> XAI explanation module (SHAP, Grad-CAM, attention visualization) -> Validation and interpretation layer (biological knowledge integration)

- Critical path:
  1. Preprocess biological data into appropriate format
  2. Train deep learning model on prediction task
  3. Apply XAI method to generate explanations
  4. Validate explanations against biological knowledge
  5. Iterate model architecture based on explanation insights

- Design tradeoffs:
  - Model complexity vs interpretability: More complex models may capture better patterns but are harder to explain
  - Global vs local explanations: Different XAI methods provide different levels of explanation granularity
  - Computational cost: Some XAI methods (like SHAP) are computationally expensive compared to gradient-based methods

- Failure signatures:
  - Explanations that contradict established biological knowledge
  - XAI methods producing similar explanations for very different predictions
  - Attention weights that are uniformly distributed or concentrated on irrelevant features
  - SHAP values that don't correlate with feature importance in controlled experiments

- First 3 experiments:
  1. Implement Grad-CAM on a simple CNN trained to classify protein structures, validate against known structural motifs
  2. Apply SHAP to a gene expression classifier, compare with pathway enrichment analysis results
  3. Test attention-based explanations on a sequence model, verify learned patterns match known regulatory elements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific domain-specific XAI methods should be developed for biological sequence analysis to account for positional importance and long-range residue interactions?
- Basis in paper: [explicit] The paper identifies limitations of current SHAP and Grad-CAM methods when applied to biological sequences, noting that SHAP does not account for positional importance and Grad-CAM struggles with discrete data and long-range interactions.
- Why unresolved: Current XAI methods are adaptations of techniques designed for other domains (text, images) and do not fully capture the unique characteristics of biological sequences.
- What evidence would resolve it: Development and validation of XAI methods specifically designed for biological sequences that demonstrate improved accuracy in identifying functionally important motifs and residues compared to existing methods.

### Open Question 2
- Question: How can XAI methods be standardized and benchmarked across different bioinformatics applications to ensure reproducibility and facilitate comparisons?
- Basis in paper: [explicit] The paper highlights the need for standardization and benchmarking of XAI methods, particularly in clinical settings where data privacy and ethical considerations are crucial.
- Why unresolved: Different bioinformatics applications have unique data types and challenges, making it difficult to establish universal standards for XAI validation and comparison.
- What evidence would resolve it: Creation of standardized evaluation metrics and benchmark datasets for XAI methods across various bioinformatics domains, along with validation studies demonstrating improved reproducibility and comparability.

### Open Question 3
- Question: What are the most effective ways to integrate biological expert knowledge into XAI methods to improve interpretability and trustworthiness of predictions?
- Basis in paper: [explicit] The paper emphasizes the importance of developing XAI methods that consider unique aspects of biological data and integrate expert knowledge to provide clear and accurate explanations.
- Why unresolved: Balancing the complexity of biological systems with the need for interpretable explanations remains a significant challenge in XAI for bioinformatics.
- What evidence would resolve it: Development and validation of XAI methods that incorporate domain-specific knowledge bases and demonstrate improved alignment between model explanations and biological understanding.

## Limitations

- XAI methods are not specifically designed for biological data, leading to limited interpretability or inaccurate explanations in certain bioinformatics applications
- The complexity of biological systems makes it challenging to validate XAI explanations, requiring additional domain expertise and careful consideration of biological context
- Current XAI approaches lack standardized validation metrics and benchmarking across different bioinformatics domains

## Confidence

- **High confidence**: The review accurately catalogs existing XAI methods and their applications in bioinformatics domains
- **Medium confidence**: Claims about XAI method effectiveness are supported by case studies but lack systematic validation across diverse biological problems
- **Medium confidence**: The identified limitations (lack of biological tailoring, validation metrics, and clinical integration) reflect genuine gaps in current practice

## Next Checks

1. **Benchmarking Study**: Compare multiple XAI methods on standardized biological datasets with known ground truth to assess which methods consistently recover biologically meaningful patterns

2. **Clinical Validation**: Partner with domain experts to test whether XAI-generated hypotheses lead to novel biological discoveries or improved clinical decision-making outcomes

3. **Robustness Analysis**: Evaluate XAI method stability under realistic biological data conditions (missing values, class imbalance, batch effects) to identify failure modes specific to bioinformatics applications