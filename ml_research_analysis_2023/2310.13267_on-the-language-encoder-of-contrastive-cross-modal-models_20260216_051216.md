---
ver: rpa2
title: On the Language Encoder of Contrastive Cross-modal Models
arxiv_id: '2310.13267'
source_url: https://arxiv.org/abs/2310.13267
tags:
- training
- language
- sentence
- clip
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the language encoder component of contrastive
  cross-modal models like CLIP and CLAP. It systematically evaluates how unsupervised
  and supervised sentence embedding training affects language encoder quality and
  cross-modal task performance.
---

# On the Language Encoder of Contrastive Cross-modal Models

## Quick Facts
- arXiv ID: 2310.13267
- Source URL: https://arxiv.org/abs/2310.13267
- Authors: 
- Reference count: 40
- Key outcome: Unsupervised sentence embedding training improves VL task performance by increasing text-space uniformity, while supervised training improves language encoder quality but not cross-modal tasks.

## Executive Summary
This paper systematically investigates how sentence embedding training affects the language encoder component of contrastive cross-modal models like CLIP and CLAP. The authors find that unsupervised sentence embedding training (specifically SimCSE) improves text-space uniformity, which benefits vision-language cross-modal tasks like zero-shot retrieval and classification. In contrast, supervised sentence embedding training improves language encoder quality on intrinsic tasks but doesn't transfer to cross-modal performance. The study also reveals that CyCLIP responds more favorably to sentence embedding training than standard CLIP, likely due to its explicit optimization for geometry consistency between text and image spaces.

## Method Summary
The paper evaluates language encoder quality and cross-modal task performance using various training objectives: cross-modal contrastive loss (Lcontra), geometry consistency losses (LC-cyclic, LI-cyclic for CyCLIP), and sentence embedding losses (Ls for unsupervised SimCSE, Ln for supervised NLI training). VL models use ResNet-50 + Transformer encoders trained from scratch on CC3M data, while AL models use RoBERTa-base + HTSAT encoders. The authors systematically compare models with and without sentence embedding training, measuring cross-modal alignment, text-space uniformity, and task performance across zero-shot retrieval, classification, and language encoder quality benchmarks.

## Key Results
- Unsupervised sentence embedding training improves VL task performance (MSCOCO/Flick30K retrieval, CIFAR/ImageNet classification) while decreasing cross-modal alignment but increasing text-space uniformity
- Supervised sentence embedding training improves language encoder quality on SentEval but doesn't transfer to cross-modal tasks
- CyCLIP benefits more from sentence embedding training than CLIP due to its geometry consistency objectives
- AL pretraining shows noisy results with limited improvements from sentence embedding training, likely due to data scarcity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unsupervised sentence embedding training improves text-space uniformity, which improves language encoder quality and aids in cross-modal tasks like zero-shot retrieval and classification.
- Mechanism: Sentence embedding training (specifically SimCSE) uses contrastive learning to make caption representations more uniformly distributed in the text space. This uniform distribution increases the effective capacity of the representation space, allowing better discrimination between different semantic concepts.
- Core assumption: A more uniform text representation space improves the quality of the language encoder's semantic representations.
- Evidence anchors:
  - [abstract] "We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity"
  - [section 5.2] "We observe that unsupervised sentence embedding training trades cross-modal alignment for improving the text space uniformity"
  - [corpus] Weak - no direct evidence in related papers, though LIP-Loc mentions contrastive learning for cross-modal alignment
- Break condition: If the increased text-space uniformity comes at too high a cost to cross-modal alignment, the benefits may be negated or reversed.

### Mechanism 2
- Claim: CyCLIP's explicit optimization for geometry consistency between text and image spaces makes it more responsive to improvements in caption representation quality.
- Mechanism: CyCLIP adds LC-cyclic and LI-cyclic losses that explicitly enforce symmetry and consistency between cross-modal similarity matrices. When caption representations become more uniform and semantically meaningful through sentence embedding training, these consistency losses can better enforce alignment between the text and image spaces.
- Core assumption: The additional geometry consistency objectives in CyCLIP create a more sensitive training signal that amplifies improvements from better caption representations.
- Evidence anchors:
  - [abstract] "CyCLIP explicitly optimizes for geometry consistency between the text and image representation spaces"
  - [section 2] "CyCLIP has improved CLIP by additionally optimizing for improved representation space geometry, such that the image and text spaces are more consistent with each other"
  - [corpus] Moderate - LIP-Loc discusses cross-modal consistency but doesn't directly address CyCLIP's specific approach
- Break condition: If the geometry consistency objectives create too rigid a constraint, they may prevent the model from adapting to improved caption representations.

### Mechanism 3
- Claim: Supervised sentence embedding training (using NLI datasets) improves language encoder quality but doesn't transfer to cross-modal tasks because it overly enforces text-space uniformity at the expense of cross-modal alignment.
- Mechanism: Supervised training with NLI datasets creates very uniform text representations by explicitly modeling semantic relationships like entailment and contradiction. While this improves the language encoder's ability to distinguish semantic relationships in text, it creates representations that are too uniform to align well with image representations.
- Core assumption: The semantic relationships in NLI datasets (entailment, contradiction) are different from the visual-semantic relationships needed for cross-modal alignment.
- Evidence anchors:
  - [abstract] "Supervised sentence embedding training improves language encoder quality, but the benefit does not necessarily transfer to VL tasks"
  - [section 5.2] "Supervised sentence embedding training overly focuses on text space uniformity while the VL space alignment deteriorates"
  - [corpus] Weak - no direct evidence in related papers about this specific tradeoff
- Break condition: If the NLI training data better matches the visual-semantic relationships in the pretraining data, the transfer to cross-modal tasks might improve.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The paper's core mechanism relies on contrastive learning for both cross-modal alignment (CLIP/CyCLIP objectives) and sentence embedding training (SimCSE).
  - Quick check question: What is the difference between the cross-modal contrastive loss Lcontra and the in-modal contrastive loss Ls in sentence embedding training?

- Concept: Representation space analysis (alignment and uniformity)
- Why needed here: The paper's key insight is that improvements in sentence embedding training trade off between text-space uniformity and cross-modal alignment, which requires understanding how to measure and analyze these properties.
- Quick check question: How do you compute the alignment score Lalign and uniformity scores LI,uniform and LT,uniform from the representation spaces?

- Concept: Vision-language pretraining architectures
  - Why needed here: Understanding the specific architectures (CLIP vs CyCLIP) and their training objectives is crucial for understanding why CyCLIP responds better to sentence embedding training.
  - Quick check question: What are the specific differences between CLIP's training objective and CyCLIP's additional objectives LC-cyclic and LI-cyclic?

## Architecture Onboarding

- Component map:
  - Image encoder: ResNet-50 (from scratch) or pretrained HTSAT (for AL)
  - Language encoder: Transformer (from scratch) or pretrained RoBERTa-base (for AL)
  - Training objectives: Lcontra (cross-modal), LC-cyclic and LI-cyclic (geometry consistency), Ls (unsupervised sentence embedding), Ln (supervised sentence embedding)
  - Datasets: CC3M for VL, Clotho/AudioCaps for AL

- Critical path:
  1. Train image and language encoders from scratch with cross-modal contrastive learning
  2. Add sentence embedding training objectives (Ls or Ln)
  3. Evaluate on VL retrieval tasks (MSCOCO, Flickr30K)
  4. Analyze representation spaces for alignment and uniformity
  5. Evaluate language encoder quality on SentEval

- Design tradeoffs:
  - From scratch vs. pretrained encoders: From scratch allows more control but requires more data and compute; pretrained encoders help with data scarcity but may be harder to adapt
  - Unsupervised vs. supervised sentence embedding: Unsupervised improves cross-modal tasks but supervised improves language encoder quality more
  - CyCLIP vs. CLIP: CyCLIP responds better to sentence embedding training but is more complex to train

- Failure signatures:
  - If retrieval performance degrades after adding sentence embedding training, check if cross-modal alignment has been sacrificed for text-space uniformity
  - If language encoder quality doesn't improve on SentEval after supervised training, the NLI supervision may not be transferring well
  - If AL pretraining shows noisy results, data scarcity is likely the limiting factor

- First 3 experiments:
  1. Train CLIP from scratch on CC3M with and without unsupervised sentence embedding training (Ls), evaluate on MSCOCO retrieval
  2. Train CyCLIP from scratch on CC3M with all objectives (Lcontra, LC-cyclic, LI-cyclic, Ls), analyze representation space uniformity vs. alignment
  3. Train AL model from scratch on Clotho with and without sentence embedding training, compare to using pretrained encoders

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of sentence embedding training vary across different types of audio data (e.g., environmental sounds vs. music vs. speech)?
- Basis in paper: [explicit] The paper notes that sentence embedding training has less noticeable effects on audio-language pretraining compared to vision-language pretraining, and suggests this may be due to data scarcity. It also mentions that music modality experiments showed some positive effects.
- Why unresolved: The paper only briefly touches on music modality and doesn't provide a comprehensive comparison across different audio types.
- What evidence would resolve it: Systematic experiments comparing sentence embedding training effects across various audio domains with different dataset sizes and characteristics.

### Open Question 2
- Question: What is the optimal balance between cross-modal alignment and text-space uniformity when incorporating sentence embedding training?
- Basis in paper: [explicit] The paper shows that sentence embedding training improves text-space uniformity but decreases cross-modal alignment, suggesting a trade-off exists.
- Why unresolved: The paper doesn't explore how to optimize this trade-off or determine the ideal balance for different tasks.
- What evidence would resolve it: Experiments varying the weighting of alignment vs. uniformity objectives and measuring task performance across different scenarios.

### Open Question 3
- Question: How would pretraining language encoders on audio descriptions before cross-modal contrastive learning affect performance?
- Basis in paper: [inferred] The paper mentions that audio-language pretraining often leverages pretrained language encoders and suggests adapting them to the audio domain might be promising.
- Why unresolved: The paper doesn't explore this pretraining approach, only considering continued pretraining with existing encoders.
- What evidence would resolve it: Comparative experiments between different pretraining strategies for language encoders in audio-language models.

### Open Question 4
- Question: What causes the noisy results in audio-language pretraining when using sentence embedding training?
- Basis in paper: [explicit] The paper notes that improvements from sentence embedding training in audio-language tasks are "noisy and less noticeable" and suggests data scarcity as a possible cause.
- Why unresolved: The paper identifies data scarcity as a potential cause but doesn't systematically investigate other possible factors.
- What evidence would resolve it: Detailed analysis of model behavior across different dataset sizes and characteristics in audio-language pretraining.

## Limitations

- Data scarcity in audio-language pretraining: AL experiments use significantly less data (6K-50K pairs) than VL pretraining (1.3M pairs), making it difficult to draw strong conclusions about sentence embedding training effects in AL tasks.
- Architecture constraints: VL experiments use ResNet-50 encoders trained from scratch, which may not represent state-of-the-art performance, and the analysis of CyCLIP's advantages is speculative.
- Limited evaluation scope: The paper focuses primarily on zero-shot evaluation without extensively exploring few-shot or fine-tuned scenarios.

## Confidence

- High confidence: The core finding that unsupervised sentence embedding training improves VL task performance while supervised training improves language encoder quality but not cross-modal tasks.
- Medium confidence: The analysis of the uniformity-alignment tradeoff, though the causal relationship could be influenced by other training factors.
- Low confidence: The explanation for why CyCLIP specifically benefits more from sentence embedding training, as this is based on observed performance patterns rather than rigorous architectural analysis.

## Next Checks

1. Scale AL pretraining experiments: Repeat AL pretraining experiments using larger datasets (e.g., AudioSet) to determine if the noisy results are due to data scarcity, and compare the magnitude of sentence embedding training effects between VL and AL when both use sufficient data.

2. Ablation study on CyCLIP objectives: Perform controlled experiments isolating the LC-cyclic and LI-cyclic losses to determine their specific contribution to CyCLIP's responsiveness to sentence embedding training by training CyCLIP without these losses and measuring the impact on cross-modal alignment.

3. Extended evaluation scenarios: Test the trained models on few-shot and fine-tuning scenarios beyond zero-shot evaluation to reveal whether the improved language encoder quality from supervised training translates to better adaptation capabilities when labeled data becomes available.