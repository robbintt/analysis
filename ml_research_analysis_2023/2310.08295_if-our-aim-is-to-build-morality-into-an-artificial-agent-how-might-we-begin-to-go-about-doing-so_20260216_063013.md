---
ver: rpa2
title: If our aim is to build morality into an artificial agent, how might we begin
  to go about doing so?
arxiv_id: '2310.08295'
source_url: https://arxiv.org/abs/2310.08295
tags:
- moral
- such
- have
- could
- morality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper discusses approaches to building moral agents and addresses
  the challenges involved in doing so. The authors propose a hybrid approach combining
  top-down and bottom-up methods, along with a hierarchical structure for integrating
  different moral paradigms.
---

# If our aim is to build morality into an artificial agent, how might we begin to go about doing so?

## Quick Facts
- arXiv ID: 2310.08295
- Source URL: https://arxiv.org/abs/2310.08295
- Reference count: 22
- Key outcome: The paper discusses approaches to building moral agents and proposes a hybrid approach combining top-down and bottom-up methods, along with a hierarchical structure for integrating different moral paradigms.

## Executive Summary
This paper addresses the challenge of building moral agents by proposing a hybrid approach that combines top-down rule-based systems with bottom-up learning from moral exemplars. The authors argue that cultural variations in moral values make purely top-down approaches insufficient, and instead suggest using inverse reinforcement learning to train agents by observing human moral behaviors. They emphasize the importance of emotion and sentience in moral decision-making and discuss the role of governance and policy in ensuring ethical AI development.

## Method Summary
The paper proposes a three-pronged approach to building moral AI: (1) a hybrid design combining top-down rule-based systems with bottom-up inverse reinforcement learning from moral exemplars, (2) a hierarchical structure for integrating different moral paradigms (virtue ethics, deontology, and utilitarianism), and (3) the use of regulatory sandboxes for safe testing and refinement of moral AI systems. The authors suggest that this combination can address cultural variability in moral values while maintaining consistency in ethical decision-making.

## Key Results
- Cultural variations in moral values make purely top-down approaches insufficient for moral AI
- Inverse reinforcement learning can be used to train AI systems by observing human moral exemplars
- A hierarchical structure combining different moral paradigms can enable flexible moral reasoning across contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hybrid approach combining top-down and bottom-up methods mitigates the cultural variability problem in moral AI.
- Mechanism: Top-down rules encode universal principles (e.g., deontology), while bottom-up inverse reinforcement learning learns nuanced behaviors from moral exemplars. The hybrid structure ensures both consistency and adaptability.
- Core assumption: Moral exemplars exist whose behaviors can be reliably observed and generalized.
- Evidence anchors:
  - [abstract] The authors argue that a purely top-down approach is insufficient due to cultural variations in moral values.
  - [section] "With the advent of apprenticeship learning, i.e. inverse reinforcement learning, it would be reasonable to try and develop similar artificial systems that would learn by viewing the behaviors of humans."
- Break condition: If no reliable set of moral exemplars can be identified, or if observed behaviors are too context-dependent to generalize.

### Mechanism 2
- Claim: Hierarchical integration of moral paradigms (virtue ethics, deontology, utilitarianism) enables flexible moral reasoning across contexts.
- Mechanism: Each paradigm handles a different level of moral decision-making: virtue ethics for character formation, deontology for rule enforcement, and utilitarianism for consequence evaluation. The hierarchy resolves conflicts by prioritization.
- Core assumption: A consistent ordering of moral paradigms can be established without violating the internal logic of each.
- Evidence anchors:
  - [abstract] "We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms."
  - [section] "We could use this as part of a hierarchical structure for building moral AI, in which we could combine choice aspects of different moral paradigms, such as virtue ethics, deontology and utilitarianism."
- Break condition: If the prioritization leads to systematic contradictions between paradigms, undermining moral coherence.

### Mechanism 3
- Claim: Regulatory sandboxes and governance frameworks provide a safe environment to test and refine moral AI before deployment.
- Mechanism: Controlled real-life environments allow iterative testing of AI moral decisions while enabling oversight, transparency, and accountability measures. Feedback loops improve both the AI and the governance framework.
- Core assumption: Governance bodies can effectively monitor and adjust AI behavior without stifling innovation.
- Evidence anchors:
  - [abstract] "We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI."
  - [section] "Various governments, including the UK, US and Japan, have leaned towards the use of 'regulatory sandboxes', defined as a testbed for innovation in a controlled real-life environment prior to launch."
- Break condition: If sandbox environments fail to simulate real-world complexity, or if governance becomes too rigid to allow meaningful innovation.

## Foundational Learning

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: IRL allows the AI to infer human moral goals by observing behavior, avoiding the need to encode every possible rule explicitly.
  - Quick check question: What is the key difference between IRL and standard reinforcement learning in the context of moral AI?

- Concept: Moral paradigms (virtue ethics, deontology, utilitarianism)
  - Why needed here: Understanding these paradigms is essential to design a hierarchical system that can integrate them effectively.
  - Quick check question: How does virtue ethics differ from deontology in terms of the source of moral guidance?

- Concept: Cultural variability in moral judgments
  - Why needed here: Recognizing cultural differences is crucial for avoiding the assumption that a single set of rules will work globally.
  - Quick check question: What did the Moral Machine experiment reveal about cross-cultural differences in moral decision-making?

## Architecture Onboarding

- Component map:
  - Data collection layer: Moral exemplars and their behavioral data
  - Learning layer: IRL to infer moral goals
  - Rule enforcement layer: Deontological constraints
  - Consequence evaluation layer: Utilitarian calculations
  - Oversight layer: Governance and transparency mechanisms
  - Feedback layer: Continuous refinement from sandbox testing

- Critical path:
  1. Identify and vet moral exemplars
  2. Collect behavioral data
  3. Train IRL model to infer moral goals
  4. Apply deontological filters to generated behaviors
  5. Evaluate consequences using utilitarian metrics
  6. Run in regulatory sandbox with oversight
  7. Iterate based on feedback

- Design tradeoffs:
  - Granularity vs. generalization: More detailed exemplar data improves learning but risks overfitting to specific contexts.
  - Rule strictness vs. adaptability: Tighter deontological rules ensure consistency but may limit flexibility in novel situations.
  - Transparency vs. performance: More interpretable models aid oversight but may sacrifice decision-making efficiency.

- Failure signatures:
  - AI produces outputs that satisfy all rules but are morally questionable (over-reliance on top-down).
  - AI learns behaviors that mimic exemplars but violate core ethical principles (misinterpretation of bottom-up).
  - Governance delays deployment indefinitely, preventing real-world testing (overly cautious sandboxing).

- First 3 experiments:
  1. Train an IRL model on a small, vetted dataset of moral exemplar behaviors; evaluate its ability to reproduce simple ethical decisions.
  2. Integrate a deontological rule filter into the IRL pipeline; test on edge cases where exemplar behavior conflicts with the rule.
  3. Deploy a simplified version in a regulatory sandbox with simulated stakeholders; measure decision-making time and stakeholder satisfaction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective methods for developing moral exemplars for training AI systems using inverse reinforcement learning?
- Basis in paper: [explicit] The paper proposes using inverse reinforcement learning to train AI systems by observing human moral exemplars, but acknowledges the need for careful consideration of which exemplars to use.
- Why unresolved: The paper identifies the importance of selecting appropriate moral exemplars but does not provide specific criteria or methods for identifying them.
- What evidence would resolve it: Research demonstrating successful identification and implementation of moral exemplars in AI training, along with evaluations of their effectiveness in producing ethical AI behavior.

### Open Question 2
- Question: How can we develop a universal set of 'rules' or 'duties' for AI ethics that can be applied across different cultures and societies?
- Basis in paper: [explicit] The paper highlights the influence of culture on moral values and the challenges of creating a global consensus on ethical AI principles.
- Why unresolved: The paper suggests that cultural variations in moral values make it difficult to implement a purely top-down approach to AI ethics, but does not provide a solution for creating universally applicable rules.
- What evidence would resolve it: Studies comparing the effectiveness of different approaches to AI ethics across various cultural contexts, along with proposals for creating a framework that can accommodate diverse moral perspectives.

### Open Question 3
- Question: What role should emotion and sentience play in the development of moral AI, and how can we incorporate these elements into AI systems?
- Basis in paper: [explicit] The paper discusses the debate surrounding the importance of emotion and sentience in moral decision-making and their potential inclusion in AI systems.
- Why unresolved: The paper raises questions about the benefits and challenges of incorporating emotion and sentience into AI systems but does not provide concrete methods for doing so.
- What evidence would resolve it: Research demonstrating successful integration of emotional and sentient elements into AI systems, along with evaluations of their impact on moral decision-making and overall system performance.

## Limitations
- The paper lacks concrete implementation details for key components of the proposed hybrid approach
- The hierarchical integration of moral paradigms remains underspecified, particularly regarding conflict resolution
- The proposed governance and regulatory sandbox approach faces practical implementation challenges and may not capture real-world complexity

## Confidence
- Hybrid approach effectiveness: **Medium** - The theoretical foundation is strong, but empirical validation is lacking
- Hierarchical paradigm integration: **Low** - The mechanism for combining paradigms is vaguely defined and untested
- Regulatory sandbox utility: **Medium** - Supported by existing government initiatives but practical implementation challenges remain

## Next Checks
1. Conduct a pilot study using a small dataset of moral exemplar behaviors to test the inverse reinforcement learning component and assess its ability to capture nuanced ethical decision-making
2. Design and implement a prototype system that demonstrates the hierarchical integration of two moral paradigms (e.g., deontology and utilitarianism) in a controlled environment
3. Develop a detailed evaluation framework for the regulatory sandbox approach, including specific metrics for measuring AI moral performance and stakeholder satisfaction across different cultural contexts