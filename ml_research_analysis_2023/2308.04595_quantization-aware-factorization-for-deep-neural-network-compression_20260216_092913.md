---
ver: rpa2
title: Quantization Aware Factorization for Deep Neural Network Compression
arxiv_id: '2308.04595'
source_url: https://arxiv.org/abs/2308.04595
tags:
- quantization
- tensor
- layers
- neural
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of compressing and quantizing deep
  neural networks for deployment on memory- and power-constrained mobile or embedded
  devices. The authors propose a method that jointly performs tensor decomposition
  and quantization of convolutional and fully-connected layers using an ADMM-based
  algorithm that finds tensor approximations directly with quantized factors.
---

# Quantization Aware Factorization for Deep Neural Network Compression

## Quick Facts
- arXiv ID: 2308.04595
- Source URL: https://arxiv.org/abs/2308.04595
- Reference count: 27
- Key outcome: ADMM-based method for joint tensor decomposition and quantization achieves better accuracy-efficiency trade-offs than post-training quantization, especially at lower bit-widths.

## Executive Summary
This paper proposes a method for compressing and quantizing deep neural networks using tensor decomposition with quantization-aware optimization. The approach uses Alternating Direction Method of Multipliers (ADMM) to directly optimize tensor factors under quantization constraints, rather than decomposing first and quantizing afterward. Experiments on ResNet18 show improved accuracy-efficiency trade-offs compared to state-of-the-art post-training quantization techniques, particularly at lower bit-widths. The method provides flexibility in achieving desired quality-performance trade-offs by varying factorization rank.

## Method Summary
The method jointly performs tensor decomposition and quantization of convolutional and fully-connected layers using an ADMM-based algorithm. Each convolutional layer is replaced with a sequence of smaller convolutions, with weights represented in lower precision formats (INT8/INT6/INT4) instead of FLOAT32. The ADMM algorithm minimizes a constrained least-squares objective that enforces factors to lie on a specified quantization grid. The approach uses CPD-EPC initialization to improve quantized reconstruction error and employs symmetric per-tensor MinMax quantization.

## Key Results
- ResNet18 accuracy drops from 69.76% (FP32) to 67.59% (INT4) using the proposed method
- Outperforms conventional tensor approximation followed by quantization
- Provides better accuracy-efficiency trade-offs compared to state-of-the-art post-training quantization techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADMM enables direct quantization-aware tensor factorization by enforcing quantized constraints during optimization.
- Mechanism: ADMM introduces auxiliary variables to decouple factorization and quantization, alternating between least-squares subproblem and quantization grid projection.
- Core assumption: The quantization grid constraint is approximately convex enough that ADMM's alternating updates lead to good solutions.
- Evidence anchors: Abstract mentions ADMM for CP decomposition with quantized factors; section describes alternating optimization with dual variable U.
- Break condition: Extremely coarse quantization grid or too low tensor rank may prevent meaningful convergence.

### Mechanism 2
- Claim: CPD-EPC initialization improves quantized reconstruction error compared to random initialization.
- Mechanism: CPD-EPC minimizes sensitivity of decomposition, leading to factors with smaller dynamic range that quantize better under uniform quantization.
- Core assumption: Factors with lower value range lead to smaller quantization error under symmetric uniform quantization.
- Evidence anchors: Section explains CPD-EPC minimizes Sensitivity resulting in factors with lower value range; abstract states better compression/accuracy tradeoff than conventional methods.
- Break condition: Initialization phase taking too many iterations may outweigh benefits in quantized accuracy.

### Mechanism 3
- Claim: Joint factorization and quantization allows better accuracy-efficiency trade-offs than successive factorization-then-quantization.
- Mechanism: Direct optimization over quantized factors prevents error accumulation from separate factorization and quantization steps.
- Core assumption: Errors from separate factorization and quantization steps are non-negligible and compound each other.
- Evidence anchors: Abstract notes conventional post-training quantization causes accuracy drop; section states better tradeoff than conventional tensor approximation followed by quantization.
- Break condition: Joint optimization becoming too computationally expensive may not justify benefits in deployment scenarios.

## Foundational Learning

- Concept: Canonical Polyadic (CP) decomposition
  - Why needed here: The method relies on decomposing convolutional layer weights into a sum of rank-one tensors for compression.
  - Quick check question: How is a 4D convolutional kernel reshaped and decomposed into three smaller convolutions using CP decomposition?

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM is used to solve the constrained optimization problem where factors must lie on a quantization grid.
  - Quick check question: What are the three alternating steps in ADMM when applied to a matrix factorization problem with quantization constraints?

- Concept: Symmetric uniform quantization
  - Why needed here: The method uses symmetric quantization (zero-centered grid) to reduce bit-width of factors while preserving model accuracy.
  - Quick check question: How is the scale parameter calculated for symmetric per-tensor MinMax quantization when the value distribution is symmetric?

## Architecture Onboarding

- Component map: Pre-trained CNN -> CP Decomposition with ADMM -> Compressed model with quantized factors -> Calibrated BatchNorm -> Evaluated on test set
- Critical path: 1) Initialize factors using CPD-EPC, 2) Run ADMM iterations to enforce quantization constraints, 3) Quantize activations and remaining layers, 4) Calibrate BatchNorm parameters
- Design tradeoffs:
  - Rank selection vs. accuracy: Higher rank preserves accuracy but reduces compression
  - Bit-width selection vs. efficiency: Lower bit-width saves memory but may degrade accuracy
  - Initialization strategy: CPD-EPC vs. random affects convergence and final quantized error
- Failure signatures:
  - Large accuracy drop after quantization → likely quantization grid too coarse or rank too low
  - Slow or non-convergent ADMM iterations → scaling issues in Gram matrix or poor initialization
  - Memory errors during factorization → rank set too high for available resources
- First 3 experiments:
  1. Apply the method to a single convolutional layer of ResNet18 with rank=2 and INT8 quantization; measure quantized reconstruction error.
  2. Compare CPD-EPC initialization vs. random initialization on the same layer; observe convergence behavior and final accuracy.
  3. Vary the bit-width (INT8, INT6, INT4) while keeping rank fixed; plot accuracy vs. bit-width to identify the optimal trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different tensor decomposition techniques (e.g., Tucker, Tensor Train) compared to CP decomposition on the accuracy and efficiency trade-off in joint factorization and quantization?
- Basis in paper: The paper focuses on CP decomposition but mentions other tensor approximations like Tucker or block term decompositions in related work.
- Why unresolved: The paper does not provide a comparative analysis of different tensor decomposition techniques in the context of joint factorization and quantization.
- What evidence would resolve it: Experiments comparing the proposed ADMM-based CP decomposition approach with other tensor decomposition techniques (e.g., Tucker, Tensor Train) in terms of accuracy and efficiency trade-off.

### Open Question 2
- Question: How does the choice of bit-width allocation for different layers affect the overall accuracy and efficiency of the compressed model?
- Basis in paper: The paper mentions that determining the best trade-off between bit-allocation and reduction rate is not trivial and presents ablation studies to choose between different configurations.
- Why unresolved: The paper does not provide a systematic method for choosing the optimal bit-width allocation for different layers.
- What evidence would resolve it: A study that systematically varies the bit-width allocation for different layers and evaluates the impact on accuracy and efficiency, potentially using techniques like Bayesian optimization or integer programming.

### Open Question 3
- Question: What is the impact of using per-channel quantization instead of per-tensor quantization on the accuracy of the compressed model?
- Basis in paper: The paper uses per-tensor quantization, while some related work (e.g., AdaQuant) uses per-channel quantization.
- Why unresolved: The paper does not compare the performance of per-tensor and per-channel quantization in the context of joint factorization and quantization.
- What evidence would resolve it: Experiments comparing the proposed ADMM-based approach with per-tensor quantization to the same approach with per-channel quantization, evaluating the impact on accuracy and efficiency.

## Limitations
- Limited evaluation to ResNet18 on ImageNet, raising questions about generalizability to other architectures and datasets.
- Specific implementation details of CPD-EPC initialization are not fully specified, making exact reproduction challenging.
- Hyperparameter choices and convergence criteria for the ADMM algorithm are not thoroughly detailed, which could significantly impact results.

## Confidence

- **High Confidence**: The core mechanism of using ADMM for quantization-aware tensor factorization is well-supported by the mathematical formulation and experimental results.
- **Medium Confidence**: The claim that CPD-EPC initialization improves quantized reconstruction error is supported by experimental evidence but lacks extensive ablation studies.
- **Low Confidence**: The assertion that the method outperforms all state-of-the-art post-training quantization techniques across all scenarios is not fully substantiated due to limited comparisons and hyperparameter variations.

## Next Checks

1. Reproduce the ADMM-based CP decomposition with quantized factors on a single convolutional layer, comparing CPD-EPC and random initialization for convergence behavior and final accuracy.
2. Conduct an ablation study varying the quantization bit-width (INT8, INT6, INT4) while keeping rank fixed, to identify the optimal accuracy-efficiency trade-off.
3. Extend the evaluation to other network architectures (e.g., MobileNet, EfficientNet) and datasets to assess the generalizability of the proposed method.