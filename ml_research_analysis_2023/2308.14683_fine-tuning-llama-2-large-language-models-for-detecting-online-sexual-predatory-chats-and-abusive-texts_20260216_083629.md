---
ver: rpa2
title: Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory
  Chats and Abusive Texts
arxiv_id: '2308.14683'
source_url: https://arxiv.org/abs/2308.14683
tags:
- urdu
- data
- llama
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for detecting online sexual
  predatory chats and abusive language using fine-tuned Llama 2 large language models
  (LLMs). The method involves fine-tuning the open-source Llama 2 7B-parameter model
  on datasets with different sizes, imbalance degrees, and languages (English, Roman
  Urdu, and Urdu) using the LoRA parameter-efficient fine-tuning technique.
---

# Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts

## Quick Facts
- arXiv ID: 2308.14683
- Source URL: https://arxiv.org/abs/2308.14683
- Authors: [not specified]
- Reference count: 40
- One-line primary result: Fine-tuned Llama 2 achieves F0.5 score of 0.98 on PAN12 dataset for sexual predator detection

## Executive Summary
This paper proposes a novel approach for detecting online sexual predatory chats and abusive language using fine-tuned Llama 2 large language models. The method involves fine-tuning the open-source Llama 2 7B-parameter model on datasets with different sizes, imbalance degrees, and languages (English, Roman Urdu, and Urdu) using the LoRA parameter-efficient fine-tuning technique. The proposed approach demonstrates strong performance across three distinct datasets, outperforming state-of-the-art methods in terms of accuracy, F1 score, and F0.5 score. Notably, the method achieves an F0.5 score of 0.98 on the PAN12 dataset for sexual predator detection, indicating its applicability in real-world scenarios for flagging online predators, offensive content, hate speech, and discriminatory language. The study highlights the potential of fine-tuned LLMs for text classification tasks in various domains, including cybersecurity, legal and compliance, social media monitoring, healthcare, and customer support.

## Method Summary
The method involves fine-tuning Llama 2 7B-parameter model using LoRA parameter-efficient fine-tuning with 20 epochs, AdamW optimizer (lr=2e-5), and cross-entropy loss. The approach is evaluated on three datasets: PAN12 (English, predatory chats), Roman Urdu (abusive comments), and Urdu (abusive comments). The LoRA technique reduces the number of trainable parameters from 6.6B to ~4.2M (0.064% of the total), lowering GPU memory requirements and enabling efficient adaptation. Cross-entropy loss with weighted classes handles the imbalanced PAN12 dataset effectively. The base model's pretraining on diverse multilingual text enables effective fine-tuning on Urdu and Roman Urdu with minimal data.

## Key Results
- Achieves F0.5 score of 0.98 on PAN12 dataset for sexual predator detection
- Demonstrates strong performance across three distinct datasets (English, Roman Urdu, and Urdu)
- Outperforms state-of-the-art methods in terms of accuracy, F1 score, and F0.5 score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Llama 2 with LoRA significantly reduces parameter updates while preserving performance.
- Mechanism: The original model weights are frozen, and low-rank matrices WA and WB are trained to approximate the update matrix δW = WAWB. This reduces the number of trainable parameters from 6.6B to ~4.2M (0.064% of the total), lowering GPU memory requirements and enabling efficient adaptation.
- Core assumption: The intrinsic dimensionality of the update matrix is low enough that rank r << min(d, k) holds, so the approximation remains accurate.
- Evidence anchors:
  - [section] Equation (1) defines δW = WAWB and the rank constraint; the text states this "reduces the overall number of updates" and "significantly" lowers GPU memory.
  - [abstract] "demonstrates strong performance across three distinct datasets" with "accuracy, F1 score, and F0.5 score" comparable to larger models.
  - [corpus] Weak evidence—no explicit citations in corpus; only inferred from the paper's claims.
- Break condition: If the rank r is chosen too small or the task requires large parameter changes, the approximation error may grow, degrading performance.

### Mechanism 2
- Claim: Cross-entropy loss with weighted classes handles the imbalanced PAN12 dataset effectively.
- Mechanism: The loss function (Eq. 3) incorporates a weight vector w for each class, allowing higher penalty for misclassifying the minority (predatory) class, thereby balancing gradients during training.
- Core assumption: Class imbalance is the main source of bias and can be corrected by loss weighting without additional sampling or data augmentation.
- Evidence anchors:
  - [section] The loss formula explicitly uses w to weight classes; the PAN12 dataset has ~8.5% predatory cases, noted in Table II.
  - [abstract] "performs proficiently and consistently across three distinct datasets," implying robustness to imbalance.
  - [corpus] No direct evidence; only inferred from the described loss formulation.
- Break condition: If the imbalance is extreme or the minority class is inherently harder to classify, weighting alone may not suffice.

### Mechanism 3
- Claim: Llama 2's pretraining on diverse multilingual text enables effective fine-tuning on Urdu and Roman Urdu with minimal data.
- Mechanism: The base model has broad world knowledge and language understanding; fine-tuning adapts it to task-specific patterns while retaining multilingual capabilities, allowing high performance on low-resource languages.
- Core assumption: The pretraining corpus includes sufficient exposure to Urdu/Roman Urdu (even if <0.005%) to bootstrap meaningful representations that fine-tuning can refine.
- Evidence anchors:
  - [section] "Urdu and Roman Urdu are two distinct writing systems... challenging to build a text classification model that could perform effectively across the languages," yet the approach succeeds.
  - [abstract] "method can be implemented in real-world applications (even with non-English languages)" and "demonstrates strong performance" on Urdu datasets.
  - [corpus] Weak evidence—corpus does not mention multilingual fine-tuning success.
- Break condition: If the pretraining exposure to a language is too sparse, fine-tuning may fail to produce accurate representations, especially with very small datasets.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Llama 2 is built on auto-regressive transformers; understanding attention patterns and positional encodings is essential for debugging and extending the model.
  - Quick check question: What is the role of rotary positional embeddings in Llama 2, and how do they differ from absolute positional encodings?

- Concept: Low-rank adaptation (LoRA) and parameter-efficient fine-tuning
  - Why needed here: LoRA is the core technique enabling efficient fine-tuning; knowing how to set rank r and interpret its effect on memory/accuracy is critical for tuning experiments.
  - Quick check question: How does the rank r parameter influence the trade-off between parameter efficiency and model capacity in LoRA?

- Concept: Cross-entropy loss with class weighting
  - Why needed here: The datasets are imbalanced; understanding how the loss function penalizes minority class errors is key to interpreting model performance metrics.
  - Quick check question: How does the weight vector w in the cross-entropy formula affect the gradient updates for each class?

## Architecture Onboarding

- Component map:
  - Pretrained Llama 2 7B model (frozen)
  - PEFT-LoRA framework (wrapper)
  - Hugging Face transformer format (interface)
  - LoRA attention dimension (rank r=8)
  - LoRA alpha scaling (α=16)
  - AdamW optimizer (lr=2e-5, eps=1e-8)
  - Cross-entropy loss with class weights
  - Tokenizer (LLaMA BPE, vocab size=32k)

- Critical path:
  1. Convert Llama 2 weights to Hugging Face format
  2. Wrap with PEFT-LoRA, set rank and alpha
  3. Tokenize input text to IDs
  4. Forward pass through frozen base + LoRA update
  5. Compute weighted cross-entropy loss
  6. Backpropagate only through LoRA parameters
  7. Update LoRA matrices, repeat for 20 epochs

- Design tradeoffs:
  - LoRA rank r=8 vs higher r: smaller r saves memory but may limit expressiveness; higher r improves fit but increases GPU usage.
  - LoRA alpha=16 vs other values: scales the LoRA update; too high may destabilize training, too low may underfit.
  - Fixed hyperparameters across datasets vs dataset-specific tuning: consistency simplifies experimentation but may miss optimal settings for each language/domain.

- Failure signatures:
  - Overfitting: Training loss drops to near zero while validation F1 plateaus or drops.
  - Underfitting: Both training and validation losses remain high; F1 scores stay low.
  - Class imbalance bias: FPR much higher than desired, indicating the model favors the majority class.
  - Memory issues: CUDA out of memory errors during training, suggesting rank or batch size is too large.

- First 3 experiments:
  1. Train on PAN12 with r=4, α=8, batch size 16; check training loss/accuracy curve.
  2. Train on Roman Urdu with r=8, α=16, batch size 8; evaluate FPR and TPR.
  3. Train on Urdu with r=8, α=16, batch size 4; compare F1 score to baseline from [9] and [10].

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the fine-tuned Llama 2 model generalize to detecting abusive language in languages other than English, Roman Urdu, and Urdu?
- Basis in paper: [inferred] The paper demonstrates strong performance on these three languages but does not explore other languages.
- Why unresolved: The study focuses on these specific languages and does not provide evidence for performance in other languages.
- What evidence would resolve it: Testing the model on datasets from additional languages and comparing performance metrics.

### Open Question 2
- Question: What is the impact of using larger Llama 2 models (e.g., 13B, 34B, 70B parameters) on the detection of online sexual predatory chats and abusive language?
- Basis in paper: [explicit] The paper suggests experimenting with larger models but does not provide results.
- Why unresolved: The study only uses the 7B-parameter model and does not explore the potential benefits of larger models.
- What evidence would resolve it: Conducting experiments with larger models and comparing their performance to the 7B-parameter model.

### Open Question 3
- Question: How does the performance of the Llama 2-based approach compare to other state-of-the-art methods in terms of computational efficiency and resource usage?
- Basis in paper: [inferred] The paper mentions the efficiency of the LoRA fine-tuning method but does not provide a detailed comparison with other methods.
- Why unresolved: The study focuses on performance metrics but does not delve into the computational efficiency of the approach.
- What evidence would resolve it: Benchmarking the Llama 2-based approach against other methods in terms of training time, memory usage, and inference speed.

## Limitations

- The multilingual generalization capability is uncertain, particularly for Urdu and Roman Urdu datasets where pretraining exposure is minimal (<0.005% of training data).
- The absence of ablation studies on LoRA rank and alpha parameters prevents definitive attribution of performance gains to specific architectural choices.
- The F0.5 score of 0.98 on PAN12 requires verification given the dataset's inherent complexity and the potential for overfitting on the 8.5% minority class.

## Confidence

- **Performance claims (F1, F0.5 scores)**: Medium confidence - results are internally consistent but lack independent verification and detailed hyperparameter sensitivity analysis
- **Multilingual capability claims**: Low confidence - minimal pretraining exposure to target languages combined with no explicit multilingual training strategy
- **Real-world applicability claims**: Medium confidence - theoretical framework is sound but deployment considerations and false positive/negative rates in production scenarios are not addressed

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary LoRA rank (r=4, 8, 16) and alpha (α=8, 16, 32) across all three datasets to identify optimal configurations per language and assess whether fixed parameters truly generalize
2. **Cross-validation robustness test**: Implement k-fold cross-validation (k=5) on PAN12 to verify the stability of the 0.98 F0.5 score and assess variance across different data splits
3. **Multilingual pretraining verification**: Analyze the actual pretraining data distribution for Urdu/Roman Urdu to quantify exposure and conduct controlled experiments comparing fine-tuning with and without additional multilingual data augmentation