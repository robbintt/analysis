---
ver: rpa2
title: 'HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data'
arxiv_id: '2311.13614'
source_url: https://arxiv.org/abs/2311.13614
tags:
- hallucinations
- visual
- instruction
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates hallucinations in machine-generated visual
  instruction datasets used to train multimodal large language models (MLLMs). The
  authors define three types of hallucinations: object, relation, and attribute, and
  find that such data can have significant hallucinatory toxicity, with up to 28.1%
  of sentences containing object hallucinations.'
---

# HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data

## Quick Facts
- arXiv ID: 2311.13614
- Source URL: https://arxiv.org/abs/2311.13614
- Reference count: 40
- Key outcome: HalluciDoctor successfully mitigates 44.6% hallucinations in machine-generated visual instruction data while maintaining competitive performance compared to LLaVA

## Executive Summary
This paper addresses hallucinations in machine-generated visual instruction datasets used to train multimodal large language models (MLLMs). The authors identify three types of hallucinations—object, relation, and attribute—and find that up to 28.1% of sentences contain object hallucinations. They propose HalluciDoctor, a novel framework that detects and eliminates hallucinations using consistency cross-checking between multiple MLLM experts, while also identifying spurious correlations from long-tail object co-occurrences as a key cause. The framework further employs counterfactual instruction generation to balance data distributions and enhance MLLM resistance to hallucinations, achieving significant improvements without sacrificing performance.

## Method Summary
HalluciDoctor operates through a multi-stage process: first, it extracts semantic chunks (objects, relations, attributes) from generated descriptions using a textual scene graph parser, then generates corresponding questions for each chunk. Multiple MLLM experts answer these questions based on the image, and a consistency metric (BEM) measures agreement—low consistency indicates hallucination. The framework eliminates detected hallucinations and identifies spurious correlations from long-tail object co-occurrences. To address these correlations, HalluciDoctor employs a seesaw-based strategy that selects target scenes where hallucinatory objects rarely co-occur but remain contextually plausible, generating counterfactual instructions to balance the data distribution and improve model robustness.

## Key Results
- HalluciDoctor successfully mitigates 44.6% hallucinations in LLaVA-158K dataset
- The framework maintains competitive performance compared to LLaVA on standard benchmarks
- Counterfactual instruction expansion effectively balances long-tail object co-occurrence distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucination detection via cross-checking consistency between description-oriented and image-oriented answers reduces hallucination errors.
- Mechanism: The framework extracts semantic chunks from the generated description (objects, relations, attributes) and generates corresponding questions. Multiple MLLM experts answer these questions based on the image, and a consistency metric (BEM) measures agreement. Low consistency indicates hallucination.
- Core assumption: Different MLLM experts will disagree when describing hallucinated content versus real content, allowing consistency scoring to detect hallucinations.
- Evidence anchors:
  - [abstract] "Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm."
  - [section] "The key insight is that when asked about the hallucinatory content of a given image, the responses from different MLLM experts typically tend to vary and can even contradict each other."
  - [corpus] Weak - no direct citations found, but similar hallucination detection methods exist in related works.
- Break condition: If MLLMs share similar hallucination patterns or if consistency scoring fails to differentiate hallucinated from real content.

### Mechanism 2
- Claim: Counterfactual instruction expansion balances long-tail object co-occurrence distributions, reducing spurious correlations that cause hallucinations.
- Mechanism: HalluciDoctor identifies hallucinatory objects and their co-occurrence patterns. A seesaw-based strategy uses enhancement and inhibiting factors to select target scenes where hallucinatory objects rarely co-occur but are contextually plausible. Counterfactual instructions are generated to balance the data distribution.
- Core assumption: Long-tail co-occurrence patterns create spurious correlations that mislead models into hallucinating objects.
- Evidence anchors:
  - [abstract] "Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations."
  - [section] "we have observed a notable pattern: these hallucinations frequently occur alongside objects that often appear together due to the long-tail distribution of object co-occurrences."
  - [corpus] Weak - similar concepts exist in counterfactual generation literature but not specifically for MLLM hallucination mitigation.
- Break condition: If the co-occurrence patterns are not the primary cause of hallucinations or if counterfactual generation introduces new errors.

### Mechanism 3
- Claim: Eliminating hallucinations at the data level is more effective than adding post-hoc correction mechanisms during inference.
- Mechanism: Rather than training additional correction models or collecting more data, HalluciDoctor directly removes hallucinatory chunks from the training data, creating cleaner datasets (LLaVA+ and LLaVA++) that produce fewer hallucinations when fine-tuned.
- Core assumption: Hallucinations in training data directly cause hallucinations in model outputs, and removing them improves model reliability.
- Evidence anchors:
  - [abstract] "In contrast to the methods above, we aim to eradicate hallucinations in machine-generated visual instruction data."
  - [section] "This pushes us to focus on mitigating those hallucinatory toxicities... Rather than raising training labor costs or prolonging inference time."
  - [corpus] Moderate - related works on hallucination detection exist but this specific data-level elimination approach appears novel.
- Break condition: If hallucinations are primarily caused by model architecture rather than training data quality.

## Foundational Learning

- Concept: Cross-modal alignment in MLLMs
  - Why needed here: Understanding how visual encoders are aligned with language models is crucial for grasping why hallucinations occur and how cross-checking works.
  - Quick check question: How does the cross-modal alignment layer (e.g., linear projection in LLaVA) affect the model's ability to ground visual information in text?

- Concept: Textual scene graph parsing
  - Why needed here: The framework relies on extracting objects, relations, and attributes from text to create answer chunks for consistency checking.
  - Quick check question: What information does a textual scene graph parser extract from a sentence, and how does this enable semantic analysis of hallucinatory content?

- Concept: Consistency metrics and evaluation
  - Why needed here: Understanding how consistency is measured (BEM metric) is essential for implementing the cross-checking paradigm.
  - Quick check question: How does the BERT-based evaluation metric (BEM) differ from simple exact matching when evaluating answer consistency?

## Architecture Onboarding

- Component map: Answer Chunks Generation -> Question Generation -> Consistency Cross-Checking -> Hallucination Elimination -> Counterfactual Expansion

- Critical path:
  1. Extract answer chunks from generated descriptions
  2. Generate questions for each answer chunk
  3. Get image-oriented answers from multiple MLLMs
  4. Compute consistency scores
  5. Identify and eliminate hallucinatory chunks
  6. Perform counterfactual expansion if needed

- Design tradeoffs:
  - Using multiple MLLM experts increases robustness but requires more computation
  - ChatGPT-based hallucination elimination preserves context but may introduce new errors
  - Counterfactual expansion improves robustness but increases dataset size and training time

- Failure signatures:
  - High false positive rate in hallucination detection (removing correct information)
  - MLLMs converge to overly conservative descriptions to avoid hallucinations
  - Counterfactual instructions introduce implausible scenarios

- First 3 experiments:
  1. Implement basic consistency cross-checking on a small dataset and measure hallucination reduction
  2. Test different threshold values for consistency scoring to optimize precision-recall tradeoff
  3. Evaluate the impact of adding counterfactual instructions on model robustness to spurious correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of hallucinations (object, relation, attribute) affect the performance of multimodal large language models (MLLMs) in various downstream tasks?
- Basis in paper: [explicit] The paper investigates three types of hallucinations and their impact on MLLM performance.
- Why unresolved: The paper focuses on the frequency and mitigation of hallucinations but does not extensively analyze their differential impact on specific downstream tasks.
- What evidence would resolve it: Experiments comparing MLLM performance on various tasks (e.g., image captioning, visual question answering) before and after hallucination mitigation, with a breakdown by hallucination type.

### Open Question 2
- Question: What are the long-term effects of hallucination mitigation on the generalization capabilities of MLLMs?
- Basis in paper: [inferred] The paper suggests that hallucination mitigation improves MLLM performance and reliability, but does not explore long-term effects.
- Why unresolved: The paper's experiments are limited to immediate performance improvements, without considering potential changes in generalization over time or with additional training.
- What evidence would resolve it: Longitudinal studies tracking MLLM performance and generalization capabilities over extended periods and across diverse datasets after hallucination mitigation.

### Open Question 3
- Question: How do different MLLM architectures respond to hallucination mitigation techniques like HalluciDoctor?
- Basis in paper: [explicit] The paper applies HalluciDoctor to two specific MLLM architectures (MiniGPT-4 and mPLUG-Owl) and observes improvements.
- Why unresolved: The paper does not explore the efficacy of HalluciDoctor across a broader range of MLLM architectures, which could have varying sensitivities to hallucination mitigation.
- What evidence would resolve it: Comparative studies applying HalluciDoctor to multiple MLLM architectures (e.g., Flamingo, BLIP-2, Qwen-VL) and analyzing the differential impacts on hallucination reduction and performance.

## Limitations
- The framework's effectiveness relies heavily on the consistency metric's ability to distinguish hallucinated from real content across diverse MLLMs
- The reliance on ChatGPT for both question generation and hallucination elimination introduces potential bias and variability
- The generalizability of HalluciDoctor to other MLLM architectures and datasets beyond LLaVA-158K remains uncertain

## Confidence
- High confidence: The framework's basic architecture and the observation that long-tail co-occurrence patterns contribute to hallucinations are well-supported by empirical results
- Medium confidence: The effectiveness of the seesaw-based counterfactual expansion strategy, as the paper provides limited analysis of whether the expanded data truly balances spurious correlations or merely increases dataset size
- Low confidence: The generalizability of HalluciDoctor to other MLLM architectures and datasets beyond LLaVA-158K, as the paper doesn't extensively test cross-model applicability

## Next Checks
1. **Cross-model consistency validation**: Test HalluciDoctor on multiple MLLM architectures (not just LLaVA) to verify that consistency scoring reliably detects hallucinations across different model families and training paradigms

2. **Ablation study on counterfactual expansion**: Conduct experiments comparing models trained on HalluciDoctor-eliminated data versus HalluciDoctor-eliminated plus counterfactual-expanded data to isolate the specific contribution of the balancing strategy

3. **Human evaluation of hallucination elimination**: Perform controlled human studies where annotators rate descriptions from original, HalluciDoctor-eliminated, and counterfactual-expanded datasets to verify that the framework doesn't over-correct and remove valid content while eliminating hallucinations