---
ver: rpa2
title: Projected Task-Specific Layers for Multi-Task Reinforcement Learning
arxiv_id: '2309.08776'
source_url: https://arxiv.org/abs/2309.08776
tags:
- learning
- ptsl
- layers
- tasks
- care
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Projected Task-Specific Layers (PTSL), a novel
  architecture for multi-task reinforcement learning that combines a shared policy
  with dense task-specific corrections. PTSL improves sample efficiency and overall
  performance on the MT10 and MT50 benchmarks from Meta-World, achieving success rates
  of 0.511 and 0.370 respectively, outperforming prior state-of-the-art methods.
---

# Projected Task-Specific Layers for Multi-Task Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.08776
- Source URL: https://arxiv.org/abs/2309.08776
- Reference count: 36
- Key outcome: PTSL achieves success rates of 0.511 on MT10 and 0.370 on MT50, outperforming prior state-of-the-art methods.

## Executive Summary
This paper introduces Projected Task-Specific Layers (PTSL), a novel architecture for multi-task reinforcement learning that combines a shared policy with dense task-specific corrections. The method uses low-rank task-specific layers with shared and task-specific projections to better capture both shared and task-specific information. PTSL demonstrates significant improvements in sample efficiency and overall performance on the MT10 and MT50 benchmarks from Meta-World, outperforming prior state-of-the-art methods.

## Method Summary
PTSL is an architecture that combines a large, shared fully-connected policy with low-rank task-specific layers. Each task-specific layer consists of down-projection, a small task-specific linear transformation, and up-projection modules. The method can be used standalone or on top of a CARE encoder. Training is performed using multi-task SAC with disentangled alphas, and the architecture includes ablation studies on projection sharing and residual functions to optimize performance.

## Key Results
- Achieves success rates of 0.511 on MT10 and 0.370 on MT50 benchmarks
- Outperforms CARE [4] on both MT10 and MT50 benchmarks
- Particularly effective when scaling to larger task sets, demonstrating strong generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The low-rank task-specific layers allow each task to make fine-grained corrections to the shared policy without disrupting shared representations.
- Mechanism: Task-specific projections compress the shared hidden state to a lower dimension, apply a small task-specific linear transformation, then project back. This enables efficient, targeted modifications while keeping most parameters in the shared backbone.
- Core assumption: Most task-relevant information is preserved in the low-dimensional projection, and the shared backbone captures the majority of generalizable patterns.
- Evidence anchors:
  - [abstract] "combines a large, shared fully-connected policy with low-rank task-specific layers"
  - [section] "low-rank task-specific layers as detailed in Fig. 2"
- Break condition: If the task-specific projection dimension D is too small to capture task-relevant features, performance will degrade.

### Mechanism 2
- Claim: Shared projections stabilize learning by maintaining a consistent mapping between shared and task-specific embedding spaces.
- Mechanism: A single shared down-projection P*down and up-projection P*up are used across all layers (except input/output), ensuring that the transformation from shared to task-specific space is coherent across the network depth.
- Core assumption: Consistency in the embedding space mapping improves gradient flow and reduces interference between tasks.
- Evidence anchors:
  - [abstract] "PTSL outperforms CARE [4] on both the MT10 and MT50 benchmarks"
  - [section] "we experiment with both options: a single shared projection and a projection for each layer"
- Break condition: If the shared projection is not expressive enough, task-specific layers may fail to capture necessary variations.

### Mechanism 3
- Claim: Residual connections between task-specific layers (when used) enable better gradient flow and preserve information across layers.
- Mechanism: The output of the previous task-specific layer is combined with the projected input (via addition, learnable sum, or learnable projection) before feeding into the current task-specific layer.
- Core assumption: Direct connections between task-specific layers help maintain task-specific information throughout the network depth.
- Evidence anchors:
  - [section] "we also add a residual connection between the task-specific layers"
  - [section] "having no residuals is better on the short horizon"
- Break condition: If residuals introduce too much complexity or interfere with the shared backbone, sample efficiency may suffer.

## Foundational Learning

- Concept: Multi-task reinforcement learning
  - Why needed here: The method is designed to learn multiple manipulation tasks simultaneously using a single policy architecture.
  - Quick check question: What is the difference between multi-task learning and meta-learning in the context of RL?

- Concept: Soft Actor-Critic (SAC) algorithm
  - Why needed here: PTSL is evaluated using SAC with disentangled alphas, so understanding this algorithm is crucial for implementation.
  - Quick check question: How does SAC with disentangled alphas differ from standard SAC in multi-task settings?

- Concept: Transformer attention mechanisms
  - Why needed here: PTSL is inspired by Projected Attention Layers from NLP, so familiarity with transformer concepts is helpful for understanding the architecture.
  - Quick check question: What is the role of the attention mechanism in transformers, and how does it differ from standard fully-connected layers?

## Architecture Onboarding

- Component map: State → Shared backbone → (Projection → Task-specific layer → Residual) × N → Action
- Critical path: State → Shared backbone → (Projection → Task-specific layer → Residual) × N → Action
- Design tradeoffs:
  - Number of parameters vs. expressiveness: More task-specific parameters improve performance but increase memory usage
  - Projection dimension D vs. task complexity: Larger D captures more task-specific information but reduces parameter efficiency
  - Residual function choice: Simpler residuals (no residual) may be more sample-efficient than complex ones
- Failure signatures:
  - Training instability: Could indicate projection dimension is too small or residual function is inappropriate
  - Poor performance on certain tasks: May suggest task interference not adequately addressed by current architecture
  - Slow convergence: Could indicate need for larger projection dimension or different residual strategy
- First 3 experiments:
  1. Compare shared vs. independent projections on a small subset of tasks
  2. Test different residual functions (no residual, addition, learnable sum) on MT10
  3. Evaluate effect of projection dimension D on both performance and sample efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PTSL architecture perform on transfer learning tasks, where individual layers are transferred from MT10 to MT50?
- Basis in paper: [inferred] The paper mentions future work including "transfer learning of individual layers from MT10 to MT50" but does not provide results for this.
- Why unresolved: The paper only presents results for training from scratch on MT10 and MT50, without exploring layer transfer between these benchmarks.
- What evidence would resolve it: Experimental results comparing PTSL performance with and without layer transfer from MT10 to MT50, including success rates and sample efficiency metrics.

### Open Question 2
- Question: How would implementing a routing network for the individual layers affect PTSL's performance and parameter sharing efficiency?
- Basis in paper: [inferred] The paper mentions future work including "the implementation of a routing network for the individual layers to better share parameters" but does not explore this approach.
- Why unresolved: The current PTSL architecture uses fixed task-specific layers without dynamic routing mechanisms for parameter sharing.
- What evidence would resolve it: Comparative experiments between standard PTSL and PTSL with routing networks, measuring performance on MT10/MT50 and parameter efficiency.

### Open Question 3
- Question: What is the optimal number of hidden layers for PTSL when scaling to even larger task sets beyond MT50?
- Basis in paper: [inferred] The paper uses 3 hidden layers for MT10 and MT50, but notes that "The benefit of PTSL becomes even more obvious with more diverse tasks (MT50)" suggesting potential scalability benefits.
- Why unresolved: The paper only tests PTSL with 3 hidden layers, and does not explore how the architecture scales with task complexity or number.
- What evidence would resolve it: Systematic experiments varying the number of hidden layers in PTSL while training on benchmarks with increasingly large numbers of tasks, measuring success rates and sample efficiency.

## Limitations

- The paper reports success rates but does not quantify the computational efficiency gains relative to parameter count increases.
- The claim that PTSL is particularly effective when scaling to larger task sets is supported by MT50 results but lacks extensive scaling experiments beyond MT50.
- The comparative analysis between standalone PTSL and CARE+PTSL configurations is limited, with specific benefits of each not fully explored.

## Confidence

- **High Confidence**: The core architectural design of combining shared and task-specific layers is well-founded and the empirical results show clear performance improvements over baselines on the MT10 and MT50 benchmarks.
- **Medium Confidence**: The claim that PTSL is particularly effective when scaling to larger task sets is supported by the MT50 results, but the paper lacks extensive scaling experiments beyond MT50 to definitively establish this claim.
- **Low Confidence**: The paper mentions that PTSL can be used standalone or on top of CARE encoder, but the comparative analysis between these two usage modes is limited.

## Next Checks

1. **Scalability Test**: Evaluate PTSL on a benchmark with more than 50 tasks (e.g., a custom expanded Meta-World suite) to rigorously test the claim about effectiveness at scale.
2. **Efficiency Analysis**: Quantify the trade-off between the added task-specific parameters and the performance gains. Measure training/inference time and memory usage compared to baseline methods.
3. **Architecture Ablation**: Conduct a more comprehensive ablation study on the residual function choice (addition, learnable sum, learnable projection) across different task complexities and horizons to identify optimal configurations.