---
ver: rpa2
title: 'Building RadiologyNET: Unsupervised annotation of a large-scale multimodal
  medical database'
arxiv_id: '2308.08517'
source_url: https://arxiv.org/abs/2308.08517
tags:
- data
- were
- dicom
- images
- tags
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a multimodal, unsupervised approach to automatically
  annotating a large-scale medical radiology database with 1.3 million images using
  DICOM metadata, raw images, and narrative diagnoses. The proposed method employs
  a combination of feature extraction techniques (PCA, autoencoders, and neural networks)
  and clustering algorithms (k-means, k-medoids) to group semantically similar images
  into 50 clusters.
---

# Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database

## Quick Facts
- arXiv ID: 2308.08517
- Source URL: https://arxiv.org/abs/2308.08517
- Reference count: 40
- Primary result: Multimodal fusion of image, DICOM metadata, and diagnosis embeddings achieves best clustering performance for unsupervised annotation of 1.3M medical images

## Executive Summary
This paper presents a multimodal, unsupervised approach to automatically annotate a large-scale medical radiology database containing 1.3 million images. The method employs feature extraction techniques (PCA, autoencoders, neural networks) and clustering algorithms (k-means, k-medoids) to group semantically similar images into 50 clusters. The proposed approach fuses embeddings from three data sources—raw images, DICOM metadata, and narrative diagnoses—using direct concatenation, achieving superior clustering performance compared to single-source methods. The resulting annotated dataset enables transfer learning for medical image processing applications.

## Method Summary
The method processes DICOM files to extract three complementary data sources: raw medical images, DICOM metadata, and narrative diagnoses. Each source undergoes independent feature extraction using appropriate techniques (autoencoders for images, PCA for metadata, doc2vec for text), followed by k-means and k-medoids clustering. The clustering performance is evaluated using homogeneity scores, mutual information, and harmonic mean metrics. The best-performing feature extractors for each source are then fused using direct concatenation of embeddings, and the fused representation is clustered to produce the final annotated dataset. The approach prioritizes clusters that are homogeneous regarding imaging modality and examined body part while maintaining similarity of images and diagnoses within each cluster.

## Key Results
- Multimodal fusion of all three data sources (images, DICOM metadata, diagnoses) yields the most concise clusters with homogeneity score of 0.666 and mutual information score of 0.571
- Narrative diagnoses excel at grouping images by anatomical region due to their focus on disease descriptions affecting specific body parts
- DICOM metadata significantly improves modality homogeneity in clusters through modality-specific tagging conventions
- Direct concatenation of embeddings proves more effective than cluster-space distance or probability-based fusion methods

## Why This Works (Mechanism)

### Mechanism 1
Fusing embeddings from all three data sources (images, DICOM metadata, and narrative diagnoses) produces more semantically cohesive clusters than any single source alone. Each data source captures complementary aspects of medical images—visual patterns (images), acquisition context (DICOM tags), and clinical descriptions (diagnoses). Concatenating their embeddings into a unified representation allows clustering algorithms to leverage multiple semantic dimensions simultaneously.

### Mechanism 2
Narrative diagnoses are particularly effective at grouping images by anatomical region due to their focus on disease descriptions affecting specific body parts. Diagnoses contain explicit textual references to anatomical regions and medical conditions. When converted to embeddings (via PV-DBOW), they preserve semantic relationships that cluster images depicting similar body parts together.

### Mechanism 3
DICOM metadata contributes significantly to modality homogeneity in clusters due to its modality-specific tagging conventions. DICOM tags often contain modality-specific values and acquisition parameters that distinguish between CT, MR, XA, etc. When embedded and clustered, these tags help ensure that images of the same modality are grouped together.

## Foundational Learning

- **Concept: Unsupervised feature extraction and clustering**
  - Why needed here: The paper relies on extracting meaningful features from medical data without labeled examples, then grouping similar items using clustering algorithms.
  - Quick check question: Can you explain the difference between supervised and unsupervised learning, and why unsupervised methods are necessary when labeled data is scarce?

- **Concept: Multimodal data fusion**
  - Why needed here: The paper combines information from images, metadata, and text to create richer representations for clustering.
  - Quick check question: What are the challenges of combining different data modalities, and how might you evaluate whether fusion improves clustering quality?

- **Concept: Dimensionality reduction techniques**
  - Why needed here: High-dimensional medical data (especially images) needs to be reduced to manageable embeddings for clustering.
  - Quick check question: What are PCA and autoencoders, and how do they differ in their approach to dimensionality reduction?

## Architecture Onboarding

- **Component map:** DICOM files → Preprocessing (parsing, conversion, stemming) → Feature extraction (PCA, autoencoders, neural networks, BOW/TF-IDF/doc2vec) → Clustering (k-means, k-medoids) → Evaluation (homogeneity, mutual information, similarity metrics) → Fusion strategies → Final annotated clusters

- **Critical path:** 1. Preprocess each data source independently 2. Extract features using optimal models for each source 3. Evaluate individual source clustering quality 4. Fuse the best embeddings using different strategies 5. Evaluate fused clustering quality 6. Apply the best model to the full dataset

- **Design tradeoffs:** Computational cost vs. clustering quality (more complex models may yield better clusters but require more resources); Granularity of clusters (too few clusters may be too heterogeneous; too many may overfit); Choice of fusion method (direct concatenation is simple but may not capture cross-modal relationships as well as cluster-space methods)

- **Failure signatures:** Poor homogeneity scores indicating mixed modalities or body parts in clusters; High similarity distances within clusters suggesting visual or textual dissimilarity; Empty or near-empty clusters suggesting over-partitioning; Models that perform well on validation but poorly on test data (overfitting)

- **First 3 experiments:** 1. Run clustering on each data source independently with k-means (k=30-50) and evaluate homogeneity and mutual information scores 2. Test different fusion strategies (embeddings, clusterdists, clusterprobs) on the best-performing individual sources and compare S scores 3. Perform ablation studies by removing one data source at a time to quantify its contribution to overall clustering quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the RadiologyNET dataset compare to ImageNet for transfer learning in medical image processing tasks? The paper mentions that medical radiology datasets are more suitable than natural image datasets like ImageNet for transfer learning in medical image processing, but the evaluation of TL models on open challenges in medical radiology image processing has not been conducted yet.

### Open Question 2
What is the impact of including different imaging modalities and protocols on the performance of transfer learning models trained on the RadiologyNET dataset? The current version of the RadiologyNET dataset does not include all imaging modalities and protocols, and the impact of this limitation on TL performance has not been assessed yet.

### Open Question 3
How can the RadiologyNET annotation system be improved by incorporating supervised learning techniques? The current RadiologyNET annotation system relies solely on unsupervised learning techniques, and the potential benefits of incorporating supervised learning methods have not been explored yet.

## Limitations

- The dataset comes from a single institution (Clinical Hospital Centre Rijeka, Croatia), which may introduce selection bias and limit the diversity of cases represented
- The study does not report computational requirements or processing times, making it difficult to assess the practical feasibility of scaling this approach
- Evaluation relies heavily on internal consistency metrics rather than external validation with clinical outcomes or expert annotations

## Confidence

**High Confidence**: The core finding that multimodal fusion improves clustering quality over single-modality approaches is well-supported by the comparative analysis across different feature extraction methods and fusion strategies.

**Medium Confidence**: The specific performance metrics (homogeneity score of 0.666, mutual information score of 0.571) are likely accurate for the tested dataset, but their absolute meaning and clinical relevance are less certain.

**Low Confidence**: The generalizability of these results to other medical imaging datasets, different imaging modalities, or different healthcare systems remains uncertain due to the single-institution dataset and lack of external validation.

## Next Checks

1. **External Dataset Validation**: Apply the same multimodal clustering pipeline to a different, publicly available medical imaging dataset (such as MIMIC-CXR or CheXpert) to assess whether the reported performance metrics generalize across institutions and patient populations.

2. **Expert Annotation Comparison**: Have radiologists manually review and annotate a subset of the clusters to determine whether the algorithm's groupings align with clinical expertise and whether the clusters capture meaningful clinical patterns beyond what internal metrics suggest.

3. **Ablation Study on Data Sources**: Systematically remove each data source (images, DICOM metadata, or diagnoses) from the fusion pipeline and quantify the specific contribution of each modality to overall clustering quality, providing clearer insight into which data sources are most critical for different types of grouping tasks.