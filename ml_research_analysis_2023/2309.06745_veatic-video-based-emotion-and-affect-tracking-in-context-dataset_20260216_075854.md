---
ver: rpa2
title: 'VEATIC: Video-based Emotion and Affect Tracking in Context Dataset'
arxiv_id: '2309.06745'
source_url: https://arxiv.org/abs/2309.06745
tags:
- video
- emotion
- dataset
- recognition
- arousal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VEATIC, the first large video dataset for
  continuous valence and arousal prediction that includes both facial features and
  contextual information. The dataset contains 124 video clips from Hollywood movies,
  documentaries, and home videos, annotated by 192 observers via real-time continuous
  rating.
---

# VEATIC: Video-based Emotion and Affect Tracking in Context Dataset

## Quick Facts
- arXiv ID: 2309.06745
- Source URL: https://arxiv.org/abs/2309.06745
- Reference count: 40
- Primary result: Proposed CNN-ViT baseline achieves CCC scores of 0.6678 for valence and 0.6296 for arousal on VEATIC dataset

## Executive Summary
This paper introduces VEATIC, the first large video dataset for continuous valence and arousal prediction that includes both facial features and contextual information. The dataset contains 124 video clips from Hollywood movies, documentaries, and home videos, annotated by 192 observers via real-time continuous rating. A new computer vision task is proposed to infer the affect of a selected character using both context and character information in each frame. The authors present a simple baseline model using CNN feature extraction and visual transformer for temporal processing, demonstrating that incorporating both context and character information significantly improves emotion recognition performance.

## Method Summary
The VEATIC dataset comprises 124 video clips from diverse sources (Hollywood movies, documentaries, home videos) with continuous valence and arousal annotations collected from 192 observers. The baseline model uses a two-stage pipeline: ResNet50 extracts spatial features from individual frames, followed by a 6-layer visual transformer that processes temporal relationships across frames. The model is trained end-to-end with a weighted loss combining MSE and CCC regularization, using Adam optimizer with cosine annealing learning rate schedule. The dataset and model are designed to capture the dynamic nature of emotional states by leveraging both character expressions and contextual cues within each video frame.

## Key Results
- Baseline model achieves CCC scores of 0.6678 for valence and 0.6296 for arousal
- Model performs best when using both context and character information compared to character-only or context-only conditions
- VEATIC dataset demonstrates generalizability when fine-tuned on other emotion recognition datasets (EMOTIC and CAER-S)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's strength lies in its multi-source video collection (Hollywood, documentaries, home videos) which increases generalization potential compared to lab-controlled datasets
- Mechanism: By exposing the model to varied lighting conditions, contexts, and emotional scenarios across diverse video sources, the learned representations capture more robust and transferable features for emotion recognition
- Core assumption: Emotional expressions and their contextual cues are sufficiently similar across these video types that the model can learn general principles rather than dataset-specific patterns
- Evidence anchors: [abstract] "VEATIC has 124 video clips from Hollywood movies, documentaries, and home videos"; [section] "Unlike other datasets where the source of video clips is unique, video clips of VEATIC come from different sources"
- Break condition: If emotional expressions differ fundamentally across these video types, the model might learn contradictory patterns that harm generalization

### Mechanism 2
- Claim: Real-time continuous annotation by 192 observers provides richer temporal data than static frame labeling
- Mechanism: Continuous ratings capture the dynamic nature of emotional states, allowing the model to learn temporal dependencies and smooth transitions rather than discrete emotional categories
- Core assumption: Continuous valence-arousal ratings provide sufficient granularity to represent emotional dynamics and that observer variability averages out across many annotators
- Evidence anchors: [abstract] "continuous valence and arousal ratings of each frame via real-time annotation"; [section] "Participants watched and rated a total of 124 videos in the dataset... to annotate the data"
- Break condition: If individual annotator biases are systematic rather than random, the consensus ratings may not accurately reflect ground truth emotional states

### Mechanism 3
- Claim: The proposed model architecture effectively integrates both character and context information through a CNN-ViT pipeline
- Mechanism: CNN extracts spatial features from individual frames, while ViT processes temporal relationships across frames, allowing the model to jointly reason about character expressions and contextual influences over time
- Core assumption: The attention mechanism in ViT can effectively learn which contextual elements are most relevant for emotion recognition in each frame
- Evidence anchors: [section] "The pipeline of the model is shown in Figure 8. We adopted two simple submodules: a convolutional neural network (CNN) module for feature extraction and a visual transformer module for temporal information processing"
- Break condition: If the attention mechanism fails to prioritize relevant contextual features, the model may overweight irrelevant background information

## Foundational Learning

- Concept: Valence-arousal circumplex model
  - Why needed here: The dataset uses continuous valence and arousal ratings as the emotion representation, requiring understanding of how these dimensions capture affective states
  - Quick check question: Can you explain why valence and arousal are chosen as continuous dimensions rather than discrete emotion categories?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model uses a visual transformer to process temporal information across frames, requiring understanding of self-attention and multi-head attention
  - Quick check question: How does the multi-head attention mechanism help the model focus on different aspects of the video context?

- Concept: Concordance correlation coefficient (CCC) as evaluation metric
  - Why needed here: CCC is used as the primary evaluation metric for the continuous prediction task, measuring agreement between predictions and ground truth
  - Quick check question: What is the difference between CCC and Pearson correlation coefficient in evaluating continuous predictions?

## Architecture Onboarding

- Component map: Input frames → CNN feature extraction (ResNet50) → Position embedding → Visual Transformer (6 attention layers) → MLP head → Valence/Arousal predictions
- Critical path: Frame encoding → Temporal processing → Prediction
- Design tradeoffs: Using a simple CNN-ViT pipeline prioritizes generalizability and simplicity over potentially more complex architectures that might overfit to this specific dataset
- Failure signatures: High RMSE but moderate CCC suggests systematic bias; low CCC across both metrics indicates poor alignment with ground truth; large variance between valence and arousal performance suggests modality-specific issues
- First 3 experiments:
  1. Ablation study: Train with character-only frames vs. context-only frames vs. full frames to quantify the contribution of each information source
  2. Annotation quality check: Analyze prediction performance across videos with different annotator agreement levels to validate the importance of the large annotator pool
  3. Cross-dataset validation: Fine-tune on a different emotion recognition dataset and measure performance degradation to assess true generalization ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of annotators affect the reliability and consistency of emotion ratings in video datasets?
- Basis in paper: [explicit] The paper compares the standard deviation of consensus ratings across videos when using five versus all annotators, finding that more annotators lead to smaller standard deviations and more accurate representations of ground truth emotion
- Why unresolved: While the paper demonstrates that more annotators reduce variance, it doesn't explore the optimal number of annotators needed for reliable ratings or how this might vary with different types of videos or emotional expressions
- What evidence would resolve it: Experiments systematically varying the number of annotators across different video types and measuring inter-rater reliability and agreement with ground truth emotions would provide insights into the optimal number of annotators needed

### Open Question 2
- Question: How do contextual factors influence emotion recognition compared to facial expressions alone?
- Basis in paper: [explicit] The paper proposes a new task to infer the affect of a selected character using both context and character information in each frame, and shows that the model performs best when using both context and character information compared to character-only or context-only conditions
- Why unresolved: While the paper demonstrates the importance of context, it doesn't quantify the relative contribution of context versus facial expressions to emotion recognition accuracy or explore how this might vary across different emotional states or contexts
- What evidence would resolve it: Controlled experiments systematically manipulating the presence and salience of contextual information while measuring emotion recognition accuracy would quantify the relative contributions of context and facial expressions

### Open Question 3
- Question: How generalizable is the VEATIC dataset to real-world emotion recognition scenarios?
- Basis in paper: [explicit] The paper shows that a model pretrained on VEATIC achieves competitive results when fine-tuned on other datasets like EMOTIC and CAER-S, indicating the generalizability of VEATIC
- Why unresolved: While the paper demonstrates that VEATIC is generalizable to some extent, it doesn't explore the limits of this generalizability or how well models trained on VEATIC perform on more challenging real-world emotion recognition tasks with diverse participants and contexts
- What evidence would resolve it: Evaluating models trained on VEATIC on a wide range of real-world emotion recognition tasks with diverse participants, contexts, and emotional expressions would provide insights into the generalizability of the dataset

## Limitations

- The dataset contains only 124 video clips, which may limit the diversity and generalizability of learned representations
- Real-time annotation by 192 observers introduces potential individual biases that may not fully average out
- The proposed baseline model is relatively simple and may not capture complex emotional dynamics

## Confidence

- High confidence in the dataset's contribution to the field due to its unique multi-source collection and continuous annotation approach
- Medium confidence in the effectiveness of the proposed CNN-ViT architecture for emotion recognition, as it is a straightforward baseline
- Medium confidence in the generalizability results, as cross-dataset validation is limited to a single fine-tuning experiment

## Next Checks

1. **Ablation study**: Evaluate the relative contributions of character vs. context information by training separate models on each modality and comparing performance
2. **Annotator agreement analysis**: Examine prediction performance across videos with varying levels of annotator agreement to validate the importance of the large annotator pool
3. **Temporal consistency validation**: Assess the model's ability to capture emotional transitions by analyzing prediction smoothness and alignment with ground truth valence-arousal trajectories