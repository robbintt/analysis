---
ver: rpa2
title: Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start
  Recommendations
arxiv_id: '2306.17256'
source_url: https://arxiv.org/abs/2306.17256
tags:
- cold-start
- recommendation
- user
- language
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PromptRec, a novel approach using prompt learning
  to tackle system cold-start recommendation, where no historical user-item interactions
  are available. The method transforms the recommendation task into sentiment analysis
  using natural language prompts, enabling small language models to make personalized
  recommendations.
---

# Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations

## Quick Facts
- arXiv ID: 2306.17256
- Source URL: https://arxiv.org/abs/2306.17256
- Reference count: 40
- The paper introduces PromptRec, a novel approach using prompt learning to tackle system cold-start recommendation, where no historical user-item interactions are available.

## Executive Summary
This paper proposes PromptRec, a prompt learning approach to address cold-start recommendation problems by transforming the task into sentiment analysis using natural language prompts. The method leverages pre-trained language models (PLMs) to make personalized recommendations without requiring historical interaction data. A data-centric pipeline enhances small PLMs through refined corpus pre-training and decomposed prompt templates, achieving comparable performance to large models while significantly reducing inference time.

## Method Summary
PromptRec transforms recommendation into sentiment analysis by verbalizing user and item profile features into natural language, combining them via structured templates with a [MASK] token for sentiment prediction. The PLM estimates the probability of sentiment words (e.g., "good" vs "bad") appearing at [MASK], which directly serves as the recommendation score. The approach employs a data-centric pipeline consisting of refined corpus pre-training and decomposed prompt templates to enhance small language models, compensating for their weaker in-context learning capabilities.

## Key Results
- Enhanced small models achieved comparable performance to large models on cold-start recommendation tasks
- Inference time reduced to 17% compared to large models while maintaining similar accuracy
- Validated on three public datasets (ML-100K, Coupon, Restaurant) with binary CTR prediction using Group-AUC (GAUC) metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt learning transforms cold-start recommendation into sentiment analysis, avoiding the need for historical interaction data.
- Mechanism: User and item profile features are verbalized into natural language, then combined via templates with a [MASK] token for sentiment prediction. PLMs estimate the probability of sentiment words appearing at [MASK], which directly serves as the recommendation score.
- Core assumption: The PLM has internalized the relation between user profiles, item attributes, and sentiment in its pre-training corpus.
- Evidence anchors:
  - [abstract]: "transform the recommendation task into sentiment analysis using natural language prompts"
  - [section]: "transform them into templated sentences, and predict recommendations through PLMs"
- Break condition: If the PLM's pre-training corpus lacks coherent patterns between user-item contexts and sentiment words, predictions will be noisy or random.

### Mechanism 2
- Claim: Enhancing small language models with data-centric pre-training reduces reliance on large model size while maintaining performance.
- Mechanism: Two-stage enhancement pipeline: (1) refined corpus pre-training aligns the model's representations with recommendation-relevant language; (2) decomposed prompt templates enable fine-grained sentiment extraction. This compensates for the weaker in-context learning of smaller models.
- Core assumption: Targeted pre-training on recommendation-style corpora transfers useful inductive biases without overfitting.
- Evidence anchors:
  - [abstract]: "enhance small language models... through refined corpus pre-training and decomposed prompt templates"
  - [section]: "data-centric pipeline, which consists of: (1) constructing a refined corpus for model pre-training; (2) constructing a decomposed prompt template via prompt pre-training"
- Break condition: If the refined corpus does not cover the distribution of user-item profile patterns, model performance will plateau regardless of prompt design.

### Mechanism 3
- Claim: Decomposing prompts into user context, item context, and task description reduces ambiguity and improves sentiment prediction accuracy.
- Mechanism: Separate verbalizers for user and item profiles, followed by a structured template filler, creates cleaner context for the PLM to parse, leading to more reliable masked token predictions.
- Core assumption: The PLM's attention mechanism can more easily resolve sentiment polarity when user and item information are explicitly separated.
- Evidence anchors:
  - [section]: "Each template has the following parts: the user profile, the item profile, and the connection between the above profiles and the recommendation task."
- Break condition: If the separation introduces too much template rigidity, the model may lose flexibility to generalize across diverse user-item combinations.

## Foundational Learning

- Concept: Prompt Learning
  - Why needed here: Avoids the need for large-scale supervised training by framing recommendation as a language modeling task.
  - Quick check question: What is the role of the [MASK] token in the PromptRec framework?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Provides the core inference mechanism where sentiment word probability predicts user preference.
  - Quick check question: How does the model decide between "good" and "bad" without fine-tuning?

- Concept: Zero-Shot Recommendation
  - Why needed here: Allows recommendation in the absence of any user-item interaction data during training or inference.
  - Quick check question: What distinguishes zero-shot from few-shot recommendation in this context?

## Architecture Onboarding

- Component map: Verbalizer → Template Filler → PLM → Sentiment Probability Extractor → Recommendation Score
- Critical path: Profile → Verbalization → Templating → MLM Inference → Probability Normalization → Score Output
- Design tradeoffs: Larger PLMs yield better zero-shot performance but higher latency; smaller PLMs benefit from pre-training but may underfit.
- Failure signatures: Random GAUC near 50% indicates model cannot distinguish preferences; low variance in predicted probabilities suggests template rigidity.
- First 3 experiments:
  1. Baseline: Apply PromptRec with BERT-large-uncased on ML-100K; verify GAUC > 50%.
  2. Scale test: Compare PromptRec performance across BERT sizes (TinyBERT to BERT-large) on Coupon dataset.
  3. Template ablation: Remove decomposed context separation; measure impact on Coupon dataset GAUC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we optimize the design of templates for better releasing the potential of PLMs in cold-start recommendation?
- Basis in paper: The paper discusses the importance of template design in PromptRec and mentions that human experts initialize the design of templates, verbalizers, and labelers for each dataset. It also suggests that future work could explore the optimization of template design.
- Why unresolved: While the paper demonstrates the effectiveness of PromptRec with human-designed templates, it does not provide a systematic approach to optimize template design. The impact of different template designs on the performance of PLMs in cold-start recommendation remains unclear.
- What evidence would resolve it: Empirical studies comparing the performance of different template designs in PromptRec, and the development of a systematic approach to optimize template design based on the characteristics of the recommendation task and the PLM used.

### Open Question 2
- Question: How can we choose the pre-trained language model from various available candidates for cold-start recommendation?
- Basis in paper: The paper mentions that the performance of different PLMs in cold-start recommendation is hard to predict by their properties, and future work could explore how to choose the pre-trained language model from various available candidates.
- Why unresolved: The paper demonstrates that the performance of PLMs in cold-start recommendation varies across different datasets and tasks, and it is non-trivial to predict their performance without validation data. A systematic approach to choose the most suitable PLM for a specific cold-start recommendation task is needed.
- What evidence would resolve it: Empirical studies comparing the performance of different PLMs in cold-start recommendation across various datasets and tasks, and the development of a systematic approach to choose the most suitable PLM based on the characteristics of the recommendation task and the PLM used.

### Open Question 3
- Question: How can we strike a balance between the trade-off of model size and prompt learning performance in cold-start recommendation?
- Basis in paper: The paper discusses the impact of model size on the performance of PLMs in cold-start recommendation and mentions that future work could explore how to strike a balance between the trade-off of model size and prompt learning performance.
- Why unresolved: While the paper demonstrates that increasing the scale of PLMs can generally improve cold-start recommendation performance, it also mentions that small language models can make personalized recommendations. The optimal model size for cold-start recommendation remains unclear, and a balance between model size and performance needs to be struck.
- What evidence would resolve it: Empirical studies comparing the performance of different model sizes in cold-start recommendation across various datasets and tasks, and the development of a systematic approach to choose the optimal model size based on the characteristics of the recommendation task and the available computational resources.

## Limitations

- The exact template designs and verbalizers are described as "expert-designed" but not publicly specified, making exact reproduction difficult.
- The claim that small models achieve large-model performance solely through the proposed data-centric pre-training lacks ablation studies isolating each component's contribution.
- Validation is limited to three datasets with binary classification targets; scaling to multi-class or real-time ranking scenarios is untested.

## Confidence

- **High confidence**: The framing of cold-start recommendation as sentiment analysis via prompt learning is sound and aligns with established prompt engineering principles.
- **Medium confidence**: The reported GAUC improvements and inference speed gains are plausible given the method, but depend on unverified template implementations and lack of component-level ablation.
- **Low confidence**: The assertion that small models achieve large-model performance solely through the proposed data-centric pre-training, without fine-tuning, is weakly supported due to missing ablations and limited dataset diversity.

## Next Checks

1. **Template Ablation**: Reproduce the PromptRec pipeline on ML-100K with and without decomposed prompt templates; measure GAUC delta to isolate the impact of context separation.

2. **Pre-training Contribution**: Train two variants—one with refined corpus pre-training, one without—then evaluate both under identical zero-shot conditions to quantify the pre-training effect.

3. **Model Size Scaling**: Systematically evaluate PromptRec across a spectrum of PLM sizes (TinyBERT → BERT-base → BERT-large) on the Coupon dataset; plot GAUC vs. inference latency to verify the claimed 17% speedup at comparable performance.