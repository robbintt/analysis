---
ver: rpa2
title: 'PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity'
arxiv_id: '2305.07893'
source_url: https://arxiv.org/abs/2305.07893
tags:
- similarity
- semantic
- sentence
- pairs
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PESTS, the first Persian-English cross-lingual
  semantic textual similarity dataset, containing 5,375 sentence pairs annotated by
  linguistic experts. The dataset addresses the challenge of evaluating cross-lingual
  semantic similarity in low-resource languages like Persian without relying on machine
  translation, which can propagate errors.
---

# PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity

## Quick Facts
- **arXiv ID:** 2305.07893
- **Source URL:** https://arxiv.org/abs/2305.07893
- **Reference count:** 15
- **Primary result:** PESTS is the first Persian-English cross-lingual semantic textual similarity dataset with 5,375 sentence pairs annotated by experts

## Executive Summary
This paper introduces PESTS, the first Persian-English cross-lingual semantic textual similarity dataset, containing 5,375 sentence pairs annotated by linguistic experts. The dataset addresses the challenge of evaluating cross-lingual semantic similarity in low-resource languages like Persian without relying on machine translation, which can propagate errors. Models based on transformers (e.g., XLM-RoBERTa, paraphrase-xlm-r-multilingual-v1) were fine-tuned using PESTS, showing significant performance improvements, with the paraphrase-xlm-r-multilingual-v1 model's Pearson correlation increasing from 85.87% to 95.62%.

## Method Summary
The PESTS dataset was created by collecting Persian sentences, annotating Persian-Persian similarity, translating Persian sentences to English, and then fine-tuning transformer-based models on this data. The evaluation used Pearson correlation coefficient to measure semantic similarity between English-Persian sentence pairs. Models were fine-tuned with PESTS and evaluated on test sets to demonstrate performance improvements over baseline approaches.

## Key Results
- PESTS is the first Persian-English cross-lingual semantic textual similarity dataset with 5,375 expert-annotated sentence pairs
- Transformer models fine-tuned with PESTS showed significant performance improvements
- paraphrase-xlm-r-multilingual-v1 model improved Pearson correlation from 85.87% to 95.62%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The PESTS dataset enables cross-lingual semantic similarity evaluation without introducing machine translation error.
- **Mechanism:** By providing parallel Persian-English sentence pairs with human-annotated similarity scores, the dataset bypasses the need for machine translation during evaluation. This prevents error propagation that typically degrades similarity model performance.
- **Core assumption:** Human annotators can reliably judge semantic similarity across language pairs without relying on translation.
- **Evidence anchors:** [abstract] "Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model." [section] "In Table 1, a sample of PESTS with their semantic similarity, which is in the range of 0 to 5, can be seen."
- **Break condition:** If annotators cannot reliably judge semantic similarity across languages without translation, or if the dataset does not cover a sufficiently diverse range of semantic relationships.

### Mechanism 2
- **Claim:** Fine-tuning transformer-based models with PESTS improves their cross-lingual semantic similarity performance.
- **Mechanism:** The dataset provides training signals that align semantic representations across Persian and English, allowing transformers to learn cross-lingual embeddings that capture semantic similarity more effectively.
- **Core assumption:** Multilingual transformers can leverage the Persian-English corpus to improve cross-lingual semantic understanding.
- **Evidence anchors:** [abstract] "Results show a significant improvement in Pearson correlation for the paraphrase-xlm-r-multilingual-v1 model, increasing from 85.87% to 95.62%." [section] "Also, different models based on transformers have been fine-tuned using this dataset."
- **Break condition:** If the transformers cannot effectively learn from the dataset due to architecture limitations or insufficient training data.

### Mechanism 3
- **Claim:** PESTS addresses the data scarcity problem for Persian in cross-lingual NLP tasks.
- **Mechanism:** By creating a high-quality, annotated dataset for Persian-English semantic similarity, the research provides a foundation for developing and evaluating models for this low-resource language pair.
- **Core assumption:** Persian is indeed a low-resource language for NLP, and the dataset sufficiently covers the language's characteristics.
- **Evidence anchors:** [abstract] "For Persian, which is one of the low resource languages, no effort has been made in this regard and the need for a model that can understand the context of two languages is felt more than ever." [section] "For Persian, which is one of the low resource languages, no effort has been made in this regard and the need for a model that can understand the context of two languages is felt more than ever."
- **Break condition:** If Persian is not actually low-resource, or if the dataset fails to capture the language's diversity.

## Foundational Learning

- **Concept: Semantic Textual Similarity (STS)**
  - Why needed here: The paper's core contribution is a dataset for evaluating STS across Persian and English, so understanding STS is fundamental.
  - Quick check question: What is the difference between semantic textual similarity and textual entailment?

- **Concept: Cross-lingual embeddings**
  - Why needed here: The models used (like XLM-RoBERTa) rely on cross-lingual embeddings to compare sentences in different languages.
  - Quick check question: How do cross-lingual embeddings enable comparison of sentences in different languages?

- **Concept: Pearson correlation**
  - Why needed here: The paper uses Pearson correlation to evaluate model performance, so understanding this metric is important.
  - Quick check question: What does a Pearson correlation of 0.95 indicate about model performance?

## Architecture Onboarding

- **Component map:** Data collection and annotation pipeline -> Persian-Persian similarity corpus -> English translation of Persian sentences -> Transformer-based models (XLM-RoBERTa, paraphrase-xlm-r-multilingual-v1, etc.) -> Fine-tuning and evaluation framework

- **Critical path:** 1. Collect and preprocess Persian sentences 2. Annotate Persian-Persian similarity 3. Translate Persian sentences to English 4. Fine-tune transformer models with PESTS 5. Evaluate models on test set

- **Design tradeoffs:** Manual annotation vs. automated methods for similarity scoring; Quality of human translation vs. machine translation; Model complexity vs. training data size

- **Failure signatures:** Low inter-annotator agreement indicating ambiguous similarity definitions; Poor model performance suggesting dataset quality issues or model limitations; High computational cost during fine-tuning

- **First 3 experiments:** 1. Fine-tune XLM-RoBERTa with PESTS and evaluate on test set 2. Compare performance of different transformer models on PESTS 3. Analyze the effect of training data size on model performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several potential research directions emerge:

### Open Question 1
- **Question:** How does the PESTS dataset's performance compare to other existing cross-lingual semantic textual similarity datasets in terms of Pearson correlation and Spearman correlation when fine-tuning the same transformer-based models?
- **Basis in paper:** [explicit] The paper introduces PESTS and demonstrates its effectiveness by fine-tuning transformer models, but does not compare its performance directly to other existing cross-lingual datasets.
- **Why unresolved:** The paper does not provide a comparative analysis with other datasets, which could validate the relative effectiveness of PESTS.
- **What evidence would resolve it:** Conducting experiments using the same transformer models on other cross-lingual datasets and comparing the Pearson and Spearman correlation results with those obtained from PESTS.

### Open Question 2
- **Question:** What is the impact of using different preprocessing techniques on the performance of transformer models fine-tuned with the PESTS dataset?
- **Basis in paper:** [inferred] The paper mentions various preprocessing steps applied to the Persian sentences, but does not explore how different preprocessing techniques might affect model performance.
- **Why unresolved:** The paper does not investigate the influence of preprocessing variations on the semantic similarity results.
- **What evidence would resolve it:** Experimenting with different preprocessing techniques (e.g., tokenization, normalization) and measuring the changes in model performance metrics like Pearson correlation.

### Open Question 3
- **Question:** How does the performance of transformer models fine-tuned with PESTS generalize to other low-resource language pairs beyond Persian-English?
- **Basis in paper:** [explicit] The paper focuses on Persian-English cross-lingual semantic textual similarity and does not explore the generalization to other language pairs.
- **Why unresolved:** The dataset and experiments are specific to Persian-English, and there is no evidence of its applicability to other low-resource languages.
- **What evidence would resolve it:** Applying the PESTS-based models to other low-resource language pairs and evaluating their performance in terms of semantic similarity metrics.

## Limitations
- Focus on only two languages (Persian and English) restricts generalizability to other language pairs
- Dataset size of 5,375 sentence pairs may limit model's ability to capture full complexity of semantic relationships
- Reliance on human translation introduces potential subjectivity and bias in parallel sentence creation
- Evaluation exclusively uses Pearson correlation, which may not fully capture all aspects of semantic similarity performance

## Confidence

**High Confidence:** The dataset creation methodology and its role in addressing machine translation error propagation - well-supported by the evidence showing the need for Persian-English cross-lingual resources and the dataset's design.

**Medium Confidence:** The improvement claims for specific transformer models - while the correlation increase is reported, the exact implementation details and comparison baselines are not fully specified.

**Medium Confidence:** The characterization of Persian as a low-resource language - generally accurate but could benefit from more comprehensive linguistic resource analysis.

## Next Checks
1. Evaluate model performance across different semantic similarity thresholds to understand where the improvements are most pronounced.
2. Conduct cross-validation studies with different human translators to assess the consistency and potential bias in the parallel sentence creation process.
3. Test the transferability of PESTS-trained models to other low-resource language pairs to validate the dataset's broader applicability.