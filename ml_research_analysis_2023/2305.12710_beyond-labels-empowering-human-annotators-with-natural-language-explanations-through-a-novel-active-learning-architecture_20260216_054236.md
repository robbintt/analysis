---
ver: rpa2
title: 'Beyond Labels: Empowering Human Annotators with Natural Language Explanations
  through a Novel Active-Learning Architecture'
arxiv_id: '2305.12710'
source_url: https://arxiv.org/abs/2305.12710
tags:
- data
- explanations
- system
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a dual-model active learning framework that
  jointly learns label prediction and natural language explanation generation, addressing
  the gap between real-world annotation practices and traditional active learning
  that focuses solely on labels. The system uses a data diversity-based selection
  strategy leveraging human-annotated explanations, and two models: an explanation
  generator guided by human rationales, and a prediction model that takes generated
  explanations as input.'
---

# Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture

## Quick Facts
- **arXiv ID**: 2305.12710
- **Source URL**: https://arxiv.org/abs/2305.12710
- **Reference count**: 29
- **Key outcome**: Introduces a dual-model active learning framework that jointly learns label prediction and natural language explanation generation, achieving consistent performance gains over baselines on e-SNLI with human-evaluated explanation quality.

## Executive Summary
This work addresses the gap between real-world annotation practices and traditional active learning by introducing a dual-model framework that learns both label prediction and natural language explanation generation. The system uses a data diversity-based active learning selector that leverages human-annotated explanations to choose representative samples, then trains two models: an explanation generator guided by human rationales and a prediction model that takes generated explanations as input. Evaluations on e-SNLI demonstrate consistent performance improvements over random and traditional diversity baselines, with human evaluations confirming that generated explanations are more valid and trustworthy than self-rationalization baselines, though still behind human annotations.

## Method Summary
The proposed system implements a dual-model active learning architecture using T5-base models for both explanation generation and prediction. The process begins with an explanation-based data diversity selector that chooses unlabeled examples based on similarity to previously-labeled human explanations. After human annotation, the explanation-generation model is trained first, followed by the prediction model which is fine-tuned on the generated explanations rather than human ones to better match inference-time conditions. The framework iterates through AL cycles, evaluating performance on accuracy and explanation quality.

## Key Results
- Consistent performance gains over random and content-based diversity baselines in AL simulations with 180 and 450 annotations
- Human evaluation shows generated explanations are more valid and trustworthy than self-rationalization baselines
- Ablation studies demonstrate potential for transfer learning to MultiNLI with minimal additional training
- The explanation-based data diversity selector effectively improves annotation efficiency and model trustworthiness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using human-annotated explanations in AL data selection leads to more representative samples than using only data content
- Mechanism: The selector computes similarity between each unlabeled example's content and previously-labeled examples' human explanations, selecting data with high similarity but diverse coverage
- Core assumption: Human explanations capture richer semantic information about label decisions than raw text alone
- Evidence anchors: Abstract mentions incorporating explanation-generation model for explicit explanations; section compares to data-based clustering and core-set strategies
- Break condition: If explanations become too noisy or task-irrelevant, the similarity measure degrades and selection quality drops

### Mechanism 2
- Claim: Finetuning a prediction model on generated explanations (rather than human explanations) improves model's ability to use explanations at inference
- Mechanism: The prediction model is trained with explanations generated by the explanation-generation model, so it learns to process the style and quality of explanations it will actually receive during inference
- Core assumption: Generated explanations have similar distribution to what the model will produce in deployment
- Evidence anchors: Abstract mentions supporting predictions with generated explanations; section explains fine-tuning with generated explanations for realism
- Break condition: If generated explanations diverge significantly from human explanations, the model may learn incorrect patterns

### Mechanism 3
- Claim: Dual-model architecture with explicit explanation generation improves annotation efficiency by reducing the need for human explanations
- Mechanism: The explanation-generation model produces explanations that the prediction model can use, so human annotators only need to provide labels, not explanations, after initial training
- Core assumption: Generated explanations are sufficiently accurate and trustworthy to replace human explanations for most instances
- Evidence anchors: Abstract mentions improved annotation efficiency and trustworthiness; section demonstrates AL selector outperforming baselines
- Break condition: If generated explanations lack validity, human annotators must provide explanations, negating efficiency gains

## Foundational Learning

- **Concept**: Active Learning data selection strategies
  - Why needed here: The paper compares a novel explanation-based diversity strategy against random and traditional content-based diversity baselines
  - Quick check question: What distinguishes data diversity-based from model probability-based AL selection strategies?

- **Concept**: Natural language explanation generation
  - Why needed here: The system uses explanations both as training targets and as inputs to the prediction model, requiring understanding of how explanations can be modeled and evaluated
  - Quick check question: How does prompt-based explanation generation differ from post-hoc explanation extraction?

- **Concept**: Transfer learning and few-shot adaptation
  - Why needed here: The ablation study shows the explanation-generation model trained on e-SNLI can improve performance on MultiNLI with minimal additional training
  - Quick check question: What factors affect the transferability of explanation-generation models across related NLI tasks?

## Architecture Onboarding

- **Component map**: AL selector → human annotation → explanation generator training → prediction model training → evaluation
- **Critical path**: AL selector selects diverse examples → human annotates labels → explanation generator produces explanations → prediction model trained on generated explanations → evaluation of accuracy and explanation quality; repeat per iteration
- **Design tradeoffs**: Using generated explanations for training prediction model vs. human explanations (realism vs. quality), explanation-based vs. content-based AL selection (richness vs. simplicity)
- **Failure signatures**: Performance stalls or degrades when explanation quality drops, AL selector fails to find diverse examples, or prediction model cannot leverage explanations effectively
- **First 3 experiments**:
  1. Run preliminary experiment to find optimal training data size and confirm dual-model works on full dataset
  2. Run AL simulation with 180 examples to compare novel selector against baselines and verify consistent performance gains
  3. Run human evaluation on generated explanations to validate quality and trustworthiness compared to baseline and human explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed active learning architecture generalize effectively to other natural language processing tasks beyond natural language inference (NLI)?
- Basis in paper: The paper discusses limitations and future research directions, mentioning that the framework's generalizability to other NLP tasks like question answering and commonsense reasoning remains unexplored
- Why unresolved: The current study primarily evaluates the system on the e-SNLI dataset, leaving uncertainty about its effectiveness on diverse NLP tasks with different data characteristics and requirements
- What evidence would resolve it: Systematic experiments applying the dual-model active learning framework to multiple NLP tasks (e.g., question answering, sentiment analysis, named entity recognition) and comparing performance with task-specific baselines would clarify generalizability

### Open Question 2
- Question: How does the performance of the proposed data diversity-based active learning selector compare to model probability-based approaches in terms of annotation efficiency and final model accuracy?
- Basis in paper: The paper mentions that model probability-based approaches have not been evaluated in this work, though they are noted as an alternative to data diversity strategies
- Why unresolved: The current study only benchmarks against traditional data diversity and random baselines, leaving uncertainty about whether probability-based methods might offer superior performance for explanation-augmented active learning
- What evidence would resolve it: Comparative experiments implementing both data diversity and model probability active learning selectors with the dual-model framework, measuring annotation efficiency (examples needed for target accuracy) and final model performance across multiple datasets

### Open Question 3
- Question: What is the optimal balance between explanation quality and label prediction accuracy when fine-tuning the explanation-generation model with varying amounts of training data?
- Basis in paper: The ablation study on Multi-NLI suggests that explanation-generation models fine-tuned on more data perform better, but the relationship between training data size, explanation quality, and prediction performance is not fully characterized
- Why unresolved: The study shows that more training data improves performance, but does not systematically explore the trade-offs or identify diminishing returns, leaving uncertainty about resource allocation in practical deployment
- What evidence would resolve it: Controlled experiments varying training data sizes for the explanation-generation model while measuring both explanation quality (via human evaluation) and prediction accuracy, identifying the point of diminishing returns for each

## Limitations

- The system's performance depends heavily on the quality and representativeness of human explanations in the training data, which may introduce bias or inconsistency
- The assumption that generated explanations can effectively substitute for human input in most cases remains empirically limited, particularly for complex or ambiguous instances
- The long-term effectiveness and scalability of the explanation-based AL selector across extended iterations and diverse NLP tasks require further validation

## Confidence

- **High Confidence**: The dual-model architecture and its integration with active learning is well-defined and reproducible. The ablation study showing transfer learning potential is supported by the experimental setup.
- **Medium Confidence**: The explanation-based AL selector's superiority over baselines is supported by experimental results, but the long-term effectiveness and scalability require further validation. The claim of improved annotation efficiency is plausible but not fully quantified.
- **Low Confidence**: The system's ability to generalize to tasks beyond NLI (e.g., MultiNLI) and its robustness to explanation quality degradation are not thoroughly explored.

## Next Checks

1. **Longitudinal Study**: Evaluate the system's performance over extended AL iterations (e.g., 50+ iterations) to assess stability and scalability
2. **Cross-Domain Generalization**: Test the explanation-generation and prediction models on diverse NLP tasks (e.g., sentiment analysis, fact-checking) to validate transfer learning claims
3. **Explanation Quality Analysis**: Conduct a detailed analysis of generated explanations' validity and trustworthiness across varying complexity levels of input data, comparing against human explanations and self-rationalization baselines