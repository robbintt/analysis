---
ver: rpa2
title: 'S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question
  Answering'
arxiv_id: '2312.16511'
source_url: https://arxiv.org/abs/2312.16511
tags:
- datasets
- question
- pairs
- context
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a method to convert single-turn question-answering
  datasets into multi-turn conversational datasets to improve conversational question-answering
  (CQA) model performance. The approach consists of three main components: a single-turn
  QA pair generator, a knowledge graph-based QA pair reassembler, and a conversational
  question rewriter.'
---

# S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering

## Quick Facts
- arXiv ID: 2312.16511
- Source URL: https://arxiv.org/abs/2312.16511
- Reference count: 38
- Primary result: S2M method converts single-turn QA datasets to multi-turn conversations, achieving state-of-the-art performance on QuAC benchmark

## Executive Summary
This paper addresses the distribution gap between single-turn and multi-turn conversational question-answering datasets by proposing a method to convert single-turn datasets into multi-turn conversational datasets. The approach consists of three main components: a single-turn QA pair generator, a knowledge graph-based QA pair reassembler, and a conversational question rewriter. The method generates a new dataset called S2M from the SQuAD dataset and evaluates it on the QuAC benchmark. The results show that models trained on S2M outperform those trained on existing single-turn and multi-turn datasets, with the S2M-enhanced model achieving state-of-the-art performance on the QuAC leaderboard.

## Method Summary
The S2M method converts single-turn question-answering datasets to multi-turn conversational datasets through a three-component pipeline. First, a QA pair generator creates candidate QA pairs from the context using an RGX-based framework with credit scores for quality filtering. Second, a knowledge graph reassembler extracts triples from the context using OpenIE6, connects them using three principles, and traverses the graph to create sequential QA pairs. Third, a conversational question rewriter trained on a reverse-annotated CANARD dataset converts self-contained questions into follow-up questions that depend on conversation history. The method generates the S2M dataset from SQuAD and evaluates its effectiveness on the QuAC benchmark.

## Key Results
- Models trained on S2M outperform those trained on SQuAD and other single-turn datasets on QuAC
- S2M ranks 1st place on the QuAC leaderboard at time of submission
- Human evaluation shows S2M conversations are more adequate, informative, and relevant than other synthetic datasets
- The method effectively reduces the performance gap between unsupervised and supervised settings on QuAC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method successfully converts single-turn QA datasets to multi-turn conversational datasets, addressing the distribution gap between single-turn and multi-turn datasets.
- Mechanism: The method consists of three components: a QA pair generator that creates candidate QA pairs from the context, a knowledge graph-based QA pair reassembler that constructs sequential QA pairs using extracted triples, and a conversational question rewriter that converts self-contained questions into follow-up questions that depend on conversation history.
- Core assumption: The knowledge graph extracted from the context can effectively guide the construction of coherent conversational flows by representing semantic relationships between information elements.
- Evidence anchors:
  - [abstract]: "The proposed method consists of three parts, namely, a QA pair Generator, a QA pair Reassembler, and a question Rewriter."
  - [section]: "The Reassembler utilizes the knowledge graph to get sequential QA pairs, and the Rewriter converts questions from a conversational perspective to obtain a multi-turn dataset S2M."
- Break condition: If the knowledge graph extraction fails to capture meaningful relationships in the context, or if the question rewriting doesn't maintain semantic coherence with the conversation history.

### Mechanism 2
- Claim: The generated multi-turn dataset S2M effectively improves conversational question-answering model performance on the QuAC benchmark.
- Mechanism: By training models on the S2M dataset, which contains synthetically generated conversational QA pairs, the models learn to handle the conversational context and follow-up questions better than training on single-turn datasets alone.
- Core assumption: The synthetic conversations generated by the proposed method are sufficiently realistic and representative of actual conversational patterns to improve model generalization.
- Evidence anchors:
  - [abstract]: "Our experiments show that our method can synthesize effective training resources for CQA. Notably, S2M ranks 1st place on the QuAC leaderboard at the time of submission."
  - [section]: "In the unsupervised case, our experimental results demonstrate the effectiveness of the synthetic conversations from S2M that improve performance of baselines and reduce the performance gap between the unsupervised setting and the supervised setting."
- Break condition: If the synthetic conversations contain too much noise or unrealistic patterns, the model performance could degrade or fail to improve.

### Mechanism 3
- Claim: The question rewriting component effectively converts self-contained questions into conversational follow-up questions that maintain coherence with the conversation history.
- Mechanism: The rewriter model is trained on a reverse-annotated dataset (R-CANARD) to convert self-contained questions into questions that depend on the conversation context, using techniques like pronoun replacement and omitting redundant information.
- Core assumption: The reverse-annotation approach (converting self-contained to conversational questions) can learn effective rewriting patterns that produce natural-sounding conversational questions.
- Evidence anchors:
  - [section]: "We build our rewriting dataset R-CANARD based on CANARD... we reverse the question rewriting process by converting self-contained questions to questions that depend on the conversation."
  - [section]: "Note that the rewritten questions are more realistic" (referring to Figure 5 examples).
- Break condition: If the rewriting model over-generalizes or produces questions that lose essential information from the original question, the conversation quality could suffer.

## Foundational Learning

- Concept: Knowledge Graph Construction and Traversal
  - Why needed here: The knowledge graph serves as the backbone for creating coherent conversational flows by representing semantic relationships between information elements in the context.
  - Quick check question: Can you explain how the triple extraction algorithm (OpenIE6) and the three principles for connecting triples work together to build a coherent knowledge graph?

- Concept: Question Generation and Rewriting
  - Why needed here: Both components are essential for creating realistic conversational questions - the generator creates candidate questions, while the rewriter converts them into follow-up questions that depend on conversation history.
  - Quick check question: What's the difference between generating questions based on context and answers versus generating them based on context, answers, and conversation history?

- Concept: Multi-Turn Conversational QA vs Single-Turn QA
  - Why needed here: Understanding the key differences between these task formats is crucial for appreciating why the conversion method is necessary and how it addresses the distribution gap.
  - Quick check question: What are the main challenges that conversational QA models face that single-turn QA models don't, and how does the S2M dataset help address these?

## Architecture Onboarding

- Component map: Context → Generator → Knowledge Graph → Reassembler → Rewriter → S2M dataset
- Critical path: Context → Generator → Knowledge Graph → Reassembler → Rewriter → S2M dataset
- Design tradeoffs:
  - Single-turn vs multi-turn generation: The method trades off the simplicity and scalability of single-turn generation for the realism and effectiveness of multi-turn conversations
  - Knowledge graph complexity vs coherence: More complex graphs may capture richer relationships but could also introduce noise or complexity that harms conversational flow
  - Dataset size vs quality: S2M is smaller than SIMSEEK but achieves better results, suggesting quality matters more than quantity
- Failure signatures:
  - Poor question rewriting: Questions become too generic, lose essential information, or don't reference conversation history appropriately
  - Knowledge graph issues: Triples don't connect meaningfully, resulting in disjointed conversations or missing logical connections
  - Generator problems: Produces too many low-quality QA pairs or fails to generate questions that span the full range of conversational topics
- First 3 experiments:
  1. Test the QA pair generator on a small subset of SQuAD to verify it produces reasonable candidate pairs with appropriate confidence scores
  2. Validate the knowledge graph construction by manually inspecting the extracted triples and their connections for a few contexts
  3. Evaluate the question rewriting component by comparing rewritten questions against ground truth conversational questions from QuAC or CANARD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method handle ambiguous contexts when converting single-turn datasets to multi-turn datasets?
- Basis in paper: [inferred] The paper mentions constructing a knowledge graph from the context and using it to obtain sequential QA pairs, but does not explicitly discuss handling ambiguous contexts.
- Why unresolved: The paper does not provide details on how the method deals with ambiguous contexts, which could lead to inaccurate or irrelevant QA pairs in the generated multi-turn dataset.
- What evidence would resolve it: Experimental results showing the method's performance on ambiguous contexts, or a detailed explanation of how the knowledge graph construction and QA pair selection process handles ambiguity.

### Open Question 2
- Question: What is the impact of the number of turns in the generated conversations on the performance of conversational question-answering models?
- Basis in paper: [inferred] The paper generates conversations until the twelfth turn or until a termination condition is met, but does not discuss the optimal number of turns for model performance.
- Why unresolved: The paper does not explore the relationship between the number of turns in the generated conversations and the performance of CQA models, which could provide insights into the ideal conversation length for training.
- What evidence would resolve it: Experiments varying the number of turns in the generated conversations and evaluating the performance of CQA models on these datasets.

### Open Question 3
- Question: How does the quality of the synthetic conversations generated by the proposed method compare to human-annotated conversations in terms of diversity and complexity?
- Basis in paper: [explicit] The paper mentions that human evaluation showed the conversations in S2M to be more popular than other synthetic datasets, but does not compare them to human-annotated conversations.
- Why unresolved: The paper does not provide a direct comparison between the synthetic conversations generated by the proposed method and human-annotated conversations, which would give a better understanding of the method's ability to capture the complexity and diversity of human conversations.
- What evidence would resolve it: A comparative study between the synthetic conversations generated by the proposed method and human-annotated conversations, evaluating them on metrics such as diversity, complexity, and naturalness.

## Limitations
- Knowledge graph construction relies heavily on OpenIE6, which may introduce noise or miss important relationships in complex contexts
- Method's performance on datasets other than QuAC remains unexplored, limiting generalizability claims
- Human evaluation conducted by authors themselves could introduce bias in adequacy, informativeness, and relevance assessments

## Confidence
- **High confidence**: The paper's core methodology (three-component conversion system) and its demonstrated effectiveness on QuAC are well-supported by results
- **Medium confidence**: Claims about S2M being "more effective than SQuAD and other S2M variants" are supported by QuAC results but need validation on additional benchmarks
- **Medium confidence**: The claim that the method "can effectively alleviate the problem of distribution gap" is demonstrated but could benefit from more rigorous statistical analysis

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of each component (Generator, Reassembler, Rewriter) to overall performance improvements
2. Test the S2M conversion method on alternative single-turn datasets (e.g., Natural Questions) to evaluate cross-dataset generalizability
3. Perform inter-annotator agreement studies for human evaluations to establish reliability of the adequacy, informativeness, and relevance scores