---
ver: rpa2
title: 'Learning to Select and Rank from Choice-Based Feedback: A Simple Nested Approach'
arxiv_id: '2307.09295'
source_url: https://arxiv.org/abs/2307.09295
tags:
- algorithm
- item
- nested
- elimination
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of best-item identification from
  choice-based feedback, where a company aims to identify the most preferred item
  among a set of alternatives by displaying subsets of items to customers and collecting
  their choices. The authors propose a simple and efficient algorithm called Nested
  Elimination (NE) that maintains an active item set and eliminates sub-optimal items
  based on a voting score mechanism.
---

# Learning to Select and Rank from Choice-Based Feedback: A Simple Nested Approach

## Quick Facts
- arXiv ID: 2307.09295
- Source URL: https://arxiv.org/abs/2307.09295
- Reference count: 40
- Key outcome: Proposed Nested Elimination algorithm achieves worst-case asymptotic optimality for best-item identification from choice-based feedback

## Executive Summary
This paper addresses the problem of identifying the most preferred item from a set of alternatives using choice-based feedback. The authors propose a simple algorithm called Nested Elimination (NE) that maintains an active item set and eliminates sub-optimal items based on a voting score mechanism. The algorithm is inspired by the nested structure in the information-theoretic lower bound and achieves strong theoretical guarantees, including instance-specific and non-asymptotic bounds on expected sample complexity. Numerical experiments demonstrate the computational and sample efficiency of NE compared to the state-of-the-art method MTP.

## Method Summary
The Nested Elimination (NE) algorithm maintains an active item set and eliminates sub-optimal items based on a voting score mechanism. At each time step, the algorithm displays the current active set to a customer, observes their choice, and updates voting scores for each item. Items are eliminated if the sum of the top k voting scores minus k times the (k+1)th score exceeds a threshold M. The algorithm is designed for the p-Separable family of choice models, which includes many common models like the MNL and Mallows choice models. Theoretical analysis shows that NE is worst-case asymptotically optimal and achieves higher-order optimality compared to previous methods.

## Key Results
- NE algorithm achieves worst-case asymptotic optimality with only constant factor difference from information-theoretic lower bound
- Sample complexity bounds are instance-specific and non-asymptotic, providing strong theoretical guarantees
- Numerical experiments show NE outperforms state-of-the-art MTP method in both computational efficiency and sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm maintains an active item set and eliminates sub-optimal items based on a voting score mechanism inspired by the nested structure in the information-theoretic lower bound.
- Mechanism: At each time step, the algorithm displays the current active set to a customer, observes their choice, and updates voting scores. Items are then eliminated if the sum of the top k voting scores minus k times the (k+1)th score exceeds a threshold M.
- Core assumption: The choice probabilities are consistent with some unknown true strict ranking, and the more preferred item is always chosen with (strictly) higher probabilities.
- Evidence anchors:
  - [abstract]: "The algorithm is inspired by the nested structure implied by the information-theoretic lower bound."
  - [section]: "NE utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem."
  - [corpus]: Weak - corpus neighbors focus on learning-to-rank and recommendation systems, not choice-based feedback elimination.
- Break condition: If the choice probabilities do not satisfy the consistency and separability conditions assumed by the p-Separable family Mp, the elimination criterion may incorrectly eliminate items.

### Mechanism 2
- Claim: The voting scores behave like a (biased) random walk on the integer lattice, and the elimination criterion follows a sequence of hitting times of the corresponding random walk.
- Mechanism: As items are chosen over time, their voting scores increment, creating a random walk. The elimination happens when this walk hits a certain threshold determined by the nested structure.
- Core assumption: The random walk structure accurately models the accumulation of voting scores over time, and the hitting times correspond to when elimination should occur.
- Evidence anchors:
  - [abstract]: "The algorithm maintains an active item set and eliminates sub-optimal items based on a voting score mechanism."
  - [section]: "One main observation we make from NE is that at every stage...the 'active' voting scores {Wt(i) : i ∈ Sactive} behave like a (biased) random walk on the integer lattice Z|Sactive|."
  - [corpus]: Weak - corpus neighbors do not discuss random walk analysis in the context of ranking or choice feedback.
- Break condition: If the voting scores do not accumulate in a way that can be modeled by a random walk, the theoretical analysis of hitting times and sample complexity bounds may not hold.

### Mechanism 3
- Claim: The algorithm achieves higher-order worst-case asymptotic optimality compared to previous methods by matching the information-theoretic lower bound more closely.
- Mechanism: By directly exploiting the nested structure in the optimal solution to the max-min problem, the algorithm's sample complexity approaches the theoretical lower bound, with only a constant difference rather than a logarithmic term.
- Core assumption: The nested structure exploited by the algorithm corresponds to the optimal solution of the information-theoretic lower bound problem, and this structure is preserved under the elimination process.
- Evidence anchors:
  - [abstract]: "We thus show that NE is worst-case asymptotically optimal, and NP is optimal up to a constant factor."
  - [section]: "Together with Theorems 2.5 and 4.1, this implies that NE has a 'higher-order' worst-case asymptotic optimality than MTP."
  - [corpus]: Weak - corpus neighbors focus on different aspects of ranking and do not discuss information-theoretic optimality in choice-based feedback.
- Break condition: If the nested structure assumption does not hold for the specific choice model or if the information-theoretic lower bound analysis does not accurately capture the problem complexity, the claimed optimality may not be achieved.

## Foundational Learning

- Concept: Random walk theory and hitting time analysis
  - Why needed here: The algorithm's performance analysis relies on characterizing the voting scores as random walks and determining when they hit elimination thresholds.
  - Quick check question: Given a biased random walk that increments with probability p and decrements with probability 1-p, what is the expected number of steps to hit a threshold of M starting from 0?

- Concept: Martingale theory and optional stopping theorem
  - Why needed here: The analysis of expected stopping times and error probabilities uses martingale properties and the optional stopping theorem to bound quantities like E[τ] and P(iout ≠ 1).
  - Quick check question: If X_t is a martingale and τ is a bounded stopping time, what does the optional stopping theorem tell us about E[X_τ]?

- Concept: Information-theoretic lower bounds and max-min optimization
  - Why needed here: The algorithm design is inspired by the nested structure in the optimal solution to the information-theoretic lower bound problem, which characterizes the minimum sample complexity needed to identify the best item.
  - Quick check question: In a hypothesis testing problem with K hypotheses, what is the minimum expected sample complexity needed to identify the true hypothesis with error probability at most δ?

## Architecture Onboarding

- Component map:
  - Voting score tracker -> Active set manager -> Display scheduler -> Choice observer -> Termination detector -> Output generator

- Critical path:
  1. Initialize voting scores and active set
  2. While multiple items remain in active set:
     a. Display active set and observe choice
     b. Update voting scores based on choice
     c. Check elimination criteria and update active set
  3. Return the single remaining item as output

- Design tradeoffs:
  - Parameter M controls elimination aggressiveness: larger M reduces error but increases sample complexity
  - Active set size affects exploration: smaller sets reduce computational cost but may miss information
  - Elimination timing affects convergence: earlier eliminations speed up but risk incorrect removals

- Failure signatures:
  - Algorithm fails to terminate: active set never reduces to one item
  - High error rate: P(iout ≠ 1) exceeds the desired confidence level
  - Excessive sample complexity: E[τ] much larger than theoretical bound
  - Computational inefficiency: runtime grows too quickly with problem size

- First 3 experiments:
  1. Verify elimination correctness: Run algorithm on a synthetic problem where the true ranking is known, and check if the correct item is identified
  2. Test parameter sensitivity: Vary the threshold parameter M and measure the tradeoff between error rate and sample complexity
  3. Benchmark against MTP: Compare the algorithm's performance (sample complexity, runtime) with the state-of-the-art MTP method on both synthetic and real-world datasets

## Open Questions the Paper Calls Out

- Question: Can the algorithm be extended to a fixed-budget setting where the total number of time steps is strictly bounded?
  - Basis in paper: [explicit] The paper mentions this as a potential future direction, stating "A promising future direction would be to investigate the fixed-budget setting, where the total number of time steps is strictly bounded, by combining the ideas from the multi-armed bandit literature."
  - Why unresolved: The current paper focuses on the fixed-confidence setting and does not explore the fixed-budget scenario.
  - What evidence would resolve it: A theoretical analysis and empirical evaluation of the algorithm's performance in a fixed-budget setting, comparing it to existing methods designed for this scenario.

- Question: How does the algorithm perform when the separation parameter p is unknown or only partially known?
  - Basis in paper: [explicit] The paper states "Second, this paper considers a setting for a fixed separation parameter p < 1 (or at least when a conservative estimate of p is available). It will be interesting to design an algorithm that is fully agnostic to the value of p as well."
  - Why unresolved: The current algorithm assumes knowledge of the separation parameter p, which may not always be available in practice.
  - What evidence would resolve it: An algorithm that can adapt to unknown or partially known values of p, along with theoretical guarantees and empirical results demonstrating its effectiveness.

- Question: What is the impact of the choice of the tuning parameter M on the algorithm's performance?
  - Basis in paper: [explicit] The paper discusses the role of M in controlling the accuracy of eliminations but does not provide a systematic study of its impact on performance.
  - Why unresolved: While the paper mentions the importance of M, it does not explore how different choices of M affect the algorithm's sample complexity, error probability, or computational efficiency.
  - What evidence would resolve it: A comprehensive study of the algorithm's performance under different values of M, including theoretical analysis and empirical results, to determine the optimal choice of M for various problem instances.

## Limitations
- The algorithm assumes choice probabilities satisfy the p-Separable family conditions, which may not hold for all choice models
- Random walk analysis assumes sufficient mixing time, which may not hold in all scenarios
- Empirical validation is limited to synthetic data from specific models (OA and MNL) and two real datasets

## Confidence
- Worst-case asymptotic optimality claims: **High**
- Sample complexity bounds: **Medium**
- Computational efficiency claims: **High**

## Next Checks
1. Test algorithm robustness on choice models outside the p-Separable family to identify conditions under which the elimination criterion fails
2. Conduct sensitivity analysis varying the threshold parameter M to map the tradeoff between error probability and sample complexity across different problem scales
3. Validate the random walk approximation by comparing empirical voting score distributions against theoretical predictions for various problem configurations