---
ver: rpa2
title: A Lightweight Method to Generate Unanswerable Questions in English
arxiv_id: '2310.19403'
source_url: https://arxiv.org/abs/2310.19403
tags:
- question
- questions
- unanswerable
- data
- answerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a lightweight approach for generating unanswerable
  questions in English by performing antonym and entity swaps on answerable questions.
  This method outperforms previous state-of-the-art models that use more complex techniques
  and significantly larger parameter counts.
---

# A Lightweight Method to Generate Unanswerable Questions in English

## Quick Facts
- **arXiv ID**: 2310.19403
- **Source URL**: https://arxiv.org/abs/2310.19403
- **Authors**: 
- **Reference count**: 23
- **Key outcome**: Simple antonym and entity swaps on answerable questions generate effective unanswerable questions that improve QA model performance by up to 1.6 F1 points on SQuAD 2.0 with BERT-large

## Executive Summary
This paper introduces a training-free, lightweight approach for generating unanswerable questions in English by performing antonym and entity swaps on answerable questions. The method significantly outperforms previous state-of-the-art models that use more complex techniques and larger parameter counts. On the SQuAD 2.0 dataset, the proposed approach improves F1 scores by up to 1.6 points when using BERT-large, while also demonstrating higher human-judged relatedness and readability. The approach is simple to implement, computationally efficient, and effective across multiple encoder models and datasets, establishing it as a strong baseline for future work in unanswerable question generation.

## Method Summary
The method generates unanswerable questions through two simple mechanisms: antonym swaps and entity swaps. For antonym swaps, the approach replaces nouns, adjectives, or verbs with their antonyms while avoiding swaps that would create grammatically incorrect or still-answerable questions (such as adjectives in dependency relations with question words). For entity swaps, the method replaces entities with same-type entities from the context passage, excluding the original answer entity. The generated questions are then filtered using either random sampling or GPT-2 perplexity before being used to augment training data for QA models. This approach requires no model training for question generation itself, making it computationally efficient compared to neural generation methods.

## Key Results
- Outperforms prior state-of-the-art by +1.6 F1 points on SQuAD 2.0 with BERT-large
- Higher human-judged relatedness and readability compared to other automatic methods
- Training-free and lightweight approach that works across multiple encoder models (BERT, RoBERTa, ALBERT) and datasets (SQuAD 2.0, TydiQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swapping antonyms or entities on answerable questions creates unanswerable questions that maintain high relatedness and readability, making them effective training data for QA models.
- Mechanism: By performing minimal syntactic modifications (antonym or entity swaps) on answerable questions, the generated questions remain grammatically correct and contextually relevant to the passage, but become unanswerable because the answer no longer exists in the context.
- Core assumption: The swapped question retains sufficient semantic similarity to the original to be considered "related" while becoming truly unanswerable.
- Evidence anchors:
  - [abstract] "data generated with our training-free and lightweight strategy results in better models (+1.6 F1 points on SQuAD 2.0 data with BERT-large), and has higher human-judged relatedness and readability"
  - [section 5.3] "Our method to be an all-rounder with high relatedness, near-human unanswerability, and the highest readability of any automatic method"
- Break condition: If the antonym or entity swap creates a question that is either answerable or completely unrelated to the context, the effectiveness of the generated data diminishes significantly.

### Mechanism 2
- Claim: The simplicity of the swap-based approach allows it to outperform more complex methods with significantly fewer parameters.
- Mechanism: By avoiding complex neural architectures for generation and instead using simple rule-based swaps, the method maintains question quality while being computationally efficient, leading to better downstream QA performance.
- Core assumption: Simpler generation methods can produce higher quality training data than complex neural models when the task involves specific types of modifications.
- Evidence anchors:
  - [abstract] "Compared to the prior state-of-the-art, data generated with our training-free and lightweight strategy results in better models"
  - [section 5.1] "Our proposed data augmentation methods perform better than other more compute-intensive unanswerable question generation methods"
- Break condition: If the task requires more sophisticated understanding of semantics beyond simple word swaps, the rule-based approach would fail to generate appropriate unanswerable questions.

### Mechanism 3
- Claim: The generated unanswerable questions create a distribution shift that helps QA models learn to distinguish between answerable and unanswerable questions.
- Mechanism: By exposing models to unanswerable questions that are highly similar to answerable ones (same syntactic structure, related content), the models learn to focus on semantic content rather than superficial patterns to determine answerability.
- Core assumption: QA models can generalize from seeing many examples of nearly-answerable-but-unanswerable questions to correctly identifying unanswerable questions in the wild.
- Evidence anchors:
  - [section 5.2] "our simpler structural methods perform comparably with or better than more complex methods, even with less data"
  - [section 6] "our method benefits SQuAD 2.0 performance across model types" and "Using BERT models, the results on TydiQA show very large improvements over the baselines"
- Break condition: If the generated questions are too dissimilar from real unanswerable questions, the model may not generalize well to actual unanswerable questions encountered during inference.

## Foundational Learning

- Concept: Question answering fundamentals and extractive QA evaluation metrics
  - Why needed here: Understanding how exact match (EM) and F1 scores work is crucial for interpreting the paper's results and designing experiments
  - Quick check question: What's the difference between EM and F1 in the context of extractive QA, and why are both metrics reported?

- Concept: Data augmentation techniques and their impact on model training
  - Why needed here: The paper's core contribution is a data augmentation method, so understanding how synthetic data affects model learning is essential
  - Quick check question: How does adding synthetic unanswerable questions to training data help QA models improve their ability to detect unanswerable questions?

- Concept: Part-of-speech tagging and dependency parsing
  - Why needed here: The antonym augmentation method relies on these NLP tools to identify which words can be safely swapped
  - Quick check question: Why does the antonym augmentation method avoid swapping adjectives in a dependency relation with a question word?

## Architecture Onboarding

- Component map: Answerable question + Context paragraph → POS/NER processing → Antonym/Entity swap application → Question generation → Model training → EM/F1 evaluation
- Critical path: Context → Answerable question → POS/NER processing → Swap application → Question generation → Model training → Evaluation
- Design tradeoffs: The method prioritizes simplicity and efficiency over perfect generation quality, accepting some noise in exchange for training-free implementation and better downstream performance
- Failure signatures: Generated questions that are still answerable, completely unrelated to context, or grammatically broken; performance degradation on answerable questions due to overfitting to unanswerable examples
- First 3 experiments:
  1. Implement antonym augmentation on a small sample of answerable questions and verify the generated questions are unanswerable
  2. Test entity augmentation with random entity selection and measure the proportion of still-answerable questions
  3. Fine-tune a small BERT model on data augmented with both methods and compare F1 on SQuAD 2.0 dev set against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or semantic properties of unanswerable questions make them difficult for QA models to handle, beyond their surface-level unanswerability?
- Basis in paper: [explicit] The paper mentions that the reasons why unanswerable questions are hard for models are unclear and relates to factors like domain, model type, and size.
- Why unresolved: The paper acknowledges that while certain types of unanswerable questions (e.g., those with false presuppositions) are challenging, a comprehensive understanding of what makes unanswerable questions difficult is lacking.
- What evidence would resolve it: Systematic experiments comparing model performance on different types of unanswerable questions, varying linguistic complexity, semantic properties, and domain specificity, would help identify the key factors contributing to difficulty.

### Open Question 2
- Question: How does the diversity of generated unanswerable questions compare to human-written unanswerable questions, and what impact does this have on model performance?
- Basis in paper: [inferred] The paper notes that the generated questions are syntactically close to the original data, suggesting a potential lack of diversity compared to human-written questions.
- Why unresolved: The paper does not explicitly evaluate the diversity of generated questions compared to human-written ones, leaving the impact of this difference on model performance unexplored.
- What evidence would resolve it: A detailed comparison of the linguistic and semantic diversity of generated versus human-written unanswerable questions, along with experiments measuring the effect of diversity on model robustness and generalization, would provide insights.

### Open Question 3
- Question: Can the lightweight augmentation method be effectively adapted for languages with rich morphology or different syntactic structures, and what modifications would be necessary?
- Basis in paper: [explicit] The paper discusses the limitations of the method for morphologically complex languages like German and syntactically different languages like Hindi.
- Why unresolved: The paper highlights the challenges but does not explore potential solutions or modifications to adapt the method for such languages.
- What evidence would resolve it: Experiments applying the method to various languages with different morphological and syntactic characteristics, along with adaptations to handle specific language features (e.g., inflection, different question structures), would demonstrate its effectiveness and necessary modifications.

## Limitations

- Limited generalization beyond English due to reliance on WordNet and English-specific NLP tools
- Trade-off between answerable and unanswerable performance not thoroughly analyzed
- Unclear filtering mechanism impact on final model performance

## Confidence

**High confidence (8/10)**: The core claim that lightweight antonym and entity swaps can generate effective unanswerable questions is well-supported by quantitative results across multiple models and datasets.

**Medium confidence (6/10)**: The claim about human-judged relatedness and readability being higher than other automatic methods is supported but relies on sound evaluation methodology assumptions.

**Low confidence (4/10)**: The claim that the method is "training-free" is somewhat misleading since the approach still requires fine-tuning downstream QA models.

## Next Checks

**Check 1: Ablation study on filtering methods**
Systematically vary the filtering thresholds (both random sampling rate and perplexity cutoff) to determine their optimal values and quantify their impact on final QA performance.

**Check 2: Cross-lingual transferability test**
Apply the method to a non-English QA dataset to evaluate whether the swap-based approach generalizes beyond English.

**Check 3: Long-tail unanswerable question analysis**
Analyze the types of unanswerable questions that the generated data fails to capture, particularly rare or complex cases that require deeper semantic understanding.