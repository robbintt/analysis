---
ver: rpa2
title: Development of a Knowledge Graph Embeddings Model for Pain
arxiv_id: '2308.08904'
source_url: https://arxiv.org/abs/2308.08904
tags:
- pain
- knowledge
- data
- health
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the development of knowledge graph embedding
  models for pain concepts extracted from mental health electronic health records
  and enriched with external knowledge from SNOMED CT. Three variations of models
  were built: one using only lexicon triples, one combining lexicon triples with pain
  concepts from EHR data, and one adding sentence embeddings from EHR data.'
---

# Development of a Knowledge Graph Embeddings Model for Pain

## Quick Facts
- arXiv ID: 2308.08904
- Source URL: https://arxiv.org/abs/2308.08904
- Reference count: 0
- Primary result: Achieved MRR 0.83 and Hits@10 0.87 in link prediction for pain concepts in mental health EHRs

## Executive Summary
This paper presents the development of knowledge graph embedding models for pain concepts extracted from mental health electronic health records and enriched with external knowledge from SNOMED CT. Three variations of models were built: one using only lexicon triples, one combining lexicon triples with pain concepts from EHR data, and one adding sentence embeddings from EHR data. The models were evaluated using link prediction tasks and compared against biomedical and non-biomedical benchmark models. The best performing model incorporated both pain concepts and sentence embeddings from EHR data, achieving a Mean Reciprocal Rank (MRR) of 0.83 and Hits@10 of 0.87, outperforming both the biomedical and non-biomedical benchmarks. The ComplEx model generally outperformed TransE across all variations, particularly when incorporating more data from EHRs. These models will be used for downstream tasks such as binary sentence classification to identify pain-related content in clinical text, potentially improving pain research in mental health contexts.

## Method Summary
The research extracted 15,336 triples from SNOMED CT for pain keywords and 5,644 sentences from CRIS data containing 206 unique pain concepts. Three model variations were created: lexicon-only, lexicon plus EHR pain concepts, and lexicon plus EHR pain concepts plus sentence embeddings. Both ComplEx and TransE embedding models were trained on 80/20 splits with 10 epochs, batch size 100, and embedding dimension 150. The models were evaluated using link prediction tasks with MRR and Hits@N metrics, and compared against biomedical and non-biomedical benchmarks to assess performance improvements from incorporating EHR data.

## Key Results
- Best model achieved MRR 0.83, Hits@10 0.87, and Hits@1 0.80
- ComplEx model outperformed TransE across all variations, especially with more EHR data
- Model with pain concepts and sentence embeddings from EHR data performed best
- Outperformed both biomedical and non-biomedical benchmark models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The knowledge graph embedding models learn semantic relationships between pain concepts by representing triples in a low-dimensional vector space.
- Mechanism: The models map entities (subjects/objects) and relations (predicates) into vector embeddings, where scoring functions compute the likelihood of a triple being true based on geometric properties in the embedding space.
- Core assumption: Semantic relationships between pain concepts can be effectively captured through geometric properties in a low-dimensional vector space.
- Evidence anchors:
  - [abstract] "Knowledge graph embeddings help to resolve this by representing the graphs in a low-dimensional vector space"
  - [section] "KGE models learn embeddings of the entities and relations based on scoring functions that predict the probability that a given triple is a fact"
- Break condition: If the scoring functions fail to capture the true semantic relationships between pain concepts, the model's link prediction performance will degrade significantly.

### Mechanism 2
- Claim: Incorporating EHR data and sentence embeddings enriches the knowledge graph with real-world context, improving model performance.
- Mechanism: By adding pain concepts and their contextual sentences from EHRs to the knowledge graph, the model learns embeddings that capture both the structured knowledge from SNOMED CT and the real-world usage patterns from clinical text.
- Core assumption: Real-world usage patterns in clinical text provide valuable context that complements structured medical knowledge.
- Evidence anchors:
  - [abstract] "A knowledge graph built in this way could be further enriched with real-world examples of pain and its relations extracted from electronic health records"
  - [section] "The best performing model incorporated both pain concepts and sentence embeddings from EHR data"
- Break condition: If the added EHR data introduces too much noise or inconsistency, it could degrade model performance rather than improve it.

### Mechanism 3
- Claim: The ComplEx model outperforms TransE because it can better capture complex relationships in biomedical ontologies.
- Mechanism: ComplEx uses complex-valued embeddings and tensor factorization, allowing it to model asymmetric, symmetric, and one-to-many relationships, which are common in biomedical ontologies like SNOMED CT.
- Core assumption: Biomedical ontologies contain complex relationship patterns that require sophisticated modeling approaches.
- Evidence anchors:
  - [abstract] "The ComplEx model generally outperformed TransE across all variations, particularly when incorporating more data from EHRs"
  - [section] "ComplEx was used since it is considered better at representing multi-dimensional data and preserving asymmetry between concepts such as those defined in biomedical ontologies"
- Break condition: If the dataset is too small or lacks complex relationship patterns, the advantage of ComplEx over simpler models like TransE may diminish.

## Foundational Learning

- Knowledge Graphs:
  - Why needed here: Knowledge graphs provide a structured way to represent pain concepts and their relationships, enabling semantic reasoning about pain in mental health contexts.
  - Quick check question: What are the three components of a triple in a knowledge graph?

- Knowledge Graph Embeddings:
  - Why needed here: Embeddings reduce the computational complexity of large knowledge graphs while preserving semantic relationships, making them practical for downstream tasks like link prediction and classification.
  - Quick check question: How do knowledge graph embeddings enable efficient link prediction?

- Link Prediction:
  - Why needed here: Link prediction evaluates how well the model has learned the relationships between entities by testing its ability to predict missing elements in triples.
  - Quick check question: What metrics are commonly used to evaluate link prediction performance in knowledge graph embeddings?

## Architecture Onboarding

- Component map:
  - Data extraction: Extract pain-related concepts from EHRs and SNOMED CT
  - Knowledge graph construction: Build a knowledge graph from triples
  - Embedding models: Implement ComplEx and TransE models
  - Evaluation: Perform link prediction tasks and compute metrics

- Critical path:
  - Extract data → Build knowledge graph → Train embedding models → Evaluate with link prediction

- Design tradeoffs:
  - Model complexity vs. interpretability: ComplEx is more complex but captures richer relationships
  - Data inclusion vs. noise: Including more EHR data adds context but may introduce noise
  - Vector dimensionality vs. computational efficiency: Higher dimensions capture more information but are more computationally expensive

- Failure signatures:
  - Poor link prediction performance: Indicates the model hasn't learned meaningful relationships
  - Overfitting: Model performs well on training data but poorly on test data
  - Inconsistent embeddings: Similar concepts have dissimilar embeddings, suggesting poor learning

- First 3 experiments:
  1. Train both ComplEx and TransE on the basic knowledge graph (without EHR data) and compare their link prediction performance
  2. Add pain concepts from EHR data to the knowledge graph and evaluate the impact on both models
  3. Include sentence embeddings from EHR data and measure the improvement in link prediction for ComplEx vs TransE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the knowledge graph embeddings models perform on downstream binary sentence classification tasks for pain identification in mental health EHRs?
- Basis in paper: [explicit] The paper states that the ComplEx KGE model built in variation 3 will be used in a downstream binary sentence classification task to classify sentences as relevant to pain or not.
- Why unresolved: The paper mentions this as future work and has not yet conducted the classification experiments.
- What evidence would resolve it: Experimental results showing classification metrics (precision, recall, F1-score) when using the KGE models for binary sentence classification, compared to classifiers built without external knowledge incorporation.

### Open Question 2
- Question: How would increasing the size and diversity of the dataset affect the performance of the knowledge graph embeddings models for pain concepts?
- Basis in paper: [inferred] The paper notes that the dataset used was quite small and specific to pain concepts, which could explain why it outperformed larger biomedical and non-biomedical benchmarks.
- Why unresolved: The paper only used a limited dataset focused on pain concepts and did not experiment with larger or more diverse datasets.
- What evidence would resolve it: Comparative experiments using larger and more diverse datasets to evaluate how model performance scales with increased data volume and variety.

### Open Question 3
- Question: How would incorporating additional types of relations beyond first-order parent and child nodes from SNOMED CT affect model performance?
- Basis in paper: [inferred] The paper only extracted first-order parent and child triples from SNOMED CT, suggesting potential for exploring additional relation types.
- Why unresolved: The paper limited itself to first-order hierarchical relations and did not explore other potential relation types available in SNOMED CT.
- What evidence would resolve it: Experimental results comparing model performance when incorporating various relation types (e.g., "may be treated by," "may be finding of disease") beyond simple hierarchical relationships.

## Limitations
- Limited pain concept set (206 unique concepts) may not capture full complexity of pain experiences
- Evaluation focuses on link prediction metrics without validation on intended downstream classification tasks
- Comparison with benchmarks provides context but doesn't establish clinical utility
- Potential biases in EHR data and variations across different mental health institutions not addressed

## Confidence
- **High Confidence**: The link prediction performance results (MRR 0.83, Hits@10 0.87) and the relative performance comparison between ComplEx and TransE models are well-supported by the methodology and evaluation framework.
- **Medium Confidence**: The claim that incorporating EHR data and sentence embeddings improves model performance is supported by experimental results, but the clinical significance of this improvement remains unclear without downstream task validation.
- **Medium Confidence**: The assertion that ComplEx outperforms TransE for biomedical ontologies is supported by results but would benefit from additional validation on diverse biomedical datasets.

## Next Checks
1. Validate the trained models on actual binary sentence classification tasks using a held-out test set of clinical text to confirm practical utility beyond link prediction performance.
2. Test model robustness across different mental health institutions and EHR systems to assess generalizability and potential institutional biases in the training data.
3. Conduct ablation studies to quantify the specific contribution of sentence embeddings versus pain concept additions from EHR data, isolating their individual impacts on model performance.