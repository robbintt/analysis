---
ver: rpa2
title: Enhancing Coherence of Extractive Summarization with Multitask Learning
arxiv_id: '2305.12851'
source_url: https://arxiv.org/abs/2305.12851
tags:
- coherence
- coherent
- sentences
- sentence
- extractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a multitask learning architecture for extractive
  summarization that significantly improves coherence by training a coherent discriminator
  online on sentence representations. The method uses sentence shuffling as data augmentation
  and jointly optimizes three objectives: extractive loss, discriminator loss, and
  coherent loss.'
---

# Enhancing Coherence of Extractive Summarization with Multitask Learning

## Quick Facts
- arXiv ID: 2305.12851
- Source URL: https://arxiv.org/abs/2305.12851
- Authors: 
- Reference count: 8
- Key outcome: This paper presents a multitask learning architecture for extractive summarization that significantly improves coherence by training a coherent discriminator online on sentence representations.

## Executive Summary
This paper addresses the challenge of improving coherence in extractive summarization by introducing a novel multitask learning architecture. The approach combines an extractive summarizer with a coherent discriminator trained simultaneously on both original and shuffled sentence sequences. By leveraging sentence shuffling as data augmentation and optimizing three joint objectives (extractive loss, discriminator loss, and coherent loss), the method achieves significant improvements in the proportion of consecutive sentences in extracted summaries while preserving traditional evaluation metrics like ROUGE scores and BERTScores. Human evaluation confirms the coherence improvements are meaningful and also enhance relevance and factual consistency.

## Method Summary
The method employs a joint multitask learning architecture with three parallel paths: an extractive summarizer for importance scoring, a coherent discriminator for coherence scoring, and differentiable sentence selection mechanisms. The coherent discriminator is trained online on sentence representations from both original and shuffled versions of the input text, learning to distinguish coherent from incoherent sequences. The architecture uses Gumble-Softmax or matrix-based merging techniques to enable differentiable sentence selection, allowing gradients to flow through the selection process. The model is jointly optimized with three losses: extractive loss for content importance, discriminator loss for coherence discrimination, and coherent loss for maximizing coherence scores.

## Key Results
- Achieved up to 72.8% consecutive sentence proportion on CNNDM dataset
- Preserved traditional metrics (ROUGE scores, BERTScores) while improving coherence
- Human evaluation confirmed improvements in coherence, relevance, and factual consistency
- Demonstrated effectiveness across both English (CNNDM, NYT) and Chinese (CNewSum) datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Training a coherent discriminator online on shuffled sentences improves its general ability to judge coherence.
- **Mechanism**: The model uses sentence shuffling as data augmentation, creating incoherent examples by randomly switching sentence positions. The coherent discriminator is trained on both original and shuffled versions, learning to distinguish coherent from incoherent sequences.
- **Core assumption**: Sentence shuffling preserves local semantic content while destroying global coherence, providing effective training signals for coherence discrimination.

### Mechanism 2
- **Claim**: Joint multitask learning with three objectives enables simultaneous coherence enhancement and summary quality preservation.
- **Mechanism**: The architecture contains three parallel paths: extractive summarizer for importance scoring, coherent discriminator for coherence scoring, and differentiable sentence selection. Gradients flow separately to different components based on the loss type.
- **Core assumption**: Differentiable sentence selection through Gumble-Softmax or matrix-based merging allows backpropagating coherence signals without disrupting extractive learning.

### Mechanism 3
- **Claim**: Representation merging techniques enable differentiable sentence selection for extractive summarization.
- **Mechanism**: Two approaches for merging selected sentence representations: (1) MAT-based uses transformation matrix to align selected representations, (2) Model-based uses pre-trained transformer encoder to convert binary selection vector to conversion matrix.
- **Core assumption**: Differentiable merging preserves the ability to backpropagate through the selection process while maintaining semantic coherence of the selected sentences.

## Foundational Learning

- **Concept**: Differentiable sentence selection
  - **Why needed here**: Traditional top-K selection is non-differentiable, preventing gradient flow for coherence optimization
  - **Quick check question**: Can you explain why standard argmax or top-K operations block backpropagation?

- **Concept**: Multi-task learning with shared parameters
  - **Why needed here**: Enables simultaneous optimization of coherence and importance scoring without separate training stages
  - **Quick check question**: What are the risks of parameter sharing between the extractive summarizer and coherent discriminator?

- **Concept**: Data augmentation through sentence shuffling
  - **Why needed here**: Creates synthetic incoherent examples to train the coherent discriminator effectively
  - **Quick check question**: How does Poisson distribution control the degree of shuffling, and why is this important?

## Architecture Onboarding

- **Component map**: BERT encoder → [CLS] vectors → Extractive scorer + Coherent discriminator → Gumble-Softmax selection → Merging → Final summary

- **Critical path**: BERT → [CLS] vectors → Extractive scorer + Coherent discriminator → Gumble-Softmax selection → Merging → Final summary

- **Design tradeoffs**:
  - MAT-based vs Model-based merging: MAT is simpler but may lack adaptability; Model-based is more flexible but adds complexity
  - Online vs offline discriminator training: Online provides task-specific adaptation but may be less stable
  - Sentence shuffling degree: Higher λ creates more incoherent examples but may lose semantic coherence

- **Failure signatures**:
  - Coherence improvements without ROUGE preservation: Loss balance issues
  - Degraded performance on longer documents: Representation merging breakdown
  - Training instability: Incorrect gradient flow configuration
  - Poor cross-lingual performance: Language-specific shuffling patterns

- **First 3 experiments**:
  1. Verify differentiable sentence selection works by testing MAT-based merging on small dataset
  2. Test coherent discriminator training with varying λ (shuffling intensity) values
  3. Compare MAT-based vs Model-based merging performance on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a more reliable automatic evaluation metric for measuring sentence-level coherence in extractive summarization?
- Basis in paper: [explicit] The authors note that there is no reliable automatic evaluation metric for measuring sentence-level coherence and use the proportion of consecutive sentences as a proxy.
- Why unresolved: Current metrics like the proportion of consecutive sentences do not fully capture coherence, as non-consecutive sentences can still be locally coherent.
- What evidence would resolve it: Development and validation of a new metric that accurately measures sentence-level coherence, potentially through human evaluation studies or correlation with other coherence indicators.

### Open Question 2
- Question: How does the choice of sentence sampling method (e.g., topK, Gumble TopK, Gumble-Softmax TopK) impact the coherence and overall quality of extracted summaries?
- Basis in paper: [explicit] The authors compare three sampling methods and find that Gumble-Softmax TopK generally performs best, but the impact on different datasets and settings is not fully explored.
- Why unresolved: The optimal sampling method may depend on the specific characteristics of the dataset and the desired balance between coherence and other metrics.
- What evidence would resolve it: Systematic experiments across diverse datasets and settings to determine the conditions under which each sampling method is most effective.

### Open Question 3
- Question: Can the proposed multitask learning architecture be effectively extended to phrase/sub-sentence level extractive summarization?
- Basis in paper: [explicit] The authors suggest that their method can be applied to phrase/sub-sentence level extraction, but do not provide experimental results.
- Why unresolved: The challenges and potential benefits of extending the method to finer-grained extraction levels are not yet explored.
- What evidence would resolve it: Experiments demonstrating the effectiveness of the method for phrase/sub-sentence level summarization, including comparisons with existing approaches.

## Limitations

- Sentence shuffling as augmentation may not generalize well to non-news domains or languages with different syntactic properties
- The joint multitask learning approach requires careful balancing of multiple objectives, which may not scale to all domains
- Additional computational overhead from online discriminator training and differentiable selection mechanisms

## Confidence

**High confidence (supported by extensive empirical evidence):**
- The multitask learning architecture successfully improves the proportion of consecutive sentences in extracted summaries across multiple datasets
- The method preserves traditional summarization metrics (ROUGE, BERTScore) while enhancing coherence
- Human evaluation confirms improvements in coherence, relevance, and factual consistency

**Medium confidence (supported by experiments but with some caveats):**
- The claim that training the coherent discriminator online on shuffled sentences improves its general coherence-judging ability
- The assertion that the method offers faster training compared to reinforcement learning approaches
- Cross-lingual effectiveness claims based on results from English and Chinese datasets only

**Low confidence (limited evidence or strong assumptions):**
- The generalizability of the sentence shuffling augmentation strategy to non-news domains and different languages
- The long-term stability of the joint optimization approach when applied to very long documents or documents with complex structures

## Next Checks

1. **Domain generalization test**: Apply the method to non-news domains (scientific papers, legal documents, conversational transcripts) to verify if sentence shuffling remains an effective augmentation strategy and if coherence improvements transfer.

2. **Ablation study on shuffling parameters**: Systematically vary the Poisson λ parameter controlling shuffling intensity to determine the optimal balance between creating challenging incoherent examples and preserving local semantic coherence.

3. **Human coherence evaluation validation**: Conduct extensive human studies comparing the proposed method against strong baselines on multiple dimensions (readability, logical flow, topic consistency) to verify that increased consecutive sentence proportion translates to improved perceived coherence.