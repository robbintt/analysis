---
ver: rpa2
title: '3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking'
arxiv_id: '2308.15316'
source_url: https://arxiv.org/abs/2308.15316
tags:
- data
- tracking
- pose
- estimation
- pigeons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 3D-MuPPET is a framework for estimating and tracking 3D poses of
  up to 10 pigeons using multiple camera views. It trains a pose estimator to infer
  2D keypoints and bounding boxes, then triangulates them to 3D.
---

# 3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking

## Quick Facts
- arXiv ID: 2308.15316
- Source URL: https://arxiv.org/abs/2308.15316
- Reference count: 21
- 3D pose estimation and tracking of up to 10 pigeons using multiple camera views

## Executive Summary
3D-MuPPET is a framework for estimating and tracking 3D poses of up to 10 pigeons using multiple camera views. It trains a pose estimator to infer 2D keypoints and bounding boxes, then triangulates them to 3D. Identity matching is achieved via dynamic matching in the first frame followed by 2D tracking. The framework achieves comparable accuracy to a state-of-the-art 3D pose estimator, with median error and PCK metrics. It runs at up to 9.45 fps in 2D and 1.89 fps in 3D, and performs quantitative tracking evaluation with encouraging results. Notably, it works for both indoor and outdoor environments without additional annotations, and can be trained on single-pigeon data to track multiple pigeons, reducing annotation effort.

## Method Summary
The 3D-MuPPET framework estimates 3D poses of pigeons from multi-view video data through a two-stage approach. First, 2D keypoint and bounding box detections are obtained using either KeypointRCNN or a modified DeepLabCut (DLC*) with YOLOv8. Second, these 2D detections are matched across views using bounding box overlap and the Hungarian algorithm, then triangulated to 3D using sparse bundle adjustment. Identity tracking is maintained by performing dynamic matching in the first frame to assign global IDs, then using 2D trackers (SORT) in subsequent frames. The framework is trained on single-pigeon data but generalizes to multi-pigeon scenes, reducing annotation requirements.

## Key Results
- Achieves comparable accuracy to state-of-the-art 3D pose estimators with median error and PCK metrics
- Runs at up to 9.45 fps in 2D and 1.89 fps in 3D
- Performs quantitative tracking evaluation with encouraging results
- Works for both indoor and outdoor environments without additional annotations
- Can be trained on single-pigeon data to track multiple pigeons, reducing annotation effort

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The triangulation of 2D keypoints from multiple views enables accurate 3D pose estimation even when individual views are occluded or noisy.
- Mechanism: Multiple camera views provide redundant 2D keypoint observations, which are mathematically combined via triangulation to reconstruct the 3D position of each keypoint. The redundancy mitigates errors from occlusion or detection failure in any single view.
- Core assumption: The camera calibration is accurate and the 2D keypoint detections are sufficiently precise to allow reliable triangulation.
- Evidence anchors:
  - [abstract] "train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D."
  - [section] "We use the 2D postures of all four camera views obtained from KeypointRCNN and DLC* to acquire 3D keypoint estimates using triangulation with sparse bundle adjustment."
  - [corpus] Weak - related papers discuss 3D animal pose estimation but do not detail triangulation-based approaches.
- Break condition: Significant camera calibration errors, extreme occlusion across all views, or poor 2D keypoint detection accuracy would break the triangulation step.

### Mechanism 2
- Claim: Dynamic first-frame matching followed by 2D tracking efficiently maintains identity across views and time without the computational cost of re-matching in every frame.
- Mechanism: In the first frame, a dynamic matching algorithm uses 3D pose estimates to associate 2D detections from different views to a global identity. In subsequent frames, 2D trackers (e.g., SORT) maintain these identities locally, reducing computational load.
- Core assumption: The first-frame matching is sufficiently accurate and identities remain stable enough that 2D trackers do not make errors requiring re-matching.
- Evidence anchors:
  - [abstract] "For identity matching of individuals in all views, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain IDs across views in subsequent frames."
  - [section] "To determine the correct correspondences between 2D tracks of each view, we use a dynamic matching algorithm based on Huang et al. (2020) in the first frame to assign each SORT ID from each view to a global ID... After the dynamic matching is completed, we maintain 3D correspondences in subsequent frames and triangulate based on 2D tracklets."
  - [corpus] Weak - tracking papers focus on different approaches but do not describe this specific two-stage method.
- Break condition: High motion speed, frequent occlusions, or identity switches in 2D tracking would break the assumption of stable identities and require re-matching.

### Mechanism 3
- Claim: Training on single-animal data enables accurate multi-animal pose estimation, reducing annotation effort and facilitating domain adaptation.
- Mechanism: A pose estimation model trained on single-animal images learns the spatial relationships and appearance of keypoints. When applied to multi-animal scenes, the model can detect and localize keypoints for each individual, leveraging the learned priors without needing explicit multi-animal annotations.
- Core assumption: The learned features and spatial relationships generalize from single to multi-animal contexts without significant degradation.
- Evidence anchors:
  - [abstract] "we train a model with data of single pigeons and achieve comparable results in 2D and 3D posture estimation for up to 5 pigeons."
  - [section] "From this analysis we find that using a learning rate of 0.005... works best... For DLC* (cf. Sec. 3.2), we train YOLOv8... and DLC... separately."
  - [corpus] Weak - related papers discuss pose estimation but do not demonstrate single-to-multi generalization.
- Break condition: Significant overlap or occlusion between animals that violates the single-animal training assumptions would degrade performance.

## Foundational Learning

- Concept: Multi-view geometry and camera calibration
  - Why needed here: Accurate 3D reconstruction from 2D views requires understanding epipolar geometry, camera projection matrices, and calibration parameters.
  - Quick check question: Given two camera views of the same 3D point, can you derive the triangulation equations to compute the 3D position?

- Concept: 2D object detection and keypoint estimation
  - Why needed here: The framework relies on detecting individual animals and localizing their keypoints in 2D images before triangulation.
  - Quick check question: How does a keypoint RCNN differ from a standard object detector, and what additional supervision is needed for keypoint estimation?

- Concept: Multi-object tracking (MOT) principles
  - Why needed here: Maintaining identity across frames and views requires understanding tracking metrics, association algorithms, and handling occlusions.
  - Quick check question: What are the main differences between SORT and DeepSORT, and when would you prefer one over the other?

## Architecture Onboarding

- Component map: Video frames -> Pose Estimation -> Correspondence Matching -> Triangulation -> 3D Output
- Critical path: Video frames → Pose Estimation → Correspondence Matching → Triangulation → 3D Output
- Design tradeoffs:
  - KeypointRCNN vs. DLC*: KeypointRCNN processes full frames (faster for many animals) but may be less accurate; DLC* processes cropped individuals (more accurate) but slower with many animals.
  - First-frame matching vs. continuous matching: First-frame matching + 2D tracking reduces computation but assumes stable identities.
  - Filtering threshold (100mm): Removes correspondence errors but may discard valid predictions in crowded scenes.
- Failure signatures:
  - High RMSE with low median error: Indicates outliers from correspondence errors or occlusion.
  - Identity switches in tracking: Suggests 2D tracker failure or inadequate first-frame matching.
  - Low PCK with acceptable RMSE: May indicate scale or localization issues in keypoint estimation.
- First 3 experiments:
  1. Run inference on a single-view sequence to verify 2D keypoint detection works before adding multi-view complexity.
  2. Test triangulation on synthetic data with known 3D ground truth to validate the geometric pipeline.
  3. Evaluate tracking on a short multi-view sequence with ground truth identities to verify correspondence matching.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 3D-MuPPET framework be effectively extended to track larger groups of animals beyond 10 individuals?
- Basis in paper: [explicit] The paper states "We track up to ten pigeons (the upper limit in Naik et al. (2023))" and "we hope that our work thus leads to further systematic progress in developing automated quantitative methods in the study of animal collective behaviour."
- Why unresolved: The paper only evaluates the framework on groups of up to 10 pigeons, leaving the performance on larger groups untested.
- What evidence would resolve it: Testing the framework on datasets containing more than 10 animals and comparing its accuracy, speed, and tracking performance to state-of-the-art methods.

### Open Question 2
- Question: How well does the 3D-MuPPET framework generalize to different animal species with varying body shapes and sizes?
- Basis in paper: [inferred] The paper demonstrates the framework's performance on pigeons, mice, and cowbirds, but only evaluates its accuracy on pigeons. It also mentions that "Our approach is also not limited to pigeons and can be applied to other species, given 2D posture annotations are available."
- Why unresolved: The paper does not provide a comprehensive evaluation of the framework's performance across a diverse range of animal species.
- What evidence would resolve it: Testing the framework on a wide variety of animal species with different body shapes and sizes, and comparing its accuracy, speed, and tracking performance to species-specific methods.

### Open Question 3
- Question: Can the 3D-MuPPET framework be adapted to work with different camera setups and environmental conditions?
- Basis in paper: [explicit] The paper states that "3D-MuPPET also works in natural environments without model fine-tuning on additional annotations" and demonstrates its performance on outdoor pigeon data.
- Why unresolved: The paper only evaluates the framework on a specific camera setup (4 synchronized and calibrated cameras) and does not explore its performance under different lighting conditions or camera configurations.
- What evidence would resolve it: Testing the framework on data collected with different camera setups (e.g., varying number of cameras, camera angles, resolutions) and under different environmental conditions (e.g., different lighting, weather conditions) and comparing its accuracy, speed, and tracking performance to the baseline results.

## Limitations

- **Camera calibration sensitivity**: The framework relies on accurate multi-view geometry, but calibration errors or lens distortion could significantly impact triangulation accuracy, particularly in the outdoor Wild-MuPPET dataset where conditions are uncontrolled.
- **Identity maintenance assumptions**: The two-stage tracking approach (first-frame matching + 2D tracking) may fail when pigeons exhibit rapid motion, frequent occlusions, or visual similarity that exceeds the 200mm correspondence threshold.
- **Single-to-multi generalization**: While training on single-pigeon data shows promising results, the framework's performance could degrade with higher animal density or complex interactions not present in the training distribution.

## Confidence

- **High confidence**: The core triangulation mechanism and its geometric foundations are well-established. The reported RMSE and PCK metrics align with expectations for this type of multi-view reconstruction problem.
- **Medium confidence**: The dynamic first-frame matching approach is novel for this application, and while it works on the tested datasets, its robustness to challenging scenarios remains to be proven.
- **Low confidence**: The claim that single-pigeon training generalizes to multi-pigeon tracking needs further validation, particularly regarding failure modes and scalability limits.

## Next Checks

1. **Calibration robustness test**: Systematically degrade camera calibration parameters (projection matrix errors, baseline variations) and measure the impact on 3D reconstruction accuracy to establish the framework's sensitivity to geometric errors.

2. **Stress test tracking stability**: Create synthetic sequences with increasing animal density, motion speed, and occlusion frequency to identify the breaking points of the first-frame matching + 2D tracking approach and quantify when re-matching becomes necessary.

3. **Cross-domain generalization evaluation**: Test the single-pigeon-trained model on datasets with significantly different animal appearances, backgrounds, and interaction patterns to validate the claimed generalization capability and identify domain shift effects.