---
ver: rpa2
title: The effectiveness of MAE pre-pretraining for billion-scale pretraining
arxiv_id: '2303.13496'
source_url: https://arxiv.org/abs/2303.13496
tags:
- pretraining
- in1k
- dataset
- pre-pretraining
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAE pre-pretraining with Masked Autoencoders improves foundation
  model performance across model sizes (86M-2B params) and dataset scales (millions
  to billions of images). By initializing weakly supervised pretraining (WSP) with
  MAE, convergence is accelerated and downstream transfer improves across 10 vision
  tasks including image classification, video action recognition, object detection,
  and low/zero-shot learning.
---

# The effectiveness of MAE pre-pretraining for billion-scale pretraining

## Quick Facts
- arXiv ID: 2303.13496
- Source URL: https://arxiv.org/abs/2303.13496
- Reference count: 40
- Primary result: MAE pre-pretraining improves foundation model performance across scales, achieving SotA on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and Food-101 zero-shot (96.0%)

## Executive Summary
This paper demonstrates that Masked Autoencoders (MAE) pre-pretraining significantly improves foundation model performance when combined with weakly supervised pretraining (WSP) on billion-scale datasets. The approach accelerates convergence and enhances downstream transfer across multiple vision tasks including image classification, video action recognition, object detection, and zero-shot learning. The largest models achieve state-of-the-art results on several benchmarks. Notably, MAE pre-pretraining scales effectively with both model size (86M-2B parameters) and dataset scale (millions to billions of images), making it a practical approach for training foundation models at web scale.

## Method Summary
The study combines MAE self-supervised pre-pretraining with weakly supervised pretraining (WSP) using Instagram-3B (IG-3B) multi-label data. Models are trained using Vision Transformers (ViT) with MAE pre-pretraining (75% masking ratio for 1 epoch) followed by WSP using hashtag-derived labels. The pre-pretrained MAE weights initialize the WSP stage, which uses cross-entropy loss for multi-label classification. Evaluation spans 10 vision tasks including standard benchmarks (ImageNet-1k, COCO, LVIS) and specialized datasets (iNaturalist-18, Food-101, Kinetics-400). The study systematically varies model scale (86M-2B parameters) and training duration to assess scaling properties.

## Key Results
- MAE pre-pretraining accelerates WSP convergence and improves downstream transfer across model scales (86M-2B parameters)
- Consistent improvements over standard WSP pretraining across 10 vision tasks including image classification, video action recognition, and object detection
- Achieves state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and Food-101 zero-shot (96.0%)
- MAE scales with dataset size, not just model size, providing gains from IG-3B over ImageNet-1k for all vision tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAE pre-pretraining accelerates convergence of weakly supervised pretraining by providing a better initialization.
- Mechanism: MAE pretraining learns general visual features without labels by reconstructing masked patches. This initialization primes the model with useful low-level and mid-level representations before WSP training, reducing the distance to convergence.
- Core assumption: The features learned by MAE are transferable to downstream tasks and improve upon random initialization for WSP.
- Evidence anchors:
  - [abstract] "Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales..."
  - [section] "Pre-pretraining improves results over the standard pretraining (random initialization, w/o pre-pretraining), and provides large gains with fewer WSP pretraining epochs."
- Break condition: If MAE pre-pretraining features are too task-specific or if WSP's task-specific objectives are incompatible with MAE features, improvements may not materialize.

### Mechanism 2
- Claim: MAE scales with dataset size, not just model size.
- Mechanism: Larger datasets provide more diverse masked patch reconstruction tasks, leading to richer feature learning. The reconstruction objective generalizes well to new data distributions.
- Core assumption: The diversity of reconstruction targets in larger datasets leads to better generalization.
- Evidence anchors:
  - [abstract] "While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well."
  - [section] "We observe that using the IG-3B data provides consistent gains over IN1k for all vision tasks, and the gain increases for larger models."
- Break condition: If dataset diversity plateaus or if the reconstruction task becomes too easy on larger datasets, scaling benefits may diminish.

### Mechanism 3
- Claim: Combining self-supervised and weakly supervised learning captures complementary strengths.
- Mechanism: MAE excels at learning robust low-level and mid-level features (good for detection and linear probing), while WSP learns task-specific discriminative features (good for fine-tuning and zero-shot). Pre-pretraining with MAE provides the best of both worlds.
- Core assumption: MAE and WSP capture different aspects of visual understanding that are complementary.
- Evidence anchors:
  - [abstract] "Our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models."
  - [section] "MAE→WSP outperforms either of MAE or WSP pretraining on most evaluations, across image classification, video recognition, zero-shot evaluation, object detection, etc."
- Break condition: If the combined approach leads to interference or if the benefits of one stage are negated by the other, performance may not improve.

## Foundational Learning

- Concept: Masked Autoencoders (MAE)
  - Why needed here: MAE is the self-supervised pre-pretraining method that provides the better initialization for WSP.
  - Quick check question: How does MAE handle the reconstruction of masked patches, and why is this efficient?

- Concept: Weakly Supervised Pretraining (WSP)
  - Why needed here: WSP is the standard pretraining method on large-scale weakly labeled data that MAE pre-pretraining improves.
  - Quick check question: What type of supervision does WSP use, and how does it differ from supervised pretraining?

- Concept: Vision Transformers (ViT)
  - Why needed here: ViT is the architecture used for all models in the study, providing a standard backbone for comparing MAE and WSP.
  - Quick check question: How do ViTs handle image patches, and what are the key architectural differences from CNNs?

## Architecture Onboarding

- Component map: ViT encoder → MAE decoder (during pre-pretraining) → WSP head (during pretraining) → Downstream task head (during finetuning)
- Critical path: MAE pre-pretraining → WSP pretraining → Downstream finetuning
- Design tradeoffs:
  - MAE: Masking ratio (75% used), decoder size (smaller than encoder), reconstruction target (normalized pixel values)
  - WSP: Label quality (weak supervision), multi-label vs. single-label, loss function (softmax cross-entropy)
- Failure signatures:
  - MAE pre-pretraining fails: Downstream performance similar to random initialization, no acceleration in WSP convergence
  - WSP fails: Poor performance on tasks requiring fine-grained understanding, sensitivity to label noise
- First 3 experiments:
  1. Run MAE pre-pretraining on a small dataset (e.g., ImageNet-1k) and compare features to random initialization using linear probing.
  2. Train WSP from random initialization and from MAE pre-pretraining initialization, compare convergence speed and final performance on a downstream task.
  3. Vary the masking ratio in MAE pre-pretraining (e.g., 50%, 75%, 90%) and measure impact on WSP pretraining and downstream transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MAE pre-pretraining continue to improve performance beyond 1 epoch, and if so, what is the optimal number of epochs for different model scales?
- Basis in paper: [explicit] The paper states "the gains saturate at 1 epoch of pre-pretraining" but also notes that "pre-pretraining's gains do not diminish even after 4 epochs of WSP (20 billion samples)"
- Why unresolved: The study only tested up to 1 epoch of pre-pretraining, leaving uncertainty about whether additional epochs could provide further benefits or if there's a diminishing return
- What evidence would resolve it: Systematic experiments varying pre-pretraining epochs from 0.1 to 5+ while keeping all other factors constant would clarify the optimal pre-pretraining duration

### Open Question 2
- Question: How does MAE pre-pretraining perform when combined with other forms of weakly-supervised pretraining beyond hashtag-based multi-label classification?
- Basis in paper: [inferred] The paper focuses exclusively on hashtag-based WSP but mentions "we explore WSP in conjunction with self-supervised approaches" without exploring other WSP variants
- Why unresolved: The study only tested one specific form of WSP (hashtag-based), leaving uncertainty about generalizability to other weakly-supervised approaches like image-text contrastive learning
- What evidence would resolve it: Direct comparisons of MAE→WSP using different WSP variants (CLIP-style image-text, multi-modal, etc.) while keeping other factors constant would reveal if benefits are universal

### Open Question 3
- Question: What is the theoretical mechanism behind MAE pre-pretraining's ability to improve WSP performance, particularly for rare object detection?
- Basis in paper: [inferred] The paper observes that MAE→WSP improves detection of rare objects but doesn't explain the underlying mechanism
- Why unresolved: While empirical results show improvements, the paper doesn't provide theoretical justification for why MAE's reconstruction task translates to better rare object detection
- What evidence would resolve it: Ablation studies examining intermediate representations, attention patterns, and feature correlations between MAE and WSP models would reveal the transfer mechanism

## Limitations

- Relies on hashtag-derived labels whose quality and noise characteristics are not fully characterized
- Comparisons primarily against random initialization rather than other state-of-the-art pretraining methods
- Computational costs of MAE pre-pretraining at billion-scale are not discussed, limiting practical deployment insights

## Confidence

- High Confidence: Claims about MAE pre-pretraining improving convergence speed and providing better initialization for WSP pretraining
- Medium Confidence: Claims about state-of-the-art results on specific benchmarks and the scalability of MAE with dataset size
- Medium Confidence: Claims about the complementary nature of self-supervised and weakly supervised learning

## Next Checks

1. Conduct an analysis of hashtag-derived label quality by comparing performance using clean supervised labels versus hashtag labels on a subset of data.

2. Replicate the main experiments replacing MAE pre-pretraining with other self-supervised methods (SimCLR, DINO, SwAV) to determine whether improvements are specific to MAE or general to self-supervised pretraining.

3. Measure and report the computational overhead of MAE pre-pretraining relative to the improvements gained, including GPU hours, memory usage, and throughput comparisons across different model scales.