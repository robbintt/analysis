---
ver: rpa2
title: 'Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian
  Processes'
arxiv_id: '2310.16597'
source_url: https://arxiv.org/abs/2310.16597
tags:
- pseudo
- gaussian
- networks
- distribution
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the theory of Gaussian process limits for infinitely
  wide neural networks beyond the classical IID weight assumption. The authors introduce
  the PSEUDO-IID regime, which includes structured weight matrices like low-rank and
  block-sparse patterns, while still ensuring Gaussian process convergence.
---

# Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes

## Quick Facts
- arXiv ID: 2310.16597
- Source URL: https://arxiv.org/abs/2310.16597
- Reference count: 40
- Primary result: Structured weight matrices (sparse, low-rank) in infinitely wide neural networks still converge to Gaussian processes under pseudo-IID assumptions

## Executive Summary
This paper establishes that neural networks with structured weight matrices—such as sparse and low-rank patterns—converge to Gaussian processes in the infinite width limit, extending classical results beyond the traditional IID weight assumption. The authors introduce the pseudo-IID regime, which replaces independence with exchangeability and controlled higher moments, enabling rigorous analysis of networks with efficient, structured initializations. This theoretical framework allows for edge-of-chaos analysis and critical tuning of non-IID networks, bridging a gap between theoretical GP limits and practical structured architectures.

## Method Summary
The authors prove that fully-connected and convolutional networks with pseudo-IID weight matrices converge to Gaussian processes by verifying conditions for the exchangeable central limit theorem. They introduce Definition 1 for pseudo-IID distributions, requiring row- and column-exchangeability, uncorrelated entries with variance σ²/n, and bounded high-order moments. The proof involves checking these conditions across network layers and deriving recursive covariance updates that preserve Gaussian process structure. Experiments validate convergence across various structured weight distributions (uniform IID, Gaussian IID with dropout, Gaussian low-rank, structured sparse) by comparing empirical pre-activation distributions against theoretical Gaussian process predictions.

## Key Results
- Networks with pseudo-IID weights (including sparse and low-rank patterns) converge to Gaussian processes in the infinite width limit
- Convolutional networks with structured pseudo-IID filters also converge to Gaussian processes via matricization
- The pseudo-IID framework enables rigorous edge-of-chaos analysis for efficient, structured initializations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-IID weight matrices still generate Gaussian processes in the infinite-width limit despite lack of entry-wise independence.
- Mechanism: Row- and column-exchangeability combined with uncorrelatedness and controlled higher moments allows the exchangeable central limit theorem to apply. The covariance structure is preserved through layers, ensuring Gaussian process convergence.
- Core assumption: Exchangeability and bounded high-order moments are sufficient to replace independence in the CLT for infinite-width neural networks.
- Evidence anchors:
  - [abstract] "The P SEUDO -IID distribution is defined in Definition 1 from the exchangeable distribution Definition 2 combined with a specified variance and a bounded high order moment."
  - [section] "We need to check if the summands Xn,j := γ(ℓ) j (α, L)[n] satisfy the conditions of Theorem 3. We will carefully verify each condition..."
  - [corpus] Weak evidence from neighboring work on low-rank neural network optimization.
- Break condition: If moment bounds fail or exchangeability is broken (e.g., block structure breaks under certain permutations), the proof no longer holds.

### Mechanism 2
- Claim: Convolutional networks with structured pseudo-IID filters also converge to Gaussian processes.
- Mechanism: Matricization of the convolutional operator allows the same exchangeable CLT machinery to apply. The covariance recursion generalizes from fully-connected to convolutional settings.
- Core assumption: The convolution can be expressed as matrix multiplication after reshaping, preserving exchangeability and moment structure.
- Evidence anchors:
  - [abstract] "We extend our results to the convolutional neural networks (CNNs) in Section 2.3."
  - [section] "There exist multiple ways to compute convolutions between a tensor filter U and a 2-dimensional signal X..."
  - [corpus] Limited direct evidence; this is a novel extension not strongly supported by corpus neighbors.
- Break condition: If the reshaping approach fails to preserve the energy-preserving property or exchangeability, convergence is not guaranteed.

### Mechanism 3
- Claim: Structured sparse and low-rank initializations allow for rigorous edge-of-chaos analysis.
- Mechanism: Since the network remains in the pseudo-IID regime, the Gaussian process covariance recursions still apply, enabling stability analysis as in prior work.
- Core assumption: The variance and moment conditions of pseudo-IID are maintained under structured sparsity and low-rank patterns.
- Evidence anchors:
  - [abstract] "Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance their training."
  - [section] "The P SEUDO -IID distribution includes structured low-dimensional matrices such as low-rank and structured sparse settings..."
  - [corpus] Weak; no direct citations to edge-of-chaos literature in neighbors.
- Break condition: If sparsity or low-rank structure introduces correlations that violate the pseudo-IID conditions, stability analysis breaks down.

## Foundational Learning

- Concept: Exchangeability in random matrices
  - Why needed here: The core of the proof replaces independence with exchangeability to handle structured weights.
  - Quick check question: What is the difference between row-exchangeability and entry-wise exchangeability?

- Concept: Central limit theorem for exchangeable random variables
  - Why needed here: The proof relies on an extension of CLT that does not require independence.
  - Quick check question: Under what conditions does the exchangeable CLT apply?

- Concept: Gaussian process covariance recursion
  - Why needed here: The Gaussian process limit is characterized by a recursive covariance formula that must be verified at each layer.
  - Quick check question: How does the covariance update when propagating through a ReLU activation?

## Architecture Onboarding

- Component map: Weight matrix initialization → pseudo-IID verification → network propagation → covariance recursion → Gaussian process limit
- Critical path: Ensuring pseudo-IID conditions → applying exchangeable CLT → verifying covariance convergence
- Design tradeoffs: Structured weights (sparse/low-rank) improve efficiency but require careful moment control to preserve Gaussian process behavior
- Failure signatures: Non-Gaussian pre-activations at finite width; unstable covariance growth across layers
- First 3 experiments:
  1. Initialize a fully-connected network with low-rank Gaussian weights and verify the pre-activation distribution approaches Gaussian as width increases
  2. Repeat with structured sparse weights and compare convergence rate to IID case
  3. Apply edge-of-chaos analysis using the derived covariance recursions and compare training stability to IID initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the sharpest variant of condition (iii) in the PSEUDO-IID definition, and how does it impact the convergence rate of neural networks to Gaussian processes?
- Basis in paper: [explicit] The paper states "the sharpest variant of Definition 1 condition (iii) remains an open question; its importance is expanded upon in Appendix D."
- Why unresolved: The authors acknowledge that while they have a sufficient condition for PSEUDO-IID distributions, they have not identified the tightest possible formulation of condition (iii), which relates to the bounded moments of the weight matrices.
- What evidence would resolve it: Mathematical analysis proving the optimal form of condition (iii) and experimental validation showing its impact on convergence rates across different network architectures and weight distributions.

### Open Question 2
- Question: How do finite-width corrections affect the Gaussian process approximation for PSEUDO-IID networks, and can we derive explicit bounds on the approximation error?
- Basis in paper: [inferred] The authors mention "the theory presented here can be further refined to determine finite dimensional corrections, following the approach of Roberts et al. (2022), such as the rate of convergence of these quantities and the variance of these quantities for finite dimensions."
- Why unresolved: While the paper establishes convergence to Gaussian processes in the infinite-width limit, it does not provide quantitative bounds on the approximation quality for finite networks, which is crucial for practical applications.
- What evidence would resolve it: Mathematical derivation of explicit error bounds for finite-width approximations and numerical experiments validating these bounds across different network sizes and weight distributions.

### Open Question 3
- Question: Can the PSEUDO-IID framework be extended to more complex network architectures beyond fully-connected and convolutional networks, such as residual networks or attention-based models?
- Basis in paper: [inferred] The authors conclude by stating "Theorems 1 and 2 can be expected to form the foundation of developing initialization theory for networks designed for greater efficiency...Moreover, the theory presented here can be further refined to determine finite dimensional corrections."
- Why unresolved: The paper focuses on fully-connected and convolutional networks, but modern deep learning architectures often incorporate more complex structures that may not fit directly into the PSEUDO-IID framework.
- What evidence would resolve it: Extension of the PSEUDO-IID proof technique to additional network architectures, accompanied by numerical validation of Gaussian process behavior in these more complex settings.

## Limitations

- Exchangeability requirements may be more restrictive than suggested; breaking exchangeability in structured patterns could invalidate the limit theorems
- The bounded eighth-moment condition may be difficult to verify for complex structured distributions
- Edge-of-chaos application is theoretically enabled but not empirically demonstrated

## Confidence

- Gaussian process convergence for pseudo-IID weights: High
- Extension to convolutional networks: Medium (novel but follows logically)
- Edge-of-chaos analysis applicability: Medium-Low (enabled but not demonstrated)

## Next Checks

1. Verify that structured sparse matrices maintain exchangeability under all required permutations by explicitly checking the covariance structure
2. Test the bounded moment condition empirically across different structured weight patterns and scaling regimes
3. Demonstrate edge-of-chaos stability empirically by comparing training dynamics of structured vs IID initialized networks at criticality