---
ver: rpa2
title: 'PASTA: Pretrained Action-State Transformer Agents'
arxiv_id: '2307.10936'
source_url: https://arxiv.org/abs/2307.10936
tags:
- learning
- tasks
- arxiv
- masking
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents PASTA, a method for pre-training action-state
  transformer agents in reinforcement learning. It compares tokenization at the component
  level (individual state and action components) versus modality level (entire states
  and actions), finding component-level tokenization significantly outperforms modality-level.
---

# PASTA: Pretrained Action-State Transformer Agents

## Quick Facts
- **arXiv ID**: 2307.10936
- **Source URL**: https://arxiv.org/abs/2307.10936
- **Reference count**: 19
- **Primary result**: Component-level tokenization and first-principles pre-training objectives achieve strong performance in RL tasks with fewer than 10M parameters

## Executive Summary
PASTA introduces a pretraining approach for action-state transformer agents in reinforcement learning that emphasizes simplicity and efficiency. The method breaks from conventional practice by using component-level tokenization (individual state/action components rather than whole vectors) and first-principles pre-training objectives like random masking and next token prediction. Despite containing fewer than 10 million parameters, PASTA models demonstrate strong performance across behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation tasks. The approach also enables parameter-efficient fine-tuning with fewer than 10,000 parameters, making it both practical and reproducible.

## Method Summary
PASTA pre-trains transformer models on state-action trajectories using component-level tokenization and simple self-supervised objectives (C-GPT for next token prediction, C-BERT for random masking). The method processes state-action trajectories as sequences of individual components rather than entire vectors, using mu-law encoding and quantization for discretization. Pre-training occurs on datasets collected from multiple Brax environments (HalfCheetah, Hopper, Walker2d) simultaneously. Fine-tuning employs parameter-efficient methods like IA3 adapters with fewer than 10,000 parameters. The approach is evaluated across four downstream tasks: Imitation Learning, Offline RL, Sensor Failure robustness, and Dynamics Change adaptation, with zero-shot transfer capabilities.

## Key Results
- Component-level tokenization significantly outperforms modality-level tokenization in all evaluated tasks
- First-principles pre-training objectives (random masking, next token prediction) match or exceed performance of complex, task-specific objectives
- Multi-domain pre-training on datasets from multiple environments simultaneously improves performance across all environments compared to single-domain pre-training
- Models with fewer than 10 million parameters achieve strong performance while enabling parameter-efficient fine-tuning with fewer than 10,000 parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Component-level tokenization significantly improves performance over modality-level tokenization.
- **Mechanism**: By breaking states and actions into individual components, the model can capture fine-grained temporal and intra-modality dependencies that would be lost if treating states/actions as monolithic entities.
- **Core assumption**: The underlying physics and control dynamics are better represented when state/action components are treated as individual sequence elements rather than as whole vectors.
- **Evidence anchors**:
  - [abstract]: "Tokenization at the component level for actions and states" and "significantly outperforms tokenization at the modality level"
  - [section]: "Instead of considering each trajectory as a sequence of state, action (and often return) tuples... we break the sequences down into individual state and action components"
- **Break condition**: If the state/action components are highly correlated or if the environment dynamics are better captured at the modality level, component-level tokenization may not provide benefits.

### Mechanism 2
- **Claim**: First-principles pre-training objectives (random masking, next token prediction) match or outperform complex, task-specific objectives.
- **Mechanism**: Simple objectives like BERT's masked language modeling and GPT's next token prediction provide sufficient signal for learning generalizable representations without the need for intricate masking schedules or multi-task objectives.
- **Core assumption**: The underlying structure of RL trajectories can be effectively learned through basic self-supervised tasks without requiring specialized objectives like those in SMART or MTM.
- **Evidence anchors**:
  - [abstract]: "using fundamental pre-training objectives like next token prediction or masked language modeling"
  - [section]: "we compare first principles tokenization techniques... with state-of-the-art transformer RL methods... which incorporate more customized design choices"
- **Break condition**: If the downstream tasks require specific temporal dependencies or if the data distribution is highly complex, more sophisticated objectives might be necessary.

### Mechanism 3
- **Claim**: Pre-training on multi-domain datasets improves generalization across all environments compared to single-domain pre-training.
- **Mechanism**: Simultaneous exposure to diverse environments forces the model to learn more general representations that capture common underlying patterns across domains, leading to better transfer performance.
- **Core assumption**: There are shared structural patterns across different control environments that can be learned and leveraged for better generalization.
- **Evidence anchors**:
  - [abstract]: "Simultaneously pre-training the model on datasets from the three environments leads to enhanced performance across all three environments"
  - [section]: "We also compare the performance of C-GPT against specialized models trained independently in each environment (C-GPT (single-domain))"
- **Break condition**: If the environments are too dissimilar or if the model capacity is insufficient to capture shared patterns, multi-domain pre-training might not provide benefits.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs)
  - Why needed here: The paper's entire framework is built on MDP theory, defining states, actions, rewards, and policies
  - Quick check question: What are the four components of an MDP tuple M = {S, A, P, R, γ}?

- **Concept**: Self-supervised learning
  - Why needed here: The pre-training approach relies on self-supervised objectives like masked language modeling and next token prediction
  - Quick check question: How does self-supervised learning differ from supervised learning in terms of label requirements?

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: The core model uses transformer networks with attention to process sequential state-action data
  - Quick check question: What is the difference between causal and bidirectional attention in transformer models?

## Architecture Onboarding

- **Component map**: Tokenizer → Transformer encoder → Output projection → Fine-tuning head
- **Critical path**: Tokenization → Transformer processing → Token prediction → Fine-tuning adaptation → Downstream task execution
- **Design tradeoffs**:
  - Token granularity: Component-level vs modality-level affects sequence length and computational cost
  - Pre-training objective: Simple (BERT/GPT) vs complex (SMART/MTM) affects training stability and generalization
  - Model capacity: ~10M parameters balances performance with practical hardware requirements
- **Failure signatures**:
  - Poor tokenization: Vanishing gradients or unstable training
  - Inadequate masking: Overfitting to training distribution or poor generalization
  - Insufficient context window: Loss of long-term dependencies in trajectories
- **First 3 experiments**:
  1. Component-level vs modality-level tokenization ablation study on a single environment
  2. Simple vs complex pre-training objective comparison on multi-domain data
  3. Single-domain vs multi-domain pre-training performance evaluation across environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PASTA models scale with larger model sizes (e.g., 100M+ parameters) while maintaining component-level tokenization and first-principles objectives?
- Basis in paper: [inferred] The paper explicitly states that PASTA models contain fewer than 10 million parameters and demonstrate strong performance. However, it does not investigate the scaling behavior with larger models.
- Why unresolved: The paper focuses on demonstrating effectiveness with small, efficient models, but does not explore whether these findings hold for larger architectures.
- What evidence would resolve it: Training and evaluating PASTA models with 100M+ parameters using the same component-level tokenization and first-principles objectives, then comparing performance against smaller models and state-of-the-art methods.

### Open Question 2
- Question: How robust are PASTA models to different tokenization granularities beyond component-level (e.g., mixing component-level and modality-level tokenization, or using hierarchical tokenization schemes)?
- Basis in paper: [explicit] The paper compares component-level versus modality-level tokenization and finds component-level significantly outperforms modality-level. It does not explore intermediate or hybrid approaches.
- Why unresolved: The study establishes a binary comparison but leaves open whether other tokenization strategies might yield even better performance.
- What evidence would resolve it: Experimenting with hybrid tokenization schemes that combine component-level and modality-level approaches, or hierarchical tokenization where different parts of the state/action space use different granularities.

### Open Question 3
- Question: How do PASTA models perform in environments with high-dimensional visual observations compared to their strong performance with low-dimensional state vectors?
- Basis in paper: [inferred] The paper focuses exclusively on low-dimensional state and action vectors from Brax environments. While it mentions SMART's focus on visual observations, it does not test PASTA's capability with visual inputs.
- Why unresolved: The study demonstrates effectiveness in control tasks with compact state representations but does not address the more challenging scenario of learning from raw visual observations.
- What evidence would resolve it: Applying PASTA's component-level tokenization and first-principles objectives to environments with visual observations (e.g., Atari, DeepMind Control Suite), potentially with a visual tokenizer that can capture spatial relationships at the component level.

## Limitations

- Generalization scope is limited to three Brax environments with structured state-action spaces, raising questions about performance in more complex domains
- Ablation studies lack systematic exploration of interactions between tokenization granularity and pre-training objectives
- The mechanism behind multi-domain pre-training benefits remains unclear without representation analysis or interpretability studies

## Confidence

- **High Confidence**: The empirical results comparing component-level tokenization against modality-level tokenization are well-supported by controlled ablation studies with substantial and consistent performance gains
- **Medium Confidence**: The claim that first-principles pre-training objectives match or outperform complex alternatives is supported but lacks head-to-head comparisons with all major state-of-the-art methods
- **Medium Confidence**: Multi-domain pre-training benefits are demonstrated through direct comparison but the underlying mechanism remains unexplained

## Next Checks

1. **Cross-domain transfer test**: Evaluate PASTA models on entirely different control domains (e.g., robotic manipulation or navigation tasks) that share minimal structural similarity with Brax environments to test the limits of learned representations

2. **Representation analysis**: Conduct activation similarity analysis and probing tasks to verify whether multi-domain pre-training actually learns shared representations versus simply memorizing environment-specific patterns

3. **Objective interaction study**: Systematically test combinations of tokenization granularity and pre-training objectives (e.g., component-level with complex objectives) to determine whether the benefits are additive, synergistic, or independent