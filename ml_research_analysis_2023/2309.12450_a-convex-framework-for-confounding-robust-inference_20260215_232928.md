---
ver: rpa2
title: A Convex Framework for Confounding Robust Inference
arxiv_id: '2309.12450'
source_url: https://arxiv.org/abs/2309.12450
tags:
- kcmc
- policy
- pobs
- estimator
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a general convex framework for sensitivity
  analysis under unobserved confounders in offline contextual bandits. The key contribution
  is a novel estimator based on kernel conditional moment constraints (KCMC) that
  provides a sharp lower bound of the policy value, overcoming the limitations of
  previous approaches that relied on coarse relaxations leading to overly conservative
  estimates.
---

# A Convex Framework for Confounding Robust Inference

## Quick Facts
- arXiv ID: 2309.12450
- Source URL: https://arxiv.org/abs/2309.12450
- Reference count: 10
- Key outcome: Novel KCMC estimator provides sharp lower bounds for policy value under unobserved confounders

## Executive Summary
This paper introduces a general convex framework for sensitivity analysis under unobserved confounders in offline contextual bandits. The key contribution is a novel estimator based on kernel conditional moment constraints (KCMC) that provides a sharp lower bound of the policy value, overcoming limitations of previous approaches that relied on coarse relaxations leading to overly conservative estimates. The KCMC estimator reformulates the sensitivity analysis problem as an empirical risk minimization, enabling the use of standard M-estimation techniques to derive strong theoretical guarantees including consistency and asymptotic normality. Experiments demonstrate the effectiveness of the KCMC estimator in providing tighter bounds compared to baselines, especially in settings with continuous action spaces or f-sensitivity models.

## Method Summary
The KCMC estimator reformulates the sensitivity analysis problem as an empirical risk minimization using strong duality, enabling convex programming instead of intractable infinite-dimensional constraints. The method uses kernel ridge regression to approximate conditional moments, with orthogonal functions obtained via kernel PCA. The convex optimization problem is solved to find inverse probability weights, which are then used to compute a sharp lower bound estimate of the policy value. The framework allows for various extensions including sensitivity analysis with f-divergence, model selection via cross-validation or information criteria, and robust policy learning using the sharp lower bound.

## Key Results
- KCMC estimator provides tighter bounds than ZSB baseline on synthetic binary data with varying Γ
- Sharp bounds can be achieved with low-rank KCMC constraints under certain conditions
- Asymptotic normality enables construction of confidence intervals for policy value estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel conditional moment constraints (KCMC) provide a tractable approximation of infinite-dimensional conditional moment constraints
- Mechanism: The KCMC estimator reformulates the sensitivity analysis problem as an empirical risk minimization using strong duality, enabling convex programming instead of intractable infinite-dimensional constraints
- Core assumption: The low-rank approximation via kernel PCA preserves sufficient information about the conditional moments while making the problem computationally tractable
- Evidence anchors:
  - [abstract] "the key contribution is a novel estimator based on kernel conditional moment constraints (KCMC) that provides a sharp lower bound"
  - [section 3] "With the kernel method, we develop a tractable approximation of the infinite dimensional conditional moment constraints"
  - [corpus] Weak evidence - related papers mention kernel methods but don't explicitly discuss this approximation mechanism
- Break condition: If the low-rank approximation loses critical information about the conditional moments, the estimator will fail to provide sharp bounds

### Mechanism 2
- Claim: Strong duality enables reformulation as empirical risk minimization, providing theoretical guarantees
- Mechanism: The convex optimization problem can be transformed into a dual problem where the objective becomes an empirical risk minimization, allowing application of M-estimation theory
- Core assumption: The feasibility sets have non-empty relative interior, satisfying Slater's constraint qualification for strong duality
- Evidence anchors:
  - [abstract] "our estimation method can be reformulated as an empirical risk minimization problem thanks to the strong duality"
  - [section 4.1] "we know that the strong duality holds for the above convex optimizations, as their feasibility sets have a non-empty relative interior"
  - [corpus] Weak evidence - related papers mention duality but don't explicitly discuss this reformulation mechanism
- Break condition: If Slater's condition is violated, strong duality fails and the M-estimation framework cannot be applied

### Mechanism 3
- Claim: The KCMC estimator can achieve zero specification error under certain conditions
- Mechanism: When the true dual solution lies in the span of the chosen orthogonal functions, the KCMC approximation exactly captures the conditional moment constraints
- Core assumption: The orthogonal function set {ψd(t,x)}D contains sufficient basis functions to represent the true dual solution
- Evidence anchors:
  - [section 4.2] "Let η*CMC(t,x) be the solution of dual problem (33) for V CMC inf (π). Then, if η*CMC ∈ span ({ψ1,...,ψD}), we have V CMC inf (π) = V KCMC inf (π)"
  - [corpus] Weak evidence - related papers don't explicitly discuss specification error conditions
- Break condition: If the true dual solution requires basis functions outside the chosen span, specification error will occur and bounds will be loose

## Foundational Learning

- Concept: Kernel methods and reproducing kernel Hilbert spaces (RKHS)
  - Why needed here: The KCMC estimator relies on kernel ridge regression to approximate conditional moments, which requires understanding of RKHS theory
  - Quick check question: What is the reproducing property of a kernel and why is it important for the KCMC approximation?

- Concept: Convex duality and M-estimation theory
  - Why needed here: The theoretical guarantees for consistency and asymptotic normality rely on reformulating the problem as an empirical risk minimization and applying M-estimation theory
  - Quick check question: What are the conditions required for strong duality to hold in convex optimization?

- Concept: Sensitivity analysis and confounding in causal inference
  - Why needed here: The problem setting involves unobserved confounders and the goal is to provide robust policy evaluation under worst-case confounding scenarios
  - Quick check question: What is the difference between unconfoundedness and the sensitivity analysis framework used in this paper?

## Architecture Onboarding

- Component map: Data preprocessing → Kernel approximation → Optimization → Policy evaluation
- Critical path: Data → Kernel approximation → Optimization → Policy evaluation
- Design tradeoffs:
  - Higher rank KCMC constraints provide tighter bounds but increase computational cost
  - Different kernel choices affect approximation quality and computational efficiency
  - Trade-off between sharp bounds and computational tractability
- Failure signatures:
  - Very loose bounds compared to baseline methods indicate poor kernel approximation
  - Non-convergence of optimization suggests infeasible constraints or numerical issues
  - High variance in estimates suggests insufficient sample size or poor kernel choice
- First 3 experiments:
  1. Verify KCMC estimator provides tighter bounds than ZSB baseline on synthetic binary data with varying Γ
  2. Test specification error convergence by increasing rank of KCMC constraints and measuring bound improvement
  3. Validate asymptotic normality by constructing confidence intervals and checking coverage rates on simulated data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the conditions under which the specification error in the KCMC estimator is zero for general policy evaluation, not just average treatment effect estimation?
- Basis in paper: [explicit] The paper mentions that Dorn and Guo [2022] showed the specification error is zero for average treatment effect estimation with Tan's marginal sensitivity model, but states this result does not extend to general policy evaluation, non-binary action spaces, and general box and f-constraints.
- Why unresolved: The paper states it is an open question they could not address, as deriving realizability results for more general settings is difficult.
- What evidence would resolve it: A proof showing the existence of a minimizer of the KCMC objective that is realizable for general policy evaluation, non-binary action spaces, and general box and f-constraints.

### Open Question 2
- Question: What is the magnitude of the specification error when the kernel cannot be chosen optimally to achieve zero bias in the KCMC estimator?
- Basis in paper: [explicit] The paper discusses the specification error of the KCMC lower bound and provides an upper bound of the specification error when the condition for no specification error is violated, but states deriving a uniform bound on the magnitude of the specification error is difficult.
- Why unresolved: The paper states deriving such a uniform bound is difficult, and only provides pointwise error bounds for fixed policy.
- What evidence would resolve it: A derivation of a uniform bound on the magnitude of the specification error for the KCMC estimator under general conditions.

### Open Question 3
- Question: How does the choice of orthogonal functions in the KCMC affect the bias and variance of the estimator?
- Basis in paper: [inferred] The paper discusses using kernel PCA to obtain orthogonal functions for the KCMC, and shows that this can reduce the specification error under certain conditions. However, it does not analyze the impact of the choice of orthogonal functions on the bias and variance of the estimator.
- Why unresolved: The paper does not provide a theoretical analysis of the bias-variance tradeoff for different choices of orthogonal functions in the KCMC.
- What evidence would resolve it: A theoretical analysis showing how the choice of orthogonal functions in the KCMC affects the bias and variance of the estimator, potentially leading to guidelines for choosing optimal orthogonal functions.

## Limitations

- The KCMC approximation may fail to capture true conditional moments when the dual solution requires basis functions outside the chosen span, leading to specification error
- Performance is primarily validated on synthetic data with known confounding structure, limiting generalizability to complex real-world scenarios
- The kernel choice and its hyperparameters are not thoroughly explored, potentially affecting estimator performance across different data distributions

## Confidence

- **High confidence** in the theoretical framework and duality arguments, as they follow established convex optimization principles
- **Medium confidence** in practical performance claims, given that experiments are primarily on synthetic data with known confounding structure
- **Low confidence** in generalizability to highly complex real-world scenarios with unknown confounding patterns

## Next Checks

1. **Specification error analysis**: Systematically vary the rank of KCMC constraints and measure the gap between the KCMC lower bound and the true policy value on synthetic data with known ground truth. This would quantify the approximation error.

2. **Kernel sensitivity test**: Compare the KCMC estimator's performance using different kernel functions (linear, RBF, polynomial) and their hyperparameters on the same datasets to assess robustness to kernel choice.

3. **Real-world scalability validation**: Apply the KCMC estimator to a more complex real-world dataset with multiple continuous treatment variables and compare results with domain expert analysis of potential confounding effects.