---
ver: rpa2
title: 'AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents'
arxiv_id: '2310.09971'
source_url: https://arxiv.org/abs/2310.09971
tags:
- amago
- learning
- arxiv
- figure
- timesteps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMAGO advances in-context RL by combining off-policy actor-critic
  learning with long-sequence Transformers, enabling stable training on entire rollouts
  in parallel. It overcomes key bottlenecks in memory length, model size, and planning
  horizon by sharing a single Transformer between actor and critic networks without
  gradient detachment, using multi-gamma updates for long-horizon credit assignment,
  and introducing hindsight instruction relabeling for multi-step goals.
---

# AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents

## Quick Facts
- arXiv ID: 2310.09971
- Source URL: https://arxiv.org/abs/2310.09971
- Authors: 
- Reference count: 40
- Key outcome: AMAGO achieves state-of-the-art performance in POPGym's meta-RL and memory benchmarks, scaling to context lengths up to 10,000 timesteps.

## Executive Summary
AMAGO advances in-context reinforcement learning by combining off-policy actor-critic learning with long-sequence Transformers, enabling stable training on entire rollouts in parallel. It overcomes key bottlenecks in memory length, model size, and planning horizon through a shared Transformer architecture, multi-gamma updates for credit assignment, and hindsight instruction relabeling for exploration. The method demonstrates strong stability and sample efficiency without extensive hyperparameter tuning, making it a robust baseline for long-horizon and sparse-reward RL tasks.

## Method Summary
AMAGO uses a shared Transformer architecture as a trajectory encoder for both actor and critic networks, avoiding the instability of gradient detachment by computing actor and critic losses in parallel. The method employs off-policy learning to train on entire rollouts simultaneously, using multi-gamma updates to improve credit assignment across different planning horizons. For exploration in sparse-reward goal-conditioned domains, AMAGO introduces hindsight instruction relabeling with multi-step goals. The approach is evaluated on POPGym's meta-RL and memory benchmarks as well as procedurally generated goal-conditioned domains like Crafter.

## Key Results
- Achieves state-of-the-art performance in POPGym meta-RL and memory benchmarks
- Scales to context lengths up to 10,000 timesteps, solving the Passive T-Maze memory experiment
- Outperforms existing methods in procedurally generated goal-conditioned domains like Crafter
- Demonstrates strong stability and sample efficiency without extensive hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared Transformer actor-critic avoids instability by detaching critic gradients from actor loss
- Mechanism: By computing actor and critic objectives in parallel and detaching critic gradients from the actor loss term, AMAGO prevents the actor from receiving conflicting updates from the critic. This allows simultaneous optimization without the instability seen in previous shared-actor-critic models.
- Core assumption: The critic can learn effectively without gradient flow through the actor loss, and the actor can still benefit from shared representations.
- Evidence anchors:
  - [abstract]: "without gradient detachment"
  - [section]: "carefully detach the critic from the actor loss"
  - [corpus]: Weak - no direct citation on gradient detachment approach
- Break condition: If the critic requires actor gradient flow for stable learning, or if the shared representation is insufficient for both tasks.

### Mechanism 2
- Claim: Multi-gamma learning improves credit assignment and planning by jointly optimizing value estimates at multiple discount factors
- Mechanism: Training critics and actors at multiple discount factors (γ) in parallel allows faster convergence of shorter-horizon value estimates while still learning long-horizon returns. The shared Transformer benefits from the diverse learning signals across γ values.
- Core assumption: Different γ values converge at different rates and provide complementary learning signals to the shared representation.
- Evidence anchors:
  - [abstract]: "using multi-gamma updates for long-horizon credit assignment"
  - [section]: "jointly optimizes many different values of γ in parallel"
  - [corpus]: Weak - no direct citation on multi-gamma learning approach
- Break condition: If the computational overhead of multiple γ values outweighs the benefit, or if the shared representation cannot effectively learn from the diverse value scales.

### Mechanism 3
- Claim: Hindsight instruction relabeling with multi-step goals enables exploration in sparse-reward goal-conditioned domains
- Mechanism: By relabeling trajectories with alternative instructions based on achieved subgoals, AMAGO creates automatic exploration plans. This is particularly effective for multi-step instructions where completing earlier subgoals naturally leads to discovering later ones.
- Core assumption: The goal space is rich enough that relabeling creates meaningful alternative instructions, and the agent can learn from these relabeled experiences.
- Evidence anchors:
  - [abstract]: "introducing a variant of HER for multi-step goals"
  - [section]: "relabeling multi-step instructions creates automatic plans"
  - [corpus]: Weak - no direct citation on multi-step hindsight relabeling
- Break condition: If the relabeling distribution is too sparse to provide useful learning signals, or if the agent cannot generalize from relabeled experiences.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: AMAGO treats CMDPs as POMDPs with additional inputs, requiring understanding of how agents infer state from observation history
  - Quick check question: What distinguishes a POMDP from an MDP, and why is this distinction important for in-context RL?

- Concept: Off-policy vs On-policy Reinforcement Learning
  - Why needed here: AMAGO uses off-policy learning to enable training on entire rollouts in parallel, a key departure from previous on-policy approaches
  - Quick check question: What are the main advantages and disadvantages of off-policy learning compared to on-policy learning in the context of in-context RL?

- Concept: Transformer Architecture and Attention Mechanisms
  - Why needed here: AMAGO uses Transformers as trajectory encoders, requiring understanding of how self-attention enables long-term memory without recurrent processing
  - Quick check question: How does the self-attention mechanism in Transformers enable efficient processing of long sequences compared to recurrent networks?

## Architecture Onboarding

- Component map: Trajectory encoder (Transformer) → timestep representations → actor/critic MLPs
- Critical path: Data collection → replay buffer → batch sampling → Transformer forward pass → actor/critic loss computation → gradient update
- Design tradeoffs: Shared vs separate sequence models (stability vs efficiency), multi-gamma learning (better credit assignment vs increased computation), hindsight relabeling (improved exploration vs potential noise in training data)
- Failure signatures: Gradient collapse in Transformers, performance plateaus due to insufficient exploration, instability when scaling to very long sequences, sensitivity to hyperparameters in multi-gamma setup
- First 3 experiments:
  1. Run AMAGO on a simple POPGym environment (like ConcentrationEasy) to verify basic functionality and stability
  2. Compare shared Transformer vs separate RNN architectures on a memory-intensive POPGym environment to observe memory benefits
  3. Test multi-gamma learning by training with and without it on a sparse-reward goal-conditioned task to measure exploration benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AMAGO's performance scale with context length beyond 10,000 timesteps?
- Basis in paper: [explicit] The paper notes AMAGO solves the Passive T-Maze memory experiment up to the GPU memory limit of l = H = 10,000 timesteps.
- Why unresolved: The paper stops at 10,000 timesteps due to GPU memory constraints, but does not explore performance at even longer context lengths.
- What evidence would resolve it: Experiments showing AMAGO's performance and stability on tasks with context lengths of 20,000+ timesteps, or theoretical analysis of how memory and credit assignment scale with context length.

### Open Question 2
- Question: How critical is the modified Transformer architecture (with Leaky ReLUs, extra LayerNorms, and σReparam) to AMAGO's stability and performance?
- Basis in paper: [explicit] The paper states that standard Pre-LayerNorm Transformers led to performance collapse, and that AMAGO's architectural modifications were necessary to prevent attention entropy collapse.
- Why unresolved: The paper does not provide ablation studies isolating the impact of each architectural change (e.g., just Leaky ReLUs vs. all three modifications together).
- What evidence would resolve it: Controlled experiments comparing AMAGO with different combinations of the three architectural modifications (standard Transformer, +Leaky ReLUs, +LayerNorms, +σReparam) on the same tasks.

### Open Question 3
- Question: How does AMAGO's sample efficiency compare to optimal baselines when using much higher update-to-data ratios?
- Basis in paper: [inferred] The paper notes that AMAGO's update-to-data ratios are "essentially never tuned" and that there is "significant room for improvement" in POPGym results, suggesting potential for better sample efficiency.
- Why unresolved: The paper does not explore systematically higher update-to-data ratios to find optimal trade-offs between sample efficiency and final performance.
- What evidence would resolve it: Experiments systematically varying update-to-data ratios (e.g., 10x, 100x, 1000x higher than current) and measuring both sample efficiency and final performance on benchmark tasks like POPGym or Crafter.

## Limitations

- Empirical scope remains limited to specific task domains without systematic comparison to alternative sequence model architectures
- Shared Transformer actor-critic design lacks theoretical guarantees for stability despite empirical success
- Multi-gamma learning approach has not been validated against simpler credit assignment methods
- Hindsight instruction relabeling assumes sufficient goal space diversity which may not hold in constrained environments

## Confidence

- High confidence: The general approach of using off-policy learning with Transformers for in-context RL is well-grounded, and the POPGym performance improvements are empirically demonstrated
- Medium confidence: The specific mechanisms (shared Transformer without gradient detachment, multi-gamma learning, hindsight relabeling) are supported by results but lack rigorous ablation studies
- Low confidence: Claims about the method's robustness and minimal hyperparameter tuning are based on limited experimental evidence

## Next Checks

1. **Ablation study on gradient detachment**: Compare AMAGO's shared Transformer approach against both separate sequence models and shared models with explicit gradient detachment to quantify the stability benefits claimed in the paper.

2. **Multi-gamma learning efficiency analysis**: Measure the computational overhead of multi-gamma updates against the performance gains, and test whether a subset of γ values provides similar benefits with reduced computation.

3. **Relabeling distribution validation**: Analyze the quality and diversity of relabeled goals in sparse-reward environments to verify that the hindsight instruction relabeling creates meaningful learning signals rather than noisy supervision.