---
ver: rpa2
title: Prevalence and prevention of large language model use in crowd work
arxiv_id: '2310.15683'
source_url: https://arxiv.org/abs/2310.15683
tags:
- summaries
- text
- llms
- workers
- crowd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated the prevalence of large language model (LLM)
  use among crowd workers and explored strategies to mitigate it. In an uninstructed
  text summarization task, the estimated LLM usage was around 30%.
---

# Prevalence and prevention of large language model use in crowd work

## Quick Facts
- arXiv ID: 2310.15683
- Source URL: https://arxiv.org/abs/2310.15683
- Reference count: 31
- Key outcome: LLM usage in crowd work estimated at ~30%, reduced by half with explicit requests and copy-paste blocking.

## Executive Summary
This study investigates the prevalence of large language model (LLM) use among crowd workers and evaluates strategies to mitigate it. The researchers found that approximately 30% of crowd workers used LLMs for text summarization tasks. This prevalence was significantly reduced by explicitly asking workers not to use LLMs and by disabling copy-pasting, each intervention cutting usage roughly in half. The study also reveals a trade-off: while LLM-generated summaries are of higher quality, preventing their use may compromise the quality of human-generated summaries, as workers produce fewer essential keywords when denied LLM assistance.

## Method Summary
The study used a crowd task setup where workers summarized scientific abstracts. The researchers trained an e5-base-v2 classifier on a mix of human and synthetic (LLM-generated) summaries to detect LLM usage. They estimated prevalence using three methods: classify-and-count, probabilistic classify-and-count, and adjusted classify-and-count. Interventions included explicit requests not to use LLMs and technical hurdles like disabling copy-paste. Quality and homogeneity were assessed via keyword retention and linguistic analysis.

## Key Results
- LLM usage prevalence estimated at ~30% in uninstructed summarization tasks.
- Explicit requests and copy-paste blocking each reduced LLM usage by about half.
- LLM-generated summaries were of higher quality but more homogeneous than human-generated ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly asking workers not to use LLMs and disabling copy-pasting each cut LLM usage roughly in half.
- Mechanism: Adding explicit prohibitions raises the perceived social cost of LLM use, while technical barriers (disabling copy-paste) increase the effort and reduce the ease of integrating LLM output.
- Core assumption: Workers are responsive to both normative requests and small frictions when choosing whether to use LLMs.
- Evidence anchors:
  - [abstract] "This prevalence was reduced by about half when workers were explicitly asked not to use LLMs and when copy-pasting was disabled."
  - [section] "When workers were directly requested not to use LLMs and shown the text to be summarized as an image (thus preventing copy-pasting), LLM usage almost halved, dropping from 27.6% to 15.9%."
- Break condition: If workers become habituated to LLM use or if new tools bypass the copy-paste restriction (e.g., selecting text directly), the effect will diminish.

### Mechanism 2
- Claim: LLM-generated summaries are of higher quality but more homogeneous than human-generated ones.
- Mechanism: LLMs are trained to optimize for coherence and coverage, producing summaries that preserve more keywords and key information but with less variation in style or phrasing.
- Core assumption: The training objective of LLMs prioritizes factual accuracy and informativeness over stylistic diversity.
- Evidence anchors:
  - [abstract] "LLM-generated summaries were of higher quality but more homogeneous than human-generated ones."
  - [section] "Using this metric as a proxy for quality, we found that summaries labeled as synthetic preserved more keywords (40.1%) than summaries labeled as human (31.2%)."
- Break condition: If LLMs are fine-tuned to increase diversity or if human workers receive targeted training, the gap in homogeneity may close.

### Mechanism 3
- Claim: Preventing LLM use may compromise summary quality, as workers told not to use LLMs produced summaries with fewer essential keywords.
- Mechanism: Workers who are denied LLM assistance may struggle to identify and retain all essential information, leading to less complete summaries.
- Core assumption: LLMs help workers extract and retain key content more effectively than unaided human summarization.
- Evidence anchors:
  - [abstract] "At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information."
- Break condition: If workers develop alternative strategies (e.g., structured note-taking or reference tools), the quality gap may narrow.

## Foundational Learning

- Concept: Binary classification and probability calibration
  - Why needed here: The study relies on a fine-tuned language model to classify summaries as LLM-generated or human, and uses calibration to adjust probability estimates.
  - Quick check question: What is the purpose of temperature scaling in model calibration?
- Concept: Linear probability models for effect estimation
  - Why needed here: The paper uses linear models to estimate the impact of each intervention on LLM usage and to analyze correlates.
  - Quick check question: How does misclassification bias affect the coefficients in a linear probability model?
- Concept: Heuristic-based validation
  - Why needed here: High-precision heuristics (e.g., copy-paste artifacts, time feasibility) are used to cross-validate model predictions.
  - Quick check question: Why might using only high-precision heuristics underestimate LLM usage?

## Architecture Onboarding

- Component map: Data collection (crowd task, logging) -> Detection pipeline (classifier, heuristics) -> Analysis pipeline (prevalence, effects, content analysis)
- Critical path: Worker completes task → System logs metadata → Detection model classifies → Aggregated metrics calculated → Experimental effects analyzed
- Design tradeoffs: Balancing detection accuracy vs. worker friction; using high-precision heuristics for validation but low recall; choosing between self-reports and model predictions
- Failure signatures: High false positive/negative rates in classification; worker adaptation to new tools; self-report bias
- First 3 experiments:
  1. Vary the strictness of LLM-use prohibitions and measure usage with both classifier and heuristics.
  2. Introduce new LLM bypass methods (e.g., image-to-text tools) and test if detection still works.
  3. Compare summary quality and diversity when workers are allowed vs. denied LLM use.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated summaries compare to human-generated summaries when evaluated by independent human raters?
- Basis in paper: [inferred] The paper mentions that LLM-generated summaries were of higher quality but more homogeneous than human-generated ones. However, it does not provide direct comparison from independent human raters.
- Why unresolved: The study used keyword retention and homogeneity metrics, but these may not fully capture the nuances of quality that human raters could provide.
- What evidence would resolve it: A study where independent human raters evaluate and compare the quality of LLM-generated and human-generated summaries without knowing their source.

### Open Question 2
- Question: What is the long-term impact of LLM-generated data on the performance of models trained on such data?
- Basis in paper: [explicit] The paper mentions that LLM-generated data may degrade subsequent models trained on it, but does not provide long-term empirical evidence.
- Why unresolved: While the concern is raised, there is a lack of empirical studies showing the long-term effects on model performance.
- What evidence would resolve it: Longitudinal studies tracking the performance of models trained on increasingly LLM-generated data over time.

### Open Question 3
- Question: How do different LLM models (e.g., GPT-4, Claude, PaLM) compare in terms of their impact on crowd work tasks?
- Basis in paper: [inferred] The paper discusses LLMs in general but does not differentiate between various models.
- Why unresolved: Different LLM models may have varying capabilities and impacts on crowd work tasks, which is not explored in the study.
- What evidence would resolve it: Comparative studies analyzing the performance and impact of different LLM models on specific crowd work tasks.

## Limitations
- Prevalence estimates are sensitive to classifier performance and method choice.
- Interventions tested are limited to two strategies; broader mitigation approaches unexplored.
- Quality trade-offs when preventing LLM use are correlational, not causal.

## Confidence
- **Mechanism 1**: Medium. Supported by within-study experimental results, but external validation is limited.
- **Mechanism 2**: Medium. Based on internal metrics, but not independently replicated.
- **Mechanism 3**: Low. The evidence is correlational and based on a single study design; causation is not firmly established.

## Next Checks
1. Validate classifier robustness by applying it to an independent dataset of crowd-sourced summaries and comparing prevalence estimates using multiple aggregation methods.
2. Test alternative intervention strategies (e.g., time limits, alternative input methods) and measure their impact on both usage rates and summary quality.
3. Conduct a longitudinal study to detect worker adaptation or circumvention behaviors over repeated exposure to the same interventions.