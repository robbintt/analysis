---
ver: rpa2
title: 'INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision
  Transformers'
arxiv_id: '2307.03712'
source_url: https://arxiv.org/abs/2307.03712
tags:
- abfp
- activations
- weights
- quantization
- integer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INT-FP-QSim, an open-source simulator for
  evaluating large language models (LLMs) and vision transformers at various numerical
  precisions and formats. The simulator supports 4-bit integer and floating-point
  weights combined with 4-bit or 8-bit activations.
---

# INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers

## Quick Facts
- arXiv ID: 2307.03712
- Source URL: https://arxiv.org/abs/2307.03712
- Reference count: 15
- Primary result: 4-bit weights with 8-bit activations maintain near-FP32 performance for LLMs and vision transformers

## Executive Summary
This paper introduces INT-FP-QSim, an open-source simulator for evaluating large language models (LLMs) and vision transformers at various numerical precisions and formats. The simulator supports 4-bit integer and floating-point weights combined with 4-bit or 8-bit activations, enabling flexible evaluation of post-training quantization methods like Adaptive Block Floating Point (ABFP), SmoothQuant, GPTQ, and RPTQ. Experiments on models including OPT, CodeGen, BERT, ViT, and Stable Diffusion show that 4-bit weights with 8-bit activations maintain near-FP32 performance, especially when combined with ABFP and SmoothQuant. ABFP-QAT and ABFP-SQ further improve accuracy, with ABFP-SQ achieving results close to fine-tuning for larger models. Vision models perform better at 4-bit activations than LLMs, indicating task-specific quantization behavior.

## Method Summary
INT-FP-QSim is an open-source simulator that evaluates LLMs and vision transformers at 4-bit weights with 4-bit or 8-bit activations using various quantization methods. The simulator supports ABFP, SmoothQuant, GPTQ, and RPTQ, and can be used for both post-training quantization and quantization-aware training. Models are evaluated on tasks including language modeling (perplexity), image classification, and diffusion, with comparisons to FP32 baselines across multiple model sizes.

## Key Results
- 4-bit weights with 8-bit activations maintain near-FP32 performance for LLMs
- ABFP-QAT and ABFP-SQ achieve results close to fine-tuning for larger models
- Vision models show better tolerance to 4-bit activations than LLMs
- ABFP combined with SmoothQuant provides optimal accuracy across model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ABFP improves low-precision model accuracy by dynamically scaling vectors of length n instead of entire matrices.
- Mechanism: For each vector of length n within a matrix, ABFP computes a scale factor as the maximum value in that vector. This scale is then used to quantize only that vector segment. By limiting quantization to smaller segments, ABFP reduces information loss from outliers and preserves critical values.
- Core assumption: Outliers in activation matrices are localized to certain vector segments, not uniformly distributed, so scaling per segment preserves accuracy better than global scaling.
- Evidence anchors:
  - [abstract] Mentions ABFP as a technique for accuracy recovery with 4-bit integer weights and activations.
  - [section] Describes ABFP computation: "si j = max(xi j) where xi j ∈ {xi 1, xi 2, . . ., xil |xi |n m}" and explains that this minimizes information loss.
  - [corpus] No direct evidence in corpus about ABFP; the related papers focus on FP8 and other quantization formats, not ABFP specifically.
- Break condition: If outliers are uniformly distributed across the matrix, vector-wise scaling offers no advantage over global scaling.

### Mechanism 2
- Claim: SmoothQuant improves quantization accuracy by transferring quantization difficulty from activations to weights.
- Mechanism: SmoothQuant applies a smoothing factor (set to 0.5 in experiments) to redistribute the dynamic range. It migrates the "hard-to-quantize" parts of activations to the weights, making activations easier to quantize while keeping overall model accuracy.
- Core assumption: LLM activations contain more outliers and varying ranges than weights, so shifting the burden to weights improves overall quantization stability.
- Evidence anchors:
  - [abstract] Lists SmoothQuant as one of the post-training quantization methods compared.
  - [section] Explains motivation: "LLM activations are harder to quantize than weights owing to the presence of a significant amount of outliers."
  - [corpus] Related work on SmoothQuant not directly present; corpus focuses on other low-bit formats.
- Break condition: If the weight distribution is already tight or activations are not significantly harder to quantize, the migration offers no benefit.

### Mechanism 3
- Claim: Using 4-bit integer weights with 8-bit floating point activations (W4A8) provides a better accuracy-latency tradeoff than using both at 4-bit.
- Mechanism: 8-bit floating point activations (E4M3 format) provide a wider dynamic range and more precise representation of outliers compared to 4-bit formats. Combined with 4-bit integer weights, this mixed precision reduces quantization error while still achieving significant compression.
- Core assumption: Activations benefit more from increased precision than weights, so allocating more bits to activations yields higher accuracy gains.
- Evidence anchors:
  - [abstract] States that 4-bit weights with 8-bit activations "maintain near-FP32 performance."
  - [section] Shows in Table V that W4A8 with ABFP achieves "model PPL close to FLOAT32" and combining with SmoothQuant further improves results.
  - [corpus] No direct evidence in corpus for W4A8 specifically; related works focus on FP8 training and quantization but not this exact mixed setup.
- Break condition: If hardware constraints make 8-bit activations too costly, or if model behavior does not depend heavily on activation precision, the benefit diminishes.

## Foundational Learning

- Concept: Quantization and de-quantization process (Q→DQ).
  - Why needed here: The simulator must accurately emulate low-precision arithmetic while keeping internal computations in higher precision (FLOAT32) to estimate real-world performance.
  - Quick check question: In the simulator, are weights and activations stored in lower precision during the forward pass?
    - Answer: No, they are stored in FLOAT32 after de-quantization; only the quantized values are used for matmul operations.

- Concept: Per-channel vs per-tensor scaling in quantization.
  - Why needed here: Different calibration strategies (MSE, max, ABFP) use different granularities of scaling, affecting quantization error and model accuracy.
  - Quick check question: Which calibration method uses per-channel max for weights and MSE for activations?
    - Answer: Static Quantization with MSE Calibration.

- Concept: Post-training quantization (PTQ) vs quantization-aware training (QAT).
  - Why needed here: PTQ applies calibration after training, while QAT simulates quantization during training to recover accuracy. The paper compares both.
  - Quick check question: Which method achieves PPL close to baseline FLOAT32 in the experiments?
    - Answer: ABFP-QAT (ABFP combined with QAT).

## Architecture Onboarding

- Component map: User provides model + quantizer functions -> Simulator replaces matmul layers with quantized versions -> Forward pass applies quantizers -> Optional calibration or QAT applied -> Performance metrics collected
- Critical path:
  1. User specifies model and quantizer functions
  2. Simulator replaces matmul layers with quantized versions
  3. Forward pass applies quantizers to inputs, weights, outputs
  4. Optional calibration or QAT applied
  5. Performance metrics collected
- Design tradeoffs:
  - Using FLOAT32 internally for matmuls gives upper-bound accuracy but not real hardware performance
  - ABFP vector length n trades memory overhead (storing scales) vs accuracy
  - PTQ methods like SmoothQuant and RPTQ have model compatibility constraints (e.g., OPT 350M errors)
- Failure signatures:
  - Calibration producing extreme scale values → clipping and accuracy collapse
  - Mismatch between quantizer function and model architecture → runtime errors
  - Over-aggressive vector length n in ABFP → loss of precision
- First 3 experiments:
  1. Run OPT 125M with W4A4 and ABFP (n=64) to verify basic functionality and compare to FLOAT32 baseline
  2. Test W4A8 with ABFP on the same model to evaluate mixed precision gains
  3. Apply ABFP-SQ on W4A8 and measure PPL improvement over ABFP alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 4-bit integer weights and 8-bit floating-point activations compare to 4-bit floating-point weights and 8-bit floating-point activations across different model sizes?
- Basis in paper: [explicit] The paper mentions comparing 4-bit integer weights with 8-bit integer activations to 4-bit floating-point weights with 8-bit floating-point activations (E4M3 format), but does not provide detailed comparative results for all model sizes.
- Why unresolved: The paper focuses on comparing integer and floating-point formats separately but does not provide a direct comparison between integer weights with floating-point activations and floating-point weights with floating-point activations across all model sizes.
- What evidence would resolve it: Experimental results showing the performance (e.g., perplexity, accuracy) of models using 4-bit integer weights with 8-bit floating-point activations versus 4-bit floating-point weights with 8-bit floating-point activations for each model size investigated.

### Open Question 2
- Question: What is the impact of second-level quantization of scale factors in ABFP on model performance and compression efficiency?
- Basis in paper: [explicit] The paper mentions that a second-level quantization for scale factors could be utilized to achieve further compression but does not explore it in the current work.
- Why unresolved: The authors chose not to explore second-level quantization to focus on best-case performance analysis, leaving the potential benefits and trade-offs unexplored.
- What evidence would resolve it: Experimental results comparing model performance and compression ratios with and without second-level quantization of scale factors in ABFP across various models and tasks.

### Open Question 3
- Question: How does the performance of models using RPTQ compare to ABFP when both are applied to 4-bit integer weights and 8-bit floating-point activations?
- Basis in paper: [explicit] The paper compares RPTQ and ABFP for 4-bit integer weights with 4-bit and 8-bit integer activations but does not include results for 8-bit floating-point activations.
- Why unresolved: The authors did not extend the comparison to include 8-bit floating-point activations, which could reveal different performance characteristics.
- What evidence would resolve it: Experimental results showing the performance of models using RPTQ versus ABFP with 4-bit integer weights and 8-bit floating-point activations across various models and tasks.

## Limitations
- ABFP's claimed advantages over global scaling methods lack empirical validation
- SmoothQuant's effectiveness depends on specific activation quantization difficulty assumptions
- Mixed precision results may not generalize across all hardware platforms

## Confidence
- High: The simulator's technical implementation and experimental methodology are well-documented and reproducible
- Medium: Claims about ABFP's accuracy benefits and SmoothQuant's effectiveness require more rigorous ablation studies
- Low: Generalization of results across diverse model architectures and tasks needs further validation

## Next Checks
1. **Ablation Study**: Run controlled experiments comparing ABFP against global scaling across multiple vector lengths (n=16, 32, 64, 128) on the same models to quantify the actual benefit and identify break conditions.

2. **Cross-Architecture Testing**: Evaluate the same quantization methods on architectures not mentioned in the paper (e.g., Llama, Phi models) to test generalizability of the findings.

3. **Hardware-Realistic Simulation**: Modify the simulator to perform matmul operations in the target precision (4-bit) rather than using FLOAT32 internally, then compare accuracy-latency tradeoffs with the current approach.