---
ver: rpa2
title: 'ResMem: Learn what you can and memorize the rest'
arxiv_id: '2302.01576'
source_url: https://arxiv.org/abs/2302.01576
tags:
- resmem
- training
- deepnet
- accuracy
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to boost the generalization performance
  of neural networks by explicitly memorizing training residuals. The core idea is
  to first train a base neural network, compute its prediction residuals on the training
  set, and then fit these residuals with a k-nearest neighbor (k-NN) regressor.
---

# ResMem: Learn what you can and memorize the rest

## Quick Facts
- arXiv ID: 2302.01576
- Source URL: https://arxiv.org/abs/2302.01576
- Reference count: 40
- Primary result: ResMem improves test accuracy by combining parametric ERM with non-parametric memorization of training residuals

## Executive Summary
This paper introduces ResMem, a method that boosts neural network generalization by explicitly memorizing training residuals. The approach first trains a base network, computes prediction residuals on the training set, and fits these residuals with a k-NN regressor. The final prediction is the sum of the base network output and the k-NN residual correction. ResMem is shown to improve test accuracy across vision and language benchmarks, particularly when the base network cannot easily memorize the training data. Theoretically, the method achieves faster convergence to zero test risk compared to the base predictor alone by combining parametric and non-parametric learning.

## Method Summary
ResMem decomposes the prediction task into two components: a base neural network that learns coarse structure through standard training, and a k-nearest neighbor regressor that memorizes the residuals (differences between true labels and base predictions). The method first trains the base network on the training data, then computes residuals using the trained model. These residuals are fit using k-NN regression with embeddings from a chosen layer of the base network. During inference, the final prediction is the sum of the base network output and the k-NN residual correction, weighted by temperature-scaled softmax probabilities. This approach is particularly effective when the base network has limited capacity or when explicit memorization is more efficient than implicit memorization through larger models.

## Key Results
- ResMem improves test accuracy on CIFAR-100, ImageNet, and causal language modeling tasks compared to base networks alone
- The method is most beneficial when the base network is small or when implicit memorization is infeasible
- Theoretical analysis shows ResMem achieves faster convergence to zero test risk compared to base predictor alone in a stylized linear regression setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ResMem improves test accuracy by decomposing the prediction into coarse structure learned by the base network and fine-grained details memorized by the kNN residual regressor.
- Mechanism: The base network captures large-scale patterns that are learnable within its capacity. The kNN component memorizes the residuals—differences between the true labels and base network predictions—capturing fine-grained distinctions the base network misses.
- Core assumption: The base network learns some structure correctly but fails on fine-grained distinctions; the residual memorization fills this gap.
- Evidence anchors:
  - [abstract] "Specifically, we posit that the initial DeepNet fDeepNet learns some coarse structure, and ResMem rkNN supplements the DeepNet prediction with fine-grained details"
  - [section] "The empirical analysis suggests the DeepNet in isolation can already learn some large scale structures, but is unable to make fine-grained distinctions. This is where ResMem helps"
- Break condition: If the base network already memorizes perfectly (zero residuals), the kNN component adds no value.

### Mechanism 2
- Claim: ResMem is especially beneficial when implicit memorization is infeasible due to large dataset size or model capacity constraints.
- Mechanism: In scenarios where the base network cannot achieve zero training error (hard generalization, hard interpolation, or efficiency constraints), ResMem provides explicit memorization through kNN, bypassing the need for implicit memorization.
- Core assumption: Implicit memorization via larger models is computationally expensive or infeasible; explicit memorization via kNN is more efficient.
- Evidence anchors:
  - [abstract] "We posit that fResMem yields the largest margin of improvement over fDeepNet when the learning task is 'hard' (and thus implicit memorization is infeasible)"
  - [section] "Smaller k or σ corresponds to putting higher weight on residuals of the closest neighboring training examples. For sufficiently small k and σ, fResMem achieves exact memorization"
- Break condition: If the base network can already achieve zero training error, the kNN component provides no additional benefit.

### Mechanism 3
- Claim: ResMem achieves faster convergence to zero test risk compared to base predictor alone through two-stage learning.
- Mechanism: The base network achieves parametric convergence rate (fast for small samples), while the kNN residual memorization achieves non-parametric convergence (slow but ensures asymptotic zero error), combining for overall faster convergence than either alone.
- Core assumption: The problem contains components that are learnable within the base network's capacity and components that require non-parametric memorization.
- Evidence anchors:
  - [abstract] "Theoretically, we formulate a stylized linear regression problem and rigorously show that ResMem results in a more favorable test risk over the base predictor"
  - [section] "Our analysis (Theorem 5.3) shows that as the number of samples goes to infinity, the test risk of the base predictor decreases to an irreducible constant, whereas the risk of ResMem decreases to zero"
- Break condition: If all components of the target function are learnable within the base network's capacity, the kNN component provides no asymptotic benefit.

## Foundational Learning

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: ResMem relies on first training a base network via ERM, then fitting residuals. Understanding ERM helps grasp why the base network's prediction is imperfect initially.
  - Quick check question: What happens to ERM's test risk as the number of samples approaches infinity when the hypothesis class doesn't contain the true function?

- Concept: k-Nearest Neighbors (kNN)
  - Why needed here: ResMem uses kNN to memorize residuals. Understanding kNN's properties (locality, non-parametric nature) is essential for understanding how it captures fine-grained details.
  - Quick check question: How does the choice of k affect the bias-variance tradeoff in kNN regression?

- Concept: Memorization vs. Generalization
  - Why needed here: ResMem explicitly memorizes residuals while aiming for generalization. Understanding this distinction helps explain why ResMem works even when implicit memorization is infeasible.
  - Quick check question: What is the difference between interpolation (memorization) and generalization in the context of neural network training?

## Architecture Onboarding

- Component map:
  - Base neural network (fDeepNet) -> Residual computation -> kNN regressor (rkNN) -> Final prediction (base + kNN correction)

- Critical path:
  1. Train base network on training data
  2. Compute residuals (true labels - base predictions)
  3. Extract embeddings from base network for nearest neighbor search
  4. Train kNN regressor on residuals using embeddings
  5. For inference: combine base network output with kNN residual correction

- Design tradeoffs:
  - k vs. σ: Smaller values increase locality but risk overfitting; larger values increase smoothing but may miss fine details
  - Temperature T: Higher values soften probability distributions, affecting both base network and kNN weight computation
  - Embedding layer choice: Affects nearest neighbor search quality; deeper layers capture more semantic information
  - Model size: Smaller base networks benefit more from ResMem but may have higher residual variance

- Failure signatures:
  - No improvement over base network: Likely means base network already memorizes perfectly or residuals are too noisy
  - Performance degradation: Could indicate poor hyperparameter choices (k, σ, T) or inappropriate embedding layer
  - High memory usage: Large kNN component with many neighbors can be memory-intensive
  - Slow inference: kNN search can be computationally expensive without approximation schemes

- First 3 experiments:
  1. CIFAR10 with ResNet8: Quick validation on a simpler dataset to verify implementation
  2. Varying k and σ: Grid search to find optimal hyperparameters for a specific dataset
  3. Base network ablation: Test with different network architectures to confirm larger networks benefit less from ResMem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ResMem scale when the base neural network is already highly memorizing (i.e., achieving very low training error)?
- Basis in paper: [explicit] The paper notes that ResMem is most beneficial when implicit memorization is infeasible, such as with smaller networks or larger datasets. It also states that ResMem yields the largest improvement when the base network is small and cannot easily interpolate the data.
- Why unresolved: The theoretical analysis assumes a linear regression setting where the base model cannot perfectly fit the data, but real neural networks may achieve near-zero training error. The empirical evaluation focuses on cases where the base model has non-trivial residuals.
- What evidence would resolve it: Controlled experiments comparing ResMem performance across networks of varying memorization capacity (e.g., ResNet vs. wider/deeper variants) on the same dataset, measuring both training and test error.

### Open Question 2
- Question: Can the two-stage sequential training of ResMem be improved by joint training of the base network and k-NN residual component?
- Basis in paper: [explicit] The discussion section explicitly mentions that the current formulation builds the base network and kNN components sequentially, and suggests joint training as a natural direction for future work.
- Why unresolved: The paper only evaluates the sequential approach and does not explore joint optimization strategies where the base network could be trained with awareness of the residual memorization component.
- What evidence would resolve it: Empirical comparison of sequential vs. joint training approaches on multiple benchmarks, measuring both accuracy gains and training efficiency.

### Open Question 3
- Question: How does ResMem perform under covariate shift or domain adaptation scenarios?
- Basis in paper: [inferred] The discussion mentions that ResMem could be promising for test-time covariate shift because the kNN component adapts predictions based on training samples closest to the test sample. However, this is not empirically evaluated.
- Why unresolved: All experiments use i.i.d. train/test splits without distribution shift. The theoretical analysis also assumes fixed data distribution.
- What evidence would resolve it: Experiments applying ResMem to domain adaptation benchmarks or controlled covariate shift scenarios, comparing against standard adaptation methods.

## Limitations

- The theoretical guarantees rely on idealized assumptions that may not translate to real neural networks with learned representations
- The empirical evaluation is limited in scope and does not extensively explore failure modes across diverse tasks and architectures
- The k-NN component introduces computational overhead during inference, which is not thoroughly discussed in terms of scalability

## Confidence

- **High Confidence**: The core mechanism of decomposing predictions into base network outputs and k-NN residual corrections is well-supported by empirical results and intuitive reasoning
- **Medium Confidence**: The theoretical analysis showing faster convergence to zero test risk is mathematically sound within the stylized setting but may not fully generalize to complex deep learning scenarios
- **Medium Confidence**: The claim that ResMem is especially beneficial when implicit memorization is infeasible is supported by empirical evidence but lacks extensive ablation studies across diverse hard learning scenarios

## Next Checks

1. **Ablation on Embedding Layer Choice**: Conduct experiments varying the layer from which embeddings are extracted for k-NN search to determine optimal feature representation for residual memorization
2. **Scalability Analysis**: Evaluate ResMem on larger-scale datasets (e.g., JFT-300M) to assess computational overhead and memory requirements of the k-NN component
3. **Failure Mode Analysis**: Systematically investigate scenarios where ResMem degrades performance, such as when base networks already achieve near-zero training error or when residuals are too noisy to memorize effectively