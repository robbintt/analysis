---
ver: rpa2
title: Hierarchical Generative Adversarial Imitation Learning with Mid-level Input
  Generation for Autonomous Driving on Urban Environments
arxiv_id: '2302.04823'
source_url: https://arxiv.org/abs/2302.04823
tags:
- learning
- agent
- policy
- input
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical Generative Adversarial Imitation
  Learning (hGAIL) architecture for autonomous driving in urban environments. The
  key idea is to decouple representation learning from the driving task by using a
  Conditional GAN to generate a Bird's-Eye View (BEV) representation from the vehicle's
  frontal camera images, sparse trajectory, and high-level command.
---

# Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments

## Quick Facts
- arXiv ID: 2302.04823
- Source URL: https://arxiv.org/abs/2302.04823
- Reference count: 40
- Key outcome: hGAIL achieves 98% intersection completion rate in new city after training on one city, compared to 26% for GAIL from cameras alone

## Executive Summary
This paper proposes a hierarchical Generative Adversarial Imitation Learning (hGAIL) architecture for autonomous driving in urban environments. The key innovation is decoupling representation learning from policy learning by using a Conditional GAN to generate a Bird's-Eye View (BEV) representation from frontal camera images, sparse trajectory, and high-level command. This BEV representation is then used as input to a GAIL module that learns the driving policy. The experiments show that hGAIL successfully navigates 98% of intersections in a new city, while GAIL exclusively from cameras fails to learn the task effectively.

## Method Summary
The hGAIL architecture consists of two main modules: a Conditional GAN that generates BEV representations from three frontal camera images, sparse trajectory points, and high-level commands, and a GAIL module that learns the driving policy based on the generated BEV. The BEV generation and policy learning are trained simultaneously while the agent interacts with the CARLA simulation environment. The policy uses a Beta distribution for bounded continuous actions, and training employs Proximal Policy Optimization (PPO) with Behavior Cloning augmentation. The system is trained on expert demonstrations from CARLA town01 and tested on intersection navigation in both familiar and new urban environments.

## Key Results
- hGAIL achieves 98% intersection completion rate in a new city after training on one city
- GAIL from cameras alone achieves only 26% intersection completion rate
- hGAIL outperforms both Behavior Cloning and GAIL with real BEV input
- The hierarchical architecture successfully handles complex urban navigation tasks including lane following, turning, and obstacle avoidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling representation learning from policy learning stabilizes training.
- Mechanism: The hierarchical structure uses a GAN to learn a BEV representation while a GAIL module learns the driving policy. This separation prevents high-dimensional image processing from destabilizing the policy training.
- Core assumption: The BEV representation captures essential environmental information needed for navigation, and learning it separately from the policy allows both to converge more reliably.
- Evidence anchors: [abstract] states the architecture "decouples representation learning from the driving task," and [section] describes the two-module structure. Corpus papers do not directly address this mechanism.

### Mechanism 2
- Claim: Interactive imitation learning with expert demonstrations improves policy robustness.
- Mechanism: GAIL learns from expert demonstrations while interacting with the environment, using a discriminator to distinguish between expert and learned behaviors.
- Core assumption: Expert demonstrations provide high-quality training data that can guide the policy toward safe and effective driving behaviors.
- Evidence anchors: [abstract] notes GAIL agents "learn from expert demonstrations while interacting with the environment," and [section] describes the adversarial training between policy and discriminator. Corpus papers do not provide direct evidence for this specific claim.

### Mechanism 3
- Claim: Beta distribution for bounded action space improves policy performance.
- Mechanism: Using a Beta distribution for the policy's action space allows the agent to model bounded continuous actions, which is more suitable for real-world applications like autonomous driving where actions are naturally bounded.
- Core assumption: The Beta distribution can effectively capture the nuances of driving actions such as steering and acceleration.
- Evidence anchors: [section] explains the use of Beta distribution "due to its bounded support, which allows us to model bounded continuous action distributions." Corpus papers do not discuss Beta distributions for action spaces in GAIL.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs are used to generate the BEV representation from frontal camera images, providing a mid-level input for the policy.
  - Quick check question: How does the GAN's generator learn to translate RGB images into BEV representations?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used to optimize the policy in GAIL, providing stable policy updates during training.
  - Quick check question: What is the role of the value function in PPO, and how does it contribute to variance reduction?

- Concept: Bird's-Eye View (BEV) representation
  - Why needed here: BEV provides a top-down view of the vehicle's environment, simplifying the policy's decision-making process.
  - Quick check question: How does the BEV representation encode information about the desired route and drivable area?

## Architecture Onboarding

- Component map: Frontal camera images + sparse trajectory + high-level command -> CGAN -> BEV representation -> GAIL policy + discriminator -> driving actions
- Critical path: 1) Frontal camera images, sparse trajectory, and high-level command are fed into the GAN 2) GAN generates BEV representation 3) BEV representation, current speed, and last actions are fed into the GAIL policy 4) GAIL outputs steering and acceleration commands
- Design tradeoffs: Decoupling representation learning from policy learning improves stability but adds complexity; using BEV representation simplifies the policy but requires accurate BEV generation; Beta distribution for action space allows bounded actions but may be harder to learn than Gaussian
- Failure signatures: GAN fails to generate accurate BEV representation leading to policy receiving incorrect input; GAIL policy does not converge due to discriminator failing to provide useful feedback; BEV representation does not capture critical features preventing effective policy learning
- First 3 experiments: 1) Train GAN alone on synthetic BEV data to verify its ability to generate accurate representations 2) Train GAIL policy with real BEV data to establish baseline for policy performance 3) Combine GAN and GAIL in hGAIL architecture and evaluate performance on simple driving tasks before moving to complex urban environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the hGAIL architecture perform in more complex urban environments with dynamic obstacles like pedestrians and other vehicles?
- Basis in paper: [explicit] The authors mention this as future work, stating they are interested in adding dynamic obstacles such as pedestrians and other vehicles, as well as traffic lights and other weather conditions.
- Why unresolved: The current work focuses on a simpler environment without these dynamic elements, so the model's robustness to more realistic scenarios remains untested.
- What evidence would resolve it: Running hGAIL in a simulated environment with pedestrians, other vehicles, and varying weather conditions, and evaluating its success rate in navigating intersections and handling unexpected obstacles.

### Open Question 2
- Question: How well does the hGAIL policy generalize to completely new cities not seen during training?
- Basis in paper: [explicit] The authors mention testing generalization to new cities as future work, having only tested on one city (town01) in their experiments.
- Why unresolved: While hGAIL showed good performance in the training city, its ability to adapt to unfamiliar urban layouts with different road structures and intersections is unknown.
- What evidence would resolve it: Training hGAIL on one city and then evaluating its performance in navigating intersections in several completely different cities, measuring success rates and comparing to agents trained specifically on those cities.

### Open Question 3
- Question: What is the impact of the BEV generation quality on the overall hGAIL performance, and how does it compare to using ground truth BEV?
- Basis in paper: [explicit] The authors compare hGAIL (which uses generated BEV) to a GAIL agent with real BEV input, showing that hGAIL performs better. However, they don't quantify the impact of BEV generation quality.
- Why unresolved: While the comparison shows hGAIL's superiority, it doesn't isolate the effect of BEV generation accuracy on policy learning. The quality of the generated BEV could be a bottleneck.
- What evidence would resolve it: Systematically degrading the quality of the generated BEV input to the GAIL module and measuring the corresponding drop in policy performance, comparing this to the performance of GAIL with ground truth BEV at various levels of BEV generation accuracy.

## Limitations

- The paper does not provide quantitative metrics for BEV generation quality, making it difficult to assess the impact of BEV accuracy on policy performance
- Testing is limited to one training city and one test city, with limited evaluation of generalization to diverse urban environments
- The hierarchical architecture adds complexity compared to end-to-end approaches, potentially limiting real-time deployment capabilities

## Confidence

- **High confidence**: The hierarchical architecture improves urban navigation performance compared to GAIL without BEV representation
- **Medium confidence**: The BEV representation meaningfully simplifies the learning task for the policy
- **Low confidence**: The Beta distribution for action space is the primary reason for handling extreme driving scenarios

## Next Checks

1. **Quantify BEV quality**: Measure BEV reconstruction error on held-out expert data to establish baseline BEV generation quality before policy training

2. **Cross-city generalization**: Test hGAIL trained on town01 in town02 and town03 with varying intersection complexity to validate robustness claims

3. **Ablation of BEV channels**: Remove individual BEV channels (route, vehicle, drivable area) to determine which features are most critical for policy performance