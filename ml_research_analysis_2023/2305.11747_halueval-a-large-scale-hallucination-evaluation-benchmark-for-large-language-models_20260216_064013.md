---
ver: rpa2
title: 'HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language
  Models'
arxiv_id: '2305.11747'
source_url: https://arxiv.org/abs/2305.11747
tags:
- hallucination
- hallucinated
- answer
- chatgpt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HaluEval, a large-scale hallucination evaluation
  benchmark for large language models (LLMs). The key idea is to automatically generate
  hallucinated samples using a ChatGPT-based two-step framework: first sampling diverse
  hallucinated content using task-specific instructions, then filtering to select
  the most plausible samples.'
---

# HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2305.11747
- Source URL: https://arxiv.org/abs/2305.11747
- Authors: 
- Reference count: 16
- Primary result: Existing LLMs, including ChatGPT, struggle to recognize hallucinations in text; providing external knowledge or reasoning steps can improve detection accuracy

## Executive Summary
This paper introduces HaluEval, a large-scale benchmark for evaluating hallucination detection in large language models across three tasks: question answering, knowledge-grounded dialogue, and text summarization. The benchmark is generated using a ChatGPT-based two-step framework that first samples diverse hallucinated content and then filters to select the most plausible samples. The authors also include human annotations of hallucinations in ChatGPT responses. Experimental results show that existing LLMs perform poorly at recognizing hallucinations, but accuracy improves significantly when external knowledge is provided. The benchmark reveals that hallucination detection is topic-sensitive and that certain hallucination patterns (comprehension, factualness, specificity, inference) pose varying levels of difficulty for LLMs.

## Method Summary
The benchmark is created using a two-step ChatGPT-based framework. First, diverse hallucinated samples are generated using task-specific instructions with different sampling methods (one-pass vs conversational). Second, the most plausible hallucinated samples are selected using example-enhanced filtering with ground-truth demonstrations. The benchmark includes 35,000 samples across three tasks, with 5,000 human-annotated ChatGPT responses. LLMs are evaluated on their ability to detect hallucinations using temperature=0 for deterministic outputs. The evaluation also tests whether providing external knowledge from Wikipedia or adding reasoning steps improves hallucination recognition accuracy.

## Key Results
- Existing LLMs struggle to recognize hallucinations, with ChatGPT achieving only 62.59% accuracy on QA tasks
- Providing external knowledge significantly improves hallucination recognition (accuracy increases from 62.59 to 76.83 in QA)
- ChatGPT faces particular challenges with hallucinations in technology, climate, and language topics
- Over half of detection failures in QA and dialogue are due to type-I hallucination patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The two-step sampling-then-filtering approach improves hallucination diversity and plausibility
- **Mechanism:** First step uses task-specific instructions with different sampling methods to generate diverse hallucinated content. Second step uses example-enhanced filtering with ground-truth demonstrations to select the most plausible hallucinated sample
- **Core assumption:** ChatGPT can follow complex instructions with role-play and generate diverse hallucinations based on different patterns
- **Evidence anchors:** [abstract] "we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering"; [section 2.1] describes two different sampling methods and filtering instruction design

### Mechanism 2
- **Claim:** Providing external knowledge improves LLM hallucination recognition accuracy
- **Mechanism:** When evaluating whether text contains hallucinations, supplying retrieved knowledge from Wikipedia helps the model make more accurate judgments by providing factual context
- **Core assumption:** LLMs lack sufficient world knowledge to identify factual contradictions without external support
- **Evidence anchors:** [abstract] "providing external knowledge or adding reasoning steps can help improve hallucination recognition"; [section 3.2.2] shows accuracy increases from 62.59 to 76.83 in QA

### Mechanism 3
- **Claim:** Topic sensitivity affects hallucination recognition performance
- **Mechanism:** LLMs perform differently on hallucination detection across different topics, with some topics being more challenging than others
- **Core assumption:** The model's knowledge distribution and training data may not be uniform across all topics
- **Evidence anchors:** [section 3.2.1] identifies technology, climate, and language as challenging topics; [section 2.3] finds hallucination is topic-sensitive with visualization of failed samples

## Foundational Learning

- **Concept:** Prompt engineering with role-play instructions
  - Why needed here: The benchmark relies on carefully crafted prompts to generate and evaluate hallucinations
  - Quick check question: Can you design a prompt that asks the model to act as a "hallucination generator" while following specific patterns?

- **Concept:** Hallucination taxonomy and pattern recognition
  - Why needed here: Understanding different types of hallucinations (comprehension, factualness, specificity, inference) is crucial for creating and evaluating the benchmark
  - Quick check question: Can you categorize a given hallucination example into one of the four types mentioned in the paper?

- **Concept:** Evaluation metrics for text generation
  - Why needed here: The benchmark requires understanding how to measure hallucination recognition accuracy
  - Quick check question: How would you compute accuracy for a binary classification task where the model predicts "Yes/No" for hallucination presence?

## Architecture Onboarding

- **Component map:** Data generation pipeline (ChatGPT API with temperature=1.0) -> Human annotation system (Labelers with search engine access) -> Knowledge retrieval module (Wikipedia integration) -> Evaluation framework (Instruction-based assessment using different LLMs)

- **Critical path:** Data generation → Human annotation → Evaluation → Analysis

- **Design tradeoffs:**
  - Using ChatGPT for both generation and evaluation introduces potential bias but enables scalability
  - Automatic generation vs human annotation: Automatic is faster but may miss nuanced hallucinations
  - Knowledge retrieval vs CoT: Knowledge provides explicit facts while CoT adds reasoning steps that may interfere

- **Failure signatures:**
  - Low agreement between human labelers (κ < 0.8)
  - Poor performance across all topics suggesting systematic issues
  - Inconsistent results between different LLMs on same samples

- **First 3 experiments:**
  1. Run the generation pipeline on a small subset of questions and verify the diversity of hallucinations
  2. Test human annotation agreement on 50 samples before scaling up
  3. Compare evaluation results between ChatGPT and GPT-3 on the same hallucinated samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in recognizing hallucinations vary across different types of hallucination patterns (e.g., comprehension, factualness, specificity, inference)?
- Basis in paper: Explicit - The paper categorizes hallucination patterns in question answering into four types and evaluates the performance of LLMs in recognizing each type
- Why unresolved: While the paper provides a categorization of hallucination patterns and evaluates the performance of LLMs, it does not provide a detailed analysis of how the performance varies across different types of hallucination patterns
- What evidence would resolve it: A detailed analysis of the performance of LLMs in recognizing different types of hallucination patterns, possibly through a more granular evaluation or a comparative study

### Open Question 2
- Question: To what extent can external knowledge and reasoning steps improve the performance of LLMs in recognizing hallucinations?
- Basis in paper: Explicit - The paper suggests that providing external knowledge and adding reasoning steps can help improve the performance of LLMs in recognizing hallucinations
- Why unresolved: While the paper suggests that these strategies can improve performance, it does not provide a quantitative analysis of the extent of improvement
- What evidence would resolve it: A quantitative analysis of the improvement in performance when using external knowledge and reasoning steps, possibly through a controlled experiment or a comparative study

### Open Question 3
- Question: How does the performance of LLMs in recognizing hallucinations compare to human performance?
- Basis in paper: Explicit - The paper mentions that human labelers are used to annotate the hallucinations in ChatGPT responses, suggesting a comparison with human performance
- Why unresolved: While the paper mentions the use of human labelers, it does not provide a direct comparison of the performance of LLMs and humans in recognizing hallucinations
- What evidence would resolve it: A direct comparison of the performance of LLMs and humans in recognizing hallucinations, possibly through a controlled experiment or a comparative study

## Limitations

- ChatGPT bias in benchmark creation due to using the same model for generation and evaluation
- Human annotation reliability issues despite high inter-annotator agreement
- Limited topic coverage in the benchmark may not represent real-world usage scenarios

## Confidence

**High Confidence:** LLMs struggle with hallucination recognition, with ChatGPT achieving only 62.59% accuracy on QA tasks; providing external knowledge significantly improves accuracy (62.59 to 76.83)

**Medium Confidence:** The two-step sampling-then-filtering framework generates diverse and plausible hallucinated samples, though ChatGPT's inherent biases may influence the final benchmark

**Low Confidence:** Topic sensitivity affecting hallucination recognition is based on limited empirical evidence and may be influenced by confounding factors

## Next Checks

1. Evaluate the same HaluEval benchmark using smaller, open-source LLMs (e.g., LLaMA, BLOOM) to determine whether performance patterns are consistent across different model architectures

2. Conduct a systematic comparison between human annotation results and model predictions on a held-out subset of samples to quantify agreement and identify systematic biases

3. Perform ablation studies on the knowledge retrieval component by varying the relevance and completeness of retrieved information to determine minimum effective knowledge support needed for reliable hallucination detection across different task types