---
ver: rpa2
title: Scalability of Message Encoding Techniques for Continuous Communication Learned
  with Multi-Agent Reinforcement Learning
arxiv_id: '2308.04844'
source_url: https://arxiv.org/abs/2308.04844
tags:
- message
- agents
- communication
- mean
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how two message encoding techniques, mean
  and attention, perform as the complexity of a multi-agent communication task increases.
  The mean encoder averages all incoming messages, while the attention encoder weights
  messages based on their relevance to the receiver.
---

# Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.04844
- Source URL: https://arxiv.org/abs/2308.04844
- Reference count: 26
- Key outcome: Mean message encoder outperforms attention encoder in scalable matrix environments by learning exponential and logarithmic encoding strategies that preserve information

## Executive Summary
This work investigates how mean and attention message encoding techniques perform as multi-agent communication tasks increase in complexity. Experiments in a scalable matrix environment reveal that the simple mean encoder consistently outperforms the more complex attention encoder across varying numbers of labels and agents. The mean encoder remains effective even when the number of labels exceeds the message size by employing learned non-linear encoding strategies that preserve information through averaging. These results challenge conventional wisdom about attention mechanisms and suggest that simple averaging can be surprisingly effective for scalable communication learning.

## Method Summary
The study uses a matrix environment where N agents each observe a one-hot encoded label from L possible labels, and must communicate to determine how many agents share the same label. Three communication variants are tested: no communication, mean encoding (averaging hidden states), and attention encoding (scaled dot-product attention). The CommNet architecture with parameter sharing is used, and agents are trained using RLlib/Tune with Adam optimizer, learning rate 0.002, discount factor 0.99, and entropy regularization. Performance is measured as normalized reward during the final 10% of training.

## Key Results
- Mean encoder consistently outperforms attention encoder across all tested configurations of agents and labels
- Mean encoder remains effective even when message size is smaller than the number of labels through learned exponential/logarithmic encoding
- Simple averaging provides sufficient flexibility when agents learn appropriate encoding strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean message encoder outperforms attention encoder due to learned exponential and logarithmic encoding that preserves information even when message size < number of labels.
- Mechanism: Agents learn to encode labels using non-linear functions (exponential for one message dimension, logarithmic for another), ensuring unique encodings that remain distinguishable after averaging.
- Core assumption: The mean encoder can preserve separable information if agents learn a parametric encoding that ensures distinct mean values for all label combinations.
- Evidence anchors:
  - [abstract]: "agents use a combination of an exponential and a logarithmic function in their communication policy to avoid the loss of important information"
  - [section]: "By representing this curve as a parametric equation, we have determined that each of the message values can be represented using a different non-linear function"
  - [corpus]: No direct evidence found in corpus about exponential/logarithmic encoding strategies.
- Break condition: If the number of labels becomes too large relative to message size, or if agents cannot learn the necessary non-linear mapping.

### Mechanism 2
- Claim: Attention encoder requires more parameters and slower training, making it less scalable for complex environments.
- Mechanism: Attention encoder needs separate query, key, and value networks for each agent, increasing model complexity and training time compared to simple averaging.
- Core assumption: Additional parameters in attention mechanism lead to slower convergence and worse performance in scalable settings.
- Evidence anchors:
  - [abstract]: "attention message encoder consistently outperforms the mean message encoder" (actually the opposite, supporting that attention is worse)
  - [section]: "we expect training to be slower" and "the attention message encoder trains slower than the mean message encoder"
  - [corpus]: No direct evidence found in corpus about training speed differences.
- Break condition: In simple environments with few agents/labels, attention might perform better once trained.

### Mechanism 3
- Claim: Mean encoder provides sufficient flexibility when agents learn appropriate encoding strategies, eliminating need for attention's selective weighting.
- Mechanism: Simple averaging works because agents can strategically encode information to ensure separability of combined messages.
- Core assumption: Averaging is not inherently lossy if input messages are properly structured.
- Evidence anchors:
  - [abstract]: "Surprisingly, our results show that the mean message encoder consistently outperforms the attention message encoder"
  - [section]: "we can see that each of the means is separable from the other values" and "By learning a representation combining an exponential function and a logarithmic function"
  - [corpus]: No direct evidence found in corpus about separability through averaging.
- Break condition: When message size becomes too small relative to information complexity.

## Foundational Learning

- Concept: Markov Decision Processes (dec-MDP)
  - Why needed here: The framework defines how agents make decisions based on partial observations and communicate to overcome observability limitations.
  - Quick check question: In a dec-MDP, can a single agent observe the complete state of the environment?

- Concept: Self-Attention Mechanism
  - Why needed here: Understanding how attention computes query-key-value relationships is crucial for comparing it with simpler encoding methods.
  - Quick check question: What mathematical operation ensures the attention scores sum to 1?

- Concept: Reinforcement Learning with Policy Gradients
  - Why needed here: The communication protocols are learned through backpropagation of REINFORCE loss, requiring understanding of policy gradient methods.
  - Quick check question: What is the purpose of subtracting the mean reward in the REINFORCE loss?

## Architecture Onboarding

- Component map: Observation -> Encoding network -> Message encoder (mean/attention) -> Communication network -> Decoding network -> Action policy
- Critical path: Observation → Encoding → Message Processing → Hidden State Update → Action Selection
- Design tradeoffs: Mean encoder is simpler and faster but less flexible; attention encoder is more complex but theoretically better at filtering information
- Failure signatures: Mean encoder fails when message size << labels; attention encoder fails when training is too slow or parameters don't converge
- First 3 experiments:
  1. Test mean vs attention encoder with small number of labels (L=3) and agents (N=3)
  2. Scale up number of labels while keeping agents constant (L=8,16,24 with N=3)
  3. Scale up number of agents while keeping labels constant (N=8,16,24 with L=3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of mean and attention message encoders compare when applied to environments with varying numbers of agents and longer episode lengths?
- Basis in paper: [inferred] The paper suggests exploring more complex environments with varying numbers of agents and longer episodes as future work.
- Why unresolved: The experiments conducted used a constant number of agents and one-timestep episodes, limiting the generalizability of the findings to more dynamic environments.
- What evidence would resolve it: Comparative experiments in environments with varying numbers of agents and longer episodes would provide evidence of how well each encoder adapts to changes over time and varying agent counts.

### Open Question 2
- Question: What are the performance implications of using different communication learning techniques with mean and attention message encoders in both continuous and discrete message settings?
- Basis in paper: [explicit] The paper mentions investigating more message encoding techniques and applying them to both continuous and discrete messages as future work.
- Why unresolved: The current study focuses on continuous communication and does not explore the impact of different communication learning techniques or discrete message settings.
- What evidence would resolve it: Experimental results comparing the performance of various communication learning techniques with both encoder types in continuous and discrete message settings would provide insights into their effectiveness.

### Open Question 3
- Question: How do RNNs and combinations of RNNs and attention perform compared to mean and attention message encoders in terms of scalability and information retention?
- Basis in paper: [explicit] The paper references the successful application of RNNs and combinations of RNNs and attention in past research, suggesting a need to explore these methods further.
- Why unresolved: The study primarily focuses on mean and attention encoders without direct comparison to RNN-based methods, leaving their relative performance and scalability unexplored.
- What evidence would resolve it: Comparative experiments evaluating RNN-based methods alongside mean and attention encoders in scalable communication tasks would clarify their relative strengths and weaknesses.

## Limitations

- The exponential and logarithmic encoding mechanism lacks empirical validation in the literature corpus
- The attention mechanism implementation details remain underspecified, making fair comparison difficult
- Results may not generalize beyond the specific matrix environment with discrete labels
- The study focuses on continuous communication and doesn't explore discrete message settings

## Confidence

**High confidence**: The empirical finding that mean encoder outperforms attention encoder in the tested matrix environment. The experimental methodology appears sound with proper ablation studies and clear performance metrics.

**Medium confidence**: The scalability claims across varying numbers of agents and labels. While results show consistent trends, the underlying reasons for performance differences could be influenced by implementation details not fully specified in the paper.

**Low confidence**: The mechanism explanation involving exponential and logarithmic encoding strategies. The corpus provides no supporting evidence for this specific learning behavior, and the claim appears to be based primarily on post-hoc analysis of trained policies.

## Next Checks

1. Replicate the exponential/logarithmic encoding analysis: Extract and visualize message values from trained mean encoder policies across different L values to verify whether agents actually learn the proposed non-linear encoding functions.

2. Test on continuous observation spaces: Implement a variant of the matrix environment where labels are continuous values rather than discrete categories to assess whether mean encoding still provides advantages.

3. Compare with attention variants: Test alternative attention implementations (e.g., with learned keys, different query generation methods) to determine if the observed performance gap is due to the specific attention design or inherent limitations of attention mechanisms in this context.