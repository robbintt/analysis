---
ver: rpa2
title: Landmark Guided Active Exploration with State-specific Balance Coefficient
arxiv_id: '2306.17484'
source_url: https://arxiv.org/abs/2306.17484
tags:
- learning
- policy
- exploration
- hierarchical
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effective exploration in
  goal-conditioned hierarchical reinforcement learning (GCHRL), where the high-level
  policy's large action space leads to inefficient training. The authors propose LESP,
  a landmark-guided active exploration strategy that integrates prospect and novelty
  measures to guide exploration efficiently.
---

# Landmark Guided Active Exploration with State-specific Balance Coefficient

## Quick Facts
- arXiv ID: 2306.17484
- Source URL: https://arxiv.org/abs/2306.17484
- Reference count: 40
- This paper proposes LESP, a landmark-guided exploration strategy that significantly outperforms baseline methods in sparse reward goal-conditioned hierarchical RL tasks.

## Executive Summary
This paper addresses the challenge of effective exploration in goal-conditioned hierarchical reinforcement learning (GCHRL), where the high-level policy's large action space leads to inefficient training. The authors propose LESP, a landmark-guided active exploration strategy that integrates prospect and novelty measures to guide exploration efficiently. Prospect is designed by planning in the goal space using a goal-conditioned value function, reflecting the likelihood of exploring in the direction of a subgoal leading to the final task goal. Novelty, based on visit counts, maintains exploration of new states. Additionally, state-specific regularization is applied to the low-level policy to mitigate non-stationarity in high-level state transitions. Experiments on Mujoco environments demonstrate that LESP significantly outperforms baseline methods in sparse reward tasks, with improved sample efficiency and task success rates. Ablation studies confirm the importance of prospect and the stability component in enhancing exploration and performance.

## Method Summary
LESP is a landmark-guided exploration strategy for goal-conditioned hierarchical reinforcement learning that combines prospect and novelty measures. The method uses farthest point sampling to select landmarks in the goal space, builds a reachability graph using a goal-conditioned value function, and plans paths toward the final goal. Subgoals are selected based on a weighted combination of normalized novelty (visit count-based) and prospect measures, with distance constraints from the current state. A state-specific regularization term is added to the low-level policy loss to stabilize learning by limiting value function estimation differences across time steps for frequently visited state-subgoal pairs.

## Key Results
- LESP significantly outperforms baseline methods in sparse reward tasks across multiple Mujoco environments
- The combination of prospect and novelty measures improves sample efficiency and task success rates
- State-specific regularization effectively stabilizes learning by mitigating non-stationarity in high-level state transitions
- Ablation studies confirm the importance of both prospect measure and stability component for enhanced exploration and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Landmark-guided prospect measure directs exploration toward subgoals that are likely to lead to the final task goal.
- Mechanism: The algorithm plans a feasible path in the goal space using a goal-conditioned value function, selecting landmarks along this path. The prospect measure then prioritizes subgoals that are close to the selected landmark (lsel), which lies on a path toward the final goal.
- Core assumption: The goal-conditioned value function accurately reflects reachability between states in the latent goal space, making the planned path reliable.
- Evidence anchors:
  - [abstract]: "We design a measure of prospect for subgoals by planning in the goal space based on the goal-conditioned value function."
  - [section]: "The prospect measure takes into account the influence of the goal on the subgoal selection by considering the feasible path to the goal in the latent space."
  - [corpus]: Weak evidence. The corpus mentions landmark-guided exploration but lacks detailed mechanism descriptions matching this specific prospect measure.
- Break condition: If the goal-conditioned value function is inaccurate or if the goal space is poorly structured, the planned path and prospect measure become unreliable.

### Mechanism 2
- Claim: State-specific regularization stabilizes low-level policy learning by limiting value function estimation differences across time steps.
- Mechanism: A regularization term Lr is added to the low-level policy loss, which penalizes differences between current and old Q-function estimates for frequently visited state-subgoal pairs. The regularization weight is higher for the k% most stable pairs.
- Core assumption: The low-level policy's dynamic changes cause significant non-stationarity in high-level state transitions, which can be mitigated by stabilizing value function estimates.
- Evidence anchors:
  - [abstract]: "we apply a state-specific regularization to the learning of low-level policy to alleviate the non-stationarity caused by the dynamic changes in the low-level policy."
  - [section]: "Lr is designed to limit the estimation differences of the low-level Q-function at different time steps without compromising learning efficiency."
  - [corpus]: Weak evidence. The corpus mentions stable learning but doesn't detail the specific regularization mechanism.
- Break condition: If the regularization is too aggressive, it may prevent the low-level policy from adapting to new situations, harming performance.

### Mechanism 3
- Claim: Combining prospect and novelty measures balances exploitation of promising regions with exploration of new states.
- Mechanism: Subgoals are selected based on a weighted sum of normalized novelty (visit count-based) and prospect measures, subject to distance constraints from the current state.
- Core assumption: Both novelty and prospect are necessary for efficient exploration—novelty ensures coverage of new areas while prospect guides toward goal-relevant regions.
- Evidence anchors:
  - [abstract]: "Building upon the measure of prospect, we propose a landmark-guided exploration strategy by integrating the measures of prospect and novelty."
  - [section]: "Considering the measure of prospect and novelty for subgoals, we propose a Landmark-guided active Exploration strategy."
  - [corpus]: Moderate evidence. The corpus discusses novelty-based exploration and landmark guidance separately but doesn't detail their integration.
- Break condition: If the balance coefficient α is poorly tuned, the exploration strategy may overemphasize either novelty (leading to aimless wandering) or prospect (leading to premature exploitation).

## Foundational Learning

- Concept: Goal-conditioned value functions
  - Why needed here: They provide a metric for reachability between states in the latent goal space, enabling path planning and prospect calculation.
  - Quick check question: What is the difference between a state-value function and a goal-conditioned value function?

- Concept: Hierarchical reinforcement learning (HRL)
  - Why needed here: The method operates within a two-level hierarchy where the high-level policy selects subgoals and the low-level policy executes actions to achieve them.
  - Quick check question: How does the high-level policy's reward differ from the low-level policy's reward in this framework?

- Concept: Non-stationarity in HRL
  - Why needed here: The low-level policy's dynamic changes affect the high-level state transition function, creating learning instability that the regularization term addresses.
  - Quick check question: Why does the low-level policy's learning introduce non-stationarity for the high-level policy?

## Architecture Onboarding

- Component map: 
  - High-level policy (πh) -> Selects subgoals based on prospect and novelty measures
  - Low-level policy (πl) -> Executes actions to achieve subgoals, trained with state-specific regularization
  - Subgoal representation function (ϕ) -> Maps states to goal space, trained with triplet loss
  - Goal-conditioned value function (V) -> Estimates reachability between states in goal space
  - Landmark sampling -> Uses Farthest Point Sampling to cover goal space
  - Exploration strategy -> Combines prospect and novelty measures with distance constraints

- Critical path:
  1. Sample landmarks using FPS in goal space
  2. Build graph with landmarks, current state, and goal as nodes
  3. Plan path using shortest path algorithm weighted by value function
  4. Select landmark (lsel) closest to current state on planned path
  5. Calculate prospect for candidate subgoals based on distance to lsel
  6. Select subgoal maximizing weighted sum of prospect and novelty

- Design tradeoffs:
  - Balancing coefficient α: Higher values prioritize prospect over novelty
  - Number of landmarks (ncov): More landmarks improve path planning but increase computation
  - Low-level policy length (c): Longer horizons provide more stable subgoals but reduce adaptability

- Failure signatures:
  - Poor performance: Likely issues with value function accuracy or balance coefficient tuning
  - High variance in learning: May indicate insufficient regularization or unstable value function estimates
  - Slow exploration: Could be caused by overemphasis on prospect or insufficient novelty weight

- First 3 experiments:
  1. Test landmark sampling and path planning independently with a pre-trained value function
  2. Evaluate prospect measure calculation by visualizing prospect values for random subgoals
  3. Validate state-specific regularization by comparing learning curves with and without regularization term

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The method's performance is sensitive to the balance coefficient α between prospect and novelty measures, requiring careful tuning
- The approach relies on the accuracy of the goal-conditioned value function, which may be challenging to learn in complex environments
- The paper focuses on sparse reward Mujoco environments and doesn't extensively test performance on more diverse task distributions

## Confidence
- **High confidence**: Core mechanism of combining prospect and novelty measures for landmark-guided exploration, supported by experimental results
- **Medium confidence**: Specific implementation details of state-specific regularization and the exact balance between prospect and novelty measures, as these components are less extensively validated
- **Low confidence**: Optimal hyperparameter settings (α, ncov) across different environments and the method's robustness to different goal space representations

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary the balance coefficient α and number of landmarks to determine the sensitivity of performance to these key hyperparameters across different environments.

2. **Value function accuracy evaluation**: Conduct controlled experiments to measure how inaccuracies in the goal-conditioned value function affect the reliability of prospect-based exploration and overall performance.

3. **Generalization testing**: Evaluate the method on tasks with different reward structures (e.g., dense rewards) and more complex goal spaces to assess the robustness of the exploration strategy beyond sparse reward settings.