---
ver: rpa2
title: Stealing the Decoding Algorithms of Language Models
arxiv_id: '2303.04729'
source_url: https://arxiv.org/abs/2303.04729
tags:
- decoding
- sampling
- probability
- temperature
- top-k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present the first algorithm to steal decoding strategies
  (e.g., sampling vs. greedy, temperature, nucleus sampling) used by language model
  APIs.
---

# Stealing the Decoding Algorithms of Language Models

## Quick Facts
- arXiv ID: 2303.04729
- Source URL: https://arxiv.org/abs/2303.04729
- Authors: 
- Reference count: 40
- Key outcome: Authors present first algorithm to steal decoding strategies from LM APIs using only top-2 token probabilities, achieving near-perfect accuracy with very low cost ($0.8-$40)

## Executive Summary
This paper presents the first successful algorithm to steal decoding strategies (sampling vs greedy, temperature, nucleus sampling) from language model APIs through black-box access. The attack exploits the fact that different decoding algorithms leave distinguishable signatures on generated text by analyzing the top-2 token probabilities. Applied to GPT-2 and GPT-3, the method achieves near-perfect accuracy in detecting decoding type and accurate hyperparameter estimation at very low cost. The authors also evaluate defense mechanisms, finding that adding noise to output probabilities can effectively protect against the attack.

## Method Summary
The attack uses a multi-stage approach that leverages statistical properties of different decoding algorithms. It begins by distinguishing sampling-based methods from deterministic ones through repeated queries, then identifies specific decoding types (greedy, beam search, temperature sampling, top-k sampling, nucleus sampling) and estimates their hyperparameters through mathematical formulas. For temperature estimation, the relationship τ = ln(p_i/p_j) / ln(p'_i/p'_j) is used, while for nucleus sampling, the ratio p_i/p'_i approximates the threshold. Beam search size is inferred by observing token rank stability across repeated queries. The attack requires only black-box access and the top-2 token probabilities from the API.

## Key Results
- Near-perfect accuracy in detecting decoding type across GPT-2 and GPT-3
- Accurate hyperparameter estimation at very low cost ($0.8-$40)
- Noise injection as defense was effective but degraded output quality
- Multi-stage approach successfully handles combined decoding strategies

## Why This Works (Mechanism)

### Mechanism 1
Different decoding algorithms and hyperparameter configurations leave distinguishable signatures on generated text. The statistical properties of token selection differ across decoding methods - greedy and beam search produce deterministic outputs while sampling-based methods introduce stochastic variation. The final probability distribution is shaped by how the decoding algorithm truncates and reweights the LM's output distribution.

### Mechanism 2
Temperature scaling and Nucleus Sampling can be mathematically inverted to recover hyperparameters from probability ratios. For temperature τ, the relationship follows: τ = ln(p_i/p_j) / ln(p'_i/p'_j). For Nucleus Sampling, the ratio p_i/p'_i approximates the Nucleus threshold p. These formulas hold even when algorithms are combined.

### Mechanism 3
Beam search size can be inferred by observing token rank stability across repeated queries. In beam search, tokens at earlier positions may change across queries, but the maximum rank observed across multiple attempts indicates the beam size. This works because beam search maintains multiple hypotheses and the rank of a token in the probability distribution corresponds to its likelihood of being in the beam.

## Foundational Learning

- Concept: Probability distribution truncation and reweighting
  - Why needed here: Decoding algorithms work by selecting subsets of the full vocabulary and renormalizing probabilities. Understanding this is crucial for recognizing how different algorithms create distinct output distributions.
  - Quick check question: If an algorithm selects the top-k tokens and renormalizes, what happens to tokens outside the top-k?

- Concept: Statistical hypothesis testing (Kolmogorov-Smirnov test)
  - Why needed here: Used to compare the probability distributions from the victim API and the reconstructed model to verify attack accuracy.
  - Quick check question: What does a high p-value in a KS test indicate about two probability distributions?

- Concept: Kullback-Leibler divergence
  - Why needed here: Another metric for comparing probability distributions, used to quantify the difference between the victim's and reconstructed distributions.
  - Quick check question: If two distributions are identical, what is their KL divergence?

## Architecture Onboarding

- Component map: Language Model (inner probability distribution) → Decoding Algorithm (selection/truncation) → API Output (final probability distribution)
- Critical path: Query API → Observe top-2 probabilities → Apply multi-stage detection → Estimate hyperparameters → Validate with KS/KL tests
- Design tradeoffs: More queries increase accuracy but also cost; noise injection as defense improves security but degrades output quality
- Failure signatures: Inconsistent detection across repeated runs, high KL divergence between victim and reconstructed distributions, failure to detect sampling methods
- First 3 experiments:
  1. Test deterministic vs sampling detection by sending same query multiple times
  2. Test temperature estimation using formula with known test cases
  3. Test beam size detection by observing rank stability across queries

## Open Questions the Paper Calls Out

### Open Question 1
How do the authors' proposed defense mechanisms affect the quality and diversity of the generated text in practice? The authors provide theoretical analysis and some empirical results, but a comprehensive evaluation of the trade-off between security and text quality across different decoding algorithms and hyperparameters is missing.

### Open Question 2
Can the attack methodology be extended to other language models beyond GPT-2 and GPT-3, such as BERT or T5? The authors focus on GPT-2 and GPT-3 for their experiments, but do not explicitly rule out the applicability to other models.

### Open Question 3
How does the attack perform against language models with continuous output spaces or non-text generation tasks? The authors focus on discrete text generation tasks and do not address continuous output spaces or other modalities.

### Open Question 4
Are there any countermeasures that can effectively defend against the attack without significantly degrading the quality of the generated text? The authors propose adding noise to the probability distribution as a countermeasure and evaluate its impact on the quality of the generated text.

## Limitations

- Statistical variability requires significant query counts (100-20,000 depending on stage), potentially making attack impractical
- Mathematical assumptions about probability relationships may break down in real-world implementations
- Defense effectiveness evaluated only on GPT-2 and GPT-3, not on larger or proprietary models

## Confidence

**High Confidence**: Core mechanism of distinguishing sampling from deterministic decoding algorithms through repeated queries - straightforward statistical test with clear theoretical grounding and strong empirical evidence.

**Medium Confidence**: Mathematical formulas for temperature and nucleus sampling hyperparameter estimation - theoretical derivations are sound but practical implementation details and numerical stability aren't fully explored.

**Low Confidence**: Beam search size estimation method - relies on sequential token generation and observing rank changes, which is more heuristic than analytical, with effectiveness unproven for larger beam sizes.

## Next Checks

1. **Statistical Validation**: Run the attack across 50+ different prompts with varying lengths and topics. Calculate confidence intervals for detection accuracy and hyperparameter estimation error to establish reliability and identify failure scenarios.

2. **Defense Robustness**: Test the noise-injection defense against more sophisticated attacks using Bayesian inference or machine learning to denoise probability distributions. Evaluate whether defense degrades output quality beyond acceptable thresholds.

3. **Generalization Testing**: Apply the attack to models with different architectures (like LLaMA, Claude, or proprietary models) and with different decoding implementations. Determine whether mathematical formulas need adjustment for different probability distribution characteristics.