---
ver: rpa2
title: Learning Domain-Independent Heuristics for Grounded and Lifted Planning
arxiv_id: '2312.11143'
source_url: https://arxiv.org/abs/2312.11143
tags:
- graph
- planning
- learning
- heuristics
- goose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of learning domain-independent
  heuristics for classical planning using Graph Neural Networks (GNNs). The authors
  propose three novel graph representations of planning tasks: STRIPS Learning Graph
  (SLG), FDR Learning Graph (FLG), and Lifted Learning Graph (LLG).'
---

# Learning Domain-Independent Heuristics for Grounded and Lifted Planning

## Quick Facts
- **arXiv ID**: 2312.11143
- **Source URL**: https://arxiv.org/abs/2312.11143
- **Reference count**: 5
- **Primary result**: Learning domain-independent heuristics using GNNs on novel graph representations (SLG, FLG, LLG) that generalize across planning domains and solve larger problems than previous approaches

## Executive Summary
This paper introduces a novel approach to learning domain-independent heuristics for classical planning using Graph Neural Networks (GNNs). The authors propose three new graph representations - STRIPS Learning Graph (SLG), FDR Learning Graph (FLG), and Lifted Learning Graph (LLG) - that encode planning tasks in ways amenable to GNN processing. These representations enable MPNNs to learn heuristics that generalize across domains without requiring domain-specific training. The approach, implemented in the GOOSE planner, demonstrates significant improvements over previous methods, solving problems an order of magnitude larger than those in the training set while maintaining competitive plan quality.

## Method Summary
The method involves three key components: (1) converting planning tasks into specialized graph representations (SLG for STRIPS, FLG for FDR, LLG for lifted planning), (2) training RGCN models to predict heuristic values from these graphs using supervised learning on optimal plans, and (3) integrating the learned heuristics into a heuristic search framework (GOOSE) that uses A* search with Fast Downward. The training uses MSE loss and learning rate scheduling, while evaluation leverages GPU parallelization for efficient heuristic computation across multiple states.

## Key Results
- MPNNs on grounded graphs (SLG, FLG) can learn domain-independent heuristics like hmax and hadd, while LLG provides a lifted alternative
- GOOSE solves problems 10-100× larger than training instances, vastly outperforming STRIPS-HGN
- On domain-dependent benchmarks, LLG achieves better coverage and plan quality than hFF in 3/6 domains
- The approach maintains competitive plan quality while significantly expanding the reach of automated planners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPNNs on grounded graphs can learn domain-independent heuristics like hmax and hadd through iterative message passing that aggregates state information
- Mechanism: The graph representations (SLG, FLG) encode planning task structure as nodes (actions, propositions) and labeled edges (preconditions, add, delete effects). MPNN message passing iteratively updates node embeddings by aggregating neighbor information under each edge label. After L iterations, a readout function produces heuristic estimates that approximate the target heuristic values.
- Core assumption: The MPNN architecture with sufficient depth and appropriate aggregation functions can approximate the computation of domain-independent heuristics when the planning task structure is properly encoded.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that MPNNs on these graphs can learn certain domain-independent heuristics like hmax and hadd"
  - [section 4] "we have established the theoretical expressiveness of Message-Passing Neural Networks (MPNN) acting upon our graphs in terms of the known domain-independent heuristics they are able to learn"
- Break condition: If the graph representation loses critical structural information (like delete effects in STRIPS-HGN), or if the MPNN cannot extract the necessary information from the graph structure due to expressiveness limitations.

### Mechanism 2
- Claim: The lifted learning graph (LLG) enables learning domain-independent heuristics without grounding the planning task
- Mechanism: LLG encodes the lifted planning task structure with nodes for predicates, objects, action schemas, and instantiated propositions. Edge labels capture semantic relationships like predicate instantiation and action schema semantics. IF features encode argument indexes using normalized random vectors, allowing the model to handle unbounded arities without fixed maximum assumptions.
- Core assumption: The lifted graph representation preserves enough information about the planning task structure for MPNNs to learn meaningful domain-independent heuristics, and the IF features provide sufficient positional encoding for argument indexing.
- Evidence anchors:
  - [abstract] "to mitigate the issues caused by large grounded GNNs we present the first method for learning domain-independent heuristics with only the lifted representation of a planning task"
  - [section 3] "LLG provides a lifted graph representation that is amenable to learning"
- Break condition: If the lifted representation becomes too condensed for MPNNs to extract necessary information for certain heuristics, or if the IF features fail to provide distinguishable encoding for argument positions.

### Mechanism 3
- Claim: GOOSE combines learned heuristics with heuristic search to solve larger problems than previous approaches
- Mechanism: GOOSE transforms each state s of a planning task into a new planning subtask ⟨S, A, s, G⟩, converts it to the chosen graph representation (SLG, FLG, or LLG), and feeds it to a trained MPNN to compute heuristic values. These heuristic values guide A* search (via Fast Downward's eager GBFS). GPU parallelization accelerates heuristic evaluation across multiple states.
- Core assumption: The learned heuristics, even if not perfectly accurate, provide sufficient guidance for A* search to outperform baseline heuristics like hFF on larger problems.
- Evidence anchors:
  - [abstract] "Our experiments show that our heuristics generalise to much larger problems than those in the training set, vastly surpassing STRIPS-HGN heuristics"
  - [section 5] "Planners guided by heuristics learnt using our new graphs solve significantly larger problems than those considered by Shen, Trevizan, and Thi´ebaux (2020), Karia and Srivastava (2021) and St ˚ahlberg, Bonet, and Geffner (2022b)"
- Break condition: If the learned heuristics are too inaccurate or computationally expensive to evaluate, causing search to expand more nodes or take longer than baseline approaches.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: MPNNs are the core learning mechanism that extracts heuristic values from graph representations of planning tasks. Understanding how message passing aggregates neighborhood information is essential for grasping how the model learns heuristics.
  - Quick check question: How does the choice of aggregator function (sum, mean, max) affect the information retained during message passing?

- Concept: Planning Formalisms (STRIPS, FDR, Lifted Planning)
  - Why needed here: The paper builds graph representations for three different planning formalisms. Understanding the differences between these representations (propositional vs. state-variable vs. first-order) is crucial for understanding the design choices in each graph representation.
  - Quick check question: What structural information is preserved in the lifted representation that is lost in the grounded representations?

- Concept: Heuristic Functions in Planning (hmax, hadd, h+, h*)
  - Why needed here: The paper theoretically analyzes which domain-independent heuristics can be learned by MPNNs on different graph representations. Understanding these heuristics and their computational properties is necessary to interpret the theoretical results.
  - Quick check question: Why is h+ NP-complete while hmax and hadd are polynomial-time computable?

## Architecture Onboarding

- Component map: Graph Generation -> MPNN Model -> GOOSE Planner -> Plan Output
- Critical path: Graph Generation → MPNN Prediction → Heuristic Search → Plan Output
  The most critical components are the graph representation quality and MPNN predictive accuracy, as these directly determine search performance.

- Design tradeoffs:
  - Grounded vs. Lifted: Grounded graphs (SLG, FLG) are more expressive but larger and slower to evaluate; lifted graph (LLG) is smaller and faster but may lose some information
  - Number of MPNN layers: More layers increase expressiveness but also computational cost and training difficulty
  - Aggregator choice: Different aggregators (max, mean) have different information retention properties affecting learning and generalization

- Failure signatures:
  - Poor coverage on test problems indicates the learned heuristics don't generalize well
  - High node expansions relative to baselines suggests the heuristics aren't informative enough
  - Long evaluation times indicate computational bottlenecks in graph generation or MPNN inference
  - Overfitting symptoms: Good performance on training problems but poor generalization

- First 3 experiments:
  1. Train and evaluate GOOSE on a simple domain (like Blocksworld) with varying numbers of MPNN layers to understand the depth-expressiveness tradeoff
  2. Compare coverage and node expansions of GOOSE with different graph representations (SLG, FLG, LLG) on the same domain to understand representation impact
  3. Evaluate the runtime performance of GOOSE with and without GPU acceleration to quantify the computational benefits of parallelization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How expressive are MPNNs on the LLG graph compared to other representations?
- Basis in paper: [explicit] The paper shows that MPNNs on LLG cannot learn hadd and hmax, unlike on grounded graphs.
- Why unresolved: The paper only provides a negative result, showing that LLG is less expressive for certain heuristics.
- What evidence would resolve it: Further theoretical analysis or experiments demonstrating the specific heuristics LLG can learn.

### Open Question 2
- Question: How does the choice of aggregation function impact the performance of GOOSE?
- Basis in paper: [explicit] The paper mentions that max and mean aggregations have different effects on various domains and graph representations.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different aggregation functions on performance.
- What evidence would resolve it: Experiments systematically varying the aggregation function and measuring the impact on coverage and plan quality.

### Open Question 3
- Question: How important is finding the right graph neural network parameters for GOOSE's performance?
- Basis in paper: [explicit] The paper shows that the effectiveness of GOOSE varies significantly depending on the number of layers and aggregator.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different GNN parameters on performance.
- What evidence would resolve it: Experiments systematically varying the GNN parameters and measuring the impact on coverage and plan quality.

## Limitations
- Theoretical expressiveness results only cover grounded graphs (SLG, FLG), not the lifted representation (LLG)
- Empirical evaluation is limited to comparison with hFF and STRIPS-HGN, missing other modern heuristic learning approaches
- Computational analysis lacks detailed characterization of runtime trade-offs across different problem sizes and configurations

## Confidence
- **High**: Theoretical expressiveness results showing MPNNs can learn hmax and hadd on grounded graphs; empirical evidence that GOOSE solves larger problems than previous approaches
- **Medium**: Claims about LLG learning effectiveness; comparative results against hFF in domain-dependent settings
- **Low**: Generalization guarantees beyond the specific domains tested; computational efficiency claims without detailed runtime analysis

## Next Checks
1. **Theoretical extension**: Prove or disprove whether MPNNs on LLG can learn hmax or hadd heuristics, or characterize what class of heuristics is learnable with the lifted representation and IF features.

2. **Empirical robustness**: Evaluate GOOSE on a broader set of domains including those with complex action schemas and higher-arity predicates to test the limits of both grounded and lifted graph representations.

3. **Computational characterization**: Systematically measure the runtime of GOOSE across different problem sizes, graph representations, and MPNN configurations to establish the practical scalability limits and identify optimal configuration choices.