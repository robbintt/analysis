---
ver: rpa2
title: Accelerating Exploration with Unlabeled Prior Data
arxiv_id: '2311.05067'
source_url: https://arxiv.org/abs/2311.05067
tags:
- data
- reward
- online
- prior
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning sparse reward tasks
  from scratch in reinforcement learning by leveraging unlabeled prior data. The authors
  propose a simple approach that labels prior data with optimistic rewards using an
  upper confidence bound (UCB) estimate from online experience.
---

# Accelerating Exploration with Unlabeled Prior Data

## Quick Facts
- arXiv ID: 2311.05067
- Source URL: https://arxiv.org/abs/2311.05067
- Reference count: 40
- The paper presents a simple method to accelerate exploration in sparse reward tasks by labeling prior data with optimistic rewards, outperforming baselines without prior data and matching oracle performance.

## Executive Summary
This paper addresses the challenge of learning sparse reward tasks from scratch in reinforcement learning by leveraging unlabeled prior data. The authors propose a simple approach that labels prior data with optimistic rewards using an upper confidence bound (UCB) estimate from online experience. This optimistic labeling encourages exploration towards states present in the prior data, leading to more efficient learning. The method is evaluated on various domains, including AntMaze, Adroit hand manipulation, and visual robotic manipulation tasks. Results show that the approach significantly outperforms baselines that do not use prior data, often matching the performance of an oracle baseline with access to labeled rewards. The authors also demonstrate that incorporating pre-trained representations can further improve sample efficiency, especially in high-dimensional image-based tasks.

## Method Summary
The paper proposes a simple yet effective method for accelerating exploration in sparse reward tasks by leveraging unlabeled prior data. The approach labels prior data with optimistic rewards using an upper confidence bound (UCB) estimate from online experience. This optimistic labeling encourages exploration towards states present in the prior data. The method is evaluated on various domains, including AntMaze, Adroit hand manipulation, and visual robotic manipulation tasks. Results show that the approach significantly outperforms baselines that do not use prior data, often matching the performance of an oracle baseline with access to labeled rewards. The authors also demonstrate that incorporating pre-trained representations can further improve sample efficiency, especially in high-dimensional image-based tasks.

## Key Results
- The proposed method significantly outperforms baselines that do not use prior data in sparse reward tasks.
- The approach often matches the performance of an oracle baseline with access to labeled rewards.
- Incorporating pre-trained representations can further improve sample efficiency, especially in high-dimensional image-based tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimistic reward labeling on prior data encourages directed exploration towards high-value regions.
- **Mechanism**: The algorithm assigns upper-confidence-bound (UCB) reward estimates to prior transitions. This creates a "pull" effect, guiding the policy toward states in the prior data that are either known to have high rewards or are highly uncertain. Over time, as the agent visits these states, the reward estimates regress to true values, refining the exploration trajectory.
- **Core assumption**: The UCB estimate of the reward is sufficiently optimistic to motivate exploration, but not so optimistic as to mislead the agent indefinitely.
- **Evidence anchors**:
  - [abstract] The paper states that optimistic labeling "encourages exploration towards states present in the prior data, leading to more efficient learning."
  - [section 3] "Early in training, these optimistic reward labels will be high for states from the prior data, and optimizing the RL objective with this data will elicit policies that attempt to reach regions of state present in the prior data."
  - [corpus] "Accelerating Exploration with Unlabeled Prior Data" is a strong match, indicating thematic alignment.
- **Break condition**: If the UCB estimate is not well-calibrated (e.g., too pessimistic or too optimistic), the agent may either fail to explore or waste time on irrelevant states.

### Mechanism 2
- **Claim**: Using prior data with learned representations improves reward estimation quality.
- **Mechanism**: Pre-training representations (e.g., via ICVF) on the prior dataset provides a structured feature space where similar states are close together. This structure helps the reward model and novelty estimator (RND) make smoother, more coherent reward predictions, especially in high-dimensional image observations.
- **Core assumption**: The pre-trained representations capture meaningful structure of the environment that aligns with task-relevant features.
- **Evidence anchors**:
  - [abstract] The authors state that "incorporating pre-trained representations can further improve sample efficiency, especially in high-dimensional image-based tasks."
  - [section 5.4] The paper shows that using ICVF representations leads to smoother reward estimates along trajectories, forming a natural curriculum for exploration.
  - [corpus] "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration" is a relevant match, supporting the idea of using prior data for skill or representation learning.
- **Break condition**: If the pre-trained representations do not capture task-relevant structure, the reward model may not benefit, and exploration may not improve.

### Mechanism 3
- **Claim**: The combination of UCB-based optimism and RND novelty estimation balances exploitation and exploration.
- **Mechanism**: The reward model provides a baseline estimate of the reward, while RND adds a novelty bonus based on prediction error. This combination ensures that the agent explores both high-reward regions and novel states, preventing premature convergence to suboptimal policies.
- **Core assumption**: The novelty signal from RND is a reliable proxy for uncertainty in the reward function.
- **Evidence anchors**:
  - [section 3] The paper explains that RND "quantifies the uncertainty of the estimate, allowing us to get a UCB reward estimate."
  - [corpus] "Reinforcement Learning with Action Chunking" and "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning" are thematically related, supporting the use of novelty-based exploration methods.
- **Break condition**: If the RND network fails to capture meaningful novelty (e.g., due to poor feature extraction), the exploration may become inefficient or biased.

## Foundational Learning

- **Concept**: Upper Confidence Bound (UCB) in reinforcement learning
  - **Why needed here**: UCB is used to estimate optimistic rewards for unlabeled prior data, encouraging exploration toward potentially high-reward states.
  - **Quick check question**: What is the purpose of adding a novelty term (RND bonus) to the reward model in the UCB estimate?

- **Concept**: Off-policy reinforcement learning with experience replay
  - **Why needed here**: The method relies on storing and reusing both online and prior data to update the policy and value functions efficiently.
  - **Quick check question**: Why is it important to relabel prior data with optimistic rewards before adding it to the replay buffer?

- **Concept**: Representation learning for high-dimensional observations
  - **Why needed here**: Pre-trained representations (e.g., ICVF) help the reward model and novelty estimator generalize better in image-based tasks.
  - **Quick check question**: How does using pre-trained representations affect the smoothness of reward estimates along trajectories?

## Architecture Onboarding

- **Component map**:
  - Prior data (states, actions) -> Reward model -> RND network -> UCB reward estimates -> Policy network -> Critic network -> Replay buffer

- **Critical path**: Collect online data -> Update reward and RND models -> Relabel prior data with UCB rewards -> Train policy and critic on combined data -> Repeat

- **Design tradeoffs**:
  - Using optimistic rewards encourages exploration but may lead to overestimation if not well-calibrated.
  - Pre-training representations improves reward estimation but adds computational overhead.
  - Periodically resetting the reward model can prevent overfitting but may disrupt learning stability.

- **Failure signatures**:
  - If the agent fails to explore, the UCB estimate may be too pessimistic or the novelty signal too weak.
  - If the agent explores inefficiently, the reward model or RND network may not generalize well.
  - If learning is unstable, the reward estimates may change too rapidly or the policy may overfit to prior data.

- **First 3 experiments**:
  1. Run the method on a simple grid-world with sparse rewards and prior data covering most states. Check if the agent explores efficiently compared to a baseline without prior data.
  2. Test the method on an image-based task (e.g., COG) with and without pre-trained representations. Compare exploration efficiency and final performance.
  3. Corrupt the prior data (e.g., remove transitions near the goal) and evaluate if the method still learns effectively, especially when combined with online RND bonuses.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The method's effectiveness depends heavily on the quality and coverage of the prior data.
- If the prior dataset lacks states near the goal or contains significant biases, the optimistic labeling mechanism may misguide exploration.
- The prior data was not specifically collected for the downstream tasks, which introduces uncertainty about generalizability.

## Confidence
- **High confidence**: The core mechanism of optimistic reward labeling on prior data effectively accelerates exploration in domains where prior data contains relevant states (AntMaze, Adroit hand manipulation)
- **Medium confidence**: The improvement from pre-trained representations is well-demonstrated for image-based tasks but lacks ablation studies showing performance without any prior data
- **Medium confidence**: The claim of matching oracle performance is supported by quantitative results but the comparison is limited to specific benchmark tasks

## Next Checks
1. Test the method's robustness when prior data is deliberately corrupted or incomplete, particularly by removing states near the goal region.
2. Conduct an ablation study comparing performance with and without pre-trained representations on non-image domains to isolate the benefit of representation learning.
3. Evaluate whether the optimistic labeling mechanism can be integrated with other exploration strategies beyond RND, such as ensemble-based uncertainty estimates.