---
ver: rpa2
title: 'LLM-Rec: Personalized Recommendation via Prompting Large Language Models'
arxiv_id: '2307.15780'
source_url: https://arxiv.org/abs/2307.15780
tags:
- recommendation
- prompting
- content
- prec
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLM-Rec leverages large language models for personalized recommendation
  by augmenting input text with diverse prompting strategies. Four strategies are
  evaluated: basic prompting (paraphrase, summarize, characterize), recommendation-driven
  prompting (emphasizing recommendation intent), engagement-guided prompting (incorporating
  user behavior signals), and a combined approach.'
---

# LLM-Rec: Personalized Recommendation via Prompting Large Language Models

## Quick Facts
- arXiv ID: 2307.15780
- Source URL: https://arxiv.org/abs/2307.15780
- Reference count: 16
- Key outcome: LLM-Rec achieves up to +2.65% NDCG@10 improvement on MovieLens-1M by augmenting movie descriptions with LLM-generated text via prompting strategies.

## Executive Summary
LLM-Rec explores leveraging large language models (LLMs) for personalized recommendation by augmenting input text with diverse prompting strategies. The approach generates enhanced movie descriptions using GPT-3 and tests four prompting strategies: basic (paraphrase, summarize, characterize), recommendation-driven, engagement-guided, and a combined approach. Experiments on MovieLens-1M show that augmented input consistently outperforms original content alone, with the combined recommendation-driven and engagement-guided approach achieving the highest gains. The method highlights the importance of strategic prompt design in enhancing LLM-generated text for recommendation tasks.

## Method Summary
LLM-Rec uses GPT-3 to generate augmented text descriptions for movies, which are then encoded and concatenated with original content embeddings. A multi-layer perceptron (MLP) processes these concatenated embeddings to produce item representations, which are combined with user representations via dot product and Sigmoid to calculate relevance scores. The model is trained using Binary Cross Entropy Loss and AdamW optimizer. Four prompting strategies are evaluated: basic prompting (paraphrase, summarize, characterize), recommendation-driven prompting (emphasizing recommendation intent), engagement-guided prompting (incorporating user behavior signals like Personalized PageRank scores), and a combined approach. The MovieLens-1M dataset with GPT-3-generated movie descriptions serves as the evaluation benchmark.

## Key Results
- Augmented text consistently outperforms original content alone, with NDCG@10 improving by up to +2.65% using the combined prompting approach.
- Recommendation-driven prompting enhances context and guided generation, improving recommendation performance.
- Engagement-guided prompting leverages user engagement signals to better align content with user preferences, boosting recommendation quality.
- Basic prompting strategies (p1, p2, p3) show varying effectiveness, with characterization (p3) yielding better results than paraphrase or summarize.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining augmented text with original content description improves recommendation performance.
- Mechanism: Augmented text provides additional context and features that align better with user preferences and item characteristics.
- Core assumption: The LLM can generate meaningful and relevant augmented text that captures useful aspects beyond the original description.
- Evidence anchors:
  - [abstract]: "using LLM-augmented text significantly enhances recommendation quality."
  - [section]: "the combination of augmented text and the original content description leads to an improvement in recommendation performance."
  - [corpus]: No direct corpus evidence for this specific mechanism; however, the paper's experimental results (e.g., NDCG@10 +2.65%) support the claim.
- Break condition: If the LLM generates irrelevant or misleading augmented text, it could degrade performance rather than improve it.

### Mechanism 2
- Claim: Recommendation-driven prompting enhances the generation of text input specifically for recommendation tasks.
- Mechanism: By explicitly mentioning "recommendation" in the prompt, the LLM focuses on generating content descriptions that are more suited for recommendation scenarios.
- Core assumption: The LLM understands the task context and can generate more relevant content when instructed to recommend.
- Evidence anchors:
  - [section]: "The use of recommendation-driven prompting exhibits several compelling characteristics... Enhanced Context, Guided Generation, Improved Relevance."
  - [corpus]: No direct corpus evidence for this specific mechanism; however, the paper's experimental results (e.g., improved performance with recommendation-driven prompting) support the claim.
- Break condition: If the LLM fails to understand the task context or generates content that is not aligned with user preferences, the recommendation performance may not improve.

### Mechanism 3
- Claim: Engagement-guided prompting allows the LLM to generate content descriptions that better align with user preferences.
- Mechanism: By incorporating user engagement signals (e.g., Personalized PageRank scores) into the prompt, the LLM can capture the characteristics of items that are frequently engaged by users.
- Core assumption: Items frequently engaged by users share similar characteristics that are appealing to the target users.
- Evidence anchors:
  - [section]: "This prompting strategy is to leverage user behavior (i.e., user-item engagement) to design prompts with the intention to guide LLM to better capture the characteristics inside the content description that align with user preferences."
  - [corpus]: No direct corpus evidence for this specific mechanism; however, the paper's experimental results (e.g., improved performance with engagement-guided prompting) support the claim.
- Break condition: If the engagement signals do not accurately reflect user preferences or the LLM fails to capture the relevant characteristics, the recommendation performance may not improve.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the core component used for generating augmented text input for personalized recommendation.
  - Quick check question: What are the key capabilities of LLMs that make them suitable for generating augmented text input for recommendation tasks?

- Concept: Prompting Strategies
  - Why needed here: Different prompting strategies (basic, recommendation-driven, engagement-guided) are used to guide the LLM in generating augmented text input.
  - Quick check question: How do the different prompting strategies differ in terms of their approach to generating augmented text input?

- Concept: Personalized PageRank (PPR)
  - Why needed here: PPR is used as an importance measurement for engagement-guided prompting, quantifying the relative importance of items based on user engagement.
  - Quick check question: How does PPR help in identifying important neighbor items for engagement-guided prompting?

## Architecture Onboarding

- Component map:
  LLM → Text Encoder → MLP → Recommendation Module
- Critical path: LLM generates augmented text → Text Encoder converts original and augmented text into embeddings → MLP processes concatenated embeddings → Recommendation Module calculates relevance scores using dot product and Sigmoid function
- Design tradeoffs:
  - Using LLM for input augmentation vs. using LLM as a recommender model directly.
  - Different prompting strategies (basic, recommendation-driven, engagement-guided) and their impact on performance.
  - Tradeoff between augmented text quality and computational cost.
- Failure signatures:
  - Poor recommendation performance due to irrelevant or misleading augmented text.
  - Overfitting or underfitting of the MLP due to suboptimal hyperparameters.
  - Inaccurate user preference alignment due to flawed engagement-guided prompting.
- First 3 experiments:
  1. Evaluate the impact of basic prompting strategies (p1, p2, p3) on recommendation performance.
  2. Compare the performance of recommendation-driven prompting (prec_1, prec_2, prec_3) against basic prompting.
  3. Assess the effectiveness of engagement-guided prompting (peng) by incorporating user engagement signals into the prompt.

## Open Questions the Paper Calls Out
No explicit open questions are called out in the paper.

## Limitations
- The approach depends heavily on the quality of LLM-generated augmented text, which may degrade if the LLM produces irrelevant or misleading descriptions.
- Significant computational resources are required for LLM inference during training and inference.
- The study is limited to MovieLens-1M with movie data, restricting generalizability to other recommendation domains.
- The engagement-guided prompting assumes that frequently engaged items share appealing characteristics, which may not always hold true.

## Confidence
- **High confidence**: The experimental results showing that LLM-augmented text improves recommendation metrics over original content alone (NDCG@10 +2.65% for combined approach)
- **Medium confidence**: The effectiveness of different prompting strategies (basic vs. recommendation-driven vs. engagement-guided) as the paper doesn't provide ablation studies on prompt design variations
- **Low confidence**: Claims about the LLM's ability to capture user preferences through engagement signals, as this mechanism is not directly validated

## Next Checks
1. Conduct robustness tests by evaluating the model on datasets with varying levels of sparsity to assess performance across different data conditions.
2. Perform ablation studies on the MLP architecture and hyperparameters to determine if performance gains are primarily from the augmented text or model architecture.
3. Test the approach on non-text recommendation domains (e.g., product recommendation with categorical features) to evaluate generalizability beyond MovieLens-1M.