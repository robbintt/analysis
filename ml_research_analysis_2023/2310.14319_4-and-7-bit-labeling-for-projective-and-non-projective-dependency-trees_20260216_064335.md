---
ver: rpa2
title: 4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees
arxiv_id: '2310.14319'
source_url: https://arxiv.org/abs/2310.14319
tags:
- encoding
- dependency
- trees
- arcs
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents two new sequence labeling encodings for dependency
  parsing: a 4-bit encoding for projective trees and a 7-bit extension for non-projective
  trees. The encodings represent dependency structures using compact bit patterns
  derived from syntactic properties of each node, achieving full coverage of projective
  trees and near-complete coverage (99.9%) of non-projective trees.'
---

# 4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees

## Quick Facts
- **arXiv ID**: 2310.14319
- **Source URL**: https://arxiv.org/abs/2310.14319
- **Reference count**: 18
- **Primary result**: 7-bit encoding achieves 1.96 LAS improvement over previous methods while using only 128 labels

## Executive Summary
This paper introduces two compact sequence labeling encodings for dependency parsing: a 4-bit encoding for projective trees and a 7-bit extension for non-projective trees. The encodings represent dependency structures using compact bit patterns derived from syntactic properties of each node, achieving full coverage of projective trees and near-complete coverage (>99.9%) of non-projective trees. Empirical results show that the 7-bit encoding outperforms previous sequence labeling methods by 1.96 LAS points on average across diverse treebanks, while the 4-bit encoding excels on highly projective structures. The encodings require only 16 and 128 distinct labels respectively, compared to hundreds needed by prior methods, demonstrating significant improvements in both compactness and accuracy.

## Method Summary
The paper presents a sequence labeling approach that encodes dependency trees as sequences of compact bit patterns. The 4-bit encoding uses 4 bits per word to capture syntactic properties (direction, outermost dependent, presence of children) and achieves injective mapping for projective trees. The 7-bit extension adds two bits for plane information, allowing representation of non-projective structures by partitioning arcs into two planes. The system uses XLM-RoBERTa encoder with separate feed-forward networks for predicting syntactic labels and dependency types. Decoding reconstructs trees using stack-based algorithms that process right and left arcs separately, with heuristics for handling invalid sequences.

## Key Results
- 7-bit encoding achieves 1.96 LAS improvement over previous methods across 16 treebanks
- 4-bit encoding provides full coverage for projective trees with only 16 labels
- 7-bit encoding achieves >99.9% coverage of non-projective arcs while using only 128 labels
- Compact label sets significantly reduce sparsity compared to previous methods requiring hundreds of labels

## Why This Works (Mechanism)

### Mechanism 1
The 4-bit encoding provides an injective mapping from projective trees to label sequences. Each word receives a 4-bit label encoding syntactic properties, and the decoding algorithm reconstructs the tree by separately processing right and left arcs using a stack-based approach that ensures projectivity. The label bits uniquely determine parent-child relationships in projective trees.

### Mechanism 2
The 7-bit encoding extends coverage to nearly all non-projective trees by partitioning dependency trees into two planes and assigning each word a 7-bit label encoding both plane information and syntactic properties. This allows encoding trees where arcs only cross between planes, achieving >99.9% empirical arc coverage.

### Mechanism 3
The encoding achieves superior accuracy by using compact label sets. By using only 16 labels for projective trees and 128 for non-projective trees, the encoding reduces sparsity compared to unbounded methods, leading to better generalization and higher LAS scores.

## Foundational Learning

- **Concept**: Dependency parsing as sequence labeling
  - Why needed here: This paper's encoding methods rely on converting dependency trees into sequences of labels, which is the core task of sequence labeling dependency parsing.
  - Quick check question: What is the main difference between sequence labeling and graph-based dependency parsing approaches?

- **Concept**: Projective vs non-projective dependency trees
  - Why needed here: The 4-bit encoding is designed for projective trees, while the 7-bit encoding handles non-projective trees. Understanding these concepts is crucial for grasping the encoding strategies.
  - Quick check question: How do projective and non-projective dependency trees differ in terms of arc crossings?

- **Concept**: Bracketing encodings
  - Why needed here: The paper compares its compact encodings to previous bracketing methods. Understanding bracketing encodings helps contextualize the improvements made by the new approach.
  - Quick check question: What is the main limitation of unbounded bracketing encodings that this paper addresses?

## Architecture Onboarding

- **Component map**: Tree → 4/7-bit encoding → Label sequence → Sequence labeling model → Predicted labels → Decoding heuristics → Parsed tree
- **Critical path**: The encoder converts trees to label sequences, the XLM-RoBERTa model predicts labels, and the decoder reconstructs trees using stack-based algorithms
- **Design tradeoffs**: The encoding sacrifices some coverage of highly non-projective trees for compactness and simplicity. The choice between 4-bit and 7-bit encodings depends on the treebank's projective characteristics.
- **Failure signatures**: Invalid label sequences during decoding (e.g., trying to pop an empty stack), low coverage on highly non-projective treebanks with 4-bit encoding, suboptimal accuracy due to model architecture limitations.
- **First 3 experiments**:
  1. Implement the 4-bit encoding and test on a simple projective treebank (e.g., PTB) to verify injectivity and decoding.
  2. Extend to the 7-bit encoding and test on a non-projective treebank (e.g., Ancient Greek) to evaluate coverage and accuracy.
  3. Compare the 7-bit encoding against the 2-planar bracketing baseline on a diverse set of treebanks to measure accuracy improvements.

## Open Questions the Paper Calls Out

### Open Question 1
How does the 4-bit encoding's coverage compare to the basic bracketing encoding when handling non-projective trees, and what specific structural differences account for this discrepancy? The paper states that the 4-bit encoding has less coverage than the basic bracketing encoding for non-projective trees, but does not provide a detailed enumeration or analysis of all the cases where the 4-bit encoding fails compared to the basic bracketing encoding.

### Open Question 2
What is the impact of different bit-splitting strategies on the performance of the 4-bit and 7-bit encodings, and how can the optimal splitting be determined? The paper explores a preliminary bit-splitting strategy but only investigates a preliminary division of bits and does not determine the optimal splitting for maximizing performance.

### Open Question 3
How do the processing speeds of the 4-bit and 7-bit encodings compare to other sequence-labeling parsers, and what factors contribute to their speed differences? The paper notes that while SuPar is optimized, MaChAmp (used for the encodings) is a general sequence labeling framework without specific optimization for speed, but does not provide a detailed analysis of the factors contributing to the speed differences.

## Limitations

- The 7-bit encoding's >99.9% coverage claim is theoretical based on the two-plane partition approach rather than exhaustive validation on high non-projectivity treebanks
- Decoding heuristics for invalid label sequences are described but not empirically validated for effectiveness or potential error introduction
- Performance improvements assume compact encodings are the primary driver, but the contribution relative to other factors (model architecture, hyperparameters) is not isolated

## Confidence

- **High confidence**: Theoretical foundations of 4-bit and 7-bit encodings, including injectivity proofs and linear-time decoding algorithms
- **Medium confidence**: Empirical coverage claims (>99.9% for 7-bit encoding) and LAS improvements, as actual performance on edge cases is not fully validated
- **Medium confidence**: Claim that compact label sets improve generalization and accuracy, as this effect is not isolated through ablation studies

## Next Checks

1. **Coverage validation on high non-projectivity treebanks**: Test the 7-bit encoding on the 6 treebanks that didn't achieve >99.9% coverage to determine actual coverage limits and identify specific tree structures that fail.

2. **Decoding heuristic effectiveness**: Implement the decoding heuristics for invalid sequences and measure their accuracy. Create synthetic invalid label sequences to test whether the heuristics correctly reconstruct valid trees or introduce errors.

3. **Encoding contribution isolation**: Conduct an ablation study comparing the 7-bit encoding against a version using the same model architecture but with traditional bracketing encodings to isolate whether performance gains come from the compact encoding or other factors like the transformer backbone.