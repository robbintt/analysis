---
ver: rpa2
title: Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile
  Communications
arxiv_id: '2303.16737'
source_url: https://arxiv.org/abs/2303.16737
tags:
- users
- user
- action
- power
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of optimizing UAV trajectories
  and NOMA power allocation to maximize throughput in mobile communications for disaster
  recovery scenarios. The authors propose a two-step solution: first, a weighted K-means
  clustering algorithm for UAV-user associations; second, a Shared Deep Q-Network
  (SDQN) with action masking for joint optimization of UAV 3D trajectories and power
  allocation.'
---

# Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications

## Quick Facts
- arXiv ID: 2303.16737
- Source URL: https://arxiv.org/abs/2303.16737
- Reference count: 40
- Primary result: SDQN approach achieves 20% higher maximum system throughput and 10% faster convergence compared to conventional DQN methods

## Executive Summary
This paper addresses the challenge of optimizing UAV trajectories and NOMA power allocation for maximizing throughput in disaster recovery scenarios. The authors propose a two-step solution combining weighted K-means clustering for UAV-user associations with a Shared Deep Q-Network (SDQN) approach that includes action masking. The SDQN method enables training of multi-agent systems with differing action spaces, yielding significant performance improvements over conventional approaches. Simulation results demonstrate the effectiveness of the proposed method in maximizing system throughput while maintaining quality of service requirements.

## Method Summary
The method employs a two-step approach: first, weighted K-means clustering establishes UAV-user associations periodically (every Tr seconds), ensuring each UAV serves a maximum of M users; second, a Shared Deep Q-Network (SDQN) jointly optimizes UAV 3D trajectories and NOMA power allocation. The SDQN uses state abstraction to standardize input formats across multiple UAVs and employs action masking to prevent selection of invalid actions when UAVs have different numbers of associated users. Training uses epsilon-greedy policy with Adam optimizer, and experiences are shared across agents to reduce training time compared to separate DQN training.

## Key Results
- SDQN approach achieves 20% higher maximum system throughput compared to conventional DQN methods
- Training convergence is 10% faster with SDQN compared to separate DQN training
- SDQN with action masking yields 9% increased throughput compared to mutual learning algorithms for multi-agent systems with differing action spaces
- NOMA with SDQN results in better sum rates compared to baseline schemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SDQN reduces training time by sharing experiences across multiple UAV agents
- Mechanism: Multiple UAVs sequentially train a single neural network, allowing each agent to benefit from others' experiences
- Core assumption: State abstraction properly aligns state information from different UAVs into common input format
- Evidence anchors: Abstract states shared learning reduces training time; section describes state abstraction as re-organization of experiences

### Mechanism 2
- Claim: Invalid action masking prevents agents from selecting impossible actions in shared DQN
- Mechanism: Impossible actions are masked by setting their Q-values to negative infinity
- Core assumption: Action masking correctly identifies which actions are impossible for each agent
- Evidence anchors: Section describes concatenating action spaces and using masking to prevent impossible action selection

### Mechanism 3
- Claim: Periodic re-clustering maintains optimal UAV-user associations as users move
- Mechanism: Users are periodically re-associated with UAVs using weighted K-means clustering based on current positions
- Core assumption: Weighted K-means algorithm effectively re-associates users with nearest UAV
- Evidence anchors: Abstract mentions clustering establishes associations at regular intervals; section describes periodic re-clustering after time interval Tr

## Foundational Learning

- Concept: Reinforcement Learning basics (states, actions, rewards, Q-learning)
  - Why needed here: The paper uses DQN/SDQN which are RL algorithms requiring understanding of Q-learning fundamentals
  - Quick check question: What is the Bellman equation and how does it relate to Q-learning?

- Concept: Neural network function approximation for Q-values
  - Why needed here: The paper replaces Q-tables with deep neural networks to handle large state/action spaces
  - Quick check question: Why can't we use traditional Q-learning with tables for this problem?

- Concept: Action masking in multi-agent systems
  - Why needed here: Different UAVs serve different numbers of users, requiring different action spaces in shared DQN
  - Quick check question: How does setting Q-values to negative infinity prevent selection of invalid actions?

## Architecture Onboarding

- Component map: User movement simulation -> Periodic clustering and association -> SDQN experience collection and training -> Action selection with masking -> Trajectory execution and power allocation

- Critical path: 1) User movement simulation 2) Periodic clustering and association 3) SDQN experience collection and training 4) Action selection with masking 5) Trajectory execution and power allocation

- Design tradeoffs:
  - Shared DQN vs separate DQNs: faster training vs potential interference
  - Periodic vs continuous clustering: computational efficiency vs optimality
  - Action masking vs separate networks: simpler architecture vs more complex training

- Failure signatures:
  - Slow convergence: state abstraction or reward design issues
  - Suboptimal trajectories: incorrect clustering or insufficient training
  - Training instability: action masking failures or improper experience replay

- First 3 experiments:
  1. Compare throughput with and without periodic clustering
  2. Test SDQN convergence vs separate DQN training
  3. Validate action masking prevents invalid action selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SDQN approach perform with varying numbers of UAVs and users compared to the mutual learning scheme?
- Basis in paper: The paper states that SDQN yields 9% increased throughput for the service period compared to mutual learning algorithm, but notes mutual learning performs better during short intervals and when users are in close vicinity
- Why unresolved: Paper does not provide comprehensive comparison across different numbers of UAVs and users or explore scalability
- What evidence would resolve it: Conducting simulations with varying numbers of UAVs and users to compare SDQN and mutual learning schemes

### Open Question 2
- Question: How does the trajectory optimization change when considering UAV hovering and propulsion in the SDQN approach?
- Basis in paper: Paper mentions key limitation is not taking UAV hovering and/or propulsion into account while moving
- Why unresolved: Paper does not provide simulation results or analysis on how incorporating hovering and propulsion would affect trajectory optimization
- What evidence would resolve it: Modifying SDQN approach to include UAV hovering and propulsion constraints and comparing resulting trajectories and system performance

### Open Question 3
- Question: How does the SDQN approach perform in scenarios with different user mobility patterns, such as urban vs. rural environments?
- Basis in paper: Paper uses Manhattan grid model with spatial dependencies to simulate user mobility in disaster-hit area
- Why unresolved: Paper only provides simulation results for single user mobility model and does not investigate how SDQN approach adapts to different mobility patterns
- What evidence would resolve it: Conducting simulations with different user mobility models, such as urban and rural environments, and comparing SDQN performance

## Limitations

- Limited performance comparison with state-of-the-art RL methods like PPO or actor-critic approaches
- Action masking mechanism validation lacks empirical evidence demonstrating its necessity for stable training
- Simulation assumptions using Manhattan mobility model may not accurately represent disaster recovery scenarios

## Confidence

- High confidence: Weighted K-means clustering algorithm and its periodic application are well-defined and straightforward to implement
- Medium confidence: Claim about 20% higher maximum system throughput and 10% faster convergence requires careful validation due to incomplete baseline performance details
- Low confidence: Assertion that action masking is key enabler for training multi-agent systems with different action spaces lacks direct experimental evidence

## Next Checks

1. Action masking validation: Run experiments with and without action masking to empirically verify that invalid actions are being prevented and that masking is essential for stable training with variable action spaces across agents

2. Alternative RL algorithm comparison: Implement and compare SDQN against PPO and actor-critic methods to determine if performance improvements are specific to SDQN approach or achievable with other modern RL algorithms

3. Robustness to mobility patterns: Test the system under different user mobility models (random waypoint, Gauss-Markov) to validate that performance claims hold beyond Manhattan grid assumption used in the paper