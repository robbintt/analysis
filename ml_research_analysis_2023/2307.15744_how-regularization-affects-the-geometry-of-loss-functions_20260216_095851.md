---
ver: rpa2
title: How regularization affects the geometry of loss functions
arxiv_id: '2307.15744'
source_url: https://arxiv.org/abs/2307.15744
tags:
- function
- morse
- loss
- regularizer
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how regularization affects the geometry
  of loss functions in neural networks. The authors study whether regularized loss
  functions become Morse functions, a property that ensures isolated critical points.
---

# How regularization affects the geometry of loss functions

## Quick Facts
- arXiv ID: 2307.15744
- Source URL: https://arxiv.org/abs/2307.15744
- Reference count: 15
- Key outcome: Regularization affects the geometry of loss functions in neural networks, with generalized L2 regularizer making functions Morse generically, while standard L2 and multiplicative regularizers have different effects.

## Executive Summary
This paper investigates how different regularization techniques affect the geometry of loss functions in neural networks, specifically examining whether regularized loss functions become Morse functions. The authors prove that the generalized L2 regularizer can make loss functions Morse for a generic choice of parameters by breaking rotational symmetries. However, they show that standard L2 regularization fails to achieve this for linear networks, and multiplicative regularizers also fail to produce Morse functions. These results provide insights into how regularization shapes the optimization landscape and have implications for understanding neural network training dynamics.

## Method Summary
The paper uses theoretical analysis of neural network loss functions under various regularization schemes. The authors construct neural networks with smooth activation functions, define loss functions on given datasets, and analyze the critical points of regularized loss functions. They employ mathematical techniques from differential geometry and algebraic topology, including the concept of Morse functions and their properties. The analysis involves examining how different regularizers affect the gradient and Hessian of the loss function, particularly focusing on whether critical points become isolated and non-degenerate.

## Key Results
- Generalized L2 regularization makes loss functions Morse for generic parameter choices by breaking rotational symmetries
- Standard L2 regularization preserves rotational symmetries in linear networks, preventing Morse-ness
- Multiplicative regularizers create core loci where all derivatives vanish to order greater than 2, preventing Morse functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For the generalized L2 regularizer, the regularized loss function becomes Morse for a generic choice of parameters.
- Mechanism: The generalized L2 regularizer adds a sum of squared parameters weighted by distinct coefficients (ϵ₁α₁² + ... + ϵdαd²). This perturbation breaks the rotational symmetries that prevent Morse-ness in the unregularized case, making the critical locus discrete.
- Core assumption: The loss function L is smooth and the generalized L2 regularizer acts as a generic perturbation in the sense of Lerario's theorem.
- Evidence anchors:
  - [abstract]: "For a generic choice of parameters, the regularized loss function Lε becomes Morse"
  - [section 4]: "Theorem 4.1. Let N be a neural network with a smooth activation function... For a generic choice of ⃗ϵ ∈ Rd, the generalized-L2-regularized loss function L⃗ϵ = L + R⃗ϵ is Morse"
  - [corpus]: Weak evidence - corpus neighbors discuss regularizers but not specifically generalized L2
- Break condition: If the activation function is not smooth (e.g., ReLU) or if the data set is empty, the theorem's assumptions fail.

### Mechanism 2
- Claim: Standard L2 regularization does not make the loss function Morse for linear networks.
- Mechanism: The standard L2 regularizer (ϵ∑αᵢ²) preserves the rotational symmetries of the loss function for linear networks, maintaining a positive-dimensional locus of critical points.
- Core assumption: The neural network uses linear activation functions and the data set does not lie in a constant linear subspace.
- Evidence anchors:
  - [section 5.2]: "Theorem 5.4. Consider a linear fully connected feedforward neural network... The regularized function Lϵ = L + ϵRs has a positive-dimensional locus of global minima, and in particular is not Morse, for any ϵ small enough"
  - [abstract]: "for the standard L2 regularizer, they show that linear networks do not become Morse under regularization"
  - [corpus]: Weak evidence - corpus neighbors don't specifically address linear networks
- Break condition: If the activation function is nonlinear or if the network architecture breaks the symmetry group.

### Mechanism 3
- Claim: Multiplicative regularizers fail to produce Morse functions for both linear and nonlinear networks.
- Mechanism: The multiplicative regularizer (ϵ∥M₁∥²₂ · · · ∥Mℓ₊₁∥²₂) creates a core locus where all derivatives vanish to order greater than 2, preventing Morse-ness.
- Core assumption: The network has depth ℓ ≥ 4 (for the full result) or ℓ ≥ 2 (for the origin being degenerate).
- Evidence anchors:
  - [section 6]: "Proposition 6.1. Let L denote the L2 loss function for a fully connected feedforward neural network. For this choice of regularizer, if the depth ℓ of the neural network is greater than or equal to 4, not only does Lϵ = L + ϵRm fail to be Morse, but it has a positive-dimensional locus of degenerate critical points"
  - [abstract]: "The authors also demonstrate that a multiplicative regularizer does not lead to Morse functions"
  - [corpus]: Weak evidence - corpus neighbors discuss regularizers but not multiplicative ones specifically
- Break condition: If the network depth is less than 2 or if the regularizer is modified to break the core locus structure.

## Foundational Learning

- Concept: Morse functions and their properties
  - Why needed here: Understanding whether a function is Morse determines if its critical locus is discrete, which is crucial for optimization and generalization in neural networks
  - Quick check question: What is the defining property of a Morse function, and why is it important that critical points are nondegenerate?
- Concept: Regularization and its effect on function geometry
  - Why needed here: Different regularization techniques can fundamentally change the landscape of the loss function, affecting optimization dynamics
  - Quick check question: How does adding an L2 regularizer change the gradient and Hessian of the loss function?
- Concept: Neural network architecture and loss function construction
  - Why needed here: The specific architecture (activation functions, depth, connectivity) determines how regularization affects the loss function's geometry
  - Quick check question: For a fully connected feedforward network, how is the loss function L(α) constructed from the network parameters and data?

## Architecture Onboarding

- Component map:
  - Data points (xᵢ, yᵢ) -> Parameter vectors w, b for each layer -> Smooth activation functions σ (e.g., tanh, sigmoid) -> Loss function L(α) = Σᵢ ζ(fα(xᵢ) - yᵢ) -> Regularizers R⃗ϵ(α), Rs(α), Rm(α)
- Critical path:
  1. Construct neural network architecture
  2. Define smooth activation functions
  3. Specify data set D
  4. Choose loss function ζ
  5. Add appropriate regularizer
  6. Analyze whether Lϵ becomes Morse
- Design tradeoffs:
  - Smooth vs. non-smooth activation functions (ReLU fails for this analysis)
  - Generalized vs. standard L2 regularization (generalized works for all architectures, standard fails for linear)
  - Depth of network (affects multiplicative regularizer's impact)
- Failure signatures:
  - Non-smooth activation functions → theorem assumptions fail
  - Linear networks with standard L2 regularization → positive-dimensional critical locus
  - Multiplicative regularizers → core locus with degenerate critical points
- First 3 experiments:
  1. Implement a fully connected feedforward network with tanh activation, apply generalized L2 regularization, and verify critical points are isolated using numerical optimization
  2. Implement a linear network with standard L2 regularization, compute critical locus, and demonstrate it's positive-dimensional
  3. Implement a deep network with multiplicative regularization, find core locus, and verify all derivatives vanish to order > 2 at these points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we characterize the set Qs of smooth functions h: R^d → R such that for all but finitely many ε, h + εRs is Morse?
- Basis in paper: [inferred] The paper discusses this question in the context of L2 regularization and its effect on the Morse property of functions.
- Why unresolved: The paper provides an example showing that the answer to this question is no for a specific function, but a general characterization of Qs remains open.
- What evidence would resolve it: A complete characterization of the set Qs, possibly involving conditions on the rotational symmetry or other geometric properties of the function h.

### Open Question 2
- Question: Does Conjecture 5.6 hold, stating that if the activation function σ is a smooth function that is not linear, and the data set contains ≥ 2 data points, then the loss function L is not an element of Rot?
- Basis in paper: [explicit] The paper states this as an open question and provides a roadmap for its resolution.
- Why unresolved: The conjecture remains unproven, and its resolution would require studying the rotational symmetry of the unregularized function L.
- What evidence would resolve it: A proof or counterexample to Conjecture 5.6, potentially using techniques from differential or semialgebraic geometry.

### Open Question 3
- Question: Can Proposition A.7 be extended to the setting of neural networks with polynomial activation functions?
- Basis in paper: [inferred] The paper mentions that Proposition A.7 applies to polynomials and suggests that it may be possible to extend it to neural networks with polynomial activation functions.
- Why unresolved: The proposition is currently stated for polynomials, and extending it to neural networks with polynomial activation functions would require additional work.
- What evidence would resolve it: A proof or counterexample showing that the proposition can or cannot be extended to neural networks with polynomial activation functions.

## Limitations

- The analysis relies on smoothness assumptions for activation functions, excluding ReLU and other non-smooth activations commonly used in practice
- The results focus on fully connected feedforward networks, potentially limiting generalizability to other architectures
- The open question regarding nonlinear networks with standard L2 regularization represents a significant theoretical gap

## Confidence

- High confidence in generalized L2 results for smooth activations
- Medium confidence in standard L2 linear network results
- Medium confidence in multiplicative regularizer findings for ℓ ≥ 4
- Low confidence in nonlinear network behavior with standard L2 regularization

## Next Checks

1. Implement numerical experiments to verify the positive-dimensional critical locus for linear networks under standard L2 regularization
2. Extend the multiplicative regularizer analysis to networks of depth 2 and 3 to understand the boundary of the current results
3. Test whether the generalized L2 regularizer produces isolated critical points for a broader class of smooth activation functions beyond those theoretically proven