---
ver: rpa2
title: Fine-Tuning Language Models with Advantage-Induced Policy Alignment
arxiv_id: '2306.02231'
source_url: https://arxiv.org/abs/2306.02231
tags:
- policy
- reward
- language
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses instability and mode collapse issues in reinforcement
  learning from human feedback (RLHF) for aligning large language models (LLMs) to
  human preferences. The authors propose Advantage-Induced Policy Alignment (APA),
  a novel algorithm that leverages a squared error loss function based on estimated
  advantages.
---

# Fine-Tuning Language Models with Advantage-Induced Policy Alignment

## Quick Facts
- arXiv ID: 2306.02231
- Source URL: https://arxiv.org/abs/2306.02231
- Reference count: 38
- One-line primary result: APA achieves higher rewards on evaluation sets while maintaining better control over KL divergence from the initial policy compared to PPO

## Executive Summary
This paper addresses key challenges in reinforcement learning from human feedback (RLHF) for aligning large language models, specifically instability, mode collapse, and poor sample efficiency. The authors propose Advantage-Induced Policy Alignment (APA), a novel algorithm that leverages squared error loss based on estimated advantages to improve alignment stability and efficiency. APA demonstrates superior performance compared to Proximal Policy Optimization (PPO) on fine-tuning GPT-J-6B models using the Helpfulness and Harmlessness dataset.

## Method Summary
APA uses a squared error loss function in log probability space to align the policy with an advantage-weighted target policy. The target policy combines the initial language model policy with an advantage-induced correction term estimated from previous online samples. Unlike PPO, APA avoids importance sampling ratios and adaptive KL penalties, instead directly optimizing a loss that includes the initial policy's log probabilities for natural KL control. The method is tested on GPT-J-6B models fine-tuned using the HH dataset with a separate reward model as evaluator.

## Key Results
- APA consistently outperforms PPO in language tasks by achieving higher rewards on evaluation sets
- APA provides better control over KL divergence from the initial policy without requiring adaptive controllers
- APA demonstrates improved sample efficiency, requiring fewer samples to achieve comparable or better performance than PPO and AWR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APA uses squared error loss in log probability space to align policy with advantage-weighted target, avoiding importance sampling bias
- Mechanism: The loss function minimizes squared difference between log probabilities of current policy and target policy constructed from initial policy and advantage estimates
- Core assumption: Advantage function estimates are accurate enough that their log-transformed contributions dominate over initial policy log terms
- Evidence anchors: [abstract] "leverages a squared error loss function based on the estimated advantages"; [section 3.3] "we employ the squared error between log probabilities"
- Break condition: If advantage estimates are highly noisy or biased, squared error in log space could amplify errors and destabilize training

### Mechanism 2
- Claim: APA provides stable KL control without adaptive controllers, reducing hyperparameter sensitivity
- Mechanism: By directly optimizing a loss that includes the initial policy's log probabilities, APA naturally constrains policy deviation without requiring adaptive KL penalties or clipping
- Core assumption: The λ hyperparameter in the loss provides sufficient control over policy deviation magnitude
- Evidence anchors: [abstract] "offers a more stable form of control over the deviation from the model's initial policy"; [section 4.1] "APA provides similar or better KL control than PPO and AWR"
- Break condition: If λ is set too high, policy changes become too conservative and learning stalls; if too low, deviation control fails

### Mechanism 3
- Claim: APA achieves better sample efficiency by avoiding importance sampling reconstruction errors
- Mechanism: The squared error loss directly minimizes distance to target policy without needing to reconstruct the sampling policy distribution
- Core assumption: The dataset covers the relevant state-action space sufficiently for the squared error loss to be effective
- Evidence anchors: [abstract] "consistently outperforms PPO in language tasks by a large margin"; [section 4.1] "APA is more sample-efficient"
- Break condition: In offline settings with limited coverage, the squared error loss may fail to learn effective policies for underrepresented state-action pairs

## Foundational Learning

- Concept: Markov Decision Processes and policy gradient methods
  - Why needed here: The paper builds on RL theory to formulate language model fine-tuning as policy optimization
  - Quick check question: How does the policy gradient theorem justify using advantage estimates in policy updates?

- Concept: KL divergence and trust region methods
  - Why needed here: Understanding how KL regularization prevents mode collapse is crucial for grasping APA's design choices
  - Quick check question: What role does the KL constraint play in preventing the policy from collapsing to deterministic output?

- Concept: Importance sampling and its variance
  - Why needed here: PPO's reliance on importance sampling ratios is a key limitation that APA addresses
  - Quick check question: Why does importance sampling introduce high variance in policy gradient estimates, and how does this affect training stability?

## Architecture Onboarding

- Component map: Reward model -> Policy model -> Advantage estimator -> Loss function
- Critical path:
  1. Generate responses using current policy
  2. Score responses with reward model
  3. Estimate advantages using value network
  4. Update policy using chosen algorithm's loss
  5. Repeat for multiple epochs

- Design tradeoffs:
  - APA vs PPO: APA offers better stability and sample efficiency but may require careful λ tuning
  - Batch size: Smaller batch sizes used for larger models due to memory constraints
  - Learning rate: Fixed across experiments, but may need adjustment for different model sizes

- Failure signatures:
  - Reward collapse: Indicates mode collapse or poor advantage estimation
  - KL divergence spikes: Suggests policy deviation control is failing
  - Slow reward improvement: May indicate suboptimal λ or learning rate

- First 3 experiments:
  1. Run APA with λ=0.1 on 125M model, compare reward and KL curves to PPO baseline
  2. Vary λ in APA (0.1, 1.0, 10.0) on same model, observe tradeoff between reward and KL control
  3. Test APA on 1B model with same λ, verify scalability of sample efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of APA compare to PPO and AWR in terms of sample efficiency and KL control when applied to larger language models?
- Basis in paper: [explicit] The paper demonstrates the efficacy of APA on fine-tuning GPT-J-6B models using the Helpfulness and Harmlessness dataset, outperforming PPO in sample efficiency and KL control. However, the paper does not provide results for larger models.
- Why unresolved: The paper focuses on fine-tuning GPT-J-6B models, and it is unclear how APA would perform on even larger models.
- What evidence would resolve it: Experiments comparing the performance of APA, PPO, and AWR on larger language models, such as GPT-3 or GPT-4, would provide insights into the scalability of APA.

### Open Question 2
- Question: Can APA be effectively adapted to the offline learning setting, where a fixed dataset is given and new samples are not available?
- Basis in paper: [inferred] The paper primarily examines the online case, where new data can be collected during the training process. The authors mention that the offline setting may yield qualitatively different results, but they do not provide experimental results for offline learning.
- Why unresolved: The paper does not provide experimental results for offline learning, and it is unclear how APA would perform in this setting.
- What evidence would resolve it: Experiments comparing the performance of APA, PPO, and AWR in the offline learning setting, using a fixed dataset, would provide insights into the applicability of APA to this scenario.

### Open Question 3
- Question: How does the choice of the hyperparameter λ in APA affect the trade-off between KL control and model performance?
- Basis in paper: [explicit] The paper mentions that the choice of λ directly determines the level of KL control and the convergent point APA reaches. However, the paper does not provide a detailed analysis of how different values of λ affect the trade-off between KL control and model performance.
- Why unresolved: The paper provides a brief discussion on the effect of λ, but it does not provide a comprehensive analysis of the trade-off between KL control and model performance for different values of λ.
- What evidence would resolve it: A detailed analysis of the effect of different values of λ on the trade-off between KL control and model performance, including experiments and visualizations, would provide insights into the optimal choice of λ for different tasks and models.

## Limitations

- Limited ablation studies on the critical λ hyperparameter make it unclear how sensitive the method is to its single tuning parameter
- Theoretical analysis assumes idealized conditions that may not hold in practice, particularly regarding the accuracy of advantage estimates
- Experiments focus on a single dataset (HH) and reward model architecture, limiting generalizability to other alignment scenarios

## Confidence

- High confidence: APA's superior sample efficiency compared to PPO (supported by consistent reward improvements across multiple model sizes)
- Medium confidence: APA's stability claims (KL control is demonstrated but without extensive hyperparameter sweeps to prove robustness)
- Medium confidence: Theoretical convergence claims (analysis is provided but assumes perfect advantage estimation and may not reflect practical conditions)

## Next Checks

1. Conduct ablation studies varying λ across multiple orders of magnitude to characterize the sensitivity of APA's performance and stability to this critical hyperparameter
2. Test APA on additional datasets beyond HH to verify robustness across different alignment tasks and reward signal characteristics
3. Implement a variant of APA using learned advantage estimators rather than generalized advantage estimation to assess sensitivity to advantage estimation quality