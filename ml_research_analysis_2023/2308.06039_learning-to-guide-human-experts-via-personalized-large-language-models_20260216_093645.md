---
ver: rpa2
title: Learning to Guide Human Experts via Personalized Large Language Models
arxiv_id: '2308.06039'
source_url: https://arxiv.org/abs/2308.06039
tags:
- human
- learning
- guidance
- decision
- guide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach for hybrid decision-making
  in medical diagnosis that aims to reduce over-reliance on machine predictions while
  still providing useful assistance to human experts. The proposed method, called
  Learning to Guide (LTG), replaces the traditional learning to defer setup with one
  where the machine generates interpretable textual guidance rather than making direct
  predictions.
---

# Learning to Guide Human Experts via Personalized Large Language Models

## Quick Facts
- arXiv ID: 2308.06039
- Source URL: https://arxiv.org/abs/2308.06039
- Reference count: 3
- One-line primary result: SLOG reduces over-reliance on machine predictions in medical diagnosis by generating interpretable textual guidance rather than direct decisions

## Executive Summary
This paper introduces Learning to Guide (LTG), a novel approach for hybrid decision-making in medical diagnosis that addresses over-reliance on machine predictions. Instead of making direct diagnostic decisions, the machine generates interpretable textual guidance for human experts to use in their own decision-making process. The authors develop SLOG, an implementation that fine-tunes a pre-trained large language model to generate high-quality guidance captions for chest X-ray images. By using a surrogate model to generalize sparse human feedback, SLOG efficiently trains the LLM to produce tailored guidance that supports human experts while maintaining their decision-making autonomy.

## Method Summary
SLOG fine-tunes a pre-trained large language model (LLM) to generate textual guidance for chest X-ray images, using sparse human feedback to guide the training process. The approach employs a surrogate model (non-linear ridge regression) that learns to predict human quality judgments from the LLM's latent representations. This surrogate model enables efficient fine-tuning of the LLM without requiring extensive human annotation. The method uses the MIMIC-CXR-IV dataset, filtering to examples with relevant findings or impression fields, and iteratively refines the LLM's guidance generation capability through a combined loss function that balances standard language modeling objectives with surrogate-based penalties.

## Key Results
- Surrogate model effectively predicts human judgments of guidance quality with RMSE of 0.1828, close to the variance of human judgments (0.1817)
- Iterative fine-tuning with surrogate guidance gradually improves LLM guidance quality while maintaining text coherence
- SLOG successfully generates interpretable textual guidance that supports human decision-making in medical diagnosis tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLOG reduces anchoring bias by replacing direct decision-making with interpretable guidance generation.
- Mechanism: By having the machine generate textual guidance instead of direct predictions, the human expert must actively interpret and use the guidance to make their own decision, maintaining full control and reducing over-reliance on machine outputs.
- Core assumption: Textual guidance is sufficiently informative and interpretable for human experts to make sound decisions.
- Evidence anchors:
  - [abstract] "Instead of proposing potential decisions, in LTG the machine supplies its human partner with interpretable guidance"
  - [section] "Instead of suggesting ready-made decisions – the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision."
- Break condition: If generated guidance is too vague, overly technical, or misleading, experts may default to anchoring on their own prior knowledge or make incorrect decisions.

### Mechanism 2
- Claim: The surrogate model effectively generalizes sparse human feedback to improve LLM guidance generation.
- Mechanism: Human experts provide quality scores on a small set of generated guidance captions. A surrogate model learns to predict these scores from the LLM's latent representations, enabling fine-tuning of the LLM to produce higher-quality guidance without requiring extensive human annotation.
- Core assumption: Human quality judgments are consistent enough to be learned by a surrogate model, and the latent representation space contains sufficient signal for prediction.
- Evidence anchors:
  - [abstract] "Since human feedback is expensive and therefore scarce, SLOG makes use of a surrogate model that generalizes from a handful of quality judgments to fine-tune the LLM."
  - [section] "The surrogate model itself is a non-linear ridge regression model... The RMSE on the training and test sets are reported in Fig. 1... the test RMSE is 0.1828, which is well within the variance of the ground-truth human judgments (0.1817)."
- Break condition: If human judgments are highly inconsistent or the surrogate model fails to generalize, the LLM fine-tuning process will not improve guidance quality.

### Mechanism 3
- Claim: Iterative fine-tuning with surrogate guidance gradually improves LLM guidance quality while maintaining text coherence.
- Mechanism: The SLOG algorithm iteratively fine-tunes the LLM using a combined loss function that includes both the standard language modeling objective and a surrogate-based penalty term. This ensures that the LLM learns to generate task-specific guidance while still producing grammatically correct and meaningful text.
- Core assumption: The augmented loss function (Equation 1) effectively balances task-specific guidance quality with general text generation capability.
- Evidence anchors:
  - [section] "The LLM gradually learns to output image captions that work well as textual guidance and that are tailored for the specific task and human expert at hand."
  - [section] "Since the LLM's embedding space changes during fine-tuning, the surrogate is fit anew in each iteration."
- Break condition: If the λ hyperparameter is poorly tuned, the LLM may either focus too much on surrogate scores (losing text quality) or too little (failing to improve guidance).

## Foundational Learning

- Concept: Learning to defer vs. learning to guide
  - Why needed here: Understanding the key difference between traditional LTD (where the model directly suggests decisions) and LTG (where the model provides guidance) is crucial for grasping the novelty and motivation of the approach.
  - Quick check question: In LTD, who makes the final decision when the model defers? In LTG, who is responsible for the decision?

- Concept: Fine-tuning with sparse human feedback
  - Why needed here: The SLOG approach relies on efficiently using limited human annotations through surrogate modeling, which requires understanding how to leverage small amounts of high-quality feedback.
  - Quick check question: Why is using a surrogate model preferable to directly fine-tuning the LLM with human feedback in this setting?

- Concept: Large language model adaptation for specialized tasks
  - Why needed here: Converting a generic pre-trained LLM into a specialized guidance generator requires understanding how fine-tuning works and what architectural considerations are important.
  - Quick check question: What is the role of the latent representation z in the fine-tuning process, and why is it important that the surrogate model operates on this space?

## Architecture Onboarding

- Component map:
  Pre-trained LLM (caption generator) -> Decision Maker (DM) -> Surrogate Model -> Fine-tuning Loop

- Critical path:
  1. LLM generates guidance for a batch of X-ray images.
  2. Human expert scores the quality of each guidance caption.
  3. Surrogate model is trained on (z, q) pairs to predict scores from latent representations.
  4. LLM is fine-tuned using the augmented loss function with frozen surrogate.
  5. Repeat from step 1 with updated LLM.

- Design tradeoffs:
  - Using a surrogate model vs. direct human-in-the-loop fine-tuning: Surrogate is cheaper but may not perfectly capture human judgment nuances.
  - Non-linear ridge regression vs. more complex surrogate architectures: Simpler models are faster to train and less prone to overfitting with limited data.
  - Frequency of surrogate retraining: Retraining each iteration ensures alignment with updated LLM space but adds computational overhead.

- Failure signatures:
  - Guidance quality plateaus or degrades: May indicate surrogate model failure to generalize or suboptimal λ hyperparameter.
  - Human experts over-rely on guidance: Could suggest guidance is too directive or not interpretable enough.
  - Surrogate model overfits: Training RMSE much lower than validation RMSE, indicating poor generalization.

- First 3 experiments:
  1. Train surrogate model with varying amounts of human feedback (e.g., 10, 50, 100 examples) and measure generalization performance.
  2. Test different surrogate architectures (linear regression, random forest, neural network) to find best balance of performance and training efficiency.
  3. Perform ablation study on λ hyperparameter to determine optimal weight for surrogate penalty term in fine-tuning.

## Open Questions the Paper Calls Out
- How effective is the surrogate model in generalizing human judgments of textual guidance quality?
- How does the quality of textual guidance generated by SLOG compare to that generated by other methods?
- How does the choice of the LLM architecture and fine-tuning strategy impact the quality of textual guidance generated by SLOG?

## Limitations
- The evaluation focuses on surrogate model performance rather than actual human decision-making outcomes, leaving open questions about real-world effectiveness.
- The human feedback collection process and specific criteria for guidance quality assessment are not fully detailed.
- The approach assumes textual guidance can effectively replace direct decision-making without introducing new forms of bias.

## Confidence
- **High Confidence**: The technical implementation of SLOG, including the surrogate modeling approach and iterative fine-tuning procedure, is well-specified and grounded in established machine learning principles.
- **Medium Confidence**: The claim that guidance generation reduces anchoring bias is plausible but requires empirical validation with actual human experts making diagnostic decisions.
- **Low Confidence**: The assertion that this approach generalizes to different medical specialties or expertise levels without modification.

## Next Checks
1. Conduct a user study where human experts make actual diagnostic decisions using the generated guidance, measuring decision accuracy, time, and subjective reliance on the guidance compared to baseline approaches.
2. Test the surrogate model's ability to generalize across different subsets of the data (e.g., different finding types or severity levels) to identify potential blind spots in the guidance generation.
3. Perform a systematic analysis of guidance quality variance across different human raters to quantify the consistency of the feedback signal and its impact on surrogate model performance.