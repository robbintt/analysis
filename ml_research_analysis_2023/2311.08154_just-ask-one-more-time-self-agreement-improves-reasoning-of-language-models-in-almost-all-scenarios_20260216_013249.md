---
ver: rpa2
title: Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models
  in (Almost) All Scenarios
arxiv_id: '2311.08154'
source_url: https://arxiv.org/abs/2311.08154
tags:
- answer
- reasoning
- language
- arxiv
- paths
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning performance
  in language models when dealing with unknown question types or answer formats. The
  proposed method, called Self-Agreement, involves generating multiple diverse reasoning
  paths using sampling strategies and then prompting the language model to select
  the most agreed-upon answer among them.
---

# Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios

## Quick Facts
- arXiv ID: 2311.08154
- Source URL: https://arxiv.org/abs/2311.08154
- Reference count: 26
- Key outcome: Self-agreement improves reasoning performance in unknown question types by 5.5-6.2% accuracy

## Executive Summary
This paper introduces Self-Agreement, a method that improves language model reasoning performance when dealing with unknown question types or answer formats. The approach generates multiple diverse reasoning paths using sampling strategies, then prompts the model to select the most agreed-upon answer among them. Experiments on six reasoning benchmarks show consistent improvements over baseline methods, with average accuracy gains of 5.5% on GPT-3.5-turbo and 6.2% on Llama-2-13C-Chat. The method is simple, unsupervised, and generalizes across different reasoning scenarios without requiring task-specific annotations.

## Method Summary
Self-Agreement operates in two phases: first, it samples k diverse reasoning paths using stochastic decoding strategies; second, it prompts the language model one more time to extract final answers from each path and select the most frequently occurring answer through majority voting. This approach leverages the model's ability to act as both a reasoning generator and answer comparator, avoiding the need for external verifiers or task-specific post-processing. The method works in zero-shot or few-shot settings and handles unknown question types by mixing tasks into a single benchmark.

## Key Results
- Average accuracy improvement of 5.5% on GPT-3.5-turbo across six reasoning benchmarks
- Average accuracy improvement of 6.2% on Llama-2-13B-Chat across the same benchmarks
- Performance scales well with more sampled paths, plateauing around 5-10 paths
- Robust to different sampling strategies and model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-agreement leverages the language model's emergent ability to act as a high-quality answer extractor and comparator when given the right prompt.
- Mechanism: The model first samples diverse reasoning paths via stochastic decoding, then is prompted to extract final answers from each path and select the most frequently occurring answer.
- Core assumption: The correct answer is more likely to appear multiple times among diverse reasoning paths if the model actually knows how to solve the problem.
- Evidence anchors: Language models are good answer extractors given specific prompts and are suitable for comparing consistency of final answers extracted from multiple reasoning paths.

### Mechanism 2
- Claim: Self-agreement avoids the need for task-specific annotations or post-processing rules by using the model itself to determine answer agreement.
- Mechanism: Instead of training a verifier or using rule-based majority voting, the model is prompted to analyze and compare final answers from multiple paths and select the majority answer.
- Core assumption: The language model's internal understanding of answer consistency is sufficient for selecting the correct answer without external supervision.
- Evidence anchors: Self-agreement is completely unsupervised, requires no additional human annotations or auxiliary models, and the most agreed answer selected from multiple reasoning paths is likely to be the correct answer.

### Mechanism 3
- Claim: Self-agreement generalizes across unknown question types and answer formats by not relying on task-specific knowledge.
- Mechanism: The method uses zero-shot CoT for unknown formats and mixed few-shot CoT for unknown types, allowing it to handle diverse reasoning scenarios without prior task identification.
- Core assumption: The language model's zero-shot and few-shot reasoning capabilities are sufficient to handle unknown question types and formats.
- Evidence anchors: Self-agreement simultaneously achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Self-agreement builds on CoT by generating multiple reasoning paths rather than a single greedy path
  - Quick check question: What is the difference between zero-shot CoT and few-shot CoT, and when would each be used?

- Concept: Sampling strategies for language models
  - Why needed here: Self-agreement requires generating diverse reasoning paths through sampling rather than greedy decoding
  - Quick check question: How do temperature sampling and top-k sampling affect the diversity of generated outputs?

- Concept: Prompt engineering for answer extraction
  - Why needed here: The ask one more time phase requires carefully crafted prompts to extract and compare answers from multiple paths
  - Quick check question: What are the key components of an effective prompt for extracting final answers from reasoning paths?

## Architecture Onboarding

- Component map: Input → Path generation → Answer extraction → Agreement selection → Output
- Critical path: Input → Path generation → Answer extraction → Agreement selection → Output
- Design tradeoffs:
  - Sampling diversity vs computational cost (more samples = better performance but higher cost)
  - Prompt specificity vs generalization (detailed prompts may work better but be less flexible)
  - Model size vs performance (larger models show better agreement but are more expensive)
- Failure signatures:
  - Low diversity in sampled paths (temperature too low or top-k too restrictive)
  - Poor answer extraction (prompt not clear about what to extract)
  - Incorrect majority selection (model not understanding agreement concept)
- First 3 experiments:
  1. Test different sampling strategies (temperature 0.5 vs 0.7) on a single task to find optimal diversity
  2. Compare zero-shot vs few-shot CoT on mixed tasks to validate generalization
  3. Vary number of sampled paths (5, 10, 20) to find performance plateau point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does self-agreement perform when applied to reasoning tasks with more complex answer formats beyond single values or simple strings?
- Basis in paper: The paper mentions that self-consistency struggles with evaluating majority voted answers in the third scenario, suggesting limitations with complex answer formats.
- Why unresolved: The paper only evaluates self-agreement on tasks with relatively simple answer formats (e.g., numbers, multiple choice, string concatenation). It's unclear how the method would generalize to tasks requiring more nuanced or structured answers.
- What evidence would resolve it: Experiments applying self-agreement to reasoning benchmarks with more complex answer formats, such as multi-step mathematical proofs, open-ended question answering, or tasks requiring symbolic manipulation.

### Open Question 2
- Question: Can self-agreement be extended to handle reasoning tasks that require aggregating information from multiple sources or modalities?
- Basis in paper: The paper focuses on language-only reasoning tasks. There's no discussion of how self-agreement might handle multimodal reasoning or information aggregation.
- Why unresolved: Modern language models are increasingly being used for tasks involving multiple modalities (e.g., text and images). It's unclear whether self-agreement could be adapted to improve reasoning in these more complex scenarios.
- What evidence would resolve it: Experiments applying self-agreement to multimodal reasoning tasks, such as visual question answering or tasks requiring integration of text and tabular data.

### Open Question 3
- Question: How does the performance of self-agreement scale with the number of reasoning paths sampled, and is there an optimal number of paths for different task types or model sizes?
- Basis in paper: The paper shows that performance improves with more sampled paths but begins to plateau around 5-10 paths. However, it doesn't explore whether this optimal number varies across task types or model sizes.
- Why unresolved: The paper uses a fixed number of 20 sampled paths across all experiments. It's unclear whether this is the most efficient choice for all scenarios or if there's a more optimal number that varies depending on the specific task or model being used.
- What evidence would resolve it: A systematic study varying the number of sampled paths across different task types and model sizes to determine if there are task- or model-specific optimal numbers of paths for self-agreement.

## Limitations

- The method requires generating k+1 inference calls, representing a 21× increase in computation for k=20 compared to standard CoT
- Performance may be sensitive to sampling hyperparameters that vary by model (temperature 0.7 for GPT-3.5-turbo vs 0.5 for Llama-2-Chat)
- Claims of "superior in all scenarios" are overstated, as the method hasn't been tested on completely novel reasoning types or tasks requiring external knowledge

## Confidence

**High Confidence**: The core mechanism of using self-agreement for answer selection is well-supported by empirical results. The 5.5% average accuracy gain on GPT-3.5-turbo and 6.2% on Llama-2-13B-Chat are statistically significant and consistent across multiple benchmarks.

**Medium Confidence**: The claim that Self-agreement is "completely unsupervised" is accurate in terms of not requiring task-specific annotations, but the method still requires careful prompt engineering and sampling hyperparameter tuning, which may constitute an implicit form of supervision.

**Low Confidence**: The claim that Self-agreement is "superior in all scenarios" is overstated. The paper shows strong performance across the tested benchmarks but does not demonstrate superiority in all possible reasoning scenarios, particularly those involving complex reasoning chains or requiring external knowledge.

## Next Checks

1. **Sampling Strategy Robustness Test**: Systematically evaluate Self-agreement performance across multiple sampling strategies (temperature, top-p, top-k) and temperature values (0.1 to 1.0) to determine if the method is truly robust to sampling choices or if performance is highly sensitive to specific parameters.

2. **Cross-Domain Generalization**: Test Self-agreement on reasoning tasks from domains not represented in the original six benchmarks (e.g., legal reasoning, medical diagnosis, scientific reasoning) to evaluate whether the method generalizes beyond the arithmetic and commonsense reasoning tasks studied.

3. **Efficiency Analysis**: Conduct a cost-performance analysis comparing Self-agreement with varying numbers of sampled paths (k=5, 10, 20, 50) against baseline methods to identify the point of diminishing returns and provide practical guidance on choosing k based on performance requirements and computational budget.