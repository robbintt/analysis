---
ver: rpa2
title: Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts
arxiv_id: '2309.17415'
source_url: https://arxiv.org/abs/2309.17415
tags:
- answer
- question
- context
- robustness
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies how Large Language Models (LLMs) handle conflicting
  information between external prompts and internal memory. It introduces a benchmarking
  framework to measure factual robustness (ability to discern correct facts) and decision-making
  style (consistent preference toward prompts or memory).
---

# Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts

## Quick Facts
- **arXiv ID:** 2309.17415
- **Source URL:** https://arxiv.org/abs/2309.17415
- **Reference count:** 40
- **Key outcome:** LLMs are more vulnerable to misleading prompts than capable of using correct external knowledge, with better performance on factual versus commonsense knowledge.

## Executive Summary
This paper investigates how Large Language Models handle conflicting information between external prompts and internal memory. The authors introduce a benchmarking framework to measure factual robustness (ability to discern correct facts) and decision-making style (consistent preference toward prompts or memory). Through experiments with seven models across 11,684 samples, they find that LLMs exhibit varying degrees of vulnerability to misleading prompts and different behavioral styles ranging from prompt-dependent to memory-reliant. The work provides insights into optimizing model robustness in real-world applications where conflicting information is common.

## Method Summary
The framework evaluates LLMs through five key steps: memory assessment (checking if models know correct answers without conflicting context), factual robustness evaluation (introducing conflicting golden or negative context), few-shot example influence, decision-making style analysis (categorizing models as intuitive, dependent, or rational), and role play intervention (using role prompts to shift behavior). The study uses 4 existing datasets (MuSiQue, SQuAD v2.0, ECQA, e-CARE) extended with conflicting cases, and tests 7 LLMs including GPT-4, Claude, Bard, Vicuna-13B, ChatGPT, LLaMA, and LLaMA2.

## Key Results
- LLMs show higher vulnerability to misleading prompts than resilience in utilizing correct external knowledge
- Better performance on factual knowledge versus commonsense knowledge in conflicting scenarios
- Role-playing interventions can shift model behavior, though adaptivity varies across models
- Instruction tuning helps models follow prompts but can increase invalid outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit a preference for either their internal memory or external prompts when handling conflicting information, which can be categorized into dependent, intuitive, or rational styles.
- Mechanism: The paper establishes a benchmarking framework to quantify this preference by introducing conflicting scenarios in knowledge-intensive tasks and measuring the model's choice between prompt and memory.
- Core assumption: LLMs can be reliably categorized into these styles based on their behavior in controlled conflicting scenarios.
- Evidence anchors:
  - [abstract]: "categorize LLMs' preference into dependent, intuitive, and rational/irrational styles"
  - [section]: "Drawing from prior research (Harren, 1979; Phillips et al., 1984), we define three types of decision styles: intuitive, dependent, and rational"
  - [corpus]: Weak evidence; no direct citations in corpus about decision style categorization.
- Break condition: If models cannot be reliably categorized due to random behavior or if the framework cannot distinguish between styles.

### Mechanism 2
- Claim: Factual robustness can be measured by assessing a model's ability to discern correct facts from conflicting prompts or memory.
- Mechanism: The framework introduces two aspects of factual robustness: Vulnerable Robustness (VR) and Resilient Robustness (RR), which measure the model's ability to maintain correct knowledge and utilize accurate external knowledge, respectively.
- Core assumption: The model's performance in conflicting scenarios accurately reflects its factual robustness.
- Evidence anchors:
  - [abstract]: "factual robustness (ability to discern correct facts)"
  - [section]: "For Vulnerable Robustness, marked as (D+, C−), and Resilient Robustness, symbolized as (D−, C+)"
  - [corpus]: Weak evidence; no direct citations in corpus about robustness measurement methods.
- Break condition: If the model's performance is not significantly influenced by the conflicting scenarios or if the metrics do not accurately capture robustness.

### Mechanism 3
- Claim: Role-playing interventions can change LLMs' robustness by explicitly guiding their behavior using carefully crafted role prompts.
- Mechanism: The paper introduces "Role Play Intervention" where models are given role prompts to prioritize either internal memory or external prompts, thus changing their decision-making style.
- Core assumption: Models can be influenced by role prompts to change their behavior in a predictable manner.
- Evidence anchors:
  - [abstract]: "role-playing interventions can shift model behavior, though adaptivity varies"
  - [section]: "We designed two distinct role prompts to steer the models into specific decision-making pathways"
  - [corpus]: Weak evidence; no direct citations in corpus about role-playing interventions.
- Break condition: If models do not respond to role prompts or if their behavior becomes unpredictable.

## Foundational Learning

- **Concept: Memory Assessment**
  - Why needed here: To determine if the LLM has memorized the correct knowledge before introducing conflicting information.
  - Quick check question: How does the framework assess whether an LLM has memorized the correct knowledge for a given question?

- **Concept: Decision-Making Style Analysis**
  - Why needed here: To categorize the model's behavior in making consistent choices between memory and prompt.
  - Quick check question: What are the three types of decision-making styles defined in the paper?

- **Concept: Role Play Intervention**
  - Why needed here: To explore the potential for modulating the decision-making tendencies of LLMs by explicitly guiding their behavior.
  - Quick check question: How do the role prompts in the intervention influence the model's decision-making style?

## Architecture Onboarding

- **Component map:** Memory Assessment -> Factual Robustness Evaluation -> Few-shot Example Influence -> Decision-Making Style Analysis -> Role Play Intervention
- **Critical path:** Memory Assessment to Factual Robustness Evaluation, as it directly measures the model's ability to handle conflicting information
- **Design tradeoffs:** The choice between zero-shot and few-shot settings affects the complexity and resource requirements of the evaluation
- **Failure signatures:** If the model consistently fails to choose the correct answer in conflicting scenarios, it indicates a lack of robustness
- **First 3 experiments:**
  1. Memory Assessment: Assess the LLM's memory by asking questions directly without additional context
  2. Factual Robustness Evaluation: Introduce conflicting scenarios with either golden or negative context to measure VR and RR
  3. Role Play Intervention: Apply role prompts to observe changes in the model's decision-making style

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically improve LLMs' resilience to misleading prompts while maintaining their ability to utilize correct external knowledge?
- Basis in paper: [explicit] The paper notes that LLMs are more vulnerable to misleading prompts than capable of using correct external knowledge, with better performance on factual versus commonsense knowledge.
- Why unresolved: The paper identifies this as a key finding but doesn't propose specific architectural or training modifications to address the imbalance.
- What evidence would resolve it: Empirical comparison of different training strategies (e.g., adversarial training with conflicting prompts, knowledge distillation from fact-checking models) showing improved resilience without sacrificing factual knowledge utilization.

### Open Question 2
- Question: What is the optimal balance between instruction-following capabilities and reliance on internal memory for different model scales?
- Basis in paper: [explicit] The paper finds that medium-sized LLMs with instruction tuning tend to be prompt-dependent, while larger models like GPT-4 are more rational in considering both memory and prompt.
- Why unresolved: The paper doesn't explore how to calibrate this balance across different model sizes or whether there's an optimal point that maximizes both robustness and accuracy.
- What evidence would resolve it: Systematic experiments varying model size and instruction tuning intensity, measuring the trade-off between factual robustness and decision-making consistency across multiple tasks.

### Open Question 3
- Question: How can role-playing interventions be optimized to dynamically adjust model behavior based on prompt quality?
- Basis in paper: [explicit] The paper demonstrates that role-playing can shift model behavior but notes varying adaptivity across models, suggesting potential for dynamic adjustment in RAG applications.
- Why unresolved: The paper doesn't provide a framework for determining when to apply different role instructions or how to assess prompt quality in real-time.
- What evidence would resolve it: Development and validation of a meta-model that predicts prompt quality and automatically selects appropriate role instructions, tested in retrieval-augmented generation scenarios.

## Limitations
- The framework's reliability depends on consistent categorization of models into distinct decision-making styles, which lacks direct corpus support
- Role-play intervention mechanisms show promise but have limited validation across diverse scenarios
- Generalization to production environments is limited by the controlled nature of the evaluation framework

## Confidence
- **High Confidence:** Measurement of factual robustness through VR and RR metrics is well-defined and methodologically sound
- **Medium Confidence:** Decision-making style categorization and role-play intervention mechanisms require more extensive validation
- **Low Confidence:** Generalization of findings to production environments due to controlled evaluation settings

## Next Checks
1. Cross-dataset validation: Test the framework on additional knowledge domains beyond the current 4 datasets to assess robustness of style categorization across diverse subject matter
2. Temporal stability assessment: Evaluate whether models maintain consistent decision-making styles across multiple evaluation sessions and whether role-play effects persist beyond immediate prompt influence
3. Real-world scenario testing: Apply the framework to practical use cases where conflicting information naturally occurs (e.g., medical diagnosis with conflicting symptoms) to validate the controlled environment findings