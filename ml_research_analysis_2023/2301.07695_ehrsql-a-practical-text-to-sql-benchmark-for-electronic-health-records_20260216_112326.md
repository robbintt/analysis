---
ver: rpa2
title: 'EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records'
arxiv_id: '2301.07695'
source_url: https://arxiv.org/abs/2301.07695
tags:
- time
- questions
- question
- dataset
- hospital
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EHRSQL, a text-to-SQL dataset for electronic
  health records (EHRs). The dataset is built from a poll conducted at a university
  hospital, collecting questions frequently asked by hospital staff.
---

# EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records

## Quick Facts
- arXiv ID: 2301.07695
- Source URL: https://arxiv.org/abs/2301.07695
- Reference count: 40
- Primary result: Introduces EHRSQL, a text-to-SQL dataset for electronic health records that includes unanswerable questions and time-sensitive queries to improve QA system reliability

## Executive Summary
This paper introduces EHRSQL, a text-to-SQL dataset specifically designed for electronic health records (EHRs). The dataset was built from a poll conducted at a university hospital, collecting questions frequently asked by hospital staff. What makes EHRSQL unique is its inclusion of unanswerable questions and time-sensitive queries, which pose significant challenges for existing QA systems. The dataset covers a wide range of needs from simple retrieval to complex operations like calculating survival rates, and includes manually labeled SQL queries for two open-source EHR databases: MIMIC-III and eICU.

## Method Summary
The EHRSQL dataset was constructed by conducting a poll at a university hospital to collect questions frequently asked on structured EHR data from 222 hospital staff members. The dataset includes 10,821 utterances covering various time expressions and unanswerable questions. SQL queries were manually labeled for two open-source EHR databases: MIMIC-III and eICU. Two baseline models were used for evaluation: T5-base and T5-base with schema serialization. The models were trained using a threshold-based approach to distinguish answerable from unanswerable questions, with thresholds determined by clustering-based or percentile-based methods.

## Key Results
- EHRSQL dataset contains 10,821 utterances covering diverse hospital needs including complex operations like survival rate calculations
- Baseline T5 models achieved 31.0% F1ans and 32.0% F1exe on the validation set
- The dataset poses unique challenges: generating SQL queries covering diverse needs, understanding various time expressions, and distinguishing answerable from unanswerable questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's unique challenge of distinguishing answerable from unanswerable questions improves the reliability of QA systems in real-world deployment.
- Mechanism: By including unanswerable questions collected from actual hospital staff poll responses, the dataset forces models to learn to recognize and refuse to answer questions that are beyond the database schema or require external knowledge.
- Core assumption: Unanswerable questions can be reliably identified and will lead to more trustworthy systems.
- Evidence anchors:
  - [abstract]: "The dataset poses a unique set of challenges: ... 3) distinguish whether a given question is answerable or unanswerable based on the prediction confidence."
  - [section]: "Unanswerable questions in QA ... we include unanswerable questions in the dataset, utilizing the remaining utterances from the poll result."
  - [corpus]: Weak evidence; no direct mention of unanswerable questions in neighbor papers.
- Break condition: If unanswerable questions cannot be reliably identified, the system's reliability will not improve.

### Mechanism 2
- Claim: The inclusion of time-sensitive questions with various time expressions makes the dataset more reflective of real-world healthcare queries.
- Mechanism: By categorizing time into multiple expression types, units, and interval types, and combining them with question templates, the dataset simulates the time-sensitive nature of healthcare queries.
- Core assumption: Time expressions in healthcare queries are diverse and crucial for accurate information retrieval.
- Evidence anchors:
  - [abstract]: "Our dataset poses a unique set of challenges: ... 2) understand various time expressions to answer time-sensitive questions in healthcare..."
  - [section]: "Time-sensitive questions: ... we developed three time filter types and assigned different time factors... to compose a single time template."
  - [corpus]: No direct evidence in neighbor papers about time-sensitive questions.
- Break condition: If time expressions are not accurately represented or understood, the system's performance will degrade.

### Mechanism 3
- Claim: The dataset's coverage of a wide range of questions, from simple retrieval to complex operations like calculating survival rates, makes it a practical benchmark for QA models.
- Mechanism: By collecting questions from 222 hospital staff members with varying experience levels and covering diverse needs, the dataset ensures a comprehensive evaluation of QA models.
- Core assumption: A wide range of questions is necessary to assess the full capabilities of QA models.
- Evidence anchors:
  - [abstract]: "The utterances were collected from 222 hospital staff members... The questions cover a wide range of needs, including simple retrieval and complex operations like calculating survival rates."
  - [section]: "Wide range of questions: We conducted a poll at a university hospital to collect questions frequently asked on structured EHR data."
  - [corpus]: Weak evidence; neighbor papers do not provide detailed information on question diversity.
- Break condition: If the question range is too narrow, the dataset will not effectively evaluate QA models.

## Foundational Learning

- Concept: Text-to-SQL generation
  - Why needed here: To translate natural language questions into SQL queries that can be executed on EHR databases.
  - Quick check question: What are the key challenges in generating SQL queries from natural language questions?
- Concept: Semantic parsing
  - Why needed here: To convert natural language utterances into logical form representations, such as SQL queries.
  - Quick check question: How does semantic parsing differ from other NLP tasks like text classification?
- Concept: Out-of-Domain (OOD) detection
  - Why needed here: To identify and handle unanswerable questions that are beyond the scope of the database schema or require external knowledge.
  - Quick check question: What are the common approaches to detect and handle OOD questions in QA systems?

## Architecture Onboarding

- Component map: Question template generation module -> Time template sampling module -> Condition value sampling module -> SQL annotation module
- Critical path: The critical path is the generation of SQL queries from natural language questions, which involves the question template, time template, and condition value sampling modules.
- Design tradeoffs: The choice between using JOIN operations or nested queries for SQL generation, and the balance between question diversity and dataset size.
- Failure signatures: Inability to generate correct SQL queries, failure to distinguish answerable from unanswerable questions, and poor performance on time-sensitive queries.
- First 3 experiments:
  1. Evaluate the performance of the baseline T5 model on the EHRSQL dataset.
  2. Test the impact of including time-sensitive questions on model performance.
  3. Assess the effectiveness of the unanswerable question detection mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop uncertainty-aware semantic parsing models that can effectively distinguish between answerable and unanswerable questions in EHRSQL without relying on explicit training with unanswerable examples?
- Basis in paper: [explicit] The paper introduces the challenge of distinguishing answerable from unanswerable questions and proposes using maximum entropy during decoding as a confidence score, but notes that the current models' entropy values are not linearly separable across predictions.
- Why unresolved: Current methods like entropy-based uncertainty estimation show promise but require further refinement to reliably detect unanswerable questions in real-world scenarios.
- What evidence would resolve it: Development and evaluation of a new semantic parsing model that significantly outperforms baseline T5 models in F1ans and F1exe metrics while demonstrating reliable unanswerable question detection through ablation studies and comparison with human performance.

### Open Question 2
- Question: What is the impact of domain adaptation techniques on improving zero-shot cross-domain transfer performance for text-to-SQL models when applied to healthcare datasets like EHRSQL?
- Basis in paper: [explicit] The paper shows that state-of-the-art cross-domain semantic parsing models like GAP perform poorly on EHRSQL compared to MIMICSQL, achieving only 5.8% execution accuracy on a subset of answerable questions.
- Why unresolved: The significant performance gap between general domain and healthcare datasets suggests that current domain adaptation methods are insufficient for handling the unique challenges of healthcare text-to-SQL tasks.
- What evidence would resolve it: Development and evaluation of a domain adaptation technique that significantly improves zero-shot transfer performance on EHRSQL, demonstrating at least 20% absolute improvement in execution accuracy compared to current baselines.

### Open Question 3
- Question: How can we expand the scope and diversity of EHRSQL to better represent the global healthcare landscape and improve its applicability across different healthcare systems?
- Basis in paper: [inferred] The paper acknowledges limitations including the dataset being sourced from one Korean university hospital and using general-domain paraphrasers, suggesting potential for expansion and refinement.
- Why unresolved: The current dataset, while extensive, may not fully capture the diversity of healthcare systems, practices, and language use across different countries and cultures.
- What evidence would resolve it: Creation of an expanded version of EHRSQL incorporating data from multiple hospitals across different countries, showing improved performance of text-to-SQL models when trained on the expanded dataset and tested on diverse healthcare settings.

## Limitations
- The dataset is sourced from one Korean university hospital, potentially limiting its generalizability to other healthcare systems
- Baseline model comparisons use T5-base without extensive hyperparameter tuning or comparison to more recent, larger models
- The quality and representativeness of unanswerable questions are not thoroughly validated against real-world scenarios

## Confidence

- **High Confidence**: The dataset construction methodology and the core concept of including unanswerable questions and time-sensitive queries are well-established. The basic statistics about question distribution and the two database schemas are clearly presented.
- **Medium Confidence**: The claim that this dataset provides a practical benchmark for real-world healthcare QA systems. While the methodology is sound, the actual impact on system reliability in production environments would require additional validation.
- **Low Confidence**: The specific claim about the unique set of challenges posed by the dataset, particularly regarding the balance between question diversity and dataset size, as this isn't thoroughly validated against other existing benchmarks.

## Next Checks

1. **External Validation**: Test the EHRSQL benchmark with models from different institutions and compare performance consistency across MIMIC-III and eICU schemas to validate the dataset's generalizability.

2. **Real-World Deployment Study**: Deploy a model trained on EHRSQL in a hospital setting and measure its actual impact on clinical decision-making workflows, particularly focusing on its ability to correctly identify unanswerable questions.

3. **Cross-Domain Transferability**: Evaluate whether models trained on EHRSQL can effectively transfer to other domains with different schemas (e.g., financial or educational databases) while maintaining their ability to handle time-sensitive queries and unanswerable questions.