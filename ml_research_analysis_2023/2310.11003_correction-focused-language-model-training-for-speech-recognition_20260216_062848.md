---
ver: rpa2
title: Correction Focused Language Model Training for Speech Recognition
arxiv_id: '2310.11003'
source_url: https://arxiv.org/abs/2310.11003
tags:
- training
- fallibility
- domain
- text
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving automatic speech
  recognition (ASR) performance in domain adaptation tasks by developing a correction-focused
  language model (LM) training approach. The key idea is to prioritize ASR fallible
  words during LM training by incorporating word-level ASR fallibility scores into
  the training process.
---

# Correction Focused Language Model Training for Speech Recognition

## Quick Facts
- arXiv ID: 2310.11003
- Source URL: https://arxiv.org/abs/2310.11003
- Reference count: 0
- This paper introduces correction-focused language model training that prioritizes ASR-prone words, achieving up to 5.5% relative WER reduction in sufficient text scenarios and up to 6% additional reduction in insufficient text scenarios.

## Executive Summary
This paper addresses the challenge of domain adaptation in automatic speech recognition (ASR) by developing a correction-focused language model (LM) training approach. The key innovation is incorporating word-level ASR fallibility scores into the LM training process, shifting probability mass toward words that the ASR system is more likely to mis-recognize. To enable this approach with text-only corpora, the authors leverage large language models (LLMs) as fallibility score predictors and text generators through multi-task fine-tuning. The method demonstrates significant WER improvements across various domains and text availability scenarios, particularly excelling when target domain text is scarce.

## Method Summary
The approach defines word-level ASR fallibility scores and uses them as weights in the LM training objective, prioritizing training on ASR-prone tokens rather than uniformly minimizing perplexity. Since real-time ASR decoding is expensive, the method employs LLMs fine-tuned on paired utterance-reference data to predict fallibility scores for any text input. For scenarios with insufficient target domain text, a multi-task LLM fine-tuning strategy enables simultaneous text generation and fallibility score prediction. The trained neural network LM (NNLM) is then integrated with ASR through shallow fusion, yielding improved recognition accuracy across domain adaptation tasks.

## Key Results
- Correction-focused training achieves up to 5.5% relative WER reduction in sufficient text scenarios
- In insufficient text scenarios, the method provides up to 6% additional WER reduction compared to conventional training
- The approach is particularly effective when target domain text is scarce, leveraging LLM-generated synthetic data with predicted fallibility scores

## Why This Works (Mechanism)

### Mechanism 1
Incorporating ASR fallibility scores during LM training shifts probability mass toward words that the ASR system is more likely to mis-recognize, thereby reducing overall WER. The method defines a word-level fallibility score S_x = 1 - P_M(x|x_<t) representing the likelihood of ASR error. These scores are used as weights in the KL divergence loss: min_θ Σ(z,s) -1/|z| Σ_j α s_j log P_θ(z_j|z_<j). This prioritizes training on ASR-prone tokens rather than uniformly minimizing perplexity.

### Mechanism 2
LLM fine-tuning on paired utterance-reference data enables accurate prediction of word-level ASR fallibility scores without requiring real-time ASR decoding. The LLM is fine-tuned as a token classifier on annotated ASR errors from a small target-domain corpus. The fine-tuned model then predicts fallibility scores for any text input, enabling correction-focused training on text-only corpora.

### Mechanism 3
Multi-task LLM fine-tuning enables generation of high-quality target-domain text with predicted fallibility scores, solving the insufficient text scenario. The LLM is fine-tuned with both LM generation head and fallibility classification head. It simultaneously generates text and predicts fallibility scores for each token, producing synthetic data suitable for correction-focused NNLM training.

## Foundational Learning

- **Automatic Speech Recognition (ASR) error patterns**: ASR systems produce hypotheses with word error rates (WER) that vary by domain and word context. Why needed: The entire correction-focused approach depends on identifying which words the ASR is likely to mis-recognize; without understanding ASR error patterns, the method cannot prioritize effectively. Quick check: How does ASR error distribution typically differ between conversational speech and read speech domains?

- **Language Model training objectives**: Conventional LM training minimizes perplexity across all words, but this paper argues that equal treatment of all words is suboptimal for ASR performance. Why needed: Understanding this distinction is key to grasping the correction focus. Quick check: Why might minimizing perplexity not always lead to lower WER in ASR systems?

- **Kullback-Leibler (KL) divergence**: The correction-focused training objective uses KL(Q||P_θ) where Q is the prior fallibility-weighted distribution. Why needed: Understanding KL divergence is essential to follow the training objective formulation. Quick check: How does minimizing KL(Q||P) differ from maximizing log-likelihood of the data?

## Architecture Onboarding

- **Component map**: ASR system → utterance-reference annotation → LLM fine-tuning → fallibility score prediction/generation → correction-focused NNLM training → shallow fusion integration → WER evaluation
- **Critical path**: The pipeline flows from ASR decoding to fallibility annotation, through LLM fine-tuning for prediction/generation, to NNLM training with correction-focused objective, and finally integration with ASR via shallow fusion.
- **Design tradeoffs**: Real-time ASR decoding for fallibility scores vs. LLM prediction (accuracy vs. cost); single-task vs. multi-task LLM fine-tuning (simplicity vs. synthetic data generation); α hyperparameter tuning (focus vs. overall fluency).
- **Failure signatures**: WER increases or plateaus despite correction-focused training (poorly calibrated fallibility scores or α too high); generated text is incoherent or domain-mismatched (degraded multi-task fine-tuning); NNLM perplexity increases significantly (overly aggressive weighting or domain mismatch).
- **First 3 experiments**: 1) Fine-tune LLM on small utterance-reference corpus; evaluate fallibility score prediction accuracy. 2) Train NNLM using correction-focused objective with varying α; measure WER improvement. 3) Generate synthetic text with fallibility scores using multi-task LLM; train NNLM and compare WER to model trained on real text-only data.

## Open Questions the Paper Calls Out

The paper mentions exploring leveraging confusion word pairs and other ASR decoding information beyond binary error annotations for more sophisticated fallibility signals, though it does not provide details on how this would be implemented or its potential impact on performance.

## Limitations

- The method's performance in truly low-resource scenarios with minimal target domain data remains unclear
- The robustness when domain mismatch exists between the utterance-reference corpus used for LLM fine-tuning and the target domain text corpus is not thoroughly explored
- The quality of generated text and fallibility score accuracy from multi-task fine-tuning are not independently verified, only inferred from downstream WER gains

## Confidence

- **High confidence**: The core mechanism of incorporating word-level fallibility scores into LM training objective is well-grounded and demonstrates measurable WER improvements
- **Medium confidence**: The LLM's ability to accurately predict fallibility scores from text-only data is supported by downstream WER gains but lacks direct validation of prediction accuracy or calibration
- **Medium confidence**: The multi-task fine-tuning approach for synthetic data generation shows effectiveness through WER improvements, but the quality of generated text and fallibility score accuracy are not independently verified

## Next Checks

1. Conduct ablation studies comparing WER when using LLM-predicted fallibility scores versus real-time ASR decoding-based scores to quantify the accuracy trade-off and identify calibration issues
2. Evaluate the quality of synthetic text generated through multi-task fine-tuning using standard language model perplexity and human evaluation metrics, alongside direct assessment of predicted fallibility score accuracy
3. Test the method's robustness by intentionally introducing domain mismatch between the LLM fine-tuning corpus and target domain text, measuring degradation in both fallibility prediction accuracy and WER improvement