---
ver: rpa2
title: Exploration with Principles for Diverse AI Supervision
arxiv_id: '2310.08899'
source_url: https://arxiv.org/abs/2310.08899
tags:
- data
- arxiv
- human
- question
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Exploratory AI (EAI), a method to generate
  diverse training data for language models by autonomously exploring the natural
  language space. EAI uses a large language model as an "actor" to generate novel
  content and another large language model as a "critic" to evaluate the novelty of
  the generated content and provide critiques to guide the actor's exploration.
---

# Exploration with Principles for Diverse AI Supervision

## Quick Facts
- arXiv ID: 2310.08899
- Source URL: https://arxiv.org/abs/2310.08899
- Reference count: 40
- Key outcome: EAI significantly improves mathematical reasoning performance and generates more diverse training data compared to supervised finetuning and rejection sampling baselines.

## Executive Summary
This paper proposes Exploratory AI (EAI), a method to generate diverse training data for language models by autonomously exploring the natural language space. EAI uses a large language model as an "actor" to generate novel content and another large language model as a "critic" to evaluate the novelty of the generated content and provide critiques to guide the actor's exploration. The authors evaluate EAI on mathematical reasoning tasks and show that it significantly improves model performance compared to supervised finetuning and rejection sampling baselines, while also generating more diverse data. EAI demonstrates the potential for AI models to generate high-quality training data without extensive human supervision.

## Method Summary
EAI employs an actor-critic framework where a large language model generates novel content conditioned on replay buffer samples and exploration principles, while another LLM evaluates novelty and provides critiques. The method uses four principles (rephrase, new topic, restructure, new scenario) to systematically explore question variations. The replay buffer can be initialized with human data (GSM8K) or left empty for zero-shot exploration. The actor-critic loop iteratively refines generations until acceptance, balancing novelty and correctness without explicit RL training.

## Key Results
- EAI significantly outperforms supervised finetuning and rejection sampling baselines on GSM8K and MATH benchmarks
- Generated data shows higher submodularity diversity gain compared to baseline methods
- EAI demonstrates the ability to generate high-quality training data without extensive human supervision

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The actor-critic loop generates novel training data by iterative refinement guided by novelty critiques
- **Mechanism:** The actor generates content conditioned on replay buffer samples and an exploration principle, the critic evaluates novelty, and critiques guide the actor to refine until accepted. This loop avoids human input while maintaining diversity
- **Core assumption:** Large language models can serve as both generative agents (actor) and evaluators (critic) without explicit RL training
- **Evidence anchors:**
  - [abstract] "Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the generated content, offering critiques to guide the actor"
  - [section] "We accomplish this by harnessing large language models to assess the novelty of generated content"
  - [corpus] Weak: corpus contains related Embodied AI papers but not direct parallels to this actor-critic LLM-based exploration method
- **Break condition:** If the critic cannot meaningfully differentiate novelty or if actor generations collapse into repetitive patterns, the loop stalls

### Mechanism 2
- **Claim:** Pre-initializing the replay buffer with human data accelerates exploration and yields more long-term diverse behavior
- **Mechanism:** By seeding with GSM8K examples, the actor has reference points to diverge from, avoiding the cold-start problem and guiding initial generations toward mathematically valid content
- **Core assumption:** Existing human-curated datasets provide useful semantic anchors that prevent degenerate generations
- **Evidence anchors:**
  - [section] "Similar to APT, we found having pre-existing samples accelerates learning and encourages the actor to have more long term exploratory behaviors"
  - [section] "The replay buffer can be initialized with a pre-existing human-created dataset (e.g., GSM8K training set) or can remain empty for zero-shot exploration"
  - [corpus] Weak: no corpus evidence directly supporting replay buffer seeding effect
- **Break condition:** If the replay buffer biases actor too strongly toward existing patterns, diversity gains diminish

### Mechanism 3
- **Claim:** Guided exploration via structured principles (rephrase, new topic, restructure, new scenario) ensures systematic diversity across data dimensions
- **Mechanism:** Each principle directs actor to transform questions in a specific way; critic evaluates novelty within that principle. Uniform sampling of principles balances coverage
- **Core assumption:** A small set of transformation rules can systematically explore the combinatorial space of question variations
- **Evidence anchors:**
  - [section] "We equip both the actor and critic with a curated set of guiding principles to facilitate the generation and evaluation of diverse questions"
  - [section] "We uniformly sample one principle at a time and input it to both the actor and critic"
  - [corpus] Weak: corpus discusses multimodal grounding and navigation, not principle-based content generation
- **Break condition:** If principles are too narrow, exploration space collapses; if too broad, novelty evaluation becomes unreliable

## Foundational Learning

- **Concept:** Unsupervised RL pretraining (e.g., APT) and particle-based entropy estimation for novelty
  - **Why needed here:** The method is explicitly inspired by APT's state-novelty exploration, but adapted to natural language without explicit RL
  - **Quick check question:** How does APT measure novelty in continuous state spaces, and why is that approach difficult to apply directly to language models?

- **Concept:** Rejection sampling and supervised finetuning baselines for LLM math reasoning
  - **Why needed here:** Understanding prior work establishes why EAI's diversity gain matters; RFT saturates while EAI continues improving
  - **Quick check question:** Why does rejection sampling tend to generate less diverse data compared to iterative actor-critic refinement?

- **Concept:** Submodularity and diversity metrics for dataset evaluation
  - **Why needed here:** The paper uses submodularity-based diversity gain to quantify EAI's advantage over RFT
  - **Quick check question:** What property of submodular functions makes them suitable for measuring diversity gain in generated datasets?

## Architecture Onboarding

- **Component map:** Replay buffer -> Principle sampler -> Actor LLM -> Critic LLM -> Storage loop
- **Critical path:** Sample → Generate → Evaluate → Accept/Refine → Store → Train
- **Design tradeoffs:**
  - 2-round max interaction balances quality and compute; deeper refinement could improve novelty but increases cost
  - Uniform principle sampling ensures coverage but may underweight high-yield principles (e.g., restructure)
  - Seeded vs. zero-shot replay buffer trades exploration breadth for faster convergence
- **Failure signatures:**
  - Actor produces repetitive generations → critic critiques focus on novelty but fails to improve → stagnation
  - Critic incorrectly accepts invalid math → downstream model learns wrong patterns → accuracy drops
  - Replay buffer becomes saturated with similar examples → diversity metric plateaus early
- **First 3 experiments:**
  1. Run actor-critic loop on empty replay buffer (zero-shot) and compare diversity gain vs. seeded buffer
  2. Vary number of replay buffer samples per batch (0→8) and measure accuracy/diversity trade-off
  3. Disable each exploration principle in turn and measure impact on GSM8K pass@1 to identify most critical transformations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of data generated by EAI compare to data generated by human experts in terms of submodularity diversity gain?
- Basis in paper: [explicit] The paper compares EAI's diversity gain to RFT's using submodularity diversity gain metric.
- Why unresolved: While the paper shows EAI outperforms RFT, it does not provide a direct comparison with human-generated data.
- What evidence would resolve it: Conduct an experiment comparing the submodularity diversity gain of EAI-generated data to human-generated data on the same task.

### Open Question 2
- Question: Can EAI be effectively extended to evaluate novelty across the entire data buffer, rather than just comparing to sampled inputs?
- Basis in paper: [inferred] The paper mentions this as a potential future direction but does not explore it.
- Why unresolved: The paper only demonstrates EAI's effectiveness when comparing to sampled inputs, not the entire buffer.
- What evidence would resolve it: Implement and test an EAI variant that evaluates novelty against the entire buffer, measuring its impact on diversity and performance.

### Open Question 3
- Question: How does the performance of EAI scale with different model sizes and architectures beyond the LLaMA-based models tested?
- Basis in paper: [explicit] The paper tests EAI with Vicuna (based on LLaMA) but does not explore other model architectures.
- Why unresolved: The experiments are limited to Vicuna models, leaving open the question of EAI's effectiveness with other architectures.
- What evidence would resolve it: Conduct experiments applying EAI to models with different architectures (e.g., GPT, PaLM) and sizes to measure performance scaling.

## Limitations

- The method relies on LLM-based critic evaluations without human verification of novelty or correctness
- Lack of ablation studies isolating the contribution of each exploration principle
- Limited scope to mathematical reasoning tasks without validation on other domains

## Confidence

- **High**: Claims about the actor-critic loop structure and its ability to generate diverse data through iterative refinement
- **Medium**: Claims about diversity improvements and accuracy gains relative to baselines, given the empirical results but limited ablations
- **Low**: Claims about scalability to non-mathematical domains and long-term exploration without human supervision

## Next Checks

1. Conduct ablation studies to isolate the impact of each exploration principle on both diversity metrics and accuracy gains
2. Test EAI's effectiveness on non-mathematical reasoning tasks to assess domain generalization
3. Implement human evaluation of critic judgments to verify the reliability of novelty and correctness assessments