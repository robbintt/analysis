---
ver: rpa2
title: Conditional expectation network for SHAP
arxiv_id: '2307.10654'
source_url: https://arxiv.org/abs/2307.10654
tags:
- conditional
- shap
- network
- feature
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently computing conditional
  expectations of regression models for subsets of feature components, which is crucial
  for model explainability tools like SHAP. The authors propose a conditional expectation
  network that approximates these expectations simultaneously for all possible feature
  subsets.
---

# Conditional expectation network for SHAP

## Quick Facts
- arXiv ID: 2307.10654
- Source URL: https://arxiv.org/abs/2307.10654
- Reference count: 38
- Primary result: A neural network method for efficiently computing conditional SHAP values by simultaneously approximating all possible conditional expectations through random feature masking

## Executive Summary
This paper addresses the challenge of efficiently computing conditional expectations of regression models for subsets of feature components, which is crucial for model explainability tools like SHAP. The authors propose a conditional expectation network that approximates these expectations simultaneously for all possible feature subsets. The network is trained using a specific fitting procedure involving random masking of input features.

The method is applied to a French motor third-party liability claims frequency dataset, demonstrating its effectiveness in providing variable importance analyses similar to ANOVA and drop1 methods for generalized linear models, as well as marginal conditional expectation plots that correctly account for feature dependencies. The conditional SHAP approach is shown to outperform unconditional methods in handling colinear features and extrapolating to undefined regions of the feature space.

## Method Summary
The proposed method uses a neural network to approximate conditional expectations E[μ(X)|X_C=x_C] for all subsets C of features. The network is trained with randomly masked inputs where each feature has a 50% chance of being replaced with a mask value m. The training procedure includes three types of instances: full model instances (x, μ(x)), null model instances (mask value m, μ0), and masked instances. This approach enables efficient calculation of conditional SHAP values while properly accounting for feature dependencies.

## Key Results
- The conditional expectation network successfully approximates conditional expectations for all feature subsets simultaneously
- The method provides fair variable importance decompositions through SHAP values that respect feature dependencies
- Conditional SHAP values outperform unconditional methods in handling colinear features and extrapolating to undefined regions of the feature space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional expectation network simultaneously approximates all possible conditional expectations by masking features and using a neural network to learn the mapping.
- Mechanism: The network architecture takes the full feature vector as input but masks components based on a random subset C. By training with randomly masked inputs and target conditional expectations, the network learns to approximate E[μ(X)|X_C=x_C] for all subsets C.
- Core assumption: A single neural network can approximate all conditional expectation functions simultaneously when trained with appropriate masking and targets.
- Evidence anchors:
  - [abstract] "propose a conditional expectation network that approximates these expectations simultaneously for all possible feature subsets"
  - [section 2] "The idea now is to approximate the functions xC 7→ µC(x) simultaneously for all subsets C ⊆ Q by a neural network"
- Break condition: If the neural network architecture is too small to capture the complexity of all conditional expectation functions, or if the masking strategy doesn't properly represent all subsets.

### Mechanism 2
- Claim: The network can properly handle extreme cases (null model and full model) through specific calibration instances in training.
- Mechanism: The training procedure includes three types of instances: full model instances (x, μ(x)), null model instances (mask value m, μ0), and masked instances. This ensures the network can reproduce both extreme cases.
- Core assumption: Including specific calibration instances in training allows the network to learn the correct behavior at the boundaries.
- Evidence anchors:
  - [section 2] "We use the above cases (a) and (b) with indices 1 ≤ l ≤ 2n to ensure that the extreme cases (2.2), the null model µ0 and the full model µ(x), can be approximated by the estimated neural network NNbϑ"
  - [section 2] "We set x[3]l = m and eµ(x[3]l) = µ0. These instances are used to ensure that we can replicate the null model"
- Break condition: If the mask value m is not chosen appropriately to satisfy μ(m) = μ0, or if the network architecture cannot represent the extreme cases.

### Mechanism 3
- Claim: The method provides fair variable importance decomposition through SHAP values that properly account for feature dependencies.
- Mechanism: By computing conditional expectations that respect the true dependence structure between features, the SHAP values reflect the actual contribution of each feature to model predictions, unlike unconditional methods.
- Core assumption: Conditional SHAP values that respect feature dependencies provide more accurate variable importance than methods that assume independence.
- Evidence anchors:
  - [abstract] "The conditional SHAP approach is shown to outperform unconditional methods in handling colinear features"
  - [section 3.3] "The unconditional expectation (3.4) sets the j-th component of the feature equal to xj, and averages over the remaining feature components X Q\{j} without considering the correct dependence structure"
- Break condition: If the feature dependencies are so complex that even the conditional approach cannot capture them accurately, or if computational constraints limit the quality of the approximation.

## Foundational Learning

- Concept: Conditional expectation in regression models
  - Why needed here: The entire method relies on computing E[μ(X)|X_C=x_C] for various subsets C, which is the core mathematical operation being approximated.
  - Quick check question: What is the difference between conditional expectation E[Y|X=x] and unconditional expectation E[Y]?

- Concept: SHAP (SHapley Additive exPlanation) values and their axioms
  - Why needed here: The paper uses SHAP as the primary application for variable importance, and the conditional version requires understanding how SHAP works with conditional expectations.
  - Quick check question: What are the four axioms that define fair Shapley value distributions?

- Concept: Neural network universal approximation property
  - Why needed here: The method relies on neural networks to approximate multiple conditional expectation functions simultaneously.
  - Quick check question: What does the universal approximation theorem state about neural networks' ability to approximate functions?

## Architecture Onboarding

- Component map: Data preprocessing -> Network architecture definition -> Mask value selection (m) -> Training with full/masked/null instances -> Validation of conditional expectation approximation -> SHAP value calculation
- Critical path: Data preprocessing → Network architecture definition → Mask value selection (m) → Training with full/masked/null instances → Validation of conditional expectation approximation → SHAP value calculation
- Design tradeoffs: The method trades computational complexity (training one network vs. 2^q networks) for approximation accuracy. The choice of mask value m affects both the null model calibration and the network's ability to learn conditional expectations.
- Failure signatures: Poor approximation of conditional expectations (high MSE on validation), inability to reproduce null model (NN_ϑ(m) ≠ μ0), or SHAP values that don't align with domain knowledge. Also watch for overfitting if the network is too complex relative to the data.
- First 3 experiments:
  1. Verify mask value selection: Check that the chosen mask m satisfies μ(m) ≈ μ0 and is representative of the feature distribution.
  2. Test conditional expectation approximation: Compare NN_ϑ(x_C^m) against direct Monte Carlo estimation of E[μ(X)|X_C=x_C] for a few subsets C.
  3. Validate SHAP decomposition: Check that the SHAP values sum to the difference between full model prediction and null model prediction for test instances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the conditional expectation network compare to other surrogate models like Gaussian approximations or tree-based methods in terms of computational efficiency and accuracy for calculating conditional SHAP values?
- Basis in paper: [explicit] The paper mentions that the conditional expectation network is proposed as an efficient method for calculating conditional SHAP values, but does not directly compare its performance to other methods like Gaussian approximations mentioned in Aas et al. [1] or tree-based methods.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the conditional expectation network for the French motor third-party liability claims frequency dataset but does not provide a comprehensive comparison with alternative methods for calculating conditional SHAP values.
- What evidence would resolve it: A comparative study evaluating the computational efficiency and accuracy of the conditional expectation network against Gaussian approximations and tree-based methods for calculating conditional SHAP values on various datasets.

### Open Question 2
- Question: How sensitive are the results of the conditional SHAP analysis to the choice of the mask value in the conditional expectation network?
- Basis in paper: [explicit] The paper discusses the choice of the mask value in the conditional expectation network, stating that it should be chosen such that the full model has the same prediction in the mask value as the null model. However, it does not explore how different choices of the mask value might affect the results of the conditional SHAP analysis.
- Why unresolved: While the paper provides a method for choosing the mask value, it does not investigate the sensitivity of the conditional SHAP results to this choice or provide guidelines for selecting the optimal mask value in different scenarios.
- What evidence would resolve it: A sensitivity analysis exploring the impact of different mask value choices on the results of the conditional SHAP analysis for various datasets and regression models.

### Open Question 3
- Question: Can the conditional expectation network be extended to handle higher-order interactions and provide more granular explanations of the regression model's behavior?
- Basis in paper: [inferred] The paper mentions that there is interesting work extending Shapley values to higher-order decompositions and representations, which can partly mitigate the difficulty of deciding whether to work with conditional or unconditional expectations. This suggests that the conditional expectation network could potentially be extended to handle higher-order interactions.
- Why unresolved: The paper focuses on the basic application of the conditional expectation network for calculating conditional SHAP values but does not explore its potential for handling higher-order interactions or providing more detailed explanations of the regression model's behavior.
- What evidence would resolve it: An extension of the conditional expectation network to incorporate higher-order interactions and a demonstration of its ability to provide more granular explanations of the regression model's behavior on complex datasets.

## Limitations
- The neural network may not perfectly capture complex conditional expectation functions, particularly for high-dimensional data or highly nonlinear relationships
- Finding an appropriate mask value m that satisfies μ(m) = μ0 may be challenging in practice
- The method inherits limitations of SHAP values, including potential inconsistencies with respect to model modifications

## Confidence
- **High Confidence**: The theoretical framework for conditional expectations and SHAP values is well-established, and the proposed training procedure with random masking is methodologically sound.
- **Medium Confidence**: The empirical validation on the motor insurance dataset demonstrates practical utility, but the results are limited to a single application domain.
- **Low Confidence**: The claims about extrapolation capabilities to undefined regions of the feature space are not thoroughly validated.

## Next Checks
1. **Mask Value Sensitivity Analysis**: Systematically test different mask values m and analyze their impact on the null model calibration and overall approximation quality.
2. **Cross-Dataset Generalization**: Apply the conditional expectation network approach to datasets from different domains to evaluate its generalization capabilities.
3. **Comparison with Alternative Approximation Methods**: Compare the proposed neural network approach with other approximation techniques for conditional expectations, such as Gaussian process regression or random forest-based methods.