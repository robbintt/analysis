---
ver: rpa2
title: 'ADGym: Design Choices for Deep Anomaly Detection'
arxiv_id: '2309.15376'
source_url: https://arxiv.org/abs/2309.15376
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000026
- uni00000035
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ADGym is a benchmarking platform for deep anomaly detection (AD)
  that evaluates and automates the selection of design choices in AD models. It addresses
  the lack of understanding of how individual components like loss functions and architectures
  contribute to AD performance.
---

# ADGym: Design Choices for Deep Anomaly Detection

## Quick Facts
- arXiv ID: 2309.15376
- Source URL: https://arxiv.org/abs/2309.15376
- Reference count: 40
- One-line primary result: ADGym automates design choice selection for deep anomaly detection, achieving significant performance improvements over state-of-the-art methods

## Executive Summary
ADGym is a benchmarking platform for deep anomaly detection that systematically evaluates and automates the selection of design choices in AD models. By decoupling AD pipelines into data handling, network construction, and network training components, ADGym enables meta-learning to transfer knowledge from historical datasets to select optimal design choices for new datasets. The platform demonstrates that no single design choice excels across all datasets, justifying the need for automated selection through meta-learning.

## Method Summary
ADGym evaluates various design choices within data handling (augmentation, preprocessing), network construction (architecture, layers, activation, dropout, initialization), and network training (loss functions, optimizers, epochs, batch size, learning rate, weight decay). The platform uses meta-learning to automatically select optimal design choices for a given dataset by extracting meta-features and training a meta-predictor on historical performance data. ADGym achieves significant improvements in AUCROC and AUCPR metrics compared to existing state-of-the-art AD methods.

## Key Results
- ADGym achieves significant improvements in AUCROC and AUCPR metrics compared to existing state-of-the-art AD methods
- Models constructed using ADGym surpass current state-of-the-art techniques by a considerable margin
- Tree-based ensemble models (XGBoost/CatBoost) outperform neural network meta-predictors for tabular AD tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADGym improves anomaly detection performance by systematically evaluating and automating design choice selection at the component level rather than treating deep AD models as monolithic entities.
- Mechanism: By decomposing AD pipelines into data handling, network construction, and network training components, ADGym enables meta-learning to transfer knowledge from historical datasets to select optimal design choices for new datasets. This component-level analysis reveals that individual design choices (e.g., loss functions, network architectures) have varying impacts across different anomaly types and datasets.
- Core assumption: Design choices can be meaningfully evaluated independently and their interactions can be captured through meta-learning from historical performance data.
- Evidence anchors:
  - [abstract] "ADGym decouples AD pipelines into data handling, network construction, and network training, and evaluates various design choices within each"
  - [section] "Our extensive experiments reveal that merely adopting existing leading methods is not ideal. Models crafted using ADGym markedly surpass current state-of-the-art techniques"
  - [corpus] Weak evidence - corpus contains related works on AD benchmarks but no direct evidence of component-level meta-learning for AD design choices
- Break condition: If design choices interact in non-additive ways that cannot be captured through simple meta-learning from historical performance data, or if the historical dataset distribution does not represent new datasets.

### Mechanism 2
- Claim: ADGym's meta-predictor achieves superior performance by leveraging tree-based ensemble models (XGBoost/CatBoost) over neural network meta-predictors for tabular AD tasks.
- Mechanism: Tree-based ensemble models are more effective at capturing the complex relationships between dataset characteristics and optimal design choices for tabular data. The meta-predictor uses both statistical features and landmarker features extracted from unsupervised AD algorithms to characterize datasets, then predicts optimal design choices.
- Core assumption: Tree-based ensemble models are inherently better suited for the tabular AD task characteristics than neural network meta-predictors.
- Evidence anchors:
  - [section] "we still find that the tree-based ensemble model(s) are better solutions for tabular-based AD tasks, where the performance of meta-predictors are improved when the MLP trainer used in meta-predictor is replaced by the ensemble models like XGBoost and CatBoost"
  - [abstract] "ADGym is a benchmarking platform for deep anomaly detection (AD) that evaluates and automates the selection of design choices in AD models"
  - [corpus] Weak evidence - corpus contains related works on AD but no direct evidence comparing tree-based vs neural network meta-predictors for AD design choice selection
- Break condition: If the dataset characteristics shift significantly from the training distribution, or if the complexity of design choice interactions exceeds what tree-based models can capture.

### Mechanism 3
- Claim: ADGym achieves significant performance improvements by exploring a comprehensive design space that includes data augmentation, advanced network architectures, and sophisticated loss functions often overlooked in traditional AD research.
- Mechanism: By systematically evaluating design choices like ResNet, FTTransformer, GAN-based augmentation, and various loss functions (hinge, deviation, focal), ADGym identifies combinations that outperform traditional MLP + BCE loss baselines. The large-scale evaluation reveals that no single design choice excels across all datasets, justifying the need for automated selection.
- Core assumption: The performance improvements come from finding optimal combinations of design choices rather than from any single innovative component.
- Evidence anchors:
  - [section] "Surprisingly, the optimal model composed of different design choices outperforms existing state-of-the-art AD models by a considerable margin"
  - [section] "We investigate various design dimensions of network construction... MLP is still a competitive baseline in AD tasks, and even outperforms other more complex architecture designs like ResNet and FTTransformer"
  - [corpus] Weak evidence - corpus contains related works on AD benchmarks but no direct evidence of comprehensive design space evaluation for AD
- Break condition: If the computational cost of exploring the design space becomes prohibitive, or if the optimal combinations are highly dataset-specific and cannot be generalized through meta-learning.

## Foundational Learning

- Concept: Meta-learning and transfer learning principles
  - Why needed here: ADGym relies on transferring knowledge from historical datasets to select optimal design choices for new datasets. Understanding how meta-features capture dataset characteristics and how meta-predictors generalize is crucial.
  - Quick check question: How does the meta-predictor handle datasets that are significantly different from those in the training set? What features make datasets "similar" for the purposes of meta-learning?

- Concept: Design space exploration and combinatorial optimization
  - Why needed here: ADGym evaluates thousands of design combinations. Understanding how to efficiently explore high-dimensional design spaces and avoid combinatorial explosion is essential.
  - Quick check question: What strategies does ADGym use to sample the design space effectively? How does it balance exploration vs exploitation in finding optimal design combinations?

- Concept: Anomaly detection evaluation metrics and their limitations
  - Why needed here: ADGym uses AUCROC and AUCPR metrics. Understanding their strengths, weaknesses, and appropriate use cases is important for interpreting results correctly.
  - Quick check question: When would AUCROC be more appropriate than AUCPR for evaluating AD models? How do these metrics behave with highly imbalanced datasets?

## Architecture Onboarding

- Component map:
  - Data Handling -> Network Construction -> Network Training
  - Meta-learning pipeline: Feature extraction -> Meta-predictor training -> Design choice selection

- Critical path:
  1. Extract meta-features from historical datasets
  2. Train meta-predictor on historical performance data
  3. For new dataset: extract meta-features, predict optimal design choices
  4. Construct and train AD model with selected design choices
  5. Evaluate performance

- Design tradeoffs:
  - Larger design space → better potential performance but higher computational cost
  - More historical datasets → better meta-predictor generalization but more training time
  - Complex meta-features → better characterization but potential overfitting
  - Tree-based vs neural network meta-predictors → different strengths for different dataset characteristics

- Failure signatures:
  - Poor performance on new datasets → meta-predictor not generalizing well
  - High variance across runs → insufficient sampling of design space
  - Computational bottlenecks → design space too large or inefficient implementation
  - Inconsistent results across metrics → design choices not robust to evaluation metric choice

- First 3 experiments:
  1. Replicate baseline results: Compare ADGym's automatically selected design choices against state-of-the-art methods on a standard benchmark dataset
  2. Meta-predictor validation: Test whether the meta-predictor can correctly identify known good design choices for datasets similar to those in the training set
  3. Design space ablation: Systematically remove design dimensions to determine which contribute most to performance improvements

## Open Questions the Paper Calls Out
The paper identifies several future research directions, including incorporating time-series AD tasks, addressing the computational overhead of meta-learning, and exploring alternative meta-learning techniques for improved design choice selection.

## Limitations
- Weak empirical evidence for component-level meta-learning effectiveness
- Assumptions about tree-based vs neural network meta-predictor superiority not directly tested
- Limited evidence for the generalization capability of the meta-predictor across diverse dataset distributions

## Confidence
- High Confidence: The general framework of ADGym as a benchmarking platform for deep AD
- Medium Confidence: The performance improvements over state-of-the-art methods
- Low Confidence: The specific mechanisms of meta-learning for design choice selection and the superiority of tree-based vs neural network meta-predictors

## Next Checks
1. Cross-validation of meta-predictor generalization: Test the meta-predictor on held-out datasets from the same distribution as the training set, then on datasets from different distributions to quantify generalization performance and identify failure modes.

2. Ablation study of design space components: Systematically remove individual design dimensions (e.g., data augmentation, specific architectures, loss functions) and measure the impact on performance to determine which components contribute most to improvements.

3. Meta-predictor architecture comparison: Directly compare tree-based ensemble models (XGBoost/CatBoost) against neural network meta-predictors on the same tabular AD tasks to empirically validate the claimed superiority of tree-based approaches.