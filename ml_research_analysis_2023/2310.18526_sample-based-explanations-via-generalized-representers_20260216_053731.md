---
ver: rpa2
title: Sample based Explanations via Generalized Representers
arxiv_id: '2310.18526'
source_url: https://arxiv.org/abs/2310.18526
tags:
- training
- kernel
- function
- sample
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces generalized representers, a framework for
  sample-based explanations of machine learning models that satisfies a set of desirable
  axiomatic properties. The key idea is that any sample-based explanation satisfying
  these axioms must take the form of a global importance weight multiplied by a kernel
  similarity between training and test samples.
---

# Sample based Explanations via Generalized Representers

## Quick Facts
- arXiv ID: 2310.18526
- Source URL: https://arxiv.org/abs/2310.18526
- Reference count: 40
- Primary result: Introduces generalized representers as a framework for sample-based explanations satisfying axiomatic properties

## Executive Summary
This paper introduces generalized representers, a framework for sample-based explanations of machine learning models that satisfies a set of desirable axiomatic properties. The key insight is that any sample-based explanation satisfying these axioms must take the form of a global importance weight multiplied by a kernel similarity between training and test samples. The paper provides methods to compute global importance weights through RKHS regression and discusses natural kernel choices for modern neural networks. Experiments demonstrate that generalized representers outperform existing methods in case deletion diagnostics.

## Method Summary
The paper proposes generalized representers as a framework for sample-based explanations that satisfy five axiomatic properties: efficiency, self-explanation, symmetric zero, symmetric cycle, and irreducibility. The framework represents explanations as global importance weights multiplied by kernel similarities between training and test samples. Three methods are proposed to compute global importance weights: projecting the target function onto the RKHS subspace spanned by training representers, using the target derivative as an approximation, and tracking gradient descent trajectories. The paper discusses kernel choices including last-layer embeddings, neural tangent kernels, and influence function kernels.

## Key Results
- Generalized representers are the only sample-based explanations satisfying the five proposed axioms
- Case deletion diagnostics show generalized representers outperform existing methods like influence functions and TracIn
- Different kernel choices (last-layer, NTK, influence function) show varying performance across datasets
- Target derivative method provides a computationally efficient approximation to the RKHS projection approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalized representers are the only sample-based explanations satisfying a natural set of axiomatic properties
- Mechanism: The paper proves that any explanation satisfying continuity, self-explanation, symmetric zero, symmetric cycle, and irreducibility axioms must take the form of global importance weights multiplied by kernel similarities between training and test samples
- Core assumption: The axioms capture the essential properties we want from sample-based explanations
- Evidence anchors:
  - [abstract]: "A key contribution of the paper is to show that generalized representers are the only class of sample based explanations satisfying a natural set of axiomatic properties"
  - [section 3]: The paper defines five axioms (Efficiency, Self-Explanation, Symmetric Zero, Symmetric Cycle, Continuity, Irreducibility) and proves Theorem 7 showing any explanation satisfying these must be of the form αᵢK(xᵢ, x)
  - [corpus]: Weak evidence - no direct citations to this specific axiomatic framework, though related work on representer theorems exists
- Break condition: If any of the axioms are violated or if the model function doesn't lie in the RKHS subspace spanned by training kernel representers

### Mechanism 2
- Claim: Computing global importance weights can be cast as solving an RKHS regression problem
- Mechanism: Given a kernel K, the global importance weights α are found by projecting the target function onto the RKHS subspace spanned by kernel evaluations on training points, minimizing a loss function plus regularization
- Core assumption: The target function can be well-approximated by functions in the RKHS defined by kernel K
- Evidence anchors:
  - [section 4.1]: "given a kernel, extracting global importances can be cast as solving an RKHS regression problem, by recourse to RKHS representer theorems"
  - [section 4.1]: The paper formulates this as minimizing L(f_K(x_i), f(x_i)) + λ∥f_K∥²_HK where f_K is in the RKHS
  - [corpus]: Moderate evidence - representer theorems are well-established in machine learning literature, though this specific application to sample-based explanations appears novel
- Break condition: If the target function is too complex to be well-approximated by the RKHS subspace, or if the kernel is poorly chosen

### Mechanism 3
- Claim: Many existing sample-based explanation methods can be viewed as specific instances of generalized representers
- Mechanism: By choosing appropriate kernels (last-layer embeddings, neural tangent kernels, influence function kernels) and methods for computing global importance (surrogate derivative, target derivative, tracking), existing methods like representer points, influence functions, and TracIn can be recovered as special cases
- Core assumption: These existing methods can be reformulated in the generalized representer framework
- Evidence anchors:
  - [section 5]: "Many existing sample based explanation methods such as representer point selection [7], and influence functions [5] can be viewed as specific instances of the broad class of generalized representers"
  - [section 5.1]: Shows representer point selection uses last-layer embeddings as kernel and surrogate derivative for global importance
  - [corpus]: Strong evidence - the paper cites and builds upon existing work on representer points [7] and influence functions [5]
- Break condition: If the existing methods cannot be reformulated in this framework, or if their approximations break down

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: The paper's theoretical framework relies on RKHS theory to prove that explanations must take a specific form and to provide methods for computing global importance weights
  - Quick check question: Can you explain what it means for a function to be in the RKHS defined by a kernel K?

- Concept: Mercer's Theorem
  - Why needed here: The irreducibility axiom requires the kernel to be positive semi-definite, which is guaranteed by Mercer's theorem for continuous positive-definite kernels
  - Quick check question: What conditions must a function K(x,z) satisfy to be a valid Mercer kernel?

- Concept: Representer Theorem
  - Why needed here: The representer theorem guarantees that the solution to the RKHS regression problem lies in the span of kernel evaluations on training points, which is crucial for the form of the explanations
  - Quick check question: Can you state the representer theorem and explain why it's important for this work?

## Architecture Onboarding

- Component map:
  Input -> Kernel selection -> Global importance computation -> Output

- Critical path:
  1. Choose kernel K based on model architecture and domain knowledge
  2. Compute global importance weights α using one of the three methods
  3. Calculate sample-based explanations as αᵢ * K(xᵢ, x) for each training point

- Design tradeoffs:
  - Kernel choice: Last-layer is simpler but may miss important information; NTK captures more but is computationally heavier; influence function is most informative but requires Hessian computation
  - Global importance method: Surrogate derivative is most accurate but requires solving an optimization problem; target derivative is faster but approximate; tracking is scalable but requires access to training trajectory

- Failure signatures:
  - Poor performance on case deletion diagnostics
  - Explanations that don't satisfy the axiomatic properties
  - High variance in explanations across different runs
  - Computational intractability for large models or datasets

- First 3 experiments:
  1. Implement generalized representers with last-layer kernel and target derivative on a small CNN (e.g., MNIST) and verify they satisfy the axioms
  2. Compare different kernel choices (last-layer vs NTK) on the same model and dataset using case deletion diagnostics
  3. Implement tracking representers and compare their performance and computational cost to the other global importance methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of kernel function impact the accuracy and computational efficiency of generalized representers in practice?
- Basis in paper: [explicit] The paper discusses various kernel choices including last-layer embeddings, neural tangent kernels, and influence function kernels, and shows that different kernels perform differently on different datasets.
- Why unresolved: While the paper provides some experimental results comparing different kernels, it does not provide a comprehensive analysis of the trade-offs between accuracy and computational efficiency for different kernel choices across various datasets and model architectures.
- What evidence would resolve it: Systematic experiments comparing the performance of different kernels on a wide range of datasets and model architectures, including both accuracy and computational efficiency metrics, would help resolve this question.

### Open Question 2
- Question: How does the choice of global importance computation method affect the performance of generalized representers?
- Basis in paper: [explicit] The paper discusses three methods for computing global importance: projecting the target function onto the RKHS subspace, using the target derivative as an approximation, and tracking gradient descent trajectories.
- Why unresolved: The paper only provides limited experimental results comparing these methods on a few datasets, and does not provide a comprehensive analysis of their relative strengths and weaknesses in different scenarios.
- What evidence would resolve it: Systematic experiments comparing the performance of different global importance computation methods on a wide range of datasets and model architectures, including both accuracy and computational efficiency metrics, would help resolve this question.

### Open Question 3
- Question: Can the framework of generalized representers be extended to handle more complex data types and model architectures beyond standard neural networks?
- Basis in paper: [inferred] The paper focuses on explaining standard neural network models, but the axiomatic framework could potentially be extended to other types of models or data.
- Why unresolved: The paper does not explore the applicability of the generalized representers framework to other types of models or data, such as graph neural networks, transformers, or structured data.
- What evidence would resolve it: Experiments demonstrating the effectiveness of generalized representers for explaining other types of models or data, as well as theoretical analysis of the applicability of the axiomatic framework to these cases, would help resolve this question.

## Limitations

- The axiomatic framework, while elegant, relies heavily on the assumption that these five axioms capture all desirable properties of sample-based explanations
- Computational requirements of computing influence function kernels (requiring Hessian computations) or tracking representers (requiring access to training trajectories) may be prohibitive for large-scale models
- The paper's experimental evaluation is limited to relatively small-scale image and text classification tasks, leaving uncertainty about scalability to modern deep learning architectures

## Confidence

**High Confidence:**
- The characterization theorem showing that any explanation satisfying the five axioms must take the form αᵢK(xᵢ, x)
- The representer theorem application showing that global importance weights can be computed via RKHS projection
- The relationship between existing methods (representer points, influence functions, TracIn) and generalized representers

**Medium Confidence:**
- The practical utility of the axiomatic properties for real-world explanation tasks
- The effectiveness of different kernel choices (last-layer vs NTK vs influence function) across diverse model architectures
- The computational scalability of the proposed methods to large-scale deep learning models

**Low Confidence:**
- The robustness of explanations under distribution shift or adversarial perturbations
- The sensitivity of results to hyperparameter choices (λ regularization, kernel bandwidth)
- The generalization of findings to domains beyond image and text classification

## Next Checks

1. **Axiomatic Property Validation**: Test whether explanations from state-of-the-art black-box explanation methods (e.g., SHAP, LIME) violate any of the five axioms, and whether violating these axioms leads to demonstrably worse explanations in practice.

2. **Scalability Experiment**: Implement tracking representers for a large transformer-based model (e.g., BERT) on a text classification task, measuring both explanation quality (via case deletion diagnostics) and computational cost compared to baseline methods.

3. **Distribution Shift Robustness**: Evaluate the stability of generalized representer explanations under covariate shift by testing on out-of-distribution samples and measuring changes in attribution weights compared to in-distribution samples.