---
ver: rpa2
title: A Simple and Effective Pruning Approach for Large Language Models
arxiv_id: '2306.11695'
source_url: https://arxiv.org/abs/2306.11695
tags:
- pruning
- wanda
- magnitude
- sparsity
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Wanda, a novel and effective pruning method
  for large language models (LLMs). Motivated by emergent large magnitude features
  in LLMs, Wanda prunes weights based on the product of their magnitude and the norm
  of corresponding input activations.
---

# A Simple and Effective Pruning Approach for Large Language Models

## Quick Facts
- arXiv ID: 2306.11695
- Source URL: https://arxiv.org/abs/2306.11695
- Authors: Not specified in source
- Reference count: 40
- Key outcome: Introduces Wanda, a pruning method that combines weight magnitude with input activation norm, achieving high sparsity without retraining and outperforming magnitude pruning on LLaMA models.

## Executive Summary
This paper presents Wanda, a novel pruning method for large language models that leverages emergent large magnitude features in LLMs. By multiplying weight magnitudes with the ℓ₂ norm of corresponding input activations, Wanda creates importance scores that better preserve critical weights. The method requires no retraining or weight updates and can be executed in a single forward pass, making it computationally efficient. Evaluated on LLaMA models, Wanda significantly outperforms magnitude pruning and matches or exceeds the performance of recent methods involving intensive weight updates.

## Method Summary
Wanda prunes weights based on the product of their magnitude and the norm of corresponding input activations. The method computes importance scores S_ij = |W_ij| · ||X_j||_2 for all weights, then removes the lowest-scoring weights per output neuron according to target sparsity. Unlike prior methods, Wanda requires no retraining or weight updates and can be executed in a single forward pass. The approach uses per-output pruning granularity and can be extended to structured N:M sparsity patterns.

## Key Results
- Wanda significantly outperforms magnitude pruning on LLaMA models at various sparsity levels
- Achieves competitive or superior performance compared to recent pruning methods involving intensive weight updates
- Successfully generalizes to structured N:M sparsity and image classifiers
- Requires only a single forward pass with no retraining or weight updates

## Why This Works (Mechanism)

### Mechanism 1
Weight importance scores that combine magnitude and activation norm preserve critical weights better than magnitude alone. By multiplying weight magnitude by the norm of its input activation, the importance score becomes sensitive to whether the weight connects to a large-magnitude feature, not just how large the weight itself is. The ℓ₂ norm of input activations serves as a reliable proxy for feature importance in LLMs.

### Mechanism 2
Per-output pruning granularity yields better sparse sub-networks than global or layer-wise pruning. Comparing and removing weights locally within each output neuron prevents the pruning process from disproportionately removing weights connected to certain outputs, maintaining balanced expressiveness across outputs.

### Mechanism 3
Wanda's pruning metric approximates a diagonal Hessian approximation of the layer-wise reconstruction loss, reducing computational cost. The metric |W_ij| · ||X_j||_2 can be derived as a simplification of second-order methods by keeping only diagonal Hessian terms, thus avoiding expensive matrix inverses.

## Foundational Learning

- Concept: ℓ₂ norm as activation importance measure
  - Why needed here: Used to weight each weight's contribution based on the magnitude of its input feature
  - Quick check question: Why does ℓ₂ norm tend to work better than ℓ₁ or ℓ∞ for measuring activation magnitudes?

- Concept: Structured vs unstructured sparsity
  - Why needed here: Understanding why Wanda can be extended to N:M sparsity and the practical trade-offs of each
  - Quick check question: What is the main difference between unstructured and structured sparsity in terms of GPU acceleration?

- Concept: Layer-wise vs global vs per-output pruning granularity
  - Why needed here: Critical to understanding why Wanda's per-output approach works better for LLMs
  - Quick check question: In what scenario might global pruning outperform per-output pruning?

## Architecture Onboarding

- Component map: Pretrained LLM weights -> Forward pass calibration -> Activation norm collection -> Importance score computation -> Per-output weight sorting -> Weight removal -> Sparse weight matrices
- Critical path: 1) Forward pass calibration data → collect activation norms per channel 2) Compute importance scores for all weights 3) For each output neuron, sort and remove lowest s% weights 4) Repeat per layer; no retraining needed
- Design tradeoffs: Per-output granularity → better accuracy but more complex bookkeeping; ℓ₂ norm → stable but may miss some activation patterns; No weight update → computationally cheap but may leave sub-optimal sparse configurations
- Failure signatures: Excessive accuracy drop → calibration data too small or unrepresentative; Slow inference despite sparsity → incorrect structured sparsity conversion; High variance across runs → randomness in calibration data selection
- First 3 experiments: 1) Run Wanda on a single small LLM layer with a synthetic calibration set; verify weight removal count matches target sparsity 2) Compare per-output vs layer-wise pruning on a toy 2-layer model; record accuracy difference 3) Extend to N:M structured sparsity; validate that at most N out of every M consecutive weights are kept

## Open Questions the Paper Calls Out

### Open Question 1
How do emergent large magnitude features in LLMs impact the effectiveness of pruning methods, and can their identification be leveraged to develop more efficient pruning strategies? The paper discusses the observation of emergent large magnitude features in LLMs and their significance for pruning effectiveness, but does not provide a detailed analysis of their specific impact or systematic identification methods.

### Open Question 2
Can Wanda's pruning approach be generalized to other types of neural networks beyond LLMs, and how does its performance compare to established pruning methods in these contexts? The paper mentions the potential of Wanda to be a general pruning approach and provides preliminary results on image classifiers, but does not conduct extensive experiments on various types of neural networks.

### Open Question 3
How does the choice of pruning granularity (e.g., per layer, per output) affect the efficiency and effectiveness of pruning in different neural network architectures? The paper discusses the importance of pruning granularity in LLM pruning and suggests that local granularity levels may be beneficial, but does not provide a thorough analysis across various architectures and tasks.

### Open Question 4
What are the potential applications of Wanda in real-world scenarios, such as in resource-constrained environments or for real-time processing, and how does it perform in these contexts? The paper mentions the potential of Wanda to be used in real-time processing and its computational efficiency, but does not explore practical applications in specific real-world scenarios.

## Limitations
- Empirical evaluation relies heavily on calibration data from a single C4 shard without specifying random seeds
- Theoretical justification for pruning metric's effectiveness is largely heuristic with minimal rigorous proof
- Limited testing on non-LLaMA architectures makes generalization claims uncertain
- Does not address performance at extremely high sparsity levels (>90%) or on multi-modal models

## Confidence

**High Confidence**: The core claim that Wanda works without retraining and achieves competitive performance with existing methods. The ablation studies on calibration data size and structured sparsity generalization are internally consistent.

**Medium Confidence**: The claim that per-output pruning granularity is consistently superior for LLMs. While supported by experimental results, the paper lacks theoretical justification and doesn't explore edge cases where this might fail.

**Low Confidence**: The theoretical connection to diagonal Hessian approximation. The paper states this relationship but provides minimal derivation or validation that this approximation captures meaningful curvature information.

## Next Checks

1. Reproduce the 50% sparsity baseline: Run Wanda on LLaMA-7B with the exact calibration procedure (128 sequences from C4 shard 1) and verify the reported WikiText perplexity improvement over magnitude pruning.

2. Test across model families: Apply Wanda to non-LLaMA architectures (e.g., OPT or BLOOM) to assess generalization beyond the specific model family used in the paper.

3. Evaluate structured sparsity trade-offs: Systematically measure the performance gap between unstructured and N:M structured sparsity across different sparsity levels to quantify the practical cost of hardware efficiency.