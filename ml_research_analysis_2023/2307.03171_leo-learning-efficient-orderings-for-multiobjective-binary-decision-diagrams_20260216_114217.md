---
ver: rpa2
title: 'LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams'
arxiv_id: '2307.03171'
source_url: https://arxiv.org/abs/2307.03171
tags:
- variable
- time
- ordering
- problem
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents LEO, a machine learning framework to accelerate
  multiobjective integer programming using binary decision diagrams (BDDs). The authors
  address the problem of variable ordering in BDD construction, which significantly
  impacts the time required to enumerate the Pareto frontier.
---

# LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams

## Quick Facts
- arXiv ID: 2307.03171
- Source URL: https://arxiv.org/abs/2307.03171
- Reference count: 18
- Primary result: LEO achieves 30-300% and 10-200% faster Pareto frontier enumeration than common ordering strategies and algorithm configuration, respectively

## Executive Summary
This paper introduces LEO, a machine learning framework that accelerates multiobjective integer programming using binary decision diagrams (BDDs). LEO addresses the critical challenge of variable ordering in BDD construction, which significantly impacts the time required to enumerate the Pareto frontier. The approach combines black-box optimization with supervised learning to predict efficient variable orderings that minimize BDD size and, consequently, Pareto frontier enumeration time.

## Method Summary
LEO operates in two phases: (1) black-box optimization using SMAC to find high-quality variable orderings for training instances by minimizing Pareto frontier enumeration time, and (2) supervised learning to predict orderings for new instances using variable and context features. The method employs a parameter configuration space based on linear combinations of interpretable variable features, avoiding the factorial search space of direct permutation optimization. Learning-to-rank models (linear, ridge, lasso, decision trees, GBT with MSE; SVM, GBT with pairwise loss) are trained on featurized data and evaluated using Kendall's Tau and geometric mean of enumeration time.

## Key Results
- LEO achieves 30-300% and 10-200% faster Pareto frontier enumeration than common ordering strategies and algorithm configuration, respectively
- The best model uses extreme gradient boosted ranking trees with Kendall's Tau of 0.67-0.81 on validation set
- Size-independent ML models perform remarkably well, sometimes outperforming size-specific models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variable ordering directly impacts Pareto frontier enumeration time in multiobjective BDDs.
- Mechanism: The number and arrangement of BDD nodes depends on variable ordering; certain orderings lead to smaller BDDs with fewer intermediate solutions and Pareto-dominance checks, which reduces enumeration time.
- Core assumption: The BDD construction algorithm and PF enumeration procedure are sensitive to the size and structure of the BDD.
- Evidence anchors:
  - [abstract] "We first showcase a similar impact of variable ordering on the Pareto frontier (PF) enumeration time for the multiobjective knapsack problem..."
  - [section] "Table 3 summarizes the results of this experiment. We work with 250 MKP instances... values smaller than one indicate that a heuristic ordering is faster than a random one, on average."
- Break condition: If BDD construction becomes insensitive to ordering (e.g., fixed-width compilation or if the problem structure changes such that size is invariant to ordering).

### Mechanism 2
- Claim: Black-box optimization over a small property-weight space efficiently discovers near-optimal variable orderings.
- Mechanism: Instead of searching over n! permutations, the method maps each variable to a score via a linear combination of interpretable features; the optimizer tunes a small set of weights (|K| << n!) to minimize enumeration time.
- Core assumption: A small set of variable features (properties) captures sufficient information about the problem structure to guide ordering decisions.
- Evidence anchors:
  - [abstract] "We derive a novel parameter configuration space based on variable scoring functions which are linear in a small set of interpretable and easy-to-compute variable features."
  - [section] "The search for oe is conducted in the surrogate search space [−1, 1]|K|, which only depends on |K| and not on the number of variables n."
- Break condition: If the chosen features do not correlate with enumeration time, or if the linear scoring model cannot capture necessary variable interactions.

### Mechanism 3
- Claim: Supervised learning models trained on black-box optimization outputs generalize to unseen instances and predict orderings that reduce enumeration time.
- Mechanism: Each variable-instance pair is featurized independently of instance size; a size-independent model predicts variable ranks, enabling fast ordering prediction without re-running black-box optimization.
- Core assumption: The learned ranking function transfers across problem sizes and instance variations due to permutation invariance and feature engineering.
- Evidence anchors:
  - [abstract] "Experiments on benchmark sets from the knapsack problem with 3-7 objectives and up to 80 variables show that LEO is ∼30-300% and ∼10-200% faster..."
  - [section] "These two models perform remarkably well, sometimes even outperforming the size-specific ML models... This finding points to a great potential for the use of ML in multiobjective BDD problems."
- Break condition: If test instances differ structurally from training data, or if the feature engineering fails to capture new problem classes.

## Foundational Learning

- Concept: Binary Decision Diagrams (BDDs) and their construction
  - Why needed here: LEO's effectiveness depends on understanding how variable ordering affects BDD size and downstream enumeration time.
  - Quick check question: What determines the width of a BDD layer and why does this matter for enumeration time?

- Concept: Pareto dominance and Pareto frontier enumeration
  - Why needed here: LEO targets reducing the time to compute the PF; understanding dominance checks and intermediate solutions is essential.
  - Quick check question: How does the number of intermediate solutions in a BDD relate to the number of Pareto-dominance checks during enumeration?

- Concept: Feature engineering and permutation invariance
  - Why needed here: LEO's ML models rely on variable-level features that are invariant to instance size; knowing how to construct such features is critical.
  - Quick check question: Why must variable features be independent of the total number of variables in the instance?

## Architecture Onboarding

- Component map:
  - Black-box optimizer (SMAC) -> finds property-weight labels per training instance
  - BDD Manager -> constructs BDDs and measures PF enumeration time given a variable order
  - Featurizer -> extracts variable and context features for ML
  - Learning-to-rank models (GBT, linear, SVM) -> predict variable orderings
  - Evaluation pipeline -> compares predicted orderings to baselines and measures enumeration time

- Critical path:
  1. Generate training instances -> run SMAC to label with efficient orderings
  2. Extract features -> train size-independent ranking model
  3. Predict ordering for new instance -> construct BDD -> enumerate PF
  4. Measure and compare runtime to baselines

- Design tradeoffs:
  - Size-specific vs. size-independent ML models: size-specific models may fit better but require separate training; size-independent models generalize but may lose fine-grained performance.
  - Number of features: more features can capture more structure but increase risk of overfitting and computational cost.
  - Black-box optimization budget: more seeds/time may yield better labels but increase labeling cost.

- Failure signatures:
  - ML predictions yield no improvement over random or heuristic orderings
  - BDD construction fails or memory usage spikes for predicted orderings
  - Feature importance analysis shows no consistent pattern across sizes

- First 3 experiments:
  1. Verify that MinWt and MaxRatio heuristics consistently outperform random orderings on small test instances.
  2. Run SMAC with one seed on a small instance to confirm it improves over MinWt within the time budget.
  3. Train a size-specific GBT model on labeled data from one size and evaluate Kendall's Tau on a held-out validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed property-weight search space and black-box optimization approach be effectively extended to other multiobjective integer programming problems beyond the knapsack problem?
- Basis in paper: [explicit] The authors mention that LEO could be directly extended to other problems "assuming that their BDD construction is significantly influenced by the variable ordering" and that "much of LEO can be directly extended to such other problems."
- Why unresolved: The paper only presents results on the knapsack problem, so the generalizability of the approach to other MOIP problems remains untested. Different problem structures may require different variable properties and potentially different search strategies.
- What evidence would resolve it: Experiments demonstrating LEO's performance on at least two other MOIP problems with different structures (e.g., multiobjective assignment, multiobjective facility location) would provide evidence for or against generalizability.

### Open Question 2
- Question: How would dynamic variable ordering strategies compare to the static approach used in LEO, particularly for problems where the optimal ordering changes during BDD construction?
- Basis in paper: [explicit] The authors discuss that "While this was sufficient to improve on non-ML orderings for the knapsack problem, it may be interesting to consider dynamic variable orderings that observe the BDD construction process layer by layer and choose the next variable accordingly."
- Why unresolved: The paper focuses exclusively on static variable orderings, so the potential benefits of dynamic ordering remain unexplored. The trade-off between computational overhead of dynamic ordering and potential BDD size reduction is unknown.
- What evidence would resolve it: Implementing and comparing a dynamic ordering variant of LEO against the static approach on benchmark problems, measuring both BDD construction time and final BDD size, would provide empirical evidence of the benefits or drawbacks.

### Open Question 3
- Question: Could more sophisticated deep learning approaches, such as graph neural networks or transformers, outperform the XGBoost models used in LEO, particularly for larger instances with complex variable interactions?
- Basis in paper: [inferred] The authors mention that "we have opted for rather interpretable ML model classes but the exploration of more sophisticated deep learning approaches may enable closing some of the remaining gap in training/validation loss, which may improve downstream solving performance."
- Why unresolved: The paper uses XGBoost models which are relatively simple and interpretable, but does not explore more complex deep learning architectures that might better capture complex variable relationships in larger instances.
- What evidence would resolve it: Implementing LEO with various deep learning models (GNNs, transformers, etc.) and comparing their performance against XGBoost on problems of increasing size and complexity would reveal whether more sophisticated models provide meaningful improvements.

## Limitations
- Speedup results are based on experiments on a single problem domain (multiobjective knapsack) with controlled instance distributions
- Black-box optimization phase requires significant computational resources (100+ seeds per instance), limiting practical applicability
- Feature engineering approach assumes variable-level properties capture sufficient problem structure, which may not hold for problems with different structure or constraints

## Confidence

- **High Confidence**: The core mechanism linking variable ordering to BDD size and enumeration time (Mechanism 1), supported by experimental results showing heuristic orderings outperform random ones.
- **Medium Confidence**: The black-box optimization approach (Mechanism 2), as the linear scoring model is shown to work but the sensitivity to feature selection is not fully explored.
- **Medium Confidence**: The generalization capability of supervised models (Mechanism 3), as size-independent models show promise but are only validated on similar knapsack instances.

## Next Checks

1. **Cross-domain validation**: Test LEO on multiobjective versions of other combinatorial problems (e.g., vertex cover, set covering) to assess generalizability beyond knapsack problems.

2. **Ablation study on features**: Systematically remove individual features from the model to identify which contribute most to performance and test robustness to missing information.

3. **Real-world benchmark evaluation**: Apply LEO to benchmark instances from optimization libraries (e.g., Mibench, OR-Library) with known Pareto frontiers to validate performance on realistic problems.