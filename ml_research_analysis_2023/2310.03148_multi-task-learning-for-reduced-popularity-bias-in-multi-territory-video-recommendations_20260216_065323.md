---
ver: rpa2
title: Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations
arxiv_id: '2310.03148'
source_url: https://arxiv.org/abs/2310.03148
tags:
- learning
- data
- title
- bias
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the popularity bias problem in global video
  recommendation systems, where locally popular items can be overshadowed by globally
  prevalent ones due to data imbalances across territories. They propose a multi-task
  learning (MTL) approach with adaptive upsampling to mitigate this bias.
---

# Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations

## Quick Facts
- arXiv ID: 2310.03148
- Source URL: https://arxiv.org/abs/2310.03148
- Reference count: 40
- Multi-task learning approach achieves up to 65.27% relative gain in PR-AUC for reducing popularity bias in multi-territory video recommendations

## Executive Summary
This paper addresses popularity bias in global video recommendation systems where locally popular items are overshadowed by globally prevalent ones due to data imbalances across territories. The authors propose a multi-task learning (MTL) approach with adaptive upsampling to mitigate this bias. By learning territory-specific user embeddings through hard parameter sharing MTL while enriching training data with active user representations, the method significantly improves recommendations for locally popular content while attenuating global item dominance across 200+ territories.

## Method Summary
The approach employs hard parameter sharing multi-task learning where user and title towers are shared across all territories, but territory-specific blocks are added to the user tower. Active user upsampling enriches training data by adding a disjoint set of active users sampled independently from random users. The model uses masked binary cross-entropy loss to ensure territory-specific blocks only update on relevant data. Four territory groups are created based on geographic proximity and viewership patterns, balancing major and minor territories.

## Key Results
- Achieved up to 65.27% relative gain in PR-AUC compared to baseline models
- Successfully reduced popularity bias by adjusting score distributions for local and global titles
- Demonstrated improved recommendations for locally popular content while maintaining global recommendation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MTL reduces popularity bias by sharing representations across territories while allowing territory-specific refinements
- Mechanism: Hard parameter sharing forces the model to learn a common representation space that captures shared patterns across territories, while task-specific layers adapt these shared embeddings to each territory's unique distribution
- Core assumption: Territories with similar viewership patterns can benefit from shared representations, while territory-specific layers can handle local variations
- Evidence anchors:
  - [abstract] "MTL learns shared representations that can be used by countries with less data allowing the transfer of knowledge from countries with large data."
  - [section 3.3] "MTL has strong regularisation properties [6, 36] that improves the generalisation power of the model by preventing overfitting on global titles."
  - [corpus] Weak evidence - related papers focus on popularity bias mitigation but don't explicitly validate MTL's regularization effect
- Break condition: If territory viewership patterns are completely disjoint or adversarial, shared representations may introduce harmful interference rather than benefit

### Mechanism 2
- Claim: Active user upsampling enriches training data with more informative examples to learn from
- Mechanism: By adding a disjoint set of active users sampled independently to the random user set, the training dataset gains more positive examples, particularly benefiting minor territories with fewer interactions
- Core assumption: Active users provide more diverse and representative interactions than random sampling, improving model's ability to generalize to local preferences
- Evidence anchors:
  - [section 3.2] "This would enrich our training dataset with more number of active user embeddings. Also, this would improve the likelihood of having more number of positive examples in training particularly helping the minor territories."
  - [section 4.1] "We created two versions of the training dataset where several million users (order of 10M) are sampled randomly and then added the set of active users using the upsampling process"
  - [corpus] Weak evidence - no direct comparison between active user sampling vs other rebalancing strategies in corpus
- Break condition: If active user definition is too narrow or time-bound, upsampling may overfit to short-term trends rather than stable preferences

### Mechanism 3
- Claim: MTL's conditional loss computation ensures territory-specific blocks only update on relevant data
- Mechanism: One-hot encoded masks in the loss function prevent gradients from propagating to task-specific blocks for data points outside their territory group, maintaining clean separation while sharing base representations
- Core assumption: Territory groups are well-defined and data points can be cleanly assigned to single groups without ambiguity
- Evidence anchors:
  - [section 3.3] "We use one-hot encoded mask while calculating the loss to ensure that only data points from the respective territory group can update the group-specific blocks, blocking the gradients of other blocks."
  - [section 4.2] "For MTL-based experiments, we use random batch strategy where each batch can contain examples from all the tasks."
  - [corpus] No evidence found - this specific masking technique not mentioned in related work
- Break condition: If territory boundaries are fuzzy or users frequently cross group boundaries, hard masking may prevent beneficial cross-pollination of knowledge

## Foundational Learning

- Concept: Multi-Task Learning (MTL) architecture patterns
  - Why needed here: Understanding how shared and task-specific layers interact is critical for debugging training dynamics and interpreting results
  - Quick check question: In hard parameter sharing, which layers are shared vs task-specific, and why does this matter for regularization?

- Concept: Popularity bias in recommender systems
  - Why needed here: Recognizing how global item dominance emerges from data imbalance helps in designing effective mitigation strategies
  - Quick check question: What metrics best capture popularity bias, and how do they differ from general recommendation accuracy?

- Concept: Two-tower neural network architecture
  - Why needed here: The baseline and MTL models both use this architecture, so understanding its components and training dynamics is essential
  - Quick check question: How does dot product similarity between user and title embeddings translate to propensity scores for first-time discovery?

## Architecture Onboarding

- Component map: Input embeddings (768-dim BERT for titles, 512-dim ResNet for users) -> Shared encoder blocks -> Task-specific encoder blocks -> Dot product similarity layer -> Masked binary cross-entropy loss

- Critical path: Input embeddings → Shared encoder → Task-specific encoder → Dot product → Loss computation with masking

- Design tradeoffs:
  - Hard parameter sharing vs soft parameter sharing: Hard sharing provides stronger regularization but less flexibility
  - Number of territory groups: Too few loses local specificity, too many increases model complexity
  - Upsampling ratio: Must balance enrichment vs introducing sampling bias

- Failure signatures:
  - If PR-AUC doesn't improve despite MTL: Check that territory groups are well-defined and that masking is working correctly
  - If training is unstable: Verify learning rates and batch sizes are appropriate for the increased parameter count
  - If local titles aren't being promoted: Examine score distributions to ensure task-specific layers are learning meaningful adjustments

- First 3 experiments:
  1. Compare baseline vs MTL on a single territory pair to verify learning dynamics
  2. Test different territory grouping strategies to optimize transfer learning
  3. Vary upsampling ratios to find optimal balance between enrichment and bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MTL-based approach scale when applied to a larger number of territory groups beyond the 4 groups used in this study?
- Basis in paper: [explicit] The authors mention grouping 200+ territories into four groups based on geo-partition and balancing major/minor territories. They acknowledge this is a limitation but do not explore scaling to more groups.
- Why unresolved: The paper only evaluates with 4 territory groups, so the effectiveness and computational feasibility of using more granular territory groupings remains untested.
- What evidence would resolve it: Empirical results comparing performance and computational costs across different numbers of territory groups (e.g., 4, 8, 16) on the same dataset.

### Open Question 2
- Question: What is the optimal trade-off between the number of shared parameters and task-specific parameters in the MTL architecture for different levels of data imbalance across territories?
- Basis in paper: [inferred] The authors use a hard parameter sharing approach with separate task-specific layers, but do not systematically explore how varying the balance between shared and task-specific parameters affects performance across territories with different data volumes.
- Why unresolved: The paper uses a fixed architecture but does not analyze how parameter allocation affects performance in territories with varying data availability.
- What evidence would resolve it: Ablation studies varying the proportion of shared vs. task-specific parameters and measuring performance across territories with different data volumes.

### Open Question 3
- Question: How does the proposed approach perform on datasets with different levels of item popularity bias beyond video recommendations?
- Basis in paper: [inferred] The authors focus specifically on video recommendations and demonstrate effectiveness for this domain, but do not test generalizability to other recommendation domains with different bias characteristics.
- Why unresolved: The methodology is domain-specific to video recommendations without testing on other types of recommendation systems.
- What evidence would resolve it: Experiments applying the same methodology to other recommendation domains (e.g., music, books, e-commerce) with different popularity distributions.

## Limitations
- The optimal number of territory groups and their composition criteria remain empirically determined rather than theoretically grounded
- Active user definition and sampling strategy could introduce temporal bias if not carefully calibrated
- The approach assumes territory groups are well-defined with clear boundaries, which may not hold for all recommendation scenarios

## Confidence

**High confidence**: The core mechanism of MTL reducing popularity bias through shared representations and task-specific adaptations is theoretically sound and supported by empirical results (65.27% PR-AUC improvement). The architectural design choices (hard parameter sharing, masked loss computation) are well-established in the MTL literature.

**Medium confidence**: The effectiveness of the active user upsampling strategy depends heavily on the specific definition of "active" and sampling ratio. Without clear specifications, it's difficult to assess whether the observed improvements stem from the MTL architecture itself or the data enrichment process.

**Low confidence**: The generalization of results across different content domains and platform scales is uncertain. The paper focuses on video recommendations with specific embedding architectures (BERT for titles, ResNet for users), which may not transfer directly to other recommendation scenarios.

## Next Checks
1. **Territory Grouping Sensitivity Analysis**: Systematically test different territory grouping strategies (varying number of groups, different clustering algorithms) to determine optimal configuration and assess sensitivity to grouping decisions.

2. **Active User Definition Impact**: Conduct ablation studies varying the definition of "active users" (different time windows, activity thresholds) to quantify the contribution of upsampling versus pure MTL architecture effects.

3. **Cross-Domain Transferability**: Apply the MTL approach to a different recommendation domain (e.g., e-commerce or music) with different content embedding strategies to validate generalizability beyond video recommendations.