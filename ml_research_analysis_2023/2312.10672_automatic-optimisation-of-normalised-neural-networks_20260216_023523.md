---
ver: rpa2
title: Automatic Optimisation of Normalised Neural Networks
arxiv_id: '2312.10672'
source_url: https://arxiv.org/abs/2312.10672
tags:
- neural
- optimisation
- learning
- automatic
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents automatic optimisation methods for normalised
  neural networks using geometry of matrix manifold. The proposed methods utilise
  layerwise weight normalisation to bound the Lipschitz constant and improve gradient
  reliability.
---

# Automatic Optimisation of Normalised Neural Networks

## Quick Facts
- arXiv ID: 2312.10672
- Source URL: https://arxiv.org/abs/2312.10672
- Reference count: 13
- Key outcome: Automatic optimisation methods for normalised neural networks using geometry of matrix manifold

## Executive Summary
This paper presents automatic optimisation methods for normalised neural networks using geometry of matrix manifold. The proposed methods utilise layerwise weight normalisation to bound the Lipschitz constant and improve gradient reliability. Two approaches are developed to determine the stepsize for descent: one using automatic differentiation of the objective function along the update curve, and another using the majorisation-minimisation framework. The methods avoid manual tuning of the learning rate, providing an automated pipeline for optimising normalised neural networks.

## Method Summary
The paper develops two automatic optimisation methods for normalised neural networks using spherical geometry. The first method computes stepsizes using Hessian-free automatic differentiation along exponential map curves on the matrix manifold. The second method employs a majorisation-minimisation framework with architecture-aware bounds. Both methods leverage layerwise weight normalisation to maintain bounded Lipschitz constants and improve gradient reliability. The algorithms automatically determine appropriate stepsizes without manual learning rate tuning.

## Key Results
- Achieved rapid convergence on quadrotor dataset without manual learning rate scheduling
- Consistent prediction accuracy across various layer configurations (d₀=12, d_L=3, d_i=W for i=2,...,L-1 with W∈[15,35] and L∈[4,12])
- RMS errors comparable to manually tuned SGD while eliminating the need for learning rate selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The exponential map on the combined matrix spheres provides geometrically consistent updates that respect the Frobenius norm constraints.
- Mechanism: The update curve Γᵢ(t) = Wᵢ cos(t) + μᵢVᵢ sin(t) ensures that Wᵢ is moved along the geodesic on the sphere while maintaining ||Wᵢ|| = μᵢ throughout the optimization process.
- Core assumption: The parameters evolve on a product manifold of spheres where each layer's weight matrix maintains constant Frobenius norm.
- Evidence anchors:
  - [abstract]: "Then, the proposed algorithms take the update structure based on the exponential map on high-dimensional spheres."
  - [section 2.2]: "The exponential map is given by ExpX(tV) = cos(||V||t)X + sin(||V||t)μV/||V|| for V ∈ TX M and t ∈ R."
  - [corpus]: Weak - no direct corpus support found for this specific mechanism.
- Break condition: The method fails when the exponential map approximation breaks down for large stepsizes, requiring small angle approximations to hold.

### Mechanism 2
- Claim: Automatic differentiation along the update curve provides Hessian-free second-order information for stepsize determination.
- Mechanism: By parameterizing the update curve as γ(t) = (Γ₁(t), ..., Γᴸ(t)) and using Taylor-mode AD, the directional second derivatives are computed without explicitly constructing the Hessian matrix.
- Core assumption: The objective function L(γ(t)) can be accurately approximated by a low-order Taylor expansion along the update curve.
- Evidence anchors:
  - [abstract]: "The first algorithm utilises automatic differentiation of the objective function along the update curve defined on the combined manifold of spheres."
  - [section 3.2.1]: "The r-th order polynomial approximation is given by ¯Lᵣ(t) = Σ(k=0 to r) ¯L⁽ᵏ⁾(0)tᵏ/k! where the higher-order coefficients for k > 2 can be obtained by using Taylor-mode AD."
  - [corpus]: Weak - no direct corpus support for the Hessian-free AD approach described.
- Break condition: The method fails when the Taylor approximation becomes inaccurate due to high curvature of the objective function along the update curve.

### Mechanism 3
- Claim: The majorisation-minimisation framework with architecture-aware bounds provides conservative but reliable stepsize selection.
- Mechanism: By upper bounding the functional perturbation ||∆fₓ||² and deriving a majorant of the form α(cos t - 1) - β sin t + Q/2(P₁(t) - P₂)², the MM procedure finds stepsizes that guarantee descent.
- Core assumption: Assumption 1 holds, where first-order perturbations in f are approximately equal to those in γ at t = 0.
- Evidence anchors:
  - [section 3.2.2]: "Expanding the loss function in f leads to l(f(x;γ(t)), y) = l(f(x;γ(0)), y) + ⟨∇ₗl(f(x;γ(0)), y), ∆fₓ⟩ + 1/2||∆fₓ||² + ..."
  - [section 3.2.2]: "Assumption 1 The first-order expansion of l in f is approximately equal to that in γ at t = 0"
  - [corpus]: Weak - no direct corpus support for the specific MM formulation used.
- Break condition: The method fails when the majorant becomes too conservative, leading to excessively small stepsizes that slow convergence.

## Foundational Learning

- Concept: Riemannian geometry of matrix manifolds
  - Why needed here: Understanding the spherical geometry of normalized weight matrices is crucial for deriving correct update rules that respect the Frobenius norm constraints.
  - Quick check question: What is the tangent space at a point X on the sphere Sⁿᵐ⁻¹μ, and how does it relate to the ambient space ℝⁿ×ᵐ?

- Concept: Automatic differentiation and Taylor-mode computation
  - Why needed here: The Hessian-free AD approach requires understanding how to compute higher-order directional derivatives without explicitly forming the Hessian matrix.
  - Quick check question: How does Taylor-mode AD differ from reverse-mode AD in terms of what derivatives it computes and when?

- Concept: Majorisation-minimisation framework
  - Why needed here: The MM approach provides a principled way to derive optimization algorithms that guarantee descent while being computationally tractable.
  - Quick check question: What are the key requirements for a function to serve as a valid majorant in the MM framework?

## Architecture Onboarding

- Component map: Data → Initialization → Training loop: Compute gradient → Project to tangent space → Determine update direction → Choose stepsize method → Update parameters → Repeat
- Critical path: Data → Initialization → Training loop: Compute gradient → Project to tangent space → Determine update direction → Choose stepsize method → Update parameters → Repeat
- Design tradeoffs: Method 1 (AD-based) offers faster convergence but requires more computational overhead per iteration, while Method 2 (MM-based) is computationally lighter but may converge more slowly due to conservative stepsizes.
- Failure signatures: Divergence when stepsizes become too large, slow convergence when stepsizes are too conservative, poor generalization when initialization is improper.
- First 3 experiments:
  1. Verify that the exponential map maintains the Frobenius norm constraint throughout optimization on a simple 2-layer network.
  2. Compare convergence speed of Method 1 vs Method 2 on a standard benchmark dataset (e.g., MNIST) with varying network depths.
  3. Test the robustness of the initialization procedure by measuring prediction accuracy across different random seeds and network configurations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed methods scale with increasingly deep networks beyond the tested range of depths?
- Basis in paper: [inferred] The paper tested depths up to 12 layers and observed that increasing depth tended to yield larger RMS errors. However, the performance for even deeper networks is unknown.
- Why unresolved: The experiments only covered a limited range of depths (4-12 layers). Scaling to much deeper networks may reveal new challenges or performance characteristics not captured in the current study.
- What evidence would resolve it: Experiments with networks containing significantly more layers (e.g., 20+ layers) to evaluate the methods' effectiveness and identify any potential issues or limitations when scaling to very deep architectures.

### Open Question 2
- Question: How do the proposed methods perform when applied to networks with convolutional or recurrent layers instead of fully-connected layers?
- Basis in paper: [inferred] The paper focuses on fully-connected networks with layerwise normalization. It's unclear how well the methods would generalize to other network architectures.
- Why unresolved: The proposed methods are tailored for fully-connected networks with specific normalization. Extending them to handle convolutional or recurrent layers would require additional development and validation.
- What evidence would resolve it: Experiments applying the methods to convolutional and recurrent neural networks, comparing their performance to standard optimization techniques for these architectures.

### Open Question 3
- Question: What is the impact of using different activation functions (beyond ReLU) on the performance of the proposed methods?
- Basis in paper: [explicit] The paper uses ReLU as the activation function and assumes it satisfies certain Lipschitz continuity properties. The performance with other activations is not explored.
- Why unresolved: The methods rely on the specific properties of ReLU. It's unknown how well they would work with other activation functions that may have different characteristics.
- What evidence would resolve it: Experiments using various activation functions (e.g., sigmoid, tanh, leaky ReLU) and comparing the performance of the proposed methods to standard optimization techniques with these activations.

## Limitations
- The methods rely on low-order Taylor expansions that may break down for highly non-convex loss landscapes
- Computational overhead of Method 1's Taylor-mode AD could be prohibitive for very deep networks
- Initialization procedure's effectiveness depends on proper data scaling and spectral initialization

## Confidence

- **High Confidence**: The geometric framework using matrix manifolds and exponential maps for normalized networks is mathematically sound and well-established in differential geometry.
- **Medium Confidence**: The experimental results on the quadrotor dataset are promising, but the evaluation is limited to a single domain and specific network architectures, requiring broader validation.
- **Low Confidence**: The computational efficiency claims relative to standard SGD methods need more thorough benchmarking across diverse problem sets and hardware configurations.

## Next Checks

1. **Generalization Testing**: Validate the methods across multiple domains (e.g., image classification, natural language processing) with varying network depths and widths to assess robustness beyond the quadrotor dataset.

2. **Scalability Analysis**: Benchmark the computational overhead of Method 1 (Taylor-mode AD) against standard optimization methods on large-scale datasets and deep networks to quantify the trade-off between automatic stepsize selection and computational cost.

3. **Convergence Robustness**: Systematically test the methods under various initialization conditions and data scaling scenarios to identify failure modes and determine the sensitivity of the algorithms to initialization quality and data preprocessing.