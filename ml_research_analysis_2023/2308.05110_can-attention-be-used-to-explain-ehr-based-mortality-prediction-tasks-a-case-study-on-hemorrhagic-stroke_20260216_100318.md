---
ver: rpa2
title: 'Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case
  Study on Hemorrhagic Stroke'
arxiv_id: '2308.05110'
source_url: https://arxiv.org/abs/2308.05110
tags:
- data
- attention
- prediction
- mortality
- vital
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an attention-based transformer model for
  early mortality prediction in hemorrhagic stroke patients using longitudinal EHR
  data. The model outperforms traditional logistic regression and LSTM-Fusion baselines
  with an AUROC of 0.8487 and AUPRC of 0.9726.
---

# Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke

## Quick Facts
- arXiv ID: 2308.05110
- Source URL: https://arxiv.org/abs/2308.05110
- Reference count: 22
- Primary result: Attention-based transformer model achieves AUROC 0.8487 and AUPRC 0.9726 for 2-7 day mortality prediction in hemorrhagic stroke patients

## Executive Summary
This study introduces an attention-based transformer model for early mortality prediction in hemorrhagic stroke patients using longitudinal EHR data. The model outperforms traditional logistic regression and LSTM-Fusion baselines with an AUROC of 0.8487 and AUPRC of 0.9726. Attention scores serve as intrinsic explanations, effectively identifying key physiological indicators and vital signs, particularly at later time points indicating patient deterioration. The attention-based approach shows superior fidelity compared to SHAP values, accurately identifying both important and non-essential features.

## Method Summary
The model uses a two-stage training approach: first, a self-supervised pre-training phase where a transformer encoder learns temporal patterns through masked future prediction and reconstruction tasks; second, a supervised fine-tuning phase for mortality prediction. The architecture combines time series vital signs (processed by transformer encoder) with aggregated features (processed by MLPs) through a self-attention fusion layer. The model processes 24 hours of vital signs data (7 channels) and 196 aggregated features to predict 2-7 day mortality outcomes.

## Key Results
- Attention-based transformer model achieves AUROC of 0.8487 and AUPRC of 0.9726
- Attention scores effectively identify critical physiological indicators and vital signs
- Attention-based explanations show superior fidelity compared to SHAP values
- Model successfully captures temporal deterioration patterns in later time points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention scores provide intrinsic interpretability by highlighting influential features during prediction.
- Mechanism: The attention mechanism assigns weights to different input features based on their relevance to the prediction task. Higher attention scores indicate greater influence on the model's output.
- Core assumption: Attention scores directly correlate with feature importance and can be trusted as explanations without additional post-hoc methods.
- Evidence anchors:
  - [abstract]: "Attention scores serve as intrinsic explanations, effectively identifying key physiological indicators and vital signs, particularly at later time points indicating patient deterioration."
  - [section]: "In our research, the attention score, extracted from the self-attention module, serves as the explanation method. Thus, we introduce an element of interpretability into the model: features attributed with higher attention scores are considered more influential for the prediction task."
- Break condition: If attention scores do not consistently identify important features across different patients or if they fail to capture non-linear relationships between features.

### Mechanism 2
- Claim: The transformer model effectively captures temporal dependencies in longitudinal EHR data.
- Mechanism: By using self-supervised learning with a reconstruction task, the model learns to embed time series data into a latent space that preserves temporal information. This allows the model to identify patterns over time, such as deterioration in vital signs.
- Core assumption: The temporal patterns in EHR data are sufficiently regular and predictable to be captured by a transformer model.
- Evidence anchors:
  - [abstract]: "The model outperforms traditional logistic regression and LSTM-Fusion baselines with an AUROC of 0.8487 and AUPRC of 0.9726."
  - [section]: "To address the challenges posed by the heterogeneous nature of time series data and the limited availability of training data, we design a training objective that includes two tasks: a predictive task and a reconstruction task."
- Break condition: If the model fails to generalize to patients with atypical temporal patterns or if the reconstruction task does not improve predictive performance.

### Mechanism 3
- Claim: The attention-based model provides superior fidelity compared to SHAP values.
- Mechanism: Fidelity measures how well the explanation method identifies important and non-essential features. The attention-based method directly influences the model's predictions, while SHAP values are computed post-hoc and may not accurately reflect the model's decision-making process.
- Core assumption: Intrinsic explanations (attention scores) are more faithful to the model's internal workings than post-hoc explanations (SHAP values).
- Evidence anchors:
  - [abstract]: "The attention-based approach shows superior fidelity compared to SHAP values, accurately identifying both important and non-essential features."
  - [section]: "Our Attention model outperforms both the Logistic and LSTM-Fusion models in terms of Fidelity+, which signifies the importance of the model's features, across all aspects - AUROC, AUPRC, and Probability."
- Break condition: If attention scores fail to consistently identify important features or if SHAP values provide more accurate explanations in certain scenarios.

## Foundational Learning

- Concept: Attention mechanisms in neural networks
  - Why needed here: Understanding how attention mechanisms work is crucial for interpreting the model's explanations and for designing the architecture.
  - Quick check question: How does the attention mechanism in this model differ from traditional attention mechanisms used in natural language processing?

- Concept: Transformer models and self-supervised learning
  - Why needed here: The model uses a transformer architecture with self-supervised learning to capture temporal dependencies in EHR data. Understanding these concepts is essential for implementing and troubleshooting the model.
  - Quick check question: What are the advantages of using self-supervised learning with a reconstruction task in this context?

- Concept: Fidelity in model explanations
  - Why needed here: Evaluating the quality of explanations is crucial for ensuring that the model's predictions are trustworthy and interpretable. Understanding fidelity metrics helps in comparing different explanation methods.
  - Quick check question: How does the Fidelity+ metric differ from the Fidelity- metric, and why are both important for evaluating explanation quality?

## Architecture Onboarding

- Component map: Time series data processing module -> Mortality prediction module -> Embedding layer for aggregated data features
- Critical path:
  1. Time series data is processed by the transformer encoder to generate embeddings.
  2. Aggregated data features are embedded using MLPs.
  3. The self-attention layer combines the embeddings from both modules.
  4. The feed-forward layer generates the final mortality prediction.
- Design tradeoffs:
  - Using a transformer model allows for capturing complex temporal dependencies but increases computational complexity compared to simpler models like logistic regression.
  - The self-supervised learning approach helps in learning useful representations but requires careful tuning of the reconstruction task.
- Failure signatures:
  - If the model performs poorly on patients with atypical temporal patterns, it may indicate that the transformer is not capturing the necessary temporal dependencies.
  - If the attention scores do not align with clinical intuition about important features, it may suggest that the attention mechanism is not functioning as intended.
- First 3 experiments:
  1. Compare the model's performance with and without the reconstruction task in the self-supervised learning phase.
  2. Evaluate the model's explanations by comparing attention scores with SHAP values for a subset of patients.
  3. Test the model's robustness by introducing synthetic noise into the time series data and observing the impact on predictions and explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the attention-based transformer model perform in predicting mortality for other critical conditions beyond hemorrhagic stroke, such as ischemic stroke or sepsis?
- Basis in paper: [inferred] The paper focuses on hemorrhagic stroke mortality prediction and suggests validating findings in other disease states as a future direction.
- Why unresolved: The study is limited to hemorrhagic stroke patients, and there is no comparative data for other critical conditions.
- What evidence would resolve it: Conducting similar studies using the attention-based transformer model on datasets from other critical conditions like ischemic stroke or sepsis, and comparing the performance metrics (AUROC, AUPRC) with those from the hemorrhagic stroke study.

### Open Question 2
- Question: How do the attention scores compare with other intrinsic explanation methods, such as gradient-based or perturbation-based approaches, in terms of fidelity and interpretability?
- Basis in paper: [explicit] The study compares attention scores with SHAP values but does not explore other intrinsic explanation methods like gradient-based or perturbation-based approaches.
- Why unresolved: The paper only compares attention scores with SHAP values, leaving a gap in understanding how attention scores perform against other intrinsic explanation methods.
- What evidence would resolve it: Implementing and evaluating other intrinsic explanation methods (e.g., gradient-based, perturbation-based) on the same dataset and comparing their fidelity and interpretability scores with attention scores.

### Open Question 3
- Question: What is the impact of different observation and prediction window lengths on the model's performance and interpretability?
- Basis in paper: [inferred] The study uses a 24-hour observation window and a 2-7 day prediction window, but does not explore the effects of varying these lengths.
- Why unresolved: The paper does not investigate how changing the observation and prediction window lengths might affect the model's performance and interpretability.
- What evidence would resolve it: Conducting experiments with different observation and prediction window lengths and analyzing the resulting changes in model performance (AUROC, AUPRC) and interpretability metrics (fidelity scores).

## Limitations

- The model architecture details are not fully specified, particularly regarding transformer encoder configurations and attention mechanism parameters.
- The dataset used (MIMIC-III) is limited to a single institution, raising concerns about generalizability to other healthcare systems.
- The study does not report results on external validation datasets, which would strengthen confidence in the model's performance.

## Confidence

**High Confidence Claims:**
- The attention-based transformer model achieves superior AUROC (0.8487) and AUPRC (0.9726) compared to baseline models.
- Attention scores effectively identify key physiological indicators and vital signs as important features.
- The model provides interpretable explanations through attention weights.

**Medium Confidence Claims:**
- Attention-based explanations show superior fidelity compared to SHAP values.
- The model successfully captures temporal deterioration patterns in vital signs.
- The self-supervised learning approach with reconstruction tasks improves model performance.

**Low Confidence Claims:**
- The generalizability of findings to other EHR systems and patient populations.
- The robustness of attention-based explanations across different clinical scenarios.
- The optimal configuration of model hyperparameters for different healthcare settings.

## Next Checks

1. **External Validation**: Test the model on an independent dataset from a different healthcare system to assess generalizability and performance consistency.

2. **Clinical Expert Review**: Have domain experts evaluate the attention-based explanations against clinical expectations for mortality prediction in hemorrhagic stroke patients.

3. **Ablation Study**: Conduct experiments to determine the individual contributions of the self-supervised learning component and the attention mechanism to overall model performance.