---
ver: rpa2
title: Dataset Distillation with Convexified Implicit Gradients
arxiv_id: '2302.06755'
source_url: https://arxiv.org/abs/2302.06755
tags:
- dataset
- distillation
- implicit
- gradients
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new dataset distillation algorithm called
  Reparameterized Convexified Implicit Gradients (RCIG) that significantly outperforms
  existing methods. RCIG formulates dataset distillation as a bilevel optimization
  problem and uses implicit gradients to compute meta-gradient updates.
---

# Dataset Distillation with Convexified Implicit Gradients

## Quick Facts
- arXiv ID: 2302.06755
- Source URL: https://arxiv.org/abs/2302.06755
- Reference count: 40
- Primary result: New state-of-the-art on 17 out of 22 dataset distillation benchmarks

## Executive Summary
RCIG introduces a novel dataset distillation algorithm that significantly outperforms existing methods by reformulating the problem as a bilevel optimization with implicit gradients. The key innovation lies in a reparameterization technique that analytically computes optimal final-layer parameters given body parameters, combined with a convexification approximation that enables stable implicit gradient computation. The algorithm achieves remarkable improvements across multiple benchmark datasets, including a 108% improvement on resized ImageNet with one image per class.

## Method Summary
RCIG formulates dataset distillation as a bilevel optimization problem where the inner loop optimizes network parameters on a distilled set, and the outer loop optimizes the distilled set itself. The method uses implicit gradients to compute meta-gradients efficiently, avoiding the need to store intermediate optimization states. A critical reparameterization enables analytical computation of final-layer parameters from body parameters using the Moore-Penrose pseudoinverse, while a convexification approximation linearizes the network dynamics via the neural tangent kernel. This combination reduces bias in implicit gradients and stabilizes the optimization process.

## Key Results
- Establishes new state-of-the-art on 17 out of 22 benchmark tasks
- Achieves 108% improvement over previous methods on resized ImageNet with one image per class
- Shows 66% improvement on Tiny-ImageNet and 37% on CIFAR-100
- Demonstrates strong cross-architecture generalization and privacy preservation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Convexification of the inner objective stabilizes implicit gradient computation by ensuring a unique minimum exists.
- **Mechanism:** Linearizes network dynamics via first-order Taylor expansion around initialization, making inner optimization strongly convex with added L2 regularization.
- **Core assumption:** Linearization accurately approximates true neural network behavior, especially in kernel regime or after initial NTK evolution.
- **Break condition:** Network too narrow or training dynamics deviate significantly from NTK regime, making linearization inaccurate.

### Mechanism 2
- **Claim:** Reparameterization to analytically compute optimal final-layer parameters reduces bias from truncated inner optimization.
- **Mechanism:** Analytically solves for optimal final-layer weights using Moore-Penrose pseudoinverse of kernel matrix, while only learning body parameters.
- **Core assumption:** Final-layer optimization problem is convex and can be solved efficiently given body parameters and support set.
- **Break condition:** Hessian becomes ill-conditioned or kernel matrix is singular due to insufficient support set size.

### Mechanism 3
- **Claim:** Using pool of partially trained models to evolve NTK improves linearization accuracy beyond initial frozen kernel.
- **Mechanism:** Maintains set of models trained for limited steps to capture rapid early NTK evolution, providing more accurate linearization for support set optimization.
- **Core assumption:** Neural networks undergo brief but significant NTK evolution early in training, which can be approximated by finite pool of partially trained models.
- **Break condition:** NTK evolution is too complex or pool size is insufficient, limiting approximation benefits.

## Foundational Learning

- **Concept:** Bilevel optimization (outer objective depends on solution to inner optimization)
  - **Why needed here:** Dataset distillation framed as bilevel problem: inner loop optimizes network parameters on distilled set, outer loop optimizes distilled set to maximize performance on full data.
  - **Quick check question:** What is the role of the implicit function theorem in bilevel optimization, and why is it necessary here?

- **Concept:** Implicit gradients (computing gradients through solution of an optimization problem)
  - **Why needed here:** Standard backpropagation through unrolled optimization is too expensive; implicit gradients allow efficient computation of meta-gradients without storing intermediate states.
  - **Quick check question:** How does the implicit gradient formula relate to the Hessian of the inner objective, and why does non-convexity break it?

- **Concept:** Neural Tangent Kernel (NTK) and kernel regime behavior
  - **Why needed here:** Linearization via NTK enables convexification of inner objective, making implicit gradients feasible and stable.
  - **Quick check question:** Under what conditions does a neural network behave as a linear model in its parameters, and how does this relate to the width of the network?

## Architecture Onboarding

- **Component map:** Distilled dataset parameters (images + labels + Platt temperature) -> Model pool (m partially trained networks for NTK evolution) -> Inner optimizer (optimizes body parameters, analytically sets final layer) -> Hessian-inverse vector approximator (stochastic optimization to approximate H⁻¹g) -> Meta-optimizer (updates distilled dataset using direct + implicit gradients)
- **Critical path:** Sample model → optimize inner objective → compute Hessian-inverse vector → compute meta-gradient → update distilled dataset → retrain sampled model
- **Design tradeoffs:**
  - More inner steps → better inner optimization but higher cost and potential overfitting
  - More Hessian-inverse steps → more accurate implicit gradients but higher cost
  - Larger model pool → better NTK approximation but higher memory and computation
- **Failure signatures:**
  - Diverging inner or Hessian-inverse loss → reduce corresponding learning rate
  - Poor distillation accuracy → increase inner/Hessian-inverse steps or adjust regularization
  - Memory overflow → apply subsampling or reduce model pool size
- **First 3 experiments:**
  1. Run RCIG on MNIST 1 image per class with default hyperparameters; verify accuracy > 90%
  2. Vary ninner and nH−1 on CIFAR-10 1 image per class; plot accuracy vs runtime
  3. Test cross-architecture generalization by training RCIG on CIFAR-10 and evaluating on ResNet-18

## Open Questions the Paper Calls Out

- **Open Question 1:** How can RCIG's performance on datasets with many classes and few training samples (like CUB-200) be improved to avoid overfitting?
  - **Basis in paper:** Authors note RCIG tends to overfit on datasets with fewer training samples, as observed in CUB-200 where they achieved 100% training accuracy with 10 images per class.
  - **Why unresolved:** Paper mentions data augmentation during dataset distillation could help but was not applied to keep method simple; specific techniques and their effectiveness not explored.
  - **What evidence would resolve it:** Comparative experiments applying various data augmentation strategies during distillation on datasets like CUB-200, measuring both training and test performance.

- **Open Question 2:** What is the impact of different normalization layers (BatchNorm, InstanceNorm, etc.) during training on RCIG's distillation performance and cross-architecture generalization?
  - **Basis in paper:** Authors observed that using BatchNorm during training improved cross-architecture generalization, but did not investigate normalization's role further or compare with other normalization types.
  - **Why unresolved:** Paper only briefly mentions positive effect of BatchNorm and suggests future work should investigate normalization's role further, but does not provide systematic comparison.
  - **What evidence would resolve it:** Experiments comparing RCIG with various normalization layers during training on multiple datasets, evaluating both distillation performance and cross-architecture transfer accuracy.

- **Open Question 3:** Can memory requirements of RCIG be further reduced by eliminating need for full forward pass on entire distilled dataset?
  - **Basis in paper:** Authors discuss subsampling technique requiring full forward pass but only partial backward pass to reduce memory, noting this still requires storing full computation graph for forward pass.
  - **Why unresolved:** While paper presents this memory-saving technique, it acknowledges it's still limited and suggests future work could look at removing requirement for full forward pass.
  - **What evidence would resolve it:** Development and demonstration of variant of RCIG that can compute gradients without full forward pass, comparing memory usage and performance to original method.

## Limitations
- Poor performance on datasets with many classes and few images per class due to overfitting tendencies
- Memory requirements become prohibitive for very large datasets like ImageNet without subsampling techniques
- Sensitivity to hyperparameters like regularization coefficient λ and number of Hessian-inverse steps

## Confidence
- **High Confidence:** Core mathematical formulation of RCIG including reparameterization and convexification techniques is well-established and theoretically sound; reported performance improvements on standard benchmarks are reproducible under controlled conditions
- **Medium Confidence:** 108% improvement claim on resized ImageNet relies on single experiment with limited ablation studies; cross-architecture generalization results based on relatively small set of architectures
- **Low Confidence:** Privacy preservation claims based on single membership inference attack framework; scalability to datasets significantly larger than ImageNet remains unverified

## Next Checks
1. **Ablation Study on Hessian-Inverse Steps:** Systematically vary nH−1 on CIFAR-10 with 1 image per class to determine minimum steps required for optimal performance, measuring both accuracy and computational cost
2. **Cross-Dataset Generalization Test:** Train RCIG on CIFAR-10 and evaluate on both ResNet-18 and different architecture (e.g., EfficientNet-B0) to verify robustness of distilled datasets across architectural families
3. **Privacy Analysis Expansion:** Conduct membership inference attacks using multiple attack frameworks (shadow model training, gradient-based attacks) on distilled datasets to comprehensively assess privacy preservation capabilities