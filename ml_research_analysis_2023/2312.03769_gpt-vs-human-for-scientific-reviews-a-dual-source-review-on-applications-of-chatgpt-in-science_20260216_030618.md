---
ver: rpa2
title: 'GPT vs Human for Scientific Reviews: A Dual Source Review on Applications
  of ChatGPT in Science'
arxiv_id: '2312.03769'
source_url: https://arxiv.org/abs/2312.03769
tags:
- scispace
- responses
- human
- page
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of SciSpace, an LLM-based
  tool, against a human reviewer in assessing scientific papers on ChatGPT applications
  across various fields. SciSpace and a human reviewer answered questions on 13 papers,
  with responses evaluated by GPT-3.5, a crowd panel, and GPT-4.
---

# GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science

## Quick Facts
- arXiv ID: 2312.03769
- Source URL: https://arxiv.org/abs/2312.03769
- Reference count: 40
- Primary result: SciSpace (LLM) vs human reviewer: 50% agreement on objective questions; GPT-4 favors human accuracy, SciSpace structure/completeness; crowd panel prefers human for subjective questions.

## Executive Summary
This study evaluates SciSpace, an LLM-based tool, against a human reviewer in assessing scientific papers on ChatGPT applications across various fields. SciSpace and a human reviewer answered questions on 13 papers, with responses evaluated by GPT-3.5, a crowd panel, and GPT-4. Results show 50% agreement between SciSpace and the human reviewer on objective questions, with GPT-4 favoring the human reviewer for accuracy and SciSpace for structure and completeness. For subjective questions, the crowd panel preferred human responses, while GPT-4 rated both equally for accuracy and structure but favored SciSpace for completeness. SciSpace excels in speed and broad question handling but struggles with clarity, structural comprehension, and interpreting graphical data.

## Method Summary
The study compares SciSpace (GPT-3 based) with a human reviewer on 13 GPT-related scientific papers across Medicine, ML, Engineering, and Geography domains. Both reviewers independently answer 5-6 structured questions per paper focusing on objectives, methods, and findings. Responses are evaluated by three distinct assessors: GPT-3.5, a crowd panel of 25 researchers, and GPT-4 with access to the original papers. Evaluations rate responses on accuracy, structure & clarity, and completeness. The study employs both informed evaluators (with paper access) and uninformed evaluators to provide comprehensive performance assessment.

## Key Results
- SciSpace and human reviewer achieved 50% agreement on objective questions
- GPT-4 rated human reviewer higher in accuracy but SciSpace higher in structure and completeness
- Crowd panel preferred human responses for subjective questions while GPT-4 rated both equally for accuracy and structure but favored SciSpace for completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SciSpace and human reviewers achieve similar overall accuracy when evaluated by informed GPT-4, but SciSpace excels in structure, clarity, and completeness.
- Mechanism: SciSpace leverages GPT-3 as a foundation to rapidly parse and summarize scientific papers, while the human reviewer applies domain expertise and critical reasoning. The informed evaluator (GPT-4) can cross-reference responses against the actual paper content, revealing that SciSpace's structured, comprehensive answers compensate for its occasional factual inaccuracies.
- Core assumption: GPT-4's evaluation criteria (accuracy, structure & clarity, completeness) are valid proxies for real-world review quality, and the paper's structure and graphical elements are important factors in determining review effectiveness.
- Evidence anchors:
  - [abstract] "GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness."
  - [section] "In the case of subjective questions, the different evaluators prefer the responses by SciSpace slightly more than, if not equally as, the human response."
  - [corpus] Weak - related papers focus on AI-generated reviews but don't directly address SciSpace's specific performance characteristics.
- Break condition: If the informed evaluator's access to paper content is limited or biased, or if the evaluation criteria don't align with actual review quality standards.

### Mechanism 2
- Claim: SciSpace's inability to interpret graphical data and complex model structures leads to less precise and less complete responses.
- Mechanism: SciSpace relies on text-based processing and cannot extract information from figures, diagrams, or complex visual representations. This limitation results in responses that lack depth and miss crucial details, particularly when questions require understanding of model architectures or workflows.
- Core assumption: Graphical information in scientific papers often contains essential details that are not fully captured in the text, and SciSpace's inability to process this information significantly impacts its review quality.
- Evidence anchors:
  - [section] "SciSpace's performance is hindered by its lack of capability to interpret graphical data, leading to significant oversights in its responses."
  - [section] "SciSpace's textual summaries alone are insufficient for capturing the intricate details and nuances often communicated through these visual representations."
  - [corpus] Weak - related papers focus on AI-generated reviews but don't directly address the importance of graphical data interpretation in scientific reviews.
- Break condition: If the importance of graphical information in scientific papers decreases or if alternative methods are developed to extract information from figures without direct visual processing.

### Mechanism 3
- Claim: SciSpace's responses are often repetitive, verbose, and lack coherent structure, leading to lower clarity and effectiveness.
- Mechanism: SciSpace generates responses in bullet-point format, but these points often overlap, lack interconnection, and fail to follow a logical progression. This results in verbose, redundant answers that are harder to read and understand compared to more concise human responses.
- Core assumption: Effective communication in scientific reviews requires clear, concise, and logically structured responses, and SciSpace's current output format fails to meet these criteria consistently.
- Evidence anchors:
  - [section] "SciSpace's responses are typically presented in bullet points... However, in SciSpace's responses, the bullet points often lack interconnection. They tend not to follow a parallel or progressive format, impacting the overall coherence of the response."
  - [section] "The verbosity in SciSpace's responses makes the answers harder to read than the paper's abstract, counteracting SciSpace's goal of simplifying information comprehension."
  - [corpus] Weak - related papers focus on AI-generated reviews but don't directly address the importance of response structure and clarity in scientific reviews.
- Break condition: If SciSpace's output format is improved to ensure parallel or progressive structure in bullet points, or if evaluation criteria are adjusted to prioritize completeness over clarity.

## Foundational Learning

- Concept: Chain-of-Thought reasoning in large language models
  - Why needed here: Understanding how SciSpace and other LLM-based tools use iterative questioning and reasoning to improve their responses is crucial for interpreting the study's findings on their performance.
  - Quick check question: How does the Chain-of-Thought model format (Thought, Action, Action Input, Observation) contribute to the effectiveness of LLM-based scientific review tools?

- Concept: Domain-specific tool integration with LLMs
  - Why needed here: The study discusses how combining GPT models with domain-specific tools can enhance their performance in scientific disciplines. Understanding this integration is key to interpreting the results and limitations of SciSpace.
  - Quick check question: What are the potential benefits and challenges of integrating external databases or specialized tools with large language models for scientific review tasks?

- Concept: Evaluation methodology for AI-generated scientific reviews
  - Why needed here: The study employs a unique evaluation approach using informed and uninformed evaluators. Understanding this methodology is crucial for interpreting the results and drawing meaningful conclusions about the performance of SciSpace and human reviewers.
  - Quick check question: How do the roles of informed evaluators (with access to paper content) and uninformed evaluators (without access) differ in assessing the quality of AI-generated scientific reviews?

## Architecture Onboarding

- Component map: Scientific papers (PDFs) -> SciSpace and human reviewer -> GPT-3.5, crowd panel, GPT-4 evaluations -> Comparative analysis
- Critical path:
  1. Paper selection and question formulation
  2. Independent review by SciSpace and human reviewer
  3. Evaluation by three distinct evaluators
  4. Data aggregation and analysis
  5. Interpretation of results and identification of limitations
- Design tradeoffs:
  - Speed vs. accuracy: SciSpace offers rapid review but may sacrifice some accuracy
  - Structure vs. flexibility: SciSpace provides structured responses but may lack adaptability
  - Completeness vs. clarity: SciSpace tends to provide longer, more complete answers but at the cost of clarity
- Failure signatures:
  - Inability to interpret graphical data
  - Repetitive and verbose responses
  - Lack of coherent structure in bullet-point answers
  - Sensitivity to question phrasing
- First 3 experiments:
  1. Test SciSpace's ability to extract information from papers with complex graphical content
  2. Compare the effectiveness of different response formats (bullet points vs. prose) in conveying scientific information
  3. Evaluate the impact of question phrasing on SciSpace's response quality and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs achieve the same level of accuracy and depth in scientific literature reviews as human experts, particularly in specialized fields?
- Basis in paper: [explicit] The paper discusses the potential of LLMs in scientific literature reviews but also highlights their limitations in understanding complex methodologies, evaluating innovative claims, and assessing ethical issues.
- Why unresolved: The study provides a partial comparison between SciSpace (an LLM) and a human reviewer, but it does not definitively conclude whether LLMs can match human experts' performance across all aspects of scientific literature reviews.
- What evidence would resolve it: A comprehensive, large-scale study comparing the performance of various LLMs and human experts across multiple scientific disciplines and types of literature reviews.

### Open Question 2
- Question: How can LLMs be improved to better interpret and analyze graphical data, such as figures and charts, in scientific papers?
- Basis in paper: [explicit] The paper mentions that SciSpace struggles with interpreting graphical information, which limits its ability to fully comprehend and summarize scientific papers.
- Why unresolved: The study does not provide specific solutions or methods for enhancing LLMs' capabilities in processing and understanding graphical data.
- What evidence would resolve it: Development and testing of new techniques or models that enable LLMs to effectively interpret and analyze graphical data in scientific papers, followed by evaluation of their performance.

### Open Question 3
- Question: What are the most effective ways to integrate domain-specific tools and databases with LLMs to improve their performance in scientific research?
- Basis in paper: [explicit] The paper discusses various strategies for integrating LLMs with domain-specific tools and databases, such as iterative questioning, chain-of-thought workflows, and external databases.
- Why unresolved: The study provides examples of these integrations but does not compare their effectiveness or identify the most promising approaches.
- What evidence would resolve it: Comparative studies evaluating the performance of LLMs integrated with different domain-specific tools and databases across various scientific disciplines.

## Limitations

- Limited sample size of 13 papers may not be representative of broader scientific literature
- Potential bias in evaluation process due to unconfirmed expertise of crowd panel members
- Question phrasing impact on SciSpace's performance not systematically accounted for
- Evaluation criteria may not fully capture the importance of graphical data interpretation in scientific reviews

## Confidence

- High Confidence: SciSpace's superior speed and ability to handle a broad range of questions are well-supported by the study's results and align with the known capabilities of LLM-based tools.
- Medium Confidence: The study's findings on SciSpace's struggles with clarity, structural comprehension, and graphical data interpretation are reasonably supported, but could benefit from additional validation across a wider range of papers and question types.
- Low Confidence: The comparative performance of SciSpace and human reviewers on subjective questions, as evaluated by the crowd panel and GPT-4, is less certain due to the limited sample size and potential evaluator bias.

## Next Checks

1. Conduct a follow-up study with a larger and more diverse set of scientific papers, including those with complex graphical content, to further validate SciSpace's performance across different domains and question types.

2. Implement a standardized question phrasing protocol to minimize the impact of question interpretation on SciSpace's responses and ensure fair comparison with human reviewers.

3. Develop and test alternative evaluation criteria that place greater emphasis on the importance of graphical data interpretation in scientific reviews, to better assess the true impact of this limitation on SciSpace's performance.