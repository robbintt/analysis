---
ver: rpa2
title: Training Transitive and Commutative Multimodal Transformers with LoReTTa
arxiv_id: '2305.14243'
source_url: https://arxiv.org/abs/2305.14243
tags:
- modalities
- modality
- learning
- loretta
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LoReTTa addresses the problem of training multimodal models when
  certain modality combinations are entirely missing from the training data. The core
  method uses transitive pre-training combined with commutative and causal modeling
  to learn conditional distributions between modalities, allowing models to handle
  previously unseen modality pairs at inference time.
---

# Training Transitive and Commutative Multimodal Transformers with LoReTTa

## Quick Facts
- **arXiv ID**: 2305.14243
- **Source URL**: https://arxiv.org/abs/2305.14243
- **Reference count**: 40
- **Key outcome**: LoReTTa achieves 5.04 perplexity on unseen modality pairs vs 104.27 for baselines on synthetic data, and 0.660 C-index on unseen pairs in medical data

## Executive Summary
LoReTTa addresses the challenge of training multimodal transformers when certain modality combinations are missing from training data. The method uses transitive pre-training combined with commutative and causal modeling to learn conditional distributions between modalities, enabling the model to handle previously unseen modality pairs at inference time. On both synthetic and real-world datasets, LoReTTa significantly outperforms baseline methods including BERT, GPT, CLIP, and C2M3, achieving lower perplexity and higher classification accuracy on never-seen modality combinations.

## Method Summary
LoReTTa uses a transformer decoder architecture with causal masked modeling, where masked tokens are moved to the sequence end to maintain next-token prediction while incorporating bidirectional context. The model employs commutative modeling by randomly mixing modality order during training to learn bidirectional transitions, and transitive modeling to bridge missing modality combinations by using linking modalities as intermediaries. Training involves AdamW optimizer with learning rate 6e-4, weight decay 0.1, and gradient clipping at 1. The approach can handle any number of modalities and combinations, though experiments focus on three-modality scenarios.

## Key Results
- Achieved 5.04 perplexity on unseen modality pairs in synthetic SVL-MNIST dataset compared to 104.27 for baseline methods
- Improved classification accuracy by up to 15% on previously unseen modality combinations
- Achieved highest C-index of 0.660 on unseen modality pairs and 0.657 on triplet combinations in TCGA-OMICS medical dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoReTTa enables training on disjoint modality pairs by using transitive modeling to bridge missing modality combinations.
- Mechanism: Given datasets (A,B) and (B,C), the model learns to predict C from B, then uses B to reconstruct pseudo samples of C, which are used to train the missing A↔C relationship.
- Core assumption: The linking modality B shares sufficient statistical alignment with both A and C to enable meaningful pseudo generation.
- Evidence anchors:
  - [abstract] "Given a dataset containing only the disjoint combinations (A, B) and (B, C), LoReTTa can model the relation A <-> C with A <-> B <-> C."
  - [section] "In transitive modeling, we randomly select a sample and use the linking modality to predict the missing modality, which is then used to reconstruct the existing modality, ensuring alignment across all modalities."
  - [corpus] Weak corpus evidence - no direct matches found for transitive multimodal modeling in the neighbor papers.
- Break condition: If the linking modality B has poor alignment with either A or C (e.g., low perplexity under each other's distributions), transitive bridging fails.

### Mechanism 2
- Claim: Commutative modeling allows the model to learn bidirectional transitions within modality pairs.
- Mechanism: During training, sequences (A,B) and (B,A) are both used, enabling the model to learn both A→B and B→A transitions in a single architecture.
- Core assumption: Modality order does not affect the underlying joint probability distribution.
- Evidence anchors:
  - [abstract] "Our self-supervised framework unifies causal modeling and masked modeling with the rules of commutativity and transitivity."
  - [section] "We ensure commutativity by randomly mixing the order of concatenation when appending tokens from two modalities to an input sequence."
  - [corpus] No direct corpus evidence found for commutative modeling in multimodal transformers.
- Break condition: If modality order encodes meaningful information (e.g., temporal sequence), commutative mixing would corrupt the learning signal.

### Mechanism 3
- Claim: Causal masked modeling combines bidirectional context with autoregressive generation capabilities.
- Mechanism: Masked tokens are moved to the end of the sequence, allowing the model to use right-side context while maintaining the next-token prediction objective.
- Core assumption: Moving masked spans to sequence end preserves sufficient context for accurate reconstruction.
- Evidence anchors:
  - [section] "Since generative pre-trained transformers only consider information from the left and ignore information from the right, they are known to lack bidirectional context... we use causal masked modeling... mitigating these problems."
  - [abstract] "LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy) to address this understudied problem. Our self-supervised framework unifies causal modeling and masked modeling..."
  - [corpus] No direct corpus evidence found for causal masked modeling in multimodal transformers.
- Break condition: If the masked span is too long or contains too many tokens, the model may not have sufficient context to accurately reconstruct it.

## Foundational Learning

- Concept: Modality alignment via perplexity comparison
  - Why needed here: Determines whether two datasets can be considered the same modality or need separate treatment
  - Quick check question: Given two datasets X and Y, if PPL(X|Y) > σ²·PPL(Y|Y), are they different modalities?

- Concept: Conditional probability factorization
  - Why needed here: Underlies the mathematical foundation for transitive modeling
  - Quick check question: If we have (A,B) and (B,C), what factorization allows us to approximate P(A,C)?

- Concept: Chain rule of probability
  - Why needed here: Enables the autoregressive next-token prediction framework
  - Quick check question: How does the chain rule justify the next-token prediction objective in transformers?

## Architecture Onboarding

- Component map: Tokenizer → Embedding layer → Transformer decoder (8 layers, 8 heads) → Output projection
- Critical path: Input tokens → Positional embeddings → Transformer layers → Next-token logits
- Design tradeoffs: Using 8-layer transformer balances expressivity with computational efficiency; RMSNorm vs LayerNorm affects training stability
- Failure signatures: High perplexity on seen pairs indicates tokenization/embedding issues; poor performance on unseen pairs suggests broken transitive linking
- First 3 experiments:
  1. Train LoReTTa on (I,T) and (T,W) pairs, evaluate perplexity on unseen (I,W) pair
  2. Compare LoReTTa vs C2M3 on classification accuracy for unseen modality combinations
  3. Test LoReTTa with different linking modalities to identify robustness boundaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LoReTTa's performance scale with the number of modalities beyond three? Would the transitive pre-training approach remain effective for higher-dimensional multimodal problems?
- Basis in paper: [explicit] The paper mentions that while only three modalities were evaluated, LoReTTa "can be used with any number and combination of modalities" and provides theoretical analysis suggesting effectiveness for longer chains
- Why unresolved: The experiments only tested three modalities (speech, vision, language in synthetic data; mRNA, miRNA, RPPA in medical data). No experiments were conducted with four or more modalities to empirically validate the theoretical claims.
- What evidence would resolve it: Empirical results showing LoReTTa's performance on datasets with four or more modalities, comparing perplexity and classification accuracy against baseline methods.

### Open Question 2
- Question: How sensitive is LoReTTa to the choice of linking modality in the transitive pre-training process? Does the selection of which modality serves as the "bridge" significantly impact final performance?
- Basis in paper: [inferred] The paper states "LoReTTa is able to learn the transition (Xi → ... → Xj), ..., (Xi → ... → Xk)" and uses mRNA as the linking modality in TCGA experiments, but does not systematically explore the impact of different linking modality choices
- Why unresolved: The paper only uses one linking modality (mRNA) in the real-world medical dataset experiment and does not compare performance across different choices of linking modalities or analyze sensitivity to this choice.
- What evidence would resolve it: Comparative experiments where different modalities are chosen as the linking modality in the same dataset, measuring performance differences in terms of perplexity, classification accuracy, and downstream task performance.

### Open Question 3
- Question: What is the exact mechanism by which errors propagate and are bounded during transitive pre-training, and under what conditions might this error bounding fail?
- Basis in paper: [explicit] Section 4 provides theoretical analysis with error propagation formulas showing "e1,3 := Jf (x3|f (x2|x1))Jf (x2|x1)e1,2 + e2,3" and states "Since we are minimizing the reconstruction error of x1 given ˆx3 = x3 + e1,3 during backpropagation, the error terms e1,2 and e2,3 must be bounded"
- Why unresolved: The theoretical analysis provides a mathematical framework but does not empirically validate the error bounds or identify conditions under which the error bounding mechanism might break down (e.g., very long chains, highly dissimilar modalities, etc.)
- What evidence would resolve it: Empirical measurements of error propagation at each step of the transitive chain, validation of the theoretical error bounds, and identification of conditions (modality types, chain length, etc.) where error accumulation becomes problematic.

## Limitations
- Experimental scope limited to three modalities despite theoretical claims about scalability
- Synthetic dataset may not fully capture real-world multimodal complexity
- Medical dataset represents only three specific modalities with potentially unique characteristics

## Confidence
- **High Confidence**: Mathematical framework of commutative and transitive modeling is well-specified and internally consistent
- **Medium Confidence**: Experimental results show significant improvements over baselines, but comparison set is relatively small
- **Low Confidence**: Generalizability to more than three modalities or different domain types remains unclear

## Next Checks
1. Cross-modal perplexity alignment test: For each pair of modalities, compute perplexity under each other's distributions. If PPL(A|B) > σ²·PPL(B|B), the model should struggle with transitive bridging. Validate this prediction experimentally.

2. Ablation on commutative modeling: Train variants with and without commutative mixing to quantify its contribution to performance. Measure whether bidirectional learning provides measurable gains beyond unidirectional training.

3. Scalability test with additional modalities: Extend the synthetic dataset to include 4-5 modalities and evaluate whether transitive modeling maintains effectiveness as the modality graph becomes more complex.