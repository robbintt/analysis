---
ver: rpa2
title: 'Large Language Models for Semantic Monitoring of Corporate Disclosures: A
  Case Study on Korea''s Top 50 KOSPI Companies'
arxiv_id: '2309.00208'
source_url: https://arxiv.org/abs/2309.00208
tags:
- gpt-4
- financial
- disclosures
- condition
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of GPT-3.5-turbo and GPT-4 for
  sentiment analysis of Korean corporate disclosures. The models were used to rate
  815 disclosure summaries from the top 50 KOSPI companies over 17 months, with ratings
  compared to those from human experts.
---

# Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea's Top 50 KOSPI Companies

## Quick Facts
- arXiv ID: 2309.00208
- Source URL: https://arxiv.org/abs/2309.00208
- Reference count: 15
- Large language models GPT-3.5-turbo and GPT-4 were used to rate 815 Korean corporate disclosure summaries; GPT-4 showed higher concordance and correlation with human experts.

## Executive Summary
This study evaluates the performance of GPT-3.5-turbo and GPT-4 for sentiment analysis of Korean corporate disclosures from the top 50 KOSPI companies. The models were used to rate 815 disclosure summaries over 17 months, with ratings compared to those from human experts. GPT-4 demonstrated superior performance with a concordance rate of 0.82 and Spearman correlation of 0.61 when a rating adjustment condition was applied. While both models showed capability in analyzing sentiment, GPT-4 was more consistent and accurate. Limitations include the lack of background knowledge and inability to integrate external data sources.

## Method Summary
The study used GPT-3.5-turbo and GPT-4 to rate Korean corporate disclosure summaries translated to English. Disclosures were collected monthly (limited to 15 per company) from the top 50 KOSPI companies over 17 months. Each disclosure was summarized by GPT-3.5, translated to English, and rated on a 1-5 scale by both models using provided criteria. Ratings were compared to those from two human experts using concordance rate, Spearman correlation, and Kendall correlation. Rating adjustment conditions were applied to align model outputs with human expert distributions.

## Key Results
- GPT-4 outperformed GPT-3.5-turbo in concordance (0.82 vs. lower) and Spearman correlation (0.61 vs. lower) when rating adjustments were applied.
- GPT-4 demonstrated higher correlations than GPT-3.5 in all conditions without adjustment.
- Rating adjustments exposed systematic model bias but could also mask true performance if misapplied.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5-turbo in sentiment consistency when a rating adjustment is applied.
- Mechanism: The study applies a conditional rating adjustment where scores ≥4 are decremented by 1 and scores ≤2 are incremented by 1. This correction aligns model outputs closer to the skewed human distribution (75% rated neutral), improving correlation.
- Core assumption: Human raters exhibit a strong neutral bias, and model outputs are systematically skewed toward extremes.
- Evidence anchors:
  - [abstract] "GPT-4 showed superior performance with a concordance rate of 0.82 and a Spearman correlation coefficient of 0.61 when a rating adjustment condition was applied."
  - [section 3.2] "The GPT-4 model in Condition 2 shows the highest performance across all settings for every measure."
  - [corpus] Weak correlation; no direct match to the rating adjustment condition in related papers.
- Break condition: If human raters' score distribution shifts (e.g., more extremes), the adjustment would overcompensate and reduce model accuracy.

### Mechanism 2
- Claim: GPT-4's higher performance stems from better handling of domain-specific financial language.
- Mechanism: GPT-4's architecture and training data include more diverse and recent financial texts, enabling it to better parse complex sentiment cues in corporate disclosures.
- Core assumption: Model capability differences are significant enough to affect task-specific accuracy in specialized domains.
- Evidence anchors:
  - [section 3.2] "The GPT-4 model demonstrates higher correlations than GPT-3.5 in all conditions."
  - [abstract] "GPT-4 showed superior performance... The Spearman correlation coefficient was registered at 0.61, while the simple concordance rate was recorded at 0.82."
  - [corpus] Indirect support: ESGReveal and Climate AI papers show LLMs' increasing use in domain-specific text extraction, implying model improvements help.
- Break condition: If financial language shifts or if models become equally exposed to domain texts, the advantage may vanish.

### Mechanism 3
- Claim: Rating adjustments expose systematic model bias but can also mask true performance if applied incorrectly.
- Mechanism: Adjusting ratings in one direction (e.g., subtracting from high scores) corrects overestimation bias but introduces artificial constraints that may reduce sensitivity to true sentiment extremes.
- Core assumption: Model bias is directional and consistent across samples.
- Evidence anchors:
  - [section 3.2] "The effects of rating adjustments are clearly observed in correlation measures."
  - [section 4] "Interestingly, this tendency is not symmetrical... Conditions 4 and 5 yielded suboptimal performance."
  - [corpus] Weak; no direct study of bias correction in sentiment scoring.
- Break condition: If bias direction changes per context, a single adjustment rule will degrade rather than improve accuracy.

## Foundational Learning

- Concept: Spearman correlation coefficient
  - Why needed here: It measures rank correlation between human and model ratings, critical for assessing ordinal sentiment alignment.
  - Quick check question: What does a Spearman coefficient of 0.61 imply about the agreement between model and human rankings?

- Concept: Concordance rate
  - Why needed here: It counts exact score matches, showing raw alignment without considering order.
  - Quick check question: Why might a high concordance rate coexist with a lower Spearman coefficient?

- Concept: Cohen's Kappa statistic
  - Why needed here: It quantifies inter-rater reliability among human experts, establishing baseline consistency.
  - Quick check question: What does a Kappa of 0.352 indicate about human agreement in this study?

## Architecture Onboarding

- Component map:
  Data ingestion pipeline (KIND API → summarization preprocessing) → Prompt construction module (rating criteria → English translation) → Model inference layer (GPT-3.5-turbo and GPT-4 API calls) → Post-processing adjustment module (conditional score corrections) → Evaluation engine (correlation/Kappa calculation)

- Critical path:
  1. Fetch monthly disclosures (limit 15 per company).
  2. Summarize each disclosure via GPT-3.5.
  3. Translate summaries to English.
  4. Generate sentiment ratings using both GPT models.
  5. Apply rating adjustment conditions.
  6. Compare results to human expert ratings.

- Design tradeoffs:
  - Limiting to 15 disclosures/month avoids context truncation but may miss relevant signals.
  - Translating to English for model input risks nuance loss in Korean sentiment.
  - Applying uniform rating adjustments may overcorrect for some companies.

- Failure signatures:
  - Sharp drops in concordance when rating adjustment is applied (suggests overcompensation).
  - Low Spearman with high concordance (indicates mismatched rank ordering).
  - Large variance in inter-rater Kappa across companies (suggests inconsistent expert calibration).

- First 3 experiments:
  1. Run both GPT models without any rating adjustment; record baseline concordance and Spearman.
  2. Apply Condition 2 adjustment; verify if concordance increases without sacrificing Spearman.
  3. Randomly shuffle disclosure order; confirm model stability against sequence effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating company-specific contextual information and external financial data sources impact the accuracy of sentiment analysis in corporate disclosures using large language models?
- Basis in paper: [inferred] from "Limitations and Challenges" section discussing lack of background knowledge and absence of external data sources
- Why unresolved: The study acknowledges these limitations but doesn't test solutions for incorporating contextual information or external data
- What evidence would resolve it: Experiments comparing model performance with and without integration of company-specific context and external financial data sources

### Open Question 2
- Question: Can the observed bias in GPT models toward overestimating positive tones in corporate disclosures be mitigated through prompt engineering or model fine-tuning?
- Basis in paper: [explicit] from "Rating Adjustments" section discussing GPT models' tendency to overestimate positive disclosures
- Why unresolved: The study only tested simple rating adjustments, not alternative approaches to address the bias
- What evidence would resolve it: Comparative analysis of different prompt engineering techniques or fine-tuning approaches to reduce positive bias in sentiment analysis

### Open Question 3
- Question: How does the performance of large language models in sentiment analysis vary across different industries or types of corporate disclosures?
- Basis in paper: [inferred] from individual company results showing varying concordance rates and correlation indices
- Why unresolved: The study provides company-level results but doesn't analyze patterns across industries or disclosure types
- What evidence would resolve it: Systematic analysis of model performance across different industry sectors and types of corporate disclosures, identifying specific strengths and weaknesses for each category

## Limitations
- The study focuses solely on sentiment analysis of disclosure texts without integrating broader market or contextual data, limiting comprehensive sentiment understanding.
- Reliance on English translations of Korean text may introduce semantic drift and loss of nuanced sentiment cues.
- The study does not address model calibration over time or test robustness to varying disclosure volumes and sentiment distributions.

## Confidence
- High confidence: GPT-4 outperforms GPT-3.5-turbo in concordance and Spearman correlation under rating adjustment (well-supported by reported metrics).
- Medium confidence: Rating adjustments improve alignment with human experts (supported, but mechanism relies on assumed human bias).
- Low confidence: Claims about models' domain-specific financial language handling (not directly tested; inferred from performance).

## Next Checks
1. Re-run sentiment scoring without rating adjustment to establish baseline performance and test if correlation improvements are robust.
2. Conduct inter-annotator agreement tests with additional human raters to confirm stability of the human benchmark.
3. Translate a subset of summaries back to Korean and re-score to assess semantic drift from translation.