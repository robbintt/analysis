---
ver: rpa2
title: Multimodal Shannon Game with Images
arxiv_id: '2303.11192'
source_url: https://arxiv.org/abs/2303.11192
tags:
- labels
- image
- word
- language
- dence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the Shannon game to a multimodal setting, using
  human participants and a language model (GPT-2) to predict the next word in a sentence,
  optionally given image information. The experiment shows that the addition of image
  information improves both self-reported confidence and accuracy for both humans
  and the language model.
---

# Multimodal Shannon Game with Images

## Quick Facts
- arXiv ID: 2303.11192
- Source URL: https://arxiv.org/abs/2303.11192
- Reference count: 28
- Key outcome: This paper extends the Shannon game to a multimodal setting, using human participants and a language model (GPT-2) to predict the next word in a sentence, optionally given image information. The experiment shows that the addition of image information improves both self-reported confidence and accuracy for both humans and the language model. Certain word classes, such as nouns and determiners, benefit more from the additional modality information. The priming effect in both humans and the language model becomes more apparent as the context size (extra modality information + sentence context) increases. These findings highlight the potential of multimodal information in improving language understanding and modeling.

## Executive Summary
This paper extends the classic Shannon game to a multimodal setting, where participants predict the next word in a sentence, optionally given image information. The experiment involves both human participants and a language model (GPT-2), comparing their performance across different configurations of visual context. The study finds that multimodal information improves both confidence and accuracy in next-word prediction, with certain word classes benefiting more from the additional modality. The results also suggest a connection between semantic priming in humans and prompting in language models, with the effect becoming stronger as context size increases.

## Method Summary
The experiment uses 17 English sentences of length 8-15 words, presented to participants through a web application with five different modality configurations: no image, original image, labels all, labels crop, and labels text. Participants predict the next word and rate their confidence, then self-evaluate their accuracy after seeing the actual word. The study involves 24 non-native English speakers aged 24-40 with advanced language proficiency. Results are compared against GPT-2 predictions to analyze human-LM correlations across conditions.

## Key Results
- Addition of image information improves both self-reported confidence and accuracy for both humans and the language model
- Certain word classes, such as nouns and determiners, benefit more from multimodal information
- The priming effect becomes more apparent as context size (extra modality information + sentence context) increases for both humans and the language model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal information improves confidence and accuracy in next-word prediction for both humans and language models.
- Mechanism: The addition of visual context provides complementary semantic information that resolves ambiguity and guides prediction toward more probable words.
- Core assumption: Visual information is semantically relevant to the sentence content and can disambiguate linguistic uncertainty.
- Evidence anchors:
  - [abstract] "the addition of image information improves both self-reported confidence and accuracy for both humans and LM"
  - [section 5.1] "The original configuration (where the entire image was shown to the participants) yielded both the highest confidence and self-evaluation scores while no image configuration the lowest."
  - [corpus] Weak evidence - corpus shows related multimodal work but no direct citation of confidence/accuracy improvements.
- Break condition: If visual information is semantically irrelevant or misleading, confidence and accuracy may decrease rather than increase.

### Mechanism 2
- Claim: Certain word classes benefit more from multimodal information, particularly nouns and determiners.
- Mechanism: Nouns are concrete entities that can be visually represented, making them easier to predict when images are present. Determiners benefit from visual disambiguation of grammatical number.
- Core assumption: The visual modality provides sufficient information to identify objects (nouns) and their quantities (affecting determiners).
- Evidence anchors:
  - [abstract] "Certain word classes, such as nouns and determiners, benefit more from the additional modality information"
  - [section 5.3] "the users performed systematically better on determiners than other POS, like nouns" and "For the nouns, we see the labels text configuration yields the worst confidence and accuracy score"
  - [corpus] Weak evidence - corpus mentions multimodal information extraction but not specific POS effects.
- Break condition: If the visual modality cannot reliably identify objects or quantities, the benefit for these word classes disappears.

### Mechanism 3
- Claim: The priming effect becomes more apparent as context size increases, for both humans and language models.
- Mechanism: Additional visual information acts as a priming stimulus that facilitates processing of semantically related words, with the effect strengthening as more contextual information is available.
- Core assumption: Semantic priming operates similarly in humans and neural language models when exposed to multimodal context.
- Evidence anchors:
  - [abstract] "The priming effect in both humans and the LM becomes more apparent as the context size (extra modality information + sentence context) increases"
  - [section 6] "we see from the experiments that priming, the effects of which are well studied in humans can be related to prompting in large language models"
  - [corpus] Weak evidence - corpus mentions semantic priming but not specifically in relation to context size effects.
- Break condition: If the priming effect is primarily driven by other factors (e.g., frequency effects), the relationship with context size may not hold.

## Foundational Learning

- Concept: Shannon Game and cloze procedure
  - Why needed here: Understanding the baseline task that this work extends from is crucial for grasping the experimental design and comparisons being made.
  - Quick check question: What is the key difference between the Shannon Game and the cloze procedure?

- Concept: Multimodal information fusion
  - Why needed here: The paper compares different ways of incorporating visual information (full images, object labels, cropped snippets), so understanding how these modalities combine is essential.
  - Quick check question: How does the paper's approach to multimodal fusion differ from typical vision-language model architectures?

- Concept: Priming and prompting
  - Why needed here: The paper draws a parallel between psychological priming and the machine learning concept of prompting, which is central to its theoretical contribution.
  - Quick check question: How does semantic priming in humans relate to the concept of prompting in language models according to this paper?

## Architecture Onboarding

- Component map: Annotation environment -> Image processing pipeline -> Prediction interface -> GPT-2 integration
- Critical path:
  1. Present sentence with one of five modality configurations
  2. Participant predicts next word and rates confidence
  3. Actual word revealed and participant self-evaluates accuracy
  4. Repeat until sentence completion
  5. Aggregate results across participants and configurations

- Design tradeoffs:
  - Using non-native speakers provides a broader user base but may affect language processing patterns
  - Short sentences (8-15 words) ensure focused attention but limit generalizability to longer texts
  - Five modality configurations provide comprehensive comparison but increase experimental complexity

- Failure signatures:
  - If image configurations don't improve prediction accuracy over no-image baseline
  - If human-LM correlation decreases with additional modality, suggesting different processing mechanisms
  - If certain word classes show unexpected patterns (e.g., determiners not benefiting from visual information)

- First 3 experiments:
  1. Replicate the human experiment with native English speakers to verify the robustness of findings
  2. Test additional modality configurations (e.g., video or audio) to explore the space of extra modalities
  3. Apply standard psycholinguistic tools (EEG or eye-tracking) to the multimodal task to investigate cognitive processing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multimodal models compare to humans when predicting the next word in a sentence with image information?
- Basis in paper: The paper compares human performance with a language model (GPT-2) on the multimodal Shannon game, showing that both humans and the model benefit from image information. However, the paper does not directly compare the performance of multimodal models to humans.
- Why unresolved: The paper focuses on comparing human performance with a unimodal language model, but does not explore the potential of multimodal models in this task.
- What evidence would resolve it: A direct comparison of human performance with multimodal models (e.g., CLIP, Flamingo) on the multimodal Shannon game would provide insights into the relative strengths and weaknesses of each approach.

### Open Question 2
- Question: How do different types of image information (e.g., bounding boxes, labels, cropped objects) affect the performance of humans and language models in the multimodal Shannon game?
- Basis in paper: The paper explores the impact of different types of image information on human performance, but does not investigate the effect on language models.
- Why unresolved: The paper focuses on the human side of the multimodal Shannon game, but does not extend the analysis to language models.
- What evidence would resolve it: An experiment that tests the performance of language models with different types of image information in the multimodal Shannon game would provide insights into the importance of image representation for language models.

### Open Question 3
- Question: How does the multimodal Shannon game performance vary across different languages and language families?
- Basis in paper: The paper uses English sentences and English-speaking participants, but does not explore the impact of language on the multimodal Shannon game performance.
- Why unresolved: The paper focuses on English, which may have different predictability patterns compared to other languages.
- What evidence would resolve it: An experiment that tests the multimodal Shannon game performance across different languages and language families would provide insights into the generalizability of the findings.

## Limitations

- The use of non-native English speakers as participants may affect language processing patterns compared to native speakers
- Short sentence lengths (8-15 words) limit generalizability to longer, more complex texts
- The study doesn't account for individual differences in visual processing abilities or familiarity with the image content

## Confidence

- High confidence: The core finding that multimodal information improves prediction confidence and accuracy
- Medium confidence: The specific benefits for nouns and determiners
- Medium confidence: The relationship between context size and priming effects

## Next Checks

1. Replicate the human experiment with native English speakers to verify the robustness of the confidence and accuracy improvements across language proficiency levels
2. Test additional modality configurations (e.g., video or audio) to explore whether the benefits extend to other extra modalities beyond visual information
3. Apply standard psycholinguistic tools (EEG or eye-tracking) to the multimodal task to investigate the cognitive processing mechanisms underlying the observed improvements