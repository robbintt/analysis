---
ver: rpa2
title: Cross-lingual Argument Mining in the Medical Domain
arxiv_id: '2301.10527'
source_url: https://arxiv.org/abs/2301.10527
tags:
- data
- argument
- spanish
- english
- deepl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates cross-lingual argument mining in the medical
  domain for Spanish, a language without annotated data. The authors propose generating
  Spanish data from English via machine translation and label projection, using DeepL
  and OPUS-MT for translation and SimAlign/AwesomeAlign for projection.
---

# Cross-lingual Argument Mining in the Medical Domain

## Quick Facts
- arXiv ID: 2301.10527
- Source URL: https://arxiv.org/abs/2301.10527
- Reference count: 10
- Primary result: Translation and projection approach generates Spanish argument mining data achieving 80-84% F1-score, outperforming zero-shot transfer

## Executive Summary
This work addresses the challenge of argument mining in medical texts for low-resource languages by developing a pipeline that translates English annotated data to Spanish and projects argument labels. The authors demonstrate that this approach generates high-quality Spanish data for argument mining and that this automatically generated data can also improve English results through data augmentation. Their experiments show that translation and projection significantly outperforms zero-shot cross-lingual transfer using mBERT.

## Method Summary
The authors propose generating Spanish argument mining data by translating English medical abstracts using DeepL and OPUS-MT, then projecting argument labels using SimAlign and AwesomeAlign. The pipeline includes post-processing and manual correction of alignment errors. They evaluate the generated corpus using mBERT and BETO across three settings: zero-shot (train on English, test on Spanish), monolingual (train and test on Spanish), and multilingual (train on both languages). The method is applied to the AbstRCT corpus, which contains argument components (Major Claim, Claim, Premise) and relations (Support, Attack, Partial-Attack, No relation).

## Key Results
- Translation and projection approach achieves 80-84% F1-score for argument component detection in Spanish
- Generated Spanish data improves English argument mining results by 1-2% through data augmentation
- Translation+projection significantly outperforms zero-shot cross-lingual transfer with mBERT for medical argument mining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation + label projection generates high-quality Spanish data for argument mining
- Mechanism: DeepL and OPUS-MT translate English medical abstracts; SimAlign/AwesomeAlign project argument labels onto translations; post-processing and manual correction fix systematic alignment errors
- Core assumption: Translation preserves argumentative structure and label boundaries sufficiently for accurate projection
- Evidence anchors:
  - [abstract]: "automatically translating and projecting annotations (data-transfer) from English to a given target language is an effective way to generate annotated data"
  - [section]: "We provide the first medical corpus in Spanish for argument mining by using machine translation and label projection methods"
  - [corpus]: [weak] no direct evaluation of translation quality preservation for argumentative structure
- Break condition: Translation introduces semantic shifts that change argument component boundaries or types

### Mechanism 2
- Claim: Data augmentation with automatically generated Spanish data improves English argument mining results
- Mechanism: Fine-tuning mBERT on combined English + Spanish corpus outperforms monolingual English training
- Core assumption: Spanish data is semantically aligned enough with English to provide useful regularization
- Evidence anchors:
  - [abstract]: "the automatically generated data in Spanish can also be used to improve results in the original English monolingual setting"
  - [section]: "this data augmentation approach increased the quality of the overall performance by 1-2% compared to training on monolingual and zero-shot results"
  - [corpus]: [missing] no cross-linguistic semantic similarity analysis
- Break condition: Spanish data introduces conflicting patterns that hurt English model performance

### Mechanism 3
- Claim: Translation+projection outperforms zero-shot cross-lingual transfer with mBERT for medical argument mining
- Mechanism: Supervised training on translated+projected Spanish data gives better predictions than directly applying English-trained mBERT to Spanish
- Core assumption: mBERT's cross-lingual transfer capabilities are insufficient for this domain without additional supervision
- Evidence anchors:
  - [abstract]: "our experiments demonstrate that the translation and projection approach outperforms zero-shot cross-lingual approaches using a large masked multilingual language model"
  - [section]: "better predictions were obtained under a monolingual setting for the Spanish corpus than in a zero-shot cross-lingual setting using a multilingual model such as mBERT"
  - [corpus]: [implicit] zero-shot vs supervised results comparison
- Break condition: mBERT's cross-lingual transfer improves enough to eliminate this gap

## Foundational Learning

- Concept: Argument mining components and relations
  - Why needed here: Task requires identifying Claims, Premises, and their Support/Attack links
  - Quick check question: What distinguishes a Claim from a Premise in medical abstracts?

- Concept: Cross-lingual transfer learning
  - Why needed here: Need to apply English-annotated data to Spanish without manual annotation
  - Quick check question: When does zero-shot transfer fail compared to supervised transfer?

- Concept: Data augmentation benefits
  - Why needed here: Generated Spanish data used to improve English model performance
  - Quick check question: How can low-resource language data improve high-resource language models?

## Architecture Onboarding

- Component map: Translation system (DeepL/OPUS-MT) → Projection system (SimAlign/AwesomeAlign) → Post-processing pipeline → Model training (mBERT/BETO)
- Critical path: Translation → Projection → Manual correction → Model training → Evaluation
- Design tradeoffs: Translation quality vs speed; projection accuracy vs coverage; manual correction effort vs data quality
- Failure signatures: Misaligned labels at article boundaries; wrong IOB tags; incorrect argument component types
- First 3 experiments:
  1. Train mBERT on English, test on manually corrected Spanish (zero-shot)
  2. Train mBERT on manually corrected Spanish, test on same (monolingual)
  3. Train mBERT on merged English+Spanish, test on English (data augmentation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the automatically generated Spanish corpus compare to a manually translated and annotated corpus for argument mining?
- Basis in paper: [explicit] The authors mention that their automatically generated Spanish corpus is "not ideal" and requires manual correction. They also compare the performance of models trained on the automatically generated data to those trained on English data.
- Why unresolved: The paper does not provide a direct comparison between the automatically generated Spanish corpus and a manually translated and annotated one.
- What evidence would resolve it: A study comparing the performance of argument mining models trained on a manually translated and annotated Spanish corpus to those trained on the automatically generated corpus would provide the necessary evidence.

### Open Question 2
- Question: How do different machine translation systems perform in generating Spanish data for argument mining?
- Basis in paper: [explicit] The authors experiment with DeepL and OPUS-MT for translating the English corpus to Spanish and observe differences in their performance.
- Why unresolved: The paper does not provide a comprehensive comparison of the performance of different machine translation systems for this task.
- What evidence would resolve it: A study comparing the performance of different machine translation systems (e.g., DeepL, OPUS-MT, m2m-100, mBART) in generating Spanish data for argument mining would provide the necessary evidence.

### Open Question 3
- Question: How does the choice of word alignment tool affect the quality of the projected annotations in the target language?
- Basis in paper: [explicit] The authors experiment with SimAlign and AwesomeAlign for projecting annotations from English to Spanish and observe differences in their performance.
- Why unresolved: The paper does not provide a comprehensive comparison of the performance of different word alignment tools for this task.
- What evidence would resolve it: A study comparing the performance of different word alignment tools (e.g., SimAlign, AwesomeAlign, GIZA++) in projecting annotations from English to Spanish for argument mining would provide the necessary evidence.

### Open Question 4
- Question: How does the performance of argument relation classification compare to argument component detection in cross-lingual settings?
- Basis in paper: [explicit] The authors report results for both argument component detection and argument relation classification in cross-lingual settings and observe differences in their performance.
- Why unresolved: The paper does not provide a detailed analysis of the reasons behind the differences in performance between the two tasks.
- What evidence would resolve it: A study analyzing the challenges and factors affecting the performance of argument relation classification compared to argument component detection in cross-lingual settings would provide the necessary evidence.

## Limitations
- Translation quality preservation for argumentative structure is not directly evaluated
- Manual correction process details are vague, affecting reproducibility
- Missing cross-linguistic semantic similarity analysis for data augmentation claims

## Confidence
- High: Translation + projection outperforms zero-shot transfer (supported by direct comparison results)
- Medium: Data augmentation improves English results (shown but mechanism unclear)
- Low: Generated corpus quality matches human annotation standards (insufficient evaluation evidence)

## Next Checks
1. Evaluate translation quality specifically for argumentative structure preservation by comparing translated medical abstracts against professionally translated versions with argument annotations
2. Conduct ablation studies removing different components of the pipeline (translation, projection, post-processing) to isolate contribution of each step
3. Test the cross-lingual data augmentation hypothesis with different language pairs to verify it's not specific to Spanish-English characteristics