---
ver: rpa2
title: 'Label Words are Anchors: An Information Flow Perspective for Understanding
  In-Context Learning'
arxiv_id: '2305.14160'
source_url: https://arxiv.org/abs/2305.14160
tags:
- label
- words
- layers
- information
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the working mechanism of in-context learning
  (ICL) in large language models (LLMs) by analyzing the information flow through
  label words. The authors find that label words function as anchors: (1) in shallow
  layers, they aggregate semantic information from demonstration examples; (2) in
  deep layers, the model makes predictions by extracting information from label words.'
---

# Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning

## Quick Facts
- **arXiv ID**: 2305.14160
- **Source URL**: https://arxiv.org/abs/2305.14160
- **Authors**: 
- **Reference count**: 18
- **Key outcome**: This paper investigates the working mechanism of in-context learning (ICL) in large language models (LLMs) by analyzing the information flow through label words. The authors find that label words function as anchors: (1) in shallow layers, they aggregate semantic information from demonstration examples; (2) in deep layers, the model makes predictions by extracting information from label words. To validate this hypothesis, the authors manipulate attention mechanisms to isolate label words in different layers and measure the impact on model performance. They also examine the correlation between attention distributions on label words and final predictions. Based on these insights, the authors propose three applications: an anchor re-weighting method to improve ICL accuracy, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors. The anchor re-weighting method achieves a 16.7% average accuracy improvement over vanilla ICL baselines, while the compression technique can accelerate inference by up to 1.8x with negligible performance degradation.

## Executive Summary
This paper investigates the working mechanism of in-context learning (ICL) in large language models (LLMs) by analyzing the information flow through label words. The authors find that label words function as anchors: (1) in shallow layers, they aggregate semantic information from demonstration examples; (2) in deep layers, the model makes predictions by extracting information from label words. To validate this hypothesis, the authors manipulate attention mechanisms to isolate label words in different layers and measure the impact on model performance. They also examine the correlation between attention distributions on label words and final predictions. Based on these insights, the authors propose three applications: an anchor re-weighting method to improve ICL accuracy, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors. The anchor re-weighting method achieves a 16.7% average accuracy improvement over vanilla ICL baselines, while the compression technique can accelerate inference by up to 1.8x with negligible performance degradation.

## Method Summary
The authors propose a novel framework to understand the working mechanism of in-context learning (ICL) in large language models (LLMs) by analyzing the information flow through label words. They first define quantitative metrics (Swp, Spq, Sww) based on saliency scores to characterize the information flow in ICL. Through experiments on GPT2-XL and GPT-J models, they demonstrate that label words function as anchors by aggregating semantic information from demonstration examples in shallow layers and serving as reference points for final predictions in deep layers. The authors then manipulate the attention mechanism to isolate label words in different layers, showing that this disrupts the information flow and impairs model performance. They also propose three applications based on their findings: an anchor re-weighting method to improve ICL accuracy, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors.

## Key Results
- The anchor re-weighting method achieves a 16.7% average accuracy improvement over vanilla ICL baselines.
- The context compression technique can accelerate inference by up to 1.8x with negligible performance degradation.
- The anchor-based error diagnosis framework effectively identifies and explains the reasons behind ICL errors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label words function as anchors by aggregating semantic information from demonstration examples in shallow layers
- Mechanism: During forward propagation in shallow layers, attention mechanisms direct information flow from demonstration tokens to label tokens, consolidating task-relevant information into label word representations
- Core assumption: The attention mechanism in transformers can effectively aggregate and consolidate information from multiple demonstration tokens into single label word representations
- Evidence anchors:
  - [abstract]: "semantic information aggregates into label word representations during the shallow computation layers' processing"
  - [section]: "label words gather information from demonstration examples to form semantic representations for later computations"
  - [corpus]: Weak - no direct corpus evidence found for information aggregation into label words specifically
- Break condition: If attention mechanisms fail to properly direct information flow from demonstrations to label words, or if label words cannot effectively consolidate information from multiple sources

### Mechanism 2
- Claim: Label words serve as reference points for final predictions in deep layers
- Mechanism: In deep layers, the model extracts information from label word representations to form final predictions, with attention distributions on label words strongly correlating with output categories
- Core assumption: The consolidated information in label word representations is sufficient and necessary for the model to make accurate final predictions
- Evidence anchors:
  - [abstract]: "the consolidated information in label words serves as a reference for LLMs' final predictions"
  - [section]: "the model extracts the information from label words to form the final prediction"
  - [corpus]: Weak - corpus shows related work on label words but no direct evidence for this specific mechanism
- Break condition: If the model can make accurate predictions without relying on label word representations, or if label word representations lose critical information during aggregation

### Mechanism 3
- Claim: Information flow through label words follows a specific pattern that can be quantified and manipulated
- Mechanism: The importance of information flow shifts from label words receiving information (Swp) in shallow layers to label words providing information (Spq) in deep layers, enabling targeted interventions
- Core assumption: The quantitative metrics (Swp, Spq, Sww) accurately capture the information flow patterns and can guide effective interventions
- Evidence anchors:
  - [abstract]: "We compute two metrics based on saliency scores to portray the information flow in ICL"
  - [section]: "We propose the following quantitative metrics based on Il" showing the shift from Swp dominance to Spq dominance
  - [corpus]: Weak - corpus mentions related work but no specific evidence for these quantitative metrics
- Break condition: If the quantitative metrics fail to capture the actual information flow patterns, or if interventions based on these metrics do not improve performance

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper relies heavily on manipulating attention mechanisms to validate hypotheses about information flow through label words
  - Quick check question: How does the attention mechanism in transformers compute the importance of information flow between tokens?

- Concept: Saliency scores and their interpretation
  - Why needed here: The paper uses saliency scores to quantify information flow patterns and validate the label word anchor hypothesis
  - Quick check question: What does the saliency score Il(i,j) represent in the context of information flow between tokens?

- Concept: In-context learning (ICL) setup and evaluation
  - Why needed here: Understanding how ICL demonstrations are constructed and evaluated is crucial for interpreting the experimental results
  - Quick check question: How are demonstration examples typically formatted in ICL tasks, and what metrics are used to evaluate performance?

## Architecture Onboarding

- Component map: GPT2-XL or GPT-J models processing ICL tasks, with attention mechanisms directing information flow through label words, and quantitative metrics (Swp, Spq, Sww) measuring this flow
- Critical path: Information flow from demonstration tokens → label words (shallow layers) → final prediction (deep layers) via attention mechanisms
- Design tradeoffs: Balancing model capacity (GPT2-XL vs GPT-J) with computational resources, and choosing between preserving full input context versus compression techniques
- Failure signatures: Poor performance when isolating label words in shallow layers, low correlation between attention distributions and predictions in deep layers, or failure of quantitative metrics to capture information flow patterns
- First 3 experiments:
  1. Validate the shift from Swp to Spq dominance by computing metrics across different layer depths
  2. Test the impact of isolating label words in different layers on model performance
  3. Measure the correlation between attention distributions on label words and final predictions using AUC-ROC scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the information aggregation process in shallow layers specifically contribute to the model's ability to perform few-shot learning across diverse tasks?
- Basis in paper: [explicit] The paper identifies that label words in shallow layers gather information from demonstration examples to form semantic representations for deeper layers, but does not explore the specific mechanisms by which this aggregation enables task generalization.
- Why unresolved: The paper focuses on the existence and importance of information aggregation but does not delve into the underlying mechanisms that allow this aggregation to facilitate few-shot learning across different tasks.
- What evidence would resolve it: Experiments that systematically manipulate the information aggregation process in shallow layers and measure the impact on few-shot learning performance across a range of tasks would provide insights into the specific contributions of this process.

### Open Question 2
- Question: Can the anchor re-weighting method be extended to dynamically adjust the contribution of label words during inference based on the input context?
- Basis in paper: [inferred] The paper proposes a static anchor re-weighting method that uses a learnable vector to adjust the significance of label words, but does not explore the possibility of dynamically adjusting these weights during inference.
- Why unresolved: The current anchor re-weighting method is static and does not account for the potential variability in the importance of label words across different input contexts.
- What evidence would resolve it: Experiments that compare the performance of static and dynamic anchor re-weighting methods on various tasks and input contexts would demonstrate the potential benefits of dynamic adjustment.

### Open Question 3
- Question: How does the anchor-based analysis framework for error diagnosis generalize to other types of language models beyond GPT?
- Basis in paper: [explicit] The paper applies the anchor-based error diagnosis framework to GPT2-XL and GPT-J models, but does not explore its applicability to other transformer-based models.
- Why unresolved: The paper demonstrates the effectiveness of the anchor-based error diagnosis framework for GPT models but does not investigate whether similar principles can be applied to other transformer architectures.
- What evidence would resolve it: Experiments that apply the anchor-based error diagnosis framework to a variety of transformer-based models and evaluate its effectiveness in identifying and explaining errors would provide insights into its generalizability.

## Limitations

- The evidence supporting the label word anchor hypothesis is primarily correlational rather than definitively causal.
- The proposed applications (anchor re-weighting, compression, error diagnosis) were tested on a limited set of datasets and model architectures.
- The quantitative metrics (Swp, Spq, Sww) may have limited interpretability and sensitivity to hyperparameters.

## Confidence

**Confidence: Medium** - The core hypothesis that label words aggregate semantic information in shallow layers and serve as reference points in deep layers is supported by multiple experimental observations. However, the evidence is largely correlational rather than definitively causal. The attention manipulation experiments show performance degradation when label words are isolated, but alternative explanations (such as disruption of overall attention patterns) cannot be ruled out.

**Confidence: Low-Medium** - The quantitative metrics (Swp, Spq, Sww) show consistent patterns across layers, but their interpretability and sensitivity to hyperparameters remain unclear. The shift from Swp dominance in shallow layers to Spq dominance in deep layers is observed, but whether this pattern holds across different model architectures, dataset types, or ICL configurations requires further validation.

**Confidence: Medium-High** - The empirical improvements from the anchor re-weighting method (16.7% average accuracy gain) and compression technique (1.8x speedup) are compelling, but the robustness of these gains across diverse NLP tasks and model scales needs broader testing. The methods show promise but may have task-specific limitations.

## Next Checks

1. **Causal Attribution Test**: Design ablation studies that systematically vary label word positions, numbers, and semantic content while keeping other demonstration elements constant. This would help isolate whether label words specifically (versus other demonstration components) drive the observed information flow patterns and performance improvements.

2. **Cross-Architecture Generalization**: Test the label word anchor hypothesis and proposed applications across diverse model architectures (not just GPT2-XL and GPT-J) and scales (from small transformer variants to frontier LLMs). This would validate whether the observed patterns are fundamental properties of transformer attention mechanisms or artifacts of specific architectural choices.

3. **Alternative Metric Validation**: Compare the proposed saliency-based metrics (Swp, Spq, Sww) against alternative information flow quantification methods, such as integrated gradients, attention rollout, or causal mediation analysis. This would establish whether the observed patterns are artifacts of the specific metric choice or robust features of ICL information flow.