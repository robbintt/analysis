---
ver: rpa2
title: 'Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval'
arxiv_id: '2311.15564'
source_url: https://arxiv.org/abs/2311.15564
tags:
- training
- dense
- abel
- retriever
- reranker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ABEL, a simple yet effective unsupervised method
  to enhance passage retrieval in zero-shot settings. The core idea is to alternate
  distillation between a dense retriever and a reranker in iterations, allowing them
  to mutually enhance each other's performance.
---

# Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval

## Quick Facts
- arXiv ID: 2311.15564
- Source URL: https://arxiv.org/abs/2311.15564
- Reference count: 40
- Primary result: ABEL achieves state-of-the-art unsupervised dense retrieval performance on BEIR benchmark with 46.5% average nDCG@10

## Executive Summary
This paper introduces ABEL (Alternating Bootstrapping and Distillation), a simple yet effective unsupervised method for enhancing zero-shot dense retrieval. The approach alternates between training a dense retriever using supervision signals from a reranker, then updating the reranker based on improved retrieval results. This iterative process creates a mutual learning loop where both components progressively enhance each other's performance. The method achieves remarkable results on the BEIR benchmark, surpassing both leading supervised and unsupervised retrievers, with an average nDCG@10 score of 46.5% across 18 datasets.

## Method Summary
ABEL works by iteratively bootstrapping between a dense retriever and a reranker. Starting with BM25-initialized retriever, it retrieves top-k passages for each query, then trains a cross-encoder reranker using soft labels (retriever scores) rather than hard binary labels. The improved reranker is used to refine the top-k results, and the process repeats for multiple iterations. Noise injection during training prevents overfitting by adding word-level perturbations to inputs. The method uses KL divergence loss for soft label training and contrastive loss for retriever training, creating a self-supervised learning loop without requiring labeled data.

## Key Results
- Achieves 46.5% average nDCG@10 on BEIR benchmark across 18 datasets
- Outperforms previous best unsupervised methods by significant margin
- Shows strong zero-shot generalization to unseen tasks and domains
- Noise injection improves both retriever and reranker performance
- Multiple iterations progressively improve performance up to 3 iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating distillation creates mutual learning where each model improves the other's supervision signals
- Mechanism: Retriever provides coarse-grained candidate selection while reranker refines this selection and generates soft labels capturing nuanced semantic relationships
- Core assumption: Reranker's cross-encoder architecture provides more accurate soft labels than retriever's predictions
- Evidence anchors: [abstract] "dense retriever learns from supervision signals provided by a reranker, and subsequently, the reranker is updated based on feedback from the improved retriever"
- Break condition: If reranker's soft labels are consistently worse than retriever's own predictions

### Mechanism 2
- Claim: Iterative refinement progressively narrows candidate space and increases label accuracy
- Mechanism: Each iteration improves retriever's proposal distribution, then reranker scores candidates more accurately, creating better training signals
- Core assumption: Retriever can progressively improve proposal distribution without losing exploration ability
- Evidence anchors: [abstract] "By iterating this loop, the two components mutually enhance one another's performance"
- Break condition: If performance plateaus or degrades after several iterations

### Mechanism 3
- Claim: Noise injection prevents overfitting and encourages generalization
- Mechanism: Adding word-level perturbations (shuffling, deletion, masking) forces models to learn robust matching patterns
- Core assumption: 10% noise level is sufficient to prevent overfitting without destroying semantic meaning
- Evidence anchors: [section 3.3.2] "injecting noise into the inputs of the retriever during training results in improved performance"
- Break condition: If noise injection causes performance degradation

## Foundational Learning

- Concept: Contrastive learning for dense retrieval
  - Why needed here: Retriever trained using contrastive loss that pushes positive passages away from negatives in embedding space
  - Quick check question: Can you explain why contrastive loss encourages retriever to place relevant passages closer to queries than irrelevant ones in embedding space?

- Concept: Cross-encoder architecture for reranking
  - Why needed here: Reranker uses joint encoding of query and passage to capture fine-grained interactions that dual-encoder cannot
  - Quick check question: What is the key architectural difference between cross-encoder and dual-encoder that allows reranker to capture more nuanced semantic relationships?

- Concept: Knowledge distillation and soft labels
  - Why needed here: Reranker learns from soft labels (retriever scores) rather than hard binary labels to capture relative relevance between passages
  - Quick check question: How do soft labels differ from hard labels in terms of information they provide about passage relevance, and why might this be beneficial for training?

## Architecture Onboarding

- Component map: Query Construction -> Retriever (Dual-Encoder) -> Reranker (Cross-Encoder) -> BM25 -> Noise Injection -> Alternating Loop

- Critical path:
  1. Initialize retriever from warm-up BM25
  2. Retrieve top-k passages for each query
  3. Extract training data (positives/negatives) from retrieval results
  4. Train reranker on soft labels from retriever
  5. Rerank top-k passages with improved reranker
  6. Extract refined training data from reranked results
  7. Fine-tune retriever with new data
  8. Repeat steps 2-7 for multiple iterations

- Design tradeoffs:
  - Model size vs efficiency: Cross-encoder reranker is more accurate but slower than dual-encoder retriever
  - Noise level: Too little noise doesn't prevent overfitting, too much destroys semantics
  - Number of iterations: More iterations can improve performance but risk overfitting or error accumulation
  - Top-k selection: Larger k provides more candidates but increases computational cost

- Failure signatures:
  - Retriever performance plateaus or degrades over iterations
  - Reranker fails to improve despite better retriever proposals
  - Models overfit to specific query-passage patterns (check with noise injection ablation)
  - Soft labels become too uniform or too extreme

- First 3 experiments:
  1. Baseline: Train retriever with hard labels from BM25 only (no reranker, no iterations)
  2. Single iteration: Train retriever → reranker → retriever once, compare to baseline
  3. Multiple iterations: Train for 2-3 iterations, compare to single iteration and baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would scaling up the dense retriever model size impact ABEL's performance in zero-shot settings?
- Basis in paper: [inferred] Authors mention scaling up model size improves reranking performance and reference prior work showing larger models improve dense retriever performance
- Why unresolved: Authors have not experimented with larger dense retriever models in their ABEL framework
- What evidence would resolve it: Empirical results comparing ABEL performance with different dense retriever model sizes on BEIR benchmark

### Open Question 2
- Question: How would incorporating more advanced dense retrieval architectures like ColBERT affect ABEL's performance?
- Basis in paper: [explicit] Authors state they focused on standard dual-encoder paradigm and did not explore more advanced architectures like ColBERT
- Why unresolved: Paper does not provide results for ABEL using architectures beyond standard dual-encoder
- What evidence would resolve it: Comparative results showing ABEL performance using ColBERT versus standard dual-encoder on BEIR tasks

### Open Question 3
- Question: How would ABEL's effectiveness change when applied to multilingual retrieval tasks?
- Basis in paper: [inferred] Authors mention BEIR is monolingual and express interest in validating ABEL in multilingual settings
- Why unresolved: Paper only evaluates ABEL on English-language tasks
- What evidence would resolve it: Experimental results showing ABEL performance on multilingual retrieval benchmarks compared to current state-of-the-art multilingual retrievers

## Limitations

- Unknown implementation details of noise injection parameters (word shuffling, deletion, masking percentages)
- Limited exploration of optimal iteration counts beyond 3 iterations
- No validation of ABEL's effectiveness on multilingual retrieval tasks
- Uncertainty about whether alternating distillation is the primary mechanism for performance gains versus other factors

## Confidence

- High confidence: ABEL outperforms existing unsupervised methods on BEIR benchmark (empirical results clearly demonstrate this)
- Medium confidence: Alternating distillation between retriever and reranker is the primary mechanism for performance gains (supported by ablation but not isolated)
- Low confidence: Specific noise injection parameters and iteration counts are optimal (not thoroughly explored in paper)

## Next Checks

1. Run ablation studies isolating effects of proposal distribution narrowing vs label accuracy improvement to determine which contributes more to performance gains

2. Test different noise injection levels (0%, 5%, 10%, 15%, 20%) and types (shuffling, deletion, masking) to identify optimal settings and understand sensitivity

3. Evaluate method's sensitivity to number of iterations (1, 2, 3, 4, 5) and measure performance stability across iterations to identify optimal stopping points