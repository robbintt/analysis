---
ver: rpa2
title: Effective Matching of Patients to Clinical Trials using Entity Extraction and
  Neural Re-ranking
arxiv_id: '2307.00381'
source_url: https://arxiv.org/abs/2307.00381
tags:
- trials
- clinical
- retrieval
- criteria
- trec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of matching patients to clinical
  trials by improving the first-stage retrieval and re-ranking process. The authors
  propose a two-step approach: (1) enriching both queries and documents with extracted
  medical entities and negation information to boost lexical retrieval, and (2) using
  a BERT-based neural re-ranker trained on two objectives - topical relevance and
  eligibility classification.'
---

# Effective Matching of Patients to Clinical Trials using Entity Extraction and Neural Re-ranking

## Quick Facts
- arXiv ID: 2307.00381
- Source URL: https://arxiv.org/abs/2307.00381
- Authors: 
- Reference count: 40
- Primary result: Entity enrichment improves lexical retrieval precision by 15%; two-stage BERT re-ranking outperforms larger models

## Executive Summary
This paper addresses the challenge of matching patients to clinical trials by improving the first-stage retrieval and re-ranking process. The authors propose a two-step approach: (1) enriching both queries and documents with extracted medical entities and negation information to boost lexical retrieval, and (2) using a BERT-based neural re-ranker trained on two objectives - topical relevance and eligibility classification. Experiments on TREC Clinical Trials data show that the inclusion criteria section is most important for retrieval, and that entity enrichment improves precision by 15%. The proposed re-ranking method outperforms larger neural models, even with limited training data, demonstrating the benefit of structured entity extraction and eligibility-focused fine-tuning for this task.

## Method Summary
The authors propose a two-stage pipeline for clinical trial matching: First, they enrich patient descriptions and clinical trial documents using named entity recognition and negation detection to extract medical entities and classify them into current, past, and family history categories. These enriched tokens are used to boost lexical retrieval using BM25+. Second, they employ a BERT-based cross-encoder for re-ranking, trained in two steps: first for topical relevance between patient and trial description sections, then for eligibility classification using the criteria section. The system also filters trials based on demographic constraints like age and gender.

## Key Results
- Entity enrichment with NER and negation detection improves lexical retrieval precision by 15%
- Inclusion criteria section alone outperforms other CT fields for BM25+ retrieval
- Two-stage BERT re-ranking outperforms larger neural models despite limited training data
- The approach achieves top performance on TREC Clinical Trials 2021 and 2022 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity extraction and negation detection improve matching accuracy by structuring noisy free-text into labeled medical condition states.
- Mechanism: The system identifies disease/drug entities in both patient and trial text, classifies them into current, past, and family history, and marks negations. This turns unstructured EHR text and semi-structured eligibility criteria into a structured set of labeled tokens, which are then prepended with section and negation prefixes (e.g., `cmc_no_diabetes`). BM25+ indexing on these enriched tokens boosts recall of relevant trials while avoiding false positives from negated conditions.
- Core assumption: Correctly extracted entities and their negation status directly map to eligibility relevance.
- Evidence anchors:
  - [abstract] "We use named entity recognition and negation detection in both patient description and the eligibility section of CTs. We further classify patient descriptions and CT eligibility criteria into current, past, and family medical conditions."
  - [section] "We parse the content of a clinical trial document to split it into specific sections... Our CT processing is focused on making the eligibility criteria as fine-grained as possible."
- Break condition: If the NER model mislabels entities or misclassifies negation, enriched tokens will introduce noise and degrade retrieval performance.

### Mechanism 2
- Claim: Two-stage re-ranking with curriculum-style objectives outperforms single-stage relevance ranking.
- Mechanism: First, a BERT-based cross-encoder is trained to predict topical relevance between patient and trial description sections, using BM25 top-50 as candidates. Then, the same model is further fine-tuned on eligibility classification (eligible vs. excluded) using the eligibility criteria. This staged training allows the model to first learn general relevance before focusing on eligibility logic, improving precision for the final ranking.
- Core assumption: Learning topical relevance first provides a useful inductive bias for the harder eligibility classification task.
- Evidence anchors:
  - [abstract] "We propose a two-step training schema for the Transformer network used to re-rank the results from the lexical retrieval... The first step focuses on matching patient information with the descriptive sections of trials, while the second step aims to determine eligibility by matching patient information with the criteria section."
  - [section] "We use a pairwise loss function and train the model for re-ranking outputs from the process described in previous sections... we follow the heuristic that the CT retrieval task can be decomposed into two sub-tasks."
- Break condition: If the training data is too small or noisy, the staged training could overfit early and fail to generalize to the eligibility objective.

### Mechanism 3
- Claim: Inclusion criteria section alone is more predictive than other CT fields for lexical retrieval.
- Mechanism: Experiments show that BM25+ with only the inclusion section outperforms models using summary, description, or condition fields. This suggests eligibility logic dominates relevance scoring for patient-trial matching.
- Core assumption: Eligibility criteria capture the essential filtering logic that lexical models can exploit more directly than narrative sections.
- Evidence anchors:
  - [abstract] "Our findings indicate that the inclusion criteria section of the CT has a great influence on the relevance score in lexical models."
  - [section] "The eligibility criteria section contains a crucial component of the trial used to distinguish if a patient is eligible for a given trial."
- Break condition: If trials have missing or ambiguous inclusion criteria, this mechanism will break down and other sections may become more important.

## Foundational Learning

- Concept: Negation handling in clinical text.
  - Why needed here: Eligibility criteria often contain double negations or negated exclusion rules; failing to detect them leads to matching ineligible trials.
  - Quick check question: What is the difference between "Patient is not smoking" and "Patient is not not smoking" in eligibility filtering?

- Concept: Curriculum learning in neural ranking.
  - Why needed here: CT retrieval involves two distinct tasks (topical relevance and eligibility), and staged training can improve learning efficiency and performance.
  - Quick check question: Why might training a model first on relevance and then on eligibility classification perform better than training only on eligibility?

- Concept: BM25+ term weighting and document expansion.
  - Why needed here: The system expands queries and documents with prefixed entity tokens; understanding how BM25+ scores these is critical for tuning effectiveness.
  - Quick check question: How does adding prefixed tokens like `cmc_no_asthma` affect term frequency and inverse document frequency in BM25+ scoring?

## Architecture Onboarding

- Component map:
  NER + negation detection (ScispaCy + medspaCy) -> Entity classification -> Query/document enrichment with prefixed tokens -> BM25+ lexical retrieval (Rank-BM25) -> BERT-based cross-encoder re-ranker (two-stage training) -> Filtering layer (age, gender, smoking, alcohol)

- Critical path:
  1. Parse CT XML → extract inclusion/exclusion → classify sentences → NER + negation → token expansion.
  2. Apply same process to patient topic.
  3. BM25+ rank all trials.
  4. Filter by demographics.
  5. Re-rank top-50 with topical BERT model.
  6. Re-rank again with eligibility BERT model.

- Design tradeoffs:
  - Using only inclusion section improves precision but may miss context from other fields.
  - Entity enrichment adds computational overhead and depends on NER accuracy.
  - Two-stage re-ranking doubles inference time but improves eligibility precision.

- Failure signatures:
  - Low precision: NER mislabels negations or entity types.
  - High recall but low nDCG: BM25+ expansion overweights rare entity tokens.
  - Model collapse: Curriculum training without enough data leads to overfitting.

- First 3 experiments:
  1. Run BM25+ on raw text vs. enriched tokens; compare P@10.
  2. Train BERT re-ranker with only relevance objective; measure vs. curriculum setup.
  3. Remove entity extraction; evaluate impact on eligibility precision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of entity extraction and negation detection models impact the overall retrieval effectiveness, and what is the optimal level of precision for these components?
- Basis in paper: [inferred] The paper discusses using neural network-based information extraction models and mentions that extracted entities contribute positively to retrieval but the relative performance gain is low. The authors note that both entity extraction and section classification models generate false positives and false negatives which influence the final retrieval result.
- Why unresolved: The paper doesn't provide a detailed analysis of how the quality of entity extraction directly impacts retrieval effectiveness or what threshold of precision would be optimal for the extraction components.
- What evidence would resolve it: Experiments comparing retrieval effectiveness across different levels of entity extraction precision, or ablation studies showing the impact of different extraction quality levels on retrieval performance.

### Open Question 2
- Question: Would extending the re-ranking cutoff beyond the top 50 trials significantly improve the number of eligible trials found, or would it lead to diminishing returns?
- Basis in paper: [explicit] The paper mentions that the TCRR neural re-ranking helps in removing ineligible trials only for the first 15 trials and that the re-ranking is performed on the top 50 trials from the first-stage ranker.
- Why unresolved: The authors only tested re-ranking up to 50 trials and didn't explore whether extending this cutoff would provide additional benefits or not.
- What evidence would resolve it: Experiments showing retrieval effectiveness metrics as a function of different re-ranking cutoffs (e.g., 20, 30, 40, 50, 100 trials) to determine the point of diminishing returns.

### Open Question 3
- Question: How would the proposed approach perform on longer patient descriptions or real-world EHR data that contains more complex medical information compared to the TREC Clinical Trials track topics?
- Basis in paper: [explicit] The paper acknowledges that the TREC CT collection uses relatively short patient descriptions in EHR admission note-style and that the approaches could have problems handling longer sequences.
- Why unresolved: The experiments were conducted only on the TREC CT dataset which has specific characteristics (short descriptions, English-only, etc.), so the generalizability to real-world EHR data remains untested.
- What evidence would resolve it: Experiments using longer patient descriptions or real-world EHR data from clinical settings to validate the approach's effectiveness and scalability.

## Limitations
- Limited generalizability to real-world EHR data with longer, more complex patient descriptions
- Reliance on NER and negation detection accuracy, which can introduce noise if models perform poorly
- No comparison against alternative training strategies for the two-stage BERT re-ranker

## Confidence
- Medium confidence in the entity enrichment mechanism: Improved precision shown but NER accuracy impact not fully analyzed
- Medium confidence in the two-stage training approach: Better results than single-stage but lack of ablation studies
- Low confidence in generalizability: Only tested on TREC Clinical Trials collections with specific characteristics

## Next Checks
1. Manually evaluate the entity extraction and negation detection accuracy on a sample of patient descriptions and trial criteria to quantify the quality of the enriched tokens.
2. Train a BERT re-ranker with only the eligibility objective and compare against the two-stage approach to isolate the contribution of the relevance pre-training phase.
3. Test the complete pipeline on a different clinical trial matching dataset to assess robustness beyond the TREC collections.