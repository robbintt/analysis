---
ver: rpa2
title: 'DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization'
arxiv_id: '2310.19668'
source_url: https://arxiv.org/abs/2310.19668
tags:
- dormant
- ratio
- learning
- exploration
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a critical issue in existing visual reinforcement
  learning (RL) methods where agents often exhibit sustained inactivity during early
  training, hindering effective exploration. The authors propose a method called DrM
  that introduces three mechanisms to guide agents' exploration-exploitation trade-offs
  by actively minimizing the dormant ratio, a metric that quantifies inactivity in
  the RL agent's network.
---

# DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization

## Quick Facts
- **arXiv ID**: 2310.19668
- **Source URL**: https://arxiv.org/abs/2310.19668
- **Reference count**: 40
- **Key outcome**: DrM achieves state-of-the-art visual RL performance on 19 tasks by minimizing dormant ratio to improve exploration-exploitation balance

## Executive Summary
DrM introduces a novel approach to visual reinforcement learning by addressing sustained inactivity during early training. The method tracks the dormant ratio - a metric quantifying neuron inactivity in the agent's network - and uses it to guide three mechanisms: periodic weight perturbation, adaptive exploration noise scheduling, and exploitation parameter adjustment. By dynamically balancing exploration and exploitation based on network activity, DrM achieves significant improvements in sample efficiency and asymptotic performance across multiple benchmark environments, becoming the first model-free algorithm to consistently solve challenging tasks in Dog, Manipulator, and Adroit domains without demonstrations.

## Method Summary
DrM builds upon DrQ-v2 with three key modifications based on dormant ratio monitoring. First, it implements periodic weight perturbation that interpolates current weights with random initialization when dormant ratio is high, restoring network expressivity. Second, it uses a dormant-ratio-based exploration scheduler that increases exploration noise when dormant ratio is high and decreases it when low. Third, it adjusts an exploitation parameter λ that increases as dormant ratio decreases, shifting focus toward exploiting learned skills. The dormant ratio is calculated by identifying neurons with activations below a threshold τ, with the proportion of such neurons serving as the metric that drives all three mechanisms.

## Key Results
- DrM achieves state-of-the-art performance on 19 visual RL tasks across three benchmark suites
- It is the first model-free algorithm to consistently solve Dog and Manipulator domains from DeepMind Control Suite
- DrM solves dexterous hand manipulation tasks in Adroit without demonstrations, unlike previous methods
- The method demonstrates significant improvements in both sample efficiency and asymptotic performance

## Why This Works (Mechanism)

### Mechanism 1: Weight Perturbation
- **Claim**: Perturbing model weights when dormant ratio is high reduces neuron inactivity and accelerates skill acquisition.
- **Core assumption**: High dormant ratio indicates loss of network expressivity that can be recovered through weight perturbation.
- **Evidence**: The mechanism is introduced to address high dormant ratios that cause loss of expressivity in the agent's network.
- **Break condition**: If the dormant ratio does not decrease after perturbation, the mechanism fails to restore expressivity.

### Mechanism 2: Exploration Scheduling
- **Claim**: Adjusting exploration noise based on dormant ratio improves exploration efficiency compared to static schedules.
- **Core assumption**: High dormant ratio indicates need for exploration while low dormant ratio indicates readiness for exploitation.
- **Evidence**: The scheduler is designed to emphasize exploration with large noise when dormant ratio is high and reduce it when low.
- **Break condition**: If exploration noise adjustments based on dormant ratio do not lead to faster learning or better performance.

### Mechanism 3: Exploitation Guidance
- **Claim**: Using dormant ratio to guide exploitation parameter improves exploitation efficiency.
- **Core assumption**: Low dormant ratio indicates agent has acquired useful skills that should be exploited.
- **Evidence**: The exploitation parameter λ increases as dormant ratio decreases, focusing on exploiting past successes.
- **Break condition**: If adjusting λ based on dormant ratio does not improve final performance or convergence speed.

## Foundational Learning

- **Concept**: Dormant ratio measurement
  - **Why needed**: Core metric that drives all three mechanisms in DrM
  - **Quick check**: How do you calculate dormant ratio for a given neural network layer?

- **Concept**: Exploration-exploitation tradeoff
  - **Why needed**: DrM's mechanisms are designed to balance this tradeoff based on dormant ratio
  - **Quick check**: What happens if an agent explores too much or exploits too early?

- **Concept**: Parameter perturbation in neural networks
  - **Why needed**: One of DrM's mechanisms uses weight perturbation to reduce dormant ratio
  - **Quick check**: How does interpolating weights with random initialization affect network behavior?

## Architecture Onboarding

- **Component map**: Dormant ratio calculation -> Weight perturbation mechanism, Exploration scheduler, Exploitation parameter adjustment
- **Critical path**: The dormant ratio calculation feeds into all three mechanisms, which modify the RL agent's behavior. Weight perturbation runs periodically, exploration scheduler updates every step, and exploitation parameter is used during value function updates.
- **Design tradeoffs**: Balancing exploration vs exploitation dynamically; frequency and magnitude of weight perturbations could help escape local minima or destabilize learning.
- **Failure signatures**: (1) dormant ratio remains high throughout training, (2) exploration noise adjustments don't improve learning, (3) weight perturbations don't reduce dormant ratio, or (4) worse performance than baseline methods.
- **First 3 experiments**:
  1. Verify dormant ratio calculation by testing on a known network with expected dormant neurons
  2. Test weight perturbation mechanism on a simple task to confirm it reduces dormant ratio
  3. Compare exploration noise schedules (static vs dormant-ratio-based) on a basic continuous control task

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several unresolved issues including the theoretical relationship between dormant ratio and exploration-exploitation trade-off, optimal perturbation frequency and magnitude, and performance in stochastic or partially observable environments.

## Limitations

- The paper lacks ablation studies to isolate the individual contributions of the three mechanisms to performance improvements
- Implementation details for dormant ratio calculation and expectile regression are underspecified, making faithful reproduction challenging
- The evaluation focuses on demonstrating superiority over baselines rather than understanding failure modes or limitations of the approach

## Confidence

**High Confidence**: Claims about DrM's sample efficiency and asymptotic performance improvements over baselines (DrQ-v2, ALIX, TACO) on the 19 evaluated tasks, supported by quantitative results.

**Medium Confidence**: Claims about DrM being the first model-free algorithm to consistently solve Dog, Manipulator, and Adroit tasks without demonstrations. While results support this, lack of ablation studies and comparison with other recent visual RL methods limits confidence.

**Low Confidence**: Claims about the specific mechanisms driving performance improvements. The paper asserts dormant ratio minimization addresses sustained inactivity but doesn't provide direct evidence linking dormant ratio reduction to improved exploration or skill acquisition.

## Next Checks

1. **Ablation Study**: Implement and evaluate DrM variants where each of the three mechanisms (weight perturbation, exploration scheduling, exploitation guidance) is disabled individually to quantify their individual contributions to overall performance.

2. **Dormant Ratio Analysis**: Track dormant ratio dynamics across training and correlate with agent behavior statistics (action entropy, exploration coverage) to validate that dormant ratio reduction actually corresponds to improved exploration and skill acquisition.

3. **Robustness Testing**: Evaluate DrM on additional visual RL benchmarks not included in the paper (e.g., DMControl Hard, Procgen) and test sensitivity to hyperparameters like dormant ratio thresholds, perturbation magnitudes, and exploration noise schedules.