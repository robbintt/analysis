---
ver: rpa2
title: Lightweight Online Learning for Sets of Related Problems in Automated Reasoning
arxiv_id: '2305.11087'
source_url: https://arxiv.org/abs/2305.11087
tags:
- btor2
- kissat
- portfolio
- btor
- arbitrated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Driven Strategy Learning (sdsl), a lightweight
  online learning methodology for solving sets of related automated reasoning problems.
  The approach automatically constructs a dataset while solving earlier problems and
  uses it to fit a machine learning model, which adjusts the solving strategy for
  later problems.
---

# Lightweight Online Learning for Sets of Related Problems in Automated Reasoning

## Quick Facts
- **arXiv ID**: 2305.11087
- **Source URL**: https://arxiv.org/abs/2305.11087
- **Reference count**: 40
- **Primary result**: KISSAT+sdsl certifies larger bounds and finds more counter-examples than state-of-the-art approaches on Hardware Model Checking Competition benchmarks

## Executive Summary
This paper introduces Self-Driven Strategy Learning (sdsl), a lightweight online learning methodology for solving sets of related automated reasoning problems. The approach automatically constructs a dataset while solving earlier problems and uses it to fit a machine learning model, which adjusts the solving strategy for later problems. Experimental results on bounded model checking benchmarks from the Hardware Model Checking Competition show that KISSAT+sdsl significantly outperforms baseline solvers and algorithm portfolios, demonstrating the effectiveness of online learning for related problem sets.

## Method Summary
sdsl implements online learning by continuously collecting data as it solves problems, using Metropolis-Hastings MCMC sampling to explore the strategy space efficiently, and training random forest models to predict optimal strategies. The system integrates with the KISSAT SAT solver and applies to bounded model checking problems by incrementally solving formulas with increasing step sizes. The learning process is designed to be lightweight, avoiding the overhead of offline tuning while adapting to the specific characteristics of the problem set.

## Key Results
- KISSAT+sdsl finds 43 more counter-examples than AVR, 62 more than PONO, and 122 more than KISSAT alone on HWMCC benchmarks
- sdsl certifies larger bounds for 74 benchmarks compared to 49 for AVR, 54 for PONO, and 41 for KISSAT
- Learning overhead is minimized while still providing significant performance improvements over baseline solvers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online learning strategy selection outperforms offline tuning for related problem sets
- Mechanism: sdsl continuously collects data by solving the same problem multiple times with different strategies, trains a lightweight model on-the-fly, and selects strategies for later problems based on predictions
- Core assumption: The structural similarity within the problem set makes earlier learning predictive for later problems
- Evidence anchors: abstract transition rules and concrete implementation details
- Break condition: If the problem set lacks sufficient structural similarity, earlier learning becomes irrelevant and the overhead outweighs benefits

### Mechanism 2
- Claim: MCMC-based conditional sampling finds low-cost strategies more efficiently than uniform sampling
- Mechanism: The Metropolis-Hastings algorithm samples strategies with probability proportional to their performance, allowing greedy movement to better strategies while maintaining exploration capability through acceptance of occasional worse proposals
- Core assumption: The strategy space has a reasonable structure where good strategies are clustered and reachable via local moves
- Evidence anchors: MCMC sampling implementation and acceptance probability formula
- Break condition: If the strategy space is highly multimodal with isolated good strategies, MCMC may get stuck in local minima

### Mechanism 3
- Claim: Random forests provide the right balance of speed and accuracy for online strategy prediction
- Mechanism: Tree-based ensemble methods train efficiently on sparse data, handle mixed feature types well, and make fast predictions, making them suitable for real-time strategy selection in sdsl
- Core assumption: The relationship between parameter settings and problem difficulty can be captured by piecewise constant approximations
- Evidence anchors: Random forest efficiency and robustness claims
- Break condition: If the relationship between parameters and performance is highly nonlinear or smooth, random forests may underfit and provide poor guidance

## Foundational Learning

- Concept: **Metropolis-Hastings algorithm**
  - Why needed here: It provides a principled way to sample strategies weighted by performance without exhaustively testing all candidates
  - Quick check question: What property must the proposal distribution have for the Metropolis-Hastings algorithm to converge to the target distribution?

- Concept: **Random forest regression**
  - Why needed here: It's a fast, robust model that works well with tabular data and limited training samples, essential for online learning
  - Quick check question: How does a random forest make predictions from individual tree outputs?

- Concept: **Incremental bounded model checking**
  - Why needed here: Understanding the baseline technique helps evaluate whether sdsl's online learning provides meaningful improvement
  - Quick check question: In BMC, what does it mean when a formula becomes unsatisfiable?

## Architecture Onboarding

- Component map: Data collector -> Model trainer -> Strategy selector -> Solver interface -> BMC engine
- Critical path: Data collection → Model training → Strategy selection → Solver execution
- Design tradeoffs:
  - Sample size (m) vs. learning time: Larger m improves model quality but reduces solving time
  - Tree depth vs. overfitting: Deeper trees capture more complex patterns but risk fitting noise
  - Learning budget vs. baseline performance: More learning improves later problems but may hurt overall if baseline is already good
- Failure signatures:
  - Learning time exceeds threshold but no improvement in later problems
  - Random forest R² score remains low despite many samples
  - Best strategy found during sampling performs worse than default
- First 3 experiments:
  1. Run KISSAT + sdsl on a small BMC benchmark with step size 1, measure learning time vs. performance gain
  2. Compare MCMC sampling with uniform sampling on a simple strategy space
  3. Test random forest with varying tree depths on collected data from experiment 1

## Open Questions the Paper Calls Out

- What is the optimal step size for Bounded Model Checking with Self-Driven Strategy Learning?
- How does the strategy space size affect the performance of Self-Driven Strategy Learning?
- How can the learning budget be optimally determined for Self-Driven Strategy Learning?

## Limitations
- Exact parameter configurations for the strategy space Vdev are unspecified
- MCMC implementation details including proposal distribution and temperature parameter tuning are underspecified
- Limited empirical justification for choosing random forests over other model types

## Confidence
- **High confidence**: The fundamental sdsl framework (abstract transition rules, online learning mechanism) is well-defined and theoretically sound
- **Medium confidence**: The concrete implementation choices (MCMC sampling, random forests) are reasonable given the online learning constraints, though empirical justification is limited
- **Low confidence**: Specific performance claims relative to baseline solvers, as exact parameter configurations and implementation details are missing

## Next Checks
1. Implement a simplified version of sdsl with fixed parameter configurations and verify the core learning loop functions correctly on small benchmarks
2. Conduct ablation studies comparing MCMC sampling vs. uniform sampling and random forests vs. simpler models on the same dataset
3. Analyze the trade-off between learning time and performance gain across different sample sizes m to determine optimal resource allocation