---
ver: rpa2
title: Diffusion Model Alignment Using Direct Preference Optimization
arxiv_id: '2311.12908'
source_url: https://arxiv.org/abs/2311.12908
tags:
- pref
- diffusion
- human
- reward
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion-DPO extends Direct Preference Optimization to diffusion
  models by reformulating the DPO objective to use diffusion model likelihoods via
  the ELBO, enabling direct training on human preference data. Fine-tuning SDXL-1.0
  with Diffusion-DPO on 851K pairwise preferences from the Pick-a-Pic dataset significantly
  improves visual appeal and text alignment.
---

# Diffusion Model Alignment Using Direct Preference Optimization

## Quick Facts
- arXiv ID: 2311.12908
- Source URL: https://arxiv.org/abs/2311.12908
- Reference count: 40
- Key outcome: Diffusion-DPO extends Direct Preference Optimization to diffusion models by reformulating the DPO objective to use diffusion model likelihoods via the ELBO, enabling direct training on human preference data. Fine-tuning SDXL-1.0 with Diffusion-DPO on 851K pairwise preferences from the Pick-a-Pic dataset significantly improves visual appeal and text alignment. Human evaluation shows DPO-SDXL is preferred over SDXL (base + refinement) 69% of the time on PartiPrompts and 64% on HPSv2, while using only 53% of the parameters. DPO-SDXL also outperforms baselines on image-to-image editing tasks and can learn effectively from AI feedback, opening scaling possibilities for diffusion model alignment.

## Executive Summary
Diffusion-DPO extends Direct Preference Optimization from language models to text-to-image diffusion models by reformulating the DPO objective to use diffusion model likelihoods via the evidence lower bound (ELBO). The method fine-tunes the SDXL-1.0 model on 851K crowdsourced pairwise human preferences from the Pick-a-Pic dataset, significantly improving visual appeal and text alignment. Human evaluation shows DPO-SDXL is preferred over SDXL (base + refinement) 69% of the time on PartiPrompts and 64% on HPSv2, while using only 53% of the parameters. The approach demonstrates effective scaling to large diffusion models and shows promise for learning from AI feedback, opening new avenues for diffusion model alignment.

## Method Summary
Diffusion-DPO reformulates the Direct Preference Optimization objective for diffusion models by expressing the reward as a difference in denoising errors between winning and losing samples, weighted by the signal-to-noise ratio and scaled by a hyperparameter β. The method implicitly learns a reward model through this optimization objective while maintaining distributional stability through KL regularization. The training procedure samples random timesteps, adds noise to both preferred and non-preferred images, computes diffusion model errors, and applies a sigmoid-based DPO loss. The approach was evaluated on the SDXL-1.0 model using the Pick-a-Pic dataset of 851K pairwise preferences, with human evaluation on Amazon Mechanical Turk comparing General Preference, Visual Appeal, and Prompt Alignment.

## Key Results
- DPO-SDXL preferred over SDXL (base + refinement) 69% of the time on PartiPrompts and 64% on HPSv2
- DPO-SDXL uses only 53% of parameters compared to SDXL (base + refinement)
- Significant improvements in visual appeal and text alignment on human evaluation
- Effective scaling to large diffusion models and ability to learn from AI feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion-DPO reformulates DPO to use diffusion model likelihoods via the ELBO, enabling direct training on human preference data
- Mechanism: The method connects human preference pairs to diffusion model training by expressing the reward as a difference in denoising errors between winning and losing samples
- Core assumption: The evidence lower bound (ELBO) provides a valid approximation for the intractable diffusion model likelihood
- Evidence anchors:
  - [abstract] "Diffusion-DPO extends Direct Preference Optimization to diffusion models by reformulating the DPO objective to use diffusion model likelihoods via the ELBO"
  - [section 4] Derivation showing how the ELBO simplifies to a difference in denoising errors weighted by β
  - [corpus] Weak - no direct corpus support for ELBO approximation validity
- Break condition: If the ELBO approximation becomes too loose (high noise levels), the preference signal could be drowned out

### Mechanism 2
- Claim: The method implicitly learns a reward model through the preference optimization objective
- Mechanism: By optimizing the diffusion model to produce lower denoising error on preferred samples, the model effectively learns which image characteristics are preferred without explicitly modeling rewards
- Core assumption: The preference data contains sufficient signal to guide the implicit reward learning
- Evidence anchors:
  - [section 4] "As a consequence of the theoretical framework, our DPO scheme implicitly learns a reward model"
  - [section 5.5] "Our learned models (DPO-SD1.5 and DPO-SDXL) perform well at binary preference classification"
  - [corpus] Weak - no corpus evidence about implicit reward learning in diffusion models
- Break condition: If the preference dataset is too small or noisy, the implicit reward learning could fail to generalize

### Mechanism 3
- Claim: The method provides distributional guarantees through KL regularization
- Mechanism: The β parameter in the objective controls the trade-off between following preferences and staying close to the reference distribution
- Core assumption: The KL regularization prevents mode collapse while still allowing meaningful preference optimization
- Evidence anchors:
  - [section 4] "While DDPO [6] is an RL-based method as is DPOK [11], their target objective and distributional guarantees are different"
  - [section 5.2] "DPO-SDXL shows superior generation of anatomical features such as teeth, hands, and eyes" suggesting maintained quality
  - [corpus] Moderate - comparison with DDPO/DPOK suggests distributional benefits
- Break condition: If β is too large, the model won't adapt to preferences; if too small, mode collapse could occur

## Foundational Learning

- Concept: Evidence Lower Bound (ELBO)
  - Why needed here: The ELBO is crucial for making the diffusion model likelihood tractable in the preference optimization objective
  - Quick check question: Why can't we directly compute the likelihood of a diffusion model for a given image?

- Concept: Diffusion model reverse process parameterization
  - Why needed here: Understanding the Gaussian reverse process formulation is essential for deriving the denoising error terms
  - Quick check question: What is the relationship between the predicted noise and the actual noise in a diffusion model?

- Concept: Bradley-Terry preference model
  - Why needed here: The Bradley-Terry model provides the theoretical foundation for learning from pairwise preferences
  - Quick check question: How does the Bradley-Terry model relate the probability of preferring one item over another to their reward values?

## Architecture Onboarding

- Component map:
  Diffusion model (SDXL base model) -> Reference diffusion model -> Pick-a-Pic dataset -> Reward model (implicit) -> Automated metrics (PickScore, HPSv2)

- Critical path:
  1. Sample a random timestep t
  2. Add noise to both preferred and non-preferred images
  3. Generate noise predictions from both the model and reference
  4. Compute denoising errors for both samples
  5. Calculate the loss based on the error difference weighted by β
  6. Update model parameters via gradient descent

- Design tradeoffs:
  - β value: Controls preference strength vs. distributional stability
  - Training duration: Longer training improves preferences but risks overfitting
  - Noise schedule: Affects the quality of the denoising error approximation
  - Dataset size: More preferences enable better learning but increase compute

- Failure signatures:
  - Mode collapse: Images become repetitive or lose diversity
  - Preference degradation: Images stop improving or get worse
  - Training instability: Loss becomes NaN or explodes
  - Distribution shift: Generated images look nothing like the reference

- First 3 experiments:
  1. Ablation study: Train with different β values (2000, 5000, 10000) and measure preference win rates
  2. Data efficiency: Train on subsets of the Pick-a-Pic dataset (10%, 50%, 100%) and track performance scaling
  3. Distribution analysis: Compare image diversity metrics (LPIPS, IS) between baseline and DPO-tuned models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Diffusion-DPO method scale to larger diffusion models or does performance plateau as model size increases?
- Basis in paper: [inferred] The paper demonstrates Diffusion-DPO on SDXL-1.0 (3.5B parameters) and compares it to SDXL-base (6.6B parameters). However, it does not explore scaling beyond these sizes or analyze performance trends with increasing model capacity.
- Why unresolved: The experiments are limited to a single model scale. Scaling laws for diffusion model alignment are unknown.
- What evidence would resolve it: Training Diffusion-DPO on progressively larger diffusion models (e.g., SDXL 2.0, SD3) and measuring improvements in human preference scores and other metrics would reveal whether benefits continue to scale or saturate.

### Open Question 2
- Question: How does the choice of reward model (e.g., PickScore vs. CLIP vs. Aesthetics) affect the qualitative properties of the generated images?
- Basis in paper: [explicit] The paper shows that training on different reward models (PickScore, HPS, CLIP, Aesthetics) leads to varying improvements in specific metrics. However, it does not provide a detailed qualitative analysis of how these different reward functions shape the aesthetic characteristics of the output.
- Why unresolved: The experiments focus on quantitative metrics. The qualitative impact of different reward functions on image style, composition, and content is not explored.
- What evidence would resolve it: Conducting a detailed human evaluation comparing the stylistic and compositional differences in images generated by models trained on different reward functions would reveal how these choices influence the artistic properties of the outputs.

### Open Question 3
- Question: Can Diffusion-DPO be effectively combined with other alignment techniques like supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF)?
- Basis in paper: [explicit] The paper briefly mentions that SFT is beneficial for LLMs but does not explore its combination with Diffusion-DPO for diffusion models. It also does not compare Diffusion-DPO to RLHF-based approaches for diffusion model alignment.
- Why unresolved: The experiments focus solely on Diffusion-DPO as a standalone method. The potential synergies or trade-offs with other alignment techniques are unexplored.
- What evidence would resolve it: Conducting experiments that combine Diffusion-DPO with SFT or RLHF and comparing their performance to each method individually would reveal whether there are complementary benefits or potential conflicts between these approaches.

## Limitations

- The method's performance depends heavily on the quality and quantity of human preference data, with limited exploration of data efficiency
- The ELBO approximation's validity for high-noise timesteps in diffusion models is not thoroughly validated, potentially limiting the method's robustness
- The paper does not extensively analyze the distributional guarantees provided by KL regularization or quantify potential mode collapse

## Confidence

**High Confidence**: The technical formulation of Diffusion-DPO and its implementation details (mechanism of reformulating DPO for diffusion models using ELBO, the training procedure with specific hyperparameters, and the general improvement in human evaluation metrics).

**Medium Confidence**: The claim that Diffusion-DPO implicitly learns a reward model without explicitly modeling rewards, as this is demonstrated through classification performance but not thoroughly explored for generalization to unseen preference patterns.

**Low Confidence**: The distributional guarantees provided by the KL regularization, as the paper mentions this benefit but doesn't provide extensive empirical evidence of preventing mode collapse or maintaining diversity compared to unregularized approaches.

## Next Checks

1. **Ablation on Noise Schedules**: Test the method across different noise levels (t values) to determine at which points the ELBO approximation breaks down and whether the preference signal degrades significantly at high noise timesteps.

2. **Out-of-Distribution Preference Transfer**: Evaluate whether the implicit reward model learned by Diffusion-DPO can generalize to preference pairs from different datasets or distributions than those used in training, testing the robustness of the learned preferences.

3. **Diversity and Mode Coverage Analysis**: Conduct a thorough analysis comparing the diversity of generated images (using metrics like LPIPS, Inception Score, or learned perceptual metrics) between the base SDXL model and DPO-SDXL to quantify any potential mode collapse despite the KL regularization.