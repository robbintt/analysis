---
ver: rpa2
title: Importance Estimation with Random Gradient for Neural Network Pruning
arxiv_id: '2310.20203'
source_url: https://arxiv.org/abs/2310.20203
tags:
- pruning
- methods
- importance
- neural
- molchanov
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses global neuron importance estimation for neural
  network pruning, focusing on methods that do not require labeled examples. It proposes
  two new Taylor First Order approximation-based methods, TaylorFO-abs and TaylorFO-sq,
  derived from heuristics combining forward and backward signals.
---

# Importance Estimation with Random Gradient for Neural Network Pruning

## Quick Facts
- arXiv ID: 2310.20203
- Source URL: https://arxiv.org/abs/2310.20203
- Reference count: 8
- Primary result: Random gradient propagation from last layer combined with normalization improves neuron pruning accuracy without labeled examples

## Executive Summary
This paper addresses the challenge of global neuron importance estimation for neural network pruning without requiring labeled examples. The authors propose two novel Taylor First Order approximation-based methods (TaylorFO-abs and TaylorFO-sq) that combine forward activations with backward signals. The key innovations are using random gradients from the last layer instead of backpropagation from loss, and normalizing gradient magnitudes to ensure equal contribution from all data points. These techniques improve pruning performance compared to previous methods across ResNet and VGG architectures on CIFAR-100 and STL-10 datasets, particularly when labels are scarce.

## Method Summary
The method computes neuron importance scores using Taylor First Order approximation where importance Ii combines forward activation xi with backward gradient δxi. Instead of backpropagating loss gradients, random unit vectors are propagated from the output layer, then normalized to unit norm. The importance metric is calculated as either the absolute value or squared value of xi·δxi to ensure all output changes are counted. Neurons are then globally pruned based on these importance scores, with evaluation showing accuracy retention versus number of neurons pruned.

## Key Results
- Random gradient propagation combined with normalization improves pruning accuracy compared to baseline methods
- TaylorFO-abs and TaylorFO-sq variants outperform original TaylorFO method, especially with normalized gradients
- Method shows consistent performance across ResNet and VGG architectures on CIFAR-100 and STL-10 datasets
- Random gradient approach complements existing methods when combined with them

## Why This Works (Mechanism)

### Mechanism 1
Random gradient propagation from the last layer approximates backward signal for importance estimation without requiring loss labels. The method propagates random unit vectors from the output layer and normalizes gradient magnitudes to give each data point equal contribution. This assumes the shape and direction of the gradient signal are sufficient for importance estimation, with absolute magnitude normalized away without losing discriminative power.

### Mechanism 2
Using absolute value or squared values of xi·δxi removes bias from negative gradients that could falsely indicate low importance. The raw TaylorFO importance Ii = (1/M) Σ xi·δxi can be small when gradients are negative even if neuron removal would significantly change the output. Taking |xi·δxi| (TaylorFO-abs) or (xi·δxi)² (TaylorFO-sq) ensures all changes in output magnitude are counted as importance.

### Mechanism 3
Normalizing gradient magnitude before propagation equalizes each data point's contribution to importance scores, preventing dominance by high-gradient examples. Before using gradients in importance computation, they are scaled to unit norm, so each example contributes equally to the summed importance metric. This assumes importance estimation should be data-balanced, and high-gradient samples should not disproportionately influence which neurons are pruned.

## Foundational Learning

- **Taylor First Order (TaylorFO) approximation for importance estimation**: Forms the baseline method being improved; understanding how activation and gradient combine is essential to see why random gradients and normalization help. Quick check: In TaylorFO, if a neuron has high activation but zero gradient, what is its importance score?

- **Random gradient propagation in neural networks**: The core novelty is replacing true loss gradients with random unit vectors; knowing how gradient flow works in backprop is necessary to understand the modification. Quick check: What changes in the backprop graph when random gradients replace loss gradients at the output layer?

- **Gradient normalization and its effect on optimization**: Normalization ensures equal data point contribution; understanding how normalization changes optimization dynamics is key to seeing why it helps pruning. Quick check: How does normalizing gradients before using them in importance computation affect the scale of importance scores?

## Architecture Onboarding

- **Component map**: Input layer → Network forward pass → Pre-activation extraction → Random gradient injection at output → Backward pass with normalized gradients → Importance score accumulation (abs or sq) → Global pruning step

- **Critical path**: Forward pass to get activations → Random gradient generation and normalization → Backward pass through network → Element-wise product and aggregation → Sorting and pruning neurons

- **Design tradeoffs**:
  - Using random gradients vs true loss gradients: avoids label dependence but may lose task-specific sensitivity
  - Absolute vs squared importance: abs preserves sign information about magnitude changes, sq penalizes large changes more heavily
  - Normalization: prevents high-gradient samples from dominating but may suppress rare-class importance

- **Failure signatures**:
  - Large drop in accuracy after pruning despite high importance scores: indicates random gradients are not capturing relevant task structure
  - Uniform importance scores across neurons: suggests normalization is overcompensating or gradients are too random
  - Over-pruning leading to instability: importance metric may be missing interactions between neurons

- **First 3 experiments**:
  1. Run baseline TaylorFO on a small ResNet with CIFAR-10 and verify pruning curve matches prior literature
  2. Replace loss gradients with random unit gradients, keep original magnitude, measure accuracy drop
  3. Add gradient normalization, compare pruning performance against step 2, verify equal contribution effect

## Open Questions the Paper Calls Out

- **How does the proposed method perform on larger networks like ResNet-50 or DenseNet, and on more complex datasets like ImageNet?**
  The paper mentions testing on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets, but does not explore larger networks or more complex datasets. The experiments conducted are limited to specific architectures and datasets, leaving the scalability and generalizability of the method to larger networks and more complex datasets unexplored.

- **How does the proposed method perform in comparison to other neuron importance estimation methods that do not require labeled examples?**
  The paper focuses on methods that do not require labeled examples and proposes using random gradients, but does not compare its performance to other similar methods. The paper does not provide a direct comparison with other neuron importance estimation methods that also do not require labeled examples, leaving the relative performance of the proposed method unclear.

- **How does the proposed method affect the training time and memory usage of the pruned networks?**
  The paper focuses on the accuracy of the pruned networks but does not discuss the training time and memory usage of the pruned networks. The paper does not provide information on the training time and memory usage of the pruned networks, leaving the practical implications of the proposed method unclear.

## Limitations
- Core innovation of random gradient propagation lacks direct empirical validation in the paper
- Normalization approach assumes equal data point contribution is optimal, which may not hold for imbalanced datasets
- Squared vs absolute value variants show no systematic comparison of their relative merits

## Confidence
- **High confidence**: The basic TaylorFO framework and its mathematical formulation; the claim that gradient normalization affects importance score distribution
- **Medium confidence**: The pruning performance improvements on CIFAR-100 and STL-10; the claim that random gradients avoid label dependence
- **Low confidence**: The mechanism by which random gradients capture neuron importance; the general applicability to other architectures and datasets

## Next Checks
1. **Gradient ablation study**: Compare pruning performance using random gradients vs. true loss gradients on a subset of the data to quantify the information loss from randomization
2. **Normalization sensitivity analysis**: Vary the normalization approach (per-layer, per-data-point, no normalization) and measure impact on pruning accuracy across different architectures
3. **Sign preservation test**: Implement a variant that preserves gradient sign information while still using random gradients, and compare performance to abs and sq variants to understand the role of sign in importance estimation